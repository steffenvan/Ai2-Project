<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000482">
<title confidence="0.93867">
Joint and conditional estimation of tagging and parsing models*
</title>
<author confidence="0.988147">
Mark Johnson
</author>
<affiliation confidence="0.972922">
Brown University
</affiliation>
<email confidence="0.945914">
Mark Johnson@Brown.edu
</email>
<sectionHeader confidence="0.997227" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999985941176471">
This paper compares two different ways
of estimating statistical language mod-
els. Many statistical NLP tagging and
parsing models are estimated by max-
imizing the (joint) likelihood of the
fully-observed training data. How-
ever, since these applications only re-
quire the conditional probability distri-
butions, these distributions can in prin-
ciple be learnt by maximizing the con-
ditional likelihood of the training data.
Perhaps somewhat surprisingly, models
estimated by maximizing the joint were
superior to models estimated by max-
imizing the conditional, even though
some of the latter models intuitively
had access to “more information”.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997749333333333">
Many statistical NLP applications, such as tag-
ging and parsing, involve finding the value
of some hidden variable Y (e.g., a tag or a
parse tree) which maximizes a conditional prob-
ability distribution P(Y X), where X is a
given word string. The model parameters 
are typically estimated by maximum likelihood:
i.e., maximizing the likelihood of the training
*I would like to thank Eugene Charniak and the other
members of BLLIP for their comments and suggestions. Fer-
nando Pereira was especially generous with comments and
suggestions, as were the ACL reviewers; I apologize for not
being able to follow up all of your good suggestions. This re-
search was supported by NSF awards 9720368 and 9721276
and NIH award R01 MH60922-01A2.
data. Given a (fully observed) training cor-
pus D = ((y1, x1), ... , (yn, xn)), the maximum
(joint) likelihood estimate (MLE) of  is:
</bodyText>
<equation confidence="0.537622">
P(yi, xi). (1)
</equation>
<bodyText confidence="0.9992825">
However, it turns out there is another maximum
likelihood estimation method which maximizes
the conditional likelihood or “pseudo-likelihood”
of the training data (Besag, 1975). Maximum
conditional likelihood is consistent for the con-
ditional distribution. Given a training corpus
D, the maximum conditional likelihood estimate
(MCLE) of the model parameters  is:
</bodyText>
<equation confidence="0.613368">
P(yi xi). (2)
</equation>
<bodyText confidence="0.992336736842105">
Figure 1 graphically depicts the difference be-
tween the MLE and MCLE. Let Q be the universe
of all possible pairs (y, x) of hidden and visible
values. Informally, the MLE selects the model
parameter  which make the training data pairs
(yi, xi) as likely as possible relative to all other
pairs (y&apos;, x&apos;) in Q. The MCLE, on the other hand,
selects the model parameter  in order to make the
training data pair (yi, xi) more likely than other
pairs (y&apos;, xi) in Q, i.e., pairs with the same visible
value xi as the training datum.
In statistical computational linguistics, max-
imum conditional likelihood estimators have
mostly been used with general exponential or
“maximum entropy” models because standard
maximum likelihood estimation is usually com-
putationally intractable (Berger et al., 1996; Della
Pietra et al., 1997; Jelinek, 1997). Well-
known computational linguistic models such as
</bodyText>
<equation confidence="0.991680875">
ˆ = argmax

n
i=1
ˆ = argmax

n
i=1
</equation>
<figureCaption confidence="0.997769333333333">
Figure 1: The MLE makes the training data (yi, xi) as
likely as possible (relative to 0), while the MCLE makes
(yi, xi) as likely as possible relative to other pairs (y&apos;, xi).
</figureCaption>
<bodyText confidence="0.999626321839081">
Maximum-Entropy Markov Models (McCallum
et al., 2000) and Stochastic Unification-based
Grammars (Johnson et al., 1999) are standardly
estimated with conditional estimators, and it
would be interesting to know whether conditional
estimation affects the quality of the estimated
model. It should be noted that in practice, the
MCLE of a model with a large number of features
with complex dependencies may yield far better
performance than the MLE of the much smaller
model that could be estimated with the same
computational effort. Nevertheless, as this paper
shows, conditional estimators can be used with
other kinds of models besides MaxEnt models,
and in any event it is interesting to ask whether
the MLE differs from the MCLE in actual appli-
cations, and if so, how.
Because the MLE is consistent for the joint
distribution P(Y, X) (e.g., in a tagging applica-
tion, the distribution of word-tag sequences), it
is also consistent for the conditional distribution
P(Y |X) (e.g., the distribution of tag sequences
given word sequences) and the marginal distribu-
tion P(X) (e.g., the distribution of word strings).
On the other hand, the MCLE is consistent for the
conditional distribution P(Y |X) alone, and pro-
vides no information about either the joint or the
marginal distributions. Applications such as lan-
guage modelling for speech recognition and EM
procedures for estimating from hidden data ei-
ther explicitly or implicitly require marginal dis-
tributions over the visible data (i.e., word strings),
so it is not statistically sound to use MCLEs for
such applications. On the other hand, applications
which involve predicting the value of the hidden
variable from the visible variable (such as tagging
or parsing) usually only involve the conditional
distribution, which the MCLE estimates directly.
Since both the MLE and MCLE are consistent
for the conditional distribution, both converge in
the limit to the “true” distribution if the true dis-
tribution is in the model class. However, given
that we often have insufficient data in computa-
tional linguistics, and there are good reasons to
believe that the true distribution of sentences or
parses cannot be described by our models, there
is no reason to expect these asymptotic results to
hold in practice, and in the experiments reported
below the MLE and MCLE behave differently ex-
perimentally.
A priori, one can advance plausible arguments
in favour of both the MLE and the MCLE. Infor-
mally, the MLE and the MCLE differ in the fol-
lowing way. Since the MLE is obtained by maxi-
mizing Hi PO(yi|xi)PO(xi), the MLE exploits in-
formation about the distribution of word strings xi
in the training data that the MCLE does not. Thus
one might expect the MLE to converge faster than
the MCLE in situations where training data is not
over-abundant, which is often the case in compu-
tational linguistics.
On the other hand, since the intended applica-
tion requires a conditional distribution, it seems
reasonable to directly estimate this conditional
distribution from the training data as the MCLE
does. Furthermore, suppose that the model class
is wrong (as is surely true of all our current lan-
guage models), i.e., the “true” model P(Y, X) =�
PO(Y, X) for all 0, and that our best models are
particularly poor approximations to the true dis-
tribution of word strings P(X). Then ignoring
the distribution of word strings in the training data
as the MCLE does might indeed be a reasonable
thing to do.
The rest of this paper is structured as fol-
lows. The next section formulates the MCLEs
for HMMs and PCFGs as constrained optimiza-
tion problems and describes an iterative dynamic-
programming method for solving them. Because
of the computational complexity of these prob-
lems, the method is only applied to a simple
PCFG based on the ATIS corpus. For this ex-
ample, the MCLE PCFG does perhaps produce
slightly better parsing results than the standard
MLE (relative-frequency) PCFG, although the re-
sult does not reach statistical significance.
It seems to be difficult to find model classes for
</bodyText>
<equation confidence="0.995251666666667">
(MLE)
(MCLE)
Q Y = yi, X = xi
Y = yi, X = xi
Q
X = xi
</equation>
<bodyText confidence="0.99995752173913">
which the MLE and MCLE are both easy to com-
pute. However, often it is possible to find two
closely related model classes, one of which has
an easily computed MLE and the other which has
an easily computed MCLE. Typically, the model
classes which have an easily computed MLE de-
fine joint probability distributions over both the
hidden and the visible data (e.g., over word-
tag pair sequences for tagging), while the model
classes which have an easily computed MCLE de-
fine conditional probability distributions over the
hidden data given the visible data (e.g., over tag
sequences given word sequences).
Section 3 investigates closely related joint
and conditional tagging models (the lat-
ter can be regarded as a simplification of
the Maximum Entropy Markov Models of
McCallum et al. (2000)), and shows that MLEs
outperform the MCLEs in this application. The
final empirical section investigates two different
kinds of stochastic shift-reduce parsers, and
shows that the model estimated by the MLE
outperforms the model estimated by the MCLE.
</bodyText>
<sectionHeader confidence="0.979503" genericHeader="method">
2 PCFG parsing
</sectionHeader>
<bodyText confidence="0.99998275">
In this application, the pairs (y, x) consist of a
parse tree y and its terminal string or yield x (it
may be simpler to think of y containing all of the
parse tree except for the string x). Recall that
in a PCFG with production set R, each produc-
tion (A)  R is associated with a parameter
A. These parameters satisfy a normalization
constraint for each nonterminal A:
</bodyText>
<equation confidence="0.979061">
E A = 1 (3)
:(A)ER
</equation>
<bodyText confidence="0.98976125">
For each production r  R, let fr(y) be the num-
ber of times r is used in the derivation of the tree
y. Then the PCFG defines a probability distribu-
tion over trees:
</bodyText>
<equation confidence="0.7665565">
P(Y ) = H AfA(Y )
(A)ER
</equation>
<bodyText confidence="0.932790166666667">
The MLE for  is the well-known “relative-
frequency” estimator:
Unfortunately the MCLE for a PCFG is more
complicated. If x is a word string, then let (x) be
the set of parse trees with terminal string or yield
x generated by the PCFG. Then given a training
corpus D = ((y1, x1), ... , (yn, xn)), where yi is
a parse tree for the string xi, the log conditional
likelihood of the training data log P(y|x) and its
derivative are given by:
�
�
</bodyText>
<equation confidence="0.866890666666667">
�log P(yi) − log P(y)
y(xi)
(fA-(yi) − E(fA_Jxi))
</equation>
<bodyText confidence="0.999983358974359">
Here E(f|x) denotes the expectation of f with
respect to P conditioned on Y  (x). There
does not seem to be a closed-form solution for
the  that maximizes P(y|x) subject to the con-
straints (3), so we used an iterative numerical gra-
dient ascent method, with the constraints (3) im-
posed at each iteration using Lagrange multipli-
ers. Note that Eni=1 E(fA|xi) is a quantity
calculated in the Inside-Outside algorithm (Lari
and Young, 1990) and P(y|x) is easily computed
as a by-product of the same dynamic program-
ming calculation.
Since the expected production counts E(f|x)
depend on the production weights , the entire
training corpus must be reparsed on each itera-
tion (as is true of the Inside-Outside algorithm).
This is computationally expensive with a large
grammar and training corpus; for this reason the
MCLE PCFG experiments described here were
performed with the relatively small ATIS tree-
bank corpus of air travel reservations distributed
by LDC.
In this experiment, the PCFGs were always
trained on the 1088 sentences of the ATIS1 corpus
and evaluated on the 294 sentences of the ATIS2
corpus. Lexical items were ignored; the PCFGs
generate preterminal strings. The iterative algo-
rithm for the MCLE was initialized with the MLE
parameters, i.e., the “standard” PCFG estimated
from a treebank. Table 1 compares the MLE and
MCLE PCFGs.
The data in table 1 shows that compared to the
MLE PCFG, the MCLE PCFG assigns a higher
conditional probability of the parses in the train-
ing data given their yields, at the expense of as-
signing a lower marginal probability to the yields
themselves. The labelled precision and recall
parsing results for the MCLE PCFG were slightly
higher than those of the MLE PCFG. Because
</bodyText>
<equation confidence="0.964639705882353">
ˆ
A = �n �:(A)�R fA(yi
i=1 Eni=1 fA(yi)
).
log P(yJx) = �n
i=1
 log P(yJx)
A-.
1
=
�n
i=1
A-.
··· � Tj � Tj+1 � ·· ·
··· � Tj �
··· � Tj �
· · �· �
</equation>
<figure confidence="0.903060896551724">
�����������
��
�����������
Wj Wj+1
Wj
Wj
A
Wj


Tj �
A
��
���������
�����������
Wj+1
Wj+1
Tj+1 �
Tj+1 � ��
����������
A �\���
Wj+1
Tj+1 �
A


· · ·
· · ·
· · ·
</figure>
<table confidence="0.994712333333333">
MLE MCLE
− log P(y) 13857 13896
− log P(91x) 1833 1769
− log P(x) 12025 12127
Labelled precision 0.815 0.817
Labelled recall 0.789 0.794
</table>
<tableCaption confidence="0.9620386">
Table 1: The likelihood P(y-) and conditional likelihood
P(-y|-�) of the ATIS1 training trees, and the marginal likeli-
hood P(-�) of the ATIS1 training strings, as well as the la-
belled precision and recall of the ATIS2 test trees, using the
MLE and MCLE PCFGs.
</tableCaption>
<bodyText confidence="0.999690555555556">
both the test data set and the differences are so
small, the significance of these results was esti-
mated using a bootstrap method with the differ-
ence in F-score in precision and recall as the test
statistic (Cohen, 1995). This test showed that the
difference was not significant (p  0.1). Thus the
MCLE PCFG did not perform significantly bet-
ter than the MLE PCFG in terms of precision and
recall.
</bodyText>
<sectionHeader confidence="0.99543" genericHeader="method">
3 HMM tagging
</sectionHeader>
<bodyText confidence="0.999895518518519">
As noted in the previous section, maximizing the
conditional likelihood of a PCFG or a HMM can
be computationally intensive. This section and
the next pursues an alternative strategy for com-
paring MLEs and MCLEs: we compare similiar
(but not identical) model classes, one of which
has an easily computed MLE, and the other of
which has an easily computed MCLE. The appli-
cation considered in this section is bitag POS tag-
ging, but the techniques extend straight-forwardly
to n-tag tagging. In this application, the data pairs
(y, x) consist of a tag sequence y = t1 ... tm
and a word sequence x = w1 ... wm, where tj
is the tag for word wj (to simplify the formu-
lae, w0, t0, wm+1 and tm+1 are always taken to
be end-markers). Standard HMM tagging models
define a joint distribution over word-tag sequence
pairs; these are most straight-forwardly estimated
by maximizing the likelihood of the joint train-
ing distribution. However, it is straight-forward
to devise closely related HMM tagging models
which define a conditional distribution over tag
sequences given word sequences, and which are
most straight-forwardly estimated by maximizing
the conditional likelihood of the distribution of
tag sequences given word sequences in the train-
ing data.
</bodyText>
<figureCaption confidence="0.9971855">
Figure 2: The HMMs depicted as “Bayes net” graphical
models.
</figureCaption>
<bodyText confidence="0.994695090909091">
All of the HMM models investigated in this
section are instances of a certain kind of graph-
ical model that Pearl (1988) calls “Bayes nets”;
Figure 2 sketches the networks that correspond to
all of the models discussed here. (In such a graph,
the set of incoming arcs to a node depicting a vari-
able indicate the set of variables on which this
variable is conditioned).
Recall the standard bitag HMM model, which
defines a joint distribution over word and tag se-
quences:
</bodyText>
<equation confidence="0.9973785">
P(Y,X) = m+1H ˆP(Tj|Tj−1) ˆP(Wj|Tj) (4)
j=1
</equation>
<bodyText confidence="0.999121">
As is well-known, the MLE for (4) sets Pˆ to the
empirical distributions on the training data.
Now consider the following conditional model
of the conditional distribution of tags given words
(this is a simplified form of the model described
in McCallum et al. (2000)):
</bodyText>
<equation confidence="0.973399">
P(Y |X) = m+1H P0(Tj|Wj, Tj−1) (5)
j=1
</equation>
<bodyText confidence="0.986270206896552">
The MCLE of (5) is easily calculated: P0 should
be set the empirical distribution of the training
data. However, to minimize sparse data prob-
lems we estimated P0(Tj|Wj,Tj−1) as a mixture
where the Pˆ are empirical probabilities and the
(bucketted) mixing parameters are determined us-
ing deleted interpolation from heldout data (Je-
linek, 1997).
These models were trained on sections 2-21
of the Penn tree-bank corpus. Section 22 was
used as heldout data to evaluate the interpola-
tion parameters A. The tagging accuracy of the
models was evaluated on section 23 of the tree-
bank corpus (in both cases, the tag t j assigned to
word wj is the one which maximizes the marginal
P(tj  |w1 ... wm), since this minimizes the ex-
pected loss on a tag-by-tag basis).
The conditional model (5) has the worst perfor-
mance of any of the tagging models investigated
in this section: its tagging accuracy is 94.4%. The
joint model (4) has a considerably lower error
rate: its tagging accuracy is 95.5%.
One possible explanation for this result is that
the way in which the interpolated estimate of P0
is calculated, rather than conditional likelihood
estimation per se, is lowering tagger accuracy
somehow. To investigate this possibility, two ad-
ditional joint models were estimated and tested,
based on the formulae below.
</bodyText>
<equation confidence="0.9998565">
ˆP(Wj|Tj)P1(Tj|Wj−1, Tj−1) (6)
P0(Tj|Wj, Tj−1) ˆP(Wj|Tj−1) (7)
</equation>
<bodyText confidence="0.997889115384615">
The MLEs for both (6) and (7) are easy to cal-
culate. (6) contains a conditional distribution P1
which would seem to be of roughly equal com-
plexity to P0, and it was estimated using deleted
interpolation in exactly the same way as P0, so
if the poor performance of the conditional model
was due to some artifact of the interpolation pro-
cedure, we would expect the model based on (6)
to perform poorly. Yet the tagger based on (6)
performs the best of all the taggers investigated in
this section: its tagging accuracy is 96.2%.
(7) is admitted a rather strange model, since
the right hand term in effect predicts the follow-
ing word from the current word’s tag. However,
note that (7) differs from (5) only via the pres-
ence of this rather unusual term, which effectively
converts (5) from a conditional model to a joint
model. Yet adding this term improves tagging ac-
curacy considerably, to 95.3%. Thus for bitag tag-
ging at least, the conditional model has a consid-
erably higher error rate than any of the joint mod-
els examined here. (While a test of significance
was not conducted here, previous experience with
this test set shows that performance differences
of this magnitude are extremely significant statis-
tically).
</bodyText>
<sectionHeader confidence="0.996431" genericHeader="method">
4 Shift-reduce parsing
</sectionHeader>
<bodyText confidence="0.999940388888889">
The previous section compared similiar joint and
conditional tagging models. This section com-
pares a pair of joint and conditional parsing mod-
els. The models are both stochastic shift-reduce
parsers; they differ only in how the distribution
over possible next moves are calculated. These
parsers are direct simplifications of the Structured
Language Model (Jelinek, 2000). Because the
parsers’ moves are determined solely by the top
two category labels on the stack and possibly the
look-ahead symbol, they are much simpler than
stochastic LR parsers (Briscoe and Carroll, 1993;
Inui et al., 1997). The distribution over trees
generated by the joint model is a probabilistic
context-free language (Abney et al., 1999). As
with the PCFG models discussed earlier, these
parsers are not lexicalized; lexical items are ig-
nored, and the POS tags are used as the terminals.
These two parsers only produce trees with
unary or binary nodes, so we binarized the train-
ing data before training the parser, and debina-
rize the trees the parsers produce before evaluat-
ing them with respect to the test data (Johnson,
1998). We binarized by inserting n − 2 additional
nodes into each local tree with n &gt; 2 children.
We binarized by first joining the head to all of the
constituents to its right, and then joining the re-
sulting structure with constituents to the left. The
label of a new node is the label of the head fol-
lowed by the suffix “-1” if the head is (contained
in) the right child or “-2” if the head is (contained
in) the left child. Figure 3 depicts an example of
this transformation.
The Structured Language Model is described
in detail in Jelinek (2000), so it is only reviewed
here. Each parser’s stack is a sequence of node
</bodyText>
<equation confidence="0.976692666666667">
P(Tj I u&apos;j)�
of P (Tj I Wj , Tj _1),
P(Tj ITj_1) and
m+1
P(Y,X) _ H
j=1
m+1
P(Y,X) =
j=1
</equation>
<figure confidence="0.676311">
(a) VP
</figure>
<figureCaption confidence="0.9983795">
Figure 3: The binarization transformation used in the shift-
reduce parser experiments transforms tree (a) into tree (b).
</figureCaption>
<bodyText confidence="0.9940968">
labels (possibly including labels introduced by bi-
narization). In what follows, s1 refers to the top
element of the stack, or ‘*’ if the stack is empty;
similarly s2 refers to the next-to-top element of
the stack or ‘*’ if the stack contains less than two
elements. We also append a ‘*’ to end of the ac-
tual terminal string being parsed (just as with the
HMMs above), as this simplifies the formulation
of the parsers, i.e., if the string to be parsed is
w1 ... wm, then we take wm+1 = *.
A shift-reduce parse is defined in terms of
moves. A move is either shift(w), reduce1(c) or
reduce2(c), where c is a nonterminal label and w
is either a terminal label or ‘*’. Moves are par-
tial functions from stacks to stacks: a shift(w)
move pushes a w onto the top of stack, while a
reducei(c) move pops the top i terminal or non-
terminal labels off the stack and pushes a c onto
the stack. A shift-reduce parse is a sequence of
moves which (when composed) map the empty
stack to the two-element stack whose top element
is ‘*’ and whose next-to-top element is the start
symbol. (Note that the last move in a shift-reduce
parse must always be a shift(*) move; this cor-
responds to the final “accept” move in an LR
parser). The isomorphism between shift-reduce
parses and standard parse trees is well-known
(Hopcroft and Ullman, 1979), and so is not de-
scribed here.
A (joint) shift-reduce parser is defined by
a distribution P(m|s1, s2) over next moves m
given the top and next-to-top stack labels s1
and s2. To ensure that the next move is in
fact a possible move given the current stack,
we require that P(reduce1(c)|*, *) = 0 and
</bodyText>
<equation confidence="0.661206">
P(reduce2(c)|c&apos;, *) = 0 for all c, c&apos;, and that
P(shift(*)|s1, s2) = 0 unless s1 is the start sym-
</equation>
<bodyText confidence="0.998769159090909">
bol and s2 = *. Note that this extends to a
probability distribution over shift-reduce parses
(and hence parse trees) in a particularly simple
way: the probability of a parse is the product of
the probabilities of the moves it consists of. As-
suming that P meets certain tightness conditions,
this distribution over parses is properly normal-
ized because there are no “dead” stack configura-
tions: we require that the distribution over moves
be defined for all possible stacks.
A conditional shift-reduce parser differs only
minimally from the shift-reduce parser just
described: it is defined by a distribution
P(m|s1, s2, t) over next moves m given the top
and next-to-top stack labels s1, s2 and the next
input symbol w (w is called the look-ahead sym-
bol). In addition to the requirements on P
above, we also require that if w&apos; = w then
P(shift(w&apos;)|s1, s2, w) = 0 for all s1, s2; i.e.,
shift moves can only shift the current look-ahead
symbol. This restriction implies that all non-zero
probability derivations are derivations of the parse
string, since the parse string forces a single se-
quence of symbols to be shifted in all derivations.
As before, since there are no “dead” stack con-
figurations, so long as P obeys certain tightness
conditions, this defines a properly normalized dis-
tribution over parses. Since all the parses are re-
quired to be parses of of the input string, this de-
fines a conditional distribution over parses given
the input string.
It is easy to show that the MLE for the joint
model, and the MCLE for the conditional model,
are just the empirical distributions from the train-
ing data. We ran into sparse data problems using
the empirical training distribution as an estimate
for P(m|s1, s2, w) in the conditional model, so
in fact we used deleted interpolation to interpo-
late ˆP(m|s1, s2, w), and ˆP(m|s1, s2) to estimate
P(m|s1, s2, w). The models were estimated from
sections 2–21 of the Penn treebank, and tested on
the 2245 sentences of length 40 or less in section
23. The deleted interpolation parameters were es-
timated using heldout training data from section
</bodyText>
<figure confidence="0.955072208333334">
RB
VBZ-1
eats
pizza
RB
RB
VBZ
NP
usually
only
pizza
eats
ADVP
quickly
VP
usually
RB
VBZ-2
VBZ
quickly
only
ADVP
VBZ-2
NP
</figure>
<table confidence="0.980392666666667">
Joint SR Conditional SR PCFG
Precision 0.666 0.633 0.700
Recall 0.650 0.639 0.657
</table>
<tableCaption confidence="0.9841875">
Table 2: Labelled precision and recall results for joint and
conditional shift-reduce parsers, and for a PCFG.
</tableCaption>
<bodyText confidence="0.998798412698413">
22.
We calculated the most probable parses using
a dynamic programming algorithm based on the
one described in Jelinek (2000). Jelinek notes that
this algorithm’s running time is n6 (where n is the
length of sentence being parsed), and we found
exhaustive parsing to be computationally imprac-
tical. We used a beam search procedure which
thresholded the best analyses of each prefix of the
string being parsed, and only considered analyses
whose top two stack symbols had been observed
in the training data. In order to help guard against
the possibility that this stochastic pruning influ-
enced the results, we ran the parsers twice, once
with a beam threshold of 10−6 (i.e., edges whose
probability was less than 10−6 of the best edge
spanning the same prefix were pruned) and again
with a beam threshold of 10−9. The results of
the latter runs are reported in table 2; the labelled
precision and recall results from the run with the
more restrictive beam threshold differ by less than
0.001, i.e., at the level of precision reported here,
are identical with the results presented in table 2
except for the Precision of the Joint SR parser,
which was 0.665. For comparision, table 2 also
reports results from the non-lexicalized treebank
PCFG estimated from the transformed trees in
sections 2-21 of the treebank; here exhaustive
CKY parsing was used to find the most probable
parses.
All of the precision and recall results, including
those for the PCFG, presented in table 2 are much
lower than those from a standard treebank PCFG;
presumably this is because the binarization trans-
formation depicted in Figure 3 loses informa-
tion about pairs of non-head constituents in the
same local tree (Johnson (1998) reports similiar
performance degradation for other binarization
transformations). Both the joint and the condi-
tional shift-reduce parsers performed much worse
than the PCFG. This may be due to the pruning
effect of the beam search, although this seems
unlikely given that varying the beam threshold
did not affect the results. The performance dif-
ference between the joint and conditional shift-
reduce parsers bears directly on the issue ad-
dressed by this paper: the joint shift-reduce parser
performed much better than the conditional shift-
reduce parser. The differences are around a per-
centage point, which is quite large in parsing re-
search (and certainly highly significant).
The fact that the joint shift-reduce parser out-
performs the conditional shift-reduce parser is
somewhat surprising. Because the conditional
parser predicts its next move on the basis of the
lookahead symbol as well as the two top stack
categories, one might expect it to predict this next
move more accurately than the joint shift-reduce
parser. The results presented here show that this
is not the case, at least for non-lexicalized pars-
ing. The label bias of conditional models may be
responsible for this (Bottou, 1991; Lafferty et al.,
2001).
</bodyText>
<sectionHeader confidence="0.999348" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999972649122807">
This paper has investigated the difference be-
tween maximum likelihood estimation and max-
imum conditional likelihood estimation for three
different kinds of models: PCFG parsers, HMM
taggers and shift-reduce parsers. The results for
the PCFG parsers suggested that conditional es-
timation might provide a slight performance im-
provement, although the results were not statis-
tically significant since computational difficulty
of conditional estimation of a PCFG made it
necessary to perform the experiment on a tiny
training and test corpus. In order to avoid the
computational difficulty of conditional estima-
tion, we compared closely related (but not identi-
cal) HMM tagging and shift-reduce parsing mod-
els, for some of which the maximum likelihood
estimates were easy to compute and for others of
which the maximum conditional likelihood esti-
mates could be easily computed. In both cases,
the joint models outperformed the conditional
models by quite large amounts. This suggests
that it may be worthwhile investigating meth-
ods for maximum (joint) likelihood estimation
for model classes for which only maximum con-
ditional likelihood estimators are currently used,
such as Maximum Entropy models and MEMMs,
since if the results of the experiments presented
in this paper extend to these models, one might
expect a modest performance improvement.
As explained in the introduction, because max-
imum likelihood estimation exploits not just the
conditional distribution of hidden variable (e.g.,
the tags or the parse) conditioned on the visible
variable (the terminal string) but also the marginal
distribution of the visible variable, it is reason-
able to expect that it should outperform maxi-
mum conditional likelihood estimation. Yet it
is counter-intuitive that joint tagging and shift-
reduce parsing models, which predict the next tag
or parsing move on the basis of what seems to
be less information than the corresponding con-
ditional model, should nevertheless outperform
that conditional model, as the experimental re-
sults presented here show. The recent theoreti-
cal and simulation results of Lafferty et al. (2001)
suggest that conditional models may suffer from
label bias (the discovery of which Lafferty et. al.
attribute to Bottou (1991)), which may provide an
insightful explanation of these results.
None of the models investigated here are state-
of-the-art; the goal here is to compare two dif-
ferent estimation procedures, and for that rea-
son this paper concentrated on simple, easily im-
plemented models. However, it would also be
interesting to compare the performance of joint
and conditional estimators on more sophisticated
models.
</bodyText>
<sectionHeader confidence="0.99955" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999752885714286">
Steven Abney, David McAllester, and Fernando
Pereira. 1999. Relating probabilistic grammars and
automata. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 542–549, San Francisco. Morgan Kauf-
mann.
Adam L. Berger, Vincent J. Della Pietra, and
Stephen A. Della Pietra. 1996. A maximum
entropy approach to natural language processing.
Computational Linguistics, 22(1):39–71.
J. Besag. 1975. Statistical analysis of non-lattice data.
The Statistician, 24:179–195.
L´eon Bottou. 1991. Une Approche th´eorique de
l’Apprentissage Connexionniste: Applications a` la
Reconnaissance de la Parole. Ph.D. thesis, Univer-
sit´e de Paris XI.
Ted Briscoe and John Carroll. 1993. Generalized
probabilistic LR parsing of natural language (cor-
pora) with unification-based methods. Computa-
tional Linguistics, 19:25–59.
Paul R. Cohen. 1995. Empirical Methods for Artifi-
cialIntelligence. The MIT Press, Cambridge, Mas-
sachusetts.
Stephen Della Pietra, Vincent Della Pietra, and John
Lafferty. 1997. Inducing features of random fields.
IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 19(4):380–393.
John E. Hopcroft and Jeffrey D. Ullman. 1979. Intro-
duction to Automata Theory, Languages and Com-
putation. Addison-Wesley.
K. Inui, V. Sornlertlamvanich, H. Tanaka, and T. Toku-
naga. 1997. A new formalization of probabilistic
GLR parsing. In Proceedings of the Fifth Interna-
tional Workshop on Parsing Technologies (IWPT-
97), pages 123–134, MIT.
Frederick Jelinek. 1997. Statistical Methods for
Speech Recognition. The MIT Press, Cambridge,
Massachusetts.
Frederick Jelinek. 2000. Stochastic analysis of struc-
tured language modeling. Technical report, Center
for Language and Speech Modeling, Johns Hopkins
University.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi
Chi, and Stefan Riezler. 1999. Estimators for
stochastic “unification-based” grammars. In The
Proceedings of the 37th Annual Conference of the
Association for Computational Linguistics, pages
535–541, San Francisco. Morgan Kaufmann.
Mark Johnson. 1998. PCFG models of linguistic
tree representations. Computational Linguistics,
24(4):613–632.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Machine Learning: Proceedings
of the Eighteenth International Conference (ICML
2001).
K. Lari and S.J. Young. 1990. The estimation
of Stochastic Context-Free Grammars using the
Inside-Outside algorithm. Computer Speech and
Language, 4(35-56).
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum Entropy Markov Mod-
els for information extraction and segmentation. In
Machine Learning: Proceedings of the Seventeenth
International Conference (ICML 2000), pages 591–
598, Stanford, California.
Judea Pearl. 1988. Probabalistic Reasoning in In-
telligent Systems: Networks of Plausible Inference.
Morgan Kaufmann, San Mateo, California.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.936045">
<title confidence="0.999255">and conditional estimation of tagging and parsing</title>
<author confidence="0.999998">Mark Johnson</author>
<affiliation confidence="0.999906">Brown University</affiliation>
<author confidence="0.997902">Mark JohnsonBrown edu</author>
<abstract confidence="0.9965405">This paper compares two different ways of estimating statistical language models. Many statistical NLP tagging and parsing models are estimated by maximizing the (joint) likelihood of the fully-observed training data. However, since these applications only require the conditional probability distributions, these distributions can in principle be learnt by maximizing the conditional likelihood of the training data. Perhaps somewhat surprisingly, models estimated by maximizing the joint were superior to models estimated by maximizing the conditional, even though some of the latter models intuitively had access to “more information”.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
<author>David McAllester</author>
<author>Fernando Pereira</author>
</authors>
<title>Relating probabilistic grammars and automata.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>542--549</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>San Francisco.</location>
<contexts>
<context position="17739" citStr="Abney et al., 1999" startWordPosition="2996" endWordPosition="2999"> a pair of joint and conditional parsing models. The models are both stochastic shift-reduce parsers; they differ only in how the distribution over possible next moves are calculated. These parsers are direct simplifications of the Structured Language Model (Jelinek, 2000). Because the parsers’ moves are determined solely by the top two category labels on the stack and possibly the look-ahead symbol, they are much simpler than stochastic LR parsers (Briscoe and Carroll, 1993; Inui et al., 1997). The distribution over trees generated by the joint model is a probabilistic context-free language (Abney et al., 1999). As with the PCFG models discussed earlier, these parsers are not lexicalized; lexical items are ignored, and the POS tags are used as the terminals. These two parsers only produce trees with unary or binary nodes, so we binarized the training data before training the parser, and debinarize the trees the parsers produce before evaluating them with respect to the test data (Johnson, 1998). We binarized by inserting n − 2 additional nodes into each local tree with n &gt; 2 children. We binarized by first joining the head to all of the constituents to its right, and then joining the resulting struc</context>
</contexts>
<marker>Abney, McAllester, Pereira, 1999</marker>
<rawString>Steven Abney, David McAllester, and Fernando Pereira. 1999. Relating probabilistic grammars and automata. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 542–549, San Francisco. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="2836" citStr="Berger et al., 1996" startWordPosition="446" endWordPosition="449">ects the model parameter  which make the training data pairs (yi, xi) as likely as possible relative to all other pairs (y&apos;, x&apos;) in Q. The MCLE, on the other hand, selects the model parameter  in order to make the training data pair (yi, xi) more likely than other pairs (y&apos;, xi) in Q, i.e., pairs with the same visible value xi as the training datum. In statistical computational linguistics, maximum conditional likelihood estimators have mostly been used with general exponential or “maximum entropy” models because standard maximum likelihood estimation is usually computationally intractable (Berger et al., 1996; Della Pietra et al., 1997; Jelinek, 1997). Wellknown computational linguistic models such as ˆ = argmax  n i=1 ˆ = argmax  n i=1 Figure 1: The MLE makes the training data (yi, xi) as likely as possible (relative to 0), while the MCLE makes (yi, xi) as likely as possible relative to other pairs (y&apos;, xi). Maximum-Entropy Markov Models (McCallum et al., 2000) and Stochastic Unification-based Grammars (Johnson et al., 1999) are standardly estimated with conditional estimators, and it would be interesting to know whether conditional estimation affects the quality of the estimated model. It sh</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Vincent J. Della Pietra, and Stephen A. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Besag</author>
</authors>
<title>Statistical analysis of non-lattice data.</title>
<date>1975</date>
<journal>The Statistician,</journal>
<pages>24--179</pages>
<contexts>
<context position="1839" citStr="Besag, 1975" startWordPosition="285" endWordPosition="286">s and suggestions. Fernando Pereira was especially generous with comments and suggestions, as were the ACL reviewers; I apologize for not being able to follow up all of your good suggestions. This research was supported by NSF awards 9720368 and 9721276 and NIH award R01 MH60922-01A2. data. Given a (fully observed) training corpus D = ((y1, x1), ... , (yn, xn)), the maximum (joint) likelihood estimate (MLE) of  is: P(yi, xi). (1) However, it turns out there is another maximum likelihood estimation method which maximizes the conditional likelihood or “pseudo-likelihood” of the training data (Besag, 1975). Maximum conditional likelihood is consistent for the conditional distribution. Given a training corpus D, the maximum conditional likelihood estimate (MCLE) of the model parameters  is: P(yi xi). (2) Figure 1 graphically depicts the difference between the MLE and MCLE. Let Q be the universe of all possible pairs (y, x) of hidden and visible values. Informally, the MLE selects the model parameter  which make the training data pairs (yi, xi) as likely as possible relative to all other pairs (y&apos;, x&apos;) in Q. The MCLE, on the other hand, selects the model parameter  in order to make the traini</context>
</contexts>
<marker>Besag, 1975</marker>
<rawString>J. Besag. 1975. Statistical analysis of non-lattice data. The Statistician, 24:179–195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L´eon Bottou</author>
</authors>
<title>Une Approche th´eorique de l’Apprentissage Connexionniste: Applications a` la Reconnaissance de la Parole.</title>
<date>1991</date>
<tech>Ph.D. thesis,</tech>
<institution>Universit´e de Paris XI.</institution>
<contexts>
<context position="25995" citStr="Bottou, 1991" startWordPosition="4430" endWordPosition="4431">entage point, which is quite large in parsing research (and certainly highly significant). The fact that the joint shift-reduce parser outperforms the conditional shift-reduce parser is somewhat surprising. Because the conditional parser predicts its next move on the basis of the lookahead symbol as well as the two top stack categories, one might expect it to predict this next move more accurately than the joint shift-reduce parser. The results presented here show that this is not the case, at least for non-lexicalized parsing. The label bias of conditional models may be responsible for this (Bottou, 1991; Lafferty et al., 2001). 5 Conclusion This paper has investigated the difference between maximum likelihood estimation and maximum conditional likelihood estimation for three different kinds of models: PCFG parsers, HMM taggers and shift-reduce parsers. The results for the PCFG parsers suggested that conditional estimation might provide a slight performance improvement, although the results were not statistically significant since computational difficulty of conditional estimation of a PCFG made it necessary to perform the experiment on a tiny training and test corpus. In order to avoid the c</context>
</contexts>
<marker>Bottou, 1991</marker>
<rawString>L´eon Bottou. 1991. Une Approche th´eorique de l’Apprentissage Connexionniste: Applications a` la Reconnaissance de la Parole. Ph.D. thesis, Universit´e de Paris XI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Generalized probabilistic LR parsing of natural language (corpora) with unification-based methods.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--25</pages>
<contexts>
<context position="17599" citStr="Briscoe and Carroll, 1993" startWordPosition="2974" endWordPosition="2977">nificant statistically). 4 Shift-reduce parsing The previous section compared similiar joint and conditional tagging models. This section compares a pair of joint and conditional parsing models. The models are both stochastic shift-reduce parsers; they differ only in how the distribution over possible next moves are calculated. These parsers are direct simplifications of the Structured Language Model (Jelinek, 2000). Because the parsers’ moves are determined solely by the top two category labels on the stack and possibly the look-ahead symbol, they are much simpler than stochastic LR parsers (Briscoe and Carroll, 1993; Inui et al., 1997). The distribution over trees generated by the joint model is a probabilistic context-free language (Abney et al., 1999). As with the PCFG models discussed earlier, these parsers are not lexicalized; lexical items are ignored, and the POS tags are used as the terminals. These two parsers only produce trees with unary or binary nodes, so we binarized the training data before training the parser, and debinarize the trees the parsers produce before evaluating them with respect to the test data (Johnson, 1998). We binarized by inserting n − 2 additional nodes into each local tr</context>
</contexts>
<marker>Briscoe, Carroll, 1993</marker>
<rawString>Ted Briscoe and John Carroll. 1993. Generalized probabilistic LR parsing of natural language (corpora) with unification-based methods. Computational Linguistics, 19:25–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul R Cohen</author>
</authors>
<title>Empirical Methods for ArtificialIntelligence.</title>
<date>1995</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="12091" citStr="Cohen, 1995" startWordPosition="2054" endWordPosition="2055">CLE − log P(y) 13857 13896 − log P(91x) 1833 1769 − log P(x) 12025 12127 Labelled precision 0.815 0.817 Labelled recall 0.789 0.794 Table 1: The likelihood P(y-) and conditional likelihood P(-y|-�) of the ATIS1 training trees, and the marginal likelihood P(-�) of the ATIS1 training strings, as well as the labelled precision and recall of the ATIS2 test trees, using the MLE and MCLE PCFGs. both the test data set and the differences are so small, the significance of these results was estimated using a bootstrap method with the difference in F-score in precision and recall as the test statistic (Cohen, 1995). This test showed that the difference was not significant (p  0.1). Thus the MCLE PCFG did not perform significantly better than the MLE PCFG in terms of precision and recall. 3 HMM tagging As noted in the previous section, maximizing the conditional likelihood of a PCFG or a HMM can be computationally intensive. This section and the next pursues an alternative strategy for comparing MLEs and MCLEs: we compare similiar (but not identical) model classes, one of which has an easily computed MLE, and the other of which has an easily computed MCLE. The application considered in this section is b</context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>Paul R. Cohen. 1995. Empirical Methods for ArtificialIntelligence. The MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
<author>John Lafferty</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1997</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="2863" citStr="Pietra et al., 1997" startWordPosition="451" endWordPosition="454">which make the training data pairs (yi, xi) as likely as possible relative to all other pairs (y&apos;, x&apos;) in Q. The MCLE, on the other hand, selects the model parameter  in order to make the training data pair (yi, xi) more likely than other pairs (y&apos;, xi) in Q, i.e., pairs with the same visible value xi as the training datum. In statistical computational linguistics, maximum conditional likelihood estimators have mostly been used with general exponential or “maximum entropy” models because standard maximum likelihood estimation is usually computationally intractable (Berger et al., 1996; Della Pietra et al., 1997; Jelinek, 1997). Wellknown computational linguistic models such as ˆ = argmax  n i=1 ˆ = argmax  n i=1 Figure 1: The MLE makes the training data (yi, xi) as likely as possible (relative to 0), while the MCLE makes (yi, xi) as likely as possible relative to other pairs (y&apos;, xi). Maximum-Entropy Markov Models (McCallum et al., 2000) and Stochastic Unification-based Grammars (Johnson et al., 1999) are standardly estimated with conditional estimators, and it would be interesting to know whether conditional estimation affects the quality of the estimated model. It should be noted that in pract</context>
</contexts>
<marker>Pietra, Pietra, Lafferty, 1997</marker>
<rawString>Stephen Della Pietra, Vincent Della Pietra, and John Lafferty. 1997. Inducing features of random fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4):380–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John E Hopcroft</author>
<author>Jeffrey D Ullman</author>
</authors>
<title>Introduction to Automata Theory, Languages and Computation.</title>
<date>1979</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="20284" citStr="Hopcroft and Ullman, 1979" startWordPosition="3465" endWordPosition="3468">om stacks to stacks: a shift(w) move pushes a w onto the top of stack, while a reducei(c) move pops the top i terminal or nonterminal labels off the stack and pushes a c onto the stack. A shift-reduce parse is a sequence of moves which (when composed) map the empty stack to the two-element stack whose top element is ‘*’ and whose next-to-top element is the start symbol. (Note that the last move in a shift-reduce parse must always be a shift(*) move; this corresponds to the final “accept” move in an LR parser). The isomorphism between shift-reduce parses and standard parse trees is well-known (Hopcroft and Ullman, 1979), and so is not described here. A (joint) shift-reduce parser is defined by a distribution P(m|s1, s2) over next moves m given the top and next-to-top stack labels s1 and s2. To ensure that the next move is in fact a possible move given the current stack, we require that P(reduce1(c)|*, *) = 0 and P(reduce2(c)|c&apos;, *) = 0 for all c, c&apos;, and that P(shift(*)|s1, s2) = 0 unless s1 is the start symbol and s2 = *. Note that this extends to a probability distribution over shift-reduce parses (and hence parse trees) in a particularly simple way: the probability of a parse is the product of the probabi</context>
</contexts>
<marker>Hopcroft, Ullman, 1979</marker>
<rawString>John E. Hopcroft and Jeffrey D. Ullman. 1979. Introduction to Automata Theory, Languages and Computation. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Inui</author>
<author>V Sornlertlamvanich</author>
<author>H Tanaka</author>
<author>T Tokunaga</author>
</authors>
<title>A new formalization of probabilistic GLR parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth International Workshop on Parsing Technologies (IWPT97),</booktitle>
<pages>123--134</pages>
<contexts>
<context position="17619" citStr="Inui et al., 1997" startWordPosition="2978" endWordPosition="2981">Shift-reduce parsing The previous section compared similiar joint and conditional tagging models. This section compares a pair of joint and conditional parsing models. The models are both stochastic shift-reduce parsers; they differ only in how the distribution over possible next moves are calculated. These parsers are direct simplifications of the Structured Language Model (Jelinek, 2000). Because the parsers’ moves are determined solely by the top two category labels on the stack and possibly the look-ahead symbol, they are much simpler than stochastic LR parsers (Briscoe and Carroll, 1993; Inui et al., 1997). The distribution over trees generated by the joint model is a probabilistic context-free language (Abney et al., 1999). As with the PCFG models discussed earlier, these parsers are not lexicalized; lexical items are ignored, and the POS tags are used as the terminals. These two parsers only produce trees with unary or binary nodes, so we binarized the training data before training the parser, and debinarize the trees the parsers produce before evaluating them with respect to the test data (Johnson, 1998). We binarized by inserting n − 2 additional nodes into each local tree with n &gt; 2 childr</context>
</contexts>
<marker>Inui, Sornlertlamvanich, Tanaka, Tokunaga, 1997</marker>
<rawString>K. Inui, V. Sornlertlamvanich, H. Tanaka, and T. Tokunaga. 1997. A new formalization of probabilistic GLR parsing. In Proceedings of the Fifth International Workshop on Parsing Technologies (IWPT97), pages 123–134, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
</authors>
<title>Statistical Methods for Speech Recognition.</title>
<date>1997</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="2879" citStr="Jelinek, 1997" startWordPosition="455" endWordPosition="456">ng data pairs (yi, xi) as likely as possible relative to all other pairs (y&apos;, x&apos;) in Q. The MCLE, on the other hand, selects the model parameter  in order to make the training data pair (yi, xi) more likely than other pairs (y&apos;, xi) in Q, i.e., pairs with the same visible value xi as the training datum. In statistical computational linguistics, maximum conditional likelihood estimators have mostly been used with general exponential or “maximum entropy” models because standard maximum likelihood estimation is usually computationally intractable (Berger et al., 1996; Della Pietra et al., 1997; Jelinek, 1997). Wellknown computational linguistic models such as ˆ = argmax  n i=1 ˆ = argmax  n i=1 Figure 1: The MLE makes the training data (yi, xi) as likely as possible (relative to 0), while the MCLE makes (yi, xi) as likely as possible relative to other pairs (y&apos;, xi). Maximum-Entropy Markov Models (McCallum et al., 2000) and Stochastic Unification-based Grammars (Johnson et al., 1999) are standardly estimated with conditional estimators, and it would be interesting to know whether conditional estimation affects the quality of the estimated model. It should be noted that in practice, the MCLE of</context>
<context position="14755" citStr="Jelinek, 1997" startWordPosition="2501" endWordPosition="2503">s Pˆ to the empirical distributions on the training data. Now consider the following conditional model of the conditional distribution of tags given words (this is a simplified form of the model described in McCallum et al. (2000)): P(Y |X) = m+1H P0(Tj|Wj, Tj−1) (5) j=1 The MCLE of (5) is easily calculated: P0 should be set the empirical distribution of the training data. However, to minimize sparse data problems we estimated P0(Tj|Wj,Tj−1) as a mixture where the Pˆ are empirical probabilities and the (bucketted) mixing parameters are determined using deleted interpolation from heldout data (Jelinek, 1997). These models were trained on sections 2-21 of the Penn tree-bank corpus. Section 22 was used as heldout data to evaluate the interpolation parameters A. The tagging accuracy of the models was evaluated on section 23 of the treebank corpus (in both cases, the tag t j assigned to word wj is the one which maximizes the marginal P(tj |w1 ... wm), since this minimizes the expected loss on a tag-by-tag basis). The conditional model (5) has the worst performance of any of the tagging models investigated in this section: its tagging accuracy is 94.4%. The joint model (4) has a considerably lower err</context>
</contexts>
<marker>Jelinek, 1997</marker>
<rawString>Frederick Jelinek. 1997. Statistical Methods for Speech Recognition. The MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
</authors>
<title>Stochastic analysis of structured language modeling.</title>
<date>2000</date>
<tech>Technical report,</tech>
<institution>Center for Language and Speech Modeling, Johns Hopkins University.</institution>
<contexts>
<context position="17393" citStr="Jelinek, 2000" startWordPosition="2943" endWordPosition="2944">the joint models examined here. (While a test of significance was not conducted here, previous experience with this test set shows that performance differences of this magnitude are extremely significant statistically). 4 Shift-reduce parsing The previous section compared similiar joint and conditional tagging models. This section compares a pair of joint and conditional parsing models. The models are both stochastic shift-reduce parsers; they differ only in how the distribution over possible next moves are calculated. These parsers are direct simplifications of the Structured Language Model (Jelinek, 2000). Because the parsers’ moves are determined solely by the top two category labels on the stack and possibly the look-ahead symbol, they are much simpler than stochastic LR parsers (Briscoe and Carroll, 1993; Inui et al., 1997). The distribution over trees generated by the joint model is a probabilistic context-free language (Abney et al., 1999). As with the PCFG models discussed earlier, these parsers are not lexicalized; lexical items are ignored, and the POS tags are used as the terminals. These two parsers only produce trees with unary or binary nodes, so we binarized the training data befo</context>
<context position="18674" citStr="Jelinek (2000)" startWordPosition="3168" endWordPosition="3169">e before evaluating them with respect to the test data (Johnson, 1998). We binarized by inserting n − 2 additional nodes into each local tree with n &gt; 2 children. We binarized by first joining the head to all of the constituents to its right, and then joining the resulting structure with constituents to the left. The label of a new node is the label of the head followed by the suffix “-1” if the head is (contained in) the right child or “-2” if the head is (contained in) the left child. Figure 3 depicts an example of this transformation. The Structured Language Model is described in detail in Jelinek (2000), so it is only reviewed here. Each parser’s stack is a sequence of node P(Tj I u&apos;j)� of P (Tj I Wj , Tj _1), P(Tj ITj_1) and m+1 P(Y,X) _ H j=1 m+1 P(Y,X) = j=1 (a) VP Figure 3: The binarization transformation used in the shiftreduce parser experiments transforms tree (a) into tree (b). labels (possibly including labels introduced by binarization). In what follows, s1 refers to the top element of the stack, or ‘*’ if the stack is empty; similarly s2 refers to the next-to-top element of the stack or ‘*’ if the stack contains less than two elements. We also append a ‘*’ to end of the actual ter</context>
<context position="23213" citStr="Jelinek (2000)" startWordPosition="3975" endWordPosition="3976">nn treebank, and tested on the 2245 sentences of length 40 or less in section 23. The deleted interpolation parameters were estimated using heldout training data from section RB VBZ-1 eats pizza RB RB VBZ NP usually only pizza eats ADVP quickly VP usually RB VBZ-2 VBZ quickly only ADVP VBZ-2 NP Joint SR Conditional SR PCFG Precision 0.666 0.633 0.700 Recall 0.650 0.639 0.657 Table 2: Labelled precision and recall results for joint and conditional shift-reduce parsers, and for a PCFG. 22. We calculated the most probable parses using a dynamic programming algorithm based on the one described in Jelinek (2000). Jelinek notes that this algorithm’s running time is n6 (where n is the length of sentence being parsed), and we found exhaustive parsing to be computationally impractical. We used a beam search procedure which thresholded the best analyses of each prefix of the string being parsed, and only considered analyses whose top two stack symbols had been observed in the training data. In order to help guard against the possibility that this stochastic pruning influenced the results, we ran the parsers twice, once with a beam threshold of 10−6 (i.e., edges whose probability was less than 10−6 of the </context>
</contexts>
<marker>Jelinek, 2000</marker>
<rawString>Frederick Jelinek. 2000. Stochastic analysis of structured language modeling. Technical report, Center for Language and Speech Modeling, Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Stuart Geman</author>
<author>Stephen Canon</author>
<author>Zhiyi Chi</author>
<author>Stefan Riezler</author>
</authors>
<title>Estimators for stochastic “unification-based” grammars.</title>
<date>1999</date>
<booktitle>In The Proceedings of the 37th Annual Conference of the Association for Computational Linguistics,</booktitle>
<pages>535--541</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>San Francisco.</location>
<contexts>
<context position="3265" citStr="Johnson et al., 1999" startWordPosition="520" endWordPosition="523">estimators have mostly been used with general exponential or “maximum entropy” models because standard maximum likelihood estimation is usually computationally intractable (Berger et al., 1996; Della Pietra et al., 1997; Jelinek, 1997). Wellknown computational linguistic models such as ˆ = argmax  n i=1 ˆ = argmax  n i=1 Figure 1: The MLE makes the training data (yi, xi) as likely as possible (relative to 0), while the MCLE makes (yi, xi) as likely as possible relative to other pairs (y&apos;, xi). Maximum-Entropy Markov Models (McCallum et al., 2000) and Stochastic Unification-based Grammars (Johnson et al., 1999) are standardly estimated with conditional estimators, and it would be interesting to know whether conditional estimation affects the quality of the estimated model. It should be noted that in practice, the MCLE of a model with a large number of features with complex dependencies may yield far better performance than the MLE of the much smaller model that could be estimated with the same computational effort. Nevertheless, as this paper shows, conditional estimators can be used with other kinds of models besides MaxEnt models, and in any event it is interesting to ask whether the MLE differs f</context>
</contexts>
<marker>Johnson, Geman, Canon, Chi, Riezler, 1999</marker>
<rawString>Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi, and Stefan Riezler. 1999. Estimators for stochastic “unification-based” grammars. In The Proceedings of the 37th Annual Conference of the Association for Computational Linguistics, pages 535–541, San Francisco. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>PCFG models of linguistic tree representations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="18130" citStr="Johnson, 1998" startWordPosition="3066" endWordPosition="3067">symbol, they are much simpler than stochastic LR parsers (Briscoe and Carroll, 1993; Inui et al., 1997). The distribution over trees generated by the joint model is a probabilistic context-free language (Abney et al., 1999). As with the PCFG models discussed earlier, these parsers are not lexicalized; lexical items are ignored, and the POS tags are used as the terminals. These two parsers only produce trees with unary or binary nodes, so we binarized the training data before training the parser, and debinarize the trees the parsers produce before evaluating them with respect to the test data (Johnson, 1998). We binarized by inserting n − 2 additional nodes into each local tree with n &gt; 2 children. We binarized by first joining the head to all of the constituents to its right, and then joining the resulting structure with constituents to the left. The label of a new node is the label of the head followed by the suffix “-1” if the head is (contained in) the right child or “-2” if the head is (contained in) the left child. Figure 3 depicts an example of this transformation. The Structured Language Model is described in detail in Jelinek (2000), so it is only reviewed here. Each parser’s stack is a </context>
<context position="24795" citStr="Johnson (1998)" startWordPosition="4240" endWordPosition="4241">xcept for the Precision of the Joint SR parser, which was 0.665. For comparision, table 2 also reports results from the non-lexicalized treebank PCFG estimated from the transformed trees in sections 2-21 of the treebank; here exhaustive CKY parsing was used to find the most probable parses. All of the precision and recall results, including those for the PCFG, presented in table 2 are much lower than those from a standard treebank PCFG; presumably this is because the binarization transformation depicted in Figure 3 loses information about pairs of non-head constituents in the same local tree (Johnson (1998) reports similiar performance degradation for other binarization transformations). Both the joint and the conditional shift-reduce parsers performed much worse than the PCFG. This may be due to the pruning effect of the beam search, although this seems unlikely given that varying the beam threshold did not affect the results. The performance difference between the joint and conditional shiftreduce parsers bears directly on the issue addressed by this paper: the joint shift-reduce parser performed much better than the conditional shiftreduce parser. The differences are around a percentage point</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Mark Johnson. 1998. PCFG models of linguistic tree representations. Computational Linguistics, 24(4):613–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>Machine Learning: Proceedings of the Eighteenth International Conference (ICML</booktitle>
<contexts>
<context position="26019" citStr="Lafferty et al., 2001" startWordPosition="4432" endWordPosition="4435">which is quite large in parsing research (and certainly highly significant). The fact that the joint shift-reduce parser outperforms the conditional shift-reduce parser is somewhat surprising. Because the conditional parser predicts its next move on the basis of the lookahead symbol as well as the two top stack categories, one might expect it to predict this next move more accurately than the joint shift-reduce parser. The results presented here show that this is not the case, at least for non-lexicalized parsing. The label bias of conditional models may be responsible for this (Bottou, 1991; Lafferty et al., 2001). 5 Conclusion This paper has investigated the difference between maximum likelihood estimation and maximum conditional likelihood estimation for three different kinds of models: PCFG parsers, HMM taggers and shift-reduce parsers. The results for the PCFG parsers suggested that conditional estimation might provide a slight performance improvement, although the results were not statistically significant since computational difficulty of conditional estimation of a PCFG made it necessary to perform the experiment on a tiny training and test corpus. In order to avoid the computational difficulty </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional Random Fields: Probabilistic models for segmenting and labeling sequence data. In Machine Learning: Proceedings of the Eighteenth International Conference (ICML 2001).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lari</author>
<author>S J Young</author>
</authors>
<title>The estimation of Stochastic Context-Free Grammars using the Inside-Outside algorithm.</title>
<date>1990</date>
<journal>Computer Speech and Language,</journal>
<pages>4--35</pages>
<contexts>
<context position="9833" citStr="Lari and Young, 1990" startWordPosition="1645" endWordPosition="1648">r the string xi, the log conditional likelihood of the training data log P(y|x) and its derivative are given by: � � �log P(yi) − log P(y) y(xi) (fA-(yi) − E(fA_Jxi)) Here E(f|x) denotes the expectation of f with respect to P conditioned on Y  (x). There does not seem to be a closed-form solution for the  that maximizes P(y|x) subject to the constraints (3), so we used an iterative numerical gradient ascent method, with the constraints (3) imposed at each iteration using Lagrange multipliers. Note that Eni=1 E(fA|xi) is a quantity calculated in the Inside-Outside algorithm (Lari and Young, 1990) and P(y|x) is easily computed as a by-product of the same dynamic programming calculation. Since the expected production counts E(f|x) depend on the production weights , the entire training corpus must be reparsed on each iteration (as is true of the Inside-Outside algorithm). This is computationally expensive with a large grammar and training corpus; for this reason the MCLE PCFG experiments described here were performed with the relatively small ATIS treebank corpus of air travel reservations distributed by LDC. In this experiment, the PCFGs were always trained on the 1088 sentences of </context>
</contexts>
<marker>Lari, Young, 1990</marker>
<rawString>K. Lari and S.J. Young. 1990. The estimation of Stochastic Context-Free Grammars using the Inside-Outside algorithm. Computer Speech and Language, 4(35-56).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Dayne Freitag</author>
<author>Fernando Pereira</author>
</authors>
<title>Maximum Entropy Markov Models for information extraction and segmentation.</title>
<date>2000</date>
<booktitle>In Machine Learning: Proceedings of the Seventeenth International Conference (ICML</booktitle>
<pages>591--598</pages>
<location>Stanford, California.</location>
<contexts>
<context position="3200" citStr="McCallum et al., 2000" startWordPosition="512" endWordPosition="515">istical computational linguistics, maximum conditional likelihood estimators have mostly been used with general exponential or “maximum entropy” models because standard maximum likelihood estimation is usually computationally intractable (Berger et al., 1996; Della Pietra et al., 1997; Jelinek, 1997). Wellknown computational linguistic models such as ˆ = argmax  n i=1 ˆ = argmax  n i=1 Figure 1: The MLE makes the training data (yi, xi) as likely as possible (relative to 0), while the MCLE makes (yi, xi) as likely as possible relative to other pairs (y&apos;, xi). Maximum-Entropy Markov Models (McCallum et al., 2000) and Stochastic Unification-based Grammars (Johnson et al., 1999) are standardly estimated with conditional estimators, and it would be interesting to know whether conditional estimation affects the quality of the estimated model. It should be noted that in practice, the MCLE of a model with a large number of features with complex dependencies may yield far better performance than the MLE of the much smaller model that could be estimated with the same computational effort. Nevertheless, as this paper shows, conditional estimators can be used with other kinds of models besides MaxEnt models, an</context>
<context position="8024" citStr="McCallum et al. (2000)" startWordPosition="1317" endWordPosition="1320"> the other which has an easily computed MCLE. Typically, the model classes which have an easily computed MLE define joint probability distributions over both the hidden and the visible data (e.g., over wordtag pair sequences for tagging), while the model classes which have an easily computed MCLE define conditional probability distributions over the hidden data given the visible data (e.g., over tag sequences given word sequences). Section 3 investigates closely related joint and conditional tagging models (the latter can be regarded as a simplification of the Maximum Entropy Markov Models of McCallum et al. (2000)), and shows that MLEs outperform the MCLEs in this application. The final empirical section investigates two different kinds of stochastic shift-reduce parsers, and shows that the model estimated by the MLE outperforms the model estimated by the MCLE. 2 PCFG parsing In this application, the pairs (y, x) consist of a parse tree y and its terminal string or yield x (it may be simpler to think of y containing all of the parse tree except for the string x). Recall that in a PCFG with production set R, each production (A)  R is associated with a parameter A. These parameters satisfy a normal</context>
<context position="14371" citStr="McCallum et al. (2000)" startWordPosition="2438" endWordPosition="2441">the networks that correspond to all of the models discussed here. (In such a graph, the set of incoming arcs to a node depicting a variable indicate the set of variables on which this variable is conditioned). Recall the standard bitag HMM model, which defines a joint distribution over word and tag sequences: P(Y,X) = m+1H ˆP(Tj|Tj−1) ˆP(Wj|Tj) (4) j=1 As is well-known, the MLE for (4) sets Pˆ to the empirical distributions on the training data. Now consider the following conditional model of the conditional distribution of tags given words (this is a simplified form of the model described in McCallum et al. (2000)): P(Y |X) = m+1H P0(Tj|Wj, Tj−1) (5) j=1 The MCLE of (5) is easily calculated: P0 should be set the empirical distribution of the training data. However, to minimize sparse data problems we estimated P0(Tj|Wj,Tj−1) as a mixture where the Pˆ are empirical probabilities and the (bucketted) mixing parameters are determined using deleted interpolation from heldout data (Jelinek, 1997). These models were trained on sections 2-21 of the Penn tree-bank corpus. Section 22 was used as heldout data to evaluate the interpolation parameters A. The tagging accuracy of the models was evaluated on section 2</context>
</contexts>
<marker>McCallum, Freitag, Pereira, 2000</marker>
<rawString>Andrew McCallum, Dayne Freitag, and Fernando Pereira. 2000. Maximum Entropy Markov Models for information extraction and segmentation. In Machine Learning: Proceedings of the Seventeenth International Conference (ICML 2000), pages 591– 598, Stanford, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judea Pearl</author>
</authors>
<title>Probabalistic Reasoning in Intelligent Systems: Networks of Plausible Inference.</title>
<date>1988</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Mateo, California.</location>
<contexts>
<context position="13710" citStr="Pearl (1988)" startWordPosition="2327" endWordPosition="2328">e are most straight-forwardly estimated by maximizing the likelihood of the joint training distribution. However, it is straight-forward to devise closely related HMM tagging models which define a conditional distribution over tag sequences given word sequences, and which are most straight-forwardly estimated by maximizing the conditional likelihood of the distribution of tag sequences given word sequences in the training data. Figure 2: The HMMs depicted as “Bayes net” graphical models. All of the HMM models investigated in this section are instances of a certain kind of graphical model that Pearl (1988) calls “Bayes nets”; Figure 2 sketches the networks that correspond to all of the models discussed here. (In such a graph, the set of incoming arcs to a node depicting a variable indicate the set of variables on which this variable is conditioned). Recall the standard bitag HMM model, which defines a joint distribution over word and tag sequences: P(Y,X) = m+1H ˆP(Tj|Tj−1) ˆP(Wj|Tj) (4) j=1 As is well-known, the MLE for (4) sets Pˆ to the empirical distributions on the training data. Now consider the following conditional model of the conditional distribution of tags given words (this is a sim</context>
</contexts>
<marker>Pearl, 1988</marker>
<rawString>Judea Pearl. 1988. Probabalistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann, San Mateo, California.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>