<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.69088">
Collective Generation of Natural Image Descriptions
</title>
<author confidence="0.8832895">
Polina Kuznetsova, Vicente Ordonez, Alexander C. Berg,
Tamara L. Berg and Yejin Choi
</author>
<affiliation confidence="0.957772">
Department of Computer Science
Stony Brook University
</affiliation>
<address confidence="0.952211">
Stony Brook, NY 11794-4400
</address>
<email confidence="0.999594">
{pkuznetsova,vordonezroma,aberg,tlberg,ychoi}@cs.stonybrook.edu
</email>
<sectionHeader confidence="0.989596" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99994045">
We present a holistic data-driven approach
to image description generation, exploit-
ing the vast amount of (noisy) parallel im-
age data and associated natural language
descriptions available on the web. More
specifically, given a query image, we re-
trieve existing human-composed phrases
used to describe visually similar images,
then selectively combine those phrases
to generate a novel description for the
query image. We cast the generation pro-
cess as constraint optimization problems,
collectively incorporating multiple inter-
connected aspects of language composition
for content planning, surface realization
and discourse structure. Evaluation by hu-
man annotators indicates that our final
system generates more semantically cor-
rect and linguistically appealing descrip-
tions than two nontrivial baselines.
</bodyText>
<sectionHeader confidence="0.996398" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999967413043478">
Automatically describing images in natural lan-
guage is an intriguing, but complex AI task, re-
quiring accurate computational visual recogni-
tion, comprehensive world knowledge, and natu-
ral language generation. Some past research has
simplified the general image description goal by
assuming that relevant text for an image is pro-
vided (e.g., Aker and Gaizauskas (2010), Feng
and Lapata (2010)). This allows descriptions to
be generated using effective summarization tech-
niques with relatively surface level image under-
standing. However, such text (e.g., news articles
or encyclopedic text) is often only loosely related
to an image’s specific content and many natu-
ral images do not come with associated text for
summarization.
In contrast, other recent work has focused
more on the visual recognition aspect by de-
tecting content elements (e.g., scenes, objects,
attributes, actions, etc) and then composing de-
scriptions from scratch (e.g., Yao et al. (2010),
Kulkarni et al. (2011), Yang et al. (2011), Li
et al. (2011)), or by retrieving existing whole
descriptions from visually similar images (e.g.,
Farhadi et al. (2010), Ordonez et al. (2011)). For
the latter approaches, it is unrealistic to expect
that there will always exist a single complete de-
scription for retrieval that is pertinent to a given
query image. For the former approaches, visual
recognition first generates an intermediate rep-
resentation of image content using a set of En-
glish words, then language generation constructs
a full description by adding function words and
optionally applying simple re-ordering. Because
the generation process sticks relatively closely
to the recognized content, the resulting descrip-
tions often lack the kind of coverage, creativ-
ity, and complexity typically found in human-
written text.
In this paper, we propose a holistic data-
driven approach that combines and extends the
best aspects of these previous approaches – a)
using visual recognition to directly predict indi-
vidual image content elements, and b) using re-
trieval from existing human-composed descrip-
tions to generate natural, creative, and inter-
</bodyText>
<page confidence="0.984686">
359
</page>
<note confidence="0.9857695">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 359–368,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999837571428572">
esting captions. We also lift the restriction of
retrieving existing whole descriptions by gather-
ing visually relevant phrases which we combine
to produce novel and query-image specific de-
scriptions. By judiciously exploiting the corre-
spondence between image content elements and
phrases, it is possible to generate natural lan-
guage descriptions that are substantially richer
in content and more linguistically interesting
than previous work.
At a high level, our approach can be moti-
vated by linguistic theories about the connection
between reading activities and writing skills,
i.e., substantial reading enriches writing skills,
(e.g., Hafiz and Tudor (1989), Tsang (1996)).
Analogously, our generation algorithm attains a
higher level of linguistic sophistication by read-
ing large amounts of descriptive text available
online. Our approach is also motivated by lan-
guage grounding by visual worlds (e.g., Roy
(2002), Dindo and Zambuto (2010), Monner and
Reggia (2011)), as in our approach the mean-
ing of a phrase in a description is implicitly
grounded by the relevant content of the image.
Another important thrust of this work is col-
lective image-level content-planning, integrating
saliency, content relations, and discourse struc-
ture based on statistics drawn from a large
image-text parallel corpus. This contrasts with
previous approaches that generate multiple sen-
tences without considering discourse flow or re-
dundancy (e.g., Li et al. (2011)). For example,
for an image showing a flock of birds, generating
a large number of sentences stating the relative
position of each bird is probably not useful.
Content planning and phrase synthesis can
be naturally viewed as constraint optimization
problems. We employ Integer Linear Program-
ming (ILP) as an optimization framework that
has been used successfully in other generation
tasks (e.g., Clarke and Lapata (2006), Mar-
tins and Smith (2009), Woodsend and Lapata
(2010)). Our ILP formulation encodes a rich
set of linguistically motivated constraints and
weights that incorporate multiple aspects of the
generation process. Empirical results demon-
strate that our final system generates linguisti-
cally more appealing and semantically more cor-
rect descriptions than two nontrivial baselines.
</bodyText>
<subsectionHeader confidence="0.997384">
1.1 System Overview
</subsectionHeader>
<bodyText confidence="0.999389857142857">
Our system consists of two parts. For a query
image, we first retrieve candidate descriptive
phrases from a large image-caption database us-
ing measures of visual similarity (§2). We then
generate a coherent description from these can-
didates using ILP formulations for content plan-
ning (§4) and surface realization (§5).
</bodyText>
<sectionHeader confidence="0.756384" genericHeader="method">
2 Vision &amp; Phrase Retrieval
</sectionHeader>
<bodyText confidence="0.999233428571429">
For a query image, we retrieve relevant candi-
date natural language phrases by visually com-
paring the query image to database images from
the SBU Captioned Photo Collection (Ordonez
et al., 2011) (1 million photographs with asso-
ciated human-composed descriptions). Visual
similarity for several kinds of image content are
used to compare the query image to images from
the database, including: 1) object detections for
89 common object categories (Felzenszwalb et
al., 2010), 2) scene classifications for 26 com-
mon scene categories (Xiao et al., 2010), and
3) region based detections for stuff categories
(e.g. grass, road, sky) (Ordonez et al., 2011).
All content types are pre-computed on the mil-
lion database photos, and caption parsing is per-
formed using the Berkeley PCFG parser (Petrov
et al., 2006; Petrov and Klein, 2007).
Given a query image, we identify content el-
ements present using the above classifiers and
detectors and then retrieve phrases referring to
those content elements from the database. For
example, if we detect a horse in a query im-
age, then we retrieve phrases referring to vi-
sually similar horses in the database by com-
paring the color, texture (Leung and Malik,
1999), or shape (Dalal and Triggs, 2005; Lowe,
2004) of the detected horse to detected horses
in the database images. We collect four types of
phrases for each query image as follows:
[1] NPs We retrieve noun phrases for each
query object detection (e.g., “the brown cow&amp;quot;)
from database captions using visual similar-
ity between object detections computed as an
equally weighted linear combination of L2 dis-
</bodyText>
<page confidence="0.995162">
360
</page>
<bodyText confidence="0.992517">
tances on histograms of color, texton (Leung and
Malik, 1999), HoG (Dalal and Triggs, 2005) and
SIFT (Lowe, 2004) features.
</bodyText>
<listItem confidence="0.963187227272727">
[2] VPs We retrieve verb phrases for each
query object detection (e.g. “boy running”)
from database captions using the same mea-
sure of visual similarity as for NPs, but restrict-
ing the search to only those database instances
whose captions contain a verb phrase referring
to the object category.
[3] Region/Stuff PPs We collect preposi-
tional phrases for each query stuff detection (e.g.
“in the sky&amp;quot;, “on the road”) by measuring visual
similarity of appearance (color, texton, HoG)
and geometric configuration (object-stuff rela-
tive location and distance) between query and
database detections.
[4] Scene PPs We also collect prepositonal
phrases referring to general image scene context
(e.g. “at the market”, “on hot summer days”,
“in Sweden”) based on global scene similarity
computed using L2 distance between scene clas-
sification score vectors (Xiao et al., 2010) com-
puted on the query and database images.
3 Overview of ILP Formulation
</listItem>
<bodyText confidence="0.99923325">
For each image, we aim to generate multiple
sentences, each sentence corresponding to a sin-
gle distinct object detected in the given image.
Each sentence comprises of the NP for the main
object, and a subset of the corresponding VP,
region/stuff PP, and scene PP retrieved in §2.
We consider four different types of operations
to generate the final description for each image:
</bodyText>
<listItem confidence="0.966885142857143">
T1. Selecting the set of objects to describe (one
object per sentence).
T2. Re-ordering sentences (i.e., re-ordering ob-
jects).
T3. Selecting the set of phrases for each sen-
tence.
T4. Re-ordering phrases within each sentence.
</listItem>
<bodyText confidence="0.980309">
The ILP formulation of §4 addresses T1 &amp; T2,
i.e., content-planning, and the ILP of §5 ad-
dresses T3 &amp; T4, i.e., surface realization.1
</bodyText>
<footnote confidence="0.7932545">
&apos;It is possible to create one conjoined ILP formulation
to address all four operations T1—T4 at once. For com-
</footnote>
<sectionHeader confidence="0.897917" genericHeader="method">
4 Image-level Content Planning
</sectionHeader>
<bodyText confidence="0.9999466">
First we describe image-level content planning,
i.e., abstract generation. The goals are to (1) se-
lect a subset of the objects based on saliency and
semantically compatibility, and (2) order the se-
lected objects based on their content relations.
</bodyText>
<subsectionHeader confidence="0.941995">
4.1 Variables and Objective Function
</subsectionHeader>
<bodyText confidence="0.971163666666667">
The following set of indicator variables encodes
the selection of objects and ordering:
I 1, if object s is selected
for position k
0, otherwise
where k = 1, ..., S encodes the position (order)
of the selected objects, and s indexes one of the
objects. In addition, we define a set of variables
indicating specific pairs of adjacent objects:
</bodyText>
<equation confidence="0.9620265">
yskt(k+1) = 1, if ysk = yt(k+1) = 1 (2)
�0, otherwise
</equation>
<bodyText confidence="0.99980375">
The objective function, F, that we will maxi-
mize is a weighted linear combination of these
indicator variables and can be optimized using
integer linear programming:
</bodyText>
<equation confidence="0.996599">
yskt(k+1) (3)
</equation>
<bodyText confidence="0.999967142857143">
where Fs quantifies the salience/confidence of
the object s, and Fst quantifies the seman-
tic compatibility between the objects s and t.
These coefficients (weights) will be described in
§4.3 and §4.4. We use IBM CPLEX to optimize
this objective function subject to the constraints
introduced next in §4.2.
</bodyText>
<subsectionHeader confidence="0.978351">
4.2 Constraints
</subsectionHeader>
<bodyText confidence="0.99886875">
Consistency Constraints: We enforce consis-
tency between indicator variables for indivisual
objects (Eq. 1) and consecutive objects (Eq. 2)
so that yskt(k+1) = 1 iff ysk = 1 and yt(k+1) = 1:
</bodyText>
<equation confidence="0.999494">
dstk, yskt(k+1) C ysk (4)
yskt(k+1) C yt(k+1) (5)
yskt(k+1) + (1 − ysk) + (1 − yt(k+1)) &gt; 1 (6)
</equation>
<bodyText confidence="0.98866">
putational and implementation efficiency however, we opt
for the two-step approach.
</bodyText>
<equation confidence="0.9585078">
ysk =
(1)
EF = Fs · S Eysk − Fst · S−1E
s E st k=1
k=1
</equation>
<page confidence="0.941762">
361
</page>
<bodyText confidence="0.866889142857143">
We then transform
Fst so that
then set Fst = 1 −
Fst ∈ [0,1], and
Fst so that smaller values
To avoid empty descriptions, we enforce that the
result includes at least one object:
</bodyText>
<equation confidence="0.968949">
X
ys1 = 1 (7)
s
To enforce contiguous positions be selected:
X∀k = 2,..., S − 1, Xys(k+1) ≤ ysk (8)
s s
</equation>
<bodyText confidence="0.999396666666667">
Discourse constraints: To avoid spurious de-
scriptions, we allow at most two objects of the
same type, where cs is the type of object s:
</bodyText>
<equation confidence="0.5143462">
S
∀c ∈ objTypes, X X ysk ≤ 2 (9)
{s: cs=c} k=1
4.3 Weight Fs: Object Detection
Confidence
</equation>
<bodyText confidence="0.9994475">
In order to quantify the confidence of the object
detector for the object s, we define 0 ≤ Fs ≤ 1
as the mean of the detector scores for that object
type in the image.
</bodyText>
<subsectionHeader confidence="0.8362285">
4.4 Weight Fst: Ordering and
Compatibility
</subsectionHeader>
<bodyText confidence="0.99745125">
The weight 0 ≤ Fst ≤ 1 quantifies the compat-
ibility of the object pairing (s, t). Note that in
the objective function, we subtract this quan-
tity from the function to be maximized. This
way, we create a competing tension between the
single object selection scores and the pairwise
compatibility scores, so that variable number of
objects can be selected.
</bodyText>
<subsectionHeader confidence="0.666964">
Object Ordering Statistics: People have bi-
</subsectionHeader>
<bodyText confidence="0.999964384615385">
ases on the order of topic or content flow. We
measure these biases by collecting statistics on
ordering of object names from the 1 million im-
age descriptions in the SBU Captioned Dataset
(Ordonez et al., 2011). Let ford(w1, w2) be
the number of times w1 appeared before w2.
For instance, ford(window, house) = 2895 and
ford(house, window) = 1250, suggesting that
people are more likely to mention a window be-
fore mentioning a house/building2. We use these
ordering statistics to enhance content flow. We
define score for the order of objects using Z-score
for normalization as follows:
</bodyText>
<equation confidence="0.957787">
F^ ford(cs,ct) − mean (ford) (10)
st = std dev(ford)
</equation>
<bodyText confidence="0.387442">
2We take into account synonyms.
correspond to better choices.
</bodyText>
<sectionHeader confidence="0.665422" genericHeader="method">
5 Surface Realization
</sectionHeader>
<bodyText confidence="0.999902222222222">
Recall that for each image, the computer vi-
sion system identifies phrases from descriptions
of images that are similar in a variety of aspects.
The result is a set of phrases representing four
different types of information (§2). From this
assortment of phrases, we aim to select a subset
and glue them together to compose a complete
sentence that is linguistically plausible and se-
mantically truthful to the content of the image.
</bodyText>
<subsectionHeader confidence="0.55992">
5.1 Variables and Objective Function
</subsectionHeader>
<bodyText confidence="0.990781722222222">
The following set of variables encodes the selec-
tion of phrases and their ordering in construct-
ing S&apos; sentences.
1, if phrase i of type j
is selected
for position k (11)
in sentence s
0, otherwise
where k = 1, ..., N encodes the ordering of the
selected phrases, and j indexes one of the four
phrases types (object-NPs, action-VPs, region-
PPs, scene-PPs), i = 1, ..., M indexes one of
the M candidate phrases of each phrase type,
and s = 1, ..., S&apos; encodes the sentence (object).
In addition, we define indicator variables for
adjacent pairs of phrases: xsijkpq(k+1) = 1 if
xsijk = xspq(k+1) = 1 and 0 otherwise. Finally,
we define the objective function F as:
</bodyText>
<equation confidence="0.997696">
XF =
sij
xsijkpq(k+1) (12)
</equation>
<bodyText confidence="0.99998">
where Fsij weights individual phrase goodness
and Fsijpq adjacent phrase goodness. All coeffi-
cients (weights) will be described in Section 5.3
and 5.4.
We optionally prepend the first sentence in a
generated description with a cognitive phrase.3
</bodyText>
<footnote confidence="0.746414666666667">
3We collect most frequent 200 phrases of length 1-
7 that start a caption from the SBU Captioned Photo
Collection.
</footnote>
<equation confidence="0.839384545454546">
⎧
⎨⎪⎪⎪⎪
⎪⎪⎪⎪⎩
xsijk =
N
X
k=1
Fsij ·
xsijk
X− Fsijpq · N−1X
sijpq k=1
</equation>
<page confidence="0.984178">
362
</page>
<bodyText confidence="0.988928818181818">
HMM: These was taken on the flowers growing in a rock garden in the field in two sorts. This
little flower sprouted up in defiance in the field in two sorts. A full open flower sprouted up in
defiance in the field in gardens. Bright yellow flowers growing in a rock garden in the field.
ILP: Found trucks parked on first avenue in the east village.
HMM: This is the first cellar door left back bedroom in center
and clothes dryer to the right to the building in the house.
This HUGE screen hanging on the wall outside a burned down
building in the house. My truck parked on first avenue in the
east village by the glass buildings in the house.
Human: Flat bed Chisholms truck on display at the vintage
vehicle rall y at Astley Green Colliery near Leigh Lancs
</bodyText>
<sectionHeader confidence="0.490678" genericHeader="method">
ILP:
</sectionHeader>
<bodyText confidence="0.829467">
This is a photo of this little flower sprouted up in defiance against grass.
Bright yellow flowers growing in a rock garden at Volcan Mombacho.
Human: Yellow flower in my field
ILP: I think this is a boy’s bike
lied in saltwater for quite a
while.
HMM: I liked the way bicycles
leaning against a wall in
Copenhagen Denmark in a
windy sky in a Singapore
bathroom. Boy’s bike lied in
saltwater for quite a while in a
windy sky in a Singapore
bathroom. Fruit rubbing his
face in the encrusted snow in a
windy sky in a Singapore
bathroom.
Human: You re nobody in
Oxford, unless you have a old
bike with a basket
</bodyText>
<figureCaption confidence="0.994645833333333">
Figure 1: ILP &amp; HMM generated captions. In HMM generated captions, underlined phrases show redundancy
across different objects (due to lack of discourse constraints), and phrases in boldface show awkward topic
flow (due to lack of content planning). Note that in the bicycle image, the visual recognizer detected two
separate bicycles and some fruits, as can be seen in the HMM result. Via collective image-level content
planning (see §4), some of these erroneous detection can be corrected, as shown in the ILP result. Spurious
and redundant phrases can be suppressed via discourse constraints (see §5).
</figureCaption>
<bodyText confidence="0.9998862">
These are generic constructs that are often used
to start a description about an image, for in-
stance, “This is an image of...&amp;quot;. We treat these
phrases as an additional type, but omit corre-
sponding variables and constraints for brevity.
</bodyText>
<subsectionHeader confidence="0.947726">
5.2 Constraints
</subsectionHeader>
<bodyText confidence="0.82707325">
Consistency Constraints: First we enforce
consistency between the unary variables (Eq.
11) and the pairwise variables so that xsijkpqm =
1 iff xsijk = 1 and xspqm = 1:
</bodyText>
<equation confidence="0.797119333333333">
∀ijkpqm, xsijkpqm ≤ xsijk (13)
xsijkpqm ≤ xspqm
xsijkpqm + (1 − xsijk) + (1 − xspqm) ≥ 1
</equation>
<bodyText confidence="0.9997663">
Next we include constraints similar to Eq. 8
(contiguous slots are filled), but omit them for
brevity. Finally, we add constraints to ensure at
least two phrases are selected for each sentence,
to promote informative descriptions.
Linguistic constraints: We include linguisti-
cally motivated constraints to generate syntacti-
cally and semantically plausible sentences. First
we enforce a noun-phrase to be selected to en-
sure semantic relevance to the image:
</bodyText>
<equation confidence="0.9358085">
X∀s, xsiNPk = 1 (16)
ik
</equation>
<bodyText confidence="0.841906">
Also, to avoid content redundancy, we allow at
most one phrase of each type:
</bodyText>
<equation confidence="0.997812666666667">
N
X∀sj, X xsijk ≤ 1 (17)
i k=1
</equation>
<bodyText confidence="0.997668">
Discourse constraints: We allow at most
one prepositional scene phrase for the whole de-
scription to avoid redundancy:
</bodyText>
<equation confidence="0.778807">
XFor j = PPscene, xsijk ≤ 1 (18)
sik
</equation>
<bodyText confidence="0.971142">
We add constraints that prevent the inclusion of
more than one phrase with identical head words:
∀s, ij, pq with the same heads,
xspqk ≤ 1 (19)
</bodyText>
<subsectionHeader confidence="0.993359">
5.3 Unary Phrase Selection
</subsectionHeader>
<bodyText confidence="0.999989285714286">
Let Msij be the confidence score for phrase
xsij given by the image–phrase matching al-
gorithm (§2). To make the scores across dif-
ferent phrase types comparable, we normalize
them using Z-score: Fsij = norm&apos;(Msij) =
(Msij − meanj)/devj, and then transform the
values into the range of [0,1].
</bodyText>
<subsectionHeader confidence="0.977864">
5.4 Pairwise Phrase Cohesion
</subsectionHeader>
<bodyText confidence="0.9950175">
In this section, we describe the pairwise phrase
cohesion score Fsijpq defined for each xsijpq in
</bodyText>
<equation confidence="0.997123571428572">
N
X
k=1
xsijk +
N
X
k=1
</equation>
<page confidence="0.992526">
363
</page>
<bodyText confidence="0.574020157894737">
ILP: This is a photo of this bird hopping
around eating things off of the ground by
river.
ILP: This is a sporty little red convertible made for
a great day in Key West FL. This car was in the 4th
parade of the apartment buildings.
Human: IMG_6892 Lookn up in the sky its a
bird its a plane its ah you
Human: Hard rock casino exotic car show in June
Human: H happily rests his armpit on a
warm Gatorade bottle of water (a small
bottle wrapped in a rag)
ILP: Taken in front of my cat sitting in a shoe
box. Cat likes hanging around in my recliner.
Human: Car was raised on the wall over a bridge
facing traffic..paramedics were attending the
driver on the ground
ILP: I like the way the clouds hanging down by
the ground in Dupnitsa of Avikwalal.
</bodyText>
<figureCaption confidence="0.996947">
Figure 2: In some cases (16%), ILP generated captions were preferred over human written ones!
</figureCaption>
<bodyText confidence="0.999615625">
the objective function (Eq. 12). Via Fsijpq,
we aim to quantify the degree of syntactic and
semantic cohesion across two phrases xsij and
xspq. Note that we subtract this cohesion score
from the objective function. This trick helps the
ILP solver to generate sentences with varying
number of phrases, rather than always selecting
the maximum number of phrases allowed.
</bodyText>
<subsectionHeader confidence="0.410684">
N-gram Cohesion Score: We use n-gram
</subsectionHeader>
<bodyText confidence="0.99962125">
statistics from the Google Web 1-T dataset
(Brants and Franz., 2006) Let Lsijpq be the set
of all n-grams (2 &lt; n &lt; 5) across xsij and xspq.
Then the n-gram cohesion score is computed as:
</bodyText>
<equation confidence="0.9993864">
E NPMI(l)
lELg;jpq
size(Lsijpq)
PMI(ngr) − PMImin
NPMI(ngr) = (21) PMImax − PMImin
</equation>
<bodyText confidence="0.952431">
Where NPMI is the normalized point-wise mu-
tual information.4
Co-occurrence Cohesion Score: To cap-
ture long-distance cohesion, we introduce a co-
occurrence-based score, which measures order-
preserved co-occurrence statistics between the
head words hsij and hspq 5. Let fΣ(hsij, hspq)
be the sum frequency of all n-grams that start
with hsij, end with hspq and contain a prepo-
sition prep(spq) of the phrase spq. Then the
</bodyText>
<footnote confidence="0.938546666666667">
4We include the n-gram cohesion for the sentence
boundaries as well, by approximating statistics for sen-
tence boundaries with punctuation marks in the Google
Web 1-T data.
5For simplicity, we use the last word of a phrase as
the head word, except VPs where we take the main verb.
</footnote>
<equation confidence="0.950843333333333">
co-occurrence cohesion is computed as:
FCO — max(fr) − fr(hsij, hspq) (22)
sijpq — max(fr) − min(fr)
</equation>
<bodyText confidence="0.996801692307692">
Final Cohesion Score: Finally, the pairwise
phrase cohesion score Fijpq is a weighted sum of
n-gram and co-occurrence cohesion scores:
where α and β can be tuned via grid search,
and F gRAm and F ° are normalized E [0, 1]
for comparability. Notice that Fsijpq is in the
range [0,1] as well.
TestSet: Because computer vision is a challeng-
ing and unsolved problem, we restrict our query
set to images where we have high confidence that
visual recognition algorithms perform well. We
collect 1000 test images by running a large num-
ber (89) of object detectors on 20,000 images
and selecting images that receive confident ob-
ject detection scores, with some preference for
images with multiple object detections to obtain
good examples for testing discourse constraints.
Baselines: We compare our ILP approaches
with two nontrivial baselines: the first is an
HMM approach (comparable to Yang et al.
(2011)), which takes as input the same set of
candidate phrases described in §2, but for de-
coding, we fix the ordering of phrases as [ NP
– VP – Region PP – Scene PP] and find the
best combination of phrases using the Viterbi
algorithm. We use the same rich set of pairwise
</bodyText>
<equation confidence="0.846635666666667">
FNGRAM = 1
sijpq
Fsijpq =
α · FNGRAM
sijpq + β · FCO
sijpq
(23)
α + β
(20) 6 Evaluation
</equation>
<page confidence="0.996014">
364
</page>
<table confidence="0.998531666666667">
cognitive phrases: HMM HMM ILP ILP
with w/o with w/o
0.111 0.114 0.114 0.116
</table>
<tableCaption confidence="0.95228">
Table 1: Automatic Evaluation
</tableCaption>
<table confidence="0.998498666666667">
ILP selection rate
ILP V.S. HMM (w/o cogn) 67.2%
ILP V.S. HMM (with cogn) 66.3%
</table>
<tableCaption confidence="0.946513">
Table 2: Human Evaluation (without images)
</tableCaption>
<table confidence="0.9996578">
ILP selection rate
ILP V.S. HMM (w/o cogn) 53.17%
ILP V.S. HMM (with cogn) 54.5%
ILP V.S. RETRIEVAL 71.8%
ILP V.S. Human 16%
</table>
<tableCaption confidence="0.999818">
Table 3: Human Evaluation (with images)
</tableCaption>
<bodyText confidence="0.985794269230769">
phrase cohesion scores (§5.4) used for the ILP
formulation, producing a strong baselines.
The second baseline is a recent RETRIEVAL
based description method (Ordonez et al., 2011),
that searches the large parallel corpus of im-
ages and captions, and transfers a caption from
a visually similar database image to the query.
This again is a very strong baseline, as it ex-
ploits the vast amount of image-caption data,
and produces a description high in linguistic
quality (since the captions were written by hu-
man annotators).
Automatic Evaluation: Automatically quan-
tifying the quality of machine generated sen-
tences is known to be difficult. BLEU score
(Papineni et al., 2002), despite its simplicity
and limitations, has been one of the common
choices for automatic evaluation of image de-
scriptions (Farhadi et al., 2010; Kulkarni et al.,
2011; Li et al., 2011; Ordonez et al., 2011), as
it correlates reasonably well with human evalu-
ation (Belz and Reiter, 2006).
Table 1 shows the the BLEU 01 against the
original caption of 1000 images. We see that the
ILP improves the score over HMM consistently,
with or without the use of cognitive phrases.
</bodyText>
<footnote confidence="0.994408">
6Including other long-distance scores in HMM decod-
ing would make the problem NP-hard and require more
sophisticated decoding, e.g. ILP.
</footnote>
<table confidence="0.99935675">
Grammar Cognitive Relevance
HMM 3.40(u =.82) 3.40(u =.88) 2.25(u =1.37)
ILP 3.56(u =.90) 3.60(u =.98) 2.37(u =1.49)
Hum. 4.36(u =.79) 4.77(u =.66) 3.86(u =1.60)
</table>
<tableCaption confidence="0.9802985">
Table 4: Human Evaluation: Multi-Aspect Rating
(u is a standard deviation)
</tableCaption>
<bodyText confidence="0.984244631578947">
Human Evaluation I – Ranking: We com-
plement the automatic evaluation with Mechan-
ical Turk evaluation. In ranking evaluation, we
ask raters to choose a better caption between
two choices7. We do this rating with and with-
out showing the images, as summarized in Ta-
ble 2 &amp; 3. When images are shown, raters evalu-
ate content relevance as well as linguistic quality
of the captions. Without images, raters evaluate
only linguistic quality.
We found that raters generally prefer ILP gen-
erated captions over HMM generated ones, twice
as much (67.2% ILP V.S. 32.8% HMM), if im-
ages are not presented. However the difference is
less pronounced when images are shown. There
could be two possible reasons. The first is that
when images are shown, the Turkers do not try
as hard to tell apart the subtle difference be-
tween the two imperfect captions. The second
is that the relative content relevance of ILP gen-
erated captions is negating the superiority in lin-
guistic quality. We explore this question using
multi-aspect rating, described below.
Note that ILP generated captions are exceed-
ingly (71.8 %) preferred over the RETRIEVAL
baseline (Ordonez et al., 2011), despite the gen-
erated captions tendency to be more prone to
grammatical and cognitive errors than retrieved
ones. This indicates that the generated captions
must have substantially better content relevance
to the query image, supporting the direction of
this research. Finally, notice that as much as
16% of the time, ILP generated captions are pre-
ferred over the original human generated ones
(examples in Figure 2).
Human Evaluation II – Multi-Aspect Rat-
ing: Table 4 presents rating in the 1–5 scale (5:
perfect, 4: almost perfect, 3: 70∼80% good, 2:
</bodyText>
<page confidence="0.977106">
7 W present two captions in a randomized order.
365
</page>
<figureCaption confidence="0.993935">
Figure 3: Examples with different aspects of prob-
lems in the ILP generated captions.
</figureCaption>
<bodyText confidence="0.998645444444444">
50∼70% good, 1: totally bad) in three different
aspects: grammar, cognitive correctness,8 and
relevance. We find that ILP improves over HMM
in all aspects, however, the relevance score is no-
ticeably worse than scores of two other criteria.
It turns out human raters are generally more
critical against the relevance aspect, as can be
seen in the ratings given to the original human
generated captions.
</bodyText>
<sectionHeader confidence="0.556638" genericHeader="method">
Discussion with Examples: Figure 1 shows
</sectionHeader>
<bodyText confidence="0.999574272727273">
contrastive examples of HMM vs ILP gener-
ated captions. Notice that HMM captions
look robotic, containing spurious and redundant
phrases due to lack of discourse constraints, and
often discussing an awkward set of objects due
to lack of image-level content planning. Also
notice how image-level content planning under-
pinned by language statistics helps correct some
of the erroneous vision detections. Figure 3
shows some example mistakes in the ILP gen-
erated captions.
</bodyText>
<sectionHeader confidence="0.999425" genericHeader="conclusions">
7 Related Work &amp; Discussion
</sectionHeader>
<bodyText confidence="0.997650740740741">
Although not directly focused on image descrip-
tion generation, some previous work in the realm
of summarization shares the similar problem of
content planning and surface realization. There
&apos;E.g., “A desk on top of a cat” is grammatically cor-
rect, but cognitively absurd.
are subtle, but important differences however.
First, sentence compression is hardly the goal
of image description generation, as human writ-
ten descriptions are not necessarily succinct.9
Second, unlike summarization, we are not given
with a set of coherent text snippet to begin with,
and the level of noise coming from the visual
recognition errors is much higher than that of
starting with clean text. As a result, choosing
an additional phrase in the image description is
much riskier than it is in summarization.
Some recent research proposed very elegant
approaches to summarization using ILP for col-
lective content planning and/or surface realiza-
tion (e.g., Martins and Smith (2009), Woodsend
and Lapata (2010), Woodsend et al. (2010)).
Perhaps the most important difference in our
approach is the use of negative weights in the
objective function to create the necessary ten-
sion between selection (salience) and compatibil-
ity, which makes it possible for ILP to generate
variable length descriptions, effectively correct-
ing some of the erroneous vision detections. In
contrast, all previous work operates with a pre-
defined upper limit in length, hence the ILP was
formulated to include as many textual units as
possible modulo constraints.
To conclude, we have presented a collective
approach to generating natural image descrip-
tions. Our approach is the first to systematically
incorporate state of the art computer vision
to retrieve visually relevant candidate phrases,
then produce images descriptions that are sub-
stantially more complex and human-like than
previous attempts.
Acknowledgments T. L. Berg is supported
in part by NSF CAREER award #1054133; A.
C. Berg and Y. Choi are partially supported by
the Stony Brook University Office of the Vice
President for Research. We thank K. Yam-
aguchi, X. Han, M. Mitchell, H. Daume III, A.
Goyal, K. Stratos, A. Mensch, J. Dodge for data
pre-processing and useful initial discussions.
9On a related note, the notion of saliency also differs
in that human written captions often digress on details
that might be tangential to the visible content of the
image. E.g., “This is a dress my mom made.”, where the
picture does not show a woman making the dress.
</bodyText>
<figure confidence="0.875995">
Found MIT boy
gave me this
quizical expression.
Grammar Problems
Content Irrelevance
One of the most shirt
in the wall of the
house.
Here you can see a
bright red flower taken
near our apartment in
Torremolinos the Costa
Del Sol.
This is a shoulder bag with
a blended rainbow effect.
Here you can see a cross
by the frog in the sky.
Cognitive Absurdity
</figure>
<page confidence="0.992619">
366
</page>
<sectionHeader confidence="0.9624" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999356981132076">
Ahmet Aker and Robert Gaizauskas. 2010. Gen-
erating image descriptions using dependency rela-
tional patterns. In ACL.
Anja Belz and Ehud Reiter. 2006. Comparing au-
tomatic and human evaluation of nlg systems.
In EACL 2006, 11st Conference of the European
Chapter of the Association for Computational Lin-
guistics, Proceedings of the Conference, April 3-7,
2006, Trento, Italy. The Association for Computer
Linguistics.
Thorsten Brants and Alex Franz. 2006. Web 1t 5-
gram version 1. In Linguistic Data Consortium.
James Clarke and Mirella Lapata. 2006. Constraint-
based sentence compression: An integer program-
ming approach. In Proceedings of the COL-
ING/ACL 2006 Main Conference Poster Sessions,
pages 144–151, Sydney, Australia, July. Associa-
tion for Computational Linguistics.
Navneet Dalal and Bill Triggs. 2005. Histograms of
oriented gradients for human detection. In Pro-
ceedings of the 2005 IEEE Computer Society Con-
ference on Computer Vision and Pattern Recogni-
tion (CVPR&apos;05) - Volume 1 - Volume 01, CVPR
&apos;05, pages 886–893, Washington, DC, USA. IEEE
Computer Society.
Haris Dindo and Daniele Zambuto. 2010. A prob-
abilistic approach to learning a visually grounded
language model through human-robot interaction.
In IROS, pages 790–796. IEEE.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every
picture tells a story: generating sentences for im-
ages. In ECCV.
Pedro F. Felzenszwalb, Ross B. Girshick, David
McAllester, and Deva Ramanan. 2010. Object
detection with discriminatively trained part based
models. tPAMI, Sept.
Yansong Feng and Mirella Lapata. 2010. How many
words is a picture worth? automatic caption gen-
eration for news images. In ACL.
Fateh Muhammad Hafiz and Ian Tudor. 1989. Ex-
tensive reading and the development of language
skills. ELT Journal, 43(1):4–13.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar,
Siming Li, Yejin Choi, Alexander C Berg, and
Tamara L Berg. 2011. Babytalk: Understand-
ing and generating simple image descriptions. In
CVPR.
Thomas K. Leung and Jitendra Malik. 1999. Rec-
ognizing surfaces using three-dimensional textons.
In ICCV.
Siming Li, Girish Kulkarni, Tamara L. Berg, Alexan-
der C. Berg, and Yejin Choi. 2011. Compos-
ing simple image descriptions using web-scale n-
grams. In Proceedings of the Fifteenth Confer-
ence on Computational Natural Language Learn-
ing, pages 220–228, Portland, Oregon, USA, June.
Association for Computational Linguistics.
David G. Lowe. 2004. Distinctive image features
from scale-invariant keypoints. Int. J. Comput.
Vision, 60:91–110, November.
Andre Martins and Noah A. Smith. 2009. Summa-
rization with a joint model for sentence extraction
and compression. In Proceedings of the Workshop
on Integer Linear Programming for Natural Lan-
guage Processing, pages 1–9, Boulder, Colorado,
June. Association for Computational Linguistics.
Derek D. Monner and James A. Reggia. 2011. Sys-
tematically grounding language through vision in
a deep, recurrent neural network. In Proceed-
ings of the 4th international conference on Arti-
ficial general intelligence, AGI&apos;11, pages 112–121,
Berlin, Heidelberg. Springer-Verlag.
Vicente Ordonez, Girish Kulkarni, and Tamara L.
Berg. 2011. Im2text: Describing images using 1
million captioned photographs. In Neural Infor-
mation Processing Systems (NIPS).
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In ACL.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In HLT-NAACL.
Slav Petrov, Leon Barrett, Romain Thibaux, and
Dan Klein. 2006. Learning accurate, com-
pact, and interpretable tree annotation. In COL-
ING/ACL.
Deb K. Roy. 2002. Learning visually-grounded
words and syntax for a scene description task.
Computer Speech and Language, In review.
Wai-King Tsang. 1996. Comparing the effects of
reading and writing on writing performance. Ap-
plied Linguistics, 17(2):210–233.
Kristian Woodsend and Mirella Lapata. 2010. Au-
tomatic generation of story highlights. In Pro-
ceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 565–
574, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Kristian Woodsend, Yansong Feng, and Mirella
Lapata. 2010. Title generation with quasi-
synchronous grammar. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP &apos;10, pages 513–523,
Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
</reference>
<page confidence="0.979375">
367
</page>
<reference confidence="0.9992105">
Jianxiong Xiao, James Hays, Krista A. Ehinger,
Aude Oliva, and Antonio Torralba. 2010. Sun
database: Large-scale scene recognition from
abbey to zoo. In CVPR.
Yezhou Yang, Ching Teo, Hal Daume III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gen-
eration of natural images. In Proceedings of the
2011 Conference on Empirical Methods in Nat-
ural Language Processing, pages 444–454, Edin-
burgh, Scotland, UK., July. Association for Com-
putational Linguistics.
Benjamin Z. Yao, Xiong Yang, Liang Lin, Mun Wai
Lee, and Song-Chun Zhu. 2010. I2t: Image pars-
ing to text description. Proc. IEEE, 98(8).
</reference>
<page confidence="0.998255">
368
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.835200">
<title confidence="0.999877">Collective Generation of Natural Image Descriptions</title>
<author confidence="0.9798435">Polina Kuznetsova</author>
<author confidence="0.9798435">Vicente Ordonez</author>
<author confidence="0.9798435">Alexander C Tamara L Berg</author>
<author confidence="0.9798435">Yejin</author>
<affiliation confidence="0.993661">Department of Computer</affiliation>
<author confidence="0.9410865">Stony Brook Stony Brook</author>
<author confidence="0.9410865">NY</author>
<abstract confidence="0.998617476190476">We present a holistic data-driven approach to image description generation, exploiting the vast amount of (noisy) parallel image data and associated natural language descriptions available on the web. More specifically, given a query image, we retrieve existing human-composed phrases used to describe visually similar images, then selectively combine those phrases to generate a novel description for the query image. We cast the generation process as constraint optimization problems, collectively incorporating multiple interconnected aspects of language composition realization Evaluation by human annotators indicates that our final system generates more semantically correct and linguistically appealing descriptions than two nontrivial baselines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ahmet Aker</author>
<author>Robert Gaizauskas</author>
</authors>
<title>Generating image descriptions using dependency relational patterns.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1481" citStr="Aker and Gaizauskas (2010)" startWordPosition="198" endWordPosition="201">ition for content planning, surface realization and discourse structure. Evaluation by human annotators indicates that our final system generates more semantically correct and linguistically appealing descriptions than two nontrivial baselines. 1 Introduction Automatically describing images in natural language is an intriguing, but complex AI task, requiring accurate computational visual recognition, comprehensive world knowledge, and natural language generation. Some past research has simplified the general image description goal by assuming that relevant text for an image is provided (e.g., Aker and Gaizauskas (2010), Feng and Lapata (2010)). This allows descriptions to be generated using effective summarization techniques with relatively surface level image understanding. However, such text (e.g., news articles or encyclopedic text) is often only loosely related to an image’s specific content and many natural images do not come with associated text for summarization. In contrast, other recent work has focused more on the visual recognition aspect by detecting content elements (e.g., scenes, objects, attributes, actions, etc) and then composing descriptions from scratch (e.g., Yao et al. (2010), Kulkarni </context>
</contexts>
<marker>Aker, Gaizauskas, 2010</marker>
<rawString>Ahmet Aker and Robert Gaizauskas. 2010. Generating image descriptions using dependency relational patterns. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
<author>Ehud Reiter</author>
</authors>
<title>Comparing automatic and human evaluation of nlg systems.</title>
<date>2006</date>
<booktitle>In EACL 2006, 11st Conference of the European Chapter of the Association for Computational Linguistics, Proceedings of the Conference,</booktitle>
<institution>Trento, Italy. The Association for Computer Linguistics.</institution>
<contexts>
<context position="23399" citStr="Belz and Reiter, 2006" startWordPosition="3879" endWordPosition="3882">ry strong baseline, as it exploits the vast amount of image-caption data, and produces a description high in linguistic quality (since the captions were written by human annotators). Automatic Evaluation: Automatically quantifying the quality of machine generated sentences is known to be difficult. BLEU score (Papineni et al., 2002), despite its simplicity and limitations, has been one of the common choices for automatic evaluation of image descriptions (Farhadi et al., 2010; Kulkarni et al., 2011; Li et al., 2011; Ordonez et al., 2011), as it correlates reasonably well with human evaluation (Belz and Reiter, 2006). Table 1 shows the the BLEU 01 against the original caption of 1000 images. We see that the ILP improves the score over HMM consistently, with or without the use of cognitive phrases. 6Including other long-distance scores in HMM decoding would make the problem NP-hard and require more sophisticated decoding, e.g. ILP. Grammar Cognitive Relevance HMM 3.40(u =.82) 3.40(u =.88) 2.25(u =1.37) ILP 3.56(u =.90) 3.60(u =.98) 2.37(u =1.49) Hum. 4.36(u =.79) 4.77(u =.66) 3.86(u =1.60) Table 4: Human Evaluation: Multi-Aspect Rating (u is a standard deviation) Human Evaluation I – Ranking: We complement</context>
</contexts>
<marker>Belz, Reiter, 2006</marker>
<rawString>Anja Belz and Ehud Reiter. 2006. Comparing automatic and human evaluation of nlg systems. In EACL 2006, 11st Conference of the European Chapter of the Association for Computational Linguistics, Proceedings of the Conference, April 3-7, 2006, Trento, Italy. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Web 1t 5-gram version 1.</title>
<date>2006</date>
<booktitle>In Linguistic Data Consortium.</booktitle>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram version 1. In Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Constraintbased sentence compression: An integer programming approach.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions,</booktitle>
<pages>144--151</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="5297" citStr="Clarke and Lapata (2006)" startWordPosition="775" endWordPosition="778">statistics drawn from a large image-text parallel corpus. This contrasts with previous approaches that generate multiple sentences without considering discourse flow or redundancy (e.g., Li et al. (2011)). For example, for an image showing a flock of birds, generating a large number of sentences stating the relative position of each bird is probably not useful. Content planning and phrase synthesis can be naturally viewed as constraint optimization problems. We employ Integer Linear Programming (ILP) as an optimization framework that has been used successfully in other generation tasks (e.g., Clarke and Lapata (2006), Martins and Smith (2009), Woodsend and Lapata (2010)). Our ILP formulation encodes a rich set of linguistically motivated constraints and weights that incorporate multiple aspects of the generation process. Empirical results demonstrate that our final system generates linguistically more appealing and semantically more correct descriptions than two nontrivial baselines. 1.1 System Overview Our system consists of two parts. For a query image, we first retrieve candidate descriptive phrases from a large image-caption database using measures of visual similarity (§2). We then generate a coheren</context>
</contexts>
<marker>Clarke, Lapata, 2006</marker>
<rawString>James Clarke and Mirella Lapata. 2006. Constraintbased sentence compression: An integer programming approach. In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 144–151, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Navneet Dalal</author>
<author>Bill Triggs</author>
</authors>
<title>Histograms of oriented gradients for human detection.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05) - Volume 1 - Volume 01, CVPR &apos;05,</booktitle>
<pages>886--893</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="7270" citStr="Dalal and Triggs, 2005" startWordPosition="1087" endWordPosition="1090">. grass, road, sky) (Ordonez et al., 2011). All content types are pre-computed on the million database photos, and caption parsing is performed using the Berkeley PCFG parser (Petrov et al., 2006; Petrov and Klein, 2007). Given a query image, we identify content elements present using the above classifiers and detectors and then retrieve phrases referring to those content elements from the database. For example, if we detect a horse in a query image, then we retrieve phrases referring to visually similar horses in the database by comparing the color, texture (Leung and Malik, 1999), or shape (Dalal and Triggs, 2005; Lowe, 2004) of the detected horse to detected horses in the database images. We collect four types of phrases for each query image as follows: [1] NPs We retrieve noun phrases for each query object detection (e.g., “the brown cow&amp;quot;) from database captions using visual similarity between object detections computed as an equally weighted linear combination of L2 dis360 tances on histograms of color, texton (Leung and Malik, 1999), HoG (Dalal and Triggs, 2005) and SIFT (Lowe, 2004) features. [2] VPs We retrieve verb phrases for each query object detection (e.g. “boy running”) from database capti</context>
</contexts>
<marker>Dalal, Triggs, 2005</marker>
<rawString>Navneet Dalal and Bill Triggs. 2005. Histograms of oriented gradients for human detection. In Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05) - Volume 1 - Volume 01, CVPR &apos;05, pages 886–893, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haris Dindo</author>
<author>Daniele Zambuto</author>
</authors>
<title>A probabilistic approach to learning a visually grounded language model through human-robot interaction.</title>
<date>2010</date>
<booktitle>In IROS,</booktitle>
<pages>790--796</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="4367" citStr="Dindo and Zambuto (2010)" startWordPosition="632" endWordPosition="635">ral language descriptions that are substantially richer in content and more linguistically interesting than previous work. At a high level, our approach can be motivated by linguistic theories about the connection between reading activities and writing skills, i.e., substantial reading enriches writing skills, (e.g., Hafiz and Tudor (1989), Tsang (1996)). Analogously, our generation algorithm attains a higher level of linguistic sophistication by reading large amounts of descriptive text available online. Our approach is also motivated by language grounding by visual worlds (e.g., Roy (2002), Dindo and Zambuto (2010), Monner and Reggia (2011)), as in our approach the meaning of a phrase in a description is implicitly grounded by the relevant content of the image. Another important thrust of this work is collective image-level content-planning, integrating saliency, content relations, and discourse structure based on statistics drawn from a large image-text parallel corpus. This contrasts with previous approaches that generate multiple sentences without considering discourse flow or redundancy (e.g., Li et al. (2011)). For example, for an image showing a flock of birds, generating a large number of sentenc</context>
</contexts>
<marker>Dindo, Zambuto, 2010</marker>
<rawString>Haris Dindo and Daniele Zambuto. 2010. A probabilistic approach to learning a visually grounded language model through human-robot interaction. In IROS, pages 790–796. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ali Farhadi</author>
<author>Mohsen Hejrati</author>
<author>Mohammad Amin Sadeghi</author>
<author>Peter Young</author>
<author>Cyrus Rashtchian</author>
<author>Julia Hockenmaier</author>
<author>David Forsyth</author>
</authors>
<title>Every picture tells a story: generating sentences for images.</title>
<date>2010</date>
<booktitle>In ECCV.</booktitle>
<contexts>
<context position="2237" citStr="Farhadi et al. (2010)" startWordPosition="315" endWordPosition="318">vel image understanding. However, such text (e.g., news articles or encyclopedic text) is often only loosely related to an image’s specific content and many natural images do not come with associated text for summarization. In contrast, other recent work has focused more on the visual recognition aspect by detecting content elements (e.g., scenes, objects, attributes, actions, etc) and then composing descriptions from scratch (e.g., Yao et al. (2010), Kulkarni et al. (2011), Yang et al. (2011), Li et al. (2011)), or by retrieving existing whole descriptions from visually similar images (e.g., Farhadi et al. (2010), Ordonez et al. (2011)). For the latter approaches, it is unrealistic to expect that there will always exist a single complete description for retrieval that is pertinent to a given query image. For the former approaches, visual recognition first generates an intermediate representation of image content using a set of English words, then language generation constructs a full description by adding function words and optionally applying simple re-ordering. Because the generation process sticks relatively closely to the recognized content, the resulting descriptions often lack the kind of covera</context>
<context position="23256" citStr="Farhadi et al., 2010" startWordPosition="3854" endWordPosition="3857">large parallel corpus of images and captions, and transfers a caption from a visually similar database image to the query. This again is a very strong baseline, as it exploits the vast amount of image-caption data, and produces a description high in linguistic quality (since the captions were written by human annotators). Automatic Evaluation: Automatically quantifying the quality of machine generated sentences is known to be difficult. BLEU score (Papineni et al., 2002), despite its simplicity and limitations, has been one of the common choices for automatic evaluation of image descriptions (Farhadi et al., 2010; Kulkarni et al., 2011; Li et al., 2011; Ordonez et al., 2011), as it correlates reasonably well with human evaluation (Belz and Reiter, 2006). Table 1 shows the the BLEU 01 against the original caption of 1000 images. We see that the ILP improves the score over HMM consistently, with or without the use of cognitive phrases. 6Including other long-distance scores in HMM decoding would make the problem NP-hard and require more sophisticated decoding, e.g. ILP. Grammar Cognitive Relevance HMM 3.40(u =.82) 3.40(u =.88) 2.25(u =1.37) ILP 3.56(u =.90) 3.60(u =.98) 2.37(u =1.49) Hum. 4.36(u =.79) 4.</context>
</contexts>
<marker>Farhadi, Hejrati, Sadeghi, Young, Rashtchian, Hockenmaier, Forsyth, 2010</marker>
<rawString>Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Peter Young, Cyrus Rashtchian, Julia Hockenmaier, and David Forsyth. 2010. Every picture tells a story: generating sentences for images. In ECCV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro F Felzenszwalb</author>
<author>Ross B Girshick</author>
<author>David McAllester</author>
<author>Deva Ramanan</author>
</authors>
<title>Object detection with discriminatively trained part based models.</title>
<date>2010</date>
<tech>tPAMI,</tech>
<contexts>
<context position="6513" citStr="Felzenszwalb et al., 2010" startWordPosition="959" endWordPosition="962"> a coherent description from these candidates using ILP formulations for content planning (§4) and surface realization (§5). 2 Vision &amp; Phrase Retrieval For a query image, we retrieve relevant candidate natural language phrases by visually comparing the query image to database images from the SBU Captioned Photo Collection (Ordonez et al., 2011) (1 million photographs with associated human-composed descriptions). Visual similarity for several kinds of image content are used to compare the query image to images from the database, including: 1) object detections for 89 common object categories (Felzenszwalb et al., 2010), 2) scene classifications for 26 common scene categories (Xiao et al., 2010), and 3) region based detections for stuff categories (e.g. grass, road, sky) (Ordonez et al., 2011). All content types are pre-computed on the million database photos, and caption parsing is performed using the Berkeley PCFG parser (Petrov et al., 2006; Petrov and Klein, 2007). Given a query image, we identify content elements present using the above classifiers and detectors and then retrieve phrases referring to those content elements from the database. For example, if we detect a horse in a query image, then we re</context>
</contexts>
<marker>Felzenszwalb, Girshick, McAllester, Ramanan, 2010</marker>
<rawString>Pedro F. Felzenszwalb, Ross B. Girshick, David McAllester, and Deva Ramanan. 2010. Object detection with discriminatively trained part based models. tPAMI, Sept.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>How many words is a picture worth? automatic caption generation for news images.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1505" citStr="Feng and Lapata (2010)" startWordPosition="202" endWordPosition="205">surface realization and discourse structure. Evaluation by human annotators indicates that our final system generates more semantically correct and linguistically appealing descriptions than two nontrivial baselines. 1 Introduction Automatically describing images in natural language is an intriguing, but complex AI task, requiring accurate computational visual recognition, comprehensive world knowledge, and natural language generation. Some past research has simplified the general image description goal by assuming that relevant text for an image is provided (e.g., Aker and Gaizauskas (2010), Feng and Lapata (2010)). This allows descriptions to be generated using effective summarization techniques with relatively surface level image understanding. However, such text (e.g., news articles or encyclopedic text) is often only loosely related to an image’s specific content and many natural images do not come with associated text for summarization. In contrast, other recent work has focused more on the visual recognition aspect by detecting content elements (e.g., scenes, objects, attributes, actions, etc) and then composing descriptions from scratch (e.g., Yao et al. (2010), Kulkarni et al. (2011), Yang et a</context>
</contexts>
<marker>Feng, Lapata, 2010</marker>
<rawString>Yansong Feng and Mirella Lapata. 2010. How many words is a picture worth? automatic caption generation for news images. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fateh Muhammad Hafiz</author>
<author>Ian Tudor</author>
</authors>
<title>Extensive reading and the development of language skills.</title>
<date>1989</date>
<journal>ELT Journal,</journal>
<volume>43</volume>
<issue>1</issue>
<contexts>
<context position="4084" citStr="Hafiz and Tudor (1989)" startWordPosition="590" endWordPosition="593">iction of retrieving existing whole descriptions by gathering visually relevant phrases which we combine to produce novel and query-image specific descriptions. By judiciously exploiting the correspondence between image content elements and phrases, it is possible to generate natural language descriptions that are substantially richer in content and more linguistically interesting than previous work. At a high level, our approach can be motivated by linguistic theories about the connection between reading activities and writing skills, i.e., substantial reading enriches writing skills, (e.g., Hafiz and Tudor (1989), Tsang (1996)). Analogously, our generation algorithm attains a higher level of linguistic sophistication by reading large amounts of descriptive text available online. Our approach is also motivated by language grounding by visual worlds (e.g., Roy (2002), Dindo and Zambuto (2010), Monner and Reggia (2011)), as in our approach the meaning of a phrase in a description is implicitly grounded by the relevant content of the image. Another important thrust of this work is collective image-level content-planning, integrating saliency, content relations, and discourse structure based on statistics </context>
</contexts>
<marker>Hafiz, Tudor, 1989</marker>
<rawString>Fateh Muhammad Hafiz and Ian Tudor. 1989. Extensive reading and the development of language skills. ELT Journal, 43(1):4–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Girish Kulkarni</author>
<author>Visruth Premraj</author>
<author>Sagnik Dhar</author>
<author>Siming Li</author>
<author>Yejin Choi</author>
<author>Alexander C Berg</author>
<author>Tamara L Berg</author>
</authors>
<title>Babytalk: Understanding and generating simple image descriptions.</title>
<date>2011</date>
<booktitle>In CVPR.</booktitle>
<contexts>
<context position="2094" citStr="Kulkarni et al. (2011)" startWordPosition="292" endWordPosition="295">as (2010), Feng and Lapata (2010)). This allows descriptions to be generated using effective summarization techniques with relatively surface level image understanding. However, such text (e.g., news articles or encyclopedic text) is often only loosely related to an image’s specific content and many natural images do not come with associated text for summarization. In contrast, other recent work has focused more on the visual recognition aspect by detecting content elements (e.g., scenes, objects, attributes, actions, etc) and then composing descriptions from scratch (e.g., Yao et al. (2010), Kulkarni et al. (2011), Yang et al. (2011), Li et al. (2011)), or by retrieving existing whole descriptions from visually similar images (e.g., Farhadi et al. (2010), Ordonez et al. (2011)). For the latter approaches, it is unrealistic to expect that there will always exist a single complete description for retrieval that is pertinent to a given query image. For the former approaches, visual recognition first generates an intermediate representation of image content using a set of English words, then language generation constructs a full description by adding function words and optionally applying simple re-orderin</context>
<context position="23279" citStr="Kulkarni et al., 2011" startWordPosition="3858" endWordPosition="3861">of images and captions, and transfers a caption from a visually similar database image to the query. This again is a very strong baseline, as it exploits the vast amount of image-caption data, and produces a description high in linguistic quality (since the captions were written by human annotators). Automatic Evaluation: Automatically quantifying the quality of machine generated sentences is known to be difficult. BLEU score (Papineni et al., 2002), despite its simplicity and limitations, has been one of the common choices for automatic evaluation of image descriptions (Farhadi et al., 2010; Kulkarni et al., 2011; Li et al., 2011; Ordonez et al., 2011), as it correlates reasonably well with human evaluation (Belz and Reiter, 2006). Table 1 shows the the BLEU 01 against the original caption of 1000 images. We see that the ILP improves the score over HMM consistently, with or without the use of cognitive phrases. 6Including other long-distance scores in HMM decoding would make the problem NP-hard and require more sophisticated decoding, e.g. ILP. Grammar Cognitive Relevance HMM 3.40(u =.82) 3.40(u =.88) 2.25(u =1.37) ILP 3.56(u =.90) 3.60(u =.98) 2.37(u =1.49) Hum. 4.36(u =.79) 4.77(u =.66) 3.86(u =1.60</context>
</contexts>
<marker>Kulkarni, Premraj, Dhar, Li, Choi, Berg, Berg, 2011</marker>
<rawString>Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C Berg, and Tamara L Berg. 2011. Babytalk: Understanding and generating simple image descriptions. In CVPR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Leung</author>
<author>Jitendra Malik</author>
</authors>
<title>Recognizing surfaces using three-dimensional textons.</title>
<date>1999</date>
<booktitle>In ICCV.</booktitle>
<contexts>
<context position="7236" citStr="Leung and Malik, 1999" startWordPosition="1081" endWordPosition="1084">tections for stuff categories (e.g. grass, road, sky) (Ordonez et al., 2011). All content types are pre-computed on the million database photos, and caption parsing is performed using the Berkeley PCFG parser (Petrov et al., 2006; Petrov and Klein, 2007). Given a query image, we identify content elements present using the above classifiers and detectors and then retrieve phrases referring to those content elements from the database. For example, if we detect a horse in a query image, then we retrieve phrases referring to visually similar horses in the database by comparing the color, texture (Leung and Malik, 1999), or shape (Dalal and Triggs, 2005; Lowe, 2004) of the detected horse to detected horses in the database images. We collect four types of phrases for each query image as follows: [1] NPs We retrieve noun phrases for each query object detection (e.g., “the brown cow&amp;quot;) from database captions using visual similarity between object detections computed as an equally weighted linear combination of L2 dis360 tances on histograms of color, texton (Leung and Malik, 1999), HoG (Dalal and Triggs, 2005) and SIFT (Lowe, 2004) features. [2] VPs We retrieve verb phrases for each query object detection (e.g. </context>
</contexts>
<marker>Leung, Malik, 1999</marker>
<rawString>Thomas K. Leung and Jitendra Malik. 1999. Recognizing surfaces using three-dimensional textons. In ICCV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siming Li</author>
<author>Girish Kulkarni</author>
<author>Tamara L Berg</author>
<author>Alexander C Berg</author>
<author>Yejin Choi</author>
</authors>
<title>Composing simple image descriptions using web-scale ngrams.</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>220--228</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="2132" citStr="Li et al. (2011)" startWordPosition="300" endWordPosition="303">ows descriptions to be generated using effective summarization techniques with relatively surface level image understanding. However, such text (e.g., news articles or encyclopedic text) is often only loosely related to an image’s specific content and many natural images do not come with associated text for summarization. In contrast, other recent work has focused more on the visual recognition aspect by detecting content elements (e.g., scenes, objects, attributes, actions, etc) and then composing descriptions from scratch (e.g., Yao et al. (2010), Kulkarni et al. (2011), Yang et al. (2011), Li et al. (2011)), or by retrieving existing whole descriptions from visually similar images (e.g., Farhadi et al. (2010), Ordonez et al. (2011)). For the latter approaches, it is unrealistic to expect that there will always exist a single complete description for retrieval that is pertinent to a given query image. For the former approaches, visual recognition first generates an intermediate representation of image content using a set of English words, then language generation constructs a full description by adding function words and optionally applying simple re-ordering. Because the generation process stic</context>
<context position="4876" citStr="Li et al. (2011)" startWordPosition="710" endWordPosition="713">proach is also motivated by language grounding by visual worlds (e.g., Roy (2002), Dindo and Zambuto (2010), Monner and Reggia (2011)), as in our approach the meaning of a phrase in a description is implicitly grounded by the relevant content of the image. Another important thrust of this work is collective image-level content-planning, integrating saliency, content relations, and discourse structure based on statistics drawn from a large image-text parallel corpus. This contrasts with previous approaches that generate multiple sentences without considering discourse flow or redundancy (e.g., Li et al. (2011)). For example, for an image showing a flock of birds, generating a large number of sentences stating the relative position of each bird is probably not useful. Content planning and phrase synthesis can be naturally viewed as constraint optimization problems. We employ Integer Linear Programming (ILP) as an optimization framework that has been used successfully in other generation tasks (e.g., Clarke and Lapata (2006), Martins and Smith (2009), Woodsend and Lapata (2010)). Our ILP formulation encodes a rich set of linguistically motivated constraints and weights that incorporate multiple aspec</context>
<context position="23296" citStr="Li et al., 2011" startWordPosition="3862" endWordPosition="3865"> and transfers a caption from a visually similar database image to the query. This again is a very strong baseline, as it exploits the vast amount of image-caption data, and produces a description high in linguistic quality (since the captions were written by human annotators). Automatic Evaluation: Automatically quantifying the quality of machine generated sentences is known to be difficult. BLEU score (Papineni et al., 2002), despite its simplicity and limitations, has been one of the common choices for automatic evaluation of image descriptions (Farhadi et al., 2010; Kulkarni et al., 2011; Li et al., 2011; Ordonez et al., 2011), as it correlates reasonably well with human evaluation (Belz and Reiter, 2006). Table 1 shows the the BLEU 01 against the original caption of 1000 images. We see that the ILP improves the score over HMM consistently, with or without the use of cognitive phrases. 6Including other long-distance scores in HMM decoding would make the problem NP-hard and require more sophisticated decoding, e.g. ILP. Grammar Cognitive Relevance HMM 3.40(u =.82) 3.40(u =.88) 2.25(u =1.37) ILP 3.56(u =.90) 3.60(u =.98) 2.37(u =1.49) Hum. 4.36(u =.79) 4.77(u =.66) 3.86(u =1.60) Table 4: Human </context>
</contexts>
<marker>Li, Kulkarni, Berg, Berg, Choi, 2011</marker>
<rawString>Siming Li, Girish Kulkarni, Tamara L. Berg, Alexander C. Berg, and Yejin Choi. 2011. Composing simple image descriptions using web-scale ngrams. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 220–228, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David G Lowe</author>
</authors>
<title>Distinctive image features from scale-invariant keypoints.</title>
<date>2004</date>
<journal>Int. J. Comput. Vision,</journal>
<pages>60--91</pages>
<contexts>
<context position="7283" citStr="Lowe, 2004" startWordPosition="1091" endWordPosition="1092">onez et al., 2011). All content types are pre-computed on the million database photos, and caption parsing is performed using the Berkeley PCFG parser (Petrov et al., 2006; Petrov and Klein, 2007). Given a query image, we identify content elements present using the above classifiers and detectors and then retrieve phrases referring to those content elements from the database. For example, if we detect a horse in a query image, then we retrieve phrases referring to visually similar horses in the database by comparing the color, texture (Leung and Malik, 1999), or shape (Dalal and Triggs, 2005; Lowe, 2004) of the detected horse to detected horses in the database images. We collect four types of phrases for each query image as follows: [1] NPs We retrieve noun phrases for each query object detection (e.g., “the brown cow&amp;quot;) from database captions using visual similarity between object detections computed as an equally weighted linear combination of L2 dis360 tances on histograms of color, texton (Leung and Malik, 1999), HoG (Dalal and Triggs, 2005) and SIFT (Lowe, 2004) features. [2] VPs We retrieve verb phrases for each query object detection (e.g. “boy running”) from database captions using the</context>
</contexts>
<marker>Lowe, 2004</marker>
<rawString>David G. Lowe. 2004. Distinctive image features from scale-invariant keypoints. Int. J. Comput. Vision, 60:91–110, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andre Martins</author>
<author>Noah A Smith</author>
</authors>
<title>Summarization with a joint model for sentence extraction and compression.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Integer Linear Programming for Natural Language Processing,</booktitle>
<pages>1--9</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="5323" citStr="Martins and Smith (2009)" startWordPosition="779" endWordPosition="783">rge image-text parallel corpus. This contrasts with previous approaches that generate multiple sentences without considering discourse flow or redundancy (e.g., Li et al. (2011)). For example, for an image showing a flock of birds, generating a large number of sentences stating the relative position of each bird is probably not useful. Content planning and phrase synthesis can be naturally viewed as constraint optimization problems. We employ Integer Linear Programming (ILP) as an optimization framework that has been used successfully in other generation tasks (e.g., Clarke and Lapata (2006), Martins and Smith (2009), Woodsend and Lapata (2010)). Our ILP formulation encodes a rich set of linguistically motivated constraints and weights that incorporate multiple aspects of the generation process. Empirical results demonstrate that our final system generates linguistically more appealing and semantically more correct descriptions than two nontrivial baselines. 1.1 System Overview Our system consists of two parts. For a query image, we first retrieve candidate descriptive phrases from a large image-caption database using measures of visual similarity (§2). We then generate a coherent description from these c</context>
<context position="27700" citStr="Martins and Smith (2009)" startWordPosition="4573" endWordPosition="4576">mpression is hardly the goal of image description generation, as human written descriptions are not necessarily succinct.9 Second, unlike summarization, we are not given with a set of coherent text snippet to begin with, and the level of noise coming from the visual recognition errors is much higher than that of starting with clean text. As a result, choosing an additional phrase in the image description is much riskier than it is in summarization. Some recent research proposed very elegant approaches to summarization using ILP for collective content planning and/or surface realization (e.g., Martins and Smith (2009), Woodsend and Lapata (2010), Woodsend et al. (2010)). Perhaps the most important difference in our approach is the use of negative weights in the objective function to create the necessary tension between selection (salience) and compatibility, which makes it possible for ILP to generate variable length descriptions, effectively correcting some of the erroneous vision detections. In contrast, all previous work operates with a predefined upper limit in length, hence the ILP was formulated to include as many textual units as possible modulo constraints. To conclude, we have presented a collecti</context>
</contexts>
<marker>Martins, Smith, 2009</marker>
<rawString>Andre Martins and Noah A. Smith. 2009. Summarization with a joint model for sentence extraction and compression. In Proceedings of the Workshop on Integer Linear Programming for Natural Language Processing, pages 1–9, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Derek D Monner</author>
<author>James A Reggia</author>
</authors>
<title>Systematically grounding language through vision in a deep, recurrent neural network.</title>
<date>2011</date>
<booktitle>In Proceedings of the 4th international conference on Artificial general intelligence, AGI&apos;11,</booktitle>
<pages>112--121</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="4393" citStr="Monner and Reggia (2011)" startWordPosition="636" endWordPosition="639">that are substantially richer in content and more linguistically interesting than previous work. At a high level, our approach can be motivated by linguistic theories about the connection between reading activities and writing skills, i.e., substantial reading enriches writing skills, (e.g., Hafiz and Tudor (1989), Tsang (1996)). Analogously, our generation algorithm attains a higher level of linguistic sophistication by reading large amounts of descriptive text available online. Our approach is also motivated by language grounding by visual worlds (e.g., Roy (2002), Dindo and Zambuto (2010), Monner and Reggia (2011)), as in our approach the meaning of a phrase in a description is implicitly grounded by the relevant content of the image. Another important thrust of this work is collective image-level content-planning, integrating saliency, content relations, and discourse structure based on statistics drawn from a large image-text parallel corpus. This contrasts with previous approaches that generate multiple sentences without considering discourse flow or redundancy (e.g., Li et al. (2011)). For example, for an image showing a flock of birds, generating a large number of sentences stating the relative po</context>
</contexts>
<marker>Monner, Reggia, 2011</marker>
<rawString>Derek D. Monner and James A. Reggia. 2011. Systematically grounding language through vision in a deep, recurrent neural network. In Proceedings of the 4th international conference on Artificial general intelligence, AGI&apos;11, pages 112–121, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vicente Ordonez</author>
<author>Girish Kulkarni</author>
<author>Tamara L Berg</author>
</authors>
<title>Im2text: Describing images using 1 million captioned photographs.</title>
<date>2011</date>
<booktitle>In Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="2260" citStr="Ordonez et al. (2011)" startWordPosition="319" endWordPosition="322">. However, such text (e.g., news articles or encyclopedic text) is often only loosely related to an image’s specific content and many natural images do not come with associated text for summarization. In contrast, other recent work has focused more on the visual recognition aspect by detecting content elements (e.g., scenes, objects, attributes, actions, etc) and then composing descriptions from scratch (e.g., Yao et al. (2010), Kulkarni et al. (2011), Yang et al. (2011), Li et al. (2011)), or by retrieving existing whole descriptions from visually similar images (e.g., Farhadi et al. (2010), Ordonez et al. (2011)). For the latter approaches, it is unrealistic to expect that there will always exist a single complete description for retrieval that is pertinent to a given query image. For the former approaches, visual recognition first generates an intermediate representation of image content using a set of English words, then language generation constructs a full description by adding function words and optionally applying simple re-ordering. Because the generation process sticks relatively closely to the recognized content, the resulting descriptions often lack the kind of coverage, creativity, and com</context>
<context position="6234" citStr="Ordonez et al., 2011" startWordPosition="918" endWordPosition="921">y more correct descriptions than two nontrivial baselines. 1.1 System Overview Our system consists of two parts. For a query image, we first retrieve candidate descriptive phrases from a large image-caption database using measures of visual similarity (§2). We then generate a coherent description from these candidates using ILP formulations for content planning (§4) and surface realization (§5). 2 Vision &amp; Phrase Retrieval For a query image, we retrieve relevant candidate natural language phrases by visually comparing the query image to database images from the SBU Captioned Photo Collection (Ordonez et al., 2011) (1 million photographs with associated human-composed descriptions). Visual similarity for several kinds of image content are used to compare the query image to images from the database, including: 1) object detections for 89 common object categories (Felzenszwalb et al., 2010), 2) scene classifications for 26 common scene categories (Xiao et al., 2010), and 3) region based detections for stuff categories (e.g. grass, road, sky) (Ordonez et al., 2011). All content types are pre-computed on the million database photos, and caption parsing is performed using the Berkeley PCFG parser (Petrov et </context>
<context position="12518" citStr="Ordonez et al., 2011" startWordPosition="1987" endWordPosition="1990"> Compatibility The weight 0 ≤ Fst ≤ 1 quantifies the compatibility of the object pairing (s, t). Note that in the objective function, we subtract this quantity from the function to be maximized. This way, we create a competing tension between the single object selection scores and the pairwise compatibility scores, so that variable number of objects can be selected. Object Ordering Statistics: People have biases on the order of topic or content flow. We measure these biases by collecting statistics on ordering of object names from the 1 million image descriptions in the SBU Captioned Dataset (Ordonez et al., 2011). Let ford(w1, w2) be the number of times w1 appeared before w2. For instance, ford(window, house) = 2895 and ford(house, window) = 1250, suggesting that people are more likely to mention a window before mentioning a house/building2. We use these ordering statistics to enhance content flow. We define score for the order of objects using Z-score for normalization as follows: F^ ford(cs,ct) − mean (ford) (10) st = std dev(ford) 2We take into account synonyms. correspond to better choices. 5 Surface Realization Recall that for each image, the computer vision system identifies phrases from descrip</context>
<context position="22616" citStr="Ordonez et al., 2011" startWordPosition="3750" endWordPosition="3753">β · FCO sijpq (23) α + β (20) 6 Evaluation 364 cognitive phrases: HMM HMM ILP ILP with w/o with w/o 0.111 0.114 0.114 0.116 Table 1: Automatic Evaluation ILP selection rate ILP V.S. HMM (w/o cogn) 67.2% ILP V.S. HMM (with cogn) 66.3% Table 2: Human Evaluation (without images) ILP selection rate ILP V.S. HMM (w/o cogn) 53.17% ILP V.S. HMM (with cogn) 54.5% ILP V.S. RETRIEVAL 71.8% ILP V.S. Human 16% Table 3: Human Evaluation (with images) phrase cohesion scores (§5.4) used for the ILP formulation, producing a strong baselines. The second baseline is a recent RETRIEVAL based description method (Ordonez et al., 2011), that searches the large parallel corpus of images and captions, and transfers a caption from a visually similar database image to the query. This again is a very strong baseline, as it exploits the vast amount of image-caption data, and produces a description high in linguistic quality (since the captions were written by human annotators). Automatic Evaluation: Automatically quantifying the quality of machine generated sentences is known to be difficult. BLEU score (Papineni et al., 2002), despite its simplicity and limitations, has been one of the common choices for automatic evaluation of </context>
<context position="25107" citStr="Ordonez et al., 2011" startWordPosition="4160" endWordPosition="4163">uch (67.2% ILP V.S. 32.8% HMM), if images are not presented. However the difference is less pronounced when images are shown. There could be two possible reasons. The first is that when images are shown, the Turkers do not try as hard to tell apart the subtle difference between the two imperfect captions. The second is that the relative content relevance of ILP generated captions is negating the superiority in linguistic quality. We explore this question using multi-aspect rating, described below. Note that ILP generated captions are exceedingly (71.8 %) preferred over the RETRIEVAL baseline (Ordonez et al., 2011), despite the generated captions tendency to be more prone to grammatical and cognitive errors than retrieved ones. This indicates that the generated captions must have substantially better content relevance to the query image, supporting the direction of this research. Finally, notice that as much as 16% of the time, ILP generated captions are preferred over the original human generated ones (examples in Figure 2). Human Evaluation II – Multi-Aspect Rating: Table 4 presents rating in the 1–5 scale (5: perfect, 4: almost perfect, 3: 70∼80% good, 2: 7 W present two captions in a randomized orde</context>
</contexts>
<marker>Ordonez, Kulkarni, Berg, 2011</marker>
<rawString>Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. 2011. Im2text: Describing images using 1 million captioned photographs. In Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei jing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="23111" citStr="Papineni et al., 2002" startWordPosition="3831" endWordPosition="3834">lation, producing a strong baselines. The second baseline is a recent RETRIEVAL based description method (Ordonez et al., 2011), that searches the large parallel corpus of images and captions, and transfers a caption from a visually similar database image to the query. This again is a very strong baseline, as it exploits the vast amount of image-caption data, and produces a description high in linguistic quality (since the captions were written by human annotators). Automatic Evaluation: Automatically quantifying the quality of machine generated sentences is known to be difficult. BLEU score (Papineni et al., 2002), despite its simplicity and limitations, has been one of the common choices for automatic evaluation of image descriptions (Farhadi et al., 2010; Kulkarni et al., 2011; Li et al., 2011; Ordonez et al., 2011), as it correlates reasonably well with human evaluation (Belz and Reiter, 2006). Table 1 shows the the BLEU 01 against the original caption of 1000 images. We see that the ILP improves the score over HMM consistently, with or without the use of cognitive phrases. 6Including other long-distance scores in HMM decoding would make the problem NP-hard and require more sophisticated decoding, e</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="6868" citStr="Petrov and Klein, 2007" startWordPosition="1018" endWordPosition="1021"> photographs with associated human-composed descriptions). Visual similarity for several kinds of image content are used to compare the query image to images from the database, including: 1) object detections for 89 common object categories (Felzenszwalb et al., 2010), 2) scene classifications for 26 common scene categories (Xiao et al., 2010), and 3) region based detections for stuff categories (e.g. grass, road, sky) (Ordonez et al., 2011). All content types are pre-computed on the million database photos, and caption parsing is performed using the Berkeley PCFG parser (Petrov et al., 2006; Petrov and Klein, 2007). Given a query image, we identify content elements present using the above classifiers and detectors and then retrieve phrases referring to those content elements from the database. For example, if we detect a horse in a query image, then we retrieve phrases referring to visually similar horses in the database by comparing the color, texture (Leung and Malik, 1999), or shape (Dalal and Triggs, 2005; Lowe, 2004) of the detected horse to detected horses in the database images. We collect four types of phrases for each query image as follows: [1] NPs We retrieve noun phrases for each query objec</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In COLING/ACL.</booktitle>
<contexts>
<context position="6843" citStr="Petrov et al., 2006" startWordPosition="1014" endWordPosition="1017">al., 2011) (1 million photographs with associated human-composed descriptions). Visual similarity for several kinds of image content are used to compare the query image to images from the database, including: 1) object detections for 89 common object categories (Felzenszwalb et al., 2010), 2) scene classifications for 26 common scene categories (Xiao et al., 2010), and 3) region based detections for stuff categories (e.g. grass, road, sky) (Ordonez et al., 2011). All content types are pre-computed on the million database photos, and caption parsing is performed using the Berkeley PCFG parser (Petrov et al., 2006; Petrov and Klein, 2007). Given a query image, we identify content elements present using the above classifiers and detectors and then retrieve phrases referring to those content elements from the database. For example, if we detect a horse in a query image, then we retrieve phrases referring to visually similar horses in the database by comparing the color, texture (Leung and Malik, 1999), or shape (Dalal and Triggs, 2005; Lowe, 2004) of the detected horse to detected horses in the database images. We collect four types of phrases for each query image as follows: [1] NPs We retrieve noun phr</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In COLING/ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deb K Roy</author>
</authors>
<title>Learning visually-grounded words and syntax for a scene description task. Computer Speech and Language, In review.</title>
<date>2002</date>
<contexts>
<context position="4341" citStr="Roy (2002)" startWordPosition="630" endWordPosition="631">enerate natural language descriptions that are substantially richer in content and more linguistically interesting than previous work. At a high level, our approach can be motivated by linguistic theories about the connection between reading activities and writing skills, i.e., substantial reading enriches writing skills, (e.g., Hafiz and Tudor (1989), Tsang (1996)). Analogously, our generation algorithm attains a higher level of linguistic sophistication by reading large amounts of descriptive text available online. Our approach is also motivated by language grounding by visual worlds (e.g., Roy (2002), Dindo and Zambuto (2010), Monner and Reggia (2011)), as in our approach the meaning of a phrase in a description is implicitly grounded by the relevant content of the image. Another important thrust of this work is collective image-level content-planning, integrating saliency, content relations, and discourse structure based on statistics drawn from a large image-text parallel corpus. This contrasts with previous approaches that generate multiple sentences without considering discourse flow or redundancy (e.g., Li et al. (2011)). For example, for an image showing a flock of birds, generating</context>
</contexts>
<marker>Roy, 2002</marker>
<rawString>Deb K. Roy. 2002. Learning visually-grounded words and syntax for a scene description task. Computer Speech and Language, In review.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wai-King Tsang</author>
</authors>
<title>Comparing the effects of reading and writing on writing performance.</title>
<date>1996</date>
<journal>Applied Linguistics,</journal>
<volume>17</volume>
<issue>2</issue>
<contexts>
<context position="4098" citStr="Tsang (1996)" startWordPosition="594" endWordPosition="595">sting whole descriptions by gathering visually relevant phrases which we combine to produce novel and query-image specific descriptions. By judiciously exploiting the correspondence between image content elements and phrases, it is possible to generate natural language descriptions that are substantially richer in content and more linguistically interesting than previous work. At a high level, our approach can be motivated by linguistic theories about the connection between reading activities and writing skills, i.e., substantial reading enriches writing skills, (e.g., Hafiz and Tudor (1989), Tsang (1996)). Analogously, our generation algorithm attains a higher level of linguistic sophistication by reading large amounts of descriptive text available online. Our approach is also motivated by language grounding by visual worlds (e.g., Roy (2002), Dindo and Zambuto (2010), Monner and Reggia (2011)), as in our approach the meaning of a phrase in a description is implicitly grounded by the relevant content of the image. Another important thrust of this work is collective image-level content-planning, integrating saliency, content relations, and discourse structure based on statistics drawn from a l</context>
</contexts>
<marker>Tsang, 1996</marker>
<rawString>Wai-King Tsang. 1996. Comparing the effects of reading and writing on writing performance. Applied Linguistics, 17(2):210–233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristian Woodsend</author>
<author>Mirella Lapata</author>
</authors>
<title>Automatic generation of story highlights.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>565--574</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="5351" citStr="Woodsend and Lapata (2010)" startWordPosition="784" endWordPosition="787">rpus. This contrasts with previous approaches that generate multiple sentences without considering discourse flow or redundancy (e.g., Li et al. (2011)). For example, for an image showing a flock of birds, generating a large number of sentences stating the relative position of each bird is probably not useful. Content planning and phrase synthesis can be naturally viewed as constraint optimization problems. We employ Integer Linear Programming (ILP) as an optimization framework that has been used successfully in other generation tasks (e.g., Clarke and Lapata (2006), Martins and Smith (2009), Woodsend and Lapata (2010)). Our ILP formulation encodes a rich set of linguistically motivated constraints and weights that incorporate multiple aspects of the generation process. Empirical results demonstrate that our final system generates linguistically more appealing and semantically more correct descriptions than two nontrivial baselines. 1.1 System Overview Our system consists of two parts. For a query image, we first retrieve candidate descriptive phrases from a large image-caption database using measures of visual similarity (§2). We then generate a coherent description from these candidates using ILP formulat</context>
<context position="27728" citStr="Woodsend and Lapata (2010)" startWordPosition="4577" endWordPosition="4580">al of image description generation, as human written descriptions are not necessarily succinct.9 Second, unlike summarization, we are not given with a set of coherent text snippet to begin with, and the level of noise coming from the visual recognition errors is much higher than that of starting with clean text. As a result, choosing an additional phrase in the image description is much riskier than it is in summarization. Some recent research proposed very elegant approaches to summarization using ILP for collective content planning and/or surface realization (e.g., Martins and Smith (2009), Woodsend and Lapata (2010), Woodsend et al. (2010)). Perhaps the most important difference in our approach is the use of negative weights in the objective function to create the necessary tension between selection (salience) and compatibility, which makes it possible for ILP to generate variable length descriptions, effectively correcting some of the erroneous vision detections. In contrast, all previous work operates with a predefined upper limit in length, hence the ILP was formulated to include as many textual units as possible modulo constraints. To conclude, we have presented a collective approach to generating na</context>
</contexts>
<marker>Woodsend, Lapata, 2010</marker>
<rawString>Kristian Woodsend and Mirella Lapata. 2010. Automatic generation of story highlights. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 565– 574, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristian Woodsend</author>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>Title generation with quasisynchronous grammar.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;10,</booktitle>
<pages>513--523</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="27752" citStr="Woodsend et al. (2010)" startWordPosition="4581" endWordPosition="4584">ration, as human written descriptions are not necessarily succinct.9 Second, unlike summarization, we are not given with a set of coherent text snippet to begin with, and the level of noise coming from the visual recognition errors is much higher than that of starting with clean text. As a result, choosing an additional phrase in the image description is much riskier than it is in summarization. Some recent research proposed very elegant approaches to summarization using ILP for collective content planning and/or surface realization (e.g., Martins and Smith (2009), Woodsend and Lapata (2010), Woodsend et al. (2010)). Perhaps the most important difference in our approach is the use of negative weights in the objective function to create the necessary tension between selection (salience) and compatibility, which makes it possible for ILP to generate variable length descriptions, effectively correcting some of the erroneous vision detections. In contrast, all previous work operates with a predefined upper limit in length, hence the ILP was formulated to include as many textual units as possible modulo constraints. To conclude, we have presented a collective approach to generating natural image descriptions</context>
</contexts>
<marker>Woodsend, Feng, Lapata, 2010</marker>
<rawString>Kristian Woodsend, Yansong Feng, and Mirella Lapata. 2010. Title generation with quasisynchronous grammar. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP &apos;10, pages 513–523, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianxiong Xiao</author>
<author>James Hays</author>
<author>Krista A Ehinger</author>
<author>Aude Oliva</author>
<author>Antonio Torralba</author>
</authors>
<title>Sun database: Large-scale scene recognition from abbey to zoo.</title>
<date>2010</date>
<booktitle>In CVPR.</booktitle>
<contexts>
<context position="6590" citStr="Xiao et al., 2010" startWordPosition="972" endWordPosition="975">ning (§4) and surface realization (§5). 2 Vision &amp; Phrase Retrieval For a query image, we retrieve relevant candidate natural language phrases by visually comparing the query image to database images from the SBU Captioned Photo Collection (Ordonez et al., 2011) (1 million photographs with associated human-composed descriptions). Visual similarity for several kinds of image content are used to compare the query image to images from the database, including: 1) object detections for 89 common object categories (Felzenszwalb et al., 2010), 2) scene classifications for 26 common scene categories (Xiao et al., 2010), and 3) region based detections for stuff categories (e.g. grass, road, sky) (Ordonez et al., 2011). All content types are pre-computed on the million database photos, and caption parsing is performed using the Berkeley PCFG parser (Petrov et al., 2006; Petrov and Klein, 2007). Given a query image, we identify content elements present using the above classifiers and detectors and then retrieve phrases referring to those content elements from the database. For example, if we detect a horse in a query image, then we retrieve phrases referring to visually similar horses in the database by compar</context>
<context position="8632" citStr="Xiao et al., 2010" startWordPosition="1303" endWordPosition="1306"> verb phrase referring to the object category. [3] Region/Stuff PPs We collect prepositional phrases for each query stuff detection (e.g. “in the sky&amp;quot;, “on the road”) by measuring visual similarity of appearance (color, texton, HoG) and geometric configuration (object-stuff relative location and distance) between query and database detections. [4] Scene PPs We also collect prepositonal phrases referring to general image scene context (e.g. “at the market”, “on hot summer days”, “in Sweden”) based on global scene similarity computed using L2 distance between scene classification score vectors (Xiao et al., 2010) computed on the query and database images. 3 Overview of ILP Formulation For each image, we aim to generate multiple sentences, each sentence corresponding to a single distinct object detected in the given image. Each sentence comprises of the NP for the main object, and a subset of the corresponding VP, region/stuff PP, and scene PP retrieved in §2. We consider four different types of operations to generate the final description for each image: T1. Selecting the set of objects to describe (one object per sentence). T2. Re-ordering sentences (i.e., re-ordering objects). T3. Selecting the set </context>
</contexts>
<marker>Xiao, Hays, Ehinger, Oliva, Torralba, 2010</marker>
<rawString>Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. 2010. Sun database: Large-scale scene recognition from abbey to zoo. In CVPR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yezhou Yang</author>
<author>Ching Teo</author>
<author>Hal Daume</author>
<author>Yiannis Aloimonos</author>
</authors>
<title>Corpus-guided sentence generation of natural images.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>444--454</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="2114" citStr="Yang et al. (2011)" startWordPosition="296" endWordPosition="299">ta (2010)). This allows descriptions to be generated using effective summarization techniques with relatively surface level image understanding. However, such text (e.g., news articles or encyclopedic text) is often only loosely related to an image’s specific content and many natural images do not come with associated text for summarization. In contrast, other recent work has focused more on the visual recognition aspect by detecting content elements (e.g., scenes, objects, attributes, actions, etc) and then composing descriptions from scratch (e.g., Yao et al. (2010), Kulkarni et al. (2011), Yang et al. (2011), Li et al. (2011)), or by retrieving existing whole descriptions from visually similar images (e.g., Farhadi et al. (2010), Ordonez et al. (2011)). For the latter approaches, it is unrealistic to expect that there will always exist a single complete description for retrieval that is pertinent to a given query image. For the former approaches, visual recognition first generates an intermediate representation of image content using a set of English words, then language generation constructs a full description by adding function words and optionally applying simple re-ordering. Because the gener</context>
<context position="21682" citStr="Yang et al. (2011)" startWordPosition="3579" endWordPosition="3582">as well. TestSet: Because computer vision is a challenging and unsolved problem, we restrict our query set to images where we have high confidence that visual recognition algorithms perform well. We collect 1000 test images by running a large number (89) of object detectors on 20,000 images and selecting images that receive confident object detection scores, with some preference for images with multiple object detections to obtain good examples for testing discourse constraints. Baselines: We compare our ILP approaches with two nontrivial baselines: the first is an HMM approach (comparable to Yang et al. (2011)), which takes as input the same set of candidate phrases described in §2, but for decoding, we fix the ordering of phrases as [ NP – VP – Region PP – Scene PP] and find the best combination of phrases using the Viterbi algorithm. We use the same rich set of pairwise FNGRAM = 1 sijpq Fsijpq = α · FNGRAM sijpq + β · FCO sijpq (23) α + β (20) 6 Evaluation 364 cognitive phrases: HMM HMM ILP ILP with w/o with w/o 0.111 0.114 0.114 0.116 Table 1: Automatic Evaluation ILP selection rate ILP V.S. HMM (w/o cogn) 67.2% ILP V.S. HMM (with cogn) 66.3% Table 2: Human Evaluation (without images) ILP select</context>
</contexts>
<marker>Yang, Teo, Daume, Aloimonos, 2011</marker>
<rawString>Yezhou Yang, Ching Teo, Hal Daume III, and Yiannis Aloimonos. 2011. Corpus-guided sentence generation of natural images. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 444–454, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Z Yao</author>
<author>Xiong Yang</author>
<author>Liang Lin</author>
<author>Mun Wai Lee</author>
<author>Song-Chun Zhu</author>
</authors>
<title>I2t: Image parsing to text description.</title>
<date>2010</date>
<booktitle>Proc. IEEE,</booktitle>
<volume>98</volume>
<issue>8</issue>
<contexts>
<context position="2070" citStr="Yao et al. (2010)" startWordPosition="288" endWordPosition="291">, Aker and Gaizauskas (2010), Feng and Lapata (2010)). This allows descriptions to be generated using effective summarization techniques with relatively surface level image understanding. However, such text (e.g., news articles or encyclopedic text) is often only loosely related to an image’s specific content and many natural images do not come with associated text for summarization. In contrast, other recent work has focused more on the visual recognition aspect by detecting content elements (e.g., scenes, objects, attributes, actions, etc) and then composing descriptions from scratch (e.g., Yao et al. (2010), Kulkarni et al. (2011), Yang et al. (2011), Li et al. (2011)), or by retrieving existing whole descriptions from visually similar images (e.g., Farhadi et al. (2010), Ordonez et al. (2011)). For the latter approaches, it is unrealistic to expect that there will always exist a single complete description for retrieval that is pertinent to a given query image. For the former approaches, visual recognition first generates an intermediate representation of image content using a set of English words, then language generation constructs a full description by adding function words and optionally ap</context>
</contexts>
<marker>Yao, Yang, Lin, Lee, Zhu, 2010</marker>
<rawString>Benjamin Z. Yao, Xiong Yang, Liang Lin, Mun Wai Lee, and Song-Chun Zhu. 2010. I2t: Image parsing to text description. Proc. IEEE, 98(8).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>