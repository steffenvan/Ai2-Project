<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000058">
<title confidence="0.995047">
Statistical Script Learning with Multi-Argument Events
</title>
<author confidence="0.998366">
Karl Pichotta
</author>
<affiliation confidence="0.9980505">
Department of Computer Science
The University of Texas at Austin
</affiliation>
<email confidence="0.991111">
pichotta@cs.utexas.edu
</email>
<author confidence="0.948603">
Raymond J. Mooney
</author>
<affiliation confidence="0.99704">
Department of Computer Science
The University of Texas at Austin
</affiliation>
<email confidence="0.995406">
mooney@cs.utexas.edu
</email>
<sectionHeader confidence="0.993822" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99986205882353">
Scripts represent knowledge of stereotyp-
ical event sequences that can aid text un-
derstanding. Initial statistical methods
have been developed to learn probabilis-
tic scripts from raw text corpora; how-
ever, they utilize a very impoverished rep-
resentation of events, consisting of a verb
and one dependent argument. We present
a script learning approach that employs
events with multiple arguments. Unlike
previous work, we model the interactions
between multiple entities in a script. Ex-
periments on a large corpus using the task
of inferring held-out events (the “narrative
cloze evaluation”) demonstrate that mod-
eling multi-argument events improves pre-
dictive accuracy.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999928">
Scripts encode knowledge of stereotypical events,
including information about their typical ordered
sequences of sub-events and corresponding argu-
ments (Schank and Abelson, 1977). The clas-
sic example is the “restaurant script,” which en-
codes knowledge about what normally happens
when dining out. Such knowledge can be used
to improve text understanding by supporting in-
ference of missing actions and events, as well as
resolution of lexical and syntactic ambiguities and
anaphora (Rahman and Ng, 2012). For example,
given the text “John went to Olive Garden and or-
dered lasagna. He left a big tip and left,” an infer-
ence that scripts would ideally allow us to make is
“John ate lasagna.”
There is a small body of recent research on auto-
matically learning probabilistic models of scripts
from large corpora of raw text (Manshadi et al.,
2008; Chambers and Jurafsky, 2008; Chambers
and Jurafsky, 2009; Jans et al., 2012). However,
this work uses a very impoverished representation
of events that only includes a verb and a single de-
pendent entity. We propose a more complex multi-
argument event representation for use in statistical
script models, capable of directly capturing inter-
actions between multiple entities. We present a
method for learning such a model, and provide ex-
perimental evidence that modeling entity interac-
tions allows for better prediction of events in docu-
ments, compared to previous single-entity “chain”
models. We also compare to a competitive base-
line not used in previous work, and introduce a
novel evaluation metric.
</bodyText>
<sectionHeader confidence="0.981127" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999935">
The idea of representing stereotypical event se-
quences for textual inference originates in the
seminal work of Schank and Abelson (1977).
Early scripts were manually engineered for spe-
cific domains; however, Mooney and DeJong
(1985) present an early knowledge-based method
for learning scripts from a single document. These
early scripts (and methods for learning them) were
non-statistical and fairly brittle.
Chambers and Jurafsky (2008) introduced a
method for learning statistical scripts that, using a
much simpler event representation that allows for
efficient learning and inference. Jans et al. (2012)
use the same simple event representation, but in-
troduce a new model that more accurately predicts
test data. These methods only model the actions of
a single participant, called the protagonist. Cham-
bers and Jurafsky (2009) extended their approach
to the multi-participant case, modeling the events
in which all of the entities in a document are in-
volved; however, their method cannot represent in-
teractions between multiple entities.
Balasubramanian et al. (2012; 2013) describe
the Rel-gram system, a Markov model similar to
that of Jans et al. (2012), but with tuples instead
of (verb, dependency) pairs. Our approach is sim-
</bodyText>
<page confidence="0.956376">
220
</page>
<note confidence="0.9927515">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 220–229,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999112666666667">
ilar, but instead of modeling a distribution over co-
occurring verbs and nominal arguments, we model
interactions between entities directly by incorpo-
rating coreference information into the model.
Previous statistical script learning systems pro-
ceed broadly as follows. For a document D:
</bodyText>
<listItem confidence="0.973893833333333">
1. Run a dependency parser on D, to match up
verbs with their argument NPs.
2. Run a coreference resolver on D to determine
which NPs likely refer to the same entity.
3. Construct a sequence of event objects, using
syntactic and coreference information.
</listItem>
<bodyText confidence="0.999822307692308">
One can then build a statistical model of the event
sequences produced by Step 3. Such a model may
be evaluated using the narrative cloze evaluation,
described in Section 4.1, in which we hold out an
event from a sequence and attempt to infer it.
The major difference between the current work
and previous work is that the event sequences pro-
duced in Step 3 are of a different sort from those
in other models. Our events are more structured,
as described in Section 3.1, and we produce one
event sequence per document, instead of one event
sequence per entity. This requires a different sta-
tistical model, as described in Section 3.2.
</bodyText>
<sectionHeader confidence="0.998528" genericHeader="method">
3 Script Models
</sectionHeader>
<bodyText confidence="0.9998736">
In Section 3.1, we describe the multi-argument
events we use as the basis of our script models.
Section 3.2 describes a script model using these
events, and Section 3.3 describes the baseline sys-
tems to which we compare.
</bodyText>
<subsectionHeader confidence="0.998987">
3.1 Multi-Argument Events
</subsectionHeader>
<bodyText confidence="0.907207642857143">
Statistical scripts are models of stereotypical se-
quences of events. In Chambers and Juraf-
sky (2008; 2009) and Jans et al. (2012), events
are (verb, dependency) pairs, forming “chains,”
grouped according to the entity involved. For ex-
ample, the text
(1) Mary emailed Jim and he responded to her
immediately.
yields two chains. First, there is a chain for Mary:
(email, subject)
(respond, object)
indicating that Mary was the subject of an email-
ing event and the object of a responding event.
Second, there is a chain for Jim:
</bodyText>
<equation confidence="0.55405">
(email, object)
(respond, subject)
</equation>
<bodyText confidence="0.9998814">
indicating that Jim was the object of an emailing
event and the subject of a responding event. Thus,
one document produces many chains, each cor-
responding to an entity. Note that a single verb
may produce multiple pair events, each present
in a chain corresponding to one of the verb’s ar-
guments. Note also that there is no connection
between the different events produced by a verb:
there is nothing connecting (email, subject) in
Mary’s chain with (email, object) in Jim’s chain.
We propose a richer event representation, in
which a document is represented as a single se-
quence of event tuples, the arguments of which are
entities. Each entity may be mentioned in many
events, and, unlike previous work, each event may
involve multiple entities. For example, sentence
(1) will produce a single two-event sequence, the
first event representing Mary emailing Jim, and the
second representing Jim responding to Mary.
Formally, an entity is represented by a con-
stant, and noun phrases are mapped to entities,
where two noun phrases are mapped to the same
constant if and only if they corefer. A multi-
argument event is a relational atom v(es, eo, ep),
where v is a verb lemma, and es, eo, and ep are
possibly-null entities. The first entity, es, stands
in a subject relation to the verb v; the second, eo,
is the direct object of v; the third ep stands in
a prepositional relation to v. One of these enti-
ties is null (written as “·”) if and only if no noun
phrase stands in the appropriate relation to v. For
example, Mary hopped would be represented as
hop(mary, ·, ·), while Mary gave the book to John
would be give(mary, book, john). In this formula-
tion, Example (1) produces the sequence
</bodyText>
<equation confidence="0.947622">
email(m, j, ·)
respond(j, m, ·)
</equation>
<bodyText confidence="0.998121545454546">
where m and j are entity constants representing
all mentions of Mary and Jim, respectively. Note
that this formulation is capable of capturing inter-
actions between entities: we directly encode the
fact that after one person emails another, the lat-
ter responds to the former. In contrast, pair events
can capture only that after an entity emails, they
are responded to (or after they are emailed, they
respond). Multi-argument events capture more of
the basic event structure of text, and are therefore
well-suited as a representation for scripts.
</bodyText>
<page confidence="0.995596">
221
</page>
<subsectionHeader confidence="0.999916">
3.2 Multi-argument Statistical Scripts
</subsectionHeader>
<bodyText confidence="0.999940333333333">
We now describe our script model. Section 3.2.1
describes our method of estimating a joint prob-
ability distribution over pairs of events, modeling
event co-occurrence, and Section 3.2.2 shows how
this co-occurrence probability can be used to infer
new events from a set of known events.
</bodyText>
<subsectionHeader confidence="0.984034">
3.2.1 Estimating Joint Probabilities
</subsectionHeader>
<bodyText confidence="0.998811666666667">
Suppose we have a sequence of multi-argument
events, each of which is a verb with entities as ar-
guments. We are interested in predicting which
event is most likely to have happened at some
point in the sequence. Our model will require
a conditional probability P(ala0), the probability
of seeing event a after event a0, given we have
observed a0. However, as described below, di-
rectly estimating this probability is more compli-
cated than in previous work because events now
have additional structure.
By definition, we have
</bodyText>
<equation confidence="0.999361">
P(a2�a1) = P(a1)
</equation>
<bodyText confidence="0.999384608695652">
where P(a1, a2) is the probability of seeing a1
and a2, in order. The most straightforward way
to estimate P(a1, a2) is, if possible, by counting
the number of times we observe a1 and a2 co-
occurring and normalizing the function to sum to
1 over all pairs (a1, a2). For Chambers and Ju-
rafsky (2008; 2009) and Jans et al. (2012), such a
Maximum Likelihood Estimate is straightforward
to arrive at: events are (verb, dependency) pairs,
and two events co-occur when they are in the same
event chain, relating to the same entity (Jans et al.
(2012) further require a1 and a2 to be near each
other). One need simply traverse a training corpus
and count the number of times each pair (a1, a2)
co-occurs. The Rel-grams of Balasubramanian et
al. (2012; 2013) admit a similar strategy: to arrive
at a joint distribution of pairwise co-occurrence,
one can simply count co-occurrence of ground re-
lations in a corpus and normalize.
However, given two multi-argument events of
the form v(es, eo, ep), this strategy will not suffice.
For example, if during training we observe the two
co-occurring events
</bodyText>
<listItem confidence="0.894136733333333">
(2) ask(mary, bob, question)
answer(bob, ·, ·)
we would like this to lend evidence to the
co-occurrence of events ask(x, y, z) and
Algorithm 1 Learning with entity substitution
1: for a1, a2 E evs do
2: N(a1, a2) +— 0
3: end for
4: for D E documents do
5: for a1, a2 E coocurEvs(D) do
6: for u E subs(a1, a2) do
7: N(u(a1), u(a2)) += 1
8: end for
9: end for
10: end for
</listItem>
<bodyText confidence="0.998557487179487">
answer(y, ·, ·) for all distinct entities x, y,
and z. If we were to simply keep the entities as
they are and calculate raw co-occurrence counts,
we would get evidence only for x = mary,
y = bob, and z = question.
One approach to this problem would be to de-
ploy one of many previously described Statistical
Relational Learning methods, for example Logi-
cal Hidden Markov Models (Kersting et al., 2006)
or Relational Markov Models (Anderson et al.,
2002). These methods can learn various statisti-
cal relationships between relational logical atoms
with variables, of the sort considered here. How-
ever, we investigate a simpler option.
The most important relationship between the
entities in two multi-argument events concerns
their overlapping entities. For example, to de-
scribe the relationship between the three entities
in (2), it is most important to note that the object
of the first event is identical with the subject of the
second (namely, both are bob). The identity of the
non-overlapping entities mary and question is not
important for capturing the relationship between
the two events.
We note that two multi-argument events
v(es, eo, ep) and v0(e0 s, e0 o, e0p), share at most three
entities. We thus introduce four variables x, y, z,
and O. The three variables x, y, and z repre-
sent arbitrary distinct entities, and the fourth, O,
stands for “Other,” for entities not shared between
the two events. We can rewrite the entities in our
two multi-argument events using these variables,
with the constraint that two identical (i.e. corefer-
ent) entities must be mapped to the same variable
in {x, y, z}, and no two distinct entities may map
to the same variable in {x, y, z}. This formulation
simplifies calculations while still capturing pair-
wise entity relationships between events.
Algorithm 1 gives the pseudocode for the learn-
</bodyText>
<equation confidence="0.975141">
P(a1, a2)
</equation>
<page confidence="0.986748">
222
</page>
<bodyText confidence="0.999943037037037">
ing method. This populates a co-occurrence
matrix N, where entry N(a1, a2) gives the co-
occurrence count of events a1 and a2. The vari-
able evs in line 1 is the set of all events in our
model, which are of the form v(es, eo, ep), with v
a verb lemma and es, eo, ep E {x, y, z, O}. The
variable documents in line 4 is the collection
of documents in our training corpus. The func-
tion cooccurEvs in line 5 takes a document D
and returns all ordered pairs of co-occurring events
in D, where, following the 2-skip bigram model
of Jans et al. (2012), and similar to Balasubrama-
nian et al. (2012; 2013), two events a1 and a2 are
said to co-occur if they occur in order, in the same
document, with at most two intervening events be-
tween them.1 The function subs in line 6 takes
two events and returns all variable substitutions σ
mapping from entities mentioned in the events a1
and a2 to the set {x, y, z, O}, such that two coref-
erent entities map to the same element of {x, y, z}.
A substitution σ applied to an event v(es, eo, ep),
as in line 7, is defined as v(σ(es), σ(eo), σ(ep)),
with the null entity mapped to itself.
Once we have calculated N(a1, a2) using Algo-
rithm 1, we may define P(a1, a2) for two events
a1 and a2, giving an estimate for the probability
of observing a2 occurring after a1, as
</bodyText>
<equation confidence="0.992348333333333">
N(
P(a1, a2) = Ea N( a�) a2) . (3)
1,a2
</equation>
<bodyText confidence="0.946088">
We may then define the conditional probability of
seeing a2 after a1, given an observation of a1:
</bodyText>
<equation confidence="0.799478">
P(a1, a2)
</equation>
<footnote confidence="0.868587">
1Other notions of co-occurrence could easily be substi-
tuted here.
</footnote>
<bodyText confidence="0.999891">
model takes an ordered sequence of events and
a position in that sequence, and guesses events
that likely occurred at that position. In that work,
events are (verb, dependency) pairs, and an event
sequence consists of all such pairs involving a par-
ticular entity. We use this model in the multi-
argument event setting, in which a document pro-
duces a single sequence of multi-argument events.
Let A be an ordered list of events, and let p be an
integer between 1 and |A|, the length of A. For i =
1, ... , |A|, define ai to be the ith element of A.
We follow Jans et al. (2012) by scoring a candidate
event a according to its probability of following all
of the events before position p, and preceding all
events after position p. That is, we rank candidate
events a by maximizing S(a), defined as
</bodyText>
<equation confidence="0.999198666666667">
p−1 |A|
S(a) = log P(a|ai) + L log P(ai|a) (5)
i=1 i=p
</equation>
<bodyText confidence="0.999641833333333">
with conditional probabilities P(a|a0) calculated
using (4). Each event in ai E A independently
contributes to a candidate a’s score; the ordering
between a and ai is taken into account, but the or-
dering between the different events ai E A does
not directly affect a’s score.
</bodyText>
<subsectionHeader confidence="0.999481">
3.3 Baseline Systems
</subsectionHeader>
<bodyText confidence="0.99997775">
We describe the baseline systems against which
we compare the performance of the multi-
argument script system described in section 3.2.
These systems infer new events (either multi-
argument or pair events) given the events con-
tained in a document.
Performance of these systems is measured using
the narrative cloze task, in which we hold out a sin-
gle event (either a multi-argument or pair event),
and rate a system by its ability to infer this event,
given the other events in a document. The narra-
tive cloze task is described in detail in Section 4.1.
</bodyText>
<sectionHeader confidence="0.546194" genericHeader="method">
3.3.1 Random Model
</sectionHeader>
<bodyText confidence="0.99978475">
The simplest baseline we compare to is the ran-
dom baseline, which outputs randomly selected
events observed during training. This model can
guess either multi-argument or pair events.
</bodyText>
<sectionHeader confidence="0.452639" genericHeader="method">
3.3.2 Unigram Model
</sectionHeader>
<bodyText confidence="0.999178">
The unigram system guesses events ordered by
prior probability, as calculated from the train-
ing set. If scripts are viewed as n-gram models
</bodyText>
<equation confidence="0.9893385">
P(a2|a1) =
Ea, P(a1,a0)
N(a1, a2) =(4)
Ea, N(a1, a0).
</equation>
<subsubsectionHeader confidence="0.527534">
3.2.2 Inferring Events
</subsubsectionHeader>
<bodyText confidence="0.999979454545455">
Suppose we have a sequence of multi-argument
events extracted from a document. A natural task
for a statistical script model is to infer what other
events likely occurred, given the events explic-
itly stated in a document. Chambers and Jurafsky
(2008; 2009) treat the events involving an entity
as an unordered set, inferring the most likely ad-
ditional event, with no relative ordering between
the inferred event and known events. We adopt
the model of Jans et al. (2012), which was demon-
strated to give better empirical performance. This
</bodyText>
<page confidence="0.993929">
223
</page>
<bodyText confidence="0.9997538">
over events, this baseline corresponds to a bag-of-
words unigram model. In this model, events are
assumed to occur independently, drawn from a sin-
gle distribution. This model can be used to guess
either multi-argument or pair events.
</bodyText>
<subsectionHeader confidence="0.703169">
3.3.3 Single Protagonist Model
</subsectionHeader>
<bodyText confidence="0.999770777777778">
We refer to the system of Jans et al. (2012) as the
single protagonist system. This model takes a
single sequence of (verb, dependency) pair events,
all relating to a single entity. It then produces
a list of pair events, giving the model’s top pre-
dictions for additional events involving the entity.
This model maximizes the objective given in (5),
with the sequence A (and the candidate guesses a)
comprised of pair events.
</bodyText>
<sectionHeader confidence="0.775399" genericHeader="method">
3.3.4 Multiple Protagonist Model
</sectionHeader>
<bodyText confidence="0.986441242424242">
The multiple protagonist system infers multi-
argument events. While this method is not de-
scribed in previous work, it is the most direct way
of guessing a full multi-argument event using a
single protagonist script model.
The multiple protagonist system uses a single-
protagonist model, which models pair events, to
predict multi-argument events, given a sequence
of known multi-argument events. Suppose we
have a non-empty set E of entities mentioned in
the known events. We describe the most direct
method of using a single-protagonist system to in-
fer additional multi-argument events involving E.
A multi-argument event a = v(es, eo, ep) repre-
sents three pairs: (v, es), (v, eo), and (v, ep). The
multiple protagonist model scores an event a ac-
cording to the score the single protagonist model
assigns to these three pairs individually.
For entity e E E in some multi-argument event
in a document, we first extract the sequence of
(verb, dependency) pairs corresponding to e from
all known multi-argument events. For a pair d,
we calculate the score Se(d), the score the sin-
gle protagonist system assigns the pair d, given the
known pairs corresponding to e. If e has no known
pairs corresponding to it (in the cloze evaluation
described below, this will happen if e occurs only
in the held-out event), we fall back to calculating
Se(d) with a unigram model, as described in Sec-
tion 3.3.2, over (verb, dependency) pair events.
We then rank a multi-argument event a =
v(es, eo, ep), with es, eo, ep E E, with the follow-
ing objective function:
</bodyText>
<equation confidence="0.997961">
M(a) =Se3((v, subj)) + Seo((v, obj))+
Sep((v, prep)) (6)
</equation>
<bodyText confidence="0.999975333333333">
where, for null entity e, we define Se(d) = 0 for
all d. In the cloze evaluation, E will be the entities
in the held-out event. Each entity in a contributes
independently to the score M(a), based on the
known (verb, dependency) pairs involving that en-
tity. This model scores a multi-argument event a
by combining one independent single-protagonist
model for every entity in a.
This model is similar to the multi-participant
narrative schemas described in Chambers and Ju-
rafsky (2009), but whereas they infer bare verbs,
we infer an entire multi-argument event.
</bodyText>
<sectionHeader confidence="0.999845" genericHeader="method">
4 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.996528">
4.1 Evaluation Task
</subsectionHeader>
<bodyText confidence="0.99995521875">
We follow previous work in using the narrative
cloze task to evaluate statistical scripts (Chambers
and Jurafsky, 2008; Chambers and Jurafsky, 2009;
Jans et al., 2012). The task is as follows: given
a sequence of events al, ... , an from a document,
hold out some event ap and attempt to predict that
event, given the other events in the sequence. As
we cannot automatically evaluate the prediction of
truly unmentioned events in a document, this eval-
uation acts as a straightforward proxy.
In the aforementioned work, the cloze task is
to guess a pair event, given the other events in
which the held-out pair’s entity occurs. In Section
4.2.2, we evaluate directly on this task of guess-
ing pair events. However, in Section 4.2.1, we
evaluate on the task of guessing a multi-argument
event, given all other events in a document and the
entities mentioned in the held-out event. This is,
we argue, the most natural way to adapt the cloze
evaluation to the multi-argument event setting: in-
stead of guessing a held-out pair event based on
the other events involving its lone entity, we will
guess a held-out multi-argument event based on
the other events involving any of its entities.
A document may contain arbitrarily many enti-
ties. The script model described in Section 3.2.1,
however, only models events involving entities
from a closed class of four variables {x, y, z, O}.
We therefore rewrite entities in a document’s se-
quences of events to the variables {x, y, z, O} in
a way that maintains all pairwise relationships be-
tween the held-out event and others. That is, if the
</bodyText>
<page confidence="0.995908">
224
</page>
<bodyText confidence="0.99815625">
held-out event shares an entity with another event,
this remains true after rewriting.
We perform entity rewriting relative to a single
held-out event, proceeding as follows:
</bodyText>
<listItem confidence="0.993541888888889">
• Any entity in the held-out event that is men-
tioned at least once in another event gets
rewritten consistently to one of x, y, or z,
such that distinct entities never get rewritten
to the same variable.
• Any entity mentioned only in the held-out
event is rewritten as O.
• All entities not present in the held-out event
are rewritten as O.
</listItem>
<bodyText confidence="0.9796765">
This simplification removes structure from the
original sequence, but retains the important pair-
wise entity relationships between the held-out
event and the other events.
</bodyText>
<subsectionHeader confidence="0.935441">
4.2 Experimental Evaluation
</subsectionHeader>
<bodyText confidence="0.99183604">
For each document, we use the Stanford depen-
dency parser (De Marneffe et al., 2006) to get syn-
tactic information about the document; we then
use the Stanford coreference resolution engine
(Raghunathan et al., 2010) to get (noisy) equiva-
lence classes of coreferent noun phrases in a doc-
ument.2 We train on approximately 1.1M arti-
cles from years 1994-2006 of the NYT portion
of the Gigaword Corpus, Third Edition (Graff et
al., 2007), holding out a random subset of the arti-
cles from 1999 for development and test sets. Our
test set consists of 10,000 randomly selected held-
out events, and our development set is 500 disjoint
randomly selected held-out events. To remove du-
plicate documents, we hash the first 500 characters
of each article and remove any articles with hash
collisions. We use add-one smoothing on all joint
probabilities. To reduce the size of our model, we
remove all events that occur fewer than 50 times.3
We evaluate performance using the following
two metrics:
1. Recall at 10: Following Jans et al. (2012),
we measure performance by outputting the
top 10 guesses for each held-out event and
calculating the percentage of such lists con-
</bodyText>
<footnote confidence="0.976275">
2We use version 1.3.4 of the Stanford CoreNLP system.
3A manual inspection reveals that the majority of these
removed events come from noisy text or parse errors.
</footnote>
<bodyText confidence="0.997984333333333">
taining the correct answer.4 This value will
be between 0 and 1, with 1 indicating perfect
system performance.
</bodyText>
<listItem confidence="0.928249">
2. Accuracy: A multi-argument event
</listItem>
<bodyText confidence="0.950533413043478">
v(es, eo, ep) has four components; a pair
event has two components. For a held-out
event, we may judge the accuracy of a
system’s top guess by giving one point for
getting each of its components correct and
dividing by the number of possible points.
We average this value over the test set,
yielding a value between 0 and 1, with 1
indicating perfect system performance. This
is a novel evaluation metric for the script
learning task.
These metrics target a system’s most confident
predicted events: we argue that a script system is
best evaluated by its top inferences.
In Section 4.2.1, we evaluate on the task of in-
ferring multi-argument events. In Section 4.2.2,
we evaluate on the task of guessing pair events.
4.2.1 System Comparison on Multi-argument
Events
We first compare system performance on inferring
multi-argument events, evaluated on the narrative
cloze task as described in Section 4.1, using the
corpora and metrics described in Section 4.2. We
compare against three baselines: the uninformed
random baseline from Section 3.3.1, the unigram
system from 3.3.2, and the multiple protagonist
system from Section 3.3.4.
The joint system guesses the held-out event,
given the other events in the document that involve
the entities in that held-out tuple. The system or-
ders candidate events a by their scores S(a), as
given in Equation (5). This is the primary sys-
tem described in this paper, modeling full multi-
argument events directly.
Table 1 gives the recall at 10 (“R@10”) and ac-
curacy scores for the different systems. The uni-
gram system is quite competitive, achieving per-
formance comparable to the multiple protagonist
system on accuracy, and superior performance on
recall at 10.
Evaluating by the recall at 10 metric, the joint
system provides a 2.9% absolute (13.2% relative)
improvement over the unigram system, and a 3.6%
4Jans et al. (2012) instead use recall at 50, but we observe,
as they also report, that the comparative differences between
systems using recall at k for various values of k is similar.
</bodyText>
<page confidence="0.996842">
225
</page>
<table confidence="0.9989444">
Method R@10 Accuracy
Random 0.001 0.334
Unigram 0.216 0.507
Multiple Protagonist 0.209 0.504
Joint 0.245 0.549
</table>
<tableCaption confidence="0.866254">
Table 1: Results for multi-argument events.
</tableCaption>
<table confidence="0.99969">
Method R@10 Accuracy
Random 0.001 0.495
Unigram 0.297 0.552
Single Protagonist 0.282 0.553
Joint Pair 0.336 0.561
</table>
<tableCaption confidence="0.994238">
Table 2: Results for pair events.
</tableCaption>
<bodyText confidence="0.999786928571429">
absolute (17.2% relative) improvement over the
multiple protagonist system. These differences
are statistically significant (p &lt; 0.01) by McNe-
mar’s test. By accuracy, the joint system provides
a 4.2% absolute (8.3% relative) improvement over
the unigram model, and a 4.5% absolute (8.9%
relative) improvement over the multiple protago-
nist model. Accuracy differences are significant
(p &lt; 0.01) by a Wilcoxon signed-rank test.
These results provide evidence that directly
modeling full multi-argument events, as opposed
to modeling chains of (verb, dependency) pairs for
single entities, allows us to better infer held-out
verbs with all participating entities.
</bodyText>
<subsectionHeader confidence="0.529469">
4.2.2 System Comparison on Pair Events
</subsectionHeader>
<bodyText confidence="0.999810957446809">
In Section 4.2.1, we adapted a baseline pair-event
system to the task of guessing multi-argument
events. We may also do the converse, adapting our
multi-argument event system to the task of guess-
ing the simpler pair events. That is, we infer a full
multi-argument event and extract from it a (sub-
ject,verb) pair relating to a particular entity. This
allows us to compare directly to previously pub-
lished methods.
The random, unigram, and single protagonist
systems are pair-event systems described in Sec-
tions 3.3.1, 3.3.2, and 3.3.3, respectively. The
joint pair system takes the multi-argument events
guessed by the joint system of Section 4.2.1 and
converts them to pair events by discarding any in-
formation not related to the target entity; that is, if
the held-out pair event relates to an entity e, then
every occurrence of e as an argument of a guessed
multi-argument event will be converted into a sin-
gle pair event, scored identically to its original
multi-argument event. Ties are broken arbitrarily.
Table 2 gives the comparative results for these
four systems. The test set is constructed by ex-
tracting one pair event from each of the 10,000
multi-argument events in the test set used in Sec-
tion 4.2.1, such that the extracted pair event relates
to an entity with at least one additional known pair
event. Evaluating by recall at 10, the joint sys-
tem provides a 3.9% absolute (13.1% relative) im-
provement over the unigram baseline, and a 5.4%
absolute (19.1% relative) improvement over the
single protagonist system. These differences are
significant (p &lt; 0.01) by McNemar’s test. By
accuracy, the joint system provides a 0.9% abso-
lute (1.6% relative) improvement over the unigram
model, and a 0.8% absolute (1.4% relative) im-
provement over the single protagonist model. Ac-
curacy differences are significant (p &lt; 0.01) by a
Wilcoxon signed-rank test.
These results indicate that modeling multi-
argument event sequences allows better inference
of simpler pair events. These performance im-
provements may be due to the fact that the joint
model conditions on information not representable
in the single protagonist model (namely, all of the
events in which a multi-argument event’s entities
are involved).
</bodyText>
<sectionHeader confidence="0.999855" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.99575515">
The procedural encoding of common situations
for automated reasoning dates back decades. The
frames of Minsky (1974), schemas of Rumelhart
(1975), and scripts of Schank and Abelson (1977)
are early examples. These models use quite com-
plex representations for events, with many differ-
ent relations between events. They are not statis-
tical, and use separate models for different scenar-
ios (e.g. the “restaurant script” is different from
the “bank script”). Generally, they require humans
to encode procedural information by hand; see,
however, Mooney and DeJong (1985) for an early
method for learning scripts automatically from a
document. Miikkulainen (1990; 1993) gives a hi-
erarchical Neural Network system which stores
sequences of events from text in episodic memory,
capable of simple question answering.
Regneri et al. (2010) and Li et al. (2012)
give methods for using crowdsourcing to cre-
ate situation-specific scripts. These methods
</bodyText>
<page confidence="0.99618">
226
</page>
<bodyText confidence="0.999985018867924">
help alleviate the bottleneck of the knowledge-
engineering required for traditionally conceived
script systems. These systems are precision-
oriented: they create small, highly accurate scripts
for very limited scenarios. The current work,
in contrast, focuses on building high-recall mod-
els of general event sequences. There are also a
number of systems addressing the related problem
of modeling domain-specific human-human dia-
log for building dialog systems (Bangalore et al.,
2006; Chotimongkol, 2008; Boyer et al., 2009).
There have been a number of recent approaches
to learning statistical scripts. Chambers and Ju-
rafsky (2008) and Jans et al. (2012) give methods
for learning models of (verb, dependency) pairs,
as described above. Manshadi et al. (2008) give
an n-gram model for sequences of verbs and their
patients. McIntyre and Lapata (2009; 2010) use
script objects learned from corpora of fairy tales
to automatically generate stories. Chambers and
Jurafsky (2009) extend their previous model to
incorporate multiple entities, but do not directly
model the different arguments of an event. Bam-
man et al. (2013) learn latent character personas
from film summaries, associating character types
with stereotypical actions; they focus on identify-
ing persona types, rather than event inference.
Manshadi et al. (2008) and Balasubramanian et
al. (2012; 2013) give approaches similar to the
current work for modeling sequences of events as
n-grams. These methods differ from the current
work in that they do not model entities directly, in-
stead modeling co-occurrence of particular nouns
standing as arguments to particular verbs. Lewis
and Steedman (2013) build clusters of relations
similar to these events, finding such clusters help-
ful to question answering and textual inference.
There has also been recent interest in the related
problem of automatically learning event frames
(Bejan, 2008; Chambers and Jurafsky, 2011; Che-
ung et al., 2013; Chambers, 2013). These ap-
proaches focus on identifying frames for infor-
mation extraction tasks, as opposed to inferring
events directly. Balasubramanian et al. (2013) give
an event frame identification method, developed in
parallel with the current work, using sequences of
tuples similar to our multi-argument events, noting
coherence issues with pair events. Their formu-
lation differs from ours primarily in that they do
not incorporate coreference information into their
event co-occurrence distribution, and evaluate us-
ing human judgments of frame coherence rather
than a narrative cloze test.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="method">
6 Future Work
</sectionHeader>
<bodyText confidence="0.999855">
We have evaluated only one type of multi-
argument event inference, in which a script infers
an event given a set of entities and the events in-
volving those entities. We claim that this is the
most natural adaptation of the cloze evaluation to
the multi-argument event setting. However, other
types of inferences would be useful as well for
question-answering. Additional script inferences,
and their applications to question answering, are
worth investigating more fully.
The evaluation methodology used here has two
serious benefits: it is totally automatic, and it does
not require labeled data. The cloze evaluation is
intuitively reasonable: a good script system should
be able to predict stated events as having taken
place. Basic pragmatic reasoning, however, tells
us that the most obvious inferable events are not
typically stated in text. This evaluation thus fails
to capture some of the most important common-
sense inferences. Further investigation into evalu-
ation methodologies for script systems is needed.
</bodyText>
<sectionHeader confidence="0.99837" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999985785714286">
We described multi-argument events for statisti-
cal scripts, which can directly encode the pair-
wise entity relationships between events in a doc-
ument. We described a script model that can han-
dle the important aspects of the additional com-
plexity introduced by these events, and a baseline
model that can infer multi-argument events using
single-protagonist chains instead of directly mod-
eling full relations. We introduced the novel uni-
gram baseline model for comparison, as well as
the novel accuracy metric, and provided empir-
ical evidence that modeling full multi-argument
events provides more predictive power than mod-
eling event chains individually.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.993385625">
Thanks to Katrin Erk, Amelia Harrison, and the
DEFT group at UT Austin for helpful discussions.
Thanks also to the anonymous reviewers for their
helpful comments. This research was supported in
part by the DARPA DEFT program under AFRL
grant FA8750-13-2-0026. Some of our experi-
ments were run on the Mastodon Cluster, sup-
ported by NSF Grant EIA-0303609.
</bodyText>
<page confidence="0.995066">
227
</page>
<sectionHeader confidence="0.909377" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.992160145454545">
Corin R Anderson, Pedro Domingos, and Daniel S
Weld. 2002. Relational Markov models and their
application to adaptive web navigation. In Proceed-
ings of the Eighth ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining
(KDD-2002), pages 143–152.
Niranjan Balasubramanian, Stephen Soderland,
Mausam, and Oren Etzioni. 2012. Rel-grams: a
probabilistic model of relations in text. In Proceed-
ings of the Joint Workshop on Automatic Knowledge
Base Construction and Web-scale Knowledge
Extraction at NAACL-HLT 2012 (AKBC-WEKEX
2012), pages 101–105.
Niranjan Balasubramanian, Stephen Soderland,
Mausam, and Oren Etzioni. 2013. Generating
coherent event schemas at scale. In Proceedings
of the 2013 Conference on Empirical Methods in
Natural Language Processing (EMNLP-2013).
David Bamman, Brendan O’Connor, and Noah A.
Smith. 2013. Learning latent personas of film char-
acters. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics
(ACL-13), pages 352–361.
Srinivas Bangalore, Giuseppe Di Fabbrizio, and
Amanda Stent. 2006. Learning the structure of task-
driven human–human dialogs. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics (COLING/ACL-
06), pages 201–208.
Cosmin Adrian Bejan. 2008. Unsupervised discov-
ery of event scenarios from texts. In Prodeedings of
the 21st International Florida Artificial Intelligence
Research Society Conference (FLAIRS-2008), pages
124–129.
Kristy Elizabeth Boyer, Robert Phillips, Eun Young
Ha, Michael D. Wallis, Mladen A. Vouk, and
James C. Lester. 2009. Modeling dialogue structure
with adjacency pair analysis and Hidden Markov
Models. In Proceedings of Human Language Tech-
nologies: The Conference of the North American
Chapter of the Association for Computational Lin-
guistics, Companion Volume: Short Paper (NAACL-
HLT-09 Short), pages 49–52.
Nathanael Chambers and Daniel Jurafsky. 2008. Un-
supervised learning of narrative event chains. In
Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics (ACL-08),
pages 789–797.
Nathanael Chambers and Dan Jurafsky. 2009. Un-
supervised learning of narrative schemas and their
participants. In Joint Conference of the 47th An-
nual Meeting of the Association for Computational
Linguistics and the 4th International Joint Confer-
ence on Natural Language Processing of the Asian
Federation of Natural Language Processing (ACL-
IJCNLP), pages 602–610.
Nathanael Chambers and Dan Jurafsky. 2011.
Template-based information extraction without the
templates. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies (ACL-HLT-
11), pages 976–986.
Nathanael Chambers. 2013. Event schema induc-
tion with a probabilistic entity-driven model. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing (EMNLP-
2013).
Jackie Chi Kit Cheung, Hoifung Poon, and Lucy Van-
derwende. 2013. Probabilistic frame induction. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-13).
Ananlada Chotimongkol. 2008. Learning the struc-
ture of task-oriented conversations from the corpus
of in-domain dialogs. Ph.D. thesis, Carnegie Mellon
University.
Marie-Catherine De Marneffe, Bill MacCartney,
Christopher D Manning, et al. 2006. Generat-
ing typed dependency parses from phrase structure
parses. In Proceedings of the 5th International
Conference on Language Resources &amp; Evaluation
(LREC-2006), volume 6, pages 449–454.
David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda. 2007. English Gigaword Third Edition.
Linguistic Data Consortium.
Bram Jans, Steven Bethard, Ivan Vuli´c, and
Marie Francine Moens. 2012. Skip n-grams
and ranking functions for predicting script events.
In Proceedings of the 13th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL-12), pages 336–344.
Kristian Kersting, Luc De Raedt, and Tapani Raiko.
2006. Logical Hidden Markov Models. Journal of
Artificial Intelligence Research, 25:425–456.
Mike Lewis and Mark Steedman. 2013. Combined
distributional and logical semantics. Transactions
of the Association for Computational Linguistics,
1:179–192.
Boyang Li, Stephen Lee-Urban, Darren Scott Appling,
and Mark O Riedl. 2012. Crowdsourcing narrative
intelligence. Advances in Cognitive Systems, 2:25–
42.
Mehdi Manshadi, Reid Swanson, and Andrew S Gor-
don. 2008. Learning a probabilistic model of event
sequences from internet weblog stories. In Prodeed-
ings of the 21st International Florida Artificial In-
telligence Research Society Conference (FLAIRS-
2008), pages 159–164.
</reference>
<page confidence="0.974043">
228
</page>
<reference confidence="0.998232596153846">
Neil McIntyre and Mirella Lapata. 2009. Learn-
ing to tell tales: A data-driven approach to story
generation. In Joint Conference of the 47th An-
nual Meeting of the Association for Computational
Linguistics and the 4th International Joint Confer-
ence on Natural Language Processing of the Asian
Federation of Natural Language Processing (ACL-
IJCNLP), pages 217–225.
Neil McIntyre and Mirella Lapata. 2010. Plot induc-
tion and evolutionary search for story generation. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics (ACL-10),
pages 1562–1572.
Risto Miikkulainen. 1990. DISCERN: A Distributed
Artificial Neural Network Model of Script Process-
ing and Memory. Ph.D. thesis, University of Cali-
fornia.
Risto Miikkulainen. 1993. Subsymbolic Natural Lan-
guage Processing: An Integrated Model of Scripts,
Lexicon, and Memory. MIT Press, Cambridge, MA.
Marvin Minsky. 1974. A framework for representing
knowledge. Technical report, MIT-AI Laboratory.
Raymond J. Mooney and Gerald F. DeJong. 1985.
Learning schemata for natural language processing.
In Proceedings of the Ninth International Joint Con-
ference on Artificial Intelligence (IJCAI-85), pages
681–687, Los Angeles, CA, August.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP-2010), pages
492–501.
Altaf Rahman and Vincent Ng. 2012. Resolving
complex cases of definite pronouns: the Winograd
schema challenge. In Proceedings of the 2012 Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL-12), pages 777–789.
Michaela Regneri, Alexander Koller, and Manfred
Pinkal. 2010. Learning script knowledge with web
experiments. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics (ACL-10), Uppsala, Sweden, July.
David Rumelhart. 1975. Notes on a schema for sto-
ries. Representation and Understanding: Studies in
Cognitive Science.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
Plans, Goals and Understanding: An Inquiry into
Human Knowledge Structures. Lawrence Erlbaum
and Associates, Hillsdale, NJ.
</reference>
<page confidence="0.998913">
229
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.905426">
<title confidence="0.999814">Statistical Script Learning with Multi-Argument Events</title>
<author confidence="0.999817">Karl Pichotta</author>
<affiliation confidence="0.99573">Department of Computer The University of Texas at</affiliation>
<email confidence="0.995357">pichotta@cs.utexas.edu</email>
<author confidence="0.999902">J Raymond</author>
<affiliation confidence="0.9988335">Department of Computer The University of Texas at</affiliation>
<email confidence="0.998161">mooney@cs.utexas.edu</email>
<abstract confidence="0.995628055555556">Scripts represent knowledge of stereotypical event sequences that can aid text understanding. Initial statistical methods have been developed to learn probabilistic scripts from raw text corpora; however, they utilize a very impoverished representation of events, consisting of a verb and one dependent argument. We present a script learning approach that employs events with multiple arguments. Unlike previous work, we model the interactions between multiple entities in a script. Experiments on a large corpus using the task of inferring held-out events (the “narrative cloze evaluation”) demonstrate that modeling multi-argument events improves predictive accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Corin R Anderson</author>
<author>Pedro Domingos</author>
<author>Daniel S Weld</author>
</authors>
<title>Relational Markov models and their application to adaptive web navigation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2002),</booktitle>
<pages>143--152</pages>
<contexts>
<context position="11053" citStr="Anderson et al., 2002" startWordPosition="1815" endWordPosition="1818">2: N(a1, a2) +— 0 3: end for 4: for D E documents do 5: for a1, a2 E coocurEvs(D) do 6: for u E subs(a1, a2) do 7: N(u(a1), u(a2)) += 1 8: end for 9: end for 10: end for answer(y, ·, ·) for all distinct entities x, y, and z. If we were to simply keep the entities as they are and calculate raw co-occurrence counts, we would get evidence only for x = mary, y = bob, and z = question. One approach to this problem would be to deploy one of many previously described Statistical Relational Learning methods, for example Logical Hidden Markov Models (Kersting et al., 2006) or Relational Markov Models (Anderson et al., 2002). These methods can learn various statistical relationships between relational logical atoms with variables, of the sort considered here. However, we investigate a simpler option. The most important relationship between the entities in two multi-argument events concerns their overlapping entities. For example, to describe the relationship between the three entities in (2), it is most important to note that the object of the first event is identical with the subject of the second (namely, both are bob). The identity of the non-overlapping entities mary and question is not important for capturin</context>
</contexts>
<marker>Anderson, Domingos, Weld, 2002</marker>
<rawString>Corin R Anderson, Pedro Domingos, and Daniel S Weld. 2002. Relational Markov models and their application to adaptive web navigation. In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2002), pages 143–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niranjan Balasubramanian</author>
<author>Stephen Soderland</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>Rel-grams: a probabilistic model of relations in text.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction at NAACL-HLT</booktitle>
<pages>101--105</pages>
<contexts>
<context position="3586" citStr="Balasubramanian et al. (2012" startWordPosition="542" endWordPosition="545">duced a method for learning statistical scripts that, using a much simpler event representation that allows for efficient learning and inference. Jans et al. (2012) use the same simple event representation, but introduce a new model that more accurately predicts test data. These methods only model the actions of a single participant, called the protagonist. Chambers and Jurafsky (2009) extended their approach to the multi-participant case, modeling the events in which all of the entities in a document are involved; however, their method cannot represent interactions between multiple entities. Balasubramanian et al. (2012; 2013) describe the Rel-gram system, a Markov model similar to that of Jans et al. (2012), but with tuples instead of (verb, dependency) pairs. Our approach is sim220 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 220–229, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics ilar, but instead of modeling a distribution over cooccurring verbs and nominal arguments, we model interactions between entities directly by incorporating coreference information into the model. Previous statistical scri</context>
<context position="9883" citStr="Balasubramanian et al. (2012" startWordPosition="1601" endWordPosition="1604">2) is, if possible, by counting the number of times we observe a1 and a2 cooccurring and normalizing the function to sum to 1 over all pairs (a1, a2). For Chambers and Jurafsky (2008; 2009) and Jans et al. (2012), such a Maximum Likelihood Estimate is straightforward to arrive at: events are (verb, dependency) pairs, and two events co-occur when they are in the same event chain, relating to the same entity (Jans et al. (2012) further require a1 and a2 to be near each other). One need simply traverse a training corpus and count the number of times each pair (a1, a2) co-occurs. The Rel-grams of Balasubramanian et al. (2012; 2013) admit a similar strategy: to arrive at a joint distribution of pairwise co-occurrence, one can simply count co-occurrence of ground relations in a corpus and normalize. However, given two multi-argument events of the form v(es, eo, ep), this strategy will not suffice. For example, if during training we observe the two co-occurring events (2) ask(mary, bob, question) answer(bob, ·, ·) we would like this to lend evidence to the co-occurrence of events ask(x, y, z) and Algorithm 1 Learning with entity substitution 1: for a1, a2 E evs do 2: N(a1, a2) +— 0 3: end for 4: for D E documents do</context>
<context position="13038" citStr="Balasubramanian et al. (2012" startWordPosition="2158" endWordPosition="2162">the pseudocode for the learnP(a1, a2) 222 ing method. This populates a co-occurrence matrix N, where entry N(a1, a2) gives the cooccurrence count of events a1 and a2. The variable evs in line 1 is the set of all events in our model, which are of the form v(es, eo, ep), with v a verb lemma and es, eo, ep E {x, y, z, O}. The variable documents in line 4 is the collection of documents in our training corpus. The function cooccurEvs in line 5 takes a document D and returns all ordered pairs of co-occurring events in D, where, following the 2-skip bigram model of Jans et al. (2012), and similar to Balasubramanian et al. (2012; 2013), two events a1 and a2 are said to co-occur if they occur in order, in the same document, with at most two intervening events between them.1 The function subs in line 6 takes two events and returns all variable substitutions σ mapping from entities mentioned in the events a1 and a2 to the set {x, y, z, O}, such that two coreferent entities map to the same element of {x, y, z}. A substitution σ applied to an event v(es, eo, ep), as in line 7, is defined as v(σ(es), σ(eo), σ(ep)), with the null entity mapped to itself. Once we have calculated N(a1, a2) using Algorithm 1, we may define P(a</context>
<context position="30898" citStr="Balasubramanian et al. (2012" startWordPosition="5116" endWordPosition="5119">d above. Manshadi et al. (2008) give an n-gram model for sequences of verbs and their patients. McIntyre and Lapata (2009; 2010) use script objects learned from corpora of fairy tales to automatically generate stories. Chambers and Jurafsky (2009) extend their previous model to incorporate multiple entities, but do not directly model the different arguments of an event. Bamman et al. (2013) learn latent character personas from film summaries, associating character types with stereotypical actions; they focus on identifying persona types, rather than event inference. Manshadi et al. (2008) and Balasubramanian et al. (2012; 2013) give approaches similar to the current work for modeling sequences of events as n-grams. These methods differ from the current work in that they do not model entities directly, instead modeling co-occurrence of particular nouns standing as arguments to particular verbs. Lewis and Steedman (2013) build clusters of relations similar to these events, finding such clusters helpful to question answering and textual inference. There has also been recent interest in the related problem of automatically learning event frames (Bejan, 2008; Chambers and Jurafsky, 2011; Cheung et al., 2013; Chamb</context>
</contexts>
<marker>Balasubramanian, Soderland, Mausam, Etzioni, 2012</marker>
<rawString>Niranjan Balasubramanian, Stephen Soderland, Mausam, and Oren Etzioni. 2012. Rel-grams: a probabilistic model of relations in text. In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction at NAACL-HLT 2012 (AKBC-WEKEX 2012), pages 101–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niranjan Balasubramanian</author>
<author>Stephen Soderland</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>Generating coherent event schemas at scale.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP-2013).</booktitle>
<contexts>
<context position="31659" citStr="Balasubramanian et al. (2013)" startWordPosition="5232" endWordPosition="5235">nt work in that they do not model entities directly, instead modeling co-occurrence of particular nouns standing as arguments to particular verbs. Lewis and Steedman (2013) build clusters of relations similar to these events, finding such clusters helpful to question answering and textual inference. There has also been recent interest in the related problem of automatically learning event frames (Bejan, 2008; Chambers and Jurafsky, 2011; Cheung et al., 2013; Chambers, 2013). These approaches focus on identifying frames for information extraction tasks, as opposed to inferring events directly. Balasubramanian et al. (2013) give an event frame identification method, developed in parallel with the current work, using sequences of tuples similar to our multi-argument events, noting coherence issues with pair events. Their formulation differs from ours primarily in that they do not incorporate coreference information into their event co-occurrence distribution, and evaluate using human judgments of frame coherence rather than a narrative cloze test. 6 Future Work We have evaluated only one type of multiargument event inference, in which a script infers an event given a set of entities and the events involving those</context>
</contexts>
<marker>Balasubramanian, Soderland, Mausam, Etzioni, 2013</marker>
<rawString>Niranjan Balasubramanian, Stephen Soderland, Mausam, and Oren Etzioni. 2013. Generating coherent event schemas at scale. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP-2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Bamman</author>
<author>Brendan O’Connor</author>
<author>Noah A Smith</author>
</authors>
<title>Learning latent personas of film characters.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL-13),</booktitle>
<pages>352--361</pages>
<marker>Bamman, O’Connor, Smith, 2013</marker>
<rawString>David Bamman, Brendan O’Connor, and Noah A. Smith. 2013. Learning latent personas of film characters. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL-13), pages 352–361.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Giuseppe Di Fabbrizio</author>
<author>Amanda Stent</author>
</authors>
<title>Learning the structure of taskdriven human–human dialogs.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL06),</booktitle>
<pages>201--208</pages>
<marker>Bangalore, Di Fabbrizio, Stent, 2006</marker>
<rawString>Srinivas Bangalore, Giuseppe Di Fabbrizio, and Amanda Stent. 2006. Learning the structure of taskdriven human–human dialogs. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL06), pages 201–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cosmin Adrian Bejan</author>
</authors>
<title>Unsupervised discovery of event scenarios from texts.</title>
<date>2008</date>
<booktitle>In Prodeedings of the 21st International Florida Artificial Intelligence Research Society Conference (FLAIRS-2008),</booktitle>
<pages>124--129</pages>
<contexts>
<context position="31441" citStr="Bejan, 2008" startWordPosition="5201" endWordPosition="5202">t inference. Manshadi et al. (2008) and Balasubramanian et al. (2012; 2013) give approaches similar to the current work for modeling sequences of events as n-grams. These methods differ from the current work in that they do not model entities directly, instead modeling co-occurrence of particular nouns standing as arguments to particular verbs. Lewis and Steedman (2013) build clusters of relations similar to these events, finding such clusters helpful to question answering and textual inference. There has also been recent interest in the related problem of automatically learning event frames (Bejan, 2008; Chambers and Jurafsky, 2011; Cheung et al., 2013; Chambers, 2013). These approaches focus on identifying frames for information extraction tasks, as opposed to inferring events directly. Balasubramanian et al. (2013) give an event frame identification method, developed in parallel with the current work, using sequences of tuples similar to our multi-argument events, noting coherence issues with pair events. Their formulation differs from ours primarily in that they do not incorporate coreference information into their event co-occurrence distribution, and evaluate using human judgments of fr</context>
</contexts>
<marker>Bejan, 2008</marker>
<rawString>Cosmin Adrian Bejan. 2008. Unsupervised discovery of event scenarios from texts. In Prodeedings of the 21st International Florida Artificial Intelligence Research Society Conference (FLAIRS-2008), pages 124–129.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kristy Elizabeth Boyer</author>
<author>Robert Phillips</author>
<author>Eun Young Ha</author>
<author>Michael D Wallis</author>
<author>Mladen A Vouk</author>
<author>James C Lester</author>
</authors>
<title>Modeling dialogue structure with adjacency pair analysis and Hidden Markov Models.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Paper (NAACLHLT-09 Short),</booktitle>
<pages>49--52</pages>
<contexts>
<context position="30064" citStr="Boyer et al., 2009" startWordPosition="4988" endWordPosition="4991">12) give methods for using crowdsourcing to create situation-specific scripts. These methods 226 help alleviate the bottleneck of the knowledgeengineering required for traditionally conceived script systems. These systems are precisionoriented: they create small, highly accurate scripts for very limited scenarios. The current work, in contrast, focuses on building high-recall models of general event sequences. There are also a number of systems addressing the related problem of modeling domain-specific human-human dialog for building dialog systems (Bangalore et al., 2006; Chotimongkol, 2008; Boyer et al., 2009). There have been a number of recent approaches to learning statistical scripts. Chambers and Jurafsky (2008) and Jans et al. (2012) give methods for learning models of (verb, dependency) pairs, as described above. Manshadi et al. (2008) give an n-gram model for sequences of verbs and their patients. McIntyre and Lapata (2009; 2010) use script objects learned from corpora of fairy tales to automatically generate stories. Chambers and Jurafsky (2009) extend their previous model to incorporate multiple entities, but do not directly model the different arguments of an event. Bamman et al. (2013) </context>
</contexts>
<marker>Boyer, Phillips, Ha, Wallis, Vouk, Lester, 2009</marker>
<rawString>Kristy Elizabeth Boyer, Robert Phillips, Eun Young Ha, Michael D. Wallis, Mladen A. Vouk, and James C. Lester. 2009. Modeling dialogue structure with adjacency pair analysis and Hidden Markov Models. In Proceedings of Human Language Technologies: The Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Paper (NAACLHLT-09 Short), pages 49–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Unsupervised learning of narrative event chains.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL-08),</booktitle>
<pages>789--797</pages>
<contexts>
<context position="1825" citStr="Chambers and Jurafsky, 2008" startWordPosition="273" endWordPosition="276">nowledge about what normally happens when dining out. Such knowledge can be used to improve text understanding by supporting inference of missing actions and events, as well as resolution of lexical and syntactic ambiguities and anaphora (Rahman and Ng, 2012). For example, given the text “John went to Olive Garden and ordered lasagna. He left a big tip and left,” an inference that scripts would ideally allow us to make is “John ate lasagna.” There is a small body of recent research on automatically learning probabilistic models of scripts from large corpora of raw text (Manshadi et al., 2008; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012). However, this work uses a very impoverished representation of events that only includes a verb and a single dependent entity. We propose a more complex multiargument event representation for use in statistical script models, capable of directly capturing interactions between multiple entities. We present a method for learning such a model, and provide experimental evidence that modeling entity interactions allows for better prediction of events in documents, compared to previous single-entity “chain” models. We also compare to a competitive ba</context>
<context position="5503" citStr="Chambers and Jurafsky (2008" startWordPosition="855" endWordPosition="859">ferent sort from those in other models. Our events are more structured, as described in Section 3.1, and we produce one event sequence per document, instead of one event sequence per entity. This requires a different statistical model, as described in Section 3.2. 3 Script Models In Section 3.1, we describe the multi-argument events we use as the basis of our script models. Section 3.2 describes a script model using these events, and Section 3.3 describes the baseline systems to which we compare. 3.1 Multi-Argument Events Statistical scripts are models of stereotypical sequences of events. In Chambers and Jurafsky (2008; 2009) and Jans et al. (2012), events are (verb, dependency) pairs, forming “chains,” grouped according to the entity involved. For example, the text (1) Mary emailed Jim and he responded to her immediately. yields two chains. First, there is a chain for Mary: (email, subject) (respond, object) indicating that Mary was the subject of an emailing event and the object of a responding event. Second, there is a chain for Jim: (email, object) (respond, subject) indicating that Jim was the object of an emailing event and the subject of a responding event. Thus, one document produces many chains, ea</context>
<context position="9437" citStr="Chambers and Jurafsky (2008" startWordPosition="1523" endWordPosition="1527">. Our model will require a conditional probability P(ala0), the probability of seeing event a after event a0, given we have observed a0. However, as described below, directly estimating this probability is more complicated than in previous work because events now have additional structure. By definition, we have P(a2�a1) = P(a1) where P(a1, a2) is the probability of seeing a1 and a2, in order. The most straightforward way to estimate P(a1, a2) is, if possible, by counting the number of times we observe a1 and a2 cooccurring and normalizing the function to sum to 1 over all pairs (a1, a2). For Chambers and Jurafsky (2008; 2009) and Jans et al. (2012), such a Maximum Likelihood Estimate is straightforward to arrive at: events are (verb, dependency) pairs, and two events co-occur when they are in the same event chain, relating to the same entity (Jans et al. (2012) further require a1 and a2 to be near each other). One need simply traverse a training corpus and count the number of times each pair (a1, a2) co-occurs. The Rel-grams of Balasubramanian et al. (2012; 2013) admit a similar strategy: to arrive at a joint distribution of pairwise co-occurrence, one can simply count co-occurrence of ground relations in a</context>
<context position="16352" citStr="Chambers and Jurafsky (2008" startWordPosition="2753" endWordPosition="2756">random baseline, which outputs randomly selected events observed during training. This model can guess either multi-argument or pair events. 3.3.2 Unigram Model The unigram system guesses events ordered by prior probability, as calculated from the training set. If scripts are viewed as n-gram models P(a2|a1) = Ea, P(a1,a0) N(a1, a2) =(4) Ea, N(a1, a0). 3.2.2 Inferring Events Suppose we have a sequence of multi-argument events extracted from a document. A natural task for a statistical script model is to infer what other events likely occurred, given the events explicitly stated in a document. Chambers and Jurafsky (2008; 2009) treat the events involving an entity as an unordered set, inferring the most likely additional event, with no relative ordering between the inferred event and known events. We adopt the model of Jans et al. (2012), which was demonstrated to give better empirical performance. This 223 over events, this baseline corresponds to a bag-ofwords unigram model. In this model, events are assumed to occur independently, drawn from a single distribution. This model can be used to guess either multi-argument or pair events. 3.3.3 Single Protagonist Model We refer to the system of Jans et al. (2012</context>
<context position="19680" citStr="Chambers and Jurafsky, 2008" startWordPosition="3307" endWordPosition="3310">ill be the entities in the held-out event. Each entity in a contributes independently to the score M(a), based on the known (verb, dependency) pairs involving that entity. This model scores a multi-argument event a by combining one independent single-protagonist model for every entity in a. This model is similar to the multi-participant narrative schemas described in Chambers and Jurafsky (2009), but whereas they infer bare verbs, we infer an entire multi-argument event. 4 Evaluation 4.1 Evaluation Task We follow previous work in using the narrative cloze task to evaluate statistical scripts (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012). The task is as follows: given a sequence of events al, ... , an from a document, hold out some event ap and attempt to predict that event, given the other events in the sequence. As we cannot automatically evaluate the prediction of truly unmentioned events in a document, this evaluation acts as a straightforward proxy. In the aforementioned work, the cloze task is to guess a pair event, given the other events in which the held-out pair’s entity occurs. In Section 4.2.2, we evaluate directly on this task of guessing pair events. However, in Se</context>
<context position="30173" citStr="Chambers and Jurafsky (2008)" startWordPosition="5004" endWordPosition="5008">p alleviate the bottleneck of the knowledgeengineering required for traditionally conceived script systems. These systems are precisionoriented: they create small, highly accurate scripts for very limited scenarios. The current work, in contrast, focuses on building high-recall models of general event sequences. There are also a number of systems addressing the related problem of modeling domain-specific human-human dialog for building dialog systems (Bangalore et al., 2006; Chotimongkol, 2008; Boyer et al., 2009). There have been a number of recent approaches to learning statistical scripts. Chambers and Jurafsky (2008) and Jans et al. (2012) give methods for learning models of (verb, dependency) pairs, as described above. Manshadi et al. (2008) give an n-gram model for sequences of verbs and their patients. McIntyre and Lapata (2009; 2010) use script objects learned from corpora of fairy tales to automatically generate stories. Chambers and Jurafsky (2009) extend their previous model to incorporate multiple entities, but do not directly model the different arguments of an event. Bamman et al. (2013) learn latent character personas from film summaries, associating character types with stereotypical actions; </context>
</contexts>
<marker>Chambers, Jurafsky, 2008</marker>
<rawString>Nathanael Chambers and Daniel Jurafsky. 2008. Unsupervised learning of narrative event chains. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL-08), pages 789–797.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Unsupervised learning of narrative schemas and their participants.</title>
<date>2009</date>
<booktitle>In Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing (ACLIJCNLP),</booktitle>
<pages>602--610</pages>
<contexts>
<context position="1854" citStr="Chambers and Jurafsky, 2009" startWordPosition="277" endWordPosition="280">happens when dining out. Such knowledge can be used to improve text understanding by supporting inference of missing actions and events, as well as resolution of lexical and syntactic ambiguities and anaphora (Rahman and Ng, 2012). For example, given the text “John went to Olive Garden and ordered lasagna. He left a big tip and left,” an inference that scripts would ideally allow us to make is “John ate lasagna.” There is a small body of recent research on automatically learning probabilistic models of scripts from large corpora of raw text (Manshadi et al., 2008; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012). However, this work uses a very impoverished representation of events that only includes a verb and a single dependent entity. We propose a more complex multiargument event representation for use in statistical script models, capable of directly capturing interactions between multiple entities. We present a method for learning such a model, and provide experimental evidence that modeling entity interactions allows for better prediction of events in documents, compared to previous single-entity “chain” models. We also compare to a competitive baseline not used in previous w</context>
<context position="3346" citStr="Chambers and Jurafsky (2009)" startWordPosition="505" endWordPosition="509">r, Mooney and DeJong (1985) present an early knowledge-based method for learning scripts from a single document. These early scripts (and methods for learning them) were non-statistical and fairly brittle. Chambers and Jurafsky (2008) introduced a method for learning statistical scripts that, using a much simpler event representation that allows for efficient learning and inference. Jans et al. (2012) use the same simple event representation, but introduce a new model that more accurately predicts test data. These methods only model the actions of a single participant, called the protagonist. Chambers and Jurafsky (2009) extended their approach to the multi-participant case, modeling the events in which all of the entities in a document are involved; however, their method cannot represent interactions between multiple entities. Balasubramanian et al. (2012; 2013) describe the Rel-gram system, a Markov model similar to that of Jans et al. (2012), but with tuples instead of (verb, dependency) pairs. Our approach is sim220 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 220–229, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computa</context>
<context position="19451" citStr="Chambers and Jurafsky (2009)" startWordPosition="3271" endWordPosition="3275">nt event a = v(es, eo, ep), with es, eo, ep E E, with the following objective function: M(a) =Se3((v, subj)) + Seo((v, obj))+ Sep((v, prep)) (6) where, for null entity e, we define Se(d) = 0 for all d. In the cloze evaluation, E will be the entities in the held-out event. Each entity in a contributes independently to the score M(a), based on the known (verb, dependency) pairs involving that entity. This model scores a multi-argument event a by combining one independent single-protagonist model for every entity in a. This model is similar to the multi-participant narrative schemas described in Chambers and Jurafsky (2009), but whereas they infer bare verbs, we infer an entire multi-argument event. 4 Evaluation 4.1 Evaluation Task We follow previous work in using the narrative cloze task to evaluate statistical scripts (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012). The task is as follows: given a sequence of events al, ... , an from a document, hold out some event ap and attempt to predict that event, given the other events in the sequence. As we cannot automatically evaluate the prediction of truly unmentioned events in a document, this evaluation acts as a straightforward proxy</context>
<context position="30517" citStr="Chambers and Jurafsky (2009)" startWordPosition="5059" endWordPosition="5062">tems addressing the related problem of modeling domain-specific human-human dialog for building dialog systems (Bangalore et al., 2006; Chotimongkol, 2008; Boyer et al., 2009). There have been a number of recent approaches to learning statistical scripts. Chambers and Jurafsky (2008) and Jans et al. (2012) give methods for learning models of (verb, dependency) pairs, as described above. Manshadi et al. (2008) give an n-gram model for sequences of verbs and their patients. McIntyre and Lapata (2009; 2010) use script objects learned from corpora of fairy tales to automatically generate stories. Chambers and Jurafsky (2009) extend their previous model to incorporate multiple entities, but do not directly model the different arguments of an event. Bamman et al. (2013) learn latent character personas from film summaries, associating character types with stereotypical actions; they focus on identifying persona types, rather than event inference. Manshadi et al. (2008) and Balasubramanian et al. (2012; 2013) give approaches similar to the current work for modeling sequences of events as n-grams. These methods differ from the current work in that they do not model entities directly, instead modeling co-occurrence of </context>
</contexts>
<marker>Chambers, Jurafsky, 2009</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2009. Unsupervised learning of narrative schemas and their participants. In Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing (ACLIJCNLP), pages 602–610.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Template-based information extraction without the templates.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT11),</booktitle>
<pages>976--986</pages>
<contexts>
<context position="31470" citStr="Chambers and Jurafsky, 2011" startWordPosition="5203" endWordPosition="5206">Manshadi et al. (2008) and Balasubramanian et al. (2012; 2013) give approaches similar to the current work for modeling sequences of events as n-grams. These methods differ from the current work in that they do not model entities directly, instead modeling co-occurrence of particular nouns standing as arguments to particular verbs. Lewis and Steedman (2013) build clusters of relations similar to these events, finding such clusters helpful to question answering and textual inference. There has also been recent interest in the related problem of automatically learning event frames (Bejan, 2008; Chambers and Jurafsky, 2011; Cheung et al., 2013; Chambers, 2013). These approaches focus on identifying frames for information extraction tasks, as opposed to inferring events directly. Balasubramanian et al. (2013) give an event frame identification method, developed in parallel with the current work, using sequences of tuples similar to our multi-argument events, noting coherence issues with pair events. Their formulation differs from ours primarily in that they do not incorporate coreference information into their event co-occurrence distribution, and evaluate using human judgments of frame coherence rather than a n</context>
</contexts>
<marker>Chambers, Jurafsky, 2011</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2011. Template-based information extraction without the templates. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT11), pages 976–986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
</authors>
<title>Event schema induction with a probabilistic entity-driven model.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP2013).</booktitle>
<contexts>
<context position="31508" citStr="Chambers, 2013" startWordPosition="5212" endWordPosition="5213">(2012; 2013) give approaches similar to the current work for modeling sequences of events as n-grams. These methods differ from the current work in that they do not model entities directly, instead modeling co-occurrence of particular nouns standing as arguments to particular verbs. Lewis and Steedman (2013) build clusters of relations similar to these events, finding such clusters helpful to question answering and textual inference. There has also been recent interest in the related problem of automatically learning event frames (Bejan, 2008; Chambers and Jurafsky, 2011; Cheung et al., 2013; Chambers, 2013). These approaches focus on identifying frames for information extraction tasks, as opposed to inferring events directly. Balasubramanian et al. (2013) give an event frame identification method, developed in parallel with the current work, using sequences of tuples similar to our multi-argument events, noting coherence issues with pair events. Their formulation differs from ours primarily in that they do not incorporate coreference information into their event co-occurrence distribution, and evaluate using human judgments of frame coherence rather than a narrative cloze test. 6 Future Work We </context>
</contexts>
<marker>Chambers, 2013</marker>
<rawString>Nathanael Chambers. 2013. Event schema induction with a probabilistic entity-driven model. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jackie Chi Kit Cheung</author>
<author>Hoifung Poon</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Probabilistic frame induction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-13).</booktitle>
<contexts>
<context position="31491" citStr="Cheung et al., 2013" startWordPosition="5207" endWordPosition="5211">lasubramanian et al. (2012; 2013) give approaches similar to the current work for modeling sequences of events as n-grams. These methods differ from the current work in that they do not model entities directly, instead modeling co-occurrence of particular nouns standing as arguments to particular verbs. Lewis and Steedman (2013) build clusters of relations similar to these events, finding such clusters helpful to question answering and textual inference. There has also been recent interest in the related problem of automatically learning event frames (Bejan, 2008; Chambers and Jurafsky, 2011; Cheung et al., 2013; Chambers, 2013). These approaches focus on identifying frames for information extraction tasks, as opposed to inferring events directly. Balasubramanian et al. (2013) give an event frame identification method, developed in parallel with the current work, using sequences of tuples similar to our multi-argument events, noting coherence issues with pair events. Their formulation differs from ours primarily in that they do not incorporate coreference information into their event co-occurrence distribution, and evaluate using human judgments of frame coherence rather than a narrative cloze test. </context>
</contexts>
<marker>Cheung, Poon, Vanderwende, 2013</marker>
<rawString>Jackie Chi Kit Cheung, Hoifung Poon, and Lucy Vanderwende. 2013. Probabilistic frame induction. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-13).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ananlada Chotimongkol</author>
</authors>
<title>Learning the structure of task-oriented conversations from the corpus of in-domain dialogs.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="30043" citStr="Chotimongkol, 2008" startWordPosition="4986" endWordPosition="4987">0) and Li et al. (2012) give methods for using crowdsourcing to create situation-specific scripts. These methods 226 help alleviate the bottleneck of the knowledgeengineering required for traditionally conceived script systems. These systems are precisionoriented: they create small, highly accurate scripts for very limited scenarios. The current work, in contrast, focuses on building high-recall models of general event sequences. There are also a number of systems addressing the related problem of modeling domain-specific human-human dialog for building dialog systems (Bangalore et al., 2006; Chotimongkol, 2008; Boyer et al., 2009). There have been a number of recent approaches to learning statistical scripts. Chambers and Jurafsky (2008) and Jans et al. (2012) give methods for learning models of (verb, dependency) pairs, as described above. Manshadi et al. (2008) give an n-gram model for sequences of verbs and their patients. McIntyre and Lapata (2009; 2010) use script objects learned from corpora of fairy tales to automatically generate stories. Chambers and Jurafsky (2009) extend their previous model to incorporate multiple entities, but do not directly model the different arguments of an event. </context>
</contexts>
<marker>Chotimongkol, 2008</marker>
<rawString>Ananlada Chotimongkol. 2008. Learning the structure of task-oriented conversations from the corpus of in-domain dialogs. Ph.D. thesis, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources &amp; Evaluation (LREC-2006),</booktitle>
<volume>6</volume>
<pages>449--454</pages>
<marker>De Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine De Marneffe, Bill MacCartney, Christopher D Manning, et al. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of the 5th International Conference on Language Resources &amp; Evaluation (LREC-2006), volume 6, pages 449–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
<author>Junbo Kong</author>
<author>Ke Chen</author>
<author>Kazuaki Maeda</author>
</authors>
<title>English Gigaword Third Edition. Linguistic Data Consortium.</title>
<date>2007</date>
<contexts>
<context position="22292" citStr="Graff et al., 2007" startWordPosition="3751" endWordPosition="3754">fication removes structure from the original sequence, but retains the important pairwise entity relationships between the held-out event and the other events. 4.2 Experimental Evaluation For each document, we use the Stanford dependency parser (De Marneffe et al., 2006) to get syntactic information about the document; we then use the Stanford coreference resolution engine (Raghunathan et al., 2010) to get (noisy) equivalence classes of coreferent noun phrases in a document.2 We train on approximately 1.1M articles from years 1994-2006 of the NYT portion of the Gigaword Corpus, Third Edition (Graff et al., 2007), holding out a random subset of the articles from 1999 for development and test sets. Our test set consists of 10,000 randomly selected heldout events, and our development set is 500 disjoint randomly selected held-out events. To remove duplicate documents, we hash the first 500 characters of each article and remove any articles with hash collisions. We use add-one smoothing on all joint probabilities. To reduce the size of our model, we remove all events that occur fewer than 50 times.3 We evaluate performance using the following two metrics: 1. Recall at 10: Following Jans et al. (2012), we</context>
</contexts>
<marker>Graff, Kong, Chen, Maeda, 2007</marker>
<rawString>David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2007. English Gigaword Third Edition. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bram Jans</author>
<author>Steven Bethard</author>
<author>Ivan Vuli´c</author>
<author>Marie Francine Moens</author>
</authors>
<title>Skip n-grams and ranking functions for predicting script events.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL-12),</booktitle>
<pages>336--344</pages>
<marker>Jans, Bethard, Vuli´c, Moens, 2012</marker>
<rawString>Bram Jans, Steven Bethard, Ivan Vuli´c, and Marie Francine Moens. 2012. Skip n-grams and ranking functions for predicting script events. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL-12), pages 336–344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristian Kersting</author>
<author>Luc De Raedt</author>
<author>Tapani Raiko</author>
</authors>
<title>Logical Hidden Markov Models.</title>
<date>2006</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>25--425</pages>
<marker>Kersting, De Raedt, Raiko, 2006</marker>
<rawString>Kristian Kersting, Luc De Raedt, and Tapani Raiko. 2006. Logical Hidden Markov Models. Journal of Artificial Intelligence Research, 25:425–456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Lewis</author>
<author>Mark Steedman</author>
</authors>
<title>Combined distributional and logical semantics.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>1--179</pages>
<contexts>
<context position="31202" citStr="Lewis and Steedman (2013)" startWordPosition="5163" endWordPosition="5166">s, but do not directly model the different arguments of an event. Bamman et al. (2013) learn latent character personas from film summaries, associating character types with stereotypical actions; they focus on identifying persona types, rather than event inference. Manshadi et al. (2008) and Balasubramanian et al. (2012; 2013) give approaches similar to the current work for modeling sequences of events as n-grams. These methods differ from the current work in that they do not model entities directly, instead modeling co-occurrence of particular nouns standing as arguments to particular verbs. Lewis and Steedman (2013) build clusters of relations similar to these events, finding such clusters helpful to question answering and textual inference. There has also been recent interest in the related problem of automatically learning event frames (Bejan, 2008; Chambers and Jurafsky, 2011; Cheung et al., 2013; Chambers, 2013). These approaches focus on identifying frames for information extraction tasks, as opposed to inferring events directly. Balasubramanian et al. (2013) give an event frame identification method, developed in parallel with the current work, using sequences of tuples similar to our multi-argumen</context>
</contexts>
<marker>Lewis, Steedman, 2013</marker>
<rawString>Mike Lewis and Mark Steedman. 2013. Combined distributional and logical semantics. Transactions of the Association for Computational Linguistics, 1:179–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boyang Li</author>
<author>Stephen Lee-Urban</author>
<author>Darren Scott Appling</author>
<author>Mark O Riedl</author>
</authors>
<title>Crowdsourcing narrative intelligence.</title>
<date>2012</date>
<journal>Advances in Cognitive Systems,</journal>
<volume>2</volume>
<pages>42</pages>
<contexts>
<context position="29448" citStr="Li et al. (2012)" startWordPosition="4900" endWordPosition="4903">lex representations for events, with many different relations between events. They are not statistical, and use separate models for different scenarios (e.g. the “restaurant script” is different from the “bank script”). Generally, they require humans to encode procedural information by hand; see, however, Mooney and DeJong (1985) for an early method for learning scripts automatically from a document. Miikkulainen (1990; 1993) gives a hierarchical Neural Network system which stores sequences of events from text in episodic memory, capable of simple question answering. Regneri et al. (2010) and Li et al. (2012) give methods for using crowdsourcing to create situation-specific scripts. These methods 226 help alleviate the bottleneck of the knowledgeengineering required for traditionally conceived script systems. These systems are precisionoriented: they create small, highly accurate scripts for very limited scenarios. The current work, in contrast, focuses on building high-recall models of general event sequences. There are also a number of systems addressing the related problem of modeling domain-specific human-human dialog for building dialog systems (Bangalore et al., 2006; Chotimongkol, 2008; Boy</context>
</contexts>
<marker>Li, Lee-Urban, Appling, Riedl, 2012</marker>
<rawString>Boyang Li, Stephen Lee-Urban, Darren Scott Appling, and Mark O Riedl. 2012. Crowdsourcing narrative intelligence. Advances in Cognitive Systems, 2:25– 42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehdi Manshadi</author>
<author>Reid Swanson</author>
<author>Andrew S Gordon</author>
</authors>
<title>Learning a probabilistic model of event sequences from internet weblog stories.</title>
<date>2008</date>
<booktitle>In Prodeedings of the 21st International Florida Artificial Intelligence Research Society Conference (FLAIRS2008),</booktitle>
<pages>159--164</pages>
<contexts>
<context position="1796" citStr="Manshadi et al., 2008" startWordPosition="269" endWordPosition="272">cript,” which encodes knowledge about what normally happens when dining out. Such knowledge can be used to improve text understanding by supporting inference of missing actions and events, as well as resolution of lexical and syntactic ambiguities and anaphora (Rahman and Ng, 2012). For example, given the text “John went to Olive Garden and ordered lasagna. He left a big tip and left,” an inference that scripts would ideally allow us to make is “John ate lasagna.” There is a small body of recent research on automatically learning probabilistic models of scripts from large corpora of raw text (Manshadi et al., 2008; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012). However, this work uses a very impoverished representation of events that only includes a verb and a single dependent entity. We propose a more complex multiargument event representation for use in statistical script models, capable of directly capturing interactions between multiple entities. We present a method for learning such a model, and provide experimental evidence that modeling entity interactions allows for better prediction of events in documents, compared to previous single-entity “chain” models. We als</context>
<context position="30301" citStr="Manshadi et al. (2008)" startWordPosition="5026" endWordPosition="5029">noriented: they create small, highly accurate scripts for very limited scenarios. The current work, in contrast, focuses on building high-recall models of general event sequences. There are also a number of systems addressing the related problem of modeling domain-specific human-human dialog for building dialog systems (Bangalore et al., 2006; Chotimongkol, 2008; Boyer et al., 2009). There have been a number of recent approaches to learning statistical scripts. Chambers and Jurafsky (2008) and Jans et al. (2012) give methods for learning models of (verb, dependency) pairs, as described above. Manshadi et al. (2008) give an n-gram model for sequences of verbs and their patients. McIntyre and Lapata (2009; 2010) use script objects learned from corpora of fairy tales to automatically generate stories. Chambers and Jurafsky (2009) extend their previous model to incorporate multiple entities, but do not directly model the different arguments of an event. Bamman et al. (2013) learn latent character personas from film summaries, associating character types with stereotypical actions; they focus on identifying persona types, rather than event inference. Manshadi et al. (2008) and Balasubramanian et al. (2012; 2</context>
</contexts>
<marker>Manshadi, Swanson, Gordon, 2008</marker>
<rawString>Mehdi Manshadi, Reid Swanson, and Andrew S Gordon. 2008. Learning a probabilistic model of event sequences from internet weblog stories. In Prodeedings of the 21st International Florida Artificial Intelligence Research Society Conference (FLAIRS2008), pages 159–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Neil McIntyre</author>
<author>Mirella Lapata</author>
</authors>
<title>Learning to tell tales: A data-driven approach to story generation.</title>
<date>2009</date>
<booktitle>In Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing (ACLIJCNLP),</booktitle>
<pages>217--225</pages>
<contexts>
<context position="30391" citStr="McIntyre and Lapata (2009" startWordPosition="5041" endWordPosition="5044">urrent work, in contrast, focuses on building high-recall models of general event sequences. There are also a number of systems addressing the related problem of modeling domain-specific human-human dialog for building dialog systems (Bangalore et al., 2006; Chotimongkol, 2008; Boyer et al., 2009). There have been a number of recent approaches to learning statistical scripts. Chambers and Jurafsky (2008) and Jans et al. (2012) give methods for learning models of (verb, dependency) pairs, as described above. Manshadi et al. (2008) give an n-gram model for sequences of verbs and their patients. McIntyre and Lapata (2009; 2010) use script objects learned from corpora of fairy tales to automatically generate stories. Chambers and Jurafsky (2009) extend their previous model to incorporate multiple entities, but do not directly model the different arguments of an event. Bamman et al. (2013) learn latent character personas from film summaries, associating character types with stereotypical actions; they focus on identifying persona types, rather than event inference. Manshadi et al. (2008) and Balasubramanian et al. (2012; 2013) give approaches similar to the current work for modeling sequences of events as n-gra</context>
</contexts>
<marker>McIntyre, Lapata, 2009</marker>
<rawString>Neil McIntyre and Mirella Lapata. 2009. Learning to tell tales: A data-driven approach to story generation. In Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing (ACLIJCNLP), pages 217–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Neil McIntyre</author>
<author>Mirella Lapata</author>
</authors>
<title>Plot induction and evolutionary search for story generation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10),</booktitle>
<pages>1562--1572</pages>
<marker>McIntyre, Lapata, 2010</marker>
<rawString>Neil McIntyre and Mirella Lapata. 2010. Plot induction and evolutionary search for story generation. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10), pages 1562–1572.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Risto Miikkulainen</author>
</authors>
<title>DISCERN: A Distributed Artificial Neural Network Model of Script Processing and Memory.</title>
<date>1990</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California.</institution>
<contexts>
<context position="29254" citStr="Miikkulainen (1990" startWordPosition="4870" endWordPosition="4871">tions for automated reasoning dates back decades. The frames of Minsky (1974), schemas of Rumelhart (1975), and scripts of Schank and Abelson (1977) are early examples. These models use quite complex representations for events, with many different relations between events. They are not statistical, and use separate models for different scenarios (e.g. the “restaurant script” is different from the “bank script”). Generally, they require humans to encode procedural information by hand; see, however, Mooney and DeJong (1985) for an early method for learning scripts automatically from a document. Miikkulainen (1990; 1993) gives a hierarchical Neural Network system which stores sequences of events from text in episodic memory, capable of simple question answering. Regneri et al. (2010) and Li et al. (2012) give methods for using crowdsourcing to create situation-specific scripts. These methods 226 help alleviate the bottleneck of the knowledgeengineering required for traditionally conceived script systems. These systems are precisionoriented: they create small, highly accurate scripts for very limited scenarios. The current work, in contrast, focuses on building high-recall models of general event sequen</context>
</contexts>
<marker>Miikkulainen, 1990</marker>
<rawString>Risto Miikkulainen. 1990. DISCERN: A Distributed Artificial Neural Network Model of Script Processing and Memory. Ph.D. thesis, University of California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Risto Miikkulainen</author>
</authors>
<title>Subsymbolic Natural Language Processing: An Integrated Model of Scripts, Lexicon, and Memory.</title>
<date>1993</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Miikkulainen, 1993</marker>
<rawString>Risto Miikkulainen. 1993. Subsymbolic Natural Language Processing: An Integrated Model of Scripts, Lexicon, and Memory. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marvin Minsky</author>
</authors>
<title>A framework for representing knowledge.</title>
<date>1974</date>
<tech>Technical report,</tech>
<institution>MIT-AI Laboratory.</institution>
<contexts>
<context position="28713" citStr="Minsky (1974)" startWordPosition="4788" endWordPosition="4789">ve) improvement over the single protagonist model. Accuracy differences are significant (p &lt; 0.01) by a Wilcoxon signed-rank test. These results indicate that modeling multiargument event sequences allows better inference of simpler pair events. These performance improvements may be due to the fact that the joint model conditions on information not representable in the single protagonist model (namely, all of the events in which a multi-argument event’s entities are involved). 5 Related Work The procedural encoding of common situations for automated reasoning dates back decades. The frames of Minsky (1974), schemas of Rumelhart (1975), and scripts of Schank and Abelson (1977) are early examples. These models use quite complex representations for events, with many different relations between events. They are not statistical, and use separate models for different scenarios (e.g. the “restaurant script” is different from the “bank script”). Generally, they require humans to encode procedural information by hand; see, however, Mooney and DeJong (1985) for an early method for learning scripts automatically from a document. Miikkulainen (1990; 1993) gives a hierarchical Neural Network system which st</context>
</contexts>
<marker>Minsky, 1974</marker>
<rawString>Marvin Minsky. 1974. A framework for representing knowledge. Technical report, MIT-AI Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond J Mooney</author>
<author>Gerald F DeJong</author>
</authors>
<title>Learning schemata for natural language processing.</title>
<date>1985</date>
<booktitle>In Proceedings of the Ninth International Joint Conference on Artificial Intelligence (IJCAI-85),</booktitle>
<pages>681--687</pages>
<location>Los Angeles, CA,</location>
<contexts>
<context position="2745" citStr="Mooney and DeJong (1985)" startWordPosition="416" endWordPosition="419">capturing interactions between multiple entities. We present a method for learning such a model, and provide experimental evidence that modeling entity interactions allows for better prediction of events in documents, compared to previous single-entity “chain” models. We also compare to a competitive baseline not used in previous work, and introduce a novel evaluation metric. 2 Background The idea of representing stereotypical event sequences for textual inference originates in the seminal work of Schank and Abelson (1977). Early scripts were manually engineered for specific domains; however, Mooney and DeJong (1985) present an early knowledge-based method for learning scripts from a single document. These early scripts (and methods for learning them) were non-statistical and fairly brittle. Chambers and Jurafsky (2008) introduced a method for learning statistical scripts that, using a much simpler event representation that allows for efficient learning and inference. Jans et al. (2012) use the same simple event representation, but introduce a new model that more accurately predicts test data. These methods only model the actions of a single participant, called the protagonist. Chambers and Jurafsky (2009</context>
<context position="29163" citStr="Mooney and DeJong (1985)" startWordPosition="4855" endWordPosition="4858">i-argument event’s entities are involved). 5 Related Work The procedural encoding of common situations for automated reasoning dates back decades. The frames of Minsky (1974), schemas of Rumelhart (1975), and scripts of Schank and Abelson (1977) are early examples. These models use quite complex representations for events, with many different relations between events. They are not statistical, and use separate models for different scenarios (e.g. the “restaurant script” is different from the “bank script”). Generally, they require humans to encode procedural information by hand; see, however, Mooney and DeJong (1985) for an early method for learning scripts automatically from a document. Miikkulainen (1990; 1993) gives a hierarchical Neural Network system which stores sequences of events from text in episodic memory, capable of simple question answering. Regneri et al. (2010) and Li et al. (2012) give methods for using crowdsourcing to create situation-specific scripts. These methods 226 help alleviate the bottleneck of the knowledgeengineering required for traditionally conceived script systems. These systems are precisionoriented: they create small, highly accurate scripts for very limited scenarios. Th</context>
</contexts>
<marker>Mooney, DeJong, 1985</marker>
<rawString>Raymond J. Mooney and Gerald F. DeJong. 1985. Learning schemata for natural language processing. In Proceedings of the Ninth International Joint Conference on Artificial Intelligence (IJCAI-85), pages 681–687, Los Angeles, CA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karthik Raghunathan</author>
<author>Heeyoung Lee</author>
<author>Sudarshan Rangarajan</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A multipass sieve for coreference resolution.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2010),</booktitle>
<pages>492--501</pages>
<contexts>
<context position="22075" citStr="Raghunathan et al., 2010" startWordPosition="3713" endWordPosition="3716"> z, such that distinct entities never get rewritten to the same variable. • Any entity mentioned only in the held-out event is rewritten as O. • All entities not present in the held-out event are rewritten as O. This simplification removes structure from the original sequence, but retains the important pairwise entity relationships between the held-out event and the other events. 4.2 Experimental Evaluation For each document, we use the Stanford dependency parser (De Marneffe et al., 2006) to get syntactic information about the document; we then use the Stanford coreference resolution engine (Raghunathan et al., 2010) to get (noisy) equivalence classes of coreferent noun phrases in a document.2 We train on approximately 1.1M articles from years 1994-2006 of the NYT portion of the Gigaword Corpus, Third Edition (Graff et al., 2007), holding out a random subset of the articles from 1999 for development and test sets. Our test set consists of 10,000 randomly selected heldout events, and our development set is 500 disjoint randomly selected held-out events. To remove duplicate documents, we hash the first 500 characters of each article and remove any articles with hash collisions. We use add-one smoothing on a</context>
</contexts>
<marker>Raghunathan, Lee, Rangarajan, Chambers, Surdeanu, Jurafsky, Manning, 2010</marker>
<rawString>Karthik Raghunathan, Heeyoung Lee, Sudarshan Rangarajan, Nathanael Chambers, Mihai Surdeanu, Dan Jurafsky, and Christopher Manning. 2010. A multipass sieve for coreference resolution. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2010), pages 492–501.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Altaf Rahman</author>
<author>Vincent Ng</author>
</authors>
<title>Resolving complex cases of definite pronouns: the Winograd schema challenge.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL-12),</booktitle>
<pages>777--789</pages>
<contexts>
<context position="1457" citStr="Rahman and Ng, 2012" startWordPosition="207" endWordPosition="210">ative cloze evaluation”) demonstrate that modeling multi-argument events improves predictive accuracy. 1 Introduction Scripts encode knowledge of stereotypical events, including information about their typical ordered sequences of sub-events and corresponding arguments (Schank and Abelson, 1977). The classic example is the “restaurant script,” which encodes knowledge about what normally happens when dining out. Such knowledge can be used to improve text understanding by supporting inference of missing actions and events, as well as resolution of lexical and syntactic ambiguities and anaphora (Rahman and Ng, 2012). For example, given the text “John went to Olive Garden and ordered lasagna. He left a big tip and left,” an inference that scripts would ideally allow us to make is “John ate lasagna.” There is a small body of recent research on automatically learning probabilistic models of scripts from large corpora of raw text (Manshadi et al., 2008; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012). However, this work uses a very impoverished representation of events that only includes a verb and a single dependent entity. We propose a more complex multiargument event represent</context>
</contexts>
<marker>Rahman, Ng, 2012</marker>
<rawString>Altaf Rahman and Vincent Ng. 2012. Resolving complex cases of definite pronouns: the Winograd schema challenge. In Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL-12), pages 777–789.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michaela Regneri</author>
<author>Alexander Koller</author>
<author>Manfred Pinkal</author>
</authors>
<title>Learning script knowledge with web experiments.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10),</booktitle>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="29427" citStr="Regneri et al. (2010)" startWordPosition="4895" endWordPosition="4898">hese models use quite complex representations for events, with many different relations between events. They are not statistical, and use separate models for different scenarios (e.g. the “restaurant script” is different from the “bank script”). Generally, they require humans to encode procedural information by hand; see, however, Mooney and DeJong (1985) for an early method for learning scripts automatically from a document. Miikkulainen (1990; 1993) gives a hierarchical Neural Network system which stores sequences of events from text in episodic memory, capable of simple question answering. Regneri et al. (2010) and Li et al. (2012) give methods for using crowdsourcing to create situation-specific scripts. These methods 226 help alleviate the bottleneck of the knowledgeengineering required for traditionally conceived script systems. These systems are precisionoriented: they create small, highly accurate scripts for very limited scenarios. The current work, in contrast, focuses on building high-recall models of general event sequences. There are also a number of systems addressing the related problem of modeling domain-specific human-human dialog for building dialog systems (Bangalore et al., 2006; Ch</context>
</contexts>
<marker>Regneri, Koller, Pinkal, 2010</marker>
<rawString>Michaela Regneri, Alexander Koller, and Manfred Pinkal. 2010. Learning script knowledge with web experiments. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL-10), Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Rumelhart</author>
</authors>
<title>Notes on a schema for stories. Representation and Understanding: Studies in Cognitive Science.</title>
<date>1975</date>
<contexts>
<context position="28742" citStr="Rumelhart (1975)" startWordPosition="4792" endWordPosition="4793">ingle protagonist model. Accuracy differences are significant (p &lt; 0.01) by a Wilcoxon signed-rank test. These results indicate that modeling multiargument event sequences allows better inference of simpler pair events. These performance improvements may be due to the fact that the joint model conditions on information not representable in the single protagonist model (namely, all of the events in which a multi-argument event’s entities are involved). 5 Related Work The procedural encoding of common situations for automated reasoning dates back decades. The frames of Minsky (1974), schemas of Rumelhart (1975), and scripts of Schank and Abelson (1977) are early examples. These models use quite complex representations for events, with many different relations between events. They are not statistical, and use separate models for different scenarios (e.g. the “restaurant script” is different from the “bank script”). Generally, they require humans to encode procedural information by hand; see, however, Mooney and DeJong (1985) for an early method for learning scripts automatically from a document. Miikkulainen (1990; 1993) gives a hierarchical Neural Network system which stores sequences of events from</context>
</contexts>
<marker>Rumelhart, 1975</marker>
<rawString>David Rumelhart. 1975. Notes on a schema for stories. Representation and Understanding: Studies in Cognitive Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger C Schank</author>
<author>Robert P Abelson</author>
</authors>
<title>Scripts, Plans, Goals and Understanding: An Inquiry into Human Knowledge Structures. Lawrence Erlbaum and Associates,</title>
<date>1977</date>
<location>Hillsdale, NJ.</location>
<contexts>
<context position="1133" citStr="Schank and Abelson, 1977" startWordPosition="155" endWordPosition="158">representation of events, consisting of a verb and one dependent argument. We present a script learning approach that employs events with multiple arguments. Unlike previous work, we model the interactions between multiple entities in a script. Experiments on a large corpus using the task of inferring held-out events (the “narrative cloze evaluation”) demonstrate that modeling multi-argument events improves predictive accuracy. 1 Introduction Scripts encode knowledge of stereotypical events, including information about their typical ordered sequences of sub-events and corresponding arguments (Schank and Abelson, 1977). The classic example is the “restaurant script,” which encodes knowledge about what normally happens when dining out. Such knowledge can be used to improve text understanding by supporting inference of missing actions and events, as well as resolution of lexical and syntactic ambiguities and anaphora (Rahman and Ng, 2012). For example, given the text “John went to Olive Garden and ordered lasagna. He left a big tip and left,” an inference that scripts would ideally allow us to make is “John ate lasagna.” There is a small body of recent research on automatically learning probabilistic models o</context>
<context position="2649" citStr="Schank and Abelson (1977)" startWordPosition="402" endWordPosition="405">lex multiargument event representation for use in statistical script models, capable of directly capturing interactions between multiple entities. We present a method for learning such a model, and provide experimental evidence that modeling entity interactions allows for better prediction of events in documents, compared to previous single-entity “chain” models. We also compare to a competitive baseline not used in previous work, and introduce a novel evaluation metric. 2 Background The idea of representing stereotypical event sequences for textual inference originates in the seminal work of Schank and Abelson (1977). Early scripts were manually engineered for specific domains; however, Mooney and DeJong (1985) present an early knowledge-based method for learning scripts from a single document. These early scripts (and methods for learning them) were non-statistical and fairly brittle. Chambers and Jurafsky (2008) introduced a method for learning statistical scripts that, using a much simpler event representation that allows for efficient learning and inference. Jans et al. (2012) use the same simple event representation, but introduce a new model that more accurately predicts test data. These methods onl</context>
<context position="28784" citStr="Schank and Abelson (1977)" startWordPosition="4797" endWordPosition="4800"> differences are significant (p &lt; 0.01) by a Wilcoxon signed-rank test. These results indicate that modeling multiargument event sequences allows better inference of simpler pair events. These performance improvements may be due to the fact that the joint model conditions on information not representable in the single protagonist model (namely, all of the events in which a multi-argument event’s entities are involved). 5 Related Work The procedural encoding of common situations for automated reasoning dates back decades. The frames of Minsky (1974), schemas of Rumelhart (1975), and scripts of Schank and Abelson (1977) are early examples. These models use quite complex representations for events, with many different relations between events. They are not statistical, and use separate models for different scenarios (e.g. the “restaurant script” is different from the “bank script”). Generally, they require humans to encode procedural information by hand; see, however, Mooney and DeJong (1985) for an early method for learning scripts automatically from a document. Miikkulainen (1990; 1993) gives a hierarchical Neural Network system which stores sequences of events from text in episodic memory, capable of simpl</context>
</contexts>
<marker>Schank, Abelson, 1977</marker>
<rawString>Roger C. Schank and Robert P. Abelson. 1977. Scripts, Plans, Goals and Understanding: An Inquiry into Human Knowledge Structures. Lawrence Erlbaum and Associates, Hillsdale, NJ.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>