<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000039">
<title confidence="0.984824">
A Synchronous Hyperedge Replacement Grammar based approach for
AMR parsing
</title>
<author confidence="0.999303">
Xiaochang Peng, Linfeng Song and Daniel Gildea
</author>
<affiliation confidence="0.998506">
Department of Computer Science
University of Rochester
</affiliation>
<address confidence="0.337922">
Rochester, NY 14627
</address>
<sectionHeader confidence="0.976683" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99977465">
This paper presents a synchronous-graph-
grammar-based approach for string-to-
AMR parsing. We apply Markov Chain
Monte Carlo (MCMC) algorithms to
learn Synchronous Hyperedge Replace-
ment Grammar (SHRG) rules from a for-
est that represents likely derivations con-
sistent with a fixed string-to-graph align-
ment. We make an analogy of string-to-
AMR parsing to the task of phrase-based
machine translation and come up with an
efficient algorithm to learn graph gram-
mars from string-graph pairs. We pro-
pose an effective approximation strategy
to resolve the complexity issue of graph
compositions. We also show some useful
strategies to overcome existing problems
in an SHRG-based parser and present pre-
liminary results of a graph-grammar-based
approach.
</bodyText>
<sectionHeader confidence="0.999505" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.973844367346939">
Abstract Meaning Representation (AMR) (Ba-
narescu et al., 2013) is a semantic formalism
where the meaning of a sentence is encoded as a
rooted, directed graph. Figure 1 shows an exam-
ple of the edge-labeled representation of an AMR
graph where the edges are labeled while the nodes
are not. The label of the leaf edge going out of
a node represents the concept of the node, and
the label of a non-leaf edge shows the relation be-
tween the concepts of the two nodes it connects to.
This formalism is based on propositional logic and
neo-Davidsonian event representations (Parsons,
1990; Davidson, 1967). AMR does not encode
quantifiers, tense and modality, but it jointly en-
codes a set of selected semantic phenomena which
renders it useful in applications like question an-
swering and semantics-based machine translation.
Figure 1: An example of AMR graph representing
the meaning of: “The boy wants the girl to believe
him”
The task of AMR graph parsing is to map nat-
ural language strings to AMR semantic graphs.
Flanigan et al. (2014) propose a two-stage pars-
ing algorithm which first maps meaningful contin-
uous spans on the string side to concept fragments
on the graph side, and then in the second stage
adds additional edges to make all these fragments
connected. Concept identification (Flanigan et al.,
2014; Pourdamghani et al., 2014) can be consid-
ered as an important first step to relate components
of the string to components in the graph.
Wang et al. (2015) also present a two-stage pro-
cedure where they first use a dependency parser
trained on a large corpus to generate a depen-
dency tree for each sentence. In the second step,
a transition-based algorithm is used to greedily
modify the dependency tree into an AMR graph.
The benefit of starting with a dependency tree in-
stead of the original sentence is that the depen-
dency structure is more linguistically similar to an
AMR graph and provides more direct feature in-
formation within limited context.
Hyperedge replacement grammar (HRG) is a
context-free rewriting formalism for generating
graphs (Drewes et al., 1997). Its synchronous
counterpart, SHRG, can be used for transforming
a graph from/to another structured representation
such as a string or tree structure. HRG has great
potential for applications in natural language un-
</bodyText>
<figure confidence="0.99554175">
want-01
ARG1
believe-01
ARG0
ARG0
girl
ARG1
boy
</figure>
<page confidence="0.992033">
32
</page>
<note confidence="0.9801145">
Proceedings of the 19th Conference on Computational Language Learning, pages 32–41,
Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.99828516">
derstanding and generation, and also semantics-
based machine translation.
Given a graph as input, finding its derivation of
HRG rules is NP-complete (Drewes et al., 1997).
Chiang et al. (2013) describe in detail a graph
recognition algorithm and present an optimization
scheme which enables the parsing algorithm to run
in polynomial time when the treewidth and degree
of the graph are bounded. However, there is still
no real system available for parsing large graphs.
An SHRG can be used for AMR graph pars-
ing where each SHRG rule consists of a pair of
a CFG rule and an HRG rule, which can gener-
ate strings and AMR graphs in parallel. Jones et
al. (2012) present a Syntactic Semantic Algorithm
that learns SHRG by matching minimal parse con-
stituents to aligned graph fragments and incremen-
tally collapses them into hyperedge nonterminals.
The basic idea is to use the string-to-graph align-
ment and syntax information to constrain the pos-
sible HRGs.
Learning SHRG rules from fixed string-to-
graph alignments is a similar problem to extracting
machine translation rules from fixed word align-
ments, where we wish to automatically learn the
best granularity for the rules with which to ana-
lyze each sentence. Chung et al. (2014) present
an MCMC sampling schedule to learn Hiero-style
SCFG rules (Chiang, 2007) by sampling tree frag-
ments from phrase decomposition forests, which
represent all possible rules that are consistent with
a set of fixed word alignments, making use of the
property that each SCFG rule in the derivation is in
essence the decomposition of a larger phrase pair
into smaller ones.
In this paper, we make an analogy to treat frag-
ments in the graph language as phrases in the natu-
ral language string and SHRG rules as decomposi-
tions of larger substring, graph fragment pairs into
smaller ones. Graph language is different from
string language in that there is no explicit order
to compose the graph and there is an exponen-
tial number of possible compositions. We pro-
pose a strategy that uses the left-to-right order of
the string to constrain the structure of the deriva-
tion forest and experiment with different tactics in
dealing with unaligned words on the string side
and unaligned edges on the graph side.
Specifically, we make the following contribu-
tions:
</bodyText>
<footnote confidence="0.567442">
1. We come up an alternative SHRG-based
</footnote>
<figureCaption confidence="0.946522888888889">
Figure 2: The series of HRG rules applied to de-
rive the AMR graph of “The boy wants the girl to
believe him”. The first rule is directly shown. The
other HRG rules are either above or below each
right arrow. The white circle shows the root of
each hyperedge. The indexes in each rule show
the one-to-one mapping between the attachment
nodes of l.h.s. nonterminal edges and the external
nodes of the r.h.s. subgraph
</figureCaption>
<bodyText confidence="0.998819833333333">
AMR parsing strategy and present a reason-
able preliminary result without additional de-
pendency information and global features,
showing promising future applications when
a language model is applied or larger datasets
are available.
</bodyText>
<listItem confidence="0.95987015">
2. We present the novel notion of fragment de-
composition forest and come up with an ef-
ficient algorithm to construct the forest from
fixed string-to-graph alignment.
3. We propose an MCMC algorithm which sam-
ples a special type of SHRG rules which
helps maintain the properties of AMR graphs,
which should be able to generalize to learning
other synchronous grammar with a CFG left
side.
4. We augment the concept identification proce-
dure of Flanigan et al. (2014) with a phrase-
to-graph-fragment alignment table which
makes use of the dependency between con-
cepts.
5. We discovered that an SHRG-based approach
is especially sensitive to missing alignment
information. We present some simple yet ef-
fective ways motivated by the AMR guide-
line to deal with this issue.
</listItem>
<sectionHeader confidence="0.968883" genericHeader="method">
2 Hyperedge Replacement Grammar
</sectionHeader>
<bodyText confidence="0.995844666666667">
Hyperedge replacement grammar (HRG) is a
context-free rewriting formalism for graph gener-
ation (Drewes et al., 1997). HRG is like CFG in
</bodyText>
<figure confidence="0.978415534883721">
boy
X1
ARG1
X1 girl
X0
X3
X2
1 2
X3
want-01
ARG1
ARG0
1
2
want-01
ARG1
ARG0
X2
1 1
X1
X3
X2
X1
X1 boy
boy
X3
want-01
ARG1
believe-01
ARG1
ARG0
want-01
ARG1
ARG0
X3
1
believe-01
1
ARG0
ARG0
girl
boy
2 2
</figure>
<page confidence="0.998029">
33
</page>
<bodyText confidence="0.9996385">
that it rewrites nonterminals independently. While
CFG generates natural language strings by suc-
cessively rewriting nonterminal tokens, the non-
terminals in HRG are hyperedges, and each rewrit-
ing step in HRG replaces a hyperedge nonterminal
with a subgraph instead of a span of a string.
</bodyText>
<subsectionHeader confidence="0.967996">
2.1 Definitions
</subsectionHeader>
<bodyText confidence="0.971755093023256">
In this paper we only use edge-labeled graphs be-
cause using both node and edge labels complicates
the definitions in our HRG-based approach. Fig-
ure 2 shows a series of HRG rules applied to derive
the AMR graph shown in Figure 1.
We start with the definition of hypergraphs. An
edge-labeled, directed hypergraph is a tuple H =
(V, E,l, X), where V is a finite set of nodes, E C
V + is a finite set of hyperedges, each of which will
connect to one or more nodes in V . l : E —* L de-
fines a mapping from each hyperedge to its label
from a finite set L. Each hyperedge is an atomic
item with an ordered list of nodes it connects to,
which are called attachment nodes. The type of a
hyperedge is defined as the number of its attach-
ment nodes. X E V ∗ defines an ordered list of
distinct nodes called external nodes. The ordered
external nodes specify how to fuse a hypergraph
with another graph, as we will see below. In this
paper, we alternately use the terms of hypergraph
and graph, hyperedge and edge, and also phrase,
substring and span for brevity.
An HRG is a rewriting formalism G =
(N, T, P, 5), where N and T define two disjoint
finite sets called nonterminals and terminals. 5 E
N is a special nonterminal called the start sym-
bol. P is a finite set of productions of the form
A —* R, where A E N and R is a hypergraph
with edge labels over N U T and with nonempty
external nodes XR. We have the constraint that
the type of the hyperedge with label A should co-
incide with the number of nodes in XR. In our
grammar, each nonterminal has the form of Xn,
where n indicates the type of the hyperedge. Our
special start symbol is separately denoted as X0.
The rewriting mechanism replaces a nontermi-
nal hyperedge with the graph fragment specified
by a production’s righthand side (r.h.s), attaching
each external node of the r.h.s. to the correspond-
ing attachment node of the lefthand side. Take
Figure 2 as an example. Starting from our initial
hypergraph with one edge labeled with the start
symbol “X0”, we select one edge with nontermi-
</bodyText>
<figureCaption confidence="0.929648">
Figure 3: A series of symbol-refined SHRG rules
used to derive the AMR graph for the sentence
“The boy wants the girl to believe him”.
</figureCaption>
<bodyText confidence="0.99987395">
nal label in our current hypergraph, and rewrite it
using a rule in our HRG. The first rule rewrites the
start symbol with a subgraph shown on the r.h.s..
We continue the rewriting steps until there are no
more nonterminal-labeled edges.
The synchronous counterpart of HRG can be
used for transforming graphs from/to another form
of natural language representation. Productions
have the form (A —* (5, R), —), where A E N
and 5 and R are called the source and the target
and at least one of them should be hypergraphs
over N U T. — is a bijection linking nonterminals
mentions in 5 and R. In our case, the source side
is a CFG and the target side is an HRG. Given
such a synchronous grammar and a string as in-
put, we can parse the string with the CFG side
and then derive the counterpart graph by deduc-
tion from the derivation. The benefit of parsing
with SHRG is that the complexity is bounded by a
CFG-like parsing.
</bodyText>
<subsectionHeader confidence="0.988529">
2.2 SHRG-based AMR graph parsing
</subsectionHeader>
<bodyText confidence="0.999986388888889">
We write down AMR graphs as rooted, directed,
edge-labeled graphs. There is exactly one leaf
edge going out of each node, the label of which
represents the concept of the node. We define this
leaf edge as concept edge. In Figure 1, for ex-
ample, the edge labeled with “boy”, “want-01”,
“girl” or “believe-01” connects to only one node
in the AMR graph and each label represents the
concept of that node. AMR concepts are either En-
glish words (“boy”), PropBank framesets (“want-
01”), or special keywords like special entity types,
quantities, and logical conjunctions. The label of
each non-leaf edge shows the relation between the
AMR concepts of the two nodes it connects to.
The constraint of having exactly one concept
edge for each node is not guaranteed in general
SHRG. Our strategy for maintaining the AMR
graph structure is to refine the edge nontermi-
</bodyText>
<figure confidence="0.99763896">
[X0-1] The [X1-1, 1] [X3-100, 2] the [X2-10, 3] him |
1
2
[X1-1] boy |
boy
[X3-100] wants |
want-01
ARG1
2
[X1-1,1]
ARG0
[X3-100, 2]
[X1-1, 1]
1
1
believe-01 ARG1
ARG0
[X1-1, 1] to [X3-100, 2] |
[X2-10]
believe |
[X3-100]
girl
[X1-1] girl |
[X3-100,2]
[X2-10,3]
</figure>
<page confidence="0.99423">
34
</page>
<bodyText confidence="0.9999806">
nal label with an extra binary flag, representing
whether it will have a concept edge in the final
rewriting result, for each external node. The ba-
sic intuition is to explicitly enforce the one con-
cept edge constraint in each nonterminal so that no
additional concept edge is introduced after apply-
ing each rule. The graph derived from this type of
SHRG is guaranteed to have exactly one concept
edge at each node.
Figure 3 shows one example of our symbol-
refined SHRG. For each nonterminal Xi-b1 · · · bi,
i defines the type of the nonterminal, while each bi
indicates whether the i-th external node will have
a concept edge in the rewriting result.1 The sec-
ond rule, for example, rewrites nonterminal X3-
100 with wants on the string side and a hypergraph
with three external nodes where the root has a con-
cept edge :want-01 as the first binary flag 1 indi-
cates, while the other two external nodes do not
with the binary flag 0. This guarantees that when
we integrate the r.h.s. into another graph, it will in-
troduce the concept edge :want-01 to the first fus-
ing position and no concept edge to the next two.
While this refinement might result in an expo-
nential number of nonterminals with respect to the
maximum type of hyperedges, we found in our ex-
periment that most of the nonterminals do not ap-
pear in our grammar. We use a maximum edge
type of 5, which also results in a relatively small
nonterminal set.
</bodyText>
<sectionHeader confidence="0.922661" genericHeader="method">
3 Sampling SHRG from forests
</sectionHeader>
<bodyText confidence="0.999988583333333">
The fragment decomposition forest provides a
compact representation of all possible SHRG rules
that are consistent with a fixed string-to-graph
alignment. Each SHRG rule in the derivation is in
essence the decomposition of larger phrase, graph
fragment pairs on the left hand side (l.h.s.) into
smaller ones on the r.h.s. and is encoded in a tree
fragment in the forest. Our goal is to learn an
SHRG from this forest. We first build a forest rep-
resentation of possible derivations and then use an
MCMC algorithm to sample tree fragments from
this forest representing each rule in the derivation.
</bodyText>
<subsectionHeader confidence="0.998821">
3.1 Fragment Decomposition Forest
</subsectionHeader>
<bodyText confidence="0.997265923076923">
We first proceed to define the fragment decompo-
sition forest. The fragment decomposition forest
is a variation of the phrase decomposition forest
1X0-1 is different as XO is the start symbol of type one
and should always have a concept edge at the root
defined by Chung et al. (2014) where the target
side is a graph instead of a string.
A phrase p = [i, j] is a set of continuous word
indices {i, i + 1, ... , j − 11. A fragment f is
a hypergraph with external nodes Xf. A string-
to-graph alignment h : P —* F defines the map-
ping from spans in the sentence to fragments in the
graph. Our smallest phrase-fragment pairs are the
string-to-graph alignments extracted using heuris-
tic rules from Flanigan et al. (2014). The figure
above shows an example of the alignments for
the sentence “The boy wants the girl to believe
him”. The symbol 0 represents that the word is
not aligned to any concept in the AMR graph and
this word is called an unaligned word. After this
alignment, there are also left-over edges that are
not aligned from any substrings, which are called
unaligned edges.
Given an aligned string, AMR graph pair, a
phrase-fragment pair n is a pair ([i, j], f) which
defines a pair of a phrase [i, j] and a fragment f
such that words in positions [i, j] are only aligned
to concepts in the fragment f and vice versa (with
unaligned words and edges omitted). A fragment
forest H = (V, E) is a hypergraph made of a set
of hypernodes V and hyperedges E. Each node
n = ([i, j], f) is tight on the string side similar to
the definition by Koehn et al. (2003), i.e., n con-
tains no unaligned words at its boundaries. Note
here we do not have the constraint that f should be
connected or single rooted, but we will deal with
these constraints separately in the sampling proce-
dure.
We define two phrases [i1, j1], [i2, j2] to be ad-
jacent if word indices {j1, j1 + 1, ... , i2 − 11
are all unaligned. We also define two fragments
f1 = (V1, E1), f2 = (V2, E2) to be disjoint if
E1 n E2 = 0. And f1 and f2 are adjacent if they
are disjoint and f = (V1 U V2, E1 U E2) is con-
nected. We also define the compose operation of
two nodes: it takes two nodes n1 = ([i1, j1], f1)
and n2 = ([i2, j2], f2) (j1 &lt;— i2) as input, and com-
putes f = (V1 U V2, E1 U E2), the output is a
composed node n = ([i1, j2], f). We say n1 and
n2 are immediately adjacent if f is connected and
single-rooted.
We keep composing larger phrase-fragment
</bodyText>
<figure confidence="0.972281346153846">
0
z}|{
The
boy
z}|{
boy
0
z}|{
the
girl
z}|{
girl
0
z}|{
to
believe-01
z } |{
believe
0
z}|{
him
want-01
z } |{
wants
35
The boy wants the girl to believe him.
</figure>
<figureCaption confidence="0.941740333333333">
Figure 4: The fragment decomposition forest for
the (sentence, AMR graph) pair for “The boy
wants the girl to believe him”
</figureCaption>
<bodyText confidence="0.999764685714286">
pairs (each one kept in a node of the forest) from
smaller ones until we reach the root of the forest
whose phrase side is the whole sentence and the
fragment side is the complete AMR graph. We de-
fine fragment decomposition forest to be made
of all possible phrase-fragments pairs that can be
decomposed from the sentence AMR graph pair.
The fragment decomposition forest has the im-
portant property that any SHRG rule consistent
with the string-to-graph alignment corresponds to
a continuous tree fragment of a complete tree
found in the forest.
While we can compose larger phrases from
smaller ones from left to right, there is no explicit
order of composing the graph fragments. Also, the
number of possible graph fragments is highly ex-
ponential as we need to make a binary decision to
decide each boundary node of the fragment and
also choose the edges going out of each boundary
node of the fragment, unlike the polynomial num-
bers of phrases for fixed string alignment.
Our bottom-up construction procedure starts
from the smallest phrase-fragment pairs. We
first index these smallest phrase-fragment pairs
([ik, jk], fk), k = 1, 2, ... , n based on ascending
order of their start positions on the string side, i.e.,
jk ≤ ik+1 for k = 1, 2, ... , n − 1. Even with
this left-to-right order constraint from the string
side, the complexity of building the forest is still
exponential due to the possible choices in attach-
ing graphs edges that are not aligned to the string.
Our strategy is to deterministically attach each un-
aligned relation edge to one of the identified con-
cept fragments it connects to. We attach ARGs and
ops to its head node and each other types of un-
</bodyText>
<listItem confidence="0.948472769230769">
Algorithm 1 A CYK-like algorithm for building a
fragment decomposition forest
1: For each smallest phrase-fragment pairs ([ik, jk], fk), k
= 1, 2, ... , n, attach unaligned edges to fragment fk, de-
noting the result as fk. Build a node for ([ik, jk], fk) and
add it to chart item c[k][k + 1].
2: Extract all the remaining unaligned fragments, build a
special unaligned node for each of them and add it to
unaligned node set unaligned nodes
3: Keep composing unaligned nodes with nodes in different
chart items if they are immediate adjacent and add it to
the same chart item
4: for span from 2 to n do
5: for i from 1 to n-span+1 do
6: j=i+span
7: for k from i + 1 to j − 1 do
8: for n1 = ([start,, end,], f,) in c[i][k] do
9: for n2 = ([start2, end2], f2) in c[k][j] do
10: if f, and f2 are disjoint then
11: new node = compose(n1, n2)
12: add incoming edge (n1, n2) to
new node
13: if n, and n2 are not immediate adja-
cent then
14: new node.nosample cut=True
15: insert node(new node, c[i][j])
</listItem>
<bodyText confidence="0.982622875">
aligned relations to its tail node.2
Algorithm 1 shows our CYK-like forest con-
struction algorithm. We maintain the length 1
chart items according to the order of each smallest
phrase-fragment pair instead of its position in the
string.3 In line 1, we first attach unaligned edges
to the smallest phrase-fragment pairs as stated be-
fore. After this procedure, we build a node for
the k-th phrase-fragment (with unaligned edges
added) pair and add it to chart item c[k][k + 1].
Note here that we still have remaining unaligned
edges; in line 2 we attach all unaligned edges go-
ing out from the same node as a single fragment
and build a special unaligned node with empty
phrase side and add it to unaligned nodes set.
In line 3, we try to compose each unaligned node
with one of the nodes in the length 1 chart items
c[k][k + 1]. If they are immediately adjacent, we
add the composed node to c[k][k + 1]. The al-
gorithm then composes smaller phrase-fragment
pairs into larger ones (line 4). When we have com-
posed two nodes n1, n2, we need to keep track
2Our intuition is that the ARG types for verbs and ops
structure usually go with the concept of the head node. We
assume that other relations are additional introduced to the
head node, which resembles a simple binarization step for
other relations.
3We use this strategy mainly because the alignments avail-
able do not have overlapping alignments, while our algo-
rithm could still be easily adapted to a version that maintains
the chart items with string positions when overlapping align-
ments are available
</bodyText>
<figure confidence="0.999476878787879">
boy
want-01
boy ARG0
girl
ARG0
ARG1
want-01
want-01
ARG1
ARG0ARG0 girl
boy
ARG0
ARG1
want-01
boy
want-01
ARG0
ARG1 girl ARG0
believe-01
girl
ARG1
ARG0
ARG1
want-01
believe-01
girl ARG1
ARG0
believe-01
ARG1
ARG0ARG1
girl
believe-01
ARG1
</figure>
<page confidence="0.994353">
36
</page>
<bodyText confidence="0.999952095238095">
of this incoming edge. We have the constraint in
our grammar that the r.h.s. hypergraph of each rule
should be connected and single rooted.4 Lines 13
to 14 enforce this constraint by marking this node
with a nosample cut flag, which we will use in
the MCMC sampling stage. The insert node
function will check if the node already exists in
the chart item. If it already exists, then we only
update the incoming edges for that node. Other-
wise we will add it to the chart item.
For some sentence-AMR pairs where there are
too many nodes with unaligned edges going out,
considering all possible compositions would re-
sult in huge complexity overhead. One solution
we have adopted is to disallow disconnected graph
fragments and do not add them to the chart items
(Line 15). In practice, this pruning procedure does
not affect much of the final performance in our
current setting. Figure 4 shows the procedure of
building the fragment decomposition forest for the
sentence “The boy wants the girl to believe him”.
</bodyText>
<subsectionHeader confidence="0.998848">
3.2 MCMC sampling
</subsectionHeader>
<bodyText confidence="0.941576793103448">
Sampling methods have been used to learn Tree
Substitution Grammar (TSG) rules from deriva-
tion trees (Cohn et al., 2009; Post and Gildea,
2009) for TSG learning. The basic intuition is
to automatically learn the best granularity for the
rules with which to analyze our data. Our prob-
lem, however, is different in that we need to sam-
ple rules from a compact forest representation.
We need to sample one tree from the forest, and
then sample one derivation from this tree structure,
where each tree fragment represents one rule in the
derivation. Sampling tree fragments from forests
is described in detail in Chung et al. (2014) and
Peng and Gildea (2014).
We formulate the rule sampling procedure with
two types of variables: an edge variable en rep-
resenting which incoming hyperedge is chosen at
a given node n in the forest (allowing us to sam-
ple one tree from a forest) and a cut variable zn
representing whether node n in forest is a bound-
ary between two SHRG rules or is internal to an
SHRG rule (allowing us to sample rules from a
tree). Figure 5 shows one sampled derivation from
the forest. We have sampled one tree from the for-
est using the edge variables. We also have a 0-1
variable at each node in this tree where 0 repre-
4We should be able to get rid of both constraints as we are
parsing on the string side.
The boy wants the girl to believe him.
</bodyText>
<figureCaption confidence="0.745890333333333">
Figure 5: The sampled derivation for the (sen-
tence, AMR graph) pair for “The boy wants the
girl to believe him”
</figureCaption>
<bodyText confidence="0.934054666666667">
sents the current node is internal to an SHRG rule,
while 1 represents the current node is the boundary
of two SHRG rules.
Let all the edge variables form the random vec-
tor Y and all the cut variables form the random
vector Z. Given an assignment y to the edge vari-
ables and assignment z to the cut variables, our de-
sired distribution is proportional to the product of
weights of the rules specified by the assignment:
</bodyText>
<equation confidence="0.9983985">
Pt(Y = y,Z = z) a 11 w(r) (1)
r∈T(y,z)
</equation>
<bodyText confidence="0.999987166666667">
where T(y, z) is the set of rules identified by the
assignment and w(r) is the weight for each indi-
vidual rule. We use a generative model based on
a Dirichlet Process (DP) defined over composed
rules. We draw a distribution G over rules from a
DP, and then rules from G.
</bodyText>
<equation confidence="0.997736">
G  |α, P0 —Dir(α, P0)
r  |G —G
</equation>
<bodyText confidence="0.999810538461538">
We define two rules to have the same rule type
if they have the same string and hypergraph rep-
resentation (including order of external nodes) on
the r.h.s..For the base distribution P0, we use a uni-
form distribution where all rules of the same size
have equal probability. By marginalizing out G
we get a simple posterior distribution over rules
which can be derived using the Chinese Restaurant
Process (CRP). We define a table of counts N =
{NC}C∈I which memorizes different categories
of counts in the previous assignments, where I
is an index set for different categories of counts.
Each NC is a vector of counts for category C. We
</bodyText>
<figure confidence="0.998099318181818">
want-01
ARG1
1
boy
ARG0
believe-01
girl
ARG0
ARG1
0
1
ARG1
want-01
ARG0
ARG1
believe-01
ARG0
boy girl
1
1 1
boy 1 want-01 believe-01
ARG0ARG1 girl ARG1 ARG0
</figure>
<page confidence="0.998117">
37
</page>
<bodyText confidence="0.999893">
have the following probability over rule r given
the previous count table N:
</bodyText>
<equation confidence="0.9976865">
P(ri = r|N) = NR (r)
+ aP0(r) (2)
</equation>
<bodyText confidence="0.999628181818182">
here in the case of DP, I = {R}, where R is the
index for the category of rule counts.
We use the top-down sampling algorithm of
Chung et al. (2014) which samples cut and edge
variables from top down and one at a time. For
each node n, we denote the composed rule type
that we get when we set the cut of node n to 0 as
r1 and the two split rule types that we get when we
set the cut to 1 as r2, r3. We sample the cut value
zi of the current node according to the posterior
probability:
</bodyText>
<equation confidence="0.9969245">
P(r1|N) if z = 0
P zi = z N P(r1|N +P r2|N)P(r3|N&apos;) (3)
(2 — ) = P�r2|N�P(r3|N&apos;)
1P(r1|N)+P(r2|N)P(r3|N&apos;) otherwise
</equation>
<bodyText confidence="0.998723777777778">
where the posterior probability P(ri|N) is accord-
ing to a DP, and N, N0 are tables of counts. In the
case of DP, N, N0 differ only in the rule counts of
r2, where N0R(r2) = NR(r2) + 1.
As for edge variables ei, we refer to the set of
composed rules turned on below n including the
composed rule fragments having n as an internal
or root node as {r1, ... , rm}. We have the follow-
ing posterior probability over the edge variable ei:
</bodyText>
<equation confidence="0.9996">
P(ei = e|N) ∝ 11m P(ri|Ni−1) 11 deg(v) (4)
i=1 v∈τ(e)∩in(n)
</equation>
<bodyText confidence="0.96649375">
where deg(v) is the number of incoming edges for
node v, in(n) is the set of nodes in all subtrees
under n, and T(e) is the tree specified when we
set ei = e. N0 to Nm are tables of counts where
</bodyText>
<equation confidence="0.990179">
N0 = N, NiR(ri) = Ni−1
R (ri) + 1 in the case of
</equation>
<bodyText confidence="0.972895842105263">
DP.
After we have sampled one SHRG derivation
from the forest, we still need to keep track of the
place where each nonterminal edge attaches. As
we have maintained the graph fragment it repre-
sents in each node of the forest, we can retrieve
the attachment nodes of each hyperedge in the
r.h.s. by tracing at which graph nodes two frag-
ments fuse with each other. We perform this rule
extraction procedure from top-down and maintain
the order of attachment nodes of each r.h.s. non-
terminal edge. When we further rewrite a nonter-
minal edge, we need to make sure that it keeps the
order of the attachment nodes in its parent rule.
As for the unaligned words, we just insert all the
omitted unaligned words in the composition pro-
cedure. We also add additional rules including the
surrounding 2 unaligned words context to make
sure there are terminals on the string side.
</bodyText>
<subsectionHeader confidence="0.99735">
3.3 Phrase-to-Graph-Fragment Alignment
Extraction
</subsectionHeader>
<bodyText confidence="0.999984631578947">
Aside from the rules sampled using the MCMC
algorithm, we also extract a phrase-to-graph-
fragment alignment table from the fragment de-
composition forest. This step can be considered as
a mapping of larger phrases made of multiple iden-
tified spans (plus unaligned words) to a larger frag-
ments made of multiple concept fragments (plus
the way they connect using unaligned edges).
Our extraction happens along with the forest
construction procedure. In line 1 of Algorithm 1
we extract one rule for each smallest phrase-
fragment pairs before and after the unaligned
edges are attached. We also extract one rule for
each newly constructed node after line 11 if the
fragment side of the node is single-rooted.5 We do
not extract rules after line 2 because it usually in-
troduces additional noise of meaningful concepts
which are unrecognized in the concepts identifica-
tion stage.
</bodyText>
<sectionHeader confidence="0.999087" genericHeader="method">
4 Decoding
</sectionHeader>
<subsectionHeader confidence="0.999739">
4.1 Concept identification
</subsectionHeader>
<bodyText confidence="0.999899647058824">
During the decoding stage, first we need to iden-
tify meaningful spans in the sentence and map
them to graph fragments on the graph side. Then
we use SHRG rules to parse each sentence from
bottom up and left to right, which is similar to con-
stituent parsing. The recall of the concept identi-
fication stage from Flanigan et al. (2014) is 0.79,
which means 21% of the meaningful concepts are
already lost at the beginning of the next stage.
Our strategy is to use lemma and POS tags in-
formation after the concept identification stage,
we use it to recall some meaningful concepts.
We find that, except for some special function
words, most nouns, verbs and, adjectives should
be aligned. We use the lemma information to re-
trieve unaligned words whose morphological form
does not appear in our training data. We also use
</bodyText>
<footnote confidence="0.994384666666667">
5Here we will also look at the surrounding 2 unaligned
words to fix partial alignment and noise introduced by mean-
ingful unaligned words
</footnote>
<page confidence="0.998903">
38
</page>
<bodyText confidence="0.99993975">
POS tag information to deal with nouns and quan-
tities. Motivated by the fact that AMR makes ex-
tensive use of PropBank framesets, we look up
the argument structure of the verbs from the Prop-
Bank. Although the complicated abstraction of
AMR makes it hard to get the correct concept for
each word, the more complete structure can reduce
the propagation of errors along the derivation tree.
</bodyText>
<subsectionHeader confidence="0.981565">
4.2 AMR graph parsing
</subsectionHeader>
<bodyText confidence="0.999982166666667">
We use Earley algorithm with cube-pruning (Chi-
ang, 2007) for the string-to-AMR parsing. For
each synchronous rule with N nonterminals on
its l.h.s., we build an N + 1 dimensional cube
and generate top K candidates. Out of all the
hypotheses generated by all satisfied rules within
each span (i, j),we keep at most K candidates for
this span. Our glue rules will create a pseudo
R/ROOT concept and use ARGs relations to
connect disconnected components to make a con-
nected graph.
We use the following local features:
</bodyText>
<listItem confidence="0.946731083333333">
1. StringToGraphProbability: the probability of a hyper-
graph given the input string
2. RuleCount: number of rules used to compose the AMR
graph
3. RuleEdgeCount: the number of edges in the r.h.s. hy-
pergraph
4. EdgeType: the type of the l.h.s. nonterminal. For rules
with same source side tokens, we prefer rules with
smaller edge types.
5. AllNonTerminalPunish: one for rules which only have
non-terminals on the source side.
6. GlueCount: one for glue rules.
</listItem>
<bodyText confidence="0.999831857142857">
As our forest structure is highly binarized, it is
hard to capture the :opn structure when n is large
because we limit the number of external nodes to
5. The most common :op structure in the AMR
annotation is the coordinate structure of items sep-
arated by “;” or separated by “,” along with and.
We add the following two rules:
</bodyText>
<equation confidence="0.98289825">
[X1-1]− &gt; [X1-1, 1]; [X1-1, 2]; ··· ; [X1-1, n] |
(.:a/and :op1 [X1-1,1] :opt [X1-1,2] · · · :opn [X1-1, n])
[X1-1]− &gt; [X1-1, 1], [X1-1, 2], · · · and [X1-1, n] |
(.:a/and :op1 [X1-1, 1] :opt [X1-1, 2] · · · :opn [X1-1, n])
</equation>
<bodyText confidence="0.99999">
where the HRG side is a :a/and coordinate struc-
ture of X1-1s connected with relation :ops.
</bodyText>
<sectionHeader confidence="0.999895" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.992929">
We use the same newswire section of
LDC2013E117 as Flanigan et al. (2014), which
</bodyText>
<table confidence="0.9999014">
Precision Recall F-score
Concept id only 0.37 0.53 0.44
+ MCMC 0.57 0.53 0.55
+ MCMC + phrase table 0.60 0.54 0.57
+ All 0.59 0.58 0.58
</table>
<tableCaption confidence="0.900659">
Table 1: Comparisons of different strategies of ex-
tracting lexical rules on dev.
</tableCaption>
<bodyText confidence="0.997960162790698">
consists of 3955 training sentences, 2132 dev
sentences and 2132 test sentences. We also use
the string-to-graph alignment from Flanigan et al.
(2014) to construct the fragment decomposition
forest and to extract the phrase-to-fragment table.
In the fragment decomposition forest construc-
tion procedure, we have experimented with differ-
ent ways of dealing with the unaligned edges. First
we have tried to directly use the alignment, and
group all unaligned edges going out from the same
node as an unaligned fragment. Using this con-
straint would take a few hours or longer for some
sentences. The reason for this is because the many
number of unaligned edges can connect to each
branch of the aligned or unaligned fragments be-
low it. And there is no explicit order of composi-
tion with each branch. Another constraint we have
tried is to attach all unaligned edges to the head
node concept. The problem with this constraint is
that it is very hard to generalize and introduces a
lot of additional redundant relation edges.
As for sampling, we initialize all cut variables
in the forest as 1 (except for nodes that are marked
as nosample cut, which indicates we initialize it
with 0 and keep it fixed) and uniformly sample an
incoming edge for each node. We evaluate the per-
formance of our SHRG-based parser using Smatch
v1.0 (Cai and Knight, 2013), which evaluates the
precision, recall and F1 of the concepts and rela-
tions all together. Table 1 shows the dev results of
our sampled grammar using different lexical rules
that maps substrings to graph fragments. Concept
id only is the result of using the concepts identi-
fied by Flanigan et al. (2014). From second line,
we replace the concept identification result with
the lexical rules we have extracted from the train-
ing data (except for named entities and time ex-
pressions). +MCMC shows the result using ad-
ditional alignments identified using our sampling
approach. We can see that using the phrase to
graph fragment alignment learned from our train-
ing data can significantly improve the smatch. We
have also tried extracting all phrase-to-fragment
</bodyText>
<page confidence="0.998253">
39
</page>
<table confidence="0.99993425">
Precision Recall F-score
JAMR 0.67 0.58 0.62
Wang et al. 0.64 0.62 0.63
Our approach 0.59 0.57 0.58
</table>
<tableCaption confidence="0.999639">
Table 2: Comparisons of smatch score results
</tableCaption>
<bodyText confidence="0.999975535714286">
alignments of length 6 on the string side from
our constructed forest. We can see that using this
alignment table further improves the smatch score.
This is because the larger phrase-fragment pairs
can make better use of the dependency informa-
tion between continuous concepts. The improve-
ment is not much in comparison with MCMC, this
is perhaps MCMC can also learn some meaning
blocks that frequently appear together. As the
dataset is relatively small, so there are a lot of
meaningful concepts that are not aligned. We use
lemma as a backoff strategy to find the alignment
for the unaligned words. We have also used the
POS tag information to retrieve some unaligned
nouns and a PropBank dictionary to retrieve the
argument structure of the first sense of the verbs.
+All shows the result after using lemma, POS tag
and PropBank information, we can see that fixing
the alignment can improve the recall, but the pre-
cision does not change much.
Table 2 shows our result on test data. JAMR
is the baseline result from Flanigan et al. (2014).
Wang et al. (2015) shows the current state-of-
art for string-to-AMR parsing. Without the de-
pendency parse information and complex global
features, our SHRG-based approach can already
achieve competitive results in comparison with
these two algorithms.
</bodyText>
<sectionHeader confidence="0.999814" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999977296296296">
In comparison to the spanning tree algorithm of
Flanigan et al. (2014), an SHRG-based approach
is more sensitive to the alignment. If a lot of the
meaningful concepts are not aligned, then the lost
information would break down the structure of our
grammar. Using more data would definitely help
ease this issue. Building overlapping alignments
for the training data with more concepts alignment
would also be helpful.
Another thing to note is that Flanigan et al.
(2014) have used path information of dependency
arc labels and part of speech tags. Using these
global information can help the predication of the
relation edge labels. One interesting way to in-
clude such kind of path information is to add
a graph language model into our CFG decoder,
which should also help improve the performance.
All the weights of the local features mentioned
in Section 4.2 are tuned by hand. We have tried
tuning with MERT (Och, 2003), but the computa-
tion of smatch score for the k-best list has become
a major overhead. This issue might come from the
NP-Completeness of the problem smatch tries to
evaluate, unlike the simple counting of N-grams
in BLEU (Papineni et al., 2001). Parallelization
might be a consideration for tuning smatch score
with MERT.
</bodyText>
<sectionHeader confidence="0.999347" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999968368421053">
We presented an MCMC sampling schedule for
learning SHRG rules from a fragment decompo-
sition forest constructed from a fixed string-to-
AMR-graph alignment. While the complexity of
building a fragment decomposition forest is highly
exponential, we have come up with an effective
constraint from the string side that enables an effi-
cient construction algorithm. We have also eval-
uated our sampled SHRG on a string-to-AMR
graph parsing task and achieved some reasonable
result without using a dependency parse. Inter-
esting future work might include adding language
model on graph structure and also learning SHRG
from overlapping alignments.
Acknowledgments Funded by NSF IIS-
1446996, NSF IIS-1218209, the 2014 Jelinek
Memorial Workshop, and a Google Faculty Re-
search Award. We are grateful to Jeff Flanigan for
providing the results of his concept identification.
</bodyText>
<sectionHeader confidence="0.999475" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999113">
Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking. In Proceedings of the 7th Linguis-
tic Annotation Workshop and Interoperability with
Discourse.
Shu Cai and Kevin Knight. 2013. Smatch: an evalua-
tion metric for semantic feature structures. In ACL
(2), pages 748–752.
David Chiang, Jacob Andreas, Daniel Bauer,
Karl Moritz Hermann, Bevan Jones, and Kevin
Knight. 2013. Parsing graphs with hyperedge
replacement grammars. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages
924–932.
</reference>
<page confidence="0.9671">
40
</page>
<reference confidence="0.999947575757576">
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.
Tagyoung Chung, Licheng Fang, Daniel Gildea, and
Daniel ˇStefankoviˇc. 2014. Sampling tree fragments
from forests. Computational Linguistics, 40:203–
229.
Trevor Cohn, Sharon Goldwater, and Phil Blun-
som. 2009. Inducing compact but accurate tree-
substitution grammars. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 548–556,
Boulder, Colorado, June. Association for Computa-
tional Linguistics.
Donald Davidson. 1967. The logical form of action
sentences. In Nicholas Rescher, editor, The Logic of
Decision and Action, pages 81–120. Univ. of Pitts-
burgh Press.
Frank Drewes, Hans-J¨org Kreowski, and Annegret Ha-
bel. 1997. Hyperedge replacement, graph gram-
mars. In Handbook of Graph Grammars, volume 1,
pages 95–162. World Scientific, Singapore.
Jeffrey Flanigan, Sam Thomson, Jaime Carbonell,
Chris Dyer, and Noah A Smith. 2014. A discrim-
inative graph-based parser for the abstract meaning
representation. In Proc. of ACL, pages 1426–1436.
Bevan Jones, Jacob Andreas, Daniel Bauer,
Karl Moritz Hermann, and Kevin Knight. 2012.
Semantics-based machine translation with hyper-
edge replacement grammars. In COLING, pages
1359–1376.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of NAACL-03, pages 48–54, Edmonton,
Alberta.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of ACL-03, pages 160–167, Sapporo, Japan.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2001.
BLEU: a method for automatic evaluation of MT.
Technical Report Research Report, Computer Sci-
ence RC22176 (W0109-022), IBM Research Divi-
sion, T.J.Watson Research Center.
Terence Parsons. 1990. Events in the Semantics of
English, volume 5. Cambridge, Ma: MIT Press.
Xiaochang Peng and Daniel Gildea. 2014. Type-based
MCMC for sampling tree fragments from forests. In
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-14).
Matt Post and Daniel Gildea. 2009. Bayesian learning
of a tree substitution grammar. In Proc. Association
for Computational Linguistics (short paper), pages
45–48, Singapore.
Nima Pourdamghani, Yang Gao, Ulf Hermjakob, and
Kevin Knight. 2014. Aligning english strings with
abstract meaning representation graphs. In Proceed-
ings of the 2014 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
425–429.
Chuan Wang, Nianwen Xue, and Sameer Pradhan.
2015. A transition-based algorithm for amr parsing.
In Proceedings of the 2015 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 366–375, Denver, Colorado, May–June. As-
sociation for Computational Linguistics.
</reference>
<page confidence="0.999448">
41
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.667911">
<title confidence="0.934915">A Synchronous Hyperedge Replacement Grammar based approach for AMR parsing</title>
<author confidence="0.995132">Linfeng Song Peng</author>
<affiliation confidence="0.999818">Department of Computer University of</affiliation>
<address confidence="0.999061">Rochester, NY 14627</address>
<abstract confidence="0.988453095238095">This paper presents a synchronous-graphgrammar-based approach for string-to- AMR parsing. We apply Markov Chain Monte Carlo (MCMC) algorithms to learn Synchronous Hyperedge Replacement Grammar (SHRG) rules from a forest that represents likely derivations consistent with a fixed string-to-graph alignment. We make an analogy of string-to- AMR parsing to the task of phrase-based machine translation and come up with an efficient algorithm to learn graph grammars from string-graph pairs. We propose an effective approximation strategy to resolve the complexity issue of graph compositions. We also show some useful strategies to overcome existing problems in an SHRG-based parser and present preliminary results of a graph-grammar-based approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Laura Banarescu</author>
<author>Claire Bonial</author>
<author>Shu Cai</author>
<author>Madalina Georgescu</author>
<author>Kira Griffitt</author>
<author>Ulf Hermjakob</author>
<author>Kevin Knight</author>
<author>Philipp Koehn</author>
<author>Martha Palmer</author>
<author>Nathan Schneider</author>
</authors>
<title>Abstract meaning representation for sembanking.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse.</booktitle>
<contexts>
<context position="1026" citStr="Banarescu et al., 2013" startWordPosition="146" endWordPosition="150"> rules from a forest that represents likely derivations consistent with a fixed string-to-graph alignment. We make an analogy of string-toAMR parsing to the task of phrase-based machine translation and come up with an efficient algorithm to learn graph grammars from string-graph pairs. We propose an effective approximation strategy to resolve the complexity issue of graph compositions. We also show some useful strategies to overcome existing problems in an SHRG-based parser and present preliminary results of a graph-grammar-based approach. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph. Figure 1 shows an example of the edge-labeled representation of an AMR graph where the edges are labeled while the nodes are not. The label of the leaf edge going out of a node represents the concept of the node, and the label of a non-leaf edge shows the relation between the concepts of the two nodes it connects to. This formalism is based on propositional logic and neo-Davidsonian event representations (Parsons, 1990; Davidson, 1967). AMR does not encode quantifiers, tense and modality, but it jo</context>
</contexts>
<marker>Banarescu, Bonial, Cai, Georgescu, Griffitt, Hermjakob, Knight, Koehn, Palmer, Schneider, 2013</marker>
<rawString>Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract meaning representation for sembanking. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shu Cai</author>
<author>Kevin Knight</author>
</authors>
<title>Smatch: an evaluation metric for semantic feature structures.</title>
<date>2013</date>
<booktitle>In ACL (2),</booktitle>
<pages>748--752</pages>
<contexts>
<context position="33333" citStr="Cai and Knight, 2013" startWordPosition="5934" endWordPosition="5937">elow it. And there is no explicit order of composition with each branch. Another constraint we have tried is to attach all unaligned edges to the head node concept. The problem with this constraint is that it is very hard to generalize and introduces a lot of additional redundant relation edges. As for sampling, we initialize all cut variables in the forest as 1 (except for nodes that are marked as nosample cut, which indicates we initialize it with 0 and keep it fixed) and uniformly sample an incoming edge for each node. We evaluate the performance of our SHRG-based parser using Smatch v1.0 (Cai and Knight, 2013), which evaluates the precision, recall and F1 of the concepts and relations all together. Table 1 shows the dev results of our sampled grammar using different lexical rules that maps substrings to graph fragments. Concept id only is the result of using the concepts identified by Flanigan et al. (2014). From second line, we replace the concept identification result with the lexical rules we have extracted from the training data (except for named entities and time expressions). +MCMC shows the result using additional alignments identified using our sampling approach. We can see that using the p</context>
</contexts>
<marker>Cai, Knight, 2013</marker>
<rawString>Shu Cai and Kevin Knight. 2013. Smatch: an evaluation metric for semantic feature structures. In ACL (2), pages 748–752.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Jacob Andreas</author>
<author>Daniel Bauer</author>
<author>Karl Moritz Hermann</author>
<author>Bevan Jones</author>
<author>Kevin Knight</author>
</authors>
<title>Parsing graphs with hyperedge replacement grammars.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>924--932</pages>
<contexts>
<context position="3666" citStr="Chiang et al. (2013)" startWordPosition="579" endWordPosition="582">s synchronous counterpart, SHRG, can be used for transforming a graph from/to another structured representation such as a string or tree structure. HRG has great potential for applications in natural language unwant-01 ARG1 believe-01 ARG0 ARG0 girl ARG1 boy 32 Proceedings of the 19th Conference on Computational Language Learning, pages 32–41, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics derstanding and generation, and also semanticsbased machine translation. Given a graph as input, finding its derivation of HRG rules is NP-complete (Drewes et al., 1997). Chiang et al. (2013) describe in detail a graph recognition algorithm and present an optimization scheme which enables the parsing algorithm to run in polynomial time when the treewidth and degree of the graph are bounded. However, there is still no real system available for parsing large graphs. An SHRG can be used for AMR graph parsing where each SHRG rule consists of a pair of a CFG rule and an HRG rule, which can generate strings and AMR graphs in parallel. Jones et al. (2012) present a Syntactic Semantic Algorithm that learns SHRG by matching minimal parse constituents to aligned graph fragments and incremen</context>
</contexts>
<marker>Chiang, Andreas, Bauer, Hermann, Jones, Knight, 2013</marker>
<rawString>David Chiang, Jacob Andreas, Daniel Bauer, Karl Moritz Hermann, Bevan Jones, and Kevin Knight. 2013. Parsing graphs with hyperedge replacement grammars. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 924–932.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="4781" citStr="Chiang, 2007" startWordPosition="767" endWordPosition="768">hm that learns SHRG by matching minimal parse constituents to aligned graph fragments and incrementally collapses them into hyperedge nonterminals. The basic idea is to use the string-to-graph alignment and syntax information to constrain the possible HRGs. Learning SHRG rules from fixed string-tograph alignments is a similar problem to extracting machine translation rules from fixed word alignments, where we wish to automatically learn the best granularity for the rules with which to analyze each sentence. Chung et al. (2014) present an MCMC sampling schedule to learn Hiero-style SCFG rules (Chiang, 2007) by sampling tree fragments from phrase decomposition forests, which represent all possible rules that are consistent with a set of fixed word alignments, making use of the property that each SCFG rule in the derivation is in essence the decomposition of a larger phrase pair into smaller ones. In this paper, we make an analogy to treat fragments in the graph language as phrases in the natural language string and SHRG rules as decompositions of larger substring, graph fragment pairs into smaller ones. Graph language is different from string language in that there is no explicit order to compose</context>
<context position="30113" citStr="Chiang, 2007" startWordPosition="5372" endWordPosition="5374">lso use 5Here we will also look at the surrounding 2 unaligned words to fix partial alignment and noise introduced by meaningful unaligned words 38 POS tag information to deal with nouns and quantities. Motivated by the fact that AMR makes extensive use of PropBank framesets, we look up the argument structure of the verbs from the PropBank. Although the complicated abstraction of AMR makes it hard to get the correct concept for each word, the more complete structure can reduce the propagation of errors along the derivation tree. 4.2 AMR graph parsing We use Earley algorithm with cube-pruning (Chiang, 2007) for the string-to-AMR parsing. For each synchronous rule with N nonterminals on its l.h.s., we build an N + 1 dimensional cube and generate top K candidates. Out of all the hypotheses generated by all satisfied rules within each span (i, j),we keep at most K candidates for this span. Our glue rules will create a pseudo R/ROOT concept and use ARGs relations to connect disconnected components to make a connected graph. We use the following local features: 1. StringToGraphProbability: the probability of a hypergraph given the input string 2. RuleCount: number of rules used to compose the AMR gra</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tagyoung Chung</author>
<author>Licheng Fang</author>
<author>Daniel Gildea</author>
<author>Daniel ˇStefankoviˇc</author>
</authors>
<title>Sampling tree fragments from forests. Computational Linguistics,</title>
<date>2014</date>
<pages>40--203</pages>
<marker>Chung, Fang, Gildea, ˇStefankoviˇc, 2014</marker>
<rawString>Tagyoung Chung, Licheng Fang, Daniel Gildea, and Daniel ˇStefankoviˇc. 2014. Sampling tree fragments from forests. Computational Linguistics, 40:203– 229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Sharon Goldwater</author>
<author>Phil Blunsom</author>
</authors>
<title>Inducing compact but accurate treesubstitution grammars.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>548--556</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="22509" citStr="Cohn et al., 2009" startWordPosition="3978" endWordPosition="3981">nodes with unaligned edges going out, considering all possible compositions would result in huge complexity overhead. One solution we have adopted is to disallow disconnected graph fragments and do not add them to the chart items (Line 15). In practice, this pruning procedure does not affect much of the final performance in our current setting. Figure 4 shows the procedure of building the fragment decomposition forest for the sentence “The boy wants the girl to believe him”. 3.2 MCMC sampling Sampling methods have been used to learn Tree Substitution Grammar (TSG) rules from derivation trees (Cohn et al., 2009; Post and Gildea, 2009) for TSG learning. The basic intuition is to automatically learn the best granularity for the rules with which to analyze our data. Our problem, however, is different in that we need to sample rules from a compact forest representation. We need to sample one tree from the forest, and then sample one derivation from this tree structure, where each tree fragment represents one rule in the derivation. Sampling tree fragments from forests is described in detail in Chung et al. (2014) and Peng and Gildea (2014). We formulate the rule sampling procedure with two types of vari</context>
</contexts>
<marker>Cohn, Goldwater, Blunsom, 2009</marker>
<rawString>Trevor Cohn, Sharon Goldwater, and Phil Blunsom. 2009. Inducing compact but accurate treesubstitution grammars. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 548–556, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Davidson</author>
</authors>
<title>The logical form of action sentences.</title>
<date>1967</date>
<booktitle>The Logic of Decision and Action,</booktitle>
<pages>81--120</pages>
<editor>In Nicholas Rescher, editor,</editor>
<publisher>Univ. of Pittsburgh Press.</publisher>
<contexts>
<context position="1562" citStr="Davidson, 1967" startWordPosition="243" endWordPosition="244">. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph. Figure 1 shows an example of the edge-labeled representation of an AMR graph where the edges are labeled while the nodes are not. The label of the leaf edge going out of a node represents the concept of the node, and the label of a non-leaf edge shows the relation between the concepts of the two nodes it connects to. This formalism is based on propositional logic and neo-Davidsonian event representations (Parsons, 1990; Davidson, 1967). AMR does not encode quantifiers, tense and modality, but it jointly encodes a set of selected semantic phenomena which renders it useful in applications like question answering and semantics-based machine translation. Figure 1: An example of AMR graph representing the meaning of: “The boy wants the girl to believe him” The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Flanigan et al. (2014) propose a two-stage parsing algorithm which first maps meaningful continuous spans on the string side to concept fragments on the graph side, and then in the second </context>
</contexts>
<marker>Davidson, 1967</marker>
<rawString>Donald Davidson. 1967. The logical form of action sentences. In Nicholas Rescher, editor, The Logic of Decision and Action, pages 81–120. Univ. of Pittsburgh Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Drewes</author>
<author>Hans-J¨org Kreowski</author>
<author>Annegret Habel</author>
</authors>
<title>Hyperedge replacement, graph grammars.</title>
<date>1997</date>
<booktitle>In Handbook of Graph Grammars,</booktitle>
<volume>1</volume>
<pages>95--162</pages>
<publisher>World Scientific,</publisher>
<contexts>
<context position="3042" citStr="Drewes et al., 1997" startWordPosition="487" endWordPosition="490"> (2015) also present a two-stage procedure where they first use a dependency parser trained on a large corpus to generate a dependency tree for each sentence. In the second step, a transition-based algorithm is used to greedily modify the dependency tree into an AMR graph. The benefit of starting with a dependency tree instead of the original sentence is that the dependency structure is more linguistically similar to an AMR graph and provides more direct feature information within limited context. Hyperedge replacement grammar (HRG) is a context-free rewriting formalism for generating graphs (Drewes et al., 1997). Its synchronous counterpart, SHRG, can be used for transforming a graph from/to another structured representation such as a string or tree structure. HRG has great potential for applications in natural language unwant-01 ARG1 believe-01 ARG0 ARG0 girl ARG1 boy 32 Proceedings of the 19th Conference on Computational Language Learning, pages 32–41, Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics derstanding and generation, and also semanticsbased machine translation. Given a graph as input, finding its derivation of HRG rules is NP-complete (Drewes et al., 199</context>
<context position="7354" citStr="Drewes et al., 1997" startWordPosition="1193" endWordPosition="1196">which should be able to generalize to learning other synchronous grammar with a CFG left side. 4. We augment the concept identification procedure of Flanigan et al. (2014) with a phraseto-graph-fragment alignment table which makes use of the dependency between concepts. 5. We discovered that an SHRG-based approach is especially sensitive to missing alignment information. We present some simple yet effective ways motivated by the AMR guideline to deal with this issue. 2 Hyperedge Replacement Grammar Hyperedge replacement grammar (HRG) is a context-free rewriting formalism for graph generation (Drewes et al., 1997). HRG is like CFG in boy X1 ARG1 X1 girl X0 X3 X2 1 2 X3 want-01 ARG1 ARG0 1 2 want-01 ARG1 ARG0 X2 1 1 X1 X3 X2 X1 X1 boy boy X3 want-01 ARG1 believe-01 ARG1 ARG0 want-01 ARG1 ARG0 X3 1 believe-01 1 ARG0 ARG0 girl boy 2 2 33 that it rewrites nonterminals independently. While CFG generates natural language strings by successively rewriting nonterminal tokens, the nonterminals in HRG are hyperedges, and each rewriting step in HRG replaces a hyperedge nonterminal with a subgraph instead of a span of a string. 2.1 Definitions In this paper we only use edge-labeled graphs because using both node a</context>
</contexts>
<marker>Drewes, Kreowski, Habel, 1997</marker>
<rawString>Frank Drewes, Hans-J¨org Kreowski, and Annegret Habel. 1997. Hyperedge replacement, graph grammars. In Handbook of Graph Grammars, volume 1, pages 95–162. World Scientific, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Flanigan</author>
<author>Sam Thomson</author>
<author>Jaime Carbonell</author>
<author>Chris Dyer</author>
<author>Noah A Smith</author>
</authors>
<title>A discriminative graph-based parser for the abstract meaning representation.</title>
<date>2014</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>1426--1436</pages>
<contexts>
<context position="1996" citStr="Flanigan et al. (2014)" startWordPosition="314" endWordPosition="317"> the relation between the concepts of the two nodes it connects to. This formalism is based on propositional logic and neo-Davidsonian event representations (Parsons, 1990; Davidson, 1967). AMR does not encode quantifiers, tense and modality, but it jointly encodes a set of selected semantic phenomena which renders it useful in applications like question answering and semantics-based machine translation. Figure 1: An example of AMR graph representing the meaning of: “The boy wants the girl to believe him” The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Flanigan et al. (2014) propose a two-stage parsing algorithm which first maps meaningful continuous spans on the string side to concept fragments on the graph side, and then in the second stage adds additional edges to make all these fragments connected. Concept identification (Flanigan et al., 2014; Pourdamghani et al., 2014) can be considered as an important first step to relate components of the string to components in the graph. Wang et al. (2015) also present a two-stage procedure where they first use a dependency parser trained on a large corpus to generate a dependency tree for each sentence. In the second s</context>
<context position="6905" citStr="Flanigan et al. (2014)" startWordPosition="1124" endWordPosition="1127">dditional dependency information and global features, showing promising future applications when a language model is applied or larger datasets are available. 2. We present the novel notion of fragment decomposition forest and come up with an efficient algorithm to construct the forest from fixed string-to-graph alignment. 3. We propose an MCMC algorithm which samples a special type of SHRG rules which helps maintain the properties of AMR graphs, which should be able to generalize to learning other synchronous grammar with a CFG left side. 4. We augment the concept identification procedure of Flanigan et al. (2014) with a phraseto-graph-fragment alignment table which makes use of the dependency between concepts. 5. We discovered that an SHRG-based approach is especially sensitive to missing alignment information. We present some simple yet effective ways motivated by the AMR guideline to deal with this issue. 2 Hyperedge Replacement Grammar Hyperedge replacement grammar (HRG) is a context-free rewriting formalism for graph generation (Drewes et al., 1997). HRG is like CFG in boy X1 ARG1 X1 girl X0 X3 X2 1 2 X3 want-01 ARG1 ARG0 1 2 want-01 ARG1 ARG0 X2 1 1 X1 X3 X2 X1 X1 boy boy X3 want-01 ARG1 believe-</context>
<context position="14915" citStr="Flanigan et al. (2014)" startWordPosition="2577" endWordPosition="2580">rest is a variation of the phrase decomposition forest 1X0-1 is different as XO is the start symbol of type one and should always have a concept edge at the root defined by Chung et al. (2014) where the target side is a graph instead of a string. A phrase p = [i, j] is a set of continuous word indices {i, i + 1, ... , j − 11. A fragment f is a hypergraph with external nodes Xf. A stringto-graph alignment h : P —* F defines the mapping from spans in the sentence to fragments in the graph. Our smallest phrase-fragment pairs are the string-to-graph alignments extracted using heuristic rules from Flanigan et al. (2014). The figure above shows an example of the alignments for the sentence “The boy wants the girl to believe him”. The symbol 0 represents that the word is not aligned to any concept in the AMR graph and this word is called an unaligned word. After this alignment, there are also left-over edges that are not aligned from any substrings, which are called unaligned edges. Given an aligned string, AMR graph pair, a phrase-fragment pair n is a pair ([i, j], f) which defines a pair of a phrase [i, j] and a fragment f such that words in positions [i, j] are only aligned to concepts in the fragment f and</context>
<context position="29023" citStr="Flanigan et al. (2014)" startWordPosition="5184" endWordPosition="5187"> constructed node after line 11 if the fragment side of the node is single-rooted.5 We do not extract rules after line 2 because it usually introduces additional noise of meaningful concepts which are unrecognized in the concepts identification stage. 4 Decoding 4.1 Concept identification During the decoding stage, first we need to identify meaningful spans in the sentence and map them to graph fragments on the graph side. Then we use SHRG rules to parse each sentence from bottom up and left to right, which is similar to constituent parsing. The recall of the concept identification stage from Flanigan et al. (2014) is 0.79, which means 21% of the meaningful concepts are already lost at the beginning of the next stage. Our strategy is to use lemma and POS tags information after the concept identification stage, we use it to recall some meaningful concepts. We find that, except for some special function words, most nouns, verbs and, adjectives should be aligned. We use the lemma information to retrieve unaligned words whose morphological form does not appear in our training data. We also use 5Here we will also look at the surrounding 2 unaligned words to fix partial alignment and noise introduced by meani</context>
<context position="31762" citStr="Flanigan et al. (2014)" startWordPosition="5665" endWordPosition="5668">e we limit the number of external nodes to 5. The most common :op structure in the AMR annotation is the coordinate structure of items separated by “;” or separated by “,” along with and. We add the following two rules: [X1-1]− &gt; [X1-1, 1]; [X1-1, 2]; ··· ; [X1-1, n] | (.:a/and :op1 [X1-1,1] :opt [X1-1,2] · · · :opn [X1-1, n]) [X1-1]− &gt; [X1-1, 1], [X1-1, 2], · · · and [X1-1, n] | (.:a/and :op1 [X1-1, 1] :opt [X1-1, 2] · · · :opn [X1-1, n]) where the HRG side is a :a/and coordinate structure of X1-1s connected with relation :ops. 5 Experiments We use the same newswire section of LDC2013E117 as Flanigan et al. (2014), which Precision Recall F-score Concept id only 0.37 0.53 0.44 + MCMC 0.57 0.53 0.55 + MCMC + phrase table 0.60 0.54 0.57 + All 0.59 0.58 0.58 Table 1: Comparisons of different strategies of extracting lexical rules on dev. consists of 3955 training sentences, 2132 dev sentences and 2132 test sentences. We also use the string-to-graph alignment from Flanigan et al. (2014) to construct the fragment decomposition forest and to extract the phrase-to-fragment table. In the fragment decomposition forest construction procedure, we have experimented with different ways of dealing with the unaligned </context>
<context position="33636" citStr="Flanigan et al. (2014)" startWordPosition="5986" endWordPosition="5989">r sampling, we initialize all cut variables in the forest as 1 (except for nodes that are marked as nosample cut, which indicates we initialize it with 0 and keep it fixed) and uniformly sample an incoming edge for each node. We evaluate the performance of our SHRG-based parser using Smatch v1.0 (Cai and Knight, 2013), which evaluates the precision, recall and F1 of the concepts and relations all together. Table 1 shows the dev results of our sampled grammar using different lexical rules that maps substrings to graph fragments. Concept id only is the result of using the concepts identified by Flanigan et al. (2014). From second line, we replace the concept identification result with the lexical rules we have extracted from the training data (except for named entities and time expressions). +MCMC shows the result using additional alignments identified using our sampling approach. We can see that using the phrase to graph fragment alignment learned from our training data can significantly improve the smatch. We have also tried extracting all phrase-to-fragment 39 Precision Recall F-score JAMR 0.67 0.58 0.62 Wang et al. 0.64 0.62 0.63 Our approach 0.59 0.57 0.58 Table 2: Comparisons of smatch score results</context>
<context position="35277" citStr="Flanigan et al. (2014)" startWordPosition="6262" endWordPosition="6265"> As the dataset is relatively small, so there are a lot of meaningful concepts that are not aligned. We use lemma as a backoff strategy to find the alignment for the unaligned words. We have also used the POS tag information to retrieve some unaligned nouns and a PropBank dictionary to retrieve the argument structure of the first sense of the verbs. +All shows the result after using lemma, POS tag and PropBank information, we can see that fixing the alignment can improve the recall, but the precision does not change much. Table 2 shows our result on test data. JAMR is the baseline result from Flanigan et al. (2014). Wang et al. (2015) shows the current state-ofart for string-to-AMR parsing. Without the dependency parse information and complex global features, our SHRG-based approach can already achieve competitive results in comparison with these two algorithms. 6 Discussion In comparison to the spanning tree algorithm of Flanigan et al. (2014), an SHRG-based approach is more sensitive to the alignment. If a lot of the meaningful concepts are not aligned, then the lost information would break down the structure of our grammar. Using more data would definitely help ease this issue. Building overlapping a</context>
</contexts>
<marker>Flanigan, Thomson, Carbonell, Dyer, Smith, 2014</marker>
<rawString>Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, Chris Dyer, and Noah A Smith. 2014. A discriminative graph-based parser for the abstract meaning representation. In Proc. of ACL, pages 1426–1436.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bevan Jones</author>
<author>Jacob Andreas</author>
<author>Daniel Bauer</author>
<author>Karl Moritz Hermann</author>
<author>Kevin Knight</author>
</authors>
<title>Semantics-based machine translation with hyperedge replacement grammars.</title>
<date>2012</date>
<booktitle>In COLING,</booktitle>
<pages>1359--1376</pages>
<contexts>
<context position="4131" citStr="Jones et al. (2012)" startWordPosition="663" endWordPosition="666">semanticsbased machine translation. Given a graph as input, finding its derivation of HRG rules is NP-complete (Drewes et al., 1997). Chiang et al. (2013) describe in detail a graph recognition algorithm and present an optimization scheme which enables the parsing algorithm to run in polynomial time when the treewidth and degree of the graph are bounded. However, there is still no real system available for parsing large graphs. An SHRG can be used for AMR graph parsing where each SHRG rule consists of a pair of a CFG rule and an HRG rule, which can generate strings and AMR graphs in parallel. Jones et al. (2012) present a Syntactic Semantic Algorithm that learns SHRG by matching minimal parse constituents to aligned graph fragments and incrementally collapses them into hyperedge nonterminals. The basic idea is to use the string-to-graph alignment and syntax information to constrain the possible HRGs. Learning SHRG rules from fixed string-tograph alignments is a similar problem to extracting machine translation rules from fixed word alignments, where we wish to automatically learn the best granularity for the rules with which to analyze each sentence. Chung et al. (2014) present an MCMC sampling sched</context>
</contexts>
<marker>Jones, Andreas, Bauer, Hermann, Knight, 2012</marker>
<rawString>Bevan Jones, Jacob Andreas, Daniel Bauer, Karl Moritz Hermann, and Kevin Knight. 2012. Semantics-based machine translation with hyperedge replacement grammars. In COLING, pages 1359–1376.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL-03,</booktitle>
<pages>48--54</pages>
<location>Edmonton, Alberta.</location>
<contexts>
<context position="15764" citStr="Koehn et al. (2003)" startWordPosition="2739" endWordPosition="2742">ned word. After this alignment, there are also left-over edges that are not aligned from any substrings, which are called unaligned edges. Given an aligned string, AMR graph pair, a phrase-fragment pair n is a pair ([i, j], f) which defines a pair of a phrase [i, j] and a fragment f such that words in positions [i, j] are only aligned to concepts in the fragment f and vice versa (with unaligned words and edges omitted). A fragment forest H = (V, E) is a hypergraph made of a set of hypernodes V and hyperedges E. Each node n = ([i, j], f) is tight on the string side similar to the definition by Koehn et al. (2003), i.e., n contains no unaligned words at its boundaries. Note here we do not have the constraint that f should be connected or single rooted, but we will deal with these constraints separately in the sampling procedure. We define two phrases [i1, j1], [i2, j2] to be adjacent if word indices {j1, j1 + 1, ... , i2 − 11 are all unaligned. We also define two fragments f1 = (V1, E1), f2 = (V2, E2) to be disjoint if E1 n E2 = 0. And f1 and f2 are adjacent if they are disjoint and f = (V1 U V2, E1 U E2) is connected. We also define the compose operation of two nodes: it takes two nodes n1 = ([i1, j1]</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of NAACL-03, pages 48–54, Edmonton, Alberta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL-03,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="36464" citStr="Och, 2003" startWordPosition="6457" endWordPosition="6458"> Building overlapping alignments for the training data with more concepts alignment would also be helpful. Another thing to note is that Flanigan et al. (2014) have used path information of dependency arc labels and part of speech tags. Using these global information can help the predication of the relation edge labels. One interesting way to include such kind of path information is to add a graph language model into our CFG decoder, which should also help improve the performance. All the weights of the local features mentioned in Section 4.2 are tuned by hand. We have tried tuning with MERT (Och, 2003), but the computation of smatch score for the k-best list has become a major overhead. This issue might come from the NP-Completeness of the problem smatch tries to evaluate, unlike the simple counting of N-grams in BLEU (Papineni et al., 2001). Parallelization might be a consideration for tuning smatch score with MERT. 7 Conclusion We presented an MCMC sampling schedule for learning SHRG rules from a fragment decomposition forest constructed from a fixed string-toAMR-graph alignment. While the complexity of building a fragment decomposition forest is highly exponential, we have come up with a</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training for statistical machine translation. In Proceedings of ACL-03, pages 160–167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of MT.</title>
<date>2001</date>
<booktitle>Computer Science RC22176 (W0109-022), IBM Research Division, T.J.Watson Research</booktitle>
<tech>Technical Report Research Report,</tech>
<location>Center.</location>
<contexts>
<context position="36708" citStr="Papineni et al., 2001" startWordPosition="6497" endWordPosition="6500">ags. Using these global information can help the predication of the relation edge labels. One interesting way to include such kind of path information is to add a graph language model into our CFG decoder, which should also help improve the performance. All the weights of the local features mentioned in Section 4.2 are tuned by hand. We have tried tuning with MERT (Och, 2003), but the computation of smatch score for the k-best list has become a major overhead. This issue might come from the NP-Completeness of the problem smatch tries to evaluate, unlike the simple counting of N-grams in BLEU (Papineni et al., 2001). Parallelization might be a consideration for tuning smatch score with MERT. 7 Conclusion We presented an MCMC sampling schedule for learning SHRG rules from a fragment decomposition forest constructed from a fixed string-toAMR-graph alignment. While the complexity of building a fragment decomposition forest is highly exponential, we have come up with an effective constraint from the string side that enables an efficient construction algorithm. We have also evaluated our sampled SHRG on a string-to-AMR graph parsing task and achieved some reasonable result without using a dependency parse. In</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2001. BLEU: a method for automatic evaluation of MT. Technical Report Research Report, Computer Science RC22176 (W0109-022), IBM Research Division, T.J.Watson Research Center.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terence Parsons</author>
</authors>
<title>Events in the Semantics of English,</title>
<date>1990</date>
<volume>5</volume>
<publisher>MIT Press.</publisher>
<location>Cambridge, Ma:</location>
<contexts>
<context position="1545" citStr="Parsons, 1990" startWordPosition="241" endWordPosition="242">-based approach. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic formalism where the meaning of a sentence is encoded as a rooted, directed graph. Figure 1 shows an example of the edge-labeled representation of an AMR graph where the edges are labeled while the nodes are not. The label of the leaf edge going out of a node represents the concept of the node, and the label of a non-leaf edge shows the relation between the concepts of the two nodes it connects to. This formalism is based on propositional logic and neo-Davidsonian event representations (Parsons, 1990; Davidson, 1967). AMR does not encode quantifiers, tense and modality, but it jointly encodes a set of selected semantic phenomena which renders it useful in applications like question answering and semantics-based machine translation. Figure 1: An example of AMR graph representing the meaning of: “The boy wants the girl to believe him” The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Flanigan et al. (2014) propose a two-stage parsing algorithm which first maps meaningful continuous spans on the string side to concept fragments on the graph side, and th</context>
</contexts>
<marker>Parsons, 1990</marker>
<rawString>Terence Parsons. 1990. Events in the Semantics of English, volume 5. Cambridge, Ma: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaochang Peng</author>
<author>Daniel Gildea</author>
</authors>
<title>Type-based MCMC for sampling tree fragments from forests.</title>
<date>2014</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing (EMNLP-14).</booktitle>
<contexts>
<context position="23044" citStr="Peng and Gildea (2014)" startWordPosition="4070" endWordPosition="4073">to learn Tree Substitution Grammar (TSG) rules from derivation trees (Cohn et al., 2009; Post and Gildea, 2009) for TSG learning. The basic intuition is to automatically learn the best granularity for the rules with which to analyze our data. Our problem, however, is different in that we need to sample rules from a compact forest representation. We need to sample one tree from the forest, and then sample one derivation from this tree structure, where each tree fragment represents one rule in the derivation. Sampling tree fragments from forests is described in detail in Chung et al. (2014) and Peng and Gildea (2014). We formulate the rule sampling procedure with two types of variables: an edge variable en representing which incoming hyperedge is chosen at a given node n in the forest (allowing us to sample one tree from a forest) and a cut variable zn representing whether node n in forest is a boundary between two SHRG rules or is internal to an SHRG rule (allowing us to sample rules from a tree). Figure 5 shows one sampled derivation from the forest. We have sampled one tree from the forest using the edge variables. We also have a 0-1 variable at each node in this tree where 0 repre4We should be able to</context>
</contexts>
<marker>Peng, Gildea, 2014</marker>
<rawString>Xiaochang Peng and Daniel Gildea. 2014. Type-based MCMC for sampling tree fragments from forests. In Conference on Empirical Methods in Natural Language Processing (EMNLP-14).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Post</author>
<author>Daniel Gildea</author>
</authors>
<title>Bayesian learning of a tree substitution grammar.</title>
<date>2009</date>
<booktitle>In Proc. Association for Computational Linguistics (short paper),</booktitle>
<pages>45--48</pages>
<contexts>
<context position="22533" citStr="Post and Gildea, 2009" startWordPosition="3982" endWordPosition="3985">d edges going out, considering all possible compositions would result in huge complexity overhead. One solution we have adopted is to disallow disconnected graph fragments and do not add them to the chart items (Line 15). In practice, this pruning procedure does not affect much of the final performance in our current setting. Figure 4 shows the procedure of building the fragment decomposition forest for the sentence “The boy wants the girl to believe him”. 3.2 MCMC sampling Sampling methods have been used to learn Tree Substitution Grammar (TSG) rules from derivation trees (Cohn et al., 2009; Post and Gildea, 2009) for TSG learning. The basic intuition is to automatically learn the best granularity for the rules with which to analyze our data. Our problem, however, is different in that we need to sample rules from a compact forest representation. We need to sample one tree from the forest, and then sample one derivation from this tree structure, where each tree fragment represents one rule in the derivation. Sampling tree fragments from forests is described in detail in Chung et al. (2014) and Peng and Gildea (2014). We formulate the rule sampling procedure with two types of variables: an edge variable </context>
</contexts>
<marker>Post, Gildea, 2009</marker>
<rawString>Matt Post and Daniel Gildea. 2009. Bayesian learning of a tree substitution grammar. In Proc. Association for Computational Linguistics (short paper), pages 45–48, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nima Pourdamghani</author>
<author>Yang Gao</author>
<author>Ulf Hermjakob</author>
<author>Kevin Knight</author>
</authors>
<title>Aligning english strings with abstract meaning representation graphs.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>425--429</pages>
<contexts>
<context position="2302" citStr="Pourdamghani et al., 2014" startWordPosition="363" endWordPosition="366"> which renders it useful in applications like question answering and semantics-based machine translation. Figure 1: An example of AMR graph representing the meaning of: “The boy wants the girl to believe him” The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Flanigan et al. (2014) propose a two-stage parsing algorithm which first maps meaningful continuous spans on the string side to concept fragments on the graph side, and then in the second stage adds additional edges to make all these fragments connected. Concept identification (Flanigan et al., 2014; Pourdamghani et al., 2014) can be considered as an important first step to relate components of the string to components in the graph. Wang et al. (2015) also present a two-stage procedure where they first use a dependency parser trained on a large corpus to generate a dependency tree for each sentence. In the second step, a transition-based algorithm is used to greedily modify the dependency tree into an AMR graph. The benefit of starting with a dependency tree instead of the original sentence is that the dependency structure is more linguistically similar to an AMR graph and provides more direct feature information w</context>
</contexts>
<marker>Pourdamghani, Gao, Hermjakob, Knight, 2014</marker>
<rawString>Nima Pourdamghani, Yang Gao, Ulf Hermjakob, and Kevin Knight. 2014. Aligning english strings with abstract meaning representation graphs. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 425–429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chuan Wang</author>
<author>Nianwen Xue</author>
<author>Sameer Pradhan</author>
</authors>
<title>A transition-based algorithm for amr parsing.</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>366--375</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Denver, Colorado, May–June.</location>
<contexts>
<context position="2429" citStr="Wang et al. (2015)" startWordPosition="387" endWordPosition="390">raph representing the meaning of: “The boy wants the girl to believe him” The task of AMR graph parsing is to map natural language strings to AMR semantic graphs. Flanigan et al. (2014) propose a two-stage parsing algorithm which first maps meaningful continuous spans on the string side to concept fragments on the graph side, and then in the second stage adds additional edges to make all these fragments connected. Concept identification (Flanigan et al., 2014; Pourdamghani et al., 2014) can be considered as an important first step to relate components of the string to components in the graph. Wang et al. (2015) also present a two-stage procedure where they first use a dependency parser trained on a large corpus to generate a dependency tree for each sentence. In the second step, a transition-based algorithm is used to greedily modify the dependency tree into an AMR graph. The benefit of starting with a dependency tree instead of the original sentence is that the dependency structure is more linguistically similar to an AMR graph and provides more direct feature information within limited context. Hyperedge replacement grammar (HRG) is a context-free rewriting formalism for generating graphs (Drewes </context>
<context position="35297" citStr="Wang et al. (2015)" startWordPosition="6266" endWordPosition="6269">ively small, so there are a lot of meaningful concepts that are not aligned. We use lemma as a backoff strategy to find the alignment for the unaligned words. We have also used the POS tag information to retrieve some unaligned nouns and a PropBank dictionary to retrieve the argument structure of the first sense of the verbs. +All shows the result after using lemma, POS tag and PropBank information, we can see that fixing the alignment can improve the recall, but the precision does not change much. Table 2 shows our result on test data. JAMR is the baseline result from Flanigan et al. (2014). Wang et al. (2015) shows the current state-ofart for string-to-AMR parsing. Without the dependency parse information and complex global features, our SHRG-based approach can already achieve competitive results in comparison with these two algorithms. 6 Discussion In comparison to the spanning tree algorithm of Flanigan et al. (2014), an SHRG-based approach is more sensitive to the alignment. If a lot of the meaningful concepts are not aligned, then the lost information would break down the structure of our grammar. Using more data would definitely help ease this issue. Building overlapping alignments for the tr</context>
</contexts>
<marker>Wang, Xue, Pradhan, 2015</marker>
<rawString>Chuan Wang, Nianwen Xue, and Sameer Pradhan. 2015. A transition-based algorithm for amr parsing. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 366–375, Denver, Colorado, May–June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>