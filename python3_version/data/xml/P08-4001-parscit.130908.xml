<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.252538">
<title confidence="0.994102">
Demonstration of a POMDP Voice Dialer
</title>
<author confidence="0.94387">
Jason Williams
</author>
<affiliation confidence="0.804207">
AT&amp;T Labs – Research, Shannon Laboratory
</affiliation>
<address confidence="0.849114">
180 Park Ave., Florham Park, NJ 07932, USA
</address>
<email confidence="0.998792">
jdw@research.att.com
</email>
<sectionHeader confidence="0.995645" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998859769230769">
This is a demonstration of a voice di-
aler, implemented as a partially observable
Markov decision process (POMDP). A real-
time graphical display shows the POMDP’s
probability distribution over different possi-
ble dialog states, and shows how system out-
put is generated and selected. The system
demonstrated here includes several recent ad-
vances, including an action selection mecha-
nism which unifies a hand-crafted controller
and reinforcement learning. The voice dialer
itself is in use today in AT&amp;T Labs and re-
ceives daily calls.
</bodyText>
<sectionHeader confidence="0.998975" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999175277777778">
Partially observable Markov decision processes
(POMDPs) provide a principled formalism for plan-
ning under uncertainty, and past work has argued
that POMDPs are an attractive framework for build-
ing spoken dialog systems (Williams and Young,
2007a). POMDPs differ from conventional dialog
systems in two respects. First, rather than main-
taining a single hypotheses for the dialog state,
POMDPs maintain a probability distribution called
a belief state over many possible dialog states. A
distribution over a multiple dialog state hypothe-
ses adds inherent robustness, because even if an er-
ror is introduced into one dialog hypothesis, it can
later be discarded in favor of other, uncontaminated
dialog hypotheses. Second, POMDPs choose ac-
tions using an optimization process, in which a de-
veloper specifies high-level goals and the optimiza-
tion works out the detailed dialog plan. Because
</bodyText>
<page confidence="0.801045">
1
</page>
<bodyText confidence="0.999977103448276">
of these innovations, POMDP-based dialog systems
have, in research settings, shown more resilience
to speech recognition errors, yielding shorter di-
alogs with higher task completion rates (Williams
and Young, 2007a; Williams and Young, 2007b).
Because POMDPs differ significantly from con-
ventional techniques, their operation can be difficult
to conceptualize. This demonstration provides an
accessible illustration of the operation of a state-of-
the-art POMDP-based dialog system. The system
itself is a voice dialer, which has been operational
for several months in AT&amp;T Labs. The system in-
corporates several recent advances, including effi-
cient large-scale belief monitoring (akin to Young et
al., 2006), policy compression (Williams and Young,
2007b), and a hybrid hand-crafted/optimized dialog
manager (Williams, 2008). All of these elements
are depicted in a graphical display, which is updated
in real time, as a call is progressing. Whereas pre-
vious demonstrations of POMDP-based dialog sys-
tems have focused on showing the probability distri-
bution over dialog states (Young et al., 2007), this
demonstration adds new detail to convey how ac-
tions are chosen by the dialog manager.
In the remainder of this paper, Section 2 presents
the dialog system and explains how the POMDP ap-
proach has been applied. Then, section 3 explains
the graphical display which illustrates the operation
of the POMDP.
</bodyText>
<sectionHeader confidence="0.897777" genericHeader="method">
2 System description
</sectionHeader>
<bodyText confidence="0.997371011904762">
This application demonstrated here is a voice dialer
application, which is accessible within the AT&amp;T re-
search lab and receives daily calls. The dialer’s vo-
Proceedings of the ACL-08: HLT Demo Session (Companion Volume), pages 1–4,
Columbus, June 2008. c�2008 Association for Computational Linguistics
cabulary consists of 50,000 AT&amp;T employees.
The dialog manager in the dialer is implemented
as a POMDP. In the POMDP approach, a distribu-
tion called a belief state is maintained over many
possible dialog states, and actions are chosen us-
ing reinforcement learning (Williams and Young,
2007a). In this application, a distribution is main-
tained over all of the employees’ phone listings in
the dialer’s vocabulary, such as Jason Williams’ of-
fice phone or Srinivas Bangalore’s cell phone. As
speech recognition results are received, this distri-
bution is updated using probability models of how
users are likely to respond to questions and how the
speech recognition process is likely to corrupt user
speech. The benefit of tracking this belief state is
that it synthesizes all of the ASR N-Best lists over
the whole dialog – i.e., it makes the most possible
use of the information from the speech recognizer.
POMDPs then choose actions based on this be-
lief state using reinforcement learning (Sutton and
Barto, 1998). A developer writes a reward func-
tion which assigns a real number to each state/action
pair, and an optimization algorithm determines how
to choose actions in order to maximize the expected
sum of rewards. In other words, the optimization
performs planning and this allows a developer to
specify the trade-off to use between task comple-
tion and dialog length. In this system, a simple re-
ward function assigns -1 per system action plus +/-
20 for correctly/incorrectly transferring the caller at
the end of the call. Optimization was performed
roughly following (Williams and Young, 2007b), by
running dialogs in simulation.
Despite their theoretical elegance, applying a
POMDP to this spoken dialog system has presented
several interesting research challenges. First, scal-
ing the number of listings quickly prevents the be-
lief state from being updated in real-time, and here
we track a distribution over partitions, which is akin
to a beam search in ASR (Young et al., 2006). At
first, all listings are undifferentiated in a single mas-
ter partition. If a listing appears on the N-Best list,
it is separated into its own partition and tracked sep-
arately. If the number of partitions grows too large,
then low-probability partitions are folded back into
the master undifferentiated partition. This technique
allows a well-formed distribution to be maintained
over an arbitrary number of concepts in real-time.
Second, the optimization process which chooses
actions is also difficult to scale. To tackle this,
the so-called “summary POMDP” has been adopted,
which performs optimization in a compressed space
(Williams and Young, 2007b). Actions are mapped
into clusters called mnemonics, and states are com-
pressed into state feature vectors. During opti-
mization, a set of template state feature vectors are
sampled, and values are computed for each action
mnemonic at each template state feature vector.
Finally, in the classical POMDP approach there is
no straightforward way to impose rules on system
behavior because the optimization algorithm con-
siders taking any action at any point. This makes
it impossible to impose design constraints or busi-
ness rules, and also needlessly re-discovers obvious
domain properties during optimization. In this sys-
tem, a hybrid POMDP/hand-crafted dialog manager
is used (Williams, 2008). The POMDP and con-
ventional dialog manager run in parallel; the con-
ventional dialog manager nominates a set of one or
more allowed actions, and the POMDP chooses the
optimal action from this set. This approach enables
rules to be imposed and allows prompts to easily be
made context-specific.
The POMDP dialer has been compared to a con-
vention version in dialog simulation, and improved
task completion from 92% to 97% while keeping di-
alog length relatively stable. The system has been
deployed in the lab and we are currently collecting
data to assess performance with real callers.
</bodyText>
<sectionHeader confidence="0.998385" genericHeader="method">
3 Demonstration
</sectionHeader>
<bodyText confidence="0.999968071428571">
A browser-based graphical display has been created
which shows the operation of the POMDP dialer
in real time, shown in Figure 1. The page is up-
dated after the user speech has been processed, and
before the next system action has been played to
the user. The left-most column shows the system
prompt which was just played to the user, and the
N-Best list of recognized text strings, each with its
confidence score.
The center column shows the POMDP belief
state. Initially, all of the belief is held by the mas-
ter, undifferentiated partition, which is shown as a
green bar and always shown first. As names are rec-
ognized, they are tracked separately, and the top 10
</bodyText>
<page confidence="0.996218">
2
</page>
<figure confidence="0.99775725">
POMDP belief
state
Previous
system action
N-Best
recognition
with
confidence
scores
Features of the
current dialog
state
Allowed
actions
Values of the
allowed
actions
Resulting
system action,
output to TTS
</figure>
<figureCaption confidence="0.999951">
Figure 1: Overview of the graphical display. Contents are described in the text.
</figureCaption>
<bodyText confidence="0.999957291666667">
names are shown as blue bars, sorted by their belief.
If the system asks for the phone type (office or mo-
bile), then the bars sub-divide into a light blue (for
office) and dark blue (for mobile).
The right column shows how actions are selected.
The top area shows the features of the current state
used to choose actions. Red bars show the two con-
tinuous features: the belief in the most likely name
and most likely type of phone. Below that, three
discrete features are shown: how many phones are
available (none, one, or both); whether the most
likely name has been confirmed (yes or no); and
whether the most likely name is ambiguous (yes
or no). Below this, the allowed actions (i.e., those
which are nominated by the hand-crafted dialog
manager) are shown. Each action is preceded by the
action mnemonic, shown in bold. Below the allowed
actions, the action selection process is shown. The
values of the action mnemonic at the closest tem-
plate point are shown next to each action mnemonic.
Finally the text of this action, which is output to the
caller, is shown at the bottom of the right-hand col-
umn. Figure 2 shows the audio and video transcrip-
tion of an interaction with the demonstration.
</bodyText>
<sectionHeader confidence="0.999535" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999943375">
This demonstration has shown the operation of a
POMDP-based dialog system, which incorporates
recent advances including efficient large-scale belief
monitoring, policy compression, and a unified hand-
crafted/optimized dialog manager. A graphical dis-
play shows the operation of the system in real-time,
as a call progresses, which helps make the POMDP
approach accessible to a non-specialist.
</bodyText>
<sectionHeader confidence="0.997479" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9974135">
Thanks to Iker Arizmendi and Vincent Goffin for
help with the implementation.
</bodyText>
<sectionHeader confidence="0.998523" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9989984">
R Sutton and A Barto. 1998. Reinforcement Learning:
an Introduction. MIT Press.
JD Williams and SJ Young. 2007a. Partially observable
Markov decision processes for spoken dialog systems.
Computer Speech and Language, 21(2):393–422.
JD Williams and SJ Young. 2007b. Scaling POMDPs for
spoken dialog management. IEEE Trans. on Audio,
Speech, and Language Processing, 15(7):2116–2129.
JD Williams. 2008. The best of both worlds: Unifying
conventional dialog systems and POMDPs. In (In sub-
mission).
SJ Young, JD Williams, J Schatzmann, MN Stuttle, and
K Weilhammer. 2006. The hidden information state
approach to dialogue management. Technical Re-
port CUED/F-INFENG/TR.544, Cambridge Univer-
sity Engineering Department.
SJ Young, J Schatzmann, B R M Thomson, KWeilham-
mer, and H Ye. 2007. The hidden information state
dialogue manager: A real-world POMDP-based sys-
tem. In Proc NAACL-HLT, Rochester, New York, USA.
</reference>
<page confidence="0.989063">
3
</page>
<figure confidence="0.885391625">
Transcript of audio Screenshots of graphical display
S1: First and last name?
U1: Junlan Feng
S1: Sorry, first and last name?
U1: Junlan Feng
S1: Junlan Feng.
U1: Yes
S1: Dialing
</figure>
<figureCaption confidence="0.9716358">
Figure 2: The demonstration’s graphical display during a call. The graphical display has been cropped and re-arranged fo
Feng” twice, and although each name recognition alone carries a low confidence score, the beliefstate aggregates this info
the call to progress faster than in the conventional system and illustrates one benefit of the POMDP approach. We have ob
not in a baseline conventional dialer: for example, the POMDP-based system will confirm a callee’s name at different confid
callee has a phone number listed or not; and uses yes/no confirmation questions to disambiguate when there are two ambigu
</figureCaption>
<page confidence="0.988308">
4
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.968231">
<title confidence="0.999867">Demonstration of a POMDP Voice Dialer</title>
<author confidence="0.999916">Jason Williams</author>
<affiliation confidence="0.999151">AT&amp;T Labs – Research, Shannon Laboratory</affiliation>
<address confidence="0.999959">180 Park Ave., Florham Park, NJ 07932, USA</address>
<email confidence="0.999898">jdw@research.att.com</email>
<abstract confidence="0.997791071428571">This is a demonstration of a voice dialer, implemented as a partially observable Markov decision process (POMDP). A realtime graphical display shows the POMDP’s probability distribution over different possible dialog states, and shows how system output is generated and selected. The system demonstrated here includes several recent advances, including an action selection mechanism which unifies a hand-crafted controller and reinforcement learning. The voice dialer itself is in use today in AT&amp;T Labs and receives daily calls.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Sutton</author>
<author>A Barto</author>
</authors>
<title>Reinforcement Learning: an Introduction.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4337" citStr="Sutton and Barto, 1998" startWordPosition="666" endWordPosition="669">e dialer’s vocabulary, such as Jason Williams’ office phone or Srinivas Bangalore’s cell phone. As speech recognition results are received, this distribution is updated using probability models of how users are likely to respond to questions and how the speech recognition process is likely to corrupt user speech. The benefit of tracking this belief state is that it synthesizes all of the ASR N-Best lists over the whole dialog – i.e., it makes the most possible use of the information from the speech recognizer. POMDPs then choose actions based on this belief state using reinforcement learning (Sutton and Barto, 1998). A developer writes a reward function which assigns a real number to each state/action pair, and an optimization algorithm determines how to choose actions in order to maximize the expected sum of rewards. In other words, the optimization performs planning and this allows a developer to specify the trade-off to use between task completion and dialog length. In this system, a simple reward function assigns -1 per system action plus +/- 20 for correctly/incorrectly transferring the caller at the end of the call. Optimization was performed roughly following (Williams and Young, 2007b), by runnin</context>
</contexts>
<marker>Sutton, Barto, 1998</marker>
<rawString>R Sutton and A Barto. 1998. Reinforcement Learning: an Introduction. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>JD Williams</author>
<author>SJ Young</author>
</authors>
<title>Partially observable Markov decision processes for spoken dialog systems.</title>
<date>2007</date>
<journal>Computer Speech and Language,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="956" citStr="Williams and Young, 2007" startWordPosition="141" endWordPosition="144">ability distribution over different possible dialog states, and shows how system output is generated and selected. The system demonstrated here includes several recent advances, including an action selection mechanism which unifies a hand-crafted controller and reinforcement learning. The voice dialer itself is in use today in AT&amp;T Labs and receives daily calls. 1 Introduction Partially observable Markov decision processes (POMDPs) provide a principled formalism for planning under uncertainty, and past work has argued that POMDPs are an attractive framework for building spoken dialog systems (Williams and Young, 2007a). POMDPs differ from conventional dialog systems in two respects. First, rather than maintaining a single hypotheses for the dialog state, POMDPs maintain a probability distribution called a belief state over many possible dialog states. A distribution over a multiple dialog state hypotheses adds inherent robustness, because even if an error is introduced into one dialog hypothesis, it can later be discarded in favor of other, uncontaminated dialog hypotheses. Second, POMDPs choose actions using an optimization process, in which a developer specifies high-level goals and the optimization wor</context>
<context position="2350" citStr="Williams and Young, 2007" startWordPosition="348" endWordPosition="351">, yielding shorter dialogs with higher task completion rates (Williams and Young, 2007a; Williams and Young, 2007b). Because POMDPs differ significantly from conventional techniques, their operation can be difficult to conceptualize. This demonstration provides an accessible illustration of the operation of a state-ofthe-art POMDP-based dialog system. The system itself is a voice dialer, which has been operational for several months in AT&amp;T Labs. The system incorporates several recent advances, including efficient large-scale belief monitoring (akin to Young et al., 2006), policy compression (Williams and Young, 2007b), and a hybrid hand-crafted/optimized dialog manager (Williams, 2008). All of these elements are depicted in a graphical display, which is updated in real time, as a call is progressing. Whereas previous demonstrations of POMDP-based dialog systems have focused on showing the probability distribution over dialog states (Young et al., 2007), this demonstration adds new detail to convey how actions are chosen by the dialog manager. In the remainder of this paper, Section 2 presents the dialog system and explains how the POMDP approach has been applied. Then, section 3 explains the graphical di</context>
<context position="3613" citStr="Williams and Young, 2007" startWordPosition="547" endWordPosition="550">f the POMDP. 2 System description This application demonstrated here is a voice dialer application, which is accessible within the AT&amp;T research lab and receives daily calls. The dialer’s voProceedings of the ACL-08: HLT Demo Session (Companion Volume), pages 1–4, Columbus, June 2008. c�2008 Association for Computational Linguistics cabulary consists of 50,000 AT&amp;T employees. The dialog manager in the dialer is implemented as a POMDP. In the POMDP approach, a distribution called a belief state is maintained over many possible dialog states, and actions are chosen using reinforcement learning (Williams and Young, 2007a). In this application, a distribution is maintained over all of the employees’ phone listings in the dialer’s vocabulary, such as Jason Williams’ office phone or Srinivas Bangalore’s cell phone. As speech recognition results are received, this distribution is updated using probability models of how users are likely to respond to questions and how the speech recognition process is likely to corrupt user speech. The benefit of tracking this belief state is that it synthesizes all of the ASR N-Best lists over the whole dialog – i.e., it makes the most possible use of the information from the sp</context>
<context position="4924" citStr="Williams and Young, 2007" startWordPosition="761" endWordPosition="764">ent learning (Sutton and Barto, 1998). A developer writes a reward function which assigns a real number to each state/action pair, and an optimization algorithm determines how to choose actions in order to maximize the expected sum of rewards. In other words, the optimization performs planning and this allows a developer to specify the trade-off to use between task completion and dialog length. In this system, a simple reward function assigns -1 per system action plus +/- 20 for correctly/incorrectly transferring the caller at the end of the call. Optimization was performed roughly following (Williams and Young, 2007b), by running dialogs in simulation. Despite their theoretical elegance, applying a POMDP to this spoken dialog system has presented several interesting research challenges. First, scaling the number of listings quickly prevents the belief state from being updated in real-time, and here we track a distribution over partitions, which is akin to a beam search in ASR (Young et al., 2006). At first, all listings are undifferentiated in a single master partition. If a listing appears on the N-Best list, it is separated into its own partition and tracked separately. If the number of partitions grow</context>
</contexts>
<marker>Williams, Young, 2007</marker>
<rawString>JD Williams and SJ Young. 2007a. Partially observable Markov decision processes for spoken dialog systems. Computer Speech and Language, 21(2):393–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>JD Williams</author>
<author>SJ Young</author>
</authors>
<title>Scaling POMDPs for spoken dialog management.</title>
<date>2007</date>
<journal>IEEE Trans. on Audio, Speech, and Language Processing,</journal>
<volume>15</volume>
<issue>7</issue>
<contexts>
<context position="956" citStr="Williams and Young, 2007" startWordPosition="141" endWordPosition="144">ability distribution over different possible dialog states, and shows how system output is generated and selected. The system demonstrated here includes several recent advances, including an action selection mechanism which unifies a hand-crafted controller and reinforcement learning. The voice dialer itself is in use today in AT&amp;T Labs and receives daily calls. 1 Introduction Partially observable Markov decision processes (POMDPs) provide a principled formalism for planning under uncertainty, and past work has argued that POMDPs are an attractive framework for building spoken dialog systems (Williams and Young, 2007a). POMDPs differ from conventional dialog systems in two respects. First, rather than maintaining a single hypotheses for the dialog state, POMDPs maintain a probability distribution called a belief state over many possible dialog states. A distribution over a multiple dialog state hypotheses adds inherent robustness, because even if an error is introduced into one dialog hypothesis, it can later be discarded in favor of other, uncontaminated dialog hypotheses. Second, POMDPs choose actions using an optimization process, in which a developer specifies high-level goals and the optimization wor</context>
<context position="2350" citStr="Williams and Young, 2007" startWordPosition="348" endWordPosition="351">, yielding shorter dialogs with higher task completion rates (Williams and Young, 2007a; Williams and Young, 2007b). Because POMDPs differ significantly from conventional techniques, their operation can be difficult to conceptualize. This demonstration provides an accessible illustration of the operation of a state-ofthe-art POMDP-based dialog system. The system itself is a voice dialer, which has been operational for several months in AT&amp;T Labs. The system incorporates several recent advances, including efficient large-scale belief monitoring (akin to Young et al., 2006), policy compression (Williams and Young, 2007b), and a hybrid hand-crafted/optimized dialog manager (Williams, 2008). All of these elements are depicted in a graphical display, which is updated in real time, as a call is progressing. Whereas previous demonstrations of POMDP-based dialog systems have focused on showing the probability distribution over dialog states (Young et al., 2007), this demonstration adds new detail to convey how actions are chosen by the dialog manager. In the remainder of this paper, Section 2 presents the dialog system and explains how the POMDP approach has been applied. Then, section 3 explains the graphical di</context>
<context position="3613" citStr="Williams and Young, 2007" startWordPosition="547" endWordPosition="550">f the POMDP. 2 System description This application demonstrated here is a voice dialer application, which is accessible within the AT&amp;T research lab and receives daily calls. The dialer’s voProceedings of the ACL-08: HLT Demo Session (Companion Volume), pages 1–4, Columbus, June 2008. c�2008 Association for Computational Linguistics cabulary consists of 50,000 AT&amp;T employees. The dialog manager in the dialer is implemented as a POMDP. In the POMDP approach, a distribution called a belief state is maintained over many possible dialog states, and actions are chosen using reinforcement learning (Williams and Young, 2007a). In this application, a distribution is maintained over all of the employees’ phone listings in the dialer’s vocabulary, such as Jason Williams’ office phone or Srinivas Bangalore’s cell phone. As speech recognition results are received, this distribution is updated using probability models of how users are likely to respond to questions and how the speech recognition process is likely to corrupt user speech. The benefit of tracking this belief state is that it synthesizes all of the ASR N-Best lists over the whole dialog – i.e., it makes the most possible use of the information from the sp</context>
<context position="4924" citStr="Williams and Young, 2007" startWordPosition="761" endWordPosition="764">ent learning (Sutton and Barto, 1998). A developer writes a reward function which assigns a real number to each state/action pair, and an optimization algorithm determines how to choose actions in order to maximize the expected sum of rewards. In other words, the optimization performs planning and this allows a developer to specify the trade-off to use between task completion and dialog length. In this system, a simple reward function assigns -1 per system action plus +/- 20 for correctly/incorrectly transferring the caller at the end of the call. Optimization was performed roughly following (Williams and Young, 2007b), by running dialogs in simulation. Despite their theoretical elegance, applying a POMDP to this spoken dialog system has presented several interesting research challenges. First, scaling the number of listings quickly prevents the belief state from being updated in real-time, and here we track a distribution over partitions, which is akin to a beam search in ASR (Young et al., 2006). At first, all listings are undifferentiated in a single master partition. If a listing appears on the N-Best list, it is separated into its own partition and tracked separately. If the number of partitions grow</context>
</contexts>
<marker>Williams, Young, 2007</marker>
<rawString>JD Williams and SJ Young. 2007b. Scaling POMDPs for spoken dialog management. IEEE Trans. on Audio, Speech, and Language Processing, 15(7):2116–2129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>JD Williams</author>
</authors>
<title>The best of both worlds: Unifying conventional dialog systems and POMDPs.</title>
<date>2008</date>
<booktitle>In (In submission).</booktitle>
<contexts>
<context position="2421" citStr="Williams, 2008" startWordPosition="358" endWordPosition="359">, 2007a; Williams and Young, 2007b). Because POMDPs differ significantly from conventional techniques, their operation can be difficult to conceptualize. This demonstration provides an accessible illustration of the operation of a state-ofthe-art POMDP-based dialog system. The system itself is a voice dialer, which has been operational for several months in AT&amp;T Labs. The system incorporates several recent advances, including efficient large-scale belief monitoring (akin to Young et al., 2006), policy compression (Williams and Young, 2007b), and a hybrid hand-crafted/optimized dialog manager (Williams, 2008). All of these elements are depicted in a graphical display, which is updated in real time, as a call is progressing. Whereas previous demonstrations of POMDP-based dialog systems have focused on showing the probability distribution over dialog states (Young et al., 2007), this demonstration adds new detail to convey how actions are chosen by the dialog manager. In the remainder of this paper, Section 2 presents the dialog system and explains how the POMDP approach has been applied. Then, section 3 explains the graphical display which illustrates the operation of the POMDP. 2 System descriptio</context>
<context position="6660" citStr="Williams, 2008" startWordPosition="1030" endWordPosition="1031">sed into state feature vectors. During optimization, a set of template state feature vectors are sampled, and values are computed for each action mnemonic at each template state feature vector. Finally, in the classical POMDP approach there is no straightforward way to impose rules on system behavior because the optimization algorithm considers taking any action at any point. This makes it impossible to impose design constraints or business rules, and also needlessly re-discovers obvious domain properties during optimization. In this system, a hybrid POMDP/hand-crafted dialog manager is used (Williams, 2008). The POMDP and conventional dialog manager run in parallel; the conventional dialog manager nominates a set of one or more allowed actions, and the POMDP chooses the optimal action from this set. This approach enables rules to be imposed and allows prompts to easily be made context-specific. The POMDP dialer has been compared to a convention version in dialog simulation, and improved task completion from 92% to 97% while keeping dialog length relatively stable. The system has been deployed in the lab and we are currently collecting data to assess performance with real callers. 3 Demonstration</context>
</contexts>
<marker>Williams, 2008</marker>
<rawString>JD Williams. 2008. The best of both worlds: Unifying conventional dialog systems and POMDPs. In (In submission).</rawString>
</citation>
<citation valid="true">
<authors>
<author>SJ Young</author>
<author>JD Williams</author>
<author>J Schatzmann</author>
<author>MN Stuttle</author>
<author>K Weilhammer</author>
</authors>
<title>The hidden information state approach to dialogue management.</title>
<date>2006</date>
<tech>Technical Report CUED/F-INFENG/TR.544,</tech>
<institution>Cambridge University Engineering Department.</institution>
<contexts>
<context position="2304" citStr="Young et al., 2006" startWordPosition="342" endWordPosition="345">e resilience to speech recognition errors, yielding shorter dialogs with higher task completion rates (Williams and Young, 2007a; Williams and Young, 2007b). Because POMDPs differ significantly from conventional techniques, their operation can be difficult to conceptualize. This demonstration provides an accessible illustration of the operation of a state-ofthe-art POMDP-based dialog system. The system itself is a voice dialer, which has been operational for several months in AT&amp;T Labs. The system incorporates several recent advances, including efficient large-scale belief monitoring (akin to Young et al., 2006), policy compression (Williams and Young, 2007b), and a hybrid hand-crafted/optimized dialog manager (Williams, 2008). All of these elements are depicted in a graphical display, which is updated in real time, as a call is progressing. Whereas previous demonstrations of POMDP-based dialog systems have focused on showing the probability distribution over dialog states (Young et al., 2007), this demonstration adds new detail to convey how actions are chosen by the dialog manager. In the remainder of this paper, Section 2 presents the dialog system and explains how the POMDP approach has been appl</context>
<context position="5312" citStr="Young et al., 2006" startWordPosition="823" endWordPosition="826">gth. In this system, a simple reward function assigns -1 per system action plus +/- 20 for correctly/incorrectly transferring the caller at the end of the call. Optimization was performed roughly following (Williams and Young, 2007b), by running dialogs in simulation. Despite their theoretical elegance, applying a POMDP to this spoken dialog system has presented several interesting research challenges. First, scaling the number of listings quickly prevents the belief state from being updated in real-time, and here we track a distribution over partitions, which is akin to a beam search in ASR (Young et al., 2006). At first, all listings are undifferentiated in a single master partition. If a listing appears on the N-Best list, it is separated into its own partition and tracked separately. If the number of partitions grows too large, then low-probability partitions are folded back into the master undifferentiated partition. This technique allows a well-formed distribution to be maintained over an arbitrary number of concepts in real-time. Second, the optimization process which chooses actions is also difficult to scale. To tackle this, the so-called “summary POMDP” has been adopted, which performs opti</context>
</contexts>
<marker>Young, Williams, Schatzmann, Stuttle, Weilhammer, 2006</marker>
<rawString>SJ Young, JD Williams, J Schatzmann, MN Stuttle, and K Weilhammer. 2006. The hidden information state approach to dialogue management. Technical Report CUED/F-INFENG/TR.544, Cambridge University Engineering Department.</rawString>
</citation>
<citation valid="true">
<authors>
<author>SJ Young</author>
<author>J Schatzmann</author>
<author>B R M Thomson</author>
<author>KWeilhammer</author>
<author>H Ye</author>
</authors>
<title>The hidden information state dialogue manager: A real-world POMDP-based system.</title>
<date>2007</date>
<booktitle>In Proc NAACL-HLT,</booktitle>
<location>Rochester, New York, USA.</location>
<contexts>
<context position="2693" citStr="Young et al., 2007" startWordPosition="401" endWordPosition="404">stem. The system itself is a voice dialer, which has been operational for several months in AT&amp;T Labs. The system incorporates several recent advances, including efficient large-scale belief monitoring (akin to Young et al., 2006), policy compression (Williams and Young, 2007b), and a hybrid hand-crafted/optimized dialog manager (Williams, 2008). All of these elements are depicted in a graphical display, which is updated in real time, as a call is progressing. Whereas previous demonstrations of POMDP-based dialog systems have focused on showing the probability distribution over dialog states (Young et al., 2007), this demonstration adds new detail to convey how actions are chosen by the dialog manager. In the remainder of this paper, Section 2 presents the dialog system and explains how the POMDP approach has been applied. Then, section 3 explains the graphical display which illustrates the operation of the POMDP. 2 System description This application demonstrated here is a voice dialer application, which is accessible within the AT&amp;T research lab and receives daily calls. The dialer’s voProceedings of the ACL-08: HLT Demo Session (Companion Volume), pages 1–4, Columbus, June 2008. c�2008 Association</context>
</contexts>
<marker>Young, Schatzmann, Thomson, KWeilhammer, Ye, 2007</marker>
<rawString>SJ Young, J Schatzmann, B R M Thomson, KWeilhammer, and H Ye. 2007. The hidden information state dialogue manager: A real-world POMDP-based system. In Proc NAACL-HLT, Rochester, New York, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>