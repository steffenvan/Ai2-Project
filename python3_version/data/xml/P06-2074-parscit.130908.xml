<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000019">
<title confidence="0.994198">
ARE: Instance Splitting Strategies for Dependency Relation-based
Information Extraction
</title>
<author confidence="0.993325">
Mstislav Maslennikov Hai-Kiat Goh Tat-Seng Chua
</author>
<affiliation confidence="0.999724">
Department of Computer Science
School of Computing
National University of Singapore
</affiliation>
<email confidence="0.37174">
{maslenni, gohhaiki, chuats}@ comp.nus.edu.sg
</email>
<sectionHeader confidence="0.982414" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999856173913044">
Information Extraction (IE) is a fundamen-
tal technology for NLP. Previous methods
for IE were relying on co-occurrence rela-
tions, soft patterns and properties of the
target (for example, syntactic role), which
result in problems of handling paraphrasing
and alignment of instances. Our system
ARE (Anchor and Relation) is based on the
dependency relation model and tackles
these problems by unifying entities accord-
ing to their dependency relations, which we
found to provide more invariant relations
between entities in many cases. In order to
exploit the complexity and characteristics
of relation paths, we further classify the re-
lation paths into the categories of ‘easy’,
‘average’ and ‘hard’, and utilize different
extraction strategies based on the character-
istics of those categories. Our extraction
method leads to improvement in perform-
ance by 3% and 6% for MUC4 and MUC6
respectively as compared to the state-of-art
IE systems.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999716984126985">
Information Extraction (IE) is one of the funda-
mental problems of natural language processing.
Progress in IE is important to enhance results in
such tasks as Question Answering, Information
Retrieval and Text Summarization. Multiple efforts
in MUC series allowed IE systems to achieve near-
human performance in such domains as biological
(Humphreys et al., 2000), terrorism (Kaufmann,
1992; Kaufmann, 1993) and management succes-
sion (Kaufmann, 1995).
The IE task is formulated for MUC series as
filling of several predefined slots in a template. The
terrorism template consists of slots Perpetrator,
Victim and Target; the slots in the management
succession template are Org, PersonIn, PersonOut
and Post. We decided to choose both terrorism and
management succession domains, from MUC4 and
MUC6 respectively, in order to demonstrate that
our idea is applicable to multiple domains.
Paraphrasing of instances is one of the crucial
problems in IE. This problem leads to data sparse-
ness in situations when information is expressed in
different ways. As an example, consider the ex-
cerpts “Terrorists attacked victims” and “Victims
were attacked by unidentified terrorists”. These
instances have very similar semantic meaning.
However, context-based approaches such as
Autoslog-TS by Riloff (1996) and Yangarber et al.
(2002) may face difficulties in handling these in-
stances effectively because the context of entity
‘victims’ is located on the left context in the first
instance and on the right context in the second. For
these cases, we found that we are able to verify the
context by performing dependency relation parsing
(Lin, 1997), which outputs the word ‘victims’ as an
object in both instances, with ‘attacked’ as a verb
and ‘terrorists’ as a subject. After grouping of same
syntactic roles in the above examples, we are able
to unify these instances.
Another problem in IE systems is word align-
ment. Insertion or deletion of tokens prevents in-
stances from being generalized effectively during
learning. Therefore, the instances “Victims were
attacked by terrorists” and “Victims were recently
attacked by terrorists” are difficult to unify. The
common approach adopted in GRID by Xiao et al.
(2003) is to apply more stable chunks such as noun
phrases and verb phrases. Another recent approach
by Cui et al. (2005) utilizes soft patterns for prob-
abilistic matching of tokens. However, a longer
insertion leads to a more complicated structure, as
in the instance “Victims, living near the shop, went
out for a walk and were attacked by terrorists”.
Since there may be many inserted words, both ap-
proaches may also be inefficient for this case. Simi-
lar to the paraphrasing problem, the word align-
ment problem may be handled with dependency
relations in many cases. We found that the relation
subject-verb-object for words ‘victims’, ‘attacked’
and ‘terrorists’ remains invariant for the above two
instances.
Before IE can be performed, we need to iden-
tify sentences containing possible slots. This is
</bodyText>
<page confidence="0.960232">
571
</page>
<note confidence="0.7262215">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 571–578,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999981909090909">
done through the identification of cue phrases
which we call anchors or anchor cues. However,
natural texts tend to have diverse terminologies,
which require semantic features for generalization.
These features include semantic classes, Named
Entities (NE) and support from ontology (for ex-
ample, synsets in Wordnet). If such features are
predefined, then changes in terminology (for in-
stance, addition of new terrorism organization) will
lead to a loss in recall. To avoid this, we exploit
automatic mining techniques for anchor cues. Ex-
amples of anchors are the words “terrorists” or
“guerrilla” that signify a possible candidate for the
Perpetrator slot.
From the reviewed works, we observe that the
inefficient use of relations causes problems of
paraphrasing and alignment and the related data
sparseness problem in current IE systems. As a re-
sult, training and testing instances in the systems
often lack generality. This paper aims to tackle
these problems with the help of dependency rela-
tion-based model for IE. Although dependency re-
lations provide invariant structures for many in-
stances as illustrated above, they tend to be effi-
cient only for short sentences and make errors on
long distance relations. To tackle this problem, we
classify relations into ‘simple’, ‘average’ and
‘hard’ categories, depending on the complexity of
the dependency relation paths. We then employ
different strategies to perform IE in each category.
The main contributions of our work are as fol-
lows. First, we propose a dependency relation
based model for IE. Second, we perform classifica-
tion of instances into several categories based on
the complexity of dependency relation structures,
and employ the action promotion strategy to tackle
the problem of long distance relations.
The remaining parts of the paper are organized
as follows. Section 2 discusses related work and
Section 3 introduces our approach for constructing
ARE. Section 4 introduces our method for splitting
instances into categories. Section 5 describes our
experimental setups and results and, finally, Sec-
tion 6 concludes the paper.
</bodyText>
<sectionHeader confidence="0.999621" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999951059701493">
There are several research directions in Information
Extraction. We highlight a few directions in IE
such as case frame based modeling in PALKA by
Kim and Moldovan (1995) and CRYSTAL by So-
derland et al. (1995); rule-based learning in
Autoslog-TS by Riloff et al. (1996); and classifica-
tion-based learning by Chieu et al. (2002). Al-
though systems representing these directions have
very different learning models, paraphrasing and
alignment problems still have no reliable solution.
Case frame based IE systems incorporate do-
main-dependent knowledge in the processing and
learning of semantic constraints. However, concept
hierarchy used in case frames is typically encoded
manually and requires additional human labor for
porting across domains. Moreover, the systems
tend to rely on heuristics in order to match case
frames. PALKA by Kim and Moldovan (1995) per-
forms keyword-based matching of concepts, while
CRYSTAL by Soderland et al. (1995) relied on
additional domain-specific annotation and associ-
ated lexicon for matching.
Rule-based IE models allow differentiation of
rules according to their performance. Autoslog-TS
by Riloff (1996) learns the context rules for extrac-
tion and ranks them according to their performance
on the training corpus. Although this approach is
suitable for automatic training, Xiao et al. (2004)
stated that hard matching techniques tend to have
low recall due to data sparseness problem. To over-
come this problem, (LP)2 by Ciravegna (2002) util-
izes rules with high precision in order to improve
the precision of rules with average recall. However,
(LP)2 is developed for semi-structured textual do-
main, where we can find consistent lexical patterns
at surface text level. This is not the same for free-
text, in which different order of words or an extra
clause in a sentence may cause paraphrasing and
alignment problems respectively, such as the ex-
ample excerpts “terrorists attacked peasants” and
“peasants were attacked 2 months ago by terrorists”.
The classification-based approaches such as by
Chieu and Ng (2002) tend to outperform rule-based
approaches. However, Ciravegna (2001) argued
that it is difficult to examine the result obtained by
classifiers. Thus, interpretability of the learned
knowledge is a serious bottleneck of the classifica-
tion approach. Additionally, Zhou and Su (2002)
trained classifiers for Named Entity extraction and
reported that performance degrades rapidly if the
training corpus size is below 100KB. It implies that
human experts have to spend long hours to annotate
a sufficiently large amount of training corpus.
Several recent researches focused on the ex-
traction of relationships using classifiers. Roth and
Yih (2002) learned the entities and relations to-
gether. The joint learning improves the perform-
ance of NE recognition in cases such as “X killed
Y”. It also prevents the propagation of mistakes in
NE extraction to the extraction of relations. How-
ever, long distance relations between entities are
likely to cause mistakes in relation extraction. A
possible approach for modeling relations of differ-
ent complexity is the use of dependency-based ker-
nel trees in support vector machines by Culotta and
Sorensen (2004). The authors reported that non-
relation instances are very heterogeneous, and
</bodyText>
<page confidence="0.995247">
572
</page>
<bodyText confidence="0.999298">
hence they suggested the additional step of extract-
ing candidate relations before classification.
After preprocessing and feature extraction, we ob-
tain the linguistic features in Table 1.
</bodyText>
<sectionHeader confidence="0.942365" genericHeader="method">
3 Our approach
</sectionHeader>
<bodyText confidence="0.999973269230769">
Differing from previous systems, the language
model in ARE is based on dependency relations
obtained from Minipar by Lin (1997). In the first
stage, ARE tries to identify possible candidates for
filling slots in a sentence. For example, words such
as ‘terrorist’ or ‘guerrilla’ can fill the slot for Per-
petrator in the terrorism domain. We refer to these
candidates as anchors or anchor cues. In the sec-
ond stage, ARE defines the dependency relations
that connect anchor cues. We exploit dependency
relations to provide more invariant structures for
similar sentences with different syntactic structures.
After extracting the possible relations between an-
chor cues, we form several possible parsing paths
and rank them. Based on the ranking, we choose
the optimal filling of slots.
Ranking strategy may be unnecessary in cases
when entities are represented in the SVO form.
Ranking strategy may also fail in situations of long
distance relations. To handle such problems, we
categorize the sentences into 3 categories of: sim-
ple, average and hard, depending on the complexity
of the dependency relations. We then apply differ-
ent strategies to tackle sentences in each category
effectively. The following subsections discuss de-
tails of our approach.
</bodyText>
<table confidence="0.993602266666667">
Features Perpetrator_Cue Action_Cue Victim_Cue Target_Cue
(A) (D) (A) (A)
Lexical terrorists, attacked, mayor, bridge,
(Head individuals, murder, general, house,
noun) soldiers massacre priests ministry
Part-of- Noun Verb Noun Noun
Speech
Named Soldiers - Jesuit priests WTC
Entities (PERSON) (PERSON) (OBJECT)
Synonyms Synset 130, 166 Synset 22 Synset 68 Synset 71
Concept ID 2, 3 ID 9 ID 22, 43 ID 61, 48
Class
Co- He -&gt; terrorist, - They -&gt; -
referenced soldier peasants
entity
</table>
<tableCaption confidence="0.999403">
Table 1. Linguistic features for anchor extraction
</tableCaption>
<bodyText confidence="0.9995066">
Every token in ARE may be represented at a
different level of representations, including: Lexi-
cal, Part-of-Speech, Named Entities, Synonyms and
Concept classes. The synonym set and concept
classes are mainly obtained from Wordnet. We use
NLProcessor from Infogistics Ltd for the extraction
of part-of-speech, noun phrases and verb phrases
(we refer to them as phrases). Named Entities are
extracted with the program used in Yang et al.
(2003). Additionally, we employed the co-
reference module for the extraction of meaningful
pronouns. It is used for linking entities across
clauses or sentences, for example in “John works in
XYZ Corp. He was appointed as a vice-president a
month ago” and could achieve an accuracy of 62%.
</bodyText>
<subsectionHeader confidence="0.999978">
3.1 Mining of anchor cues
</subsectionHeader>
<bodyText confidence="0.999924272727272">
In order to extract possible anchors and relations
from every sentence, we need to select features to
support the generalization of words. This generali-
zation may be different for different classes of
words. For example, person names may be general-
ized as a Named Entity PERSON, whereas for
‘murder’ and ‘assassinate’, the optimal generaliza-
tion would be the concept class ‘kill’ in the Word-
Net hypernym tree. To support several generaliza-
tions, we need to store multiple representations of
every word or token.
Mining of anchor cues or anchors is crucial in
order to unify meaningful entities in a sentence, for
example words ‘terrorists’, ‘individuals’ and ‘sol-
diers’ from Table 1. In the terrorism domain, we
consider 4 types of anchor cues: Perpetrator, Action,
Victim, and Target of destruction. For management
succession domain, we have 6 types: Post, Person
In, Person Out, Action and Organization. Each set
of anchor cues may be seen as a pre-defined se-
mantic type where the tokens are mined automati-
cally. The anchor cues are further classified into
two categories: general type A and action type D.
Action type anchor cues are those with verbs or
verb phrases describing a particular action or
movement. General type encompasses any prede-
fined type that does not fall under the action type
cues.
In the first stage, we need to extract anchor
cues for every type. Let P be an input phrase, and
Aj be the anchor of type j that we want to match.
The similarity score of P for Aj in sentence S is
given by:
</bodyText>
<equation confidence="0.999346666666667">
Phrase_Scores(P,Aj)=δ1* S_lexicalS(P,Aj+δ2* S_POSS(P,Aj)
+δ3* S_NES(P,Aj) +δ4 * S_SynS(P,Aj)
+δ5* S_Concept-ClassS(P,Aj) (1)
</equation>
<bodyText confidence="0.999860666666667">
where S_XXXS(P,Aj) is a score function for the type
Aj and δi is the importance weight for Aj. In order to
extract the score function, we use entities from
slots in the training instances. Each S_XXXS(P,Aj) is
calculated as a ratio of occurrence in positive slots
versus all the slots:
</bodyText>
<equation confidence="0.694016">
S_XXXS(P,Aj) = # (all slots of the type Aj )
</equation>
<bodyText confidence="0.99253025">
We classify the phrase P as belonging to an anchor
cue A of type j if Phrase_ScoreS(P,Aj) ≥ ω, where
ω is an empirically determined threshold. The
weights 5 = (51,..., 55 ) are learned automatically
using Expectation Maximization by Dempster et al.
(1977). Using anchors from training instances as
ground truth, we iteratively input different sets of
weights into EM to maximize the overall score.
</bodyText>
<figure confidence="0.6981985">
#(P in positive slots of the type Aj )
(2)
</figure>
<page confidence="0.985056">
573
</page>
<bodyText confidence="0.999889571428571">
Consider the excerpts “Terrorists attacked
victims”, “Peasants were murdered by unidentified
individuals” and “Soldiers participated in massacre
of Jesuit priests”. Let Wi denotes the position of
token i in the instances. After mining of anchors,
we are able to extract meaningful anchor cues in
these sentences as shown in Table 2:
</bodyText>
<table confidence="0.88975125">
W-3 W-2 W-1 W0 W1 W2 W3
Perp_Cue Action_Cue Victim_Cue
Victim_Cue were Action_Cue by
In Action_Cue Of Victim_Cue
</table>
<tableCaption confidence="0.99988">
Table 2. Instances with anchor cues
</tableCaption>
<subsectionHeader confidence="0.99955">
3.2 Relationship extraction and ranking
</subsectionHeader>
<bodyText confidence="0.99966515">
In the next stage, we need to
find meaningful relations to
unify instances using the anchor
cues. This unification is done
using dependency trees of sen-
tences. The dependency
relations for the first sentence
are given in Figure 1.
From the dependency tree, we need to identify
the SVO relations between anchor cues. In cases
when there are multiple relations linking many po-
tential subjects, verbs or objects, we need to select
the best relations under the circumstances. Our
scheme for relation ranking is as follows.
First, we rank each single relation individually
based on the probability that it appears in the re-
spective context template slot in the training data.
We use the following formula to capture the quality
of a relation Rel which gives higher weight to more
frequently occurring relations:
</bodyText>
<equation confidence="0.955932">
(Rel ,A1,A2) = ∑S ||{Ri|Ri ∈ R,Ri = Rel
∑S||{Ri|Ri∈ Si} ||
</equation>
<bodyText confidence="0.988425">
where S is a set of sentences containing relation
Rel, anchors A1 and A2; R denotes relation path con-
necting A1 and A2 in a sentence Si; ||X ||denotes size
of the set X.
Second, we need to take into account the entity
height in the dependency tree. We calculate height
as a distance to the root node. Our intuition is that
the nodes on the higher level of dependency tree
are more important, because they may be linked to
more nodes or entities. The following example in
Figure 2 illustrates it.
Here, the node ‘terrorists’ is the most representative
in the whole tree, and thus relations nearer to ‘ter-
rorists’ should have higher weight. Therefore, we
give a slightly higher weight to the links that are
closer to the root node as follows:
</bodyText>
<equation confidence="0.999263">
Heights(Rel) = log2(Const – Distance(Root, Rel)) (4)
</equation>
<bodyText confidence="0.999970857142857">
where Const is set to be larger than the depth of
nodes in the tree.
Third, we need to calculate the score of rela-
tion path Ri-&gt;j between each pair of anchors Ai and
Aj, where Ai and Aj belong to different anchor cue
types. The path score of Ri-&gt;j depends on both qual-
ity and height of participating relations:
</bodyText>
<subsubsectionHeader confidence="0.481387">
Scores(Ai, Aj)=ΣRi∈R {Heights(Ri)*Quality(Ri)}/Lengthij (5)
</subsubsectionHeader>
<bodyText confidence="0.999950416666667">
where Lengthij is the length of path Ri-&gt;j. Division
on Lengthij allows normalizing Score against the
length of Ri-&gt;j. The formula (5) tends to give higher
scores to shorter paths. Therefore, the path ending
with ‘terrorist’ will be preferred in the previous
example to the equivalent path ending with
‘MRTA’.
Finally, we need to find optimal filling of a
template T. Let C = {C1, .. , CK} be the set of slot
types in T and A = {A1, .., AL} be the set of ex-
tracted anchors. First, we regroup anchors A ac-
cording to their respective types. Let
</bodyText>
<equation confidence="0.998129571428572">
( ) { ( ) ,..., ( ) }
k A be the projection of A onto
k
A = A
k
1 L
k
</equation>
<bodyText confidence="0.999435">
the type Ck, ∀k∈N, k ≤ K. Let F = A(1) × A(2) ×..×
A(K) be the set of possible template fillings. The
elements of F are denoted as F1, ..,FM, where every
Fi ∈ F is represented as Fi = {Ai(1),..,Ai(K)}. Our aim
is to evaluate F and find the optimal filling F0 ∈ F.
For this purpose, we use the previously calculated
scores of relation paths between every two anchors
Ai and Aj.
Based on the previously defined ScoreS(Ai, Aj),
it is possible to rank all the fillings in F. For each
filling Fi∈F we calculate the aggregate score for all
the involved anchor pairs:
</bodyText>
<equation confidence="0.998466">
∑ ≤ ≤ S core A A
S i j
( , )
1 ,
i j K
M
</equation>
<bodyText confidence="0.999805857142857">
where K is number of slot types and M denotes the
number of relation paths between anchors in Fi.
After calculating Relation_ScoreS(Fi), it is used
for ranking all possible template fillings. The next
step is to join entity and relation scores. We defined
the entity score of Fi as an average of the scores of
participating anchors:
</bodyText>
<equation confidence="0.985549875">
_ ( ) 1 Phrase Score A K
_ ( ) / (8
k
( )
S i
Entity Score F
S i = k K
∑ ≤ ≤
</equation>
<bodyText confidence="0.98868">
We combine entity and relation scores of Fi into the
overall formula for ranking.
</bodyText>
<equation confidence="0.596378">
RankS(Fi)=λ*Entity_ScoreS(Fi)+(1-λ)*Relation_ScoreS(Fi ) (9)
</equation>
<bodyText confidence="0.7803335">
The application of Subject-Verb-Object (SVO)
relations facilitates the grouping of subjects,
</bodyText>
<figureCaption confidence="0.887484">
Figure 1.
Dependency tree
</figureCaption>
<figure confidence="0.9934755">
Quality
}  ||( 3)
</figure>
<figureCaption confidence="0.999952">
Figure 2. Example of entity in a dependency tree
</figureCaption>
<equation confidence="0.890393333333333">
Relation _ScoreS (Fi) =
(7)
)
</equation>
<page confidence="0.998083">
574
</page>
<tableCaption confidence="0.892190666666667">
verbs and objects together. For the 3 instances in
Table 2 containing the anchor cues, the unified
SVO relations are given in Table 3.
</tableCaption>
<figure confidence="0.511934">
W-2 W-1 W0 Instance is
Perp_Cue attacked Victim_Cue +
Perp_Cue murdered Victim_Cue +
Perp_Cue participated ? -
</figure>
<tableCaption confidence="0.999696">
Table 3. Unification based on SVO relations
</tableCaption>
<bodyText confidence="0.99490875">
The first 2 instances are unified correctly. The
only exception is the slot in the third case, which
is missing because the target is not an object of
‘participated’.
</bodyText>
<sectionHeader confidence="0.979284" genericHeader="method">
4 Category Splitting
</sectionHeader>
<bodyText confidence="0.994986583333333">
Through our experiments, we found that the com-
bination of relations and anchors are essential for
improving IE performance. However, relations
alone are not applicable across all situations be-
cause of long distance relations and possible de-
pendency relation parsing errors, especially for
long sentences. Since the relations in long sen-
tences are often complicated, parsing errors are
very difficult to avoid. Furthermore, application of
dependency relations on long sentences may lead to
incorrect extractions and decrease the performance.
Through the analysis of instances, we noticed
that dependency trees have different complexity for
different sentences. Therefore, we decided to clas-
sify sentences into 3 categories based on the com-
plexity of dependency relations between the action
cues (V) and the likely subject (S) and object cues
(O). Category 1 is when the potential SVO’s are
connected directly to each other (simple category);
Category 2 is when S or O is one link away from V
in terms of nouns or verbs (average category); and
Category 3 is when the path distances between po-
tential S, V, and Os are more than 2 links away
(hard category).
</bodyText>
<figureCaption confidence="0.963576">
Figure 3. Simple category Figure 4. Average category
</figureCaption>
<bodyText confidence="0.998340117647059">
Figure 3 and Figure 4 illustrate the dependency
parse trees for the simple and average categories
respectively derived from the sentences: “50 peas-
ants of have been kidnapped by terrorists” and “a
colonel was involved in the massacre of the Jesu-
its”. These trees represent 2 common structures in
the MUC4 domain. By taking advantage of this
commonality, we can further improve the perform-
ance of extraction. We notice that in the simple
category, the perpetrator cue (‘terrorists’) is always
a subject, action cue (‘kidnapped’) a verb, and vic-
tim cue (‘peasants’) an object. For the average
category, perpetrator and victim commonly appear
under 3 relations: subject, object and pcomp-n. The
most difficult category is the hard category, since
in this category relations can be distant. We thus
primarily rely on anchors for extraction and have to
give less importance to dependency parsing.
In order to process the different categories, we
utilize the specific strategies for each category. As
an example, the instance “X murdered Y” requires
only the analysis of the context verb ‘murdered’ in
the simple category. It is different from the in-
stances “X investigated murder of Y” and “X con-
ducted murder of Y” in the average category, in
which transition of word ‘investigated’ into ‘con-
ducted’ makes X a perpetrator. We refer to the an-
chor ‘murder’ in the first and second instances as
promotable and non-promotable respectively. Ad-
ditionally, we denote that the token ‘conducted’ is
the optimal node for promotion of ‘murder’,
whereas the anchor ‘investigate’ is not. This exam-
ple illustrates the importance of support verb analy-
sis specifically for the average category.
</bodyText>
<figure confidence="0.779684357142857">
Algorithm
1) Analyze category
If(simple)
- Perform token reordering based on SVO relations
Else if (average) ProcessAverage
Else ProcessHard
2) Fill template slots
Function ProcessAverage
1) Find the nearest missing anchor in the previous sentences
2) Find the optimal linking node for action anchor in every Fi
3) Find the filling Fi(0) = argmaxi Rank(Fi)
4) Use Fi for filling the template if Rank0 &gt; 92, where 92 is an
empirical threshold
Function ProcessHard
</figure>
<listItem confidence="0.950182142857143">
1) Perform token reordering based on anchors
2) Use linguistic+ syntactic + semantic feature of the head
noun. Eg. Caps, ‘subj’, etc
3) Find the optimal linking node for action anchor in every Fi
4) Find the filling Fi(0) = argmaxi Rank(Fi)
5) Use Fi for filling the template if Rank0 &gt; 93, where 93 is an
empirical threshold
</listItem>
<figureCaption confidence="0.994053">
Figure 5. Category processing
</figureCaption>
<bodyText confidence="0.99993775">
The main steps of our algorithm for performing IE
in different categories are given in Figure 5. Al-
though some steps are common for every category,
the processing strategies are different.
</bodyText>
<subsectionHeader confidence="0.99193">
Simple category
</subsectionHeader>
<bodyText confidence="0.997269">
For simple category, we reorder tokens according
to their slot types. Based on this reordering, we fill
the template.
</bodyText>
<page confidence="0.995113">
575
</page>
<subsectionHeader confidence="0.881534">
Average category
</subsectionHeader>
<bodyText confidence="0.999992375">
For average category, our strategy consists of 4
steps. First, in the case of missing anchor type we
try to find it in the nearest previous sentence. Con-
sider an example from MUC-6: “Look at what hap-
pened to John Sculley, Apple Computer&apos;s former
chairman. Earlier this month he abruptly resigned
as chairman of troubled Spectrum Information
Technologies.” In this example, a noisy cue ‘he’
needs to be substituted with “John Sculley”, which
is a strong anchor cue. Second, we need to find an
optimal promotion of a support verb. For example,
in “X conducted murder of Y”, the verb ‘murder’
should be linked with X and in the excerpt “X in-
vestigated murder of Y”, it should not be promoted.
Thus, we need to make 2 steps for promotion: (a)
calculate importance of every word connecting the
action cue such as ‘murder’ and ‘distributed’ and (b)
find the optimal promotion for the word ‘murder’.
Third, using the predefined threshold λ we cutoff
the instances with irrelevant support verbs (e.g.,
‘investigated’). Fourth, we reorder the tokens in
order to group them according to the anchor types.
The following algorithm in Figure 6 estimates
the importance of a token W for type D in the sup-
port verb structure. The input of the algorithm con-
sists of sentences S1...SN and two sets of tokens
Vneg, Vpos co-occurring with anchor cue of type D.
Vneg and Vpos are automatically tagged as irrelevant
and relevant respectively based on preliminary
marked keys in the training instances. The algo-
rithm output represents the importance value be-
tween 0 to 1.
</bodyText>
<subsectionHeader confidence="0.482946">
CalculateImportance (W, D)
</subsectionHeader>
<listItem confidence="0.6051398">
1) Select sentences that contain anchor cue D
2) Extract linguistic features of Vpos, Vneg and D
3) Train using SVM on instances (Vpos,D) and
instances (Vneg,D)
4) Return Importance(W) using SVM
</listItem>
<figureCaption confidence="0.998653">
Figure 6. Evaluation of word importance
</figureCaption>
<bodyText confidence="0.9999915">
We use the linguistic features for W and D as given
in Table 1 to form the instances.
</bodyText>
<subsectionHeader confidence="0.965075">
Hard category
</subsectionHeader>
<bodyText confidence="0.99975319047619">
In the hard category, we have to deal with long-
distance relations: at least 2 anchors are more than
2 links away in the dependency tree. Consequently,
dependency tree alone is not reliable for connecting
nodes. To find an optimal connection, we primarily
rely on comparison between several possible fill-
ings of slots based on previously extracted anchor
cues. Depending on the results of such comparison,
we chose the filling that has the highest score. As
an example, consider the hard category in the ex-
cerpt “MRTA today distributed leaflets claiming
responsibility for the murder of former defense
minister Enrique Lopez Albujar”. The dependency
tree for this instance is given in Figure 7.
Although words ‘MRTA’, ‘murder’ and ‘min-
ister’ might be correctly extracted as anchors, the
challenging problem is to decide whether ‘MRTA’
is a perpetrator. Anchors ‘MRTA’ and ‘minister’
are connected via the verb ‘distributed’. However,
the word ‘murder’ belongs to another branch of this
verb.
</bodyText>
<figureCaption confidence="0.987314">
Figure 7. Hard case
</figureCaption>
<bodyText confidence="0.999951266666667">
Processing of such categories is challenging.
Since relations are not reliable, we first need to rely
on the anchor extraction stage. Nevertheless, the
promotion strategy for the anchor cue ‘murder’ is
still possible, although the corresponding branch in
the dependency tree is long. Henceforth, we try to
replace the verb ‘distributed’ by promoting the an-
chor ‘murder’. To do so, we need to evaluate
whether the nodes in between may be eliminated.
For example, such elimination is possible in the
pairs ‘conducted’ -&gt; ‘murder’ and not possible in
the pair ‘investigated’ -&gt; ‘murder’, since in the ex-
cerpt “X investigated murder” X is not a perpetra-
tor. If the elimination is possible, we apply the
promotion algorithm given on Figure 8:
</bodyText>
<subsectionHeader confidence="0.5201">
FindOptimalPromotion (Fi)
</subsectionHeader>
<equation confidence="0.86473325">
1) Z = ∅
2) For each Ai(j1), Ai(j2) e Fi
Z = Z U Pj1-&gt;j2
End_for
</equation>
<listItem confidence="0.41837">
3) Output Top(Z)
</listItem>
<figureCaption confidence="0.996662">
Figure 8. Token promotion algorithm
</figureCaption>
<bodyText confidence="0.999989">
The algorithm checks path Pj1-&gt;j2 that connect an-
chors Ai(j1) and Ai(j2) in the filling Fi; the nodes from
Pj1-&gt;j2 are added to the set Z. Finally, the top node
of the set Z is chosen as an optimal node for the
promotion. The example optimal node for promo-
tion of the word ‘murder’ on Figure 7 is the node
‘distributed’.
Another important difference between the hard
and average cases is in the calculation of RankS (Fi)
in Equation (9). We set λhard &gt; λaverage because long
distance relations are less reliable in the hard case
than in the average case.
</bodyText>
<page confidence="0.997313">
576
</page>
<sectionHeader confidence="0.998853" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999777242424242">
In order to evaluate the efficiency of our method,
we conduct our experiments in 2 domains: MUC4
(Kaufmann, 1992) and MUC6 (Kaufmann, 1995).
The official corpus of MUC4 is released with
MUC3; it covers terrorism in the Latin America
region and consists of 1,700 texts. Among them,
1,300 documents belong to the training corpus.
Testing was done on 25 relevant and 25 irrelevant
texts from TST3, plus 25 relevant and 25 irrelevant
texts from TST4, as is done in Xiao et al. (2004).
MUC6 covers news articles in Management Suc-
cession domain. Its training corpus consists of 1201
instances, whereas the testing corpus consists of 76
person-ins, 82 person-outs, 123 positions, and 79
organizations. These slots we extracted in order to
fill templates on a sentence-by-sentence basis, as is
done by Chieu et al. (2002) and Soderland (1999).
Our experiments were designed to test the
effectiveness of both case splitting and action verb
promotion. The performance of ARE is compared
to both the state-of-art systems and our baseline
approach. We use 2 state-of-art systems for MUC4
and 1 system for MUC6. Our baseline system,
Anc+rel, utilizes only anchors and relations
without category splitting as described in Section 3.
For our ARE system with case splitting, we present
the results on Overall corpus, as well as separate
results on Simple, Average and Hard categories.
The Overall performance of ARE represents the
result for all the categories combined together.
Additionally, we test the impact of the action
promotion (in the right column) for the average and
hard categories.
</bodyText>
<table confidence="0.999856666666667">
Case (%) Without promotion With promotion
P R F1 P R F1
GRID 58% 56% 57% - - -
Riloff’05 46% 52% 48% - - -
Anc+rel (100%) 58% 59% 58% 58% 59% 58%
Overall (100%) 57% 60% 59% 58% 61% 60%
Simple (13%) 79% 86% 82% 79% 86% 82%
Average (22%) 64% 70% 67% 67% 71% 69%
Hard (65%) 50% 52% 51% 51% 53% 52%
</table>
<tableCaption confidence="0.999831">
Table 4. Results on MUC4 with case splitting
</tableCaption>
<bodyText confidence="0.999563865671642">
The comparative results are presented in Table
4 and Table 5 for MUC4 and MUC6, respectively.
First, we review our experimental results on MUC4
corpus without promotion (left column) before pro-
ceeding to the right column.
a) From the results on Table 4 we observe that our
baseline approach Anc+rel outperforms all the
state-of-art systems. It demonstrates that both an-
chors and relations are useful. Anchors allow us to
group entities according to their semantic meanings
and thus to select of the most prominent candidates.
Relations allow us to capture more invariant repre-
sentation of instances. However, a sentence may
contain very few high-quality relations. It implies
that the relations ranking step is fuzzy in nature. In
addition, we noticed that some anchor cues may be
missing, whereas the other anchor types may be
represented by several anchor cues. All these fac-
tors lead only to moderate improvement in per-
formance, especially in comparison with GRID
system.
b) Overall, the splitting of instances into categories
turned out to be useful. Due to the application of
specific strategies the performance increased by 1%
over the baseline. However, the large dominance of
the hard cases (65%) made this improvement mod-
est.
c) We notice that the amount of variations for con-
necting anchor cues in the Simple category is rela-
tively small. Therefore, the overall performance for
this case reaches F1=82%. The main errors here
come from missing anchors resulting partly from
mistakes in such component as NE detection.
d) The performance in the Average category is
F1=67%. It is lower than that for the simple cate-
gory because of higher variability in relations and
negative influence of support verbs. For example,
for excerpt such as “X investigated murder of Y”,
the processing tends to make mistake without the
analysis of semantic value of support verb ‘investi-
gated’.
e) Hard category achieves the lowest performance
of F1=51% among all the categories. Since for this
category we have to rely mostly on anchors, the
problem arises if these anchors provide the wrong
clues. It happens if some of them are missing or are
wrongly extracted. The other cause of mistakes is
when ARE finds several anchor cues which belong
to the same type.
Additional usage of promotion strategies al-
lowed us to improve the performance further.
f) Overall, the addition of promotion strategy en-
ables the system to further boost the performance to
F1=60%. It means that the promotion strategy is
useful, especially for the average case. The im-
provement in comparison to the state-of-art system
GRID is about 3%.
g) It achieved an F1=69%, which is an improve-
ment of 2%, for the Average category. It implies
that the analysis of support verbs helps in revealing
the differences between the instances such as “X
was involved in kidnapping of Y” and “X reported
kidnapping of Y”.
h) The results in the Hard category improved mod-
erately to F1=52%. The reason for the improvement
is that more anchor cues are captured after the
promotion. Still, there are 2 types of common mis-
</bodyText>
<page confidence="0.986641">
577
</page>
<bodyText confidence="0.995758">
takes: 1) multiple or missing anchor cues of the
same type and 2) anchors can be spread across sev-
eral sentences or several clauses in the same sen-
tence.
</bodyText>
<table confidence="0.99959625">
Case (%) Without promotion With promotion
P R F1 P R F1
Chieu et al.’02 74% 49% 59% - - -
Anc+rel (100%) 78% 52% 62% 78% 52% 62%
Overall (100%) 72% 58% 64% 73% 58% 65%
Simple (45%) 85% 67% 75% 87% 68% 76%
Average (27%) 61% 55% 58% 64% 56% 60%
Hard (28%) 59% 44% 50% 59% 44% 50%
</table>
<tableCaption confidence="0.999835">
Table 5. Results on MUC6 with case splitting
</tableCaption>
<bodyText confidence="0.99998725">
For the MUC6 results given in Table 5, we ob-
serve that the overall improvement in performance
of ARE system over Chieu et al.’02 is 6%. The
trends of results for MUC6 are similar to that in
MUC4. However, there are few important differ-
ences. First, 45% of instances in MUC6 fall into
the Simple category, therefore this category domi-
nates. The reason for this is that the terminologies
used in Management Succession domain are more
stable in comparison to the Terrorism domain. Sec-
ond, there are more anchor types for this case and
therefore the promotion strategy is applicable also
to the simple case. Third, there is no improvement
in performance for the Hard category. We believe
the primary reason for it is that more stable lan-
guage patterns are used in MUC6. Therefore, de-
pendency relations are also more stable in MUC6
and the promotion strategy is not very important.
Similar to MUC4, there are problems of missing
anchors and mistakes in dependency parsing.
</bodyText>
<sectionHeader confidence="0.999578" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999913842105263">
The current state-of-art IE methods tend to use co-
occurrence relations for extraction of entities. Al-
though context may provide a meaningful clue, the
use of co-occurrence relations alone has serious
limitations because of alignment and paraphrasing
problems. In our work, we proposed to utilize de-
pendency relations to tackle these problems. Based
on the extracted anchor cues and relations between
them, we split instances into ‘simple’, ‘average’
and ‘hard’ categories. For each category, we ap-
plied specific strategy. This approach allowed us to
outperform the existing state-of-art approaches by
3% on Terrorism domain and 6% on Management
Succession domain. In our future work we plan to
investigate the role of semantic relations and inte-
grate ontology in the rule generation process. An-
other direction is to explore the use of bootstrap-
ping and transduction approaches that may require
less training instances.
</bodyText>
<sectionHeader confidence="0.996368" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998399530612245">
H.L. Chieu and H.T. Ng. 2002. A Maximum Entropy Ap-
proach to Information Extraction from Semi-Structured
and Free Text. In Proc of AAAI-2002, 786-791.
H. Cui, M.Y. Kan, and Chua T.S. 2005. Generic Soft Pat-
tern Models for Definitional Question Answering. In
Proc of ACM SIGIR-2005.
A. Culotta and J. Sorensen J. 2004. Dependency tree kernels
for relation extraction. In Proc of ACL-2004.
F. Ciravegna. 2001. Adaptive Information Extraction from
Text by Rule Induction and Generalization. In Proc of
IJCAI-2001.
A. Dempster, N. Laird, and D. Rubin. 1977. Maximum like-
lihood from incomplete data via the EM algorithm. Jour-
nal of the Royal Statistical Society B, 39(1):1–38
K. Humphreys, G. Demetriou and R. Gaizuskas. 2000. Two
applications of Information Extraction to Biological Sci-
ence: Enzyme interactions and Protein structures. In
Proc of the Pacific Symposium on Biocomputing, 502-
513
M. Kaufmann. 1992. MUC-4. In Proc of MUC-4.
M. Kaufmann. 1995. MUC-6. In Proc of MUC-6.
J. Kim and D. Moldovan. 1995. Acquisition of linguistic
patterns for knowledge-based information extraction.
IEEE Transactions on KDE, 7(5): 713-724
D. Lin. 1997. Using Syntactic Dependency as Local Context
to Resolve Word Sense Ambiguity. In Proc of ACL-97.
E. Riloff. 1996. Automatically Generating Extraction Pat-
terns from Untagged Text. In Proc of AAAI-96, 1044-
1049.
D. Roth and W. Yih. 2002. Probabilistic Reasoning for En-
tity &amp; Relation Recognition. In Proc of COLING-2002.
S. Soderland, D. Fisher, J. Aseltine and W. Lehnert. 1995.
Crystal: Inducing a Conceptual Dictionary. In Proc of
IJCAI-95, 1314-1319.
S. Soderland. 1999. Learning Information Extraction Rules
for Semi-Structured and Free Text. Machine Learning
34:233-272.
J. Xiao, T.S. Chua and H. Cui. 2004. Cascading Use of Soft
and Hard Matching Pattern Rules for Weakly Supervised
Information Extraction. In Proc of COLING-2004.
H. Yang, H. Cui, M.-Y. Kan, M. Maslennikov, L. Qiu and
T.-S. Chua. 2003. QUALIFIER in TREC 12 QA Main
Task. In Proc of TREC-12, 54-65.
R. Yangarber, W. Lin, R. Grishman. 2002. Unsupervised
Learning of Generalized Names. In Proc of COLING-
2002.
G.D. Zhou and J. Su. 2002. Named entity recognition using
an HMM-based chunk tagger. In Proc of ACL-2002,
473-480
</reference>
<page confidence="0.996636">
578
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.882299">
<title confidence="0.9988535">ARE: Instance Splitting Strategies for Dependency Relation-based Information Extraction</title>
<author confidence="0.994694">Mstislav Maslennikov Hai-Kiat Goh Tat-Seng Chua</author>
<affiliation confidence="0.999734333333333">Department of Computer Science School of Computing National University of Singapore</affiliation>
<email confidence="0.97024">maslenni@comp.nus.edu.sg</email>
<email confidence="0.97024">gohhaiki@comp.nus.edu.sg</email>
<email confidence="0.97024">chuats@comp.nus.edu.sg</email>
<abstract confidence="0.996500958333333">Information Extraction (IE) is a fundamental technology for NLP. Previous methods for IE were relying on co-occurrence relations, soft patterns and properties of the target (for example, syntactic role), which result in problems of handling paraphrasing and alignment of instances. Our system ARE (Anchor and Relation) is based on the dependency relation model and tackles these problems by unifying entities according to their dependency relations, which we found to provide more invariant relations between entities in many cases. In order to exploit the complexity and characteristics of relation paths, we further classify the relation paths into the categories of ‘easy’, ‘average’ and ‘hard’, and utilize different extraction strategies based on the characteristics of those categories. Our extraction method leads to improvement in performance by 3% and 6% for MUC4 and MUC6 respectively as compared to the state-of-art IE systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H L Chieu</author>
<author>H T Ng</author>
</authors>
<title>A Maximum Entropy Approach to Information Extraction from Semi-Structured and Free Text.</title>
<date>2002</date>
<booktitle>In Proc of AAAI-2002,</booktitle>
<pages>786--791</pages>
<contexts>
<context position="8556" citStr="Chieu and Ng (2002)" startWordPosition="1312" endWordPosition="1315">e this problem, (LP)2 by Ciravegna (2002) utilizes rules with high precision in order to improve the precision of rules with average recall. However, (LP)2 is developed for semi-structured textual domain, where we can find consistent lexical patterns at surface text level. This is not the same for freetext, in which different order of words or an extra clause in a sentence may cause paraphrasing and alignment problems respectively, such as the example excerpts “terrorists attacked peasants” and “peasants were attacked 2 months ago by terrorists”. The classification-based approaches such as by Chieu and Ng (2002) tend to outperform rule-based approaches. However, Ciravegna (2001) argued that it is difficult to examine the result obtained by classifiers. Thus, interpretability of the learned knowledge is a serious bottleneck of the classification approach. Additionally, Zhou and Su (2002) trained classifiers for Named Entity extraction and reported that performance degrades rapidly if the training corpus size is below 100KB. It implies that human experts have to spend long hours to annotate a sufficiently large amount of training corpus. Several recent researches focused on the extraction of relationsh</context>
</contexts>
<marker>Chieu, Ng, 2002</marker>
<rawString>H.L. Chieu and H.T. Ng. 2002. A Maximum Entropy Approach to Information Extraction from Semi-Structured and Free Text. In Proc of AAAI-2002, 786-791.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cui</author>
<author>M Y Kan</author>
<author>T S Chua</author>
</authors>
<title>Generic Soft Pattern Models for Definitional Question Answering.</title>
<date>2005</date>
<booktitle>In Proc of ACM SIGIR-2005.</booktitle>
<contexts>
<context position="3550" citStr="Cui et al. (2005)" startWordPosition="537" endWordPosition="540">th ‘attacked’ as a verb and ‘terrorists’ as a subject. After grouping of same syntactic roles in the above examples, we are able to unify these instances. Another problem in IE systems is word alignment. Insertion or deletion of tokens prevents instances from being generalized effectively during learning. Therefore, the instances “Victims were attacked by terrorists” and “Victims were recently attacked by terrorists” are difficult to unify. The common approach adopted in GRID by Xiao et al. (2003) is to apply more stable chunks such as noun phrases and verb phrases. Another recent approach by Cui et al. (2005) utilizes soft patterns for probabilistic matching of tokens. However, a longer insertion leads to a more complicated structure, as in the instance “Victims, living near the shop, went out for a walk and were attacked by terrorists”. Since there may be many inserted words, both approaches may also be inefficient for this case. Similar to the paraphrasing problem, the word alignment problem may be handled with dependency relations in many cases. We found that the relation subject-verb-object for words ‘victims’, ‘attacked’ and ‘terrorists’ remains invariant for the above two instances. Before I</context>
</contexts>
<marker>Cui, Kan, Chua, 2005</marker>
<rawString>H. Cui, M.Y. Kan, and Chua T.S. 2005. Generic Soft Pattern Models for Definitional Question Answering. In Proc of ACM SIGIR-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Culotta</author>
<author>J Sorensen J</author>
</authors>
<title>Dependency tree kernels for relation extraction.</title>
<date>2004</date>
<booktitle>In Proc of ACL-2004.</booktitle>
<marker>Culotta, J, 2004</marker>
<rawString>A. Culotta and J. Sorensen J. 2004. Dependency tree kernels for relation extraction. In Proc of ACL-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Ciravegna</author>
</authors>
<title>Adaptive Information Extraction from Text by Rule Induction and Generalization.</title>
<date>2001</date>
<booktitle>In Proc of IJCAI-2001.</booktitle>
<contexts>
<context position="8624" citStr="Ciravegna (2001)" startWordPosition="1322" endWordPosition="1323">ision in order to improve the precision of rules with average recall. However, (LP)2 is developed for semi-structured textual domain, where we can find consistent lexical patterns at surface text level. This is not the same for freetext, in which different order of words or an extra clause in a sentence may cause paraphrasing and alignment problems respectively, such as the example excerpts “terrorists attacked peasants” and “peasants were attacked 2 months ago by terrorists”. The classification-based approaches such as by Chieu and Ng (2002) tend to outperform rule-based approaches. However, Ciravegna (2001) argued that it is difficult to examine the result obtained by classifiers. Thus, interpretability of the learned knowledge is a serious bottleneck of the classification approach. Additionally, Zhou and Su (2002) trained classifiers for Named Entity extraction and reported that performance degrades rapidly if the training corpus size is below 100KB. It implies that human experts have to spend long hours to annotate a sufficiently large amount of training corpus. Several recent researches focused on the extraction of relationships using classifiers. Roth and Yih (2002) learned the entities and </context>
</contexts>
<marker>Ciravegna, 2001</marker>
<rawString>F. Ciravegna. 2001. Adaptive Information Extraction from Text by Rule Induction and Generalization. In Proc of IJCAI-2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dempster</author>
<author>N Laird</author>
<author>D Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society B,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="14743" citStr="Dempster et al. (1977)" startWordPosition="2305" endWordPosition="2308">Aj) +δ5* S_Concept-ClassS(P,Aj) (1) where S_XXXS(P,Aj) is a score function for the type Aj and δi is the importance weight for Aj. In order to extract the score function, we use entities from slots in the training instances. Each S_XXXS(P,Aj) is calculated as a ratio of occurrence in positive slots versus all the slots: S_XXXS(P,Aj) = # (all slots of the type Aj ) We classify the phrase P as belonging to an anchor cue A of type j if Phrase_ScoreS(P,Aj) ≥ ω, where ω is an empirically determined threshold. The weights 5 = (51,..., 55 ) are learned automatically using Expectation Maximization by Dempster et al. (1977). Using anchors from training instances as ground truth, we iteratively input different sets of weights into EM to maximize the overall score. #(P in positive slots of the type Aj ) (2) 573 Consider the excerpts “Terrorists attacked victims”, “Peasants were murdered by unidentified individuals” and “Soldiers participated in massacre of Jesuit priests”. Let Wi denotes the position of token i in the instances. After mining of anchors, we are able to extract meaningful anchor cues in these sentences as shown in Table 2: W-3 W-2 W-1 W0 W1 W2 W3 Perp_Cue Action_Cue Victim_Cue Victim_Cue were Action</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. Dempster, N. Laird, and D. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society B, 39(1):1–38</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Humphreys</author>
<author>G Demetriou</author>
<author>R Gaizuskas</author>
</authors>
<title>Two applications of Information Extraction to Biological Science: Enzyme interactions and Protein structures.</title>
<date>2000</date>
<booktitle>In Proc of the Pacific Symposium on Biocomputing,</booktitle>
<pages>502--513</pages>
<contexts>
<context position="1591" citStr="Humphreys et al., 2000" startWordPosition="229" endWordPosition="232">ard’, and utilize different extraction strategies based on the characteristics of those categories. Our extraction method leads to improvement in performance by 3% and 6% for MUC4 and MUC6 respectively as compared to the state-of-art IE systems. 1 Introduction Information Extraction (IE) is one of the fundamental problems of natural language processing. Progress in IE is important to enhance results in such tasks as Question Answering, Information Retrieval and Text Summarization. Multiple efforts in MUC series allowed IE systems to achieve nearhuman performance in such domains as biological (Humphreys et al., 2000), terrorism (Kaufmann, 1992; Kaufmann, 1993) and management succession (Kaufmann, 1995). The IE task is formulated for MUC series as filling of several predefined slots in a template. The terrorism template consists of slots Perpetrator, Victim and Target; the slots in the management succession template are Org, PersonIn, PersonOut and Post. We decided to choose both terrorism and management succession domains, from MUC4 and MUC6 respectively, in order to demonstrate that our idea is applicable to multiple domains. Paraphrasing of instances is one of the crucial problems in IE. This problem le</context>
</contexts>
<marker>Humphreys, Demetriou, Gaizuskas, 2000</marker>
<rawString>K. Humphreys, G. Demetriou and R. Gaizuskas. 2000. Two applications of Information Extraction to Biological Science: Enzyme interactions and Protein structures. In Proc of the Pacific Symposium on Biocomputing, 502-513</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kaufmann</author>
</authors>
<date>1992</date>
<booktitle>MUC-4. In Proc of MUC-4.</booktitle>
<publisher>M. Kaufmann.</publisher>
<contexts>
<context position="1618" citStr="Kaufmann, 1992" startWordPosition="234" endWordPosition="235">on strategies based on the characteristics of those categories. Our extraction method leads to improvement in performance by 3% and 6% for MUC4 and MUC6 respectively as compared to the state-of-art IE systems. 1 Introduction Information Extraction (IE) is one of the fundamental problems of natural language processing. Progress in IE is important to enhance results in such tasks as Question Answering, Information Retrieval and Text Summarization. Multiple efforts in MUC series allowed IE systems to achieve nearhuman performance in such domains as biological (Humphreys et al., 2000), terrorism (Kaufmann, 1992; Kaufmann, 1993) and management succession (Kaufmann, 1995). The IE task is formulated for MUC series as filling of several predefined slots in a template. The terrorism template consists of slots Perpetrator, Victim and Target; the slots in the management succession template are Org, PersonIn, PersonOut and Post. We decided to choose both terrorism and management succession domains, from MUC4 and MUC6 respectively, in order to demonstrate that our idea is applicable to multiple domains. Paraphrasing of instances is one of the crucial problems in IE. This problem leads to data sparseness in s</context>
<context position="28487" citStr="Kaufmann, 1992" startWordPosition="4656" endWordPosition="4657">2) in the filling Fi; the nodes from Pj1-&gt;j2 are added to the set Z. Finally, the top node of the set Z is chosen as an optimal node for the promotion. The example optimal node for promotion of the word ‘murder’ on Figure 7 is the node ‘distributed’. Another important difference between the hard and average cases is in the calculation of RankS (Fi) in Equation (9). We set λhard &gt; λaverage because long distance relations are less reliable in the hard case than in the average case. 576 5 Evaluation In order to evaluate the efficiency of our method, we conduct our experiments in 2 domains: MUC4 (Kaufmann, 1992) and MUC6 (Kaufmann, 1995). The official corpus of MUC4 is released with MUC3; it covers terrorism in the Latin America region and consists of 1,700 texts. Among them, 1,300 documents belong to the training corpus. Testing was done on 25 relevant and 25 irrelevant texts from TST3, plus 25 relevant and 25 irrelevant texts from TST4, as is done in Xiao et al. (2004). MUC6 covers news articles in Management Succession domain. Its training corpus consists of 1201 instances, whereas the testing corpus consists of 76 person-ins, 82 person-outs, 123 positions, and 79 organizations. These slots we ext</context>
</contexts>
<marker>Kaufmann, 1992</marker>
<rawString>M. Kaufmann. 1992. MUC-4. In Proc of MUC-4. M. Kaufmann. 1995. MUC-6. In Proc of MUC-6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kim</author>
<author>D Moldovan</author>
</authors>
<title>Acquisition of linguistic patterns for knowledge-based information extraction.</title>
<date>1995</date>
<journal>IEEE Transactions on KDE,</journal>
<volume>7</volume>
<issue>5</issue>
<pages>713--724</pages>
<contexts>
<context position="6673" citStr="Kim and Moldovan (1995)" startWordPosition="1023" endWordPosition="1026">relation structures, and employ the action promotion strategy to tackle the problem of long distance relations. The remaining parts of the paper are organized as follows. Section 2 discusses related work and Section 3 introduces our approach for constructing ARE. Section 4 introduces our method for splitting instances into categories. Section 5 describes our experimental setups and results and, finally, Section 6 concludes the paper. 2 Related work There are several research directions in Information Extraction. We highlight a few directions in IE such as case frame based modeling in PALKA by Kim and Moldovan (1995) and CRYSTAL by Soderland et al. (1995); rule-based learning in Autoslog-TS by Riloff et al. (1996); and classification-based learning by Chieu et al. (2002). Although systems representing these directions have very different learning models, paraphrasing and alignment problems still have no reliable solution. Case frame based IE systems incorporate domain-dependent knowledge in the processing and learning of semantic constraints. However, concept hierarchy used in case frames is typically encoded manually and requires additional human labor for porting across domains. Moreover, the systems te</context>
</contexts>
<marker>Kim, Moldovan, 1995</marker>
<rawString>J. Kim and D. Moldovan. 1995. Acquisition of linguistic patterns for knowledge-based information extraction. IEEE Transactions on KDE, 7(5): 713-724</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Using Syntactic Dependency as Local Context to Resolve Word Sense Ambiguity.</title>
<date>1997</date>
<booktitle>In Proc of ACL-97.</booktitle>
<contexts>
<context position="2864" citStr="Lin, 1997" startWordPosition="427" endWordPosition="428">sed in different ways. As an example, consider the excerpts “Terrorists attacked victims” and “Victims were attacked by unidentified terrorists”. These instances have very similar semantic meaning. However, context-based approaches such as Autoslog-TS by Riloff (1996) and Yangarber et al. (2002) may face difficulties in handling these instances effectively because the context of entity ‘victims’ is located on the left context in the first instance and on the right context in the second. For these cases, we found that we are able to verify the context by performing dependency relation parsing (Lin, 1997), which outputs the word ‘victims’ as an object in both instances, with ‘attacked’ as a verb and ‘terrorists’ as a subject. After grouping of same syntactic roles in the above examples, we are able to unify these instances. Another problem in IE systems is word alignment. Insertion or deletion of tokens prevents instances from being generalized effectively during learning. Therefore, the instances “Victims were attacked by terrorists” and “Victims were recently attacked by terrorists” are difficult to unify. The common approach adopted in GRID by Xiao et al. (2003) is to apply more stable chun</context>
<context position="10114" citStr="Lin (1997)" startWordPosition="1552" endWordPosition="1553">takes in relation extraction. A possible approach for modeling relations of different complexity is the use of dependency-based kernel trees in support vector machines by Culotta and Sorensen (2004). The authors reported that nonrelation instances are very heterogeneous, and 572 hence they suggested the additional step of extracting candidate relations before classification. After preprocessing and feature extraction, we obtain the linguistic features in Table 1. 3 Our approach Differing from previous systems, the language model in ARE is based on dependency relations obtained from Minipar by Lin (1997). In the first stage, ARE tries to identify possible candidates for filling slots in a sentence. For example, words such as ‘terrorist’ or ‘guerrilla’ can fill the slot for Perpetrator in the terrorism domain. We refer to these candidates as anchors or anchor cues. In the second stage, ARE defines the dependency relations that connect anchor cues. We exploit dependency relations to provide more invariant structures for similar sentences with different syntactic structures. After extracting the possible relations between anchor cues, we form several possible parsing paths and rank them. Based o</context>
</contexts>
<marker>Lin, 1997</marker>
<rawString>D. Lin. 1997. Using Syntactic Dependency as Local Context to Resolve Word Sense Ambiguity. In Proc of ACL-97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
</authors>
<title>Automatically Generating Extraction Patterns from Untagged Text.</title>
<date>1996</date>
<booktitle>In Proc of AAAI-96,</booktitle>
<pages>1044--1049</pages>
<contexts>
<context position="2522" citStr="Riloff (1996)" startWordPosition="369" endWordPosition="370">onIn, PersonOut and Post. We decided to choose both terrorism and management succession domains, from MUC4 and MUC6 respectively, in order to demonstrate that our idea is applicable to multiple domains. Paraphrasing of instances is one of the crucial problems in IE. This problem leads to data sparseness in situations when information is expressed in different ways. As an example, consider the excerpts “Terrorists attacked victims” and “Victims were attacked by unidentified terrorists”. These instances have very similar semantic meaning. However, context-based approaches such as Autoslog-TS by Riloff (1996) and Yangarber et al. (2002) may face difficulties in handling these instances effectively because the context of entity ‘victims’ is located on the left context in the first instance and on the right context in the second. For these cases, we found that we are able to verify the context by performing dependency relation parsing (Lin, 1997), which outputs the word ‘victims’ as an object in both instances, with ‘attacked’ as a verb and ‘terrorists’ as a subject. After grouping of same syntactic roles in the above examples, we are able to unify these instances. Another problem in IE systems is w</context>
<context position="7645" citStr="Riloff (1996)" startWordPosition="1167" endWordPosition="1168">dependent knowledge in the processing and learning of semantic constraints. However, concept hierarchy used in case frames is typically encoded manually and requires additional human labor for porting across domains. Moreover, the systems tend to rely on heuristics in order to match case frames. PALKA by Kim and Moldovan (1995) performs keyword-based matching of concepts, while CRYSTAL by Soderland et al. (1995) relied on additional domain-specific annotation and associated lexicon for matching. Rule-based IE models allow differentiation of rules according to their performance. Autoslog-TS by Riloff (1996) learns the context rules for extraction and ranks them according to their performance on the training corpus. Although this approach is suitable for automatic training, Xiao et al. (2004) stated that hard matching techniques tend to have low recall due to data sparseness problem. To overcome this problem, (LP)2 by Ciravegna (2002) utilizes rules with high precision in order to improve the precision of rules with average recall. However, (LP)2 is developed for semi-structured textual domain, where we can find consistent lexical patterns at surface text level. This is not the same for freetext,</context>
</contexts>
<marker>Riloff, 1996</marker>
<rawString>E. Riloff. 1996. Automatically Generating Extraction Patterns from Untagged Text. In Proc of AAAI-96, 1044-1049.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>Probabilistic Reasoning for Entity &amp; Relation Recognition.</title>
<date>2002</date>
<booktitle>In Proc of COLING-2002.</booktitle>
<contexts>
<context position="9198" citStr="Roth and Yih (2002)" startWordPosition="1407" endWordPosition="1410">e-based approaches. However, Ciravegna (2001) argued that it is difficult to examine the result obtained by classifiers. Thus, interpretability of the learned knowledge is a serious bottleneck of the classification approach. Additionally, Zhou and Su (2002) trained classifiers for Named Entity extraction and reported that performance degrades rapidly if the training corpus size is below 100KB. It implies that human experts have to spend long hours to annotate a sufficiently large amount of training corpus. Several recent researches focused on the extraction of relationships using classifiers. Roth and Yih (2002) learned the entities and relations together. The joint learning improves the performance of NE recognition in cases such as “X killed Y”. It also prevents the propagation of mistakes in NE extraction to the extraction of relations. However, long distance relations between entities are likely to cause mistakes in relation extraction. A possible approach for modeling relations of different complexity is the use of dependency-based kernel trees in support vector machines by Culotta and Sorensen (2004). The authors reported that nonrelation instances are very heterogeneous, and 572 hence they sug</context>
</contexts>
<marker>Roth, Yih, 2002</marker>
<rawString>D. Roth and W. Yih. 2002. Probabilistic Reasoning for Entity &amp; Relation Recognition. In Proc of COLING-2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Soderland</author>
<author>D Fisher</author>
<author>J Aseltine</author>
<author>W Lehnert</author>
</authors>
<title>Crystal: Inducing a Conceptual Dictionary.</title>
<date>1995</date>
<booktitle>In Proc of IJCAI-95,</booktitle>
<pages>1314--1319</pages>
<contexts>
<context position="6712" citStr="Soderland et al. (1995)" startWordPosition="1030" endWordPosition="1034">ion promotion strategy to tackle the problem of long distance relations. The remaining parts of the paper are organized as follows. Section 2 discusses related work and Section 3 introduces our approach for constructing ARE. Section 4 introduces our method for splitting instances into categories. Section 5 describes our experimental setups and results and, finally, Section 6 concludes the paper. 2 Related work There are several research directions in Information Extraction. We highlight a few directions in IE such as case frame based modeling in PALKA by Kim and Moldovan (1995) and CRYSTAL by Soderland et al. (1995); rule-based learning in Autoslog-TS by Riloff et al. (1996); and classification-based learning by Chieu et al. (2002). Although systems representing these directions have very different learning models, paraphrasing and alignment problems still have no reliable solution. Case frame based IE systems incorporate domain-dependent knowledge in the processing and learning of semantic constraints. However, concept hierarchy used in case frames is typically encoded manually and requires additional human labor for porting across domains. Moreover, the systems tend to rely on heuristics in order to ma</context>
</contexts>
<marker>Soderland, Fisher, Aseltine, Lehnert, 1995</marker>
<rawString>S. Soderland, D. Fisher, J. Aseltine and W. Lehnert. 1995. Crystal: Inducing a Conceptual Dictionary. In Proc of IJCAI-95, 1314-1319.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Soderland</author>
</authors>
<title>Learning Information Extraction Rules for Semi-Structured and Free Text.</title>
<date>1999</date>
<journal>Machine Learning</journal>
<pages>34--233</pages>
<contexts>
<context position="29208" citStr="Soderland (1999)" startWordPosition="4775" endWordPosition="4776"> Latin America region and consists of 1,700 texts. Among them, 1,300 documents belong to the training corpus. Testing was done on 25 relevant and 25 irrelevant texts from TST3, plus 25 relevant and 25 irrelevant texts from TST4, as is done in Xiao et al. (2004). MUC6 covers news articles in Management Succession domain. Its training corpus consists of 1201 instances, whereas the testing corpus consists of 76 person-ins, 82 person-outs, 123 positions, and 79 organizations. These slots we extracted in order to fill templates on a sentence-by-sentence basis, as is done by Chieu et al. (2002) and Soderland (1999). Our experiments were designed to test the effectiveness of both case splitting and action verb promotion. The performance of ARE is compared to both the state-of-art systems and our baseline approach. We use 2 state-of-art systems for MUC4 and 1 system for MUC6. Our baseline system, Anc+rel, utilizes only anchors and relations without category splitting as described in Section 3. For our ARE system with case splitting, we present the results on Overall corpus, as well as separate results on Simple, Average and Hard categories. The Overall performance of ARE represents the result for all the </context>
</contexts>
<marker>Soderland, 1999</marker>
<rawString>S. Soderland. 1999. Learning Information Extraction Rules for Semi-Structured and Free Text. Machine Learning 34:233-272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Xiao</author>
<author>T S Chua</author>
<author>H Cui</author>
</authors>
<title>Cascading Use of Soft and Hard Matching Pattern Rules for Weakly Supervised Information Extraction.</title>
<date>2004</date>
<booktitle>In Proc of COLING-2004.</booktitle>
<contexts>
<context position="7833" citStr="Xiao et al. (2004)" startWordPosition="1195" endWordPosition="1198">labor for porting across domains. Moreover, the systems tend to rely on heuristics in order to match case frames. PALKA by Kim and Moldovan (1995) performs keyword-based matching of concepts, while CRYSTAL by Soderland et al. (1995) relied on additional domain-specific annotation and associated lexicon for matching. Rule-based IE models allow differentiation of rules according to their performance. Autoslog-TS by Riloff (1996) learns the context rules for extraction and ranks them according to their performance on the training corpus. Although this approach is suitable for automatic training, Xiao et al. (2004) stated that hard matching techniques tend to have low recall due to data sparseness problem. To overcome this problem, (LP)2 by Ciravegna (2002) utilizes rules with high precision in order to improve the precision of rules with average recall. However, (LP)2 is developed for semi-structured textual domain, where we can find consistent lexical patterns at surface text level. This is not the same for freetext, in which different order of words or an extra clause in a sentence may cause paraphrasing and alignment problems respectively, such as the example excerpts “terrorists attacked peasants” </context>
<context position="28853" citStr="Xiao et al. (2004)" startWordPosition="4718" endWordPosition="4721">(9). We set λhard &gt; λaverage because long distance relations are less reliable in the hard case than in the average case. 576 5 Evaluation In order to evaluate the efficiency of our method, we conduct our experiments in 2 domains: MUC4 (Kaufmann, 1992) and MUC6 (Kaufmann, 1995). The official corpus of MUC4 is released with MUC3; it covers terrorism in the Latin America region and consists of 1,700 texts. Among them, 1,300 documents belong to the training corpus. Testing was done on 25 relevant and 25 irrelevant texts from TST3, plus 25 relevant and 25 irrelevant texts from TST4, as is done in Xiao et al. (2004). MUC6 covers news articles in Management Succession domain. Its training corpus consists of 1201 instances, whereas the testing corpus consists of 76 person-ins, 82 person-outs, 123 positions, and 79 organizations. These slots we extracted in order to fill templates on a sentence-by-sentence basis, as is done by Chieu et al. (2002) and Soderland (1999). Our experiments were designed to test the effectiveness of both case splitting and action verb promotion. The performance of ARE is compared to both the state-of-art systems and our baseline approach. We use 2 state-of-art systems for MUC4 and</context>
</contexts>
<marker>Xiao, Chua, Cui, 2004</marker>
<rawString>J. Xiao, T.S. Chua and H. Cui. 2004. Cascading Use of Soft and Hard Matching Pattern Rules for Weakly Supervised Information Extraction. In Proc of COLING-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yang</author>
<author>H Cui</author>
<author>M-Y Kan</author>
<author>M Maslennikov</author>
<author>L Qiu</author>
<author>T-S Chua</author>
</authors>
<date>2003</date>
<booktitle>QUALIFIER in TREC 12 QA Main Task. In Proc of TREC-12,</booktitle>
<pages>54--65</pages>
<contexts>
<context position="12210" citStr="Yang et al. (2003)" startWordPosition="1878" endWordPosition="1881">Synset 71 Concept ID 2, 3 ID 9 ID 22, 43 ID 61, 48 Class Co- He -&gt; terrorist, - They -&gt; - referenced soldier peasants entity Table 1. Linguistic features for anchor extraction Every token in ARE may be represented at a different level of representations, including: Lexical, Part-of-Speech, Named Entities, Synonyms and Concept classes. The synonym set and concept classes are mainly obtained from Wordnet. We use NLProcessor from Infogistics Ltd for the extraction of part-of-speech, noun phrases and verb phrases (we refer to them as phrases). Named Entities are extracted with the program used in Yang et al. (2003). Additionally, we employed the coreference module for the extraction of meaningful pronouns. It is used for linking entities across clauses or sentences, for example in “John works in XYZ Corp. He was appointed as a vice-president a month ago” and could achieve an accuracy of 62%. 3.1 Mining of anchor cues In order to extract possible anchors and relations from every sentence, we need to select features to support the generalization of words. This generalization may be different for different classes of words. For example, person names may be generalized as a Named Entity PERSON, whereas for </context>
</contexts>
<marker>Yang, Cui, Kan, Maslennikov, Qiu, Chua, 2003</marker>
<rawString>H. Yang, H. Cui, M.-Y. Kan, M. Maslennikov, L. Qiu and T.-S. Chua. 2003. QUALIFIER in TREC 12 QA Main Task. In Proc of TREC-12, 54-65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Yangarber</author>
<author>W Lin</author>
<author>R Grishman</author>
</authors>
<title>Unsupervised Learning of Generalized Names.</title>
<date>2002</date>
<booktitle>In Proc of COLING2002.</booktitle>
<contexts>
<context position="2550" citStr="Yangarber et al. (2002)" startWordPosition="372" endWordPosition="375">d Post. We decided to choose both terrorism and management succession domains, from MUC4 and MUC6 respectively, in order to demonstrate that our idea is applicable to multiple domains. Paraphrasing of instances is one of the crucial problems in IE. This problem leads to data sparseness in situations when information is expressed in different ways. As an example, consider the excerpts “Terrorists attacked victims” and “Victims were attacked by unidentified terrorists”. These instances have very similar semantic meaning. However, context-based approaches such as Autoslog-TS by Riloff (1996) and Yangarber et al. (2002) may face difficulties in handling these instances effectively because the context of entity ‘victims’ is located on the left context in the first instance and on the right context in the second. For these cases, we found that we are able to verify the context by performing dependency relation parsing (Lin, 1997), which outputs the word ‘victims’ as an object in both instances, with ‘attacked’ as a verb and ‘terrorists’ as a subject. After grouping of same syntactic roles in the above examples, we are able to unify these instances. Another problem in IE systems is word alignment. Insertion or </context>
</contexts>
<marker>Yangarber, Lin, Grishman, 2002</marker>
<rawString>R. Yangarber, W. Lin, R. Grishman. 2002. Unsupervised Learning of Generalized Names. In Proc of COLING2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G D Zhou</author>
<author>J Su</author>
</authors>
<title>Named entity recognition using an HMM-based chunk tagger.</title>
<date>2002</date>
<booktitle>In Proc of ACL-2002,</booktitle>
<pages>473--480</pages>
<contexts>
<context position="8836" citStr="Zhou and Su (2002)" startWordPosition="1352" endWordPosition="1355"> not the same for freetext, in which different order of words or an extra clause in a sentence may cause paraphrasing and alignment problems respectively, such as the example excerpts “terrorists attacked peasants” and “peasants were attacked 2 months ago by terrorists”. The classification-based approaches such as by Chieu and Ng (2002) tend to outperform rule-based approaches. However, Ciravegna (2001) argued that it is difficult to examine the result obtained by classifiers. Thus, interpretability of the learned knowledge is a serious bottleneck of the classification approach. Additionally, Zhou and Su (2002) trained classifiers for Named Entity extraction and reported that performance degrades rapidly if the training corpus size is below 100KB. It implies that human experts have to spend long hours to annotate a sufficiently large amount of training corpus. Several recent researches focused on the extraction of relationships using classifiers. Roth and Yih (2002) learned the entities and relations together. The joint learning improves the performance of NE recognition in cases such as “X killed Y”. It also prevents the propagation of mistakes in NE extraction to the extraction of relations. Howev</context>
</contexts>
<marker>Zhou, Su, 2002</marker>
<rawString>G.D. Zhou and J. Su. 2002. Named entity recognition using an HMM-based chunk tagger. In Proc of ACL-2002, 473-480</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>