<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001024">
<title confidence="0.981965">
Hierarchical Non-Emitting Markov Models
</title>
<author confidence="0.970874">
Eric Sven Ristad and Robert G. Thomas
</author>
<affiliation confidence="0.9750905">
Department of Computer Science
Princeton University
</affiliation>
<address confidence="0.913446">
Princeton, NJ 08544-2087
</address>
<email confidence="0.978972">
{ristad,rgt}Ocs.princeton.edu
</email>
<sectionHeader confidence="0.993193" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999715">
We describe a simple variant of the inter-
polated Markov model with non-emitting
state transitions and prove that it is strictly
more powerful than any Markov model.
Empirical results demonstrate that the
non-emitting model outperforms the inter-
polated model on the Brown corpus and
on the Wall Street Journal under a wide
range of experimental conditions. The non-
emitting model is also much less prone to
overtraining.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999995307692308">
The Markov model has long been the core technol-
ogy of statistical language modeling. Many other
models have been proposed, but none has offered a
better combination of predictive performance, com-
putational efficiency, and ease of implementation.
Here we add hierarchical non-emitting state tran-
sitions to the Markov model. Although the states
in our model remain Markovian, the model itself
is no longer Markovian because it can represent
unbounded dependencies in the state distribution.
Consequently, the non-emitting Markov model is
strictly more powerful than any Markov model, in-
cluding the context model (Rissanen, 1983; Rissa-
nen, 1986), the backoff model (Cleary and Witten,
1984; Katz, 1987), and the interpolated Markov
model (Jelinek and Mercer, 1980; MacKay and Peto,
1994).
More importantly, the non-emitting model consis-
tently outperforms the interpolated Markov model
on natural language texts, under a wide range of
experimental conditions. We believe that the su-
perior performance of the non-emitting model is
due to its ability to better model conditional inde-
pendence. Thus, the non-emitting model is better
able to represent both conditional independence and
long-distance dependence, ie., it is simply a better
statistical model. The non-emitting model is also
nearly as computationally efficient and easy to im-
plement as the interpolated model.
The remainder of our article consists of four sec-
tions. In section 2, we review the interpolated
Markov model and briefly demonstrate that all inter-
polated models are equivalent to some basic Markov
model of the same model order. Next, we introduce
the hierarchical non-emitting Markov model in sec-
tion 3, and prove that even a lowly second order
non-emitting model is strictly more powerful than
any basic Markov model, of any model order. In
section 4, we report empirical results for the inter-
polated model and the non-emitting model on the
Brown corpus and Wall Street Journal. Finally, in
section 5 we conjecture that the empirical success of
the non-emitting model is due to its ability to bet-
ter model a point of apparent independence, such as
may occur at a sentence boundary.
Our notation is as follows. Let A be a finite alpha-
bet of distinct symbols, At = k, and let XT E AT
denote an arbitrary string of length T over the al-
phabet A. Then xi denotes the substring of XT that
begins at position i and ends at position j. For con-
venience, we abbreviate the unit length substring x:
as xi and the length t prefix of XT as xt.
</bodyText>
<sectionHeader confidence="0.971086" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.99993625">
Here we review the basic Markov model and the in-
terpolated Markov model, and establish their equiv-
alence.
A basic Markov model 0 = (A, n, on) consists of
an alphabet A, a model order n, n &gt; 0, and the
state transition probabilities 6„, : An x A [0, 1].
With probability 6„(yle), a Markov model in the
state xn will emit the symbol y and transition to the
state x`21y. Therefore, the probability prn(xtIxt-1, 0)
assigned by an order n basic Markov model 0 to a
symbol x&apos; in the history xt-i depends only on the
last n symbols of the history.
</bodyText>
<equation confidence="0.9075155">
pm(xiixt-i,
) (1)
</equation>
<bodyText confidence="0.9939084">
An interpolated Markov model 0 = (A, n, 6)
consists of a finite alphabet A, a maximal model or-
der n,. the state transition probabilities 6 = 6o • • •6n,
bi : A&apos; x A --■ [0, 1], and the state-conditional inter-
polation parameters A = Ao ... An, Ai : A&apos; —■ [0, 1].
</bodyText>
<page confidence="0.99766">
381
</page>
<bodyText confidence="0.999781666666667">
The probability assigned by an interpolated model
is a linear combination of the probabilities assigned
by all the lower order Markov models.
</bodyText>
<equation confidence="0.9982325">
pe(yixi , 0) = Ai(xi)bi (yle)
+(1 — )i(xi))pc(ylxf2, 0) (2)
</equation>
<bodyText confidence="0.995456444444444">
where Ai(xi) = 0 for i &gt; n, and and therefore
= pc (xt Ixtt-1, 0), ie., the prediction
depends only on the last n symbols of the history.
In the interpolated model, the interpolation pa-
rameters smooth the conditional probabilities esti-
mated from longer histories with those estimated
from shorter histories (Jelinek and Mercer, 1980).
Longer histories support stronger predictions, while
shorter histories have more accurate statistics. In-
terpolating the predictions from histories of different
lengths results in more accurate predictions than can
be obtained from any fixed history length.
A quick glance at the form of (2) and (1) re-
veals the fundamental simplicity of the interpolated
Markov model. Every interpolated model q5 is equiv-
alent to some basic Markov model 0&apos; (lemma 2.1),
and every basic Markov model 0 is equivalent to
some interpolated context model q5&apos; (lemma 2.2).
</bodyText>
<equation confidence="0.99461">
Lemma 2.1
V0 30&apos; VxT E A [pm(xTI.P&apos;,T) = pc(xTIO,T)]
</equation>
<bodyText confidence="0.9987055">
Proof. We may convert the interpolated model 0
into a basic model 0&apos; of the same model order n,
simply by setting 6,c(ylxn) equal to pc(ylx&amp;quot;, 0) for
all states xn E An and symbols y E A. 0
</bodyText>
<equation confidence="0.826729">
Lemma 2.2
VO 30&apos; VxT E A* [pc(xT ,T) = Pm(xTIO,T)]
</equation>
<bodyText confidence="0.991941833333333">
Proof. Every basic model is equivalent to an inter-
polated model whose interpolation values are unity
for states of order n. 0
The lemmas suffice to establish the following the-
orem.
Theorem 1 The class of interpolated Markov mod-
els is equivalent to the class of basic Markov models.
Proof. By lemmas 2.1 and 2.2. 0
A similar argument applies to the backoff model.
Every backoff model can be converted into an equiv-
alent basic model, and every basic model is a backoff
model.
</bodyText>
<sectionHeader confidence="0.974969" genericHeader="method">
3 Non-Emitting Markov Models
</sectionHeader>
<bodyText confidence="0.999834785714286">
A hierarchical non-emitting Markov model 0 =
(A, n, 6) consists of an alphabet A, a maximal
model order n, the state transition probabilities,
= 60 ...bn, bi : Ai x A [0, 1], and the non-
emitting state transition probabilities A = .Ao • • • A.,
Ai : A&apos; [0, 1]. With probability 1 — Ai(xs), a non-
emitting model will transition from the state x&apos; to
the state x&apos;2 without emitting a symbol. With proba-
bility A1(e)62(y1x2), anon-emitting model will tran-
sition from the state x&apos; to the state x&apos;y and emit the
symbol y.
Therefore, the probability p,(y1 Ix&apos;, (0) assigned to
a string yi in the history x&apos; by a non-emitting model
0 has the recursive form (3),
</bodyText>
<equation confidence="0.771434333333333">
Pe(Yj ixi,)= Ai(x&apos; )6i(yi lxi)Pc(9i2
+(1 )i(xj))p,(y)Ix&apos;2, 0)
(3)
</equation>
<bodyText confidence="0.9984515">
where Ai(xi) = 0 for i&gt; n and Ao(f) = 1. Note that,
unlike the basic Markov model, pc(xtlx&apos; ,c1))
pE(xtlxr_nl , 0) because the state distribution of the
non-emitting model depends on the prefix xi&apos;.
This simple fact will allow us to establish that there
exists a non-emitting model that is not equivalent to
any basic model.
Lemma 3.1 states that there exists a non-emitting
model 0 that cannot be converted into an equivalent
basic model of any order. There will always be a
string xT that distinguishes the non-emitting model
0 from any given basic model 0&apos; because the non-
emitting model can encode unbounded dependencies
in its state distribution.
</bodyText>
<equation confidence="0.981866">
Lemma 3.1
Bq5VqY 3xT EA* [236(xT , T) p(xT 10&apos; ,
</equation>
<bodyText confidence="0.9999588">
Proof. The idea of the proof is that our non-
emitting model will encode the first symbol x1 of
the string XT in its state distribution, for an un-
bounded distance. This will allow it to predict the
last symbol xT using its knowledge of the first sym-
bol xi. The basic model will only be able predict the
last symbol XT using the preceding n symbols, and
therefore when T is greater than n, we can arrange
for pc(xTIO,T) to differ from any p(xT , T), sim-
ply by our choice of xi.
The smallest non-emitting model capable of ex-
hibiting the required behavior has order 2. The
non-emitting transition probabilities A and the in-
terior of the string x1 will be chosen so that the
non-emitting model is either in an order 2 state or
an order 0 state, with no way to transition from one
to the other. The first symbol xi will determine
whether the non-emitting model goes to the order 2
state or stays in the order 0 state. No matter what
probability the basic model assigns to the final sym-
bol xT, the non-emitting model can assign a different
probability by the appropriate choice of xi,
and 62 (
Consider the second order non-emitting model
over a binary alphabet with A(0) = 1, A(1) = 0, and
A(11) = 1 on strings in Al&amp;quot; A. When xi = 0, then x2
will be predicted using the 1st order model 61(x2lx 1),
and all subsequent xt will be predicted by the second
order model b2(xt Ix). When xi = 0, then all sub-
sequent st will be predicted by the 0th order model
.50(xt). Thus for all t &gt; p, Pf(xtixt-1) pc(xtlxr_pl)
for any fixed p, and no basic model is equivalent to
this simple non-emitting model. 0
It is obvious that every basic model is also a non-
emitting model, with the appropriate choice of non-
</bodyText>
<page confidence="0.982757">
382
</page>
<bodyText confidence="0.69696">
emitting transition probabilities.
</bodyText>
<equation confidence="0.8636935">
Lemma 3.2
V42 34/ VxT E A* {p,(xT 10&apos; , T) p„,(xT ,
</equation>
<bodyText confidence="0.999020909090909">
These lemmas suffice to establish the following
theorem.
Theorem 2 The class of non-emitting Markov
models is strictly more powerful than the class of ba-
sic Markov models, because it is able to represent a
larger class of probability distributions on strings.
Proof. By lemmas 3.1 and 3.2. 0
Since interpolated models and backoff models are
equivalent to basic Markov models, we have as
a corollary that non-emitting Markov models are
strictly more powerful than interpolated models and
backoff models as well. Note that non-emitting
Markov models are considerably less powerful than
the full class of stochastic finite state automaton
(SFSA) because their states are Markovian. Non-
emitting models are also less powerful than the full
class of hidden Markov models.
Algorithms to evaluate the probability of a string
according to a non-emitting model, and to opti-
mize the non-emitting state transitions on a train-
ing corpus are provided in related work (Ristad and
Thomas, 1997).
</bodyText>
<sectionHeader confidence="0.997176" genericHeader="method">
4 Empirical Results
</sectionHeader>
<bodyText confidence="0.999975571428571">
The ultimate measure of a statistical model is its
predictive performance in the domain of interest.
To take the true measure of non-emitting models
for natural language texts, we evaluate their per-
formance as character models on the Brown corpus
(Francis and Kucera, 1982) and as word models on
the Wall Street Journal. Our results show that the
non-emitting Markov model consistently gives better
predictions than the traditional interpolated Markov
model under equivalent experimental conditions. In
all cases we compare non-emitting and interpolated
models of identical model orders, with the same
number of parameters. Note that the non-emitting
bigram and the interpolated bigram are equivalent.
</bodyText>
<table confidence="0.99788725">
Corpus Size Alphabet Blocks
Brown 6,004,032 90 21
WSJ 1989 6,219,350 20,293 22
WSJ 1987-89 42,373,513 20,092 152
</table>
<bodyText confidence="0.949588">
All ,\ values were initialized uniformly to 0.5 and
then optimized using deleted estimation on the first
90% of each corpus (Jelinek and Mercer, 1980).
</bodyText>
<equation confidence="0.299204">
DELETED-ESTIMATION(B4O)
1.Until convergence
</equation>
<listItem confidence="0.988682">
2. Initialize A+, A- to zero;
3. For each block Bi in B
4. Initialize 6 using B — Bi;
5. ExPEcTATi0N-sTEP(B1,42,)+,Al;
6. mAximizATIoN-sTEP(4),AtA);
7. Initialize 6 using B;
</listItem>
<bodyText confidence="0.998447857142857">
Here A+ (xi) accumulates the expectations of emit-
ting a, symbol from state xi while A- (e) accumu-
lates the expectations of transitioning to the state
e2 without emitting a symbol.
The remaining 10% percent of each corpus was
used to evaluate model performance. No parameter
tying was performed.&apos;
</bodyText>
<subsectionHeader confidence="0.99805">
4.1 Brown Corpus
</subsectionHeader>
<bodyText confidence="0.999667857142857">
Our first set of experiments were with character
models on the Brown corpus. The Brown cor-
pus is an eclectic collection of English prose, con-
taining 6,004,032 characters partitioned into 500
files. Deleted estimation used 21 blocks. Re-
sults are reported as per-character test message
entropies (bits/char), — log2 p(y° Iv). The non-
emitting model outperforms the interpolated model
for all nontrivial model orders, particularly for larger
model orders. The non-emitting model is consider-
ably less prone to overtraining. After 10 EM itera-
tions, the order 9 non-emitting model scores 2.0085
bits/char while the order 9 interpolated model scores
2.3338 bits/char after 10 EM iterations.
</bodyText>
<subsectionHeader confidence="0.461782">
Brown Corpus
</subsectionHeader>
<table confidence="0.9679464">
Non-omitting Modok Bost EM honks, Ar—
Intorpolatod Model: Beat EM horotion
Nan-omitting Moder: 10th EM Iteration .0-
Interpolated Modol: 10th EM Iteration —
2 3 4 Modo?Ordor 7
</table>
<figureCaption confidence="0.9795105">
Figure 1: Test message entropies as a function of
model order on the Brown corpus.
</figureCaption>
<subsectionHeader confidence="0.966706">
4.2 WSJ 1989
</subsectionHeader>
<bodyText confidence="0.995271214285714">
The second set of experiments was on the 1989
Wall Street Journal corpus, which contains 6,219,350
words. Our vocabulary consisted of the 20,293
words that occurred at least 10 times in the en-
tire WSJ 1989 corpus. All out-of-vocabulary words
&apos;In forthcoming work, we compare the performance of
the interpolated and non-emitting models on the Brown
corpus and Wall Street Journal with ten different pa-
rameter tying schemes. Our experiments confirm that
some parameter tying schemes improve model perfor-
mance, although only slightly. The non-emitting model
consistently outperformed the interpolated model on all
the corpora for all the parameter tying schemes that we
evaluated.
</bodyText>
<figure confidence="0.9951503">
Test Msssage Entropy (bits/char) 3.8
18
3.4
3.2
3
2.8
2.8
2.4
2.2
1.8
</figure>
<page confidence="0.980486">
383
</page>
<subsectionHeader confidence="0.442031">
Test Message Perplenety
</subsectionHeader>
<bodyText confidence="0.999707571428571">
were mapped to a unique 00V symbol. Deleted
estimation used 22 blocks. Following standard prac-
tice in the speech recognition community, results
are reported as per-word test message perplexities,
p(yv 1v)- . Again, the non-emitting model outper-
forms the interpolated Markov model for all nontriv-
ial model orders.
</bodyText>
<note confidence="0.629434">
WSJ 1989
</note>
<figureCaption confidence="0.9793065">
Figure 2: Test message perplexities as a function of
model order on WSJ 1989.
</figureCaption>
<figure confidence="0.982652">
3
Modol Order
</figure>
<figureCaption confidence="0.9896695">
Figure 3: Test message perplexities as a function of
model order on WSJ 1987-89.
</figureCaption>
<bodyText confidence="0.994761142857143">
for i &lt; n. To simplify the example, we assume that
6(yi lx7-1 y1-1) = 0 for j &gt; land i &lt; n.
In such a situation, the interpolated model must
repeatedly transition past some suffix of the history
xn-1 for each of the next n-1 predictions, and so the
total probability assigned to My&apos; le) by the interpo-
lated model is a product of n(n - 1)/2 probabilities.
</bodyText>
<figure confidence="0.980140026315789">
trr B EM r.:.
rp=2 B= ;tn
2
3
Model Order
4 5
WSJ 1987-89
Non-emitting Modet Best EM Itstation -.—
Intsmolstsd Model: Bost EPA Mershon
160
150
140
130
120
110
100
90
80
Test Message Peepleaky
180
170
160
150
140
130
120
110
100
pc(y.1
H - A(x7-1))1 p(yi le)
[n-1
1-1(1 - A(4-1Y1)) P(Y2IY1)
• • •
(1 - A(xn_orrl))p(yn
[n-1 n-1
H A(4-)(X1-1))] My&apos; le)
10=1
1.2
</figure>
<subsectionHeader confidence="0.822471">
4.3 WSJ 1987-89
</subsectionHeader>
<bodyText confidence="0.9999667">
The third set of experiments was on the 1987-89 Wall
Street Journal corpus, which contains 42,373,513
words. Our vocabulary consisted of the 20,092 words
that occurred at least 63 times in the entire WSJ
1987-89 corpus. Again, all out-of-vocabulary words
were mapped to a unique 00V symbol. Deleted es-
timation used 152 blocks. Results are reported as
test message perplexities. As with the WSJ 1989
corpus, the non-emitting model outperforms the in-
terpolated model for all nontrivial model orders.
</bodyText>
<sectionHeader confidence="0.996511" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.998698">
The power of the non-emitting model comes from
its ability to represent additional information in its
state distribution. In the proof of lemma 3.1 above,
we used the state distribution to represent a long dis-
tance dependency. We conjecture, however, that the
empirical success of the non-emitting model is due
to its ability to remember to ignore (ie., to forget) a
misleading history at a point of apparent indepen-
dence.
A point of apparent independence occurs when
we have adequate statistics for two strings xTh-1 and
yn but not yet for their concatenation xn-10. In
the most extreme case, the frequencies of xn-1 and
yr are high, but the frequency of even the medial
bigram xn_ in, is low. In such a situation, we would
like to ignore the entire history xn-1 when predicting
yn , because all 6(yi 14-1g1 -1) will be close to zero
(4)
In contrast, the non-emitting model will imme-
diately transition to the empty context in order to
predict the first symbol yi , and then it need never
again transition past any suffix of xn-1. Conse-
quently, the total probability assigned to pc(yn le)
by the non-emitting model is a product of only n 1
probabilities.
</bodyText>
<equation confidence="0.9364375">
n-1
PE(Yn IXn-1) = [H(1 - A(17-1))1 p(yl) (5)
</equation>
<bodyText confidence="0.999561428571429">
Given the same state transition probabilities, note
that (4) must be considerably less than (5) because
probabilities lie in [0, 1]. Thus, we believe that the
empirical success of the non-emitting model comes
from its ability to effectively ignore a misleading his-
tory rather than from its ability to remember distant
events.
</bodyText>
<page confidence="0.995865">
384
</page>
<bodyText confidence="0.99997475">
Finally, we note the use of hierarchical non-
emitting transitions is a general technique that may
be employed in any time series model, including con-
text models and backoff models.
</bodyText>
<sectionHeader confidence="0.97978" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998368666666667">
Both authors are partially supported by Young
Investigator Award IRI-0258517 to Eric Ristad from
the National Science Foundation.
</bodyText>
<sectionHeader confidence="0.997293" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.990563974358974">
Lalit R. Bahl, Peter F. Brown, Peter V. de Souza,
Robert L. Mercer, and David Nahamoo. 1991. A
fast algorithm for deleted interpolation. In Proc.
EUROSPEECH &apos;91, pages 1209-1212, Genoa.
J.G. Cleary and LH. Witten. 1984. Data com-
pression using adaptive coding and partial string
matching. IEEE Trans. Comm., COM-32(4):396-
402.
W. Nelson Francis and Henry Kucera. 1982. Fre-
quency analysis of English usage: lexicon and
grammar. Houghton Mifflin, Boston.
Fred Jelinek and Robert L. Mercer. 1980. Inter-
polated estimation of Markov source parameters
from sparse data. In Edzard S. Gelsema and
Laveen N. Kanal, editors, Pattern Recognition in
Practice, pages 381-397, Amsterdam, May 21-23.
North Holland.
Slava Katz. 1987. Estimation of probabilities from
sparse data for the language model component of
a speech recognizer. IEEE Trans. ASSP, 35:400-
401.
David J.C. MacKay and Linda C. Bauman Peto.
1994. A hierarchical Dirichlet language model.
Natural Language Engineering, 1(1).
Jorma Rissanen. 1983. A universal data compres-
sion system. IEEE Trans. Information Theory,
IT-29(5):656-664.
Jorma Rissanen. 1986. Complexity of strings in the
class of Markov sources. IEEE Trans. Information
Theory, IT-32(4):526-532.
Eric Sven Ristad and Robert G. Thomas. 1997. Hi-
erarchical non-emitting Markov models. Techni-
cal Report CS-TR-544-96, Department of Com-
puter Science, Princeton University, Princeton,
NJ, March.
Frans M. J. Willems, Yuri M. Shtarkov, and
Tjalling J. Tjalkens. 1995. The context-tree
weighting method: basic properties. IEEE Trans.
Inf. Theory, 41(3):653-664.
</reference>
<page confidence="0.999089">
385
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.912816">
<title confidence="0.999915">Hierarchical Non-Emitting Markov Models</title>
<author confidence="0.9998">Sven Ristad G Thomas</author>
<affiliation confidence="0.99995">Department of Computer Science Princeton University</affiliation>
<address confidence="0.999745">Princeton, NJ 08544-2087</address>
<email confidence="0.999842">ristadOcs.princeton.edu</email>
<email confidence="0.999842">rgtOcs.princeton.edu</email>
<abstract confidence="0.992665916666667">We describe a simple variant of the interpolated Markov model with non-emitting state transitions and prove that it is strictly more powerful than any Markov model. Empirical results demonstrate that the non-emitting model outperforms the interpolated model on the Brown corpus and on the Wall Street Journal under a wide range of experimental conditions. The nonemitting model is also much less prone to overtraining.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Lalit R Bahl</author>
<author>Peter F Brown</author>
<author>Peter V de Souza</author>
<author>Robert L Mercer</author>
<author>David Nahamoo</author>
</authors>
<title>A fast algorithm for deleted interpolation.</title>
<date>1991</date>
<booktitle>In Proc. EUROSPEECH &apos;91,</booktitle>
<pages>1209--1212</pages>
<location>Genoa.</location>
<marker>Bahl, Brown, de Souza, Mercer, Nahamoo, 1991</marker>
<rawString>Lalit R. Bahl, Peter F. Brown, Peter V. de Souza, Robert L. Mercer, and David Nahamoo. 1991. A fast algorithm for deleted interpolation. In Proc. EUROSPEECH &apos;91, pages 1209-1212, Genoa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Witten</author>
</authors>
<title>Data compression using adaptive coding and partial string matching.</title>
<date>1984</date>
<journal>IEEE Trans. Comm.,</journal>
<pages>32--4</pages>
<contexts>
<context position="1314" citStr="Witten, 1984" startWordPosition="193" endWordPosition="194"> modeling. Many other models have been proposed, but none has offered a better combination of predictive performance, computational efficiency, and ease of implementation. Here we add hierarchical non-emitting state transitions to the Markov model. Although the states in our model remain Markovian, the model itself is no longer Markovian because it can represent unbounded dependencies in the state distribution. Consequently, the non-emitting Markov model is strictly more powerful than any Markov model, including the context model (Rissanen, 1983; Rissanen, 1986), the backoff model (Cleary and Witten, 1984; Katz, 1987), and the interpolated Markov model (Jelinek and Mercer, 1980; MacKay and Peto, 1994). More importantly, the non-emitting model consistently outperforms the interpolated Markov model on natural language texts, under a wide range of experimental conditions. We believe that the superior performance of the non-emitting model is due to its ability to better model conditional independence. Thus, the non-emitting model is better able to represent both conditional independence and long-distance dependence, ie., it is simply a better statistical model. The non-emitting model is also nearl</context>
</contexts>
<marker>Witten, 1984</marker>
<rawString>J.G. Cleary and LH. Witten. 1984. Data compression using adaptive coding and partial string matching. IEEE Trans. Comm., COM-32(4):396-402.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Nelson Francis</author>
<author>Henry Kucera</author>
</authors>
<title>Frequency analysis of English usage: lexicon and grammar.</title>
<date>1982</date>
<location>Houghton Mifflin, Boston.</location>
<contexts>
<context position="10316" citStr="Francis and Kucera, 1982" startWordPosition="1785" endWordPosition="1788">se their states are Markovian. Nonemitting models are also less powerful than the full class of hidden Markov models. Algorithms to evaluate the probability of a string according to a non-emitting model, and to optimize the non-emitting state transitions on a training corpus are provided in related work (Ristad and Thomas, 1997). 4 Empirical Results The ultimate measure of a statistical model is its predictive performance in the domain of interest. To take the true measure of non-emitting models for natural language texts, we evaluate their performance as character models on the Brown corpus (Francis and Kucera, 1982) and as word models on the Wall Street Journal. Our results show that the non-emitting Markov model consistently gives better predictions than the traditional interpolated Markov model under equivalent experimental conditions. In all cases we compare non-emitting and interpolated models of identical model orders, with the same number of parameters. Note that the non-emitting bigram and the interpolated bigram are equivalent. Corpus Size Alphabet Blocks Brown 6,004,032 90 21 WSJ 1989 6,219,350 20,293 22 WSJ 1987-89 42,373,513 20,092 152 All ,\ values were initialized uniformly to 0.5 and then o</context>
</contexts>
<marker>Francis, Kucera, 1982</marker>
<rawString>W. Nelson Francis and Henry Kucera. 1982. Frequency analysis of English usage: lexicon and grammar. Houghton Mifflin, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fred Jelinek</author>
<author>Robert L Mercer</author>
</authors>
<title>Interpolated estimation of Markov source parameters from sparse data.</title>
<date>1980</date>
<booktitle>Pattern Recognition in Practice,</booktitle>
<pages>381--397</pages>
<editor>In Edzard S. Gelsema and Laveen N. Kanal, editors,</editor>
<publisher>North Holland.</publisher>
<location>Amsterdam,</location>
<contexts>
<context position="1388" citStr="Jelinek and Mercer, 1980" startWordPosition="202" endWordPosition="205">offered a better combination of predictive performance, computational efficiency, and ease of implementation. Here we add hierarchical non-emitting state transitions to the Markov model. Although the states in our model remain Markovian, the model itself is no longer Markovian because it can represent unbounded dependencies in the state distribution. Consequently, the non-emitting Markov model is strictly more powerful than any Markov model, including the context model (Rissanen, 1983; Rissanen, 1986), the backoff model (Cleary and Witten, 1984; Katz, 1987), and the interpolated Markov model (Jelinek and Mercer, 1980; MacKay and Peto, 1994). More importantly, the non-emitting model consistently outperforms the interpolated Markov model on natural language texts, under a wide range of experimental conditions. We believe that the superior performance of the non-emitting model is due to its ability to better model conditional independence. Thus, the non-emitting model is better able to represent both conditional independence and long-distance dependence, ie., it is simply a better statistical model. The non-emitting model is also nearly as computationally efficient and easy to implement as the interpolated m</context>
<context position="4501" citStr="Jelinek and Mercer, 1980" startWordPosition="754" endWordPosition="757"> and the state-conditional interpolation parameters A = Ao ... An, Ai : A&apos; —■ [0, 1]. 381 The probability assigned by an interpolated model is a linear combination of the probabilities assigned by all the lower order Markov models. pe(yixi , 0) = Ai(xi)bi (yle) +(1 — )i(xi))pc(ylxf2, 0) (2) where Ai(xi) = 0 for i &gt; n, and and therefore = pc (xt Ixtt-1, 0), ie., the prediction depends only on the last n symbols of the history. In the interpolated model, the interpolation parameters smooth the conditional probabilities estimated from longer histories with those estimated from shorter histories (Jelinek and Mercer, 1980). Longer histories support stronger predictions, while shorter histories have more accurate statistics. Interpolating the predictions from histories of different lengths results in more accurate predictions than can be obtained from any fixed history length. A quick glance at the form of (2) and (1) reveals the fundamental simplicity of the interpolated Markov model. Every interpolated model q5 is equivalent to some basic Markov model 0&apos; (lemma 2.1), and every basic Markov model 0 is equivalent to some interpolated context model q5&apos; (lemma 2.2). Lemma 2.1 V0 30&apos; VxT E A [pm(xTI.P&apos;,T) = pc(xTIO</context>
<context position="11008" citStr="Jelinek and Mercer, 1980" startWordPosition="1889" endWordPosition="1892">at the non-emitting Markov model consistently gives better predictions than the traditional interpolated Markov model under equivalent experimental conditions. In all cases we compare non-emitting and interpolated models of identical model orders, with the same number of parameters. Note that the non-emitting bigram and the interpolated bigram are equivalent. Corpus Size Alphabet Blocks Brown 6,004,032 90 21 WSJ 1989 6,219,350 20,293 22 WSJ 1987-89 42,373,513 20,092 152 All ,\ values were initialized uniformly to 0.5 and then optimized using deleted estimation on the first 90% of each corpus (Jelinek and Mercer, 1980). DELETED-ESTIMATION(B4O) 1.Until convergence 2. Initialize A+, A- to zero; 3. For each block Bi in B 4. Initialize 6 using B — Bi; 5. ExPEcTATi0N-sTEP(B1,42,)+,Al; 6. mAximizATIoN-sTEP(4),AtA); 7. Initialize 6 using B; Here A+ (xi) accumulates the expectations of emitting a, symbol from state xi while A- (e) accumulates the expectations of transitioning to the state e2 without emitting a symbol. The remaining 10% percent of each corpus was used to evaluate model performance. No parameter tying was performed.&apos; 4.1 Brown Corpus Our first set of experiments were with character models on the Brow</context>
</contexts>
<marker>Jelinek, Mercer, 1980</marker>
<rawString>Fred Jelinek and Robert L. Mercer. 1980. Interpolated estimation of Markov source parameters from sparse data. In Edzard S. Gelsema and Laveen N. Kanal, editors, Pattern Recognition in Practice, pages 381-397, Amsterdam, May 21-23. North Holland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slava Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recognizer.</title>
<date>1987</date>
<journal>IEEE Trans. ASSP,</journal>
<pages>35--400</pages>
<contexts>
<context position="1327" citStr="Katz, 1987" startWordPosition="195" endWordPosition="196">y other models have been proposed, but none has offered a better combination of predictive performance, computational efficiency, and ease of implementation. Here we add hierarchical non-emitting state transitions to the Markov model. Although the states in our model remain Markovian, the model itself is no longer Markovian because it can represent unbounded dependencies in the state distribution. Consequently, the non-emitting Markov model is strictly more powerful than any Markov model, including the context model (Rissanen, 1983; Rissanen, 1986), the backoff model (Cleary and Witten, 1984; Katz, 1987), and the interpolated Markov model (Jelinek and Mercer, 1980; MacKay and Peto, 1994). More importantly, the non-emitting model consistently outperforms the interpolated Markov model on natural language texts, under a wide range of experimental conditions. We believe that the superior performance of the non-emitting model is due to its ability to better model conditional independence. Thus, the non-emitting model is better able to represent both conditional independence and long-distance dependence, ie., it is simply a better statistical model. The non-emitting model is also nearly as computat</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Slava Katz. 1987. Estimation of probabilities from sparse data for the language model component of a speech recognizer. IEEE Trans. ASSP, 35:400-401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David J C MacKay</author>
<author>Linda C Bauman Peto</author>
</authors>
<title>A hierarchical Dirichlet language model.</title>
<date>1994</date>
<journal>Natural Language Engineering,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="1412" citStr="MacKay and Peto, 1994" startWordPosition="206" endWordPosition="209">on of predictive performance, computational efficiency, and ease of implementation. Here we add hierarchical non-emitting state transitions to the Markov model. Although the states in our model remain Markovian, the model itself is no longer Markovian because it can represent unbounded dependencies in the state distribution. Consequently, the non-emitting Markov model is strictly more powerful than any Markov model, including the context model (Rissanen, 1983; Rissanen, 1986), the backoff model (Cleary and Witten, 1984; Katz, 1987), and the interpolated Markov model (Jelinek and Mercer, 1980; MacKay and Peto, 1994). More importantly, the non-emitting model consistently outperforms the interpolated Markov model on natural language texts, under a wide range of experimental conditions. We believe that the superior performance of the non-emitting model is due to its ability to better model conditional independence. Thus, the non-emitting model is better able to represent both conditional independence and long-distance dependence, ie., it is simply a better statistical model. The non-emitting model is also nearly as computationally efficient and easy to implement as the interpolated model. The remainder of o</context>
</contexts>
<marker>MacKay, Peto, 1994</marker>
<rawString>David J.C. MacKay and Linda C. Bauman Peto. 1994. A hierarchical Dirichlet language model. Natural Language Engineering, 1(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorma Rissanen</author>
</authors>
<title>A universal data compression system.</title>
<date>1983</date>
<journal>IEEE Trans. Information Theory,</journal>
<pages>29--5</pages>
<contexts>
<context position="1253" citStr="Rissanen, 1983" startWordPosition="183" endWordPosition="184">model has long been the core technology of statistical language modeling. Many other models have been proposed, but none has offered a better combination of predictive performance, computational efficiency, and ease of implementation. Here we add hierarchical non-emitting state transitions to the Markov model. Although the states in our model remain Markovian, the model itself is no longer Markovian because it can represent unbounded dependencies in the state distribution. Consequently, the non-emitting Markov model is strictly more powerful than any Markov model, including the context model (Rissanen, 1983; Rissanen, 1986), the backoff model (Cleary and Witten, 1984; Katz, 1987), and the interpolated Markov model (Jelinek and Mercer, 1980; MacKay and Peto, 1994). More importantly, the non-emitting model consistently outperforms the interpolated Markov model on natural language texts, under a wide range of experimental conditions. We believe that the superior performance of the non-emitting model is due to its ability to better model conditional independence. Thus, the non-emitting model is better able to represent both conditional independence and long-distance dependence, ie., it is simply a b</context>
</contexts>
<marker>Rissanen, 1983</marker>
<rawString>Jorma Rissanen. 1983. A universal data compression system. IEEE Trans. Information Theory, IT-29(5):656-664.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorma Rissanen</author>
</authors>
<title>Complexity of strings in the class of Markov sources.</title>
<date>1986</date>
<journal>IEEE Trans. Information Theory,</journal>
<pages>32--4</pages>
<contexts>
<context position="1270" citStr="Rissanen, 1986" startWordPosition="185" endWordPosition="187">een the core technology of statistical language modeling. Many other models have been proposed, but none has offered a better combination of predictive performance, computational efficiency, and ease of implementation. Here we add hierarchical non-emitting state transitions to the Markov model. Although the states in our model remain Markovian, the model itself is no longer Markovian because it can represent unbounded dependencies in the state distribution. Consequently, the non-emitting Markov model is strictly more powerful than any Markov model, including the context model (Rissanen, 1983; Rissanen, 1986), the backoff model (Cleary and Witten, 1984; Katz, 1987), and the interpolated Markov model (Jelinek and Mercer, 1980; MacKay and Peto, 1994). More importantly, the non-emitting model consistently outperforms the interpolated Markov model on natural language texts, under a wide range of experimental conditions. We believe that the superior performance of the non-emitting model is due to its ability to better model conditional independence. Thus, the non-emitting model is better able to represent both conditional independence and long-distance dependence, ie., it is simply a better statistical</context>
</contexts>
<marker>Rissanen, 1986</marker>
<rawString>Jorma Rissanen. 1986. Complexity of strings in the class of Markov sources. IEEE Trans. Information Theory, IT-32(4):526-532.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Sven Ristad</author>
<author>Robert G Thomas</author>
</authors>
<title>Hierarchical non-emitting Markov models.</title>
<date>1997</date>
<tech>Technical Report CS-TR-544-96,</tech>
<institution>Department of Computer Science, Princeton University,</institution>
<location>Princeton, NJ,</location>
<contexts>
<context position="10021" citStr="Ristad and Thomas, 1997" startWordPosition="1738" endWordPosition="1741"> basic Markov models, we have as a corollary that non-emitting Markov models are strictly more powerful than interpolated models and backoff models as well. Note that non-emitting Markov models are considerably less powerful than the full class of stochastic finite state automaton (SFSA) because their states are Markovian. Nonemitting models are also less powerful than the full class of hidden Markov models. Algorithms to evaluate the probability of a string according to a non-emitting model, and to optimize the non-emitting state transitions on a training corpus are provided in related work (Ristad and Thomas, 1997). 4 Empirical Results The ultimate measure of a statistical model is its predictive performance in the domain of interest. To take the true measure of non-emitting models for natural language texts, we evaluate their performance as character models on the Brown corpus (Francis and Kucera, 1982) and as word models on the Wall Street Journal. Our results show that the non-emitting Markov model consistently gives better predictions than the traditional interpolated Markov model under equivalent experimental conditions. In all cases we compare non-emitting and interpolated models of identical mode</context>
</contexts>
<marker>Ristad, Thomas, 1997</marker>
<rawString>Eric Sven Ristad and Robert G. Thomas. 1997. Hierarchical non-emitting Markov models. Technical Report CS-TR-544-96, Department of Computer Science, Princeton University, Princeton, NJ, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frans M J Willems</author>
<author>Yuri M Shtarkov</author>
<author>Tjalling J Tjalkens</author>
</authors>
<title>The context-tree weighting method: basic properties.</title>
<date>1995</date>
<journal>IEEE Trans. Inf. Theory,</journal>
<pages>41--3</pages>
<marker>Willems, Shtarkov, Tjalkens, 1995</marker>
<rawString>Frans M. J. Willems, Yuri M. Shtarkov, and Tjalling J. Tjalkens. 1995. The context-tree weighting method: basic properties. IEEE Trans. Inf. Theory, 41(3):653-664.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>