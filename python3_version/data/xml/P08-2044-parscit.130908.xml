<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.054827">
<title confidence="0.995859">
Distributed Listening: A Parallel Processing Approach to Automatic
Speech Recognition
</title>
<author confidence="0.986061">
Yolanda McMillian Juan E. Gilbert
</author>
<affiliation confidence="0.940801">
3101 Shelby Center 3101 Shelby Center
Auburn University Auburn University
Auburn, AL 36849-5347, USA Auburn, AL 36849-5347, USA
</affiliation>
<email confidence="0.999177">
mcmilym@auburn.edu glbert@auburn.edu
</email>
<sectionHeader confidence="0.99841" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.935938722222222">
While speech recognition systems have come
a long way in the last thirty years, there is still
room for improvement. Although readily
available, these systems are sometimes inac-
curate and insufficient. The research pre-
sented here outlines a technique called
Distributed Listening which demonstrates no-
ticeable improvements to existing speech rec-
ognition methods. The Distributed Listening
architecture introduces the idea of multiple,
parallel, yet physically separate automatic
speech recognizers called listeners. Distrib-
uted Listening also uses a piece of middleware
called an interpreter. The interpreter resolves
multiple interpretations using the Phrase
Resolution Algorithm (PRA). These efforts
work together to increase the accuracy of the
transcription of spoken utterances.
</bodyText>
<sectionHeader confidence="0.971984" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.9997286">
Research in the area of natural language processing
has been on-going for over thirty years (Natural
Language Software Registry, 2004; Jurafsky and
Martin, 2000); however, there is still room for im-
provement with mainstream speech recognition
systems (Deng, 2004). Distributed Listening will
further research in this area. The concept is based
around the idea of multiple speech input sources.
Previous research activities involved a single mi-
crophone with multiple, separate recognizers that
all yielded improvements in accuracy. Distributed
Listening uses multiple, parallel speech recogniz-
ers, with each recognizer having its own input
source (Gilbert, 2005). Each recognizer is a lis-
tener. Once input is collected from the listeners,
one machine, the interpreter, processes all of the
input (see figure 1). To process the input, a phrase
resolution algorithm is used.
This approach is analogous to a crime scene
with multiple witnesses (the listeners) and a detec-
tive (the interpreter) who pieces together the sto-
ries of the witnesses using his/her knowledge of
crime scenes to form a hypothesis of the actual
event. Each witness will have a portion of the
story that is the same as the other witnesses. It is
up to the detective to fill in the blanks. With Dis-
tributed Listening, the process is very similar.
Each listener will have common recognition results
and the interpreter will use the phrase resolution
algorithm to resolve conflicts.
</bodyText>
<figureCaption confidence="0.996908">
Figure 1. Distributed Listening Architecture
</figureCaption>
<page confidence="0.984772">
173
</page>
<reference confidence="0.360175">
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 173–176,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</reference>
<footnote confidence="0.448325">
2.3 Recognized Output Voting Error Re-
duction
2 Background
</footnote>
<bodyText confidence="0.997578111111111">
Automatic speech recognition systems convert a
speech signal into a sequence of words, usually
based on the Hidden Markov Model (HMM), in
which words are constructed from a sequence of
states (Baum, 1972; Young et al., 1989; Young
1990; Furui, 2002).
There are several systems that used the HMM
along with multiple speech recognizers in an effort
to improve speech recognition, as discussed next.
</bodyText>
<sectionHeader confidence="0.814404" genericHeader="method">
2. 1 Enhanced Majority Rules
</sectionHeader>
<bodyText confidence="0.999783894736842">
Barry (et al., 1994) took three different Automatic
Speech Recognition (ASR) systems, along with an
Enhanced Majority Rules (EMR) software
algorithm. Each of the three individual systems
received the same input, performed speech
recognition, and sent the result to the master
system.
The EMR resolved inconsistencies by looking
for agreement from the individual systems for the
recognized word. If there was no majority agree-
ment, the EMR looked to the second word for
agreement before relying on the distance scores.
This architecture produced better recognition accu-
racy than each of the individual systems.
While an improvement was made, the architec-
ture can suffer from distorted input. Since each
system receives the same input, if the input signal
is not good, then all of the individual systems will
receive bad input.
</bodyText>
<subsectionHeader confidence="0.997071">
2.2 Virtual Intelligent Codriver
</subsectionHeader>
<bodyText confidence="0.999826892857143">
The Virtual Intelligent Codriver (VICO) project
also used multiple ASR systems in parallel (Brutti
et al., 2004; Cristoforetti et al., 2003). Each ASR
received the same input and had its own language
model. The resulting interpretations from each
ASR are compared to each other using confidence
scores. The interpretation with the highest
recognition accuracy is selected. While the
experiments resulted in noticeable improvements
over the individual ASR systems, there are two
shortcomings. First, if the input signal is distorted,
then each recognizer will receive bad input.
Secondly, if each recognizer contains a piece of the
optimal interpretation, then this architecture falls
short.
The Recognizer Output Voting Error Reduction
(ROVER) system is a composite of multiple ASR
systems that uses a voting process to reconcile
differences in the individual ASR system outputs
(Fiscus, 1997). Multiple interpretations are passed
from each recognition engine to the alignment
module. Once aligned, the voting module is
called. The voting module scores each word
within the alignment vertically and the words with
the highest scores are chosen. On average, this
composite ASR system produces a lower error rate
than any of the individual systems, but suffers
from order of combination and ties.
</bodyText>
<subsectionHeader confidence="0.956958">
2.4 Modified ROVER
</subsectionHeader>
<bodyText confidence="0.999727875">
To solve the problem that results from the order of
combination and ties of the original ROVER
system, Schwenk proposed a modified ROVER
system that used a dynamic programming
algorithm built on language models (Schwenk and
Gauvain, 2000). The modified ROVER system
resulted in a reduction in the word error rates over
the original ROVER system.
</bodyText>
<sectionHeader confidence="0.997689" genericHeader="method">
3 Distributed Listening
</sectionHeader>
<bodyText confidence="0.985639333333333">
Distributed Listening builds on the architectures
that use multiple speech recognizers and enhances
it with the use of multiple input sources.
Distributed Listening is made of three signifi-
cant parts: Listeners, an Interpreter, and a Phrase
Resolution Algorithm.
</bodyText>
<sectionHeader confidence="0.974802" genericHeader="method">
3. 1 Listeners
</sectionHeader>
<bodyText confidence="0.998268875">
Distributed Listening uses multiple speech recog-
nizers, working in parallel, to process the spoken
input. Each recognizer is called a listener and is
equipped with it&apos;s own input source. Each listener
is a separate, physical computing device with its
own memory, processor, and disk space. Each lis-
tener collects input from the user. The result of
each listener is passed to the interpreter.
</bodyText>
<subsectionHeader confidence="0.911854">
3.2 Interpreter
</subsectionHeader>
<bodyText confidence="0.982778">
Once input is collected from the listeners, the input
is passed to the interpreter. The interpreter will
</bodyText>
<page confidence="0.99656">
174
</page>
<bodyText confidence="0.998319">
process all of the input collected from each listener
as described next.
</bodyText>
<subsectionHeader confidence="0.999114">
3.3 Phrase Resolution Algorithm
</subsectionHeader>
<bodyText confidence="0.99998803030303">
To resolve multiple interpretations from the listen-
ers, the Phrase Resolution Algorithm (PRA) is
used.
The underlying grammar of the PRA is based on
an N-gram language model. An N-gram language
model is used by the recognizer to predict word
sequences. Distributed Listening uses an N-gram
of size 1, also known as a unigram. The grammar
consists of known utterances that can be made by
the user.
The unigram grammar is stored in a phrase
database. The grammar is organized according to
individual words and phrases. Each phrase is
placed in a table. The phrases are broken down
into their individual words and placed in another
table. The table of words keeps a count of the
number of times each word appears in each phrase,
resembling the unigram language model.
To determine the most likely spoken phrase,
queries are made against the collection of individ-
ual words, also known as the complete word set.
The queries try to identify matching phrase(s)
based on specified words. The matching phrase(s)
with the highest concentrations of words is re-
turned by the query.
The word concentration is determined by com-
paring the length of the phrase with the number of
matching words found in the complete word set.
The concentration of the number of words found
within each phrase is calculated using all interpre-
tations from the listeners. The phrase(s) with the
highest concentration of words is the most likely
spoken phrase.
</bodyText>
<sectionHeader confidence="0.9871" genericHeader="method">
4 System Architecture
</sectionHeader>
<bodyText confidence="0.999819285714286">
There are multiple models for Distributed Listen-
ing; Homogeneous, Heterogeneous, and Hybrid.
The Homogeneous model uses the same grammar
for each listener. Within the Heterogeneous
model, each listener uses a different grammar. The
Hybrid model contains a combination of the Ho-
mogenous and Heterogeneous models.
</bodyText>
<sectionHeader confidence="0.845513" genericHeader="method">
4. 1 Homogeneous
</sectionHeader>
<bodyText confidence="0.999642">
In a homogenous Distributed Listening architec-
ture, each listener has the same grammar or lan-
guage model. Although all of the listeners are
identical in capturing the input, this architecture
allows for the different perspectives of the utter-
ances to also be captured.
</bodyText>
<subsectionHeader confidence="0.990931">
4.2 Heterogeneous
</subsectionHeader>
<bodyText confidence="0.999400285714286">
Heterogeneous architectures use different gram-
mars or language models on each listener. Each
listener has its own input source and recognizer
and implies a distributed grammar/language model.
This allows for flexibility as very large grammars
and vocabularies can be distributed across several
listeners.
</bodyText>
<subsectionHeader confidence="0.997933">
4.3 Hybrid
</subsectionHeader>
<bodyText confidence="0.999710833333333">
The hybrid architecture is a homogenous architec-
ture of heterogeneous Distributed Listening nodes,
as shown in figure 2. This gives the embedded
environment the ability to recognize multiple lan-
guages, as well as accommodate translations of
inter-mixed spoken language.
</bodyText>
<figureCaption confidence="0.986055">
Figure 2. Hybrid Distributed Listening Architecture
</figureCaption>
<sectionHeader confidence="0.995453" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.998786">
The goal of Distributed Listening research is to
take a unique approach in order to enhance the
success of the traditional approaches to speech
recognition. The approach of Distributed Listen-
ing directly mimics people. The psychology do-
main has shown that people use a form of
Distributed Listening called Dichotic Listening,
where people listen to two voices, one in each ear,
</bodyText>
<page confidence="0.995933">
175
</page>
<bodyText confidence="0.992569166666667">
at the same time (Bruder, 2004). Distributed Lis-
tening is a natural extension of Dichotic Listening,
where computers are listening in the same manner
as people. Distributed Listening is an attempt to
enable computer systems to perform similar to
humans while decreasing error rates.
Preliminary studies have shown a decrease in
error rates. Early results indicate that Distributed
Listening is a viable alternative to current speech
recognition systems. Additional studies are being
planned that will effectively test the Phrase
Resolution Algorithm.
</bodyText>
<sectionHeader confidence="0.998287" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999285672413793">
Barry, T., Solz, T., Reising, J. &amp; Williamson, D. The
simultaneous use of three machine speech recogni-
tion systems to increase recognition accuracy, In
Proceedings of the IEEE 1994 National Aerospace
and Electronics Conference, vol.2, pp. 667 - 671,
1994.
Baum, L.E. An inequality and associated maximiza-
tion technique in statistical estimation for prob-
abilistic functions of Markov process. Inequalities
3, 1-8, 1972.
Bruder, G.E., Stewart, J.W., McGrath, P.J., Deliyan-
nides, D., Quitkin, F.M. Dichotic listening tests of
functional brain asymmetry predict response to
fluoxetine in depressed women and men. Neuro-
psychopharmacology, 29(9), pp. 1752-1761, 2004.
Brutti, A., Coletti, P., Cristoforetti, L., Geutner, P.,
Giacomini, A., Gretter, R., et al. Use of Multiple
Speech Recognition Units in a In-car Assistance
Systems, chapter in &amp;quot;DSP for Vehicle and Mobile
Systems&amp;quot;, Kluwer Publishers, 2004.
Cristoforetti, L., Matassoni, M., Omologo, M. &amp;
Svaizer, P., Use of parallel recognizers for robust
in-car speech interaction, In Proceedings of the
IEEE International Conference on Acoustic, Speech,
and Signal Processing [ICASSP 2003], Hong-Kong,
2003.
Deng, L. &amp; Huang, X., Challenges in adopting speech
recognition, Communications of the ACM, vol. 47,
no. 1, pp. 69-75, January 2004.
Fiscus, J. G., A post-processing system to yield re-
duced error word rates: Recognizer output voting
error reduction (ROVER). In IEEE Workshop on
Automatic Speech Recognition and Understanding,
pp. 347-354, 1997.
Furui, S., Recent progress in spontaneous speech rec-
ognition and understanding, In Proceedings of the
IEEE Workshop on Multimedia Signal Processing,
2002.
Gilbert, J. E. (2005). Distributed Listening Research.
In Proceedings of AVIOS Speech Technology Track,
San Francisco, California, SpeechTEK West, pp. 1 —
10.
Jurafsky, D. &amp; Martin, J., Speech and Language Proc-
essing, Prentice Hall, 2000.
Natural Language Software Registry, [Online]. Availa-
ble: http://registry.dfki.de/, 2004.
Schwenk, H. &amp; Gauvain, J., Improved ROVER using
Language Model Information, In ISCA ITRW
Workshop on Automatic Speech Recognition: Chal-
lenges for the new Millenium, Paris, pp. 47-52,
2000.
Young, S.R., Use of dialog, pragmatics and semantics
to enhance speech recognition, Speech Communi-
cation, vol. 9, pp. 551-564, 1990.
Young, S.R., Hauptmann, A.G. , Ward, W.H. , Smith,
E.T. &amp; Werner, P., High level knowledge sources in
usable speech recognition systems, Communica-
tions of the ACM, vol. 31, no. 2, pp. 183-194, 1989.
</reference>
<page confidence="0.998738">
176
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.674073">
<title confidence="0.947084">Distributed Listening: A Parallel Processing Approach to Automatic Speech Recognition</title>
<author confidence="0.9999">Yolanda McMillian Juan E Gilbert</author>
<affiliation confidence="0.8778705">3101 Shelby Center 3101 Shelby Center Auburn University Auburn University</affiliation>
<address confidence="0.998809">Auburn, AL 36849-5347, USA Auburn, AL 36849-5347, USA</address>
<email confidence="0.99976">mcmilym@auburn.eduglbert@auburn.edu</email>
<abstract confidence="0.999833157894737">While speech recognition systems have come a long way in the last thirty years, there is still room for improvement. Although readily available, these systems are sometimes inaccurate and insufficient. The research presented here outlines a technique called Distributed Listening which demonstrates noticeable improvements to existing speech recognition methods. The Distributed Listening architecture introduces the idea of multiple, parallel, yet physically separate automatic speech recognizers called listeners. Distributed Listening also uses a piece of middleware called an interpreter. The interpreter resolves multiple interpretations using the Phrase Resolution Algorithm (PRA). These efforts work together to increase the accuracy of the transcription of spoken utterances.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>Proceedings of ACL-08: HLT, Short Papers (Companion Volume),</booktitle>
<pages>173--176</pages>
<marker></marker>
<rawString>Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 173–176,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Columbus</author>
</authors>
<date>2008</date>
<booktitle>c�2008 Association for Computational Linguistics</booktitle>
<location>Ohio, USA,</location>
<marker>Columbus, 2008</marker>
<rawString>Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Barry</author>
<author>T Solz</author>
<author>J Reising</author>
<author>D Williamson</author>
</authors>
<title>The simultaneous use of three machine speech recognition systems to increase recognition accuracy,</title>
<date>1994</date>
<booktitle>In Proceedings of the IEEE 1994 National Aerospace and Electronics Conference, vol.2,</booktitle>
<pages>667--671</pages>
<marker>Barry, Solz, Reising, Williamson, 1994</marker>
<rawString>Barry, T., Solz, T., Reising, J. &amp; Williamson, D. The simultaneous use of three machine speech recognition systems to increase recognition accuracy, In Proceedings of the IEEE 1994 National Aerospace and Electronics Conference, vol.2, pp. 667 - 671, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L E Baum</author>
</authors>
<title>An inequality and associated maximization technique in statistical estimation for probabilistic functions of Markov process.</title>
<date>1972</date>
<journal>Inequalities</journal>
<volume>3</volume>
<pages>1--8</pages>
<marker>Baum, 1972</marker>
<rawString>Baum, L.E. An inequality and associated maximization technique in statistical estimation for probabilistic functions of Markov process. Inequalities 3, 1-8, 1972.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Bruder</author>
<author>J W Stewart</author>
<author>P J McGrath</author>
<author>D Deliyannides</author>
<author>F M Quitkin</author>
</authors>
<title>Dichotic listening tests of functional brain asymmetry predict response to fluoxetine in depressed women and men.</title>
<date>2004</date>
<journal>Neuropsychopharmacology,</journal>
<volume>29</volume>
<issue>9</issue>
<pages>1752--1761</pages>
<marker>Bruder, Stewart, McGrath, Deliyannides, Quitkin, 2004</marker>
<rawString>Bruder, G.E., Stewart, J.W., McGrath, P.J., Deliyannides, D., Quitkin, F.M. Dichotic listening tests of functional brain asymmetry predict response to fluoxetine in depressed women and men. Neuropsychopharmacology, 29(9), pp. 1752-1761, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Brutti</author>
<author>P Coletti</author>
<author>L Cristoforetti</author>
<author>P Geutner</author>
<author>A Giacomini</author>
<author>R Gretter</author>
</authors>
<title>Use of Multiple Speech Recognition Units in a In-car Assistance Systems, chapter in &amp;quot;DSP for Vehicle and Mobile Systems&amp;quot;,</title>
<date>2004</date>
<publisher>Kluwer Publishers,</publisher>
<marker>Brutti, Coletti, Cristoforetti, Geutner, Giacomini, Gretter, 2004</marker>
<rawString>Brutti, A., Coletti, P., Cristoforetti, L., Geutner, P., Giacomini, A., Gretter, R., et al. Use of Multiple Speech Recognition Units in a In-car Assistance Systems, chapter in &amp;quot;DSP for Vehicle and Mobile Systems&amp;quot;, Kluwer Publishers, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Cristoforetti</author>
<author>M Matassoni</author>
<author>M Omologo</author>
<author>P Svaizer</author>
</authors>
<title>Use of parallel recognizers for robust in-car speech interaction,</title>
<date>2003</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustic, Speech, and Signal Processing [ICASSP 2003], Hong-Kong,</booktitle>
<marker>Cristoforetti, Matassoni, Omologo, Svaizer, 2003</marker>
<rawString>Cristoforetti, L., Matassoni, M., Omologo, M. &amp; Svaizer, P., Use of parallel recognizers for robust in-car speech interaction, In Proceedings of the IEEE International Conference on Acoustic, Speech, and Signal Processing [ICASSP 2003], Hong-Kong, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Deng</author>
<author>X Huang</author>
</authors>
<title>Challenges in adopting speech recognition,</title>
<date>2004</date>
<journal>Communications of the ACM,</journal>
<volume>47</volume>
<pages>69--75</pages>
<marker>Deng, Huang, 2004</marker>
<rawString>Deng, L. &amp; Huang, X., Challenges in adopting speech recognition, Communications of the ACM, vol. 47, no. 1, pp. 69-75, January 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J G Fiscus</author>
</authors>
<title>A post-processing system to yield reduced error word rates: Recognizer output voting error reduction (ROVER).</title>
<date>1997</date>
<booktitle>In IEEE Workshop on Automatic Speech Recognition and Understanding,</booktitle>
<pages>347--354</pages>
<marker>Fiscus, 1997</marker>
<rawString>Fiscus, J. G., A post-processing system to yield reduced error word rates: Recognizer output voting error reduction (ROVER). In IEEE Workshop on Automatic Speech Recognition and Understanding, pp. 347-354, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Furui</author>
</authors>
<title>Recent progress in spontaneous speech recognition and understanding,</title>
<date>2002</date>
<booktitle>In Proceedings of the IEEE Workshop on Multimedia Signal Processing,</booktitle>
<marker>Furui, 2002</marker>
<rawString>Furui, S., Recent progress in spontaneous speech recognition and understanding, In Proceedings of the IEEE Workshop on Multimedia Signal Processing, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J E Gilbert</author>
</authors>
<title>Distributed Listening Research.</title>
<date>2005</date>
<booktitle>In Proceedings of AVIOS Speech Technology Track,</booktitle>
<contexts>
<context position="1752" citStr="Gilbert, 2005" startWordPosition="238" endWordPosition="239">sing has been on-going for over thirty years (Natural Language Software Registry, 2004; Jurafsky and Martin, 2000); however, there is still room for improvement with mainstream speech recognition systems (Deng, 2004). Distributed Listening will further research in this area. The concept is based around the idea of multiple speech input sources. Previous research activities involved a single microphone with multiple, separate recognizers that all yielded improvements in accuracy. Distributed Listening uses multiple, parallel speech recognizers, with each recognizer having its own input source (Gilbert, 2005). Each recognizer is a listener. Once input is collected from the listeners, one machine, the interpreter, processes all of the input (see figure 1). To process the input, a phrase resolution algorithm is used. This approach is analogous to a crime scene with multiple witnesses (the listeners) and a detective (the interpreter) who pieces together the stories of the witnesses using his/her knowledge of crime scenes to form a hypothesis of the actual event. Each witness will have a portion of the story that is the same as the other witnesses. It is up to the detective to fill in the blanks. With</context>
</contexts>
<marker>Gilbert, 2005</marker>
<rawString>Gilbert, J. E. (2005). Distributed Listening Research. In Proceedings of AVIOS Speech Technology Track,</rawString>
</citation>
<citation valid="false">
<authors>
<author>San Francisco</author>
<author>California</author>
</authors>
<pages>1--10</pages>
<publisher>SpeechTEK West,</publisher>
<marker>Francisco, California, </marker>
<rawString>San Francisco, California, SpeechTEK West, pp. 1 — 10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
<author>J Martin</author>
<author>Speech</author>
<author>Language Processing</author>
</authors>
<date>2000</date>
<publisher>Prentice Hall,</publisher>
<marker>Jurafsky, Martin, Speech, Processing, 2000</marker>
<rawString>Jurafsky, D. &amp; Martin, J., Speech and Language Processing, Prentice Hall, 2000.</rawString>
</citation>
<citation valid="false">
<date>2004</date>
<institution>Natural Language Software Registry, [Online].</institution>
<location>Available: http://registry.dfki.de/,</location>
<marker>2004</marker>
<rawString>Natural Language Software Registry, [Online]. Available: http://registry.dfki.de/, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schwenk</author>
<author>J Gauvain</author>
</authors>
<title>Improved ROVER using Language Model Information,</title>
<date>2000</date>
<booktitle>In ISCA ITRW Workshop on Automatic Speech Recognition: Challenges for the new Millenium,</booktitle>
<pages>47--52</pages>
<location>Paris,</location>
<marker>Schwenk, Gauvain, 2000</marker>
<rawString>Schwenk, H. &amp; Gauvain, J., Improved ROVER using Language Model Information, In ISCA ITRW Workshop on Automatic Speech Recognition: Challenges for the new Millenium, Paris, pp. 47-52, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R Young</author>
</authors>
<title>Use of dialog, pragmatics and semantics to enhance speech recognition,</title>
<date>1990</date>
<journal>Speech Communication,</journal>
<volume>9</volume>
<pages>551--564</pages>
<marker>Young, 1990</marker>
<rawString>Young, S.R., Use of dialog, pragmatics and semantics to enhance speech recognition, Speech Communication, vol. 9, pp. 551-564, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W H Ward</author>
</authors>
<title>High level knowledge sources in usable speech recognition systems,</title>
<date>1989</date>
<journal>Communications of the ACM,</journal>
<volume>31</volume>
<pages>183--194</pages>
<marker>Ward, 1989</marker>
<rawString>Young, S.R., Hauptmann, A.G. , Ward, W.H. , Smith, E.T. &amp; Werner, P., High level knowledge sources in usable speech recognition systems, Communications of the ACM, vol. 31, no. 2, pp. 183-194, 1989.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>