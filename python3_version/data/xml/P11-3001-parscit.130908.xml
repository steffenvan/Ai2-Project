<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003953">
<title confidence="0.982901">
Word Alignment Combination over Multiple Word Segmentation
</title>
<author confidence="0.997711">
Ning Xi, Guangchao Tang, Boyuan Li, Yinggong Zhao
</author>
<affiliation confidence="0.986331">
State Key Laboratory for Novel Software Technology,
Department of Computer Science and Technology,
Nanjing University, Nanjing, 210093, China
</affiliation>
<email confidence="0.993355">
{xin,tanggc,liby,zhaoyg}@nlp.nju.edu.cn
</email>
<sectionHeader confidence="0.998576" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997521">
In this paper, we present a new word alignment
combination approach on language pairs where
one language has no explicit word boundaries.
Instead of combining word alignments of dif-
ferent models (Xiang et al., 2010), we try to
combine word alignments over multiple mono-
lingually motivated word segmentation. Our
approach is based on link confidence score de-
fined over multiple segmentations, thus the
combined alignment is more robust to inappro-
priate word segmentation. Our combination al-
gorithm is simple, efficient, and easy to
implement. In the Chinese-English experiment,
our approach effectively improved word align-
ment quality as well as translation performance
on all segmentations simultaneously, which
showed that word alignment can benefit from
complementary knowledge due to the diversity
of multiple and monolingually motivated seg-
mentations.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999866636363636">
Word segmentation is the first step prior to word
alignment for building statistical machine transla-
tions (SMT) on language pairs without explicit
word boundaries such as Chinese-English. Many
works have focused on the improvement of word
alignment models. (Brown et al., 1993; Haghighi et
al., 2009; Liu et al., 2010). Most of the word
alignment models take single word segmentation
as input. However, for languages such as Chinese,
it is necessary to segment sentences into appropri-
ate words for word alignment.
</bodyText>
<page confidence="0.845809">
1
</page>
<bodyText confidence="0.999885416666667">
A large amount of works have stressed the im-
pact of word segmentation on word alignment. Xu
et al. (2004), Ma et al. (2007), Chang et al. (2008),
and Chung et al. (2009) try to learn word segmen-
tation from bilingually motivated point of view;
they use an initial alignment to learn word segmen-
tation appropriate for SMT. However, their per-
formance is limited by the quality of the initial
alignments, and the processes are time-consuming.
Some other methods try to combine multiple word
segmentation at SMT decoding step (Xu et al.,
2005; Dyer et al., 2008; Zhang et al., 2008; Dyer et
al., 2009; Xiao et al., 2010). Different segmenta-
tions are yet independently used for word align-
ment.
Instead of time-consuming segmentation optimi-
zation based on alignment or postponing segmenta-
tion combination late till SMT decoding phase, we
try to combine word alignments over multiple
monolingually motivated word segmentation on
Chinese-English pair, in order to improve word
alignment quality and translation performance for
all segmentations. We introduce a tabular structure
called word segmentation network (WSN for short)
to encode multiple segmentations of a Chinese sen-
tence, and define skeleton links (SL for short) be-
tween spans of WSN and words of English
sentence. The confidence score of a SL is defined
over multiple segmentations. Our combination al-
gorithm picks up potential SLs based on their con-
fidence scores similar to Xiang et al. (2010), and
then projects each selected SL to link in all seg-
mentation respectively. Our algorithm is simple,
efficient, easy to implement, and can effectively
improve word alignment quality on all segmenta-
tions simultaneously, and alignment errors caused
</bodyText>
<subsectionHeader confidence="0.37581">
Proceedings of the ACL-HLT 2011 Student Session, pages 1–5,
</subsectionHeader>
<bodyText confidence="0.984952176470588">
Portland, OR, USA 19-24 June 2011. c�2011 Association for Computational Linguistics
by inappropriate segmentations from single seg-
menter can be substantially reduced.
Two questions will be answered in the paper: 1)
how to define the link confidence over multiple
segmentations in combination algorithm? 2) Ac-
cording to Xiang et al. (2010), the success of their
word alignment combination of different models
lies in the complementary information that the
candidate alignments contain. In our work, are
multiple monolingually motivated segmentations
complementary enough to improve the alignments?
The rest of this paper is structured as follows:
WSN will be introduced in section 2. Combination
algorithm will be presented in section 3. Experi-
ments of word alignment and SMT will be reported
in section 4.
</bodyText>
<sectionHeader confidence="0.970307" genericHeader="method">
2 Word Segmentation Network
</sectionHeader>
<bodyText confidence="0.9733958">
We propose a new structure called word segmenta-
tion network (WSN) to encode multiple segmenta-
tions. Due to space limitation, all definitions are
presented by illustration of a running example of a
sentence pair:
</bodyText>
<equation confidence="0.9399835">
下雨路滑 (xia-yu-lu-hua)
Road is slippery when raining
</equation>
<bodyText confidence="0.9997766">
We first introduce skeleton segmentation. Given
two segmentation S1 and S2 in Table 1, the word
boundaries of their skeleton segmentation is the
union of word boundaries (marked by “/”) in S1
and S2.
</bodyText>
<table confidence="0.60340075">
Segmentation
S1 下 / 雨 / 路滑
S2 下雨 / 路 / 滑
skeleton 下 / 雨 / 路 / 滑
</table>
<tableCaption confidence="0.948977">
Table 1: The skeleton segmentation of two seg-
mentations S1 and S2.
</tableCaption>
<bodyText confidence="0.998981545454545">
The WSN of S1 and S2 is shown in Table 2. As
is depicted, line 1 and 2 represent words in S1 and
S2 respectively, line 3 represents skeleton words.
Each column, or span, comprises a skeleton word
and words of S1 and S2 with the skeleton word as
their morphemes at that position. The number of
columns of a WSN is equal to the number of skele-
ton words. It should be noted that there may be
words covering two or more spans, such as “路滑”
in S1, because the word “路滑” in S1 is split into
two words “路” and “滑” in S2.
</bodyText>
<table confidence="0.902662666666667">
S1 下 1 雨 2 路滑 3
S2 下雨 1 路 2 滑 3
skeleton 下 1 雨 2 路 3 滑 4
</table>
<tableCaption confidence="0.9978255">
Table 2: The WSN of Table 1. Subscripts
indicate indexes of words.
</tableCaption>
<bodyText confidence="0.976218916666666">
The skeleton word can be projected onto words
in the same span in S1 and S2. For clarity, words in
each segmentation are indexed (1-based), for ex-
ample, “路滑” in S1 is indexed by 3. We use a pro-
jection function 6k (i) to denote the index of the
word onto which the j-th skeleton word is project-
ed in the k-th segmentation, for example, 6i (4) =
3 and 62 (3) = 2.
In the next, we define the links between spans of
the WSN and English words as skeleton links (SL),
the subset of all SLs comprise the skeleton align-
ment (SA). Figure 1 shows an SA of the example.
</bodyText>
<figure confidence="0.7440781">
14滑 3
1 42
1 43
路 2
滑 3
下 1 雨 2 路滑 3
下雨1
下 1 雨 2 路 3 滑4
Road is slippery when raining
(a) (b)
</figure>
<figureCaption confidence="0.98296675">
Figure 1: An example alignment between WSN in
Table 2 and English sentence “Road is slippery
when raining”. (a) skeleton link; (b) skeleton
alignment.
</figureCaption>
<bodyText confidence="0.999961545454545">
Each span of the WSN comprises words from
different segmentations (Figure 1a), which indi-
cates that the confidence score of a SL can be de-
fined over words in the same span. By projection
function, a SL can be projected onto the link for
each segmentation. Therefore, the problem of
combining word alignment over different segmen-
tations can be transformed into the problem of se-
lecting SLs for SA first, and then project the
selected SLs onto links for each segmentation re-
spectively.
</bodyText>
<sectionHeader confidence="0.996549" genericHeader="method">
3 Combination Algorithm
</sectionHeader>
<bodyText confidence="0.785289">
Given k alignments ak over segmentations sk
respectively (k = 1, ..., K), and (C, E) is the pair
Road
</bodyText>
<page confidence="0.90443">
2
</page>
<table confidence="0.98145343902439">
of the Chinese WSN and its parallel English sen- links in in ascending order and evaluated
tence. Suppose is the SL between the j-th span them sequentially Compare and , A link
and i-th English word , is the link between is removed from if it is not appeared in
the j-th Chinese word in and . Inspired by , and one of the following is true:
Huang (2009), we define the confidence score of • both and are aligned in ;
each SL as follows • There is a word which is neither left nor
(  |) ∑ (1) right neighboring word of but aligned
where is the confidence score of the to in ;
link , defined as • There is a word which is neither left nor
right neighboring word of but aligned
to in .
The heuristic in step 3 is similar to Xiang et al.
(2010), which avoids adding error-prone links. We
apply the similar heuristic again in step 5 in each
to delete error-prone links. The
weights in Eq. (1) and can be tuned in a hand-
aligned dataset to maximize word alignment F-
score on any with hill climbing algorithm.
Probabilities in Eq. (2) and Eq. (3) can be estimat-
ed using GIZA.
(  |)
√ (  |)
(2)
where c-to-e link posterior probability is defined as
(  |)
∑ (3)
and I is the length of . E-to-c link posterior prob-
ability (  |) can be defined similarly,
Our alignment combination algorithm is as fol-
lows.
1. Build WSN for Chinese sentence.
2. Compute the confidence score for each SL
based on Eq. (1). A SL gets a vote from
if appears in . Denote
the set of all SLs getting at least one vote by
.
3. All SLs in are sorted in descending order
and evaluated sequentially. A SL is includ-
ed if its confidence score is higher than a tuna-
ble threshold , and one of the following is
true1:
</table>
<listItem confidence="0.908969166666667">
• Neither nor is aligned so far;
• is not aligned and its left or right neigh-
boring word is aligned to so far;
• is not aligned and its left or right
neighboring word is aligned to so far.
4. Repeat 3 until no more SLs can be included.
All included SLs comprise .
5. Map SLs in on each to get k new align-
ments respectively, i.e.
2 . For each , we sort all
4 Experiment
4.1 Data
</listItem>
<bodyText confidence="0.948267">
Our training set contains about 190K Chinese-
English sentence pairs from LDC2003E14 corpus.
The NIST’06 test set is used as our development
set and the NIST’08 test set is used as our test set.
The Chinese portions of all the data are prepro-
cessed by three monolingually motived segmenters
respectively. These segmenters differ in either
training method or specification, including
ICTCLAS (I)3, Stanford segmenters with CTB (C)
and PKU (P) specifications4 respectively. We used
a phrase-based MT system similar to (Koehn et al.,
2003), and generated two baseline alignments us-
ing GIZA++ enhanced by gdf heuristics (Koehn et
al., 2003) and a linear discriminative word align-
ment model (DIWA) (Liu et al., 2010) on training
set with the three segmentations respectively. A 5-
gram language model trained from the Xinhua por-
tion of Gigaword corpus was used. The decoding
weights were optimized with Minimum Error Rate
Training (MERT) (Och, 2003). We used the hand-
aligned set of 491 sentence pairs in Haghighi et al.
(2009), the first 250 sentence pairs were used to
tune the weights in Eq. (1), and the other 241 were
1 SLs getting votes are forced to be included without further
examination.
2 Two or more SLs in may be projected onto one links in
, in this case, we keep only one in .
</bodyText>
<figure confidence="0.991719">
3 http://www.ictclas.org/
4 http://nlp.stanford.edu/software/segmenter.shtml
3
[粮食署] [的] [380] [万] [美元] [救济金] [香港] [特别] [行政区] [行政] [长官]
relief funds worth 3.8 million us dollars from the national foodstuff department
[粮食署] [的] [380] [万] [美元] [救济金] [香港] [特别] [行政区] [行政] [长官]
chief executive in the hksar
</figure>
<figureCaption confidence="0.985661">
Figure 2: Two examples (left and right respectively) of word alignment on segmentation C. Baselines
(DIWA) are in the top half, combined alignments are in the bottom half. The solid line represents the cor-
rect link while the dashed line represents the bad link. Each word is enclosed in square brackets.
</figureCaption>
<bodyText confidence="0.891198">
used to measure the word alignment quality. Note
that we adapted the Chinese portion of this hand-
aligned set to segmentation C.
</bodyText>
<subsectionHeader confidence="0.999191">
4.2 Improvement of Word Alignment
</subsectionHeader>
<bodyText confidence="0.99997480952381">
We first evaluate our combination approach on the
hand-aligned set (on segmentation C). Table 3
shows the precision, recall and F-score of baseline
alignments and combined alignments.
As shown in Table 3, the combination align-
ments outperformed the baselines (setting C) in all
settings in both GIZA and DIWA. We notice that
the higher F-score is mainly due to the higher pre-
cision in GIZA but higher recall in DIWA. In
GIZA, the result of C+I and C+P achieve 8.4% and
9.5% higher F-score respectively, and both of them
outperformed C+P+I, we speculate it is because
GIZA favors recall rather than DIWA, i.e. GIZA
may contain more bad links than DIWA, which
would lead to more unstable F-score if more
alignments produced by GIZA are combined, just
as the poor precision (69.68%) indicated. However,
DIWA favors precision than recall (this observa-
tion is consistent with Liu et al. (2010)), which
may explain that the more diversified segmenta-
tions lead to better results in DIWA.
</bodyText>
<table confidence="0.995767166666667">
GIZA DIWA
setting P R F P R F
C 61.84 84.99 71.59 83.12 78.88 80.94
C+P 80.16 79.80 79.98 84.15 79.41 81.57
C+I 82.96 79.28 81.08 84.41 81.69 83.03
C+I+P 69.68 85.17 77.81 83.38 82.98 83.18
</table>
<tableCaption confidence="0.975701">
Table 3: Alignment precision, recall and F-score.
C: baseline, C+I: Combination of C and I.
</tableCaption>
<bodyText confidence="0.975088909090909">
Figure 2 gives baseline alignments and com-
bined alignments on two sentence pairs in the
training data. As can be seen, alignment errors
caused by inappropriate segmentations by single
segmenter were substantially reduced. For exam-
ple, in the second example, the word “香港特别行
政区 hksar” appears in segmentation I of the Chi-
nese sentence, which benefits the generation of the
three correct links connecting for words “ 香
港” ,“特别”, “行政区” respectively in the com-
bined alignment.
</bodyText>
<subsectionHeader confidence="0.99845">
4.3 Improvement in MT performance
</subsectionHeader>
<bodyText confidence="0.999978785714286">
We then evaluate our combination approach on the
SMT training data on all segmentations. For effi-
ciency, we just used the first 50k sentence pairs of
the aligned training corpus with the three segmen-
tations to build three SMT systems respectively.
Table 4 shows the BLEU scores of baselines and
combined alignment (C+P+I, and then projected
onto C, P, I respectively). Our approach achieves
improvement over baseline alignments on all seg-
mentations consistently, without using any lattice
decoding techniques as Dyer et al. (2009). The
gain of translation performance purely comes from
improvements of word alignment on all segmenta-
tions by our proposed word alignment combination.
</bodyText>
<table confidence="0.9769144">
GIZA DIWA
Segmentation B Comb B Comb
C 19.77 20.9 20.18 20.71
P 20.5 21.16 20.41 21.14
I 20.11 21.14 20.46 21.30
</table>
<tableCaption confidence="0.9972305">
Table 4: Improvement in BLEU scores. B:Baseline
alignment, Comb: Combined alignment.
</tableCaption>
<page confidence="0.997543">
4
</page>
<sectionHeader confidence="0.999353" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999982038461538">
We evaluated our word alignment combination
over three monolingually motivated segmentations
on Chinese-English pair. We showed that the com-
bined alignment significantly outperforms the
baseline alignment with both higher F-score and
higher BLEU score on all segmentations. Our work
also proved the effectiveness of link confidence
score in combining different word alignment mod-
els (Xiang et al., 2010), and extend it to combine
word alignments over different segmentations.
Xu et al. (2005) and Dyer et al. (2009) combine
different segmentations for SMT. They aim to
achieve better translation but not higher alignment
quality of all segmentations. They combine multi-
ple segmentations at SMT decoding step, while we
combine segmentation alternatives at word align-
ment step. We believe that we can further improve
the performance by combining these two kinds of
works. We also believe that combining word
alignments over both monolingually motivated and
bilingually motivated segmentations (Ma et al.,
2009) can achieve higher performance.
In the future, we will investigate combining
word alignments on language pairs where both
languages have no explicit word boundaries such
as Chinese-Japanese.
</bodyText>
<sectionHeader confidence="0.99863" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999917285714286">
This work was supported by the National Natural
Science Foundation of China under Grant No.
61003112, and the National Fundamental Research
Program of China (2010CB327903). We would
like to thank Xiuyi Jia and Shujie Liu for useful
discussions and the anonymous reviewers for their
constructive comments.
</bodyText>
<sectionHeader confidence="0.998995" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999535877192983">
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Del-
la Peitra, Robert L. Mercer. 1993. The Mathematics
of statistical machine translation: parameter estima-
tion. Computational Linguistics, 19(2):263-311.
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word segmenta-
tion for machine translation performance. In Pro-
ceedings of third workshop on SMT, Pages:224-232.
Tagyoung Chung and Daniel Gildea. 2009. Unsuper-
vised tokenization for machine translation. In Pro-
ceedings of EMNLP, Pages:718-726.
Christopher Dyer, Smaranda Muresan, and Philip Res-
nik. 2008. Generalizing word lattice translation. In
Proceedings of ACL, Pages:1012-1020.
Christopher Dyer. 2009. Using a maximum entropy
model to build segmentation lattices for mt. In Pro-
ceedings of NAACL, Pages:406-414.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of
ACL, Pages:440-447.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with supervised
ITG models. In Proceedings of ACL, Pages: 923-931.
Fei Huang. 2009. Confidence measure for word align-
ment. In Proceedings of ACL, Pages:932-940.
Philipp Koehn, Franz Josef Och and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL, Pages:48-54.
Yang Liu, Qun Liu, Shouxun Lin. 2010. Discriminative
word alignment by linear modeling. Computational
Linguistics, 36(3):303-339.
Yanjun Ma, Nicolas Stroppa, and Andy Way. 2007.
Bootstrapping word alignment via word packing. In
Proceedings of ACL, Pages:304-311.
Yanjun Ma and Andy Way. 2009. Bilingually motivated
domain-adapted word segmentation for statistical
machine translation. In Proceedings of EACL, Pag-
es:549-557.
Bing Xiang, Yonggang Deng, and Bowen Zhou. 2010.
Diversify and combine: improving word alignment
for machine translation on low-resource languages.
In Proceedings of ACL, Pages:932-940.
Xinyan Xiao, Yang Liu, Young-Sook Hwang, Qun Liu,
Shouxun Lin. 2010. Joint tokenization and transla-
tion. In Proceedings of COLING, Pages:1200-1208.
Jia Xu, Richard Zens, and Hermann Ney. 2004. Do we
need Chinese word segmentation for statistical ma-
chine translation? In Proceedings of the ACL
SIGHAN Workshop, Pages: 122-128.
Jia Xu, Evgeny Matusov, Richard Zens, and Hermann
Ney. 2005. Integrated Chinese word segmentation in
statistical machine translation. In Proceedings of
IWSLT.
Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita.
2008. Improved statistical machine translation by
multiple Chinese word segmentation. In Proceedings
of the Third Workshop on SMT, Pages:216-223.
</reference>
<page confidence="0.99433">
5
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.212972">
<title confidence="0.7544745">Word Alignment Combination over Multiple Word Segmentation Ning Xi, Guangchao Tang, Boyuan Li, Yinggong</title>
<author confidence="0.929948">State Key Laboratory for Novel Software</author>
<affiliation confidence="0.999818">Department of Computer Science and</affiliation>
<address confidence="0.493671">Nanjing University, Nanjing, 210093,</address>
<email confidence="0.956233">xin@nlp.nju.edu.cn</email>
<email confidence="0.956233">tanggc@nlp.nju.edu.cn</email>
<email confidence="0.956233">liby@nlp.nju.edu.cn</email>
<email confidence="0.956233">zhaoyg@nlp.nju.edu.cn</email>
<abstract confidence="0.994682571428571">In this paper, we present a new word alignment combination approach on language pairs where one language has no explicit word boundaries. Instead of combining word alignments of different models (Xiang et al., 2010), we try to combine word alignments over multiple monolingually motivated word segmentation. Our approach is based on link confidence score defined over multiple segmentations, thus the combined alignment is more robust to inappropriate word segmentation. Our combination algorithm is simple, efficient, and easy to implement. In the Chinese-English experiment, our approach effectively improved word alignment quality as well as translation performance on all segmentations simultaneously, which showed that word alignment can benefit from complementary knowledge due to the diversity of multiple and monolingually motivated segmentations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Peitra</author>
<author>Robert L Mercer</author>
</authors>
<title>The Mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--2</pages>
<contexts>
<context position="1447" citStr="Brown et al., 1993" startWordPosition="201" endWordPosition="204">o implement. In the Chinese-English experiment, our approach effectively improved word alignment quality as well as translation performance on all segmentations simultaneously, which showed that word alignment can benefit from complementary knowledge due to the diversity of multiple and monolingually motivated segmentations. 1 Introduction Word segmentation is the first step prior to word alignment for building statistical machine translations (SMT) on language pairs without explicit word boundaries such as Chinese-English. Many works have focused on the improvement of word alignment models. (Brown et al., 1993; Haghighi et al., 2009; Liu et al., 2010). Most of the word alignment models take single word segmentation as input. However, for languages such as Chinese, it is necessary to segment sentences into appropriate words for word alignment. 1 A large amount of works have stressed the impact of word segmentation on word alignment. Xu et al. (2004), Ma et al. (2007), Chang et al. (2008), and Chung et al. (2009) try to learn word segmentation from bilingually motivated point of view; they use an initial alignment to learn word segmentation appropriate for SMT. However, their performance is limited b</context>
</contexts>
<marker>Brown, Pietra, Peitra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Peitra, Robert L. Mercer. 1993. The Mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2):263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pi-Chuan Chang</author>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>Optimizing Chinese word segmentation for machine translation performance.</title>
<date>2008</date>
<booktitle>In Proceedings of third workshop on SMT,</booktitle>
<pages>224--232</pages>
<contexts>
<context position="1831" citStr="Chang et al. (2008)" startWordPosition="269" endWordPosition="272"> to word alignment for building statistical machine translations (SMT) on language pairs without explicit word boundaries such as Chinese-English. Many works have focused on the improvement of word alignment models. (Brown et al., 1993; Haghighi et al., 2009; Liu et al., 2010). Most of the word alignment models take single word segmentation as input. However, for languages such as Chinese, it is necessary to segment sentences into appropriate words for word alignment. 1 A large amount of works have stressed the impact of word segmentation on word alignment. Xu et al. (2004), Ma et al. (2007), Chang et al. (2008), and Chung et al. (2009) try to learn word segmentation from bilingually motivated point of view; they use an initial alignment to learn word segmentation appropriate for SMT. However, their performance is limited by the quality of the initial alignments, and the processes are time-consuming. Some other methods try to combine multiple word segmentation at SMT decoding step (Xu et al., 2005; Dyer et al., 2008; Zhang et al., 2008; Dyer et al., 2009; Xiao et al., 2010). Different segmentations are yet independently used for word alignment. Instead of time-consuming segmentation optimization base</context>
</contexts>
<marker>Chang, Galley, Manning, 2008</marker>
<rawString>Pi-Chuan Chang, Michel Galley, and Christopher D. Manning. 2008. Optimizing Chinese word segmentation for machine translation performance. In Proceedings of third workshop on SMT, Pages:224-232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tagyoung Chung</author>
<author>Daniel Gildea</author>
</authors>
<title>Unsupervised tokenization for machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>718--726</pages>
<marker>Chung, Gildea, 2009</marker>
<rawString>Tagyoung Chung and Daniel Gildea. 2009. Unsupervised tokenization for machine translation. In Proceedings of EMNLP, Pages:718-726.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Dyer</author>
<author>Smaranda Muresan</author>
<author>Philip Resnik</author>
</authors>
<title>Generalizing word lattice translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1012--1020</pages>
<contexts>
<context position="2243" citStr="Dyer et al., 2008" startWordPosition="338" endWordPosition="341">gment sentences into appropriate words for word alignment. 1 A large amount of works have stressed the impact of word segmentation on word alignment. Xu et al. (2004), Ma et al. (2007), Chang et al. (2008), and Chung et al. (2009) try to learn word segmentation from bilingually motivated point of view; they use an initial alignment to learn word segmentation appropriate for SMT. However, their performance is limited by the quality of the initial alignments, and the processes are time-consuming. Some other methods try to combine multiple word segmentation at SMT decoding step (Xu et al., 2005; Dyer et al., 2008; Zhang et al., 2008; Dyer et al., 2009; Xiao et al., 2010). Different segmentations are yet independently used for word alignment. Instead of time-consuming segmentation optimization based on alignment or postponing segmentation combination late till SMT decoding phase, we try to combine word alignments over multiple monolingually motivated word segmentation on Chinese-English pair, in order to improve word alignment quality and translation performance for all segmentations. We introduce a tabular structure called word segmentation network (WSN for short) to encode multiple segmentations of a</context>
</contexts>
<marker>Dyer, Muresan, Resnik, 2008</marker>
<rawString>Christopher Dyer, Smaranda Muresan, and Philip Resnik. 2008. Generalizing word lattice translation. In Proceedings of ACL, Pages:1012-1020.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Dyer</author>
</authors>
<title>Using a maximum entropy model to build segmentation lattices for mt.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>406--414</pages>
<marker>Dyer, 2009</marker>
<rawString>Christopher Dyer. 2009. Using a maximum entropy model to build segmentation lattices for mt. In Proceedings of NAACL, Pages:406-414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>440--447</pages>
<contexts>
<context position="9914" citStr="Och, 2003" startWordPosition="1735" endWordPosition="1736">ther training method or specification, including ICTCLAS (I)3, Stanford segmenters with CTB (C) and PKU (P) specifications4 respectively. We used a phrase-based MT system similar to (Koehn et al., 2003), and generated two baseline alignments using GIZA++ enhanced by gdf heuristics (Koehn et al., 2003) and a linear discriminative word alignment model (DIWA) (Liu et al., 2010) on training set with the three segmentations respectively. A 5- gram language model trained from the Xinhua portion of Gigaword corpus was used. The decoding weights were optimized with Minimum Error Rate Training (MERT) (Och, 2003). We used the handaligned set of 491 sentence pairs in Haghighi et al. (2009), the first 250 sentence pairs were used to tune the weights in Eq. (1), and the other 241 were 1 SLs getting votes are forced to be included without further examination. 2 Two or more SLs in may be projected onto one links in , in this case, we keep only one in . 3 http://www.ictclas.org/ 4 http://nlp.stanford.edu/software/segmenter.shtml 3 [粮食署] [的] [380] [万] [美元] [救济金] [香港] [特别] [行政区] [行政] [长官] relief funds worth 3.8 million us dollars from the national foodstuff department [粮食署] [的] [380] [万] [美元] [救济金] [香港] [特别] </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL, Pages:440-447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>John Blitzer</author>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Better word alignments with supervised ITG models.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>923--931</pages>
<contexts>
<context position="1470" citStr="Haghighi et al., 2009" startWordPosition="205" endWordPosition="208">Chinese-English experiment, our approach effectively improved word alignment quality as well as translation performance on all segmentations simultaneously, which showed that word alignment can benefit from complementary knowledge due to the diversity of multiple and monolingually motivated segmentations. 1 Introduction Word segmentation is the first step prior to word alignment for building statistical machine translations (SMT) on language pairs without explicit word boundaries such as Chinese-English. Many works have focused on the improvement of word alignment models. (Brown et al., 1993; Haghighi et al., 2009; Liu et al., 2010). Most of the word alignment models take single word segmentation as input. However, for languages such as Chinese, it is necessary to segment sentences into appropriate words for word alignment. 1 A large amount of works have stressed the impact of word segmentation on word alignment. Xu et al. (2004), Ma et al. (2007), Chang et al. (2008), and Chung et al. (2009) try to learn word segmentation from bilingually motivated point of view; they use an initial alignment to learn word segmentation appropriate for SMT. However, their performance is limited by the quality of the in</context>
<context position="9991" citStr="Haghighi et al. (2009)" startWordPosition="1748" endWordPosition="1751">anford segmenters with CTB (C) and PKU (P) specifications4 respectively. We used a phrase-based MT system similar to (Koehn et al., 2003), and generated two baseline alignments using GIZA++ enhanced by gdf heuristics (Koehn et al., 2003) and a linear discriminative word alignment model (DIWA) (Liu et al., 2010) on training set with the three segmentations respectively. A 5- gram language model trained from the Xinhua portion of Gigaword corpus was used. The decoding weights were optimized with Minimum Error Rate Training (MERT) (Och, 2003). We used the handaligned set of 491 sentence pairs in Haghighi et al. (2009), the first 250 sentence pairs were used to tune the weights in Eq. (1), and the other 241 were 1 SLs getting votes are forced to be included without further examination. 2 Two or more SLs in may be projected onto one links in , in this case, we keep only one in . 3 http://www.ictclas.org/ 4 http://nlp.stanford.edu/software/segmenter.shtml 3 [粮食署] [的] [380] [万] [美元] [救济金] [香港] [特别] [行政区] [行政] [长官] relief funds worth 3.8 million us dollars from the national foodstuff department [粮食署] [的] [380] [万] [美元] [救济金] [香港] [特别] [行政区] [行政] [长官] chief executive in the hksar Figure 2: Two examples (left and</context>
</contexts>
<marker>Haghighi, Blitzer, DeNero, Klein, 2009</marker>
<rawString>Aria Haghighi, John Blitzer, John DeNero, and Dan Klein. 2009. Better word alignments with supervised ITG models. In Proceedings of ACL, Pages: 923-931.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
</authors>
<title>Confidence measure for word alignment.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>932--940</pages>
<contexts>
<context position="7281" citStr="Huang (2009)" startWordPosition="1242" endWordPosition="1243">d into the problem of selecting SLs for SA first, and then project the selected SLs onto links for each segmentation respectively. 3 Combination Algorithm Given k alignments ak over segmentations sk respectively (k = 1, ..., K), and (C, E) is the pair Road 2 of the Chinese WSN and its parallel English sen- links in in ascending order and evaluated tence. Suppose is the SL between the j-th span them sequentially Compare and , A link and i-th English word , is the link between is removed from if it is not appeared in the j-th Chinese word in and . Inspired by , and one of the following is true: Huang (2009), we define the confidence score of • both and are aligned in ; each SL as follows • There is a word which is neither left nor ( |) ∑ (1) right neighboring word of but aligned where is the confidence score of the to in ; link , defined as • There is a word which is neither left nor right neighboring word of but aligned to in . The heuristic in step 3 is similar to Xiang et al. (2010), which avoids adding error-prone links. We apply the similar heuristic again in step 5 in each to delete error-prone links. The weights in Eq. (1) and can be tuned in a handaligned dataset to maximize word alignme</context>
</contexts>
<marker>Huang, 2009</marker>
<rawString>Fei Huang. 2009. Confidence measure for word alignment. In Proceedings of ACL, Pages:932-940.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>48--54</pages>
<contexts>
<context position="9506" citStr="Koehn et al., 2003" startWordPosition="1667" endWordPosition="1670">k new alignments respectively, i.e. 2 . For each , we sort all 4 Experiment 4.1 Data Our training set contains about 190K ChineseEnglish sentence pairs from LDC2003E14 corpus. The NIST’06 test set is used as our development set and the NIST’08 test set is used as our test set. The Chinese portions of all the data are preprocessed by three monolingually motived segmenters respectively. These segmenters differ in either training method or specification, including ICTCLAS (I)3, Stanford segmenters with CTB (C) and PKU (P) specifications4 respectively. We used a phrase-based MT system similar to (Koehn et al., 2003), and generated two baseline alignments using GIZA++ enhanced by gdf heuristics (Koehn et al., 2003) and a linear discriminative word alignment model (DIWA) (Liu et al., 2010) on training set with the three segmentations respectively. A 5- gram language model trained from the Xinhua portion of Gigaword corpus was used. The decoding weights were optimized with Minimum Error Rate Training (MERT) (Och, 2003). We used the handaligned set of 491 sentence pairs in Haghighi et al. (2009), the first 250 sentence pairs were used to tune the weights in Eq. (1), and the other 241 were 1 SLs getting votes</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLT-NAACL, Pages:48-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Discriminative word alignment by linear modeling.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<pages>36--3</pages>
<contexts>
<context position="1489" citStr="Liu et al., 2010" startWordPosition="209" endWordPosition="212">ent, our approach effectively improved word alignment quality as well as translation performance on all segmentations simultaneously, which showed that word alignment can benefit from complementary knowledge due to the diversity of multiple and monolingually motivated segmentations. 1 Introduction Word segmentation is the first step prior to word alignment for building statistical machine translations (SMT) on language pairs without explicit word boundaries such as Chinese-English. Many works have focused on the improvement of word alignment models. (Brown et al., 1993; Haghighi et al., 2009; Liu et al., 2010). Most of the word alignment models take single word segmentation as input. However, for languages such as Chinese, it is necessary to segment sentences into appropriate words for word alignment. 1 A large amount of works have stressed the impact of word segmentation on word alignment. Xu et al. (2004), Ma et al. (2007), Chang et al. (2008), and Chung et al. (2009) try to learn word segmentation from bilingually motivated point of view; they use an initial alignment to learn word segmentation appropriate for SMT. However, their performance is limited by the quality of the initial alignments, a</context>
<context position="9681" citStr="Liu et al., 2010" startWordPosition="1696" endWordPosition="1699">he NIST’06 test set is used as our development set and the NIST’08 test set is used as our test set. The Chinese portions of all the data are preprocessed by three monolingually motived segmenters respectively. These segmenters differ in either training method or specification, including ICTCLAS (I)3, Stanford segmenters with CTB (C) and PKU (P) specifications4 respectively. We used a phrase-based MT system similar to (Koehn et al., 2003), and generated two baseline alignments using GIZA++ enhanced by gdf heuristics (Koehn et al., 2003) and a linear discriminative word alignment model (DIWA) (Liu et al., 2010) on training set with the three segmentations respectively. A 5- gram language model trained from the Xinhua portion of Gigaword corpus was used. The decoding weights were optimized with Minimum Error Rate Training (MERT) (Och, 2003). We used the handaligned set of 491 sentence pairs in Haghighi et al. (2009), the first 250 sentence pairs were used to tune the weights in Eq. (1), and the other 241 were 1 SLs getting votes are forced to be included without further examination. 2 Two or more SLs in may be projected onto one links in , in this case, we keep only one in . 3 http://www.ictclas.org/</context>
<context position="11913" citStr="Liu et al. (2010)" startWordPosition="2073" endWordPosition="2076">in all settings in both GIZA and DIWA. We notice that the higher F-score is mainly due to the higher precision in GIZA but higher recall in DIWA. In GIZA, the result of C+I and C+P achieve 8.4% and 9.5% higher F-score respectively, and both of them outperformed C+P+I, we speculate it is because GIZA favors recall rather than DIWA, i.e. GIZA may contain more bad links than DIWA, which would lead to more unstable F-score if more alignments produced by GIZA are combined, just as the poor precision (69.68%) indicated. However, DIWA favors precision than recall (this observation is consistent with Liu et al. (2010)), which may explain that the more diversified segmentations lead to better results in DIWA. GIZA DIWA setting P R F P R F C 61.84 84.99 71.59 83.12 78.88 80.94 C+P 80.16 79.80 79.98 84.15 79.41 81.57 C+I 82.96 79.28 81.08 84.41 81.69 83.03 C+I+P 69.68 85.17 77.81 83.38 82.98 83.18 Table 3: Alignment precision, recall and F-score. C: baseline, C+I: Combination of C and I. Figure 2 gives baseline alignments and combined alignments on two sentence pairs in the training data. As can be seen, alignment errors caused by inappropriate segmentations by single segmenter were substantially reduced. For</context>
</contexts>
<marker>Liu, Liu, Lin, 2010</marker>
<rawString>Yang Liu, Qun Liu, Shouxun Lin. 2010. Discriminative word alignment by linear modeling. Computational Linguistics, 36(3):303-339.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanjun Ma</author>
<author>Nicolas Stroppa</author>
<author>Andy Way</author>
</authors>
<title>Bootstrapping word alignment via word packing.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>304--311</pages>
<contexts>
<context position="1810" citStr="Ma et al. (2007)" startWordPosition="265" endWordPosition="268">e first step prior to word alignment for building statistical machine translations (SMT) on language pairs without explicit word boundaries such as Chinese-English. Many works have focused on the improvement of word alignment models. (Brown et al., 1993; Haghighi et al., 2009; Liu et al., 2010). Most of the word alignment models take single word segmentation as input. However, for languages such as Chinese, it is necessary to segment sentences into appropriate words for word alignment. 1 A large amount of works have stressed the impact of word segmentation on word alignment. Xu et al. (2004), Ma et al. (2007), Chang et al. (2008), and Chung et al. (2009) try to learn word segmentation from bilingually motivated point of view; they use an initial alignment to learn word segmentation appropriate for SMT. However, their performance is limited by the quality of the initial alignments, and the processes are time-consuming. Some other methods try to combine multiple word segmentation at SMT decoding step (Xu et al., 2005; Dyer et al., 2008; Zhang et al., 2008; Dyer et al., 2009; Xiao et al., 2010). Different segmentations are yet independently used for word alignment. Instead of time-consuming segmentat</context>
</contexts>
<marker>Ma, Stroppa, Way, 2007</marker>
<rawString>Yanjun Ma, Nicolas Stroppa, and Andy Way. 2007. Bootstrapping word alignment via word packing. In Proceedings of ACL, Pages:304-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanjun Ma</author>
<author>Andy Way</author>
</authors>
<title>Bilingually motivated domain-adapted word segmentation for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>549--557</pages>
<marker>Ma, Way, 2009</marker>
<rawString>Yanjun Ma and Andy Way. 2009. Bilingually motivated domain-adapted word segmentation for statistical machine translation. In Proceedings of EACL, Pages:549-557.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Xiang</author>
<author>Yonggang Deng</author>
<author>Bowen Zhou</author>
</authors>
<title>Diversify and combine: improving word alignment for machine translation on low-resource languages.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>932--940</pages>
<contexts>
<context position="3136" citStr="Xiang et al. (2010)" startWordPosition="477" endWordPosition="480"> try to combine word alignments over multiple monolingually motivated word segmentation on Chinese-English pair, in order to improve word alignment quality and translation performance for all segmentations. We introduce a tabular structure called word segmentation network (WSN for short) to encode multiple segmentations of a Chinese sentence, and define skeleton links (SL for short) between spans of WSN and words of English sentence. The confidence score of a SL is defined over multiple segmentations. Our combination algorithm picks up potential SLs based on their confidence scores similar to Xiang et al. (2010), and then projects each selected SL to link in all segmentation respectively. Our algorithm is simple, efficient, easy to implement, and can effectively improve word alignment quality on all segmentations simultaneously, and alignment errors caused Proceedings of the ACL-HLT 2011 Student Session, pages 1–5, Portland, OR, USA 19-24 June 2011. c�2011 Association for Computational Linguistics by inappropriate segmentations from single segmenter can be substantially reduced. Two questions will be answered in the paper: 1) how to define the link confidence over multiple segmentations in combinatio</context>
<context position="7667" citStr="Xiang et al. (2010)" startWordPosition="1322" endWordPosition="1325">een the j-th span them sequentially Compare and , A link and i-th English word , is the link between is removed from if it is not appeared in the j-th Chinese word in and . Inspired by , and one of the following is true: Huang (2009), we define the confidence score of • both and are aligned in ; each SL as follows • There is a word which is neither left nor ( |) ∑ (1) right neighboring word of but aligned where is the confidence score of the to in ; link , defined as • There is a word which is neither left nor right neighboring word of but aligned to in . The heuristic in step 3 is similar to Xiang et al. (2010), which avoids adding error-prone links. We apply the similar heuristic again in step 5 in each to delete error-prone links. The weights in Eq. (1) and can be tuned in a handaligned dataset to maximize word alignment Fscore on any with hill climbing algorithm. Probabilities in Eq. (2) and Eq. (3) can be estimated using GIZA. ( |) √ ( |) (2) where c-to-e link posterior probability is defined as ( |) ∑ (3) and I is the length of . E-to-c link posterior probability ( |) can be defined similarly, Our alignment combination algorithm is as follows. 1. Build WSN for Chinese sentence. 2. Compute the c</context>
<context position="14093" citStr="Xiang et al., 2010" startWordPosition="2422" endWordPosition="2425">n. GIZA DIWA Segmentation B Comb B Comb C 19.77 20.9 20.18 20.71 P 20.5 21.16 20.41 21.14 I 20.11 21.14 20.46 21.30 Table 4: Improvement in BLEU scores. B:Baseline alignment, Comb: Combined alignment. 4 5 Conclusion We evaluated our word alignment combination over three monolingually motivated segmentations on Chinese-English pair. We showed that the combined alignment significantly outperforms the baseline alignment with both higher F-score and higher BLEU score on all segmentations. Our work also proved the effectiveness of link confidence score in combining different word alignment models (Xiang et al., 2010), and extend it to combine word alignments over different segmentations. Xu et al. (2005) and Dyer et al. (2009) combine different segmentations for SMT. They aim to achieve better translation but not higher alignment quality of all segmentations. They combine multiple segmentations at SMT decoding step, while we combine segmentation alternatives at word alignment step. We believe that we can further improve the performance by combining these two kinds of works. We also believe that combining word alignments over both monolingually motivated and bilingually motivated segmentations (Ma et al., </context>
</contexts>
<marker>Xiang, Deng, Zhou, 2010</marker>
<rawString>Bing Xiang, Yonggang Deng, and Bowen Zhou. 2010. Diversify and combine: improving word alignment for machine translation on low-resource languages. In Proceedings of ACL, Pages:932-940.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinyan Xiao</author>
</authors>
<title>Yang Liu, Young-Sook Hwang, Qun Liu, Shouxun Lin.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>1200--1208</pages>
<marker>Xiao, 2010</marker>
<rawString>Xinyan Xiao, Yang Liu, Young-Sook Hwang, Qun Liu, Shouxun Lin. 2010. Joint tokenization and translation. In Proceedings of COLING, Pages:1200-1208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Xu</author>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Do we need Chinese word segmentation for statistical machine translation?</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL SIGHAN Workshop,</booktitle>
<pages>122--128</pages>
<contexts>
<context position="1792" citStr="Xu et al. (2004)" startWordPosition="261" endWordPosition="264">segmentation is the first step prior to word alignment for building statistical machine translations (SMT) on language pairs without explicit word boundaries such as Chinese-English. Many works have focused on the improvement of word alignment models. (Brown et al., 1993; Haghighi et al., 2009; Liu et al., 2010). Most of the word alignment models take single word segmentation as input. However, for languages such as Chinese, it is necessary to segment sentences into appropriate words for word alignment. 1 A large amount of works have stressed the impact of word segmentation on word alignment. Xu et al. (2004), Ma et al. (2007), Chang et al. (2008), and Chung et al. (2009) try to learn word segmentation from bilingually motivated point of view; they use an initial alignment to learn word segmentation appropriate for SMT. However, their performance is limited by the quality of the initial alignments, and the processes are time-consuming. Some other methods try to combine multiple word segmentation at SMT decoding step (Xu et al., 2005; Dyer et al., 2008; Zhang et al., 2008; Dyer et al., 2009; Xiao et al., 2010). Different segmentations are yet independently used for word alignment. Instead of time-c</context>
</contexts>
<marker>Xu, Zens, Ney, 2004</marker>
<rawString>Jia Xu, Richard Zens, and Hermann Ney. 2004. Do we need Chinese word segmentation for statistical machine translation? In Proceedings of the ACL SIGHAN Workshop, Pages: 122-128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Xu</author>
<author>Evgeny Matusov</author>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Integrated Chinese word segmentation in statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of IWSLT.</booktitle>
<contexts>
<context position="2224" citStr="Xu et al., 2005" startWordPosition="334" endWordPosition="337">s necessary to segment sentences into appropriate words for word alignment. 1 A large amount of works have stressed the impact of word segmentation on word alignment. Xu et al. (2004), Ma et al. (2007), Chang et al. (2008), and Chung et al. (2009) try to learn word segmentation from bilingually motivated point of view; they use an initial alignment to learn word segmentation appropriate for SMT. However, their performance is limited by the quality of the initial alignments, and the processes are time-consuming. Some other methods try to combine multiple word segmentation at SMT decoding step (Xu et al., 2005; Dyer et al., 2008; Zhang et al., 2008; Dyer et al., 2009; Xiao et al., 2010). Different segmentations are yet independently used for word alignment. Instead of time-consuming segmentation optimization based on alignment or postponing segmentation combination late till SMT decoding phase, we try to combine word alignments over multiple monolingually motivated word segmentation on Chinese-English pair, in order to improve word alignment quality and translation performance for all segmentations. We introduce a tabular structure called word segmentation network (WSN for short) to encode multiple</context>
<context position="14182" citStr="Xu et al. (2005)" startWordPosition="2436" endWordPosition="2439">20.11 21.14 20.46 21.30 Table 4: Improvement in BLEU scores. B:Baseline alignment, Comb: Combined alignment. 4 5 Conclusion We evaluated our word alignment combination over three monolingually motivated segmentations on Chinese-English pair. We showed that the combined alignment significantly outperforms the baseline alignment with both higher F-score and higher BLEU score on all segmentations. Our work also proved the effectiveness of link confidence score in combining different word alignment models (Xiang et al., 2010), and extend it to combine word alignments over different segmentations. Xu et al. (2005) and Dyer et al. (2009) combine different segmentations for SMT. They aim to achieve better translation but not higher alignment quality of all segmentations. They combine multiple segmentations at SMT decoding step, while we combine segmentation alternatives at word alignment step. We believe that we can further improve the performance by combining these two kinds of works. We also believe that combining word alignments over both monolingually motivated and bilingually motivated segmentations (Ma et al., 2009) can achieve higher performance. In the future, we will investigate combining word a</context>
</contexts>
<marker>Xu, Matusov, Zens, Ney, 2005</marker>
<rawString>Jia Xu, Evgeny Matusov, Richard Zens, and Hermann Ney. 2005. Integrated Chinese word segmentation in statistical machine translation. In Proceedings of IWSLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruiqiang Zhang</author>
<author>Keiji Yasuda</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Improved statistical machine translation by multiple Chinese word segmentation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on SMT,</booktitle>
<pages>216--223</pages>
<contexts>
<context position="2263" citStr="Zhang et al., 2008" startWordPosition="342" endWordPosition="345">o appropriate words for word alignment. 1 A large amount of works have stressed the impact of word segmentation on word alignment. Xu et al. (2004), Ma et al. (2007), Chang et al. (2008), and Chung et al. (2009) try to learn word segmentation from bilingually motivated point of view; they use an initial alignment to learn word segmentation appropriate for SMT. However, their performance is limited by the quality of the initial alignments, and the processes are time-consuming. Some other methods try to combine multiple word segmentation at SMT decoding step (Xu et al., 2005; Dyer et al., 2008; Zhang et al., 2008; Dyer et al., 2009; Xiao et al., 2010). Different segmentations are yet independently used for word alignment. Instead of time-consuming segmentation optimization based on alignment or postponing segmentation combination late till SMT decoding phase, we try to combine word alignments over multiple monolingually motivated word segmentation on Chinese-English pair, in order to improve word alignment quality and translation performance for all segmentations. We introduce a tabular structure called word segmentation network (WSN for short) to encode multiple segmentations of a Chinese sentence, a</context>
</contexts>
<marker>Zhang, Yasuda, Sumita, 2008</marker>
<rawString>Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita. 2008. Improved statistical machine translation by multiple Chinese word segmentation. In Proceedings of the Third Workshop on SMT, Pages:216-223.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>