<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000113">
<title confidence="0.924285">
Subword-based Tagging for Confidence-dependent Chinese Word
Segmentation
</title>
<author confidence="0.978809">
Ruiqiang Zhang1,2 and Genichiro Kikui* and Eiichiro Sumita1,2
</author>
<affiliation confidence="0.982073">
1National Institute of Information and Communications Technology
</affiliation>
<address confidence="0.813369">
2ATR Spoken Language Communication Research Laboratories
2-2-2 Hikaridai, Seiika-cho, Soraku-gun, Kyoto, 619-0288, Japan
</address>
<email confidence="0.996534">
fruiqiang.zhang,eiichiro.sumital@atr.jp
</email>
<sectionHeader confidence="0.984775" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999552714285714">
We proposed a subword-based tagging for
Chinese word segmentation to improve
the existing character-based tagging. The
subword-based tagging was implemented
using the maximum entropy (MaxEnt)
and the conditional random fields (CRF)
methods. We found that the proposed
subword-based tagging outperformed the
character-based tagging in all compara-
tive experiments. In addition, we pro-
posed a confidence measure approach to
combine the results of a dictionary-based
and a subword-tagging-based segmenta-
tion. This approach can produce an
ideal tradeoff between the in-vocaulary
rate and out-of-vocabulary rate. Our tech-
niques were evaluated using the test data
from Sighan Bakeoff 2005. We achieved
higher F-scores than the best results in
three of the four corpora: PKU(0.951),
CITYU(0.950) and MSR(0.971).
</bodyText>
<sectionHeader confidence="0.992487" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999030375">
Many approaches have been proposed in Chinese
word segmentation in the past decades. Segmen-
tation performance has been improved significantly,
from the earliest maximal match (dictionary-based)
approaches to HMM-based (Zhang et al., 2003) ap-
proaches and recent state-of-the-art machine learn-
ing approaches such as maximum entropy (Max-
Ent) (Xue and Shen, 2003), support vector machine
</bodyText>
<note confidence="0.626411">
* Now the second author is affiliated with NTT.
</note>
<bodyText confidence="0.999550294117647">
(SVM) (Kudo and Matsumoto, 2001), conditional
random fields (CRF) (Peng and McCallum, 2004),
and minimum error rate training (Gao et al., 2004).
By analyzing the top results in the first and second
Bakeoffs, (Sproat and Emerson, 2003) and (Emer-
son, 2005), we found the top results were produced
by direct or indirect use of so-called “IOB” tagging,
which converts the problem of word segmentation
into one of character tagging so that part-of-speech
tagging approaches can be used for word segmen-
tation. This approach was also called “LMR” (Xue
and Shen, 2003) or “BIES” (Asahara et al., 2005)
tagging. Under the scheme, each character of a
word is labeled as ”B” if it is the first character of a
multiple-character word, or ”I” otherwise, and ”O”
if the character functioned as an independent word.
For example, “全(whole) 北京市(Beijing city)” is
labeled as “全/O 北/B 京/I 市/I”. Thus, the training
data in word sequences are turned into IOB-labeled
data in character sequences, which are then used as
the training data for tagging. For new test data, word
boundaries are determined based on the results of
tagging.
While the IOB tagging approach has been widely
used in Chinese word segmentation, we found that
so far all the existing implementations were using
character-based IOB tagging. In this work we pro-
pose a subword-based IOB tagging, which assigns
tags to a pre-defined lexicon subset consisting of the
most frequent multiple-character words in addition
to single Chinese characters. If only Chinese char-
acters are used, the subword-based IOB tagging is
downgraded to a character-based one. Taking the
same example mentioned above, “全北京市” is la-
</bodyText>
<note confidence="0.812211">
961
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 961–968,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999891962962963">
beled as “全/O 北京/B 市/I” in the subword-based
tagging, where “北京/B” is labeled as one unit. We
will give a detailed description of this approach in
Section 2.
There exists a clear weakness with the IOB tag-
ging approach: It yields a very low in-vocabulary
rate (R-iv) in return for a higher out-of-vocabulary
(OOV) rate (R-oov). In the results of the closed
test in Bakeoff 2005 (Emerson, 2005), the work
of (Tseng et al., 2005), using CRFs for the IOB tag-
ging, yielded a very high R-oov in all of the four
corpora used, but the R-iv rates were lower. While
OOV recognition is very important in word segmen-
tation, a higher IV rate is also desired. In this work
we propose a confidence measure approach to lessen
this weakness. By this approach we can change the
R-oov and R-iv and find an optimal tradeoff. This
approach will be described in Section 2.3.
In addition, we illustrate our word segmentation
process in Section 2, where the subword-based tag-
ging is described by the MaxEnt method. Section 3
presents our experimental results. The effects using
the MaxEnts and CRFs are shown in this section.
Section 4 describes current state-of-the-art methods
with Chinese word segmentation, with which our re-
sults were compared. Section 5 provides the con-
cluding remarks and outlines future goals.
</bodyText>
<sectionHeader confidence="0.907857" genericHeader="method">
2 Chinese word segmentation framework
</sectionHeader>
<bodyText confidence="0.999466222222222">
Our word segmentation process is illustrated in
Fig. 1. It is composed of three parts: a dictionary-
based N-gram word segmentation for segmenting IV
words, a maximum entropy subword-based tagger
for recognizing OOVs, and a confidence-dependent
word disambiguation used for merging the results
of both the dictionary-based and the IOB-tagging-
based. An example exhibiting each step’s results is
also given in the figure.
</bodyText>
<subsectionHeader confidence="0.6395365">
2.1 Dictionary-based N-gram word
segmentation
</subsectionHeader>
<bodyText confidence="0.9875945">
This approach can achieve a very high R-iv, but no
OOV detection. We combined with it the N-gram
language model (LM) to solve segmentation ambi-
guities. For a given Chinese character sequence,
C = c0c1c2 ... cN, the problem of word segmenta-
tion can be formalized as finding a word sequence,
</bodyText>
<figure confidence="0.867418666666667">
input
咘㣅᯹ԣ೼࣫ҀᏖ
+XDQJ&lt;LQJ&amp;KXQ OLYHV LQ %HLMLQJ-FLW\
Dictionary-based word segmentation
咘 㣅 ᯹ ԣ ೼ ࣫ҀᏖ
+XDQJ &lt;LQJ &amp;KXQ OLYHV LQ %HLMLQJ-FLW\
Subword-based IOB tagging
咘/%/0.94 㣅/,/0.98 ᯹/,/0.91 ԣ/2/0.9 ೼/2/0.89 ࣫Ҁ/%/0.94 Ꮦ/,/0.98
+XDQJ/% &lt;LQJ/, &amp;KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/,
Confidence-based disambiguation
咘/%/0.75 㣅/,/0.78 ᯹/,/0.73 ԣ/2/0.92 ೼/2/0.91 ࣫Ҁ/%/0.95 Ꮦ/,/0.98
+XDQJ/% &lt;LQJ/, &amp;KXQ/, OLYHV/2 LQ/2 %HLMLQJ/% FLW\/,
output
咘㣅᯹ ԣ ೼ ࣫ҀᏖ
+XDQJ&lt;LQJ&amp;KXQ OLYHV LQ %HLMLQJ-FLW\
</figure>
<figureCaption confidence="0.990569">
Figure 1: Outline of word segmentation process
</figureCaption>
<equation confidence="0.9989659">
W = wt0wt1wt2 ... wtM, which satisfies
wt0 = c0 ... ct0, wt1 = ct0+1 ... ct1
wti = cti−1+1 ... cti, wtM = ctM−1+1 ... ctM
ti &gt; ti−1, 0 ≤ ti ≤ N, 0 ≤ i ≤ M
such that
W = arg max P(W|C) = arg max P(W)P(C|W)
W W
P(wt0wt1 ... wtM)S(c0 ... ct0, wt0)
S(ct0+1 ... ct1, wt1) ... S(ctM−1+1 ... cM, wtM)
(1)
</equation>
<bodyText confidence="0.999756875">
We applied Bayes’ law in the above derivation.
Because the word sequence must keep consistent
with the character sequence, P(C|W) is expanded
to be a multiplication of a Kronecker delta function
series, S(u, v), equal to 1 if both arguments are the
same and 0 otherwise. P(wt0wt1 ... wtM) is a lan-
guage model that can be expanded by the chain rule.
If trigram LMs are used, we have
</bodyText>
<equation confidence="0.997352">
P(w0)P(w1|w0)P(w2|w0w1) ··· P(wM|wM−2wM−1)
</equation>
<bodyText confidence="0.997509">
where wi is a shorthand for wti .
Equation 1 indicates the process of dictionary-
based word segmentation. We looked up the lexicon
to find all the IVs, and evaluated the word sequences
by the LMs. We used a beam search (Jelinek, 1998)
instead of a viterbi search to decode the best word
</bodyText>
<equation confidence="0.830657">
= arg max
W
962
</equation>
<bodyText confidence="0.999853142857143">
sequence because we found that a beam search can
speed up the decoding. N-gram LMs were used to
score all the hypotheses, of which the one with the
highest LM scores is the final output. The exper-
imental results are presented in Section 3.1, where
we show the comparative results as we changed the
order of LMs.
</bodyText>
<subsectionHeader confidence="0.996777">
2.2 Subword-based IOB tagging
</subsectionHeader>
<bodyText confidence="0.9999346875">
There are several steps to train a subword-based IOB
tagger. First, we extracted a word list from the train-
ing data sorted in decreasing order by their counts
in the training data. We chose all the single charac-
ters and the top multi-character words as a lexicon
subset for the IOB tagging. If the subset consists of
Chinese characters only, it is a character-based IOB
tagger. We regard the words in the subset as the sub-
words for the IOB tagging.
Second, we re-segmented the words in the train-
ing data into subwords of the subset, and as-
signed IOB tags to them. For the character-
based IOB tagger, there is only one possibility
for re-segmentation. However, there are multi-
ple choices for the subword-based IOB tagger.
For example, “北 京 市(Beijing-city)” can be
segmented as “北 京 市(Beijing-city)/O,” or
“北 京(Beijing)/B 市(city)/I,” or ”北(north)/B
京(capital)/I 市(city)/I.” In this work we used for-
ward maximal match (FMM) for disambiguation.
Because we carried out FMMs on each words in the
manually segmented training data, the accuracy of
FMM was much higher than applying it on whole
sentences. Of course, backward maximal match
(BMM) or other approaches are also applicable. We
did not conduct comparative experiments due to triv-
ial differences in the results of these approaches.
In the third step, we used the maximum entropy
(MaxEnt) approach (the results of CRF are given in
Section 3.4) to train the IOB tagger (Xue and Shen,
2003). The mathematical expression for the MaxEnt
model is
</bodyText>
<equation confidence="0.994502">
P(t |h) = exp IEAi fi (h, t) I /Z, Z = E P(t |h) (2)
i J t
</equation>
<bodyText confidence="0.993543307692308">
where t is a tag, “I,O,B,” of the current word; h,
the context surrounding the current word, including
word and tag sequences; fi, a binary feature equal
to 1 if the i-th defined feature is activated and 0 oth-
erwise; Z, a normalization coefficient; and Ai, the
weight of the i-th feature.
Many kinds of features can be defined for improv-
ing the tagging accuracy. However, to conform to
the constraints of closed test in Bakeoff 2005, some
features, such as syntactic information and character
encodings for numbers and alphabetical characters,
are not allowed. Therefore, we used the features
available only from the provided training corpus.
</bodyText>
<listItem confidence="0.993893">
• Contextual information:
</listItem>
<equation confidence="0.973670333333333">
w0, t−1, w0t−1, w0t−1w1, t−1w1, t−1t−2, w0t−1t−2,
w0w1, w0w1w2, w−1, w0w−1, w0w−1w1,
w−1w1, w−1w−2, w0w−1w−2, w1, w1w2
</equation>
<bodyText confidence="0.8916946">
where w stands for word and t, for IOB tag.
The subscripts are position indicators, where
0 means the current word/tag; −1, −2, the first
or second word/tag to the left; 1, 2, the first or
second word/tag to the right.
</bodyText>
<listItem confidence="0.989766">
• Prefixes and suffixes. These are very useful fea-
</listItem>
<bodyText confidence="0.9916841">
tures. Using the same approach as in (Tseng
et al., 2005), we extracted the most frequent
words tagged with “B”, indicating a prefix, and
the last words tagged with “I”, denoting a suf-
fix. Features containing prefixes and suffixes
were used in the following combinations with
other features, where p stands for prefix; s, suf-
fix; p0 means the current word is a prefix and
s1 denotes that the right first word is a suffix,
and so on.
</bodyText>
<equation confidence="0.7063945">
p0, w0p−1, w0p1, s0, w0s−1, w0s1,
p0w−1, p0w1, s0w−1, s0w−2
</equation>
<listItem confidence="0.974203666666667">
• Word length. This is defined as the number
of characters in a word. The length of a Chi-
nese word has discriminative roles for word
</listItem>
<bodyText confidence="0.970266444444444">
composition. For example, single-character
words are more apt to form new words than
are multiple-character words. Features using
word length are listed below, where l0 means
the word length of the current word. Others can
be inferred similarly.
l0, w0l−1, w0l1, w0l−1l1, l0l−1, l0l1
As to feature selection, we simply adopted the ab-
solute count for each feature in the training data as
</bodyText>
<page confidence="0.709405">
963
</page>
<bodyText confidence="0.999974272727273">
the metric, and defined a cutoff value for each fea-
ture type.
We used IIS to train the maximum entropy model.
For details, refer to (Lafferty et al., 2001).
The tagging algorithm is based on the beam-
search method (Jelinek, 1998). After the IOB tag-
ging, each word is tagged with a B/I/O tag. The
word segmentation is obtained immediately. The
experimental effect of the word-based tagger and
its comparison with the character-based tagger are
made in section 3.2.
</bodyText>
<subsectionHeader confidence="0.997888">
2.3 Confidence-dependent word segmentation
</subsectionHeader>
<bodyText confidence="0.999944214285714">
In the last two steps we produced two segmentation
results: the one by the dictionary-based approach
and the one by the IOB tagging. However, nei-
ther was perfect. The dictionary-based segmenta-
tion produced a result with a higher R-iv but lower
R-oov while the IOB tagging yielded the contrary
results. In this section we introduce a confidence
measure approach to combine the two results. We
define a confidence measure, CM(tiob|w), to measure
the confidence of the results produced by the IOB
tagging by using the results from the dictionary-
based segmentation. The confidence measure comes
from two sources: IOB tagging and dictionary-based
word segmentation. Its calculation is defined as:
</bodyText>
<equation confidence="0.928369">
CM(tiob|w) = αCMiob(tiob|w) + (1 − α)S(tw, tiob)ng
(3)
</equation>
<bodyText confidence="0.999829222222222">
where tiob is the word w’s IOB tag assigned by the
IOB tagging; tw, a prior IOB tag determined by the
results of the dictionary-based segmentation. After
the dictionary-based word segmentation, the words
are re-segmented into subwords by FMM before be-
ing fed to IOB tagging. Each subword is given a
prior IOB tag, tw. CMiob(t|w), a confidence proba-
bility derived in the process of IOB tagging, which
is defined as
</bodyText>
<equation confidence="0.9986">
�hi P(t|w,hi)
CMiob(t|w) =
� �hi P(t|w, hi)
t
</equation>
<bodyText confidence="0.999462666666667">
where hi is a hypothesis in the beam search.
S(tw, tiob)ng denotes the contribution of the
dictionary-based segmentation.
</bodyText>
<equation confidence="0.76840475">
S(tw, tiob)ng is a Kronecker delta function defined
as
1 i f tw = tiob
S(tw, tiob)ng = { 0 otherwise
</equation>
<bodyText confidence="0.999943083333333">
In Eq. 3, α is a weighting between the IOB tag-
ging and the dictionary-based word segmentation.
We found an empirical value 0.8 for α.
By Eq. 3 the results of IOB tagging were re-
evaluated. A confidence measure threshold, t, was
defined for making a decision based on the value.
If the value was lower than t, the IOB tag was re-
jected and the dictionary-based segmentation was
used; otherwise, the IOB tagging segmentation was
used. A new OOV was thus created. For the two
extreme cases, t = 0 is the case of the IOB tag-
ging while t = 1 is that of the dictionary-based ap-
proach. In Section 3.3 we will present the experi-
mental segmentation results of the confidence mea-
sure approach. In a real application, we can actually
change the confidence threshold to obtain a satisfac-
tory balance between R-iv and R-oov.
An example is shown in Figure 1. In the stage of
IOB tagging, a confidence is attached to each word.
In the stage of confidence-based, a new confidence
was made after merging with dictionary-based re-
sults where all single-character words are labeled
as “O” by default except “Beijing-city” labeled as
“Beijing/B” and “city/I”.
</bodyText>
<sectionHeader confidence="0.999213" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.9997790625">
We used the data provided by Sighan Bakeoff 2005
to test our approaches described in the previous sec-
tions. The data contain four corpora from differ-
ent sources: Academia sinica, City University of
Hong Kong, Peking University and Microsoft Re-
search (Beijing). The statistics concerning the cor-
pora is listed in Table 3. The corpora provided both
unicode coding and Big5/GB coding. We used the
Big5 and CP936 encodings. Since the main purpose
of this work is to evaluate the proposed subword-
based IOB tagging, we carried out the closed test
only. Five metrics were used to evaluate the seg-
mentation results: recall (R), precision (P), F-score
(F), OOV rate (R-oov) and IV rate (R-iv). For a de-
tailed explanation of these metrics, refer to (Sproat
and Emerson, 2003).
</bodyText>
<table confidence="0.928853666666667">
964
Corpus Abbrev. Encodings Training size (words) Test size (words)
Academia Sinica AS Big5/Unicode 5.45M 122K
Beijing University PKU CP936/Unicode 1.1M 104K
City University of Hong Kong CITYU Big5/Unicode 1.46M 41K
Microsoft Research (Beijing) MSR CP936/Unicode 2.37M 107K
</table>
<tableCaption confidence="0.999899">
Table 1: Corpus statistics in Sighan Bakeoff 2005
</tableCaption>
<subsectionHeader confidence="0.998001">
3.1 Effects of N-gram LMs
</subsectionHeader>
<bodyText confidence="0.99998705">
We obtained a word list from the training data as the
vocabulary for dictionary-based segmentation. N-
gram LMs were generated using the SRI LM toolkit.
Table 2 shows the performance of N-gram segmen-
tation by changing the order of N-grams.
We found that bigram LMs can improve segmen-
tation over unigram, though we observed no effect
from the trigram LMs. For the PKU corpus, there
was a relatively strong improvement due to using bi-
grams rather than unigrams, posssibly because the
PKU corpus’ training size was smaller than the oth-
ers. For a sufficiently large training corpus, the un-
igram LMs may be enough for segmentation. This
experiment revealed that language models above bi-
grams do not improve word segmentation. Since
there were some single-character words present in
test data but not in the training data, the R-oov rates
were not zero in this experiment. In fact, we did not
use any OOV detection for the dictionary-based ap-
proach.
</bodyText>
<subsectionHeader confidence="0.993367">
3.2 Comparisons of Character-based and
Subword-based tagger
</subsectionHeader>
<bodyText confidence="0.9999455">
In Section 2.2 we described the character-based and
subword-based IOB tagging methods. The main dif-
ference between the two is the lexicon subset used
for re-segmentation. For the subword-based IOB
tagging, we need to add some multiple-character
words into the lexicon subset. Since it is hard to
decide the optimal number of words to add, we test
three different lexicon sizes, as shown in Table 3.
The first one, s1, consisting of all the characters, is
a character-based approach. The second, s2, added
2,500 top words from the training data to the lexi-
con of s1. The third, s3, added another 2,500 top
words to the lexicon of s2. All the words were
among the most frequent in the training corpora. Af-
ter choosing the subwords, the training data were re-
segmented using the subwords by FMM. The final
</bodyText>
<table confidence="0.985401">
AS CITYU MSR PKU
s1 6,087 4,916 5,150 4,685
s2 8,332 7,338 7,464 7,014
s3 10,876 9,996 9,990 9,053
</table>
<tableCaption confidence="0.943861333333333">
Table 3: Three different vocabulary sizes used in subword-
based tagging. s1 contains all the characters. s2 and s3 contains
some common words.
</tableCaption>
<bodyText confidence="0.999981692307692">
lexicons were collected again, consisting of single-
character words and multiple-character words. Ta-
ble 3 shows the sizes of the final lexicons. There-
fore, the minus of the lexicon size of s2 to s1 are not
2,500, exactly.
The segmentation results of using three lexicons
are shown in Table 4. The numbers are separated
by a “/” in the sequence of “s1/s2/s3.” We found al-
though the subword-based approach outperformed
the character-based one significantly, there was no
obvious difference between the two subword-based
approaches, s2 and s3, adding respective 2,500 and
5,000 subwords to s1. The experiments show that
we cannot find an optimal lexicon size from 2,500
to 5,000. However, there might be an optimal point
less than 2,500. We did not take much effort to find
the optimal point, and regarded 2,500 as an accept-
able size for practical usages.
The F-scores of IOB tagging shown in Table 4 are
better than that of N-gram word segmentation in Ta-
ble 2, which proves that the IOB tagging is effective
in recognizing OOV. However, we found there was a
large decrease in the R-ivs, which shows the weak-
ness of the IOB tagging approach. We use the con-
fidence measure approach to deal with this problem
in next section.
</bodyText>
<subsectionHeader confidence="0.994886">
3.3 Effects of the confidence measure
</subsectionHeader>
<bodyText confidence="0.9998986">
Up to now we had two segmentation results by using
the dictionary-based word segmentation and the IOB
tagging. In Section 2.3, we proposed a confidence
measure approach to re-evaluate the results of IOB
tagging by combining the two results. The effects of
</bodyText>
<page confidence="0.698161">
965
</page>
<table confidence="0.9999532">
R P F R-oov R-iv
AS 0.934/0.942/0.941 0.884/0.881/0.881 0.909/0.910/0.910 0.041/0.040/0.038 0.975/0.983/0.982
CITYU 0.924/0.929/0.928 0.851/0.851/0.851 0.886/0.888/0.888 0.162/0.162/0.164 0.984/0.990/0.989
PKU 0.938/0.949/0.948 0.909/0.912/0.912 0.924/0.930/0.930 0.407/0.403/0.408 0.971/0.982/0.981
MSR 0.965/0.969/0.968 0.927/0.927/0.927 0.946/0.947/0.947 0.036/0.036/0.048 0.991/0.994/0.993
</table>
<tableCaption confidence="0.9904125">
Table 2: Segmentation results of dictionary-based segmentation in closed test of Bakeoff 2005. A “/” separates the results of
unigram, bigram and trigram.
</tableCaption>
<table confidence="0.9999742">
R P F R-oov R-iv
AS 0.922/0.942/0.943 0.914/0.930/0.930 0.918/0.936/0.937 0.641/0.628/0.609 0.935/0.956/0.959
CITYU 0.906/0.933/0.934 0.905/0.929/0.927 0.906/0.931/0.930 0.668/0.671/0.671 0.925/0.954/0.955
PKU 0.913/0.934/0.936 0.922/0.938/0.940 0.918/0.936/0.938 0.744/0.724/0.713 0.924/0.946/0.949
MSR 0.929/0.953/0.953 0.934/0.955/0.952 0.932/0.954/0.952 0.656/0.684/0.665 0.936/0.961/0.961
</table>
<tableCaption confidence="0.9773685">
Table 4: Segmentation results by the pure subword-based IOB tagging. The separator “/” divides the results by three lexicon sizes
as illustrated in Table 3. The first is character-based (s1), while the other two are subword-based with different lexicons (s2/s3).
</tableCaption>
<figure confidence="0.690898">
R-oov
</figure>
<figureCaption confidence="0.997886">
Figure 2: R-iv and R-oov varing as the confidence threshold, t.
</figureCaption>
<bodyText confidence="0.999798962962963">
the confidence measure are shown in Table 5, where
we used α = 0.8 and confidence threshold t = 0.7.
These are empirical numbers. We obtained the opti-
mal values by multiple trials on held-out data. The
numbers in the slots of Table 5 are divided by a sep-
arator “/” and displayed as the sequence “s1/s2/s3”,
just as Table 4. We found that the results in Table 5
were better than those in Table 4 and Table 2, which
proved that using the confidence measure approach
yielded the best performance over the N-gram seg-
mentation and the IOB tagging approaches.
Even with the use of the confidence measure, the
subword-based IOB tagging still outperformed the
character-based IOB tagging, proving that the pro-
posed subword-based IOB tagging was very effec-
tive. Though the improvement under the confidence
measure was decreasing, it was still significant.
We can change the R-oov and R-iv by changing
the confidence threshold. The effect of R-oov and R-
iv’s varing as the threshold is shown in Fig. 2, where
R-oovs and R-ivs are moving in different directions.
When the confidence threshold t = 0, the case for
the IOB tagging, R-oovs are maximal. When t = 1,
representing the dictionary-based segmentation, R-
oovs are the minimal. The R-oovs and R-ivs varied
largely at the start and end point but little around the
middle section.
</bodyText>
<subsectionHeader confidence="0.990686">
3.4 Subword-based tagging by CRFs
</subsectionHeader>
<bodyText confidence="0.999989">
Our proposed approaches were presented and eval-
uated using the MaxEnt method in the previous
sections. When we turned to CRF-based tagging,
we found a same effect as the MaxEnt method.
Our subword-based tagging by CRFs was imple-
mented by the package “CRF++” from the site
“http://www.chasen.org/˜taku/software.”
We repeated the previous sections’ experiments
using the CRF approach except that we did one of
the two subword-based tagging, the lexicon size s3.
The same values of the confidence measure thresh-
old and α were used. The results are shown in Ta-
ble 6.
We found that the results using the CRFs were
much better than those of the MaxEnts. How-
ever, the emphasis here was not to compare CRFs
and MaxEnts but the effect of subword-based IOB
tagging. In Table 6, the results before ”/” are
the character-based IOB tagging and after ”/”, the
subword-based. It was clear that the subword-based
approaches yielded better results than the character-
based approach though the improvement was not as
higher as that of the MaxEnt approaches. There was
</bodyText>
<figure confidence="0.962044888888889">
0.98
0.94
0.95
0.96
0.97
0.99
1
t=0
t=0
t=0
t=0
t=1
AS
CITYU
PKU
MSR
t=0
t=1
</figure>
<page confidence="0.595966">
t=1
</page>
<table confidence="0.930898714285714">
0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0
966
R P F R-oov R-iv
AS 0.938/0.950/0.953 0.945/0.946/0.951 0.941/0.948/0.948 0.674/0.641/0.606 0.950/0.964/0.969
CITYU 0.932/0.949/0.946 0.944/0.933/0.944 0.938/0.941/0.945 0.705/0.597/0.667 0.950/0.977/0.968
PKU 0.941/0.948/0.949 0.945/0.947/0.947 0.943/0.948/0.948 0.672/0.662/0.660 0.958/0.966/0.966
MSR 0.944/0.959/0.961 0.959/0.964/0.963 0.951/0.961/0.962 0.671/0.674/0.631 0.951/0.967/0.970
</table>
<tableCaption confidence="0.9136745">
Table 5: Effects of combination using the confidence measure. Here we used α = 0.8 and confidence threshold t = 0.7. The
separator “/” divides the results of s1, s2, and s3.
</tableCaption>
<bodyText confidence="0.994458333333333">
no change on F-score for AS corpus, but a better re-
call rate was found. Our results are better than the
best one of Bakeoff 2005 in PKU, CITYU and MSR
corpora.
Detailed descriptions about subword tagging by
CRF can be found in our paper (Zhang et al., 2006).
</bodyText>
<sectionHeader confidence="0.984048" genericHeader="method">
4 Discussion and Related works
</sectionHeader>
<bodyText confidence="0.999975263157895">
The IOB tagging approach adopted in this work is
not a new idea. It was first implemented in Chi-
nese word segmentation by (Xue and Shen, 2003)
using the maximum entropy methods. Later, (Peng
and McCallum, 2004) implemented the idea us-
ing the CRF-based approach, which yielded bet-
ter results than the maximum entropy approach be-
cause it could solve the label bias problem (Laf-
ferty et al., 2001). However, as we mentioned be-
fore, this approach does not take advantage of the
prior knowledge of in-vocabulary words; It pro-
duced a higher R-oov but a lower R-iv. This prob-
lem has been observed by some participants in the
Bakeoff 2005 (Asahara et al., 2005), where they
applied the IOB tagging to recognize OOVs, and
added the OOVs to the lexicon used in the HMM-
based or CRF-based approaches. (Nakagawa, 2004)
used hybrid HMM models to integrate word level
and character level information seamlessly. We
used confidence measure to determine a better bal-
ance between R-oov and R-iv. The idea of us-
ing the confidence measure has appeared in (Peng
and McCallum, 2004), where it was used to recog-
nize the OOVs. In this work we used it more than
that. By way of the confidence measure we com-
bined results from the dictionary-based and the IOB-
tagging-based and as a result, we could achieve the
optimal performance.
Our main contribution is to extend the IOB tag-
ging approach from being a character-based to a
subword-based one. We proved that the new ap-
proach enhanced the word segmentation signifi-
cantly in all the experiments, MaxEnts, CRFs and
using confidence measure. We tested our approach
using the standard Sighan Bakeoff 2005 data set in
the closed test. In Table 7 we align our results with
some top runners’ in the Bakeoff 2005.
Our results were compared with the best perform-
ers’ results in the Bakeoff 2005. Two participants’
results were chosen as bases: No.15-b, ranked the
first in the AS corpus, and No.14, the best per-
former in CITYU, MSR and PKU.. The No.14
used CRF-modeled IOB tagging while No.15-b used
MaxEnt-modeled IOB tagging. Our results pro-
duced by the MaxEnt are denoted as “ours(ME)”
while “ours(CRF)” for the CRF approaches. We
achieved the highest F-scores in three corpora ex-
cept the AS corpus. We think the proposed subword-
based approach played the important role for the
achieved good results.
A second advantage of the subword-based IOB
tagging over the character-based is its speed. The
subword-based approach is faster because fewer
words than characters needed to be labeled. We ob-
served a speed increase in both training and testing.
In the training stage, the subword approach was al-
most two times faster than the character-based.
</bodyText>
<sectionHeader confidence="0.998924" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9995116">
In this work, we proposed a subword-based IOB tag-
ging method for Chinese word segmentation. The
approach outperformed the character-based method
using both the MaxEnt and CRF approaches. We
also successfully employed the confidence measure
to make a confidence-dependent word segmentation.
By setting the confidence threshold, R-oov and R-iv
can be changed accordingly. This approach is effec-
tive for performing desired segmentation based on
users’ requirements to R-oov and R-iv.
</bodyText>
<table confidence="0.909540833333333">
967
R P F R-oov R-iv
AS 0.953/0.956 0.944/0.947 0.948/0.951 0.607/0.649 0.969/0.969
CITYU 0.943/0.952 0.948/0.949 0.946/0.951 0.682/0.741 0.964/0.969
PKU 0.942/0.947 0.957/0.955 0.949/0.951 0.775/0.748 0.952/0.959
MSR 0.960/0.972 0.966/0.969 0.963/0.971 0.674/0.712 0.967/0.976
</table>
<tableCaption confidence="0.978396">
Table 6: Effects of using CRF. The separator “/” divides the results of s1, and s3.
</tableCaption>
<table confidence="0.999960714285714">
Participants R P F R-oov R-iv
Hong Kong City University
ours(CRF) 0.952 0.949 0.951 0.741 0.969
ours(ME) 0.946 0.944 0.945 0.667 0.968
14 0.941 0.946 0.943 0.698 0.961
15-b 0.937 0.946 0.941 0.736 0.953
Academia Sinica
15-b 0.952 0.951 0.952 0.696 0.963
ours(CRF) 0.956 0.947 0.951 0.649 0.969
ours(ME) 0.953 0.943 0.948 0.608 0.969
14 0.95 0.943 0.947 0.718 0.960
Microsoft Research
ours(CRF) 0.972 0.969 0.971 0.712 0.976
14 0.962 0.966 0.964 0.717 0.968
ours(ME) 0.961 0.963 0.962 0.631 0.970
15-b 0.952 0.964 0.958 0.718 0.958
Peking University
ours(CRF) 0.947 0.955 0.951 0.748 0.959
14 0.946 0.954 0.950 0.787 0.956
ours(ME) 0.949 0.947 0.948 0.660 0.966
15-b 0.93 0.951 0.941 0.76 0.941
</table>
<tableCaption confidence="0.998769">
Table 7: List of results in Sighan Bakeoff 2005
</tableCaption>
<sectionHeader confidence="0.993434" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999993333333333">
The authors thank the reviewers for the comments
and advice on the paper. Some related software for
this work will be released very soon.
</bodyText>
<sectionHeader confidence="0.99432" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999764222222222">
Masayuki Asahara, Kenta Fukuoka, Ai Azuma, Chooi-
Ling Goh, Yotaro Watanabe, Yuji Matsumoto, and
Takashi Tsuzuki. 2005. Combination of machine
learning methods for optimum chinese word seg-
mentation. In Forth SIGHAN Workshop on Chinese
Language Processing, Proceedings of the Workshop,
pages 134–137, Jeju, Korea.
Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. In Proceedings of
the Fourth SIGHAN Workshop on Chinese Language
Processing, Jeju, Korea.
Jianfeng Gao, Andi Wu, Mu Li, Chang-Ning Huang,
Hongqiao Li, Xinsong Xia, and Haowei Qin. 2004.
Adaptive chinese word segmentation. In ACL-2004,
Barcelona, July.
Frederick Jelinek. 1998. Statistical methods for speech
recognition. the MIT Press.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with
support vector machine. In Proc. of NAACL-2001,
pages 192–199.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: probabilistic models
for segmenting and labeling sequence data. In Proc. of
ICML-2001, pages 591–598.
Tetsuji Nakagawa. 2004. Chinese and japanese word
segmentation using word-level and character-level in-
formation. In Proceedings of Coling 2004, pages 466–
472, Geneva, August.
Fuchun Peng and Andrew McCallum. 2004. Chinese
segmentation and new word detection using condi-
tional random fields. In Proc. of Coling-2004, pages
562–568, Geneva, Switzerland.
Richard Sproat and Tom Emerson. 2003. The first inter-
national chinese word segmentation bakeoff. In Pro-
ceedings of the Second SIGHAN Workshop on Chinese
Language Processing, Sapporo, Japan, July.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for Sighan bake-
off 2005. In Proceedings of the Fourth SIGHAN Work-
shop on Chinese Language Processing, Jeju, Korea.
Nianwen Xue and Libin Shen. 2003. Chinese word
segmentation as LMR tagging. In Proceedings of the
Second SIGHAN Workshop on Chinese Language Pro-
cessing.
Huaping Zhang, HongKui Yu, Deyi xiong, and Qun Liu.
2003. HHMM-based Chinese lexical analyzer ICT-
CLAS. In Proceedings of the Second SIGHAN Work-
shop on Chinese Language Processing, pages 184–
187.
Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita.
2006. Subword-based tagging by conditional random
fields for chinese word segmentation. In Proc. of HLT-
NAACL.
</reference>
<page confidence="0.906819">
968
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.230010">
<title confidence="0.8847805">Subword-based Tagging for Confidence-dependent Chinese Word Segmentation</title>
<affiliation confidence="0.991381">Institute of Information and Communications Technology Spoken Language Communication Research Laboratories</affiliation>
<address confidence="0.971527">2-2-2 Hikaridai, Seiika-cho, Soraku-gun, Kyoto, 619-0288, Japan</address>
<abstract confidence="0.987435095238095">We proposed a subword-based tagging for Chinese word segmentation to improve the existing character-based tagging. The subword-based tagging was implemented using the maximum entropy (MaxEnt) and the conditional random fields (CRF) methods. We found that the proposed subword-based tagging outperformed the character-based tagging in all comparative experiments. In addition, we proposed a confidence measure approach to combine the results of a dictionary-based and a subword-tagging-based segmentation. This approach can produce an ideal tradeoff between the in-vocaulary rate and out-of-vocabulary rate. Our techniques were evaluated using the test data from Sighan Bakeoff 2005. We achieved higher F-scores than the best results in three of the four corpora: PKU(0.951),</abstract>
<note confidence="0.525615">CITYU(0.950) and MSR(0.971).</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Masayuki Asahara</author>
<author>Kenta Fukuoka</author>
<author>Ai Azuma</author>
<author>ChooiLing Goh</author>
<author>Yotaro Watanabe</author>
<author>Yuji Matsumoto</author>
<author>Takashi Tsuzuki</author>
</authors>
<title>Combination of machine learning methods for optimum chinese word segmentation.</title>
<date>2005</date>
<booktitle>In Forth SIGHAN Workshop on Chinese Language Processing, Proceedings of the Workshop,</booktitle>
<pages>134--137</pages>
<location>Jeju,</location>
<contexts>
<context position="2214" citStr="Asahara et al., 2005" startWordPosition="310" endWordPosition="313">hor is affiliated with NTT. (SVM) (Kudo and Matsumoto, 2001), conditional random fields (CRF) (Peng and McCallum, 2004), and minimum error rate training (Gao et al., 2004). By analyzing the top results in the first and second Bakeoffs, (Sproat and Emerson, 2003) and (Emerson, 2005), we found the top results were produced by direct or indirect use of so-called “IOB” tagging, which converts the problem of word segmentation into one of character tagging so that part-of-speech tagging approaches can be used for word segmentation. This approach was also called “LMR” (Xue and Shen, 2003) or “BIES” (Asahara et al., 2005) tagging. Under the scheme, each character of a word is labeled as ”B” if it is the first character of a multiple-character word, or ”I” otherwise, and ”O” if the character functioned as an independent word. For example, “全(whole) 北京市(Beijing city)” is labeled as “全/O 北/B 京/I 市/I”. Thus, the training data in word sequences are turned into IOB-labeled data in character sequences, which are then used as the training data for tagging. For new test data, word boundaries are determined based on the results of tagging. While the IOB tagging approach has been widely used in Chinese word segmentation,</context>
<context position="24113" citStr="Asahara et al., 2005" startWordPosition="3909" endWordPosition="3912"> in this work is not a new idea. It was first implemented in Chinese word segmentation by (Xue and Shen, 2003) using the maximum entropy methods. Later, (Peng and McCallum, 2004) implemented the idea using the CRF-based approach, which yielded better results than the maximum entropy approach because it could solve the label bias problem (Lafferty et al., 2001). However, as we mentioned before, this approach does not take advantage of the prior knowledge of in-vocabulary words; It produced a higher R-oov but a lower R-iv. This problem has been observed by some participants in the Bakeoff 2005 (Asahara et al., 2005), where they applied the IOB tagging to recognize OOVs, and added the OOVs to the lexicon used in the HMMbased or CRF-based approaches. (Nakagawa, 2004) used hybrid HMM models to integrate word level and character level information seamlessly. We used confidence measure to determine a better balance between R-oov and R-iv. The idea of using the confidence measure has appeared in (Peng and McCallum, 2004), where it was used to recognize the OOVs. In this work we used it more than that. By way of the confidence measure we combined results from the dictionary-based and the IOBtagging-based and as</context>
</contexts>
<marker>Asahara, Fukuoka, Azuma, Goh, Watanabe, Matsumoto, Tsuzuki, 2005</marker>
<rawString>Masayuki Asahara, Kenta Fukuoka, Ai Azuma, ChooiLing Goh, Yotaro Watanabe, Yuji Matsumoto, and Takashi Tsuzuki. 2005. Combination of machine learning methods for optimum chinese word segmentation. In Forth SIGHAN Workshop on Chinese Language Processing, Proceedings of the Workshop, pages 134–137, Jeju, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Emerson</author>
</authors>
<title>The second international chinese word segmentation bakeoff.</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<location>Jeju,</location>
<contexts>
<context position="1875" citStr="Emerson, 2005" startWordPosition="255" endWordPosition="257">ast decades. Segmentation performance has been improved significantly, from the earliest maximal match (dictionary-based) approaches to HMM-based (Zhang et al., 2003) approaches and recent state-of-the-art machine learning approaches such as maximum entropy (MaxEnt) (Xue and Shen, 2003), support vector machine * Now the second author is affiliated with NTT. (SVM) (Kudo and Matsumoto, 2001), conditional random fields (CRF) (Peng and McCallum, 2004), and minimum error rate training (Gao et al., 2004). By analyzing the top results in the first and second Bakeoffs, (Sproat and Emerson, 2003) and (Emerson, 2005), we found the top results were produced by direct or indirect use of so-called “IOB” tagging, which converts the problem of word segmentation into one of character tagging so that part-of-speech tagging approaches can be used for word segmentation. This approach was also called “LMR” (Xue and Shen, 2003) or “BIES” (Asahara et al., 2005) tagging. Under the scheme, each character of a word is labeled as ”B” if it is the first character of a multiple-character word, or ”I” otherwise, and ”O” if the character functioned as an independent word. For example, “全(whole) 北京市(Beijing city)” is labeled </context>
<context position="3819" citStr="Emerson, 2005" startWordPosition="571" endWordPosition="572">er-based one. Taking the same example mentioned above, “全北京市” is la961 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 961–968, Sydney, July 2006. c�2006 Association for Computational Linguistics beled as “全/O 北京/B 市/I” in the subword-based tagging, where “北京/B” is labeled as one unit. We will give a detailed description of this approach in Section 2. There exists a clear weakness with the IOB tagging approach: It yields a very low in-vocabulary rate (R-iv) in return for a higher out-of-vocabulary (OOV) rate (R-oov). In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al., 2005), using CRFs for the IOB tagging, yielded a very high R-oov in all of the four corpora used, but the R-iv rates were lower. While OOV recognition is very important in word segmentation, a higher IV rate is also desired. In this work we propose a confidence measure approach to lessen this weakness. By this approach we can change the R-oov and R-iv and find an optimal tradeoff. This approach will be described in Section 2.3. In addition, we illustrate our word segmentation process in Section 2, where the subword-based tagging is described by the MaxEnt method. S</context>
</contexts>
<marker>Emerson, 2005</marker>
<rawString>Thomas Emerson. 2005. The second international chinese word segmentation bakeoff. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, Jeju, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Andi Wu</author>
<author>Mu Li</author>
</authors>
<title>Chang-Ning Huang, Hongqiao Li, Xinsong Xia, and Haowei Qin.</title>
<date>2004</date>
<booktitle>In ACL-2004,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="1764" citStr="Gao et al., 2004" startWordPosition="235" endWordPosition="238">TYU(0.950) and MSR(0.971). 1 Introduction Many approaches have been proposed in Chinese word segmentation in the past decades. Segmentation performance has been improved significantly, from the earliest maximal match (dictionary-based) approaches to HMM-based (Zhang et al., 2003) approaches and recent state-of-the-art machine learning approaches such as maximum entropy (MaxEnt) (Xue and Shen, 2003), support vector machine * Now the second author is affiliated with NTT. (SVM) (Kudo and Matsumoto, 2001), conditional random fields (CRF) (Peng and McCallum, 2004), and minimum error rate training (Gao et al., 2004). By analyzing the top results in the first and second Bakeoffs, (Sproat and Emerson, 2003) and (Emerson, 2005), we found the top results were produced by direct or indirect use of so-called “IOB” tagging, which converts the problem of word segmentation into one of character tagging so that part-of-speech tagging approaches can be used for word segmentation. This approach was also called “LMR” (Xue and Shen, 2003) or “BIES” (Asahara et al., 2005) tagging. Under the scheme, each character of a word is labeled as ”B” if it is the first character of a multiple-character word, or ”I” otherwise, an</context>
</contexts>
<marker>Gao, Wu, Li, 2004</marker>
<rawString>Jianfeng Gao, Andi Wu, Mu Li, Chang-Ning Huang, Hongqiao Li, Xinsong Xia, and Haowei Qin. 2004. Adaptive chinese word segmentation. In ACL-2004, Barcelona, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
</authors>
<title>Statistical methods for speech recognition.</title>
<date>1998</date>
<publisher>the MIT Press.</publisher>
<contexts>
<context position="6998" citStr="Jelinek, 1998" startWordPosition="1102" endWordPosition="1103">n. Because the word sequence must keep consistent with the character sequence, P(C|W) is expanded to be a multiplication of a Kronecker delta function series, S(u, v), equal to 1 if both arguments are the same and 0 otherwise. P(wt0wt1 ... wtM) is a language model that can be expanded by the chain rule. If trigram LMs are used, we have P(w0)P(w1|w0)P(w2|w0w1) ··· P(wM|wM−2wM−1) where wi is a shorthand for wti . Equation 1 indicates the process of dictionarybased word segmentation. We looked up the lexicon to find all the IVs, and evaluated the word sequences by the LMs. We used a beam search (Jelinek, 1998) instead of a viterbi search to decode the best word = arg max W 962 sequence because we found that a beam search can speed up the decoding. N-gram LMs were used to score all the hypotheses, of which the one with the highest LM scores is the final output. The experimental results are presented in Section 3.1, where we show the comparative results as we changed the order of LMs. 2.2 Subword-based IOB tagging There are several steps to train a subword-based IOB tagger. First, we extracted a word list from the training data sorted in decreasing order by their counts in the training data. We chose</context>
<context position="11262" citStr="Jelinek, 1998" startWordPosition="1844" endWordPosition="1845">sition. For example, single-character words are more apt to form new words than are multiple-character words. Features using word length are listed below, where l0 means the word length of the current word. Others can be inferred similarly. l0, w0l−1, w0l1, w0l−1l1, l0l−1, l0l1 As to feature selection, we simply adopted the absolute count for each feature in the training data as 963 the metric, and defined a cutoff value for each feature type. We used IIS to train the maximum entropy model. For details, refer to (Lafferty et al., 2001). The tagging algorithm is based on the beamsearch method (Jelinek, 1998). After the IOB tagging, each word is tagged with a B/I/O tag. The word segmentation is obtained immediately. The experimental effect of the word-based tagger and its comparison with the character-based tagger are made in section 3.2. 2.3 Confidence-dependent word segmentation In the last two steps we produced two segmentation results: the one by the dictionary-based approach and the one by the IOB tagging. However, neither was perfect. The dictionary-based segmentation produced a result with a higher R-iv but lower R-oov while the IOB tagging yielded the contrary results. In this section we i</context>
</contexts>
<marker>Jelinek, 1998</marker>
<rawString>Frederick Jelinek. 1998. Statistical methods for speech recognition. the MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Chunking with support vector machine.</title>
<date>2001</date>
<booktitle>In Proc. of NAACL-2001,</booktitle>
<pages>192--199</pages>
<contexts>
<context position="1653" citStr="Kudo and Matsumoto, 2001" startWordPosition="218" endWordPosition="221">rom Sighan Bakeoff 2005. We achieved higher F-scores than the best results in three of the four corpora: PKU(0.951), CITYU(0.950) and MSR(0.971). 1 Introduction Many approaches have been proposed in Chinese word segmentation in the past decades. Segmentation performance has been improved significantly, from the earliest maximal match (dictionary-based) approaches to HMM-based (Zhang et al., 2003) approaches and recent state-of-the-art machine learning approaches such as maximum entropy (MaxEnt) (Xue and Shen, 2003), support vector machine * Now the second author is affiliated with NTT. (SVM) (Kudo and Matsumoto, 2001), conditional random fields (CRF) (Peng and McCallum, 2004), and minimum error rate training (Gao et al., 2004). By analyzing the top results in the first and second Bakeoffs, (Sproat and Emerson, 2003) and (Emerson, 2005), we found the top results were produced by direct or indirect use of so-called “IOB” tagging, which converts the problem of word segmentation into one of character tagging so that part-of-speech tagging approaches can be used for word segmentation. This approach was also called “LMR” (Xue and Shen, 2003) or “BIES” (Asahara et al., 2005) tagging. Under the scheme, each charac</context>
</contexts>
<marker>Kudo, Matsumoto, 2001</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2001. Chunking with support vector machine. In Proc. of NAACL-2001, pages 192–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proc. of ICML-2001,</booktitle>
<pages>591--598</pages>
<contexts>
<context position="11189" citStr="Lafferty et al., 2001" startWordPosition="1830" endWordPosition="1833">s in a word. The length of a Chinese word has discriminative roles for word composition. For example, single-character words are more apt to form new words than are multiple-character words. Features using word length are listed below, where l0 means the word length of the current word. Others can be inferred similarly. l0, w0l−1, w0l1, w0l−1l1, l0l−1, l0l1 As to feature selection, we simply adopted the absolute count for each feature in the training data as 963 the metric, and defined a cutoff value for each feature type. We used IIS to train the maximum entropy model. For details, refer to (Lafferty et al., 2001). The tagging algorithm is based on the beamsearch method (Jelinek, 1998). After the IOB tagging, each word is tagged with a B/I/O tag. The word segmentation is obtained immediately. The experimental effect of the word-based tagger and its comparison with the character-based tagger are made in section 3.2. 2.3 Confidence-dependent word segmentation In the last two steps we produced two segmentation results: the one by the dictionary-based approach and the one by the IOB tagging. However, neither was perfect. The dictionary-based segmentation produced a result with a higher R-iv but lower R-oov</context>
<context position="23854" citStr="Lafferty et al., 2001" startWordPosition="3862" endWordPosition="3866">as found. Our results are better than the best one of Bakeoff 2005 in PKU, CITYU and MSR corpora. Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006). 4 Discussion and Related works The IOB tagging approach adopted in this work is not a new idea. It was first implemented in Chinese word segmentation by (Xue and Shen, 2003) using the maximum entropy methods. Later, (Peng and McCallum, 2004) implemented the idea using the CRF-based approach, which yielded better results than the maximum entropy approach because it could solve the label bias problem (Lafferty et al., 2001). However, as we mentioned before, this approach does not take advantage of the prior knowledge of in-vocabulary words; It produced a higher R-oov but a lower R-iv. This problem has been observed by some participants in the Bakeoff 2005 (Asahara et al., 2005), where they applied the IOB tagging to recognize OOVs, and added the OOVs to the lexicon used in the HMMbased or CRF-based approaches. (Nakagawa, 2004) used hybrid HMM models to integrate word level and character level information seamlessly. We used confidence measure to determine a better balance between R-oov and R-iv. The idea of usin</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: probabilistic models for segmenting and labeling sequence data. In Proc. of ICML-2001, pages 591–598.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuji Nakagawa</author>
</authors>
<title>Chinese and japanese word segmentation using word-level and character-level information.</title>
<date>2004</date>
<booktitle>In Proceedings of Coling</booktitle>
<pages>466--472</pages>
<location>Geneva,</location>
<contexts>
<context position="24265" citStr="Nakagawa, 2004" startWordPosition="3937" endWordPosition="3938"> and McCallum, 2004) implemented the idea using the CRF-based approach, which yielded better results than the maximum entropy approach because it could solve the label bias problem (Lafferty et al., 2001). However, as we mentioned before, this approach does not take advantage of the prior knowledge of in-vocabulary words; It produced a higher R-oov but a lower R-iv. This problem has been observed by some participants in the Bakeoff 2005 (Asahara et al., 2005), where they applied the IOB tagging to recognize OOVs, and added the OOVs to the lexicon used in the HMMbased or CRF-based approaches. (Nakagawa, 2004) used hybrid HMM models to integrate word level and character level information seamlessly. We used confidence measure to determine a better balance between R-oov and R-iv. The idea of using the confidence measure has appeared in (Peng and McCallum, 2004), where it was used to recognize the OOVs. In this work we used it more than that. By way of the confidence measure we combined results from the dictionary-based and the IOBtagging-based and as a result, we could achieve the optimal performance. Our main contribution is to extend the IOB tagging approach from being a character-based to a subwo</context>
</contexts>
<marker>Nakagawa, 2004</marker>
<rawString>Tetsuji Nakagawa. 2004. Chinese and japanese word segmentation using word-level and character-level information. In Proceedings of Coling 2004, pages 466– 472, Geneva, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Andrew McCallum</author>
</authors>
<title>Chinese segmentation and new word detection using conditional random fields.</title>
<date>2004</date>
<booktitle>In Proc. of Coling-2004,</booktitle>
<pages>562--568</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="1712" citStr="Peng and McCallum, 2004" startWordPosition="226" endWordPosition="229">e best results in three of the four corpora: PKU(0.951), CITYU(0.950) and MSR(0.971). 1 Introduction Many approaches have been proposed in Chinese word segmentation in the past decades. Segmentation performance has been improved significantly, from the earliest maximal match (dictionary-based) approaches to HMM-based (Zhang et al., 2003) approaches and recent state-of-the-art machine learning approaches such as maximum entropy (MaxEnt) (Xue and Shen, 2003), support vector machine * Now the second author is affiliated with NTT. (SVM) (Kudo and Matsumoto, 2001), conditional random fields (CRF) (Peng and McCallum, 2004), and minimum error rate training (Gao et al., 2004). By analyzing the top results in the first and second Bakeoffs, (Sproat and Emerson, 2003) and (Emerson, 2005), we found the top results were produced by direct or indirect use of so-called “IOB” tagging, which converts the problem of word segmentation into one of character tagging so that part-of-speech tagging approaches can be used for word segmentation. This approach was also called “LMR” (Xue and Shen, 2003) or “BIES” (Asahara et al., 2005) tagging. Under the scheme, each character of a word is labeled as ”B” if it is the first characte</context>
<context position="23670" citStr="Peng and McCallum, 2004" startWordPosition="3831" endWordPosition="3834">ce measure. Here we used α = 0.8 and confidence threshold t = 0.7. The separator “/” divides the results of s1, s2, and s3. no change on F-score for AS corpus, but a better recall rate was found. Our results are better than the best one of Bakeoff 2005 in PKU, CITYU and MSR corpora. Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006). 4 Discussion and Related works The IOB tagging approach adopted in this work is not a new idea. It was first implemented in Chinese word segmentation by (Xue and Shen, 2003) using the maximum entropy methods. Later, (Peng and McCallum, 2004) implemented the idea using the CRF-based approach, which yielded better results than the maximum entropy approach because it could solve the label bias problem (Lafferty et al., 2001). However, as we mentioned before, this approach does not take advantage of the prior knowledge of in-vocabulary words; It produced a higher R-oov but a lower R-iv. This problem has been observed by some participants in the Bakeoff 2005 (Asahara et al., 2005), where they applied the IOB tagging to recognize OOVs, and added the OOVs to the lexicon used in the HMMbased or CRF-based approaches. (Nakagawa, 2004) used</context>
</contexts>
<marker>Peng, McCallum, 2004</marker>
<rawString>Fuchun Peng and Andrew McCallum. 2004. Chinese segmentation and new word detection using conditional random fields. In Proc. of Coling-2004, pages 562–568, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Tom Emerson</author>
</authors>
<title>The first international chinese word segmentation bakeoff.</title>
<date>2003</date>
<booktitle>In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing,</booktitle>
<location>Sapporo, Japan,</location>
<contexts>
<context position="1855" citStr="Sproat and Emerson, 2003" startWordPosition="250" endWordPosition="253">nese word segmentation in the past decades. Segmentation performance has been improved significantly, from the earliest maximal match (dictionary-based) approaches to HMM-based (Zhang et al., 2003) approaches and recent state-of-the-art machine learning approaches such as maximum entropy (MaxEnt) (Xue and Shen, 2003), support vector machine * Now the second author is affiliated with NTT. (SVM) (Kudo and Matsumoto, 2001), conditional random fields (CRF) (Peng and McCallum, 2004), and minimum error rate training (Gao et al., 2004). By analyzing the top results in the first and second Bakeoffs, (Sproat and Emerson, 2003) and (Emerson, 2005), we found the top results were produced by direct or indirect use of so-called “IOB” tagging, which converts the problem of word segmentation into one of character tagging so that part-of-speech tagging approaches can be used for word segmentation. This approach was also called “LMR” (Xue and Shen, 2003) or “BIES” (Asahara et al., 2005) tagging. Under the scheme, each character of a word is labeled as ”B” if it is the first character of a multiple-character word, or ”I” otherwise, and ”O” if the character functioned as an independent word. For example, “全(whole) 北京市(Beijin</context>
<context position="14886" citStr="Sproat and Emerson, 2003" startWordPosition="2453" endWordPosition="2456">om different sources: Academia sinica, City University of Hong Kong, Peking University and Microsoft Research (Beijing). The statistics concerning the corpora is listed in Table 3. The corpora provided both unicode coding and Big5/GB coding. We used the Big5 and CP936 encodings. Since the main purpose of this work is to evaluate the proposed subwordbased IOB tagging, we carried out the closed test only. Five metrics were used to evaluate the segmentation results: recall (R), precision (P), F-score (F), OOV rate (R-oov) and IV rate (R-iv). For a detailed explanation of these metrics, refer to (Sproat and Emerson, 2003). 964 Corpus Abbrev. Encodings Training size (words) Test size (words) Academia Sinica AS Big5/Unicode 5.45M 122K Beijing University PKU CP936/Unicode 1.1M 104K City University of Hong Kong CITYU Big5/Unicode 1.46M 41K Microsoft Research (Beijing) MSR CP936/Unicode 2.37M 107K Table 1: Corpus statistics in Sighan Bakeoff 2005 3.1 Effects of N-gram LMs We obtained a word list from the training data as the vocabulary for dictionary-based segmentation. Ngram LMs were generated using the SRI LM toolkit. Table 2 shows the performance of N-gram segmentation by changing the order of N-grams. We found </context>
</contexts>
<marker>Sproat, Emerson, 2003</marker>
<rawString>Richard Sproat and Tom Emerson. 2003. The first international chinese word segmentation bakeoff. In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huihsin Tseng</author>
<author>Pichuan Chang</author>
<author>Galen Andrew</author>
<author>Daniel Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A conditional random field word segmenter for Sighan bakeoff</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<location>Jeju,</location>
<contexts>
<context position="3853" citStr="Tseng et al., 2005" startWordPosition="576" endWordPosition="579"> example mentioned above, “全北京市” is la961 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 961–968, Sydney, July 2006. c�2006 Association for Computational Linguistics beled as “全/O 北京/B 市/I” in the subword-based tagging, where “北京/B” is labeled as one unit. We will give a detailed description of this approach in Section 2. There exists a clear weakness with the IOB tagging approach: It yields a very low in-vocabulary rate (R-iv) in return for a higher out-of-vocabulary (OOV) rate (R-oov). In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al., 2005), using CRFs for the IOB tagging, yielded a very high R-oov in all of the four corpora used, but the R-iv rates were lower. While OOV recognition is very important in word segmentation, a higher IV rate is also desired. In this work we propose a confidence measure approach to lessen this weakness. By this approach we can change the R-oov and R-iv and find an optimal tradeoff. This approach will be described in Section 2.3. In addition, we illustrate our word segmentation process in Section 2, where the subword-based tagging is described by the MaxEnt method. Section 3 presents our experimental</context>
<context position="10074" citStr="Tseng et al., 2005" startWordPosition="1634" endWordPosition="1637">codings for numbers and alphabetical characters, are not allowed. Therefore, we used the features available only from the provided training corpus. • Contextual information: w0, t−1, w0t−1, w0t−1w1, t−1w1, t−1t−2, w0t−1t−2, w0w1, w0w1w2, w−1, w0w−1, w0w−1w1, w−1w1, w−1w−2, w0w−1w−2, w1, w1w2 where w stands for word and t, for IOB tag. The subscripts are position indicators, where 0 means the current word/tag; −1, −2, the first or second word/tag to the left; 1, 2, the first or second word/tag to the right. • Prefixes and suffixes. These are very useful features. Using the same approach as in (Tseng et al., 2005), we extracted the most frequent words tagged with “B”, indicating a prefix, and the last words tagged with “I”, denoting a suffix. Features containing prefixes and suffixes were used in the following combinations with other features, where p stands for prefix; s, suffix; p0 means the current word is a prefix and s1 denotes that the right first word is a suffix, and so on. p0, w0p−1, w0p1, s0, w0s−1, w0s1, p0w−1, p0w1, s0w−1, s0w−2 • Word length. This is defined as the number of characters in a word. The length of a Chinese word has discriminative roles for word composition. For example, singl</context>
</contexts>
<marker>Tseng, Chang, Andrew, Jurafsky, Manning, 2005</marker>
<rawString>Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher Manning. 2005. A conditional random field word segmenter for Sighan bakeoff 2005. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, Jeju, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Libin Shen</author>
</authors>
<title>Chinese word segmentation as LMR tagging.</title>
<date>2003</date>
<booktitle>In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing.</booktitle>
<contexts>
<context position="1548" citStr="Xue and Shen, 2003" startWordPosition="201" endWordPosition="204">e in-vocaulary rate and out-of-vocabulary rate. Our techniques were evaluated using the test data from Sighan Bakeoff 2005. We achieved higher F-scores than the best results in three of the four corpora: PKU(0.951), CITYU(0.950) and MSR(0.971). 1 Introduction Many approaches have been proposed in Chinese word segmentation in the past decades. Segmentation performance has been improved significantly, from the earliest maximal match (dictionary-based) approaches to HMM-based (Zhang et al., 2003) approaches and recent state-of-the-art machine learning approaches such as maximum entropy (MaxEnt) (Xue and Shen, 2003), support vector machine * Now the second author is affiliated with NTT. (SVM) (Kudo and Matsumoto, 2001), conditional random fields (CRF) (Peng and McCallum, 2004), and minimum error rate training (Gao et al., 2004). By analyzing the top results in the first and second Bakeoffs, (Sproat and Emerson, 2003) and (Emerson, 2005), we found the top results were produced by direct or indirect use of so-called “IOB” tagging, which converts the problem of word segmentation into one of character tagging so that part-of-speech tagging approaches can be used for word segmentation. This approach was also </context>
<context position="8847" citStr="Xue and Shen, 2003" startWordPosition="1420" endWordPosition="1423">市(city)/I,” or ”北(north)/B 京(capital)/I 市(city)/I.” In this work we used forward maximal match (FMM) for disambiguation. Because we carried out FMMs on each words in the manually segmented training data, the accuracy of FMM was much higher than applying it on whole sentences. Of course, backward maximal match (BMM) or other approaches are also applicable. We did not conduct comparative experiments due to trivial differences in the results of these approaches. In the third step, we used the maximum entropy (MaxEnt) approach (the results of CRF are given in Section 3.4) to train the IOB tagger (Xue and Shen, 2003). The mathematical expression for the MaxEnt model is P(t |h) = exp IEAi fi (h, t) I /Z, Z = E P(t |h) (2) i J t where t is a tag, “I,O,B,” of the current word; h, the context surrounding the current word, including word and tag sequences; fi, a binary feature equal to 1 if the i-th defined feature is activated and 0 otherwise; Z, a normalization coefficient; and Ai, the weight of the i-th feature. Many kinds of features can be defined for improving the tagging accuracy. However, to conform to the constraints of closed test in Bakeoff 2005, some features, such as syntactic information and char</context>
<context position="23602" citStr="Xue and Shen, 2003" startWordPosition="3821" endWordPosition="3824">/0.967/0.970 Table 5: Effects of combination using the confidence measure. Here we used α = 0.8 and confidence threshold t = 0.7. The separator “/” divides the results of s1, s2, and s3. no change on F-score for AS corpus, but a better recall rate was found. Our results are better than the best one of Bakeoff 2005 in PKU, CITYU and MSR corpora. Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006). 4 Discussion and Related works The IOB tagging approach adopted in this work is not a new idea. It was first implemented in Chinese word segmentation by (Xue and Shen, 2003) using the maximum entropy methods. Later, (Peng and McCallum, 2004) implemented the idea using the CRF-based approach, which yielded better results than the maximum entropy approach because it could solve the label bias problem (Lafferty et al., 2001). However, as we mentioned before, this approach does not take advantage of the prior knowledge of in-vocabulary words; It produced a higher R-oov but a lower R-iv. This problem has been observed by some participants in the Bakeoff 2005 (Asahara et al., 2005), where they applied the IOB tagging to recognize OOVs, and added the OOVs to the lexicon</context>
</contexts>
<marker>Xue, Shen, 2003</marker>
<rawString>Nianwen Xue and Libin Shen. 2003. Chinese word segmentation as LMR tagging. In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huaping Zhang</author>
<author>HongKui Yu</author>
<author>Deyi xiong</author>
<author>Qun Liu</author>
</authors>
<title>HHMM-based Chinese lexical analyzer ICTCLAS.</title>
<date>2003</date>
<booktitle>In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>184--187</pages>
<contexts>
<context position="1427" citStr="Zhang et al., 2003" startWordPosition="182" endWordPosition="185">ts of a dictionary-based and a subword-tagging-based segmentation. This approach can produce an ideal tradeoff between the in-vocaulary rate and out-of-vocabulary rate. Our techniques were evaluated using the test data from Sighan Bakeoff 2005. We achieved higher F-scores than the best results in three of the four corpora: PKU(0.951), CITYU(0.950) and MSR(0.971). 1 Introduction Many approaches have been proposed in Chinese word segmentation in the past decades. Segmentation performance has been improved significantly, from the earliest maximal match (dictionary-based) approaches to HMM-based (Zhang et al., 2003) approaches and recent state-of-the-art machine learning approaches such as maximum entropy (MaxEnt) (Xue and Shen, 2003), support vector machine * Now the second author is affiliated with NTT. (SVM) (Kudo and Matsumoto, 2001), conditional random fields (CRF) (Peng and McCallum, 2004), and minimum error rate training (Gao et al., 2004). By analyzing the top results in the first and second Bakeoffs, (Sproat and Emerson, 2003) and (Emerson, 2005), we found the top results were produced by direct or indirect use of so-called “IOB” tagging, which converts the problem of word segmentation into one </context>
</contexts>
<marker>Zhang, Yu, xiong, Liu, 2003</marker>
<rawString>Huaping Zhang, HongKui Yu, Deyi xiong, and Qun Liu. 2003. HHMM-based Chinese lexical analyzer ICTCLAS. In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing, pages 184– 187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruiqiang Zhang</author>
<author>Genichiro Kikui</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Subword-based tagging by conditional random fields for chinese word segmentation.</title>
<date>2006</date>
<booktitle>In Proc. of HLTNAACL.</booktitle>
<contexts>
<context position="23427" citStr="Zhang et al., 2006" startWordPosition="3789" endWordPosition="3792">PKU 0.941/0.948/0.949 0.945/0.947/0.947 0.943/0.948/0.948 0.672/0.662/0.660 0.958/0.966/0.966 MSR 0.944/0.959/0.961 0.959/0.964/0.963 0.951/0.961/0.962 0.671/0.674/0.631 0.951/0.967/0.970 Table 5: Effects of combination using the confidence measure. Here we used α = 0.8 and confidence threshold t = 0.7. The separator “/” divides the results of s1, s2, and s3. no change on F-score for AS corpus, but a better recall rate was found. Our results are better than the best one of Bakeoff 2005 in PKU, CITYU and MSR corpora. Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006). 4 Discussion and Related works The IOB tagging approach adopted in this work is not a new idea. It was first implemented in Chinese word segmentation by (Xue and Shen, 2003) using the maximum entropy methods. Later, (Peng and McCallum, 2004) implemented the idea using the CRF-based approach, which yielded better results than the maximum entropy approach because it could solve the label bias problem (Lafferty et al., 2001). However, as we mentioned before, this approach does not take advantage of the prior knowledge of in-vocabulary words; It produced a higher R-oov but a lower R-iv. This pro</context>
</contexts>
<marker>Zhang, Kikui, Sumita, 2006</marker>
<rawString>Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita. 2006. Subword-based tagging by conditional random fields for chinese word segmentation. In Proc. of HLTNAACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>