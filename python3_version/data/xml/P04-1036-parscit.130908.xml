<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.016502">
<title confidence="0.981786">
Finding Predominant Word Senses in Untagged Text
</title>
<author confidence="0.999507">
Diana McCarthy &amp; Rob Koeling &amp; Julie Weeds &amp; John Carroll
</author>
<affiliation confidence="0.995654">
Department of Informatics,
University of Sussex
</affiliation>
<address confidence="0.994973">
Brighton BN1 9QH, UK
</address>
<email confidence="0.999057">
dianam,robk,juliewe,johnca @sussex.ac.uk
</email>
<sectionHeader confidence="0.993894" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999919913043478">
In word sense disambiguation (WSD), the heuristic
of choosing the most common sense is extremely
powerful because the distribution of the senses of a
word is often skewed. The problem with using the
predominant, or first sense heuristic, aside from the
fact that it does not take surrounding context into
account, is that it assumes some quantity of hand-
tagged data. Whilst there are a few hand-tagged
corpora available for some languages, one would
expect the frequency distribution of the senses of
words, particularly topical words, to depend on the
genre and domain of the text under consideration.
We present work on the use of a thesaurus acquired
from raw textual corpora and the WordNet similar-
ity package to find predominant noun senses auto-
matically. The acquired predominant senses give a
precision of 64% on the nouns of the SENSEVAL-
2 English all-words task. This is a very promising
result given that our method does not require any
hand-tagged text, such as SemCor. Furthermore,
we demonstrate that our method discovers appropri-
ate predominant senses for words from two domain-
specific corpora.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999945135593221">
The first sense heuristic which is often used as a
baseline for supervised WSD systems outperforms
many of these systems which take surrounding con-
text into account. This is shown by the results of
the English all-words task in SENSEVAL-2 (Cot-
ton et al., 1998) in figure 1 below, where the first
sense is that listed in WordNet for the PoS given
by the Penn TreeBank (Palmer et al., 2001). The
senses in WordNet are ordered according to the fre-
quency data in the manually tagged resource Sem-
Cor (Miller et al., 1993). Senses that have not oc-
curred in SemCor are ordered arbitrarily and af-
ter those senses of the word that have occurred.
The figure distinguishes systems which make use
of hand-tagged data (using HTD) such as SemCor,
from those that do not (without HTD). The high per-
formance of the first sense baseline is due to the
skewed frequency distribution of word senses. Even
systems which show superior performance to this
heuristic often make use of the heuristic where ev-
idence from the context is not sufficient (Hoste et
al., 2001). Whilst a first sense heuristic based on a
sense-tagged corpus such as SemCor is clearly use-
ful, there is a strong case for obtaining a first, or pre-
dominant, sense from untagged corpus data so that
a WSD system can be tuned to the genre or domain
at hand.
SemCor comprises a relatively small sample of
250,000 words. There are words where the first
sense in WordNet is counter-intuitive, because of
the size of the corpus, and because where the fre-
quency data does not indicate a first sense, the or-
dering is arbitrary. For example the first sense of
tiger in WordNet is audacious person whereas one
might expect that carnivorous animal is a more
common usage. There are only a couple of instances
of tiger within SemCor. Another example is em-
bryo, which does not occur at all in SemCor and
the first sense is listed as rudimentary plant rather
than the anticipated fertilised egg meaning. We be-
lieve that an automatic means of finding a predomi-
nant sense would be useful for systems that use it as
a means of backing-off (Wilks and Stevenson, 1998;
Hoste et al., 2001) and for systems that use it in lex-
ical acquisition (McCarthy, 1997; Merlo and Ley-
bold, 2001; Korhonen, 2002) because of the limited
size of hand-tagged resources. More importantly,
when working within a specific domain one would
wish to tune the first sense heuristic to the domain at
hand. The first sense of star in SemCor is celestial
body, however, if one were disambiguating popular
news celebrity would be preferred.
Assuming that one had an accurate WSD system
then one could obtain frequency counts for senses
and rank them with these counts. However, the most
accurate WSD systems are those which require man-
ually sense tagged data in the first place, and their
accuracy depends on the quantity of training exam-
ples (Yarowsky and Florian, 2002) available. We
</bodyText>
<figure confidence="0.987382888888889">
recall
✂
100
80
60
40
20
0
First Sense
</figure>
<bodyText confidence="0.985086">
the WordNet hierarchy. We use WordNet as our
sense inventory for this work.
The paper is structured as follows. We discuss
our method in the following section. Sections 3 and
4 concern experiments using predominant senses
from the BNC evaluated against the data in SemCor
and the SENSEVAL-2 English all-words task respec-
tively. In section 5 we present results of the method
on two domain specific sections of the Reuters cor-
pus for a sample of words. We describe some re-
lated work in section 6 and conclude in section 7.
</bodyText>
<figure confidence="0.950837333333333">
0 20 40 60 80 100
precision
&amp;quot;using HTD&amp;quot; &amp;quot;without HTD&amp;quot; &amp;quot;First Sense&amp;quot;
</figure>
<figureCaption confidence="0.9962775">
Figure 1: The first sense heuristic compared with
the SENSEVAL-2 English all-words task results
</figureCaption>
<bodyText confidence="0.999714228571428">
are therefore investigating a method of automati-
cally ranking WordNet senses from raw text.
Many researchers are developing thesauruses
from automatically parsed data. In these each tar-
get word is entered with an ordered list of “near-
est neighbours”. The neighbours are words ordered
in terms of the “distributional similarity” that they
have with the target. Distributional similarity is
a measure indicating the degree that two words, a
word and its neighbour, occur in similar contexts.
From inspection, one can see that the ordered neigh-
bours of such a thesaurus relate to the different
senses of the target word. For example, the neigh-
bours of star in a dependency-based thesaurus pro-
vided by Lin 1 has the ordered list of neighbours:
superstar, player, teammate, actor early in the list,
but one can also see words that are related to another
sense of star e.g. galaxy, sun, world and planet fur-
ther down the list. We expect that the quantity and
similarity of the neighbours pertaining to different
senses will reflect the dominance of the sense to
which they pertain. This is because there will be
more relational data for the more prevalent senses
compared to the less frequent senses. In this pa-
per we describe and evaluate a method for ranking
senses of nouns to obtain the predominant sense of
a word using the neighbours from automatically ac-
quired thesauruses. The neighbours for a word in a
thesaurus are words themselves, rather than senses.
In order to associate the neighbours with senses we
make use of another notion of similarity, “semantic
similarity”, which exists between senses, rather than
words. We experiment with several WordNet Sim-
ilarity measures (Patwardhan and Pedersen, 2003)
which aim to capture semantic relatedness within
</bodyText>
<footnote confidence="0.9608985">
1Available at
http://www.cs.ualberta.ca/˜lindek/demos/depsim.htm
</footnote>
<sectionHeader confidence="0.922219" genericHeader="introduction">
2 Method
</sectionHeader>
<bodyText confidence="0.999148894736842">
In order to find the predominant sense of a target
word we use a thesaurus acquired from automati-
cally parsed text based on the method of Lin (1998).
This provides the nearest neighbours to each tar-
get word, along with the distributional similarity
score between the target word and its neighbour. We
then use the WordNet similarity package (Patward-
han and Pedersen, 2003) to give us a semantic simi-
larity measure (hereafter referred to as the WordNet
similarity measure) to weight the contribution that
each neighbour makes to the various senses of the
target word.
To find the first sense of a word ( ) we
take each sense in turn and obtain a score re-
flecting the prevalence which is used for rank-
ing. Let be the ordered
set of the top scoring neighbours of from
the thesaurus with associated distributional similar-
ity scores
</bodyText>
<equation confidence="0.686177">
(1)
</equation>
<subsectionHeader confidence="0.995027">
2.1 Acquiring the Automatic Thesaurus
</subsectionHeader>
<bodyText confidence="0.986973842105263">
The thesaurus was acquired using the method de-
scribed by Lin (1998). For input we used gram-
matical relation data extracted using an automatic
where:
.
Let be the set of senses of . For each
sense of ( ) we obtain a rank-
ing score by summing over the of each
neighbour ( ) multiplied by a weight. This
weight is the WordNet similarity score (
) be-
tween the target sense ( ) and the sense of
( ) that maximises this score, di-
vided by the sum of all such WordNet similarity
scores for and . Thus we rank each
sense using:
parser (Briscoe and Carroll, 2002). For the exper-
iments in sections 3 and 4 we used the 90 mil-
lion words of written English from the BNC. For
each noun we considered the co-occurring verbs in
the direct object and subject relation, the modifying
nouns in noun-noun relations and the modifying ad-
jectives in adjective-noun relations. We could easily
extend the set of relations in the future. A noun, ,
is thus described by a set of co-occurrence triples
and associated frequencies, where
is a grammatical relation and is a possible co-
occurrence with in that relation. For every pair of
nouns, where each noun had a total frequency in the
triple data of 10 or more, we computed their distri-
butional similarity using the measure given by Lin
(1998). If is the set of co-occurrence types
such that is positive then the simi-
larity between two nouns, and , can be computed
as:
where:
A thesaurus entry of size for a target noun is
then defined as the most similar nouns to .
</bodyText>
<subsectionHeader confidence="0.997795">
2.2 The WordNet Similarity Package
</subsectionHeader>
<bodyText confidence="0.995493555555556">
We use the WordNet Similarity Package 0.05 and
WordNet version 1.6. 2 The WordNet Similarity
package supports a range of WordNet similarity
scores. We experimented using six of these to pro-
vide the in equation 1 above and obtained re-
sults well over our baseline, but because of space
limitations give results for the two which perform
the best. We briefly summarise the two measures
here; for a more detailed summary see (Patward-
han et al., 2003). The measures provide a similar-
ity score between two WordNet senses ( and ),
these being synsets within WordNet.
lesk (Banerjee and Pedersen, 2002) This score
maximises the number of overlapping words in the
gloss, or definition, of the senses. It uses the
glosses of semantically related (according to Word-
Net) senses too.
jcn (Jiang and Conrath, 1997) This score uses
corpus data to populate classes (synsets) in the
WordNet hierarchy with frequency counts. Each
2We use this version of WordNet since it allows us to map
information to WordNets of other languages more accurately.
We are of course able to apply the method to other versions of
WordNet.
synset, is incremented with the frequency counts
from the corpus of all words belonging to that
synset, directly or via the hyponymy relation. The
frequency data is used to calculate the “informa-
tion content” (IC) of a class .
Jiang and Conrath specify a distance measure:
,
where the third class ( ) is the most informative,
or most specific, superordinate synset of the two
senses and . This is transformed from a dis-
tance measure in the WN-Similarity package by tak-
ing the reciprocal:
</bodyText>
<sectionHeader confidence="0.963111" genericHeader="method">
3 Experiment with SemCor
</sectionHeader>
<bodyText confidence="0.998594">
In order to evaluate our method we use the data
in SemCor as a gold-standard. This is not ideal
since we expect that the sense frequency distribu-
tions within SemCor will differ from those in the
BNC, from which we obtain our thesaurus. Never-
theless, since many systems performed well on the
English all-words task for SENSEVAL-2 by using the
frequency information in SemCor this is a reason-
able approach for evaluation.
We generated a thesaurus entry for all polyse-
mous nouns which occurred in SemCor with a fre-
quency 2, and in the BNC with a frequency
10 in the grammatical relations listed in section 2.1
above. The jcn measure uses corpus data for the
calculation of IC. We experimented with counts ob-
tained from the BNC and the Brown corpus. The
variation in counts had negligible affect on the re-
sults. 3 The experimental results reported here are
obtained using IC counts from the BNC corpus. All
the results shown here are those with the size of the-
saurus entries ( ) set to 50. 4
We calculate the accuracy of finding the predom-
inant sense, when there is indeed one sense with a
higher frequency than the others for this word in
SemCor ( ). We also calculate the WSD accu-
racy that would be obtained on SemCor, when using
our first sense in all contexts ( ).
</bodyText>
<subsectionHeader confidence="0.821427">
3.1 Results
</subsectionHeader>
<bodyText confidence="0.999916666666667">
The results in table 1 show the accuracy of the
ranking with respect to SemCor over the entire
set of 2595 polysemous nouns in SemCor with
</bodyText>
<footnote confidence="0.987948875">
3Using the default IC counts provided with the package did
result in significantly higher results, but these default files are
obtained from the sense-tagged data within SemCor itself so
we discounted these results.
4We repeated the experiment with the BNC data for jcn us-
ing and however, the number of neighbours
used gave only minimal changes to the results so we do not
report them here.
</footnote>
<table confidence="0.9687095">
measure % %
lesk 54 48
jcn 54 46
baseline 32 24
</table>
<tableCaption confidence="0.999298">
Table 1: SemCor results
</tableCaption>
<bodyText confidence="0.998997214285714">
the jcn and lesk WordNet similarity measures.
The random baseline for choosing the predominant
sense over all these words ( )
is 32%. Both WordNet similarity measures beat
this baseline. The random baseline for
( ) is 24%. Again, the
automatic ranking outperforms this by a large mar-
gin. The first sense in SemCor provides an upper-
bound for this task of 67%.
Since both measures gave comparable results we
restricted our remaining experiments to jcn because
this gave good results for finding the predominant
sense, and is much more efficient than lesk, given
the precompilation of the IC files.
</bodyText>
<subsectionHeader confidence="0.974631">
3.2 Discussion
</subsectionHeader>
<bodyText confidence="0.999974208333334">
From manual analysis, there are cases where the ac-
quired first sense disagrees with SemCor, yet is intu-
itively plausible. This is to be expected regardless of
any inherent shortcomings of the ranking technique
since the senses within SemCor will differ com-
pared to those of the BNC. For example, in WordNet
the first listed sense ofpipe is tobacco pipe, and this
is ranked joint first according to the Brown files in
SemCor with the second sense tube made of metal
or plastic used to carry water, oil or gas etc.... The
automatic ranking from the BNC data lists the latter
tube sense first. This seems quite reasonable given
the nearest neighbours: tube, cable, wire, tank, hole,
cylinder, fitting, tap, cistern, plate.... Since SemCor
is derived from the Brown corpus, which predates
the BNC by up to 30 years 5 and contains a higher
proportion of fiction 6, the high ranking for the to-
bacco pipe sense according to SemCor seems plau-
sible.
Another example where the ranking is intuitive,
is soil. The first ranked sense according to Sem-
Cor is the filth, stain: state of being unclean sense
whereas the automatic ranking lists dirt, ground,
earth as the first sense, which is the second ranked
</bodyText>
<footnote confidence="0.611227428571429">
5The text in the Brown corpus was produced in 1961,
whereas the bulk of the written portion of the BNC contains
texts produced between 1975 and 1993.
66 out of the 15 Brown genres are fiction, including one
specifically dedicated to detective fiction, whilst only 20% of
the BNC text represents imaginative writing, the remaining
80% being classified as informative.
</footnote>
<bodyText confidence="0.999456083333333">
sense according to SemCor. This seems intuitive
given our expected relative usage of these senses in
modern British English.
Even given the difference in text type between
SemCor and the BNC the results are encouraging,
especially given that our results are for
polysemous nouns. In the English all-words SEN-
SEVAL-2, 25% of the noun data was monosemous.
Thus, if we used the sense ranking as a heuristic for
an “all nouns” task we would expect to get preci-
sion in the region of 60%. We test this below on the
SENSEVAL-2 English all-words data.
</bodyText>
<sectionHeader confidence="0.9558675" genericHeader="method">
4 Experiment on SENSEVAL-2 English
all Words Data
</sectionHeader>
<bodyText confidence="0.999711588235294">
In order to see how well the automatically ac-
quired predominant sense performs on a WSD task
from which the WordNet sense ordering has not
been taken, we use the SENSEVAL-2 all-words
data (Palmer et al., 2001). 7 This is a hand-tagged
test suite of 5,000 words of running text from three
articles from the Penn Treebank II. We use an all-
words task because the predominant senses will re-
flect the sense distributions of all nouns within the
documents, rather than a lexical sample task, where
the target words are manually determined and the
results will depend on the skew of the words in the
sample. We do not assume that the predominant
sense is a method of WSD in itself. To disambiguate
senses a system should take context into account.
However, it is important to know the performance
of this heuristic for any systems that use it.
We generated a thesaurus entry for all polyse-
mous nouns in WordNet as described in section 2.1
above. We obtained the predominant sense for each
of these words and used these to label the instances
in the noun data within the SENSEVAL-2 English all-
words task. We give the results for this WSD task in
table 2. We compare results using the first sense
listed in SemCor, and the first sense according to
the SENSEVAL-2 English all-words test data itself.
For the latter, we only take a first-sense where there
is more than one occurrence of the noun in the test
data and one sense has occurred more times than
any of the others. We trivially labelled all monose-
mous items.
Our automatically acquired predominant sense
performs nearly as well as the first sense provided
by SemCor, which is very encouraging given that
</bodyText>
<footnote confidence="0.996223">
7In order to do this we use the mapping provided at
http://www.lsi.upc.es/˜nlp/tools/mapping.html (Daud´e et al.,
2000) for obtaining the SENSEVAL-2 data in WordNet 1.6. We
discounted the few items for which there was no mapping. This
amounted to only 3% of the data.
</footnote>
<table confidence="0.99959925">
precision recall
Automatic 64 63
SemCor 69 68
SENSEVAL-2 92 72
</table>
<tableCaption confidence="0.9656025">
Table 2: Evaluating predominant sense information
on SENSEVAL-2 all-words data.
</tableCaption>
<bodyText confidence="0.994660176470588">
our method only uses raw text, with no manual la-
belling. The performance of the predominant sense
provided in the SENSEVAL-2 test data provides an
upper bound for this task. The items that were
not covered by our method were those with insuffi-
cient grammatical relations for the tuples employed.
Two such words, today and one, each occurred 5
times in the test data. Extending the grammatical
relations used for building the thesaurus should im-
prove the coverage. There were a similar number of
words that were not covered by a predominant sense
in SemCor. For these one would need to obtain
more sense-tagged text in order to use this heuris-
tic. Our automatic ranking gave 67% precision on
these items. This demonstrates that our method of
providing a first sense from raw text will help when
sense-tagged data is not available.
</bodyText>
<sectionHeader confidence="0.982694" genericHeader="method">
5 Experiments with Domain Specific
Corpora
</sectionHeader>
<bodyText confidence="0.999838222222222">
A major motivation for our work is to try to capture
changes in ranking of senses for documents from
different domains. In order to test this we applied
our method to two specific sections of the Reuters
corpus. We demonstrate that choosing texts from a
particular domain has a significant influence on the
sense ranking. We chose the domains of SPORTS
and FINANCE since there is sufficient material for
these domains in this publically available corpus.
</bodyText>
<subsectionHeader confidence="0.976064">
5.1 Reuters Corpus
</subsectionHeader>
<bodyText confidence="0.999906384615384">
The Reuters corpus (Rose et al., 2002) is a collec-
tion of about 810,000 Reuters, English Language
News stories. Many of the articles are economy re-
lated, but several other topics are included too. We
selected documents from the SPORTS domain (topic
code: GSPO) and a limited number of documents
from the FINANCE domain (topic codes: ECAT and
MCAT).
The SPORTS corpus consists of 35317 documents
(about 9.1 million words). The FINANCE corpus
consists of 117734 documents (about 32.5 million
words). We acquired thesauruses for these corpora
using the procedure described in section 2.1.
</bodyText>
<subsectionHeader confidence="0.998101">
5.2 Two Experiments
</subsectionHeader>
<bodyText confidence="0.999997555555556">
There is no existing sense-tagged data for these do-
mains that we could use for evaluation. We there-
fore decided to select a limited number of words and
to evaluate these words qualitatively. The words in-
cluded in this experiment are not a random sample,
since we anticipated different predominant senses in
the SPORTS and FINANCE domains for these words.
Additionally, we evaluated our method quanti-
tatively using the Subject Field Codes (SFC) re-
source (Magnini and Cavagli`a, 2000) which anno-
tates WordNet synsets with domain labels. The SFC
contains an economy label and a sports label. For
this domain label experiment we selected all the
words in WordNet that have at least one synset la-
belled economy and at least one synset labelled
sports. The resulting set consisted of 38 words. We
contrast the distribution of domain labels for these
words in the 2 domain specific corpora.
</bodyText>
<subsectionHeader confidence="0.941821">
5.3 Discussion
</subsectionHeader>
<bodyText confidence="0.999662161290323">
The results for 10 of the words from the quali-
tative experiment are summarized in table 3 with
the WordNet sense number for each word supplied
alongside synonyms or hypernyms from WordNet
for readability. The results are promising. Most
words show the change in predominant sense (PS)
that we anticipated. It is not always intuitively clear
which of the senses to expect as predominant sense
for either a particular domain or for the BNC, but
the first senses of words like division and goal shift
towards the more specific senses (league and score
respectively). Moreover, the chosen senses of the
word tie proved to be a textbook example of the be-
haviour we expected.
The word share is among the words whose pre-
dominant sense remained the same for all three cor-
pora. We anticipated that the stock certificate sense
would be chosen for the FINANCE domain, but this
did not happen. However, that particular sense
ended up higher in the ranking for the FINANCE do-
main.
Figure 2 displays the results of the second exper-
iment with the domain specific corpora. This figure
shows the domain labels assigned to the predomi-
nant senses for the set of 38 words after ranking the
words using the SPORTS and the FINANCE corpora.
We see that both domains have a similarly high per-
centage of factotum (domain independent) labels,
but as we would expect, the other peaks correspond
to the economy label for the FINANCE corpus, and
the sports label for the SPORTS corpus.
</bodyText>
<table confidence="0.998220727272727">
Word PS BNC PS FINANCE PS SPORTS
pass 1 (accomplishment) 14 (attempt) 15 (throw)
share 2 (portion, asset) 2 2
division 4 (admin. unit) 4 6 (league)
head 1 (body part) 4 (leader) 4
loss 2 (transf. property) 2 8 (death, departure)
competition 2 (contest, social event) 3 (rivalry) 2
match 2 (contest) 7 (equal, person) 2
tie 1 (neckwear) 2 (affiliation) 3 (draw)
strike 1 (work stoppage) 1 6 (hit, success)
goal 1 (end, mental object) 1 2 (score)
</table>
<tableCaption confidence="0.997498">
Table 3: Domain specific results
</tableCaption>
<figureCaption confidence="0.986522">
Figure 2: Distribution of domain labels of predom-
</figureCaption>
<bodyText confidence="0.653733">
inant senses for 38 polysemous words ranked using
the SPORTS and FINANCE corpus.
</bodyText>
<sectionHeader confidence="0.99981" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999614266666667">
Most research in WSD concentrates on using contex-
tual features, typically neighbouring words, to help
determine the correct sense of a target word. In con-
trast, our work is aimed at discovering the predom-
inant senses from raw text because the first sense
heuristic is such a useful one, and because hand-
tagged data is not always available.
A major benefit of our work, rather than re-
liance on hand-tagged training data such as Sem-
Cor, is that this method permits us to produce pre-
dominant senses for the domain and text type re-
quired. Buitelaar and Sacaleanu (2001) have previ-
ously explored ranking and selection of synsets in
GermaNet for specific domains using the words in a
given synset, and those related by hyponymy, and
a term relevance measure taken from information
retrieval. Buitelaar and Sacaleanu have evaluated
their method on identifying domain specific con-
cepts using human judgements on 100 items. We
have evaluated our method using publically avail-
able resources, both for balanced and domain spe-
cific text. Magnini and Cavagli`a (2000) have identi-
fied WordNet word senses with particular domains,
and this has proven useful for high precision WSD
(Magnini et al., 2001); indeed in section 5 we used
these domain labels for evaluation. Identification
of these domain labels for word senses was semi-
automatic and required a considerable amount of
hand-labelling. Our approach is complementary to
this. It only requires raw text from the given domain
and because of this it can easily be applied to a new
domain, or sense inventory, given sufficient text.
Lapata and Brew (2004) have recently also high-
lighted the importance of a good prior in WSD. They
used syntactic evidence to find a prior distribution
for verb classes, based on (Levin, 1993), and incor-
porate this in a WSD system. Lapata and Brew ob-
tain their priors for verb classes directly from sub-
categorisation evidence in a parsed corpus, whereas
we use parsed data to find distributionally similar
words (nearest neighbours) to the target word which
reflect the different senses of the word and have as-
sociated distributional similarity scores which can
be used for ranking the senses according to preva-
lence.
There has been some related work on using auto-
matic thesauruses for discovering word senses from
corpora Pantel and Lin (2002). In this work the lists
of neighbours are themselves clustered to bring out
the various senses of the word. They evaluate using
the lin measure described above in section 2.2 to
determine the precision and recall of these discov-
ered classes with respect to WordNet synsets. This
method obtains precision of 61% and recall 51%.
If WordNet sense distinctions are not ultimately re-
quired then discovering the senses directly from the
neighbours list is useful because sense distinctions
discovered are relevant to the corpus data and new
senses can be found. In contrast, we use the neigh-
bours lists and WordNet similarity measures to im-
</bodyText>
<figure confidence="0.997118689655172">
commerce
biology
industry
play
administr.
physics
economy telecom.
medicine
mathematics sports
0.45
sport
finance
0.4
0.35
0.3
➓
Percentage
e
➓
0.25
0.2
politics
law religion
factotum
free_time
0.15
0.1
0.05
0
</figure>
<bodyText confidence="0.999784636363636">
pose a prevalence ranking on the WordNet senses.
We believe automatic ranking techniques such as
ours will be useful for systems that rely on Word-
Net, for example those that use it for lexical acquisi-
tion or WSD. It would be useful however to combine
our method of finding predominant senses with one
which can automatically find new senses within text
and relate these to WordNet synsets, as Ciaramita
and Johnson (2003) do with unknown nouns.
We have restricted ourselves to nouns in this
work, since this PoS is perhaps most affected by
domain. We are currently investigating the perfor-
mance of the first sense heuristic, and this method,
for other PoS on SENSEVAL-3 data (McCarthy et
al., 2004), although not yet with rankings from do-
main specific corpora. The lesk measure can be
used when ranking adjectives, and adverbs as well
as nouns and verbs (which can also be ranked using
jcn). Another major advantage that lesk has is that it
is applicable to lexical resources which do not have
the hierarchical structure that WordNet does, but do
have definitions associated with word senses.
</bodyText>
<sectionHeader confidence="0.999209" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999974238095238">
We have devised a method that uses raw corpus data
to automatically find a predominant sense for nouns
in WordNet. We use an automatically acquired the-
saurus and a WordNet Similarity measure. The au-
tomatically acquired predominant senses were eval-
uated against the hand-tagged resources SemCor
and the SENSEVAL-2 English all-words task giving
us a WSD precision of 64% on an all-nouns task.
This is just 5% lower than results using the first
sense in the manually labelled SemCor, and we ob-
tain 67% precision on polysemous nouns that are
not in SemCor.
In many cases the sense ranking provided in Sem-
Cor differs to that obtained automatically because
we used the BNC to produce our thesaurus. In-
deed, the merit of our technique is the very possibil-
ity of obtaining predominant senses from the data
at hand. We have demonstrated the possibility of
finding predominant senses in domain specific cor-
pora on a sample of nouns. In the future, we will
perform a large scale evaluation on domain specific
corpora. In particular, we will use balanced and do-
main specific corpora to isolate words having very
different neighbours, and therefore rankings, in the
different corpora and to detect and target words for
which there is a highly skewed sense distribution in
these corpora.
There is plenty of scope for further work. We
want to investigate the effect of frequency and
choice of distributional similarity measure (Weeds
et al., 2004). Additionally, we need to determine
whether senses which do not occur in a wide variety
of grammatical contexts fare badly using distribu-
tional measures of similarity, and what can be done
to combat this problem using relation specific the-
sauruses.
Whilst we have used WordNet as our sense in-
ventory, it would be possible to use this method with
another inventory given a measure of semantic relat-
edness between the neighbours and the senses. The
lesk measure for example, can be used with defini-
tions in any standard machine readable dictionary.
</bodyText>
<sectionHeader confidence="0.994731" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999587285714286">
We would like to thank Siddharth Patwardhan and
Ted Pedersen for making the WN Similarity pack-
age publically available. This work was funded
by EU-2001-34460 project MEANING: Develop-
ing Multilingual Web-scale Language Technolo-
gies, UK EPSRC project Robust Accurate Statisti-
cal Parsing (RASP) and a UK EPSRC studentship.
</bodyText>
<sectionHeader confidence="0.978225" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.760551357142857">
Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted Lesk algorithm for word sense disam-
biguation using WordNet. In Proceedings of
the Third International Conference on Intelligent
Text Processing and Computational Linguistics
(CICLing-02), Mexico City.
Edward Briscoe and John Carroll. 2002. Robust
accurate statistical annotation of general text.
In Proceedings of the Third International Con-
ference on Language Resources and Evaluation
(LREC), pages 1499–1504, Las Palmas, Canary
Islands, Spain.
Paul Buitelaar and Bogdan Sacaleanu. 2001. Rank-
ing and selecting synsets by domain relevance.
</bodyText>
<reference confidence="0.991926192307693">
In Proceedings of WordNet and Other Lexical
Resources: Applications, Extensions and Cus-
tomizations, NAACL 2001 Workshop, Pittsburgh,
PA.
Massimiliano Ciaramita and Mark Johnson. 2003.
Supersense tagging of unknown nouns in Word-
Net. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP 2003).
Scott Cotton, Phil Edmonds, Adam Kilgarriff,
and Martha Palmer. 1998. SENSEVAL-2.
http://www.sle.sharp.co.uk/senseval2/.
Jordi Daud´e, Lluis Padr´o, and German Rigau. 2000.
Mapping wordnets using structural information.
In Proceedings of the 38th Annual Meeting of the
Association for Computational Linguistics, Hong
Kong.
V´eronique Hoste, Anne Kool, and Walter Daele-
mans. 2001. Classifier optimization and combi-
nation in the English all words task. In Proceed-
ings of the SENSEVAL-2 workshop, pages 84–86.
Jay Jiang and David Conrath. 1997. Semantic sim-
ilarity based on corpus statistics and lexical tax-
onomy. In International Conference on Research
in Computational Linguistics, Taiwan.
Anna Korhonen. 2002. Semantically motivated
subcategorization acquisition. In Proceedings of
the ACL Workshop on Unsupervised Lexical Ac-
quisition, Philadelphia, USA.
Mirella Lapata and Chris Brew. 2004. Verb class
disambiguation using informative priors. Com-
putational Linguistics, 30(1):45–75.
Beth Levin. 1993. English Verb Classes and Alter-
nations: a Preliminary Investigation. University
of Chicago Press, Chicago and London.
Dekang Lin. 1998. Automatic retrieval and clus-
tering of similar words. In Proceedings of
COLING-ACL 98, Montreal, Canada.
Bernardo Magnini and Gabriela Cavagli`a. 2000.
Integrating subject field codes into WordNet. In
Proceedings ofLREC-2000, Athens, Greece.
Bernardo Magnini, Carlo Strapparava, Giovanni
Pezzuli, and Alfio Gliozzo. 2001. Using do-
main information for word sense disambiguation.
In Proceedings of the SENSEVAL-2 workshop,
pages 111–114.
Diana McCarthy, Rob Koeling, Julie Weeds,
and John Carrolł. 2004. Using automatically
acquired predominant senses for word sense
disambiguation. In Proceedings of the ACL
SENSEVAL-3 workshop.
Diana McCarthy. 1997. Word sense disambigua-
tion for acquisition of selectional preferences. In
Proceedings of the ACL/EACL 97 Workshop Au-
tomatic Information Extraction and Building of
Lexical Semantic Resources for NLP Applica-
tions, pages 52–61.
Paola Merlo and Matthias Leybold. 2001. Auto-
matic distinction of arguments and modifiers: the
case of prepositional phrases. In Proceedings
of the Workshop on Computational Language
Learning (CoNLL 2001), Toulouse, France.
George A. Miller, Claudia Leacock, Randee Tengi,
and Ross T Bunker. 1993. A semantic concor-
dance. In Proceedings of the ARPA Workshop on
Human Language Technology, pages 303–308.
Morgan Kaufman.
Martha Palmer, Christiane Fellbaum, Scott Cotton,
Lauren Delfs, and Hoa Trang Dang. 2001. En-
glish tasks: All-words and verb lexical sample.
In Proceedings of the SENSEVAL-2 workshop,
pages 21–24.
Patrick Pantel and Dekang Lin. 2002. Discover-
ing word senses from text. In Proceedings of
ACM SIGKDD Conference on Knowledge Dis-
covery and Data Mining, pages 613–619, Ed-
monton, Canada.
Siddharth Patwardhan and Ted Pedersen. 2003.
The cpan wordnet::similarity package.
http://search.cpan.org/author/SID/WordNet-
Similarity-0.03/.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted
Pedersen. 2003. Using measures of semantic re-
latedness for word sense disambiguation. In Pro-
ceedings of the Fourth International Conference
on Intelligent Text Processing and Computational
Linguistics (CICLing 2003), Mexico City.
Tony G. Rose, Mary Stevenson, and Miles White-
head. 2002. The Reuters Corpus volume 1 -
from yesterday’s news to tomorrow’s language
resources. In Proc. of Third International Con-
ference on Language Resources and Evaluation,
Las Palmas de Gran Canaria.
Julie Weeds, David Weir, and Diana McCarthy.
2004. Characterising measures of lexical distri-
butional similarity.
Yorick Wilks and Mark Stevenson. 1998. The
grammar of sense: using part-of speech tags as
a first step in semantic disambiguation. Natural
Language Engineering, 4(2):135–143.
David Yarowsky and Radu Florian. 2002. Evaluat-
ing sense disambiguation performance across di-
verse parameter spaces. Natural Language Engi-
neering, 8(4):293–310.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.822947">
<title confidence="0.999919">Finding Predominant Word Senses in Untagged Text</title>
<author confidence="0.999904">Diana McCarthy</author>
<author confidence="0.999904">Rob Koeling</author>
<author confidence="0.999904">Julie Weeds</author>
<author confidence="0.999904">John Carroll</author>
<affiliation confidence="0.999628">Department of Informatics, University of Sussex</affiliation>
<address confidence="0.999641">Brighton BN1 9QH, UK</address>
<email confidence="0.954321">dianam,robk,juliewe,johnca@sussex.ac.uk</email>
<abstract confidence="0.994287541666666">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed. The problem with using the predominant, or first sense heuristic, aside from the fact that it does not take surrounding context into account, is that it assumes some quantity of handtagged data. Whilst there are a few hand-tagged corpora available for some languages, one would expect the frequency distribution of the senses of words, particularly topical words, to depend on the genre and domain of the text under consideration. We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically. The acquired predominant senses give a of 64% on the nouns of the 2 English all-words task. This is a very promising result given that our method does not require any hand-tagged text, such as SemCor. Furthermore, we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>In Proceedings of WordNet and Other Lexical Resources: Applications, Extensions and Customizations, NAACL 2001 Workshop,</booktitle>
<location>Pittsburgh, PA.</location>
<marker></marker>
<rawString>In Proceedings of WordNet and Other Lexical Resources: Applications, Extensions and Customizations, NAACL 2001 Workshop, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimiliano Ciaramita</author>
<author>Mark Johnson</author>
</authors>
<title>Supersense tagging of unknown nouns in WordNet.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<contexts>
<context position="26021" citStr="Ciaramita and Johnson (2003)" startWordPosition="4440" endWordPosition="4443">merce biology industry play administr. physics economy telecom. medicine mathematics sports 0.45 sport finance 0.4 0.35 0.3  Percentage e  0.25 0.2 politics law religion factotum free_time 0.15 0.1 0.05 0 pose a prevalence ranking on the WordNet senses. We believe automatic ranking techniques such as ours will be useful for systems that rely on WordNet, for example those that use it for lexical acquisition or WSD. It would be useful however to combine our method of finding predominant senses with one which can automatically find new senses within text and relate these to WordNet synsets, as Ciaramita and Johnson (2003) do with unknown nouns. We have restricted ourselves to nouns in this work, since this PoS is perhaps most affected by domain. We are currently investigating the performance of the first sense heuristic, and this method, for other PoS on SENSEVAL-3 data (McCarthy et al., 2004), although not yet with rankings from domain specific corpora. The lesk measure can be used when ranking adjectives, and adverbs as well as nouns and verbs (which can also be ranked using jcn). Another major advantage that lesk has is that it is applicable to lexical resources which do not have the hierarchical structure </context>
</contexts>
<marker>Ciaramita, Johnson, 2003</marker>
<rawString>Massimiliano Ciaramita and Mark Johnson. 2003. Supersense tagging of unknown nouns in WordNet. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Cotton</author>
<author>Phil Edmonds</author>
<author>Adam Kilgarriff</author>
<author>Martha Palmer</author>
</authors>
<date>1998</date>
<note>SENSEVAL-2. http://www.sle.sharp.co.uk/senseval2/.</note>
<contexts>
<context position="1609" citStr="Cotton et al., 1998" startWordPosition="254" endWordPosition="258">e acquired predominant senses give a precision of 64% on the nouns of the SENSEVAL2 English all-words task. This is a very promising result given that our method does not require any hand-tagged text, such as SemCor. Furthermore, we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora. 1 Introduction The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account. This is shown by the results of the English all-words task in SENSEVAL-2 (Cotton et al., 1998) in figure 1 below, where the first sense is that listed in WordNet for the PoS given by the Penn TreeBank (Palmer et al., 2001). The senses in WordNet are ordered according to the frequency data in the manually tagged resource SemCor (Miller et al., 1993). Senses that have not occurred in SemCor are ordered arbitrarily and after those senses of the word that have occurred. The figure distinguishes systems which make use of hand-tagged data (using HTD) such as SemCor, from those that do not (without HTD). The high performance of the first sense baseline is due to the skewed frequency distribut</context>
</contexts>
<marker>Cotton, Edmonds, Kilgarriff, Palmer, 1998</marker>
<rawString>Scott Cotton, Phil Edmonds, Adam Kilgarriff, and Martha Palmer. 1998. SENSEVAL-2. http://www.sle.sharp.co.uk/senseval2/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordi Daud´e</author>
<author>Lluis Padr´o</author>
<author>German Rigau</author>
</authors>
<title>Mapping wordnets using structural information.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Hong Kong.</location>
<marker>Daud´e, Padr´o, Rigau, 2000</marker>
<rawString>Jordi Daud´e, Lluis Padr´o, and German Rigau. 2000. Mapping wordnets using structural information. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V´eronique Hoste</author>
<author>Anne Kool</author>
<author>Walter Daelemans</author>
</authors>
<title>Classifier optimization and combination in the English all words task.</title>
<date>2001</date>
<booktitle>In Proceedings of the SENSEVAL-2 workshop,</booktitle>
<pages>84--86</pages>
<contexts>
<context position="2394" citStr="Hoste et al., 2001" startWordPosition="394" endWordPosition="397">cording to the frequency data in the manually tagged resource SemCor (Miller et al., 1993). Senses that have not occurred in SemCor are ordered arbitrarily and after those senses of the word that have occurred. The figure distinguishes systems which make use of hand-tagged data (using HTD) such as SemCor, from those that do not (without HTD). The high performance of the first sense baseline is due to the skewed frequency distribution of word senses. Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al., 2001). Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand. SemCor comprises a relatively small sample of 250,000 words. There are words where the first sense in WordNet is counter-intuitive, because of the size of the corpus, and because where the frequency data does not indicate a first sense, the ordering is arbitrary. For example the first sense of tiger in WordNet is audacious person whereas on</context>
</contexts>
<marker>Hoste, Kool, Daelemans, 2001</marker>
<rawString>V´eronique Hoste, Anne Kool, and Walter Daelemans. 2001. Classifier optimization and combination in the English all words task. In Proceedings of the SENSEVAL-2 workshop, pages 84–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay Jiang</author>
<author>David Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In International Conference on Research in Computational Linguistics,</booktitle>
<contexts>
<context position="9984" citStr="Jiang and Conrath, 1997" startWordPosition="1705" endWordPosition="1708"> these to provide the in equation 1 above and obtained results well over our baseline, but because of space limitations give results for the two which perform the best. We briefly summarise the two measures here; for a more detailed summary see (Patwardhan et al., 2003). The measures provide a similarity score between two WordNet senses ( and ), these being synsets within WordNet. lesk (Banerjee and Pedersen, 2002) This score maximises the number of overlapping words in the gloss, or definition, of the senses. It uses the glosses of semantically related (according to WordNet) senses too. jcn (Jiang and Conrath, 1997) This score uses corpus data to populate classes (synsets) in the WordNet hierarchy with frequency counts. Each 2We use this version of WordNet since it allows us to map information to WordNets of other languages more accurately. We are of course able to apply the method to other versions of WordNet. synset, is incremented with the frequency counts from the corpus of all words belonging to that synset, directly or via the hyponymy relation. The frequency data is used to calculate the “information content” (IC) of a class . Jiang and Conrath specify a distance measure: , where the third class (</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay Jiang and David Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In International Conference on Research in Computational Linguistics, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Korhonen</author>
</authors>
<title>Semantically motivated subcategorization acquisition.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL Workshop on Unsupervised Lexical Acquisition,</booktitle>
<location>Philadelphia, USA.</location>
<contexts>
<context position="3574" citStr="Korhonen, 2002" startWordPosition="607" endWordPosition="608">dNet is audacious person whereas one might expect that carnivorous animal is a more common usage. There are only a couple of instances of tiger within SemCor. Another example is embryo, which does not occur at all in SemCor and the first sense is listed as rudimentary plant rather than the anticipated fertilised egg meaning. We believe that an automatic means of finding a predominant sense would be useful for systems that use it as a means of backing-off (Wilks and Stevenson, 1998; Hoste et al., 2001) and for systems that use it in lexical acquisition (McCarthy, 1997; Merlo and Leybold, 2001; Korhonen, 2002) because of the limited size of hand-tagged resources. More importantly, when working within a specific domain one would wish to tune the first sense heuristic to the domain at hand. The first sense of star in SemCor is celestial body, however, if one were disambiguating popular news celebrity would be preferred. Assuming that one had an accurate WSD system then one could obtain frequency counts for senses and rank them with these counts. However, the most accurate WSD systems are those which require manually sense tagged data in the first place, and their accuracy depends on the quantity of t</context>
</contexts>
<marker>Korhonen, 2002</marker>
<rawString>Anna Korhonen. 2002. Semantically motivated subcategorization acquisition. In Proceedings of the ACL Workshop on Unsupervised Lexical Acquisition, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
<author>Chris Brew</author>
</authors>
<title>Verb class disambiguation using informative priors.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="24034" citStr="Lapata and Brew (2004)" startWordPosition="4111" endWordPosition="4114">esources, both for balanced and domain specific text. Magnini and Cavagli`a (2000) have identified WordNet word senses with particular domains, and this has proven useful for high precision WSD (Magnini et al., 2001); indeed in section 5 we used these domain labels for evaluation. Identification of these domain labels for word senses was semiautomatic and required a considerable amount of hand-labelling. Our approach is complementary to this. It only requires raw text from the given domain and because of this it can easily be applied to a new domain, or sense inventory, given sufficient text. Lapata and Brew (2004) have recently also highlighted the importance of a good prior in WSD. They used syntactic evidence to find a prior distribution for verb classes, based on (Levin, 1993), and incorporate this in a WSD system. Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus, whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence. There</context>
</contexts>
<marker>Lapata, Brew, 2004</marker>
<rawString>Mirella Lapata and Chris Brew. 2004. Verb class disambiguation using informative priors. Computational Linguistics, 30(1):45–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternations: a Preliminary Investigation.</title>
<date>1993</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago and London.</location>
<contexts>
<context position="24203" citStr="Levin, 1993" startWordPosition="4142" endWordPosition="4143"> precision WSD (Magnini et al., 2001); indeed in section 5 we used these domain labels for evaluation. Identification of these domain labels for word senses was semiautomatic and required a considerable amount of hand-labelling. Our approach is complementary to this. It only requires raw text from the given domain and because of this it can easily be applied to a new domain, or sense inventory, given sufficient text. Lapata and Brew (2004) have recently also highlighted the importance of a good prior in WSD. They used syntactic evidence to find a prior distribution for verb classes, based on (Levin, 1993), and incorporate this in a WSD system. Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus, whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence. There has been some related work on using automatic thesauruses for discovering word senses from corpora Pantel and Lin (2002). In this work the lists of neighbours are thems</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English Verb Classes and Alternations: a Preliminary Investigation. University of Chicago Press, Chicago and London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL 98,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="6939" citStr="Lin (1998)" startWordPosition="1165" endWordPosition="1166">es. The neighbours for a word in a thesaurus are words themselves, rather than senses. In order to associate the neighbours with senses we make use of another notion of similarity, “semantic similarity”, which exists between senses, rather than words. We experiment with several WordNet Similarity measures (Patwardhan and Pedersen, 2003) which aim to capture semantic relatedness within 1Available at http://www.cs.ualberta.ca/˜lindek/demos/depsim.htm 2 Method In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998). This provides the nearest neighbours to each target word, along with the distributional similarity score between the target word and its neighbour. We then use the WordNet similarity package (Patwardhan and Pedersen, 2003) to give us a semantic similarity measure (hereafter referred to as the WordNet similarity measure) to weight the contribution that each neighbour makes to the various senses of the target word. To find the first sense of a word ( ) we take each sense in turn and obtain a score reflecting the prevalence which is used for ranking. Let be the ordered set of the top scoring ne</context>
<context position="8925" citStr="Lin (1998)" startWordPosition="1523" endWordPosition="1524">. For each noun we considered the co-occurring verbs in the direct object and subject relation, the modifying nouns in noun-noun relations and the modifying adjectives in adjective-noun relations. We could easily extend the set of relations in the future. A noun, , is thus described by a set of co-occurrence triples and associated frequencies, where is a grammatical relation and is a possible cooccurrence with in that relation. For every pair of nouns, where each noun had a total frequency in the triple data of 10 or more, we computed their distributional similarity using the measure given by Lin (1998). If is the set of co-occurrence types such that is positive then the similarity between two nouns, and , can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to . 2.2 The WordNet Similarity Package We use the WordNet Similarity Package 0.05 and WordNet version 1.6. 2 The WordNet Similarity package supports a range of WordNet similarity scores. We experimented using six of these to provide the in equation 1 above and obtained results well over our baseline, but because of space limitations give results for the two which perform the be</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of COLING-ACL 98, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernardo Magnini</author>
<author>Gabriela Cavagli`a</author>
</authors>
<title>Integrating subject field codes into WordNet.</title>
<date>2000</date>
<booktitle>In Proceedings ofLREC-2000,</booktitle>
<location>Athens, Greece.</location>
<marker>Magnini, Cavagli`a, 2000</marker>
<rawString>Bernardo Magnini and Gabriela Cavagli`a. 2000. Integrating subject field codes into WordNet. In Proceedings ofLREC-2000, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernardo Magnini</author>
<author>Carlo Strapparava</author>
<author>Giovanni Pezzuli</author>
<author>Alfio Gliozzo</author>
</authors>
<title>Using domain information for word sense disambiguation.</title>
<date>2001</date>
<booktitle>In Proceedings of the SENSEVAL-2 workshop,</booktitle>
<pages>111--114</pages>
<contexts>
<context position="23628" citStr="Magnini et al., 2001" startWordPosition="4044" endWordPosition="4047">ave previously explored ranking and selection of synsets in GermaNet for specific domains using the words in a given synset, and those related by hyponymy, and a term relevance measure taken from information retrieval. Buitelaar and Sacaleanu have evaluated their method on identifying domain specific concepts using human judgements on 100 items. We have evaluated our method using publically available resources, both for balanced and domain specific text. Magnini and Cavagli`a (2000) have identified WordNet word senses with particular domains, and this has proven useful for high precision WSD (Magnini et al., 2001); indeed in section 5 we used these domain labels for evaluation. Identification of these domain labels for word senses was semiautomatic and required a considerable amount of hand-labelling. Our approach is complementary to this. It only requires raw text from the given domain and because of this it can easily be applied to a new domain, or sense inventory, given sufficient text. Lapata and Brew (2004) have recently also highlighted the importance of a good prior in WSD. They used syntactic evidence to find a prior distribution for verb classes, based on (Levin, 1993), and incorporate this in</context>
</contexts>
<marker>Magnini, Strapparava, Pezzuli, Gliozzo, 2001</marker>
<rawString>Bernardo Magnini, Carlo Strapparava, Giovanni Pezzuli, and Alfio Gliozzo. 2001. Using domain information for word sense disambiguation. In Proceedings of the SENSEVAL-2 workshop, pages 111–114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Rob Koeling</author>
<author>Julie Weeds</author>
<author>John Carrolł</author>
</authors>
<title>Using automatically acquired predominant senses for word sense disambiguation.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL SENSEVAL-3 workshop.</booktitle>
<contexts>
<context position="26298" citStr="McCarthy et al., 2004" startWordPosition="4487" endWordPosition="4490">g techniques such as ours will be useful for systems that rely on WordNet, for example those that use it for lexical acquisition or WSD. It would be useful however to combine our method of finding predominant senses with one which can automatically find new senses within text and relate these to WordNet synsets, as Ciaramita and Johnson (2003) do with unknown nouns. We have restricted ourselves to nouns in this work, since this PoS is perhaps most affected by domain. We are currently investigating the performance of the first sense heuristic, and this method, for other PoS on SENSEVAL-3 data (McCarthy et al., 2004), although not yet with rankings from domain specific corpora. The lesk measure can be used when ranking adjectives, and adverbs as well as nouns and verbs (which can also be ranked using jcn). Another major advantage that lesk has is that it is applicable to lexical resources which do not have the hierarchical structure that WordNet does, but do have definitions associated with word senses. 7 Conclusions We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet. We use an automatically acquired thesaurus and a WordNet Similarity measure.</context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carrolł, 2004</marker>
<rawString>Diana McCarthy, Rob Koeling, Julie Weeds, and John Carrolł. 2004. Using automatically acquired predominant senses for word sense disambiguation. In Proceedings of the ACL SENSEVAL-3 workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
</authors>
<title>Word sense disambiguation for acquisition of selectional preferences.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL/EACL 97 Workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications,</booktitle>
<pages>52--61</pages>
<contexts>
<context position="3532" citStr="McCarthy, 1997" startWordPosition="600" endWordPosition="601">r example the first sense of tiger in WordNet is audacious person whereas one might expect that carnivorous animal is a more common usage. There are only a couple of instances of tiger within SemCor. Another example is embryo, which does not occur at all in SemCor and the first sense is listed as rudimentary plant rather than the anticipated fertilised egg meaning. We believe that an automatic means of finding a predominant sense would be useful for systems that use it as a means of backing-off (Wilks and Stevenson, 1998; Hoste et al., 2001) and for systems that use it in lexical acquisition (McCarthy, 1997; Merlo and Leybold, 2001; Korhonen, 2002) because of the limited size of hand-tagged resources. More importantly, when working within a specific domain one would wish to tune the first sense heuristic to the domain at hand. The first sense of star in SemCor is celestial body, however, if one were disambiguating popular news celebrity would be preferred. Assuming that one had an accurate WSD system then one could obtain frequency counts for senses and rank them with these counts. However, the most accurate WSD systems are those which require manually sense tagged data in the first place, and t</context>
</contexts>
<marker>McCarthy, 1997</marker>
<rawString>Diana McCarthy. 1997. Word sense disambiguation for acquisition of selectional preferences. In Proceedings of the ACL/EACL 97 Workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications, pages 52–61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Merlo</author>
<author>Matthias Leybold</author>
</authors>
<title>Automatic distinction of arguments and modifiers: the case of prepositional phrases.</title>
<date>2001</date>
<booktitle>In Proceedings of the Workshop on Computational Language Learning (CoNLL</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="3557" citStr="Merlo and Leybold, 2001" startWordPosition="602" endWordPosition="606">rst sense of tiger in WordNet is audacious person whereas one might expect that carnivorous animal is a more common usage. There are only a couple of instances of tiger within SemCor. Another example is embryo, which does not occur at all in SemCor and the first sense is listed as rudimentary plant rather than the anticipated fertilised egg meaning. We believe that an automatic means of finding a predominant sense would be useful for systems that use it as a means of backing-off (Wilks and Stevenson, 1998; Hoste et al., 2001) and for systems that use it in lexical acquisition (McCarthy, 1997; Merlo and Leybold, 2001; Korhonen, 2002) because of the limited size of hand-tagged resources. More importantly, when working within a specific domain one would wish to tune the first sense heuristic to the domain at hand. The first sense of star in SemCor is celestial body, however, if one were disambiguating popular news celebrity would be preferred. Assuming that one had an accurate WSD system then one could obtain frequency counts for senses and rank them with these counts. However, the most accurate WSD systems are those which require manually sense tagged data in the first place, and their accuracy depends on </context>
</contexts>
<marker>Merlo, Leybold, 2001</marker>
<rawString>Paola Merlo and Matthias Leybold. 2001. Automatic distinction of arguments and modifiers: the case of prepositional phrases. In Proceedings of the Workshop on Computational Language Learning (CoNLL 2001), Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Claudia Leacock</author>
<author>Randee Tengi</author>
<author>Ross T Bunker</author>
</authors>
<title>A semantic concordance.</title>
<date>1993</date>
<booktitle>In Proceedings of the ARPA Workshop on Human Language Technology,</booktitle>
<pages>303--308</pages>
<publisher>Morgan Kaufman.</publisher>
<contexts>
<context position="1865" citStr="Miller et al., 1993" startWordPosition="303" endWordPosition="306">thod discovers appropriate predominant senses for words from two domainspecific corpora. 1 Introduction The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account. This is shown by the results of the English all-words task in SENSEVAL-2 (Cotton et al., 1998) in figure 1 below, where the first sense is that listed in WordNet for the PoS given by the Penn TreeBank (Palmer et al., 2001). The senses in WordNet are ordered according to the frequency data in the manually tagged resource SemCor (Miller et al., 1993). Senses that have not occurred in SemCor are ordered arbitrarily and after those senses of the word that have occurred. The figure distinguishes systems which make use of hand-tagged data (using HTD) such as SemCor, from those that do not (without HTD). The high performance of the first sense baseline is due to the skewed frequency distribution of word senses. Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al., 2001). Whilst a first sense heuristic based on a sense-tagged corpus such as</context>
</contexts>
<marker>Miller, Leacock, Tengi, Bunker, 1993</marker>
<rawString>George A. Miller, Claudia Leacock, Randee Tengi, and Ross T Bunker. 1993. A semantic concordance. In Proceedings of the ARPA Workshop on Human Language Technology, pages 303–308. Morgan Kaufman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Christiane Fellbaum</author>
<author>Scott Cotton</author>
<author>Lauren Delfs</author>
<author>Hoa Trang Dang</author>
</authors>
<title>English tasks: All-words and verb lexical sample.</title>
<date>2001</date>
<contexts>
<context position="1737" citStr="Palmer et al., 2001" startWordPosition="280" endWordPosition="283">sing result given that our method does not require any hand-tagged text, such as SemCor. Furthermore, we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora. 1 Introduction The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account. This is shown by the results of the English all-words task in SENSEVAL-2 (Cotton et al., 1998) in figure 1 below, where the first sense is that listed in WordNet for the PoS given by the Penn TreeBank (Palmer et al., 2001). The senses in WordNet are ordered according to the frequency data in the manually tagged resource SemCor (Miller et al., 1993). Senses that have not occurred in SemCor are ordered arbitrarily and after those senses of the word that have occurred. The figure distinguishes systems which make use of hand-tagged data (using HTD) such as SemCor, from those that do not (without HTD). The high performance of the first sense baseline is due to the skewed frequency distribution of word senses. Even systems which show superior performance to this heuristic often make use of the heuristic where evidenc</context>
<context position="15653" citStr="Palmer et al., 2001" startWordPosition="2693" endWordPosition="2696">the BNC the results are encouraging, especially given that our results are for polysemous nouns. In the English all-words SENSEVAL-2, 25% of the noun data was monosemous. Thus, if we used the sense ranking as a heuristic for an “all nouns” task we would expect to get precision in the region of 60%. We test this below on the SENSEVAL-2 English all-words data. 4 Experiment on SENSEVAL-2 English all Words Data In order to see how well the automatically acquired predominant sense performs on a WSD task from which the WordNet sense ordering has not been taken, we use the SENSEVAL-2 all-words data (Palmer et al., 2001). 7 This is a hand-tagged test suite of 5,000 words of running text from three articles from the Penn Treebank II. We use an allwords task because the predominant senses will reflect the sense distributions of all nouns within the documents, rather than a lexical sample task, where the target words are manually determined and the results will depend on the skew of the words in the sample. We do not assume that the predominant sense is a method of WSD in itself. To disambiguate senses a system should take context into account. However, it is important to know the performance of this heuristic f</context>
</contexts>
<marker>Palmer, Fellbaum, Cotton, Delfs, Dang, 2001</marker>
<rawString>Martha Palmer, Christiane Fellbaum, Scott Cotton, Lauren Delfs, and Hoa Trang Dang. 2001. English tasks: All-words and verb lexical sample.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings of the SENSEVAL-2 workshop,</booktitle>
<pages>21--24</pages>
<marker></marker>
<rawString>In Proceedings of the SENSEVAL-2 workshop, pages 21–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Dekang Lin</author>
</authors>
<title>Discovering word senses from text.</title>
<date>2002</date>
<booktitle>In Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>613--619</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="24755" citStr="Pantel and Lin (2002)" startWordPosition="4231" endWordPosition="4234">to find a prior distribution for verb classes, based on (Levin, 1993), and incorporate this in a WSD system. Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus, whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence. There has been some related work on using automatic thesauruses for discovering word senses from corpora Pantel and Lin (2002). In this work the lists of neighbours are themselves clustered to bring out the various senses of the word. They evaluate using the lin measure described above in section 2.2 to determine the precision and recall of these discovered classes with respect to WordNet synsets. This method obtains precision of 61% and recall 51%. If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found. In contrast, we use the neighbours lists an</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>Patrick Pantel and Dekang Lin. 2002. Discovering word senses from text. In Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 613–619, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Ted Pedersen</author>
</authors>
<title>The cpan wordnet::similarity package.</title>
<date>2003</date>
<note>http://search.cpan.org/author/SID/WordNetSimilarity-0.03/.</note>
<contexts>
<context position="6667" citStr="Patwardhan and Pedersen, 2003" startWordPosition="1123" endWordPosition="1126">s is because there will be more relational data for the more prevalent senses compared to the less frequent senses. In this paper we describe and evaluate a method for ranking senses of nouns to obtain the predominant sense of a word using the neighbours from automatically acquired thesauruses. The neighbours for a word in a thesaurus are words themselves, rather than senses. In order to associate the neighbours with senses we make use of another notion of similarity, “semantic similarity”, which exists between senses, rather than words. We experiment with several WordNet Similarity measures (Patwardhan and Pedersen, 2003) which aim to capture semantic relatedness within 1Available at http://www.cs.ualberta.ca/˜lindek/demos/depsim.htm 2 Method In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998). This provides the nearest neighbours to each target word, along with the distributional similarity score between the target word and its neighbour. We then use the WordNet similarity package (Patwardhan and Pedersen, 2003) to give us a semantic similarity measure (hereafter referred to as the WordNet similarity measure) to w</context>
</contexts>
<marker>Patwardhan, Pedersen, 2003</marker>
<rawString>Siddharth Patwardhan and Ted Pedersen. 2003. The cpan wordnet::similarity package. http://search.cpan.org/author/SID/WordNetSimilarity-0.03/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>Using measures of semantic relatedness for word sense disambiguation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics (CICLing 2003),</booktitle>
<location>Mexico City.</location>
<contexts>
<context position="9630" citStr="Patwardhan et al., 2003" startWordPosition="1646" endWordPosition="1650">between two nouns, and , can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to . 2.2 The WordNet Similarity Package We use the WordNet Similarity Package 0.05 and WordNet version 1.6. 2 The WordNet Similarity package supports a range of WordNet similarity scores. We experimented using six of these to provide the in equation 1 above and obtained results well over our baseline, but because of space limitations give results for the two which perform the best. We briefly summarise the two measures here; for a more detailed summary see (Patwardhan et al., 2003). The measures provide a similarity score between two WordNet senses ( and ), these being synsets within WordNet. lesk (Banerjee and Pedersen, 2002) This score maximises the number of overlapping words in the gloss, or definition, of the senses. It uses the glosses of semantically related (according to WordNet) senses too. jcn (Jiang and Conrath, 1997) This score uses corpus data to populate classes (synsets) in the WordNet hierarchy with frequency counts. Each 2We use this version of WordNet since it allows us to map information to WordNets of other languages more accurately. We are of course</context>
</contexts>
<marker>Patwardhan, Banerjee, Pedersen, 2003</marker>
<rawString>Siddharth Patwardhan, Satanjeev Banerjee, and Ted Pedersen. 2003. Using measures of semantic relatedness for word sense disambiguation. In Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics (CICLing 2003), Mexico City.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony G Rose</author>
<author>Mary Stevenson</author>
<author>Miles Whitehead</author>
</authors>
<title>The Reuters Corpus volume 1 -from yesterday’s news to tomorrow’s language resources.</title>
<date>2002</date>
<booktitle>In Proc. of Third International Conference on Language Resources and Evaluation, Las Palmas de Gran Canaria.</booktitle>
<contexts>
<context position="18893" citStr="Rose et al., 2002" startWordPosition="3246" endWordPosition="3249">om raw text will help when sense-tagged data is not available. 5 Experiments with Domain Specific Corpora A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains. In order to test this we applied our method to two specific sections of the Reuters corpus. We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking. We chose the domains of SPORTS and FINANCE since there is sufficient material for these domains in this publically available corpus. 5.1 Reuters Corpus The Reuters corpus (Rose et al., 2002) is a collection of about 810,000 Reuters, English Language News stories. Many of the articles are economy related, but several other topics are included too. We selected documents from the SPORTS domain (topic code: GSPO) and a limited number of documents from the FINANCE domain (topic codes: ECAT and MCAT). The SPORTS corpus consists of 35317 documents (about 9.1 million words). The FINANCE corpus consists of 117734 documents (about 32.5 million words). We acquired thesauruses for these corpora using the procedure described in section 2.1. 5.2 Two Experiments There is no existing sense-tagge</context>
</contexts>
<marker>Rose, Stevenson, Whitehead, 2002</marker>
<rawString>Tony G. Rose, Mary Stevenson, and Miles Whitehead. 2002. The Reuters Corpus volume 1 -from yesterday’s news to tomorrow’s language resources. In Proc. of Third International Conference on Language Resources and Evaluation, Las Palmas de Gran Canaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
<author>Diana McCarthy</author>
</authors>
<title>Characterising measures of lexical distributional similarity.</title>
<date>2004</date>
<contexts>
<context position="28139" citStr="Weeds et al., 2004" startWordPosition="4795" endWordPosition="4798">e demonstrated the possibility of finding predominant senses in domain specific corpora on a sample of nouns. In the future, we will perform a large scale evaluation on domain specific corpora. In particular, we will use balanced and domain specific corpora to isolate words having very different neighbours, and therefore rankings, in the different corpora and to detect and target words for which there is a highly skewed sense distribution in these corpora. There is plenty of scope for further work. We want to investigate the effect of frequency and choice of distributional similarity measure (Weeds et al., 2004). Additionally, we need to determine whether senses which do not occur in a wide variety of grammatical contexts fare badly using distributional measures of similarity, and what can be done to combat this problem using relation specific thesauruses. Whilst we have used WordNet as our sense inventory, it would be possible to use this method with another inventory given a measure of semantic relatedness between the neighbours and the senses. The lesk measure for example, can be used with definitions in any standard machine readable dictionary. Acknowledgements We would like to thank Siddharth Pa</context>
</contexts>
<marker>Weeds, Weir, McCarthy, 2004</marker>
<rawString>Julie Weeds, David Weir, and Diana McCarthy. 2004. Characterising measures of lexical distributional similarity.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yorick Wilks</author>
<author>Mark Stevenson</author>
</authors>
<title>The grammar of sense: using part-of speech tags as a first step in semantic disambiguation.</title>
<date>1998</date>
<journal>Natural Language Engineering,</journal>
<volume>4</volume>
<issue>2</issue>
<contexts>
<context position="3444" citStr="Wilks and Stevenson, 1998" startWordPosition="582" endWordPosition="585">and because where the frequency data does not indicate a first sense, the ordering is arbitrary. For example the first sense of tiger in WordNet is audacious person whereas one might expect that carnivorous animal is a more common usage. There are only a couple of instances of tiger within SemCor. Another example is embryo, which does not occur at all in SemCor and the first sense is listed as rudimentary plant rather than the anticipated fertilised egg meaning. We believe that an automatic means of finding a predominant sense would be useful for systems that use it as a means of backing-off (Wilks and Stevenson, 1998; Hoste et al., 2001) and for systems that use it in lexical acquisition (McCarthy, 1997; Merlo and Leybold, 2001; Korhonen, 2002) because of the limited size of hand-tagged resources. More importantly, when working within a specific domain one would wish to tune the first sense heuristic to the domain at hand. The first sense of star in SemCor is celestial body, however, if one were disambiguating popular news celebrity would be preferred. Assuming that one had an accurate WSD system then one could obtain frequency counts for senses and rank them with these counts. However, the most accurate </context>
</contexts>
<marker>Wilks, Stevenson, 1998</marker>
<rawString>Yorick Wilks and Mark Stevenson. 1998. The grammar of sense: using part-of speech tags as a first step in semantic disambiguation. Natural Language Engineering, 4(2):135–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Radu Florian</author>
</authors>
<title>Evaluating sense disambiguation performance across diverse parameter spaces.</title>
<date>2002</date>
<journal>Natural Language Engineering,</journal>
<volume>8</volume>
<issue>4</issue>
<contexts>
<context position="4219" citStr="Yarowsky and Florian, 2002" startWordPosition="712" endWordPosition="715">mited size of hand-tagged resources. More importantly, when working within a specific domain one would wish to tune the first sense heuristic to the domain at hand. The first sense of star in SemCor is celestial body, however, if one were disambiguating popular news celebrity would be preferred. Assuming that one had an accurate WSD system then one could obtain frequency counts for senses and rank them with these counts. However, the most accurate WSD systems are those which require manually sense tagged data in the first place, and their accuracy depends on the quantity of training examples (Yarowsky and Florian, 2002) available. We recall ✂ 100 80 60 40 20 0 First Sense the WordNet hierarchy. We use WordNet as our sense inventory for this work. The paper is structured as follows. We discuss our method in the following section. Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively. In section 5 we present results of the method on two domain specific sections of the Reuters corpus for a sample of words. We describe some related work in section 6 and conclude in section 7. 0 20 40 60 80 100 precisi</context>
</contexts>
<marker>Yarowsky, Florian, 2002</marker>
<rawString>David Yarowsky and Radu Florian. 2002. Evaluating sense disambiguation performance across diverse parameter spaces. Natural Language Engineering, 8(4):293–310.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>