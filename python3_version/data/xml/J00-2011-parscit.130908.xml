<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005964">
<note confidence="0.293253">
Book Reviews
</note>
<title confidence="0.975227">
Foundations of Statistical Natural Language Processing
</title>
<author confidence="0.798358">
Christopher D. Manning and Hinrich Schiitze
</author>
<affiliation confidence="0.827774">
(Stanford University and Xerox Palo Alto Research Center)
</affiliation>
<address confidence="0.362917">
Cambridge, MA: The MIT Press, 1999,
</address>
<figure confidence="0.9541196">
xxxvii + 680 pp; hardbound, ISBN
0-262-13360-1, $60.00
Reviewed by
Lillian Lee
Cornell University
</figure>
<bodyText confidence="0.916337684210526">
In 1993, Eugene Charniak published a slim volume entitled Statistical Language Learning.
At the time, empirical techniques for natural language processing were on the rise;
that year, Computational Linguistics published a special issue on such methods, and
Charniak&apos;s text was the first to treat the emerging field.
Nowadays, the revolution has become the establishment; in 1998, nearly half the
papers in Computational Linguistics concerned empirical methods (Hirschberg 1998).
Indeed, Christopher Manning and Hinrich Schtitze&apos;s new, by-no-means slim textbook
on statistical NLP—strangely, the first since Charniak&apos;sl—begins &amp;quot;The need for a thor-
ough textbook for Statistical Natural Language Processing hardly needs to be argued
for.&amp;quot; Indubitably so; the question is, is this it?
Foundations of Statistical Natural Language Processing (henceforth FSNLP) is certainly
ambitious in scope. True to its name, it contains a great deal of preparatory material,
including: gentle introductions to probability and information theory; a chapter on
linguistic concepts; and (a most welcome addition) discussion of the nitty-gritty of
doing empirical work, ranging from lists of available corpora to in-depth discussion
of the critical issue of smoothing. Scattered throughout are also topics fundamental to
doing good experimental work in general, such as hypothesis testing, cross-validation,
and baselines. Along with these preliminaries, FSNLP covers traditional tools of the
trade: Markov models, probabilistic grammars, supervised and unsupervised classi-
fication, and the vector-space model. Finally, several chapters are devoted to specific
problems, among them lexicon acquisition, word sense disambiguation, parsing, ma-
chine translation, and information retrieval.&apos; (The companion website contains further
useful material, including links to programs and a list of errata.)
In short, this is a Big Book&apos;, and this fact alone confers some benefits. For the
researcher, FSNLP offers the convenience of one-stop shopping: at present, there is
no other NLP reference in which standard empirical techniques, statistical tables, def-
initions of linguistic terms, and elements of information retrieval appear together;
furthermore, the text also summarizes and critiques many individual research pa-
pers. Similarly, someone teaching a course on statistical NLP will appreciate the large
number of topics FSNLP covers, allowing the tailoring of a syllabus to individual in-
1 In the interim, the second edition of Allen&apos;s book (Allen 1995) did include some material on
probabilistic methods, and much of Jelinek&apos;s Statistical Methods for Speech Recognition (Jelinek 1997)
concerns language processing. Also, Speech and Language Processing (Jurafsky and Martin 2000), still
forthcoming at the writing of this review, promises to cover many empirical methods.
2 The grouping of topics in this paragraph, while convenient, does not correspond to the order of
presentation in the book. Indeed, the way in which one thinks about a subject need not be the
organization that is best for teaching it, a point to which we will return later.
3 For the record: 1.66 kg (3 lb., 10.7 oz.)
</bodyText>
<page confidence="0.965656">
277
</page>
<note confidence="0.418889">
Computational Linguistics Volume 26, Number 2
</note>
<bodyText confidence="0.999965352941177">
terests. And for those entering the field, the book records &amp;quot;folklore&amp;quot; knowledge that
is typically acquired only by word of mouth or bitter experience, such as techniques
for coping with computational underflow. The abundance of numerical examples and
pointers to related references will also be of use.
Of course, encyclopedias cover many subjects, too; a good text not only contains
information, but arranges it in an edifying way. In organizing the book, the authors
have &amp;quot;decided against attempting to present Statistical NLP as homogeneous in terms
of mathematical tools and theories&amp;quot; (p. xxx), asserting that a unified theory, though
desirable, does not currently exist. As a result, instead of the ternary structure im-
plied by the third paragraph above—background, theory, applications—fundamentals
appear on a need-to-know basis. For example, the key concept of separating training
and test data (failure to do so being regarded in the community as a &amp;quot;cardinal sin&amp;quot;
[p. 2061) appears as a subsection of the chapter on n-gram language modeling. It is
therefore imperative that the &amp;quot;road map&amp;quot; section (p. xxxv) be read carefully.
This design decision enables the authors to place attractive yet accessible topics
early in the book. For instance, word sense disambiguation, a problem students seem
to find quite intuitive, is presented a full two chapters before hidden Markov models,
even though HMM&apos;s are considered a basic technology in statistical NLP. Two benefits
accrue to those who are developing courses: students not only receive a more gen-
tle (and, arguably, appetizing) introduction to the field, but can start course projects
earlier, which instructors will recognize as a nontrivial point.
However, the lack of an underlying set of principles driving the presentation has
the unfortunate consequence of obscuring some important connections. For example,
classification is not treated in a unified way: Chapter 7 introduces two supervised
classification algorithms, but several popular and important techniques, including de-
cision trees and k-nearest-neighbor, are deferred until Chapter 16. Although both chap-
ters include cross-references, the text&apos;s organization blocks detailed analysis of these
algorithms as a whole; for instance, the results of Mooney&apos;s (1996) comparison experi-
ments simply cannot be discussed. Clustering (unsupervised classification) undergoes
the same disjointed treatment, appearing both in Chapter 7 and 14.
On a related note, the level of mathematical detail fluctuates in certain places.
In general, the book tends to present helpful calculations; however, some derivations
that would provide crucial motivation and clarification have been omitted. A salient
example is (the several versions of) the EM algorithm, a general technique for param-
eter estimation that manifests itself, in different guises, in many areas of statistical
NLP. The book&apos;s suppression of computational steps in its presentations, combined
with some unfortunate typographical errors, risks leaving the reader with neither the
ability nor the confidence to develop EM formulations in his or her own work.
Finally, if FSNLP had been organized around a set of theories, it could have been
more focused. In part, this is because it could have been more selective in its choice of
research paper summaries. Of the many recent publications covered, some are surely,
sadly, not destined to make a substantive impact on the field. The book also occasion-
ally exhibits excessive reluctance to extract principles. One example of this reticence
is its treatment of the work of Chelba and Jelinek (1998); although the text hails this
paper as &amp;quot;the first clear demonstration of a probabilistic parser outperforming a tri-
gram model&amp;quot; (p. 457), it does not discuss what features of the algorithm lead to its
superior results.
Implicit in all of these comments is the belief that a mathematical foundation
for statistical natural language processing can exist and will eventually develop. The
authors, as cited above, maintain that this is not currently the case, and they might well
be right. But in considering the contents of FSNLP, one senses that perhaps already
</bodyText>
<page confidence="0.991837">
278
</page>
<subsectionHeader confidence="0.926298">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.999721125">
there is a thinner book, similar to the current volume but with the background-theory-
applications structure mentioned above, struggling to get out.
I cannot help but remember, in concluding, that I once read a review that said
something like the following: &amp;quot;I know you&apos;re going to see this movie. It doesn&apos;t matter
what my review says. I could write my hair is on fire and you wouldn&apos;t notice because
you&apos;re already out buying tickets.&amp;quot; It seems likely that the same situation exists now;
there is, currently, no other comprehensive reference for statistical NLP. Luckily, this
big book takes its responsibilities seriously, and the authors are to be commended for
their efforts.
But it is worthwhile to remember that there are uses for both Big Books and Little
Books. One of my colleagues, a computational chemist with a background in statistical
physics, recently became interested in applying methods from statistical NLP to protein
modeling.4 In particular, we briefly discussed the notion of using probabilistic context-
free grammars for modeling long-distance dependencies. Intrigued, he asked for a
reference; he wanted a source that would compactly introduce fundamental principles
that he could adapt to his application. I gave him Charniak (1993).
</bodyText>
<sectionHeader confidence="0.976481" genericHeader="abstract">
References
</sectionHeader>
<reference confidence="0.965931594594595">
Allen, James. 1995. Natural Language
Understanding. Second edition. Benjamin
Cummings.
Charniak, Eugene. 1993. Statistical Language
Learning. The MIT Press.
Chelba, Ciprian and Frederick Jelinek. 1998.
Exploiting syntactic structure for language
modeling. In Proceedings of the 36th Annual
Meeting of the Association for Computational
Linguistics and the 17th International
Conference on Computational Linguistics,
pages 225-231.
Hirschberg, Julia. 1998. &amp;quot;Every time I fire a
linguist, my performance goes up,&amp;quot; and
other myths of the statistical natural
language processing revolution. Invited
talk, Fifteenth National Conference on
Artificial Intelligence (AAAI-98).
Jelinek, Frederick. 1997. Statistical Methods
for Speech Recognition. The MIT Press.
Jurafsky, Daniel and James Martin. 2000.
Speech and Language Processing. Prentice
Hall.
Mooney, Raymond J. 1996. Comparative
experiments on disambiguating word
senses: An illustration of the role of bias
in machine learning. hi Conference on
Empirical Methods in Natural Language
Processing, pages 82-91.
Lillian Lee is an assistant professor in the Department of Computer Science at Cornell University.
Together with John Lafferty, she has led two AAAI tutorials on statistical methods in natural lan-
guage processing. She received the Stephen and Marilyn Miles Excellence in Teaching Award in
1999 from Cornell&apos;s College of Engineering. Lee&apos;s address is: Department of Computer Science,
4130 Upson Hall, Cornell University, Ithaca, NY 14853-7501; e-mail: llee@cs.cornelLedu
4 Incidentally, FSNLP&apos;s comment on bioinformatics that &amp;quot;as linguists, we find it a little hard to take
seriously problems over an alphabet of four symbols&amp;quot; (p. 340) is akin to snubbing computer science
because it only deals with zeros and ones.
</reference>
<page confidence="0.996825">
279
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.177408">
<title confidence="0.9979845">Book Reviews Foundations of Statistical Natural Language Processing</title>
<author confidence="0.987761">Christopher D Manning</author>
<author confidence="0.987761">Hinrich Schiitze</author>
<affiliation confidence="0.999908">(Stanford University and Xerox Palo Alto Research Center)</affiliation>
<address confidence="0.97535">Cambridge, MA: The MIT Press, 1999,</address>
<note confidence="0.518045">xxxvii + 680 pp; hardbound, ISBN 0-262-13360-1, $60.00 Reviewed by</note>
<author confidence="0.999651">Lillian Lee</author>
<affiliation confidence="0.999993">Cornell University</affiliation>
<address confidence="0.993796">1993, Eugene Charniak published a slim volume entitled Language Learning.</address>
<abstract confidence="0.970717636363637">At the time, empirical techniques for natural language processing were on the rise; year, Linguistics a special issue on such methods, and Charniak&apos;s text was the first to treat the emerging field. Nowadays, the revolution has become the establishment; in 1998, nearly half the in Linguistics empirical methods (Hirschberg 1998). Indeed, Christopher Manning and Hinrich Schtitze&apos;s new, by-no-means slim textbook on statistical NLP—strangely, the first since Charniak&apos;sl—begins &amp;quot;The need for a thorough textbook for Statistical Natural Language Processing hardly needs to be argued for.&amp;quot; Indubitably so; the question is, is this it? of Statistical Natural Language Processing FSNLP) is certainly ambitious in scope. True to its name, it contains a great deal of preparatory material,</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allen</author>
</authors>
<title>Natural Language Understanding. Second edition. Benjamin Cummings.</title>
<date>1995</date>
<contexts>
<context position="2830" citStr="Allen 1995" startWordPosition="400" endWordPosition="401">ct alone confers some benefits. For the researcher, FSNLP offers the convenience of one-stop shopping: at present, there is no other NLP reference in which standard empirical techniques, statistical tables, definitions of linguistic terms, and elements of information retrieval appear together; furthermore, the text also summarizes and critiques many individual research papers. Similarly, someone teaching a course on statistical NLP will appreciate the large number of topics FSNLP covers, allowing the tailoring of a syllabus to individual in1 In the interim, the second edition of Allen&apos;s book (Allen 1995) did include some material on probabilistic methods, and much of Jelinek&apos;s Statistical Methods for Speech Recognition (Jelinek 1997) concerns language processing. Also, Speech and Language Processing (Jurafsky and Martin 2000), still forthcoming at the writing of this review, promises to cover many empirical methods. 2 The grouping of topics in this paragraph, while convenient, does not correspond to the order of presentation in the book. Indeed, the way in which one thinks about a subject need not be the organization that is best for teaching it, a point to which we will return later. 3 For t</context>
</contexts>
<marker>Allen, 1995</marker>
<rawString>Allen, James. 1995. Natural Language Understanding. Second edition. Benjamin Cummings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical Language Learning.</title>
<date>1993</date>
<publisher>The MIT Press.</publisher>
<marker>Charniak, 1993</marker>
<rawString>Charniak, Eugene. 1993. Statistical Language Learning. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Frederick Jelinek</author>
</authors>
<title>Exploiting syntactic structure for language modeling.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics,</booktitle>
<pages>225--231</pages>
<contexts>
<context position="7129" citStr="Chelba and Jelinek (1998)" startWordPosition="1059" endWordPosition="1062">ographical errors, risks leaving the reader with neither the ability nor the confidence to develop EM formulations in his or her own work. Finally, if FSNLP had been organized around a set of theories, it could have been more focused. In part, this is because it could have been more selective in its choice of research paper summaries. Of the many recent publications covered, some are surely, sadly, not destined to make a substantive impact on the field. The book also occasionally exhibits excessive reluctance to extract principles. One example of this reticence is its treatment of the work of Chelba and Jelinek (1998); although the text hails this paper as &amp;quot;the first clear demonstration of a probabilistic parser outperforming a trigram model&amp;quot; (p. 457), it does not discuss what features of the algorithm lead to its superior results. Implicit in all of these comments is the belief that a mathematical foundation for statistical natural language processing can exist and will eventually develop. The authors, as cited above, maintain that this is not currently the case, and they might well be right. But in considering the contents of FSNLP, one senses that perhaps already 278 Book Reviews there is a thinner book</context>
</contexts>
<marker>Chelba, Jelinek, 1998</marker>
<rawString>Chelba, Ciprian and Frederick Jelinek. 1998. Exploiting syntactic structure for language modeling. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics, pages 225-231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hirschberg</author>
</authors>
<title>Every time I fire a linguist, my performance goes up,&amp;quot; and other myths of the statistical natural language processing revolution.</title>
<date>1998</date>
<booktitle>Invited talk, Fifteenth National Conference on Artificial Intelligence (AAAI-98).</booktitle>
<contexts>
<context position="781" citStr="Hirschberg 1998" startWordPosition="108" endWordPosition="109">ter) Cambridge, MA: The MIT Press, 1999, xxxvii + 680 pp; hardbound, ISBN 0-262-13360-1, $60.00 Reviewed by Lillian Lee Cornell University In 1993, Eugene Charniak published a slim volume entitled Statistical Language Learning. At the time, empirical techniques for natural language processing were on the rise; that year, Computational Linguistics published a special issue on such methods, and Charniak&apos;s text was the first to treat the emerging field. Nowadays, the revolution has become the establishment; in 1998, nearly half the papers in Computational Linguistics concerned empirical methods (Hirschberg 1998). Indeed, Christopher Manning and Hinrich Schtitze&apos;s new, by-no-means slim textbook on statistical NLP—strangely, the first since Charniak&apos;sl—begins &amp;quot;The need for a thorough textbook for Statistical Natural Language Processing hardly needs to be argued for.&amp;quot; Indubitably so; the question is, is this it? Foundations of Statistical Natural Language Processing (henceforth FSNLP) is certainly ambitious in scope. True to its name, it contains a great deal of preparatory material, including: gentle introductions to probability and information theory; a chapter on linguistic concepts; and (a most welc</context>
</contexts>
<marker>Hirschberg, 1998</marker>
<rawString>Hirschberg, Julia. 1998. &amp;quot;Every time I fire a linguist, my performance goes up,&amp;quot; and other myths of the statistical natural language processing revolution. Invited talk, Fifteenth National Conference on Artificial Intelligence (AAAI-98).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
</authors>
<title>Statistical Methods for Speech Recognition.</title>
<date>1997</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="2962" citStr="Jelinek 1997" startWordPosition="418" endWordPosition="419">er NLP reference in which standard empirical techniques, statistical tables, definitions of linguistic terms, and elements of information retrieval appear together; furthermore, the text also summarizes and critiques many individual research papers. Similarly, someone teaching a course on statistical NLP will appreciate the large number of topics FSNLP covers, allowing the tailoring of a syllabus to individual in1 In the interim, the second edition of Allen&apos;s book (Allen 1995) did include some material on probabilistic methods, and much of Jelinek&apos;s Statistical Methods for Speech Recognition (Jelinek 1997) concerns language processing. Also, Speech and Language Processing (Jurafsky and Martin 2000), still forthcoming at the writing of this review, promises to cover many empirical methods. 2 The grouping of topics in this paragraph, while convenient, does not correspond to the order of presentation in the book. Indeed, the way in which one thinks about a subject need not be the organization that is best for teaching it, a point to which we will return later. 3 For the record: 1.66 kg (3 lb., 10.7 oz.) 277 Computational Linguistics Volume 26, Number 2 terests. And for those entering the field, th</context>
</contexts>
<marker>Jelinek, 1997</marker>
<rawString>Jelinek, Frederick. 1997. Statistical Methods for Speech Recognition. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>James Martin</author>
</authors>
<title>Speech and Language Processing.</title>
<date>2000</date>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="3056" citStr="Jurafsky and Martin 2000" startWordPosition="428" endWordPosition="431">itions of linguistic terms, and elements of information retrieval appear together; furthermore, the text also summarizes and critiques many individual research papers. Similarly, someone teaching a course on statistical NLP will appreciate the large number of topics FSNLP covers, allowing the tailoring of a syllabus to individual in1 In the interim, the second edition of Allen&apos;s book (Allen 1995) did include some material on probabilistic methods, and much of Jelinek&apos;s Statistical Methods for Speech Recognition (Jelinek 1997) concerns language processing. Also, Speech and Language Processing (Jurafsky and Martin 2000), still forthcoming at the writing of this review, promises to cover many empirical methods. 2 The grouping of topics in this paragraph, while convenient, does not correspond to the order of presentation in the book. Indeed, the way in which one thinks about a subject need not be the organization that is best for teaching it, a point to which we will return later. 3 For the record: 1.66 kg (3 lb., 10.7 oz.) 277 Computational Linguistics Volume 26, Number 2 terests. And for those entering the field, the book records &amp;quot;folklore&amp;quot; knowledge that is typically acquired only by word of mouth or bitter</context>
</contexts>
<marker>Jurafsky, Martin, 2000</marker>
<rawString>Jurafsky, Daniel and James Martin. 2000. Speech and Language Processing. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond J Mooney</author>
</authors>
<title>Comparative experiments on disambiguating word senses: An illustration of the role of bias</title>
<date>1996</date>
<booktitle>in machine learning. hi Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>82--91</pages>
<marker>Mooney, 1996</marker>
<rawString>Mooney, Raymond J. 1996. Comparative experiments on disambiguating word senses: An illustration of the role of bias in machine learning. hi Conference on Empirical Methods in Natural Language Processing, pages 82-91.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Lillian Lee</author>
</authors>
<title>is an assistant professor in the Department of Computer Science at Cornell University. Together with John Lafferty, she has led two AAAI tutorials on statistical methods in natural language processing.</title>
<booktitle>She received the Stephen and Marilyn Miles Excellence in Teaching Award in 1999 from Cornell&apos;s College of Engineering. Lee&apos;s address is:</booktitle>
<institution>Department of Computer Science, 4130 Upson Hall, Cornell University,</institution>
<location>Ithaca, NY</location>
<note>14853-7501; e-mail: llee@cs.cornelLedu</note>
<marker>Lee, </marker>
<rawString>Lillian Lee is an assistant professor in the Department of Computer Science at Cornell University. Together with John Lafferty, she has led two AAAI tutorials on statistical methods in natural language processing. She received the Stephen and Marilyn Miles Excellence in Teaching Award in 1999 from Cornell&apos;s College of Engineering. Lee&apos;s address is: Department of Computer Science, 4130 Upson Hall, Cornell University, Ithaca, NY 14853-7501; e-mail: llee@cs.cornelLedu</rawString>
</citation>
<citation valid="false">
<authors>
<author>Incidentally</author>
</authors>
<title>FSNLP&apos;s comment on bioinformatics that &amp;quot;as linguists, we find it a little hard to take seriously problems over an alphabet of four symbols&amp;quot; (p. 340) is akin to snubbing computer science because it only deals with zeros and ones.</title>
<marker>Incidentally, </marker>
<rawString>4 Incidentally, FSNLP&apos;s comment on bioinformatics that &amp;quot;as linguists, we find it a little hard to take seriously problems over an alphabet of four symbols&amp;quot; (p. 340) is akin to snubbing computer science because it only deals with zeros and ones.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>