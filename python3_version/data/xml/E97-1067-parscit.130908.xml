<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.035142">
<title confidence="0.924557">
Choosing the Word Most Typical in Context
Using a Lexical Co-occurrence Network
</title>
<author confidence="0.993247">
Philip Edmonds
</author>
<affiliation confidence="0.999397">
Department of Computer Science, University of Toronto
</affiliation>
<address confidence="0.960263">
Toronto, Canada, M5S 3G4
</address>
<email confidence="0.974257">
pedmondsOcs.toronto.edu
</email>
<sectionHeader confidence="0.994013" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999957909090909">
This paper presents a partial solution to a com-
ponent of the problem of lexical choice: choos-
ing the synonym most typical, or expected, in
context. We apply a new statistical approach
to representing the context of a word through
lexical co-occurrence networks. The imple-
mentation was trained and evaluated on a large
corpus, and results show that the inclusion of
second-order co-occurrence relations improves
the performance of our implemented lexical
choice program.
</bodyText>
<sectionHeader confidence="0.998424" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99985275">
Recent work views lexical choice as the process of map-
ping from a set of concepts (in some representation of
knowledge) to a word or phrase (Elhadad, 1992; Stede,
1996). When the same concept admits more than one
le:cicalization, it is often difficult to choose which of
these &apos;synonyms&apos; is the most appropriate for achieving
the desired pragmatic goals; but this is necessary for high-
quality machine translation and natural language genera-
tion.
Knowledge-based approaches to representing the po-
tentially subtle differences between synonyms have suf-
fered from a serious lexical acquisition bottleneck (Di-
Marco, Hirst, and Stede, 1993; Hirst, 1995). Statistical
approaches, which have sought to explicitly represent
differences between pairs of synonyms with respect to
their occurrence with other specific words (Church et al.,
1994), are inefficient in time and space.
This paper presents a new statistical approach to mod-
eling context that provides a preliminary solution to an
important sub-problem, that of determining the near-
synonym that is most typical, or expected, if any, in a
given context. Although weaker than full lexical choice,
because it doesn&apos;t choose the &apos;best&apos; word, we believe that
it is a necessary first step, because it would allow one
to determine the effects of choosing a non-typical word
in place of the typical word. The approach relies on a
generalization of lexical co-occurrence that allows for an
implicit representation of the differences between two (or
more) words with respect to any actual context.
For example, our implemented lexical choice program
selects mistake as most typical for the &apos;gap&apos; in sen-
tence (1), and error in (2).
</bodyText>
<listItem confidence="0.985283571428571">
(1) However, such a move also would run the risk
of cutting deeply into U.S. economic growth,
which is why some economists think it would be
a big error I mistake I oversight).
(2) The ( error I mistake I oversight) was magnified
when the Army failed to charge the standard
percentage rate for packing and handling.
</listItem>
<sectionHeader confidence="0.59692" genericHeader="method">
2 Generalizing Lexical Co-occurrence
</sectionHeader>
<subsectionHeader confidence="0.945437">
2.1 Evidence-based Models of Context
</subsectionHeader>
<bodyText confidence="0.972444947368421">
Evidence-based models represent context as a set of fea-
tures, say words, that are observed to co-occur with, and
thereby predict, a word (Yarowslcy, 1992; Golding and
Schabes, 1996; Karow and Edelman, 1996; Ng and Lee,
1996). But, if we use just the context surrounding a word,
we might not be able to build up a representation satisfac-
tory to uncover the subtle differences between synonyms,
because of the massive volume of text that would be re-
quired.
Now, observe that even though a word might not co-
occur significantly with another given word, it might nev-
ertheless predict the use of that word if the two words are
mutually related to a third word. That is, we can treat
lexical co-occurrence as though it were moderately tran-
sitive. For example, in (3), learn provides evidence for
task because it co-occurs (in other contexts) with difficult,
which in turn co-occurs with task (in other contexts), even
though learn is not seen to co-occur significantly with
task.
</bodyText>
<listItem confidence="0.753916666666667">
(3) The team&apos;s most urgent task was to learn whether
Chernobyl would suggest any safety flaws at
KWU-designed plants.
</listItem>
<bodyText confidence="0.998536">
So, by augmenting the contextual representation of a
word with such second-order (and higher) co-occurrence
relations, we stand to have greater predictive power, as-
suming that we assign less weight to them in accordance
with their lower information content. And as our results
will show, this generalization of co-occurrence is neces-
sary.
</bodyText>
<page confidence="0.956815">
507
</page>
<figure confidence="0.9388221">
task/NN
I suggest/VB
Set POS
1 Jj
2 NN
3 NN
4 NN
5 NN
6 VB
7 vs
</figure>
<construct confidence="0.911391666666667">
Synonyms (with training corpus frequency)
difficult (352), hard (348), tough (230)
error (64), mistake (61), oversight (37)
job (418), task (123), duty (48)
responsibility (142), commitment (122),
obligation (96), burden (81)
material (177), stuff (79), substance (45)
give (624), provide (501), offer (302)
settle (126), resolve (79)
</construct>
<figure confidence="0.997945083333333">
1.41
1.37 1.30
l.96
urgent/JJ
/ 0.41
1.36 1.33
called/VSD
1.25
forces/NNS
1.24
1.89
learn/VB
costly/JJ
find/VB
practice/NN
1.30
plants/NNS
2.54
1.35 1.40 2.19
safety/NN
flaws/NNS
To/TO
1.36
team/NN
</figure>
<tableCaption confidence="0.833884">
Table 1: The sets of synonyms for our experiment.
</tableCaption>
<figureCaption confidence="0.966412666666667">
Figure 1: A fragment of the lexical co-occurrence net-
work for task. The dashed line is a second-order relation
implied by the network.
</figureCaption>
<bodyText confidence="0.9996352">
We can represent these relations in a lexical co-
occurrence network, as in figure 1, that connects lexi-
cal items by just their first-order co-occurrence relations.
Second-order and higher relations are then implied by
transitivity.
</bodyText>
<subsectionHeader confidence="0.999632">
2.2 Building Co-occurrence Networks
</subsectionHeader>
<bodyText confidence="0.999824714285714">
We build a lexical co-occurrence network as follows:
Given a root word, connect it to all the words that sig-
nificantly co-occur with it in the training corpus;&apos; then,
recursively connect these words to their significant co-
occurring words up to some specified depth.
We use the intersection of two well-known measures
of significance, mutual information scores and t-scores
(Church et al., 1994), to determine if a (first-order) co-
occurrence relation should be included in the network;
however, we use just the t-scores in computing signifi-
cance scores for all the relations. Given two words, wo
and wd, in a co-occurrence relation of order d, and a
shortest path P(wo5wd) = (wo, • • 5wd) between them, the
significance score is
</bodyText>
<equation confidence="0.7334445">
sig(wo, wd) = d31 E t(wi 1&apos; wi)
w■EP(wl,wd)
</equation>
<bodyText confidence="0.999502857142857">
This formula ensures that significance is inversely pro-
portional to the order of the relation. For example, in the
network of figure 1, sig(task, learn) = [t(task, difficult) +
it(difficult,learn)118 = 0.41.
A single network can be quite large. For instance,
the complete network for task (see figure 1) up to the
third-order has 8998 nodes and 37,548 edges.
</bodyText>
<subsectionHeader confidence="0.999725">
2.3 Choosing the Most Typical Word
</subsectionHeader>
<bodyText confidence="0.9997475">
The amount of evidence that a given sentence provides for
choosing a candidate word is the sum of the significance
scores of each co-occurrence of the candidate with a word
&apos;Our training corpus was the part-of-speech-tagged 1989
Wall Street Journal, which consists of N = 2,709,659 tokens.
No lemmatization or sense disambiguation was done. Stop
words were numbers, symbols, proper nouns, and any token
with a raw frequency greater than F = 800.
in the sentence. So, given a gap in a sentence S, we find
the candidate c for the gap that maximizes
</bodyText>
<equation confidence="0.9893185">
M(c, S) = E sig(c, w)
wE s
</equation>
<bodyText confidence="0.99961275">
For example, given S as sentence (3), above, and the
network of figure 1, M(task,S) = 4.40. However, job
(using its own network) matches best with a score of
5.52; duty places third with a score of 2.21.
</bodyText>
<sectionHeader confidence="0.999857" genericHeader="evaluation">
3 Results and Evaluation
</sectionHeader>
<bodyText confidence="0.999970515151515">
To evaluate the lexical choice program, we selected sev-
eral sets of near-synonyms, shown in table 1, that have
low polysemy in the corpus, and that occur with similar
frequencies. This is to reduce the confounding effects of
lexical ambiguity.
For each set, we collected all sentences from the yet-
unseen 1987 Wall Street Journal (part-of-speech-tagged)
that contained any of the members of the set, ignoring
word sense. We replaced each occurrence by a &apos;gap&apos; that
the program then had to fill. We compared the &apos;correct-
ness&apos; of the choices made by our program to the baseline
of always choosing the most frequent synonym according
to the training corpus.
But what are the &apos;correct&apos; responses? Ideally, they
should be chosen by a credible human informant. But
regrettably, we are not in a position to undertake a study
of how humans judge typical usage, so we will turn in-
stead to a less ideal source: the authors of the Wall Street
Journal. The problem is, of course, that authors aren&apos;t
always typical. A particular word might occur in a &apos;pat-
tern&apos; in which another synonym was seen more often,
making it the typical choice. Thus, we cannot expect
perfect accuracy in this evaluation.
Table 2 shows the results for all seven sets of synonyms
under different versions of the program. We varied two
parameters: (1) the window size used during the construc-
tion of the network: either narrow (±4 words), medium
(± 10 words), or wide (± 50 words); (2) the maximum
order of co-occurrence relation allowed: 1, 2, or 3.
The results show that at least second-order co-
occurrences are necessary to achieve better than baseline
accuracy in this task; regular co-occurrence relations are
insufficient. This justifies our assumption that we need
</bodyText>
<page confidence="0.991516">
508
</page>
<table confidence="0.995567583333333">
Set 1 2 3 4 5 6 7
Size 6665 1030 5402 3138 1828 10204 1568
Baseline 40.1% 33.5% 74.2% 36.6% 62.8% 45.7% 62.2%
1 31.3% 18.7% 34.5% 27.7% 28.8% 33.2% 41.3%
Narrow 2 47.2% 44.5% 66.2% 43.9% 61.9%a 48.1% 62.8%a
3 47.9% 48.9% 68.9% 44.3% 64.6%a 48.6% 65.9%
1 24.0% 25.0% 26.4% 29.3% 28.8% 20.6% 44.2%
Medium 2 42.5% 47.1% 55.3% 45.3% 61.5%° 44.3% 63.6%°
3 42.5% 47.0% 53.6%
Wide 1 9.2% 20.6% 17.5% 20.7% 21.2% 4.1% 26.5%
2 39.9%° 46.2% 47.1% 43.2% 52.7% 37.7% 58.6%
°Difference from baseline not significant.
</table>
<tableCaption confidence="0.745003666666667">
Table 2: Accuracy of several different versions of the lexical choice program. The best score for each set is in boldface.
Size refers to the size of the sample collection. All differences from baseline are significant at the 5% level according
to Pearson&apos;s X2 test, unless indicated.
</tableCaption>
<bodyText confidence="0.9990861">
more than the surrounding context to build adequate con-
textual representations.
Also, the narrow window gives consistently higher ac-
curacy than the other sizes. This can be explained, per-
haps, by the fact that differences between near-synonyms
often involve differences in short-distance collocations
with neighboring words, e.g., face the task.
There are two reasons why the approach doesn&apos;t do
as well as an automatic approach ought to. First, as
mentioned above, our method of evaluation is not ideal;
it may make our results just seem poor. Perhaps our
results actually show the level of &apos;typical usage&apos; in the
newspaper.
Second, lexical ambiguity is a major problem, affecting
both evaluation and the construction of the co-occurrence
network. For example, in sentence (3), above, it turns out
that the program uses safely as evidence for choosing job
(because job safety is a frequent collocation), but this is
the wrong sense of job. Syntactic and collocational red
herrings can add noise too.
</bodyText>
<sectionHeader confidence="0.999492" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999984375">
We introduced the problem of choosing the most typical
synonym in context, and gave a solution that relies on a
generalization of lexical co-occurrence. The results show
that a narrow window of training context (±4 words)
works best for this task, and that at least second-order
co-occurrence relations are necessary. We are planning
to extend the model to account for more structure in the
narrow window of context.
</bodyText>
<sectionHeader confidence="0.997751" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999751">
For comments and advice, I thank Graeme Hirst, Eduard
Hovy, and Stephen Green. This work is financially sup-
ported by the Natural Sciences and Engineering Council
of Canada.
</bodyText>
<sectionHeader confidence="0.999413" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999694135135135">
Church, Kenneth Ward, William Gale, Patrick Hanks, Donald
Hindle, and Rosamund Moon. 1994. Lexical substitutability.
In B.T.S. Atkins and A. Zampolli, editors, Computational
Approaches to the Lexicon. Oxford University Press, pages
153-177.
DiMarco, Chrysanne, Graeme Hirst, and Manfred Stede. 1993.
The semantic and stylistic differentiation of synonyms and
near-synonyms. In AAAI Spring Symposium on Building
Lexicons for Machine Translation, pages 114-121, Stanford,
CA, March.
Elhadad, Michael. 1992. Using Argumentation to Control
Lexical Choice: A Functional Unification Implementation.
Ph.D. thesis, Columbia University.
Golding, Andrew R. and Yves Schabes. 1996. Combin-
ing trigram-based and feature-based methods for context-
sensitive spelling correction. In Proceedings of the 34th
Annual Meeting of the Association for Computational Lin-
guistics.
Hirst, Graeme. 1995. Near-synonymy and the structure of
lexical knowledge. In AAAI Symposium on Representation
and Acquisition of Lexical Knowledge: Polysemy, Ambiguity,
and Generativity, pages 51-56, Stanford, CA, March.
Karow, Yael and Shimon Edelman. 1996. Learning similarity-
based word sense disambiguation from sparse data. In Pro-
ceedings of the Fourth Workshop on Very Large Corpora,
Copenhagen, August.
Ng, Hwee Tou and Hian Beng Lee. 1996. Integrating multiple
sources to disambiguate word sense: An exemplar-based
approach. In Proceedings of the 34th Annual Meeting of the
Association for Computational Linguistics.
Stede, Manfred. 1996. Lexical Semantics and Knowledge Rep-
resentation in Multilingual Sentence Generation. Ph.D. the-
sis, University of Toronto.
Yarowslcy, David. 1992. Word-sense disambiguation using
statistical models of Roget&apos;s categories trained on large cor-
pora. In Proceedings of the 14th International Conference on
Computational Linguistics (COLING-92), pages 454-460.
</reference>
<page confidence="0.998473">
509
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.983008">
<title confidence="0.9989095">Choosing the Word Most Typical in Context Using a Lexical Co-occurrence Network</title>
<author confidence="0.999754">Philip Edmonds</author>
<affiliation confidence="0.999981">Department of Computer Science, University of Toronto</affiliation>
<address confidence="0.999491">Toronto, Canada, M5S 3G4</address>
<email confidence="0.999683">pedmondsOcs.toronto.edu</email>
<abstract confidence="0.998825">This paper presents a partial solution to a component of the problem of lexical choice: choosing the synonym most typical, or expected, in context. We apply a new statistical approach to representing the context of a word through lexical co-occurrence networks. The implementation was trained and evaluated on a large corpus, and results show that the inclusion of second-order co-occurrence relations improves the performance of our implemented lexical choice program.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>William Gale</author>
<author>Patrick Hanks</author>
<author>Donald Hindle</author>
<author>Rosamund Moon</author>
</authors>
<title>Lexical substitutability.</title>
<date>1994</date>
<booktitle>Computational Approaches to the Lexicon.</booktitle>
<pages>153--177</pages>
<editor>In B.T.S. Atkins and A. Zampolli, editors,</editor>
<publisher>Oxford University Press,</publisher>
<contexts>
<context position="1525" citStr="Church et al., 1994" startWordPosition="225" endWordPosition="228">le:cicalization, it is often difficult to choose which of these &apos;synonyms&apos; is the most appropriate for achieving the desired pragmatic goals; but this is necessary for highquality machine translation and natural language generation. Knowledge-based approaches to representing the potentially subtle differences between synonyms have suffered from a serious lexical acquisition bottleneck (DiMarco, Hirst, and Stede, 1993; Hirst, 1995). Statistical approaches, which have sought to explicitly represent differences between pairs of synonyms with respect to their occurrence with other specific words (Church et al., 1994), are inefficient in time and space. This paper presents a new statistical approach to modeling context that provides a preliminary solution to an important sub-problem, that of determining the nearsynonym that is most typical, or expected, if any, in a given context. Although weaker than full lexical choice, because it doesn&apos;t choose the &apos;best&apos; word, we believe that it is a necessary first step, because it would allow one to determine the effects of choosing a non-typical word in place of the typical word. The approach relies on a generalization of lexical co-occurrence that allows for an imp</context>
<context position="5629" citStr="Church et al., 1994" startWordPosition="896" endWordPosition="899">ical cooccurrence network, as in figure 1, that connects lexical items by just their first-order co-occurrence relations. Second-order and higher relations are then implied by transitivity. 2.2 Building Co-occurrence Networks We build a lexical co-occurrence network as follows: Given a root word, connect it to all the words that significantly co-occur with it in the training corpus;&apos; then, recursively connect these words to their significant cooccurring words up to some specified depth. We use the intersection of two well-known measures of significance, mutual information scores and t-scores (Church et al., 1994), to determine if a (first-order) cooccurrence relation should be included in the network; however, we use just the t-scores in computing significance scores for all the relations. Given two words, wo and wd, in a co-occurrence relation of order d, and a shortest path P(wo5wd) = (wo, • • 5wd) between them, the significance score is sig(wo, wd) = d31 E t(wi 1&apos; wi) w■EP(wl,wd) This formula ensures that significance is inversely proportional to the order of the relation. For example, in the network of figure 1, sig(task, learn) = [t(task, difficult) + it(difficult,learn)118 = 0.41. A single netwo</context>
</contexts>
<marker>Church, Gale, Hanks, Hindle, Moon, 1994</marker>
<rawString>Church, Kenneth Ward, William Gale, Patrick Hanks, Donald Hindle, and Rosamund Moon. 1994. Lexical substitutability. In B.T.S. Atkins and A. Zampolli, editors, Computational Approaches to the Lexicon. Oxford University Press, pages 153-177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chrysanne DiMarco</author>
<author>Graeme Hirst</author>
<author>Manfred Stede</author>
</authors>
<title>The semantic and stylistic differentiation of synonyms and near-synonyms.</title>
<date>1993</date>
<booktitle>In AAAI Spring Symposium on Building Lexicons for Machine Translation,</booktitle>
<pages>114--121</pages>
<location>Stanford, CA,</location>
<contexts>
<context position="1325" citStr="DiMarco, Hirst, and Stede, 1993" startWordPosition="195" endWordPosition="200">cent work views lexical choice as the process of mapping from a set of concepts (in some representation of knowledge) to a word or phrase (Elhadad, 1992; Stede, 1996). When the same concept admits more than one le:cicalization, it is often difficult to choose which of these &apos;synonyms&apos; is the most appropriate for achieving the desired pragmatic goals; but this is necessary for highquality machine translation and natural language generation. Knowledge-based approaches to representing the potentially subtle differences between synonyms have suffered from a serious lexical acquisition bottleneck (DiMarco, Hirst, and Stede, 1993; Hirst, 1995). Statistical approaches, which have sought to explicitly represent differences between pairs of synonyms with respect to their occurrence with other specific words (Church et al., 1994), are inefficient in time and space. This paper presents a new statistical approach to modeling context that provides a preliminary solution to an important sub-problem, that of determining the nearsynonym that is most typical, or expected, if any, in a given context. Although weaker than full lexical choice, because it doesn&apos;t choose the &apos;best&apos; word, we believe that it is a necessary first step, </context>
</contexts>
<marker>DiMarco, Hirst, Stede, 1993</marker>
<rawString>DiMarco, Chrysanne, Graeme Hirst, and Manfred Stede. 1993. The semantic and stylistic differentiation of synonyms and near-synonyms. In AAAI Spring Symposium on Building Lexicons for Machine Translation, pages 114-121, Stanford, CA, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Elhadad</author>
</authors>
<title>Using Argumentation to Control Lexical Choice: A Functional Unification Implementation.</title>
<date>1992</date>
<tech>Ph.D. thesis,</tech>
<institution>Columbia University.</institution>
<contexts>
<context position="846" citStr="Elhadad, 1992" startWordPosition="128" endWordPosition="129">ial solution to a component of the problem of lexical choice: choosing the synonym most typical, or expected, in context. We apply a new statistical approach to representing the context of a word through lexical co-occurrence networks. The implementation was trained and evaluated on a large corpus, and results show that the inclusion of second-order co-occurrence relations improves the performance of our implemented lexical choice program. 1 Introduction Recent work views lexical choice as the process of mapping from a set of concepts (in some representation of knowledge) to a word or phrase (Elhadad, 1992; Stede, 1996). When the same concept admits more than one le:cicalization, it is often difficult to choose which of these &apos;synonyms&apos; is the most appropriate for achieving the desired pragmatic goals; but this is necessary for highquality machine translation and natural language generation. Knowledge-based approaches to representing the potentially subtle differences between synonyms have suffered from a serious lexical acquisition bottleneck (DiMarco, Hirst, and Stede, 1993; Hirst, 1995). Statistical approaches, which have sought to explicitly represent differences between pairs of synonyms w</context>
</contexts>
<marker>Elhadad, 1992</marker>
<rawString>Elhadad, Michael. 1992. Using Argumentation to Control Lexical Choice: A Functional Unification Implementation. Ph.D. thesis, Columbia University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew R Golding</author>
<author>Yves Schabes</author>
</authors>
<title>Combining trigram-based and feature-based methods for contextsensitive spelling correction.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2932" citStr="Golding and Schabes, 1996" startWordPosition="458" endWordPosition="461">pical for the &apos;gap&apos; in sentence (1), and error in (2). (1) However, such a move also would run the risk of cutting deeply into U.S. economic growth, which is why some economists think it would be a big error I mistake I oversight). (2) The ( error I mistake I oversight) was magnified when the Army failed to charge the standard percentage rate for packing and handling. 2 Generalizing Lexical Co-occurrence 2.1 Evidence-based Models of Context Evidence-based models represent context as a set of features, say words, that are observed to co-occur with, and thereby predict, a word (Yarowslcy, 1992; Golding and Schabes, 1996; Karow and Edelman, 1996; Ng and Lee, 1996). But, if we use just the context surrounding a word, we might not be able to build up a representation satisfactory to uncover the subtle differences between synonyms, because of the massive volume of text that would be required. Now, observe that even though a word might not cooccur significantly with another given word, it might nevertheless predict the use of that word if the two words are mutually related to a third word. That is, we can treat lexical co-occurrence as though it were moderately transitive. For example, in (3), learn provides evid</context>
</contexts>
<marker>Golding, Schabes, 1996</marker>
<rawString>Golding, Andrew R. and Yves Schabes. 1996. Combining trigram-based and feature-based methods for contextsensitive spelling correction. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
</authors>
<title>Near-synonymy and the structure of lexical knowledge.</title>
<date>1995</date>
<booktitle>In AAAI Symposium on Representation and Acquisition of Lexical Knowledge: Polysemy, Ambiguity, and Generativity,</booktitle>
<pages>51--56</pages>
<location>Stanford, CA,</location>
<contexts>
<context position="1339" citStr="Hirst, 1995" startWordPosition="201" endWordPosition="202"> the process of mapping from a set of concepts (in some representation of knowledge) to a word or phrase (Elhadad, 1992; Stede, 1996). When the same concept admits more than one le:cicalization, it is often difficult to choose which of these &apos;synonyms&apos; is the most appropriate for achieving the desired pragmatic goals; but this is necessary for highquality machine translation and natural language generation. Knowledge-based approaches to representing the potentially subtle differences between synonyms have suffered from a serious lexical acquisition bottleneck (DiMarco, Hirst, and Stede, 1993; Hirst, 1995). Statistical approaches, which have sought to explicitly represent differences between pairs of synonyms with respect to their occurrence with other specific words (Church et al., 1994), are inefficient in time and space. This paper presents a new statistical approach to modeling context that provides a preliminary solution to an important sub-problem, that of determining the nearsynonym that is most typical, or expected, if any, in a given context. Although weaker than full lexical choice, because it doesn&apos;t choose the &apos;best&apos; word, we believe that it is a necessary first step, because it wou</context>
</contexts>
<marker>Hirst, 1995</marker>
<rawString>Hirst, Graeme. 1995. Near-synonymy and the structure of lexical knowledge. In AAAI Symposium on Representation and Acquisition of Lexical Knowledge: Polysemy, Ambiguity, and Generativity, pages 51-56, Stanford, CA, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yael Karow</author>
<author>Shimon Edelman</author>
</authors>
<title>Learning similaritybased word sense disambiguation from sparse data.</title>
<date>1996</date>
<booktitle>In Proceedings of the Fourth Workshop on Very Large Corpora,</booktitle>
<location>Copenhagen,</location>
<contexts>
<context position="2957" citStr="Karow and Edelman, 1996" startWordPosition="462" endWordPosition="465">ence (1), and error in (2). (1) However, such a move also would run the risk of cutting deeply into U.S. economic growth, which is why some economists think it would be a big error I mistake I oversight). (2) The ( error I mistake I oversight) was magnified when the Army failed to charge the standard percentage rate for packing and handling. 2 Generalizing Lexical Co-occurrence 2.1 Evidence-based Models of Context Evidence-based models represent context as a set of features, say words, that are observed to co-occur with, and thereby predict, a word (Yarowslcy, 1992; Golding and Schabes, 1996; Karow and Edelman, 1996; Ng and Lee, 1996). But, if we use just the context surrounding a word, we might not be able to build up a representation satisfactory to uncover the subtle differences between synonyms, because of the massive volume of text that would be required. Now, observe that even though a word might not cooccur significantly with another given word, it might nevertheless predict the use of that word if the two words are mutually related to a third word. That is, we can treat lexical co-occurrence as though it were moderately transitive. For example, in (3), learn provides evidence for task because it </context>
</contexts>
<marker>Karow, Edelman, 1996</marker>
<rawString>Karow, Yael and Shimon Edelman. 1996. Learning similaritybased word sense disambiguation from sparse data. In Proceedings of the Fourth Workshop on Very Large Corpora, Copenhagen, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Hian Beng Lee</author>
</authors>
<title>Integrating multiple sources to disambiguate word sense: An exemplar-based approach.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2976" citStr="Ng and Lee, 1996" startWordPosition="466" endWordPosition="469">). (1) However, such a move also would run the risk of cutting deeply into U.S. economic growth, which is why some economists think it would be a big error I mistake I oversight). (2) The ( error I mistake I oversight) was magnified when the Army failed to charge the standard percentage rate for packing and handling. 2 Generalizing Lexical Co-occurrence 2.1 Evidence-based Models of Context Evidence-based models represent context as a set of features, say words, that are observed to co-occur with, and thereby predict, a word (Yarowslcy, 1992; Golding and Schabes, 1996; Karow and Edelman, 1996; Ng and Lee, 1996). But, if we use just the context surrounding a word, we might not be able to build up a representation satisfactory to uncover the subtle differences between synonyms, because of the massive volume of text that would be required. Now, observe that even though a word might not cooccur significantly with another given word, it might nevertheless predict the use of that word if the two words are mutually related to a third word. That is, we can treat lexical co-occurrence as though it were moderately transitive. For example, in (3), learn provides evidence for task because it co-occurs (in other</context>
</contexts>
<marker>Ng, Lee, 1996</marker>
<rawString>Ng, Hwee Tou and Hian Beng Lee. 1996. Integrating multiple sources to disambiguate word sense: An exemplar-based approach. In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manfred Stede</author>
</authors>
<title>Lexical Semantics and Knowledge Representation in Multilingual Sentence Generation.</title>
<date>1996</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Toronto.</institution>
<contexts>
<context position="860" citStr="Stede, 1996" startWordPosition="130" endWordPosition="131"> a component of the problem of lexical choice: choosing the synonym most typical, or expected, in context. We apply a new statistical approach to representing the context of a word through lexical co-occurrence networks. The implementation was trained and evaluated on a large corpus, and results show that the inclusion of second-order co-occurrence relations improves the performance of our implemented lexical choice program. 1 Introduction Recent work views lexical choice as the process of mapping from a set of concepts (in some representation of knowledge) to a word or phrase (Elhadad, 1992; Stede, 1996). When the same concept admits more than one le:cicalization, it is often difficult to choose which of these &apos;synonyms&apos; is the most appropriate for achieving the desired pragmatic goals; but this is necessary for highquality machine translation and natural language generation. Knowledge-based approaches to representing the potentially subtle differences between synonyms have suffered from a serious lexical acquisition bottleneck (DiMarco, Hirst, and Stede, 1993; Hirst, 1995). Statistical approaches, which have sought to explicitly represent differences between pairs of synonyms with respect to</context>
</contexts>
<marker>Stede, 1996</marker>
<rawString>Stede, Manfred. 1996. Lexical Semantics and Knowledge Representation in Multilingual Sentence Generation. Ph.D. thesis, University of Toronto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowslcy</author>
</authors>
<title>Word-sense disambiguation using statistical models of Roget&apos;s categories trained on large corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th International Conference on Computational Linguistics (COLING-92),</booktitle>
<pages>454--460</pages>
<contexts>
<context position="2905" citStr="Yarowslcy, 1992" startWordPosition="456" endWordPosition="457">istake as most typical for the &apos;gap&apos; in sentence (1), and error in (2). (1) However, such a move also would run the risk of cutting deeply into U.S. economic growth, which is why some economists think it would be a big error I mistake I oversight). (2) The ( error I mistake I oversight) was magnified when the Army failed to charge the standard percentage rate for packing and handling. 2 Generalizing Lexical Co-occurrence 2.1 Evidence-based Models of Context Evidence-based models represent context as a set of features, say words, that are observed to co-occur with, and thereby predict, a word (Yarowslcy, 1992; Golding and Schabes, 1996; Karow and Edelman, 1996; Ng and Lee, 1996). But, if we use just the context surrounding a word, we might not be able to build up a representation satisfactory to uncover the subtle differences between synonyms, because of the massive volume of text that would be required. Now, observe that even though a word might not cooccur significantly with another given word, it might nevertheless predict the use of that word if the two words are mutually related to a third word. That is, we can treat lexical co-occurrence as though it were moderately transitive. For example, </context>
</contexts>
<marker>Yarowslcy, 1992</marker>
<rawString>Yarowslcy, David. 1992. Word-sense disambiguation using statistical models of Roget&apos;s categories trained on large corpora. In Proceedings of the 14th International Conference on Computational Linguistics (COLING-92), pages 454-460.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>