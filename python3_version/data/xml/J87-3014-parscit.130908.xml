<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<sectionHeader confidence="0.891021" genericHeader="abstract">
ABSTRACTS OF CURRENT LITERATURE
</sectionHeader>
<bodyText confidence="0.9144475">
If you are interested in ordering any of the following University of Waterloo Computer Science Department reports,
please forward your order to:
</bodyText>
<author confidence="0.464508">
Sue DeAngelis
</author>
<affiliation confidence="0.817075666666667">
Research Report Secretary
University of Waterloo
Computer Science Dept.
</affiliation>
<address confidence="0.342604">
Waterloo, ON CAN N2L 3G1
</address>
<bodyText confidence="0.833294333333333">
An Implementation of a Computational
Model for the Analysis of Arguments
An Introduction to the First Attempt
</bodyText>
<figure confidence="0.973696">
Trevor J. Smedley
Research Report CS-86-26
July 1986
Incorporating User Models Into Expert
Systems for Educational Diagnosis
Robin Cohen, Marlene Jones
Research Report CS-86-37
September 1986
A Model for User-Specific Explanations
from Expert Systems
Peter G. van Beek
Research Report CS-86-42
September 1986
</figure>
<bodyText confidence="0.999939571428572">
The following is a description of a first attempt at a Prolog
implementation of a computational model for the analysis of
arguments. The implementation of the model can be found in
/u/tjsmedley on watdragon. For a detailed description of the
algorithms and other theoretical details, see the papers and
thesis by R. Cohen. The three different algorithms have been
implemented; pre-order, post-order, and hybrid. The code
particular to each algorithm can be found under the directories;
super_pre, super_post, and super_hybrid. The directory front_end
contains code which is common to all three, and there is a
symbolic link to this directory in each of the three directories
mentioned above.
In this paper we study a particular real-world domain, that of
educational diagnosis. We argue that expert systems for
educational diagnosis require user models, and that these user
models should include several components, including the user&apos;s
background knowledge of both the student and the domain, as
well as the user&apos;s goals. Our proposal is directed at enhancing
the particular expert system of the CGD project. We then
propose an architecture for this expert system that separates the
knowledge base into relevant components and includes a user
model. We further demonstrate that this divided model for the
system facilitates providing the best response for a particular
user, according to his background knowledge of the domain and
of the student, and his goals. Finally, we argue that the
techniques outlined here will be useful in general in expert
systems.
In this thesis we present a computational model for generating
non-misleading, user-specific explanations from expert systems.
Ideally an expert system should, as an aid in formulating
cooperative responses, both maintain a model of the user from
the ongoing dialogue and possess knowledge of a user&apos;s
expectations of cooperative expert behavior. Our model focuses
on how knowledge of the user&apos;s goals, plans, and preferences
should influence a response. Included are two important
specifications: what information about the user is needed plus
an algorithm for using that information to compute user-specific
responses. The explanation model may be seen as extending the
work of Joshi, Webber, and Weischedel to include user-specific
goals and to specify the algorithm independent of domain.
The algorithm, together with the model of the user, present a
general method of computing the responses enumerated by
</bodyText>
<page confidence="0.919982">
376 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987
</page>
<note confidence="0.892431">
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<bodyText confidence="0.996097142857143">
Joshi, et al. They also allow us to generate helpful responses
that address a particular user&apos;s preferences and goals and to
recognize cases where a direct, correct response may violate
the user&apos;s expectations of cooperative expert behavior and thus
mislead or confuse the user. This involves, among other things,
the ability to: provide a correct, direct answer to a query;
explain the failure of a query; compute better alternatives to a
user&apos;s plan as expressed in a query; and recognize when a
direct response should be modified and make the appropriate
modification.
While we focus in this thesis on explanations in the context
of expert advice-giving systems, we feel the approach is
applicable to a broad range of question types and to expert
system explanation generation in general.
</bodyText>
<subsectionHeader confidence="0.8636335">
User Modeling Bibliography
Paul Van Arragon
</subsectionHeader>
<bodyText confidence="0.2875375">
Research Report CS-87-22
March 1987
</bodyText>
<subsectionHeader confidence="0.41452125">
The Design and Implementation of an
Evidence Oracle for the Understanding
of Arguments
Mark Anthony Young
</subsectionHeader>
<footnote confidence="0.2871035">
Research Report CS-87-33
June 1987
</footnote>
<subsectionHeader confidence="0.392666">
Integrating Connective Clue Processing
into the Argument Analysis Algorithm
</subsectionHeader>
<bodyText confidence="0.999668708333333">
A user model can be defined as a computer representation of
some aspects of a computer user. Such models have been used
in many areas of Al, such as computer-aided instruction, where
they are used to represent the knowledge and misconceptions of
students [Brown and Burton, 19781. In natural language
processing, the pragmatic component usually contains a kind of
user model [Cohen, 1985].
My Ph.D. thesis topic is to build formal tools for modeling
users. I am concerned mostly with the representation of a
user&apos;s knowledge and belief. This bibliography contains the
references I have accumulated so far. Since I&apos;ve chosen articles
and books which are related to my specific approach to the
problem of user modeling, the bibliography contains several
references, which do not directly address user modeling. For
example, since my approach is to model a user as a theory-
formation system based on Theorist, developed at the
University of Waterloo, I refer to the key Theorist references
and to the related Philosophy-of-Science literature.
Most references in the bibliography are annotated briefly.
Each reference has a few keywords listed in order to classify
them. The first keyword of each reference is an acronym that
refers to the major areas represented. These keywords have
been used to create a short table of cross-references listed by
keyword and author. The following areas are included:
</bodyText>
<table confidence="0.903255142857143">
CAI - Computer-Aided Instruction
ES - Expert Systems
KR - Knowledge Representation
LP - Logic Programming
NL - Natural Language
PS - Philosophy of Science
UM - User Modeling
</table>
<bodyText confidence="0.995463444444444">
When trying to understand the thrust of another person&apos;s
argument, it is necessary to determine what his claim is, and
what evidence he provides for it. It is necessary, therefore, to
be able to recognize evidence relationships in terms of the
speaker&apos;s beliefs. This essay concerns how one goes about
building a system (an oracle) to do that, and provides a
working version of a simple oracle.
The argument analysis algorithm presented by Cohen was
implemented by Smedley. This was a basic implementation
</bodyText>
<table confidence="0.949380888888889">
Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 377
The FINITE STRING Newsletter Abstracts of Current Literature
Implementation which did not include any clue processing. This report describes
Trevor J. Smedley the addition of a basic connective clue processing to the
Research Report CS-87-34 implementation.
June 1987
The following new reports are available and can be ordered free of charge from:
Dr. Johannes Arz
Universitaet des Saarlandes
FR. 10 Informatik IV Im Stadtwald 15
D-6600 Saarbruekenll
E-mail address: wisber%sbsvax.uucp@germany.csnet
Lexical Functional Grammar and
Natural Language Generation
R. Block
Report No. 10
Universitat Hamburg, Hamburg, 1986
A Two Step Reference Problem Solver
R. Hunze
Report No. 11
Siemens AG, Muenchen, 1987
Repraesentation des Fachwissens einer
komplexen Domaene in einer KL-ONE-
artigen Repraesentationssprache
H.-G.Siedka-Bauer
Report No. 12
SCS, Hamburg, 1987
</table>
<bodyText confidence="0.984136428571428">
Lexical Functional Grammar (LFG) is one of the more
promising candidates for a general linguistic theory to emerge
from the theoretical upheavals of the seventies. It displays a
high degree of formalization and lends itself particularly well to
computer implementation. In addition, LFG seeks to shed light
on the mental representation of language in humans.
This paper addresses itself to three points:
</bodyText>
<listItem confidence="0.973857142857143">
1. How plausible are LFG&apos;s claims to &amp;quot;psychological
reality&amp;quot;?
2. To what extent is LFG superior or inferior to the Standard
Theory of Transformational Grammar from which it is
ultimately derived?
3. How realistic is LFG as a theory supporting language
generation as opposed to language interpretation?
</listItem>
<bodyText confidence="0.999930384615385">
The author concludes that, despite positive features in LFG,
transformational operations on canonical structures is a more
efficient and realistic method of generating natural language for
man and machine.
The article is based on the discourse representation theory
(DRT) of H. Kamp and Chomsky&apos;s binding Theory. A
mechanism is presented that builds a discourse representation
structure (DRS) and determines all referents that are
syntactically admissible and in agreement with the accesibility
conditions of DRT.
Imposing semantic restrictions would lead to the n imization
of the set of possible referents, but this has not beei,
investigated yet. Continuing earlier work, unification grammar is
used to develop a formalism that allows for a flexible
connection between syntax and semantic. This basic idea is to
use the equation mechanism LFG provides to encode semantic
information and model DRT explicitly. The system is written in
InterLisp-D and runs on a Siemens 5815.
An essential part of the knowledge base incorporated in the
natural language investment consultation system WISBER is the
terminological knowledge of the domain of investment. QUIRK-
aKL-ONE-like representation language - an outcome of the
WISBER - project - was just to do this work.
The specific demands of the domain knowledge with respect
to the representation and the possibilities as well as constraints
of the representation language are the issues of this report.
</bodyText>
<page confidence="0.970219">
378 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987
</page>
<note confidence="0.603615">
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<subsectionHeader confidence="0.962122">
Selected Dissertation Abstracts
</subsectionHeader>
<bodyText confidence="0.8382">
Compiled by: Susanne M. Humphrey, National Library of Medicine, Bethesda, MD 20209
Bob Krovetz, University of Massachusetts, Amherst, MA 01002
The following are citations selected by title and abstract as being related to computational linguistics or
knowledge representation, resulting from a computer search, using the BRS Information Technologies retrieval
service, of the Dissertation Abstracts International (DAI) database produced by University Microfilms Interna-
tional.
Included are the UM order number and year-month of entry into the database; author; university, degree, and,
if available, number of pages; title; DAI subject category chosen by the author of the dissertation; and abstract.
References are sorted first by DAI subject category and second by author. Citations denoted by an MAI reference
do not yet have abstracts in the database and refer to abstracts in the published Masters Abstracts International.
Unless otherwise specified, paper or microform copies of dissertations may be ordered from:
University Microfilms International
Dissertation Copies
Post Office Box 1764
Ann Arbor, MI 48106
Telephone for U.S. (except Michigan, Hawaii, Alaska): 1-800-521-3042; for Canada: 1-800-268-6090.
Price lists and other ordering and shipping information are in the introduction to the published DAI. An alternate
source for copies is sometimes provided at the end of the abstract.
The dissertation titles and abstracts contained here are published with permission of University Microfilms
International, publishers of Dissertation Abstracts International (copyright 1986) by University Microfilms Inter-
national, and may not be reproduced without their prior permission.
</bodyText>
<figure confidence="0.8132236">
PlanPower, XCON, and MUDMAN:
An In-depth Analysis Into Three
Commercial Expert Systems in Use.
DAI V47(09), SecA, pp 3473
John Julius Sviokla
Harvard University D.B.A. 1986, 448
pages
Business Administration, General
University Microfilms International
ADG86-29661
</figure>
<bodyText confidence="0.999765">
The objective of this thesis is to generate knowledge about the
effects of ESs on the organizations which use them. Three field
sites with expert systems in active use are examined, and the
implications for management are drawn from the empirical
observations.
This thesis uses a comparative, three-site, pre-post
exploratory design to describe and compare the effects of ES
use on three organizations: The Financial Collaborative (using
the PlanPower system), Digital (XCON) and Baroid
(MUDMAN). The study is guided by the notions of
organizational programs as defined by March and Simon, and
the information-processing capacity of the firm, as defined by
Galbraith, to organize, describe, and compare the effects of ES
use across the sites. Eleven exploratory hypotheses act as a
basis for theory-building and further hypothesis generation.
ESs address ill-structured problems. Ill-structured problems
are those problems for which the solution methods and criteria
are either ill-defined or non-existent. In investigating three large-
scale ESs in use, this researcher discovered that these systems
seem to create a phenomenon referred to as &amp;quot;progressive
structuring.&amp;quot; This process alters the nature of the underlying
task and the organizational mechanisms which support it. This
phenomenon is dynamic and evolves over time.
All the ESs seemed to increase the effectiveness and
efficiency of the user firm. The price of the benefits was an
increased rigidity in the task. In considering ESs a manager
should be concerned not only with the ES itself, but with the
process by which the ES is adapted, and the overall process of
creating and using the ES. In addition, the manager needs to
consider the effects of the ES on the uncertainty associated
with the task and should consciously manage that uncertainty to
foster the level of adaptation necessary to keep the ES alive
and viable in the organization.
</bodyText>
<table confidence="0.909403181818182">
Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 379
Foundations of Logic Programming
With Equality
DAI V47(10), SecB, pp4217.
Kwok Hung Chan
The University of Western Ontario
(Canada) Ph.D. 1986.
Computer Science.
University Microfilms International
This item is not available: ADG05-59521
Abstracts of Current Literature
</table>
<bodyText confidence="0.999914204545454">
One interesting issue in artificial intelligence (Al) currently is
the relative merits of, and relationship between, the &amp;quot;symbolic&amp;quot;
and &amp;quot;connectionist&amp;quot; approaches to intelligent systems building.
The performance of more traditional symbolic systems has been
striking, but getting these systems to learn truly new symbols
has proven difficult. Recently, some researchers have begun to
explore a distinctly different type of representation, similar in
some respects to the nerve nets of several decades past. In
these massively parallel, connectionist models, symbols arise
implicitly, through the interactions of many simple and
sub-symbolic elements. One of the advantages of using such
simple elements as building blocks is that several learning
algorithms work quite well. The range of application for
connectionist models has remained limited, however, and it has
been difficult to bridge the gap between this work and standard
Al.
The AIR system represents a connectionist approach to the
problem of free-text information retrieval (IR). Not only is this
an increasingly important type of data, but it provides an
excellent demonstration of the advantages of connectionist
mechanisms, particularly adaptive mechanisms. AIR&apos;s goal is to
build an indexing structure that will retrieve documents that are
likely to be found relevant. Over time, by using users&apos; browsing
patterns as an indication of approval, AIR comes to learn what
the keywords (symbols) mean so as use them to retrieve
appropriate documents. AIR thus attempts to bridge the gap
between connectionist learning techniques and symbolic
knowledge representations.
The work described was done in two phases. The first phase
concentrated on mapping the IR task into a connectionist
network; it is shown that IR is very amenable to this
representation. The second, more central phase of the research
has shown that this network can also adapt. AIR translates the
browsing behaviors of its users into a feedback signal used by
a Hebbian-like local learning rule to change the weights on
some links. Experience with a series of alternative learning
rules are reported, and the results of experiments using human
subjects to evaluate the results of AIR&apos;s learning are presented.
An obstacle to practical logic programming systems with
equality is infinite computation. In the dissertation we study
three strategies for eliminating infinite searches in Horn clause
logic programming systems and develop an extension of Prolog
that has the symmetry, transitivity, and predicate substitutivity
of equality built-in. The three strategies are:
</bodyText>
<listItem confidence="0.999623">
1. Replacing logic programs with infinite search trees by
equivalent logic programs with finite search trees;
2. Building into the inference machine the axioms that cause
infinite search trees;
3. Detecting and failing searches of infinite branches.
</listItem>
<bodyText confidence="0.9998985">
The dissertation consists of two parts. General theories of the
three strategies identified above are developed in Part I. In Part
II we apply these strategies to the problem of eliminating
infinite loops in logic programming with equality.
</bodyText>
<subsectionHeader confidence="0.802152">
Part I. General Theories
</subsectionHeader>
<bodyText confidence="0.99227">
We introduce the notion of CAS-equivalent logic programs:
</bodyText>
<table confidence="0.451124923076923">
The FINITE STRING Newsletter
Adaptive Information Retrieval:
Machine Learning in Associative
Networks
DAI V47(10), SecB, pp4216
Richard Kuehn Belew
The University of Michigan Ph.D. 1986,
328 pages.
Computer Science
University Microfilms International
ADG87-02684
380 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987
The FINITE STRING Newsletter Abstracts of Current Literature
</table>
<bodyText confidence="0.9992485">
logic programs with identical correct answer substitutions.
Fixpoint criteria for equivalent logic programs are suggested and
their correctness is established. Semantic reduction is introduced
as a means of establishing the soundness and completeness of
extensions of SLD-resolution. The possibility of avoiding infinite
searches by detecting infinite branches is explored. A class of
SLD-derivations called repetitive SLD-derivation is distinguished.
Many infinite derivations are instances of repetitive _
SLD-derivations. It is demonstrated that pruning repetitive SLD-
derivations from SLD-trees does not cause incompleteness.
</bodyText>
<subsectionHeader confidence="0.978723">
Part II. Extended Unification for Equality
</subsectionHeader>
<bodyText confidence="0.9999685">
An extension of SLD-resolution called SLDEU-resolution is
presented. The symmetry, transitivity and predicate substitutivity
of equality are built into SLDEU-resolution by extended
unification. Extended unification, if unrestricted, also introduces
infinite loops. We can eliminate some of these infinite loops by
restricting SLDEU-resolution to non-repetitive right recursive
SLDEU-resolution; this forbids extended unification of the first
terms in equality subgoals and has a built-in mechanism for
detecting repetitive derivations. The soundness and completeness
of non-repetitive right recursive SLDEU-resolution are proved.
</bodyText>
<figure confidence="0.975477952380952">
A Formal Description and Theory of
Knowledge Representation
Methodologies
DAI V47 (09), SecB, pp3847.
Nell Cooper
The University of Texas at Arlington
Ph.D. 1986, 117 pages.
Computer Science.
University Microfilms International
ADG87-01100
Input Transformations and Resolution
Implementation Techniques for
Theorem Proving in First-order Logic
DAI V47(09), SecB, pp3848.
Steven Greenbaum
University of Illinois at
Urbana-Champaign
Ph.D. 1986, 259 pages.
Computer Science.
University Microfilms International
ADG87-0I496
</figure>
<bodyText confidence="0.98750512">
The absence of a common and consistently applied terminology
in discussions of knowledge representation techniques and the
lack of a unifying theory or approach are identified as
significant needs in the area of knowledge representation.
Knowledge representation viewed as a collection of levels is
presented as an alternative to traditional definitions. The levels
and their associated primitives are discussed. The concept of
levels within each knowledge representation technique provides
resolution to many of the controversies and disagreements that
have existed among researchers concerning the equivalency of
representation methodologies.
A statement of the equivalence of a certain class of frame
knowledge representation and a certain class of logic based
knowledge representation is presented. Definitions of the classes
are included. Algorithms to convert from each class to the
other are given as evidence of their equivalence.
This thesis describes a resolution based theorem prover
designed for users with little or no knowledge of automated
theorem proving. The prover is intended for high speed solution
of small to moderate sized problems, usually with no user
guidance. This contrasts with many provers designed to use
substantial user guidance to solve hard or very hard problems,
often having huge search spaces. Such provers are often weak
when used without user interaction. Many of our methods
should be applicable to large systems as well.
Our prover uses a restricted form of locking resolution,
together with an additional resolution step. Pending resolvents
are ordered using a priority-based search strategy which
considers a number of factors, including clause complexity
measures, derivation depth of the pending resolvent, and other
features.
Also described are transformations that convert formulas from
one to another. One is a nonstandard clause-form translation
Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 381
The FINITE STRING Newsletter Abstracts of Current Literature
which often avoids the loss of structure and increase in size
resulting from the conventional translation, and also takes
advantage of repeated subexpressions. Another transformation
replaces operators in first-order formulas with their first-order
definitions, before translation to clause form. This works
particularly well with the nonstandard clause-form translation.
There is also a translation from clauses to other clauses that,
when coupled with some prover extensions, is useful for
theorem proving with equality. The equality method incorporates
Knuth-Bendix completion into the proof process to help simplify
the search.
Some implementation methods are described. Data structures
that allow fast clause storage and lookup, and efficient
implementation of various deletion methods, are discussed. A
modification of discrimination networks is described in detail.
</bodyText>
<table confidence="0.566619">
Semantic Networks as Abstract Data
Types
DAI V47(11), SecB, pp4584.
Ernesto Jose Marques Morgado
State University of New York at Buffalo
Ph.D. 1986, 234 pages.
Computer Science.
University Microfilms International
ADG86-29095
An Expert System for Providing On-
line Information Based on Knowledge
</table>
<bodyText confidence="0.998218780487805">
Abstraction has often been used to permit one to concentrate
on the relevant attributes of the domain and to disregard the
irrelevant ones. This is accompanied by a reduction in the
complexity of the domain. Researchers have extensively studied
the use of abstraction in programming languages to allow
programmers to develop software that is precise, reliable,
readable, and maintainable. In spite of the amount of research
that it has been subjected to, data abstraction has been largely
neglected by programmers, when compared with other abstract
methodologies used in programming. One problem is that it is
not always easy to characterize the correct set of operations
that defines an abstract data type; and, although many
definitions have been presented, no precise methodology has
ever been proposed to hint at the choice of those operations. A
second problem is that there is a discrepancy between the
formalism used to define an abstract specification and the
architecture of the underlying virtual machine used to implement
it. This discrepancy makes it difficult for the programmer to
map the abstract specification, written at design time, into a
concrete implementation, written at coding time. In order to
correct these problems, a theory of data abstraction is
presented, which includes a new definition of abstract data type
and a methodology to create abstract data types.
Because of their complexity, semantic networks are defined
in terms of a variety of interrelated data types. The preciseness
of the abstract data type formalism, and its emphasis on the
behavior of the data type operations, rather than on the
structure of its objects, makes the semantics of semantic
networks clearer. In addition, the design, development, and
maintenance of a semantic network processing system requires
an appropriate software engineering environment. The
methodology of data abstraction, with its philosophy of
modularity and independence of representations, provides this
kind of environment. On the other hand, the definition of a
semantic network as an abstract data type and its
implementation using the methodology of data abstraction
provide insights on the development of a new theory of abstract
data types and the opportunity for testing and refining that
theory. (Abstract shortened with permission of author.).
In many interactive systems which provide information, such as
HELP systems, the form and content of the information
</bodyText>
<page confidence="0.732995">
382 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987
</page>
<table confidence="0.979863277777778">
The FINITE STRING Newsletter Abstracts of Current Literature
of Individual User Characteristics
DAI V47(09), SecB, pp3858.
Cornelia Marie Yoder
Syracuse University Ph.D. 1986, 383
pages
Computer Science
University Microfilms International
ADG87-01283
JETR: A Robust Machine Translation
System
DAI V47(11), SecB, pp4586.
Rika Au Yoshii
University of California, Irvine Ph.D.
1986, 152 pages.
Computer Science
University Microfilms International
ADG87-03940
</table>
<bodyText confidence="0.99988001754386">
presented always seems to satisfy some people and frustrate
others. Human Factors textbooks and manuals for interactive
systems focus on the need for consistency and adherence to
some standard. This implicitly assumes that if the optimum
format and level of detail could be found for presenting
information to a user, interactive systems would only need to
adhere to the standard to be optimum for everyone. This
approach neglects one of the most important factors of all—
differences in people. If these individualizing differences in
people could be identified, a system could be designed with
options built into it to accommodate different users. The role of
the intelligent active system should be more like that of a
human expert or consultant, who answers questions by first
interpreting them in terms of the user&apos;s knowledge and the
context of his activities and then recommending actions which
may be otherwise unknown to the user.
The HELP system developed in this study is an Expert
System written in PROLOG which uses logic programming rules
to intelligently provide needed information to a terminal user. It
responds to a request with a full screen display containing
information determined by the request, the user&apos;s cognitive
style, and the user&apos;s experience level. The investigation studies
the relationship between some cognitive style and experience
level parameters and individual preferences and efficacy with an
interactive computer information system. These factors are
measured by the ability of an individual user to perform
unfamiliar tasks using a HELP function as information source.
The format of the information provided by the HELP function
is varied along three dimensions and the content of the
information is varied by three levels of detail.
Experiments were performed with the system and
experimental results are presented which show some trends
relating cognitive style and individual preferences and
performance using the system. In addition, it is argued that an
Expert System can perform such a function effectively.
This dissertation presents an expectation-based approach to
Japanese-to-English translation which deals with grammatical as
well as ungrammatical sentences and preserves the pragmatic,
semantic and syntactic information contained in the source text.
The approach is demonstrated by the JETR system, which is
composed of the particle-driven analyzer, the simultaneous
generator and the context analyzer. The particle-driven analyzer
uses the forward expectation-refinement process to handle
ungrammatical sentences in an elegant and efficient manner
without relying on the presence of particles and verbs in the
source text. To achieve extensibility and flexibility, ideas such
as the detachment of control structure from the word level, and
the combination of top-down and bottom-up processing have
been incorporated. The simultaneous generator preserves the
syntactic style of the source text without carrying syntactic
information in the internal representation of the text. No
source-language parse tree needs to be constructed for the
generator. The context analyzer is able to provide contextual
information to the other two components without fully
understanding the text. JETR operates without pre-editing and
post-editing, and without interacting with the user except in
special cases involving unknown words.
</bodyText>
<table confidence="0.943565047619048">
Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 383
The FINITE STRING Newsletter Abstracts of Current Literature
Cognitive Processes in Written
Translation
DAI V47(11), SecA, pp4008.
Mel Faleh Youssef
University of Houston Ed.D. 1986, 156
pages.
Education, Language and Literature.
University Microfilms International
ADG87-00047
Understanding Digital System
Specifications Written in Natural
Language
DAI V47(11), SecB, pp4609.
John Joseph Granacki, Jr.
University of Southern California Ph.D.
1986
Engineering, Electronics and Electrical
University Microfilms International
This item is not available: ADG05-59862.
</table>
<bodyText confidence="0.999949754385965">
Due to the fact that the purposes of translation are so diverse,
the texts are so different, and the receptors are so varied, a
person who is familiar with translation difficulties can readily
understand why many distinct principles of translation are
included in the translation paradigm. Writers who have written
on theories of translation agree that translators should know
both the source and the receptor languages, should be familiar
with the subject matter, and should have great facility of
expression in the receptor language.
The many views expressed on translation theory in the past
amount to a great deal of conflicting ideas. These ideas have
yet to coalesce into a coherent theory of translation. This
phenomena has led to a considerable methodological uncertainty
as to what particular paradigm of research should be followed.
The purpose of this study was to explore the distinct
principles and practices included in translation theory. Another
purpose was an establishment of a better understanding of how
the science of linguistics and human knowledge of language
structures can be utilized in the process of translating. At the
educational level, this study aimed at first, explaining the
translation strategies, second, indicating their pedagogical
implications.
Since the main purpose was to investigate theoretical
concepts included in the translation paradigm, no effort was
made towards formulating a complete coherent theory of
translation. Instead special attention was given to the cognitive
strategies that professional translators rely upon in the process
of translating. These cognitive strategies were operationalized in
a questionnaire format and eight professional translators were
asked to state their awareness of the cognitive strategies. Since
the cognitive strategies were initially derived from the
theoretical requirements that the scholars regard as basic factors
in any practical translation, it was hypothesized that the
subjects would express an agreement to the cognitive items
included in the questionnaire.
The significant aspects of the results obtained from this study
lie in the fact that the cognitive strategies on which the
theoreticians and the practitioners agreed can be utilized in (a)
teaching translation theory and practice, (b) can be used as
references in planning and designing a course in translation
methodology, and (c) can indirectly contribute to a better
understanding of the foreign language teaching and learning.
(Abstract shortened with permission of author.).
This thesis concerns itself with the specification of digital
systems. The specific focus of the work described here has
been on understanding system specifications written in natural
language. The long term goals of the research are to provide
methods and software to assure that the specifications are
consistent, correct, and complete.
The research described here differs from previous research in
several ways. First, the natural language input is used to
construct an internal design representation, rather than just to
query about existing design data. Second, using natural language
allows a generality of expression not found in formal models.
Finally, the natural language is not overly restricted.
A major part of the research described here involves formally
modeling the information found in system specifications. An
</bodyText>
<page confidence="0.969426">
384 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987
</page>
<note confidence="0.636199">
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<bodyText confidence="0.999745739130435">
extension of the USC Design Data Structure is described, with
emphasis on timing and control flow. Then, this extension is
used to model various concepts found in system specifications,
such as unidirectional value transfers and temporal constraints.
These models then provide a basis for the templates against
which input specifications are matched.
A semantic parser, PHRAN, is used as the basis for the
actual interface software. PHRAN contains a knowledge base of
sentence patterns along with associated concepts. PHRAN
inputs English sentences and looks for patterns in the
sentences. When it finds a pattern match, the concept
associated with the pattern is particularized with the information
found in the sentence.
After PHRAN has parsed the input, the SPAN (SPecification
ANalysis) package constructs fragments of the design data
structure described above, and informs the user what design
information has been found.
PHRAN-SPAN currently contains 13 concepts, 100+ nouns,
and 25 verbs. It can handle ambiguity, nouns used as modifiers,
and verbs used as nouns. It has processed a number of
sentences which come from actual specifications. (Copies
available exclusively from Micrographics Department, Doheny
Library, USC, Los Angeles, CA 90089-0182.)
</bodyText>
<figure confidence="0.618097555555556">
Constraints on Gaps in Coordinate
Structures
DAI V47(11), SecA, pp4076.
Carol B. Anderson
The Pennsylvania State University Ph.D.
1986, 244 pages.
Language, Linguistics
University Microfilms International
ADG87-053I9
</figure>
<bodyText confidence="0.900049333333333">
This thesis concerns the relationship between grammatical and
extra-grammatical constraints on the distribution of gaps in
coordinate structures. In a series of experiments subjects were
asked to rate the acceptability of sentences such as those in
(1), all of which contained relative clauses with coordinate
complements.
</bodyText>
<listItem confidence="0.97137925">
(1) a. That&apos;s the chair which Anne bought e and Harry threw e
out.
b. That&apos;s a movie which I liked e very much but e isn&apos;t
suitable for children.
c. That&apos;s the man who e tripped on a banana and everyone
laughed at e.
d. That&apos;s the boat which Harry bought e and everyone was
envious.
</listItem>
<bodyText confidence="0.97768006060606">
To determine the role of various factors in the acceptability
of (1), three non-structural factors — verb transitivity,
reversibility of semantic arguments of the verb and the choice
of conjunction (but vs. and) — were systematically varied with
four gap configurations: (1) symmetric (la) — where both gaps
are either matrix subjects, or non-matrix-subjects; (2)
asymmetric (lb) and (1c) — where one gap is a matrix subject
and the other is a non-matrix subject; and (3) CSC violations
(1d) — where there is a gap in only one conjunct.
The results indicated that speakers perceive a significant
three-way distinction among the symmetric (la), asymmetric
(1b)/(1c), and CSC violations (1d). The results also revealed that
the acceptability of sentences with adjacent gap configurations
(lc) was highly variable and that acceptability was significantly
affected by all three of the non-structural variables examined. In
addition, the results showed that in some cases, sentences with
asymmetric gap configurations are not judged to be significantly
different from similar sentences with symmetric configurations.
It is shown how these results do not support the predictions
of a number of existing analyses of Across-the-Board
Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 385
The FINITE STRING Newsletter Abstracts of Current Literature
dependencies which are based on structurally defined
grammatical constraints on coordination (e.g., Pesetsky 1982,
Goodall 1984, Gazdar et al. 1985).
An alternative analysis is proposed in which only CSC
violations are excluded by the grammar. The proposed analysis
assumes that the underlying structure for ATB dependencies
with relative clauses is different from that of other ATB
constructions such as constituent questions. It is shown how
the acceptability contrast between symmetric and asymmetric
configurations (e.g., (la) vs. (lb) and (1c)) can be accounted for
by extra-grammatical parsing principles.
</bodyText>
<figure confidence="0.916905">
Categories and Relations in Syntax:
The Clause-level Organization of
Information
DAI V47(09), SecA, pp3410.
William Albert Croft
Stanford University Ph.D. 1986, 380
pages.
Language, Linguistics
University Microfilms International
ADG87-00739
Lexical Cohesion and Text-as-percept
DAI V47(10), SecA, pp3749.
Joseph Sherril Mattingly
The University of Michigan Ph.D. 1986,
235 pages.
Language, Linguistics
University Microfilms International
ADG87-02789
</figure>
<bodyText confidence="0.999828227272727">
The major syntactic categories noun, verb and adjective and the
(nonspatial) grammatical relations holding between a main verb
and its dependent arguments represent the basic linguistic
structures at the level of a single clause. However, they have
resisted attempts to explain language structure in terms of
language function. This dissertation proposes functional
hypotheses which account for the basic linguistic structures in
terms of the organization of the information to be expressed for
the purpose of communication.
The empirical evidence consists of typological studies of the
structure and linguistic behavior of the major syntactic
categories and the casemarking of grammatical relations.
Extensions to the theory of markedness permit the analysis of
the major syntactic categories as NATURAL CORRELATIONS
of commonsense semantic classes and basic discourse functions
such as reference and predication. A commonsense model of
causality provides the basis for the analysis of verbal semantics,
subject and object, and a USAGE TYPE model of linguistic
semantic categories accounts for typological patterns found in
surface oblique case markers and voice types in terms of the
causal model.
The typological evidence argues for the organization of a
grammar into complex patterns of the applicability of rules
rather than distinct autonomous syntactic levels, and for
realization rules linking structure and function rather than
derivational rules relating two structures by a purely structural
operation. However, the realization rules cannot be completely
direct due to typological variation in the linguistic expression of
given situations.
This is a study of lexical cohesion — cohesive relationships
among meaning units — perceived in English basic writing
texts. Lexical cohesion is here considered to be the interfacing
of the reader with the meaning units of the text, and is thus
approached as &amp;quot;process.&amp;quot;
The study argues that we perceive cohesive ties among
specific lexical items to be meaningful principally within the
context of a greater wholeness of meaning. Several levels of
semantic and lexical wholeness are identified within which
cohesive specificities are interpreted, including generic semantic
categories of &amp;quot;problemness&amp;quot; and &amp;quot;solutionness&amp;quot; in the texts,
within which all specific reiterative and collocational ties may
be interpreted.
The entire text is reinterpreted as a percept — a
diagrammatic visualization. Lexical cohesive features are given
</bodyText>
<page confidence="0.94727">
386 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987
</page>
<note confidence="0.737844">
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<bodyText confidence="0.997514772727273">
spatial dimensions. The text-as-percept is a spatial metaphor
through which we visually think about lexical and semantic
complexities and simultaneities of cohesion. Correspondences
between visual structures of the text-as-percept and linguistic
features of lexical cohesion are considered non-arbitrary. Lexical
redundancy is associated with verticality, lexical contrast with
horizontality.
The study comes to conclusions concerning linguistic features
of reiteration, collocation, lexical item, and the &amp;quot;textual
history&amp;quot; of a lexical item. Reiterative and collocational
relationships are of two types: textual and prior-textual. Textual
cohesive relationships derive from the whole text unit in which
they occur, and are interpretable within the generic semantic
categories of that text. Prior-textual cohesive relationships occur
within independent semantic sub-units whose semantic integrity
derives from outside the text. The &amp;quot;lexical item&amp;quot; is considered
to be a discoursal, clausal, and phrasal unit, as well as a
single-word item. The &amp;quot;textual history&amp;quot; of an item is viewed as
its cohesive behavior as a member of a unit in a larger system
of lexical patterning in the text. An item&apos;s textual history
influences its cohesion-forming potential in a subsequent,
&amp;quot;smaller&amp;quot; level of lexical patterning in the same text.
</bodyText>
<table confidence="0.509669444444444">
The Formal Semantics of Point of
View
DAI V47(09), SecA, pp3414.
Jonathan Edward Mitchell
University of Massachusetts Ph.D. 1986,
187 pages.
Language, Linguistics
University Microfilms International
ADG87-01201
</table>
<bodyText confidence="0.972085578947369">
It has long been noted that propositional attitudes often involve
a special mode of reference to oneself. The sentence (i) &amp;quot;John
thinks he won the raffle&amp;quot; implies something different from (ii)
&amp;quot;John thinks that the person who holds ticket number 43 won
the raffle&amp;quot;, even if John is in fact the holder of that ticket. If
John has forgotten his number, (ii) might hold without (i)
holding. Sentence (i) implies that John thinks &amp;quot;I myself won,&amp;quot;
and the &amp;quot;first person&amp;quot; or &amp;quot;self-ascriptive&amp;quot; quality of such
propositional attitudes is not captured in most model theoretic
approaches to semantics. It has also long been noted that many
property and relation expressions are used with implicit or
surpressed argument positions, and that such expressions are
interpreted perspectivally, i.e. relative to a point of view. These
two phenomena interact, with the result that self-ascription is
more pervasive in the language than is usually recognized. The
sentence (iii) &amp;quot;John thinks that restaurant is around the corner&amp;quot;
has a prominent interpretation according to which John thinks
— in the special self-ascriptive way — &amp;quot;the restaurant is
around the corner from here (from where I am now)&amp;quot;.
In the first chapter, self-ascription and perspectivity are
explored, with attention to the ambiguities that arise from them.
In the second chapter, an analysis of self-ascription is
developed within the framework of situational semantics. It is
proposed that there be assignments in parallel of two
propositional contents to every propositional attitude. This
elaboration in the semantics is shown to account for a wide
range of complex cases, and to work recursively for multiply
embedded propositional attitudes. Here an argument is offered
that the restrictions on pronoun interpretation are grounded in
variable binding and complex property formation rather than
sameness and difference of reference. The third chapter explores
the polyadicity issues raised by implicit arguments and point of
view. An important claim of this section is that thematic
relations should be incorporated into a formal semantics
Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 387
The FINITE STRING Newsletter Abstracts of Current Literature
approach. Finally, the fourth chapter brings all the proposals
together into a formal fragment.
</bodyText>
<table confidence="0.988783411764706">
Quantification in Syntax
DAI V47(09), SecA, pp3415.
Taisuke Nishigauchi
University of Massachusetts Ph.D. 1986,
290 pages.
Language, Linguistics
University Microfilms International
ADG87-01206
Sentence Processing and The Mental
Representation of Verbs
DAI V47(11), SecA, pp4076.
Lewis Philip Shapiro
Brandeis University Ph.D. 1987, 102
pages.
Language, Linguistics, Psychology,
Experimental
University Microfilms International
</table>
<page confidence="0.577316">
ADG87-05786
</page>
<bodyText confidence="0.999916288461539">
The main concern of the present thesis is the nature of
constructions which involve WH-phrases in natural language,
mostly in Japanese and English. We will address the two
questions about this type of construction: (i) What is the nature
of the locality principle that governs the syntax and semantic
interpretation of constructions involving WH-phrases? (ii) What
is the quantificational force of the WH-phrase?
We will develop an analysis of WH-constructions in
Japanese, where it is argued that a WH-phrase that occurs in
an A-position in S-structure is moved to an A&apos;-position in LF.
We will discuss some superficial asymmetry that LF movement
in Japanese exhibits with respect to the effects of the locality
condition of Subjacency. While it obeys the WH-Island
Condition effect, it appears to be free from the Complex NP
Constraint (CNPC) effect: sentences which contain a WH-phrase
within a complex NP at S-structure are generally grammatical.
We claim in chapter 2 that the WH-phrase in the problematic
sentences does not move out of the complex NP, but it moves
only within the relative clause. This triggers movement of the
entire complex NP to the operator position of the main clause,
thus no violation of the CNPC is involved. Chapter 3 will
discuss the theoretical apparatus which substantiates the
proposal in chapter 2.
As for the second question, we will discuss some sentences
in Japanese and English where WH-phrases can be understood
as behaving as the universal quantifier.
We argue, in chapters 4 and 5, that WH-phrases are devoid
of semantic content and should be treated as &apos;variables&apos; in the
logical representation. The quantificational force of the
WH-phrase is determined by a certain class of quantificational
elements under certain structural conditions that hold with the
WH-phrase that has undergone movement at LF.
Chapters 5 and 6 discuss problems raised by pronominal
coindexing that holds between a pronoun and a WH-phrase with
reference to the restrictions on Indirect Binding.
This study seeks to determine the aspects of verb
representations that are relevant to sentence processing, and to
determine the operating characteristics of the devices that
access information pertaining to verbs during the course of
sentence comprehension.
The starting point for this inquiry involves two
complimentary accounts of verb representation in the lexicon —
syntactic subcategorization and argument structure. These
accounts are used to address the relation between
representational and processing complexity. Verb categories are
produced that diverge with respect to syntactic subcategorization
complexity and argument structure complexity. Several
experiments are then run using a cross-model lexical decision
task (CMLD) that taps into the sentence comprehension system
in the immediate temporal vicinity of the verb. ,
The data generated by these experiments are used to make
the following points:
</bodyText>
<footnote confidence="0.500879">
1. The relevant processing complexity metric for verbs must be
</footnote>
<page confidence="0.895597">
388 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987
</page>
<note confidence="0.700731">
The FINITE STRING Newsletter Abstracts of Current Literature
</note>
<bodyText confidence="0.704895125">
defined in terms of the number of different argument
structure arrangements a verb enters into, and not the
number of syntactic configurations a verb can dominate as
previously hypothesized in the literature;
2. Evidence from morphology and the data obtained in this
study show that verbs are organized in the lexicon by their
argument structure, and this is presumably why the lexical
access device is tuned to argument structure information, and
not syntactic subcategorization;
3. All argument structure entries are fair game for lexical
access. They are exhaustively accessed and made available
for further operations of the sentence processor, like parsing;
and
4. Verb access is data-driven, modular, and contextually
impenetrable: The access device is oblivious to the structural
information contained in the sentence prior to the verb.
</bodyText>
<figure confidence="0.689694875">
The Semantics of Destructive LISP
DAI V47(09), SecA, pp3449.
Ian Alistair Mason
Stanford University Ph.D. 1986, 292
pages.
Philosophy, Computer Science
University Microfilms International
ADG87-00788
An Analysis of Searle&apos;s Theory of The
Intentionality of Speech Acts
DAI V47(09), SecA, pp3450.
Shashi Motilal
State University of New York at Buffalo
Ph.D. 1986, 159 pages.
Philosophy
University Microfilms International
</figure>
<page confidence="0.958316">
ADG86-29096
</page>
<bodyText confidence="0.954956157407407">
In this thesis we investigate various equivalence relations
between expressions in first order LISP. This fragment of LISP
includes the destructive operations rplaca and rplacd. To define
the semantics we introduce the notion of a memory structure.
The equivalence relations are then defined within this model
theoretic framework.
A distinction is made between intensional relations and
extensional relations. The former class turned out to have a
much more managable theory than the latter. The principle
intensional relation studied is strong isomorphism, its properties
allow for elegant verification proofs in a style similar to that of
pure Lisp. In particular the relation is preserved under many
standard syntactic manipulations and transformations. In
particular it satisfied a Substitution theorem; any program that
is obtained from another by replacing a portion by another
strongly isomorphic one is guaranteed to be strongly isomorphic
to the original one.
A plethora of verification proofs of both simple and complex
programs was given using the intensional equivalence relation.
All of these proofs were of the transformation plus induction
variety. In contrast, we gave some verification proofs of
programs, using the extensional relations. Because the
Substitution Theorem fails for these extensional relations, the
proofs were necessarily of the hand simulation variety.
In a more theoretical light, we also proved that the
equivalence relations introduced here are decidable, and used
them to study the expressive powers of certain fragments of
Lisp.
It is an indubitable fact that our thoughts are always about
something or some state of affairs in the world. Again, it is
true that we use language to express some of our thoughts, and
that in such a use of language which philosophers call a speech
act, language also comes to be about something or some state
of affairs in the world. E.g., when someone asserts that Peter
is married to Mary, the sentence, &apos;Peter is married to Mary&apos;,
comes to be about the state of affairs of Peter&apos;s being married
to Mary. This property of being about something which
characterizes our thoughts and speech acts is called
&amp;quot;intentionality&amp;quot; by philosophers.
In Intentionality, John Searle claims that the Intentionality of
Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 389
The FINITE STRING Newsletter Abstracts of Current Literature
language and the Intentionality of speech acts performed by
using language is derived from the intrinsic Intentionality of
mental states which accompany the speech act. In the
dissertation I propose to examine this claim of Searle&apos;s.
Searle&apos;s view regarding the Intentionality of speech acts is
incomplete and wrong. Searle does not show how the
Intentionality of referring and predicating in a speech act is to
be derived from mental reference and mental predication in the
corresponding mental state. In this sense his theory is
incomplete. Further, his theory is false. Taking the case of
belief, I propose to show that it cannot be the source of the
Intentionality of the corresponding assertion. Either a belief is a
disposition and does not have any Intentionality or it is a
mental act of accepting a proposition in which case the
Intentionality of the belief and the Intentionality of the assertion
have the same linguistic nature and the former cannot be the
source of the latter. We are faced with a dilemma which shows
that Searle&apos;s view cannot be correct. In the dissertation it will
be argued that the Intentionality of a speech act is as intrinsic
to it as the Intentionality of a mental phenomenon is to the
mental phenomenon. The nature of speech act referring and
predicating, mental referring and predicating, linguistic concepts
and linguistic acts using those concepts will be discussed.
Grammar and Information: An
Investigation in Linguistic Metatheory
DAI V47(10), SecA, pp3774.
Thomas Alan Ryckman
Columbia University Ph.D. 1986, 452
pages.
Philosophy
University Microfilms International
ADG87-03076
This work examines the foundations of linguistic theory, with
particular reference to the status and justification of grammars
and theories of language structure. A metatheoretical critique of
generative grammar is followed by epistemological motivation
for, and presentation of, an alternative conception of language
structure due to Harris, together with an approach to its
justification. It is proposed that grammars have an informational
validity as structures comprised of maximally unredundant
equivalence classes whose members all demonstrably &apos;say the
same&apos; over a specified domain. Chapter 1 introduces the issues
involved and summarizes the argument. In Chapter 2, the
metatheoretical situation in American structural linguistics prior
to the advent of generative grammar is reviewed. Among the
findings are that familiar attributions to the central figures of
this period of a goal of providing &amp;quot;mechanical discovery
procedures&amp;quot; for grammars or of attempting to specify linguistic
form without regard to meaning are groundless. Chapter 3
situates the first major work of generative grammar in respect
to Quine&apos;s contemporary proposals for linguistic theory and
finds wanting arguments advanced that linguistic form can be
identified independently of considerations of meaning. Chapter 4
charts the evolution of generative grammar from a &amp;quot;formal
systems&amp;quot; view of grammars and languages to recent proposals
concerning &amp;quot;core grammar&amp;quot; and &amp;quot;markedness&amp;quot;. It is argued
that in pursuit of &amp;quot;explanatory adequacy&amp;quot; generative grammar
has increasingly been distanced from the control of empirical
evaluation, and that claims seeking a biological locus for
abstract grammatical properties are unsubstantiated and
problematic in themselves. Chapter 5 provides an
epistemological critique of &apos;information&apos; as employed by recent
writers. Grammars — as unredundant characterizations of
combinations of linguistic elements that can occur — are
characterizations of information. Chapter 6 refers to a prior
study presenting a &apos;grammar&apos; of a sublanguage of cellular
</bodyText>
<page confidence="0.912242">
390 Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987
</page>
<bodyText confidence="0.923992666666667">
The FINITE STRING Newsletter Abstracts of Current Literature
immunology. It is shown both how and that a theory of
language structure can be empirically validated by enabling the
construction of formulas of information that compactly
summarize and trace the findings and discussions in research
reports of a science.
</bodyText>
<figure confidence="0.984629875">
Reference and Intentionality
DAI V47(11), SecA, pp4104.
Nathan Tawil
Princeton University Ph.D. 1987, 124
pages.
Philosophy
University Microfilms International
ADG87-05016
</figure>
<bodyText confidence="0.975668142857143">
This thesis is a consideration of some issues that arise in
connection with theories of meaning developed along the lines
suggested by H. P. Grice. I address three questions which, left
unanswered, diminish the appeal of Grice&apos;s program. First, what
should the Gricean say about word reference (as opposed to
sentence meaning)? I argue in Chapter I that we can explain
the role of word reference in sentence meaning only if we
recognize, among the linguistic conventions observed by a
community of speakers, a conventional compositional semantics
for representing their language. Next, what should the Gricean
say about understanding (as opposed to meaning)? Chapter II
suggests a view of language-mastery that requires of speakers
considerably less knowledge about the referents of expressions,
particularly of predicates, than borne contemporary accounts.
Finally, what should the Gricean say to skeptical arguments
against the possibility of meaning or intentionality, of the sort
recently advanced by Hilary Putnam and Saul Kripke? In
chapter III, I argue that semantic theory has available at least
three defensible responses to such arguments:
I. The defender of semantics can accept the skeptical
conclusion and still talk about meaning
2. Espousing a theory according to which the content of mental
states is determined in part by features of the world not
intrinsic to the bearers of those states allows us to rebut the
skeptical arguments successfully
3. The believer in meaning can plausibly maintain that facts
about meaning or intentionality are primitive and
unanalyzable.
</bodyText>
<page confidence="0.274167">
Computational Linguistics, Volume 13, Numbers 3-4, July-December 1987 391
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.137687">
<title confidence="0.566227333333333">ABSTRACTS OF CURRENT LITERATURE If you are interested in ordering any of the following University of Waterloo Computer Science Department reports, please forward your order to:</title>
<author confidence="0.468992">Sue DeAngelis</author>
<affiliation confidence="0.972751666666667">Research Report Secretary University of Waterloo Computer Science Dept.</affiliation>
<address confidence="0.920358">Waterloo, ON CAN N2L 3G1</address>
<title confidence="0.5896075">An Implementation of a Computational Model for the Analysis of Arguments</title>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>