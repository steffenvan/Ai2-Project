<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000007">
<title confidence="0.988375">
Adding Semantics to Data-Driven Paraphrasing
</title>
<author confidence="0.998484">
Ellie Pavlick1 Johan Bos2 Malvina Nissim2 Charley Beller3 Benjamin Van Durme4 Chris Callison-Burch1
</author>
<affiliation confidence="0.9550815">
1Computer and Information Science Department, University of Pennsylvania
2Center for Language and Cognition Groningen, University of Groningen
4Human Language Technology Center of Excellence, Johns Hopkins University
3IBM Watson Group
</affiliation>
<sectionHeader confidence="0.97339" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999959277777778">
We add an interpretable semantics to
the paraphrase database (PPDB). To date,
the relationship between phrase pairs
in the database has been weakly de-
fined as approximately equivalent. We
show that these pairs represent a vari-
ety of relations, including directed entail-
ment (little girl/girl) and exclusion (no-
body/someone). We automatically assign
semantic entailment relations to entries in
PPDB using features derived from past
work on discovering inference rules from
text and semantic taxonomy induction. We
demonstrate that our model assigns these
relations with high accuracy. In a down-
stream RTE task, our labels rival relations
from WordNet and improve the coverage
of a proof-based RTE system by 17%.
</bodyText>
<sectionHeader confidence="0.990017" genericHeader="keywords">
1 Motivation
</sectionHeader>
<bodyText confidence="0.997170368421053">
A basic precursor to language understanding is the
ability to recognize when two expressions mean
the same thing. Different expressions of the same
information is the central problem addressed by
paraphrasing and the closely related task of rec-
ognizing textual entailment (RTE). In RTE, a sys-
tem is given two pieces of text, often called the
text (T) and the hypothesis (H), and asked to de-
termine whether T entails H, T contradicts H, or
T and H are unrelatable (Figure 1). In contrast,
data-driving paraphrasing typically sidesteps de-
veloping a clear definition of “meaning the same
thing” and instead “assume[s] paraphrasing is a
coherent notion and concentrate[s] on devices that
can produce paraphrases” (Barzilay, 2003). Re-
cent work on paraphrase extraction has resulted
in enormous paraphrase collections (Lin and Pan-
tel, 2001; Dolan et al., 2004; Ganitkevitch et
al., 2013), but the usefulness of these collections
</bodyText>
<figureCaption confidence="0.646131166666667">
Figure 1: An example sentence pair for the RTE task. In order
for a system to conclude that the premise (top) does not entail
the hypothesis (bottom), it should recognize that sparked im-
plies caused but that in Denmark precludes in Jordan. These
phrase-level entailment relationships are modeled by natural
logic.
</figureCaption>
<bodyText confidence="0.976110956521739">
is limited by the fast-and-loose treatment of the
meaning of paraphrases. One concrete defini-
tion that is sometimes used for paraphrases re-
quires that they be bidirectionally entailing (An-
droutsopoulos and Malakasiotis, 2010). That is,
in terms of RTE, it is assumed that if P is a para-
phrase of Q, then P entails Q and Q entails P. In
reality, paraphrases are often more nuanced (Bha-
gat and Hovy, 2013), and the entries in most para-
phrase resources certainly do not match this def-
inition. For instance, Lin and Pantel (2001) ex-
tracted 12 million “inference rules” from mono-
lingual text by exploiting shared dependency con-
texts. Their method learns paraphrases that are
truly meaning equivalent, but it just as readily
learns contradictory pairs such as hX rises, Xfallsi.
Ganitkevitch et al. (2013) extract over 150 mil-
lion paraphrase rules by pivoting through foreign
translations. This bilingual method often learns
hypernym/hyponym pairs, e.g. due to variation
in the discourse structure of translations (Callison-
Riots in Denmark were sparked by 12 editorial
cartoons that were offensive to Muhammad.
</bodyText>
<figure confidence="0.9901207">
12 - Twelve
editorial cartoons illustrations
offensive insulting
Muhammad - the prophet
sparked caused
riots unrest
in Denmark
Twelve illustrations insulting the prophet
caused unrest in Jordan.
in Jordan
</figure>
<page confidence="0.949086">
1512
</page>
<note confidence="0.983312">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1512–1522,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figure confidence="0.734923636363636">
Equivalent Entailment Exclusion Other relation Unrelated
look at/watch little girl/girl close/open swim/water girl/play
a person/someone kuwait/country minimal/significant husband/marry to found/party
clean/cleanse tower/building boy/young girl oil/oil price profit/year
away/out the cia/agency nobody/someone country/patriotic man/talk
distant/remote sneaker/footwear blue/green drive/vehicle car/family
the phone/the telephone heroin/drug france/germany family/home holiday/series
last autumn/last fall doe/deer least three/least two basketball/court green/tennis
illegal entry/smuggling typhoon/storm child/mother playing/toy sunday/tour
approve/to ratify seriously injure/injure in front/on the side islamic/jihad city/south
alliance of/coalition between sunglasses/glasses oppose/support delay/time back/view
</figure>
<tableCaption confidence="0.995188">
Table 1: Examples of different types of entailment relations appearing in PPDB.
</tableCaption>
<bodyText confidence="0.999496636363636">
Burch, 2007), and unrelated pairs, e.g. due to mis-
alignments or polysemy in the foreign language.
The unclear semantics severely limits the ap-
plicability of paraphrase resources to natural lan-
guage understanding (NLU) tasks. Some efforts
have been made to identify directionality of para-
phrases (Bhagat et al., 2007; Kotlerman et al.,
2010), but tasks like RTE require even richer se-
mantic information. For example, in the T/H pair
shown in Figure 1, a system needs information
not only about equivalent words (12/twelve) and
asymmetric entailments (riots/unrest), but also se-
mantic exclusion (Denmark/Jordan). Such lexical
entailment relations are captured by natural logic,
a formalism which views natural language itself
as a meaning representation, eschewing external
representations such as First Order Logic (FOL).
This is a great fit for automatically extracted para-
phrases, since the phrase pairs themselves can be
used as the semantic representation with minimal
additional annotation. But as is, paraphrase re-
sources lack such annotation.
As a result, NLU systems rely on manually built
resources like WordNet, which are limited in cov-
erage and often lead to incorrect inferences (Ka-
plan and Schubert, 2001). In fact, in the most
recent RTE challenge, over half of the submitted
systems used WordNet (Pontiki et al., 2014). Even
the NatLog system (MacCartney and Manning,
2007), which popularized natural logic for RTE,
relied on WordNet and did not solve the problem
of assigning natural logic relations at scale.
The main contributions of this paper are:
</bodyText>
<listItem confidence="0.995164444444445">
• We add a concrete, interpretable semantics
to the Paraphrase Database (PPDB) (Ganitke-
vitch et al., 2013), the largest paraphrase re-
source currently available. We give each en-
try in the database a label describing the en-
tailment relationship between the phrases.
• We develop a statistical model to predict
these relations. The enormous size of PPDB–
over 77 million phrase pairs!– makes it im-
possible to perform this task manually. Our
wide range of monolingual and bilingual fea-
tures results in high intrinsic accuracy.
• We demonstrate improvements to a proof-
based RTE system, showing that our auto-
matic labels increase the number of proofs
that it is able to find by 17%, while maintain-
ing the same accuracy as when using gold-
standard, manual labels.
</listItem>
<sectionHeader confidence="0.998868" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.996624961538462">
Lexical entailment resources Approaches to
paraphrase identification have exploited signal
from distributional contexts (Lin and Pantel, 2001;
Szpektor et al., 2004), comparable corpora (Dolan
et al., 2004; Xu et al., 2014), and graph structures
(Berant et al., 2011; Brockett et al., 2013). These
approaches are scalable, but they often assume that
all relations are equivalence relations (Madnani
and Dorr, 2010). Several efforts have attempted
to build or augment lexical ontologies automati-
cally, to discover other types of lexical relations
like hypernyms. Most of these approaches rely on
lexico-syntactic patterns. Hearst (1992) searched
for hand-written patterns (e.g. “an X is a Y”) in a
large corpus in order to learn taxonomic relations
between nouns. Snow et al. (2006) used depen-
dency parses to automatically learn such patterns,
which they used to augment WordNet with new
hypernym relations. Similar monolingual signals
have been used to learn fine-grained relationships
between verbs, such as enablement and happens-
before (Chklovski and Pantel, 2004; Hashimoto et
al., 2009).
Recognizing Textual Entailment The shared
RTE tasks (Dagan et al., 2006) have been a spring-
board for research in natural language inference,
</bodyText>
<page confidence="0.991369">
1513
</page>
<figureCaption confidence="0.994953">
Figure 2: Distribution of entailment relations in different sizes of PPDB. Distributions are estimated from our manual annota-
tions of randomly sampled pairs. PPDB-XXXL contains over 77MM paraphrase pairs (where the majority type is independent),
compared to only 700K in PPDB-S (where the majority type is equivalent).
</figureCaption>
<bodyText confidence="0.999927772727273">
using data motivated by the applications to infor-
mation retrieval, information extraction, summa-
rization, machine translation evaluation, and more
recently, question answering (Giampiccolo et al.,
2007) and essay grading (Clark et al., 2013). RTE
systems vary considerably in their choice of rep-
resentation and inference procedure. In the most
recent shared task on RTE, some systems used
deep logical representations of text, allowing them
to invoke theorem provers (Bjerva et al., 2014)
or Markov Logic Networks (Beltagy et al., 2014)
to perform the inference, while others used shal-
lower representations, relying on machine learn-
ing to perform inference (Lai and Hockenmaier,
2014; Zhao et al., 2014). Systems based on natural
logic (MacCartney and Manning, 2007) use natu-
ral language as a representation, but still perform
inference using a structured algebra rather than a
statistical model. Regardless of the inference pro-
cedure, improvements to external lexical resources
can improve RTE systems across the board (Clark
et al., 2007).
</bodyText>
<sectionHeader confidence="0.986497" genericHeader="method">
3 The Paraphrase Database (PPDB)
</sectionHeader>
<bodyText confidence="0.9783821">
PPDB is currently the largest available collection
of paraphrases. Compared to other paraphrase
resources such as the DIRT database (12 mil-
lion rules) (Lin and Pantel, 2001) and the MSR
paraphrase phrase table (13 million) (Dolan et
al., 2004), PPDB contains over 150 million para-
phrase rules covering three paraphrase types– lex-
ical (single word), phrasal (multiword), and syn-
tactic restructuring rules. We focus on lexical and
phrasal paraphrases, of which there are over 77
million rules. Of these, a large fraction are true
paraphrases– either equivalent (distant/remote) or
asymmetric entailment (girl/little girl)– but many
are not. PPDB contains some pairs which are
related by semantic exclusion (nobody/someone),
some of which are related by something other than
entailment (swim/water), and some which are sim-
ply unrelated (car/family). Table 1 gives examples
of pairs in PPDB falling into each of these cate-
gories.
PPDB is released in six sizes (S, M, L, XL,
XXL and XXXL), which fall roughly on a con-
tinuum from highest precision and lowest recall to
lowest average precision and highest recall. Fig-
ure 2 shows how the distribution of entailment re-
lations differs across the sizes of PPDB.1 Our goal
is to make these relations explicit, by providing
annotations for each phrase pair. Because of the
enormous scale of PPDB, this annotation must be
done automatically.
</bodyText>
<subsectionHeader confidence="0.735277">
4 Selection of Paraphrases
</subsectionHeader>
<bodyText confidence="0.998243416666667">
In this paper we focus on paraphrases pairs from
PPDB that occur in RTE data. We use the recent
SICK dataset from in the 2014 SemEval RTE chal-
lenge (Marelli et al., 2014) for our experiments.
The data consists of 10K sentences split roughly
evenly into training and testing sets. The sen-
tence pairs are labeled using a 3-way entailment
classification: ENTAILMENT, (29%) CONTRADIC-
TION (15%), or NEUTRAL (56%). We consider
all phrase pairs from PPDB (p1, p2) up to three
words in length such that there is some T/H sen-
tence pair in which p1 appears in T and p2 appears
</bodyText>
<footnote confidence="0.730706333333333">
1These distributions were estimated based on a random
sample of pairs drawn from each size of PPDB, annotated on
MTurk as described in Section 5
</footnote>
<page confidence="0.98695">
1514
</page>
<bodyText confidence="0.997623625">
Lexical We use the lemmas, POS tags, and phrase lengths of P1 and P2, the substrings shared by P1 and P2,
and the Levenstein, Jaccard, and Hamming distances between P1 and P2.
Distributional Given a dependency context vectors for P1 and P2, we compute the number of shared contexts, and
the Jaccard, Cosine, Lin1998, Weeds2004, Clarke2009, and Szpektor2008 similarities between the
vectors.
Paraphrase We include 33 paraphrase features distributed with PPDB, which include the paraphrase probabilities
as computed in Bannard and Callison-Burch (2005). We refer the reader to Ganitkevitch and Callison-
Burch (2014) for a complete description of all of the features included with PPDB.
Translation We include the number of foreign language “pivots” (translations) shared by P1 and P2 for each of 24
languages used in the construction of PPDB, as a fraction of the total number of translations observed
for each of P1 and P2.
Path We include a sparse vector of all lexico-syntactic patterns (paths through a dependency parse) which
are observed between P1 and P2 in the Annotated Gigaword corpus (Napoles et al., 2012).
WordNet We include binary features indicating whether WordNet classifies P1 and P2 according to any of the
following relations: synonym, hypernym, hyponym, antonym, holonym, meronym, cause, entailment,
derivationally-related, similar-to, also-see, or attribute.
</bodyText>
<figureCaption confidence="0.998206">
Figure 3: Summary of features extracted for each phrase pair hP1, P2i. Full descriptions of the features used are given in the
supplementary material.
</figureCaption>
<bodyText confidence="0.999747470588235">
in H. Roughly 55% of the word types and 5% of
the phrase (bigram and trigram) types in the SICK
data appear in PPDB. This gives us a list of 9,600
pairs, half from the training sentences, which we
use for development in Section 6, and half from
the test sentences, which we use for evaluation in
Section 7.
The SICK data has a relatively small vocabu-
lary, with 86% of words types and &lt;1% of the
phrase types covered by WordNet. Still, over half
of the words in SICK which are covered by PPDB
do not appear in WordNet. In general, PPDB cov-
ers a much larger vocabulary (1.6MM words) than
does WordNet (155K words), and we expect the
potential benefit of using PPDB in addition to or
in place of WordNet to be larger on datasets with
richer vocabularies.
</bodyText>
<sectionHeader confidence="0.992414" genericHeader="method">
5 Entailment Relations
</sectionHeader>
<bodyText confidence="0.9998378">
We use the relations from Bill MacCartney’s
thesis on natural language inference as the basis
for our categorization of relations (MacCartney,
2009). He outlines 7 basic entailment relation-
ships:2
</bodyText>
<listItem confidence="0.984569428571429">
Equivalence (P=Q): Vx[P(x) H Q(x)]
Forward Entailment (P@Q): Vx[P(x) Q(x)]
Reverse Entailment (PAQ): Vx[Q(x) P(x)]
Negation (PˆQ): Vx [P(x) H -, Q(x)]
Alternation (P|Q): Vx -,[P(x) A Q(x)]
Cover (P—Q): Vx[P(x) V Q(x)]
Independence (P#Q): All other cases.
</listItem>
<footnote confidence="0.436014333333333">
2To further clarify the definitions here: “negation” is XOR
(exclusive disjunction), “alternation” is NAND, and “cover”
is OR (inclusive disjunction)
</footnote>
<bodyText confidence="0.999330571428571">
These relations are based on the theory of natu-
ral logic, meaning they are defined between pairs
of natural language expressions rather than requir-
ing an external formal representation. This makes
them an ideal fit for the phrase pairs in in PPDB
and similar automatically-constructed paraphrase
resources.
</bodyText>
<figure confidence="0.7610112">
Nat. This MTurk description
Log. work
⌘ ⌘ X is the same as Y
@ @ X is more specific than/is a type of Y
A A X is more general than/encompasses Y
</figure>
<equation confidence="0.78226425">
&amp;quot; � X is the opposite of Y
� X is mutually exclusive with Y
# ⇠ X is related in some other way to Y
# X is not related to Y
</equation>
<tableCaption confidence="0.81302325">
Table 2: Column 1 gives the semantics of each label under
MacCartney’s Natural Logic. Column 2 gives the notation
we use throughout the remainder of this paper. Column 3
gives the description that was shown to Turkers.
</tableCaption>
<bodyText confidence="0.9999305">
Annotation We use Amazon Mechanical Turk
(MTurk) to collect labels for our phrase pairs. We
asked workers to choose between the options show
in Table 2, which represent a modified version
of MacCartney’s relations. We replace negation
(&amp;quot;) with the weaker notion of “opposites,” effec-
tively merging it with the alternation (|) relation;
we split the independent (#) class into two cases:
truly independent phrases and phrases which are
related by something other than entailment (which
we denote —). We omit the cover (—) relation en-
tirely, as its practicality is not obvious. We show
each pair to 5 workers, taking the majority label as
truth. Each HIT consisted of two control questions
taken from WordNet. Workers achieved good ac-
curacies on our controls (82% overall) and moder-
</bodyText>
<page confidence="0.94546">
1515
</page>
<figure confidence="0.980526333333333">
Cosine Similarity Monolingual (symmetric) Monolingual (asymmetric) Bilingual
A shades/the shade large/small A boy/little boy ⌘ dad/father
A yard/backyard ⌘ few/several A man/two men A some kid/child
# each other/man different/same A child/three children ⌘ a lot of/many
A picture/drawing other/same ⌘ is playing/play ⌘ female/woman
⇠ practice/target put/take A side/both sides ⌘ male/man
</figure>
<tableCaption confidence="0.960486666666667">
Table 3: Top scoring pairs (x/y) according to various similarity measures, along with their manually classified entailment
labels. Column 1 is cosine similarity based on dependency contexts. Column 2 is based on Lin (1998), column 3 on Weeds
(2004), and column 4 is a novel feature. Precise definitions of each metric are given in the supplementary material.
</tableCaption>
<bodyText confidence="0.996979">
ate levels of agreement (Fleiss’s n = 0.56) (Landis
and Koch, 1977). For a fuller discussion of the
annotation, refer to the supplementary material.
</bodyText>
<sectionHeader confidence="0.989727" genericHeader="method">
6 Automatic Classification
</sectionHeader>
<bodyText confidence="0.9999494375">
We aim to build a classifier to automatically assign
entailment types to entries in the PPDB, and to
demonstrate that it performs well both intrinsically
and extrinsically. We fix the direction of the @ and
A relations to create a single class and train a lo-
gistic regression classifier to distinguish between
the 5 classes {#, -, A, -,, ⇠1. We compute vari-
ety of basic lexical features and WordNet features
(summarized in Figure 3). We categorize the re-
maining features into two broad groups: monolin-
gual features, which are based on observed usage
in the Annotated Gigaword corpus (Napoles et al.,
2012), and bilingual features, which are based on
translation probabilities observed in bilingual par-
allel corpora. Full descriptions of all the features
used are provided in the supplementary material.
</bodyText>
<subsectionHeader confidence="0.992898">
6.1 Monolingual features
</subsectionHeader>
<bodyText confidence="0.999884052631579">
Path features Snow et al. (2004) used lexico-
syntactic patterns to mine taxonomic relations
(hypernyms and hyponyms) between noun pairs.
They were able to verify the earlier work of Hearst
(1992) which found that certain patterns, e.g. X
and other Y, are strong indicators of hypernymy.
Using similar path features, we learn new patterns
to differentiate between more subtle relations. For
example, we learn the pattern separate X from Y is
highly indicative of the -, relation. We learn that
the pattern X including Y suggests A more than it
suggests - whereas the pattern X known as Y sug-
gests - more than A. Table 4 gives examples of
some of the paths most indicative of the -, relation.
Distributional features Lin and Pantel (2001)
attempted to mine inference rules from text by
finding paths in a dependency tree which connect
the same nouns. The intuition is that good para-
phrases should tend to modify and be modified by
</bodyText>
<equation confidence="0.6067224">
in X and in Y in foods and in beverages
separate X from Y separate the old from the young
to X and/or to Y to the left or to the right
from X to Y from 7 a.m. to 10 p.m.
more/less X than Y more harm than good
</equation>
<tableCaption confidence="0.990844">
Table 4: Top paths associated with the - class.
</tableCaption>
<bodyText confidence="0.99992325">
the same words. Given context vectors, Lin and
Pantel (2001) used a symmetric similarity met-
ric (Lin, 1998) to find candidate paraphrases. We
build dependency context vectors for each word
in our data and compute both symmetric as well
as more recently proposed asymmetric similarity
measures (Weeds et al., 2004; Szpektor and Da-
gan, 2008; Clarke, 2009), which are potentially
better suited for identifying A paraphrases. Ta-
ble 3 gives a comparison of the pairs which are
considered “most similar” according to several of
these metrics.
</bodyText>
<subsectionHeader confidence="0.998171">
6.2 Bilingual features
</subsectionHeader>
<bodyText confidence="0.999957923076923">
We explore a variety of bilingual features, which
we expect to provide complimentary signal to the
monolingual features. Each pair in PPDB is asso-
ciated with several paraphrase probabilities, which
are based on the probabilities of aligning each
word to the foreign “pivot” phrase (a foreign trans-
lation shared by the two phrases), computed as
described in Bannard and Callison-Burch (2005).
We also compute the total number of shared for-
eign translations for each phrase pair. Table 3
shows the highest ranked pairs by this bilingual
similarity score, in comparison to several of the
monolingual scores.
</bodyText>
<subsectionHeader confidence="0.996069">
6.3 Analysis
</subsectionHeader>
<bodyText confidence="0.999928714285714">
Table 5 shows an ablation analysis. The bilingual
features are especially important for distinguish-
ing the - class, and the path and WordNet features
are important for the -, class. The lexical features
show strong performance across the board; this is
often because they capture negation words (e.g.
no) and substring features (little boy @ boy).
</bodyText>
<page confidence="0.960588">
1516
</page>
<table confidence="0.791943357142857">
Predicted label Predicted label Predicted label
(using monolingual features) (using bilingual features) (using all features)
≣
True label
⊐
¬
#
~
¬ # ~ ≣ ⊐ ¬ # ~
4% 15% 3% 62% 21% 5% 4% 8%
3% 18% 7% 27% 5% 7% 7% 54%
37% 17% 6% 6% 14% 30% 36% 14%
2% 71% 6% 1% 7% 6% 78% 8%
5% 36% 23% 8% 19% 9% 30% 35%
</table>
<figure confidence="0.991086193548387">
≣ ⊐
20%
26%
8%
15%
58%
20%
14%
13%
21%
51%
≣ ⊐ ¬ # ~
83%
2%
5%
6%
1% 4% 2% 88%
76% 2%
10%
10%
8%
73%
0% 2% 4%
3%
13%
18%
7%
64%
8%
3%
6%
</figure>
<figureCaption confidence="0.961241">
Figure 4: Confusion matrices for classifier trained using only monolingual features (distributional and path) versus bilingual
features (paraphrase and translation). True labels are shown along rows, predicted along columns. The matrix is normalized
along rows, so that the predictions for each (true) class sum to 100%. The confusion matrices reflect classifier’s performance
on held-out phrase pairs from the SICK test set.
</figureCaption>
<table confidence="0.999286">
All Lex. A F1 when excluding WN
Dist. Path Para. Tran.
# 79 -2.0 -0.2 -1.2 -1.7 -0.2 -0.1
⌘ 57 -3.5 +0.2 -0.7 -2.4 -3.7 +0.5
A 68 -4.6 -0.3 -0.8 -0.8 -0.7 -1.6
� 49 -4.0 -0.8 -2.9 +0.3 -0.0 -2.2
⇠ 51 -4.9 -0.5 -0.7 -1.2 -0.9 -0.3
</table>
<tableCaption confidence="0.9981675">
Table 5: F1 measure (⇥100) achieved by entailment classifier
using 10-fold cross validation on the training data.
</tableCaption>
<bodyText confidence="0.979605578947368">
Table 3 shines some light onto the differences
between monolingual and bilingual similarities.
While the monolingual asymmetric metrics are
good for identifying A pairs, the symmetric met-
rics consistently identify -, pairs; none of the
monolingual scores we explored were effective
in making the subtle distinction between ⌘ pairs
and the other types of paraphrase. In contrast,
the bilingual similarity metric is fairly precise
for identifying ⌘ pairs, but provides less infor-
mation for distinguishing between types of non-
equivalent paraphrase. These differences are fur-
ther exhibited in the confusion matrices shown in
Figure 4; when the classifier is trained using only
monolingual features, it misclassifies 26% of �
pairs as ⌘, whereas the bilingual features make
this error only 6% of the time. On the other hand,
the bilingual features completely fail to predict the
A class, calling over 80% of such pairs ⌘ or ⇠.
</bodyText>
<sectionHeader confidence="0.999581" genericHeader="method">
7 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.987485">
7.1 Intrinsic Evaluation
</subsectionHeader>
<bodyText confidence="0.999476777777778">
We test the performance of our classifier intrinsi-
cally, through its ability to reproduce the human
labels for the phrase pairs from the SICK test sen-
tences. Table 7 shows the precision and recall
achieved by the classifier for each of our 5 en-
tailment classes. The classifier is able to achieve
an overall 79% accuracy, reaching &gt;70% preci-
sion while maintaining good levels of recall on all
classes.
</bodyText>
<table confidence="0.310628">
True Pred. N Example misclassifications
</table>
<tableCaption confidence="0.9071355">
Table 6: Example misclassifications from some of the most
frequent and most interesting error categories.
</tableCaption>
<bodyText confidence="0.998711153846154">
Figure 4 shows the classifier’s confusion ma-
trix and Table 6 shows some examples of common
and interesting error cases. The majority of errors
(26%) come from confusing the ⇠ class with the
# class. This mistake is not too concerning from
an RTE perspective since ⇠ can be treated as a
special case of # (Section 5). There are very few
cases in which the classifier makes extreme errors,
e.g. confusing ⌘ with -, or with #; some interest-
ing examples of such errors arise when the phrases
contain pronouns (e.g. girl ⌘ she) or when the
relation uses a highly infrequent word sense (e.g.
photo ⌘ still).
</bodyText>
<subsectionHeader confidence="0.997653">
7.2 The Nutcracker RTE System
</subsectionHeader>
<bodyText confidence="0.9989648">
To further test our classifier, we evaluate the use-
fulness of the automatic entailment predictions in
a downstream RTE task. We run our experiments
using Nutcracker, a state-of-the-art RTE system
based on formal semantics (Bjerva et al., 2014).
</bodyText>
<figure confidence="0.832180727272727">
⇠ # 169
# ⇠ 114
A ⇠ 108
A # 97
A ⌘ 83
⌘ A 46
A � 29
� A 29
# ⌘ 15
⌘ # 9
⌘ � 1
boy/little, an empy/the air
little/toy, color/hair
drink/juice, ocean/surf
in front of/the face of, vehicle/horse
cat/kitten, pavement/sidewalk
big/grand, a girl/a young lady
kid/teenager, no small/a large
old man/young man, a car/a window
a person/one, a crowd/a large
he is/man is, photo/still
girl is/she is
</figure>
<page confidence="0.809714">
1517
</page>
<figureCaption confidence="0.992037875">
Figure 5: ENTAILMENT Figure 6: CONTRADICTION Figure 7: NEUTRAL
Figure 8: F1 measures achieved by Nutcracker on SICK test data when using various KBs. Baselines are in gray, this work
in blue, human references in gold. PPDB-XL refers to a run in which every pair which appears in PPDB is assumed to be
equivalent. PPDB-H refers to a run in which manual labels were used to generate axioms. PPDB+ refers to runs in which
the automatic classifications were used to generate axioms. In some cases, better proof coverage causes NC to find incorrect
proofs, illustrated by the decreased performance on CONTRADICTION when using PPDB-H. For example, using PPDB-H, NC
finds an inconsistency for the pair Someone is not playing piano./A person is playing a keyboard. Using the PPDB+, in which
piano/keyboard is falsely classified as #, NC fails to find a proof and so correctly guesses NEUTRAL.
</figureCaption>
<table confidence="0.998805333333333">
Freq. Precision Recall F score
# 39% 84.22 87.55 85.85
⌘ 8% 70.36 83.07 76.19
A 26% 79.81 76.00 77.85
� 7% 73.73 73.33 73.53
⇠ 19% 70.57 63.70 66.96
</table>
<tableCaption confidence="0.989364">
Table 7: F1 measure (⇥100) achieved by entailment classifier
on the held out phrase pairs from the sentences in SICK test.
</tableCaption>
<bodyText confidence="0.95069446">
In the SemEval 2014 RTE challenge, this system
performed in the top 5 out of the more than 20 par-
ticipating systems (Marelli et al., 2014).
Given a text/hypothesis (T/H) pair, Nutcracker
(NC) uses the Boxer parser (Bos, 2008) to produce
a formal semantic representation of both T and H,
which it translates into standard first-order logic.
The logical formulae are passed to an off-the-shelf
theorem prover, which searches for a logical en-
tailment, and to a model builder, which attempts to
find a logical contradiction. By default, when the
system fails to find a proof for either entailment or
inconsistency, it predicts the most frequent class
(in our case, NEUTRAL). Therefore, NC relies
heavily on lexical entailment resources in order
to improve the recall of the theorem prover and
model builder.
Baselines The most frequent class baseline is
achieved by labeling every sentence pair as NEU-
TRAL, and results in an accuracy of 56%. A
stronger baseline is obtained by running NC alone,
without any external axioms; in this case, words
are only equivalent if they are lemma-identical.
As an additional baseline, we generate a “basic”
Table 8: Nutcracker’s overall system accuracy and proof cov-
erage when using different sources of axioms. Coverage is
measured as the percent of sentence pairs for which NC’s
theorem prover or model builder is able to find a complete
logical proof of either entailment or contradiction. When NC
fails to find either type of proof, it guesses the most frequent
class, NEUTRAL. NC alone uses no axioms. PPDB+ refers
to the axioms generated automatically using the classifier de-
scribed in this paper. PPDB-H refers axioms generated using
the human labels on which the classifier was trained.
PPDB-XL3 knowledge base (KB), which consists
exclusively of axioms expressing synonym rela-
tionships. I.e. for every pair of phrases hp1, p2i in
PPDB-XL, the PPDB-XL KB contains the equiv-
alence axiom syn(p1, p2). We also generate the
WordNet (WN) KB, which is the default used by
NC. This KB consists of axioms for all synonyms,
antonyms, and hypernyms in WN, which generate
syn, isnota, and isa axioms, respectively.
PPDB+ We convert our classifier’s predictions
into a set of axioms for NC. When our classifier
predicts ⌘ we generate an syn axiom, when it pre-
dicts � we generate an isa axiom, and when it
predicts -, we generate an isnota axiom. # and
⇠ do not generate any axioms. To handle the di-
rectionality of the � relation, we run the classifier
</bodyText>
<footnote confidence="0.972543">
3We generated basic KBs for all six sizes of PPDB, but
XL performed best.
</footnote>
<table confidence="0.987560857142857">
Acc. # Proofs Coverage
MFC 56.4 0 0%
NC alone 74.3 878 17.8%
+ WN 77.5 1,051 21.3%
+ PPDB-XL 77.5 1,091 22.1%
+PPDB+ 78.0 1,197 24.3%
+ WN, PPDB+ 78.4 1,230 25.0%
+ WN, PPDB-H 78.6 1,232 25.0%
1518
True PPDB+ WN Text/Hypothesis pair
ENTAIL. ENTAIL. NEUTRAL A bride in a white dress is running/A girl in a white dress is running.
ENTAIL. NEUTRAL ENTAIL. A lemur is biting a person’s finger./An animal is biting a person’s finger.
CONTRA. CONTRA. NEUTRAL Someone is playing a piano./There is no one playing a piano.
CONTRA. NEUTRAL CONTRA. There is no man pouring oil into a pan./A man is pouring oil into a skillet.
</table>
<tableCaption confidence="0.999636">
Table 9: Examples of T/H pairs for which the system’s prediction differed when using PPDB+ vs. WN.
</tableCaption>
<bodyText confidence="0.999760195121951">
over every pair in both directions, and we choose
whichever direction and relation receives the high-
est confidence score to be the final prediction. We
refer to this set of automatically-predicted axioms
as PPDB+.
To calibrate our improvements, we also gener-
ate a KB using the human labels collected from
MTurk, which we refer to as PPDB-Human or
PPDB-H.
Results Table 8 reports NC’s overall prediction
accuracy and the number of proofs found when us-
ing each of the described KBs. Figure 8 shows the
performance in terms of the precision and recall
achieved for each of the three entailment classes:
ENTAILMENT, CONTRADICTION, and NEUTRAL.
Table 9 provides some examples of T/H pairs on
which predictions differed using the PPDB+ com-
pared to the WN KB, and Figure 9 shows some
illustrative misclassifications.
Our automatic labels result in a 4% improve-
ment in accuracy over the baseline of using NC
alone (Figure 8), and a 15 point improvement in F1
measure for the entailment class (Table 8). By all
performance measures, PPDB+ also outperforms
WordNet as a source of axioms for NC. More-
over, adding PPDB+ to WordNet gives a 17% rel-
ative increase in the number of proofs found com-
pared to using WordNet alone (Table 8). These
additional proofs lead NC to make a greater num-
ber of correct predictions for the “right reasons”
(i.e. finding a proof/contradiction) rather than by
lucky guessing (recall NC guesses the most fre-
quent class when it cannot find a proof).
For comparison, we run the same experiments
using a KB of oracle human labels in place of
the predicted labels in PPDB+. Using PPDB+,
NC comes very close to the performance achieved
when using PPDB-Human, demonstrating that
the automatically generated PPDB+ provides as
much utility to the end-to-end system as does a
gold-standard resource.
</bodyText>
<sectionHeader confidence="0.875576" genericHeader="method">
8 Data Release
</sectionHeader>
<bodyText confidence="0.999899642857143">
Upon publication, we are releasing a new PPDB
fully annotated with semantic relations. We are
also releasing the set of 14K manually labeled
phrase pairs occurring in RTE data, and our soft-
ware for extracting features and running the clas-
sifier, so that researchers can apply our model to
their own paraphrase collections. This will consti-
tute the largest lexical entailment resources avail-
able, while also offering new fine-grained anno-
tation necessary for challenging NLU tasks. An
evaluation of the predicted relations appearing in
the entire Paraphrase Database (not just those oc-
curring in RTE data) is given in the supplementary
material.
</bodyText>
<sectionHeader confidence="0.975101" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.969279296296297">
We argue that a significant failing of recent work
on data-driven paraphrasing is the weak definition
of paraphrases as being more-or-less equivalent.
In this paper, we show how a clear concept of se-
mantics can be applied to large-scale paraphrase
resources. In particular, the entailment relations
given by natural logic are a great fit for paraphrase
resources, since natural logic operates on pairs of
natural language expressions (like the entries in
PPDB). By classifying paraphrase entries with en-
tailment relations, we provide them with an inter-
pretable semantics. Our classifier uses extensive
feature sets to scale natural logic to the enormous
number of phrase pairs in PPDB. We rigorously
evaluate our model, demonstrating high accuracy
on an intrinsic task. On an extrinsic RTE task, our
model’s predictions allow an RTE system to find
17% more proofs and achieve a higher overall ac-
curacy than when using WordNet’s manual rela-
tions. Our new release of PPDB, annotated with
semantic entailments, will dramatically improve
PPDB’s utility for NLU tasks.
Acknowledgements This research was sup-
ported by the Allen Institute for Artificial Intel-
ligence (AI2), the Human Language Technology
Center of Excellence (HLTCOE), and by gifts
from the Alfred P. Sloan Foundation, Google, and
</bodyText>
<page confidence="0.971864">
1519
</page>
<table confidence="0.994332590909091">
# ⌘ _1 � N
38% 8% 26% 7% 18%
# 1730 9 97 49 169
40% (clear,very) (cover,front) (hand,male) (drive,park) (child,park)
(exhibit,hold) (photo,still) (man,police) (female,man) (crowded,many)
(walk,woman) (woman who,woman with) (mountain,side) (flag,ship) (note,write)
⌘ 15 368 83 9 48
10% (a big,very) (a small,the little) (a gun,a weapon) (another man,one man) (a child,kid in)
(a lot,long) (away,out) (a weapon,gun) (bike,biking) (and hold,and take)
(face a,front of) (block,slab) (legs,leg) (young girl,young woman) (his arms,his hands)
A 82 46 1004 29 97
24% (device,guy) (a call,phone call) (camera,webcam) (a car,a window) (a lady,girl)
(something,talk) (a group,bunch of) (kid,other child) (a female,a man) (field,playing)
(the man,the phone) (another man,man) (kid,the daughter) (arms,his hands) (girl,the lady)
� 35 1 29 275 33
7% (a ball,a man) (girl is,she is) (a boy,a teenager) (cat,dog) (dog,owner)
(a boy,little) (a kid,daughter) (morning,night) (ground,water)
(number,woman) (kid,little girl) (type,write) (hat,vest)
N 114 19 108 13 609
17% (leg,soccer) (chef,cook) (cut,saw) (a boat,sail) (ice,rink)
(perform,run) (fight,match) (face,hair) (dress,suit) (snow,snowy)
(sail,water) (race,ride) (the kid,the little) (light,the dark) (study by,study the)
</table>
<figureCaption confidence="0.9985805">
Figure 9: Confusion matrix for classifier (with all features) on SICK test set. True labels and their distribution are shown along
the columns, predicted along the rows.
</figureCaption>
<bodyText confidence="0.999912">
Facebook. This material is based in part on re-
search sponsored by the NSF under grant IIS-
1249516 and DARPA under agreement number
FA8750-13-2-0017 (the DEFT program). The
U.S. Government is authorized to reproduce and
distribute reprints for Governmental purposes.
The views and conclusions contained in this pub-
lication are those of the authors and should not be
interpreted as representing official policies or en-
dorsements of DARPA or the U.S. Government.
The authors would like to thank Peter Clark,
Bill MacCartney, Patrick Pantel and the anony-
mous reviews for their thoughtful suggestions.
</bodyText>
<sectionHeader confidence="0.998395" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998586381818182">
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A survey of paraphrasing and textual entail-
ment methods. Journal of Artificial Intelligence Re-
search, 38.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 597–604.
Regina Barzilay. 2003. Information fusion for multi-
document summarization: paraphrasing and gener-
ation. Ph.D. thesis, Columbia University.
Islam Beltagy, Stephen Roller, Gemma Boleda, Ka-
trin Erk, and Raymond J Mooney. 2014. UTexas:
Natural language semantics using distributional se-
mantics and probabilistic logic. SemEval 2014, page
796.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies - Volume 1, HLT ’11, pages
610–619.
Rahul Bhagat and Eduard Hovy. 2013. What is a para-
phrase? Computational Linguistics, 39.
Rahul Bhagat, Patrick Pantel, Eduard H Hovy, and
Marina Rey. 2007. Ledir: An unsupervised algo-
rithm for learning directionality of inference rules.
In EMNLP-CoNLL, pages 161–170. Citeseer.
Johannes Bjerva, Johan Bos, Rob van der Goot, and
Malvina Nissim. 2014. The meaning factory: For-
mal semantics for recognizing textual entailment
and determining semantic similarity. SemEval 2014,
page 642.
Johan Bos. 2008. Wide-coverage semantic analy-
sis with boxer. In Johan Bos and Rodolfo Del-
monte, editors, Semantics in Text Processing. STEP
2008 Conference Proceedings, Research in Compu-
tational Semantics, pages 277–286. College Publi-
cations.
Christopher John Brockett, Stanley Kok, and Dengy-
ong Zhou. 2013. Locating paraphrases through uti-
lization of a multipartite graph, July 9. US Patent
8,484,016.
Chris Callison-Burch. 2007. Paraphrasing and Trans-
lation. Ph.D. thesis, University of Edinburgh, Edin-
burgh, Scotland.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the web for fine-grained semantic verb
relations. In EMNLP, volume 2004, pages 33–40.
Peter Clark, William R. Murray, John Thompson, Phil
Harrison, Jerry Hobbs, and Christiane Fellbaum.
2007. On the role of lexical and world knowledge
in rte3. In Proceedings of the ACL-PASCAL Work-
shop on Textual Entailment and Paraphrasing, RTE
’07, pages 54–59.
</reference>
<page confidence="0.889333">
1520
</page>
<reference confidence="0.999615972477064">
Peter Clark, Myroslava O Dzikovska, Rodney D
Nielsen, Chris Brew, Claudia Leacock, Danilo Gi-
ampiccolo, Luisa Bentivogli, Ido Dagan, and Hoa T
Dang. 2013. Semeval-2013 task 7: The joint stu-
dent response analysis and 8th recognizing textual
entailment challenge.
Daoud Clarke. 2009. Context-theoretic semantics for
natural language: an overview. In Proceedings of
the Workshop on Geometrical Models of Natural
Language Semantics, pages 112–119.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The pascal recognising textual entailment
challenge. In Machine Learning Challenges. Eval-
uating Predictive Uncertainty, Visual Object Classi-
fication, and Recognising Tectual Entailment, pages
177–190. Springer.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources. In
Proceedings of the 20th international conference on
Computational Linguistics, page 350.
Juri Ganitkevitch and Chris Callison-Burch. 2014. The
multilingual paraphrase database. In The 9th edition
of the Language Resources and Evaluation Confer-
ence, Reykjavik, Iceland, May. European Language
Resources Association.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In Proceedings of NAACL-HLT, pages
758–764, Atlanta, Georgia, June. Association for
Computational Linguistics.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third PASCAL recog-
nizing textual entailment challenge. In Proceedings
of the ACL-PASCAL workshop on textual entailment
and paraphrasing, pages 1–9.
Chikara Hashimoto, Kentaro Torisawa, Kow Kuroda,
Stijn De Saeger, Masaki Murata, and Jun’ichi
Kazama. 2009. Large-scale verb entailment acqui-
sition from the web. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 3-Volume 3, pages 1172–
1181. Association for Computational Linguistics.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th Conference on Computational Linguistics -
Volume 2, COLING ’92, pages 539–545.
Aaron N Kaplan and Lenhart K Schubert. 2001.
Measuring and improving the quality of world
knowledge extracted from wordnet. University of
Rochester, Rochester, NY.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(4):359–389.
Alice Lai and Julia Hockenmaier. 2014. Illinois-LH: A
denotational and distributional approach to seman-
tics. SemEval 2014, page 329.
J Richard Landis and Gary G Koch. 1977. The mea-
surement of observer agreement for categorical data.
biometrics, pages 159–174.
Dekang Lin and Patrick Pantel. 2001. DIRT – Dis-
covery of Inference Rules from Text. In Proceed-
ings of the seventh ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
pages 323–328. ACM.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 17th inter-
national conference on Computational linguistics-
Volume 2, pages 768–774.
Bill MacCartney and Christopher D. Manning. 2007.
Natural logic for textual inference. In Proceedings
of the ACL-PASCAL Workshop on Textual Entail-
ment and Paraphrasing, RTE ’07, pages 193–200.
Bill MacCartney. 2009. Natural language inference.
Ph.D. thesis, Citeseer.
Nitin Madnani and Bonnie J. Dorr. 2010. Generating
phrasal and sentential paraphrases: A survey of data-
driven methods. Computational Linguistics, 36.
Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-
faella Bernardi, Stefano Menini, and Roberto Zam-
parelli. 2014. Semeval-2014 task 1: Evaluation of
compositional distributional semantic models on full
sentences through semantic relatedness and textual
entailment. SemEval-2014.
Courtney Napoles, Matthew Gormley, and Benjamin
Van Durme. 2012. Annotated gigaword. In Pro-
ceedings of the Joint Workshop on Automatic Knowl-
edge Base Construction and Web-scale Knowledge
Extraction, pages 95–100.
Maria Pontiki, Haris Papageorgiou, Dimitrios Galanis,
Ion Androutsopoulos, John Pavlopoulos, and Suresh
Manandhar. 2014. Semeval-2014 task 4: Aspect
based sentiment analysis. Proceedings of SemEval,
Dublin, Ireland.
Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2004.
Learning syntactic patterns for automatic hypernym
discovery. In NIPS, volume 17, pages 1297–1304.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th Annual Meeting of the Association for Compu-
tational Linguistics, ACL-44, pages 801–808.
Idan Szpektor and Ido Dagan. 2008. Learning en-
tailment rules for unary templates. In Proceedings
of the 22Nd International Conference on Computa-
tional Linguistics - Volume 1, COLING ’08, pages
849–856.
</reference>
<page confidence="0.8091">
1521
</page>
<reference confidence="0.995891625">
Idan Szpektor, Hristo Tanev, Dr Dagan, Bonaventura
Coppola, et al. 2004. Scaling web-based acquisition
of entailment relations.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of the 20th International
Conference on Computational Linguistics, COLING
’04.
Wei Xu, Alan Ritter, Chris Callison-Burch, William B.
Dolan, and Yangfeng Ji. 2014. Extracting lexically
divergent paraphrases from Twitter. Transactions of
the Association for Computational Linguistics, 2.
Jiang Zhao, Tian Tian Zhu, and Man Lan. 2014. Ecnu:
One stone two birds: Ensemble of heterogenous
measures for semantic relatedness and textual entail-
ment. SemEval 2014, page 271.
</reference>
<page confidence="0.99358">
1522
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999858">Adding Semantics to Data-Driven Paraphrasing</title>
<author confidence="0.990027">Johan Malvina Charley Benjamin Van_Chris</author>
<affiliation confidence="0.45583375">and Information Science Department, University of for Language and Cognition Groningen, University of Language Technology Center of Excellence, Johns Hopkins Watson Group</affiliation>
<abstract confidence="0.985874233766234">We add an interpretable semantics to the paraphrase database (PPDB). To date, the relationship between phrase pairs in the database has been weakly defined as approximately equivalent. We show that these pairs represent a variety of relations, including directed entailand exclusion We automatically assign semantic entailment relations to entries in PPDB using features derived from past work on discovering inference rules from text and semantic taxonomy induction. We demonstrate that our model assigns these relations with high accuracy. In a downstream RTE task, our labels rival relations from WordNet and improve the coverage of a proof-based RTE system by 17%. 1 Motivation A basic precursor to language understanding is the ability to recognize when two expressions mean the same thing. Different expressions of the same information is the central problem addressed by paraphrasing and the closely related task of recognizing textual entailment (RTE). In RTE, a system is given two pieces of text, often called the and the and asked to determine whether T entails H, T contradicts H, or T and H are unrelatable (Figure 1). In contrast, data-driving paraphrasing typically sidesteps developing a clear definition of “meaning the same thing” and instead “assume[s] paraphrasing is a coherent notion and concentrate[s] on devices that can produce paraphrases” (Barzilay, 2003). Recent work on paraphrase extraction has resulted in enormous paraphrase collections (Lin and Pantel, 2001; Dolan et al., 2004; Ganitkevitch et al., 2013), but the usefulness of these collections Figure 1: An example sentence pair for the RTE task. In order for a system to conclude that the premise (top) does not entail hypothesis (bottom), it should recognize that imthat Denmark These phrase-level entailment relationships are modeled by natural logic. is limited by the fast-and-loose treatment of the meaning of paraphrases. One concrete definition that is sometimes used for paraphrases requires that they be bidirectionally entailing (Androutsopoulos and Malakasiotis, 2010). That is, in terms of RTE, it is assumed that if P is a paraphrase of Q, then P entails Q and Q entails P. In reality, paraphrases are often more nuanced (Bhagat and Hovy, 2013), and the entries in most paraphrase resources certainly do not match this definition. For instance, Lin and Pantel (2001) extracted 12 million “inference rules” from monolingual text by exploiting shared dependency contexts. Their method learns paraphrases that are truly meaning equivalent, but it just as readily contradictory pairs such as Ganitkevitch et al. (2013) extract over 150 million paraphrase rules by pivoting through foreign translations. This bilingual method often learns hypernym/hyponym pairs, e.g. due to variation the discourse structure of translations (Callison- Riots in Denmark were sparked by 12 editorial cartoons that were offensive to Muhammad. editorial cartoons illustrations offensive insulting prophet sparked caused riots unrest in Denmark Twelve illustrations insulting the caused unrest in Jordan. in Jordan</abstract>
<date confidence="0.396896">1512</date>
<note confidence="0.908624333333333">Proceedings of the 53rd Annual Meeting of the Association for Computational the 7th International Joint Conference on Natural Language pages China, July 26-31, 2015. Association for Computational Linguistics</note>
<abstract confidence="0.980027143292684">Equivalent Entailment Exclusion Other relation Unrelated look at/watch little girl/girl close/open swim/water girl/play a person/someone kuwait/country minimal/significant husband/marry to found/party clean/cleanse tower/building boy/young girl oil/oil price profit/year away/out the cia/agency nobody/someone country/patriotic man/talk distant/remote sneaker/footwear blue/green drive/vehicle car/family the phone/the telephone heroin/drug france/germany family/home holiday/series last autumn/last fall doe/deer least three/least two basketball/court green/tennis illegal entry/smuggling typhoon/storm child/mother playing/toy sunday/tour approve/to ratify seriously injure/injure in front/on the side islamic/jihad city/south alliance of/coalition between sunglasses/glasses oppose/support delay/time back/view Table 1: Examples of different types of entailment relations appearing in PPDB. Burch, 2007), and unrelated pairs, e.g. due to misalignments or polysemy in the foreign language. The unclear semantics severely limits the applicability of paraphrase resources to natural language understanding (NLU) tasks. Some efforts have been made to identify directionality of paraphrases (Bhagat et al., 2007; Kotlerman et al., 2010), but tasks like RTE require even richer semantic information. For example, in the T/H pair shown in Figure 1, a system needs information only about equivalent words and entailments but also seexclusion Such lexical relations are captured by a formalism which views natural language itself as a meaning representation, eschewing external representations such as First Order Logic (FOL). This is a great fit for automatically extracted paraphrases, since the phrase pairs themselves can be used as the semantic representation with minimal additional annotation. But as is, paraphrase resources lack such annotation. As a result, NLU systems rely on manually built resources like WordNet, which are limited in coverage and often lead to incorrect inferences (Kaplan and Schubert, 2001). In fact, in the most recent RTE challenge, over half of the submitted systems used WordNet (Pontiki et al., 2014). Even the NatLog system (MacCartney and Manning, 2007), which popularized natural logic for RTE, relied on WordNet and did not solve the problem of assigning natural logic relations at scale. The main contributions of this paper are: • We add a concrete, interpretable semantics to the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013), the largest paraphrase resource currently available. We give each entry in the database a label describing the entailment relationship between the phrases. • We develop a statistical model to predict these relations. The enormous size of PPDB– over 77 million phrase pairs!– makes it impossible to perform this task manually. Our wide range of monolingual and bilingual features results in high intrinsic accuracy. • We demonstrate improvements to a proofbased RTE system, showing that our automatic labels increase the number of proofs that it is able to find by 17%, while maintaining the same accuracy as when using goldstandard, manual labels. 2 Related Work entailment resources to paraphrase identification have exploited signal from distributional contexts (Lin and Pantel, 2001; Szpektor et al., 2004), comparable corpora (Dolan et al., 2004; Xu et al., 2014), and graph structures (Berant et al., 2011; Brockett et al., 2013). These approaches are scalable, but they often assume that all relations are equivalence relations (Madnani and Dorr, 2010). Several efforts have attempted to build or augment lexical ontologies automatically, to discover other types of lexical relations like hypernyms. Most of these approaches rely on lexico-syntactic patterns. Hearst (1992) searched for hand-written patterns (e.g. “an X is a Y”) in a large corpus in order to learn taxonomic relations between nouns. Snow et al. (2006) used dependency parses to automatically learn such patterns, which they used to augment WordNet with new hypernym relations. Similar monolingual signals have been used to learn fine-grained relationships verbs, such as happensand Pantel, 2004; Hashimoto et al., 2009). Textual Entailment shared RTE tasks (Dagan et al., 2006) have been a springboard for research in natural language inference, 1513 Figure 2: Distribution of entailment relations in different sizes of PPDB. Distributions are estimated from our manual annotations of randomly sampled pairs. PPDB-XXXL contains over 77MM paraphrase pairs (where the majority type is independent), compared to only 700K in PPDB-S (where the majority type is equivalent). using data motivated by the applications to information retrieval, information extraction, summarization, machine translation evaluation, and more recently, question answering (Giampiccolo et al., 2007) and essay grading (Clark et al., 2013). RTE systems vary considerably in their choice of representation and inference procedure. In the most recent shared task on RTE, some systems used deep logical representations of text, allowing them to invoke theorem provers (Bjerva et al., 2014) or Markov Logic Networks (Beltagy et al., 2014) to perform the inference, while others used shallower representations, relying on machine learning to perform inference (Lai and Hockenmaier, 2014; Zhao et al., 2014). Systems based on natural logic (MacCartney and Manning, 2007) use natural language as a representation, but still perform inference using a structured algebra rather than a statistical model. Regardless of the inference procedure, improvements to external lexical resources can improve RTE systems across the board (Clark et al., 2007). 3 The Paraphrase Database (PPDB) PPDB is currently the largest available collection of paraphrases. Compared to other paraphrase resources such as the DIRT database (12 million rules) (Lin and Pantel, 2001) and the MSR paraphrase phrase table (13 million) (Dolan et al., 2004), PPDB contains over 150 million paraphrase rules covering three paraphrase types– lexical (single word), phrasal (multiword), and syntactic restructuring rules. We focus on lexical and phrasal paraphrases, of which there are over 77 million rules. Of these, a large fraction are true either equivalent or entailment but many are not. PPDB contains some pairs which are by semantic exclusion some of which are related by something other than and some which are simunrelated Table 1 gives examples of pairs in PPDB falling into each of these categories. PPDB is released in six sizes (S, M, L, XL, XXL and XXXL), which fall roughly on a continuum from highest precision and lowest recall to lowest average precision and highest recall. Figure 2 shows how the distribution of entailment rediffers across the sizes of Our goal is to make these relations explicit, by providing annotations for each phrase pair. Because of the enormous scale of PPDB, this annotation must be done automatically. 4 Selection of Paraphrases In this paper we focus on paraphrases pairs from PPDB that occur in RTE data. We use the recent SICK dataset from in the 2014 SemEval RTE challenge (Marelli et al., 2014) for our experiments. The data consists of 10K sentences split roughly evenly into training and testing sets. The sentence pairs are labeled using a 3-way entailment (29%) or We consider phrase pairs from PPDB to three words in length such that there is some T/H senpair in which in T and distributions were estimated based on a random sample of pairs drawn from each size of PPDB, annotated on MTurk as described in Section 5 1514 We use the lemmas, POS tags, and phrase lengths of the substrings shared by the Levenstein, Jaccard, and Hamming distances between Given a dependency context vectors for we compute the number of shared contexts, and the Jaccard, Cosine, Lin1998, Weeds2004, Clarke2009, and Szpektor2008 similarities between the vectors. Paraphrase We include 33 paraphrase features distributed with PPDB, which include the paraphrase as computed in Bannard and Callison-Burch (2005). We refer the reader to Ganitkevitch and Callison- Burch (2014) for a complete description of all of the features included with PPDB. We include the number of foreign language “pivots” (translations) shared by each of languages used in the construction of PPDB, as a fraction of the total number of translations observed each of Path We include a sparse vector of all lexico-syntactic patterns (paths through a dependency parse) observed between the Annotated Gigaword corpus (Napoles et al., 2012). We include binary features indicating whether WordNet classifies to any of following relations: synonym, hypernym, hyponym, antonym, holonym, meronym, cause, entailment, derivationally-related, similar-to, also-see, or attribute. 3: Summary of features extracted for each phrase pair Full descriptions of the features used are given in the supplementary material. in H. Roughly 55% of the word types and 5% of the phrase (bigram and trigram) types in the SICK data appear in PPDB. This gives us a list of 9,600 pairs, half from the training sentences, which we use for development in Section 6, and half from the test sentences, which we use for evaluation in Section 7. The SICK data has a relatively small vocabuwith 86% of words types and of the phrase types covered by WordNet. Still, over half of the words in SICK which are covered by PPDB do not appear in WordNet. In general, PPDB covers a much larger vocabulary (1.6MM words) than does WordNet (155K words), and we expect the potential benefit of using PPDB in addition to or in place of WordNet to be larger on datasets with richer vocabularies. 5 Entailment Relations We use the relations from Bill MacCartney’s thesis on natural language inference as the basis for our categorization of relations (MacCartney, 2009). He outlines 7 basic entailment relation- Entailment Q(x)] Entailment P(x)] (PˆQ): [P(x) (P—Q): Independence (P#Q): All other cases. further clarify the definitions here: “negation” is XOR (exclusive disjunction), “alternation” is NAND, and “cover” is OR (inclusive disjunction) relations are based on the theory of natumeaning they are defined between pairs of natural language expressions rather than requiring an external formal representation. This makes them an ideal fit for the phrase pairs in in PPDB and similar automatically-constructed paraphrase resources. Log. work MTurk description ⌘ X is the same as Y @ @ X is more specific than/is a type of Y A X is more general than/encompasses Y &amp;quot; � X is the opposite of Y X is mutually exclusive with Y Table 2: Column 1 gives the semantics of each label under MacCartney’s Natural Logic. Column 2 gives the notation we use throughout the remainder of this paper. Column 3 gives the description that was shown to Turkers. use Amazon Mechanical Turk (MTurk) to collect labels for our phrase pairs. We asked workers to choose between the options show in Table 2, which represent a modified version of MacCartney’s relations. We replace negation with the weaker notion of “opposites,” effecmerging it with the alternation relation; split the independent class into two cases: truly independent phrases and phrases which are related by something other than entailment (which denote We omit the cover (—) relation entirely, as its practicality is not obvious. We show each pair to 5 workers, taking the majority label as truth. Each HIT consisted of two control questions taken from WordNet. Workers achieved good acon our controls (82% overall) and moder- 1515 Cosine Similarity Monolingual (symmetric) Monolingual (asymmetric) Bilingual shade large/small boy men kid/child other/man different/same children lot of/many other/same playing/play put/take sides 3: Top scoring pairs according to various similarity measures, along with their manually classified entailment labels. Column 1 is cosine similarity based on dependency contexts. Column 2 is based on Lin (1998), column 3 on Weeds (2004), and column 4 is a novel feature. Precise definitions of each metric are given in the supplementary material. levels of agreement (Fleiss’s 0.56) (Landis and Koch, 1977). For a fuller discussion of the annotation, refer to the supplementary material. 6 Automatic Classification We aim to build a classifier to automatically assign entailment types to entries in the PPDB, and to demonstrate that it performs well both intrinsically extrinsically. We fix the direction of the to create a single class and train a logistic regression classifier to distinguish between 5 classes We compute variety of basic lexical features and WordNet features (summarized in Figure 3). We categorize the remaining features into two broad groups: monolingual features, which are based on observed usage in the Annotated Gigaword corpus (Napoles et al., 2012), and bilingual features, which are based on translation probabilities observed in bilingual parallel corpora. Full descriptions of all the features used are provided in the supplementary material. 6.1 Monolingual features features et al. (2004) used lexicosyntactic patterns to mine taxonomic relations (hypernyms and hyponyms) between noun pairs. They were able to verify the earlier work of Hearst which found that certain patterns, e.g. other are strong indicators of hypernymy. Using similar path features, we learn new patterns to differentiate between more subtle relations. For we learn the pattern X from Y indicative of the We learn that pattern including Y than it the pattern known as Y sugthan Table 4 gives examples of of the paths most indicative of the features and Pantel (2001) attempted to mine inference rules from text by finding paths in a dependency tree which connect the same nouns. The intuition is that good paraphrases should tend to modify and be modified by X and in Y foods and in beverages X from Y the old from the young X and/or to Y the left or to the right X to Y 7 a.m. to 10 p.m. X than Y harm than good 4: Top paths associated with the the same words. Given context vectors, Lin and Pantel (2001) used a symmetric similarity metric (Lin, 1998) to find candidate paraphrases. We build dependency context vectors for each word in our data and compute both symmetric as well as more recently proposed asymmetric similarity measures (Weeds et al., 2004; Szpektor and Dagan, 2008; Clarke, 2009), which are potentially suited for identifying Table 3 gives a comparison of the pairs which are considered “most similar” according to several of these metrics. 6.2 Bilingual features We explore a variety of bilingual features, which we expect to provide complimentary signal to the monolingual features. Each pair in PPDB is associated with several paraphrase probabilities, which are based on the probabilities of aligning each word to the foreign “pivot” phrase (a foreign translation shared by the two phrases), computed as described in Bannard and Callison-Burch (2005). We also compute the total number of shared foreign translations for each phrase pair. Table 3 shows the highest ranked pairs by this bilingual similarity score, in comparison to several of the monolingual scores. 6.3 Analysis Table 5 shows an ablation analysis. The bilingual features are especially important for distinguishthe and the path and WordNet features important for the The lexical features show strong performance across the board; this is often because they capture negation words (e.g. and substring features boy 1516 Predicted label Predicted label Predicted label (using monolingual features) (using bilingual features) (using all features) ≣ True label ⊐ ¬ ~ ¬ # ~ ≣ ⊐ ¬ # ~ 4% 15% 3% 62% 21% 5% 4% 8% 3% 18% 7% 27% 5% 7% 7% 54% 37% 17% 6% 6% 14% 30% 36% 14% 2% 71% 6% 1% 7% 6% 78% 8% 5% 36% 23% 8% 19% 9% 30% 35% ≣ ⊐ 20% 26% 8% 15% 58% 20% 14% 13% 21% 51% ⊐ # ~ 83% 2% 5% 6% 1% 4% 2% 88% 76% 2% 10% 10% 8% 73% 0% 2% 4% 3% 13% 18% 7% 64% 8% 3% 6% Figure 4: Confusion matrices for classifier trained using only monolingual features (distributional and path) versus bilingual features (paraphrase and translation). True labels are shown along rows, predicted along columns. The matrix is normalized along rows, so that the predictions for each (true) class sum to 100%. The confusion matrices reflect classifier’s performance on held-out phrase pairs from the SICK test set. All Lex. when excluding WN Dist. Path Para. Tran. ⌘ 57 -3.5 +0.2 -0.7 -2.4 -3.7 +0.5 A 68 -4.6 -0.3 -0.8 -0.8 -0.7 -1.6 � 49 -4.0 -0.8 -2.9 +0.3 -0.0 -2.2 ⇠ 51 -4.9 -0.5 -0.7 -1.2 -0.9 -0.3 5: F1 measure achieved by entailment classifier using 10-fold cross validation on the training data. Table 3 shines some light onto the differences between monolingual and bilingual similarities. While the monolingual asymmetric metrics are for identifying the symmetric metconsistently identify none of the monolingual scores we explored were effective making the subtle distinction between and the other types of paraphrase. In contrast, the bilingual similarity metric is fairly precise identifying but provides less information for distinguishing between types of nonequivalent paraphrase. These differences are further exhibited in the confusion matrices shown in Figure 4; when the classifier is trained using only features, it misclassifies 26% of as whereas the bilingual features make this error only 6% of the time. On the other hand, the bilingual features completely fail to predict the calling over 80% of such pairs 7 Evaluation 7.1 Intrinsic Evaluation We test the performance of our classifier intrinsically, through its ability to reproduce the human labels for the phrase pairs from the SICK test sentences. Table 7 shows the precision and recall by the classifier for each of our 5 entailment classes. The classifier is able to achieve overall 79% accuracy, reaching precision while maintaining good levels of recall on all classes. True Pred. N Example misclassifications Table 6: Example misclassifications from some of the most frequent and most interesting error categories. Figure 4 shows the classifier’s confusion matrix and Table 6 shows some examples of common and interesting error cases. The majority of errors come from confusing the with the This mistake is not too concerning from RTE perspective since be treated as a case of 5). There are very few cases in which the classifier makes extreme errors, confusing with some interesting examples of such errors arise when the phrases pronouns (e.g. or when the relation uses a highly infrequent word sense (e.g. 7.2 The Nutcracker RTE System To further test our classifier, we evaluate the usefulness of the automatic entailment predictions in a downstream RTE task. We run our experiments using Nutcracker, a state-of-the-art RTE system based on formal semantics (Bjerva et al., 2014). ⇠ # 169 A 108 A # 97 A 83 ⌘ 46 A 29 � 29 ⌘ # 9 ⌘ � 1 boy/little, an empy/the air little/toy, color/hair drink/juice, ocean/surf in front of/the face of, vehicle/horse cat/kitten, pavement/sidewalk big/grand, a girl/a young lady kid/teenager, no small/a large old man/young man, a car/a window a person/one, a crowd/a large he is/man is, photo/still girl is/she is 1517 5: 6: 7: Figure 8: F1 measures achieved by Nutcracker on SICK test data when using various KBs. Baselines are in gray, this work in blue, human references in gold. PPDB-XL refers to a run in which every pair which appears in PPDB is assumed to be PPDB-H refers to a run in which manual labels were used to generate axioms. to runs in which the automatic classifications were used to generate axioms. In some cases, better proof coverage causes NC to find incorrect illustrated by the decreased performance on using PPDB-H. For example, using PPDB-H, NC an inconsistency for the pair is not playing piano./A person is playing a Using the in which falsely classified as NC fails to find a proof and so correctly guesses Freq. Precision Recall F score ⌘ 8% 70.36 83.07 76.19 A 26% 79.81 76.00 77.85 � 7% 73.73 73.33 73.53 ⇠ 19% 70.57 63.70 66.96 7: F1 measure achieved by entailment classifier on the held out phrase pairs from the sentences in SICK test. In the SemEval 2014 RTE challenge, this system performed in the top 5 out of the more than 20 participating systems (Marelli et al., 2014). Given a text/hypothesis (T/H) pair, Nutcracker (NC) uses the Boxer parser (Bos, 2008) to produce a formal semantic representation of both T and H, which it translates into standard first-order logic. The logical formulae are passed to an off-the-shelf theorem prover, which searches for a logical entailment, and to a model builder, which attempts to find a logical contradiction. By default, when the system fails to find a proof for either entailment or inconsistency, it predicts the most frequent class our case, Therefore, NC relies heavily on lexical entailment resources in order to improve the recall of the theorem prover and model builder. most frequent class baseline is by labeling every sentence pair as and results in an accuracy of 56%. A stronger baseline is obtained by running NC alone, without any external axioms; in this case, words are only equivalent if they are lemma-identical. As an additional baseline, we generate a “basic” Table 8: Nutcracker’s overall system accuracy and proof coverage when using different sources of axioms. Coverage is measured as the percent of sentence pairs for which NC’s theorem prover or model builder is able to find a complete logical proof of either entailment or contradiction. When NC fails to find either type of proof, it guesses the most frequent NC alone uses no axioms. PPDB+ refers to the axioms generated automatically using the classifier described in this paper. PPDB-H refers axioms generated using the human labels on which the classifier was trained. base (KB), which consists exclusively of axioms expressing synonym rela- I.e. for every pair of phrases PPDB-XL, the PPDB-XL KB contains the equivaxiom We also generate the WordNet (WN) KB, which is the default used by NC. This KB consists of axioms for all synonyms, antonyms, and hypernyms in WN, which generate and respectively. convert our classifier’s predictions into a set of axioms for NC. When our classifier generate an when it pre- � we generate an and when it generate an not generate any axioms. To handle the directionality of the � relation, we run the classifier generated basic KBs for all six sizes of PPDB, but XL performed best. Acc. # Proofs Coverage MFC 56.4 0 0% NC alone 74.3 878 17.8% + WN 77.5 1,051 21.3% + PPDB-XL 77.5 1,091 22.1% 1,197 24.3% + WN, 1,230 25.0% + WN, PPDB-H 78.6 1,232 25.0% 1518 Text/Hypothesis pair a white dress is running/A a white dress is running. A biting a person’s finger./An biting a person’s playing a piano./There is one a piano. There is man oil into a man pouring oil into a 9: Examples of T/H pairs for which the system’s prediction differed when using WN. over every pair in both directions, and we choose whichever direction and relation receives the highest confidence score to be the final prediction. We refer to this set of automatically-predicted axioms To calibrate our improvements, we also generate a KB using the human labels collected from MTurk, which we refer to as PPDB-Human or PPDB-H. 8 reports NC’s overall prediction accuracy and the number of proofs found when using each of the described KBs. Figure 8 shows the performance in terms of the precision and recall achieved for each of the three entailment classes: and Table 9 provides some examples of T/H pairs on predictions differed using the compared to the WN KB, and Figure 9 shows some illustrative misclassifications. Our automatic labels result in a 4% improvement in accuracy over the baseline of using NC alone (Figure 8), and a 15 point improvement in F1 measure for the entailment class (Table 8). By all measures, outperforms WordNet as a source of axioms for NC. Moreadding WordNet gives a 17% relative increase in the number of proofs found compared to using WordNet alone (Table 8). These additional proofs lead NC to make a greater number of correct predictions for the “right reasons” (i.e. finding a proof/contradiction) rather than by lucky guessing (recall NC guesses the most frequent class when it cannot find a proof). For comparison, we run the same experiments using a KB of oracle human labels in place of predicted labels in Using NC comes very close to the performance achieved when using PPDB-Human, demonstrating that automatically generated as much utility to the end-to-end system as does a gold-standard resource. 8 Data Release Upon publication, we are releasing a new PPDB fully annotated with semantic relations. We are also releasing the set of 14K manually labeled phrase pairs occurring in RTE data, and our software for extracting features and running the classifier, so that researchers can apply our model to their own paraphrase collections. This will constitute the largest lexical entailment resources available, while also offering new fine-grained annotation necessary for challenging NLU tasks. An evaluation of the predicted relations appearing in the entire Paraphrase Database (not just those occurring in RTE data) is given in the supplementary material. 9 Conclusion We argue that a significant failing of recent work on data-driven paraphrasing is the weak definition of paraphrases as being more-or-less equivalent. In this paper, we show how a clear concept of semantics can be applied to large-scale paraphrase resources. In particular, the entailment relations given by natural logic are a great fit for paraphrase resources, since natural logic operates on pairs of natural language expressions (like the entries in PPDB). By classifying paraphrase entries with entailment relations, we provide them with an interpretable semantics. Our classifier uses extensive feature sets to scale natural logic to the enormous number of phrase pairs in PPDB. We rigorously evaluate our model, demonstrating high accuracy on an intrinsic task. On an extrinsic RTE task, our model’s predictions allow an RTE system to find 17% more proofs and achieve a higher overall accuracy than when using WordNet’s manual relations. Our new release of PPDB, annotated with semantic entailments, will dramatically improve PPDB’s utility for NLU tasks. research was supported by the Allen Institute for Artificial Intelligence (AI2), the Human Language Technology Center of Excellence (HLTCOE), and by gifts from the Alfred P. Sloan Foundation, Google, and 1519 38% 8% 26% 7% 18% 40% ⌘ (a (a small,the (a gun,a (another man,one (a child,kid 10% (a (block,slab) (a (young girl,young woman) (and hold,and (face a,front of) (legs,leg) (his arms,his hands) A (the man,the phone) 46 1004 (camera,webcam) (kid,other child) (kid,the daughter) 29 (a 24% (a call,phone (a car,a (girl,the lady) (a group,bunch (a female,a (another man,man) (arms,his hands) � (a ball,a 1 (a boy,a (type,write) (hat,vest) 7% (a (girl is,she is) (a (number,woman) (kid,little girl) N (sail,water) (race,ride) (the kid,the little) (a (study by,study the) 17% (light,the dark) Figure 9: Confusion matrix for classifier (with all features) on SICK test set. True labels and their distribution are shown along the columns, predicted along the rows. Facebook. This material is based in part on research sponsored by the NSF under grant IIS- 1249516 and DARPA under agreement number FA8750-13-2-0017 (the DEFT program). The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes. The views and conclusions contained in this publication are those of the authors and should not be interpreted as representing official policies or endorsements of DARPA or the U.S. Government.</abstract>
<note confidence="0.648697016949153">The authors would like to thank Peter Clark, Bill MacCartney, Patrick Pantel and the anonymous reviews for their thoughtful suggestions. References Ion Androutsopoulos and Prodromos Malakasiotis. 2010. A survey of paraphrasing and textual entailmethods. of Artificial Intelligence Re- 38. Colin Bannard and Chris Callison-Burch. 2005. Parawith bilingual parallel corpora. In Proceedings of the 43rd Annual Meeting on Association Computational pages 597–604. Barzilay. 2003. fusion for multidocument summarization: paraphrasing and gener- Ph.D. thesis, Columbia University. Islam Beltagy, Stephen Roller, Gemma Boleda, Katrin Erk, and Raymond J Mooney. 2014. UTexas: Natural language semantics using distributional seand probabilistic logic. page 796. Jonathan Berant, Ido Dagan, and Jacob Goldberger. 2011. Global learning of typed entailment rules. of the 49th Annual Meeting of the Association for Computational Linguistics: Human Technologies - Volume HLT ’11, pages 610–619. Rahul Bhagat and Eduard Hovy. 2013. What is a para- 39. Rahul Bhagat, Patrick Pantel, Eduard H Hovy, and Marina Rey. 2007. Ledir: An unsupervised algorithm for learning directionality of inference rules. pages 161–170. Citeseer. Johannes Bjerva, Johan Bos, Rob van der Goot, and Malvina Nissim. 2014. The meaning factory: Formal semantics for recognizing textual entailment determining semantic similarity. page 642. Johan Bos. 2008. Wide-coverage semantic analysis with boxer. In Johan Bos and Rodolfo Deleditors, in Text Processing. STEP Conference Research in Computational Semantics, pages 277–286. College Publications. Christopher John Brockett, Stanley Kok, and Dengyong Zhou. 2013. Locating paraphrases through utilization of a multipartite graph, July 9. US Patent 8,484,016. Callison-Burch. 2007. and Trans- Ph.D. thesis, University of Edinburgh, Edinburgh, Scotland. Timothy Chklovski and Patrick Pantel. 2004. VerbOcean: Mining the web for fine-grained semantic verb In volume 2004, pages 33–40. Peter Clark, William R. Murray, John Thompson, Phil Harrison, Jerry Hobbs, and Christiane Fellbaum. 2007. On the role of lexical and world knowledge rte3. In of the ACL-PASCAL Workon Textual Entailment and RTE ’07, pages 54–59.</note>
<date confidence="0.329511">1520</date>
<author confidence="0.579281">Peter Clark</author>
<author confidence="0.579281">Myroslava O Dzikovska</author>
<author confidence="0.579281">Rodney D Nielsen</author>
<author confidence="0.579281">Chris Brew</author>
<author confidence="0.579281">Claudia Leacock</author>
<author confidence="0.579281">Danilo Gi-</author>
<affiliation confidence="0.324578">ampiccolo, Luisa Bentivogli, Ido Dagan, and Hoa T</affiliation>
<abstract confidence="0.887426615384615">Dang. 2013. Semeval-2013 task 7: The joint student response analysis and 8th recognizing textual entailment challenge. Daoud Clarke. 2009. Context-theoretic semantics for language: an overview. In of the Workshop on Geometrical Models of Natural pages 112–119. Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The pascal recognising textual entailment In Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classiand Recognising Tectual pages 177–190. Springer.</abstract>
<note confidence="0.911675272727273">Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In Proceedings of the 20th international conference on page 350. Juri Ganitkevitch and Chris Callison-Burch. 2014. The paraphrase database. In 9th edition of the Language Resources and Evaluation Confer- Reykjavik, Iceland, May. European Language Resources Association. Juri Ganitkevitch, Benjamin Van Durme, and Chris</note>
<abstract confidence="0.569298777777778">Callison-Burch. 2013. PPDB: The paraphrase In of pages 758–764, Atlanta, Georgia, June. Association for Computational Linguistics. Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2007. The third PASCAL recogtextual entailment challenge. In of the ACL-PASCAL workshop on textual entailment pages 1–9.</abstract>
<note confidence="0.8410801">Chikara Hashimoto, Kentaro Torisawa, Kow Kuroda, Stijn De Saeger, Masaki Murata, and Jun’ichi Kazama. 2009. Large-scale verb entailment acquifrom the web. In of the 2009 Conference on Empirical Methods in Natural Lan- Processing: Volume 3-Volume pages 1172– 1181. Association for Computational Linguistics. Marti A. Hearst. 1992. Automatic acquisition of hyfrom large text corpora. In of the 14th Conference on Computational Linguistics - COLING ’92, pages 539–545. Aaron N Kaplan and Lenhart K Schubert. 2001. Measuring and improving the quality of world extracted from wordnet. of Rochester, Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-Geffet. 2010. Directional distribusimilarity for lexical inference. Lan- 16(4):359–389. Alice Lai and Julia Hockenmaier. 2014. Illinois-LH: A</note>
<abstract confidence="0.888566217391304">denotational and distributional approach to semanpage 329. J Richard Landis and Gary G Koch. 1977. The measurement of observer agreement for categorical data. pages 159–174. Dekang Lin and Patrick Pantel. 2001. DIRT – Disof Inference Rules from Text. In Proceedings of the seventh ACM SIGKDD international conon Knowledge discovery and data pages 323–328. ACM. Dekang Lin. 1998. Automatic retrieval and clustering similar words. In of the 17th international conference on Computational linguisticspages 768–774. Bill MacCartney and Christopher D. Manning. 2007. logic for textual inference. In of the ACL-PASCAL Workshop on Textual Entailand RTE ’07, pages 193–200. MacCartney. 2009. language Ph.D. thesis, Citeseer. Nitin Madnani and Bonnie J. Dorr. 2010. Generating phrasal and sentential paraphrases: A survey of datamethods. 36.</abstract>
<author confidence="0.9438815">Marco Marelli</author>
<author confidence="0.9438815">Luisa Bentivogli</author>
<author confidence="0.9438815">Marco Baroni</author>
<author confidence="0.9438815">Raffaella Bernardi</author>
<author confidence="0.9438815">Stefano Menini</author>
<author confidence="0.9438815">Roberto Zam-</author>
<abstract confidence="0.62373475">parelli. 2014. Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual Courtney Napoles, Matthew Gormley, and Benjamin Durme. 2012. Annotated gigaword. In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge pages 95–100. Maria Pontiki, Haris Papageorgiou, Dimitrios Galanis, Ion Androutsopoulos, John Pavlopoulos, and Suresh Manandhar. 2014. Semeval-2014 task 4: Aspect sentiment analysis. of SemEval,</abstract>
<note confidence="0.967851096774194">Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2004. Learning syntactic patterns for automatic hypernym In volume 17, pages 1297–1304. Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006. Semantic taxonomy induction from heterogenous In of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Compu- ACL-44, pages 801–808. Idan Szpektor and Ido Dagan. 2008. Learning enrules for unary templates. In of the 22Nd International Conference on Computa- Linguistics - Volume COLING ’08, pages 849–856. 1521 Idan Szpektor, Hristo Tanev, Dr Dagan, Bonaventura Coppola, et al. 2004. Scaling web-based acquisition of entailment relations. Julie Weeds, David Weir, and Diana McCarthy. 2004. Characterising measures of lexical distributional In of the 20th International on Computational COLING ’04. Wei Xu, Alan Ritter, Chris Callison-Burch, William B. Dolan, and Yangfeng Ji. 2014. Extracting lexically paraphrases from Twitter. of Association for Computational 2. Jiang Zhao, Tian Tian Zhu, and Man Lan. 2014. Ecnu: One stone two birds: Ensemble of heterogenous measures for semantic relatedness and textual entailpage 271.</note>
<intro confidence="0.458445">1522</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ion Androutsopoulos</author>
<author>Prodromos Malakasiotis</author>
</authors>
<title>A survey of paraphrasing and textual entailment methods.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>38</volume>
<contexts>
<context position="2575" citStr="Androutsopoulos and Malakasiotis, 2010" startWordPosition="385" endWordPosition="389">ntel, 2001; Dolan et al., 2004; Ganitkevitch et al., 2013), but the usefulness of these collections Figure 1: An example sentence pair for the RTE task. In order for a system to conclude that the premise (top) does not entail the hypothesis (bottom), it should recognize that sparked implies caused but that in Denmark precludes in Jordan. These phrase-level entailment relationships are modeled by natural logic. is limited by the fast-and-loose treatment of the meaning of paraphrases. One concrete definition that is sometimes used for paraphrases requires that they be bidirectionally entailing (Androutsopoulos and Malakasiotis, 2010). That is, in terms of RTE, it is assumed that if P is a paraphrase of Q, then P entails Q and Q entails P. In reality, paraphrases are often more nuanced (Bhagat and Hovy, 2013), and the entries in most paraphrase resources certainly do not match this definition. For instance, Lin and Pantel (2001) extracted 12 million “inference rules” from monolingual text by exploiting shared dependency contexts. Their method learns paraphrases that are truly meaning equivalent, but it just as readily learns contradictory pairs such as hX rises, Xfallsi. Ganitkevitch et al. (2013) extract over 150 million </context>
</contexts>
<marker>Androutsopoulos, Malakasiotis, 2010</marker>
<rawString>Ion Androutsopoulos and Prodromos Malakasiotis. 2010. A survey of paraphrasing and textual entailment methods. Journal of Artificial Intelligence Research, 38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>597--604</pages>
<contexts>
<context position="12465" citStr="Bannard and Callison-Burch (2005)" startWordPosition="1886" endWordPosition="1889">rs drawn from each size of PPDB, annotated on MTurk as described in Section 5 1514 Lexical We use the lemmas, POS tags, and phrase lengths of P1 and P2, the substrings shared by P1 and P2, and the Levenstein, Jaccard, and Hamming distances between P1 and P2. Distributional Given a dependency context vectors for P1 and P2, we compute the number of shared contexts, and the Jaccard, Cosine, Lin1998, Weeds2004, Clarke2009, and Szpektor2008 similarities between the vectors. Paraphrase We include 33 paraphrase features distributed with PPDB, which include the paraphrase probabilities as computed in Bannard and Callison-Burch (2005). We refer the reader to Ganitkevitch and CallisonBurch (2014) for a complete description of all of the features included with PPDB. Translation We include the number of foreign language “pivots” (translations) shared by P1 and P2 for each of 24 languages used in the construction of PPDB, as a fraction of the total number of translations observed for each of P1 and P2. Path We include a sparse vector of all lexico-syntactic patterns (paths through a dependency parse) which are observed between P1 and P2 in the Annotated Gigaword corpus (Napoles et al., 2012). WordNet We include binary features</context>
<context position="20287" citStr="Bannard and Callison-Burch (2005)" startWordPosition="3187" endWordPosition="3190">ektor and Dagan, 2008; Clarke, 2009), which are potentially better suited for identifying A paraphrases. Table 3 gives a comparison of the pairs which are considered “most similar” according to several of these metrics. 6.2 Bilingual features We explore a variety of bilingual features, which we expect to provide complimentary signal to the monolingual features. Each pair in PPDB is associated with several paraphrase probabilities, which are based on the probabilities of aligning each word to the foreign “pivot” phrase (a foreign translation shared by the two phrases), computed as described in Bannard and Callison-Burch (2005). We also compute the total number of shared foreign translations for each phrase pair. Table 3 shows the highest ranked pairs by this bilingual similarity score, in comparison to several of the monolingual scores. 6.3 Analysis Table 5 shows an ablation analysis. The bilingual features are especially important for distinguishing the - class, and the path and WordNet features are important for the -, class. The lexical features show strong performance across the board; this is often because they capture negation words (e.g. no) and substring features (little boy @ boy). 1516 Predicted label Pre</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 597–604.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
</authors>
<title>Information fusion for multidocument summarization: paraphrasing and generation.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>Columbia University.</institution>
<contexts>
<context position="1838" citStr="Barzilay, 2003" startWordPosition="273" endWordPosition="274">ng. Different expressions of the same information is the central problem addressed by paraphrasing and the closely related task of recognizing textual entailment (RTE). In RTE, a system is given two pieces of text, often called the text (T) and the hypothesis (H), and asked to determine whether T entails H, T contradicts H, or T and H are unrelatable (Figure 1). In contrast, data-driving paraphrasing typically sidesteps developing a clear definition of “meaning the same thing” and instead “assume[s] paraphrasing is a coherent notion and concentrate[s] on devices that can produce paraphrases” (Barzilay, 2003). Recent work on paraphrase extraction has resulted in enormous paraphrase collections (Lin and Pantel, 2001; Dolan et al., 2004; Ganitkevitch et al., 2013), but the usefulness of these collections Figure 1: An example sentence pair for the RTE task. In order for a system to conclude that the premise (top) does not entail the hypothesis (bottom), it should recognize that sparked implies caused but that in Denmark precludes in Jordan. These phrase-level entailment relationships are modeled by natural logic. is limited by the fast-and-loose treatment of the meaning of paraphrases. One concrete d</context>
</contexts>
<marker>Barzilay, 2003</marker>
<rawString>Regina Barzilay. 2003. Information fusion for multidocument summarization: paraphrasing and generation. Ph.D. thesis, Columbia University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Islam Beltagy</author>
<author>Stephen Roller</author>
<author>Gemma Boleda</author>
<author>Katrin Erk</author>
<author>Raymond J Mooney</author>
</authors>
<title>UTexas: Natural language semantics using distributional semantics and probabilistic logic. SemEval</title>
<date>2014</date>
<pages>796</pages>
<contexts>
<context position="9253" citStr="Beltagy et al., 2014" startWordPosition="1369" endWordPosition="1372">dependent), compared to only 700K in PPDB-S (where the majority type is equivalent). using data motivated by the applications to information retrieval, information extraction, summarization, machine translation evaluation, and more recently, question answering (Giampiccolo et al., 2007) and essay grading (Clark et al., 2013). RTE systems vary considerably in their choice of representation and inference procedure. In the most recent shared task on RTE, some systems used deep logical representations of text, allowing them to invoke theorem provers (Bjerva et al., 2014) or Markov Logic Networks (Beltagy et al., 2014) to perform the inference, while others used shallower representations, relying on machine learning to perform inference (Lai and Hockenmaier, 2014; Zhao et al., 2014). Systems based on natural logic (MacCartney and Manning, 2007) use natural language as a representation, but still perform inference using a structured algebra rather than a statistical model. Regardless of the inference procedure, improvements to external lexical resources can improve RTE systems across the board (Clark et al., 2007). 3 The Paraphrase Database (PPDB) PPDB is currently the largest available collection of paraphr</context>
</contexts>
<marker>Beltagy, Roller, Boleda, Erk, Mooney, 2014</marker>
<rawString>Islam Beltagy, Stephen Roller, Gemma Boleda, Katrin Erk, and Raymond J Mooney. 2014. UTexas: Natural language semantics using distributional semantics and probabilistic logic. SemEval 2014, page 796.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Ido Dagan</author>
<author>Jacob Goldberger</author>
</authors>
<title>Global learning of typed entailment rules.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>610--619</pages>
<contexts>
<context position="7426" citStr="Berant et al., 2011" startWordPosition="1095" endWordPosition="1098"> task manually. Our wide range of monolingual and bilingual features results in high intrinsic accuracy. • We demonstrate improvements to a proofbased RTE system, showing that our automatic labels increase the number of proofs that it is able to find by 17%, while maintaining the same accuracy as when using goldstandard, manual labels. 2 Related Work Lexical entailment resources Approaches to paraphrase identification have exploited signal from distributional contexts (Lin and Pantel, 2001; Szpektor et al., 2004), comparable corpora (Dolan et al., 2004; Xu et al., 2014), and graph structures (Berant et al., 2011; Brockett et al., 2013). These approaches are scalable, but they often assume that all relations are equivalence relations (Madnani and Dorr, 2010). Several efforts have attempted to build or augment lexical ontologies automatically, to discover other types of lexical relations like hypernyms. Most of these approaches rely on lexico-syntactic patterns. Hearst (1992) searched for hand-written patterns (e.g. “an X is a Y”) in a large corpus in order to learn taxonomic relations between nouns. Snow et al. (2006) used dependency parses to automatically learn such patterns, which they used to augm</context>
</contexts>
<marker>Berant, Dagan, Goldberger, 2011</marker>
<rawString>Jonathan Berant, Ido Dagan, and Jacob Goldberger. 2011. Global learning of typed entailment rules. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 610–619.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rahul Bhagat</author>
<author>Eduard Hovy</author>
</authors>
<title>What is a paraphrase?</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<contexts>
<context position="2753" citStr="Bhagat and Hovy, 2013" startWordPosition="423" endWordPosition="427"> premise (top) does not entail the hypothesis (bottom), it should recognize that sparked implies caused but that in Denmark precludes in Jordan. These phrase-level entailment relationships are modeled by natural logic. is limited by the fast-and-loose treatment of the meaning of paraphrases. One concrete definition that is sometimes used for paraphrases requires that they be bidirectionally entailing (Androutsopoulos and Malakasiotis, 2010). That is, in terms of RTE, it is assumed that if P is a paraphrase of Q, then P entails Q and Q entails P. In reality, paraphrases are often more nuanced (Bhagat and Hovy, 2013), and the entries in most paraphrase resources certainly do not match this definition. For instance, Lin and Pantel (2001) extracted 12 million “inference rules” from monolingual text by exploiting shared dependency contexts. Their method learns paraphrases that are truly meaning equivalent, but it just as readily learns contradictory pairs such as hX rises, Xfallsi. Ganitkevitch et al. (2013) extract over 150 million paraphrase rules by pivoting through foreign translations. This bilingual method often learns hypernym/hyponym pairs, e.g. due to variation in the discourse structure of translat</context>
</contexts>
<marker>Bhagat, Hovy, 2013</marker>
<rawString>Rahul Bhagat and Eduard Hovy. 2013. What is a paraphrase? Computational Linguistics, 39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rahul Bhagat</author>
<author>Patrick Pantel</author>
<author>Eduard H Hovy</author>
<author>Marina Rey</author>
</authors>
<title>Ledir: An unsupervised algorithm for learning directionality of inference rules.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL,</booktitle>
<pages>161--170</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="5137" citStr="Bhagat et al., 2007" startWordPosition="733" endWordPosition="736">typhoon/storm child/mother playing/toy sunday/tour approve/to ratify seriously injure/injure in front/on the side islamic/jihad city/south alliance of/coalition between sunglasses/glasses oppose/support delay/time back/view Table 1: Examples of different types of entailment relations appearing in PPDB. Burch, 2007), and unrelated pairs, e.g. due to misalignments or polysemy in the foreign language. The unclear semantics severely limits the applicability of paraphrase resources to natural language understanding (NLU) tasks. Some efforts have been made to identify directionality of paraphrases (Bhagat et al., 2007; Kotlerman et al., 2010), but tasks like RTE require even richer semantic information. For example, in the T/H pair shown in Figure 1, a system needs information not only about equivalent words (12/twelve) and asymmetric entailments (riots/unrest), but also semantic exclusion (Denmark/Jordan). Such lexical entailment relations are captured by natural logic, a formalism which views natural language itself as a meaning representation, eschewing external representations such as First Order Logic (FOL). This is a great fit for automatically extracted paraphrases, since the phrase pairs themselves</context>
</contexts>
<marker>Bhagat, Pantel, Hovy, Rey, 2007</marker>
<rawString>Rahul Bhagat, Patrick Pantel, Eduard H Hovy, and Marina Rey. 2007. Ledir: An unsupervised algorithm for learning directionality of inference rules. In EMNLP-CoNLL, pages 161–170. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Bjerva</author>
<author>Johan Bos</author>
<author>Rob van der Goot</author>
<author>Malvina Nissim</author>
</authors>
<title>The meaning factory: Formal semantics for recognizing textual entailment and determining semantic similarity. SemEval</title>
<date>2014</date>
<pages>642</pages>
<marker>Bjerva, Bos, van der Goot, Nissim, 2014</marker>
<rawString>Johannes Bjerva, Johan Bos, Rob van der Goot, and Malvina Nissim. 2014. The meaning factory: Formal semantics for recognizing textual entailment and determining semantic similarity. SemEval 2014, page 642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
</authors>
<title>Wide-coverage semantic analysis with boxer.</title>
<date>2008</date>
<booktitle>Semantics in Text Processing. STEP 2008 Conference Proceedings, Research in Computational Semantics,</booktitle>
<pages>277--286</pages>
<editor>In Johan Bos and Rodolfo Delmonte, editors,</editor>
<publisher>College Publications.</publisher>
<contexts>
<context position="26236" citStr="Bos, 2008" startWordPosition="4231" endWordPosition="4232">, in which piano/keyboard is falsely classified as #, NC fails to find a proof and so correctly guesses NEUTRAL. Freq. Precision Recall F score # 39% 84.22 87.55 85.85 ⌘ 8% 70.36 83.07 76.19 A 26% 79.81 76.00 77.85 � 7% 73.73 73.33 73.53 ⇠ 19% 70.57 63.70 66.96 Table 7: F1 measure (⇥100) achieved by entailment classifier on the held out phrase pairs from the sentences in SICK test. In the SemEval 2014 RTE challenge, this system performed in the top 5 out of the more than 20 participating systems (Marelli et al., 2014). Given a text/hypothesis (T/H) pair, Nutcracker (NC) uses the Boxer parser (Bos, 2008) to produce a formal semantic representation of both T and H, which it translates into standard first-order logic. The logical formulae are passed to an off-the-shelf theorem prover, which searches for a logical entailment, and to a model builder, which attempts to find a logical contradiction. By default, when the system fails to find a proof for either entailment or inconsistency, it predicts the most frequent class (in our case, NEUTRAL). Therefore, NC relies heavily on lexical entailment resources in order to improve the recall of the theorem prover and model builder. Baselines The most fr</context>
</contexts>
<marker>Bos, 2008</marker>
<rawString>Johan Bos. 2008. Wide-coverage semantic analysis with boxer. In Johan Bos and Rodolfo Delmonte, editors, Semantics in Text Processing. STEP 2008 Conference Proceedings, Research in Computational Semantics, pages 277–286. College Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher John Brockett</author>
<author>Stanley Kok</author>
<author>Dengyong Zhou</author>
</authors>
<title>Locating paraphrases through utilization of a multipartite graph,</title>
<date>2013</date>
<tech>US Patent 8,484,016.</tech>
<contexts>
<context position="7450" citStr="Brockett et al., 2013" startWordPosition="1099" endWordPosition="1102">ide range of monolingual and bilingual features results in high intrinsic accuracy. • We demonstrate improvements to a proofbased RTE system, showing that our automatic labels increase the number of proofs that it is able to find by 17%, while maintaining the same accuracy as when using goldstandard, manual labels. 2 Related Work Lexical entailment resources Approaches to paraphrase identification have exploited signal from distributional contexts (Lin and Pantel, 2001; Szpektor et al., 2004), comparable corpora (Dolan et al., 2004; Xu et al., 2014), and graph structures (Berant et al., 2011; Brockett et al., 2013). These approaches are scalable, but they often assume that all relations are equivalence relations (Madnani and Dorr, 2010). Several efforts have attempted to build or augment lexical ontologies automatically, to discover other types of lexical relations like hypernyms. Most of these approaches rely on lexico-syntactic patterns. Hearst (1992) searched for hand-written patterns (e.g. “an X is a Y”) in a large corpus in order to learn taxonomic relations between nouns. Snow et al. (2006) used dependency parses to automatically learn such patterns, which they used to augment WordNet with new hyp</context>
</contexts>
<marker>Brockett, Kok, Zhou, 2013</marker>
<rawString>Christopher John Brockett, Stanley Kok, and Dengyong Zhou. 2013. Locating paraphrases through utilization of a multipartite graph, July 9. US Patent 8,484,016.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing and Translation.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh,</institution>
<location>Edinburgh, Scotland.</location>
<marker>Callison-Burch, 2007</marker>
<rawString>Chris Callison-Burch. 2007. Paraphrasing and Translation. Ph.D. thesis, University of Edinburgh, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Chklovski</author>
<author>Patrick Pantel</author>
</authors>
<title>VerbOcean: Mining the web for fine-grained semantic verb relations.</title>
<date>2004</date>
<booktitle>In EMNLP,</booktitle>
<volume>volume</volume>
<pages>33--40</pages>
<contexts>
<context position="8225" citStr="Chklovski and Pantel, 2004" startWordPosition="1215" endWordPosition="1218">tempted to build or augment lexical ontologies automatically, to discover other types of lexical relations like hypernyms. Most of these approaches rely on lexico-syntactic patterns. Hearst (1992) searched for hand-written patterns (e.g. “an X is a Y”) in a large corpus in order to learn taxonomic relations between nouns. Snow et al. (2006) used dependency parses to automatically learn such patterns, which they used to augment WordNet with new hypernym relations. Similar monolingual signals have been used to learn fine-grained relationships between verbs, such as enablement and happensbefore (Chklovski and Pantel, 2004; Hashimoto et al., 2009). Recognizing Textual Entailment The shared RTE tasks (Dagan et al., 2006) have been a springboard for research in natural language inference, 1513 Figure 2: Distribution of entailment relations in different sizes of PPDB. Distributions are estimated from our manual annotations of randomly sampled pairs. PPDB-XXXL contains over 77MM paraphrase pairs (where the majority type is independent), compared to only 700K in PPDB-S (where the majority type is equivalent). using data motivated by the applications to information retrieval, information extraction, summarization, ma</context>
</contexts>
<marker>Chklovski, Pantel, 2004</marker>
<rawString>Timothy Chklovski and Patrick Pantel. 2004. VerbOcean: Mining the web for fine-grained semantic verb relations. In EMNLP, volume 2004, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Clark</author>
<author>William R Murray</author>
<author>John Thompson</author>
<author>Phil Harrison</author>
<author>Jerry Hobbs</author>
<author>Christiane Fellbaum</author>
</authors>
<title>On the role of lexical and world knowledge in rte3.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, RTE ’07,</booktitle>
<pages>54--59</pages>
<contexts>
<context position="9757" citStr="Clark et al., 2007" startWordPosition="1446" endWordPosition="1449">text, allowing them to invoke theorem provers (Bjerva et al., 2014) or Markov Logic Networks (Beltagy et al., 2014) to perform the inference, while others used shallower representations, relying on machine learning to perform inference (Lai and Hockenmaier, 2014; Zhao et al., 2014). Systems based on natural logic (MacCartney and Manning, 2007) use natural language as a representation, but still perform inference using a structured algebra rather than a statistical model. Regardless of the inference procedure, improvements to external lexical resources can improve RTE systems across the board (Clark et al., 2007). 3 The Paraphrase Database (PPDB) PPDB is currently the largest available collection of paraphrases. Compared to other paraphrase resources such as the DIRT database (12 million rules) (Lin and Pantel, 2001) and the MSR paraphrase phrase table (13 million) (Dolan et al., 2004), PPDB contains over 150 million paraphrase rules covering three paraphrase types– lexical (single word), phrasal (multiword), and syntactic restructuring rules. We focus on lexical and phrasal paraphrases, of which there are over 77 million rules. Of these, a large fraction are true paraphrases– either equivalent (dista</context>
</contexts>
<marker>Clark, Murray, Thompson, Harrison, Hobbs, Fellbaum, 2007</marker>
<rawString>Peter Clark, William R. Murray, John Thompson, Phil Harrison, Jerry Hobbs, and Christiane Fellbaum. 2007. On the role of lexical and world knowledge in rte3. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, RTE ’07, pages 54–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Clark</author>
<author>Myroslava O Dzikovska</author>
<author>Rodney D Nielsen</author>
<author>Chris Brew</author>
<author>Claudia Leacock</author>
<author>Danilo Giampiccolo</author>
<author>Luisa Bentivogli</author>
<author>Ido Dagan</author>
<author>Hoa T Dang</author>
</authors>
<title>Semeval-2013 task 7: The joint student response analysis and 8th recognizing textual entailment challenge.</title>
<date>2013</date>
<contexts>
<context position="8958" citStr="Clark et al., 2013" startWordPosition="1322" endWordPosition="1325">ringboard for research in natural language inference, 1513 Figure 2: Distribution of entailment relations in different sizes of PPDB. Distributions are estimated from our manual annotations of randomly sampled pairs. PPDB-XXXL contains over 77MM paraphrase pairs (where the majority type is independent), compared to only 700K in PPDB-S (where the majority type is equivalent). using data motivated by the applications to information retrieval, information extraction, summarization, machine translation evaluation, and more recently, question answering (Giampiccolo et al., 2007) and essay grading (Clark et al., 2013). RTE systems vary considerably in their choice of representation and inference procedure. In the most recent shared task on RTE, some systems used deep logical representations of text, allowing them to invoke theorem provers (Bjerva et al., 2014) or Markov Logic Networks (Beltagy et al., 2014) to perform the inference, while others used shallower representations, relying on machine learning to perform inference (Lai and Hockenmaier, 2014; Zhao et al., 2014). Systems based on natural logic (MacCartney and Manning, 2007) use natural language as a representation, but still perform inference usin</context>
</contexts>
<marker>Clark, Dzikovska, Nielsen, Brew, Leacock, Giampiccolo, Bentivogli, Dagan, Dang, 2013</marker>
<rawString>Peter Clark, Myroslava O Dzikovska, Rodney D Nielsen, Chris Brew, Claudia Leacock, Danilo Giampiccolo, Luisa Bentivogli, Ido Dagan, and Hoa T Dang. 2013. Semeval-2013 task 7: The joint student response analysis and 8th recognizing textual entailment challenge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daoud Clarke</author>
</authors>
<title>Context-theoretic semantics for natural language: an overview.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics,</booktitle>
<pages>112--119</pages>
<contexts>
<context position="19690" citStr="Clarke, 2009" startWordPosition="3096" endWordPosition="3097">by in X and in Y in foods and in beverages separate X from Y separate the old from the young to X and/or to Y to the left or to the right from X to Y from 7 a.m. to 10 p.m. more/less X than Y more harm than good Table 4: Top paths associated with the - class. the same words. Given context vectors, Lin and Pantel (2001) used a symmetric similarity metric (Lin, 1998) to find candidate paraphrases. We build dependency context vectors for each word in our data and compute both symmetric as well as more recently proposed asymmetric similarity measures (Weeds et al., 2004; Szpektor and Dagan, 2008; Clarke, 2009), which are potentially better suited for identifying A paraphrases. Table 3 gives a comparison of the pairs which are considered “most similar” according to several of these metrics. 6.2 Bilingual features We explore a variety of bilingual features, which we expect to provide complimentary signal to the monolingual features. Each pair in PPDB is associated with several paraphrase probabilities, which are based on the probabilities of aligning each word to the foreign “pivot” phrase (a foreign translation shared by the two phrases), computed as described in Bannard and Callison-Burch (2005). W</context>
</contexts>
<marker>Clarke, 2009</marker>
<rawString>Daoud Clarke. 2009. Context-theoretic semantics for natural language: an overview. In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, pages 112–119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The pascal recognising textual entailment challenge. In Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment,</title>
<date>2006</date>
<pages>177--190</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="8324" citStr="Dagan et al., 2006" startWordPosition="1230" endWordPosition="1233"> like hypernyms. Most of these approaches rely on lexico-syntactic patterns. Hearst (1992) searched for hand-written patterns (e.g. “an X is a Y”) in a large corpus in order to learn taxonomic relations between nouns. Snow et al. (2006) used dependency parses to automatically learn such patterns, which they used to augment WordNet with new hypernym relations. Similar monolingual signals have been used to learn fine-grained relationships between verbs, such as enablement and happensbefore (Chklovski and Pantel, 2004; Hashimoto et al., 2009). Recognizing Textual Entailment The shared RTE tasks (Dagan et al., 2006) have been a springboard for research in natural language inference, 1513 Figure 2: Distribution of entailment relations in different sizes of PPDB. Distributions are estimated from our manual annotations of randomly sampled pairs. PPDB-XXXL contains over 77MM paraphrase pairs (where the majority type is independent), compared to only 700K in PPDB-S (where the majority type is equivalent). using data motivated by the applications to information retrieval, information extraction, summarization, machine translation evaluation, and more recently, question answering (Giampiccolo et al., 2007) and </context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The pascal recognising textual entailment challenge. In Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment, pages 177–190. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>350</pages>
<contexts>
<context position="1966" citStr="Dolan et al., 2004" startWordPosition="292" endWordPosition="295">sk of recognizing textual entailment (RTE). In RTE, a system is given two pieces of text, often called the text (T) and the hypothesis (H), and asked to determine whether T entails H, T contradicts H, or T and H are unrelatable (Figure 1). In contrast, data-driving paraphrasing typically sidesteps developing a clear definition of “meaning the same thing” and instead “assume[s] paraphrasing is a coherent notion and concentrate[s] on devices that can produce paraphrases” (Barzilay, 2003). Recent work on paraphrase extraction has resulted in enormous paraphrase collections (Lin and Pantel, 2001; Dolan et al., 2004; Ganitkevitch et al., 2013), but the usefulness of these collections Figure 1: An example sentence pair for the RTE task. In order for a system to conclude that the premise (top) does not entail the hypothesis (bottom), it should recognize that sparked implies caused but that in Denmark precludes in Jordan. These phrase-level entailment relationships are modeled by natural logic. is limited by the fast-and-loose treatment of the meaning of paraphrases. One concrete definition that is sometimes used for paraphrases requires that they be bidirectionally entailing (Androutsopoulos and Malakasiot</context>
<context position="7365" citStr="Dolan et al., 2004" startWordPosition="1084" endWordPosition="1087">7 million phrase pairs!– makes it impossible to perform this task manually. Our wide range of monolingual and bilingual features results in high intrinsic accuracy. • We demonstrate improvements to a proofbased RTE system, showing that our automatic labels increase the number of proofs that it is able to find by 17%, while maintaining the same accuracy as when using goldstandard, manual labels. 2 Related Work Lexical entailment resources Approaches to paraphrase identification have exploited signal from distributional contexts (Lin and Pantel, 2001; Szpektor et al., 2004), comparable corpora (Dolan et al., 2004; Xu et al., 2014), and graph structures (Berant et al., 2011; Brockett et al., 2013). These approaches are scalable, but they often assume that all relations are equivalence relations (Madnani and Dorr, 2010). Several efforts have attempted to build or augment lexical ontologies automatically, to discover other types of lexical relations like hypernyms. Most of these approaches rely on lexico-syntactic patterns. Hearst (1992) searched for hand-written patterns (e.g. “an X is a Y”) in a large corpus in order to learn taxonomic relations between nouns. Snow et al. (2006) used dependency parses </context>
<context position="10035" citStr="Dolan et al., 2004" startWordPosition="1490" endWordPosition="1493">014). Systems based on natural logic (MacCartney and Manning, 2007) use natural language as a representation, but still perform inference using a structured algebra rather than a statistical model. Regardless of the inference procedure, improvements to external lexical resources can improve RTE systems across the board (Clark et al., 2007). 3 The Paraphrase Database (PPDB) PPDB is currently the largest available collection of paraphrases. Compared to other paraphrase resources such as the DIRT database (12 million rules) (Lin and Pantel, 2001) and the MSR paraphrase phrase table (13 million) (Dolan et al., 2004), PPDB contains over 150 million paraphrase rules covering three paraphrase types– lexical (single word), phrasal (multiword), and syntactic restructuring rules. We focus on lexical and phrasal paraphrases, of which there are over 77 million rules. Of these, a large fraction are true paraphrases– either equivalent (distant/remote) or asymmetric entailment (girl/little girl)– but many are not. PPDB contains some pairs which are related by semantic exclusion (nobody/someone), some of which are related by something other than entailment (swim/water), and some which are simply unrelated (car/famil</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In Proceedings of the 20th international conference on Computational Linguistics, page 350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juri Ganitkevitch</author>
<author>Chris Callison-Burch</author>
</authors>
<title>The multilingual paraphrase database.</title>
<date>2014</date>
<journal>European Language Resources Association.</journal>
<booktitle>In The 9th edition of the Language Resources and Evaluation Conference,</booktitle>
<location>Reykjavik, Iceland,</location>
<marker>Ganitkevitch, Callison-Burch, 2014</marker>
<rawString>Juri Ganitkevitch and Chris Callison-Burch. 2014. The multilingual paraphrase database. In The 9th edition of the Language Resources and Evaluation Conference, Reykjavik, Iceland, May. European Language Resources Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juri Ganitkevitch</author>
<author>Benjamin Van Durme</author>
<author>Chris Callison-Burch</author>
</authors>
<title>PPDB: The paraphrase database.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>758--764</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<marker>Ganitkevitch, Van Durme, Callison-Burch, 2013</marker>
<rawString>Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch. 2013. PPDB: The paraphrase database. In Proceedings of NAACL-HLT, pages 758–764, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
</authors>
<title>The third PASCAL recognizing textual entailment challenge.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="8919" citStr="Giampiccolo et al., 2007" startWordPosition="1315" endWordPosition="1318">RTE tasks (Dagan et al., 2006) have been a springboard for research in natural language inference, 1513 Figure 2: Distribution of entailment relations in different sizes of PPDB. Distributions are estimated from our manual annotations of randomly sampled pairs. PPDB-XXXL contains over 77MM paraphrase pairs (where the majority type is independent), compared to only 700K in PPDB-S (where the majority type is equivalent). using data motivated by the applications to information retrieval, information extraction, summarization, machine translation evaluation, and more recently, question answering (Giampiccolo et al., 2007) and essay grading (Clark et al., 2013). RTE systems vary considerably in their choice of representation and inference procedure. In the most recent shared task on RTE, some systems used deep logical representations of text, allowing them to invoke theorem provers (Bjerva et al., 2014) or Markov Logic Networks (Beltagy et al., 2014) to perform the inference, while others used shallower representations, relying on machine learning to perform inference (Lai and Hockenmaier, 2014; Zhao et al., 2014). Systems based on natural logic (MacCartney and Manning, 2007) use natural language as a represent</context>
</contexts>
<marker>Giampiccolo, Magnini, Dagan, Dolan, 2007</marker>
<rawString>Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2007. The third PASCAL recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chikara Hashimoto</author>
<author>Kentaro Torisawa</author>
<author>Kow Kuroda</author>
<author>Stijn De Saeger</author>
<author>Masaki Murata</author>
<author>Jun’ichi Kazama</author>
</authors>
<title>Large-scale verb entailment acquisition from the web.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>3</volume>
<pages>1172--1181</pages>
<institution>Association for Computational Linguistics.</institution>
<marker>Hashimoto, Torisawa, Kuroda, De Saeger, Murata, Kazama, 2009</marker>
<rawString>Chikara Hashimoto, Kentaro Torisawa, Kow Kuroda, Stijn De Saeger, Masaki Murata, and Jun’ichi Kazama. 2009. Large-scale verb entailment acquisition from the web. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3-Volume 3, pages 1172– 1181. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th Conference on Computational Linguistics -Volume 2, COLING ’92,</booktitle>
<pages>539--545</pages>
<contexts>
<context position="7795" citStr="Hearst (1992)" startWordPosition="1150" endWordPosition="1151">ources Approaches to paraphrase identification have exploited signal from distributional contexts (Lin and Pantel, 2001; Szpektor et al., 2004), comparable corpora (Dolan et al., 2004; Xu et al., 2014), and graph structures (Berant et al., 2011; Brockett et al., 2013). These approaches are scalable, but they often assume that all relations are equivalence relations (Madnani and Dorr, 2010). Several efforts have attempted to build or augment lexical ontologies automatically, to discover other types of lexical relations like hypernyms. Most of these approaches rely on lexico-syntactic patterns. Hearst (1992) searched for hand-written patterns (e.g. “an X is a Y”) in a large corpus in order to learn taxonomic relations between nouns. Snow et al. (2006) used dependency parses to automatically learn such patterns, which they used to augment WordNet with new hypernym relations. Similar monolingual signals have been used to learn fine-grained relationships between verbs, such as enablement and happensbefore (Chklovski and Pantel, 2004; Hashimoto et al., 2009). Recognizing Textual Entailment The shared RTE tasks (Dagan et al., 2006) have been a springboard for research in natural language inference, 15</context>
<context position="18346" citStr="Hearst (1992)" startWordPosition="2851" endWordPosition="2852">summarized in Figure 3). We categorize the remaining features into two broad groups: monolingual features, which are based on observed usage in the Annotated Gigaword corpus (Napoles et al., 2012), and bilingual features, which are based on translation probabilities observed in bilingual parallel corpora. Full descriptions of all the features used are provided in the supplementary material. 6.1 Monolingual features Path features Snow et al. (2004) used lexicosyntactic patterns to mine taxonomic relations (hypernyms and hyponyms) between noun pairs. They were able to verify the earlier work of Hearst (1992) which found that certain patterns, e.g. X and other Y, are strong indicators of hypernymy. Using similar path features, we learn new patterns to differentiate between more subtle relations. For example, we learn the pattern separate X from Y is highly indicative of the -, relation. We learn that the pattern X including Y suggests A more than it suggests - whereas the pattern X known as Y suggests - more than A. Table 4 gives examples of some of the paths most indicative of the -, relation. Distributional features Lin and Pantel (2001) attempted to mine inference rules from text by finding pat</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th Conference on Computational Linguistics -Volume 2, COLING ’92, pages 539–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aaron N Kaplan</author>
<author>Lenhart K Schubert</author>
</authors>
<title>Measuring and improving the quality of world knowledge extracted from wordnet.</title>
<date>2001</date>
<institution>University of Rochester,</institution>
<location>Rochester, NY.</location>
<contexts>
<context position="6039" citStr="Kaplan and Schubert, 2001" startWordPosition="869" endWordPosition="873">n (Denmark/Jordan). Such lexical entailment relations are captured by natural logic, a formalism which views natural language itself as a meaning representation, eschewing external representations such as First Order Logic (FOL). This is a great fit for automatically extracted paraphrases, since the phrase pairs themselves can be used as the semantic representation with minimal additional annotation. But as is, paraphrase resources lack such annotation. As a result, NLU systems rely on manually built resources like WordNet, which are limited in coverage and often lead to incorrect inferences (Kaplan and Schubert, 2001). In fact, in the most recent RTE challenge, over half of the submitted systems used WordNet (Pontiki et al., 2014). Even the NatLog system (MacCartney and Manning, 2007), which popularized natural logic for RTE, relied on WordNet and did not solve the problem of assigning natural logic relations at scale. The main contributions of this paper are: • We add a concrete, interpretable semantics to the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013), the largest paraphrase resource currently available. We give each entry in the database a label describing the entailment relationship between</context>
</contexts>
<marker>Kaplan, Schubert, 2001</marker>
<rawString>Aaron N Kaplan and Lenhart K Schubert. 2001. Measuring and improving the quality of world knowledge extracted from wordnet. University of Rochester, Rochester, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lili Kotlerman</author>
<author>Ido Dagan</author>
<author>Idan Szpektor</author>
<author>Maayan Zhitomirsky-Geffet</author>
</authors>
<title>Directional distributional similarity for lexical inference.</title>
<date>2010</date>
<journal>Natural Language Engineering,</journal>
<volume>16</volume>
<issue>4</issue>
<contexts>
<context position="5162" citStr="Kotlerman et al., 2010" startWordPosition="737" endWordPosition="740">other playing/toy sunday/tour approve/to ratify seriously injure/injure in front/on the side islamic/jihad city/south alliance of/coalition between sunglasses/glasses oppose/support delay/time back/view Table 1: Examples of different types of entailment relations appearing in PPDB. Burch, 2007), and unrelated pairs, e.g. due to misalignments or polysemy in the foreign language. The unclear semantics severely limits the applicability of paraphrase resources to natural language understanding (NLU) tasks. Some efforts have been made to identify directionality of paraphrases (Bhagat et al., 2007; Kotlerman et al., 2010), but tasks like RTE require even richer semantic information. For example, in the T/H pair shown in Figure 1, a system needs information not only about equivalent words (12/twelve) and asymmetric entailments (riots/unrest), but also semantic exclusion (Denmark/Jordan). Such lexical entailment relations are captured by natural logic, a formalism which views natural language itself as a meaning representation, eschewing external representations such as First Order Logic (FOL). This is a great fit for automatically extracted paraphrases, since the phrase pairs themselves can be used as the seman</context>
</contexts>
<marker>Kotlerman, Dagan, Szpektor, Zhitomirsky-Geffet, 2010</marker>
<rawString>Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-Geffet. 2010. Directional distributional similarity for lexical inference. Natural Language Engineering, 16(4):359–389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alice Lai</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Illinois-LH: A denotational and distributional approach to semantics. SemEval</title>
<date>2014</date>
<pages>329</pages>
<contexts>
<context position="9400" citStr="Lai and Hockenmaier, 2014" startWordPosition="1391" endWordPosition="1394">trieval, information extraction, summarization, machine translation evaluation, and more recently, question answering (Giampiccolo et al., 2007) and essay grading (Clark et al., 2013). RTE systems vary considerably in their choice of representation and inference procedure. In the most recent shared task on RTE, some systems used deep logical representations of text, allowing them to invoke theorem provers (Bjerva et al., 2014) or Markov Logic Networks (Beltagy et al., 2014) to perform the inference, while others used shallower representations, relying on machine learning to perform inference (Lai and Hockenmaier, 2014; Zhao et al., 2014). Systems based on natural logic (MacCartney and Manning, 2007) use natural language as a representation, but still perform inference using a structured algebra rather than a statistical model. Regardless of the inference procedure, improvements to external lexical resources can improve RTE systems across the board (Clark et al., 2007). 3 The Paraphrase Database (PPDB) PPDB is currently the largest available collection of paraphrases. Compared to other paraphrase resources such as the DIRT database (12 million rules) (Lin and Pantel, 2001) and the MSR paraphrase phrase tabl</context>
</contexts>
<marker>Lai, Hockenmaier, 2014</marker>
<rawString>Alice Lai and Julia Hockenmaier. 2014. Illinois-LH: A denotational and distributional approach to semantics. SemEval 2014, page 329.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Richard Landis</author>
<author>Gary G Koch</author>
</authors>
<title>The measurement of observer agreement for categorical data. biometrics,</title>
<date>1977</date>
<pages>159--174</pages>
<contexts>
<context position="17215" citStr="Landis and Koch, 1977" startWordPosition="2669" endWordPosition="2672">/child # each other/man different/same A child/three children ⌘ a lot of/many A picture/drawing other/same ⌘ is playing/play ⌘ female/woman ⇠ practice/target put/take A side/both sides ⌘ male/man Table 3: Top scoring pairs (x/y) according to various similarity measures, along with their manually classified entailment labels. Column 1 is cosine similarity based on dependency contexts. Column 2 is based on Lin (1998), column 3 on Weeds (2004), and column 4 is a novel feature. Precise definitions of each metric are given in the supplementary material. ate levels of agreement (Fleiss’s n = 0.56) (Landis and Koch, 1977). For a fuller discussion of the annotation, refer to the supplementary material. 6 Automatic Classification We aim to build a classifier to automatically assign entailment types to entries in the PPDB, and to demonstrate that it performs well both intrinsically and extrinsically. We fix the direction of the @ and A relations to create a single class and train a logistic regression classifier to distinguish between the 5 classes {#, -, A, -,, ⇠1. We compute variety of basic lexical features and WordNet features (summarized in Figure 3). We categorize the remaining features into two broad group</context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>J Richard Landis and Gary G Koch. 1977. The measurement of observer agreement for categorical data. biometrics, pages 159–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>DIRT – Discovery of Inference Rules from Text.</title>
<date>2001</date>
<booktitle>In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>323--328</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1946" citStr="Lin and Pantel, 2001" startWordPosition="287" endWordPosition="291">the closely related task of recognizing textual entailment (RTE). In RTE, a system is given two pieces of text, often called the text (T) and the hypothesis (H), and asked to determine whether T entails H, T contradicts H, or T and H are unrelatable (Figure 1). In contrast, data-driving paraphrasing typically sidesteps developing a clear definition of “meaning the same thing” and instead “assume[s] paraphrasing is a coherent notion and concentrate[s] on devices that can produce paraphrases” (Barzilay, 2003). Recent work on paraphrase extraction has resulted in enormous paraphrase collections (Lin and Pantel, 2001; Dolan et al., 2004; Ganitkevitch et al., 2013), but the usefulness of these collections Figure 1: An example sentence pair for the RTE task. In order for a system to conclude that the premise (top) does not entail the hypothesis (bottom), it should recognize that sparked implies caused but that in Denmark precludes in Jordan. These phrase-level entailment relationships are modeled by natural logic. is limited by the fast-and-loose treatment of the meaning of paraphrases. One concrete definition that is sometimes used for paraphrases requires that they be bidirectionally entailing (Androutsop</context>
<context position="7301" citStr="Lin and Pantel, 2001" startWordPosition="1074" endWordPosition="1077">odel to predict these relations. The enormous size of PPDB– over 77 million phrase pairs!– makes it impossible to perform this task manually. Our wide range of monolingual and bilingual features results in high intrinsic accuracy. • We demonstrate improvements to a proofbased RTE system, showing that our automatic labels increase the number of proofs that it is able to find by 17%, while maintaining the same accuracy as when using goldstandard, manual labels. 2 Related Work Lexical entailment resources Approaches to paraphrase identification have exploited signal from distributional contexts (Lin and Pantel, 2001; Szpektor et al., 2004), comparable corpora (Dolan et al., 2004; Xu et al., 2014), and graph structures (Berant et al., 2011; Brockett et al., 2013). These approaches are scalable, but they often assume that all relations are equivalence relations (Madnani and Dorr, 2010). Several efforts have attempted to build or augment lexical ontologies automatically, to discover other types of lexical relations like hypernyms. Most of these approaches rely on lexico-syntactic patterns. Hearst (1992) searched for hand-written patterns (e.g. “an X is a Y”) in a large corpus in order to learn taxonomic rel</context>
<context position="9965" citStr="Lin and Pantel, 2001" startWordPosition="1478" endWordPosition="1481">learning to perform inference (Lai and Hockenmaier, 2014; Zhao et al., 2014). Systems based on natural logic (MacCartney and Manning, 2007) use natural language as a representation, but still perform inference using a structured algebra rather than a statistical model. Regardless of the inference procedure, improvements to external lexical resources can improve RTE systems across the board (Clark et al., 2007). 3 The Paraphrase Database (PPDB) PPDB is currently the largest available collection of paraphrases. Compared to other paraphrase resources such as the DIRT database (12 million rules) (Lin and Pantel, 2001) and the MSR paraphrase phrase table (13 million) (Dolan et al., 2004), PPDB contains over 150 million paraphrase rules covering three paraphrase types– lexical (single word), phrasal (multiword), and syntactic restructuring rules. We focus on lexical and phrasal paraphrases, of which there are over 77 million rules. Of these, a large fraction are true paraphrases– either equivalent (distant/remote) or asymmetric entailment (girl/little girl)– but many are not. PPDB contains some pairs which are related by semantic exclusion (nobody/someone), some of which are related by something other than e</context>
<context position="18887" citStr="Lin and Pantel (2001)" startWordPosition="2944" endWordPosition="2947">) between noun pairs. They were able to verify the earlier work of Hearst (1992) which found that certain patterns, e.g. X and other Y, are strong indicators of hypernymy. Using similar path features, we learn new patterns to differentiate between more subtle relations. For example, we learn the pattern separate X from Y is highly indicative of the -, relation. We learn that the pattern X including Y suggests A more than it suggests - whereas the pattern X known as Y suggests - more than A. Table 4 gives examples of some of the paths most indicative of the -, relation. Distributional features Lin and Pantel (2001) attempted to mine inference rules from text by finding paths in a dependency tree which connect the same nouns. The intuition is that good paraphrases should tend to modify and be modified by in X and in Y in foods and in beverages separate X from Y separate the old from the young to X and/or to Y to the left or to the right from X to Y from 7 a.m. to 10 p.m. more/less X than Y more harm than good Table 4: Top paths associated with the - class. the same words. Given context vectors, Lin and Pantel (2001) used a symmetric similarity metric (Lin, 1998) to find candidate paraphrases. We build de</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. DIRT – Discovery of Inference Rules from Text. In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining, pages 323–328. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th international conference on Computational linguisticsVolume 2,</booktitle>
<pages>768--774</pages>
<contexts>
<context position="17011" citStr="Lin (1998)" startWordPosition="2636" endWordPosition="2637">Cosine Similarity Monolingual (symmetric) Monolingual (asymmetric) Bilingual A shades/the shade large/small A boy/little boy ⌘ dad/father A yard/backyard ⌘ few/several A man/two men A some kid/child # each other/man different/same A child/three children ⌘ a lot of/many A picture/drawing other/same ⌘ is playing/play ⌘ female/woman ⇠ practice/target put/take A side/both sides ⌘ male/man Table 3: Top scoring pairs (x/y) according to various similarity measures, along with their manually classified entailment labels. Column 1 is cosine similarity based on dependency contexts. Column 2 is based on Lin (1998), column 3 on Weeds (2004), and column 4 is a novel feature. Precise definitions of each metric are given in the supplementary material. ate levels of agreement (Fleiss’s n = 0.56) (Landis and Koch, 1977). For a fuller discussion of the annotation, refer to the supplementary material. 6 Automatic Classification We aim to build a classifier to automatically assign entailment types to entries in the PPDB, and to demonstrate that it performs well both intrinsically and extrinsically. We fix the direction of the @ and A relations to create a single class and train a logistic regression classifier </context>
<context position="19444" citStr="Lin, 1998" startWordPosition="3057" endWordPosition="3058">lation. Distributional features Lin and Pantel (2001) attempted to mine inference rules from text by finding paths in a dependency tree which connect the same nouns. The intuition is that good paraphrases should tend to modify and be modified by in X and in Y in foods and in beverages separate X from Y separate the old from the young to X and/or to Y to the left or to the right from X to Y from 7 a.m. to 10 p.m. more/less X than Y more harm than good Table 4: Top paths associated with the - class. the same words. Given context vectors, Lin and Pantel (2001) used a symmetric similarity metric (Lin, 1998) to find candidate paraphrases. We build dependency context vectors for each word in our data and compute both symmetric as well as more recently proposed asymmetric similarity measures (Weeds et al., 2004; Szpektor and Dagan, 2008; Clarke, 2009), which are potentially better suited for identifying A paraphrases. Table 3 gives a comparison of the pairs which are considered “most similar” according to several of these metrics. 6.2 Bilingual features We explore a variety of bilingual features, which we expect to provide complimentary signal to the monolingual features. Each pair in PPDB is assoc</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th international conference on Computational linguisticsVolume 2, pages 768–774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Natural logic for textual inference.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, RTE ’07,</booktitle>
<pages>193--200</pages>
<contexts>
<context position="6209" citStr="MacCartney and Manning, 2007" startWordPosition="898" endWordPosition="901">wing external representations such as First Order Logic (FOL). This is a great fit for automatically extracted paraphrases, since the phrase pairs themselves can be used as the semantic representation with minimal additional annotation. But as is, paraphrase resources lack such annotation. As a result, NLU systems rely on manually built resources like WordNet, which are limited in coverage and often lead to incorrect inferences (Kaplan and Schubert, 2001). In fact, in the most recent RTE challenge, over half of the submitted systems used WordNet (Pontiki et al., 2014). Even the NatLog system (MacCartney and Manning, 2007), which popularized natural logic for RTE, relied on WordNet and did not solve the problem of assigning natural logic relations at scale. The main contributions of this paper are: • We add a concrete, interpretable semantics to the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013), the largest paraphrase resource currently available. We give each entry in the database a label describing the entailment relationship between the phrases. • We develop a statistical model to predict these relations. The enormous size of PPDB– over 77 million phrase pairs!– makes it impossible to perform this t</context>
<context position="9483" citStr="MacCartney and Manning, 2007" startWordPosition="1404" endWordPosition="1407"> and more recently, question answering (Giampiccolo et al., 2007) and essay grading (Clark et al., 2013). RTE systems vary considerably in their choice of representation and inference procedure. In the most recent shared task on RTE, some systems used deep logical representations of text, allowing them to invoke theorem provers (Bjerva et al., 2014) or Markov Logic Networks (Beltagy et al., 2014) to perform the inference, while others used shallower representations, relying on machine learning to perform inference (Lai and Hockenmaier, 2014; Zhao et al., 2014). Systems based on natural logic (MacCartney and Manning, 2007) use natural language as a representation, but still perform inference using a structured algebra rather than a statistical model. Regardless of the inference procedure, improvements to external lexical resources can improve RTE systems across the board (Clark et al., 2007). 3 The Paraphrase Database (PPDB) PPDB is currently the largest available collection of paraphrases. Compared to other paraphrase resources such as the DIRT database (12 million rules) (Lin and Pantel, 2001) and the MSR paraphrase phrase table (13 million) (Dolan et al., 2004), PPDB contains over 150 million paraphrase rule</context>
</contexts>
<marker>MacCartney, Manning, 2007</marker>
<rawString>Bill MacCartney and Christopher D. Manning. 2007. Natural logic for textual inference. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, RTE ’07, pages 193–200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
</authors>
<title>Natural language inference.</title>
<date>2009</date>
<tech>Ph.D. thesis, Citeseer.</tech>
<contexts>
<context position="14367" citStr="MacCartney, 2009" startWordPosition="2205" endWordPosition="2206"> The SICK data has a relatively small vocabulary, with 86% of words types and &lt;1% of the phrase types covered by WordNet. Still, over half of the words in SICK which are covered by PPDB do not appear in WordNet. In general, PPDB covers a much larger vocabulary (1.6MM words) than does WordNet (155K words), and we expect the potential benefit of using PPDB in addition to or in place of WordNet to be larger on datasets with richer vocabularies. 5 Entailment Relations We use the relations from Bill MacCartney’s thesis on natural language inference as the basis for our categorization of relations (MacCartney, 2009). He outlines 7 basic entailment relationships:2 Equivalence (P=Q): Vx[P(x) H Q(x)] Forward Entailment (P@Q): Vx[P(x) Q(x)] Reverse Entailment (PAQ): Vx[Q(x) P(x)] Negation (PˆQ): Vx [P(x) H -, Q(x)] Alternation (P|Q): Vx -,[P(x) A Q(x)] Cover (P—Q): Vx[P(x) V Q(x)] Independence (P#Q): All other cases. 2To further clarify the definitions here: “negation” is XOR (exclusive disjunction), “alternation” is NAND, and “cover” is OR (inclusive disjunction) These relations are based on the theory of natural logic, meaning they are defined between pairs of natural language expressions rather than requi</context>
</contexts>
<marker>MacCartney, 2009</marker>
<rawString>Bill MacCartney. 2009. Natural language inference. Ph.D. thesis, Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Bonnie J Dorr</author>
</authors>
<title>Generating phrasal and sentential paraphrases: A survey of datadriven methods.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<contexts>
<context position="7574" citStr="Madnani and Dorr, 2010" startWordPosition="1117" endWordPosition="1120">fbased RTE system, showing that our automatic labels increase the number of proofs that it is able to find by 17%, while maintaining the same accuracy as when using goldstandard, manual labels. 2 Related Work Lexical entailment resources Approaches to paraphrase identification have exploited signal from distributional contexts (Lin and Pantel, 2001; Szpektor et al., 2004), comparable corpora (Dolan et al., 2004; Xu et al., 2014), and graph structures (Berant et al., 2011; Brockett et al., 2013). These approaches are scalable, but they often assume that all relations are equivalence relations (Madnani and Dorr, 2010). Several efforts have attempted to build or augment lexical ontologies automatically, to discover other types of lexical relations like hypernyms. Most of these approaches rely on lexico-syntactic patterns. Hearst (1992) searched for hand-written patterns (e.g. “an X is a Y”) in a large corpus in order to learn taxonomic relations between nouns. Snow et al. (2006) used dependency parses to automatically learn such patterns, which they used to augment WordNet with new hypernym relations. Similar monolingual signals have been used to learn fine-grained relationships between verbs, such as enabl</context>
</contexts>
<marker>Madnani, Dorr, 2010</marker>
<rawString>Nitin Madnani and Bonnie J. Dorr. 2010. Generating phrasal and sentential paraphrases: A survey of datadriven methods. Computational Linguistics, 36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Marelli</author>
<author>Luisa Bentivogli</author>
<author>Marco Baroni</author>
<author>Raffaella Bernardi</author>
<author>Stefano Menini</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment.</title>
<date>2014</date>
<tech>SemEval-2014.</tech>
<contexts>
<context position="11368" citStr="Marelli et al., 2014" startWordPosition="1708" endWordPosition="1711"> (S, M, L, XL, XXL and XXXL), which fall roughly on a continuum from highest precision and lowest recall to lowest average precision and highest recall. Figure 2 shows how the distribution of entailment relations differs across the sizes of PPDB.1 Our goal is to make these relations explicit, by providing annotations for each phrase pair. Because of the enormous scale of PPDB, this annotation must be done automatically. 4 Selection of Paraphrases In this paper we focus on paraphrases pairs from PPDB that occur in RTE data. We use the recent SICK dataset from in the 2014 SemEval RTE challenge (Marelli et al., 2014) for our experiments. The data consists of 10K sentences split roughly evenly into training and testing sets. The sentence pairs are labeled using a 3-way entailment classification: ENTAILMENT, (29%) CONTRADICTION (15%), or NEUTRAL (56%). We consider all phrase pairs from PPDB (p1, p2) up to three words in length such that there is some T/H sentence pair in which p1 appears in T and p2 appears 1These distributions were estimated based on a random sample of pairs drawn from each size of PPDB, annotated on MTurk as described in Section 5 1514 Lexical We use the lemmas, POS tags, and phrase lengt</context>
<context position="26149" citStr="Marelli et al., 2014" startWordPosition="4216" endWordPosition="4219">istency for the pair Someone is not playing piano./A person is playing a keyboard. Using the PPDB+, in which piano/keyboard is falsely classified as #, NC fails to find a proof and so correctly guesses NEUTRAL. Freq. Precision Recall F score # 39% 84.22 87.55 85.85 ⌘ 8% 70.36 83.07 76.19 A 26% 79.81 76.00 77.85 � 7% 73.73 73.33 73.53 ⇠ 19% 70.57 63.70 66.96 Table 7: F1 measure (⇥100) achieved by entailment classifier on the held out phrase pairs from the sentences in SICK test. In the SemEval 2014 RTE challenge, this system performed in the top 5 out of the more than 20 participating systems (Marelli et al., 2014). Given a text/hypothesis (T/H) pair, Nutcracker (NC) uses the Boxer parser (Bos, 2008) to produce a formal semantic representation of both T and H, which it translates into standard first-order logic. The logical formulae are passed to an off-the-shelf theorem prover, which searches for a logical entailment, and to a model builder, which attempts to find a logical contradiction. By default, when the system fails to find a proof for either entailment or inconsistency, it predicts the most frequent class (in our case, NEUTRAL). Therefore, NC relies heavily on lexical entailment resources in ord</context>
</contexts>
<marker>Marelli, Bentivogli, Baroni, Bernardi, Menini, Zamparelli, 2014</marker>
<rawString>Marco Marelli, Luisa Bentivogli, Marco Baroni, Raffaella Bernardi, Stefano Menini, and Roberto Zamparelli. 2014. Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. SemEval-2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Courtney Napoles</author>
<author>Matthew Gormley</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Annotated gigaword.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction,</booktitle>
<pages>95--100</pages>
<marker>Napoles, Gormley, Van Durme, 2012</marker>
<rawString>Courtney Napoles, Matthew Gormley, and Benjamin Van Durme. 2012. Annotated gigaword. In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction, pages 95–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Pontiki</author>
</authors>
<title>Haris Papageorgiou, Dimitrios Galanis, Ion Androutsopoulos, John Pavlopoulos, and Suresh Manandhar.</title>
<date>2014</date>
<booktitle>Proceedings of SemEval,</booktitle>
<location>Dublin, Ireland.</location>
<marker>Pontiki, 2014</marker>
<rawString>Maria Pontiki, Haris Papageorgiou, Dimitrios Galanis, Ion Androutsopoulos, John Pavlopoulos, and Suresh Manandhar. 2014. Semeval-2014 task 4: Aspect based sentiment analysis. Proceedings of SemEval, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning syntactic patterns for automatic hypernym discovery.</title>
<date>2004</date>
<booktitle>In NIPS,</booktitle>
<volume>17</volume>
<pages>1297--1304</pages>
<contexts>
<context position="18184" citStr="Snow et al. (2004)" startWordPosition="2824" endWordPosition="2827">s and train a logistic regression classifier to distinguish between the 5 classes {#, -, A, -,, ⇠1. We compute variety of basic lexical features and WordNet features (summarized in Figure 3). We categorize the remaining features into two broad groups: monolingual features, which are based on observed usage in the Annotated Gigaword corpus (Napoles et al., 2012), and bilingual features, which are based on translation probabilities observed in bilingual parallel corpora. Full descriptions of all the features used are provided in the supplementary material. 6.1 Monolingual features Path features Snow et al. (2004) used lexicosyntactic patterns to mine taxonomic relations (hypernyms and hyponyms) between noun pairs. They were able to verify the earlier work of Hearst (1992) which found that certain patterns, e.g. X and other Y, are strong indicators of hypernymy. Using similar path features, we learn new patterns to differentiate between more subtle relations. For example, we learn the pattern separate X from Y is highly indicative of the -, relation. We learn that the pattern X including Y suggests A more than it suggests - whereas the pattern X known as Y suggests - more than A. Table 4 gives examples</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2004</marker>
<rawString>Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2004. Learning syntactic patterns for automatic hypernym discovery. In NIPS, volume 17, pages 1297–1304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic taxonomy induction from heterogenous evidence.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, ACL-44,</booktitle>
<pages>801--808</pages>
<contexts>
<context position="7941" citStr="Snow et al. (2006)" startWordPosition="1174" endWordPosition="1177">4), comparable corpora (Dolan et al., 2004; Xu et al., 2014), and graph structures (Berant et al., 2011; Brockett et al., 2013). These approaches are scalable, but they often assume that all relations are equivalence relations (Madnani and Dorr, 2010). Several efforts have attempted to build or augment lexical ontologies automatically, to discover other types of lexical relations like hypernyms. Most of these approaches rely on lexico-syntactic patterns. Hearst (1992) searched for hand-written patterns (e.g. “an X is a Y”) in a large corpus in order to learn taxonomic relations between nouns. Snow et al. (2006) used dependency parses to automatically learn such patterns, which they used to augment WordNet with new hypernym relations. Similar monolingual signals have been used to learn fine-grained relationships between verbs, such as enablement and happensbefore (Chklovski and Pantel, 2004; Hashimoto et al., 2009). Recognizing Textual Entailment The shared RTE tasks (Dagan et al., 2006) have been a springboard for research in natural language inference, 1513 Figure 2: Distribution of entailment relations in different sizes of PPDB. Distributions are estimated from our manual annotations of randomly </context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2006</marker>
<rawString>Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006. Semantic taxonomy induction from heterogenous evidence. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, ACL-44, pages 801–808.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
<author>Ido Dagan</author>
</authors>
<title>Learning entailment rules for unary templates.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22Nd International Conference on Computational Linguistics - Volume 1, COLING ’08,</booktitle>
<pages>849--856</pages>
<contexts>
<context position="19675" citStr="Szpektor and Dagan, 2008" startWordPosition="3091" endWordPosition="3095">to modify and be modified by in X and in Y in foods and in beverages separate X from Y separate the old from the young to X and/or to Y to the left or to the right from X to Y from 7 a.m. to 10 p.m. more/less X than Y more harm than good Table 4: Top paths associated with the - class. the same words. Given context vectors, Lin and Pantel (2001) used a symmetric similarity metric (Lin, 1998) to find candidate paraphrases. We build dependency context vectors for each word in our data and compute both symmetric as well as more recently proposed asymmetric similarity measures (Weeds et al., 2004; Szpektor and Dagan, 2008; Clarke, 2009), which are potentially better suited for identifying A paraphrases. Table 3 gives a comparison of the pairs which are considered “most similar” according to several of these metrics. 6.2 Bilingual features We explore a variety of bilingual features, which we expect to provide complimentary signal to the monolingual features. Each pair in PPDB is associated with several paraphrase probabilities, which are based on the probabilities of aligning each word to the foreign “pivot” phrase (a foreign translation shared by the two phrases), computed as described in Bannard and Callison-</context>
</contexts>
<marker>Szpektor, Dagan, 2008</marker>
<rawString>Idan Szpektor and Ido Dagan. 2008. Learning entailment rules for unary templates. In Proceedings of the 22Nd International Conference on Computational Linguistics - Volume 1, COLING ’08, pages 849–856.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
<author>Hristo Tanev</author>
<author>Dr Dagan</author>
<author>Bonaventura Coppola</author>
</authors>
<title>Scaling web-based acquisition of entailment relations.</title>
<date>2004</date>
<contexts>
<context position="7325" citStr="Szpektor et al., 2004" startWordPosition="1078" endWordPosition="1081">relations. The enormous size of PPDB– over 77 million phrase pairs!– makes it impossible to perform this task manually. Our wide range of monolingual and bilingual features results in high intrinsic accuracy. • We demonstrate improvements to a proofbased RTE system, showing that our automatic labels increase the number of proofs that it is able to find by 17%, while maintaining the same accuracy as when using goldstandard, manual labels. 2 Related Work Lexical entailment resources Approaches to paraphrase identification have exploited signal from distributional contexts (Lin and Pantel, 2001; Szpektor et al., 2004), comparable corpora (Dolan et al., 2004; Xu et al., 2014), and graph structures (Berant et al., 2011; Brockett et al., 2013). These approaches are scalable, but they often assume that all relations are equivalence relations (Madnani and Dorr, 2010). Several efforts have attempted to build or augment lexical ontologies automatically, to discover other types of lexical relations like hypernyms. Most of these approaches rely on lexico-syntactic patterns. Hearst (1992) searched for hand-written patterns (e.g. “an X is a Y”) in a large corpus in order to learn taxonomic relations between nouns. Sn</context>
</contexts>
<marker>Szpektor, Tanev, Dagan, Coppola, 2004</marker>
<rawString>Idan Szpektor, Hristo Tanev, Dr Dagan, Bonaventura Coppola, et al. 2004. Scaling web-based acquisition of entailment relations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
<author>Diana McCarthy</author>
</authors>
<title>Characterising measures of lexical distributional similarity.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics, COLING ’04.</booktitle>
<contexts>
<context position="19649" citStr="Weeds et al., 2004" startWordPosition="3087" endWordPosition="3090">phrases should tend to modify and be modified by in X and in Y in foods and in beverages separate X from Y separate the old from the young to X and/or to Y to the left or to the right from X to Y from 7 a.m. to 10 p.m. more/less X than Y more harm than good Table 4: Top paths associated with the - class. the same words. Given context vectors, Lin and Pantel (2001) used a symmetric similarity metric (Lin, 1998) to find candidate paraphrases. We build dependency context vectors for each word in our data and compute both symmetric as well as more recently proposed asymmetric similarity measures (Weeds et al., 2004; Szpektor and Dagan, 2008; Clarke, 2009), which are potentially better suited for identifying A paraphrases. Table 3 gives a comparison of the pairs which are considered “most similar” according to several of these metrics. 6.2 Bilingual features We explore a variety of bilingual features, which we expect to provide complimentary signal to the monolingual features. Each pair in PPDB is associated with several paraphrase probabilities, which are based on the probabilities of aligning each word to the foreign “pivot” phrase (a foreign translation shared by the two phrases), computed as describe</context>
</contexts>
<marker>Weeds, Weir, McCarthy, 2004</marker>
<rawString>Julie Weeds, David Weir, and Diana McCarthy. 2004. Characterising measures of lexical distributional similarity. In Proceedings of the 20th International Conference on Computational Linguistics, COLING ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Alan Ritter</author>
<author>Chris Callison-Burch</author>
<author>William B Dolan</author>
<author>Yangfeng Ji</author>
</authors>
<title>Extracting lexically divergent paraphrases from Twitter.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<volume>2</volume>
<contexts>
<context position="7383" citStr="Xu et al., 2014" startWordPosition="1088" endWordPosition="1091">rs!– makes it impossible to perform this task manually. Our wide range of monolingual and bilingual features results in high intrinsic accuracy. • We demonstrate improvements to a proofbased RTE system, showing that our automatic labels increase the number of proofs that it is able to find by 17%, while maintaining the same accuracy as when using goldstandard, manual labels. 2 Related Work Lexical entailment resources Approaches to paraphrase identification have exploited signal from distributional contexts (Lin and Pantel, 2001; Szpektor et al., 2004), comparable corpora (Dolan et al., 2004; Xu et al., 2014), and graph structures (Berant et al., 2011; Brockett et al., 2013). These approaches are scalable, but they often assume that all relations are equivalence relations (Madnani and Dorr, 2010). Several efforts have attempted to build or augment lexical ontologies automatically, to discover other types of lexical relations like hypernyms. Most of these approaches rely on lexico-syntactic patterns. Hearst (1992) searched for hand-written patterns (e.g. “an X is a Y”) in a large corpus in order to learn taxonomic relations between nouns. Snow et al. (2006) used dependency parses to automatically l</context>
</contexts>
<marker>Xu, Ritter, Callison-Burch, Dolan, Ji, 2014</marker>
<rawString>Wei Xu, Alan Ritter, Chris Callison-Burch, William B. Dolan, and Yangfeng Ji. 2014. Extracting lexically divergent paraphrases from Twitter. Transactions of the Association for Computational Linguistics, 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiang Zhao</author>
<author>Tian Tian Zhu</author>
<author>Man Lan</author>
</authors>
<title>Ecnu: One stone two birds: Ensemble of heterogenous measures for semantic relatedness and textual entailment. SemEval</title>
<date>2014</date>
<pages>271</pages>
<contexts>
<context position="9420" citStr="Zhao et al., 2014" startWordPosition="1395" endWordPosition="1398">tion, summarization, machine translation evaluation, and more recently, question answering (Giampiccolo et al., 2007) and essay grading (Clark et al., 2013). RTE systems vary considerably in their choice of representation and inference procedure. In the most recent shared task on RTE, some systems used deep logical representations of text, allowing them to invoke theorem provers (Bjerva et al., 2014) or Markov Logic Networks (Beltagy et al., 2014) to perform the inference, while others used shallower representations, relying on machine learning to perform inference (Lai and Hockenmaier, 2014; Zhao et al., 2014). Systems based on natural logic (MacCartney and Manning, 2007) use natural language as a representation, but still perform inference using a structured algebra rather than a statistical model. Regardless of the inference procedure, improvements to external lexical resources can improve RTE systems across the board (Clark et al., 2007). 3 The Paraphrase Database (PPDB) PPDB is currently the largest available collection of paraphrases. Compared to other paraphrase resources such as the DIRT database (12 million rules) (Lin and Pantel, 2001) and the MSR paraphrase phrase table (13 million) (Dola</context>
</contexts>
<marker>Zhao, Zhu, Lan, 2014</marker>
<rawString>Jiang Zhao, Tian Tian Zhu, and Man Lan. 2014. Ecnu: One stone two birds: Ensemble of heterogenous measures for semantic relatedness and textual entailment. SemEval 2014, page 271.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>