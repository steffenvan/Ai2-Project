<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.995899">
A Joint Model for Discovery of Aspects in Utterances
</title>
<author confidence="0.770129">
Asli Celikyilmaz Dilek Hakkani-Tur
</author>
<affiliation confidence="0.728907">
Microsoft Microsoft
</affiliation>
<address confidence="0.560914">
Mountain View, CA, USA Mountain View, CA, USA
</address>
<email confidence="0.993728">
asli@ieee.org dilek@ieee.org
</email>
<sectionHeader confidence="0.998562" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997650055555556">
We describe a joint model for understanding
user actions in natural language utterances.
Our multi-layer generative approach uses both
labeled and unlabeled utterances to jointly
learn aspects regarding utterance’s target do-
main (e.g. movies), intention (e.g., finding a
movie) along with other semantic units (e.g.,
movie name). We inject information extracted
from unstructured web search query logs as
prior information to enhance the generative
process of the natural language utterance un-
derstanding model. Using utterances from five
domains, our approach shows up to 4.5% im-
provement on domain and dialog act perfor-
mance over cascaded approach in which each
semantic component is learned sequentially
and a supervised joint learning model (which
requires fully labeled data).
</bodyText>
<sectionHeader confidence="0.999511" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999863857142857">
Virtual personal assistance (VPA) is a human to
machine dialog system, which is designed to per-
form tasks such as making reservations at restau-
rants, checking flight statuses, or planning weekend
activities. A typical spoken language understanding
(SLU) module of a VPA (Bangalore, 2006; Tur and
Mori, 2011) defines a structured representation for
utterances, in which the constituents correspond to
meaning representations in terms of slot/value pairs
(see Table 1). While target domain corresponds to
the context of an utterance in a dialog, the dialog
act represents overall intent of an utterance. The
slots are entities, which are semantic constituents at
the word or phrase level. Learning each component
</bodyText>
<subsectionHeader confidence="0.940475">
Sample utterances on ’plan a night out’ scenario
</subsectionHeader>
<bodyText confidence="0.990051666666667">
(I) Show me theaters in [Austin] playing [iron man 2].
(II)I’m in the mood for [indian] food tonight, show me the
ones [within 5 miles] that have [patios].
</bodyText>
<note confidence="0.495192">
Extracted Class and Labels
</note>
<table confidence="0.997576166666667">
Domain Dialog Act Slots=Values
(I) Movie find Location=Austin
theater Movie-Name= iron man 2
(II) Restaurant find Rest-Cusine=indian
restaurant Location=within 5 miles
Rest-Amenities= patios
</table>
<tableCaption confidence="0.9794925">
Table 1: Examples of utterances with corresponding se-
mantic components, i.e., domain, dialog act, and slots.
</tableCaption>
<bodyText confidence="0.9984795">
is a challenging task not only because there are no
a priori constraints on what a user might say, but
also systems must generalize from a tractably small
amount of labeled training data. In this paper, we
argue that each of these components are interdepen-
dent and should be modeled simultaneously. We
build a joint understanding framework and introduce
a multi-layer context model for semantic representa-
tion of utterances of multiple domains.
Although different strategies can be applied,
typically a cascaded approach is used where
each semantic component is modeled sepa-
rately/sequentially (Begeja et al., 2004), focusing
less on interrelated aspects, i.e., dialog’s domain,
user’s intentions, and semantic tags that can be
shared across domains. Recent work on SLU
(Jeong and Lee, 2008; Wang, 2010) presents joint
modeling of two components, i.e., the domain and
slot or dialog act and slot components together.
Furthermore, most of these systems rely on labeled
training utterances, focusing little on issues such
as information sharing between the discourse and
word level components across different domains,
or variations in use of language. To deal with de-
</bodyText>
<page confidence="0.972817">
330
</page>
<note confidence="0.985725">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 330–338,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.880461689655172">
pendency and language variability issues, a model
that considers dependencies between semantic
components and utilizes information from large
bodies of unlabeled text can be beneficial for SLU.
In this paper, we present a novel generative
Bayesian model that learns domain/dialog-act/slot
semantic components as latent aspects of text ut-
terances. Our approach can identify these semantic
components simultaneously in a hierarchical frame-
work that enables the learning of dependencies. We
incorporate prior knowledge that we observe in web
search query logs as constraints on these latent as-
pects. Our model can discover associations between
words within a multi-layered aspect model, in which
some words are indicative of higher layer (meta) as-
pects (domain or dialog act components), while oth-
ers are indicative of lower layer specific entities.
The contributions of this paper are as follows:
(i) construction of a novel Bayesian framework for
semantic parsing of natural language (NL) utter-
ances in a unifying framework in §4,
(ii) representation of seed labeled data and informa-
tion from web queries as informative prior to design
a novel utterance understanding model in §3 &amp; §4,
(iii) comparison of our results to supervised sequen-
tial and joint learning methods on NL utterances in
§5. We conclude that our generative model achieves
noticeable improvement compared to discriminative
models when labeled data is scarce.
</bodyText>
<sectionHeader confidence="0.994649" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.9999574375">
Language understanding has been well studied in
the context of question/answering (Harabagiu and
Hickl, 2006; Liang et al., 2011), entailment (Sam-
mons et al., 2010), summarization (Hovy et al.,
2005; Daume-III and Marcu, 2006), spoken lan-
guage understanding (Tur and Mori, 2011; Dinarelli
et al., 2009), query understanding (Popescu et al.,
2010; Li, 2010; Reisinger and Pasca, 2011), etc.
However data sources in VPA systems pose new
challenges, such as variability and ambiguities in
natural language, or short utterances that rarely con-
tain contextual information, etc. Thus, SLU plays
an important role in allowing any sophisticated spo-
ken dialog system (e.g., DARPA Calo (Berry et al.,
2011), Siri, etc.) to take the correct machine actions.
A common approach to building SLU framework
is to model its semantic components separately, as-
suming that the context (domain) is given a pri-
ori. Earlier work takes dialog act identification as
a classification task to capture the user’s intentions
(Margolis et al., 2010) and slot filling as a sequence
learning task specific to a given domain class (Wang
et al., 2009; Li, 2010). Since these tasks are con-
sidered as a pipeline, the errors of each component
are transfered to the next, causing robustness issues.
Ideally, these components should be modeled si-
multaneously considering the dependencies between
them. For example, in a local domain application,
users may require information about a sub-domain
(movies, hotels, etc.), and for each sub-domain, they
may want to take different actions (find a movie, call
a restaurant or book a hotel) using domain specific
attributes (e.g., cuisine type of a restaurant, titles for
movies or star-rating of a hotel). There’s been little
attention in the literature on modeling the dependen-
cies of SLU’s correlated structures.
Only recent research has focused on the joint
modeling of SLU (Jeong and Lee, 2008; Wang,
2010) taking into account the dependencies at learn-
ing time. In (Jeong and Lee, 2008), a triangular
chain conditional random fields (Tri-CRF) approach
is presented to model two of the SLU’s components
in a single-pass. Their discriminative approach rep-
resents semantic slots and discourse-level utterance
labels (domain or dialog act) in a single structure
to encode dependencies. However, their model re-
quires fully labeled utterances for training, which
can be time consuming and expensive to generate for
dynamic systems. Also, they can only learn depen-
dencies between two components simultaneously.
Our approach differs from the earlier work- in
that- we take the utterance understanding as a multi-
layered learning problem, and build a hierarchical
clustering model. Our joint model can discover
domain D, and user’s act A as higher layer latent
concepts of utterances in relation to lower layer la-
tent semantic topics (slots) S such as named-entities
(”New York”) or context bearing non-named enti-
ties (”vegan”). Our work resembles the earlier work
of PAM models (Mimno et al., 2007), i.e., directed
acyclic graphs representing mixtures of hierarchical
topic structures, where upper level topics are multi-
nomial over lower level topics in a hierarchy. In an
analogical way to earlier work, the D and A in our
</bodyText>
<page confidence="0.997841">
331
</page>
<bodyText confidence="0.998945111111111">
approach represent common co-occurrence patterns
(dependencies) between semantic tags S (Fig. 2).
Concretely, correlated topics eliminate assignment
of semantic tags to segments in an utterance that
belong to other domains, e.g., we can discover that
”Show me vegan restaurants in San Francisco” has
a low probably of outputting a movie-actor slot. Be-
ing generative, our model can incorporate unlabeled
utterances and encode prior information of concepts.
</bodyText>
<sectionHeader confidence="0.975174" genericHeader="method">
3 Data and Approach Overview
</sectionHeader>
<bodyText confidence="0.999317444444444">
Here we define several abstractions of our joint
model as depicted in Fig. 1. Our corpus mainly
contains NL utterances (”show me the nearest dim-
sum places”) and some keyword queries (”iron man
2 trailers”). We represent each utterance u as a vec-
tor w„ of Nu word n-grams (segments), wuj, each
of which are chosen from a vocabulary W of fixed-
size V. We use entity lists obtained from web sources
(explained next) to identify segments in the corpus.
Our corpus contains utterances from KD=4 main
domains:E {movies, hotels, restaurants, events},
as well as out-of-domain other class. Each utterance
has one dialog act (A) associated with it. We assume
a fixed number of possible dialog acts KA for each
domain. Semantic Tags, slots (S) are lexical units
(segments) of an utterance, which we classify into
two types: domain-independent slots that are shared
across all domains, (e.g., location, time, year, etc.),
and domain-dependent slots, (e.g. movie-name,
actor-name, restaurant-name, etc.). For tractability,
we consider a fixed number of latent slot types KS.
Our algorithm assigns domain/dialog-act/slot labels
to each topic at each layer in the hierarchy using la-
beled data (explained in §4.)
We represent domain and dialog act components
as meta-variables of utterances. This is similar to
author-topic models (Rosen-Zvi et al., 2004), that
capture author-topic relations across documents. In
that case, words are generated by first selecting an
author uniformly from an observed author list and
then selecting a topic from a distribution over words
that is specific to that author. In our model, each
utterance u is associated with domain and dialog
act topics. A word wuj in u is generated by first
selecting a domain and an act topic and then slot
topic over words of u. The domain-dependent slots
</bodyText>
<equation confidence="0.921219">
P(a E A|ud) and slots
P (sj E S|wuj, d, sj_1) of words wuj in sequence.
</equation>
<bodyText confidence="0.992800472222222">
To handle language variability, and hence dis-
cover correlation between hierarchical aspects of ut-
terances1, we extract prior information from two
web resources as follows:
Web n-Grams (G). Large-scale engines such as
Bing or Google log more than 100M search queries
each day. Each query in the search logs has an as-
sociated set of URLs that were clicked after users
entered a given query. The click information can
be used to infer domain class labels, and there-
fore, can provide (noisy) supervision in training do-
main classifiers. For example, two queries (”cheap
hotels Las Vegas” and ”wine resorts in Napa”),
which resulted in clicks on the same base URL (e.g.,
www.hotels.com) probably belong to the same do-
main (”hotels” in this case).
Given query logs, we
compile sets of in-domain
queries based on their
base URLs2. Then, for
each vocabulary item
wj E W in our corpus, we calculate frequency of
wj in each set of in-domain queries and represent
each word (e.g., ”room”) as a discrete normalized
probability distribution 0&apos;G over KD domains
{���&apos;
G }E 0&apos;G. We inject them as nonuniform priors
over domain and dialog act parameters in §4.
Entity Lists (E). We limit our model to a set
of named-entity slots (e.g., movie-name, restaurant-
name) and non-named entity slots (e.g., restaurant-
cuisine, hotel-rating). For each entity slot, we ex-
tract a large collection of entity lists through the url’s
on the web that correspond to our domains, such
as movie-names listed on IMDB, restaurant-names
on OpenTable, or hotel-ratings on tripadvisor.com.
</bodyText>
<footnote confidence="0.8491978">
1Two utterances can be intrinsically related but contain no
common terms, e.g., ”has open bar” and ”serves free drinks”.
2We focus on domain specific search engines such as
IMDB.com, RottenTomatoes.com for movies, Hotels.com and
Expedia.com for hotels, etc.
</footnote>
<bodyText confidence="0.987496714285714">
in utterances are usually not dependent on the di-
alog act. For instance, while ”find [hugo] trailer”
and ”show me where [hugo] is playing” have both
a movie-name slot (”hugo”), they have different di-
alog acts, i.e., find-trailer and find-movie, respec-
tively. We predict posterior probabilities for domain
P(d E D|u) dialog act �
</bodyText>
<equation confidence="0.712472">
d|wl
ψG = P(d=hotelJwl=‘room’)
</equation>
<bodyText confidence="0.531751">
movie rest. hotel event other
</bodyText>
<page confidence="0.951656">
332
</page>
<figureCaption confidence="0.9978105">
Figure 1: Graphical model depiction of the MCM. D, A, S are
domain, dialog act and slot in a hierarchy, each consisting of
KD, KA, KS components. Shaded nodes indicate observed
variables. Hyper-parameters are omitted. Sample informative
priors over latent topics ?PE and ?PG are shown. Blue arrows
indicate frequency of vocabulary terms sampled for each topic.
</figureCaption>
<bodyText confidence="0.9999444">
We represent each entity list as observed nonuniform
priors OE and inject them into our joint learning pro-
cess as V sparse multinomial distributions over la-
tent topics D, and S to ”guide” the generation of
utterances (Fig. 1 top-left table), explained in §4.
</bodyText>
<sectionHeader confidence="0.999564" genericHeader="method">
4 Multi-Layer Context Model - MCM
</sectionHeader>
<bodyText confidence="0.999952142857143">
The generative process of our multi-layer context
model (MCM) (Fig. 1) is shown in Algorithm 1. Each
utterance u is associated with d = 1..KD multino-
mial domain-topic distributions odD. Each domain d,
is represented as a distribution over a = 1, .., KA
dialog acts odaA (odD → odaA ). In our MCM model, we
assume that each utterance is represented as a hidden
Markov model with KS slot states. Each state gen-
erates n-grams according to a multinomial n-gram
distribution. Once domain Du and act Aud topics
are sampled for u, a slot state topic Sujd is drawn
to generate each segment wuj of u by considering
the word-tag sequence frequencies based on a sim-
ple HMM assumption, similar to the content models
of (Sauper et al., 2011). Initial and transition prob-
ability distributions over the HMM states are sam-
pled from Dirichlet distribution over slots ods S .Each
slot state s generates words according to multino-
mial word distribution 0sS. We also keep track of the
frequency of vocabulary terms wj’s in a V ×KD ma-
trix MD. Every time a wj is sampled for a domain d,
we increment its count, a degree of domain bearing
words. Similarly, we keep track of dialog act and
slot bearing words in V ×KA and V ×KS matrices,
MA and MS (shown as red arrows in Fig 1). Being
Bayesian, each distribution odD, oadA, and odsS is sam-
pled from a Dirichlet prior distribution with different
parameters, described next.
</bodyText>
<construct confidence="0.413921">
Algorithm 1 Multi-Layer Context Model Generation
</construct>
<listItem confidence="0.992984">
1: for each domain d ← 1, ..., KD
2: draw domain dist. od D ∼ Dir(a? D)†,
3: for each dialog-act a ← 1, ..., KA
4: draw dialog act dist. odaA ∼ Dir(a? A),
5: for each slot type s ← 1, ..., KS
6: draw slot dist. odsS ∼ Dir(a? S).
7: endfor
8: draw 0sS ∼ Dir(β) for each slot type s ← 1, ..., KS.
9: for each utterance u ← 1, ..., |U |do
10: Sample a domain Du∼Multi(odD) and,
11: and act topic Aud∼Multi(odaA ).
12: for words wuj, j ← 1, ..., Nu do
13: - Draw Sujd∼Multi(oDu,Su(j−1)d)#
14: - Sample wuj∼Multi(0Sujd).
15: end for
16: end for
</listItem>
<equation confidence="0.731019333333333">
t Dir(α*D), Dir(α*A), Dir(α*S) are parameterized based on prior
knowledge.
t Here HMM assumption over utterance words is used.
</equation>
<bodyText confidence="0.999988043478261">
In hierarchical topic models (Blei et al., 2003;
Mimno et al., 2007), etc., topics are represented
as distributions over words, and each document ex-
presses an admixture of these topics, both of which
have symmetric Dirichlet (Dir) prior distributions.
Symmetric Dirichlet distributions are often used,
since there is typically no prior knowledge favoring
one component over another. In the topic model lit-
erature, such constraints are sometimes used to de-
terministically allocate topic assignments to known
labels (Labeled Topic Modeling (Ramage et al.,
2009)) or in terms of pre-learnt topics encoded as
prior knowledge on topic distributions in documents
(Reisinger and Pas¸ca, 2009). Similar to previous
work, we define a latent topic per each known se-
mantic component label, e.g., five domain topics for
five defined domains. Different from earlier work
though, we also inject knowledge that we extract
from several resources including entity lists from
web search query click logs as well as seed labeled
training utterances as prior information. We con-
strain the generation of the semantic components of
our model by encoding prior knowledge in terms of
</bodyText>
<figure confidence="0.868574785714286">
slot
transition
parameters
S-1 S S+1
BS
KS
topic-word
parameters
MS
w_t
(PS KS
(ψE) Entity List Prior
(ψG) Web N-Gram Context Prior
w w.t
</figure>
<table confidence="0.943680285714286">
movie restaurant hotel
wuj name name name
hotel california 0.5 0.0 0.5
zucca 0.0 1.0 0.0
wuj movie restaurant hotel
menu 0.02 0.93 0.01
rooms 0.001 0.001 0.98
</table>
<figure confidence="0.946822576923077">
n-gram
prior
from
web query logs
entity
prior
from
V⨉D
ψE
ψG
domain specific
act parameters
domain
parameters
BD
BA
dialog act
topics
A
domain topics
D
MD
web documents
Utterance
MA
slot topics
</figure>
<page confidence="0.995631">
333
</page>
<bodyText confidence="0.999472642857143">
asymmetric Dirichlet topic priors α=(αm1,...,αmK)
where each kth topic has a prior weight αk=αmk,
with varying base measure m=(m1,...,mk) 3.
We update parameter vectors of Dirichlet domain
prior αu?D ={(αD· u1 D ),..., αD· uKD D}, where αD is
the concentration parameter for domain Dirichlet
distribution and u D={ ud D }KD
d=1 is the base mea-
sure which we obtain from various resources. Be-
cause base measure updates are dependent on prior
knowledge of corpus words, each utterance u gets
a different base measure. Similarly, we update
the parameter vector of the Dirichlet dialog act
and slot priors αu? A ={(αA· u1 A ),...,(αA· uKA
</bodyText>
<equation confidence="0.742574714285714">
A )} and
αu? S ={(αS· u1
S ),...,(αS· uKS
S )} using base measures
uA={ ua A }KA
a=1 and Su={ us S }KS
s=1 respectively.
</equation>
<bodyText confidence="0.996086161290322">
Before describing base measure update for do-
main, act and slot Dirichlet priors, we explain the
constraining prior knowledge parameters below:
* Entity List Base Measure( j E): Entity fea-
tures are indicative of domain and slots and MCM
utilizes these features while sampling topics. For
instance, entities hotel-name ”Hilton” and location
”New York” are discriminative features in classi-
fying ”find nice cheap double room in New York
Hilton” into correct domain (hotel) and slot (hotel-
name) clusters. We represent entity lists correspond-
ing to known domains as multinomial distributions
jE, where each d|j
E is the probability of entity-
word wj used in the domain d. Some entities may
belong to more than one domain, e.g., ”hotel Cali-
fornia” can either be a movie, or song or hotel name.
* Web n-Gram Context Base Measure ( j G):
As explained in §3, we use the web n-grams as ad-
ditional information for calculating the base mea-
sures of the Dirichlet topic distributions. Normal-
ized word distributions jG over domains were used
as weights for domain and dialog act base measure.
* Corpus n-Gram Base Measure ( jC): Sim-
ilar to other measures, MCM also encodes n-gram
constraints as word-frequency features extracted
from labeled utterances. Concretely, we cal-
culate the frequency of vocabulary items given
domain-act label pairs from the training labeled ut-
terances and convert there into probability mea-
sures over domain-acts. We encode conditional
</bodyText>
<footnote confidence="0.9145265">
3See (Wallach, 2008) Chapter 3 for analysis of hyper-priors
on topic models.
</footnote>
<equation confidence="0.9019588">
probabilities { ad|j
C }� j C as multinomial distribu-
tions of words over domain-act pairs, e.g., ad|j
C =
P(d=”restaurant”, a=”make-reservation”|”table”).
</equation>
<bodyText confidence="0.8840825">
Base measure update: The α-base measures are
used to shape Dirichlet priors αu?D , αu?A and αu?S . We
update the base measures of each sampled domain
Du = d given each vocabulary wj as:
</bodyText>
<equation confidence="0.9688222">
�
d|j
E , d|j
dj E &gt; 0
D = (1)
</equation>
<bodyText confidence="0.947835666666667">
d|j G , otherwise
In (1) we assume that entities (E) are more indica-
tive of the domain compared to other n-grams (G)
and should be more dominant in sampling decision
for domain topics. Given an utterance u, we calcu-
late its base measure udD =(�Nu
</bodyText>
<equation confidence="0.759173">
j DM/Nu-
Once
</equation>
<bodyText confidence="0.914353">
D /Nu.Once the domain is sampled, we update the prior
weight of dialog acts Aud = a:
</bodyText>
<equation confidence="0.991181666666667">
ajA = ad|j
C * d|j (2)
G
</equation>
<bodyText confidence="0.82322">
and slot components Sujd = s:
</bodyText>
<equation confidence="0.980174">
sjS = d|j (3)
E
</equation>
<bodyText confidence="0.891198">
Then we update their base measures for a given u as:
</bodyText>
<figure confidence="0.80371175">
ua
A =(�Nu
j ajA )/Nu and us S=(�Nu
j sj S )/Nu
</figure>
<subsectionHeader confidence="0.921282">
4.1 Inference and Learning
</subsectionHeader>
<bodyText confidence="0.998223277777778">
The goal of inference is to predict the domain, user’s
act and slot distributions over each segment given
an utterance. The MCM has the following set of pa-
rameters: domain-topic distributions BdD for each u,
the act-topic distributions Bda A for each domain topic
d of u, local slot-topic distributions for each do-
main BS, and OsS for slot-word distributions. Pre-
vious work (Asuncion et al., 2009; Wallach et al.,
2009) shows that the choice of inference method has
negligible effect on the probability of testing doc-
uments or inferred topics. Thus, we use Markov
Chain Monte Carlo (MCMC) method,specifically
Gibbs sampling, to model the posterior distribution
PMCM(Du, Aud, Sujd|αu?D , αu?A , αu?S , Ci) by obtaining
samples (Du, Aud, Sujd) drawn from this distribu-
tion. For each utterance u, we sample a domain Du
and act Aud and hyper-parameters αD and αA and
their base measures udD , uaA (from Eq. 1,2):
</bodyText>
<equation confidence="0.704344111111111">
ud
Bd —_ Ndu + αD D Bda = Na |ud + αA D
D Nu+ αu? A Nud + αu?
D A
The Ndu is the number of occurrences of domain
topic d in utterance u, Na|ud is the number of occur-
rences of act a given d in u. During sampling of a
.
(4)
</equation>
<page confidence="0.990768">
334
</page>
<bodyText confidence="0.9941416">
slot state Sujd, we assume that utterance is generated
by the HMM model associated with the assigned
domain. For each segment wuj in u, we sample a
slot state Sujd given the remaining slots and hyper-
parameters αS, β and base measure ψusS (Eq. 3) by:
</bodyText>
<equation confidence="0.9269795">
NDu,s + ff(Suj−1, s) + KDαu?
(.) S
</equation>
<bodyText confidence="0.976337333333333">
The Nkujd is the number of times segment wuj is
generated from slot state s in all utterances as-
signed to domain topic d, NDu,s1
s2 is the num-
ber of transitions from slot state s1 to s2, where
s1 EISu(j−1)d,Su(j+1)dI, ff(s1, s2)=1 if slot s1=s2.
</bodyText>
<subsectionHeader confidence="0.9455">
4.2 Semantic Structure Extraction with MCM
</subsectionHeader>
<bodyText confidence="0.999039166666667">
During Gibbs sampling, we keep track of the fre-
quency of draws of domain, dialog act and slot in-
dicating n-grams wj, in MD, MA and MS matri-
ces, respectively. These n-grams are context bearing
words (examples are shown in Fig.1.). For given u
the predicted domain d∗u is determined by:
</bodyText>
<equation confidence="0.95880125">
Mjd
d∗u = arg maxd P(dIu) = arg maxd[θdD * Nu
j=1 MD ]
D
</equation>
<bodyText confidence="0.987771">
and predicted dialog act by arg maxa P�(aIud∗):
</bodyText>
<equation confidence="0.991638">
* 77�77 Mja
au = arg maxa [θA a * 11 j u1 Mq ] (6)
For each segment wuj in u, its predicted slot are de-
termined by arg maxs P(sjIwuj, d∗, sj−1):
sT*yj = arg maxs [p(Sujd* = sl.) * 11 u1 ZS
ZSjs] (7)
</equation>
<sectionHeader confidence="0.998928" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999994">
We performed several experiments to evaluate our
proposed approach. Before presenting our results,
we describe our datasets as well as two baselines.
</bodyText>
<subsectionHeader confidence="0.997678">
5.1 Datasets, Labels and Tags
</subsectionHeader>
<bodyText confidence="0.994486666666667">
Our dataset contains utterances obtained from di-
alogs between human users and our personal assis-
tant system. We use the transcribed text forms of
</bodyText>
<table confidence="0.999861090909091">
Domain Sample Dialog Acts (DAs) &amp; Slots
movie DAs: find-movie/director/actor,buy-ticket
Slots: name, mpaa-rating (g-rated), date,
director/actor-name, award(oscar winning)...
hotel DAs: find-hotel, book-hotel,
Slots: name, room-type(double), amenities,
smoking, reward-program(platinum elite)...
restaurant DAs: find-restaurant, make-reservation,
Slots: opening-hour, amenities, meal-type,...
event DAs: find-event/ticket/performers, get-info..
Slots: name, type(concert), performer....
</table>
<tableCaption confidence="0.995506">
Table 2: List of domains, dialog acts and semantic slot
tags of utterance segments. Examples for some slots val-
ues are presented in parenthesis as italicized.
</tableCaption>
<bodyText confidence="0.992907272727273">
the utterances obtained from (acoustic modeling en-
gine) to train our models 4. Thus, our dataset con-
tains 18084 NL utterances, 5034 of which are used
for measuring the performance of our models. The
dataset consists of five domain classes, i.e, movie,
restaurant, hotel, event, other, 42 unique dialog acts
and 41 slot tags. Each utterance is labeled with a
domain, dialog act and a sequence of slot tags cor-
responding to segments in utterance (see examples
in Table 1). Table 2 shows sample dialog act and
slot labels. Annotation agreement, Kappa measure
(Cohen, 1960), was around 85%.
We pulled a month of web query logs and ex-
tracted over 2 million search queries from the movie,
hotel, event, and restaurant domains. We also used
generic web queries to compile a set of ’other’ do-
main queries. Our vocabulary consists of n-grams
and segments (phrases) in utterances that are ex-
tracted using web n-grams and entity lists of §3. We
extract distributions of n-grams and entities to inject
as prior weights for entity list base (ψj.) and web
n-gram context base measures (ψj�) (see §4).
</bodyText>
<subsectionHeader confidence="0.994139">
5.2 Baselines and Experiment Setup
</subsectionHeader>
<bodyText confidence="0.998707">
We evaluated two baselines and two variants of our
joint SLU approach as follows:
</bodyText>
<listItem confidence="0.550413">
? Sequence-SLU: A traditional approach to SLU
</listItem>
<bodyText confidence="0.942025833333333">
extracts domain, dialog act and slots as seman-
tic components of utterances using three sequential
models. Typically, domain and dialog act detec-
tion models are taken as query classification, where
a given NL query is assigned domain and act la-
bels. Among supervised query classification meth-
</bodyText>
<footnote confidence="0.990475333333333">
4We submitted sample utterances used in our models as ad-
ditional resource. Due to licensing issues, we will reveal the full
train/test utterances upon acceptance of our paper.
</footnote>
<equation confidence="0.403872625">
p(Sujd = slw, Du, S−(u�d)αu?
� , β) «
Nkujd * (NDu,S us
+ β u(j-1)ds + aSψS )*
Nk (.) + V β
NDu,s
Su(j+1)d + ff(Suj−1, s) + ff(Suj+1, s) + αSψusS
(5)
</equation>
<page confidence="0.953919">
335
</page>
<figureCaption confidence="0.993543">
Figure 2: Sample topics discovered by Multi-Layer Context
Model (MCM). Given samples of utterances, MCM is able to in-
fer a meaningful set of dialog act (A) and slots (S), falling into
broad categories of domain classes (D).
</figureCaption>
<bodyText confidence="0.999263403846154">
ods, we used the Adaboost, utterance classifica-
tion method that starts from a set of weak classifiers
and builds a strong classifier by boosting the weak
classifiers. Slot discovery is taken as a sequence la-
beling task in which segments in utterances are la-
beled (Li, 2010). For segment labeling we use Semi-
Markov Conditional Random Fields (Semi-CRF)
(Sarawagi and Cohen, 2004) method as a benchmark
in evaluating semantic tagging performance.
? Tri-CRF: We used Triangular Chain CRF (Jeong
and Lee, 2008) as our supervised joint model base-
line. It is a state-of-the art method that learns the
sequence labels and utterance class (domain or dia-
log act) as meta-sequence in a joint framework. It
encodes the inter-dependence between the slot se-
quence s and meta-sequence label (d or a) using a
triangular chain (dual-layer) structure.
? Base-MCM: Our first version injects an informa-
tive prior for domain, dialog act and slot topic dis-
tributions using information extracted from only la-
beled training utterances and inject as prior con-
straints (corpus n-gram base measure ψc) during
topic assignments.
? WebPrior-MCM: Our full model encodes distri-
butions extracted from labeled training data as well
as structured web logs as asymmetric Dirichlet pri-
ors. We analyze performance gain by the informa-
tion from web sources (ψG and ψE) when injected
into our approach compared to Base-MCM.
We inject dictionary constraints as features
to train supervised discriminative methods, i.e.,
boosting and Semi-CRF in Sequence-SLU, and
Tri-CRF models. For semantic tagging, dictionary
constraints apply to the features between individual
segments and their labels, and for utterance classifi-
cation (to predict domain and dialog acts) they apply
to the features between utterance and its label. Given
a list of dictionaries, these constraints specify which
label is more likely. For discriminative methods,
we use several named entities, e.g., Movie-Name,
Restaurant-Name, Hotel-Name, etc., non-named en-
tities, e.g., Genre, Cuisine, etc., and domain inde-
pendent dictionaries, e.g., Time, Location, etc.
We train domain and dialog act classifiers via
Icsiboost (Favre et al., 2007) with 10K iterations
using lexical features (up to 3-n-grams) and con-
straining dictionary features (all dictionaries). For
feature templates of sequence learners, i.e., Semi-
CRF and Tri-CRF, we use current word, bi-gram
and dictionary features. For Base-MCM and
WebPrior-MCM, we run Gibbs sampler for 2000
iterations with the first 500 samples as burn-in.
</bodyText>
<subsectionHeader confidence="0.992774">
5.3 Evaluations and Discussions
</subsectionHeader>
<bodyText confidence="0.997027964285714">
We evaluate the performance of our joint model on
two experiments using two metrics. For domain and
dialog act detection performance we present results
in accuracy, and for slot detection we use the F1 pair-
wise measure.
Experiment 1. Encoding Prior Knowledge: A
common evaluation method in SLU tasks is to mea-
sure the performance of each individual semantic
model, i.e., domain, dialog act and semantic tagging
(slot filling). Here, we not only want to demon-
strate the performance of each component of MCM
but also their performance under limited amount of
labeled data. We randomly select subsets of labeled
training data UL C UL with different samples sizes,
nL ={γ * nL1, where nL represents the sample size
of UL and γ={10%,25%,..1 is the subset percentage.
At each random selection, the rest of the utterances
are used as unlabeled data to boost the performance
of MCM. The supervised baselines do not leverage the
unlabeled utterances.
The results reported in Figure 3 reveal both
the strengths and some shortcomings of our ap-
proach. When the number of labeled data is
small (nL G25%*nL), our WebPrior-MCM has
a better performance on domain and act predic-
tions compared to the two baselines. Compared to
Sequence-SLU, we observe 4.5% and 3% perfor-
mance improvement on the domain and dialog act
</bodyText>
<figure confidence="0.995971093749999">
DOMAIN
DIALOG
ACTS
SLOTS
movie, theater,
ticket, matinee,
fandango
iron man 2,
hugo, muppets
descendants
scary, ticket
iron-man 2,
oscar winner
movie-name
find-movie
A1
S1
movie restaurant
D1 D2
reviews, critics
ratings, mpaa,
breath-taking
tom hanks,
angelina jolie,
cameron
find-review
actor-name
A2
S2
kid-friendly
reserve, table
wait-time
amici, zucca
new york
bagel
starbucks
reservation
rest-name
S3
A3
menu, list,
vine list,
check, hotpot
check-menu
chinese,
vietnamese,
italian,
fast food
cuisine
A4
S4 Sk
menu, table,
dinner, togo
kids-friendly
chinese, coffee
nearest,
city center,
Vancouver,
New York
domain
in-
dependent
slots
location
336
Accuracy %
Utterance Domain Performance
Dialog Act Performance
88
96
Accuracy %
87
86
95
85
94
84
93
83
F - Measure
92
82
91
10 25 50 75 100
20 40 60 80 100
% Labeled Data
% Labeled Data
% Labeled Data
Semantic Tag (Slot) Performance
85
80
75
70
65
20 40 60 80 100
Sequence-SLU Tri-CRF Base-MCM WebPrior-MCM
</figure>
<figureCaption confidence="0.999929">
Figure 3: Semantic component extraction performance measures for various baselines as well as our approach with different priors.
</figureCaption>
<bodyText confidence="0.998990323529412">
models, whereas our gain is 2.6% and 1.7% over
Tri-CRF models. As the percentage of labeled ut-
terances in training data increase, Tri-CRF perfor-
mance increases, however WebPrior-MCM is still
comparable with Sequence-SLU. This is because
we utilize domain priors obtained from the web
sources as supervision during generative process as
well as unlabeled utterances that enable handling
language variability. Adding labeled data improves
the performance of all models however supervised
models benefit more compared to MCM models.
Although WebPrior-MCM’s domain and dialog
act performances are comparable (if not better than)
the other baselines, it falls short on the semantic
tagging model. This is partially due to the HMM
assumption compared to the supervised conditional
model’s used in the other baselines, i.e., Semi-CRF
in Sequence-SLU and Tri-CRF). Our work can
be extended by replacing HMM assumption with
CRF based sequence learner to enhance the capa-
bility of the sequence tagging component of MCM.
Experiment 2. Less is More? Being Bayesian,
our model can incorporate unlabeled data at train-
ing time. Here, we evaluate the performance gain on
domain, act and slot predictions as more unlabeled
data is introduced at learning time. We use only 10%
of the utterances as labeled data in this experiment
and incrementally add unlabeled data (90% of la-
beled data are treated as unlabeled).
The results are shown in Table 3. n% (n=10,25,..)
unlabeled data indicates that the WebPrior-MCM
is trained using n% of unlabeled utterances along
with training utterances. Adding unlabeled data has
a positive impact on the performance of all three se-
</bodyText>
<tableCaption confidence="0.982310666666667">
Table 3: Performance evaluation results of
WebPrior-MCM using different sizes of unlabeled
utterances at learning time.
</tableCaption>
<table confidence="0.997179571428571">
Unlabeled Domain Dialog Act Slot
% Accuracy Accuracy F-Measure
10% 94.69 84.17 52.61
25% 94.89 84.29 54.22
50% 95.08 84.39 56.58
75% 95.19 84.44 57.45
100% 95.28 84.52 58.18
</table>
<bodyText confidence="0.999353">
mantic components when WebPrior-MCM is used.
The results show that our joint modeling approach
has an advantage over the other joint models (i.e.,
Tri-CRF) in that it can leverage unlabeled NL ut-
terances. Our approach might be usefully extended
into the area of understanding search queries, where
an abundance of unlabeled queries is observed.
</bodyText>
<sectionHeader confidence="0.999823" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999831066666667">
In this work, we introduced a joint approach to
spoken language understanding that integrates two
properties (i) identifying user actions in multiple
domains in relation to semantic units, (ii) utilizing
large amounts of unlabeled web search queries that
suggest the user’s hidden intentions. We proposed a
semi-supervised generative joint learning approach
tailored for injecting prior knowledge to enhance the
semantic component extraction from utterances as a
unifying framework. Experimental results using the
new Bayesian model indicate that we can effectively
learn and discover meta-aspects in natural language
utterances, outperforming the supervised baselines,
especially when there are fewer labeled and more
unlabeled utterances.
</bodyText>
<page confidence="0.997555">
337
</page>
<sectionHeader confidence="0.998339" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999878531914894">
A. Asuncion, M. Welling, P. Smyth, and Y. W. Teh. 2009.
On smoothing and inference for topic models. UAI.
S. Bangalore. 2006. Introduction to special issue of spo-
ken language understanding in conversational systems.
In Speech Conversation, volume 48, pages 233–238.
L. Begeja, B. Renger, Z. Liu D. Gibbon, and
B. Shahraray. 2004. Interactive machine learning
techniques for improving slu models. In Proceedings
of the HLT-NAACL 2004 Workshop on Spoken Lan-
guage Understanding for Conversational Systems and
Higher Level Linguistic Information for Speech Pro-
cessing.
Pauline M. Berry, Melinda Gervasio, Bart Peintner, and
Neil Yorke-Smith. 2011. Ptime: Personalized assis-
tance for calendaring. In ACM Transactions on Intel-
ligent Systems and Technology, volume 2, pages 1–40.
D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet
allocation. Journal of Machine Learning Research.
J. Cohen. 1960. A coefficient of agreement for nominal
scales. In Educational and Psychological Measure-
ment, volume 20, pages 37–46.
H. Daume-III and D. Marcu. 2006. Bayesian query fo-
cused summarization.
M. Dinarelli, A. Moschitti, and G. Riccardi. 2009. Re-
ranking models for spoken language understanding.
Proc. European Chapter of the Annual Meeting of the
Association of Computational Linguistics (EACL).
B. Favre, D. Hakkani-T¨ur, and Sebastien Cuendet.
2007. Icsiboost. http://code.google.come/
p/icsiboost.
S. Harabagiu and A. Hickl. 2006. Methods for using
textual entailment for question answering. pages 905–
912.
E. Hovy, C.Y. Lin, and L. Zhou. 2005. A be-based multi-
document summarizer with query interpretation. Proc.
DUC.
M. Jeong and G. G. Lee. 2008. Triangular-chain con-
ditional random fields. EEE Transactions on Audio,
Speech and Language Processing (IEEE-TASLP).
X. Li. 2010. Understanding semantic structure of noun
phrase queries. Proc. of the Annual Meeting of the
Association of Computational Linguistics (ACL).
P. Liang, M. I. Jordan, and D. Klein. 2011. Learning
dependency based compositional semantics.
A. Margolis, K. Livescu, and M. Osterdorf. 2010. Do-
main adaptation with unlabeled data for dialog act tag-
ging. In Proc. Workshop on Domain Adaptation for
Natural Language Processing at the the Annual Meet-
ing of the Association of Computational Linguistics
(ACL).
D. Mimno, W. Li, and A. McCallum. 2007. Mixtures
of hierarchical topics with pachinko allocation. Proc.
ICML.
A. Popescu, P. Pantel, and G. Mishne. 2010. Semantic
lexicon adaptation for use in query interpretation. 19th
World Wide Web Conference (WWW-10).
D. Ramage, D. Hall, R. Nallapati, and C. D. Man-
ning. 2009. Labeled lda: A supervised topic model
for credit attribution in multi-labeled corpora. Proc.
EMNLP.
J. Reisinger and M. Pas¸ca. 2009. Latent variable models
of concept-attribute attachement. Proc. of the Annual
Meeting of the Association of Computational Linguis-
tics (ACL).
J. Reisinger and M. Pasca. 2011. Fine-grained class la-
bel markup of search queries. In Proc. of the Annual
Meeting of the Association of Computational Linguis-
tics (ACL).
M. Sammons, V. Vydiswaran, and D. Roth. 2010. Ask
not what textual entailment can do for you... In Proc.
of the Annual Meeting of the Association of Computa-
tional Linguistics (ACL), Uppsala, Sweden, 7.
S. Sarawagi and W. W. Cohen. 2004. Semimarkov
conditional random fields for information extraction.
Proc. NIPS.
C. Sauper, A. Haghighi, and R. Barzilay. 2011. Content
models with attitude. In Proc. of the Annual Meet-
ing of the Association of Computational Linguistics
(ACL).
G. Tur and R. De Mori. 2011. Spoken language under-
standing: Systems for extracting semantic information
from speech. Wiley.
H. Wallach, D. Mimno, and A. McCallum. 2009. Re-
thinking lda: Why priors matter. NIPS.
H. Wallach. 2008. Structured topic models for language.
Ph.D. Thesis, University of Cambridge.
Y.Y. Wang, R. Hoffman, X. Li, and J. Syzmanski.
2009. Semi-supervised learning of semantic classes
for query understanding from the web and for the
web. In The 18th ACM Conference on Information and
Knowledge Management.
Y-Y. Wang. 2010. Strategies for statistical spoken lan-
guage understanding with small amount of data - an
emprical study. Proc. Interspeech 2010.
</reference>
<page confidence="0.998377">
338
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.908743">
<title confidence="0.99979">A Joint Model for Discovery of Aspects in Utterances</title>
<author confidence="0.976452">Asli Celikyilmaz Dilek Hakkani-Tur</author>
<affiliation confidence="0.98066">Microsoft Microsoft</affiliation>
<address confidence="0.999092">Mountain View, CA, USA Mountain View, CA, USA</address>
<email confidence="0.958496">asli@ieee.orgdilek@ieee.org</email>
<abstract confidence="0.998982789473684">We describe a joint model for understanding user actions in natural language utterances. Our multi-layer generative approach uses both labeled and unlabeled utterances to jointly learn aspects regarding utterance’s target domain (e.g. movies), intention (e.g., finding a movie) along with other semantic units (e.g., movie name). We inject information extracted from unstructured web search query logs as prior information to enhance the generative process of the natural language utterance understanding model. Using utterances from five domains, our approach shows up to 4.5% improvement on domain and dialog act performance over cascaded approach in which each semantic component is learned sequentially and a supervised joint learning model (which requires fully labeled data).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Asuncion</author>
<author>M Welling</author>
<author>P Smyth</author>
<author>Y W Teh</author>
</authors>
<title>On smoothing and inference for topic models.</title>
<date>2009</date>
<publisher>UAI.</publisher>
<contexts>
<context position="20841" citStr="Asuncion et al., 2009" startWordPosition="3398" endWordPosition="3401">t of dialog acts Aud = a: ajA = ad|j C * d|j (2) G and slot components Sujd = s: sjS = d|j (3) E Then we update their base measures for a given u as: ua A =(�Nu j ajA )/Nu and us S=(�Nu j sj S )/Nu 4.1 Inference and Learning The goal of inference is to predict the domain, user’s act and slot distributions over each segment given an utterance. The MCM has the following set of parameters: domain-topic distributions BdD for each u, the act-topic distributions Bda A for each domain topic d of u, local slot-topic distributions for each domain BS, and OsS for slot-word distributions. Previous work (Asuncion et al., 2009; Wallach et al., 2009) shows that the choice of inference method has negligible effect on the probability of testing documents or inferred topics. Thus, we use Markov Chain Monte Carlo (MCMC) method,specifically Gibbs sampling, to model the posterior distribution PMCM(Du, Aud, Sujd|αu?D , αu?A , αu?S , Ci) by obtaining samples (Du, Aud, Sujd) drawn from this distribution. For each utterance u, we sample a domain Du and act Aud and hyper-parameters αD and αA and their base measures udD , uaA (from Eq. 1,2): ud Bd —_ Ndu + αD D Bda = Na |ud + αA D D Nu+ αu? A Nud + αu? D A The Ndu is the number</context>
</contexts>
<marker>Asuncion, Welling, Smyth, Teh, 2009</marker>
<rawString>A. Asuncion, M. Welling, P. Smyth, and Y. W. Teh. 2009. On smoothing and inference for topic models. UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bangalore</author>
</authors>
<title>Introduction to special issue of spoken language understanding in conversational systems.</title>
<date>2006</date>
<journal>In Speech Conversation,</journal>
<volume>48</volume>
<pages>233--238</pages>
<contexts>
<context position="1275" citStr="Bangalore, 2006" startWordPosition="185" endWordPosition="186">ral language utterance understanding model. Using utterances from five domains, our approach shows up to 4.5% improvement on domain and dialog act performance over cascaded approach in which each semantic component is learned sequentially and a supervised joint learning model (which requires fully labeled data). 1 Introduction Virtual personal assistance (VPA) is a human to machine dialog system, which is designed to perform tasks such as making reservations at restaurants, checking flight statuses, or planning weekend activities. A typical spoken language understanding (SLU) module of a VPA (Bangalore, 2006; Tur and Mori, 2011) defines a structured representation for utterances, in which the constituents correspond to meaning representations in terms of slot/value pairs (see Table 1). While target domain corresponds to the context of an utterance in a dialog, the dialog act represents overall intent of an utterance. The slots are entities, which are semantic constituents at the word or phrase level. Learning each component Sample utterances on ’plan a night out’ scenario (I) Show me theaters in [Austin] playing [iron man 2]. (II)I’m in the mood for [indian] food tonight, show me the ones [within</context>
</contexts>
<marker>Bangalore, 2006</marker>
<rawString>S. Bangalore. 2006. Introduction to special issue of spoken language understanding in conversational systems. In Speech Conversation, volume 48, pages 233–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Begeja</author>
<author>B Renger</author>
<author>Z Liu D Gibbon</author>
<author>B Shahraray</author>
</authors>
<title>Interactive machine learning techniques for improving slu models.</title>
<date>2004</date>
<booktitle>In Proceedings of the HLT-NAACL 2004 Workshop on Spoken Language Understanding for Conversational Systems and Higher Level Linguistic Information for Speech Processing.</booktitle>
<contexts>
<context position="2847" citStr="Begeja et al., 2004" startWordPosition="423" endWordPosition="426">ots. is a challenging task not only because there are no a priori constraints on what a user might say, but also systems must generalize from a tractably small amount of labeled training data. In this paper, we argue that each of these components are interdependent and should be modeled simultaneously. We build a joint understanding framework and introduce a multi-layer context model for semantic representation of utterances of multiple domains. Although different strategies can be applied, typically a cascaded approach is used where each semantic component is modeled separately/sequentially (Begeja et al., 2004), focusing less on interrelated aspects, i.e., dialog’s domain, user’s intentions, and semantic tags that can be shared across domains. Recent work on SLU (Jeong and Lee, 2008; Wang, 2010) presents joint modeling of two components, i.e., the domain and slot or dialog act and slot components together. Furthermore, most of these systems rely on labeled training utterances, focusing little on issues such as information sharing between the discourse and word level components across different domains, or variations in use of language. To deal with de330 Proceedings of the 50th Annual Meeting of the</context>
</contexts>
<marker>Begeja, Renger, Gibbon, Shahraray, 2004</marker>
<rawString>L. Begeja, B. Renger, Z. Liu D. Gibbon, and B. Shahraray. 2004. Interactive machine learning techniques for improving slu models. In Proceedings of the HLT-NAACL 2004 Workshop on Spoken Language Understanding for Conversational Systems and Higher Level Linguistic Information for Speech Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pauline M Berry</author>
<author>Melinda Gervasio</author>
<author>Bart Peintner</author>
<author>Neil Yorke-Smith</author>
</authors>
<title>Ptime: Personalized assistance for calendaring.</title>
<date>2011</date>
<booktitle>In ACM Transactions on Intelligent Systems and Technology,</booktitle>
<volume>2</volume>
<pages>1--40</pages>
<contexts>
<context position="5731" citStr="Berry et al., 2011" startWordPosition="862" endWordPosition="865">ring (Harabagiu and Hickl, 2006; Liang et al., 2011), entailment (Sammons et al., 2010), summarization (Hovy et al., 2005; Daume-III and Marcu, 2006), spoken language understanding (Tur and Mori, 2011; Dinarelli et al., 2009), query understanding (Popescu et al., 2010; Li, 2010; Reisinger and Pasca, 2011), etc. However data sources in VPA systems pose new challenges, such as variability and ambiguities in natural language, or short utterances that rarely contain contextual information, etc. Thus, SLU plays an important role in allowing any sophisticated spoken dialog system (e.g., DARPA Calo (Berry et al., 2011), Siri, etc.) to take the correct machine actions. A common approach to building SLU framework is to model its semantic components separately, assuming that the context (domain) is given a priori. Earlier work takes dialog act identification as a classification task to capture the user’s intentions (Margolis et al., 2010) and slot filling as a sequence learning task specific to a given domain class (Wang et al., 2009; Li, 2010). Since these tasks are considered as a pipeline, the errors of each component are transfered to the next, causing robustness issues. Ideally, these components should be</context>
</contexts>
<marker>Berry, Gervasio, Peintner, Yorke-Smith, 2011</marker>
<rawString>Pauline M. Berry, Melinda Gervasio, Bart Peintner, and Neil Yorke-Smith. 2011. Ptime: Personalized assistance for calendaring. In ACM Transactions on Intelligent Systems and Technology, volume 2, pages 1–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>A Ng</author>
<author>M Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research.</journal>
<contexts>
<context position="15643" citStr="Blei et al., 2003" startWordPosition="2508" endWordPosition="2511">a ← 1, ..., KA 4: draw dialog act dist. odaA ∼ Dir(a? A), 5: for each slot type s ← 1, ..., KS 6: draw slot dist. odsS ∼ Dir(a? S). 7: endfor 8: draw 0sS ∼ Dir(β) for each slot type s ← 1, ..., KS. 9: for each utterance u ← 1, ..., |U |do 10: Sample a domain Du∼Multi(odD) and, 11: and act topic Aud∼Multi(odaA ). 12: for words wuj, j ← 1, ..., Nu do 13: - Draw Sujd∼Multi(oDu,Su(j−1)d)# 14: - Sample wuj∼Multi(0Sujd). 15: end for 16: end for t Dir(α*D), Dir(α*A), Dir(α*S) are parameterized based on prior knowledge. t Here HMM assumption over utterance words is used. In hierarchical topic models (Blei et al., 2003; Mimno et al., 2007), etc., topics are represented as distributions over words, and each document expresses an admixture of these topics, both of which have symmetric Dirichlet (Dir) prior distributions. Symmetric Dirichlet distributions are often used, since there is typically no prior knowledge favoring one component over another. In the topic model literature, such constraints are sometimes used to deterministically allocate topic assignments to known labels (Labeled Topic Modeling (Ramage et al., 2009)) or in terms of pre-learnt topics encoded as prior knowledge on topic distributions in </context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>In Educational and Psychological Measurement,</booktitle>
<volume>20</volume>
<pages>37--46</pages>
<contexts>
<context position="24298" citStr="Cohen, 1960" startWordPosition="3991" endWordPosition="3992">re presented in parenthesis as italicized. the utterances obtained from (acoustic modeling engine) to train our models 4. Thus, our dataset contains 18084 NL utterances, 5034 of which are used for measuring the performance of our models. The dataset consists of five domain classes, i.e, movie, restaurant, hotel, event, other, 42 unique dialog acts and 41 slot tags. Each utterance is labeled with a domain, dialog act and a sequence of slot tags corresponding to segments in utterance (see examples in Table 1). Table 2 shows sample dialog act and slot labels. Annotation agreement, Kappa measure (Cohen, 1960), was around 85%. We pulled a month of web query logs and extracted over 2 million search queries from the movie, hotel, event, and restaurant domains. We also used generic web queries to compile a set of ’other’ domain queries. Our vocabulary consists of n-grams and segments (phrases) in utterances that are extracted using web n-grams and entity lists of §3. We extract distributions of n-grams and entities to inject as prior weights for entity list base (ψj.) and web n-gram context base measures (ψj�) (see §4). 5.2 Baselines and Experiment Setup We evaluated two baselines and two variants of </context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>J. Cohen. 1960. A coefficient of agreement for nominal scales. In Educational and Psychological Measurement, volume 20, pages 37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daume-III</author>
<author>D Marcu</author>
</authors>
<date>2006</date>
<note>Bayesian query focused summarization.</note>
<contexts>
<context position="5261" citStr="Daume-III and Marcu, 2006" startWordPosition="789" endWordPosition="792">esentation of seed labeled data and information from web queries as informative prior to design a novel utterance understanding model in §3 &amp; §4, (iii) comparison of our results to supervised sequential and joint learning methods on NL utterances in §5. We conclude that our generative model achieves noticeable improvement compared to discriminative models when labeled data is scarce. 2 Background Language understanding has been well studied in the context of question/answering (Harabagiu and Hickl, 2006; Liang et al., 2011), entailment (Sammons et al., 2010), summarization (Hovy et al., 2005; Daume-III and Marcu, 2006), spoken language understanding (Tur and Mori, 2011; Dinarelli et al., 2009), query understanding (Popescu et al., 2010; Li, 2010; Reisinger and Pasca, 2011), etc. However data sources in VPA systems pose new challenges, such as variability and ambiguities in natural language, or short utterances that rarely contain contextual information, etc. Thus, SLU plays an important role in allowing any sophisticated spoken dialog system (e.g., DARPA Calo (Berry et al., 2011), Siri, etc.) to take the correct machine actions. A common approach to building SLU framework is to model its semantic components</context>
</contexts>
<marker>Daume-III, Marcu, 2006</marker>
<rawString>H. Daume-III and D. Marcu. 2006. Bayesian query focused summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dinarelli</author>
<author>A Moschitti</author>
<author>G Riccardi</author>
</authors>
<title>Reranking models for spoken language understanding.</title>
<date>2009</date>
<booktitle>Proc. European Chapter of the Annual Meeting of the Association of Computational Linguistics (EACL).</booktitle>
<contexts>
<context position="5337" citStr="Dinarelli et al., 2009" startWordPosition="801" endWordPosition="804"> prior to design a novel utterance understanding model in §3 &amp; §4, (iii) comparison of our results to supervised sequential and joint learning methods on NL utterances in §5. We conclude that our generative model achieves noticeable improvement compared to discriminative models when labeled data is scarce. 2 Background Language understanding has been well studied in the context of question/answering (Harabagiu and Hickl, 2006; Liang et al., 2011), entailment (Sammons et al., 2010), summarization (Hovy et al., 2005; Daume-III and Marcu, 2006), spoken language understanding (Tur and Mori, 2011; Dinarelli et al., 2009), query understanding (Popescu et al., 2010; Li, 2010; Reisinger and Pasca, 2011), etc. However data sources in VPA systems pose new challenges, such as variability and ambiguities in natural language, or short utterances that rarely contain contextual information, etc. Thus, SLU plays an important role in allowing any sophisticated spoken dialog system (e.g., DARPA Calo (Berry et al., 2011), Siri, etc.) to take the correct machine actions. A common approach to building SLU framework is to model its semantic components separately, assuming that the context (domain) is given a priori. Earlier w</context>
</contexts>
<marker>Dinarelli, Moschitti, Riccardi, 2009</marker>
<rawString>M. Dinarelli, A. Moschitti, and G. Riccardi. 2009. Reranking models for spoken language understanding. Proc. European Chapter of the Annual Meeting of the Association of Computational Linguistics (EACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Favre</author>
<author>D Hakkani-T¨ur</author>
<author>Sebastien Cuendet</author>
</authors>
<date>2007</date>
<note>Icsiboost. http://code.google.come/ p/icsiboost.</note>
<marker>Favre, Hakkani-T¨ur, Cuendet, 2007</marker>
<rawString>B. Favre, D. Hakkani-T¨ur, and Sebastien Cuendet. 2007. Icsiboost. http://code.google.come/ p/icsiboost.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>A Hickl</author>
</authors>
<title>Methods for using textual entailment for question answering.</title>
<date>2006</date>
<pages>905--912</pages>
<contexts>
<context position="5143" citStr="Harabagiu and Hickl, 2006" startWordPosition="770" endWordPosition="773"> Bayesian framework for semantic parsing of natural language (NL) utterances in a unifying framework in §4, (ii) representation of seed labeled data and information from web queries as informative prior to design a novel utterance understanding model in §3 &amp; §4, (iii) comparison of our results to supervised sequential and joint learning methods on NL utterances in §5. We conclude that our generative model achieves noticeable improvement compared to discriminative models when labeled data is scarce. 2 Background Language understanding has been well studied in the context of question/answering (Harabagiu and Hickl, 2006; Liang et al., 2011), entailment (Sammons et al., 2010), summarization (Hovy et al., 2005; Daume-III and Marcu, 2006), spoken language understanding (Tur and Mori, 2011; Dinarelli et al., 2009), query understanding (Popescu et al., 2010; Li, 2010; Reisinger and Pasca, 2011), etc. However data sources in VPA systems pose new challenges, such as variability and ambiguities in natural language, or short utterances that rarely contain contextual information, etc. Thus, SLU plays an important role in allowing any sophisticated spoken dialog system (e.g., DARPA Calo (Berry et al., 2011), Siri, etc.</context>
</contexts>
<marker>Harabagiu, Hickl, 2006</marker>
<rawString>S. Harabagiu and A. Hickl. 2006. Methods for using textual entailment for question answering. pages 905– 912.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>C Y Lin</author>
<author>L Zhou</author>
</authors>
<title>A be-based multidocument summarizer with query interpretation.</title>
<date>2005</date>
<booktitle>Proc. DUC.</booktitle>
<contexts>
<context position="5233" citStr="Hovy et al., 2005" startWordPosition="785" endWordPosition="788">rk in §4, (ii) representation of seed labeled data and information from web queries as informative prior to design a novel utterance understanding model in §3 &amp; §4, (iii) comparison of our results to supervised sequential and joint learning methods on NL utterances in §5. We conclude that our generative model achieves noticeable improvement compared to discriminative models when labeled data is scarce. 2 Background Language understanding has been well studied in the context of question/answering (Harabagiu and Hickl, 2006; Liang et al., 2011), entailment (Sammons et al., 2010), summarization (Hovy et al., 2005; Daume-III and Marcu, 2006), spoken language understanding (Tur and Mori, 2011; Dinarelli et al., 2009), query understanding (Popescu et al., 2010; Li, 2010; Reisinger and Pasca, 2011), etc. However data sources in VPA systems pose new challenges, such as variability and ambiguities in natural language, or short utterances that rarely contain contextual information, etc. Thus, SLU plays an important role in allowing any sophisticated spoken dialog system (e.g., DARPA Calo (Berry et al., 2011), Siri, etc.) to take the correct machine actions. A common approach to building SLU framework is to m</context>
</contexts>
<marker>Hovy, Lin, Zhou, 2005</marker>
<rawString>E. Hovy, C.Y. Lin, and L. Zhou. 2005. A be-based multidocument summarizer with query interpretation. Proc. DUC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Jeong</author>
<author>G G Lee</author>
</authors>
<title>Triangular-chain conditional random fields.</title>
<date>2008</date>
<journal>EEE Transactions on Audio, Speech and Language Processing (IEEE-TASLP).</journal>
<contexts>
<context position="3022" citStr="Jeong and Lee, 2008" startWordPosition="450" endWordPosition="453">d training data. In this paper, we argue that each of these components are interdependent and should be modeled simultaneously. We build a joint understanding framework and introduce a multi-layer context model for semantic representation of utterances of multiple domains. Although different strategies can be applied, typically a cascaded approach is used where each semantic component is modeled separately/sequentially (Begeja et al., 2004), focusing less on interrelated aspects, i.e., dialog’s domain, user’s intentions, and semantic tags that can be shared across domains. Recent work on SLU (Jeong and Lee, 2008; Wang, 2010) presents joint modeling of two components, i.e., the domain and slot or dialog act and slot components together. Furthermore, most of these systems rely on labeled training utterances, focusing little on issues such as information sharing between the discourse and word level components across different domains, or variations in use of language. To deal with de330 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 330–338, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics pendency and language vari</context>
<context position="6937" citStr="Jeong and Lee, 2008" startWordPosition="1056" endWordPosition="1059">nts should be modeled simultaneously considering the dependencies between them. For example, in a local domain application, users may require information about a sub-domain (movies, hotels, etc.), and for each sub-domain, they may want to take different actions (find a movie, call a restaurant or book a hotel) using domain specific attributes (e.g., cuisine type of a restaurant, titles for movies or star-rating of a hotel). There’s been little attention in the literature on modeling the dependencies of SLU’s correlated structures. Only recent research has focused on the joint modeling of SLU (Jeong and Lee, 2008; Wang, 2010) taking into account the dependencies at learning time. In (Jeong and Lee, 2008), a triangular chain conditional random fields (Tri-CRF) approach is presented to model two of the SLU’s components in a single-pass. Their discriminative approach represents semantic slots and discourse-level utterance labels (domain or dialog act) in a single structure to encode dependencies. However, their model requires fully labeled utterances for training, which can be time consuming and expensive to generate for dynamic systems. Also, they can only learn dependencies between two components simul</context>
<context position="26329" citStr="Jeong and Lee, 2008" startWordPosition="4335" endWordPosition="4338">le to infer a meaningful set of dialog act (A) and slots (S), falling into broad categories of domain classes (D). ods, we used the Adaboost, utterance classification method that starts from a set of weak classifiers and builds a strong classifier by boosting the weak classifiers. Slot discovery is taken as a sequence labeling task in which segments in utterances are labeled (Li, 2010). For segment labeling we use SemiMarkov Conditional Random Fields (Semi-CRF) (Sarawagi and Cohen, 2004) method as a benchmark in evaluating semantic tagging performance. ? Tri-CRF: We used Triangular Chain CRF (Jeong and Lee, 2008) as our supervised joint model baseline. It is a state-of-the art method that learns the sequence labels and utterance class (domain or dialog act) as meta-sequence in a joint framework. It encodes the inter-dependence between the slot sequence s and meta-sequence label (d or a) using a triangular chain (dual-layer) structure. ? Base-MCM: Our first version injects an informative prior for domain, dialog act and slot topic distributions using information extracted from only labeled training utterances and inject as prior constraints (corpus n-gram base measure ψc) during topic assignments. ? We</context>
</contexts>
<marker>Jeong, Lee, 2008</marker>
<rawString>M. Jeong and G. G. Lee. 2008. Triangular-chain conditional random fields. EEE Transactions on Audio, Speech and Language Processing (IEEE-TASLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Li</author>
</authors>
<title>Understanding semantic structure of noun phrase queries.</title>
<date>2010</date>
<booktitle>Proc. of the Annual Meeting of the Association of Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="5390" citStr="Li, 2010" startWordPosition="811" endWordPosition="812">iii) comparison of our results to supervised sequential and joint learning methods on NL utterances in §5. We conclude that our generative model achieves noticeable improvement compared to discriminative models when labeled data is scarce. 2 Background Language understanding has been well studied in the context of question/answering (Harabagiu and Hickl, 2006; Liang et al., 2011), entailment (Sammons et al., 2010), summarization (Hovy et al., 2005; Daume-III and Marcu, 2006), spoken language understanding (Tur and Mori, 2011; Dinarelli et al., 2009), query understanding (Popescu et al., 2010; Li, 2010; Reisinger and Pasca, 2011), etc. However data sources in VPA systems pose new challenges, such as variability and ambiguities in natural language, or short utterances that rarely contain contextual information, etc. Thus, SLU plays an important role in allowing any sophisticated spoken dialog system (e.g., DARPA Calo (Berry et al., 2011), Siri, etc.) to take the correct machine actions. A common approach to building SLU framework is to model its semantic components separately, assuming that the context (domain) is given a priori. Earlier work takes dialog act identification as a classificati</context>
<context position="26097" citStr="Li, 2010" startWordPosition="4302" endWordPosition="4303">d * (NDu,S us + β u(j-1)ds + aSψS )* Nk (.) + V β NDu,s Su(j+1)d + ff(Suj−1, s) + ff(Suj+1, s) + αSψusS (5) 335 Figure 2: Sample topics discovered by Multi-Layer Context Model (MCM). Given samples of utterances, MCM is able to infer a meaningful set of dialog act (A) and slots (S), falling into broad categories of domain classes (D). ods, we used the Adaboost, utterance classification method that starts from a set of weak classifiers and builds a strong classifier by boosting the weak classifiers. Slot discovery is taken as a sequence labeling task in which segments in utterances are labeled (Li, 2010). For segment labeling we use SemiMarkov Conditional Random Fields (Semi-CRF) (Sarawagi and Cohen, 2004) method as a benchmark in evaluating semantic tagging performance. ? Tri-CRF: We used Triangular Chain CRF (Jeong and Lee, 2008) as our supervised joint model baseline. It is a state-of-the art method that learns the sequence labels and utterance class (domain or dialog act) as meta-sequence in a joint framework. It encodes the inter-dependence between the slot sequence s and meta-sequence label (d or a) using a triangular chain (dual-layer) structure. ? Base-MCM: Our first version injects a</context>
</contexts>
<marker>Li, 2010</marker>
<rawString>X. Li. 2010. Understanding semantic structure of noun phrase queries. Proc. of the Annual Meeting of the Association of Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>M I Jordan</author>
<author>D Klein</author>
</authors>
<title>Learning dependency based compositional semantics.</title>
<date>2011</date>
<contexts>
<context position="5164" citStr="Liang et al., 2011" startWordPosition="774" endWordPosition="777">antic parsing of natural language (NL) utterances in a unifying framework in §4, (ii) representation of seed labeled data and information from web queries as informative prior to design a novel utterance understanding model in §3 &amp; §4, (iii) comparison of our results to supervised sequential and joint learning methods on NL utterances in §5. We conclude that our generative model achieves noticeable improvement compared to discriminative models when labeled data is scarce. 2 Background Language understanding has been well studied in the context of question/answering (Harabagiu and Hickl, 2006; Liang et al., 2011), entailment (Sammons et al., 2010), summarization (Hovy et al., 2005; Daume-III and Marcu, 2006), spoken language understanding (Tur and Mori, 2011; Dinarelli et al., 2009), query understanding (Popescu et al., 2010; Li, 2010; Reisinger and Pasca, 2011), etc. However data sources in VPA systems pose new challenges, such as variability and ambiguities in natural language, or short utterances that rarely contain contextual information, etc. Thus, SLU plays an important role in allowing any sophisticated spoken dialog system (e.g., DARPA Calo (Berry et al., 2011), Siri, etc.) to take the correct</context>
</contexts>
<marker>Liang, Jordan, Klein, 2011</marker>
<rawString>P. Liang, M. I. Jordan, and D. Klein. 2011. Learning dependency based compositional semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Margolis</author>
<author>K Livescu</author>
<author>M Osterdorf</author>
</authors>
<title>Domain adaptation with unlabeled data for dialog act tagging.</title>
<date>2010</date>
<booktitle>In Proc. Workshop on Domain Adaptation for Natural Language Processing at the the Annual Meeting of the Association of Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="6054" citStr="Margolis et al., 2010" startWordPosition="914" endWordPosition="917"> data sources in VPA systems pose new challenges, such as variability and ambiguities in natural language, or short utterances that rarely contain contextual information, etc. Thus, SLU plays an important role in allowing any sophisticated spoken dialog system (e.g., DARPA Calo (Berry et al., 2011), Siri, etc.) to take the correct machine actions. A common approach to building SLU framework is to model its semantic components separately, assuming that the context (domain) is given a priori. Earlier work takes dialog act identification as a classification task to capture the user’s intentions (Margolis et al., 2010) and slot filling as a sequence learning task specific to a given domain class (Wang et al., 2009; Li, 2010). Since these tasks are considered as a pipeline, the errors of each component are transfered to the next, causing robustness issues. Ideally, these components should be modeled simultaneously considering the dependencies between them. For example, in a local domain application, users may require information about a sub-domain (movies, hotels, etc.), and for each sub-domain, they may want to take different actions (find a movie, call a restaurant or book a hotel) using domain specific at</context>
</contexts>
<marker>Margolis, Livescu, Osterdorf, 2010</marker>
<rawString>A. Margolis, K. Livescu, and M. Osterdorf. 2010. Domain adaptation with unlabeled data for dialog act tagging. In Proc. Workshop on Domain Adaptation for Natural Language Processing at the the Annual Meeting of the Association of Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Mimno</author>
<author>W Li</author>
<author>A McCallum</author>
</authors>
<title>Mixtures of hierarchical topics with pachinko allocation.</title>
<date>2007</date>
<booktitle>Proc. ICML.</booktitle>
<contexts>
<context position="8033" citStr="Mimno et al., 2007" startWordPosition="1227" endWordPosition="1230">onsuming and expensive to generate for dynamic systems. Also, they can only learn dependencies between two components simultaneously. Our approach differs from the earlier work- in that- we take the utterance understanding as a multilayered learning problem, and build a hierarchical clustering model. Our joint model can discover domain D, and user’s act A as higher layer latent concepts of utterances in relation to lower layer latent semantic topics (slots) S such as named-entities (”New York”) or context bearing non-named entities (”vegan”). Our work resembles the earlier work of PAM models (Mimno et al., 2007), i.e., directed acyclic graphs representing mixtures of hierarchical topic structures, where upper level topics are multinomial over lower level topics in a hierarchy. In an analogical way to earlier work, the D and A in our 331 approach represent common co-occurrence patterns (dependencies) between semantic tags S (Fig. 2). Concretely, correlated topics eliminate assignment of semantic tags to segments in an utterance that belong to other domains, e.g., we can discover that ”Show me vegan restaurants in San Francisco” has a low probably of outputting a movie-actor slot. Being generative, our</context>
<context position="15664" citStr="Mimno et al., 2007" startWordPosition="2512" endWordPosition="2515">raw dialog act dist. odaA ∼ Dir(a? A), 5: for each slot type s ← 1, ..., KS 6: draw slot dist. odsS ∼ Dir(a? S). 7: endfor 8: draw 0sS ∼ Dir(β) for each slot type s ← 1, ..., KS. 9: for each utterance u ← 1, ..., |U |do 10: Sample a domain Du∼Multi(odD) and, 11: and act topic Aud∼Multi(odaA ). 12: for words wuj, j ← 1, ..., Nu do 13: - Draw Sujd∼Multi(oDu,Su(j−1)d)# 14: - Sample wuj∼Multi(0Sujd). 15: end for 16: end for t Dir(α*D), Dir(α*A), Dir(α*S) are parameterized based on prior knowledge. t Here HMM assumption over utterance words is used. In hierarchical topic models (Blei et al., 2003; Mimno et al., 2007), etc., topics are represented as distributions over words, and each document expresses an admixture of these topics, both of which have symmetric Dirichlet (Dir) prior distributions. Symmetric Dirichlet distributions are often used, since there is typically no prior knowledge favoring one component over another. In the topic model literature, such constraints are sometimes used to deterministically allocate topic assignments to known labels (Labeled Topic Modeling (Ramage et al., 2009)) or in terms of pre-learnt topics encoded as prior knowledge on topic distributions in documents (Reisinger </context>
</contexts>
<marker>Mimno, Li, McCallum, 2007</marker>
<rawString>D. Mimno, W. Li, and A. McCallum. 2007. Mixtures of hierarchical topics with pachinko allocation. Proc. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Popescu</author>
<author>P Pantel</author>
<author>G Mishne</author>
</authors>
<title>Semantic lexicon adaptation for use in query interpretation. 19th World Wide Web Conference</title>
<date>2010</date>
<contexts>
<context position="5380" citStr="Popescu et al., 2010" startWordPosition="807" endWordPosition="810">ng model in §3 &amp; §4, (iii) comparison of our results to supervised sequential and joint learning methods on NL utterances in §5. We conclude that our generative model achieves noticeable improvement compared to discriminative models when labeled data is scarce. 2 Background Language understanding has been well studied in the context of question/answering (Harabagiu and Hickl, 2006; Liang et al., 2011), entailment (Sammons et al., 2010), summarization (Hovy et al., 2005; Daume-III and Marcu, 2006), spoken language understanding (Tur and Mori, 2011; Dinarelli et al., 2009), query understanding (Popescu et al., 2010; Li, 2010; Reisinger and Pasca, 2011), etc. However data sources in VPA systems pose new challenges, such as variability and ambiguities in natural language, or short utterances that rarely contain contextual information, etc. Thus, SLU plays an important role in allowing any sophisticated spoken dialog system (e.g., DARPA Calo (Berry et al., 2011), Siri, etc.) to take the correct machine actions. A common approach to building SLU framework is to model its semantic components separately, assuming that the context (domain) is given a priori. Earlier work takes dialog act identification as a cl</context>
</contexts>
<marker>Popescu, Pantel, Mishne, 2010</marker>
<rawString>A. Popescu, P. Pantel, and G. Mishne. 2010. Semantic lexicon adaptation for use in query interpretation. 19th World Wide Web Conference (WWW-10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ramage</author>
<author>D Hall</author>
<author>R Nallapati</author>
<author>C D Manning</author>
</authors>
<title>Labeled lda: A supervised topic model for credit attribution in multi-labeled corpora.</title>
<date>2009</date>
<booktitle>Proc. EMNLP.</booktitle>
<contexts>
<context position="16155" citStr="Ramage et al., 2009" startWordPosition="2584" endWordPosition="2587">owledge. t Here HMM assumption over utterance words is used. In hierarchical topic models (Blei et al., 2003; Mimno et al., 2007), etc., topics are represented as distributions over words, and each document expresses an admixture of these topics, both of which have symmetric Dirichlet (Dir) prior distributions. Symmetric Dirichlet distributions are often used, since there is typically no prior knowledge favoring one component over another. In the topic model literature, such constraints are sometimes used to deterministically allocate topic assignments to known labels (Labeled Topic Modeling (Ramage et al., 2009)) or in terms of pre-learnt topics encoded as prior knowledge on topic distributions in documents (Reisinger and Pas¸ca, 2009). Similar to previous work, we define a latent topic per each known semantic component label, e.g., five domain topics for five defined domains. Different from earlier work though, we also inject knowledge that we extract from several resources including entity lists from web search query click logs as well as seed labeled training utterances as prior information. We constrain the generation of the semantic components of our model by encoding prior knowledge in terms of</context>
</contexts>
<marker>Ramage, Hall, Nallapati, Manning, 2009</marker>
<rawString>D. Ramage, D. Hall, R. Nallapati, and C. D. Manning. 2009. Labeled lda: A supervised topic model for credit attribution in multi-labeled corpora. Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Reisinger</author>
<author>M Pas¸ca</author>
</authors>
<title>Latent variable models of concept-attribute attachement.</title>
<date>2009</date>
<booktitle>Proc. of the Annual Meeting of the Association of Computational Linguistics (ACL).</booktitle>
<marker>Reisinger, Pas¸ca, 2009</marker>
<rawString>J. Reisinger and M. Pas¸ca. 2009. Latent variable models of concept-attribute attachement. Proc. of the Annual Meeting of the Association of Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Reisinger</author>
<author>M Pasca</author>
</authors>
<title>Fine-grained class label markup of search queries.</title>
<date>2011</date>
<booktitle>In Proc. of the Annual Meeting of the Association of Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="5418" citStr="Reisinger and Pasca, 2011" startWordPosition="813" endWordPosition="816">rison of our results to supervised sequential and joint learning methods on NL utterances in §5. We conclude that our generative model achieves noticeable improvement compared to discriminative models when labeled data is scarce. 2 Background Language understanding has been well studied in the context of question/answering (Harabagiu and Hickl, 2006; Liang et al., 2011), entailment (Sammons et al., 2010), summarization (Hovy et al., 2005; Daume-III and Marcu, 2006), spoken language understanding (Tur and Mori, 2011; Dinarelli et al., 2009), query understanding (Popescu et al., 2010; Li, 2010; Reisinger and Pasca, 2011), etc. However data sources in VPA systems pose new challenges, such as variability and ambiguities in natural language, or short utterances that rarely contain contextual information, etc. Thus, SLU plays an important role in allowing any sophisticated spoken dialog system (e.g., DARPA Calo (Berry et al., 2011), Siri, etc.) to take the correct machine actions. A common approach to building SLU framework is to model its semantic components separately, assuming that the context (domain) is given a priori. Earlier work takes dialog act identification as a classification task to capture the user’</context>
</contexts>
<marker>Reisinger, Pasca, 2011</marker>
<rawString>J. Reisinger and M. Pasca. 2011. Fine-grained class label markup of search queries. In Proc. of the Annual Meeting of the Association of Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sammons</author>
<author>V Vydiswaran</author>
<author>D Roth</author>
</authors>
<title>Ask not what textual entailment can do for you...</title>
<date>2010</date>
<booktitle>In Proc. of the Annual Meeting of the Association of Computational Linguistics (ACL),</booktitle>
<location>Uppsala,</location>
<contexts>
<context position="5199" citStr="Sammons et al., 2010" startWordPosition="779" endWordPosition="783"> (NL) utterances in a unifying framework in §4, (ii) representation of seed labeled data and information from web queries as informative prior to design a novel utterance understanding model in §3 &amp; §4, (iii) comparison of our results to supervised sequential and joint learning methods on NL utterances in §5. We conclude that our generative model achieves noticeable improvement compared to discriminative models when labeled data is scarce. 2 Background Language understanding has been well studied in the context of question/answering (Harabagiu and Hickl, 2006; Liang et al., 2011), entailment (Sammons et al., 2010), summarization (Hovy et al., 2005; Daume-III and Marcu, 2006), spoken language understanding (Tur and Mori, 2011; Dinarelli et al., 2009), query understanding (Popescu et al., 2010; Li, 2010; Reisinger and Pasca, 2011), etc. However data sources in VPA systems pose new challenges, such as variability and ambiguities in natural language, or short utterances that rarely contain contextual information, etc. Thus, SLU plays an important role in allowing any sophisticated spoken dialog system (e.g., DARPA Calo (Berry et al., 2011), Siri, etc.) to take the correct machine actions. A common approach</context>
</contexts>
<marker>Sammons, Vydiswaran, Roth, 2010</marker>
<rawString>M. Sammons, V. Vydiswaran, and D. Roth. 2010. Ask not what textual entailment can do for you... In Proc. of the Annual Meeting of the Association of Computational Linguistics (ACL), Uppsala, Sweden, 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sarawagi</author>
<author>W W Cohen</author>
</authors>
<title>Semimarkov conditional random fields for information extraction.</title>
<date>2004</date>
<booktitle>Proc. NIPS.</booktitle>
<contexts>
<context position="26201" citStr="Sarawagi and Cohen, 2004" startWordPosition="4315" endWordPosition="4318">+1, s) + αSψusS (5) 335 Figure 2: Sample topics discovered by Multi-Layer Context Model (MCM). Given samples of utterances, MCM is able to infer a meaningful set of dialog act (A) and slots (S), falling into broad categories of domain classes (D). ods, we used the Adaboost, utterance classification method that starts from a set of weak classifiers and builds a strong classifier by boosting the weak classifiers. Slot discovery is taken as a sequence labeling task in which segments in utterances are labeled (Li, 2010). For segment labeling we use SemiMarkov Conditional Random Fields (Semi-CRF) (Sarawagi and Cohen, 2004) method as a benchmark in evaluating semantic tagging performance. ? Tri-CRF: We used Triangular Chain CRF (Jeong and Lee, 2008) as our supervised joint model baseline. It is a state-of-the art method that learns the sequence labels and utterance class (domain or dialog act) as meta-sequence in a joint framework. It encodes the inter-dependence between the slot sequence s and meta-sequence label (d or a) using a triangular chain (dual-layer) structure. ? Base-MCM: Our first version injects an informative prior for domain, dialog act and slot topic distributions using information extracted from</context>
</contexts>
<marker>Sarawagi, Cohen, 2004</marker>
<rawString>S. Sarawagi and W. W. Cohen. 2004. Semimarkov conditional random fields for information extraction. Proc. NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sauper</author>
<author>A Haghighi</author>
<author>R Barzilay</author>
</authors>
<title>Content models with attitude.</title>
<date>2011</date>
<booktitle>In Proc. of the Annual Meeting of the Association of Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="14208" citStr="Sauper et al., 2011" startWordPosition="2239" endWordPosition="2242">ce u is associated with d = 1..KD multinomial domain-topic distributions odD. Each domain d, is represented as a distribution over a = 1, .., KA dialog acts odaA (odD → odaA ). In our MCM model, we assume that each utterance is represented as a hidden Markov model with KS slot states. Each state generates n-grams according to a multinomial n-gram distribution. Once domain Du and act Aud topics are sampled for u, a slot state topic Sujd is drawn to generate each segment wuj of u by considering the word-tag sequence frequencies based on a simple HMM assumption, similar to the content models of (Sauper et al., 2011). Initial and transition probability distributions over the HMM states are sampled from Dirichlet distribution over slots ods S .Each slot state s generates words according to multinomial word distribution 0sS. We also keep track of the frequency of vocabulary terms wj’s in a V ×KD matrix MD. Every time a wj is sampled for a domain d, we increment its count, a degree of domain bearing words. Similarly, we keep track of dialog act and slot bearing words in V ×KA and V ×KS matrices, MA and MS (shown as red arrows in Fig 1). Being Bayesian, each distribution odD, oadA, and odsS is sampled from a </context>
</contexts>
<marker>Sauper, Haghighi, Barzilay, 2011</marker>
<rawString>C. Sauper, A. Haghighi, and R. Barzilay. 2011. Content models with attitude. In Proc. of the Annual Meeting of the Association of Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Tur</author>
<author>R De Mori</author>
</authors>
<title>Spoken language understanding: Systems for extracting semantic information from speech.</title>
<date>2011</date>
<publisher>Wiley.</publisher>
<marker>Tur, De Mori, 2011</marker>
<rawString>G. Tur and R. De Mori. 2011. Spoken language understanding: Systems for extracting semantic information from speech. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Wallach</author>
<author>D Mimno</author>
<author>A McCallum</author>
</authors>
<title>Rethinking lda: Why priors</title>
<date>2009</date>
<tech>Ph.D. Thesis,</tech>
<institution>University of Cambridge.</institution>
<contexts>
<context position="20864" citStr="Wallach et al., 2009" startWordPosition="3402" endWordPosition="3405">a: ajA = ad|j C * d|j (2) G and slot components Sujd = s: sjS = d|j (3) E Then we update their base measures for a given u as: ua A =(�Nu j ajA )/Nu and us S=(�Nu j sj S )/Nu 4.1 Inference and Learning The goal of inference is to predict the domain, user’s act and slot distributions over each segment given an utterance. The MCM has the following set of parameters: domain-topic distributions BdD for each u, the act-topic distributions Bda A for each domain topic d of u, local slot-topic distributions for each domain BS, and OsS for slot-word distributions. Previous work (Asuncion et al., 2009; Wallach et al., 2009) shows that the choice of inference method has negligible effect on the probability of testing documents or inferred topics. Thus, we use Markov Chain Monte Carlo (MCMC) method,specifically Gibbs sampling, to model the posterior distribution PMCM(Du, Aud, Sujd|αu?D , αu?A , αu?S , Ci) by obtaining samples (Du, Aud, Sujd) drawn from this distribution. For each utterance u, we sample a domain Du and act Aud and hyper-parameters αD and αA and their base measures udD , uaA (from Eq. 1,2): ud Bd —_ Ndu + αD D Bda = Na |ud + αA D D Nu+ αu? A Nud + αu? D A The Ndu is the number of occurrences of doma</context>
</contexts>
<marker>Wallach, Mimno, McCallum, 2009</marker>
<rawString>H. Wallach, D. Mimno, and A. McCallum. 2009. Rethinking lda: Why priors matter. NIPS. H. Wallach. 2008. Structured topic models for language. Ph.D. Thesis, University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Y Wang</author>
<author>R Hoffman</author>
<author>X Li</author>
<author>J Syzmanski</author>
</authors>
<title>Semi-supervised learning of semantic classes for query understanding from the web and for the web.</title>
<date>2009</date>
<booktitle>In The 18th ACM Conference on Information and Knowledge Management.</booktitle>
<contexts>
<context position="6151" citStr="Wang et al., 2009" startWordPosition="932" endWordPosition="935">age, or short utterances that rarely contain contextual information, etc. Thus, SLU plays an important role in allowing any sophisticated spoken dialog system (e.g., DARPA Calo (Berry et al., 2011), Siri, etc.) to take the correct machine actions. A common approach to building SLU framework is to model its semantic components separately, assuming that the context (domain) is given a priori. Earlier work takes dialog act identification as a classification task to capture the user’s intentions (Margolis et al., 2010) and slot filling as a sequence learning task specific to a given domain class (Wang et al., 2009; Li, 2010). Since these tasks are considered as a pipeline, the errors of each component are transfered to the next, causing robustness issues. Ideally, these components should be modeled simultaneously considering the dependencies between them. For example, in a local domain application, users may require information about a sub-domain (movies, hotels, etc.), and for each sub-domain, they may want to take different actions (find a movie, call a restaurant or book a hotel) using domain specific attributes (e.g., cuisine type of a restaurant, titles for movies or star-rating of a hotel). There</context>
</contexts>
<marker>Wang, Hoffman, Li, Syzmanski, 2009</marker>
<rawString>Y.Y. Wang, R. Hoffman, X. Li, and J. Syzmanski. 2009. Semi-supervised learning of semantic classes for query understanding from the web and for the web. In The 18th ACM Conference on Information and Knowledge Management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y-Y Wang</author>
</authors>
<title>Strategies for statistical spoken language understanding with small amount of data - an emprical study.</title>
<date>2010</date>
<booktitle>Proc. Interspeech</booktitle>
<contexts>
<context position="3035" citStr="Wang, 2010" startWordPosition="454" endWordPosition="455">his paper, we argue that each of these components are interdependent and should be modeled simultaneously. We build a joint understanding framework and introduce a multi-layer context model for semantic representation of utterances of multiple domains. Although different strategies can be applied, typically a cascaded approach is used where each semantic component is modeled separately/sequentially (Begeja et al., 2004), focusing less on interrelated aspects, i.e., dialog’s domain, user’s intentions, and semantic tags that can be shared across domains. Recent work on SLU (Jeong and Lee, 2008; Wang, 2010) presents joint modeling of two components, i.e., the domain and slot or dialog act and slot components together. Furthermore, most of these systems rely on labeled training utterances, focusing little on issues such as information sharing between the discourse and word level components across different domains, or variations in use of language. To deal with de330 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 330–338, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics pendency and language variability issue</context>
<context position="6950" citStr="Wang, 2010" startWordPosition="1060" endWordPosition="1061"> simultaneously considering the dependencies between them. For example, in a local domain application, users may require information about a sub-domain (movies, hotels, etc.), and for each sub-domain, they may want to take different actions (find a movie, call a restaurant or book a hotel) using domain specific attributes (e.g., cuisine type of a restaurant, titles for movies or star-rating of a hotel). There’s been little attention in the literature on modeling the dependencies of SLU’s correlated structures. Only recent research has focused on the joint modeling of SLU (Jeong and Lee, 2008; Wang, 2010) taking into account the dependencies at learning time. In (Jeong and Lee, 2008), a triangular chain conditional random fields (Tri-CRF) approach is presented to model two of the SLU’s components in a single-pass. Their discriminative approach represents semantic slots and discourse-level utterance labels (domain or dialog act) in a single structure to encode dependencies. However, their model requires fully labeled utterances for training, which can be time consuming and expensive to generate for dynamic systems. Also, they can only learn dependencies between two components simultaneously. Ou</context>
</contexts>
<marker>Wang, 2010</marker>
<rawString>Y-Y. Wang. 2010. Strategies for statistical spoken language understanding with small amount of data - an emprical study. Proc. Interspeech 2010.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>