<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9996565">
A Machine Learning Approach to
Modeling Scope Preferences
</title>
<author confidence="0.999599">
Derrick Higgins∗ Jerrold M. Sadock†
</author>
<affiliation confidence="0.999098">
University of Chicago University of Chicago
</affiliation>
<bodyText confidence="0.999487">
This article describes a corpus-based investigation of quantifier scope preferences. Following
recent work on multimodular grammar frameworks in theoretical linguistics and a long history
of combining multiple information sources in natural language processing, scope is treated as a
distinct module of grammar from syntax. This module incorporates multiple sources of evidence
regarding the most likely scope readingfor a sentence and is entirely data-driven. The experiments
discussed in this article evaluate the performance of our models in predicting the most likely scope
reading for a particular sentence, using Penn Treebank data both with and without syntactic
annotation. We wish to focus attention on the issue of determining scope preferences, which has
largely been ignored in theoretical linguistics, and to explore different models of the interaction
between syntax and quantifier scope.
</bodyText>
<sectionHeader confidence="0.978727" genericHeader="abstract">
1. Overview
</sectionHeader>
<bodyText confidence="0.999904142857143">
This article addresses the issue of determining the most accessible quantifier scope
reading for a sentence. Quantifiers are elements of natural and logical languages (such
as each, no, and some in English and ∀ and ∃ in predicate calculus) that have certain
semantic properties. Loosely speaking, they express that a proposition holds for some
proportion of a set of individuals. One peculiarity of these expressions is that there
can be semantic differences that depend on the order in which the quantifiers are
interpreted. These are known as scope differences.
</bodyText>
<listItem confidence="0.677578">
(1) Everyone likes two songs on this album.
</listItem>
<bodyText confidence="0.999945166666667">
As an example of the sort of interpretive differences we are talking about, consider
the sentence in (1). There are two readings of this sentence; which reading is meant
depends on which of the two quantified expressions everyone and two songs on this
album takes wide scope. The first reading, in which everyone takes wide scope, simply
implies that every person has a certain preference, not necessarily related to anyone
else’s. This reading can be paraphrased as “Pick any person, and that person will like
two songs on this album.” The second reading, in which everyone takes narrow scope,
implies that there are two specific songs on the album of which everyone is fond, say,
“Blue Moon” and “My Way.”
In theoretical linguistics, attention has been primarily focused on the issue of scope
generation. Researchers applying the techniques of quantifier raising and Cooper stor-
age have been concerned mainly with enumerating all of the scope readings for a
</bodyText>
<affiliation confidence="0.762324333333333">
∗ Department of Linguistics, University of Chicago, 1010 East 59th Street, Chicago, IL 60637. E-mail:
dchiggin@alumni.uchicago.edu.
† Department of Linguistics, University of Chicago, 1010 East 59th Street, Chicago, IL 60637. E-mail:
</affiliation>
<email confidence="0.544795">
j-sadock@uchicago.edu.
</email>
<note confidence="0.802495">
© 2003 Association for Computational Linguistics
Computational Linguistics Volume 29, Number 1
</note>
<bodyText confidence="0.999889464285714">
sentence that are possible, without regard to their relative likelihood or naturalness.
Recently, however, linguists such as Kuno, Takami, and Wu (1999) have begun to turn
their attention to scope prediction, or determining the relative accessibility of different
scope readings.
In computational linguistics, more attention has been paid to the factors that de-
termine scope preferences. Systems such as the SRI Core Language Engine (Moran
1988; Moran and Pereira 1992), LUNAR (Woods 1986), and TEAM (Martin, Appelt,
and Pereira 1986) have employed scope critics that use heuristics to decide between
alternative scopings. However, the rules that these systems use in making quantifier
scope decisions are motivated only by the researchers’ intuitions, and no empirical
results have been published regarding their accuracy.
In this article, we use the tools of machine learning to construct a data-driven
model of quantifier scope preferences. For theoretical linguistics, this model serves as
an illustration that Kuno, Takami, and Wu’s approach can capture some of the clear-
est generalizations about quantifier scoping. For computational linguistics, this article
provides a baseline result on the task of scope prediction, with which other scope
critics can be compared. In addition, it is the most extensive empirical investigation
of which we are aware that collects data of any kind regarding the relative frequency
of different quantifier scope readings in English text.1
Section 2 briefly discusses treatments of scoping issues in theoretical linguistics,
and Section 3 reviews the computational work that has been done on natural language
quantifier scope. In Section 4 we introduce the models that we use to predict quantifier
scoping, as well as the data on which they are trained and tested. Section 5 combines
the scope model of the previous section with a probabilistic context-free grammar
(PCFG) model of syntax and addresses the issue of whether these two modules of
grammar ought to be combined in serial, with information from the syntax feeding the
quantifier scope module, or in parallel, with each module constraining the structures
provided by the other.
</bodyText>
<sectionHeader confidence="0.922624" genericHeader="method">
2. Approaches to Quantifier Scope in Theoretical Linguistics
</sectionHeader>
<bodyText confidence="0.99984375">
Most, if not all, linguistic treatments of quantifier scope have closely integrated it with
the way in which the syntactic structure of a sentence is built up. Montague (1973) used
a syntactic rule to introduce a quantified expression into a derivation at the point where
it was to take scope, whereas generative semantic analyses such as McCawley (1998)
represented the scope of quantification at deep structure, transformationally lowering
quantifiers into their surface positions during the course of the derivation. More recent
work in the interpretive paradigm takes the opposite approach, extracting quantifiers
from their surface positions to their scope positions by means of a quantifier-raising
(QR) transformation (May 1985; Aoun and Li 1993; Hornstein 1995). Another popular
technique is to percolate scope information up through the syntactic tree using Cooper
storage (Cooper 1983; Hobbs and Shieber 1987; Pollard 1989; Nerbonne 1993; Park 1995;
Pollard and Yoo 1998).
The QR approach to dealing with scope in linguistics consists in the claim that
there is a covert transformation applying to syntactic structures that moves quantified
elements out of the position in which they are found on the surface and raises them to
a higher position that reflects their scope. The various incarnations of the strategy that
</bodyText>
<note confidence="0.4398845">
1 See Carden (1976), however, for a questionnaire-based approach to gathering data on the accessibility
of different quantifier scope readings.
</note>
<page confidence="0.995169">
74
</page>
<note confidence="0.923666">
Higgins and Sadock Modeling Scope Preferences
</note>
<figureCaption confidence="0.983158">
Figure 1
</figureCaption>
<bodyText confidence="0.990003735294118">
Simple illustration of the QR approach to quantifier scope generation.
follows from this claim differ in the precise characterization of this QR transforma-
tion, what conditions are placed upon it, and what tree-configurational relationship
is required for one operator to take scope over another. The general idea of QR is
represented in Figure 1, a schematic analysis of the reading of the sentence Someone
saw everyone in which someone takes wide scope (i.e., ‘there is some person x such that
for all persons y, x saw y’).
In the Cooper storage approach, quantifiers are gathered into a store and passed
upward through a syntactic tree. At certain nodes along the way, quantifiers may be
retrieved from the store and take scope. The relative scope of quantifiers is determined
by where each quantifier is retrieved from the store, with quantifiers higher in the tree
taking wide scope over lower ones. As with QR, different authors implement this
scheme in slightly different ways, but the simplest case is represented in Figure 2, the
Cooper storage analog of Figure 1.
These structural approaches, QR and Cooper storage, have in common that they
allow syntactic factors to have an effect only on the scope readings that are available for
a given sentence. They are also similar in addressing only the issue of scope generation,
or identifying all and only the accessible readings for each sentence. That is to say,
they do not address the issue of the relative salience of these readings.
Kuno, Takami, and Wu (1999, 2001) propose to model the scope of quantified
elements with a set of interacting expert systems that basically consists of a weighted
vote taken of the various factors that may influence scope readings. This model is
meant to account not only for scope generation, but also for “the relative strengths of
the potential scope interpretations of a given sentence” (1999, page 63). They illustrate
the plausibility of this approach in their paper by presenting a number of examples
that are accounted for fairly well by the approach even when an unweighted vote of
the factors is allowed to be taken.
So, for example, in Kuno, Takami and Wu’s (49b) (1999), repeated here as (2), the
correct prediction is made: that the sentence is unambiguous with the first quantified
noun phrase (NP) taking wide scope over the second (the reading in which we don’t
all have to hate the same people). Table 1 illustrates how the votes of each of Kuno,
Takami, and Wu’s “experts” contribute to this outcome. Since the expression many of
us/you receives more votes, and the numbers for the two competing quantified expres-
sions are quite far apart, the first one is predicted to take wide scope unambiguously.
</bodyText>
<listItem confidence="0.510899">
(2) Many of us/you hate some of them.
</listItem>
<page confidence="0.994317">
75
</page>
<note confidence="0.629223">
Computational Linguistics Volume 29, Number 1
</note>
<figureCaption confidence="0.947836">
Figure 2
</figureCaption>
<note confidence="0.319404">
Simple illustration of the Cooper storage approach to quantifier scope generation.
</note>
<tableCaption confidence="0.965014">
Table 1
</tableCaption>
<bodyText confidence="0.601603666666667">
Voting to determine optimal scope readings for quantifiers, according to Kuno, Takami, and
Wu (1999).
many of us/you some of them
</bodyText>
<equation confidence="0.8140705">
V V
Baseline:
V
Subject Q:
V
Lefthand Q:
V
Speaker/Hearer Q:
</equation>
<bodyText confidence="0.972645714285714">
Total: 4 1
Some adherents of the structural approaches also seem to acknowledge the ne-
cessity of eventually coming to terms with the factors that play a role in determining
scope preferences in language. Aoun and Li (2000) claim that the lexical scope pref-
erences of quantifiers “are not ruled out under a structural account” (page 140). It is
clear from the surrounding discussion, though, that they intend such lexical require-
ments to be taken care of in some nonsyntactic component of grammar. Although
Kuno, Takami, and Wu’s dialogue with Aoun and Li in Language has been portrayed
by both sides as a debate over the correct way of modeling quantifier scope, they are
not really modeling the same things. Whereas Aoun and Li (1993) provide an account
of scope generation, Kuno, Takami, and Wu (1999) intend to model both scope gen-
eration and scope prediction. The model of scope preferences provided in this article
is an empirically based refinement of the approach taken by Kuno, Takami, and Wu,
but in principle it is consistent with a structural account of scope generation.
</bodyText>
<page confidence="0.918155">
76
</page>
<note confidence="0.488758">
Higgins and Sadock Modeling Scope Preferences
</note>
<sectionHeader confidence="0.548926" genericHeader="method">
3. Approaches to Quantifier Scope in Computational Linguistics
</sectionHeader>
<bodyText confidence="0.999919633333333">
Many studies, such as Pereira (1990) and Park (1995), have dealt with the issue of
scope generation from a computational perspective. Attempts have also been made
in computational work to extend a pure Cooper storage approach to handle scope
prediction. Hobbs and Shieber (1987) discuss the possibility of incorporating some sort
of ordering heuristics into the SRI scope generation system, in the hopes of producing
a ranked list of possible scope readings, but ultimately are forced to acknowledge that
“[t]he modifications turn out to be quite complicated if we wish to order quantifiers
according to lexical heuristics, such as having each out-scope some. Because of the
recursive nature of the algorithm, there are limits to the amount of ordering that can
be done in this manner” (page 55). The stepwise nature of these scope mechanisms
makes it hard to state the factors that influence the preference for one quantifier to
take scope over another.
Those natural language processing (NLP) systems that have managed to provide
some sort of account of quantifier scope preferences have done so by using a separate
system of heuristics (or scope critics) that apply postsyntactically to determine the most
likely scoping. LUNAR (Woods 1986), TEAM (Martin, Appelt, and Pereira 1986), and
the SRI Core Language Engine as described by Moran (1988; Moran and Pereira 1992)
all employ scope rules of this sort. By and large, these rules are of an ad hoc nature,
implementing a linguist’s intuitive idea of what factors determine scope possibilities,
and no results have been published regarding the accuracy of these methods. For
example, Moran (1988) incorporates rules from other NLP systems and from VanLehn
(1978), such as a preference for a logically weaker interpretation, the tendency for each
to take wide scope, and a ban on raising a quantifier across multiple major clause
boundaries. The testing of Moran’s system is “limited to checking conformance to
the stated rules” (pages 40–41). In addition, these systems are generally incapable of
handling unrestricted text such as that found in the Wall Street Journal corpus in a
robust way, because they need to do a full semantic analysis of a sentence in order
to make scope predictions. The statistical basis of the model presented in this article
offers increased robustness and the possibility of more serious evaluation on the basis
of corpus data.
</bodyText>
<subsectionHeader confidence="0.528201">
4. Modeling Quantifier Scope
</subsectionHeader>
<bodyText confidence="0.999900153846154">
In this section, we argue for an empirically driven machine learning approach to
the identification of factors relevant to quantifier scope and the modeling of scope
preferences. Following much recent work that applies the tools of machine learning to
linguistic problems (Brill 1995; Pedersen 2000; van Halteren, Zavrel, and Daelemans
2001; Soon, Ng, and Lim 2001), we will treat the prediction of quantifier scope as
an example of a classification task. Our aim is to provide a robust model of scope
prediction based on Kuno, Takami, and Wu’s theoretical foundation and to address
the serious lack of empirical results regarding quantifier scope in computational work.
We describe here the modeling tools borrowed from the field of artificial intelligence
for the scope prediction task and the data from which the generalizations are to be
learned. Finally, we present the results of training different incarnations of our scope
module on the data and assess the implications of this exercise for theoretical and
computational linguistics.
</bodyText>
<page confidence="0.995683">
77
</page>
<note confidence="0.447926">
Computational Linguistics Volume 29, Number 1
</note>
<subsectionHeader confidence="0.997745">
4.1 Classification in Machine Learning
</subsectionHeader>
<bodyText confidence="0.999782111111111">
Determining which among multiple quantifiers in a sentence takes wide scope, given
a number of different sources of evidence, is an example of what is known in machine
learning as a classification task (Mitchell 1996). There are many types of classifiers
that may be applied to this task that both are more sophisticated than the approach
suggested by Kuno, Takami, and Wu and have a more solid probabilistic foundation.
These include the naive Bayes classifier (Manning and Sch¨utze 1999; Jurafsky and
Martin 2000), maximum-entropy models (Berger, Della Pietra, and Della Pietra 1996;
Ratnaparkhi 1997), and the single-layer perceptron (Bishop 1995). We employ these
classifier models here primarily because of their straightforward probabilistic inter-
pretation and their similarity to the scope model of Kuno, Takami, and Wu (since they
each could be said to implement a kind of weighted voting of factors). In Section 4.3,
we describe how classifiers of these types can be constructed to serve as a grammatical
module responsible for quantifier scope determination.
All of these classifiers can be trained in a supervised manner. That is, given a sam-
ple of training data that provides all of the information that is deemed to be relevant
to quantifier scope and the actual scope reading assigned to a sentence, these classi-
fiers will attempt to extract generalizations that can be fruitfully applied in classifying
as-yet-unseen examples.
</bodyText>
<subsectionHeader confidence="0.99085">
4.2 Data
</subsectionHeader>
<bodyText confidence="0.986041866666667">
The data on which the quantifier scope classifiers are trained and tested is an extract
from the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993) that we have
tagged to indicate the most salient scope interpretation of each sentence in context.
Figure 3 shows an example of a training sentence with the scope reading indicated.
The quantifier lower in the tree bears the tag “Q1,” and the higher quantifier bears the
tag “Q2,” so this sentence is interpreted such that the lower quantifier has wide scope.
Reversing the tags would have meant that the higher quantifier takes wide scope, and
while if both quantifiers had been marked “Q1,” this would have indicated that there
is no scope interaction between them (as when they are logically independent or take
scope in different conjuncts of a conjoined phrase).2
The sentences tagged were chosen from the Wall Street Journal (WSJ) section of
the Penn Treebank to have a certain set of attributes that simplify the task of design-
ing the quantifier scope module of the grammar. First, in order to simplify the coding
process, each sentence has exactly two scope-taking elements of the sort considered
for this project.3 These include most NPs that begin with a determiner, predeterminer,
or quantifier phrase (QP)4 but exclude NPs in which the determiner is a, an, or the. Ex-
2 This “no interaction” class is a sort of “elsewhere” category that results from phrasing the classification
question as “Which quantifier takes wider scope in the preferred reading?” Where there is no scope
interaction, the answer is “neither.” This includes cases in which the relative scope of operators does
not correspond to a difference in meaning, as in One woman bought one horse, or when they take scope
in different propositional domains, such as in Mary bought two horses and sold three sheep. The human
coders used in this study were instructed to choose class 0 whenever there was not a clear preference
for one of the two scope readings.
3 This restriction that each sentence contain only two quantified elements does not actually exclude
many sentences from consideration. We identified only 61 sentences with three quantifiers of the sort
we consider and 12 sentences with four. In addition, our review of these sentences revealed that many
of them simply involve lists in which the quantifiers do not interact in terms of scope (as in, for
example, “We ask that you turn off all cell phones, extinguish all cigarettes, and open any candy before
the performance begins”). Thus, the class of sentences with more than two quantifiers is small and
seems to involve even simpler quantifier interactions than those found in our corpus.
</bodyText>
<footnote confidence="0.636928">
4 These categories are intended to be understood as they are used in the tagging and parsing of the Penn
Treebank. See Santorini (1990) and Bies et al. (1995) for details; the Appendix lists selected codes used
</footnote>
<page confidence="0.994038">
78
</page>
<bodyText confidence="0.419278">
Higgins and Sadock Modeling Scope Preferences
</bodyText>
<equation confidence="0.9997748">
( (S
(NP-SBJ
(NP (DT Those) )
(SBAR
(WHNP-1 (WP who) )
(S
(NP-SBJ-2 (-NONE- *T*-1) )
(ADVP (RB still) )
(VP (VBP want)
(S
(NP-SBJ (-NONE- *-2) )
(VP (TO to)
(VP (VB do)
(NP (PRP it) ))))))))
(44 44)
(VP (MD will)
(ADVP (RB just) )
(VP (VB find)
(NP
(NP (DT-Q2 some) (NN way) )
(SBAR
(WHADVP-3 (-NONE- 0) )
(S
(NP-SBJ (-NONE- *) )
(VP (TO to)
(VP (VB get)
(PP (IN around) (’’ ’’)
(NP (DT-Q1 any) (NN attempt)
(S
(NP-SBJ (-NONE- *) )
(VP (TO to)
(VP (VB curb)
(NP (PRP it) ))))))
(ADVP-MNR (-NONE- *T*-3) ))))))))
(. .) ))
</equation>
<figureCaption confidence="0.577053">
Figure 3
</figureCaption>
<bodyText confidence="0.8396275">
Tagged Wall Street Journal text from the Penn Treebank. The lower quantifier takes wide
scope, indicated by its tag “Q1.”
cluding these determiners from consideration largely avoids the problem of generics
and the complexities of assigning scope readings to definite descriptions. In addi-
tion, only sentences that had the root node S were considered. This serves to exclude
sentence fragments and interrogative sentence types. Our data set therefore differs
systematically from the full WSJ corpus, but we believe it is sufficient to allow many
generalizations about English quantification to be induced. Given these restrictions on
the input data, the task of the scope classifier is a choice among three alternatives:5
(Class 0) There is no scopal interaction.
(Class 1) The first quantifier takes wide scope.
(Class 2) The second quantifier takes wide scope.
for annotating the Penn Treebank corpus. The category QP is particularly unintuitive in that it does not
correspond to a quantified noun phrase, but to a measure expression, such as more than half.
5 Some linguists may find it strange that we have chosen to treat the choice of preferred scoping for two
quantified elements as a tripartite decision, since the possibility of independence is seldom treated in
the linguistic literature. As we are dealing with corpus data in this experiment, we cannot afford to
ignore this possibility.
</bodyText>
<page confidence="0.986607">
79
</page>
<note confidence="0.421841">
Computational Linguistics Volume 29, Number 1
</note>
<bodyText confidence="0.999979394736842">
The result is a set of 893 sentences,6 annotated with Penn Treebank II parse trees and
hand-tagged for the primary scope reading.
To assess the reliability of the hand-tagged data used in this project, the data were
coded a second time by an independent coder, in addition to the reference coding.
The independent codings agreed with the reference coding on 76.3% of sentences. The
kappa statistic (Cohen 1960) for agreement was .52, with a 95% confidence interval
between .40 and .64. Krippendorff (1980) has been widely cited as advocating the
view that kappa values greater than .8 should be taken as indicating good reliability,
with values between .67 and .8 indicating tentative reliability, but we are satisfied
with the level of intercoder agreement on this task. As Carletta (1996) notes, many
tasks in computational linguistics are simply more difficult than the content analysis
classifications addressed by Krippendorff, and according to Fleiss (1981), kappa values
between .4 and .75 indicate fair to good agreement anyhow.
Discussion between the coders revealed that there was no single cause for their dif-
ferences in judgments when such differences existed. Many cases of disagreement stem
from different assumptions regarding the lexical quantifiers involved. For example, the
coders sometimes differed on whether a given instance of the word any corresponds
to a narrow-scope existential, as we conventionally treat it when it is in the scope of
negation, or the “free-choice” version of any. To take another example, two universal
quantifiers are independent in predicate calculus (dxdy[φ] ⇐⇒ dydx[φ]), but in creat-
ing our scope-tagged corpus, it was often difficult to decide whether two universal-like
English quantifiers (such as each, any, every, and all) were actually independent in a
given sentence. Some differences in coding stemmed from coder disagreements about
whether a quantifier within a fixed expression (e.g., all the hoopla) truly interacts with
other operators in the sentence. Of course, another major factor contributing to inter-
coder variation is the fact that our data sentences, taken from Wall Street Journal text,
are sometimes quite long and complex in structure, involving multiple scope-taking
operators in addition to the quantified NPs. In such cases, the coders sometimes had
difficulty clearly distinguishing the readings in question.
Because of the relatively small amount of data we had, we used the technique of
tenfold cross-validation in evaluating our classifiers, in each case choosing 89 of the
893 total data sentences from the data as a test set and training on the remaining 804.
We preprocessed the data in order to extract the information from each sentence that
we would be treating as relevant to the prediction of quantifier scoping in this project.
(Although the initial coding of the preferred scope reading for each sentence was done
manually, this preprocessing of the data was done automatically.) At the end of this
preprocessing, each sentence was represented as a record containing the following
information (see the Appendix for a list of annotation codes for Penn Treebank):
</bodyText>
<listItem confidence="0.998385571428572">
• the syntactic category, according to Penn Treebank conventions, of the
first quantifier (e.g., DT for each, NN for everyone, or QP for more than half)
• the first quantifier as a lexical item (e.g., each or everyone). For a QP
consisting of multiple words, this field contains the head word, or “CD”
in case the head is a cardinal number.
• the syntactic category of the second quantifier
• the second quantifier as a lexical item
</listItem>
<footnote confidence="0.767268">
6 These data have been made publicly available to all licensees of the Penn Treebank by means of a
patch file that may be retrieved from (http://humanities.uchicago.edu/linguistics/students/dchiggin/
qscope-data.tgz). This file also includes the coding guidelines used for this project.
</footnote>
<page confidence="0.982329">
80
</page>
<table confidence="0.9907322">
Higgins and Sadock Modeling Scope Preferences
� class: 2 �
� � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � �
first cat: DT
first head: some
second cat: DT
second head: any
join cat: NP
first c-commands: YES
second c-commands: NO
nodes intervening: 6
VP intervenes: YES
ADVP intervenes: NO
...
S intervenes: YES
conj intervenes: NO
, intervenes: NO
: intervenes: NO
...
” intervenes: YES
</table>
<figureCaption confidence="0.916846">
Figure 4
</figureCaption>
<bodyText confidence="0.870834">
Example record corresponding to the sentence shown in Figure 3.
</bodyText>
<listItem confidence="0.954508076923077">
• the syntactic category of the lowest node dominating both quantified
NPs (the “join” node)
• whether the first quantified NP c-commands the second
• whether the second quantified NP c-commands the first
• the number of nodes intervening7 between the two quantified NPs
• a list of the different categories of nodes that intervene between the
quantified NPs (actually, for each nonterminal category, there is a distinct
binary feature indicating whether a node of that category intervenes)
• whether a conjoined node intervenes between the quantified NPs
• a list of the punctuation types that are immediately dominated by nodes
intervening between the two NPs (again, for each punctuation tag in the
treebank there is a distinct binary feature indicating whether such
punctuation intervenes)
</listItem>
<bodyText confidence="0.921736">
Figure 4 illustrates how these features would be used to encode the example in Fig-
ure 3.
The items of information included in the record, as listed above, are not the exact
factors that Kuno, Takami, and Wu (1999) suggest be taken into consideration in mak-
ing scope predictions, and they are certainly not sufficient to determine the proper
scope reading for all sentences completely. Surely pragmatic factors and real-world
knowledge influence our interpretations as well, although these are not represented
here. This list does, however, provide information that could potentially be useful in
predicting the best scope reading for a particular sentence. For example, information
7 We take a node α to intervene between two other nodes β and γ in a tree if and only if δ is the lowest
node dominating both β and γ, δ dominates α or δ = α, and α dominates either β or γ.
</bodyText>
<page confidence="0.995187">
81
</page>
<note confidence="0.407979">
Computational Linguistics Volume 29, Number 1
</note>
<tableCaption confidence="0.993654">
Table 2
</tableCaption>
<table confidence="0.983250333333333">
Baseline performance, summed over all ten test sets.
Condition Correct Incorrect Percentage correct
First has wide scope 0 64 0/64 = 0.0%
Second has wide scope 0 281 0/281 = 0.0%
No scope interaction 545 0 545/545 = 100.0%
Total 545 345 545/890 = 61.2%
</table>
<bodyText confidence="0.999183571428572">
about whether one quantified NP in a given sentence c-commands the other corre-
sponds to Kuno, Takami, and Wu’s observation that subject quantifiers tend to take
wide scope over object quantifiers and topicalized quantifiers tend to outscope ev-
erything. The identity of each lexical quantifier clearly should allow our classifiers to
make the generalization that each tends to take wide scope, if this word is found in
the data, and perhaps even learn the regularity underlying Kuno, Takami, and Wu’s
observation that universal quantifiers tend to outscope existentials.
</bodyText>
<subsectionHeader confidence="0.994666">
4.3 Classifier Design
</subsectionHeader>
<bodyText confidence="0.999940730769231">
In this section, we present the three types of model that we have trained to predict
the preferred quantifier scoping on Penn Treebank sentences: a naive Bayes classifier,
a maximum-entropy classifier, and a single-layer perceptron.8 In evaluating how well
these models do in assigning the proper scope reading to each test sentence, it is im-
portant to have a baseline for comparison. The baseline model for this task is one that
simply guesses the most frequent category of the data (“no scope interaction”) every
time. This simplistic strategy already classifies 61.2% of the test examples correctly, as
shown in Table 2.
It may surprise some linguists that this third class of sentences in which there is
no scopal interaction between the two quantifiers is the largest. In part, this may be
due to special features of the Wall Street Journal text of which the corpus consists. For
example, newspaper articles may contain more direct quotations than other genres. In
the process of tagging the data, however, it was also apparent that in a large proportion
of cases, the two quantifiers were taking scope in different conjuncts of a conjoined
phrase. This further tendency supports the idea that people may intentionally avoid
constructions in which there is even the possibility of quantifier scope interactions,
perhaps because of some hearer-oriented pragmatic principle. Linguists may also be
concerned that this additional category in which there is no scope interaction between
quantifiers makes it difficult to compare the results of the present work with theoretical
accounts of quantifier scope that ignore this case and concentrate on instances in which
one quantifier does take scope over another. In response to such concerns, however,
we point out first that we provide a model of scope prediction rather than scope
generation, and so it is in any case not directly comparable with work in theoretical
linguistics, which has largely ignored scope preferences. Second, we point out that
the empirical nature of this study requires that we take note of cases in which the
quantifiers simply do not interact.
</bodyText>
<footnote confidence="0.726432">
8 The implementations of these classifiers are publicly available as Perl modules at (http://humanities.
uchicago.edu/linguistics/students/dchiggin/classifiers.tgz).
</footnote>
<page confidence="0.994437">
82
</page>
<note confidence="0.821823">
Higgins and Sadock Modeling Scope Preferences
</note>
<tableCaption confidence="0.9969">
Table 3
</tableCaption>
<table confidence="0.899401571428571">
Performance of the naive Bayes classifier, summed over all 10 test runs.
Condition Correct Incorrect Percentage correct
First has wide scope 177 104 177/281 = 63.0%
Second has wide scope 41 23 41/64 = 64.1%
No scope interaction 428 117 428/545 = 78.5%
Total 646 244 646/890 = 72.6%
4.3.1 Naive Bayes Classifier. Our data D will consist of a vector of features (d0 · · · dn)
</table>
<bodyText confidence="0.984654833333333">
that represent aspects of the sentence under examination, such as whether one quan-
tified expression c-commands the other, as described in Section 4.2. The fundamental
simplifying assumption that we make in designing a naive Bayes classifier is that
these features are independent of one another and therefore can be aggregated as in-
dependent sources of evidence about which class c* a given sentence belongs to. This
independence assumption is formalized in equations (1) and (2).
</bodyText>
<equation confidence="0.9644778">
c* = arg max P(c)P(d0 ··· dn  |c) (1)
c
..i arg max P(c) n P(dk  |c) (2)
c 11
k=0
</equation>
<bodyText confidence="0.996243277777778">
We constructed an empirical estimate of the prior probability P(c) by simply count-
ing the frequency with which each class occurs in the training data. We constructed
each P(dk  |c) by counting how often each feature dk co-occurs with the class c to
construct the empirical estimate ˆP(dk  |c) and interpolated this with the empirical
frequency ˆP(dk) of the feature dk, not conditioned on the class c. This interpolated
probability model was used in order to smooth the probability distribution, avoiding
the problems that can arise if certain feature-value pairs are assigned a probability of
zero.
The performance of the naive Bayes classifier is summarized in Table 3. For each
of the 10 test sets of 89 items taken from the corpus, the remaining 804 of the total
893 sentences were used to train the model. The naive Bayes classifier outperformed
the baseline by a considerable margin.
In addition to the raw counts of test examples correctly classified, though, we
would like to know something of the internal structure of the model (i.e., what sort of
features it has induced from the data). For this classifier, we can assume that a feature
f is a good predictor of a class c* when the value of P(f  |c*) is significantly larger
than the (geometric) mean value of P(f  |c) for all other values of c. Those features
with the greatest ratio P(f) X geom.mean(∀c�=c∗[P(f|c)]) are listed in Table 4.9
</bodyText>
<equation confidence="0.74063">
P(f|c∗)
</equation>
<bodyText confidence="0.9999418">
The first-ranked feature in Table 4 shows that there is a tendency for quanti-
fied elements not to interact when they are found in conjoined constituents, and the
second-ranked feature indicates a preference for quantifiers not to interact when there
is an intervening comma (presumably an indicator of greater syntactic “distance”).
Feature 3 indicates a preference for class 1 when there is an intervening S node,
</bodyText>
<footnote confidence="0.945304">
9 We include the term P(f) in the product in order to prevent sparsely instantiated features from
showing up as highly-ranked.
</footnote>
<page confidence="0.988485">
83
</page>
<table confidence="0.469612">
Computational Linguistics Volume 29, Number 1
</table>
<tableCaption confidence="0.997204">
Table 4
</tableCaption>
<table confidence="0.9765452">
Most active features from naive Bayes classifier.
Rank Feature Predicted Ratio
class
1 There is an intervening conjunct node 0 1.63
2 There is an intervening comma 0 1.51
3 There is an intervening S node 1 1.33
4 The first quantified NP does not c-command the second 0 1.25
5 Second quantifier is tagged QP 1 1.16
6 There is an intervening S node 0 1.12
15 The second quantified NP c-commands the first 2 1.00
</table>
<bodyText confidence="0.99936835">
whereas feature 6 indicates a preference for class 0 under the same conditions. Pre-
sumably, this reflects a dispreference for the second quantifier to take wide scope
when there is a clause boundary intervening between it and the first quantifier. The
fourth-ranked feature in Table 4 indicates that, if the first quantified NP does not
c-command the second, it is less likely to take wide scope. This is not surprising,
given the importance that c-command relations have had in theoretical discussions
of quantifier scope. The fifth-ranked feature expresses a preference for quantified ex-
pressions of category QP to take narrow scope, if they are the second of the two
quantifiers under consideration. This may simply be reflective of the fact that class
1 is more common than class 2, and the measure expressions found in QP phrases
in the Penn Treebank (such as more than three or about half) tend not to be logically
independent of other quantifiers. Finally, the feature 15 in Table 4 indicates a high
correlation between the second quantified expression’s c-commanding the first and
the second quantifier’s taking wide scope. We can easily see this as a translation into
our feature set of Kuno, Takami, and Wu’s claim that subjects tend to outscope ob-
jects and obliques and topicalized elements tend to take wide scope. Some of these
top-ranked features have to do with information found only in the written medium,
but on the whole, the features induced by the naive Bayes classifier seem consis-
tent with those suggested by Kuno, Takami, and Wu, although they are distinct by
necessity.
</bodyText>
<subsubsectionHeader confidence="0.606039">
4.3.2 Maximum-Entropy Classifier. The maximum-entropy classifier is a sort of log-
</subsubsectionHeader>
<bodyText confidence="0.991164333333333">
linear model, defining the joint probability of a class and a data vector (d0 · · · dn) as
the product of the prior probability of the class c with a set of features related to the
data:10
</bodyText>
<equation confidence="0.9917765">
P(d0 ··· dn, c) = PZc) ri αk (3)
k=0
</equation>
<bodyText confidence="0.9999012">
This classifier superficially resembles in form the naive Bayes classifier in equation (2),
but it differs from that classifier in that the way in which values for each α are chosen
does not assume that the features in the data are independent. For each of the 10
training sets, we used the generalized iterative scaling algorithm to train this classifier
on 654 training examples, using 150 examples for validation to choose the best set of
</bodyText>
<footnote confidence="0.6805625">
10 Z in Equation 3 is simply a normalizing constant that ensures that we end up with a probability
distribution.
</footnote>
<page confidence="0.992321">
84
</page>
<note confidence="0.82002">
Higgins and Sadock Modeling Scope Preferences
</note>
<tableCaption confidence="0.997414">
Table 5
</tableCaption>
<table confidence="0.939350833333333">
Performance of the maximum-entropy classifier, summed over all 10 test runs.
Condition Correct Incorrect Percentage correct
First has wide scope 148 133 148/281 = 52.7%
Second has wide scope 31 33 31/64 = 48.4%
No scope interaction 475 70 475/545 = 87.2%
Total 654 236 654/890 = 73.5%
</table>
<tableCaption confidence="0.989409">
Table 6
</tableCaption>
<table confidence="0.958271666666667">
Most active features from maximum-entropy classifier.
Rank Feature Predicted αc,.25
class
1 Second quantifier is each 2 1.13
2 There is an intervening comma 0 1.01
3 There is an intervening conjunct node 0 1.00
4 First quantified NP does not c-command the second 0 0.99
5 Second quantifier is every 2 0.98
6 There is an intervening quotation mark (”) 0 0.95
7 There is an intervening colon 0 0.95
12 First quantified NP c-commands the second 1 0.92
25 There is no intervening comma 1 0.90
</table>
<bodyText confidence="0.998702380952381">
values for the αs.11 Test data could then be classified by choosing the class for the data
that maximizes the joint probability in equation (3).
The results of training with the maximum-entropy classifier are shown in Table 5.
The classifier showed slightly higher performance than the naive Bayes classifier, with
the lowest error rate on the class of sentences having no scope interaction.
To determine exactly which features of the data the maximum-entropy classifier
sees as relevant to the classification problem, we can simply look at the α values (from
equation (3)) for each feature. Those features with higher values for α are weighted
more heavily in determining the proper scoping. Some of the features with the highest
values for α are listed in Table 6. Because of the way the classifier is built, predictor
features for class 2 need to have higher loadings to overcome the lower prior probabil-
ity of the class. Therefore, we actually rank the features in Table 6 according to αˆP(c)k
(which we denote as αc,k). ˆP(c) represents the empirical prior probability of a class c,
and k is simply a constant (.25 in this case) chosen to try to get a mix of features for
different classes at the top of the list.
The features ranked first and fifth in Table 6 express lexical preferences for certain
quantifiers to take wide scope, even when they are the second of the two quantifiers
according to linear order in the string of words. The tendency for each to take wide
scope is stronger than for the other quantifier, which is in line with Kuno, Takami,
and Wu’s decision to list it as the only quantifier with a lexical preference for scoping.
Feature 2 makes the “no scope interaction” class more likely if a comma intervenes, and
</bodyText>
<footnote confidence="0.99573025">
11 Overtraining is not a problem with the pure version of the generalized iterative scaling algorithm. For
efficiency reasons, however, we chose to take the training corpus as representative of the event space,
rather than enumerating the space exhaustively (see Jelinek [1998] for details). For this reason, it was
necessary to employ validation in training.
</footnote>
<page confidence="0.994742">
85
</page>
<table confidence="0.469527">
Computational Linguistics Volume 29, Number 1
</table>
<tableCaption confidence="0.99033">
Table 7
</tableCaption>
<table confidence="0.963963">
Performance of the single-layer perceptron, summed over all 10 test runs.
Condition Correct Incorrect Percentage correct
First has wide scope 182 99 182/281 = 64.8%
Second has wide scope 35 29 35/64 = 54.7%
No scope interaction 468 77 468/545 = 85.9%
Total 685 205 685/890 = 77.0%
</table>
<bodyText confidence="0.997212153846154">
feature 25 makes a wide-scope reading for the first quantifier more likely if there is no
intervening comma. The third-ranked feature expresses the tendency mentioned above
for quantifiers in conjoined clauses not to interact. Features 4 and 12 indicate that if the
first quantified expression c-commands the second, it is likely to take wide scope, and
that if this is not the case, there is likely to be no scope interaction. Finally, the sixth-
and seventh-ranked features in the table show that an intervening quotation mark or
colon will make the classifier tend toward class 0, “no scope interaction,” which is easy
to understand. Quotations are often opaque to quantifier scope interactions. The top
features found by the maximum-entropy classifier largely coincide with those found
by the naive Bayes model, which indicates that these generalizations are robust and
objectively present in the data.
4.3.3 Single-Layer Perceptron. For our neural network classifier, we employed a feed-
forward single-layer perceptron, with the softmax function used to determine the acti-
vation of nodes at the output layer, because this is a one-of-n classification task (Bridle
1990). The data to be classified are presented as a vector of features at the input layer,
and the output layer has three nodes, representing the three possible classes for the
data: “first has wide scope,” “second has wide scope,” and “no scope interaction.”
The output node with the highest activation is interpreted as the class of the datum
presented at the input layer.
For each of the 10 test sets of 89 examples, we trained the connection weights
of the network using error backpropagation on 654 training sentences, reserving 150
sentences for validation in order to choose the weights from the training epoch with the
highest classification performance. In Table 7 we present the results of the single-layer
neural network in classifying our test sentences. As the table shows, the single-layer
perceptron has much better classification performance than the naive Bayes classifier
and maximum-entropy model, possibly because the training of the network aims to
minimize error in the activation of the classification output nodes, which is directly
related to the classification task at hand, whereas the other models do not directly
make use of the notion of “classification error.” The perceptron also uses a sort of
weighted voting and could be interpreted as an implementation of Kuno, Takami,
and Wu’s proposal for scope determination. This clearly illustrates that the tenability
of their proposal hinges on the exact details of its implementation, since all of our
classifier models are reasonable interpretations of their approach, but they have very
different performance results on our scope determination task.
To determine exactly which features of the data the network sees as relevant to
the classification problem, we can simply look at the connection weights for each
feature-class pair. Higher connection weights indicate a greater correlation between
input features and output classes. For one of the 10 networks we trained, some of
the features with the highest connection weights are listed in Table 8. Since class 0 is
</bodyText>
<page confidence="0.996962">
86
</page>
<note confidence="0.84781">
Higgins and Sadock Modeling Scope Preferences
</note>
<tableCaption confidence="0.998129">
Table 8
</tableCaption>
<table confidence="0.993489272727273">
Most active features from single-layer perceptron.
Rank Feature Predicted Weight
class
1 There is an intervening comma 0 4.31
2 Second quantifier is all 0 3.77
3 There is an intervening colon 0 2.98
4 There is an intervening conjunct node 0 2.72
17 The first quantified NP c-commands the second 1 1.69
18 Second quantifier is tagged RBS 2 1.69
19 There is an intervening S node 1 1.61
20 Second quantifier is each 2 1.50
</table>
<bodyText confidence="0.99869747826087">
simply more frequent in the training data than the other two classes, the weights for
this class tend to be higher. Therefore, we also list some of the best predictor features
for classes 1 and 2 in the table.
The first- and third-ranked features in Table 8 show that an intervening comma or
colon will make the classifier tend toward class 0, “no scope interaction.” This finding
by the classifier is similar to the maximum-entropy classifier’s finding an intervening
quotation mark relevant and can be taken as an indication that quantifiers in distant
syntactic subdomains are unlikely to interact. Similarly, the fourth-ranked feature indi-
cates that quantifiers in separate conjuncts are unlikely to interact. The second-ranked
feature in the table expresses a tendency for there to be no scope interaction between
two quantifiers if the second of them is headed by all. This may be related to the
independence of universal quantifiers (dxdy[φ] dydx[φ]). Feature 17 in Table 8
indicates a high correlation between the first quantified expression’s c-commanding
the second and the first quantifier’s taking wide scope, which again supports Kuno,
Takami, and Wu’s claim that scope preferences are related to syntactic superiority re-
lations. Feature 18 expresses a preference for a quantified expression headed by most
to take wide scope, even if it is the second of the two quantifiers (since most is the
only quantifier in the corpus that bears the tag RBS). Feature 19 indicates that the
first quantifier is more likely to take wide scope if there is a clause boundary in-
tervening between the two quantifiers, which supports the notion that the syntactic
distance between the quantifiers is relevant to scope preferences. Finally, feature 20
expresses the well-known tendency for quantified expressions headed by each to take
wide scope.
</bodyText>
<subsectionHeader confidence="0.999745">
4.4 Summary of Results
</subsectionHeader>
<bodyText confidence="0.999961909090909">
Table 9 summarizes the performance of the quantifier scope models we have presented
here. All of the classifiers have test set accuracy above the baseline, which a paired
t-test reveals to be significant at the .001 level. The differences between the naive
Bayes, maximum-entropy, and single-layer perceptron classifiers are not statistically
significant.
The classifiers performed significantly better on those sentences annotated consis-
tently by both human coders at the beginning of the study, reinforcing the view that
this subset of the data is somehow simpler and more representative of the basic regu-
larities in scope preferences. For example, the single-layer perceptron classified 82.9%
of these sentences correctly. To further investigate the nature of the variation between
the two coders, we constructed a version of our single-layer network that was trained
</bodyText>
<page confidence="0.99846">
87
</page>
<note confidence="0.526514">
Computational Linguistics Volume 29, Number 1
</note>
<tableCaption confidence="0.995305">
Table 9
</tableCaption>
<table confidence="0.992393857142857">
Summary of classifier results.
Training data Validation data Test data
Baseline — — 61.2%
Naive Bayes 76.7% — 72.6%
Maximum entropy 78.3% 75.5% 73.5%
Single-layer 84.7% 76.8% 77.0%
perceptron
</table>
<bodyText confidence="0.99995212">
on the data on which both coders agreed and tested on the remaining sentences. This
classifier agreed with the reference coding (the coding of the first coder) 51.4% of the
time and with the additional independent coder 35.8% of the time. The first coder con-
structed the annotation guidelines for this project and may have been more successful
in applying them consistently. Alternatively, it is possible that different individuals use
different strategies in determining scope preferences, and the strategy of the second
coder may simply have been less similar than the strategy of the first coder to that of
the single-layer network.
These three classifiers directly implement a sort of weighted voting, the method
of aggregating evidence proposed by Kuno, Takami, and Wu (although the classifiers’
implementation is slightly more sophisticated than the unweighted voting that is ac-
tually used in Kuno, Takami, and Wu’s paper). Of course, since we do not use exactly
the set of features suggested by Kuno, Takami, and Wu, our model should not be
seen as a straightforward implementation of the theory outlined in their 1999 paper.
Nevertheless, the results in Table 9 suggest that Kuno, Takami, and Wu’s suggested
design can be used with some success in modeling scope preferences. Moreover, the
project undertaken here provides an answer to some of the objections that Aoun and
Li (2000) raise to Kuno, Takami, and Wu. Aoun and Li claim that Kuno, Takami, and
Wu’s choice of experts is seemingly arbitrary and that it is unclear how the voting
weights of each expert are to be set, but the machine learning approach we employ
in this article is capable of addressing both of these potential problems. Supervised
training of our classifiers is a straightforward approach to setting the weights and
also constitutes our approach to selecting features (or “experts” in Kuno, Takami, and
Wu’s terminology). In the training process, any feature that is irrelevant to scoping
preferences should receive weights that make its effect negligible.
</bodyText>
<sectionHeader confidence="0.929629" genericHeader="method">
5. Syntax and Scope
</sectionHeader>
<bodyText confidence="0.9997396">
In this section, we show how the classifier models of quantifier scope determination
introduced in Section 4 may be integrated with a PCFG model of syntax. We com-
pare two different ways in which the two components may be combined, which may
loosely be termed serial and parallel, and argue for the latter on the basis of empirical
results.
</bodyText>
<subsectionHeader confidence="0.971804">
5.1 Modular Design
</subsectionHeader>
<bodyText confidence="0.9997734">
Our use of a phrase structure syntactic component and a quantifier scope component
to define a combined language model is simplified by the fact that our classifiers are
probabilistic and define a conditional probability distribution over quantifier scopings.
The probability distributions that our classifiers define for quantifier scope structures
are conditional on syntactic phrase structure, because they are computed on the basis
</bodyText>
<page confidence="0.990415">
88
</page>
<note confidence="0.506319">
Higgins and Sadock Modeling Scope Preferences
</note>
<bodyText confidence="0.999983">
of syntactically provided features, such as the number of nodes of a certain type that
intervene between two quantifiers in a phrase structure tree.
Thus, the combined language model that we define in this article assigns probabil-
ities according to the pairs of structures that may be assigned to a sentence by the Q-
structure and phrase structure syntax modules. The probability of a word string w1−n
is therefore defined as in equation (4), where Q ranges over all possible Q-structures
in the set Q and S ranges over all possible syntactic structures in the set S.
</bodyText>
<equation confidence="0.99275875">
�P(w1−n) = P(S, Q  |w1−n) (4)
SES,QEQ
�= P(S  |w1−n)P(Q  |S,w1−n) (5)
SES,QEQ
</equation>
<bodyText confidence="0.9999418">
Equation (5) shows how we can use the definition of conditional probability to
break our calculation of the language model probability into two parts. The first of
these parts, P(S  |w1−n), which we may abbreviate as simply P(S), is the probability
of a particular syntactic tree structure’s being assigned to a particular word string. We
model this probability using a probabilistic phrase structure grammar (cf. Charniak
[1993, 1996]). The second distribution on the right side of equation (5) is the conditional
probability of a particular quantifier scope structure’s being assigned to a particular
word string, given the syntactic structure of that string. This probability is written as
P(Q  |S,w1−n), or simply P(Q  |S), and represents the quantity we estimated above
in constructing classifiers to predict the scopal representation of a sentence based on
aspects of its syntactic structure.
Thus, given a PCFG model of syntactic structure and a probabilistically defined
classifier of the sort introduced in Section 4, it is simple to determine the probability
of any pairing of two particular structures from each domain for a given sentence.
We simply multiply the values of P(S) and P(Q  |S) to obtain the joint probability
P(Q, S). In the current section, we examine two different models of combination for
these components: one in which scope determination is applied to the optimal syn-
tactic structure (the Viterbi parse), and one in which optimization is performed in the
space of both modules to find the optimal pairing of syntactic and quantifier scope
structures.
</bodyText>
<subsectionHeader confidence="0.999759">
5.2 The Syntactic Module
</subsectionHeader>
<bodyText confidence="0.999981846153846">
Before turning to the application of our multimodular approach to the problem of
scope determination in Section 5.3, we present here a short overview of the phrase
structure syntactic component used in these projects. As noted above, we model syn-
tax as a probabilistic phrase structure grammar (PCFG), and in particular, we use a
treebank grammar (Charniak 1996) trained on the Penn Treebank.
A PCFG defines the probability of a string of words as the sum of the probabilities
of all admissible phrase structure parses (trees) for that string. The probability of a
given tree is the product of the probability of all of the rule instances used in the
construction of that tree, where rules take the form N → 0, with N a nonterminal
symbol and 0 a finite sequence of one or more terminals or nonterminals.
To take an example, Figure 5 illustrates a phrase structure tree for the sentence Su-
san might not believe you, which is admissible according to the grammar in Table 10. (All
of the minimal subtrees in Figure 5 are instances of one of our rules.) The probability
</bodyText>
<page confidence="0.997714">
89
</page>
<figure confidence="0.587335">
Computational Linguistics Volume 29, Number 1
</figure>
<figureCaption confidence="0.748058">
Figure 5
</figureCaption>
<table confidence="0.446681">
A simple phrase structure tree.
</table>
<tableCaption confidence="0.993284">
Table 10
</tableCaption>
<table confidence="0.995950052631579">
A simple probabilistic phrase structure grammar.
Rule Probability
S NP VP .7
SVP .2
S VNPVP .1
VP V VP .3
VP ADV VP .1
VP V .1
VP VNP .3
VP VNPNP .2
NP Susan .3
NP you .4
NP Yves .3
V might .2
V believe .3
V show .3
V stay .2
ADV not .5
ADV always .5
</table>
<page confidence="0.970822">
90
</page>
<note confidence="0.625362">
Higgins and Sadock Modeling Scope Preferences
</note>
<bodyText confidence="0.894562">
of this tree, which we can indicate as T, can be calculated as in equation (6).
</bodyText>
<equation confidence="0.9967065">
P(T) = � P(P) (6)
PERules(T)
= P(S → NP VP) x P(VP → V VP) x P(VP → ADV VP)
x P(VP → V NP) x P(NP → Susan) x P(V → might)
x P(ADV → not) x P(V → believe) x P(NP → you) (7)
= .7 x .3 x .1 x .3 x .3 x .2 x .5 x .3 x .4 = 2.268 x 10−5 (8)
</equation>
<bodyText confidence="0.998908416666667">
The actual grammar rules and associated probabilities that we use in defining our
syntactic module are derived from the WSJ corpus of the Penn Treebank by maximum-
likelihood estimation. That is, for each rule N → φ used in the treebank, we add the
rule to the grammar and set its probability to C(N→φ)
ψC(N→ψ), where C(·) denotes the
“count” or a rule (i.e., the number of times it is used in the corpus). A grammar
composed in this manner is referred to as a treebank grammar, because its rules are
directly derived from those in a treebank corpus.
We used sections 00–20 of the WSJ corpus of the Penn Treebank for collecting the
rules and associated probabilities of our PCFG, which is implemented as a bottom-up
chart parser. Before constructing the grammar, the treebank was preprocessed using
known procedures (cf. Krotov et al. [1998]; Belz [2001]) to facilitate the construction of
a rule list. Functional and anaphoric annotations (basically anything following a “-”
in a node label; cf. Santorini [1990]; Bies et al. [1995]) were removed from nonterminal
labels. Nodes that dominate only “empty categories” such as traces were removed.
In addition, unary-branching constructions were removed by replacing the mother
category in such a structure with the daughter node. (For example, given an instance
of the rule X → YZ, if the daughter category Y were expanded by the unary rule
Y → W, our algorithm would induce the single rule X → WZ.) Finally, we discarded
all rules that had more than 10 symbols on the right-hand side (an arbitrary limit of
our parser implementation). This resulted in a total of 18,820 rules, of which 11,156
were discarded as hapax legomena, leaving 7,664 rules in our treebank grammar.
Table 11 shows some of the rules in our grammar with the highest and lowest corpus
counts.
</bodyText>
<subsectionHeader confidence="0.999581">
5.3 Unlabeled Scope Determination
</subsectionHeader>
<bodyText confidence="0.999748357142857">
In this section, we describe an experiment designed to assess the performance of
parallel and serial approaches to combining grammatical modules, focusing on the
task of unlabeled scope determination. This task involves predicting the most likely
Q-structure representation for a sentence, basically the same task we addressed in Sec-
tion 4, in comparing the performance levels of each type of classifier. The experiment
of this section differs, however, from the task presented in Section 4 in that instead of
providing a syntactic tree from the Penn Treebank as input to the classifier, we provide
the model only with a string of words (a sentence). Our dual-component model will
search for the optimal syntactic and scopal structures for the sentence (the pairing
(T*,x*)) and will be evaluated based on its success in identifying the correct scope
reading x*.
Our concern in this section will be to determine whether it is necessary to search
the space of possible pairings (T, x) of syntactic and scopal structures or whether
it is sufficient to use our PCFG first to fix the syntactic tree T, and then to choose
</bodyText>
<page confidence="0.996738">
91
</page>
<note confidence="0.599594">
Computational Linguistics Volume 29, Number 1
</note>
<tableCaption confidence="0.580407666666667">
Table 11
Rules derived from sections 00–20 of the Penn Treebank WSJ corpus. “TOP” is a special “start”
symbol that may expand to any of the symbols found at the root of a tree in the corpus.
</tableCaption>
<note confidence="0.293518">
Rule Corpus count
</note>
<equation confidence="0.9274492">
PP → IN NP 59,053
TOP → S 34,614
NP → DT NN 28,074
NP → NP PP 25,192
S → NP VP 14,032
S → NP VP . 12,901
VP → TO VP 11,598
...
S → CC PP NNP NNP VP . 2
NP → DT “ NN NN NN &amp;quot; 2
NP → NP PP PP PP PP PP 2
INTJ → UH UH 2
NP → DT “ NN NNS 2
SBARQ → “ WP VP . &amp;quot; 2
S → PP NP VP . &amp;quot; 2
</equation>
<bodyText confidence="0.999266">
a scope reading to maximize the probability of the pairing. That is, are syntax and
quantifier scope mutually dependent components of grammar, or can scope relations
be “read off of” syntax? The serial model suggests that the optimal syntactic structure
T* should be chosen on the basis of the syntactic module only, as in equation (9),
and the optimal quantifier scope structure X* then chosen on the basis of T*, as in
equation (10). The parallel model, on the other hand, suggests that the most likely
pairing of structures must be chosen in the joint probability space of both components,
as in equation (11).
</bodyText>
<equation confidence="0.995179833333333">
T* = arg max PS(T  |w1−n) (9)
TES
X* = arg max PQ(X  |T*,w1−n) (10)
XEQ
T* = IT  |(T, X) = arg max PS(T  |w1−n)PQ(X  |T,w1−n) } (11)
TES,XEQ
</equation>
<bodyText confidence="0.979387636363636">
5.3.1 Experimental Design. For this experiment, we implement the scoping compo-
nent as a single-layer feed-forward network, because the single-layer perceptron clas-
sifier had the best prediction rate among the three classifiers tested in Section 4. The
softmax activation function we use for the output nodes of the classifier guarantees
that the activations of all of the output nodes sum to one and can be interpreted as
class probabilities. The syntactic component, of course, is determined by the treebank
PCFG grammar described above.
Given these two models, which respectively define PQ(X  |T, w1−n) and PS(T  |w1−n)
from equation (11), it remains only to specify how to search the space of pairings
(T, X) in performing this optimization to find X*. Unfortunately, it is not feasible to
examine all values T E S, since our PCFG will generally admit a huge number of
</bodyText>
<page confidence="0.98888">
92
</page>
<note confidence="0.888463">
Higgins and Sadock Modeling Scope Preferences
</note>
<tableCaption confidence="0.995948">
Table 12
</tableCaption>
<table confidence="0.976229166666667">
Performance of models on the unlabeled scope prediction task, summed over all 10 test runs.
Condition Correct Incorrect Percentage correct
Parallel model
First has wide scope 168 113 167/281 = 59.4%
Second has wide scope 26 38 26/64 = 40.6%
No scope interaction 467 78 467/545 = 85.7%
Total 661 229 661/890 = 74.3%
Serial model
First has wide scope 163 118 163/281 = 58.0%
Second has wide scope 27 37 27/64 = 42.2%
No scope interaction 461 84 461/545 = 84.6%
Total 651 239 651/890 = 73.1%
</table>
<bodyText confidence="0.994857214285714">
trees for a sentence (especially given a mean sentence length of over 20 words in
the WSJ corpus).12 Our solution to this search problem is to make the simplifying as-
sumption that the syntactic tree that is used in the optimal set of structures (T*,x*)
will always be among the top few trees T for which PS(T  |w1−n) is the greatest.
That is, although we suppose that quantifier scope information is relevant to pars-
ing, we do not suppose that it is so strong a determinant as to completely over-
ride syntactic factors. In practice, this means that our parser will return the top 10
parses for each sentence, along with the probabilities assigned to them, and these
are the only parses that are considered in looking for the optimal set of linguistic
structures.
We again used 10-fold cross-validation in evaluating the competing models, di-
viding the scope-tagged corpus into 10 test sections of 89 sentences each, and we
used the same version of the treebank grammar for our PCFG. The first model re-
trieved the top 10 syntactic parses (T0 · · · T9) for each sentence and computed the
probability P(T, x) for each T E T0 · · · T9, x E 0,1, 2, choosing that scopal represen-
tation x that was found in the maximum-probability pairing. We call this the par-
allel model, because the properties of each probabilistic model may influence the
optimal structure chosen by the other. The second model retrieved only the Viterbi
parse T0 from the PCFG and chose the scopal representation x for which the pair-
ing (T0, x) took on the highest probability. We call this the serial model, because it
represents syntactic phrase structure as independent of other components of gram-
mar (in this case, quantifier scope), though other components are dependent
upon it.
5.3.2 Results. There was an appreciable difference in performance between these two
models on the quantifier scope test sets. As shown in Table 12, the parallel model
narrowly outperformed the serial model, by 1.2%. A 10-fold paired t-test on the test
sections of the scope-tagged corpus shows that the parallel model is significantly better
(p &lt; .05).
</bodyText>
<footnote confidence="0.869437">
12 Since we are allowing χ to range only over the three scope readings (0, 1, 2), however, it is possible to
enumerate all values of χ to be paired with a given syntactic tree τ.
</footnote>
<page confidence="0.992643">
93
</page>
<note confidence="0.704806">
Computational Linguistics Volume 29, Number 1
</note>
<bodyText confidence="0.999859857142857">
This result suggests that, in determining the syntactic structure of a sentence, we
must take aspects of structure into account that are not purely syntactic (such as quanti-
fier scope). Searching both dimensions of the hypothesis space for our dual-component
model allowed the composite model to handle the interdependencies between differ-
ent aspects of grammatical structure, whereas fixing a phrase structure tree purely
on the basis of syntactic considerations led to suboptimal performance in using that
structure as a basis for determining quantifier scope.
</bodyText>
<sectionHeader confidence="0.997546" genericHeader="conclusions">
6. Conclusion
</sectionHeader>
<bodyText confidence="0.999954">
In this article, we have taken a statistical, corpus-based approach to the modeling
of quantifier scope preferences, a subject that has previously been addressed only
with systems of ad hoc rules derived from linguists’ intuitive judgments. Our model
takes its theoretical inspiration from Kuno, Takami, and Wu (1999), who suggest an
“expert system” approach to scope preferences, and follows many other projects in the
machine learning of natural language that combine information from multiple sources
in solving linguistic problems.
0ur results are generally supportive of the design that Kuno, Takami, and Wu pro-
pose for the quantifier scope component of grammar, and some of the features induced
by our models find clear parallels in the factors that Kuno, Takami, and Wu claim to
be relevant to scoping. In addition, our final experiment, in which we combine our
quantifier scope module with a PCFG model of syntactic phrase structure, provides
evidence of a grammatical architecture in which different aspects of structure mutu-
ally constrain one another. This result casts doubt on approaches in which syntactic
processing is completed prior to the determination of other grammatical properties of
a sentence, such as quantifier scope relations.
</bodyText>
<note confidence="0.6576">
Appendix: Selected Codes Used to Annotate Syntactic Categories in the Penn Tree-
bank, from Marcus et al. (1993) and Bies et al. (1995)
</note>
<table confidence="0.978419483870968">
Part-of-speech tags
Tag Meaning Tag Meaning
CC Conjunction RB Adverb
CD Cardinal number RBR Comparative adverb
DT Determiner RBS Superlative adverb
IN Preposition TO “to”
JJ Adjective UH Interjection
JJR Comparative adjective VB Verb in base form
JJS Superlative adjective VBD Past-tense verb
NN Singular or mass noun VBG Gerundive verb
NNS Plural noun VBN Past participial verb
NNP Singular proper noun VBP Non-3sg, present-
NNPS Plural proper noun tense verb
PDT Predeterminer VBZ 3sg, present-tense
verb
PRP Personal pronoun WP WH pronoun
PRP$ Possessive pronoun WP$ Possessive WH pronoun
94
Higgins and Sadock Phrasal categories Modeling Scope Preferences
Code Meaning Code Meaning
ADJP Adjective phrase SBAR Clause introduced by
ADVP Adverb phrase a subordinating
INTJ Interjection conjunction
NP Noun phrase SBARQ Clause introduced by
PP Prepositional phrase a WH phrase
QP Quantifier phrase (i.e., SINV Inverted declarative
measure/amount sentence
phrase) SQ Inverted yes/no
S Declarative clause question following the
WH phrase in SBARQ
VP Verb phrase
</table>
<sectionHeader confidence="0.989908" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.978217444444444">
The authors are grateful for an Academic
Technology Innovation Grant from the
University of Chicago, which helped to
make this work possible, and to John
Goldsmith, Terry Regier, Anne Pycha, and
Bob Moore, whose advice and collaboration
have considerably aided the research
reported in this article. Any remaining
errors are, of course, our own.
</bodyText>
<sectionHeader confidence="0.978104" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.991014130434783">
Aoun, Joseph and Yen-hui Audrey Li. 1993.
The Syntax of Scope. MIT Press, Cambridge.
Aoun, Joseph and Yen-hui Audrey Li. 2000.
Scope, structure, and expert systems: A
reply to Kuno et al. Language,
76(1):133–155.
Belz, Anja. 2001. Optimisation of
corpus-derived probabilistic grammars. In
Proceedings of Corpus Linguistics 2001,
pages 46–57.
Berger, Adam L., Stephen A. Della Pietra,
and Vincent J. Della Pietra. 1996. A
maximum entropy approach to natural
language processing. Computational
Linguistics, 22(1):39–71.
Bies, Ann, Mark Ferguson, Karen Katz, and
Robert MacIntyre. 1995. Bracketing
guidelines for Treebank II style. Technical
report, Penn Treebank Project, University
of Pennsylvania.
Bishop, Christopher M. 1995. Neural
Networks for Pattern Recognition. Oxford
University Press, Oxford.
Bridle, John S. 1990. Probabilistic
interpretation of feedforward
classification network outputs with
relationships to statistical pattern
recognition. In F. Fougelman-Soulie and
J. Herault, editors, Neurocomputing—
Algorithms, Architectures, and Applications.
Springer-Verlag, Berlin, pages 227–236.
Brill, Eric. 1995. Transformation-based
error-driven learning and natural
language processing: A case study in
part-of-speech tagging. Computational
Linguistics, 21(4):543–565.
Carden, Guy. 1976. English Quantifiers:
Logical Structure and Linguistic Variation.
Academic Press, New York.
Carletta, Jean. 1996. Assessing agreement on
classification tasks: The kappa statistic.
Computational Linguistics, 22(2):249–254.
Charniak, Eugene. 1993. Statistical language
learning. MIT Press, Cambridge.
Charniak, Eugene. 1996. Tree-bank
grammars. In AAAI/IAAI, vol. 2,
pages 1031–1036.
Cohen, Jacob. 1960. A coefficient of
agreement for nominal scales. Educational
and Psychological Measurement, 20:37–46.
Cooper, Robin. 1983. Quantification and
Syntactic Theory. Reidel, Dordrecht.
Fleiss, Joseph L. 1981. Statistical Methods for
Rates and Proportions. John Wiley &amp; Sons,
New York.
Hobbs, Jerry R. and Stuart M. Shieber. 1987.
An algorithm for generating quantifier
scopings. Computational Linguistics,
13:47–63.
Hornstein, Norbert. 1995. Logical Form: From
GB to Minimalism. Blackwell, Oxford and
Cambridge.
Jelinek, Frederick. 1998. Statistical Methods for
Speech Recognition. MIT Press, Cambridge.
Jurafsky, Daniel and James H. Martin. 2000.
Speech and Language Processing. Prentice
Hall, Upper Saddle River, New Jersey.
Krippendorff, Klaus. 1980. Content Analysis:
An Introduction to Its Methodology. Sage
</reference>
<page confidence="0.960286">
95
</page>
<note confidence="0.350743">
Computational Linguistics Volume 29, Number 1
</note>
<reference confidence="0.999440620689656">
Publications, Beverly Hills, California.
Krotov, Alexander, Mark Hepple, Robert J.
Gaizauskas, and Yorick Wilks. 1998.
Compacting the Penn treebank grammar.
In COLING-ACL, pages 699–703.
Kuno, Susumu, Ken-Ichi Takami, and Yuru
Wu. 1999. Quantifier scope in English,
Chinese, and Japanese. Language,
75(1):63–111.
Kuno, Susumu, Ken-Ichi Takami, and Yuru
Wu. 2001. Response to Aoun and Li.
Language, 77(1):134–143.
Manning, Christopher D. and Hinrich
Sch¨utze. 1999. Foundations of Statistical
Natural Language Processing. MIT Press,
Cambridge.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19(2):313–330.
Martin, Paul, Douglas Appelt, and
Fernando Pereira. 1986. Transportability
and generality in a natural-language
interface system. In B. J. Grosz, K. Sparck
Jones, and B. L. Webber, editors, Natural
Language Processing. Kaufmann, Los Altos,
California, pages 585–593.
May, Robert. 1985. Logical Form: Its Structure
and Derivation. MIT Press, Cambridge.
McCawley, James D. 1998. The Syntactic
Phenomena of English. University of
Chicago Press, Chicago, second edition.
Mitchell, Tom M. 1996. Machine Learning.
McGraw Hill, New York.
Montague, Richard. 1973. The proper
treatment of quantification in ordinary
English. In J. Hintikka et al., editors,
Approaches to Natural Language. Reidel,
Dordrecht, pages 221–242.
Moran, Douglas B. 1988. Quantifier scoping
in the SRI core language engine. In
Proceedings of the 26th Annual Meeting of the
Association for Computational Linguistics
(ACL’88), pages 33–40.
Moran, Douglas B. and Fernando C. N.
Pereira. 1992. Quantifier scoping. In
Hiyan Alshawi, editor, The Core Language
Engine. MIT Press, Cambridge,
pages 149–172.
Nerbonne, John. 1993. A feature-based
syntax/semantics interface. In
A. Manaster-Ramer and W. Zadrozsny,
editors, Annals of Mathematics and Artificial
Intelligence (Special Issue on Mathematics of
Language), 8(1–2):107–132. Also published
as DFKI Research Report RR-92-42.
Park, Jong C. 1995. Quantifier scope and
constituency. In Proceedings of the 33rd
Annual Meeting of the Association for
Computational Linguistics (ACL’95),
pages 205–212.
Pedersen, Ted. 2000. A simple approach to
building ensembles of naive Bayesian
classifiers for word sense disambiguation.
In Proceedings of the First Meeting of the
North American Chapter of the Association for
Computational Linguistics (NAACL 2000),
pages 63–69.
Pereira, Fernando. 1990. Categorial
semantics and scoping. Computational
Linguistics, 16(1):1–10.
Pollard, Carl. 1989. The syntax-semantics
interface in a unification-based phrase
structure grammar. In S. Busemann,
C. Hauenschild, and C. Umbach, editors,
Views of the Syntax/Semantics Interface
KIT-FAST Report 74. Technical University
of Berlin, pages 167–185.
Pollard, Carl and Eun Jung Yoo. 1998. A
unified theory of scope for quantifiers
and WH-phrases. Journal of Linguistics,
34(2):415–446.
Ratnaparkhi, Adwait. 1997. A simple
introduction to maximum entropy models
for natural language processing. Technical
Report 97-08, Institute for Research in
Cognitive Science, University of
Pennsylvania.
Santorini, Beatrice. 1990. Part-of-speech
tagging guidelines for the Penn Treebank
project. Technical Report MS-CIS-90-47,
Department of Computer and Information
Science, University of Pennsylvania.
Soon, Wee Meng, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine
learning approach to coreference
resolution of noun phrases. Computational
Linguistics, 27(4):521–544.
van Halteren, Hans, Jakub Zavrel, and
Walter Daelemans. 2001. Improving
accuracy in word class tagging through
the combination of machine learning
systems. Computational Linguistics,
27(2):199–229.
VanLehn, Kurt A. 1978. Determining the
scope of English quantifiers. Technical
Report AITR-483, Massachusetts Institute
of Technology Artificial Intelligence
Laboratory, Cambridge.
Woods, William A. 1986. Semantics and
quantification in natural language
question answering. In B. J. Grosz,
K. Sparck Jones, and B. L. Webber, editors,
Natural Language Processing. Kaufmann,
Los Altos, California, pages 205–248.
</reference>
<page confidence="0.998406">
96
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999161">A Machine Learning Approach to Modeling Scope Preferences</title>
<author confidence="0.999734">M Jerrold</author>
<affiliation confidence="0.992757">University of Chicago University of Chicago</affiliation>
<abstract confidence="0.966534841269841">This article describes a corpus-based investigation of quantifier scope preferences. Following recent work on multimodular grammar frameworks in theoretical linguistics and a long history of combining multiple information sources in natural language processing, scope is treated as a distinct module of grammar from syntax. This module incorporates multiple sources of evidence regarding the most likely scope readingfor a sentence and is entirely data-driven. The experiments discussed in this article evaluate the performance of our models in predicting the most likely scope reading for a particular sentence, using Penn Treebank data both with and without syntactic annotation. We wish to focus attention on the issue of determining scope preferences, which has largely been ignored in theoretical linguistics, and to explore different models of the interaction between syntax and quantifier scope. 1. Overview This article addresses the issue of determining the most accessible quantifier scope for a sentence. elements of natural and logical languages (such and English and predicate calculus) that have certain semantic properties. Loosely speaking, they express that a proposition holds for some proportion of a set of individuals. One peculiarity of these expressions is that there can be semantic differences that depend on the order in which the quantifiers are These are known as (1) Everyone likes two songs on this album. As an example of the sort of interpretive differences we are talking about, consider the sentence in (1). There are two readings of this sentence; which reading is meant on which of the two quantified expressions songs on this wide scope. The first reading, in which wide scope, simply implies that every person has a certain preference, not necessarily related to anyone else’s. This reading can be paraphrased as “Pick any person, and that person will like songs on this album.” The second reading, in which narrow scope, implies that there are two specific songs on the album of which everyone is fond, say, “Blue Moon” and “My Way.” theoretical linguistics, attention has been primarily focused on the issue of Researchers applying the techniques of quantifier raising and Cooper storage have been concerned mainly with enumerating all of the scope readings for a of Linguistics, University of Chicago, 1010 East 59th Street, Chicago, IL 60637. E-mail: dchiggin@alumni.uchicago.edu. of Linguistics, University of Chicago, 1010 East 59th Street, Chicago, IL 60637. E-mail: j-sadock@uchicago.edu. Association for Computational Linguistics Computational Linguistics Volume 29, Number 1 sentence that are possible, without regard to their relative likelihood or naturalness. Recently, however, linguists such as Kuno, Takami, and Wu (1999) have begun to turn attention to or determining the relative accessibility of different scope readings. In computational linguistics, more attention has been paid to the factors that determine scope preferences. Systems such as the SRI Core Language Engine (Moran 1988; Moran and Pereira 1992), LUNAR (Woods 1986), and TEAM (Martin, Appelt, Pereira 1986) have employed critics use heuristics to decide between alternative scopings. However, the rules that these systems use in making quantifier scope decisions are motivated only by the researchers’ intuitions, and no empirical results have been published regarding their accuracy. In this article, we use the tools of machine learning to construct a data-driven model of quantifier scope preferences. For theoretical linguistics, this model serves as an illustration that Kuno, Takami, and Wu’s approach can capture some of the clearest generalizations about quantifier scoping. For computational linguistics, this article provides a baseline result on the task of scope prediction, with which other scope critics can be compared. In addition, it is the most extensive empirical investigation of which we are aware that collects data of any kind regarding the relative frequency different quantifier scope readings in English Section 2 briefly discusses treatments of scoping issues in theoretical linguistics, and Section 3 reviews the computational work that has been done on natural language quantifier scope. In Section 4 we introduce the models that we use to predict quantifier scoping, as well as the data on which they are trained and tested. Section 5 combines the scope model of the previous section with a probabilistic context-free grammar (PCFG) model of syntax and addresses the issue of whether these two modules of grammar ought to be combined in serial, with information from the syntax feeding the quantifier scope module, or in parallel, with each module constraining the structures provided by the other. 2. Approaches to Quantifier Scope in Theoretical Linguistics Most, if not all, linguistic treatments of quantifier scope have closely integrated it with the way in which the syntactic structure of a sentence is built up. Montague (1973) used a syntactic rule to introduce a quantified expression into a derivation at the point where it was to take scope, whereas generative semantic analyses such as McCawley (1998) represented the scope of quantification at deep structure, transformationally lowering quantifiers into their surface positions during the course of the derivation. More recent work in the interpretive paradigm takes the opposite approach, extracting quantifiers from their surface positions to their scope positions by means of a quantifier-raising (QR) transformation (May 1985; Aoun and Li 1993; Hornstein 1995). Another popular technique is to percolate scope information up through the syntactic tree using Cooper storage (Cooper 1983; Hobbs and Shieber 1987; Pollard 1989; Nerbonne 1993; Park 1995; Pollard and Yoo 1998). The QR approach to dealing with scope in linguistics consists in the claim that there is a covert transformation applying to syntactic structures that moves quantified elements out of the position in which they are found on the surface and raises them to a higher position that reflects their scope. The various incarnations of the strategy that 1 See Carden (1976), however, for a questionnaire-based approach to gathering data on the accessibility of different quantifier scope readings. 74 Higgins and Sadock Modeling Scope Preferences Figure 1 Simple illustration of the QR approach to quantifier scope generation. follows from this claim differ in the precise characterization of this QR transformation, what conditions are placed upon it, and what tree-configurational relationship is required for one operator to take scope over another. The general idea of QR is in Figure 1, a schematic analysis of the reading of the sentence everyone which wide scope (i.e., ‘there is some person that all persons the Cooper storage approach, quantifiers are gathered into a passed upward through a syntactic tree. At certain nodes along the way, quantifiers may be the store and take scope. The relative scope of quantifiers is determined by where each quantifier is retrieved from the store, with quantifiers higher in the tree taking wide scope over lower ones. As with QR, different authors implement this scheme in slightly different ways, but the simplest case is represented in Figure 2, the Cooper storage analog of Figure 1. These structural approaches, QR and Cooper storage, have in common that they allow syntactic factors to have an effect only on the scope readings that are available for a given sentence. They are also similar in addressing only the issue of scope generation, or identifying all and only the accessible readings for each sentence. That is to say, they do not address the issue of the relative salience of these readings. Kuno, Takami, and Wu (1999, 2001) propose to model the scope of quantified elements with a set of interacting expert systems that basically consists of a weighted vote taken of the various factors that may influence scope readings. This model is meant to account not only for scope generation, but also for “the relative strengths of the potential scope interpretations of a given sentence” (1999, page 63). They illustrate the plausibility of this approach in their paper by presenting a number of examples that are accounted for fairly well by the approach even when an unweighted vote of the factors is allowed to be taken. So, for example, in Kuno, Takami and Wu’s (49b) (1999), repeated here as (2), the correct prediction is made: that the sentence is unambiguous with the first quantified noun phrase (NP) taking wide scope over the second (the reading in which we don’t all have to hate the same people). Table 1 illustrates how the votes of each of Kuno, and Wu’s “experts” contribute to this outcome. Since the expression of more votes, and the numbers for the two competing quantified expressions are quite far apart, the first one is predicted to take wide scope unambiguously. Many of hate some of them. 75 Computational Linguistics Volume 29, Number 1 Figure 2 Simple illustration of the Cooper storage approach to quantifier scope generation.</abstract>
<note confidence="0.765222333333333">Table 1 Voting to determine optimal scope readings for quantifiers, according to Kuno, Takami, and Wu (1999).</note>
<title confidence="0.742415222222222">of some of them V Baseline: V Subject Q: V Lefthand Q: V Speaker/Hearer Q:</title>
<abstract confidence="0.977613982300885">Total: 4 1 Some adherents of the structural approaches also seem to acknowledge the necessity of eventually coming to terms with the factors that play a role in determining scope preferences in language. Aoun and Li (2000) claim that the lexical scope preferences of quantifiers “are not ruled out under a structural account” (page 140). It is clear from the surrounding discussion, though, that they intend such lexical requirements to be taken care of in some nonsyntactic component of grammar. Although Takami, and Wu’s dialogue with Aoun and Li in been portrayed by both sides as a debate over the correct way of modeling quantifier scope, they are not really modeling the same things. Whereas Aoun and Li (1993) provide an account of scope generation, Kuno, Takami, and Wu (1999) intend to model both scope generation and scope prediction. The model of scope preferences provided in this article is an empirically based refinement of the approach taken by Kuno, Takami, and Wu, but in principle it is consistent with a structural account of scope generation. 76 Higgins and Sadock Modeling Scope Preferences 3. Approaches to Quantifier Scope in Computational Linguistics Many studies, such as Pereira (1990) and Park (1995), have dealt with the issue of scope generation from a computational perspective. Attempts have also been made in computational work to extend a pure Cooper storage approach to handle scope prediction. Hobbs and Shieber (1987) discuss the possibility of incorporating some sort of ordering heuristics into the SRI scope generation system, in the hopes of producing a ranked list of possible scope readings, but ultimately are forced to acknowledge that “[t]he modifications turn out to be quite complicated if we wish to order quantifiers to lexical heuristics, such as having Because of the recursive nature of the algorithm, there are limits to the amount of ordering that can be done in this manner” (page 55). The stepwise nature of these scope mechanisms makes it hard to state the factors that influence the preference for one quantifier to take scope over another. Those natural language processing (NLP) systems that have managed to provide some sort of account of quantifier scope preferences have done so by using a separate system of heuristics (or scope critics) that apply postsyntactically to determine the most likely scoping. LUNAR (Woods 1986), TEAM (Martin, Appelt, and Pereira 1986), and the SRI Core Language Engine as described by Moran (1988; Moran and Pereira 1992) all employ scope rules of this sort. By and large, these rules are of an ad hoc nature, implementing a linguist’s intuitive idea of what factors determine scope possibilities, and no results have been published regarding the accuracy of these methods. For example, Moran (1988) incorporates rules from other NLP systems and from VanLehn such as a preference for a logically weaker interpretation, the tendency for to take wide scope, and a ban on raising a quantifier across multiple major clause boundaries. The testing of Moran’s system is “limited to checking conformance to the stated rules” (pages 40–41). In addition, these systems are generally incapable of handling unrestricted text such as that found in the Wall Street Journal corpus in a robust way, because they need to do a full semantic analysis of a sentence in order to make scope predictions. The statistical basis of the model presented in this article offers increased robustness and the possibility of more serious evaluation on the basis of corpus data. 4. Modeling Quantifier Scope In this section, we argue for an empirically driven machine learning approach to the identification of factors relevant to quantifier scope and the modeling of scope preferences. Following much recent work that applies the tools of machine learning to linguistic problems (Brill 1995; Pedersen 2000; van Halteren, Zavrel, and Daelemans 2001; Soon, Ng, and Lim 2001), we will treat the prediction of quantifier scope as example of a Our aim is to provide a robust model of scope prediction based on Kuno, Takami, and Wu’s theoretical foundation and to address the serious lack of empirical results regarding quantifier scope in computational work. We describe here the modeling tools borrowed from the field of artificial intelligence for the scope prediction task and the data from which the generalizations are to be learned. Finally, we present the results of training different incarnations of our scope module on the data and assess the implications of this exercise for theoretical and computational linguistics. 77 Computational Linguistics Volume 29, Number 1 4.1 Classification in Machine Learning Determining which among multiple quantifiers in a sentence takes wide scope, given a number of different sources of evidence, is an example of what is known in machine learning as a classification task (Mitchell 1996). There are many types of classifiers that may be applied to this task that both are more sophisticated than the approach suggested by Kuno, Takami, and Wu and have a more solid probabilistic foundation. These include the naive Bayes classifier (Manning and Sch¨utze 1999; Jurafsky and Martin 2000), maximum-entropy models (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1997), and the single-layer perceptron (Bishop 1995). We employ these classifier models here primarily because of their straightforward probabilistic interpretation and their similarity to the scope model of Kuno, Takami, and Wu (since they each could be said to implement a kind of weighted voting of factors). In Section 4.3, we describe how classifiers of these types can be constructed to serve as a grammatical module responsible for quantifier scope determination. All of these classifiers can be trained in a supervised manner. That is, given a sample of training data that provides all of the information that is deemed to be relevant to quantifier scope and the actual scope reading assigned to a sentence, these classifiers will attempt to extract generalizations that can be fruitfully applied in classifying as-yet-unseen examples. 4.2 Data The data on which the quantifier scope classifiers are trained and tested is an extract from the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993) that we have tagged to indicate the most salient scope interpretation of each sentence in context. Figure 3 shows an example of a training sentence with the scope reading indicated. The quantifier lower in the tree bears the tag “Q1,” and the higher quantifier bears the tag “Q2,” so this sentence is interpreted such that the lower quantifier has wide scope. Reversing the tags would have meant that the higher quantifier takes wide scope, and while if both quantifiers had been marked “Q1,” this would have indicated that there is no scope interaction between them (as when they are logically independent or take in different conjuncts of a conjoined The sentences tagged were chosen from the Wall Street Journal (WSJ) section of the Penn Treebank to have a certain set of attributes that simplify the task of designing the quantifier scope module of the grammar. First, in order to simplify the coding process, each sentence has exactly two scope-taking elements of the sort considered this These include most NPs that begin with a determiner, predeterminer, quantifier phrase but exclude NPs in which the determiner is or Ex- 2 This “no interaction” class is a sort of “elsewhere” category that results from phrasing the classification question as “Which quantifier takes wider scope in the preferred reading?” Where there is no scope interaction, the answer is “neither.” This includes cases in which the relative scope of operators does correspond to a difference in meaning, as in bought or when they take scope different propositional domains, such as in bought and sold The human coders used in this study were instructed to choose class 0 whenever there was not a clear preference for one of the two scope readings. 3 This restriction that each sentence contain only two quantified elements does not actually exclude many sentences from consideration. We identified only 61 sentences with three quantifiers of the sort we consider and 12 sentences with four. In addition, our review of these sentences revealed that many of them simply involve lists in which the quantifiers do not interact in terms of scope (as in, for example, “We ask that you turn off all cell phones, extinguish all cigarettes, and open any candy before the performance begins”). Thus, the class of sentences with more than two quantifiers is small and seems to involve even simpler quantifier interactions than those found in our corpus.</abstract>
<note confidence="0.850917810810811">4 These categories are intended to be understood as they are used in the tagging and parsing of the Penn Treebank. See Santorini (1990) and Bies et al. (1995) for details; the Appendix lists selected codes used 78 Higgins and Sadock Modeling Scope Preferences ( (S (NP-SBJ (NP (DT Those) ) (SBAR (WHNP-1 (WP who) ) (S (NP-SBJ-2 (-NONE- *T*-1) ) (ADVP (RB still) ) (VP (VBP want) (S (NP-SBJ (-NONE- *-2) ) (VP (TO to) (VP (VB do) (NP (PRP it) )))))))) (VP (MD will) (ADVP (RB just) ) (VP (VB find) (NP (NP (DT-Q2 some) (NN way) ) (SBAR (WHADVP-3 (-NONE- 0) ) (S (NP-SBJ (-NONE- *) ) (VP (TO to) (VP (VB get) (PP (IN around) (’’ ’’) (NP (DT-Q1 any) (NN attempt) (S (NP-SBJ (-NONE- *) ) (VP (TO to) (VP (VB curb) (NP (PRP it) )))))) (ADVP-MNR (-NONE- *T*-3) </note>
<abstract confidence="0.985505067961166">(. .) )) Figure 3 Tagged Wall Street Journal text from the Penn Treebank. The lower quantifier takes wide scope, indicated by its tag “Q1.” cluding these determiners from consideration largely avoids the problem of generics and the complexities of assigning scope readings to definite descriptions. In addition, only sentences that had the root node S were considered. This serves to exclude sentence fragments and interrogative sentence types. Our data set therefore differs systematically from the full WSJ corpus, but we believe it is sufficient to allow many generalizations about English quantification to be induced. Given these restrictions on input data, the task of the scope classifier is a choice among three (Class 0) There is no scopal interaction. (Class 1) The first quantifier takes wide scope. (Class 2) The second quantifier takes wide scope. for annotating the Penn Treebank corpus. The category QP is particularly unintuitive in that it does not to a quantified noun phrase, but to a measure expression, such as than 5 Some linguists may find it strange that we have chosen to treat the choice of preferred scoping for two quantified elements as a tripartite decision, since the possibility of independence is seldom treated in the linguistic literature. As we are dealing with corpus data in this experiment, we cannot afford to ignore this possibility. 79 Computational Linguistics Volume 29, Number 1 result is a set of 893 with Penn Treebank II parse trees and hand-tagged for the primary scope reading. To assess the reliability of the hand-tagged data used in this project, the data were coded a second time by an independent coder, in addition to the reference coding. The independent codings agreed with the reference coding on 76.3% of sentences. The statistic (Cohen 1960) for agreement was with a 95% confidence interval between .40 and .64. Krippendorff (1980) has been widely cited as advocating the view that kappa values greater than .8 should be taken as indicating good reliability, with values between .67 and .8 indicating tentative reliability, but we are satisfied with the level of intercoder agreement on this task. As Carletta (1996) notes, many tasks in computational linguistics are simply more difficult than the content analysis classifications addressed by Krippendorff, and according to Fleiss (1981), kappa values between .4 and .75 indicate fair to good agreement anyhow. Discussion between the coders revealed that there was no single cause for their differences in judgments when such differences existed. Many cases of disagreement stem from different assumptions regarding the lexical quantifiers involved. For example, the sometimes differed on whether a given instance of the word to a narrow-scope existential, as we conventionally treat it when it is in the scope of or the “free-choice” version of To take another example, two universal are independent in predicate calculus but in creating our scope-tagged corpus, it was often difficult to decide whether two universal-like quantifiers (such as and were actually independent in a given sentence. Some differences in coding stemmed from coder disagreements about a quantifier within a fixed expression (e.g., the truly interacts with other operators in the sentence. Of course, another major factor contributing to intercoder variation is the fact that our data sentences, taken from Wall Street Journal text, are sometimes quite long and complex in structure, involving multiple scope-taking operators in addition to the quantified NPs. In such cases, the coders sometimes had difficulty clearly distinguishing the readings in question. Because of the relatively small amount of data we had, we used the technique of tenfold cross-validation in evaluating our classifiers, in each case choosing 89 of the 893 total data sentences from the data as a test set and training on the remaining 804. We preprocessed the data in order to extract the information from each sentence that we would be treating as relevant to the prediction of quantifier scoping in this project. (Although the initial coding of the preferred scope reading for each sentence was done manually, this preprocessing of the data was done automatically.) At the end of this preprocessing, each sentence was represented as a record containing the following information (see the Appendix for a list of annotation codes for Penn Treebank): • the syntactic category, according to Penn Treebank conventions, of the quantifier (e.g., DT for NN for or QP for than the first quantifier as a lexical item (e.g., For a QP consisting of multiple words, this field contains the head word, or “CD” in case the head is a cardinal number. • the syntactic category of the second quantifier • the second quantifier as a lexical item 6 These data have been made publicly available to all licensees of the Penn Treebank by means of a file that may be retrieved from This file also includes the coding guidelines used for this project. 80 Higgins and Sadock Modeling Scope Preferences � class: 2 � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � � first cat: DT first head: some second cat: DT second head: any join cat: NP first c-commands: YES second c-commands: NO nodes intervening: 6 VP intervenes: YES ADVP intervenes: NO ... S intervenes: YES conj intervenes: NO , intervenes: NO : intervenes: NO ... ” intervenes: YES Figure 4 Example record corresponding to the sentence shown in Figure 3. • the syntactic category of the lowest node dominating both quantified NPs (the “join” node) • whether the first quantified NP c-commands the second • whether the second quantified NP c-commands the first the number of nodes between the two quantified NPs • a list of the different categories of nodes that intervene between the quantified NPs (actually, for each nonterminal category, there is a distinct binary feature indicating whether a node of that category intervenes) • whether a conjoined node intervenes between the quantified NPs • a list of the punctuation types that are immediately dominated by nodes intervening between the two NPs (again, for each punctuation tag in the treebank there is a distinct binary feature indicating whether such punctuation intervenes) Figure 4 illustrates how these features would be used to encode the example in Figure 3. The items of information included in the record, as listed above, are not the exact factors that Kuno, Takami, and Wu (1999) suggest be taken into consideration in making scope predictions, and they are certainly not sufficient to determine the proper scope reading for all sentences completely. Surely pragmatic factors and real-world knowledge influence our interpretations as well, although these are not represented here. This list does, however, provide information that could potentially be useful in predicting the best scope reading for a particular sentence. For example, information We take a node intervene between two other nodes a tree if and only if the lowest dominating both and either 81 Computational Linguistics Volume 29, Number 1 Table 2 Baseline performance, summed over all ten test sets. Condition Correct Incorrect Percentage correct First has wide scope 0 64 0/64 = 0.0% Second has wide scope 0 281 0/281 = 0.0% No scope interaction 545 0 545/545 = 100.0% Total 545 345 545/890 = 61.2% about whether one quantified NP in a given sentence c-commands the other corresponds to Kuno, Takami, and Wu’s observation that subject quantifiers tend to take wide scope over object quantifiers and topicalized quantifiers tend to outscope everything. The identity of each lexical quantifier clearly should allow our classifiers to the generalization that to take wide scope, if this word is found in the data, and perhaps even learn the regularity underlying Kuno, Takami, and Wu’s observation that universal quantifiers tend to outscope existentials. 4.3 Classifier Design In this section, we present the three types of model that we have trained to predict the preferred quantifier scoping on Penn Treebank sentences: a naive Bayes classifier, maximum-entropy classifier, and a single-layer In evaluating how well these models do in assigning the proper scope reading to each test sentence, it is important to have a baseline for comparison. The baseline model for this task is one that simply guesses the most frequent category of the data (“no scope interaction”) every time. This simplistic strategy already classifies 61.2% of the test examples correctly, as shown in Table 2. It may surprise some linguists that this third class of sentences in which there is no scopal interaction between the two quantifiers is the largest. In part, this may be due to special features of the Wall Street Journal text of which the corpus consists. For example, newspaper articles may contain more direct quotations than other genres. In the process of tagging the data, however, it was also apparent that in a large proportion of cases, the two quantifiers were taking scope in different conjuncts of a conjoined phrase. This further tendency supports the idea that people may intentionally avoid constructions in which there is even the possibility of quantifier scope interactions, perhaps because of some hearer-oriented pragmatic principle. Linguists may also be concerned that this additional category in which there is no scope interaction between quantifiers makes it difficult to compare the results of the present work with theoretical accounts of quantifier scope that ignore this case and concentrate on instances in which one quantifier does take scope over another. In response to such concerns, however, we point out first that we provide a model of scope prediction rather than scope generation, and so it is in any case not directly comparable with work in theoretical linguistics, which has largely ignored scope preferences. Second, we point out that the empirical nature of this study requires that we take note of cases in which the quantifiers simply do not interact. The implementations of these classifiers are publicly available as Perl modules at 82 Higgins and Sadock Modeling Scope Preferences Table 3 Performance of the naive Bayes classifier, summed over all 10 test runs. Condition Correct Incorrect Percentage correct First has wide scope 177 104 177/281 = 63.0% Second has wide scope 41 23 41/64 = 64.1% No scope interaction 428 117 428/545 = 78.5% Total 646 244 646/890 = 72.6% Naive Bayes Classifier. data consist of a vector of features · · that represent aspects of the sentence under examination, such as whether one quantified expression c-commands the other, as described in Section 4.2. The fundamental simplifying assumption that we make in designing a naive Bayes classifier is that these features are independent of one another and therefore can be aggregated as insources of evidence about which class a given sentence belongs to. This independence assumption is formalized in equations (1) and (2). max | c max n c 11 constructed an empirical estimate of the prior probability simply counting the frequency with which each class occurs in the training data. We constructed counting how often each feature with the class the empirical estimate interpolated this with the empirical the feature not conditioned on the class This interpolated probability model was used in order to smooth the probability distribution, avoiding the problems that can arise if certain feature-value pairs are assigned a probability of zero. The performance of the naive Bayes classifier is summarized in Table 3. For each of the 10 test sets of 89 items taken from the corpus, the remaining 804 of the total 893 sentences were used to train the model. The naive Bayes classifier outperformed the baseline by a considerable margin. In addition to the raw counts of test examples correctly classified, though, we would like to know something of the internal structure of the model (i.e., what sort of features it has induced from the data). For this classifier, we can assume that a feature a good predictor of a class when the value of significantly larger the (geometric) mean value of all other values of Those features the greatest ratio are listed in Table The first-ranked feature in Table 4 shows that there is a tendency for quantified elements not to interact when they are found in conjoined constituents, and the second-ranked feature indicates a preference for quantifiers not to interact when there is an intervening comma (presumably an indicator of greater syntactic “distance”). Feature 3 indicates a preference for class 1 when there is an intervening S node, We include the term the product in order to prevent sparsely instantiated features from showing up as highly-ranked.</abstract>
<note confidence="0.890736">83 Computational Linguistics Volume 29, Number 1 Table 4 Most active features from naive Bayes classifier. Rank Feature Predicted class Ratio 1 There is an intervening conjunct node 0 1.63 2 There is an intervening comma 0 1.51 3 There is an intervening S node 1 1.33 4 The first quantified NP does not c-command the second 0 1.25 5 Second quantifier is tagged QP 1 1.16 6 There is an intervening S node 0 1.12</note>
<abstract confidence="0.845784046511628">15 The second quantified NP c-commands the first 2 1.00 whereas feature 6 indicates a preference for class 0 under the same conditions. Presumably, this reflects a dispreference for the second quantifier to take wide scope when there is a clause boundary intervening between it and the first quantifier. The fourth-ranked feature in Table 4 indicates that, if the first quantified NP does not c-command the second, it is less likely to take wide scope. This is not surprising, given the importance that c-command relations have had in theoretical discussions of quantifier scope. The fifth-ranked feature expresses a preference for quantified expressions of category QP to take narrow scope, if they are the second of the two quantifiers under consideration. This may simply be reflective of the fact that class 1 is more common than class 2, and the measure expressions found in QP phrases the Penn Treebank (such as than three tend not to be logically independent of other quantifiers. Finally, the feature 15 in Table 4 indicates a high correlation between the second quantified expression’s c-commanding the first and the second quantifier’s taking wide scope. We can easily see this as a translation into our feature set of Kuno, Takami, and Wu’s claim that subjects tend to outscope objects and obliques and topicalized elements tend to take wide scope. Some of these top-ranked features have to do with information found only in the written medium, but on the whole, the features induced by the naive Bayes classifier seem consistent with those suggested by Kuno, Takami, and Wu, although they are distinct by necessity. Maximum-Entropy Classifier. maximum-entropy classifier is a sort of logmodel, defining the joint probability of a class and a data vector · · product of the prior probability of the class a set of features related to the = This classifier superficially resembles in form the naive Bayes classifier in equation (2), it differs from that classifier in that the way in which values for each chosen does not assume that the features in the data are independent. For each of the 10 training sets, we used the generalized iterative scaling algorithm to train this classifier on 654 training examples, using 150 examples for validation to choose the best set of Equation 3 is simply a normalizing constant that ensures that we end up with a probability distribution. 84 Higgins and Sadock Modeling Scope Preferences Table 5 Performance of the maximum-entropy classifier, summed over all 10 test runs. Condition Correct Incorrect Percentage correct First has wide scope 148 133 148/281 = 52.7% Second has wide scope 31 33 31/64 = 48.4% No scope interaction 475 70 475/545 = 87.2% Total 654 236 654/890 = 73.5% Table 6 Most active features from maximum-entropy classifier.</abstract>
<note confidence="0.938986555555556">Rank Feature Predicted class 1 quantifier is 2 1.13 2 There is an intervening comma 0 1.01 3 There is an intervening conjunct node 0 1.00 4 First quantified NP does not c-command the second 0 0.99 5 quantifier is 2 0.98 6 There is an intervening quotation mark (”) 0 0.95 7 There is an intervening colon 0 0.95 12 First quantified NP c-commands the second 1 0.92</note>
<abstract confidence="0.97442">25 There is no intervening comma 1 0.90 for the Test data could then be classified by choosing the class for the data that maximizes the joint probability in equation (3). The results of training with the maximum-entropy classifier are shown in Table 5. The classifier showed slightly higher performance than the naive Bayes classifier, with the lowest error rate on the class of sentences having no scope interaction. To determine exactly which features of the data the maximum-entropy classifier as relevant to the classification problem, we can simply look at the (from (3)) for each feature. Those features with higher values for weighted more heavily in determining the proper scoping. Some of the features with the highest for listed in Table 6. Because of the way the classifier is built, predictor features for class 2 need to have higher loadings to overcome the lower prior probabilof the class. Therefore, we actually rank the features in Table 6 according to we denote as the empirical prior probability of a class simply a constant (.25 in this case) chosen to try to get a mix of features for different classes at the top of the list. The features ranked first and fifth in Table 6 express lexical preferences for certain quantifiers to take wide scope, even when they are the second of the two quantifiers to linear order in the string of words. The tendency for take wide scope is stronger than for the other quantifier, which is in line with Kuno, Takami, and Wu’s decision to list it as the only quantifier with a lexical preference for scoping. Feature 2 makes the “no scope interaction” class more likely if a comma intervenes, and 11 Overtraining is not a problem with the pure version of the generalized iterative scaling algorithm. For efficiency reasons, however, we chose to take the training corpus as representative of the event space, rather than enumerating the space exhaustively (see Jelinek [1998] for details). For this reason, it was necessary to employ validation in training. 85 Computational Linguistics Volume 29, Number 1 Table 7 Performance of the single-layer perceptron, summed over all 10 test runs. Condition Correct Incorrect Percentage correct First has wide scope 182 99 182/281 = 64.8% Second has wide scope 35 29 35/64 = 54.7% No scope interaction 468 77 468/545 = 85.9% Total 685 205 685/890 = 77.0% feature 25 makes a wide-scope reading for the first quantifier more likely if there is no intervening comma. The third-ranked feature expresses the tendency mentioned above for quantifiers in conjoined clauses not to interact. Features 4 and 12 indicate that if the first quantified expression c-commands the second, it is likely to take wide scope, and that if this is not the case, there is likely to be no scope interaction. Finally, the sixthand seventh-ranked features in the table show that an intervening quotation mark or colon will make the classifier tend toward class 0, “no scope interaction,” which is easy to understand. Quotations are often opaque to quantifier scope interactions. The top features found by the maximum-entropy classifier largely coincide with those found by the naive Bayes model, which indicates that these generalizations are robust and objectively present in the data. Single-Layer Perceptron. our neural network classifier, we employed a feedforward single-layer perceptron, with the softmax function used to determine the actiof nodes at the output layer, because this is a task (Bridle 1990). The data to be classified are presented as a vector of features at the input layer, and the output layer has three nodes, representing the three possible classes for the data: “first has wide scope,” “second has wide scope,” and “no scope interaction.” The output node with the highest activation is interpreted as the class of the datum presented at the input layer. For each of the 10 test sets of 89 examples, we trained the connection weights of the network using error backpropagation on 654 training sentences, reserving 150 sentences for validation in order to choose the weights from the training epoch with the highest classification performance. In Table 7 we present the results of the single-layer neural network in classifying our test sentences. As the table shows, the single-layer perceptron has much better classification performance than the naive Bayes classifier and maximum-entropy model, possibly because the training of the network aims to minimize error in the activation of the classification output nodes, which is directly related to the classification task at hand, whereas the other models do not directly make use of the notion of “classification error.” The perceptron also uses a sort of weighted voting and could be interpreted as an implementation of Kuno, Takami, and Wu’s proposal for scope determination. This clearly illustrates that the tenability of their proposal hinges on the exact details of its implementation, since all of our classifier models are reasonable interpretations of their approach, but they have very different performance results on our scope determination task. To determine exactly which features of the data the network sees as relevant to the classification problem, we can simply look at the connection weights for each feature-class pair. Higher connection weights indicate a greater correlation between input features and output classes. For one of the 10 networks we trained, some of the features with the highest connection weights are listed in Table 8. Since class 0 is 86 Higgins and Sadock Modeling Scope Preferences Table 8 Most active features from single-layer perceptron.</abstract>
<note confidence="0.77943175">Rank Feature Predicted class Weight 1 There is an intervening comma 0 4.31 2 quantifier is 0 3.77 3 There is an intervening colon 0 2.98 4 There is an intervening conjunct node 0 2.72 17 The first quantified NP c-commands the second 1 1.69 18 Second quantifier is tagged RBS 2 1.69 19 There is an intervening S node 1 1.61</note>
<abstract confidence="0.994699871212122">20 quantifier is 2 1.50 simply more frequent in the training data than the other two classes, the weights for this class tend to be higher. Therefore, we also list some of the best predictor features for classes 1 and 2 in the table. The firstand third-ranked features in Table 8 show that an intervening comma or colon will make the classifier tend toward class 0, “no scope interaction.” This finding by the classifier is similar to the maximum-entropy classifier’s finding an intervening quotation mark relevant and can be taken as an indication that quantifiers in distant syntactic subdomains are unlikely to interact. Similarly, the fourth-ranked feature indicates that quantifiers in separate conjuncts are unlikely to interact. The second-ranked feature in the table expresses a tendency for there to be no scope interaction between quantifiers if the second of them is headed by This may be related to the of universal quantifiers Feature 17 in Table indicates a high correlation between the first quantified expression’s c-commanding the second and the first quantifier’s taking wide scope, which again supports Kuno, Takami, and Wu’s claim that scope preferences are related to syntactic superiority re- Feature 18 expresses a preference for a quantified expression headed by take wide scope, even if it is the second of the two quantifiers (since the only quantifier in the corpus that bears the tag RBS). Feature 19 indicates that the first quantifier is more likely to take wide scope if there is a clause boundary intervening between the two quantifiers, which supports the notion that the syntactic distance between the quantifiers is relevant to scope preferences. Finally, feature 20 the well-known tendency for quantified expressions headed by take wide scope. 4.4 Summary of Results Table 9 summarizes the performance of the quantifier scope models we have presented here. All of the classifiers have test set accuracy above the baseline, which a paired reveals to be significant at the .001 level. The differences between the naive Bayes, maximum-entropy, and single-layer perceptron classifiers are not statistically significant. The classifiers performed significantly better on those sentences annotated consistently by both human coders at the beginning of the study, reinforcing the view that this subset of the data is somehow simpler and more representative of the basic regularities in scope preferences. For example, the single-layer perceptron classified 82.9% of these sentences correctly. To further investigate the nature of the variation between the two coders, we constructed a version of our single-layer network that was trained 87 Computational Linguistics Volume 29, Number 1 Table 9 Summary of classifier results. Training data Validation data Test data Baseline — — 61.2% Naive Bayes 76.7% — 72.6% Maximum entropy 78.3% 75.5% 73.5% perceptron 84.7% 76.8% 77.0% on the data on which both coders agreed and tested on the remaining sentences. This classifier agreed with the reference coding (the coding of the first coder) 51.4% of the time and with the additional independent coder 35.8% of the time. The first coder constructed the annotation guidelines for this project and may have been more successful in applying them consistently. Alternatively, it is possible that different individuals use different strategies in determining scope preferences, and the strategy of the second coder may simply have been less similar than the strategy of the first coder to that of the single-layer network. These three classifiers directly implement a sort of weighted voting, the method of aggregating evidence proposed by Kuno, Takami, and Wu (although the classifiers’ implementation is slightly more sophisticated than the unweighted voting that is actually used in Kuno, Takami, and Wu’s paper). Of course, since we do not use exactly the set of features suggested by Kuno, Takami, and Wu, our model should not be seen as a straightforward implementation of the theory outlined in their 1999 paper. Nevertheless, the results in Table 9 suggest that Kuno, Takami, and Wu’s suggested design can be used with some success in modeling scope preferences. Moreover, the project undertaken here provides an answer to some of the objections that Aoun and Li (2000) raise to Kuno, Takami, and Wu. Aoun and Li claim that Kuno, Takami, and Wu’s choice of experts is seemingly arbitrary and that it is unclear how the voting weights of each expert are to be set, but the machine learning approach we employ in this article is capable of addressing both of these potential problems. Supervised training of our classifiers is a straightforward approach to setting the weights and also constitutes our approach to selecting features (or “experts” in Kuno, Takami, and Wu’s terminology). In the training process, any feature that is irrelevant to scoping preferences should receive weights that make its effect negligible. 5. Syntax and Scope In this section, we show how the classifier models of quantifier scope determination introduced in Section 4 may be integrated with a PCFG model of syntax. We compare two different ways in which the two components may be combined, which may be termed and argue for the latter on the basis of empirical results. 5.1 Modular Design Our use of a phrase structure syntactic component and a quantifier scope component to define a combined language model is simplified by the fact that our classifiers are probabilistic and define a conditional probability distribution over quantifier scopings. The probability distributions that our classifiers define for quantifier scope structures are conditional on syntactic phrase structure, because they are computed on the basis 88 Higgins and Sadock Modeling Scope Preferences of syntactically provided features, such as the number of nodes of a certain type that intervene between two quantifiers in a phrase structure tree. Thus, the combined language model that we define in this article assigns probabilities according to the pairs of structures that may be assigned to a sentence by the Qand phrase structure syntax modules. The probability of a word string therefore defined as in equation (4), where over all possible Q-structures the set over all possible syntactic structures in the set = Equation (5) shows how we can use the definition of conditional probability to break our calculation of the language model probability into two parts. The first of parts, which we may abbreviate as simply is the probability of a particular syntactic tree structure’s being assigned to a particular word string. We model this probability using a probabilistic phrase structure grammar (cf. Charniak [1993, 1996]). The second distribution on the right side of equation (5) is the conditional probability of a particular quantifier scope structure’s being assigned to a particular word string, given the syntactic structure of that string. This probability is written as or simply and represents the quantity we estimated above in constructing classifiers to predict the scopal representation of a sentence based on aspects of its syntactic structure. Thus, given a PCFG model of syntactic structure and a probabilistically defined classifier of the sort introduced in Section 4, it is simple to determine the probability of any pairing of two particular structures from each domain for a given sentence. simply multiply the values of obtain the joint probability In the current section, we examine two different models of combination for these components: one in which scope determination is applied to the optimal syntactic structure (the Viterbi parse), and one in which optimization is performed in the space of both modules to find the optimal pairing of syntactic and quantifier scope structures. 5.2 The Syntactic Module Before turning to the application of our multimodular approach to the problem of scope determination in Section 5.3, we present here a short overview of the phrase structure syntactic component used in these projects. As noted above, we model syntax as a probabilistic phrase structure grammar (PCFG), and in particular, we use a grammar 1996) trained on the Penn Treebank. A PCFG defines the probability of a string of words as the sum of the probabilities of all admissible phrase structure parses (trees) for that string. The probability of a given tree is the product of the probability of all of the rule instances used in the of that tree, where rules take the form N with nonterminal and finite sequence of one or more terminals or nonterminals. take an example, Figure 5 illustrates a phrase structure tree for the sentence Sumight not believe which is admissible according to the grammar in Table 10. (All of the minimal subtrees in Figure 5 are instances of one of our rules.) The probability 89 Computational Linguistics Volume 29, Number 1 Figure 5 A simple phrase structure tree. Table 10 A simple probabilistic phrase structure grammar.</abstract>
<note confidence="0.925197368421053">Rule Probability S NP VP .7 SVP .2 S VNPVP .1 VP V VP .3 VP ADV VP .1 VP V .1 VP VNP .3 VP VNPNP .2 NP Susan .3 NP you .4 NP Yves .3 V might .2 V believe .3 V show .3 V stay .2 ADV not .5 ADV always .5 90</note>
<title confidence="0.550224">Higgins and Sadock Modeling Scope Preferences</title>
<abstract confidence="0.981851340425532">this tree, which we can indicate as can be calculated as in equation (6). = (8) The actual grammar rules and associated probabilities that we use in defining our syntactic module are derived from the WSJ corpus of the Penn Treebank by maximumestimation. That is, for each rule N in the treebank, we add the to the grammar and set its probability to where the “count” or a rule (i.e., the number of times it is used in the corpus). A grammar composed in this manner is referred to as a treebank grammar, because its rules are directly derived from those in a treebank corpus. We used sections 00–20 of the WSJ corpus of the Penn Treebank for collecting the rules and associated probabilities of our PCFG, which is implemented as a bottom-up chart parser. Before constructing the grammar, the treebank was preprocessed using known procedures (cf. Krotov et al. [1998]; Belz [2001]) to facilitate the construction of a rule list. Functional and anaphoric annotations (basically anything following a “-” in a node label; cf. Santorini [1990]; Bies et al. [1995]) were removed from nonterminal labels. Nodes that dominate only “empty categories” such as traces were removed. In addition, unary-branching constructions were removed by replacing the mother category in such a structure with the daughter node. (For example, given an instance the rule X if the daughter category Y were expanded by the unary rule our algorithm would induce the single rule X Finally, we discarded all rules that had more than 10 symbols on the right-hand side (an arbitrary limit of our parser implementation). This resulted in a total of 18,820 rules, of which 11,156 were discarded as hapax legomena, leaving 7,664 rules in our treebank grammar. Table 11 shows some of the rules in our grammar with the highest and lowest corpus counts. 5.3 Unlabeled Scope Determination In this section, we describe an experiment designed to assess the performance of parallel and serial approaches to combining grammatical modules, focusing on the of scope This task involves predicting the most likely Q-structure representation for a sentence, basically the same task we addressed in Section 4, in comparing the performance levels of each type of classifier. The experiment of this section differs, however, from the task presented in Section 4 in that instead of providing a syntactic tree from the Penn Treebank as input to the classifier, we provide the model only with a string of words (a sentence). Our dual-component model will search for the optimal syntactic and scopal structures for the sentence (the pairing and will be evaluated based on its success in identifying the correct scope Our concern in this section will be to determine whether it is necessary to search space of possible pairings syntactic and scopal structures or whether is sufficient to use our PCFG first to fix the syntactic tree and then to choose 91 Computational Linguistics Volume 29, Number 1 Table 11 Rules derived from sections 00–20 of the Penn Treebank WSJ corpus. “TOP” is a special “start” symbol that may expand to any of the symbols found at the root of a tree in the corpus. Rule Corpus count</abstract>
<address confidence="0.9005986">NP 59,053 34,614 NN 28,074 PP 25,192 VP 14,032</address>
<abstract confidence="0.948794647058824">VP VP 11,598 ... PP NNP NNP VP “ NN NN NN PP PP PP PP PP 2 UH 2 “ NN NNS 2 WP VP NP VP a scope reading to maximize the probability of the pairing. That is, are syntax and quantifier scope mutually dependent components of grammar, or can scope relations be “read off of” syntax? The serial model suggests that the optimal syntactic structure be chosen on the basis of the syntactic module only, as in equation (9), the optimal quantifier scope structure then chosen on the basis of as in equation (10). The parallel model, on the other hand, suggests that the most likely pairing of structures must be chosen in the joint probability space of both components, as in equation (11). max max = max Experimental Design. this experiment, we implement the scoping component as a single-layer feed-forward network, because the single-layer perceptron classifier had the best prediction rate among the three classifiers tested in Section 4. The softmax activation function we use for the output nodes of the classifier guarantees that the activations of all of the output nodes sum to one and can be interpreted as class probabilities. The syntactic component, of course, is determined by the treebank PCFG grammar described above. these two models, which respectively define and from equation (11), it remains only to specify how to search the space of pairings performing this optimization to find Unfortunately, it is not feasible to all values since our PCFG will generally admit a huge number of 92 Higgins and Sadock Modeling Scope Preferences Table 12 Performance of models on the unlabeled scope prediction task, summed over all 10 test runs. Condition Correct Incorrect Percentage correct Parallel model First has wide scope 168 113 167/281 = 59.4% Second has wide scope 26 38 26/64 = 40.6% No scope interaction 467 78 467/545 = 85.7% Total 661 229 661/890 = 74.3% Serial model First has wide scope 163 118 163/281 = 58.0% Second has wide scope 27 37 27/64 = 42.2% No scope interaction 461 84 461/545 = 84.6% Total 651 239 651/890 = 73.1% trees for a sentence (especially given a mean sentence length of over 20 words in WSJ Our solution to this search problem is to make the simplifying asthat the syntactic tree that is used in the optimal set of structures always be among the top few trees which the greatest. That is, although we suppose that quantifier scope information is relevant to parsing, we do not suppose that it is so strong a determinant as to completely override syntactic factors. In practice, this means that our parser will return the top 10 parses for each sentence, along with the probabilities assigned to them, and these are the only parses that are considered in looking for the optimal set of linguistic structures. We again used 10-fold cross-validation in evaluating the competing models, dividing the scope-tagged corpus into 10 test sections of 89 sentences each, and we used the same version of the treebank grammar for our PCFG. The first model rethe top 10 syntactic parses · · for each sentence and computed the each · · 2, choosing that scopal represenwas found in the maximum-probability pairing. We call this the parallel model, because the properties of each probabilistic model may influence the optimal structure chosen by the other. The second model retrieved only the Viterbi the PCFG and chose the scopal representation which the pairon the highest probability. We call this the serial model, because it represents syntactic phrase structure as independent of other components of grammar (in this case, quantifier scope), though other components are dependent upon it. Results. was an appreciable difference in performance between these two models on the quantifier scope test sets. As shown in Table 12, the parallel model outperformed the serial model, by 1.2%. A 10-fold paired on the test sections of the scope-tagged corpus shows that the parallel model is significantly better Since we are allowing range only over the three scope readings 1, however, it is possible to all values of be paired with a given syntactic tree 93 Computational Linguistics Volume 29, Number 1 This result suggests that, in determining the syntactic structure of a sentence, we must take aspects of structure into account that are not purely syntactic (such as quantifier scope). Searching both dimensions of the hypothesis space for our dual-component model allowed the composite model to handle the interdependencies between different aspects of grammatical structure, whereas fixing a phrase structure tree purely on the basis of syntactic considerations led to suboptimal performance in using that structure as a basis for determining quantifier scope. 6. Conclusion In this article, we have taken a statistical, corpus-based approach to the modeling of quantifier scope preferences, a subject that has previously been addressed only with systems of ad hoc rules derived from linguists’ intuitive judgments. Our model takes its theoretical inspiration from Kuno, Takami, and Wu (1999), who suggest an “expert system” approach to scope preferences, and follows many other projects in the machine learning of natural language that combine information from multiple sources in solving linguistic problems. 0ur results are generally supportive of the design that Kuno, Takami, and Wu propose for the quantifier scope component of grammar, and some of the features induced by our models find clear parallels in the factors that Kuno, Takami, and Wu claim to be relevant to scoping. In addition, our final experiment, in which we combine our quantifier scope module with a PCFG model of syntactic phrase structure, provides evidence of a grammatical architecture in which different aspects of structure mutually constrain one another. This result casts doubt on approaches in which syntactic processing is completed prior to the determination of other grammatical properties of a sentence, such as quantifier scope relations.</abstract>
<note confidence="0.894476">Appendix: Selected Codes Used to Annotate Syntactic Categories in the Penn Treebank, from Marcus et al. (1993) and Bies et al. (1995)</note>
<title confidence="0.827869733333333">Part-of-speech tags Tag Meaning Tag Meaning CC Conjunction RB Adverb CD Cardinal number RBR Comparative adverb DT Determiner RBS Superlative adverb IN Preposition TO “to” JJ Adjective UH Interjection JJR Comparative adjective VB Verb in base form JJS Superlative adjective VBD Past-tense verb NN Singular or mass noun VBG Gerundive verb NNS Plural noun VBN Past participial verb NNP Singular proper noun VBP present- NNPS Plural proper noun tense verb PDT Predeterminer VBZ 3sg, present-tense verb PRP Personal pronoun WP WH pronoun</title>
<author confidence="0.431899">PRP Possessive pronoun WP Possessive WH pronoun</author>
<date confidence="0.337813">94</date>
<title confidence="0.786722">Higgins and Sadock Phrasal categories Modeling Scope Preferences Code Meaning Code Meaning ADJP Adjective phrase SBAR Clause introduced by ADVP Adverb phrase a subordinating INTJ Interjection conjunction NP Noun phrase SBARQ Clause introduced by PP Prepositional phrase a WH phrase</title>
<abstract confidence="0.8524054">QP Quantifier phrase (i.e., SINV Inverted declarative sentence phrase) SQ S Declarative clause question following the WH phrase in SBARQ VP Verb phrase Acknowledgments The authors are grateful for an Academic Technology Innovation Grant from the University of Chicago, which helped to make this work possible, and to John Goldsmith, Terry Regier, Anne Pycha, and Bob Moore, whose advice and collaboration have considerably aided the research reported in this article. Any remaining errors are, of course, our own.</abstract>
<note confidence="0.5630856">References Aoun, Joseph and Yen-hui Audrey Li. 1993. Syntax of MIT Press, Cambridge. Aoun, Joseph and Yen-hui Audrey Li. 2000. Scope, structure, and expert systems: A to Kuno et al. 76(1):133–155. Belz, Anja. 2001. Optimisation of corpus-derived probabilistic grammars. In of Corpus Linguistics pages 46–57. Berger, Adam L., Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural processing. 22(1):39–71. Bies, Ann, Mark Ferguson, Karen Katz, and Robert MacIntyre. 1995. Bracketing guidelines for Treebank II style. Technical report, Penn Treebank Project, University of Pennsylvania. Christopher M. 1995. for Pattern Oxford University Press, Oxford. Bridle, John S. 1990. Probabilistic</note>
<abstract confidence="0.964616">interpretation of feedforward classification network outputs with relationships to statistical pattern recognition. In F. Fougelman-Soulie and</abstract>
<note confidence="0.66380925">Herault, editors, Architectures, and Springer-Verlag, Berlin, pages 227–236. Brill, Eric. 1995. Transformation-based</note>
<abstract confidence="0.806705666666667">error-driven learning and natural language processing: A case study in tagging.</abstract>
<note confidence="0.959815882352941">21(4):543–565. Guy. 1976. Quantifiers: Structure and Linguistic Academic Press, New York. Carletta, Jean. 1996. Assessing agreement on classification tasks: The kappa statistic. 22(2):249–254. Eugene. 1993. language MIT Press, Cambridge. Charniak, Eugene. 1996. Tree-bank In vol. 2, pages 1031–1036. Cohen, Jacob. 1960. A coefficient of for nominal scales. Psychological 20:37–46. Robin. 1983. and Reidel, Dordrecht.</note>
<author confidence="0.992098">L Joseph</author>
<affiliation confidence="0.949657">and John Wiley &amp; Sons,</affiliation>
<address confidence="0.940607">New York.</address>
<note confidence="0.75584">Hobbs, Jerry R. and Stuart M. Shieber. 1987. An algorithm for generating quantifier 13:47–63. Norbert. 1995. Form: From to Blackwell, Oxford and Cambridge. Frederick. 1998. Methods for MIT Press, Cambridge. Jurafsky, Daniel and James H. Martin. 2000. and Language Prentice Hall, Upper Saddle River, New Jersey. Klaus. 1980. Analysis:</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Joseph Aoun</author>
<author>Yen-hui Audrey Li</author>
</authors>
<title>The Syntax of Scope.</title>
<date>1993</date>
<publisher>MIT Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="5965" citStr="Aoun and Li 1993" startWordPosition="899" endWordPosition="902">a sentence is built up. Montague (1973) used a syntactic rule to introduce a quantified expression into a derivation at the point where it was to take scope, whereas generative semantic analyses such as McCawley (1998) represented the scope of quantification at deep structure, transformationally lowering quantifiers into their surface positions during the course of the derivation. More recent work in the interpretive paradigm takes the opposite approach, extracting quantifiers from their surface positions to their scope positions by means of a quantifier-raising (QR) transformation (May 1985; Aoun and Li 1993; Hornstein 1995). Another popular technique is to percolate scope information up through the syntactic tree using Cooper storage (Cooper 1983; Hobbs and Shieber 1987; Pollard 1989; Nerbonne 1993; Park 1995; Pollard and Yoo 1998). The QR approach to dealing with scope in linguistics consists in the claim that there is a covert transformation applying to syntactic structures that moves quantified elements out of the position in which they are found on the surface and raises them to a higher position that reflects their scope. The various incarnations of the strategy that 1 See Carden (1976), ho</context>
<context position="10570" citStr="Aoun and Li (1993)" startWordPosition="1661" endWordPosition="1664"> terms with the factors that play a role in determining scope preferences in language. Aoun and Li (2000) claim that the lexical scope preferences of quantifiers “are not ruled out under a structural account” (page 140). It is clear from the surrounding discussion, though, that they intend such lexical requirements to be taken care of in some nonsyntactic component of grammar. Although Kuno, Takami, and Wu’s dialogue with Aoun and Li in Language has been portrayed by both sides as a debate over the correct way of modeling quantifier scope, they are not really modeling the same things. Whereas Aoun and Li (1993) provide an account of scope generation, Kuno, Takami, and Wu (1999) intend to model both scope generation and scope prediction. The model of scope preferences provided in this article is an empirically based refinement of the approach taken by Kuno, Takami, and Wu, but in principle it is consistent with a structural account of scope generation. 76 Higgins and Sadock Modeling Scope Preferences 3. Approaches to Quantifier Scope in Computational Linguistics Many studies, such as Pereira (1990) and Park (1995), have dealt with the issue of scope generation from a computational perspective. Attemp</context>
</contexts>
<marker>Aoun, Li, 1993</marker>
<rawString>Aoun, Joseph and Yen-hui Audrey Li. 1993. The Syntax of Scope. MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Aoun</author>
<author>Yen-hui Audrey Li</author>
</authors>
<title>Scope, structure, and expert systems: A reply to Kuno et al.</title>
<date>2000</date>
<journal>Language,</journal>
<volume>76</volume>
<issue>1</issue>
<contexts>
<context position="10057" citStr="Aoun and Li (2000)" startWordPosition="1574" endWordPosition="1577">pe unambiguously. (2) Many of us/you hate some of them. 75 Computational Linguistics Volume 29, Number 1 Figure 2 Simple illustration of the Cooper storage approach to quantifier scope generation. Table 1 Voting to determine optimal scope readings for quantifiers, according to Kuno, Takami, and Wu (1999). many of us/you some of them V V Baseline: V Subject Q: V Lefthand Q: V Speaker/Hearer Q: Total: 4 1 Some adherents of the structural approaches also seem to acknowledge the necessity of eventually coming to terms with the factors that play a role in determining scope preferences in language. Aoun and Li (2000) claim that the lexical scope preferences of quantifiers “are not ruled out under a structural account” (page 140). It is clear from the surrounding discussion, though, that they intend such lexical requirements to be taken care of in some nonsyntactic component of grammar. Although Kuno, Takami, and Wu’s dialogue with Aoun and Li in Language has been portrayed by both sides as a debate over the correct way of modeling quantifier scope, they are not really modeling the same things. Whereas Aoun and Li (1993) provide an account of scope generation, Kuno, Takami, and Wu (1999) intend to model bo</context>
<context position="47436" citStr="Aoun and Li (2000)" startWordPosition="7711" endWordPosition="7714">u (although the classifiers’ implementation is slightly more sophisticated than the unweighted voting that is actually used in Kuno, Takami, and Wu’s paper). Of course, since we do not use exactly the set of features suggested by Kuno, Takami, and Wu, our model should not be seen as a straightforward implementation of the theory outlined in their 1999 paper. Nevertheless, the results in Table 9 suggest that Kuno, Takami, and Wu’s suggested design can be used with some success in modeling scope preferences. Moreover, the project undertaken here provides an answer to some of the objections that Aoun and Li (2000) raise to Kuno, Takami, and Wu. Aoun and Li claim that Kuno, Takami, and Wu’s choice of experts is seemingly arbitrary and that it is unclear how the voting weights of each expert are to be set, but the machine learning approach we employ in this article is capable of addressing both of these potential problems. Supervised training of our classifiers is a straightforward approach to setting the weights and also constitutes our approach to selecting features (or “experts” in Kuno, Takami, and Wu’s terminology). In the training process, any feature that is irrelevant to scoping preferences shoul</context>
</contexts>
<marker>Aoun, Li, 2000</marker>
<rawString>Aoun, Joseph and Yen-hui Audrey Li. 2000. Scope, structure, and expert systems: A reply to Kuno et al. Language, 76(1):133–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
</authors>
<title>Optimisation of corpus-derived probabilistic grammars.</title>
<date>2001</date>
<booktitle>In Proceedings of Corpus Linguistics</booktitle>
<pages>46--57</pages>
<marker>Belz, 2001</marker>
<rawString>Belz, Anja. 2001. Optimisation of corpus-derived probabilistic grammars. In Proceedings of Corpus Linguistics 2001, pages 46–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Berger, Adam L., Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Bies</author>
<author>Mark Ferguson</author>
<author>Karen Katz</author>
<author>Robert MacIntyre</author>
</authors>
<title>Bracketing guidelines for Treebank II style.</title>
<date>1995</date>
<tech>Technical report,</tech>
<institution>Penn Treebank Project, University of Pennsylvania.</institution>
<contexts>
<context position="18891" citStr="Bies et al. (1995)" startWordPosition="3000" endWordPosition="3003">h four. In addition, our review of these sentences revealed that many of them simply involve lists in which the quantifiers do not interact in terms of scope (as in, for example, “We ask that you turn off all cell phones, extinguish all cigarettes, and open any candy before the performance begins”). Thus, the class of sentences with more than two quantifiers is small and seems to involve even simpler quantifier interactions than those found in our corpus. 4 These categories are intended to be understood as they are used in the tagging and parsing of the Penn Treebank. See Santorini (1990) and Bies et al. (1995) for details; the Appendix lists selected codes used 78 Higgins and Sadock Modeling Scope Preferences ( (S (NP-SBJ (NP (DT Those) ) (SBAR (WHNP-1 (WP who) ) (S (NP-SBJ-2 (-NONE- *T*-1) ) (ADVP (RB still) ) (VP (VBP want) (S (NP-SBJ (-NONE- *-2) ) (VP (TO to) (VP (VB do) (NP (PRP it) )))))))) (44 44) (VP (MD will) (ADVP (RB just) ) (VP (VB find) (NP (NP (DT-Q2 some) (NN way) ) (SBAR (WHADVP-3 (-NONE- 0) ) (S (NP-SBJ (-NONE- *) ) (VP (TO to) (VP (VB get) (PP (IN around) (’’ ’’) (NP (DT-Q1 any) (NN attempt) (S (NP-SBJ (-NONE- *) ) (VP (TO to) (VP (VB curb) (NP (PRP it) )))))) (ADVP-MNR (-NONE- *T</context>
<context position="62931" citStr="Bies et al. (1995)" startWordPosition="10384" endWordPosition="10387">, and Wu claim to be relevant to scoping. In addition, our final experiment, in which we combine our quantifier scope module with a PCFG model of syntactic phrase structure, provides evidence of a grammatical architecture in which different aspects of structure mutually constrain one another. This result casts doubt on approaches in which syntactic processing is completed prior to the determination of other grammatical properties of a sentence, such as quantifier scope relations. Appendix: Selected Codes Used to Annotate Syntactic Categories in the Penn Treebank, from Marcus et al. (1993) and Bies et al. (1995) Part-of-speech tags Tag Meaning Tag Meaning CC Conjunction RB Adverb CD Cardinal number RBR Comparative adverb DT Determiner RBS Superlative adverb IN Preposition TO “to” JJ Adjective UH Interjection JJR Comparative adjective VB Verb in base form JJS Superlative adjective VBD Past-tense verb NN Singular or mass noun VBG Gerundive verb NNS Plural noun VBN Past participial verb NNP Singular proper noun VBP Non-3sg, presentNNPS Plural proper noun tense verb PDT Predeterminer VBZ 3sg, present-tense verb PRP Personal pronoun WP WH pronoun PRP$ Possessive pronoun WP$ Possessive WH pronoun 94 Higgin</context>
</contexts>
<marker>Bies, Ferguson, Katz, MacIntyre, 1995</marker>
<rawString>Bies, Ann, Mark Ferguson, Karen Katz, and Robert MacIntyre. 1995. Bracketing guidelines for Treebank II style. Technical report, Penn Treebank Project, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher M Bishop</author>
</authors>
<title>Neural Networks for Pattern Recognition.</title>
<date>1995</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford.</location>
<contexts>
<context position="15250" citStr="Bishop 1995" startWordPosition="2401" endWordPosition="2402">ifiers in a sentence takes wide scope, given a number of different sources of evidence, is an example of what is known in machine learning as a classification task (Mitchell 1996). There are many types of classifiers that may be applied to this task that both are more sophisticated than the approach suggested by Kuno, Takami, and Wu and have a more solid probabilistic foundation. These include the naive Bayes classifier (Manning and Sch¨utze 1999; Jurafsky and Martin 2000), maximum-entropy models (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1997), and the single-layer perceptron (Bishop 1995). We employ these classifier models here primarily because of their straightforward probabilistic interpretation and their similarity to the scope model of Kuno, Takami, and Wu (since they each could be said to implement a kind of weighted voting of factors). In Section 4.3, we describe how classifiers of these types can be constructed to serve as a grammatical module responsible for quantifier scope determination. All of these classifiers can be trained in a supervised manner. That is, given a sample of training data that provides all of the information that is deemed to be relevant to quanti</context>
</contexts>
<marker>Bishop, 1995</marker>
<rawString>Bishop, Christopher M. 1995. Neural Networks for Pattern Recognition. Oxford University Press, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John S Bridle</author>
</authors>
<title>Probabilistic interpretation of feedforward classification network outputs with relationships to statistical pattern recognition.</title>
<date>1990</date>
<booktitle>Neurocomputing— Algorithms, Architectures, and Applications.</booktitle>
<pages>227--236</pages>
<editor>In F. Fougelman-Soulie and J. Herault, editors,</editor>
<publisher>Springer-Verlag,</publisher>
<location>Berlin,</location>
<contexts>
<context position="40540" citStr="Bridle 1990" startWordPosition="6613" endWordPosition="6614">r tend toward class 0, “no scope interaction,” which is easy to understand. Quotations are often opaque to quantifier scope interactions. The top features found by the maximum-entropy classifier largely coincide with those found by the naive Bayes model, which indicates that these generalizations are robust and objectively present in the data. 4.3.3 Single-Layer Perceptron. For our neural network classifier, we employed a feedforward single-layer perceptron, with the softmax function used to determine the activation of nodes at the output layer, because this is a one-of-n classification task (Bridle 1990). The data to be classified are presented as a vector of features at the input layer, and the output layer has three nodes, representing the three possible classes for the data: “first has wide scope,” “second has wide scope,” and “no scope interaction.” The output node with the highest activation is interpreted as the class of the datum presented at the input layer. For each of the 10 test sets of 89 examples, we trained the connection weights of the network using error backpropagation on 654 training sentences, reserving 150 sentences for validation in order to choose the weights from the tr</context>
</contexts>
<marker>Bridle, 1990</marker>
<rawString>Bridle, John S. 1990. Probabilistic interpretation of feedforward classification network outputs with relationships to statistical pattern recognition. In F. Fougelman-Soulie and J. Herault, editors, Neurocomputing— Algorithms, Architectures, and Applications. Springer-Verlag, Berlin, pages 227–236.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="13753" citStr="Brill 1995" startWordPosition="2170" endWordPosition="2171">t Journal corpus in a robust way, because they need to do a full semantic analysis of a sentence in order to make scope predictions. The statistical basis of the model presented in this article offers increased robustness and the possibility of more serious evaluation on the basis of corpus data. 4. Modeling Quantifier Scope In this section, we argue for an empirically driven machine learning approach to the identification of factors relevant to quantifier scope and the modeling of scope preferences. Following much recent work that applies the tools of machine learning to linguistic problems (Brill 1995; Pedersen 2000; van Halteren, Zavrel, and Daelemans 2001; Soon, Ng, and Lim 2001), we will treat the prediction of quantifier scope as an example of a classification task. Our aim is to provide a robust model of scope prediction based on Kuno, Takami, and Wu’s theoretical foundation and to address the serious lack of empirical results regarding quantifier scope in computational work. We describe here the modeling tools borrowed from the field of artificial intelligence for the scope prediction task and the data from which the generalizations are to be learned. Finally, we present the results </context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Brill, Eric. 1995. Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging. Computational Linguistics, 21(4):543–565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guy Carden</author>
</authors>
<title>English Quantifiers: Logical Structure and Linguistic Variation.</title>
<date>1976</date>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<contexts>
<context position="6561" citStr="Carden (1976)" startWordPosition="996" endWordPosition="997"> Aoun and Li 1993; Hornstein 1995). Another popular technique is to percolate scope information up through the syntactic tree using Cooper storage (Cooper 1983; Hobbs and Shieber 1987; Pollard 1989; Nerbonne 1993; Park 1995; Pollard and Yoo 1998). The QR approach to dealing with scope in linguistics consists in the claim that there is a covert transformation applying to syntactic structures that moves quantified elements out of the position in which they are found on the surface and raises them to a higher position that reflects their scope. The various incarnations of the strategy that 1 See Carden (1976), however, for a questionnaire-based approach to gathering data on the accessibility of different quantifier scope readings. 74 Higgins and Sadock Modeling Scope Preferences Figure 1 Simple illustration of the QR approach to quantifier scope generation. follows from this claim differ in the precise characterization of this QR transformation, what conditions are placed upon it, and what tree-configurational relationship is required for one operator to take scope over another. The general idea of QR is represented in Figure 1, a schematic analysis of the reading of the sentence Someone saw every</context>
</contexts>
<marker>Carden, 1976</marker>
<rawString>Carden, Guy. 1976. English Quantifiers: Logical Structure and Linguistic Variation. Academic Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: The kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="21760" citStr="Carletta (1996)" startWordPosition="3482" endWordPosition="3483">data used in this project, the data were coded a second time by an independent coder, in addition to the reference coding. The independent codings agreed with the reference coding on 76.3% of sentences. The kappa statistic (Cohen 1960) for agreement was .52, with a 95% confidence interval between .40 and .64. Krippendorff (1980) has been widely cited as advocating the view that kappa values greater than .8 should be taken as indicating good reliability, with values between .67 and .8 indicating tentative reliability, but we are satisfied with the level of intercoder agreement on this task. As Carletta (1996) notes, many tasks in computational linguistics are simply more difficult than the content analysis classifications addressed by Krippendorff, and according to Fleiss (1981), kappa values between .4 and .75 indicate fair to good agreement anyhow. Discussion between the coders revealed that there was no single cause for their differences in judgments when such differences existed. Many cases of disagreement stem from different assumptions regarding the lexical quantifiers involved. For example, the coders sometimes differed on whether a given instance of the word any corresponds to a narrow-sco</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Carletta, Jean. 1996. Assessing agreement on classification tasks: The kappa statistic. Computational Linguistics, 22(2):249–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical language learning.</title>
<date>1993</date>
<publisher>MIT Press,</publisher>
<location>Cambridge.</location>
<marker>Charniak, 1993</marker>
<rawString>Charniak, Eugene. 1993. Statistical language learning. MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Tree-bank grammars.</title>
<date>1996</date>
<booktitle>In AAAI/IAAI,</booktitle>
<volume>2</volume>
<pages>1031--1036</pages>
<contexts>
<context position="51559" citStr="Charniak 1996" startWordPosition="8375" endWordPosition="8376">determination is applied to the optimal syntactic structure (the Viterbi parse), and one in which optimization is performed in the space of both modules to find the optimal pairing of syntactic and quantifier scope structures. 5.2 The Syntactic Module Before turning to the application of our multimodular approach to the problem of scope determination in Section 5.3, we present here a short overview of the phrase structure syntactic component used in these projects. As noted above, we model syntax as a probabilistic phrase structure grammar (PCFG), and in particular, we use a treebank grammar (Charniak 1996) trained on the Penn Treebank. A PCFG defines the probability of a string of words as the sum of the probabilities of all admissible phrase structure parses (trees) for that string. The probability of a given tree is the product of the probability of all of the rule instances used in the construction of that tree, where rules take the form N → 0, with N a nonterminal symbol and 0 a finite sequence of one or more terminals or nonterminals. To take an example, Figure 5 illustrates a phrase structure tree for the sentence Susan might not believe you, which is admissible according to the grammar i</context>
</contexts>
<marker>Charniak, 1996</marker>
<rawString>Charniak, Eugene. 1996. Tree-bank grammars. In AAAI/IAAI, vol. 2, pages 1031–1036.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measurement,</booktitle>
<pages>20--37</pages>
<contexts>
<context position="21380" citStr="Cohen 1960" startWordPosition="3421" endWordPosition="3422">nce is seldom treated in the linguistic literature. As we are dealing with corpus data in this experiment, we cannot afford to ignore this possibility. 79 Computational Linguistics Volume 29, Number 1 The result is a set of 893 sentences,6 annotated with Penn Treebank II parse trees and hand-tagged for the primary scope reading. To assess the reliability of the hand-tagged data used in this project, the data were coded a second time by an independent coder, in addition to the reference coding. The independent codings agreed with the reference coding on 76.3% of sentences. The kappa statistic (Cohen 1960) for agreement was .52, with a 95% confidence interval between .40 and .64. Krippendorff (1980) has been widely cited as advocating the view that kappa values greater than .8 should be taken as indicating good reliability, with values between .67 and .8 indicating tentative reliability, but we are satisfied with the level of intercoder agreement on this task. As Carletta (1996) notes, many tasks in computational linguistics are simply more difficult than the content analysis classifications addressed by Krippendorff, and according to Fleiss (1981), kappa values between .4 and .75 indicate fair</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Cohen, Jacob. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20:37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robin Cooper</author>
</authors>
<title>Quantification and Syntactic Theory.</title>
<date>1983</date>
<location>Reidel, Dordrecht.</location>
<contexts>
<context position="6107" citStr="Cooper 1983" startWordPosition="921" endWordPosition="922">ake scope, whereas generative semantic analyses such as McCawley (1998) represented the scope of quantification at deep structure, transformationally lowering quantifiers into their surface positions during the course of the derivation. More recent work in the interpretive paradigm takes the opposite approach, extracting quantifiers from their surface positions to their scope positions by means of a quantifier-raising (QR) transformation (May 1985; Aoun and Li 1993; Hornstein 1995). Another popular technique is to percolate scope information up through the syntactic tree using Cooper storage (Cooper 1983; Hobbs and Shieber 1987; Pollard 1989; Nerbonne 1993; Park 1995; Pollard and Yoo 1998). The QR approach to dealing with scope in linguistics consists in the claim that there is a covert transformation applying to syntactic structures that moves quantified elements out of the position in which they are found on the surface and raises them to a higher position that reflects their scope. The various incarnations of the strategy that 1 See Carden (1976), however, for a questionnaire-based approach to gathering data on the accessibility of different quantifier scope readings. 74 Higgins and Sadock</context>
</contexts>
<marker>Cooper, 1983</marker>
<rawString>Cooper, Robin. 1983. Quantification and Syntactic Theory. Reidel, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph L Fleiss</author>
</authors>
<title>Statistical Methods for Rates and Proportions.</title>
<date>1981</date>
<publisher>John Wiley &amp; Sons,</publisher>
<location>New York.</location>
<contexts>
<context position="21933" citStr="Fleiss (1981)" startWordPosition="3505" endWordPosition="3506">ding on 76.3% of sentences. The kappa statistic (Cohen 1960) for agreement was .52, with a 95% confidence interval between .40 and .64. Krippendorff (1980) has been widely cited as advocating the view that kappa values greater than .8 should be taken as indicating good reliability, with values between .67 and .8 indicating tentative reliability, but we are satisfied with the level of intercoder agreement on this task. As Carletta (1996) notes, many tasks in computational linguistics are simply more difficult than the content analysis classifications addressed by Krippendorff, and according to Fleiss (1981), kappa values between .4 and .75 indicate fair to good agreement anyhow. Discussion between the coders revealed that there was no single cause for their differences in judgments when such differences existed. Many cases of disagreement stem from different assumptions regarding the lexical quantifiers involved. For example, the coders sometimes differed on whether a given instance of the word any corresponds to a narrow-scope existential, as we conventionally treat it when it is in the scope of negation, or the “free-choice” version of any. To take another example, two universal quantifiers ar</context>
</contexts>
<marker>Fleiss, 1981</marker>
<rawString>Fleiss, Joseph L. 1981. Statistical Methods for Rates and Proportions. John Wiley &amp; Sons, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
<author>Stuart M Shieber</author>
</authors>
<title>An algorithm for generating quantifier scopings.</title>
<date>1987</date>
<journal>Computational Linguistics,</journal>
<pages>13--47</pages>
<contexts>
<context position="6131" citStr="Hobbs and Shieber 1987" startWordPosition="923" endWordPosition="926">ereas generative semantic analyses such as McCawley (1998) represented the scope of quantification at deep structure, transformationally lowering quantifiers into their surface positions during the course of the derivation. More recent work in the interpretive paradigm takes the opposite approach, extracting quantifiers from their surface positions to their scope positions by means of a quantifier-raising (QR) transformation (May 1985; Aoun and Li 1993; Hornstein 1995). Another popular technique is to percolate scope information up through the syntactic tree using Cooper storage (Cooper 1983; Hobbs and Shieber 1987; Pollard 1989; Nerbonne 1993; Park 1995; Pollard and Yoo 1998). The QR approach to dealing with scope in linguistics consists in the claim that there is a covert transformation applying to syntactic structures that moves quantified elements out of the position in which they are found on the surface and raises them to a higher position that reflects their scope. The various incarnations of the strategy that 1 See Carden (1976), however, for a questionnaire-based approach to gathering data on the accessibility of different quantifier scope readings. 74 Higgins and Sadock Modeling Scope Preferen</context>
<context position="11308" citStr="Hobbs and Shieber (1987)" startWordPosition="1777" endWordPosition="1780">pe prediction. The model of scope preferences provided in this article is an empirically based refinement of the approach taken by Kuno, Takami, and Wu, but in principle it is consistent with a structural account of scope generation. 76 Higgins and Sadock Modeling Scope Preferences 3. Approaches to Quantifier Scope in Computational Linguistics Many studies, such as Pereira (1990) and Park (1995), have dealt with the issue of scope generation from a computational perspective. Attempts have also been made in computational work to extend a pure Cooper storage approach to handle scope prediction. Hobbs and Shieber (1987) discuss the possibility of incorporating some sort of ordering heuristics into the SRI scope generation system, in the hopes of producing a ranked list of possible scope readings, but ultimately are forced to acknowledge that “[t]he modifications turn out to be quite complicated if we wish to order quantifiers according to lexical heuristics, such as having each out-scope some. Because of the recursive nature of the algorithm, there are limits to the amount of ordering that can be done in this manner” (page 55). The stepwise nature of these scope mechanisms makes it hard to state the factors </context>
</contexts>
<marker>Hobbs, Shieber, 1987</marker>
<rawString>Hobbs, Jerry R. and Stuart M. Shieber. 1987. An algorithm for generating quantifier scopings. Computational Linguistics, 13:47–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norbert Hornstein</author>
</authors>
<title>Logical Form: From GB to Minimalism. Blackwell,</title>
<date>1995</date>
<location>Oxford and Cambridge.</location>
<contexts>
<context position="5982" citStr="Hornstein 1995" startWordPosition="903" endWordPosition="904">t up. Montague (1973) used a syntactic rule to introduce a quantified expression into a derivation at the point where it was to take scope, whereas generative semantic analyses such as McCawley (1998) represented the scope of quantification at deep structure, transformationally lowering quantifiers into their surface positions during the course of the derivation. More recent work in the interpretive paradigm takes the opposite approach, extracting quantifiers from their surface positions to their scope positions by means of a quantifier-raising (QR) transformation (May 1985; Aoun and Li 1993; Hornstein 1995). Another popular technique is to percolate scope information up through the syntactic tree using Cooper storage (Cooper 1983; Hobbs and Shieber 1987; Pollard 1989; Nerbonne 1993; Park 1995; Pollard and Yoo 1998). The QR approach to dealing with scope in linguistics consists in the claim that there is a covert transformation applying to syntactic structures that moves quantified elements out of the position in which they are found on the surface and raises them to a higher position that reflects their scope. The various incarnations of the strategy that 1 See Carden (1976), however, for a ques</context>
</contexts>
<marker>Hornstein, 1995</marker>
<rawString>Hornstein, Norbert. 1995. Logical Form: From GB to Minimalism. Blackwell, Oxford and Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
</authors>
<title>Statistical Methods for Speech Recognition.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge.</location>
<marker>Jelinek, 1998</marker>
<rawString>Jelinek, Frederick. 1998. Statistical Methods for Speech Recognition. MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>James H Martin</author>
</authors>
<title>Speech and Language Processing. Prentice Hall, Upper Saddle River,</title>
<date>2000</date>
<location>New Jersey.</location>
<contexts>
<context position="15115" citStr="Jurafsky and Martin 2000" startWordPosition="2382" endWordPosition="2385">tational linguistics. 77 Computational Linguistics Volume 29, Number 1 4.1 Classification in Machine Learning Determining which among multiple quantifiers in a sentence takes wide scope, given a number of different sources of evidence, is an example of what is known in machine learning as a classification task (Mitchell 1996). There are many types of classifiers that may be applied to this task that both are more sophisticated than the approach suggested by Kuno, Takami, and Wu and have a more solid probabilistic foundation. These include the naive Bayes classifier (Manning and Sch¨utze 1999; Jurafsky and Martin 2000), maximum-entropy models (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1997), and the single-layer perceptron (Bishop 1995). We employ these classifier models here primarily because of their straightforward probabilistic interpretation and their similarity to the scope model of Kuno, Takami, and Wu (since they each could be said to implement a kind of weighted voting of factors). In Section 4.3, we describe how classifiers of these types can be constructed to serve as a grammatical module responsible for quantifier scope determination. All of these classifiers can be trained in a s</context>
</contexts>
<marker>Jurafsky, Martin, 2000</marker>
<rawString>Jurafsky, Daniel and James H. Martin. 2000. Speech and Language Processing. Prentice Hall, Upper Saddle River, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to Its Methodology. Sage Publications,</title>
<date>1980</date>
<location>Beverly Hills, California.</location>
<contexts>
<context position="21475" citStr="Krippendorff (1980)" startWordPosition="3436" endWordPosition="3437">in this experiment, we cannot afford to ignore this possibility. 79 Computational Linguistics Volume 29, Number 1 The result is a set of 893 sentences,6 annotated with Penn Treebank II parse trees and hand-tagged for the primary scope reading. To assess the reliability of the hand-tagged data used in this project, the data were coded a second time by an independent coder, in addition to the reference coding. The independent codings agreed with the reference coding on 76.3% of sentences. The kappa statistic (Cohen 1960) for agreement was .52, with a 95% confidence interval between .40 and .64. Krippendorff (1980) has been widely cited as advocating the view that kappa values greater than .8 should be taken as indicating good reliability, with values between .67 and .8 indicating tentative reliability, but we are satisfied with the level of intercoder agreement on this task. As Carletta (1996) notes, many tasks in computational linguistics are simply more difficult than the content analysis classifications addressed by Krippendorff, and according to Fleiss (1981), kappa values between .4 and .75 indicate fair to good agreement anyhow. Discussion between the coders revealed that there was no single caus</context>
</contexts>
<marker>Krippendorff, 1980</marker>
<rawString>Krippendorff, Klaus. 1980. Content Analysis: An Introduction to Its Methodology. Sage Publications, Beverly Hills, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Krotov</author>
<author>Mark Hepple</author>
<author>Robert J Gaizauskas</author>
<author>Yorick Wilks</author>
</authors>
<title>Compacting the Penn treebank grammar.</title>
<date>1998</date>
<booktitle>In COLING-ACL,</booktitle>
<pages>699--703</pages>
<marker>Krotov, Hepple, Gaizauskas, Wilks, 1998</marker>
<rawString>Krotov, Alexander, Mark Hepple, Robert J. Gaizauskas, and Yorick Wilks. 1998. Compacting the Penn treebank grammar. In COLING-ACL, pages 699–703.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susumu Kuno</author>
<author>Ken-Ichi Takami</author>
<author>Yuru Wu</author>
</authors>
<date>1999</date>
<booktitle>Quantifier scope in English, Chinese, and Japanese. Language,</booktitle>
<volume>75</volume>
<issue>1</issue>
<marker>Kuno, Takami, Wu, 1999</marker>
<rawString>Kuno, Susumu, Ken-Ichi Takami, and Yuru Wu. 1999. Quantifier scope in English, Chinese, and Japanese. Language, 75(1):63–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susumu Kuno</author>
<author>Ken-Ichi Takami</author>
<author>Yuru Wu</author>
</authors>
<date>2001</date>
<booktitle>Response to Aoun and Li. Language,</booktitle>
<pages>77--1</pages>
<marker>Kuno, Takami, Wu, 2001</marker>
<rawString>Kuno, Susumu, Ken-Ichi Takami, and Yuru Wu. 2001. Response to Aoun and Li. Language, 77(1):134–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge.</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Manning, Christopher D. and Hinrich Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="62908" citStr="Marcus et al. (1993)" startWordPosition="10379" endWordPosition="10382">factors that Kuno, Takami, and Wu claim to be relevant to scoping. In addition, our final experiment, in which we combine our quantifier scope module with a PCFG model of syntactic phrase structure, provides evidence of a grammatical architecture in which different aspects of structure mutually constrain one another. This result casts doubt on approaches in which syntactic processing is completed prior to the determination of other grammatical properties of a sentence, such as quantifier scope relations. Appendix: Selected Codes Used to Annotate Syntactic Categories in the Penn Treebank, from Marcus et al. (1993) and Bies et al. (1995) Part-of-speech tags Tag Meaning Tag Meaning CC Conjunction RB Adverb CD Cardinal number RBR Comparative adverb DT Determiner RBS Superlative adverb IN Preposition TO “to” JJ Adjective UH Interjection JJR Comparative adjective VB Verb in base form JJS Superlative adjective VBD Past-tense verb NN Singular or mass noun VBG Gerundive verb NNS Plural noun VBN Past participial verb NNP Singular proper noun VBP Non-3sg, presentNNPS Plural proper noun tense verb PDT Predeterminer VBZ 3sg, present-tense verb PRP Personal pronoun WP WH pronoun PRP$ Possessive pronoun WP$ Possessi</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Marcus, Mitchell P., Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Martin</author>
<author>Douglas Appelt</author>
<author>Fernando Pereira</author>
</authors>
<title>Transportability and generality in a natural-language interface system. In</title>
<date>1986</date>
<booktitle>Natural Language Processing.</booktitle>
<pages>585--593</pages>
<editor>B. J. Grosz, K. Sparck Jones, and B. L. Webber, editors,</editor>
<publisher>Kaufmann,</publisher>
<location>Los Altos, California,</location>
<marker>Martin, Appelt, Pereira, 1986</marker>
<rawString>Martin, Paul, Douglas Appelt, and Fernando Pereira. 1986. Transportability and generality in a natural-language interface system. In B. J. Grosz, K. Sparck Jones, and B. L. Webber, editors, Natural Language Processing. Kaufmann, Los Altos, California, pages 585–593.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert May</author>
</authors>
<title>Logical Form: Its Structure and Derivation.</title>
<date>1985</date>
<publisher>MIT Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="5947" citStr="May 1985" startWordPosition="897" endWordPosition="898">ucture of a sentence is built up. Montague (1973) used a syntactic rule to introduce a quantified expression into a derivation at the point where it was to take scope, whereas generative semantic analyses such as McCawley (1998) represented the scope of quantification at deep structure, transformationally lowering quantifiers into their surface positions during the course of the derivation. More recent work in the interpretive paradigm takes the opposite approach, extracting quantifiers from their surface positions to their scope positions by means of a quantifier-raising (QR) transformation (May 1985; Aoun and Li 1993; Hornstein 1995). Another popular technique is to percolate scope information up through the syntactic tree using Cooper storage (Cooper 1983; Hobbs and Shieber 1987; Pollard 1989; Nerbonne 1993; Park 1995; Pollard and Yoo 1998). The QR approach to dealing with scope in linguistics consists in the claim that there is a covert transformation applying to syntactic structures that moves quantified elements out of the position in which they are found on the surface and raises them to a higher position that reflects their scope. The various incarnations of the strategy that 1 See</context>
</contexts>
<marker>May, 1985</marker>
<rawString>May, Robert. 1985. Logical Form: Its Structure and Derivation. MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James D McCawley</author>
</authors>
<title>The Syntactic Phenomena of English.</title>
<date>1998</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago,</location>
<note>second edition.</note>
<contexts>
<context position="5567" citStr="McCawley (1998)" startWordPosition="846" endWordPosition="847">es of grammar ought to be combined in serial, with information from the syntax feeding the quantifier scope module, or in parallel, with each module constraining the structures provided by the other. 2. Approaches to Quantifier Scope in Theoretical Linguistics Most, if not all, linguistic treatments of quantifier scope have closely integrated it with the way in which the syntactic structure of a sentence is built up. Montague (1973) used a syntactic rule to introduce a quantified expression into a derivation at the point where it was to take scope, whereas generative semantic analyses such as McCawley (1998) represented the scope of quantification at deep structure, transformationally lowering quantifiers into their surface positions during the course of the derivation. More recent work in the interpretive paradigm takes the opposite approach, extracting quantifiers from their surface positions to their scope positions by means of a quantifier-raising (QR) transformation (May 1985; Aoun and Li 1993; Hornstein 1995). Another popular technique is to percolate scope information up through the syntactic tree using Cooper storage (Cooper 1983; Hobbs and Shieber 1987; Pollard 1989; Nerbonne 1993; Park </context>
</contexts>
<marker>McCawley, 1998</marker>
<rawString>McCawley, James D. 1998. The Syntactic Phenomena of English. University of Chicago Press, Chicago, second edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom M Mitchell</author>
</authors>
<date>1996</date>
<booktitle>Machine Learning.</booktitle>
<publisher>McGraw Hill,</publisher>
<location>New York.</location>
<contexts>
<context position="14817" citStr="Mitchell 1996" startWordPosition="2336" endWordPosition="2337"> artificial intelligence for the scope prediction task and the data from which the generalizations are to be learned. Finally, we present the results of training different incarnations of our scope module on the data and assess the implications of this exercise for theoretical and computational linguistics. 77 Computational Linguistics Volume 29, Number 1 4.1 Classification in Machine Learning Determining which among multiple quantifiers in a sentence takes wide scope, given a number of different sources of evidence, is an example of what is known in machine learning as a classification task (Mitchell 1996). There are many types of classifiers that may be applied to this task that both are more sophisticated than the approach suggested by Kuno, Takami, and Wu and have a more solid probabilistic foundation. These include the naive Bayes classifier (Manning and Sch¨utze 1999; Jurafsky and Martin 2000), maximum-entropy models (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1997), and the single-layer perceptron (Bishop 1995). We employ these classifier models here primarily because of their straightforward probabilistic interpretation and their similarity to the scope model of Kuno, Takam</context>
</contexts>
<marker>Mitchell, 1996</marker>
<rawString>Mitchell, Tom M. 1996. Machine Learning. McGraw Hill, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Montague</author>
</authors>
<title>The proper treatment of quantification in ordinary English. In</title>
<date>1973</date>
<booktitle>Approaches to Natural Language.</booktitle>
<pages>221--242</pages>
<editor>J. Hintikka et al., editors,</editor>
<location>Reidel, Dordrecht,</location>
<contexts>
<context position="5388" citStr="Montague (1973)" startWordPosition="817" endWordPosition="818">ted. Section 5 combines the scope model of the previous section with a probabilistic context-free grammar (PCFG) model of syntax and addresses the issue of whether these two modules of grammar ought to be combined in serial, with information from the syntax feeding the quantifier scope module, or in parallel, with each module constraining the structures provided by the other. 2. Approaches to Quantifier Scope in Theoretical Linguistics Most, if not all, linguistic treatments of quantifier scope have closely integrated it with the way in which the syntactic structure of a sentence is built up. Montague (1973) used a syntactic rule to introduce a quantified expression into a derivation at the point where it was to take scope, whereas generative semantic analyses such as McCawley (1998) represented the scope of quantification at deep structure, transformationally lowering quantifiers into their surface positions during the course of the derivation. More recent work in the interpretive paradigm takes the opposite approach, extracting quantifiers from their surface positions to their scope positions by means of a quantifier-raising (QR) transformation (May 1985; Aoun and Li 1993; Hornstein 1995). Anot</context>
</contexts>
<marker>Montague, 1973</marker>
<rawString>Montague, Richard. 1973. The proper treatment of quantification in ordinary English. In J. Hintikka et al., editors, Approaches to Natural Language. Reidel, Dordrecht, pages 221–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas B Moran</author>
</authors>
<title>Quantifier scoping in the SRI core language engine.</title>
<date>1988</date>
<booktitle>In Proceedings of the 26th Annual Meeting of the Association for Computational Linguistics (ACL’88),</booktitle>
<pages>33--40</pages>
<contexts>
<context position="3418" citStr="Moran 1988" startWordPosition="513" endWordPosition="514">10 East 59th Street, Chicago, IL 60637. E-mail: j-sadock@uchicago.edu. © 2003 Association for Computational Linguistics Computational Linguistics Volume 29, Number 1 sentence that are possible, without regard to their relative likelihood or naturalness. Recently, however, linguists such as Kuno, Takami, and Wu (1999) have begun to turn their attention to scope prediction, or determining the relative accessibility of different scope readings. In computational linguistics, more attention has been paid to the factors that determine scope preferences. Systems such as the SRI Core Language Engine (Moran 1988; Moran and Pereira 1992), LUNAR (Woods 1986), and TEAM (Martin, Appelt, and Pereira 1986) have employed scope critics that use heuristics to decide between alternative scopings. However, the rules that these systems use in making quantifier scope decisions are motivated only by the researchers’ intuitions, and no empirical results have been published regarding their accuracy. In this article, we use the tools of machine learning to construct a data-driven model of quantifier scope preferences. For theoretical linguistics, this model serves as an illustration that Kuno, Takami, and Wu’s approa</context>
<context position="12376" citStr="Moran (1988" startWordPosition="1950" endWordPosition="1951">mount of ordering that can be done in this manner” (page 55). The stepwise nature of these scope mechanisms makes it hard to state the factors that influence the preference for one quantifier to take scope over another. Those natural language processing (NLP) systems that have managed to provide some sort of account of quantifier scope preferences have done so by using a separate system of heuristics (or scope critics) that apply postsyntactically to determine the most likely scoping. LUNAR (Woods 1986), TEAM (Martin, Appelt, and Pereira 1986), and the SRI Core Language Engine as described by Moran (1988; Moran and Pereira 1992) all employ scope rules of this sort. By and large, these rules are of an ad hoc nature, implementing a linguist’s intuitive idea of what factors determine scope possibilities, and no results have been published regarding the accuracy of these methods. For example, Moran (1988) incorporates rules from other NLP systems and from VanLehn (1978), such as a preference for a logically weaker interpretation, the tendency for each to take wide scope, and a ban on raising a quantifier across multiple major clause boundaries. The testing of Moran’s system is “limited to checkin</context>
</contexts>
<marker>Moran, 1988</marker>
<rawString>Moran, Douglas B. 1988. Quantifier scoping in the SRI core language engine. In Proceedings of the 26th Annual Meeting of the Association for Computational Linguistics (ACL’88), pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas B Moran</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Quantifier scoping.</title>
<date>1992</date>
<booktitle>In Hiyan Alshawi, editor, The Core Language Engine.</booktitle>
<pages>149--172</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge,</location>
<contexts>
<context position="3443" citStr="Moran and Pereira 1992" startWordPosition="515" endWordPosition="518"> Street, Chicago, IL 60637. E-mail: j-sadock@uchicago.edu. © 2003 Association for Computational Linguistics Computational Linguistics Volume 29, Number 1 sentence that are possible, without regard to their relative likelihood or naturalness. Recently, however, linguists such as Kuno, Takami, and Wu (1999) have begun to turn their attention to scope prediction, or determining the relative accessibility of different scope readings. In computational linguistics, more attention has been paid to the factors that determine scope preferences. Systems such as the SRI Core Language Engine (Moran 1988; Moran and Pereira 1992), LUNAR (Woods 1986), and TEAM (Martin, Appelt, and Pereira 1986) have employed scope critics that use heuristics to decide between alternative scopings. However, the rules that these systems use in making quantifier scope decisions are motivated only by the researchers’ intuitions, and no empirical results have been published regarding their accuracy. In this article, we use the tools of machine learning to construct a data-driven model of quantifier scope preferences. For theoretical linguistics, this model serves as an illustration that Kuno, Takami, and Wu’s approach can capture some of th</context>
<context position="12401" citStr="Moran and Pereira 1992" startWordPosition="1952" endWordPosition="1955">ring that can be done in this manner” (page 55). The stepwise nature of these scope mechanisms makes it hard to state the factors that influence the preference for one quantifier to take scope over another. Those natural language processing (NLP) systems that have managed to provide some sort of account of quantifier scope preferences have done so by using a separate system of heuristics (or scope critics) that apply postsyntactically to determine the most likely scoping. LUNAR (Woods 1986), TEAM (Martin, Appelt, and Pereira 1986), and the SRI Core Language Engine as described by Moran (1988; Moran and Pereira 1992) all employ scope rules of this sort. By and large, these rules are of an ad hoc nature, implementing a linguist’s intuitive idea of what factors determine scope possibilities, and no results have been published regarding the accuracy of these methods. For example, Moran (1988) incorporates rules from other NLP systems and from VanLehn (1978), such as a preference for a logically weaker interpretation, the tendency for each to take wide scope, and a ban on raising a quantifier across multiple major clause boundaries. The testing of Moran’s system is “limited to checking conformance to the stat</context>
</contexts>
<marker>Moran, Pereira, 1992</marker>
<rawString>Moran, Douglas B. and Fernando C. N. Pereira. 1992. Quantifier scoping. In Hiyan Alshawi, editor, The Core Language Engine. MIT Press, Cambridge, pages 149–172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Nerbonne</author>
</authors>
<title>A feature-based syntax/semantics interface.</title>
<date>1993</date>
<booktitle>Annals of Mathematics and Artificial Intelligence (Special Issue on Mathematics of Language),</booktitle>
<pages>8--1</pages>
<editor>In A. Manaster-Ramer and W. Zadrozsny, editors,</editor>
<note>Also published as DFKI Research Report RR-92-42.</note>
<contexts>
<context position="6160" citStr="Nerbonne 1993" startWordPosition="929" endWordPosition="930">h as McCawley (1998) represented the scope of quantification at deep structure, transformationally lowering quantifiers into their surface positions during the course of the derivation. More recent work in the interpretive paradigm takes the opposite approach, extracting quantifiers from their surface positions to their scope positions by means of a quantifier-raising (QR) transformation (May 1985; Aoun and Li 1993; Hornstein 1995). Another popular technique is to percolate scope information up through the syntactic tree using Cooper storage (Cooper 1983; Hobbs and Shieber 1987; Pollard 1989; Nerbonne 1993; Park 1995; Pollard and Yoo 1998). The QR approach to dealing with scope in linguistics consists in the claim that there is a covert transformation applying to syntactic structures that moves quantified elements out of the position in which they are found on the surface and raises them to a higher position that reflects their scope. The various incarnations of the strategy that 1 See Carden (1976), however, for a questionnaire-based approach to gathering data on the accessibility of different quantifier scope readings. 74 Higgins and Sadock Modeling Scope Preferences Figure 1 Simple illustrat</context>
</contexts>
<marker>Nerbonne, 1993</marker>
<rawString>Nerbonne, John. 1993. A feature-based syntax/semantics interface. In A. Manaster-Ramer and W. Zadrozsny, editors, Annals of Mathematics and Artificial Intelligence (Special Issue on Mathematics of Language), 8(1–2):107–132. Also published as DFKI Research Report RR-92-42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jong C Park</author>
</authors>
<title>Quantifier scope and constituency.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics (ACL’95),</booktitle>
<pages>205--212</pages>
<contexts>
<context position="6171" citStr="Park 1995" startWordPosition="931" endWordPosition="932">1998) represented the scope of quantification at deep structure, transformationally lowering quantifiers into their surface positions during the course of the derivation. More recent work in the interpretive paradigm takes the opposite approach, extracting quantifiers from their surface positions to their scope positions by means of a quantifier-raising (QR) transformation (May 1985; Aoun and Li 1993; Hornstein 1995). Another popular technique is to percolate scope information up through the syntactic tree using Cooper storage (Cooper 1983; Hobbs and Shieber 1987; Pollard 1989; Nerbonne 1993; Park 1995; Pollard and Yoo 1998). The QR approach to dealing with scope in linguistics consists in the claim that there is a covert transformation applying to syntactic structures that moves quantified elements out of the position in which they are found on the surface and raises them to a higher position that reflects their scope. The various incarnations of the strategy that 1 See Carden (1976), however, for a questionnaire-based approach to gathering data on the accessibility of different quantifier scope readings. 74 Higgins and Sadock Modeling Scope Preferences Figure 1 Simple illustration of the </context>
<context position="11082" citStr="Park (1995)" startWordPosition="1744" endWordPosition="1745">modeling quantifier scope, they are not really modeling the same things. Whereas Aoun and Li (1993) provide an account of scope generation, Kuno, Takami, and Wu (1999) intend to model both scope generation and scope prediction. The model of scope preferences provided in this article is an empirically based refinement of the approach taken by Kuno, Takami, and Wu, but in principle it is consistent with a structural account of scope generation. 76 Higgins and Sadock Modeling Scope Preferences 3. Approaches to Quantifier Scope in Computational Linguistics Many studies, such as Pereira (1990) and Park (1995), have dealt with the issue of scope generation from a computational perspective. Attempts have also been made in computational work to extend a pure Cooper storage approach to handle scope prediction. Hobbs and Shieber (1987) discuss the possibility of incorporating some sort of ordering heuristics into the SRI scope generation system, in the hopes of producing a ranked list of possible scope readings, but ultimately are forced to acknowledge that “[t]he modifications turn out to be quite complicated if we wish to order quantifiers according to lexical heuristics, such as having each out-scop</context>
</contexts>
<marker>Park, 1995</marker>
<rawString>Park, Jong C. 1995. Quantifier scope and constituency. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics (ACL’95), pages 205–212.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>A simple approach to building ensembles of naive Bayesian classifiers for word sense disambiguation.</title>
<date>2000</date>
<booktitle>In Proceedings of the First Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL</booktitle>
<pages>63--69</pages>
<contexts>
<context position="13768" citStr="Pedersen 2000" startWordPosition="2172" endWordPosition="2173">rpus in a robust way, because they need to do a full semantic analysis of a sentence in order to make scope predictions. The statistical basis of the model presented in this article offers increased robustness and the possibility of more serious evaluation on the basis of corpus data. 4. Modeling Quantifier Scope In this section, we argue for an empirically driven machine learning approach to the identification of factors relevant to quantifier scope and the modeling of scope preferences. Following much recent work that applies the tools of machine learning to linguistic problems (Brill 1995; Pedersen 2000; van Halteren, Zavrel, and Daelemans 2001; Soon, Ng, and Lim 2001), we will treat the prediction of quantifier scope as an example of a classification task. Our aim is to provide a robust model of scope prediction based on Kuno, Takami, and Wu’s theoretical foundation and to address the serious lack of empirical results regarding quantifier scope in computational work. We describe here the modeling tools borrowed from the field of artificial intelligence for the scope prediction task and the data from which the generalizations are to be learned. Finally, we present the results of training dif</context>
</contexts>
<marker>Pedersen, 2000</marker>
<rawString>Pedersen, Ted. 2000. A simple approach to building ensembles of naive Bayesian classifiers for word sense disambiguation. In Proceedings of the First Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL 2000), pages 63–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
</authors>
<title>Categorial semantics and scoping.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="11066" citStr="Pereira (1990)" startWordPosition="1741" endWordPosition="1742">the correct way of modeling quantifier scope, they are not really modeling the same things. Whereas Aoun and Li (1993) provide an account of scope generation, Kuno, Takami, and Wu (1999) intend to model both scope generation and scope prediction. The model of scope preferences provided in this article is an empirically based refinement of the approach taken by Kuno, Takami, and Wu, but in principle it is consistent with a structural account of scope generation. 76 Higgins and Sadock Modeling Scope Preferences 3. Approaches to Quantifier Scope in Computational Linguistics Many studies, such as Pereira (1990) and Park (1995), have dealt with the issue of scope generation from a computational perspective. Attempts have also been made in computational work to extend a pure Cooper storage approach to handle scope prediction. Hobbs and Shieber (1987) discuss the possibility of incorporating some sort of ordering heuristics into the SRI scope generation system, in the hopes of producing a ranked list of possible scope readings, but ultimately are forced to acknowledge that “[t]he modifications turn out to be quite complicated if we wish to order quantifiers according to lexical heuristics, such as havi</context>
</contexts>
<marker>Pereira, 1990</marker>
<rawString>Pereira, Fernando. 1990. Categorial semantics and scoping. Computational Linguistics, 16(1):1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
</authors>
<title>The syntax-semantics interface in a unification-based phrase structure grammar.</title>
<date>1989</date>
<booktitle>Views of the Syntax/Semantics Interface KIT-FAST Report 74. Technical University of Berlin,</booktitle>
<pages>167--185</pages>
<editor>In S. Busemann, C. Hauenschild, and C. Umbach, editors,</editor>
<contexts>
<context position="6145" citStr="Pollard 1989" startWordPosition="927" endWordPosition="928">c analyses such as McCawley (1998) represented the scope of quantification at deep structure, transformationally lowering quantifiers into their surface positions during the course of the derivation. More recent work in the interpretive paradigm takes the opposite approach, extracting quantifiers from their surface positions to their scope positions by means of a quantifier-raising (QR) transformation (May 1985; Aoun and Li 1993; Hornstein 1995). Another popular technique is to percolate scope information up through the syntactic tree using Cooper storage (Cooper 1983; Hobbs and Shieber 1987; Pollard 1989; Nerbonne 1993; Park 1995; Pollard and Yoo 1998). The QR approach to dealing with scope in linguistics consists in the claim that there is a covert transformation applying to syntactic structures that moves quantified elements out of the position in which they are found on the surface and raises them to a higher position that reflects their scope. The various incarnations of the strategy that 1 See Carden (1976), however, for a questionnaire-based approach to gathering data on the accessibility of different quantifier scope readings. 74 Higgins and Sadock Modeling Scope Preferences Figure 1 S</context>
</contexts>
<marker>Pollard, 1989</marker>
<rawString>Pollard, Carl. 1989. The syntax-semantics interface in a unification-based phrase structure grammar. In S. Busemann, C. Hauenschild, and C. Umbach, editors, Views of the Syntax/Semantics Interface KIT-FAST Report 74. Technical University of Berlin, pages 167–185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Eun Jung Yoo</author>
</authors>
<title>A unified theory of scope for quantifiers and WH-phrases.</title>
<date>1998</date>
<journal>Journal of Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="6194" citStr="Pollard and Yoo 1998" startWordPosition="933" endWordPosition="936">sented the scope of quantification at deep structure, transformationally lowering quantifiers into their surface positions during the course of the derivation. More recent work in the interpretive paradigm takes the opposite approach, extracting quantifiers from their surface positions to their scope positions by means of a quantifier-raising (QR) transformation (May 1985; Aoun and Li 1993; Hornstein 1995). Another popular technique is to percolate scope information up through the syntactic tree using Cooper storage (Cooper 1983; Hobbs and Shieber 1987; Pollard 1989; Nerbonne 1993; Park 1995; Pollard and Yoo 1998). The QR approach to dealing with scope in linguistics consists in the claim that there is a covert transformation applying to syntactic structures that moves quantified elements out of the position in which they are found on the surface and raises them to a higher position that reflects their scope. The various incarnations of the strategy that 1 See Carden (1976), however, for a questionnaire-based approach to gathering data on the accessibility of different quantifier scope readings. 74 Higgins and Sadock Modeling Scope Preferences Figure 1 Simple illustration of the QR approach to quantifi</context>
</contexts>
<marker>Pollard, Yoo, 1998</marker>
<rawString>Pollard, Carl and Eun Jung Yoo. 1998. A unified theory of scope for quantifiers and WH-phrases. Journal of Linguistics, 34(2):415–446.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A simple introduction to maximum entropy models for natural language processing.</title>
<date>1997</date>
<tech>Technical Report 97-08,</tech>
<institution>Institute for Research in Cognitive Science, University of Pennsylvania.</institution>
<contexts>
<context position="15203" citStr="Ratnaparkhi 1997" startWordPosition="2395" endWordPosition="2396">hine Learning Determining which among multiple quantifiers in a sentence takes wide scope, given a number of different sources of evidence, is an example of what is known in machine learning as a classification task (Mitchell 1996). There are many types of classifiers that may be applied to this task that both are more sophisticated than the approach suggested by Kuno, Takami, and Wu and have a more solid probabilistic foundation. These include the naive Bayes classifier (Manning and Sch¨utze 1999; Jurafsky and Martin 2000), maximum-entropy models (Berger, Della Pietra, and Della Pietra 1996; Ratnaparkhi 1997), and the single-layer perceptron (Bishop 1995). We employ these classifier models here primarily because of their straightforward probabilistic interpretation and their similarity to the scope model of Kuno, Takami, and Wu (since they each could be said to implement a kind of weighted voting of factors). In Section 4.3, we describe how classifiers of these types can be constructed to serve as a grammatical module responsible for quantifier scope determination. All of these classifiers can be trained in a supervised manner. That is, given a sample of training data that provides all of the info</context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>Ratnaparkhi, Adwait. 1997. A simple introduction to maximum entropy models for natural language processing. Technical Report 97-08, Institute for Research in Cognitive Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beatrice Santorini</author>
</authors>
<title>Part-of-speech tagging guidelines for the Penn Treebank project.</title>
<date>1990</date>
<tech>Technical Report MS-CIS-90-47,</tech>
<institution>Department of Computer and Information Science, University of Pennsylvania.</institution>
<contexts>
<context position="18868" citStr="Santorini (1990)" startWordPosition="2997" endWordPosition="2998"> and 12 sentences with four. In addition, our review of these sentences revealed that many of them simply involve lists in which the quantifiers do not interact in terms of scope (as in, for example, “We ask that you turn off all cell phones, extinguish all cigarettes, and open any candy before the performance begins”). Thus, the class of sentences with more than two quantifiers is small and seems to involve even simpler quantifier interactions than those found in our corpus. 4 These categories are intended to be understood as they are used in the tagging and parsing of the Penn Treebank. See Santorini (1990) and Bies et al. (1995) for details; the Appendix lists selected codes used 78 Higgins and Sadock Modeling Scope Preferences ( (S (NP-SBJ (NP (DT Those) ) (SBAR (WHNP-1 (WP who) ) (S (NP-SBJ-2 (-NONE- *T*-1) ) (ADVP (RB still) ) (VP (VBP want) (S (NP-SBJ (-NONE- *-2) ) (VP (TO to) (VP (VB do) (NP (PRP it) )))))))) (44 44) (VP (MD will) (ADVP (RB just) ) (VP (VB find) (NP (NP (DT-Q2 some) (NN way) ) (SBAR (WHADVP-3 (-NONE- 0) ) (S (NP-SBJ (-NONE- *) ) (VP (TO to) (VP (VB get) (PP (IN around) (’’ ’’) (NP (DT-Q1 any) (NN attempt) (S (NP-SBJ (-NONE- *) ) (VP (TO to) (VP (VB curb) (NP (PRP it) ))))</context>
</contexts>
<marker>Santorini, 1990</marker>
<rawString>Santorini, Beatrice. 1990. Part-of-speech tagging guidelines for the Penn Treebank project. Technical Report MS-CIS-90-47, Department of Computer and Information Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wee Meng Soon</author>
<author>Hwee Tou Ng</author>
<author>Daniel Chung Yong Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>Soon, Wee Meng, Hwee Tou Ng, and Daniel Chung Yong Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans van Halteren</author>
<author>Jakub Zavrel</author>
<author>Walter Daelemans</author>
</authors>
<title>Improving accuracy in word class tagging through the combination of machine learning systems.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<marker>van Halteren, Zavrel, Daelemans, 2001</marker>
<rawString>van Halteren, Hans, Jakub Zavrel, and Walter Daelemans. 2001. Improving accuracy in word class tagging through the combination of machine learning systems. Computational Linguistics, 27(2):199–229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt A VanLehn</author>
</authors>
<title>Determining the scope of English quantifiers.</title>
<date>1978</date>
<tech>Technical Report AITR-483,</tech>
<institution>Massachusetts Institute of Technology Artificial Intelligence Laboratory,</institution>
<location>Cambridge.</location>
<contexts>
<context position="12745" citStr="VanLehn (1978)" startWordPosition="2009" endWordPosition="2010">using a separate system of heuristics (or scope critics) that apply postsyntactically to determine the most likely scoping. LUNAR (Woods 1986), TEAM (Martin, Appelt, and Pereira 1986), and the SRI Core Language Engine as described by Moran (1988; Moran and Pereira 1992) all employ scope rules of this sort. By and large, these rules are of an ad hoc nature, implementing a linguist’s intuitive idea of what factors determine scope possibilities, and no results have been published regarding the accuracy of these methods. For example, Moran (1988) incorporates rules from other NLP systems and from VanLehn (1978), such as a preference for a logically weaker interpretation, the tendency for each to take wide scope, and a ban on raising a quantifier across multiple major clause boundaries. The testing of Moran’s system is “limited to checking conformance to the stated rules” (pages 40–41). In addition, these systems are generally incapable of handling unrestricted text such as that found in the Wall Street Journal corpus in a robust way, because they need to do a full semantic analysis of a sentence in order to make scope predictions. The statistical basis of the model presented in this article offers i</context>
</contexts>
<marker>VanLehn, 1978</marker>
<rawString>VanLehn, Kurt A. 1978. Determining the scope of English quantifiers. Technical Report AITR-483, Massachusetts Institute of Technology Artificial Intelligence Laboratory, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Woods</author>
</authors>
<title>Semantics and quantification in natural language question answering.</title>
<date>1986</date>
<booktitle>Natural Language Processing.</booktitle>
<pages>205--248</pages>
<editor>In B. J. Grosz, K. Sparck Jones, and B. L. Webber, editors,</editor>
<publisher>Kaufmann,</publisher>
<location>Los Altos, California,</location>
<contexts>
<context position="3463" citStr="Woods 1986" startWordPosition="520" endWordPosition="521">il: j-sadock@uchicago.edu. © 2003 Association for Computational Linguistics Computational Linguistics Volume 29, Number 1 sentence that are possible, without regard to their relative likelihood or naturalness. Recently, however, linguists such as Kuno, Takami, and Wu (1999) have begun to turn their attention to scope prediction, or determining the relative accessibility of different scope readings. In computational linguistics, more attention has been paid to the factors that determine scope preferences. Systems such as the SRI Core Language Engine (Moran 1988; Moran and Pereira 1992), LUNAR (Woods 1986), and TEAM (Martin, Appelt, and Pereira 1986) have employed scope critics that use heuristics to decide between alternative scopings. However, the rules that these systems use in making quantifier scope decisions are motivated only by the researchers’ intuitions, and no empirical results have been published regarding their accuracy. In this article, we use the tools of machine learning to construct a data-driven model of quantifier scope preferences. For theoretical linguistics, this model serves as an illustration that Kuno, Takami, and Wu’s approach can capture some of the clearest generaliz</context>
<context position="12273" citStr="Woods 1986" startWordPosition="1933" endWordPosition="1934">having each out-scope some. Because of the recursive nature of the algorithm, there are limits to the amount of ordering that can be done in this manner” (page 55). The stepwise nature of these scope mechanisms makes it hard to state the factors that influence the preference for one quantifier to take scope over another. Those natural language processing (NLP) systems that have managed to provide some sort of account of quantifier scope preferences have done so by using a separate system of heuristics (or scope critics) that apply postsyntactically to determine the most likely scoping. LUNAR (Woods 1986), TEAM (Martin, Appelt, and Pereira 1986), and the SRI Core Language Engine as described by Moran (1988; Moran and Pereira 1992) all employ scope rules of this sort. By and large, these rules are of an ad hoc nature, implementing a linguist’s intuitive idea of what factors determine scope possibilities, and no results have been published regarding the accuracy of these methods. For example, Moran (1988) incorporates rules from other NLP systems and from VanLehn (1978), such as a preference for a logically weaker interpretation, the tendency for each to take wide scope, and a ban on raising a q</context>
</contexts>
<marker>Woods, 1986</marker>
<rawString>Woods, William A. 1986. Semantics and quantification in natural language question answering. In B. J. Grosz, K. Sparck Jones, and B. L. Webber, editors, Natural Language Processing. Kaufmann, Los Altos, California, pages 205–248.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>