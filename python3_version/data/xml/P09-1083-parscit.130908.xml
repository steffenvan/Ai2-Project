<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000031">
<title confidence="0.993721">
Answering Opinion Questions with Random Walks on Graphs
</title>
<author confidence="0.98582">
Fangtao Li, Yang Tang, Minlie Huang, and Xiaoyan Zhu
</author>
<affiliation confidence="0.966124333333333">
State Key Laboratory on Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Sci. and Tech., Tsinghua University, Beijing 100084, China
</affiliation>
<email confidence="0.998074">
{fangtao06,tangyang9}@gmail.com,{aihuang,zxy-dcs}@tsinghua.edu.cn
</email>
<sectionHeader confidence="0.997276" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995153818181818">
Opinion Question Answering (Opinion
QA), which aims to find the authors’ sen-
timental opinions on a specific target, is
more challenging than traditional fact-
based question answering problems. To
extract the opinion oriented answers, we
need to consider both topic relevance and
opinion sentiment issues. Current solu-
tions to this problem are mostly ad-hoc
combinations of question topic informa-
tion and opinion information. In this pa-
per, we propose an Opinion PageRank
model and an Opinion HITS model to fully
explore the information from different re-
lations among questions and answers, an-
swers and answers, and topics and opin-
ions. By fully exploiting these relations,
the experiment results show that our pro-
posed algorithms outperform several state
of the art baselines on benchmark data set.
A gain of over 10% in F scores is achieved
as compared to many other systems.
</bodyText>
<sectionHeader confidence="0.999483" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998647482142857">
Question Answering (QA), which aims to pro-
vide answers to human-generated questions auto-
matically, is an important research area in natu-
ral language processing (NLP) and much progress
has been made on this topic in previous years.
However, the objective of most state-of-the-art QA
systems is to find answers to factual questions,
such as “What is the longest river in the United
States?” and “Who is Andrew Carnegie?” In fact,
rather than factual information, people would also
like to know about others’ opinions, thoughts and
feelings toward some specific objects, people and
events. Some examples of these questions are:
“How is Bush’s decision not to ratify the Kyoto
Protocol looked upon by Japan and other US al-
lies?”(Stoyanov et al., 2005) and “Why do peo-
ple like Subway Sandwiches?” from TAC 2008
(Dang, 2008). Systems designed to deal with such
questions are called opinion QA systems. Re-
searchers (Stoyanov et al., 2005) have found that
opinion questions have very different character-
istics when compared with fact-based questions:
opinion questions are often much longer, more
likely to represent partial answers rather than com-
plete answers and vary much more widely. These
features make opinion QA a harder problem to
tackle than fact-based QA. Also as shown in (Stoy-
anov et al., 2005), directly applying previous sys-
tems designed for fact-based QA onto opinion QA
tasks would not achieve good performances.
Similar to other complex QA tasks (Chen et al.,
2006; Cui et al., 2007), the problem of opinion QA
can be viewed as a sentence ranking problem. The
Opinion QA task needs to consider not only the
topic relevance of a sentence (to identify whether
this sentence matches the topic of the question)
but also the sentiment of a sentence (to identify
the opinion polarity of a sentence). Current solu-
tions to opinion QA tasks are generally in ad hoc
styles: the topic score and the opinion score are
usually separately calculated and then combined
via a linear combination (Varma et al., 2008) or
just filter out the candidate without matching the
question sentiment (Stoyanov et al., 2005). How-
ever, topic and opinion are not independent in re-
ality. The opinion words are closely associated
with their contexts. Another problem is that exist-
ing algorithms compute the score for each answer
candidate individually, in other words, they do not
consider the relations between answer candidates.
The quality of a answer candidate is not only de-
termined by the relevance to the question, but also
by other candidates. For example, the good an-
swer may be mentioned by many candidates.
In this paper, we propose two models to ad-
dress the above limitations of previous sentence
</bodyText>
<page confidence="0.952691">
737
</page>
<note confidence="0.9996115">
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 737–745,
Suntec, Singapore, 2-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999931833333333">
ranking models. We incorporate both the topic
relevance information and the opinion sentiment
information into our sentence ranking procedure.
Meanwhile, our sentence ranking models could
naturally consider the relationships between dif-
ferent answer candidates. More specifically, our
first model, called Opinion PageRank, incorpo-
rates opinion sentiment information into the graph
model as a condition. The second model, called
Opinion HITS model, considers the sentences as
authorities and both question topic information
and opinion sentiment information as hubs. The
experiment results on the TAC QA data set demon-
strate the effectiveness of the proposed Random
Walk based methods. Our proposed method per-
forms better than the best method in the TAC 2008
competition.
The rest of this paper is organized as follows:
Section 2 introduces some related works. We will
discuss our proposed models in Section 3. In Sec-
tion 4, we present an overview of our opinion QA
system. The experiment results are shown in Sec-
tion 5. Finally, Section 6 concludes this paper and
provides possible directions for future work.
</bodyText>
<sectionHeader confidence="0.999879" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999908307692308">
Few previous studies have been done on opin-
ion QA. To our best knowledge, (Stoyanov et
al., 2005) first created an opinion QA corpus
OpQA. They find that opinion QA is a more chal-
lenging task than factual question answering, and
they point out that traditional fact-based QA ap-
proaches may have difficulty on opinion QA tasks
if unchanged. (Somasundaran et al., 2007) argues
that making finer grained distinction of subjective
types (sentiment and arguing) further improves the
QA system. For non-English opinion QA, (Ku et
al., 2007) creates a Chinese opinion QA corpus.
They classify opinion questions into six types and
construct three components to retrieve opinion an-
swers. Relevant answers are further processed by
focus detection, opinion scope identification and
polarity detection. Some works on opinion min-
ing are motivated by opinion question answering.
(Yu and Hatzivassiloglou, 2003) discusses a nec-
essary component for an opinion question answer-
ing system: separating opinions from fact at both
the document and sentence level. (Soo-Min and
Hovy, 2005) addresses another important compo-
nent of opinion question answering: finding opin-
ion holders.
More recently, TAC 2008 QA track (evolved
from TREC) focuses on finding answers to opin-
ion questions (Dang, 2008). Opinion questions
retrieve sentences or passages as answers which
are relevant for both question topic and question
sentiment. Most TAC participants employ a strat-
egy of calculating two types of scores for answer
candidates, which are the topic score measure and
the opinion score measure (the opinion informa-
tion expressed in the answer candidate). How-
ever, most approaches simply combined these two
scores by a weighted sum, or removed candidates
that didn’t match the polarity of questions, in order
to extract the opinion answers.
Algorithms based on Markov Random Walk
have been proposed to solve different kinds of
ranking problems, most of which are inspired by
the PageRank algorithm (Page et al., 1998) and the
HITS algorithm (Kleinberg, 1999). These two al-
gorithms were initially applied to the task of Web
search and some of their variants have been proved
successful in a number of applications, including
fact-based QA and text summarization (Erkan and
Radev, 2004; Mihalcea and Tarau, 2004; Otter-
bacher et al., 2005; Wan and Yang, 2008). Gener-
ally, such models would first construct a directed
or undirected graph to represent the relationship
between sentences and then certain graph-based
ranking methods are applied on the graph to com-
pute the ranking score for each sentence. Sen-
tences with high scores are then added into the
answer set or the summary. However, to the best
of our knowledge, all previous Markov Random
Walk-based sentence ranking models only make
use of topic relevance information, i.e. whether
this sentence is relevant to the fact we are looking
for, thus they are limited to fact-based QA tasks.
To solve the opinion QA problems, we need to
consider both topic and sentiment in a non-trivial
manner.
</bodyText>
<sectionHeader confidence="0.9742815" genericHeader="method">
3 Our Models for Opinion Sentence
Ranking
</sectionHeader>
<bodyText confidence="0.999890375">
In this section, we formulate the opinion question
answering problem as a topic and sentiment based
sentence ranking task. In order to naturally inte-
grate the topic and opinion information into the
graph based sentence ranking framework, we pro-
pose two random walk based models for solving
the problem, i.e. an Opinion PageRank model and
an Opinion HITS model.
</bodyText>
<page confidence="0.995169">
738
</page>
<subsectionHeader confidence="0.980899">
3.1 Opinion PageRank Model
</subsectionHeader>
<bodyText confidence="0.998888857142857">
In order to rank sentence for opinion question an-
swering, two aspects should be taken into account.
First, the answer candidate is relevant to the ques-
tion topic; second, the answer candidate is suitable
for question sentiment.
Considering Question Topic: We first intro-
duce how to incorporate the question topic into
the Markov Random Walk model, which is simi-
lar as the Topic-sensitive LexRank (Otterbacher et
al., 2005). Given the set Vs = {vi} containing all
the sentences to be ranked, we construct a graph
where each node represents a sentence and each
edge weight between sentence vi and sentence vj
is induced from sentence similarity measure as fol-
</bodyText>
<equation confidence="0.701903">
lows: p(i → j) = f (1i→j) where f (i → j)
Ek=1 f (i→k)
</equation>
<bodyText confidence="0.999805222222222">
represents the similarity between sentence vi and
sentence vj, here is cosine similarity (Baeza-Yates
and Ribeiro-Neto, 1999). We define f(i → i) = 0
to avoid self transition. Note that p(i → j) is usu-
ally not equal to p(j → i). We also compute the
similarity rel(vi|q) of a sentence vi to the question
topic q using the cosine measure. This relevance
score is then normalized as follows to make the
sum of all relevance values of the sentences equal
</bodyText>
<equation confidence="0.993995">
to 1: rel′(vi|q) = v3�l (vi |q)
�k=1 rel(vk|q).
</equation>
<bodyText confidence="0.99993575">
The saliency score Score(vi) for sentence vi
can be calculated by mixing topic relevance score
and scores of all other sentences linked with it as
follows: Score(vi) = µ Ej6�i Score(vj) · p(j →
i)+(1−µ)rel′(vi|q), where µ is the damping fac-
tor as in the PageRank algorithm.
The matrix form is: p� = µ �MTp�+ (1 −
µ)a, where p� = [Score(vi)]|Vs|×1 is the vec-
tor of saliency scores for the sentences; M =
[p(i → j)]|Vs|×|Vs |is the graph with each entry
corresponding to the transition probability; α� =
[rel′(vi|q)]|Vs|×1 is the vector containing the rel-
evance scores of all the sentences to the ques-
tion. The above process can be considered as a
Markov chain by taking the sentences as the states
and the corresponding transition matrix is given by
</bodyText>
<equation confidence="0.610909">
A′ = µMT + (1 − µ)eaT.
</equation>
<bodyText confidence="0.995281571428572">
Considering Topics and Sentiments To-
gether: In order to incorporate the opinion infor-
mation and topic information for opinion sentence
ranking in an unified framework, we propose an
Opinion PageRank model (Figure 1) based on a
two-layer link graph (Liu and Ma, 2005; Wan and
Yang, 2008). In our opinion PageRank model, the
</bodyText>
<figureCaption confidence="0.999605">
Figure 1: Opinion PageRank
</figureCaption>
<bodyText confidence="0.976150958333334">
first layer contains all the sentiment words from a
lexicon to represent the opinion information, and
the second layer denotes the sentence relationship
in the topic sensitive Markov Random Walk model
discussed above. The dashed lines between these
two layers indicate the conditional influence be-
tween the opinion information and the sentences
to be ranked.
Formally, the new representation for the two-
layer graph is denoted as G∗ = (Vs, Vo, Ess, Eso),
where Vs = {vi} is the set of sentences and Vo =
{oj} is the set of sentiment words representing the
opinion information; Ess = {eij|vi, vj E Vs}
corresponds to all links between sentences and
Eso = {eij|vi E Vs, oj E Vo} corresponds to
the opinion correlation between a sentence and
the sentiment words. For further discussions, we
let π(oj) E [0, 1] denote the sentiment strength
of word oj, and let w(vi, oj) E [0, 1] denote the
strength of the correlation between sentence vi and
word oj. We incorporate the two factors into the
transition probability from vi to vj and the new
transition probability p(i → j|Op(vi), Op(vj)) is
defined as ��(�i→j|OP(vi),OP(vj)) when E f �
</bodyText>
<equation confidence="0.603196">
Ek=1 f(i→k|OP(vi),OP(vk))
</equation>
<bodyText confidence="0.9762277">
0, and defined as 0 otherwise, where Op(vi) is de-
noted as the opinion information of sentence vi,
and f(i → j|Op(vi), Op(vj)) is the new similar-
ity score between two sentences vi and vj, condi-
tioned on the opinion information expressed by the
sentiment words they contain. We propose to com-
pute the conditional similarity score by linearly
combining the scores conditioned on the source
opinion (i.e. f(i → j|Op(vi))) and the destina-
tion opinion (i.e. f(i → j|Op(vj))) as follows:
</bodyText>
<equation confidence="0.9950805">
f(i → j|OP(vi), OP(vj))
= A · f(i → j|OP(vi)) + (1 − A) · f(i → j|OP(vj))
= A · � f(i → j) · ir(ok) · w(ok, vi)
okEOP(vi)
+ (1 − A) · � (i → j) · ir(ok′) · w(ok′, vj) (1)
ok′ EOP(vj ))
</equation>
<bodyText confidence="0.999688">
where λ E [0, 1] is the combination weight con-
trolling the relative contributions from the source
</bodyText>
<page confidence="0.991208">
739
</page>
<bodyText confidence="0.9945369375">
opinion and the destination opinion. In this study,
for simplicity, we define π(oj) as 1, if oj ex-
ists in the sentiment lexicon, otherwise 0. And
ω(vi, oj) is described as an indicative function. In
other words, if word oj appears in the sentence vi,
ω(vi, oj) is equal to 1. Otherwise, its value is 0.
Then the new row-normalized matrix ˜M∗ is de-
fined as follows: ˜M∗ij = p(i → j|Op(i), Opj).
The final sentence score for Opinion PageR-
ank model is then denoted by: Score(vi) = µ ·
Ej6�i Score(vj) · ˜M∗ji + (1 − µ) · rel′(si|q).
The matrix form is: p˜ = µ ˜M∗Tp˜+ (1 − µ) · ~α.
The final transition matrix is then denoted as:
A∗ = µ˜M∗T +(1−µ)~e~αT and the sentence scores
are obtained by the principle eigenvector of the
new transition matrix A∗.
</bodyText>
<subsectionHeader confidence="0.980848">
3.2 Opinion HITS Model
</subsectionHeader>
<bodyText confidence="0.999188848484849">
The word’s sentiment score is fixed in Opinion
PageRank. This may encounter problem when
the sentiment score definition is not suitable for
the specific question. We propose another opin-
ion sentence ranking model based on the popular
graph ranking algorithm HITS (Kleinberg, 1999).
This model can dynamically learn the word senti-
ment score towards a specific question. HITS al-
gorithm distinguishes the hubs and authorities in
the objects. A hub object has links to many au-
thorities, and an authority object has high-quality
content and there are many hubs linking to it. The
hub scores and authority scores are computed in a
recursive way. Our proposed opinion HITS algo-
rithm contains three layers. The upper level con-
tains all the sentiment words from a lexicon, which
represent their opinion information. The lower
level contains all the words, which represent their
topic information. The middle level contains all
the opinion sentences to be ranked. We consider
both the opinion layer and topic layer as hubs and
the sentences as authorities. Figure 2 gives the bi-
partite graph representation, where the upper opin-
ion layer is merged with lower topic layer together
as the hubs, and the middle sentence layer is con-
sidered as the authority.
Formally, the representation for the bipartite
graph is denoted as G# = (Vs, Vo, Vt, Eso, Esti,
where Vs = {vi} is the set of sentences. Vo =
{oj} is the set of all the sentiment words repre-
senting opinion information, Vt = {tj} is the set
of all the words representing topic information.
Eso = {eij|vi E Vs, oj E Vo} corresponds to the
</bodyText>
<figureCaption confidence="0.997262">
Figure 2: Opinion HITS model
</figureCaption>
<bodyText confidence="0.996783275862068">
correlations between sentence and opinion words.
Each edge eij is associated with a weight owij de-
noting the strength of the relationship between the
sentence vi and the opinion word oj. The weight
owij is 1 if the sentence vi contains word oj, other-
wise 0. Est denotes the relationship between sen-
tence and topic word. Its weight twij is calculated
by tf · idf (Otterbacher et al., 2005).
We define two matrixes O = (Oij)|Vs|×|Vo |and
T = (Tij)|Vs|×|Vt |as follows, for Oij = owij,
and if sentence i contains word j, therefore owij
is assigned 1, otherwise owij is 0. Tij = twij =
tfj · idfj (Otterbacher et al., 2005).
Our new opinion HITS model is different from
the basic HITS algorithm in two aspects. First,
we consider the topic relevance when computing
the sentence authority score based on the topic hub
level as follows: AuthgeR(vi) a Etwij&gt;0 twij ·
topic score(j)·hubtopic(j), where topic score(j)
is empirically defined as 1, if the word j is in the
topic set (we will discuss in next section), and 0.1
otherwise.
Second, in our opinion HITS model, there are
two aspects to boost the sentence authority score:
we simultaneously consider both topic informa-
tion and opinion information as hubs.
The final scores for authority sentence, hub
topic and hub opinion in our opinion HITS model
are defined as:
</bodyText>
<equation confidence="0.9923211875">
Auth(n+1)
sen (vi) = (2)
γ · E twij · topic score(j) · Hub(n)
topic(tj)
twij &gt;0
+ (1 − γ) · E owij · Hub(n)
opinion(oj)
owij&gt;0
Hub(n+1)
topic (ti) = E twki · Auth(n)
sen(vi) (3)
twki&gt;0
Hubopinion
(n+1) (oi) = E owki · Auth(n)
sen(vi) (4)
owki&gt;0
</equation>
<page confidence="0.976919">
740
</page>
<figureCaption confidence="0.901709">
Figure 3: Opinion Question Answering System
The matrix form is:
</figureCaption>
<equation confidence="0.967881">
a(n+1) = ,y · T · e · tTs · I · h(n)
t + (1 − �) · O · h(n)
o (5)
h(n+1) = T T · a(n) (6)
t
h(n+1) = OT ·a(n) (7)
o
</equation>
<bodyText confidence="0.885653">
where e is a |Vt|×1 vector with all elements equal
to 1 and I is a |Vt |× |Vt |identity matrix, ts =
[topic score(j)]|Vt|×1 is the score vector for topic
words, a(n) = [Auth(n)
sen(vi)]|Vs|×1 is the vector
authority scores for the sentence in the nth itera-
tion, and the same as ht(n) = [Hub(n)
</bodyText>
<equation confidence="0.978375666666667">
topic(tj)]|Vt|×1,
h(n)
o = [Hub(n)
</equation>
<bodyText confidence="0.9798290625">
opinion(tj)]|Vo|×1. In order to guaran-
tee the convergence of the iterative form, authority
score and hub score are normalized after each iter-
ation.
For computation of the final scores, the ini-
tial scores of all nodes, including sentences, topic
words and opinion words, are set to 1 and the
above iterative steps are used to compute the new
scores until convergence. Usually the convergence
of the iteration algorithm is achieved when the dif-
ference between the scores computed at two suc-
cessive iterations for any nodes falls below a given
threshold (10e-6 in this study). We use the au-
thority scores as the saliency scores in the Opin-
ion HITS model. The sentences are then ranked
by their saliency scores.
</bodyText>
<sectionHeader confidence="0.997861" genericHeader="method">
4 System Description
</sectionHeader>
<bodyText confidence="0.98743702">
In this section, we introduce the opinion question
answering system based on the proposed graph
methods. Figure 3 shows five main modules:
Question Analysis: It mainly includes two
components. 1).Sentiment Classification: We
classify all opinion questions into two categories:
positive type or negative type. We extract several
types of features, including a set of pattern fea-
tures, and then design a classifier to identify sen-
timent polarity for each question (similar as (Yu
and Hatzivassiloglou, 2003)). 2).Topic Set Expan-
sion: The opinion question asks opinions about
a particular target. Semantic role labeling based
(Carreras and Marquez, 2005) and rule based tech-
niques can be employed to extract this target as
topic word. We also expand the topic word with
several external knowledge bases: Since all the en-
tity synonyms are redirected into the same page in
Wikipedia (Rodrigo et al., 2007), we collect these
redirection synonym words to expand topic set.
We also collect some related lists as topic words.
For example, given question “What reasons did
people give for liking Ed Norton’s movies?”, we
collect all the Norton’s movies from IMDB as this
question’s topic words.
Document Retrieval: The PRISE search en-
gine, supported by NIST (Dang, 2008), is em-
ployed to retrieve the documents with topic word.
Answer Candidate Extraction: We split re-
trieved documents into sentences, and extract sen-
tences containing topic words. In order to im-
prove recall, we carry out the following process to
handle the problem of coreference resolution: We
classify the topic word into four categories: male,
female, group and other. Several pronouns are de-
fined for each category, such as ”he”, ”him”, ”his”
for male category. If a sentence is determined to
contain the topic word, and its next sentence con-
tains the corresponding pronouns, then the next
sentence is also extracted as an answer candidate,
similar as (Chen et al., 2006).
Answer Ranking: The answer candidates
are ranked by our proposed Opinion PageRank
method or Opinion HITS method.
Answer Selection by Removing Redundancy:
We incrementally add the top ranked sentence into
the answer set, if its cosine similarity with ev-
ery extracted answer doesn’t exceed a predefined
threshold, until the number of selected sentence
(here is 40) is reached.
</bodyText>
<sectionHeader confidence="0.999926" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.8188615">
5.1 Experiment Step
5.1.1 Dataset
</subsectionHeader>
<bodyText confidence="0.9974065">
We employ the dataset from the TAC 2008 QA
track. The task contains a total of 87 squishy
</bodyText>
<page confidence="0.995016">
741
</page>
<bodyText confidence="0.999957882352941">
opinion questions.1 These questions have simple
forms, and can be easily divided into positive type
or negative type, for example “Why do people like
Mythbusters?” and “What were the specific ac-
tions or reasons given for a negative attitude to-
wards Mahmoud Ahmadinejad?”. The initial topic
word for each question (called target in TAC) is
also provided. Since our work in this paper fo-
cuses on sentence ranking for opinion QA, these
characteristics of TAC data make it easy to pro-
cess question analysis. Answers for all questions
must be retrieved from the TREC Blog06 collec-
tion (Craig Macdonald and Iadh Ounis, 2006).
The collection is a large sample of the blog sphere,
crawled over an eleven-week period from Decem-
ber 6, 2005 until February 21, 2006. We retrieve
the top 50 documents for each question.
</bodyText>
<subsubsectionHeader confidence="0.961014">
5.1.2 Evaluation Metrics
</subsubsectionHeader>
<bodyText confidence="0.999830857142857">
We adopt the evaluation metrics used in the TAC
squishy opinion QA task (Dang, 2008). The TAC
assessors create a list of acceptable information
nuggets for each question. Each nugget will be
assigned a normalized weight based on the num-
ber of assessors who judged it to be vital. We use
these nuggets and corresponding weights to assess
our approach. Three human assessors complete
the evaluation process. Every question is scored
using nugget recall (NR) and an approximation to
nugget precision (NP) based on length. The final
score will be calculated using F measure with TAC
official value Q = 3 (Dang, 2008). This means re-
call is 3 times as important as precision:
</bodyText>
<equation confidence="0.9780305">
F(β = 3) = (332 · 1) · +NP ·
NR
</equation>
<bodyText confidence="0.999890888888889">
where NP is the sum of weights of nuggets re-
turned in response over the total sum of weights
of all nuggets in nugget list, and NP = 1 −
(length − allowance)/(length) if length is no
less than allowance and 0 otherwise. Here
allowance = 100 × (♯nuggets returned) and
length equals to the number of non-white char-
acters in strings. We will use average F Score to
evaluate the performance for each system.
</bodyText>
<sectionHeader confidence="0.647113" genericHeader="method">
5.1.3 Baseline
</sectionHeader>
<bodyText confidence="0.98424875">
The baseline combines the topic score and opinion
score with a linear weight for each answer candi-
date, similar to the previous ad-hoc algorithms:
final score = (1 − α) x opinion score + α x topic score
</bodyText>
<page confidence="0.705787">
(8)
</page>
<tableCaption confidence="0.384887">
13 questions were dropped from the evaluation due to no
correct answers found in the corpus
</tableCaption>
<bodyText confidence="0.9970512">
The topic score is computed by the cosine sim-
ilarity between question topic words and answer
candidate. The opinion score is calculated using
the number of opinion words normalized by the
total number of words in candidate sentence.
</bodyText>
<subsectionHeader confidence="0.785028">
5.2 Performance Evaluation
</subsectionHeader>
<table confidence="0.988479411764706">
5.2.1 Performance on Sentimental Lexicons
Lexicon Neg Pos Description
Name Size Size
1 HowNet 2700 2009 English translation
of positive/negative
Chinese words
2 Senti- 4800 2290 Words with a positive
WordNet or negative score
above 0.6
3 Intersec- 640 518 Words appeared in
tion both 1 and 2
4 Union 6860 3781 Words appeared in
1 or 2
5 All 10228 10228 All words appeared
in 1 or 2 without
distinguishing pos
or neg
</table>
<tableCaption confidence="0.999566">
Table 1: Sentiment lexicon description
</tableCaption>
<bodyText confidence="0.993270185185185">
For lexicon-based opinion analysis, the selec-
tion of opinion thesaurus plays an important role
in the final performance. HowNet2 is a knowledge
database of the Chinese language, and provides an
online word list with tags of positive and negative
polarity. We use the English translation of those
sentiment words as the sentimental lexicon. Sen-
tiWordNet (Esuli and Sebastiani, 2006) is another
popular lexical resource for opinion mining. Ta-
ble 1 shows the detail information of our used sen-
timent lexicons. In our models, the positive opin-
ion words are used only for positive questions, and
negative opinion words just for negative questions.
We initially set parameter A in Opinion PageRank
as 0 as (Liu and Ma, 2005), and other parameters
simply as 0.5, including p in Opinion PageRank,
γ in Opinion HITS, and α in baseline. The exper-
iment results are shown in Figure 4.
We can make three conclusions from Figure 4:
1. Opinion PageRank and Opinion HITS are both
effective. The best results of Opinion PageRank
and Opinion HITS respectively achieve around
35.4% (0.199 vs 0.145), and 34.7% (0.195 vs
0.145) improvements in terms of F score over the
best baseline result. We believe this is because our
proposed models not only incorporate the topic in-
formation and opinion information, but also con-
</bodyText>
<footnote confidence="0.996445">
2http://www.keenage.com/zhiwang/e zhiwang.html
</footnote>
<page confidence="0.986042">
742
</page>
<figureCaption confidence="0.999741">
Figure 4: Sentiment Lexicon Performance
</figureCaption>
<bodyText confidence="0.999940205128205">
sider the relationship between different answers.
The experiment results demonstrate the effective-
ness of these relations. 2. Opinion PageRank and
Opinion HITS are comparable. Among five sen-
timental lexicons, Opinion PageRank achieves the
best results when using HowNet and Union lexi-
cons, and Opinion HITS achieves the best results
using the other three lexicons. This may be be-
cause when the sentiment lexicon is defined appro-
priately for the specific question set, the opinion
PageRank model performs better. While when the
sentiment lexicon is not suitable for these ques-
tions, the opinion HITS model may dynamically
learn a temporal sentiment lexicon and can yield
a satisfied performance. 3. Hownet achieves the
best overall performance among five sentiment
lexicons. In HowNet, English translations of the
Chinese sentiment words are annotated by non-
native speakers; hence most of them are common
and popular terms, which maybe more suitable for
the Blog environment (Zhang and Ye, 2008). We
will use HowNet as the sentiment thesaurus in the
following experiments.
In baseline, the parameter α shows the relative
contributions for topic score and opinion score.
We vary α from 0 to 1 with an interval of 0.1, and
find that the best baseline result 0.170 is achieved
when α=0.1. This is because the topic informa-
tion has been considered during candidate extrac-
tion, the system considering more opinion infor-
mation (lower α) achieves better. We will use this
best result as baseline score in following experi-
ments. Since F(3) score is more related with re-
call, F score and recall will be demonstrated. In
the next two sections, we will present the perfor-
mances of the parameters in each model. For sim-
plicity, we denote Opinion PageRank as PR, Opin-
ion HITS as HITS, baseline as Base, Recall as r, F
score as F.
</bodyText>
<figureCaption confidence="0.96652775">
Figure 5: Opinion PageRank Performance with
varying parameter A (p = 0.5)
Figure 6: Opinion PageRank Performance with
varying parameter p (A = 0.2)
</figureCaption>
<subsectionHeader confidence="0.52046">
5.2.2 Opinion PageRank Performance
</subsectionHeader>
<bodyText confidence="0.999892277777778">
In Opinion PageRank model, the value A com-
bines the source opinion and the destination opin-
ion. Figure 5 shows the experiment results on pa-
rameter A. When we consider lower A, the system
performs better. This demonstrates that the desti-
nation opinion score contributes more than source
opinion score in this task.
The value of p is a trade-off between answer
reinforcement relation and topic relation to calcu-
late the scores of each node. For lower value of p,
we give more importance to the relevance to the
question than the similarity with other sentences.
The experiment results are shown in Figure 6. The
best result is achieved when p = 0.8. This fig-
ure also shows the importance of reinforcement
between answer candidates. If we don’t consider
the sentence similarity(p = 0), the performance
drops significantly.
</bodyText>
<subsectionHeader confidence="0.59448">
5.2.3 Opinion HITS Performance
</subsectionHeader>
<bodyText confidence="0.999957333333333">
The parameter -y combines the opinion hub score
and topic hub score in the Opinion HITS model.
The higher -y is, the more contribution is given
</bodyText>
<figure confidence="0.99990229032258">
0.25
0.2
0.15
0.1
0.05
0
Baseline Opinion PageRank Opinion HITS
HowNet SentiWordNet Intersection Union All
0.26
0.24
0.22
0.18
0.16
0.14
0.12
F(3)
0.2
0 0.2 0.4 0.6 0.8 1
PR_r PR_F
Base_r Base_F
PR r PR F Base r Base_F
F(3)
0.26
0.24
0.22
0.2
0.18
0.16
0.14
0.12
0 0.2 0.4 0.6 0.8 1
</figure>
<page confidence="0.85693">
743
</page>
<figureCaption confidence="0.997534">
Figure 7: Opinion HITS Performance with vary-
ing parameter -y
</figureCaption>
<bodyText confidence="0.96499253125">
to topic hub level, while the less contribution is
given to opinion hub level. The experiment results
are shown in Figure 7. Similar to baseline param-
eter α, since the answer candidates are extracted
based on topic information, the systems consider-
ing opinion information heavily (α=0.1 in base-
line, -y=0.2) perform best.
Opinion HITS model ranks the sentences by au-
thority scores. It can also rank the popular opin-
ion words and popular topic words from the topic
hub layer and opinion hub layer, towards a specific
question. Take the question 1024.3 “What reasons
do people give for liking Zillow?” as an example,
its topic word is “Zillow”, and its sentiment polar-
ity is positive. Based on the final hub scores, the
top 10 topic words and opinion words are shown
as Table 2.
Opinion real, like, accurate, rich, right, interesting,
Words better, easily, free, good
Topic zillow, estate, home, house, data, value,
Words site, information, market, worth
Table 2: Question-specific popular topic words
and opinion words generated by Opinion HITS
Zillow is a real estate site for users to see the
value of houses or homes. People like it because it
is easily used, accurate and sometimes free. From
the Table 2, we can see that the top topic words
are the most related with question topic, and the
top opinion words are question-specific sentiment
words, such as “accurate”, “easily”, “free”, not
just general opinion words, like “great”, “excel-
lent” and “good”.
</bodyText>
<subsectionHeader confidence="0.611336">
5.2.4 Comparisons with TAC Systems
</subsectionHeader>
<bodyText confidence="0.996653333333333">
We are also interested in the performance compar-
ison with the systems in TAC QA 2008. From Ta-
ble 3, we can see Opinion PageRank and Opinion
</bodyText>
<table confidence="0.999799666666667">
System Precision Recall F(3)
OpPageRank 0.109 0.242 0.200
OpHITS 0.102 0.256 0.205
System 1 0.079 0.235 0.186
System 2 0.053 0.262 0.173
System 3 0.109 0.216 0.172
</table>
<tableCaption confidence="0.946809">
Table 3: Comparison results with TAC 2008 Three
Top Ranked Systems (system 1-3 demonstrate top
3 systems in TAC)
</tableCaption>
<bodyText confidence="0.990982">
HITS respectively achieve around 10% improve-
ment compared with the best result in TAC 2008,
which demonstrates that our algorithm is indeed
performing much better than the state-of-the-art
opinion QA methods.
</bodyText>
<sectionHeader confidence="0.998729" genericHeader="conclusions">
6 Conclusion and Future Works
</sectionHeader>
<bodyText confidence="0.999025121212121">
In this paper, we proposed two graph based sen-
tence ranking methods for opinion question an-
swering. Our models, called Opinion PageRank
and Opinion HITS, could naturally incorporate
topic relevance information and the opinion senti-
ment information. Furthermore, the relationships
between different answer candidates can be con-
sidered. We demonstrate the usefulness of these
relations through our experiments. The experi-
ment results also show that our proposed methods
outperform TAC 2008 QA Task top ranked sys-
tems by about 10% in terms of F score.
Our random walk based graph methods inte-
grate topic information and sentiment information
in a unified framework. They are not limited to
the sentence ranking for opinion question answer-
ing. They can be used in general opinion docu-
ment search. Moreover, these models can be more
generalized to the ranking task with two types of
influencing factors.
Acknowledgments: Special thanks to Derek
Hao Hu and Qiang Yang for their valuable
comments and great help on paper prepara-
tion. We also thank Hongning Wang, Min
Zhang, Xiaojun Wan and the anonymous re-
viewers for their useful comments, and thank
Hoa Trang Dang for providing the TAC eval-
uation results. The work was supported by
973 project in China(2007CB311003), NSFC
project(60803075), Microsoft joint project ”Opin-
ion Summarization toward Opinion Search”, and
a grant from the International Development Re-
search Center, Canada.
</bodyText>
<figure confidence="0.998636545454545">
F(3)
0.26
0.24
0.22
0.18
0.16
0.14
0.12
0.2
0 0.2 0.4 0.6 0.8 1
HITS_r HITS_F Base_r Base_F
</figure>
<page confidence="0.990724">
744
</page>
<sectionHeader confidence="0.997843" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999931796875">
Ricardo Baeza-Yates and Berthier Ribeiro-Neto. 1999.
Modern Information Retrieval. Addison Wesley,
May.
Xavier Carreras and Lluis Marquez. 2005. Introduc-
tion to the conll-2005 shared task: Semantic role la-
beling.
Yi Chen, Ming Zhou, and Shilong Wang. 2006.
Reranking answers for definitional qa using lan-
guage modeling. InACL-CoLing, pages 1081–1088.
Hang Cui, Min-Yen Kan, and Tat-Seng Chua. 2007.
Soft pattern matching models for definitional ques-
tion answering. ACM Trans. Inf. Syst., 25(2):8.
Hoa Trang Dang. 2008. Overview of the tac
2008 opinion question answering and summariza-
tion tasks (draft). In TAC.
G¨unes Erkan and Dragomir R. Radev. 2004. Lex-
pagerank: Prestige in multi-document text summa-
rization. In EMNLP.
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
wordnet: A publicly available lexical resource for
opinion mining. In LREC.
Jon M. Kleinberg. 1999. Authoritative sources in a
hyperlinked environment. J. ACM, 46(5):604–632.
Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen.
2007. Question analysis and answer passage re-
trieval for opinion question answering systems. In
ROCLING.
Tie-Yan Liu and Wei-Ying Ma. 2005. Webpage im-
portance analysis using conditional markov random
walk. In Web Intelligence, pages 515–521.
Rada Mihalcea and Paul Tarau. 2004. Textrank:
Bringing order into texts. In EMNLP.
Jahna Otterbacher, G¨unes Erkan, and Dragomir R.
Radev. 2005. Using random walks for question-
focused sentence retrieval. In HLT/EMNLP.
Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1998. The pagerank citation rank-
ing: Bringing order to the web. Technical report,
Stanford University.
Swapna Somasundaran, Theresa Wilson, Janyce
Wiebe, and Veselin Stoyanov. 2007. Qa with at-
titude: Exploiting opinion type analysis for improv-
ing question answering in online discussions and the
news. In ICWSM.
Kim Soo-Min and Eduard Hovy. 2005. Identifying
opinion holders for question answering in opinion
texts. In AAAI 2005 Workshop.
Veselin Stoyanov, Claire Cardie, and Janyce Wiebe.
2005. Multi-perspective question answering using
the opqa corpus. In HLT/EMNLP.
Vasudeva Varma, Prasad Pingali, Rahul Katragadda,
and et al. 2008. Iiit hyderabad at tac 2008. In Text
Analysis Conference.
X. Wan and J Yang. 2008. Multi-document summa-
rization using cluster-based link analysis. In SIGIR,
pages 299–306.
Hong Yu and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating facts
from opinions and identifying the polarity of opinion
sentences. In EMNLP.
Min Zhang and Xingyao Ye. 2008. A generation
model to unify topic relevance and lexicon-based
sentiment for opinion retrieval. In SIGIR, pages
411–418.
</reference>
<page confidence="0.99855">
745
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.240133">
<title confidence="0.999949">Answering Opinion Questions with Random Walks on Graphs</title>
<author confidence="0.800327">Fangtao Li</author>
<author confidence="0.800327">Yang Tang</author>
<author confidence="0.800327">Minlie Huang</author>
<author confidence="0.800327">Xiaoyan Zhu</author>
<affiliation confidence="0.4575905">State Key Laboratory on Intelligent Technology and Systems Tsinghua National Laboratory for Information Science and Technology</affiliation>
<address confidence="0.391496">Department of Computer Sci. and Tech., Tsinghua University, Beijing 100084, China</address>
<abstract confidence="0.993563478260869">Question Answering which aims to find the authors’ sentimental opinions on a specific target, is more challenging than traditional factbased question answering problems. To extract the opinion oriented answers, we need to consider both topic relevance and opinion sentiment issues. Current solutions to this problem are mostly ad-hoc combinations of question topic information and opinion information. In this pawe propose an PageRank and an HITS to fully explore the information from different relations among questions and answers, answers and answers, and topics and opinions. By fully exploiting these relations, the experiment results show that our proposed algorithms outperform several state of the art baselines on benchmark data set. A gain of over 10% in F scores is achieved as compared to many other systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ricardo Baeza-Yates</author>
<author>Berthier Ribeiro-Neto</author>
</authors>
<title>Modern Information Retrieval.</title>
<date>1999</date>
<publisher>Addison Wesley,</publisher>
<contexts>
<context position="9542" citStr="Baeza-Yates and Ribeiro-Neto, 1999" startWordPosition="1525" endWordPosition="1528">e for question sentiment. Considering Question Topic: We first introduce how to incorporate the question topic into the Markov Random Walk model, which is similar as the Topic-sensitive LexRank (Otterbacher et al., 2005). Given the set Vs = {vi} containing all the sentences to be ranked, we construct a graph where each node represents a sentence and each edge weight between sentence vi and sentence vj is induced from sentence similarity measure as follows: p(i → j) = f (1i→j) where f (i → j) Ek=1 f (i→k) represents the similarity between sentence vi and sentence vj, here is cosine similarity (Baeza-Yates and Ribeiro-Neto, 1999). We define f(i → i) = 0 to avoid self transition. Note that p(i → j) is usually not equal to p(j → i). We also compute the similarity rel(vi|q) of a sentence vi to the question topic q using the cosine measure. This relevance score is then normalized as follows to make the sum of all relevance values of the sentences equal to 1: rel′(vi|q) = v3�l (vi |q) �k=1 rel(vk|q). The saliency score Score(vi) for sentence vi can be calculated by mixing topic relevance score and scores of all other sentences linked with it as follows: Score(vi) = µ Ej6�i Score(vj) · p(j → i)+(1−µ)rel′(vi|q), where µ is t</context>
</contexts>
<marker>Baeza-Yates, Ribeiro-Neto, 1999</marker>
<rawString>Ricardo Baeza-Yates and Berthier Ribeiro-Neto. 1999. Modern Information Retrieval. Addison Wesley, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Lluis Marquez</author>
</authors>
<title>Introduction to the conll-2005 shared task: Semantic role labeling.</title>
<date>2005</date>
<contexts>
<context position="18798" citStr="Carreras and Marquez, 2005" startWordPosition="3166" endWordPosition="3169">oduce the opinion question answering system based on the proposed graph methods. Figure 3 shows five main modules: Question Analysis: It mainly includes two components. 1).Sentiment Classification: We classify all opinion questions into two categories: positive type or negative type. We extract several types of features, including a set of pattern features, and then design a classifier to identify sentiment polarity for each question (similar as (Yu and Hatzivassiloglou, 2003)). 2).Topic Set Expansion: The opinion question asks opinions about a particular target. Semantic role labeling based (Carreras and Marquez, 2005) and rule based techniques can be employed to extract this target as topic word. We also expand the topic word with several external knowledge bases: Since all the entity synonyms are redirected into the same page in Wikipedia (Rodrigo et al., 2007), we collect these redirection synonym words to expand topic set. We also collect some related lists as topic words. For example, given question “What reasons did people give for liking Ed Norton’s movies?”, we collect all the Norton’s movies from IMDB as this question’s topic words. Document Retrieval: The PRISE search engine, supported by NIST (Da</context>
</contexts>
<marker>Carreras, Marquez, 2005</marker>
<rawString>Xavier Carreras and Lluis Marquez. 2005. Introduction to the conll-2005 shared task: Semantic role labeling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Chen</author>
<author>Ming Zhou</author>
<author>Shilong Wang</author>
</authors>
<title>Reranking answers for definitional qa using language modeling. InACL-CoLing,</title>
<date>2006</date>
<pages>1081--1088</pages>
<contexts>
<context position="2758" citStr="Chen et al., 2006" startWordPosition="424" endWordPosition="427">estions are called opinion QA systems. Researchers (Stoyanov et al., 2005) have found that opinion questions have very different characteristics when compared with fact-based questions: opinion questions are often much longer, more likely to represent partial answers rather than complete answers and vary much more widely. These features make opinion QA a harder problem to tackle than fact-based QA. Also as shown in (Stoyanov et al., 2005), directly applying previous systems designed for fact-based QA onto opinion QA tasks would not achieve good performances. Similar to other complex QA tasks (Chen et al., 2006; Cui et al., 2007), the problem of opinion QA can be viewed as a sentence ranking problem. The Opinion QA task needs to consider not only the topic relevance of a sentence (to identify whether this sentence matches the topic of the question) but also the sentiment of a sentence (to identify the opinion polarity of a sentence). Current solutions to opinion QA tasks are generally in ad hoc styles: the topic score and the opinion score are usually separately calculated and then combined via a linear combination (Varma et al., 2008) or just filter out the candidate without matching the question s</context>
<context position="20079" citStr="Chen et al., 2006" startWordPosition="3379" endWordPosition="3382">d. Answer Candidate Extraction: We split retrieved documents into sentences, and extract sentences containing topic words. In order to improve recall, we carry out the following process to handle the problem of coreference resolution: We classify the topic word into four categories: male, female, group and other. Several pronouns are defined for each category, such as ”he”, ”him”, ”his” for male category. If a sentence is determined to contain the topic word, and its next sentence contains the corresponding pronouns, then the next sentence is also extracted as an answer candidate, similar as (Chen et al., 2006). Answer Ranking: The answer candidates are ranked by our proposed Opinion PageRank method or Opinion HITS method. Answer Selection by Removing Redundancy: We incrementally add the top ranked sentence into the answer set, if its cosine similarity with every extracted answer doesn’t exceed a predefined threshold, until the number of selected sentence (here is 40) is reached. 5 Experiments 5.1 Experiment Step 5.1.1 Dataset We employ the dataset from the TAC 2008 QA track. The task contains a total of 87 squishy 741 opinion questions.1 These questions have simple forms, and can be easily divided </context>
</contexts>
<marker>Chen, Zhou, Wang, 2006</marker>
<rawString>Yi Chen, Ming Zhou, and Shilong Wang. 2006. Reranking answers for definitional qa using language modeling. InACL-CoLing, pages 1081–1088.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Cui</author>
<author>Min-Yen Kan</author>
<author>Tat-Seng Chua</author>
</authors>
<title>Soft pattern matching models for definitional question answering.</title>
<date>2007</date>
<journal>ACM Trans. Inf. Syst.,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="2777" citStr="Cui et al., 2007" startWordPosition="428" endWordPosition="431">opinion QA systems. Researchers (Stoyanov et al., 2005) have found that opinion questions have very different characteristics when compared with fact-based questions: opinion questions are often much longer, more likely to represent partial answers rather than complete answers and vary much more widely. These features make opinion QA a harder problem to tackle than fact-based QA. Also as shown in (Stoyanov et al., 2005), directly applying previous systems designed for fact-based QA onto opinion QA tasks would not achieve good performances. Similar to other complex QA tasks (Chen et al., 2006; Cui et al., 2007), the problem of opinion QA can be viewed as a sentence ranking problem. The Opinion QA task needs to consider not only the topic relevance of a sentence (to identify whether this sentence matches the topic of the question) but also the sentiment of a sentence (to identify the opinion polarity of a sentence). Current solutions to opinion QA tasks are generally in ad hoc styles: the topic score and the opinion score are usually separately calculated and then combined via a linear combination (Varma et al., 2008) or just filter out the candidate without matching the question sentiment (Stoyanov </context>
</contexts>
<marker>Cui, Kan, Chua, 2007</marker>
<rawString>Hang Cui, Min-Yen Kan, and Tat-Seng Chua. 2007. Soft pattern matching models for definitional question answering. ACM Trans. Inf. Syst., 25(2):8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoa Trang Dang</author>
</authors>
<title>Overview of the tac</title>
<date>2008</date>
<booktitle>In TAC.</booktitle>
<contexts>
<context position="2102" citStr="Dang, 2008" startWordPosition="321" endWordPosition="322">opic in previous years. However, the objective of most state-of-the-art QA systems is to find answers to factual questions, such as “What is the longest river in the United States?” and “Who is Andrew Carnegie?” In fact, rather than factual information, people would also like to know about others’ opinions, thoughts and feelings toward some specific objects, people and events. Some examples of these questions are: “How is Bush’s decision not to ratify the Kyoto Protocol looked upon by Japan and other US allies?”(Stoyanov et al., 2005) and “Why do people like Subway Sandwiches?” from TAC 2008 (Dang, 2008). Systems designed to deal with such questions are called opinion QA systems. Researchers (Stoyanov et al., 2005) have found that opinion questions have very different characteristics when compared with fact-based questions: opinion questions are often much longer, more likely to represent partial answers rather than complete answers and vary much more widely. These features make opinion QA a harder problem to tackle than fact-based QA. Also as shown in (Stoyanov et al., 2005), directly applying previous systems designed for fact-based QA onto opinion QA tasks would not achieve good performanc</context>
<context position="6529" citStr="Dang, 2008" startWordPosition="1032" endWordPosition="1033">pinion answers. Relevant answers are further processed by focus detection, opinion scope identification and polarity detection. Some works on opinion mining are motivated by opinion question answering. (Yu and Hatzivassiloglou, 2003) discusses a necessary component for an opinion question answering system: separating opinions from fact at both the document and sentence level. (Soo-Min and Hovy, 2005) addresses another important component of opinion question answering: finding opinion holders. More recently, TAC 2008 QA track (evolved from TREC) focuses on finding answers to opinion questions (Dang, 2008). Opinion questions retrieve sentences or passages as answers which are relevant for both question topic and question sentiment. Most TAC participants employ a strategy of calculating two types of scores for answer candidates, which are the topic score measure and the opinion score measure (the opinion information expressed in the answer candidate). However, most approaches simply combined these two scores by a weighted sum, or removed candidates that didn’t match the polarity of questions, in order to extract the opinion answers. Algorithms based on Markov Random Walk have been proposed to so</context>
<context position="19407" citStr="Dang, 2008" startWordPosition="3270" endWordPosition="3271">5) and rule based techniques can be employed to extract this target as topic word. We also expand the topic word with several external knowledge bases: Since all the entity synonyms are redirected into the same page in Wikipedia (Rodrigo et al., 2007), we collect these redirection synonym words to expand topic set. We also collect some related lists as topic words. For example, given question “What reasons did people give for liking Ed Norton’s movies?”, we collect all the Norton’s movies from IMDB as this question’s topic words. Document Retrieval: The PRISE search engine, supported by NIST (Dang, 2008), is employed to retrieve the documents with topic word. Answer Candidate Extraction: We split retrieved documents into sentences, and extract sentences containing topic words. In order to improve recall, we carry out the following process to handle the problem of coreference resolution: We classify the topic word into four categories: male, female, group and other. Several pronouns are defined for each category, such as ”he”, ”him”, ”his” for male category. If a sentence is determined to contain the topic word, and its next sentence contains the corresponding pronouns, then the next sentence </context>
<context position="21514" citStr="Dang, 2008" startWordPosition="3619" endWordPosition="3620">uestion (called target in TAC) is also provided. Since our work in this paper focuses on sentence ranking for opinion QA, these characteristics of TAC data make it easy to process question analysis. Answers for all questions must be retrieved from the TREC Blog06 collection (Craig Macdonald and Iadh Ounis, 2006). The collection is a large sample of the blog sphere, crawled over an eleven-week period from December 6, 2005 until February 21, 2006. We retrieve the top 50 documents for each question. 5.1.2 Evaluation Metrics We adopt the evaluation metrics used in the TAC squishy opinion QA task (Dang, 2008). The TAC assessors create a list of acceptable information nuggets for each question. Each nugget will be assigned a normalized weight based on the number of assessors who judged it to be vital. We use these nuggets and corresponding weights to assess our approach. Three human assessors complete the evaluation process. Every question is scored using nugget recall (NR) and an approximation to nugget precision (NP) based on length. The final score will be calculated using F measure with TAC official value Q = 3 (Dang, 2008). This means recall is 3 times as important as precision: F(β = 3) = (33</context>
</contexts>
<marker>Dang, 2008</marker>
<rawString>Hoa Trang Dang. 2008. Overview of the tac 2008 opinion question answering and summarization tasks (draft). In TAC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unes Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Lexpagerank: Prestige in multi-document text summarization.</title>
<date>2004</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="7508" citStr="Erkan and Radev, 2004" startWordPosition="1186" endWordPosition="1189">r, most approaches simply combined these two scores by a weighted sum, or removed candidates that didn’t match the polarity of questions, in order to extract the opinion answers. Algorithms based on Markov Random Walk have been proposed to solve different kinds of ranking problems, most of which are inspired by the PageRank algorithm (Page et al., 1998) and the HITS algorithm (Kleinberg, 1999). These two algorithms were initially applied to the task of Web search and some of their variants have been proved successful in a number of applications, including fact-based QA and text summarization (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Otterbacher et al., 2005; Wan and Yang, 2008). Generally, such models would first construct a directed or undirected graph to represent the relationship between sentences and then certain graph-based ranking methods are applied on the graph to compute the ranking score for each sentence. Sentences with high scores are then added into the answer set or the summary. However, to the best of our knowledge, all previous Markov Random Walk-based sentence ranking models only make use of topic relevance information, i.e. whether this sentence is relevant to the fact we are </context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G¨unes Erkan and Dragomir R. Radev. 2004. Lexpagerank: Prestige in multi-document text summarization. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Sentiwordnet: A publicly available lexical resource for opinion mining.</title>
<date>2006</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="23946" citStr="Esuli and Sebastiani, 2006" startWordPosition="4038" endWordPosition="4041">r negative score above 0.6 3 Intersec- 640 518 Words appeared in tion both 1 and 2 4 Union 6860 3781 Words appeared in 1 or 2 5 All 10228 10228 All words appeared in 1 or 2 without distinguishing pos or neg Table 1: Sentiment lexicon description For lexicon-based opinion analysis, the selection of opinion thesaurus plays an important role in the final performance. HowNet2 is a knowledge database of the Chinese language, and provides an online word list with tags of positive and negative polarity. We use the English translation of those sentiment words as the sentimental lexicon. SentiWordNet (Esuli and Sebastiani, 2006) is another popular lexical resource for opinion mining. Table 1 shows the detail information of our used sentiment lexicons. In our models, the positive opinion words are used only for positive questions, and negative opinion words just for negative questions. We initially set parameter A in Opinion PageRank as 0 as (Liu and Ma, 2005), and other parameters simply as 0.5, including p in Opinion PageRank, γ in Opinion HITS, and α in baseline. The experiment results are shown in Figure 4. We can make three conclusions from Figure 4: 1. Opinion PageRank and Opinion HITS are both effective. The be</context>
</contexts>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>Andrea Esuli and Fabrizio Sebastiani. 2006. Sentiwordnet: A publicly available lexical resource for opinion mining. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon M Kleinberg</author>
</authors>
<title>Authoritative sources in a hyperlinked environment.</title>
<date>1999</date>
<journal>J. ACM,</journal>
<volume>46</volume>
<issue>5</issue>
<contexts>
<context position="7283" citStr="Kleinberg, 1999" startWordPosition="1151" endWordPosition="1152">participants employ a strategy of calculating two types of scores for answer candidates, which are the topic score measure and the opinion score measure (the opinion information expressed in the answer candidate). However, most approaches simply combined these two scores by a weighted sum, or removed candidates that didn’t match the polarity of questions, in order to extract the opinion answers. Algorithms based on Markov Random Walk have been proposed to solve different kinds of ranking problems, most of which are inspired by the PageRank algorithm (Page et al., 1998) and the HITS algorithm (Kleinberg, 1999). These two algorithms were initially applied to the task of Web search and some of their variants have been proved successful in a number of applications, including fact-based QA and text summarization (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Otterbacher et al., 2005; Wan and Yang, 2008). Generally, such models would first construct a directed or undirected graph to represent the relationship between sentences and then certain graph-based ranking methods are applied on the graph to compute the ranking score for each sentence. Sentences with high scores are then added into the answer </context>
<context position="14015" citStr="Kleinberg, 1999" startWordPosition="2329" endWordPosition="2330">ank model is then denoted by: Score(vi) = µ · Ej6�i Score(vj) · ˜M∗ji + (1 − µ) · rel′(si|q). The matrix form is: p˜ = µ ˜M∗Tp˜+ (1 − µ) · ~α. The final transition matrix is then denoted as: A∗ = µ˜M∗T +(1−µ)~e~αT and the sentence scores are obtained by the principle eigenvector of the new transition matrix A∗. 3.2 Opinion HITS Model The word’s sentiment score is fixed in Opinion PageRank. This may encounter problem when the sentiment score definition is not suitable for the specific question. We propose another opinion sentence ranking model based on the popular graph ranking algorithm HITS (Kleinberg, 1999). This model can dynamically learn the word sentiment score towards a specific question. HITS algorithm distinguishes the hubs and authorities in the objects. A hub object has links to many authorities, and an authority object has high-quality content and there are many hubs linking to it. The hub scores and authority scores are computed in a recursive way. Our proposed opinion HITS algorithm contains three layers. The upper level contains all the sentiment words from a lexicon, which represent their opinion information. The lower level contains all the words, which represent their topic infor</context>
</contexts>
<marker>Kleinberg, 1999</marker>
<rawString>Jon M. Kleinberg. 1999. Authoritative sources in a hyperlinked environment. J. ACM, 46(5):604–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lun-Wei Ku</author>
<author>Yu-Ting Liang</author>
<author>Hsin-Hsi Chen</author>
</authors>
<title>Question analysis and answer passage retrieval for opinion question answering systems.</title>
<date>2007</date>
<booktitle>In ROCLING.</booktitle>
<contexts>
<context position="5789" citStr="Ku et al., 2007" startWordPosition="919" endWordPosition="922">ludes this paper and provides possible directions for future work. 2 Related Work Few previous studies have been done on opinion QA. To our best knowledge, (Stoyanov et al., 2005) first created an opinion QA corpus OpQA. They find that opinion QA is a more challenging task than factual question answering, and they point out that traditional fact-based QA approaches may have difficulty on opinion QA tasks if unchanged. (Somasundaran et al., 2007) argues that making finer grained distinction of subjective types (sentiment and arguing) further improves the QA system. For non-English opinion QA, (Ku et al., 2007) creates a Chinese opinion QA corpus. They classify opinion questions into six types and construct three components to retrieve opinion answers. Relevant answers are further processed by focus detection, opinion scope identification and polarity detection. Some works on opinion mining are motivated by opinion question answering. (Yu and Hatzivassiloglou, 2003) discusses a necessary component for an opinion question answering system: separating opinions from fact at both the document and sentence level. (Soo-Min and Hovy, 2005) addresses another important component of opinion question answering</context>
</contexts>
<marker>Ku, Liang, Chen, 2007</marker>
<rawString>Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen. 2007. Question analysis and answer passage retrieval for opinion question answering systems. In ROCLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tie-Yan Liu</author>
<author>Wei-Ying Ma</author>
</authors>
<title>Webpage importance analysis using conditional markov random walk. In Web Intelligence,</title>
<date>2005</date>
<pages>515--521</pages>
<contexts>
<context position="10953" citStr="Liu and Ma, 2005" startWordPosition="1780" endWordPosition="1783"> |is the graph with each entry corresponding to the transition probability; α� = [rel′(vi|q)]|Vs|×1 is the vector containing the relevance scores of all the sentences to the question. The above process can be considered as a Markov chain by taking the sentences as the states and the corresponding transition matrix is given by A′ = µMT + (1 − µ)eaT. Considering Topics and Sentiments Together: In order to incorporate the opinion information and topic information for opinion sentence ranking in an unified framework, we propose an Opinion PageRank model (Figure 1) based on a two-layer link graph (Liu and Ma, 2005; Wan and Yang, 2008). In our opinion PageRank model, the Figure 1: Opinion PageRank first layer contains all the sentiment words from a lexicon to represent the opinion information, and the second layer denotes the sentence relationship in the topic sensitive Markov Random Walk model discussed above. The dashed lines between these two layers indicate the conditional influence between the opinion information and the sentences to be ranked. Formally, the new representation for the twolayer graph is denoted as G∗ = (Vs, Vo, Ess, Eso), where Vs = {vi} is the set of sentences and Vo = {oj} is the </context>
<context position="24283" citStr="Liu and Ma, 2005" startWordPosition="4096" endWordPosition="4099"> final performance. HowNet2 is a knowledge database of the Chinese language, and provides an online word list with tags of positive and negative polarity. We use the English translation of those sentiment words as the sentimental lexicon. SentiWordNet (Esuli and Sebastiani, 2006) is another popular lexical resource for opinion mining. Table 1 shows the detail information of our used sentiment lexicons. In our models, the positive opinion words are used only for positive questions, and negative opinion words just for negative questions. We initially set parameter A in Opinion PageRank as 0 as (Liu and Ma, 2005), and other parameters simply as 0.5, including p in Opinion PageRank, γ in Opinion HITS, and α in baseline. The experiment results are shown in Figure 4. We can make three conclusions from Figure 4: 1. Opinion PageRank and Opinion HITS are both effective. The best results of Opinion PageRank and Opinion HITS respectively achieve around 35.4% (0.199 vs 0.145), and 34.7% (0.195 vs 0.145) improvements in terms of F score over the best baseline result. We believe this is because our proposed models not only incorporate the topic information and opinion information, but also con2http://www.keenage</context>
</contexts>
<marker>Liu, Ma, 2005</marker>
<rawString>Tie-Yan Liu and Wei-Ying Ma. 2005. Webpage importance analysis using conditional markov random walk. In Web Intelligence, pages 515–521.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Paul Tarau</author>
</authors>
<title>Textrank: Bringing order into texts.</title>
<date>2004</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="7534" citStr="Mihalcea and Tarau, 2004" startWordPosition="1190" endWordPosition="1193">ly combined these two scores by a weighted sum, or removed candidates that didn’t match the polarity of questions, in order to extract the opinion answers. Algorithms based on Markov Random Walk have been proposed to solve different kinds of ranking problems, most of which are inspired by the PageRank algorithm (Page et al., 1998) and the HITS algorithm (Kleinberg, 1999). These two algorithms were initially applied to the task of Web search and some of their variants have been proved successful in a number of applications, including fact-based QA and text summarization (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Otterbacher et al., 2005; Wan and Yang, 2008). Generally, such models would first construct a directed or undirected graph to represent the relationship between sentences and then certain graph-based ranking methods are applied on the graph to compute the ranking score for each sentence. Sentences with high scores are then added into the answer set or the summary. However, to the best of our knowledge, all previous Markov Random Walk-based sentence ranking models only make use of topic relevance information, i.e. whether this sentence is relevant to the fact we are looking for, thus they are</context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>Rada Mihalcea and Paul Tarau. 2004. Textrank: Bringing order into texts. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jahna Otterbacher</author>
<author>G¨unes Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Using random walks for questionfocused sentence retrieval.</title>
<date>2005</date>
<booktitle>In HLT/EMNLP.</booktitle>
<contexts>
<context position="7560" citStr="Otterbacher et al., 2005" startWordPosition="1194" endWordPosition="1198">es by a weighted sum, or removed candidates that didn’t match the polarity of questions, in order to extract the opinion answers. Algorithms based on Markov Random Walk have been proposed to solve different kinds of ranking problems, most of which are inspired by the PageRank algorithm (Page et al., 1998) and the HITS algorithm (Kleinberg, 1999). These two algorithms were initially applied to the task of Web search and some of their variants have been proved successful in a number of applications, including fact-based QA and text summarization (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Otterbacher et al., 2005; Wan and Yang, 2008). Generally, such models would first construct a directed or undirected graph to represent the relationship between sentences and then certain graph-based ranking methods are applied on the graph to compute the ranking score for each sentence. Sentences with high scores are then added into the answer set or the summary. However, to the best of our knowledge, all previous Markov Random Walk-based sentence ranking models only make use of topic relevance information, i.e. whether this sentence is relevant to the fact we are looking for, thus they are limited to fact-based QA </context>
<context position="9127" citStr="Otterbacher et al., 2005" startWordPosition="1452" endWordPosition="1455">into the graph based sentence ranking framework, we propose two random walk based models for solving the problem, i.e. an Opinion PageRank model and an Opinion HITS model. 738 3.1 Opinion PageRank Model In order to rank sentence for opinion question answering, two aspects should be taken into account. First, the answer candidate is relevant to the question topic; second, the answer candidate is suitable for question sentiment. Considering Question Topic: We first introduce how to incorporate the question topic into the Markov Random Walk model, which is similar as the Topic-sensitive LexRank (Otterbacher et al., 2005). Given the set Vs = {vi} containing all the sentences to be ranked, we construct a graph where each node represents a sentence and each edge weight between sentence vi and sentence vj is induced from sentence similarity measure as follows: p(i → j) = f (1i→j) where f (i → j) Ek=1 f (i→k) represents the similarity between sentence vi and sentence vj, here is cosine similarity (Baeza-Yates and Ribeiro-Neto, 1999). We define f(i → i) = 0 to avoid self transition. Note that p(i → j) is usually not equal to p(j → i). We also compute the similarity rel(vi|q) of a sentence vi to the question topic q</context>
<context position="15734" citStr="Otterbacher et al., 2005" startWordPosition="2627" endWordPosition="2630">tences. Vo = {oj} is the set of all the sentiment words representing opinion information, Vt = {tj} is the set of all the words representing topic information. Eso = {eij|vi E Vs, oj E Vo} corresponds to the Figure 2: Opinion HITS model correlations between sentence and opinion words. Each edge eij is associated with a weight owij denoting the strength of the relationship between the sentence vi and the opinion word oj. The weight owij is 1 if the sentence vi contains word oj, otherwise 0. Est denotes the relationship between sentence and topic word. Its weight twij is calculated by tf · idf (Otterbacher et al., 2005). We define two matrixes O = (Oij)|Vs|×|Vo |and T = (Tij)|Vs|×|Vt |as follows, for Oij = owij, and if sentence i contains word j, therefore owij is assigned 1, otherwise owij is 0. Tij = twij = tfj · idfj (Otterbacher et al., 2005). Our new opinion HITS model is different from the basic HITS algorithm in two aspects. First, we consider the topic relevance when computing the sentence authority score based on the topic hub level as follows: AuthgeR(vi) a Etwij&gt;0 twij · topic score(j)·hubtopic(j), where topic score(j) is empirically defined as 1, if the word j is in the topic set (we will discuss</context>
</contexts>
<marker>Otterbacher, Erkan, Radev, 2005</marker>
<rawString>Jahna Otterbacher, G¨unes Erkan, and Dragomir R. Radev. 2005. Using random walks for questionfocused sentence retrieval. In HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Page</author>
<author>Sergey Brin</author>
<author>Rajeev Motwani</author>
<author>Terry Winograd</author>
</authors>
<title>The pagerank citation ranking: Bringing order to the web.</title>
<date>1998</date>
<tech>Technical report,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="7242" citStr="Page et al., 1998" startWordPosition="1143" endWordPosition="1146">ion topic and question sentiment. Most TAC participants employ a strategy of calculating two types of scores for answer candidates, which are the topic score measure and the opinion score measure (the opinion information expressed in the answer candidate). However, most approaches simply combined these two scores by a weighted sum, or removed candidates that didn’t match the polarity of questions, in order to extract the opinion answers. Algorithms based on Markov Random Walk have been proposed to solve different kinds of ranking problems, most of which are inspired by the PageRank algorithm (Page et al., 1998) and the HITS algorithm (Kleinberg, 1999). These two algorithms were initially applied to the task of Web search and some of their variants have been proved successful in a number of applications, including fact-based QA and text summarization (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Otterbacher et al., 2005; Wan and Yang, 2008). Generally, such models would first construct a directed or undirected graph to represent the relationship between sentences and then certain graph-based ranking methods are applied on the graph to compute the ranking score for each sentence. Sentences with hi</context>
</contexts>
<marker>Page, Brin, Motwani, Winograd, 1998</marker>
<rawString>Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1998. The pagerank citation ranking: Bringing order to the web. Technical report, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Swapna Somasundaran</author>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Veselin Stoyanov</author>
</authors>
<title>Qa with attitude: Exploiting opinion type analysis for improving question answering in online discussions and the news.</title>
<date>2007</date>
<booktitle>In ICWSM.</booktitle>
<contexts>
<context position="5622" citStr="Somasundaran et al., 2007" startWordPosition="894" endWordPosition="897">l discuss our proposed models in Section 3. In Section 4, we present an overview of our opinion QA system. The experiment results are shown in Section 5. Finally, Section 6 concludes this paper and provides possible directions for future work. 2 Related Work Few previous studies have been done on opinion QA. To our best knowledge, (Stoyanov et al., 2005) first created an opinion QA corpus OpQA. They find that opinion QA is a more challenging task than factual question answering, and they point out that traditional fact-based QA approaches may have difficulty on opinion QA tasks if unchanged. (Somasundaran et al., 2007) argues that making finer grained distinction of subjective types (sentiment and arguing) further improves the QA system. For non-English opinion QA, (Ku et al., 2007) creates a Chinese opinion QA corpus. They classify opinion questions into six types and construct three components to retrieve opinion answers. Relevant answers are further processed by focus detection, opinion scope identification and polarity detection. Some works on opinion mining are motivated by opinion question answering. (Yu and Hatzivassiloglou, 2003) discusses a necessary component for an opinion question answering syst</context>
</contexts>
<marker>Somasundaran, Wilson, Wiebe, Stoyanov, 2007</marker>
<rawString>Swapna Somasundaran, Theresa Wilson, Janyce Wiebe, and Veselin Stoyanov. 2007. Qa with attitude: Exploiting opinion type analysis for improving question answering in online discussions and the news. In ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kim Soo-Min</author>
<author>Eduard Hovy</author>
</authors>
<title>Identifying opinion holders for question answering in opinion texts.</title>
<date>2005</date>
<booktitle>In AAAI 2005 Workshop.</booktitle>
<contexts>
<context position="6321" citStr="Soo-Min and Hovy, 2005" startWordPosition="998" endWordPosition="1001"> and arguing) further improves the QA system. For non-English opinion QA, (Ku et al., 2007) creates a Chinese opinion QA corpus. They classify opinion questions into six types and construct three components to retrieve opinion answers. Relevant answers are further processed by focus detection, opinion scope identification and polarity detection. Some works on opinion mining are motivated by opinion question answering. (Yu and Hatzivassiloglou, 2003) discusses a necessary component for an opinion question answering system: separating opinions from fact at both the document and sentence level. (Soo-Min and Hovy, 2005) addresses another important component of opinion question answering: finding opinion holders. More recently, TAC 2008 QA track (evolved from TREC) focuses on finding answers to opinion questions (Dang, 2008). Opinion questions retrieve sentences or passages as answers which are relevant for both question topic and question sentiment. Most TAC participants employ a strategy of calculating two types of scores for answer candidates, which are the topic score measure and the opinion score measure (the opinion information expressed in the answer candidate). However, most approaches simply combined</context>
</contexts>
<marker>Soo-Min, Hovy, 2005</marker>
<rawString>Kim Soo-Min and Eduard Hovy. 2005. Identifying opinion holders for question answering in opinion texts. In AAAI 2005 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veselin Stoyanov</author>
<author>Claire Cardie</author>
<author>Janyce Wiebe</author>
</authors>
<title>Multi-perspective question answering using the opqa corpus.</title>
<date>2005</date>
<booktitle>In HLT/EMNLP.</booktitle>
<contexts>
<context position="2031" citStr="Stoyanov et al., 2005" startWordPosition="305" endWordPosition="309">rea in natural language processing (NLP) and much progress has been made on this topic in previous years. However, the objective of most state-of-the-art QA systems is to find answers to factual questions, such as “What is the longest river in the United States?” and “Who is Andrew Carnegie?” In fact, rather than factual information, people would also like to know about others’ opinions, thoughts and feelings toward some specific objects, people and events. Some examples of these questions are: “How is Bush’s decision not to ratify the Kyoto Protocol looked upon by Japan and other US allies?”(Stoyanov et al., 2005) and “Why do people like Subway Sandwiches?” from TAC 2008 (Dang, 2008). Systems designed to deal with such questions are called opinion QA systems. Researchers (Stoyanov et al., 2005) have found that opinion questions have very different characteristics when compared with fact-based questions: opinion questions are often much longer, more likely to represent partial answers rather than complete answers and vary much more widely. These features make opinion QA a harder problem to tackle than fact-based QA. Also as shown in (Stoyanov et al., 2005), directly applying previous systems designed fo</context>
<context position="3390" citStr="Stoyanov et al., 2005" startWordPosition="532" endWordPosition="535">l., 2007), the problem of opinion QA can be viewed as a sentence ranking problem. The Opinion QA task needs to consider not only the topic relevance of a sentence (to identify whether this sentence matches the topic of the question) but also the sentiment of a sentence (to identify the opinion polarity of a sentence). Current solutions to opinion QA tasks are generally in ad hoc styles: the topic score and the opinion score are usually separately calculated and then combined via a linear combination (Varma et al., 2008) or just filter out the candidate without matching the question sentiment (Stoyanov et al., 2005). However, topic and opinion are not independent in reality. The opinion words are closely associated with their contexts. Another problem is that existing algorithms compute the score for each answer candidate individually, in other words, they do not consider the relations between answer candidates. The quality of a answer candidate is not only determined by the relevance to the question, but also by other candidates. For example, the good answer may be mentioned by many candidates. In this paper, we propose two models to address the above limitations of previous sentence 737 Proceedings of </context>
<context position="5352" citStr="Stoyanov et al., 2005" startWordPosition="849" endWordPosition="852">QA data set demonstrate the effectiveness of the proposed Random Walk based methods. Our proposed method performs better than the best method in the TAC 2008 competition. The rest of this paper is organized as follows: Section 2 introduces some related works. We will discuss our proposed models in Section 3. In Section 4, we present an overview of our opinion QA system. The experiment results are shown in Section 5. Finally, Section 6 concludes this paper and provides possible directions for future work. 2 Related Work Few previous studies have been done on opinion QA. To our best knowledge, (Stoyanov et al., 2005) first created an opinion QA corpus OpQA. They find that opinion QA is a more challenging task than factual question answering, and they point out that traditional fact-based QA approaches may have difficulty on opinion QA tasks if unchanged. (Somasundaran et al., 2007) argues that making finer grained distinction of subjective types (sentiment and arguing) further improves the QA system. For non-English opinion QA, (Ku et al., 2007) creates a Chinese opinion QA corpus. They classify opinion questions into six types and construct three components to retrieve opinion answers. Relevant answers a</context>
</contexts>
<marker>Stoyanov, Cardie, Wiebe, 2005</marker>
<rawString>Veselin Stoyanov, Claire Cardie, and Janyce Wiebe. 2005. Multi-perspective question answering using the opqa corpus. In HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasudeva Varma</author>
<author>Prasad Pingali</author>
<author>Rahul Katragadda</author>
</authors>
<title>Iiit hyderabad at tac</title>
<date>2008</date>
<booktitle>In Text Analysis Conference.</booktitle>
<contexts>
<context position="3293" citStr="Varma et al., 2008" startWordPosition="517" endWordPosition="520"> not achieve good performances. Similar to other complex QA tasks (Chen et al., 2006; Cui et al., 2007), the problem of opinion QA can be viewed as a sentence ranking problem. The Opinion QA task needs to consider not only the topic relevance of a sentence (to identify whether this sentence matches the topic of the question) but also the sentiment of a sentence (to identify the opinion polarity of a sentence). Current solutions to opinion QA tasks are generally in ad hoc styles: the topic score and the opinion score are usually separately calculated and then combined via a linear combination (Varma et al., 2008) or just filter out the candidate without matching the question sentiment (Stoyanov et al., 2005). However, topic and opinion are not independent in reality. The opinion words are closely associated with their contexts. Another problem is that existing algorithms compute the score for each answer candidate individually, in other words, they do not consider the relations between answer candidates. The quality of a answer candidate is not only determined by the relevance to the question, but also by other candidates. For example, the good answer may be mentioned by many candidates. In this paper</context>
</contexts>
<marker>Varma, Pingali, Katragadda, 2008</marker>
<rawString>Vasudeva Varma, Prasad Pingali, Rahul Katragadda, and et al. 2008. Iiit hyderabad at tac 2008. In Text Analysis Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wan</author>
<author>J Yang</author>
</authors>
<title>Multi-document summarization using cluster-based link analysis.</title>
<date>2008</date>
<booktitle>In SIGIR,</booktitle>
<pages>299--306</pages>
<contexts>
<context position="7581" citStr="Wan and Yang, 2008" startWordPosition="1199" endWordPosition="1202">emoved candidates that didn’t match the polarity of questions, in order to extract the opinion answers. Algorithms based on Markov Random Walk have been proposed to solve different kinds of ranking problems, most of which are inspired by the PageRank algorithm (Page et al., 1998) and the HITS algorithm (Kleinberg, 1999). These two algorithms were initially applied to the task of Web search and some of their variants have been proved successful in a number of applications, including fact-based QA and text summarization (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Otterbacher et al., 2005; Wan and Yang, 2008). Generally, such models would first construct a directed or undirected graph to represent the relationship between sentences and then certain graph-based ranking methods are applied on the graph to compute the ranking score for each sentence. Sentences with high scores are then added into the answer set or the summary. However, to the best of our knowledge, all previous Markov Random Walk-based sentence ranking models only make use of topic relevance information, i.e. whether this sentence is relevant to the fact we are looking for, thus they are limited to fact-based QA tasks. To solve the o</context>
<context position="10974" citStr="Wan and Yang, 2008" startWordPosition="1784" endWordPosition="1787">h each entry corresponding to the transition probability; α� = [rel′(vi|q)]|Vs|×1 is the vector containing the relevance scores of all the sentences to the question. The above process can be considered as a Markov chain by taking the sentences as the states and the corresponding transition matrix is given by A′ = µMT + (1 − µ)eaT. Considering Topics and Sentiments Together: In order to incorporate the opinion information and topic information for opinion sentence ranking in an unified framework, we propose an Opinion PageRank model (Figure 1) based on a two-layer link graph (Liu and Ma, 2005; Wan and Yang, 2008). In our opinion PageRank model, the Figure 1: Opinion PageRank first layer contains all the sentiment words from a lexicon to represent the opinion information, and the second layer denotes the sentence relationship in the topic sensitive Markov Random Walk model discussed above. The dashed lines between these two layers indicate the conditional influence between the opinion information and the sentences to be ranked. Formally, the new representation for the twolayer graph is denoted as G∗ = (Vs, Vo, Ess, Eso), where Vs = {vi} is the set of sentences and Vo = {oj} is the set of sentiment word</context>
</contexts>
<marker>Wan, Yang, 2008</marker>
<rawString>X. Wan and J Yang. 2008. Multi-document summarization using cluster-based link analysis. In SIGIR, pages 299–306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong Yu</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences.</title>
<date>2003</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="6151" citStr="Yu and Hatzivassiloglou, 2003" startWordPosition="971" endWordPosition="974">based QA approaches may have difficulty on opinion QA tasks if unchanged. (Somasundaran et al., 2007) argues that making finer grained distinction of subjective types (sentiment and arguing) further improves the QA system. For non-English opinion QA, (Ku et al., 2007) creates a Chinese opinion QA corpus. They classify opinion questions into six types and construct three components to retrieve opinion answers. Relevant answers are further processed by focus detection, opinion scope identification and polarity detection. Some works on opinion mining are motivated by opinion question answering. (Yu and Hatzivassiloglou, 2003) discusses a necessary component for an opinion question answering system: separating opinions from fact at both the document and sentence level. (Soo-Min and Hovy, 2005) addresses another important component of opinion question answering: finding opinion holders. More recently, TAC 2008 QA track (evolved from TREC) focuses on finding answers to opinion questions (Dang, 2008). Opinion questions retrieve sentences or passages as answers which are relevant for both question topic and question sentiment. Most TAC participants employ a strategy of calculating two types of scores for answer candida</context>
<context position="18652" citStr="Yu and Hatzivassiloglou, 2003" startWordPosition="3145" endWordPosition="3148"> the saliency scores in the Opinion HITS model. The sentences are then ranked by their saliency scores. 4 System Description In this section, we introduce the opinion question answering system based on the proposed graph methods. Figure 3 shows five main modules: Question Analysis: It mainly includes two components. 1).Sentiment Classification: We classify all opinion questions into two categories: positive type or negative type. We extract several types of features, including a set of pattern features, and then design a classifier to identify sentiment polarity for each question (similar as (Yu and Hatzivassiloglou, 2003)). 2).Topic Set Expansion: The opinion question asks opinions about a particular target. Semantic role labeling based (Carreras and Marquez, 2005) and rule based techniques can be employed to extract this target as topic word. We also expand the topic word with several external knowledge bases: Since all the entity synonyms are redirected into the same page in Wikipedia (Rodrigo et al., 2007), we collect these redirection synonym words to expand topic set. We also collect some related lists as topic words. For example, given question “What reasons did people give for liking Ed Norton’s movies?</context>
</contexts>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Xingyao Ye</author>
</authors>
<title>A generation model to unify topic relevance and lexicon-based sentiment for opinion retrieval.</title>
<date>2008</date>
<booktitle>In SIGIR,</booktitle>
<pages>411--418</pages>
<contexts>
<context position="25949" citStr="Zhang and Ye, 2008" startWordPosition="4356" endWordPosition="4359">e because when the sentiment lexicon is defined appropriately for the specific question set, the opinion PageRank model performs better. While when the sentiment lexicon is not suitable for these questions, the opinion HITS model may dynamically learn a temporal sentiment lexicon and can yield a satisfied performance. 3. Hownet achieves the best overall performance among five sentiment lexicons. In HowNet, English translations of the Chinese sentiment words are annotated by nonnative speakers; hence most of them are common and popular terms, which maybe more suitable for the Blog environment (Zhang and Ye, 2008). We will use HowNet as the sentiment thesaurus in the following experiments. In baseline, the parameter α shows the relative contributions for topic score and opinion score. We vary α from 0 to 1 with an interval of 0.1, and find that the best baseline result 0.170 is achieved when α=0.1. This is because the topic information has been considered during candidate extraction, the system considering more opinion information (lower α) achieves better. We will use this best result as baseline score in following experiments. Since F(3) score is more related with recall, F score and recall will be d</context>
</contexts>
<marker>Zhang, Ye, 2008</marker>
<rawString>Min Zhang and Xingyao Ye. 2008. A generation model to unify topic relevance and lexicon-based sentiment for opinion retrieval. In SIGIR, pages 411–418.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>