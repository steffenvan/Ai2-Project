<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.022487">
<title confidence="0.95067">
High Throughput Modularized NLP System for Clinical Text
</title>
<author confidence="0.976165">
Serguei Pakhomov James Buntrock Patrick Duffy
</author>
<affiliation confidence="0.958792">
Mayo College of Medicine Division of Biomedical Infor- Division of Biomedical
</affiliation>
<address confidence="0.739694333333333">
matics Informatics
Mayo Clinic Mayo Clinic Mayo Clinic
Rochester, MN, 55905 Rochester, MN, 55905 Rochester, MN, 55905
</address>
<email confidence="0.998938">
pakhomov@mayo.edu Buntrock@mayo.edu duffp@mayo.edu
</email>
<sectionHeader confidence="0.993897" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999692210526316">
This paper presents the results of the de-
velopment of a high throughput, real time
modularized text analysis and information
retrieval system that identifies clinically
relevant entities in clinical notes, maps
the entities to several standardized no-
menclatures and makes them available for
subsequent information retrieval and data
mining. The performance of the system
was validated on a small collection of 351
documents partitioned into 4 query topics
and manually examined by 3 physicians
and 3 nurse abstractors for relevance to
the query topics. We find that simple key
phrase searching results in 73% recall and
77% precision. A combination of NLP
approaches to indexing improve the recall
to 92%, while lowering the precision to
67%.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999966944444445">
Until recently the NLP systems developed for
processing clinical texts have been narrowly fo-
cused on a specific type of document such as radi-
ology reports [1], discharge summaries [2],
medline abstracts [3], pathology reports [4]. In ad-
dition to being developed for a specific task, these
systems tend to fairly monolithic in that their com-
ponents have fairly strict dependencies on each
other, which make plug-and-play functionality dif-
ficult. NLP researchers and systems developers in
the field realize that modularized approaches are
beneficial for component reuse and more rapid de-
velopment and advancement of NLP technology.
In addition to the issue of modularity, the NLP sys-
tems development efforts are starting to take scal-
ability into account. The Mayo Clinic’s repository
of clinical notes contains over 16 million docu-
ments growing at the rate of 50K documents per
week. The time and space required for processing
these large amounts of data impose constraints on
the complexity of NLP systems.
Another engineering challenge is to make the
NLP systems work in real time. This is particularly
important in a clinical environment for patient re-
cruitment or patient identification for clinical re-
search use cases. In order to satisfy this
requirement, a text processing system has to inter-
face with the Electronic Health Record (EHR) sys-
tem in real time and process documents
immediately after they become available electroni-
cally. All of these are non-trivial issues and are
currently being addressed in the community. In this
poster we present the design and architecture of a
large-scale, highly modularized, real-time enabled
text analysis system as well as experimental vali-
dation results.
</bodyText>
<sectionHeader confidence="0.978204" genericHeader="introduction">
2 System Description
</sectionHeader>
<bodyText confidence="0.9991554">
Mayo Clinic and IBM have collaborated on a
Text Analytics project as part of a strategic Life
Sciences and Computational Biology partnership.
The goal of the Text Analytics collaboration was to
provide a text analysis system that would index
and retrieve clinical documents at the Mayo Clinic.
The Text Analytics architecture leveraged ex-
isting interface feeds for clinical documents by
routing them to the warehouse. A work manager
was written using messaging queues to distribute
work for text analysis for real-time and bulk proc-
essing (see Figure 1). Additional text analysis
engines can be configured and added with appro-
priate hardware to increase document throughput
of the system.
</bodyText>
<page confidence="0.983318">
25
</page>
<note confidence="0.538353">
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 25–28, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<figureCaption confidence="0.989685">
Figure 1- Text Analysis Process Flow
</figureCaption>
<bodyText confidence="0.999899824561404">
For deployment of text analysis engines we tested
two configurations. During the development phase
we used synchronous messaging using Apache
Web Server with Tomcat/Axis. The Apache Web
server provided a round robin mechanism to dis-
tributed SOAP requests for text analysis. This test-
ing was deployed on a 20 CPU Beowulf cluster
using AMD AthlonTM processors running Linux
operating system. For production deployment we
used Message Driven Beans (MDBs)using IBM
Websphere Application ServerTM (WAS) and IBM
Websphere Message QueueTM. The text engines
were deployed on 2-CPU blade servers with 4Gb
RAM. Each WAS instance had two MDBs with
text analysis engines.
Work was distributed using message queues. Each
text analysis engine was deployed to function in-
dependent of other engines. A total of 20 blade
servers were configured for text processing. The
average document throughput for each blade was
20 documents per minute.
The text analysis engine was designed by concep-
tually breaking up the task into granular functions
that could be implemented as components to be
assembled into a text processing system.
To implement the components we used an
IBM AlphaWorks package called Unstructured
Information Management Architecture (UIMA).
UIMA is a software architecture that defines roles,
interface, and communications of components for
natural language processing. The four main UIMA
services include: acquisition, unstructured informa-
tion analysis, structured information access, and
component discovery. For the Mayo project we
used the first three services. The ability to custom-
ize annotator sequences was advantageous during
the design process. Also, the ability to add annota-
tors for specific dictionaries amounted only in mi-
nor work. Once annotators are written to
conformance, UIMA provides pipeline develop-
ment and permits the developer to quickly custom-
ize processing to a specific task. The final annota-
tor layout is depicted in Figure 2.
The context free tokenizer is a finite state
transducer that parses the document text into the
smallest meaningful spans of text. A token is a set
of characters that can be classified into one of
these categories: word, punctuation, number, con-
traction, possessive, symbol without taking into
account any additional context.
The context sensitive spell corrector annotator
is used for automatic spell correction on word to-
kens. This annotator uses a combination of iso-
lated-word and context-sensitive statistical
approaches to rank the possible suggestions [5].
The suggestion with the highest ranking is stored
as a feature of a token.
</bodyText>
<figureCaption confidence="0.961948">
Figure 2 – Text Analysis Pipeline
</figureCaption>
<bodyText confidence="0.999756642857143">
The lexical normalizer annotator is applied
only to words, possessives, and contractions. It
generates a canonical form by using the National
Library of Medicine UMLS Lexical Variant Gen-
erator (LVG) tool1. Apart from generating lexical
variants and stemming optimized for the biomedi-
cal domain, it also generates a list of lemma entries
with Penn Treebank tags as input for the POS tag-
ger.
The sentence detector annotator parses the
document text into sentences. The sentence detec-
tor is based on a Maximum Entropy classifier
technology2 and is trained to recognize sentence
boundaries from hand annotated data.
</bodyText>
<footnote confidence="0.999487">
1 http://umlslex.nlm.nih.gov
2 http://maxent.sourceforge.net/
</footnote>
<page confidence="0.996227">
26
</page>
<bodyText confidence="0.981413177419355">
The context dependent tokenizer uses context
to detect complex tokens such as dates, times, and
problem lists3.
The part of speech (POS) pre-tagger annotator
is intended to execute prior to the POS tagger an-
notator. The pre-tagger loads a list of words that
are unambiguous with respect to POS and have
predetermined Penn Treebank tags. Words in the
document text are tagged with these predetermined
tags. The POS tagger can ignore these words and
focus on the remaining syntactically ambiguous
words.
The POS tagger annotator attaches a part of
speech tag to each token. The current version of
the POS tagger is from IBM based on Hidden
Markov models technology. This tagger has been
trained on a combination of the Penn Treebank
corpus of general English and a corpus of manually
tagged clinical data developed at the Mayo Clinic
[6], [7].
The shallow parser annotator makes higher
level constructs at the phrase level. The Shallow
Parser is from IBM. The shallow parser uses a set
of rules operating on tokens and their part-of-
speech category to identify linguistic phrases in the
text such as noun phrases, verb phrases, and adjec-
tival phrases.
The dictionary named entity annotator uses a
set of enriched dictionaries (SNOMED-CT, MeSH,
RxNorm and Mayo Synonym Clusters (MSC) to
lookup named entities in the document text. These
named entities include drugs, diagnoses, signs, and
symptoms. The MSC database contains a set of
clusters each consisting of diagnostic statements
that are considered to be synonymous. Synonymy
here is defined as two or more terms that have been
manually classified to the same category in the
Mayo Master Sheet repository, which contains
over 20 million manually coded diagnostic state-
ments. These diagnostic statements are used as
entry terms for dictionary lookup. A set of Mayo
compiled dictionaries are also used to detect ab-
breviations and hyphenated terms.
The abbreviation disambiguation annotator at-
tempts to detect and expand abbreviations and ac-
ronyms based on Maximum Entropy classifiers
trained on automatically generated data [8].
3 Problem lists typically consist of numbered items in the Im-
pression/Report/Plan section of the clinical notes
The negation annotator assigns a certainty at-
tribute to each named entity with the exception of
drugs. This annotator is based on a generalized
version of Chapman’s NegEx algorithm [9].
The ML (Machine Learning) Named Entity
annotator is based on a Naïve Bayes classifier
trained on a combination of the UMLS entry terms
and the MCS where each diagnostic statement is
represented as a bag-of-words and used as a train-
ing sample for generating a Naive Bayes classifier
which assigns MCS id’s to noun phrases identified
in the text of clinical notes. The architecture of this
component is given in Figure 3.
</bodyText>
<figure confidence="0.712935">
Text
</figure>
<figureCaption confidence="0.998623">
Figure 3. ML Named Entity Classifier
</figureCaption>
<bodyText confidence="0.999984">
The text of a clinical note is first looked up in the
MSC database using the dictionary named entity
annotator. If a span of text matched something in
the database, then the span is marked as a named
entity annotation and the appropriate cluster ID is
assigned to it. The portions of text where no match
was found continue to be processed with a named
entity identification algorithm that relies on the
output of the shallow parser annotator to find
noun phrases whose heads are on a list of nouns
that exist in the MSC database as individual manu-
ally coded entries. For example, a noun phrase
such as ‘metastasized cholangiocarcinoma’ will be
identified as a named entity and subsequently
automatically classified, but a noun phrase such as
‘patient’s father’ will not.
</bodyText>
<sectionHeader confidence="0.997627" genericHeader="background">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.99949275">
The system performance was evaluated using a
collection of 351 documents partitioned into 4 top-
ics: pulmonary fibrosis, cholangiocarcinoma, dia-
betes mellitus and congestive heart failure. Each of
</bodyText>
<figure confidence="0.981356181818182">
Best guess cluster
Y
Dictionary Lookup
Found
Noun Phrase Head identifier
Naïve Bayes classifier
N
Mayo Synonym Clusters
M001|cholangeocarcinoma
M001|bile duct cancer
M001|...
</figure>
<page confidence="0.995141">
27
</page>
<bodyText confidence="0.999568588235294">
the topics contained approximately 90 documents
that were manually examined by three nurse ab-
stractors and three physicians. Each note was
marked as either relevant or not relevant to a given
topic. In order to establish the reliability of this test
corpus, we used a standard weighted Kappa statis-
tic [10]. The overall Kappa for the four topics were
0.59 for pulmonary fibrosis, 0.79 for cholangiocar-
cinoma, 0.79 for diabetes mellitus and 0.59 for
congestive heart failure. We ran a set of queries for
each of the 4 topics on the partition generated for
that topic. Each query used the primary term that
represented the topic. For example, for pulmonary
fibrosis, only the term ‘pulmonary fibrosis’ was
used while other closely related terms such as ‘in-
terstitial pneumonitis’ were excluded. The baseline
query was executed using the term as a key phrase
on the original text of the documents. The rest of
the queries were executed using the concept id’s
automatically generated for each primary term. On
the back end, the text of the clinical notes was an-
notated with the Metamap program [3] for the
UMLS concepts and the ML Named Entity annota-
tor for MSC cluster id’s. On the front end, the
UMLS concept id’s were generated via the UMLS
Knowledge Server online and the MSC id’s were
generated using a combination of the same Naïve
Bayes classifier and the same dictionary lookup
mechanism as were used to annotate the clinical
notes. We also tested a query that combined
Metamap and MSC annotations and query parame-
ters. Recall, precision and f-score (α=0.5) were
calculated for each query. The results are summa-
rized in Table 1.
</bodyText>
<table confidence="0.999734">
Precision Recall F-score
Key Phrase 0.77 0.73 0.749467
MSC cluster 0.67 0.89 0.764487
Metamap 0.71 0.84 0.769548
Metamap+MSC 0.67 0.92 0.775346
</table>
<tableCaption confidence="0.999941">
Table 1. Performance of different annotation methods.
</tableCaption>
<bodyText confidence="0.999962307692308">
The f-score results are fairly close for all methods;
however, the recall is highest for the method that
combines Metamap and the MSC methodology.
This is particularly important for using this system
in recruiting patients for epidemiological research
for disease incidence or disease prevalence studies
and clinical trials where recall is valued more than
precision. A combination of Metamap and MSC
annotations and queries produced the highest recall
which shows that these systems are complemen-
tary. The modular design of our system makes it
easy to incorporate complementary annotation sys-
tems like Metamap into the annotation process.
</bodyText>
<sectionHeader confidence="0.987463" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999957166666667">
The authors wish to thank the Mayo Clinic
Emeritus Staff Physicians and Nurse Abstractors
who served as experts for this study. The authors
also wish to thank Patrick Duffy for programming
support and David Hodge for statistical analysis
and interpretation.
</bodyText>
<sectionHeader confidence="0.999193" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999883">
1. Friedman, C., et al., A general natural-language text
processor for clinical radiology. Journal of Ameri-
can Medical Informatics Association, 1994. 1(2): p.
161-174.
2. Friedman, C. Towards a Comprehensive Medical
Language Processing System: Methods and Issues.
in American Medical Informatics Association
(AMIA). 1997.
3. Aronson, A. Effective mapping of biomedical text to
the UMLS Metathesaurus: the MetaMap program. in
Proceedings of the 2001 AMIA Annual Symposium.
2001. Washington, DC.
4. Mitchell, K. and R. Crowley. GNegEx – Implemen-
tation and Evaluation of a Negation Tagger for the
Shared Pathology Iinformatics Network. in Advanc-
ing Practice, Instruction and Innovation through In-
formatics (APIII). 2003.
5. Thompson-McInness, B., S. Pakhomov, and T.
Pedersen. Automating Spelling Correction Tools Us-
ing Bigram Statistics. in Medinfo Symposium. 2004.
San Francisco, CA, USA.
6. Coden, A., et al., Domain-specific language models
and lexicons for tagging. In print in Journal of Bio-
medical Informatics, 2005.
7. Pakhomov, S., A. Coden, and C. Chute, Developing
a Corpus of Clinical Notes Manually Annotated for
Part-of-Speech. To appear in International Journal of
Medical Informatics, 2005(Special Issue on Natural
Language Processing in Biomedical Applications).
8. Pakhomov, S. Semi-Supervised Maximum Entropy
Based Approach to Acronym and Abbreviation Nor-
malization in Medical Texts. in 40th Meeting of the
Association for Computational Linguistics (ACL
2002). 2002. Philadelohia, PA.
9. Chapman, W.W., et al. Evaluation of Negation
Phrases in Narrative Clinical Reports. in American
Medical Informatics Association. 2001. Washington,
DC, USA.
10. Landis, J.R. and G.G. Koch, The Measurement of
Observer Agreement for Categorical Data. Biomet-
rics, 1977. 33: p. 159-174.
</reference>
<page confidence="0.99907">
28
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.622349">
<title confidence="0.999948">High Throughput Modularized NLP System for Clinical Text</title>
<author confidence="0.996282">Serguei Pakhomov James Buntrock Patrick Duffy</author>
<affiliation confidence="0.956987">Mayo College of Medicine Division of Biomedical Infor- Division of Biomedical matics Informatics Mayo Clinic Mayo Clinic Mayo Clinic</affiliation>
<address confidence="0.999463">Rochester, MN, 55905 Rochester, MN, 55905 Rochester, MN, 55905</address>
<email confidence="0.998951">pakhomov@mayo.eduBuntrock@mayo.eduduffp@mayo.edu</email>
<abstract confidence="0.9828261">This paper presents the results of the development of a high throughput, real time modularized text analysis and information retrieval system that identifies clinically relevant entities in clinical notes, maps the entities to several standardized nomenclatures and makes them available for subsequent information retrieval and data mining. The performance of the system was validated on a small collection of 351 documents partitioned into 4 query topics and manually examined by 3 physicians and 3 nurse abstractors for relevance to the query topics. We find that simple key phrase searching results in 73% recall and 77% precision. A combination of NLP approaches to indexing improve the recall to 92%, while lowering the precision to 67%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Friedman</author>
</authors>
<title>A general natural-language text processor for clinical radiology.</title>
<date>1994</date>
<journal>Journal of American Medical Informatics Association,</journal>
<volume>1</volume>
<issue>2</issue>
<pages>161--174</pages>
<contexts>
<context position="1275" citStr="[1]" startWordPosition="190" endWordPosition="190"> retrieval and data mining. The performance of the system was validated on a small collection of 351 documents partitioned into 4 query topics and manually examined by 3 physicians and 3 nurse abstractors for relevance to the query topics. We find that simple key phrase searching results in 73% recall and 77% precision. A combination of NLP approaches to indexing improve the recall to 92%, while lowering the precision to 67%. 1 Introduction Until recently the NLP systems developed for processing clinical texts have been narrowly focused on a specific type of document such as radiology reports [1], discharge summaries [2], medline abstracts [3], pathology reports [4]. In addition to being developed for a specific task, these systems tend to fairly monolithic in that their components have fairly strict dependencies on each other, which make plug-and-play functionality difficult. NLP researchers and systems developers in the field realize that modularized approaches are beneficial for component reuse and more rapid development and advancement of NLP technology. In addition to the issue of modularity, the NLP systems development efforts are starting to take scalability into account. The M</context>
</contexts>
<marker>1.</marker>
<rawString>Friedman, C., et al., A general natural-language text processor for clinical radiology. Journal of American Medical Informatics Association, 1994. 1(2): p. 161-174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Friedman</author>
</authors>
<title>Towards a Comprehensive Medical Language Processing System: Methods and Issues. in American Medical Informatics Association (AMIA).</title>
<date>1997</date>
<contexts>
<context position="1300" citStr="[2]" startWordPosition="193" endWordPosition="193">g. The performance of the system was validated on a small collection of 351 documents partitioned into 4 query topics and manually examined by 3 physicians and 3 nurse abstractors for relevance to the query topics. We find that simple key phrase searching results in 73% recall and 77% precision. A combination of NLP approaches to indexing improve the recall to 92%, while lowering the precision to 67%. 1 Introduction Until recently the NLP systems developed for processing clinical texts have been narrowly focused on a specific type of document such as radiology reports [1], discharge summaries [2], medline abstracts [3], pathology reports [4]. In addition to being developed for a specific task, these systems tend to fairly monolithic in that their components have fairly strict dependencies on each other, which make plug-and-play functionality difficult. NLP researchers and systems developers in the field realize that modularized approaches are beneficial for component reuse and more rapid development and advancement of NLP technology. In addition to the issue of modularity, the NLP systems development efforts are starting to take scalability into account. The Mayo Clinic’s repository o</context>
</contexts>
<marker>2.</marker>
<rawString>Friedman, C. Towards a Comprehensive Medical Language Processing System: Methods and Issues. in American Medical Informatics Association (AMIA). 1997.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A Aronson</author>
</authors>
<title>Effective mapping of biomedical text to the UMLS Metathesaurus: the MetaMap program.</title>
<booktitle>in Proceedings of the 2001 AMIA Annual Symposium.</booktitle>
<contexts>
<context position="1323" citStr="[3]" startWordPosition="196" endWordPosition="196">he system was validated on a small collection of 351 documents partitioned into 4 query topics and manually examined by 3 physicians and 3 nurse abstractors for relevance to the query topics. We find that simple key phrase searching results in 73% recall and 77% precision. A combination of NLP approaches to indexing improve the recall to 92%, while lowering the precision to 67%. 1 Introduction Until recently the NLP systems developed for processing clinical texts have been narrowly focused on a specific type of document such as radiology reports [1], discharge summaries [2], medline abstracts [3], pathology reports [4]. In addition to being developed for a specific task, these systems tend to fairly monolithic in that their components have fairly strict dependencies on each other, which make plug-and-play functionality difficult. NLP researchers and systems developers in the field realize that modularized approaches are beneficial for component reuse and more rapid development and advancement of NLP technology. In addition to the issue of modularity, the NLP systems development efforts are starting to take scalability into account. The Mayo Clinic’s repository of clinical notes contai</context>
<context position="12112" citStr="[3]" startWordPosition="1901" endWordPosition="1901"> queries for each of the 4 topics on the partition generated for that topic. Each query used the primary term that represented the topic. For example, for pulmonary fibrosis, only the term ‘pulmonary fibrosis’ was used while other closely related terms such as ‘interstitial pneumonitis’ were excluded. The baseline query was executed using the term as a key phrase on the original text of the documents. The rest of the queries were executed using the concept id’s automatically generated for each primary term. On the back end, the text of the clinical notes was annotated with the Metamap program [3] for the UMLS concepts and the ML Named Entity annotator for MSC cluster id’s. On the front end, the UMLS concept id’s were generated via the UMLS Knowledge Server online and the MSC id’s were generated using a combination of the same Naïve Bayes classifier and the same dictionary lookup mechanism as were used to annotate the clinical notes. We also tested a query that combined Metamap and MSC annotations and query parameters. Recall, precision and f-score (α=0.5) were calculated for each query. The results are summarized in Table 1. Precision Recall F-score Key Phrase 0.77 0.73 0.749467 MSC c</context>
</contexts>
<marker>3.</marker>
<rawString>Aronson, A. Effective mapping of biomedical text to the UMLS Metathesaurus: the MetaMap program. in Proceedings of the 2001 AMIA Annual Symposium.</rawString>
</citation>
<citation valid="false">
<authors>
<author>DC Washington</author>
</authors>
<marker>2001.</marker>
<rawString>Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Mitchell</author>
<author>R Crowley</author>
</authors>
<title>GNegEx – Implementation and Evaluation of a Negation Tagger for the Shared Pathology Iinformatics Network. in Advancing Practice, Instruction and Innovation through Informatics (APIII).</title>
<date>2003</date>
<contexts>
<context position="1346" citStr="[4]" startWordPosition="199" endWordPosition="199"> on a small collection of 351 documents partitioned into 4 query topics and manually examined by 3 physicians and 3 nurse abstractors for relevance to the query topics. We find that simple key phrase searching results in 73% recall and 77% precision. A combination of NLP approaches to indexing improve the recall to 92%, while lowering the precision to 67%. 1 Introduction Until recently the NLP systems developed for processing clinical texts have been narrowly focused on a specific type of document such as radiology reports [1], discharge summaries [2], medline abstracts [3], pathology reports [4]. In addition to being developed for a specific task, these systems tend to fairly monolithic in that their components have fairly strict dependencies on each other, which make plug-and-play functionality difficult. NLP researchers and systems developers in the field realize that modularized approaches are beneficial for component reuse and more rapid development and advancement of NLP technology. In addition to the issue of modularity, the NLP systems development efforts are starting to take scalability into account. The Mayo Clinic’s repository of clinical notes contains over 16 million docu</context>
</contexts>
<marker>4.</marker>
<rawString>Mitchell, K. and R. Crowley. GNegEx – Implementation and Evaluation of a Negation Tagger for the Shared Pathology Iinformatics Network. in Advancing Practice, Instruction and Innovation through Informatics (APIII). 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Thompson-McInness</author>
<author>S Pakhomov</author>
<author>T Pedersen</author>
</authors>
<title>Automating Spelling Correction Tools Using Bigram Statistics. in Medinfo Symposium.</title>
<date>2004</date>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="6235" citStr="[5]" startWordPosition="951" endWordPosition="951"> annotator layout is depicted in Figure 2. The context free tokenizer is a finite state transducer that parses the document text into the smallest meaningful spans of text. A token is a set of characters that can be classified into one of these categories: word, punctuation, number, contraction, possessive, symbol without taking into account any additional context. The context sensitive spell corrector annotator is used for automatic spell correction on word tokens. This annotator uses a combination of isolated-word and context-sensitive statistical approaches to rank the possible suggestions [5]. The suggestion with the highest ranking is stored as a feature of a token. Figure 2 – Text Analysis Pipeline The lexical normalizer annotator is applied only to words, possessives, and contractions. It generates a canonical form by using the National Library of Medicine UMLS Lexical Variant Generator (LVG) tool1. Apart from generating lexical variants and stemming optimized for the biomedical domain, it also generates a list of lemma entries with Penn Treebank tags as input for the POS tagger. The sentence detector annotator parses the document text into sentences. The sentence detector is b</context>
</contexts>
<marker>5.</marker>
<rawString>Thompson-McInness, B., S. Pakhomov, and T. Pedersen. Automating Spelling Correction Tools Using Bigram Statistics. in Medinfo Symposium. 2004. San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Coden</author>
</authors>
<title>Domain-specific language models and lexicons for tagging.</title>
<date>2005</date>
<booktitle>In print in Journal of Biomedical Informatics,</booktitle>
<contexts>
<context position="7858" citStr="[6]" startWordPosition="1212" endWordPosition="1212">oads a list of words that are unambiguous with respect to POS and have predetermined Penn Treebank tags. Words in the document text are tagged with these predetermined tags. The POS tagger can ignore these words and focus on the remaining syntactically ambiguous words. The POS tagger annotator attaches a part of speech tag to each token. The current version of the POS tagger is from IBM based on Hidden Markov models technology. This tagger has been trained on a combination of the Penn Treebank corpus of general English and a corpus of manually tagged clinical data developed at the Mayo Clinic [6], [7]. The shallow parser annotator makes higher level constructs at the phrase level. The Shallow Parser is from IBM. The shallow parser uses a set of rules operating on tokens and their part-ofspeech category to identify linguistic phrases in the text such as noun phrases, verb phrases, and adjectival phrases. The dictionary named entity annotator uses a set of enriched dictionaries (SNOMED-CT, MeSH, RxNorm and Mayo Synonym Clusters (MSC) to lookup named entities in the document text. These named entities include drugs, diagnoses, signs, and symptoms. The MSC database contains a set of clust</context>
</contexts>
<marker>6.</marker>
<rawString>Coden, A., et al., Domain-specific language models and lexicons for tagging. In print in Journal of Biomedical Informatics, 2005.</rawString>
</citation>
<citation valid="false">
<authors>
<author>S Pakhomov</author>
<author>A Coden</author>
<author>C Chute</author>
</authors>
<title>Developing a Corpus of Clinical Notes Manually Annotated for Part-of-Speech. To appear</title>
<booktitle>in International Journal of Medical Informatics, 2005(Special Issue on Natural Language Processing in Biomedical Applications).</booktitle>
<contexts>
<context position="7863" citStr="[7]" startWordPosition="1213" endWordPosition="1213">a list of words that are unambiguous with respect to POS and have predetermined Penn Treebank tags. Words in the document text are tagged with these predetermined tags. The POS tagger can ignore these words and focus on the remaining syntactically ambiguous words. The POS tagger annotator attaches a part of speech tag to each token. The current version of the POS tagger is from IBM based on Hidden Markov models technology. This tagger has been trained on a combination of the Penn Treebank corpus of general English and a corpus of manually tagged clinical data developed at the Mayo Clinic [6], [7]. The shallow parser annotator makes higher level constructs at the phrase level. The Shallow Parser is from IBM. The shallow parser uses a set of rules operating on tokens and their part-ofspeech category to identify linguistic phrases in the text such as noun phrases, verb phrases, and adjectival phrases. The dictionary named entity annotator uses a set of enriched dictionaries (SNOMED-CT, MeSH, RxNorm and Mayo Synonym Clusters (MSC) to lookup named entities in the document text. These named entities include drugs, diagnoses, signs, and symptoms. The MSC database contains a set of clusters e</context>
</contexts>
<marker>7.</marker>
<rawString>Pakhomov, S., A. Coden, and C. Chute, Developing a Corpus of Clinical Notes Manually Annotated for Part-of-Speech. To appear in International Journal of Medical Informatics, 2005(Special Issue on Natural Language Processing in Biomedical Applications).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pakhomov</author>
</authors>
<title>Semi-Supervised Maximum Entropy Based Approach to Acronym and Abbreviation Normalization in Medical Texts.</title>
<date>2002</date>
<booktitle>in 40th Meeting of the Association for Computational Linguistics (ACL</booktitle>
<location>Philadelohia, PA.</location>
<contexts>
<context position="9099" citStr="[8]" startWordPosition="1405" endWordPosition="1405">ements that are considered to be synonymous. Synonymy here is defined as two or more terms that have been manually classified to the same category in the Mayo Master Sheet repository, which contains over 20 million manually coded diagnostic statements. These diagnostic statements are used as entry terms for dictionary lookup. A set of Mayo compiled dictionaries are also used to detect abbreviations and hyphenated terms. The abbreviation disambiguation annotator attempts to detect and expand abbreviations and acronyms based on Maximum Entropy classifiers trained on automatically generated data [8]. 3 Problem lists typically consist of numbered items in the Impression/Report/Plan section of the clinical notes The negation annotator assigns a certainty attribute to each named entity with the exception of drugs. This annotator is based on a generalized version of Chapman’s NegEx algorithm [9]. The ML (Machine Learning) Named Entity annotator is based on a Naïve Bayes classifier trained on a combination of the UMLS entry terms and the MCS where each diagnostic statement is represented as a bag-of-words and used as a training sample for generating a Naive Bayes classifier which assigns MCS </context>
</contexts>
<marker>8.</marker>
<rawString>Pakhomov, S. Semi-Supervised Maximum Entropy Based Approach to Acronym and Abbreviation Normalization in Medical Texts. in 40th Meeting of the Association for Computational Linguistics (ACL 2002). 2002. Philadelohia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W W Chapman</author>
</authors>
<title>Evaluation of Negation Phrases in Narrative Clinical Reports. in American Medical Informatics Association.</title>
<date>2001</date>
<location>Washington, DC, USA.</location>
<contexts>
<context position="9397" citStr="[9]" startWordPosition="1452" endWordPosition="1452"> terms for dictionary lookup. A set of Mayo compiled dictionaries are also used to detect abbreviations and hyphenated terms. The abbreviation disambiguation annotator attempts to detect and expand abbreviations and acronyms based on Maximum Entropy classifiers trained on automatically generated data [8]. 3 Problem lists typically consist of numbered items in the Impression/Report/Plan section of the clinical notes The negation annotator assigns a certainty attribute to each named entity with the exception of drugs. This annotator is based on a generalized version of Chapman’s NegEx algorithm [9]. The ML (Machine Learning) Named Entity annotator is based on a Naïve Bayes classifier trained on a combination of the UMLS entry terms and the MCS where each diagnostic statement is represented as a bag-of-words and used as a training sample for generating a Naive Bayes classifier which assigns MCS id’s to noun phrases identified in the text of clinical notes. The architecture of this component is given in Figure 3. Text Figure 3. ML Named Entity Classifier The text of a clinical note is first looked up in the MSC database using the dictionary named entity annotator. If a span of text matche</context>
</contexts>
<marker>9.</marker>
<rawString>Chapman, W.W., et al. Evaluation of Negation Phrases in Narrative Clinical Reports. in American Medical Informatics Association. 2001. Washington, DC, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Landis</author>
<author>G G Koch</author>
</authors>
<title>The Measurement of Observer Agreement for Categorical Data. Biometrics,</title>
<date>1977</date>
<volume>33</volume>
<pages>159--174</pages>
<contexts>
<context position="11325" citStr="[10]" startWordPosition="1768" endWordPosition="1768">titioned into 4 topics: pulmonary fibrosis, cholangiocarcinoma, diabetes mellitus and congestive heart failure. Each of Best guess cluster Y Dictionary Lookup Found Noun Phrase Head identifier Naïve Bayes classifier N Mayo Synonym Clusters M001|cholangeocarcinoma M001|bile duct cancer M001|... 27 the topics contained approximately 90 documents that were manually examined by three nurse abstractors and three physicians. Each note was marked as either relevant or not relevant to a given topic. In order to establish the reliability of this test corpus, we used a standard weighted Kappa statistic [10]. The overall Kappa for the four topics were 0.59 for pulmonary fibrosis, 0.79 for cholangiocarcinoma, 0.79 for diabetes mellitus and 0.59 for congestive heart failure. We ran a set of queries for each of the 4 topics on the partition generated for that topic. Each query used the primary term that represented the topic. For example, for pulmonary fibrosis, only the term ‘pulmonary fibrosis’ was used while other closely related terms such as ‘interstitial pneumonitis’ were excluded. The baseline query was executed using the term as a key phrase on the original text of the documents. The rest of</context>
</contexts>
<marker>10.</marker>
<rawString>Landis, J.R. and G.G. Koch, The Measurement of Observer Agreement for Categorical Data. Biometrics, 1977. 33: p. 159-174.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>