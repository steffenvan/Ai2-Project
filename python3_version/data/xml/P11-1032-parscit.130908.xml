<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002022">
<title confidence="0.981522">
Finding Deceptive Opinion Spam by Any Stretch of the Imagination
</title>
<author confidence="0.952812">
Jeffrey T. Hancock
</author>
<affiliation confidence="0.9708735">
Department of Communication
Cornell University
</affiliation>
<address confidence="0.720328">
Ithaca, NY 14853
</address>
<email confidence="0.985188">
jth34@cornell.edu
</email>
<author confidence="0.987527">
Myle Ott Yejin Choi Claire Cardie
</author>
<affiliation confidence="0.99614">
Department of Computer Science
Cornell University
</affiliation>
<address confidence="0.643913">
Ithaca, NY 14853
</address>
<email confidence="0.998818">
{myleott,ychoi,cardie}@cs.cornell.edu
</email>
<sectionHeader confidence="0.993894" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99978945">
Consumers increasingly rate, review and re-
search products online (Jansen, 2010; Litvin
et al., 2008). Consequently, websites con-
taining consumer reviews are becoming tar-
gets of opinion spam. While recent work
has focused primarily on manually identifi-
able instances of opinion spam, in this work
we study deceptive opinion spam—fictitious
opinions that have been deliberately written to
sound authentic. Integrating work from psy-
chology and computational linguistics, we de-
velop and compare three approaches to detect-
ing deceptive opinion spam, and ultimately
develop a classifier that is nearly 90% accurate
on our gold-standard opinion spam dataset.
Based on feature analysis of our learned mod-
els, we additionally make several theoretical
contributions, including revealing a relation-
ship between deceptive opinions and imagina-
tive writing.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999984875">
With the ever-increasing popularity of review web-
sites that feature user-generated opinions (e.g.,
TripAdvisor1 and Yelp2), there comes an increasing
potential for monetary gain through opinion spam—
inappropriate or fraudulent reviews. Opinion spam
can range from annoying self-promotion of an un-
related website or blog to deliberate review fraud,
as in the recent case3 of a Belkin employee who
</bodyText>
<footnote confidence="0.93870875">
1http://tripadvisor.com
2http://yelp.com
3http://news.cnet.com/8301-1001_
3-10145399-92.html
</footnote>
<bodyText confidence="0.997626333333333">
hired people to write positive reviews for an other-
wise poorly reviewed product.4
While other kinds of spam have received consid-
erable computational attention, regrettably there has
been little work to date (see Section 2) on opinion
spam detection. Furthermore, most previous work in
the area has focused on the detection of DISRUPTIVE
OPINION SPAM—uncontroversial instances of spam
that are easily identified by a human reader, e.g., ad-
vertisements, questions, and other irrelevant or non-
opinion text (Jindal and Liu, 2008). And while the
presence of disruptive opinion spam is certainly a
nuisance, the risk it poses to the user is minimal,
since the user can always choose to ignore it.
We focus here on a potentially more insidi-
ous type of opinion spam: DECEPTIVE OPINION
SPAM—fictitious opinions that have been deliber-
ately written to sound authentic, in order to deceive
the reader. For example, one of the following two
hotel reviews is truthful and the other is deceptive
opinion spam:
</bodyText>
<listItem confidence="0.930224538461539">
1. I have stayed at many hotels traveling for both business
and pleasure and I can honestly stay that The James is
tops. The service at the hotel is first class. The rooms
are modern and very comfortable. The location is per-
fect within walking distance to all of the great sights and
restaurants. Highly recommend to both business trav-
ellers and couples.
2. My husband and I stayed at the James Chicago Hotel
for our anniversary. This place is fantastic! We knew
as soon as we arrived we made the right choice! The
rooms are BEAUTIFUL and the staff very attentive and
wonderful!! The area of the hotel is great, since I love
to shop I couldn’t ask for more!! We will definatly be
</listItem>
<footnote confidence="0.9946555">
4It is also possible for opinion spam to be negative, poten-
tially in order to sully the reputation of a competitor.
</footnote>
<page confidence="0.963814">
309
</page>
<note confidence="0.989454">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 309–319,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
back to Chicago and we will for sure be back to the James
Chicago.
</note>
<bodyText confidence="0.999926928571429">
Typically, these deceptive opinions are neither
easily ignored nor even identifiable by a human
reader;5 consequently, there are few good sources
of labeled data for this research. Indeed, in the ab-
sence of gold-standard data, related studies (see Sec-
tion 2) have been forced to utilize ad hoc procedures
for evaluation. In contrast, one contribution of the
work presented here is the creation of the first large-
scale, publicly available6 dataset for deceptive opin-
ion spam research, containing 400 truthful and 400
gold-standard deceptive reviews.
To obtain a deeper understanding of the nature of
deceptive opinion spam, we explore the relative util-
ity of three potentially complementary framings of
our problem. Specifically, we view the task as: (a)
a standard text categorization task, in which we use
n-gram–based classifiers to label opinions as either
deceptive or truthful (Joachims, 1998; Sebastiani,
2002); (b) an instance of psycholinguistic decep-
tion detection, in which we expect deceptive state-
ments to exemplify the psychological effects of ly-
ing, such as increased negative emotion and psycho-
logical distancing (Hancock et al., 2008; Newman et
al., 2003); and, (c) a problem of genre identification,
in which we view deceptive and truthful writing as
sub-genres of imaginative and informative writing,
respectively (Biber et al., 1999; Rayson et al., 2001).
We compare the performance of each approach
on our novel dataset. Particularly, we find that ma-
chine learning classifiers trained on features tradi-
tionally employed in (a) psychological studies of
deception and (b) genre identification are both out-
performed at statistically significant levels by n-
gram–based text categorization techniques. Notably,
a combined classifier with both n-gram and psy-
chological deception features achieves nearly 90%
cross-validated accuracy on this task. In contrast,
we find deceptive opinion spam detection to be well
beyond the capabilities of most human judges, who
perform roughly at-chance—a finding that is consis-
tent with decades of traditional deception detection
research (Bond and DePaulo, 2006).
</bodyText>
<footnote confidence="0.978726666666667">
5The second example review is deceptive opinion spam.
6Available by request at: http://www.cs.cornell.
edu/˜myleott/op_spam
</footnote>
<bodyText confidence="0.999944041666667">
Additionally, we make several theoretical con-
tributions based on an examination of the feature
weights learned by our machine learning classifiers.
Specifically, we shed light on an ongoing debate in
the deception literature regarding the importance of
considering the context and motivation of a decep-
tion, rather than simply identifying a universal set
of deception cues. We also present findings that are
consistent with recent work highlighting the difficul-
ties that liars have encoding spatial information (Vrij
et al., 2009). Lastly, our study of deceptive opinion
spam detection as a genre identification problem re-
veals relationships between deceptive opinions and
imaginative writing, and between truthful opinions
and informative writing.
The rest of this paper is organized as follows: in
Section 2, we summarize related work; in Section 3,
we explain our methodology for gathering data and
evaluate human performance; in Section 4, we de-
scribe the features and classifiers employed by our
three automated detection approaches; in Section 5,
we present and discuss experimental results; finally,
conclusions and directions for future work are given
in Section 6.
</bodyText>
<sectionHeader confidence="0.999692" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9999330625">
Spam has historically been studied in the contexts of
e-mail (Drucker et al., 2002), and the Web (Gy¨ongyi
et al., 2004; Ntoulas et al., 2006). Recently, re-
searchers have began to look at opinion spam as
well (Jindal and Liu, 2008; Wu et al., 2010; Yoo
and Gretzel, 2009).
Jindal and Liu (2008) find that opinion spam is
both widespread and different in nature from either
e-mail or Web spam. Using product review data,
and in the absence of gold-standard deceptive opin-
ions, they train models using features based on the
review text, reviewer, and product, to distinguish
between duplicate opinions7 (considered deceptive
spam) and non-duplicate opinions (considered truth-
ful). Wu et al. (2010) propose an alternative strategy
for detecting deceptive opinion spam in the absence
</bodyText>
<footnote confidence="0.975998166666667">
7Duplicate (or near-duplicate) opinions are opinions that ap-
pear more than once in the corpus with the same (or similar)
text. While these opinions are likely to be deceptive, they are
unlikely to be representative of deceptive opinion spam in gen-
eral. Moreover, they are potentially detectable via off-the-shelf
plagiarism detection software.
</footnote>
<page confidence="0.998396">
310
</page>
<bodyText confidence="0.999951432432432">
of gold-standard data, based on the distortion of pop-
ularity rankings. Both of these heuristic evaluation
approaches are unnecessary in our work, since we
compare gold-standard deceptive and truthful opin-
ions.
Yoo and Gretzel (2009) gather 40 truthful and 42
deceptive hotel reviews and, using a standard statis-
tical test, manually compare the psychologically rel-
evant linguistic differences between them. In con-
trast, we create a much larger dataset of 800 opin-
ions that we use to develop and evaluate automated
deception classifiers.
Research has also been conducted on the re-
lated task of psycholinguistic deception detection.
Newman et al. (2003), and later Mihalcea and
Strapparava (2009), ask participants to give both
their true and untrue views on personal issues
(e.g., their stance on the death penalty). Zhou et
al. (2004; 2008) consider computer-mediated decep-
tion in role-playing games designed to be played
over instant messaging and e-mail. However, while
these studies compare n-gram–based deception clas-
sifiers to a random guess baseline of 50%, we addi-
tionally evaluate and compare two other computa-
tional approaches (described in Section 4), as well
as the performance of human judges (described in
Section 3.3).
Lastly, automatic approaches to determining re-
view quality have been studied—directly (Weimer
et al., 2007), and in the contexts of helpful-
ness (Danescu-Niculescu-Mizil et al., 2009; Kim et
al., 2006; O’Mahony and Smyth, 2009) and credibil-
ity (Weerkamp and De Rijke, 2008). Unfortunately,
most measures of quality employed in those works
are based exclusively on human judgments, which
we find in Section 3 to be poorly calibrated to de-
tecting deceptive opinion spam.
</bodyText>
<sectionHeader confidence="0.9646475" genericHeader="method">
3 Dataset Construction and Human
Performance
</sectionHeader>
<bodyText confidence="0.999954">
While truthful opinions are ubiquitous online, de-
ceptive opinions are difficult to obtain without re-
sorting to heuristic methods (Jindal and Liu, 2008;
Wu et al., 2010). In this section, we report our ef-
forts to gather (and validate with human judgments)
the first publicly available opinion spam dataset with
gold-standard deceptive opinions.
Following the work of Yoo and Gretzel (2009), we
compare truthful and deceptive positive reviews for
hotels found on TripAdvisor. Specifically, we mine
all 5-star truthful reviews from the 20 most popular
hotels on TripAdvisor8 in the Chicago area.9 De-
ceptive opinions are gathered for those same 20 ho-
tels using Amazon Mechanical Turk10 (AMT). Be-
low, we provide details of the collection methodolo-
gies for deceptive (Section 3.1) and truthful opinions
(Section 3.2). Ultimately, we collect 20 truthful and
20 deceptive opinions for each of the 20 chosen ho-
tels (800 opinions total).
</bodyText>
<subsectionHeader confidence="0.999242">
3.1 Deceptive opinions via Mechanical Turk
</subsectionHeader>
<bodyText confidence="0.999176117647059">
Crowdsourcing services such as AMT have made
large-scale data annotation and collection efforts fi-
nancially affordable by granting anyone with ba-
sic programming skills access to a marketplace of
anonymous online workers (known as Turkers) will-
ing to complete small tasks.
To solicit gold-standard deceptive opinion spam
using AMT, we create a pool of 400 Human-
Intelligence Tasks (HITs) and allocate them evenly
across our 20 chosen hotels. To ensure that opin-
ions are written by unique authors, we allow only a
single submission per Turker. We also restrict our
task to Turkers who are located in the United States,
and who maintain an approval rating of at least 90%.
Turkers are allowed a maximum of 30 minutes to
work on the HIT, and are paid one US dollar for an
accepted submission.
Each HIT presents the Turker with the name and
website of a hotel. The HIT instructions ask the
Turker to assume that they work for the hotel’s mar-
keting department, and to pretend that their boss
wants them to write a fake review (as if they were
a customer) to be posted on a travel review website;
additionally, the review needs to sound realistic and
portray the hotel in a positive light. A disclaimer
8TripAdvisor utilizes a proprietary ranking system to assess
hotel popularity. We chose the 20 hotels with the greatest num-
ber of reviews, irrespective of the TripAdvisor ranking.
9It has been hypothesized that popular offerings are less
likely to become targets of deceptive opinion spam, since the
relative impact of the spam in such cases is small (Jindal and
Liu, 2008; Lim et al., 2010). By considering only the most
popular hotels, we hope to minimize the risk of mining opinion
spam and labeling it as truthful.
</bodyText>
<footnote confidence="0.940157">
10http://mturk.com
</footnote>
<page confidence="0.992315">
311
</page>
<table confidence="0.999586615384615">
Time spent t (minutes)
All submissions count: 400
tmin: 0.08, tmax: 29.78
f: 8.06, s: 6.32
Length f (words)
All submissions fmin: 25, fmax: 425
�f: 115.75, s: 61.30
Time spent t &lt; 1 count: 47
fmin: 39, fmax: 407
�f: 113.94, s: 66.24
Time spent t &gt; 1 count: 353
fmin: 25, fmax: 425
�f: 115.99, s: 60.71
</table>
<tableCaption confidence="0.986920333333333">
Table 1: Descriptive statistics for 400 deceptive opinion
spam submissions gathered using AMT. s corresponds to
the sample standard deviation.
</tableCaption>
<bodyText confidence="0.999774611111111">
indicates that any submission found to be of insuffi-
cient quality (e.g., written for the wrong hotel, unin-
telligible, unreasonably short,11 plagiarized,12 etc.)
will be rejected.
It took approximately 14 days to collect 400 sat-
isfactory deceptive opinions. Descriptive statistics
appear in Table 1. Submissions vary quite dramati-
cally both in length, and time spent on the task. Par-
ticularly, nearly 12% of the submissions were com-
pleted in under one minute. Surprisingly, an inde-
pendent two-tailed t-test between the mean length of
these submissions (�ft&lt;1) and the other submissions
(ft&gt;1) reveals no significant difference (p = 0.83).
We suspect that these “quick” users may have started
working prior to having formally accepted the HIT,
presumably to circumvent the imposed time limit.
Indeed, the quickest submission took just 5 seconds
and contained 114 words.
</bodyText>
<subsectionHeader confidence="0.999764">
3.2 Truthful opinions from TripAdvisor
</subsectionHeader>
<bodyText confidence="0.998543666666667">
For truthful opinions, we mine all 6,977 reviews
from the 20 most popular Chicago hotels on
TripAdvisor. From these we eliminate:
</bodyText>
<listItem confidence="0.9974465">
• 3,130 non-5-star reviews;
• 41 non-English reviews;13
• 75 reviews with fewer than 150 characters
since, by construction, deceptive opinions are
</listItem>
<footnote confidence="0.9061352">
11A submission is considered unreasonably short if it con-
tains fewer than 150 characters.
12Submissions are individually checked for plagiarism at
http://plagiarisma.net.
13Language is determined using http://tagthe.net.
</footnote>
<bodyText confidence="0.7646625">
at least 150 characters long (see footnote 11 in
Section 3.1);
</bodyText>
<listItem confidence="0.891160333333333">
• 1,607 reviews written by first-time authors—
new users who have not previously posted an
opinion on TripAdvisor—since these opinions
</listItem>
<bodyText confidence="0.95234075">
are more likely to contain opinion spam, which
would reduce the integrity of our truthful re-
view data (Wu et al., 2010).
Finally, we balance the number of truthful and
deceptive opinions by selecting 400 of the remain-
ing 2,124 truthful reviews, such that the document
lengths of the selected truthful reviews are similarly
distributed to those of the deceptive reviews. Work
by Serrano et al. (2009) suggests that a log-normal
distribution is appropriate for modeling document
lengths. Thus, for each of the 20 chosen hotels, we
select 20 truthful reviews from a log-normal (left-
truncated at 150 characters) distribution fit to the
lengths of the deceptive reviews.14 Combined with
the 400 deceptive reviews gathered in Section 3.1
this yields our final dataset of 800 reviews.
</bodyText>
<subsectionHeader confidence="0.999637">
3.3 Human performance
</subsectionHeader>
<bodyText confidence="0.999992954545455">
Assessing human deception detection performance
is important for several reasons. First, there are few
other baselines for our classification task; indeed, re-
lated studies (Jindal and Liu, 2008; Mihalcea and
Strapparava, 2009) have only considered a random
guess baseline. Second, assessing human perfor-
mance is necessary to validate the deceptive opin-
ions gathered in Section 3.1. If human performance
is low, then our deceptive opinions are convincing,
and therefore, deserving of further attention.
Our initial approach to assessing human perfor-
mance on this task was with Mechanical Turk. Un-
fortunately, we found that some Turkers selected
among the choices seemingly at random, presum-
ably to maximize their hourly earnings by obviating
the need to read the review. While a similar effect
has been observed previously (Akkaya et al., 2010),
there remains no universal solution.
Instead, we solicit the help of three volunteer un-
dergraduate university students to make judgments
on a subset of our data. This balanced subset, cor-
responding to the first fold of our cross-validation
</bodyText>
<footnote confidence="0.806753">
14We use the R package GAMLSS (Rigby and Stasinopoulos,
2005) to fit the left-truncated log-normal distribution.
</footnote>
<page confidence="0.97641">
312
</page>
<table confidence="0.999699428571429">
TRUTHFUL DECEPTIVE
Accuracy P R F P R F
HUMAN JUDGE 1 61.9% 57.9 87.5 69.7 74.4 36.3 48.7
JUDGE 2 56.9% 53.9 95.0 68.8 78.9 18.8 30.3
JUDGE 3 53.1% 52.3 70.0 59.9 54.7 36.3 43.6
META MAJORITY 58.1% 54.8 92.5 68.8 76.0 23.8 36.2
SKEPTIC 60.6% 60.8 60.0 60.4 60.5 61.3 60.9
</table>
<tableCaption confidence="0.996674">
Table 2: Performance of three human judges and two meta-judges on a subset of 160 opinions, corresponding to the
first fold of our cross-validation experiments in Section 5. Boldface indicates the largest value for each column.
</tableCaption>
<bodyText confidence="0.998265574468086">
experiments described in Section 5, contains all 40
reviews from each of four randomly chosen hotels.
Unlike the Turkers, our student volunteers are not
offered a monetary reward. Consequently, we con-
sider their judgements to be more honest than those
obtained via AMT.
Additionally, to test the extent to which the in-
dividual human judges are biased, we evaluate the
performance of two virtual meta-judges. Specifi-
cally, the MAJORITY meta-judge predicts “decep-
tive” when at least two out of three human judges
believe the review to be deceptive, and the SKEP-
TIC meta-judge predicts “deceptive” when any hu-
man judge believes the review to be deceptive.
Human and meta-judge performance is given in
Table 2. It is clear from the results that human
judges are not particularly effective at this task. In-
deed, a two-tailed binomial test fails to reject the
null hypothesis that JUDGE 2 and JUDGE 3 per-
form at-chance (p = 0.003, 0.10, 0.48 for the three
judges, respectively). Furthermore, all three judges
suffer from truth-bias (Vrij, 2008), a common find-
ing in deception detection research in which hu-
man judges are more likely to classify an opinion
as truthful than deceptive. In fact, JUDGE 2 clas-
sified fewer than 12% of the opinions as decep-
tive! Interestingly, this bias is effectively smoothed
by the SKEPTIC meta-judge, which produces nearly
perfectly class-balanced predictions. A subsequent
reevaluation of human performance on this task sug-
gests that the truth-bias can be reduced if judges
are given the class-proportions in advance, although
such prior knowledge is unrealistic; and ultimately,
performance remains similar to that of Table 2.
Inter-annotator agreement among the three
judges, computed using Fleiss’ kappa, is 0.11.
While there is no precise rule for interpreting
kappa scores, Landis and Koch (1977) suggest
that scores in the range (0.00, 0.20] correspond
to “slight agreement” between annotators. The
largest pairwise Cohen’s kappa is 0.12, between
JUDGE 2 and JUDGE 3—a value far below generally
accepted pairwise agreement levels. We suspect
that agreement among our human judges is so
low precisely because humans are poor judges of
deception (Vrij, 2008), and therefore they perform
nearly at-chance respective to one another.
</bodyText>
<sectionHeader confidence="0.9897855" genericHeader="method">
4 Automated Approaches to Deceptive
Opinion Spam Detection
</sectionHeader>
<bodyText confidence="0.999952">
We consider three automated approaches to detect-
ing deceptive opinion spam, each of which utilizes
classifiers (described in Section 4.4) trained on the
dataset of Section 3. The features employed by each
strategy are outlined here.
</bodyText>
<subsectionHeader confidence="0.972971">
4.1 Genre identification
</subsectionHeader>
<bodyText confidence="0.999949272727273">
Work in computational linguistics has shown that
the frequency distribution of part-of-speech (POS)
tags in a text is often dependent on the genre of the
text (Biber et al., 1999; Rayson et al., 2001). In our
genre identification approach to deceptive opinion
spam detection, we test if such a relationship exists
for truthful and deceptive reviews by constructing,
for each review, features based on the frequencies of
each POS tag.15 These features are also intended to
provide a good baseline with which to compare our
other automated approaches.
</bodyText>
<subsectionHeader confidence="0.957386">
4.2 Psycholinguistic deception detection
</subsectionHeader>
<bodyText confidence="0.9938515">
The Linguistic Inquiry and Word Count (LIWC)
software (Pennebaker et al., 2007) is a popular au-
tomated text analysis tool used widely in the so-
cial sciences. It has been used to detect personality
</bodyText>
<footnote confidence="0.532056">
15We use the Stanford Parser (Klein and Manning, 2003) to
obtain the relative POS frequencies.
</footnote>
<page confidence="0.99805">
313
</page>
<bodyText confidence="0.999656833333333">
traits (Mairesse et al., 2007), to study tutoring dy-
namics (Cade et al., 2010), and, most relevantly, to
analyze deception (Hancock et al., 2008; Mihalcea
and Strapparava, 2009; Vrij et al., 2007).
While LIWC does not include a text classifier, we
can create one with features derived from the LIWC
output. In particular, LIWC counts and groups
the number of instances of nearly 4,500 keywords
into 80 psychologically meaningful dimensions. We
construct one feature for each of the 80 LIWC di-
mensions, which can be summarized broadly under
the following four categories:
</bodyText>
<listItem confidence="0.986511363636363">
1. Linguistic processes: Functional aspects of text
(e.g., the average number of words per sen-
tence, the rate of misspelling, swearing, etc.)
2. Psychological processes: Includes all social,
emotional, cognitive, perceptual and biological
processes, as well as anything related to time or
space.
3. Personal concerns: Any references to work,
leisure, money, religion, etc.
4. Spoken categories: Primarily filler and agree-
ment words.
</listItem>
<bodyText confidence="0.9833885">
While other features have been considered in past
deception detection work, notably those of Zhou et
al. (2004), early experiments found LIWC features
to perform best. Indeed, the LIWC2007 software
used in our experiments subsumes most of the fea-
tures introduced in other work. Thus, we focus our
psycholinguistic approach to deception detection on
LIWC-based features.
</bodyText>
<subsectionHeader confidence="0.998287">
4.3 Text categorization
</subsectionHeader>
<bodyText confidence="0.999922444444444">
In contrast to the other strategies just discussed,
our text categorization approach to deception de-
tection allows us to model both content and con-
text with n-gram features. Specifically, we consider
the following three n-gram feature sets, with the
corresponding features lowercased and unstemmed:
UNIGRAMS, BIGRAMS+, TRIGRAMS+, where the
superscript + indicates that the feature set subsumes
the preceding feature set.
</bodyText>
<subsectionHeader confidence="0.985206">
4.4 Classifiers
</subsectionHeader>
<bodyText confidence="0.999723">
Features from the three approaches just introduced
are used to train Naive Bayes and Support Vector
Machine classifiers, both of which have performed
well in related work (Jindal and Liu, 2008; Mihalcea
and Strapparava, 2009; Zhou et al., 2008).
For a document x, with label y, the Naive Bayes
(NB) classifier gives us the following decision rule:
</bodyText>
<equation confidence="0.8716215">
y� = arg max Pr(y = c) · Pr(x  |y = c) (1)
c
</equation>
<bodyText confidence="0.9980735">
When the class prior is uniform, for example
when the classes are balanced (as in our case), (1)
can be simplified to the maximum likelihood classi-
fier (Peng and Schuurmans, 2003):
</bodyText>
<equation confidence="0.975112">
Pr(x  |y = c) (2)
</equation>
<bodyText confidence="0.997776277777778">
Under (2), both the NB classifier used by Mihal-
cea and Strapparava (2009) and the language model
classifier used by Zhou et al. (2008) are equivalent.
Thus, following Zhou et al. (2008), we use the SRI
Language Modeling Toolkit (Stolcke, 2002) to esti-
mate individual language models, Pr(x  |y = c),
for truthful and deceptive opinions. We consider
all three n-gram feature sets, namely UNIGRAMS,
BIGRAMS+, and TRIGRAMS+, with corresponding
language models smoothed using the interpolated
Kneser-Ney method (Chen and Goodman, 1996).
We also train Support Vector Machine (SVM)
classifiers, which find a high-dimensional separating
hyperplane between two groups of data. To simplify
feature analysis in Section 5, we restrict our evalu-
ation to linear SVMs, which learn a weight vector
w and bias term b, such that a document x can be
classified by:
</bodyText>
<equation confidence="0.992657">
y� = sign(w · x + b) (3)
</equation>
<bodyText confidence="0.999929909090909">
We use SVM&amp;quot;ght (Joachims, 1999) to train our
linear SVM models on all three approaches and
feature sets described above, namely POS, LIWC,
UNIGRAMS, BIGRAMS+, and TRIGRAMS+. We also
evaluate every combination of these features, but
for brevity include only LIWC+BIGRAMS+, which
performs best. Following standard practice, doc-
ument vectors are normalized to unit-length. For
LIWC+BIGRAMS+, we unit-length normalize LIWC
and BIGRAMS+ features individually before com-
bining them.
</bodyText>
<equation confidence="0.9672145">
y� = arg max
c
</equation>
<page confidence="0.996701">
314
</page>
<table confidence="0.999764894736842">
TRUTHFUL DECEPTIVE
Approach Features Accuracy P R F P R F
GENRE IDENTIFICATION POSSVM 73.0% 75.3 68.5 71.7 71.1 77.5 74.2
PSYCHOLINGUISTIC LIWCSVM 76.8% 77.2 76.0 76.6 76.4 77.5 76.9
DECEPTION DETECTION
TEXT CATEGORIZATION UNIGRAMSSVM 88.4% 89.9 86.5 88.2 87.0 90.3 88.6
BIGRAMS+ 89.6% 90.1 89.0 89.6 89.1 90.3 89.7
SVM
LIWC+BIGRAMS+SVM 89.8% 89.8 89.8 89.8 89.8 89.8 89.8
TRIGRAMS+ 89.0% 89.0 89.0 89.0 89.0 89.0 89.0
SVM
UNIGRAMSNB 88.4% 92.5 83.5 87.8 85.0 93.3 88.9
BIGRAMS+ 88.9% 89.8 87.8 88.7 88.0 90.0 89.0
NB
TRIGRAMS+ 87.6% 87.7 87.5 87.6 87.5 87.8 87.6
NB
HUMAN / META JUDGE 1 61.9% 57.9 87.5 69.7 74.4 36.3 48.7
JUDGE 2 56.9% 53.9 95.0 68.8 78.9 18.8 30.3
SKEPTIC 60.6% 60.8 60.0 60.4 60.5 61.3 60.9
</table>
<tableCaption confidence="0.9534456">
Table 3: Automated classifier performance for three approaches based on nested 5-fold cross-validation experiments.
Reported precision, recall and F-score are computed using a micro-average, i.e., from the aggregate true positive, false
positive and false negative rates, as suggested by Forman and Scholz (2009). Human performance is repeated here for
JUDGE 1, JUDGE 2 and the SKEPTIC meta-judge, although they cannot be directly compared since the 160-opinion
subset on which they are assessed only corresponds to the first cross-validation fold.
</tableCaption>
<sectionHeader confidence="0.999607" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.995125660377359">
The deception detection strategies described in Sec-
tion 4 are evaluated using a 5-fold nested cross-
validation (CV) procedure (Quadrianto et al., 2009),
where model parameters are selected for each test
fold based on standard CV experiments on the train-
ing folds. Folds are selected so that each contains all
reviews from four hotels; thus, learned models are
always evaluated on reviews from unseen hotels.
Results appear in Table 3. We observe that auto-
mated classifiers outperform human judges for every
metric, except truthful recall where JUDGE 2 per-
forms best.16 However, this is expected given that
untrained humans often focus on unreliable cues to
deception (Vrij, 2008). For example, one study ex-
amining deception in online dating found that hu-
mans perform at-chance detecting deceptive pro-
files because they rely on text-based cues that are
unrelated to deception, such as second-person pro-
nouns (Toma and Hancock, In Press).
Among the automated classifiers, baseline per-
formance is given by the simple genre identifica-
tion approach (POSSVM) proposed in Section 4.1.
Surprisingly, we find that even this simple auto-
16As mentioned in Section 3.3, JUDGE 2 classified fewer than
12% of opinions as deceptive. While achieving 95% truthful re-
call, this judge’s corresponding precision was not significantly
better than chance (two-tailed binomial p = 0.4).
mated classifier outperforms most human judges
(one-tailed sign test p = 0.06, 0.01, 0.001 for the
three judges, respectively, on the first fold). This
result is best explained by theories of reality mon-
itoring (Johnson and Raye, 1981), which suggest
that truthful and deceptive opinions might be clas-
sified into informative and imaginative genres, re-
spectively. Work by Rayson et al. (2001) has found
strong distributional differences between informa-
tive and imaginative writing, namely that the former
typically consists of more nouns, adjectives, prepo-
sitions, determiners, and coordinating conjunctions,
while the latter consists of more verbs,17 adverbs,18
pronouns, and pre-determiners. Indeed, we find that
the weights learned by POSSVM (found in Table 4)
are largely in agreement with these findings, no-
tably except for adjective and adverb superlatives,
the latter of which was found to be an exception by
Rayson et al. (2001). However, that deceptive opin-
ions contain more superlatives is not unexpected,
since deceptive writing (but not necessarily imagi-
native writing in general) often contains exaggerated
language (Buller and Burgoon, 1996; Hancock et al.,
2008).
Both remaining automated approaches to detect-
ing deceptive opinion spam outperform the simple
</bodyText>
<footnote confidence="0.9049235">
17Past participle verbs were an exception.
18Superlative adverbs were an exception.
</footnote>
<page confidence="0.994592">
315
</page>
<table confidence="0.999809705882353">
TRUTHFUL/INFORMATIVE DECEPTIVE/IMAGINATIVE
Category Variant Weight Category Variant Weight
NOUNS Singular 0.008 VERBS Base -0.057
Plural 0.002 Past tense 0.041
Proper, singular -0.041 Present participle -0.089
Proper, plural 0.091 Singular, present -0.031
ADJECTIVES General 0.002 Third person 0.026
singular, present
Comparative 0.058
Superlative -0.164 Modal -0.063
PREPOSITIONS General 0.064 ADVERBS General 0.001
0.009
DETERMINERS General Comparative -0.035
COORD. CONJ. General 0.094 PRONOUNS Personal -0.098
0.053
VERBS Past participle Possessive -0.303
ADVERBS Superlative -0.094 PRE-DETERMINERS General 0.017
</table>
<tableCaption confidence="0.96367775">
Table 4: Average feature weights learned by POSSVM. Based on work by Rayson et al. (2001), we expect weights on
the left to be positive (predictive of truthful opinions), and weights on the right to be negative (predictive of deceptive
opinions). Boldface entries are at odds with these expectations. We report average feature weights of unit-normalized
weight vectors, rather than raw weights vectors, to account for potential differences in magnitude between the folds.
</tableCaption>
<bodyText confidence="0.999932655172414">
genre identification baseline just discussed. Specifi-
cally, the psycholinguistic approach (LIWCSVM) pro-
posed in Section 4.2 performs 3.8% more accurately
(one-tailed sign test p = 0.02), and the standard text
categorization approach proposed in Section 4.3 per-
forms between 14.6% and 16.6% more accurately.
However, best performance overall is achieved by
combining features from these two approaches. Par-
ticularly, the combined model LIWC+BIGRAMS+SVM
is 89.8% accurate at detecting deceptive opinion
spam.19
Surprisingly, models trained only on
UNIGRAMS—the simplest n-gram feature set—
outperform all non–text-categorization approaches,
and models trained on BIGRAMS+ perform even
better (one-tailed sign test p = 0.07). This suggests
that a universal set of keyword-based deception
cues (e.g., LIWC) is not the best approach to de-
tecting deception, and a context-sensitive approach
(e.g., BIGRAMS+) might be necessary to achieve
state-of-the-art deception detection performance.
To better understand the models learned by these
automated approaches, we report in Table 5 the top
15 highest weighted features for each class (truthful
and deceptive) as learned by LIWC+BIGRAMS+SVM
and LIWCSVM. In agreement with theories of reality
monitoring (Johnson and Raye, 1981), we observe
that truthful opinions tend to include more sensorial
and concrete language than deceptive opinions; in
</bodyText>
<footnote confidence="0.965163">
19The result is not significantly better than BIGRAMS+SVM.
</footnote>
<table confidence="0.999424888888889">
LIWC+BIGRAMS+ LIWCSVM
SVM
TRUTHFUL DECEPTIVE TRUTHFUL DECEPTIVE
- chicago hear i
... my number family
on hotel allpunct perspron
location , and negemo see
) luxury dash pronoun
allpunctLIWC experience exclusive leisure
floor hilton we exclampunct
( business sexual sixletters
the hotel vacation period posemo
bathroom i otherpunct comma
small spa space cause
helpful looking human auxverb
$ while past future
hotel. husband inhibition perceptual
other my husband assent feel
</table>
<tableCaption confidence="0.996201">
Table 5: Top 15 highest weighted truthful and deceptive
</tableCaption>
<bodyText confidence="0.899655071428571">
features learned by LIWC+BIGRAMSVM and LIWCSVM.
Ambiguous features are subscripted to indicate the source
of the feature. LIWC features correspond to groups
of keywords as explained in Section 4.2; more details
about LIWC and the LIWC categories are available at
http://liwc.net.
particular, truthful opinions are more specific about
spatial configurations (e.g., small, bathroom, on, lo-
cation). This finding is also supported by recent
work by Vrij et al. (2009) suggesting that liars have
considerable difficultly encoding spatial information
into their lies. Accordingly, we observe an increased
focus in deceptive opinions on aspects external to
the hotel being reviewed (e.g., husband, business,
</bodyText>
<page confidence="0.998146">
316
</page>
<bodyText confidence="0.999676333333333">
vacation).
We also acknowledge several findings that, on the
surface, are in contrast to previous psycholinguistic
studies of deception (Hancock et al., 2008; Newman
et al., 2003). For instance, while deception is often
associated with negative emotion terms, our decep-
tive reviews have more positive and fewer negative
emotion terms. This pattern makes sense when one
considers the goal of our deceivers, namely to create
a positive review (Buller and Burgoon, 1996).
Deception has also previously been associated
with decreased usage of first person singular, an ef-
fect attributed to psychological distancing (Newman
et al., 2003). In contrast, we find increased first
person singular to be among the largest indicators
of deception, which we speculate is due to our de-
ceivers attempting to enhance the credibility of their
reviews by emphasizing their own presence in the
review. Additional work is required, but these find-
ings further suggest the importance of moving be-
yond a universal set of deceptive language features
(e.g., LIWC) by considering both the contextual (e.g.,
BIGRAMS+) and motivational parameters underly-
ing a deception as well.
</bodyText>
<sectionHeader confidence="0.996167" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999967647058823">
In this work we have developed the first large-scale
dataset containing gold-standard deceptive opinion
spam. With it, we have shown that the detection
of deceptive opinion spam is well beyond the ca-
pabilities of human judges, most of whom perform
roughly at-chance. Accordingly, we have introduced
three automated approaches to deceptive opinion
spam detection, based on insights coming from re-
search in computational linguistics and psychology.
We find that while standard n-gram–based text cate-
gorization is the best individual detection approach,
a combination approach using psycholinguistically-
motivated features and n-gram features can perform
slightly better.
Finally, we have made several theoretical con-
tributions. Specifically, our findings suggest the
importance of considering both the context (e.g.,
BIGRAMS+) and motivations underlying a decep-
tion, rather than strictly adhering to a universal set
of deception cues (e.g., LIWC). We have also pre-
sented results based on the feature weights learned
by our classifiers that illustrate the difficulties faced
by liars in encoding spatial information. Lastly, we
have discovered a plausible relationship between de-
ceptive opinion spam and imaginative writing, based
on POS distributional similarities.
Possible directions for future work include an ex-
tended evaluation of the methods proposed in this
work to both negative opinions, as well as opinions
coming from other domains. Many additional ap-
proaches to detecting deceptive opinion spam are
also possible, and a focus on approaches with high
deceptive precision might be useful for production
environments.
</bodyText>
<sectionHeader confidence="0.998564" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.996894230769231">
This work was supported in part by National
Science Foundation Grants BCS-0624277, BCS-
0904822, HSD-0624267, IIS-0968450, and NSCC-
0904822, as well as a gift from Google, and the
Jack Kent Cooke Foundation. We also thank, al-
phabetically, Rachel Boochever, Cristian Danescu-
Niculescu-Mizil, Alicia Granstein, Ulrike Gretzel,
Danielle Kirshenblat, Lillian Lee, Bin Lu, Jack
Newton, Melissa Sackler, Mark Thomas, and Angie
Yoo, as well as members of the Cornell NLP sem-
inar group and the ACL reviewers for their insight-
ful comments, suggestions and advice on various as-
pects of this work.
</bodyText>
<sectionHeader confidence="0.998599" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994103833333333">
C. Akkaya, A. Conrad, J. Wiebe, and R. Mihalcea. 2010.
Amazon mechanical turk for subjectivity word sense
disambiguation. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazons Mechanical Turk, Los Angeles,
pages 195–203.
D. Biber, S. Johansson, G. Leech, S. Conrad, E. Finegan,
and R. Quirk. 1999. Longman grammar of spoken and
written English, volume 2. MIT Press.
C.F. Bond and B.M. DePaulo. 2006. Accuracy of de-
ception judgments. Personality and Social Psychology
Review, 10(3):214.
D.B. Buller and J.K. Burgoon. 1996. Interpersonal
deception theory. Communication Theory, 6(3):203–
242.
W.L. Cade, B.A. Lehman, and A. Olney. 2010. An ex-
ploration of off topic conversation. In Human Lan-
guage Technologies: The 2010 Annual Conference of
</reference>
<page confidence="0.992792">
317
</page>
<reference confidence="0.998073153846154">
the North American Chapter of the Association for
Computational Linguistics, pages 669–672. Associa-
tion for Computational Linguistics.
S.F. Chen and J. Goodman. 1996. An empirical study of
smoothing techniques for language modeling. In Pro-
ceedings of the 34th annual meeting on Association
for Computational Linguistics, pages 310–318. Asso-
ciation for Computational Linguistics.
C. Danescu-Niculescu-Mizil, G. Kossinets, J. Kleinberg,
and L. Lee. 2009. How opinions are received by on-
line communities: a case study on amazon.com help-
fulness votes. In Proceedings of the 18th international
conference on World wide web, pages 141–150. ACM.
H. Drucker, D. Wu, and V.N. Vapnik. 2002. Support
vector machines for spam categorization. Neural Net-
works, IEEE Transactions on, 10(5):1048–1054.
G. Forman and M. Scholz. 2009. Apples-to-Apples in
Cross-Validation Studies: Pitfalls in Classifier Perfor-
mance Measurement. ACM SIGKDD Explorations,
12(1):49–57.
Z. Gy¨ongyi, H. Garcia-Molina, and J. Pedersen. 2004.
Combating web spam with trustrank. In Proceedings
of the Thirtieth international conference on Very large
data bases-Volume 30, pages 576–587. VLDB Endow-
ment.
J.T. Hancock, L.E. Curry, S. Goorha, and M. Woodworth.
2008. On lying and being lied to: A linguistic anal-
ysis of deception in computer-mediated communica-
tion. Discourse Processes, 45(1):1–23.
J. Jansen. 2010. Online product research. Pew Internet
&amp; American Life Project Report.
N. Jindal and B. Liu. 2008. Opinion spam and analysis.
In Proceedings of the international conference on Web
search and web data mining, pages 219–230. ACM.
T. Joachims. 1998. Text categorization with support vec-
tor machines: Learning with many relevant features.
Machine Learning: ECML-98, pages 137–142.
T. Joachims. 1999. Making large-scale support vec-
tor machine learning practical. In Advances in kernel
methods, page 184. MIT Press.
M.K. Johnson and C.L. Raye. 1981. Reality monitoring.
Psychological Review, 88(1):67–85.
S.M. Kim, P. Pantel, T. Chklovski, and M. Pennacchiotti.
2006. Automatically assessing review helpfulness.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 423–
430. Association for Computational Linguistics.
D. Klein and C.D. Manning. 2003. Accurate unlexical-
ized parsing. In Proceedings of the 41st Annual Meet-
ing on Association for Computational Linguistics-
Volume 1, pages 423–430. Association for Computa-
tional Linguistics.
J.R. Landis and G.G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33(1):159.
E.P. Lim, V.A. Nguyen, N. Jindal, B. Liu, and H.W.
Lauw. 2010. Detecting product review spammers us-
ing rating behaviors. In Proceedings of the 19th ACM
international conference on Information and knowl-
edge management, pages 939–948. ACM.
S.W. Litvin, R.E. Goldsmith, and B. Pan. 2008. Elec-
tronic word-of-mouth in hospitality and tourism man-
agement. Tourism management, 29(3):458–468.
F. Mairesse, M.A. Walker, M.R. Mehl, and R.K. Moore.
2007. Using linguistic cues for the automatic recogni-
tion of personality in conversation and text. Journal of
Artificial Intelligence Research, 30(1):457–500.
R. Mihalcea and C. Strapparava. 2009. The lie detector:
Explorations in the automatic recognition of deceptive
language. In Proceedings of the ACL-IJCNLP 2009
Conference Short Papers, pages 309–312. Association
for Computational Linguistics.
M.L. Newman, J.W. Pennebaker, D.S. Berry, and J.M.
Richards. 2003. Lying words: Predicting deception
from linguistic styles. Personality and Social Psychol-
ogy Bulletin, 29(5):665.
A. Ntoulas, M. Najork, M. Manasse, and D. Fetterly.
2006. Detecting spam web pages through content
analysis. In Proceedings of the 15th international con-
ference on World Wide Web, pages 83–92. ACM.
M.P. O’Mahony and B. Smyth. 2009. Learning to rec-
ommend helpful hotel reviews. In Proceedings of
the third ACM conference on Recommender systems,
pages 305–308. ACM.
F. Peng and D. Schuurmans. 2003. Combining naive
Bayes and n-gram language models for text classifica-
tion. Advances in Information Retrieval, pages 547–
547.
J.W. Pennebaker, C.K. Chung, M. Ireland, A. Gonzales,
and R.J. Booth. 2007. The development and psycho-
metric properties of LIWC2007. Austin, TX, LIWC.
Net.
N. Quadrianto, A.J. Smola, T.S. Caetano, and Q.V.
Le. 2009. Estimating labels from label proportions.
The Journal of Machine Learning Research, 10:2349–
2374.
P. Rayson, A. Wilson, and G. Leech. 2001. Grammatical
word class variation within the British National Cor-
pus sampler. Language and Computers, 36(1):295–
306.
R.A. Rigby and D.M. Stasinopoulos. 2005. Generalized
additive models for location, scale and shape. Jour-
nal of the Royal Statistical Society: Series C (Applied
Statistics), 54(3):507–554.
</reference>
<page confidence="0.98633">
318
</page>
<reference confidence="0.999723040816326">
F. Sebastiani. 2002. Machine learning in automated
text categorization. ACM computing surveys (CSUR),
34(1):1–47.
M. ´A. Serrano, A. Flammini, and F. Menczer. 2009.
Modeling statistical properties of written text. PloS
one, 4(4):5372.
A. Stolcke. 2002. SRILM-an extensible language mod-
eling toolkit. In Seventh International Conference on
Spoken Language Processing, volume 3, pages 901–
904. Citeseer.
C. Toma and J.T. Hancock. In Press. What Lies Beneath:
The Linguistic Traces of Deception in Online Dating
Profiles. Journal of Communication.
A. Vrij, S. Mann, S. Kristen, and R.P. Fisher. 2007. Cues
to deception and ability to detect lies as a function
of police interview styles. Law and human behavior,
31(5):499–518.
A. Vrij, S. Leal, P.A. Granhag, S. Mann, R.P. Fisher,
J. Hillman, and K. Sperry. 2009. Outsmarting the
liars: The benefit of asking unanticipated questions.
Law and human behavior, 33(2):159–166.
A. Vrij. 2008. Detecting lies and deceit: Pitfalls and
opportunities. Wiley-Interscience.
W. Weerkamp and M. De Rijke. 2008. Credibility im-
proves topical blog post retrieval. ACL-08: HLT,
pages 923–931.
M. Weimer, I. Gurevych, and M. M¨uhlh¨auser. 2007. Au-
tomatically assessing the post quality in online discus-
sions on software. In Proceedings of the 45th An-
nual Meeting of the ACL on Interactive Poster and
Demonstration Sessions, pages 125–128. Association
for Computational Linguistics.
G. Wu, D. Greene, B. Smyth, and P. Cunningham. 2010.
Distortion as a validation criterion in the identification
of suspicious reviews. Technical report, UCD-CSI-
2010-04, University College Dublin.
K.H. Yoo and U. Gretzel. 2009. Comparison of De-
ceptive and Truthful Travel Reviews. Information and
Communication Technologies in Tourism 2009, pages
37–47.
L. Zhou, J.K. Burgoon, D.P. Twitchell, T. Qin, and J.F.
Nunamaker Jr. 2004. A comparison of classifica-
tion methods for predicting deception in computer-
mediated communication. Journal of Management In-
formation Systems, 20(4):139–166.
L. Zhou, Y. Shi, and D. Zhang. 2008. A Statistical Lan-
guage Modeling Approach to Online Deception De-
tection. IEEE Transactions on Knowledge and Data
Engineering, 20(8):1077–1081.
</reference>
<page confidence="0.999349">
319
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.566512">
<title confidence="0.999967">Finding Deceptive Opinion Spam by Any Stretch of the Imagination</title>
<author confidence="0.999923">T Jeffrey</author>
<affiliation confidence="0.981039">Department of Cornell</affiliation>
<address confidence="0.778194">Ithaca, NY</address>
<email confidence="0.998302">jth34@cornell.edu</email>
<author confidence="0.995158">Myle Ott Yejin Choi Claire Cardie</author>
<affiliation confidence="0.962777">Department of Computer Cornell</affiliation>
<address confidence="0.879766">Ithaca, NY</address>
<email confidence="0.999494">myleott@cs.cornell.edu</email>
<email confidence="0.999494">ychoi@cs.cornell.edu</email>
<email confidence="0.999494">cardie@cs.cornell.edu</email>
<abstract confidence="0.997370666666667">Consumers increasingly rate, review and research products online (Jansen, 2010; Litvin et al., 2008). Consequently, websites containing consumer reviews are becoming tarof While recent work has focused primarily on manually identifiable instances of opinion spam, in this work study opinion opinions that have been deliberately written to sound authentic. Integrating work from psychology and computational linguistics, we develop and compare three approaches to detecting deceptive opinion spam, and ultimately develop a classifier that is nearly 90% accurate our spam dataset. Based on feature analysis of our learned models, we additionally make several theoretical contributions, including revealing a relationship between deceptive opinions and imaginative writing.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Akkaya</author>
<author>A Conrad</author>
<author>J Wiebe</author>
<author>R Mihalcea</author>
</authors>
<title>Amazon mechanical turk for subjectivity word sense disambiguation.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazons Mechanical Turk,</booktitle>
<pages>195--203</pages>
<location>Los Angeles,</location>
<contexts>
<context position="16412" citStr="Akkaya et al., 2010" startWordPosition="2556" endWordPosition="2559">09) have only considered a random guess baseline. Second, assessing human performance is necessary to validate the deceptive opinions gathered in Section 3.1. If human performance is low, then our deceptive opinions are convincing, and therefore, deserving of further attention. Our initial approach to assessing human performance on this task was with Mechanical Turk. Unfortunately, we found that some Turkers selected among the choices seemingly at random, presumably to maximize their hourly earnings by obviating the need to read the review. While a similar effect has been observed previously (Akkaya et al., 2010), there remains no universal solution. Instead, we solicit the help of three volunteer undergraduate university students to make judgments on a subset of our data. This balanced subset, corresponding to the first fold of our cross-validation 14We use the R package GAMLSS (Rigby and Stasinopoulos, 2005) to fit the left-truncated log-normal distribution. 312 TRUTHFUL DECEPTIVE Accuracy P R F P R F HUMAN JUDGE 1 61.9% 57.9 87.5 69.7 74.4 36.3 48.7 JUDGE 2 56.9% 53.9 95.0 68.8 78.9 18.8 30.3 JUDGE 3 53.1% 52.3 70.0 59.9 54.7 36.3 43.6 META MAJORITY 58.1% 54.8 92.5 68.8 76.0 23.8 36.2 SKEPTIC 60.6%</context>
</contexts>
<marker>Akkaya, Conrad, Wiebe, Mihalcea, 2010</marker>
<rawString>C. Akkaya, A. Conrad, J. Wiebe, and R. Mihalcea. 2010. Amazon mechanical turk for subjectivity word sense disambiguation. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazons Mechanical Turk, Los Angeles, pages 195–203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Biber</author>
<author>S Johansson</author>
<author>G Leech</author>
<author>S Conrad</author>
<author>E Finegan</author>
<author>R Quirk</author>
</authors>
<title>Longman grammar of spoken and written English,</title>
<date>1999</date>
<volume>2</volume>
<publisher>MIT Press.</publisher>
<contexts>
<context position="5065" citStr="Biber et al., 1999" startWordPosition="777" endWordPosition="780">iew the task as: (a) a standard text categorization task, in which we use n-gram–based classifiers to label opinions as either deceptive or truthful (Joachims, 1998; Sebastiani, 2002); (b) an instance of psycholinguistic deception detection, in which we expect deceptive statements to exemplify the psychological effects of lying, such as increased negative emotion and psychological distancing (Hancock et al., 2008; Newman et al., 2003); and, (c) a problem of genre identification, in which we view deceptive and truthful writing as sub-genres of imaginative and informative writing, respectively (Biber et al., 1999; Rayson et al., 2001). We compare the performance of each approach on our novel dataset. Particularly, we find that machine learning classifiers trained on features traditionally employed in (a) psychological studies of deception and (b) genre identification are both outperformed at statistically significant levels by ngram–based text categorization techniques. Notably, a combined classifier with both n-gram and psychological deception features achieves nearly 90% cross-validated accuracy on this task. In contrast, we find deceptive opinion spam detection to be well beyond the capabilities of</context>
<context position="20028" citStr="Biber et al., 1999" startWordPosition="3137" endWordPosition="3140">e humans are poor judges of deception (Vrij, 2008), and therefore they perform nearly at-chance respective to one another. 4 Automated Approaches to Deceptive Opinion Spam Detection We consider three automated approaches to detecting deceptive opinion spam, each of which utilizes classifiers (described in Section 4.4) trained on the dataset of Section 3. The features employed by each strategy are outlined here. 4.1 Genre identification Work in computational linguistics has shown that the frequency distribution of part-of-speech (POS) tags in a text is often dependent on the genre of the text (Biber et al., 1999; Rayson et al., 2001). In our genre identification approach to deceptive opinion spam detection, we test if such a relationship exists for truthful and deceptive reviews by constructing, for each review, features based on the frequencies of each POS tag.15 These features are also intended to provide a good baseline with which to compare our other automated approaches. 4.2 Psycholinguistic deception detection The Linguistic Inquiry and Word Count (LIWC) software (Pennebaker et al., 2007) is a popular automated text analysis tool used widely in the social sciences. It has been used to detect pe</context>
</contexts>
<marker>Biber, Johansson, Leech, Conrad, Finegan, Quirk, 1999</marker>
<rawString>D. Biber, S. Johansson, G. Leech, S. Conrad, E. Finegan, and R. Quirk. 1999. Longman grammar of spoken and written English, volume 2. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C F Bond</author>
<author>B M DePaulo</author>
</authors>
<title>Accuracy of deception judgments.</title>
<date>2006</date>
<journal>Personality and Social Psychology Review,</journal>
<volume>10</volume>
<issue>3</issue>
<contexts>
<context position="5825" citStr="Bond and DePaulo, 2006" startWordPosition="888" endWordPosition="891">ssifiers trained on features traditionally employed in (a) psychological studies of deception and (b) genre identification are both outperformed at statistically significant levels by ngram–based text categorization techniques. Notably, a combined classifier with both n-gram and psychological deception features achieves nearly 90% cross-validated accuracy on this task. In contrast, we find deceptive opinion spam detection to be well beyond the capabilities of most human judges, who perform roughly at-chance—a finding that is consistent with decades of traditional deception detection research (Bond and DePaulo, 2006). 5The second example review is deceptive opinion spam. 6Available by request at: http://www.cs.cornell. edu/˜myleott/op_spam Additionally, we make several theoretical contributions based on an examination of the feature weights learned by our machine learning classifiers. Specifically, we shed light on an ongoing debate in the deception literature regarding the importance of considering the context and motivation of a deception, rather than simply identifying a universal set of deception cues. We also present findings that are consistent with recent work highlighting the difficulties that lia</context>
</contexts>
<marker>Bond, DePaulo, 2006</marker>
<rawString>C.F. Bond and B.M. DePaulo. 2006. Accuracy of deception judgments. Personality and Social Psychology Review, 10(3):214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D B Buller</author>
<author>J K Burgoon</author>
</authors>
<title>Interpersonal deception theory.</title>
<date>1996</date>
<journal>Communication Theory,</journal>
<volume>6</volume>
<issue>3</issue>
<pages>242</pages>
<contexts>
<context position="28329" citStr="Buller and Burgoon, 1996" startWordPosition="4443" endWordPosition="4446">e nouns, adjectives, prepositions, determiners, and coordinating conjunctions, while the latter consists of more verbs,17 adverbs,18 pronouns, and pre-determiners. Indeed, we find that the weights learned by POSSVM (found in Table 4) are largely in agreement with these findings, notably except for adjective and adverb superlatives, the latter of which was found to be an exception by Rayson et al. (2001). However, that deceptive opinions contain more superlatives is not unexpected, since deceptive writing (but not necessarily imaginative writing in general) often contains exaggerated language (Buller and Burgoon, 1996; Hancock et al., 2008). Both remaining automated approaches to detecting deceptive opinion spam outperform the simple 17Past participle verbs were an exception. 18Superlative adverbs were an exception. 315 TRUTHFUL/INFORMATIVE DECEPTIVE/IMAGINATIVE Category Variant Weight Category Variant Weight NOUNS Singular 0.008 VERBS Base -0.057 Plural 0.002 Past tense 0.041 Proper, singular -0.041 Present participle -0.089 Proper, plural 0.091 Singular, present -0.031 ADJECTIVES General 0.002 Third person 0.026 singular, present Comparative 0.058 Superlative -0.164 Modal -0.063 PREPOSITIONS General 0.06</context>
<context position="32772" citStr="Buller and Burgoon, 1996" startWordPosition="5077" endWordPosition="5080">to their lies. Accordingly, we observe an increased focus in deceptive opinions on aspects external to the hotel being reviewed (e.g., husband, business, 316 vacation). We also acknowledge several findings that, on the surface, are in contrast to previous psycholinguistic studies of deception (Hancock et al., 2008; Newman et al., 2003). For instance, while deception is often associated with negative emotion terms, our deceptive reviews have more positive and fewer negative emotion terms. This pattern makes sense when one considers the goal of our deceivers, namely to create a positive review (Buller and Burgoon, 1996). Deception has also previously been associated with decreased usage of first person singular, an effect attributed to psychological distancing (Newman et al., 2003). In contrast, we find increased first person singular to be among the largest indicators of deception, which we speculate is due to our deceivers attempting to enhance the credibility of their reviews by emphasizing their own presence in the review. Additional work is required, but these findings further suggest the importance of moving beyond a universal set of deceptive language features (e.g., LIWC) by considering both the cont</context>
</contexts>
<marker>Buller, Burgoon, 1996</marker>
<rawString>D.B. Buller and J.K. Burgoon. 1996. Interpersonal deception theory. Communication Theory, 6(3):203– 242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W L Cade</author>
<author>B A Lehman</author>
<author>A Olney</author>
</authors>
<title>An exploration of off topic conversation.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of</booktitle>
<contexts>
<context position="20815" citStr="Cade et al., 2010" startWordPosition="3263" endWordPosition="3266">s by constructing, for each review, features based on the frequencies of each POS tag.15 These features are also intended to provide a good baseline with which to compare our other automated approaches. 4.2 Psycholinguistic deception detection The Linguistic Inquiry and Word Count (LIWC) software (Pennebaker et al., 2007) is a popular automated text analysis tool used widely in the social sciences. It has been used to detect personality 15We use the Stanford Parser (Klein and Manning, 2003) to obtain the relative POS frequencies. 313 traits (Mairesse et al., 2007), to study tutoring dynamics (Cade et al., 2010), and, most relevantly, to analyze deception (Hancock et al., 2008; Mihalcea and Strapparava, 2009; Vrij et al., 2007). While LIWC does not include a text classifier, we can create one with features derived from the LIWC output. In particular, LIWC counts and groups the number of instances of nearly 4,500 keywords into 80 psychologically meaningful dimensions. We construct one feature for each of the 80 LIWC dimensions, which can be summarized broadly under the following four categories: 1. Linguistic processes: Functional aspects of text (e.g., the average number of words per sentence, the ra</context>
</contexts>
<marker>Cade, Lehman, Olney, 2010</marker>
<rawString>W.L. Cade, B.A. Lehman, and A. Olney. 2010. An exploration of off topic conversation. In Human Language Technologies: The 2010 Annual Conference of</rawString>
</citation>
<citation valid="false">
<authors>
<author>the North</author>
</authors>
<title>American Chapter of the Association for Computational Linguistics,</title>
<pages>669--672</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>North, </marker>
<rawString>the North American Chapter of the Association for Computational Linguistics, pages 669–672. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>J Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th annual meeting on Association for Computational Linguistics,</booktitle>
<pages>310--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="23691" citStr="Chen and Goodman, 1996" startWordPosition="3715" endWordPosition="3718">simplified to the maximum likelihood classifier (Peng and Schuurmans, 2003): Pr(x |y = c) (2) Under (2), both the NB classifier used by Mihalcea and Strapparava (2009) and the language model classifier used by Zhou et al. (2008) are equivalent. Thus, following Zhou et al. (2008), we use the SRI Language Modeling Toolkit (Stolcke, 2002) to estimate individual language models, Pr(x |y = c), for truthful and deceptive opinions. We consider all three n-gram feature sets, namely UNIGRAMS, BIGRAMS+, and TRIGRAMS+, with corresponding language models smoothed using the interpolated Kneser-Ney method (Chen and Goodman, 1996). We also train Support Vector Machine (SVM) classifiers, which find a high-dimensional separating hyperplane between two groups of data. To simplify feature analysis in Section 5, we restrict our evaluation to linear SVMs, which learn a weight vector w and bias term b, such that a document x can be classified by: y� = sign(w · x + b) (3) We use SVM&amp;quot;ght (Joachims, 1999) to train our linear SVM models on all three approaches and feature sets described above, namely POS, LIWC, UNIGRAMS, BIGRAMS+, and TRIGRAMS+. We also evaluate every combination of these features, but for brevity include only LI</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>S.F. Chen and J. Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proceedings of the 34th annual meeting on Association for Computational Linguistics, pages 310–318. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Danescu-Niculescu-Mizil</author>
<author>G Kossinets</author>
<author>J Kleinberg</author>
<author>L Lee</author>
</authors>
<title>How opinions are received by online communities: a case study on amazon.com helpfulness votes.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th international conference on World wide web,</booktitle>
<pages>141--150</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="9681" citStr="Danescu-Niculescu-Mizil et al., 2009" startWordPosition="1478" endWordPosition="1481">, their stance on the death penalty). Zhou et al. (2004; 2008) consider computer-mediated deception in role-playing games designed to be played over instant messaging and e-mail. However, while these studies compare n-gram–based deception classifiers to a random guess baseline of 50%, we additionally evaluate and compare two other computational approaches (described in Section 4), as well as the performance of human judges (described in Section 3.3). Lastly, automatic approaches to determining review quality have been studied—directly (Weimer et al., 2007), and in the contexts of helpfulness (Danescu-Niculescu-Mizil et al., 2009; Kim et al., 2006; O’Mahony and Smyth, 2009) and credibility (Weerkamp and De Rijke, 2008). Unfortunately, most measures of quality employed in those works are based exclusively on human judgments, which we find in Section 3 to be poorly calibrated to detecting deceptive opinion spam. 3 Dataset Construction and Human Performance While truthful opinions are ubiquitous online, deceptive opinions are difficult to obtain without resorting to heuristic methods (Jindal and Liu, 2008; Wu et al., 2010). In this section, we report our efforts to gather (and validate with human judgments) the first pub</context>
</contexts>
<marker>Danescu-Niculescu-Mizil, Kossinets, Kleinberg, Lee, 2009</marker>
<rawString>C. Danescu-Niculescu-Mizil, G. Kossinets, J. Kleinberg, and L. Lee. 2009. How opinions are received by online communities: a case study on amazon.com helpfulness votes. In Proceedings of the 18th international conference on World wide web, pages 141–150. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Drucker</author>
<author>D Wu</author>
<author>V N Vapnik</author>
</authors>
<title>Support vector machines for spam categorization. Neural Networks,</title>
<date>2002</date>
<journal>IEEE Transactions on,</journal>
<volume>10</volume>
<issue>5</issue>
<contexts>
<context position="7223" citStr="Drucker et al., 2002" startWordPosition="1096" endWordPosition="1099">eceptive opinions and imaginative writing, and between truthful opinions and informative writing. The rest of this paper is organized as follows: in Section 2, we summarize related work; in Section 3, we explain our methodology for gathering data and evaluate human performance; in Section 4, we describe the features and classifiers employed by our three automated detection approaches; in Section 5, we present and discuss experimental results; finally, conclusions and directions for future work are given in Section 6. 2 Related Work Spam has historically been studied in the contexts of e-mail (Drucker et al., 2002), and the Web (Gy¨ongyi et al., 2004; Ntoulas et al., 2006). Recently, researchers have began to look at opinion spam as well (Jindal and Liu, 2008; Wu et al., 2010; Yoo and Gretzel, 2009). Jindal and Liu (2008) find that opinion spam is both widespread and different in nature from either e-mail or Web spam. Using product review data, and in the absence of gold-standard deceptive opinions, they train models using features based on the review text, reviewer, and product, to distinguish between duplicate opinions7 (considered deceptive spam) and non-duplicate opinions (considered truthful). Wu e</context>
</contexts>
<marker>Drucker, Wu, Vapnik, 2002</marker>
<rawString>H. Drucker, D. Wu, and V.N. Vapnik. 2002. Support vector machines for spam categorization. Neural Networks, IEEE Transactions on, 10(5):1048–1054.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Forman</author>
<author>M Scholz</author>
</authors>
<title>Apples-to-Apples in Cross-Validation Studies: Pitfalls in Classifier Performance Measurement.</title>
<date>2009</date>
<journal>ACM SIGKDD Explorations,</journal>
<volume>12</volume>
<issue>1</issue>
<contexts>
<context position="25552" citStr="Forman and Scholz (2009)" startWordPosition="4016" endWordPosition="4019">9.0 89.0 89.0 89.0 SVM UNIGRAMSNB 88.4% 92.5 83.5 87.8 85.0 93.3 88.9 BIGRAMS+ 88.9% 89.8 87.8 88.7 88.0 90.0 89.0 NB TRIGRAMS+ 87.6% 87.7 87.5 87.6 87.5 87.8 87.6 NB HUMAN / META JUDGE 1 61.9% 57.9 87.5 69.7 74.4 36.3 48.7 JUDGE 2 56.9% 53.9 95.0 68.8 78.9 18.8 30.3 SKEPTIC 60.6% 60.8 60.0 60.4 60.5 61.3 60.9 Table 3: Automated classifier performance for three approaches based on nested 5-fold cross-validation experiments. Reported precision, recall and F-score are computed using a micro-average, i.e., from the aggregate true positive, false positive and false negative rates, as suggested by Forman and Scholz (2009). Human performance is repeated here for JUDGE 1, JUDGE 2 and the SKEPTIC meta-judge, although they cannot be directly compared since the 160-opinion subset on which they are assessed only corresponds to the first cross-validation fold. 5 Results and Discussion The deception detection strategies described in Section 4 are evaluated using a 5-fold nested crossvalidation (CV) procedure (Quadrianto et al., 2009), where model parameters are selected for each test fold based on standard CV experiments on the training folds. Folds are selected so that each contains all reviews from four hotels; thus</context>
</contexts>
<marker>Forman, Scholz, 2009</marker>
<rawString>G. Forman and M. Scholz. 2009. Apples-to-Apples in Cross-Validation Studies: Pitfalls in Classifier Performance Measurement. ACM SIGKDD Explorations, 12(1):49–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Gy¨ongyi</author>
<author>H Garcia-Molina</author>
<author>J Pedersen</author>
</authors>
<title>Combating web spam with trustrank.</title>
<date>2004</date>
<booktitle>In Proceedings of the Thirtieth international conference on Very large data bases-Volume 30,</booktitle>
<pages>576--587</pages>
<publisher>VLDB Endowment.</publisher>
<marker>Gy¨ongyi, Garcia-Molina, Pedersen, 2004</marker>
<rawString>Z. Gy¨ongyi, H. Garcia-Molina, and J. Pedersen. 2004. Combating web spam with trustrank. In Proceedings of the Thirtieth international conference on Very large data bases-Volume 30, pages 576–587. VLDB Endowment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J T Hancock</author>
<author>L E Curry</author>
<author>S Goorha</author>
<author>M Woodworth</author>
</authors>
<title>On lying and being lied to: A linguistic analysis of deception in computer-mediated communication.</title>
<date>2008</date>
<booktitle>Discourse Processes,</booktitle>
<volume>45</volume>
<issue>1</issue>
<contexts>
<context position="4863" citStr="Hancock et al., 2008" startWordPosition="746" endWordPosition="749"> deceptive reviews. To obtain a deeper understanding of the nature of deceptive opinion spam, we explore the relative utility of three potentially complementary framings of our problem. Specifically, we view the task as: (a) a standard text categorization task, in which we use n-gram–based classifiers to label opinions as either deceptive or truthful (Joachims, 1998; Sebastiani, 2002); (b) an instance of psycholinguistic deception detection, in which we expect deceptive statements to exemplify the psychological effects of lying, such as increased negative emotion and psychological distancing (Hancock et al., 2008; Newman et al., 2003); and, (c) a problem of genre identification, in which we view deceptive and truthful writing as sub-genres of imaginative and informative writing, respectively (Biber et al., 1999; Rayson et al., 2001). We compare the performance of each approach on our novel dataset. Particularly, we find that machine learning classifiers trained on features traditionally employed in (a) psychological studies of deception and (b) genre identification are both outperformed at statistically significant levels by ngram–based text categorization techniques. Notably, a combined classifier wi</context>
<context position="20881" citStr="Hancock et al., 2008" startWordPosition="3273" endWordPosition="3276">ncies of each POS tag.15 These features are also intended to provide a good baseline with which to compare our other automated approaches. 4.2 Psycholinguistic deception detection The Linguistic Inquiry and Word Count (LIWC) software (Pennebaker et al., 2007) is a popular automated text analysis tool used widely in the social sciences. It has been used to detect personality 15We use the Stanford Parser (Klein and Manning, 2003) to obtain the relative POS frequencies. 313 traits (Mairesse et al., 2007), to study tutoring dynamics (Cade et al., 2010), and, most relevantly, to analyze deception (Hancock et al., 2008; Mihalcea and Strapparava, 2009; Vrij et al., 2007). While LIWC does not include a text classifier, we can create one with features derived from the LIWC output. In particular, LIWC counts and groups the number of instances of nearly 4,500 keywords into 80 psychologically meaningful dimensions. We construct one feature for each of the 80 LIWC dimensions, which can be summarized broadly under the following four categories: 1. Linguistic processes: Functional aspects of text (e.g., the average number of words per sentence, the rate of misspelling, swearing, etc.) 2. Psychological processes: Inc</context>
<context position="28352" citStr="Hancock et al., 2008" startWordPosition="4447" endWordPosition="4450">sitions, determiners, and coordinating conjunctions, while the latter consists of more verbs,17 adverbs,18 pronouns, and pre-determiners. Indeed, we find that the weights learned by POSSVM (found in Table 4) are largely in agreement with these findings, notably except for adjective and adverb superlatives, the latter of which was found to be an exception by Rayson et al. (2001). However, that deceptive opinions contain more superlatives is not unexpected, since deceptive writing (but not necessarily imaginative writing in general) often contains exaggerated language (Buller and Burgoon, 1996; Hancock et al., 2008). Both remaining automated approaches to detecting deceptive opinion spam outperform the simple 17Past participle verbs were an exception. 18Superlative adverbs were an exception. 315 TRUTHFUL/INFORMATIVE DECEPTIVE/IMAGINATIVE Category Variant Weight Category Variant Weight NOUNS Singular 0.008 VERBS Base -0.057 Plural 0.002 Past tense 0.041 Proper, singular -0.041 Present participle -0.089 Proper, plural 0.091 Singular, present -0.031 ADJECTIVES General 0.002 Third person 0.026 singular, present Comparative 0.058 Superlative -0.164 Modal -0.063 PREPOSITIONS General 0.064 ADVERBS General 0.001</context>
<context position="32462" citStr="Hancock et al., 2008" startWordPosition="5028" endWordPosition="5031">ies are available at http://liwc.net. particular, truthful opinions are more specific about spatial configurations (e.g., small, bathroom, on, location). This finding is also supported by recent work by Vrij et al. (2009) suggesting that liars have considerable difficultly encoding spatial information into their lies. Accordingly, we observe an increased focus in deceptive opinions on aspects external to the hotel being reviewed (e.g., husband, business, 316 vacation). We also acknowledge several findings that, on the surface, are in contrast to previous psycholinguistic studies of deception (Hancock et al., 2008; Newman et al., 2003). For instance, while deception is often associated with negative emotion terms, our deceptive reviews have more positive and fewer negative emotion terms. This pattern makes sense when one considers the goal of our deceivers, namely to create a positive review (Buller and Burgoon, 1996). Deception has also previously been associated with decreased usage of first person singular, an effect attributed to psychological distancing (Newman et al., 2003). In contrast, we find increased first person singular to be among the largest indicators of deception, which we speculate is</context>
</contexts>
<marker>Hancock, Curry, Goorha, Woodworth, 2008</marker>
<rawString>J.T. Hancock, L.E. Curry, S. Goorha, and M. Woodworth. 2008. On lying and being lied to: A linguistic analysis of deception in computer-mediated communication. Discourse Processes, 45(1):1–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jansen</author>
</authors>
<title>Online product research.</title>
<date>2010</date>
<journal>Pew Internet &amp; American Life Project Report.</journal>
<marker>Jansen, 2010</marker>
<rawString>J. Jansen. 2010. Online product research. Pew Internet &amp; American Life Project Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Jindal</author>
<author>B Liu</author>
</authors>
<title>Opinion spam and analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the international conference on Web search and web data mining,</booktitle>
<pages>219--230</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2188" citStr="Jindal and Liu, 2008" startWordPosition="304" endWordPosition="307">ho 1http://tripadvisor.com 2http://yelp.com 3http://news.cnet.com/8301-1001_ 3-10145399-92.html hired people to write positive reviews for an otherwise poorly reviewed product.4 While other kinds of spam have received considerable computational attention, regrettably there has been little work to date (see Section 2) on opinion spam detection. Furthermore, most previous work in the area has focused on the detection of DISRUPTIVE OPINION SPAM—uncontroversial instances of spam that are easily identified by a human reader, e.g., advertisements, questions, and other irrelevant or nonopinion text (Jindal and Liu, 2008). And while the presence of disruptive opinion spam is certainly a nuisance, the risk it poses to the user is minimal, since the user can always choose to ignore it. We focus here on a potentially more insidious type of opinion spam: DECEPTIVE OPINION SPAM—fictitious opinions that have been deliberately written to sound authentic, in order to deceive the reader. For example, one of the following two hotel reviews is truthful and the other is deceptive opinion spam: 1. I have stayed at many hotels traveling for both business and pleasure and I can honestly stay that The James is tops. The servi</context>
<context position="7370" citStr="Jindal and Liu, 2008" startWordPosition="1123" endWordPosition="1126"> Section 2, we summarize related work; in Section 3, we explain our methodology for gathering data and evaluate human performance; in Section 4, we describe the features and classifiers employed by our three automated detection approaches; in Section 5, we present and discuss experimental results; finally, conclusions and directions for future work are given in Section 6. 2 Related Work Spam has historically been studied in the contexts of e-mail (Drucker et al., 2002), and the Web (Gy¨ongyi et al., 2004; Ntoulas et al., 2006). Recently, researchers have began to look at opinion spam as well (Jindal and Liu, 2008; Wu et al., 2010; Yoo and Gretzel, 2009). Jindal and Liu (2008) find that opinion spam is both widespread and different in nature from either e-mail or Web spam. Using product review data, and in the absence of gold-standard deceptive opinions, they train models using features based on the review text, reviewer, and product, to distinguish between duplicate opinions7 (considered deceptive spam) and non-duplicate opinions (considered truthful). Wu et al. (2010) propose an alternative strategy for detecting deceptive opinion spam in the absence 7Duplicate (or near-duplicate) opinions are opinio</context>
<context position="10163" citStr="Jindal and Liu, 2008" startWordPosition="1554" endWordPosition="1557"> review quality have been studied—directly (Weimer et al., 2007), and in the contexts of helpfulness (Danescu-Niculescu-Mizil et al., 2009; Kim et al., 2006; O’Mahony and Smyth, 2009) and credibility (Weerkamp and De Rijke, 2008). Unfortunately, most measures of quality employed in those works are based exclusively on human judgments, which we find in Section 3 to be poorly calibrated to detecting deceptive opinion spam. 3 Dataset Construction and Human Performance While truthful opinions are ubiquitous online, deceptive opinions are difficult to obtain without resorting to heuristic methods (Jindal and Liu, 2008; Wu et al., 2010). In this section, we report our efforts to gather (and validate with human judgments) the first publicly available opinion spam dataset with gold-standard deceptive opinions. Following the work of Yoo and Gretzel (2009), we compare truthful and deceptive positive reviews for hotels found on TripAdvisor. Specifically, we mine all 5-star truthful reviews from the 20 most popular hotels on TripAdvisor8 in the Chicago area.9 Deceptive opinions are gathered for those same 20 hotels using Amazon Mechanical Turk10 (AMT). Below, we provide details of the collection methodologies for</context>
<context position="12551" citStr="Jindal and Liu, 2008" startWordPosition="1951" endWordPosition="1954">tment, and to pretend that their boss wants them to write a fake review (as if they were a customer) to be posted on a travel review website; additionally, the review needs to sound realistic and portray the hotel in a positive light. A disclaimer 8TripAdvisor utilizes a proprietary ranking system to assess hotel popularity. We chose the 20 hotels with the greatest number of reviews, irrespective of the TripAdvisor ranking. 9It has been hypothesized that popular offerings are less likely to become targets of deceptive opinion spam, since the relative impact of the spam in such cases is small (Jindal and Liu, 2008; Lim et al., 2010). By considering only the most popular hotels, we hope to minimize the risk of mining opinion spam and labeling it as truthful. 10http://mturk.com 311 Time spent t (minutes) All submissions count: 400 tmin: 0.08, tmax: 29.78 f: 8.06, s: 6.32 Length f (words) All submissions fmin: 25, fmax: 425 �f: 115.75, s: 61.30 Time spent t &lt; 1 count: 47 fmin: 39, fmax: 407 �f: 113.94, s: 66.24 Time spent t &gt; 1 count: 353 fmin: 25, fmax: 425 �f: 115.99, s: 60.71 Table 1: Descriptive statistics for 400 deceptive opinion spam submissions gathered using AMT. s corresponds to the sample stand</context>
<context position="15762" citStr="Jindal and Liu, 2008" startWordPosition="2455" endWordPosition="2458">k by Serrano et al. (2009) suggests that a log-normal distribution is appropriate for modeling document lengths. Thus, for each of the 20 chosen hotels, we select 20 truthful reviews from a log-normal (lefttruncated at 150 characters) distribution fit to the lengths of the deceptive reviews.14 Combined with the 400 deceptive reviews gathered in Section 3.1 this yields our final dataset of 800 reviews. 3.3 Human performance Assessing human deception detection performance is important for several reasons. First, there are few other baselines for our classification task; indeed, related studies (Jindal and Liu, 2008; Mihalcea and Strapparava, 2009) have only considered a random guess baseline. Second, assessing human performance is necessary to validate the deceptive opinions gathered in Section 3.1. If human performance is low, then our deceptive opinions are convincing, and therefore, deserving of further attention. Our initial approach to assessing human performance on this task was with Mechanical Turk. Unfortunately, we found that some Turkers selected among the choices seemingly at random, presumably to maximize their hourly earnings by obviating the need to read the review. While a similar effect </context>
<context position="22764" citStr="Jindal and Liu, 2008" startWordPosition="3557" endWordPosition="3560">st to the other strategies just discussed, our text categorization approach to deception detection allows us to model both content and context with n-gram features. Specifically, we consider the following three n-gram feature sets, with the corresponding features lowercased and unstemmed: UNIGRAMS, BIGRAMS+, TRIGRAMS+, where the superscript + indicates that the feature set subsumes the preceding feature set. 4.4 Classifiers Features from the three approaches just introduced are used to train Naive Bayes and Support Vector Machine classifiers, both of which have performed well in related work (Jindal and Liu, 2008; Mihalcea and Strapparava, 2009; Zhou et al., 2008). For a document x, with label y, the Naive Bayes (NB) classifier gives us the following decision rule: y� = arg max Pr(y = c) · Pr(x |y = c) (1) c When the class prior is uniform, for example when the classes are balanced (as in our case), (1) can be simplified to the maximum likelihood classifier (Peng and Schuurmans, 2003): Pr(x |y = c) (2) Under (2), both the NB classifier used by Mihalcea and Strapparava (2009) and the language model classifier used by Zhou et al. (2008) are equivalent. Thus, following Zhou et al. (2008), we use the SRI </context>
</contexts>
<marker>Jindal, Liu, 2008</marker>
<rawString>N. Jindal and B. Liu. 2008. Opinion spam and analysis. In Proceedings of the international conference on Web search and web data mining, pages 219–230. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Text categorization with support vector machines: Learning with many relevant features.</title>
<date>1998</date>
<booktitle>Machine Learning: ECML-98,</booktitle>
<pages>137--142</pages>
<contexts>
<context position="4611" citStr="Joachims, 1998" startWordPosition="710" endWordPosition="711">ize ad hoc procedures for evaluation. In contrast, one contribution of the work presented here is the creation of the first largescale, publicly available6 dataset for deceptive opinion spam research, containing 400 truthful and 400 gold-standard deceptive reviews. To obtain a deeper understanding of the nature of deceptive opinion spam, we explore the relative utility of three potentially complementary framings of our problem. Specifically, we view the task as: (a) a standard text categorization task, in which we use n-gram–based classifiers to label opinions as either deceptive or truthful (Joachims, 1998; Sebastiani, 2002); (b) an instance of psycholinguistic deception detection, in which we expect deceptive statements to exemplify the psychological effects of lying, such as increased negative emotion and psychological distancing (Hancock et al., 2008; Newman et al., 2003); and, (c) a problem of genre identification, in which we view deceptive and truthful writing as sub-genres of imaginative and informative writing, respectively (Biber et al., 1999; Rayson et al., 2001). We compare the performance of each approach on our novel dataset. Particularly, we find that machine learning classifiers </context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>T. Joachims. 1998. Text categorization with support vector machines: Learning with many relevant features. Machine Learning: ECML-98, pages 137–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making large-scale support vector machine learning practical.</title>
<date>1999</date>
<booktitle>In Advances in kernel methods,</booktitle>
<pages>184</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="24063" citStr="Joachims, 1999" startWordPosition="3783" endWordPosition="3784">(x |y = c), for truthful and deceptive opinions. We consider all three n-gram feature sets, namely UNIGRAMS, BIGRAMS+, and TRIGRAMS+, with corresponding language models smoothed using the interpolated Kneser-Ney method (Chen and Goodman, 1996). We also train Support Vector Machine (SVM) classifiers, which find a high-dimensional separating hyperplane between two groups of data. To simplify feature analysis in Section 5, we restrict our evaluation to linear SVMs, which learn a weight vector w and bias term b, such that a document x can be classified by: y� = sign(w · x + b) (3) We use SVM&amp;quot;ght (Joachims, 1999) to train our linear SVM models on all three approaches and feature sets described above, namely POS, LIWC, UNIGRAMS, BIGRAMS+, and TRIGRAMS+. We also evaluate every combination of these features, but for brevity include only LIWC+BIGRAMS+, which performs best. Following standard practice, document vectors are normalized to unit-length. For LIWC+BIGRAMS+, we unit-length normalize LIWC and BIGRAMS+ features individually before combining them. y� = arg max c 314 TRUTHFUL DECEPTIVE Approach Features Accuracy P R F P R F GENRE IDENTIFICATION POSSVM 73.0% 75.3 68.5 71.7 71.1 77.5 74.2 PSYCHOLINGUIS</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>T. Joachims. 1999. Making large-scale support vector machine learning practical. In Advances in kernel methods, page 184. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M K Johnson</author>
<author>C L Raye</author>
</authors>
<title>Reality monitoring.</title>
<date>1981</date>
<journal>Psychological Review,</journal>
<volume>88</volume>
<issue>1</issue>
<contexts>
<context position="27411" citStr="Johnson and Raye, 1981" startWordPosition="4308" endWordPosition="4311">ine performance is given by the simple genre identification approach (POSSVM) proposed in Section 4.1. Surprisingly, we find that even this simple auto16As mentioned in Section 3.3, JUDGE 2 classified fewer than 12% of opinions as deceptive. While achieving 95% truthful recall, this judge’s corresponding precision was not significantly better than chance (two-tailed binomial p = 0.4). mated classifier outperforms most human judges (one-tailed sign test p = 0.06, 0.01, 0.001 for the three judges, respectively, on the first fold). This result is best explained by theories of reality monitoring (Johnson and Raye, 1981), which suggest that truthful and deceptive opinions might be classified into informative and imaginative genres, respectively. Work by Rayson et al. (2001) has found strong distributional differences between informative and imaginative writing, namely that the former typically consists of more nouns, adjectives, prepositions, determiners, and coordinating conjunctions, while the latter consists of more verbs,17 adverbs,18 pronouns, and pre-determiners. Indeed, we find that the weights learned by POSSVM (found in Table 4) are largely in agreement with these findings, notably except for adjecti</context>
<context position="30893" citStr="Johnson and Raye, 1981" startWordPosition="4799" endWordPosition="4802">dels trained on BIGRAMS+ perform even better (one-tailed sign test p = 0.07). This suggests that a universal set of keyword-based deception cues (e.g., LIWC) is not the best approach to detecting deception, and a context-sensitive approach (e.g., BIGRAMS+) might be necessary to achieve state-of-the-art deception detection performance. To better understand the models learned by these automated approaches, we report in Table 5 the top 15 highest weighted features for each class (truthful and deceptive) as learned by LIWC+BIGRAMS+SVM and LIWCSVM. In agreement with theories of reality monitoring (Johnson and Raye, 1981), we observe that truthful opinions tend to include more sensorial and concrete language than deceptive opinions; in 19The result is not significantly better than BIGRAMS+SVM. LIWC+BIGRAMS+ LIWCSVM SVM TRUTHFUL DECEPTIVE TRUTHFUL DECEPTIVE - chicago hear i ... my number family on hotel allpunct perspron location , and negemo see ) luxury dash pronoun allpunctLIWC experience exclusive leisure floor hilton we exclampunct ( business sexual sixletters the hotel vacation period posemo bathroom i otherpunct comma small spa space cause helpful looking human auxverb $ while past future hotel. husband </context>
</contexts>
<marker>Johnson, Raye, 1981</marker>
<rawString>M.K. Johnson and C.L. Raye. 1981. Reality monitoring. Psychological Review, 88(1):67–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Kim</author>
<author>P Pantel</author>
<author>T Chklovski</author>
<author>M Pennacchiotti</author>
</authors>
<title>Automatically assessing review helpfulness.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9699" citStr="Kim et al., 2006" startWordPosition="1482" endWordPosition="1485">Zhou et al. (2004; 2008) consider computer-mediated deception in role-playing games designed to be played over instant messaging and e-mail. However, while these studies compare n-gram–based deception classifiers to a random guess baseline of 50%, we additionally evaluate and compare two other computational approaches (described in Section 4), as well as the performance of human judges (described in Section 3.3). Lastly, automatic approaches to determining review quality have been studied—directly (Weimer et al., 2007), and in the contexts of helpfulness (Danescu-Niculescu-Mizil et al., 2009; Kim et al., 2006; O’Mahony and Smyth, 2009) and credibility (Weerkamp and De Rijke, 2008). Unfortunately, most measures of quality employed in those works are based exclusively on human judgments, which we find in Section 3 to be poorly calibrated to detecting deceptive opinion spam. 3 Dataset Construction and Human Performance While truthful opinions are ubiquitous online, deceptive opinions are difficult to obtain without resorting to heuristic methods (Jindal and Liu, 2008; Wu et al., 2010). In this section, we report our efforts to gather (and validate with human judgments) the first publicly available op</context>
</contexts>
<marker>Kim, Pantel, Chklovski, Pennacchiotti, 2006</marker>
<rawString>S.M. Kim, P. Pantel, T. Chklovski, and M. Pennacchiotti. 2006. Automatically assessing review helpfulness. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 423– 430. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational LinguisticsVolume 1,</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="20692" citStr="Klein and Manning, 2003" startWordPosition="3242" endWordPosition="3245">ntification approach to deceptive opinion spam detection, we test if such a relationship exists for truthful and deceptive reviews by constructing, for each review, features based on the frequencies of each POS tag.15 These features are also intended to provide a good baseline with which to compare our other automated approaches. 4.2 Psycholinguistic deception detection The Linguistic Inquiry and Word Count (LIWC) software (Pennebaker et al., 2007) is a popular automated text analysis tool used widely in the social sciences. It has been used to detect personality 15We use the Stanford Parser (Klein and Manning, 2003) to obtain the relative POS frequencies. 313 traits (Mairesse et al., 2007), to study tutoring dynamics (Cade et al., 2010), and, most relevantly, to analyze deception (Hancock et al., 2008; Mihalcea and Strapparava, 2009; Vrij et al., 2007). While LIWC does not include a text classifier, we can create one with features derived from the LIWC output. In particular, LIWC counts and groups the number of instances of nearly 4,500 keywords into 80 psychologically meaningful dimensions. We construct one feature for each of the 80 LIWC dimensions, which can be summarized broadly under the following f</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C.D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational LinguisticsVolume 1, pages 423–430. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Landis</author>
<author>G G Koch</author>
</authors>
<title>The measurement of observer agreement for categorical data.</title>
<date>1977</date>
<journal>Biometrics,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="19099" citStr="Landis and Koch (1977)" startWordPosition="2994" endWordPosition="2997">fewer than 12% of the opinions as deceptive! Interestingly, this bias is effectively smoothed by the SKEPTIC meta-judge, which produces nearly perfectly class-balanced predictions. A subsequent reevaluation of human performance on this task suggests that the truth-bias can be reduced if judges are given the class-proportions in advance, although such prior knowledge is unrealistic; and ultimately, performance remains similar to that of Table 2. Inter-annotator agreement among the three judges, computed using Fleiss’ kappa, is 0.11. While there is no precise rule for interpreting kappa scores, Landis and Koch (1977) suggest that scores in the range (0.00, 0.20] correspond to “slight agreement” between annotators. The largest pairwise Cohen’s kappa is 0.12, between JUDGE 2 and JUDGE 3—a value far below generally accepted pairwise agreement levels. We suspect that agreement among our human judges is so low precisely because humans are poor judges of deception (Vrij, 2008), and therefore they perform nearly at-chance respective to one another. 4 Automated Approaches to Deceptive Opinion Spam Detection We consider three automated approaches to detecting deceptive opinion spam, each of which utilizes classifi</context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>J.R. Landis and G.G. Koch. 1977. The measurement of observer agreement for categorical data. Biometrics, 33(1):159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E P Lim</author>
<author>V A Nguyen</author>
<author>N Jindal</author>
<author>B Liu</author>
<author>H W Lauw</author>
</authors>
<title>Detecting product review spammers using rating behaviors.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th ACM international conference on Information and knowledge management,</booktitle>
<pages>939--948</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="12570" citStr="Lim et al., 2010" startWordPosition="1955" endWordPosition="1958">that their boss wants them to write a fake review (as if they were a customer) to be posted on a travel review website; additionally, the review needs to sound realistic and portray the hotel in a positive light. A disclaimer 8TripAdvisor utilizes a proprietary ranking system to assess hotel popularity. We chose the 20 hotels with the greatest number of reviews, irrespective of the TripAdvisor ranking. 9It has been hypothesized that popular offerings are less likely to become targets of deceptive opinion spam, since the relative impact of the spam in such cases is small (Jindal and Liu, 2008; Lim et al., 2010). By considering only the most popular hotels, we hope to minimize the risk of mining opinion spam and labeling it as truthful. 10http://mturk.com 311 Time spent t (minutes) All submissions count: 400 tmin: 0.08, tmax: 29.78 f: 8.06, s: 6.32 Length f (words) All submissions fmin: 25, fmax: 425 �f: 115.75, s: 61.30 Time spent t &lt; 1 count: 47 fmin: 39, fmax: 407 �f: 113.94, s: 66.24 Time spent t &gt; 1 count: 353 fmin: 25, fmax: 425 �f: 115.99, s: 60.71 Table 1: Descriptive statistics for 400 deceptive opinion spam submissions gathered using AMT. s corresponds to the sample standard deviation. indi</context>
</contexts>
<marker>Lim, Nguyen, Jindal, Liu, Lauw, 2010</marker>
<rawString>E.P. Lim, V.A. Nguyen, N. Jindal, B. Liu, and H.W. Lauw. 2010. Detecting product review spammers using rating behaviors. In Proceedings of the 19th ACM international conference on Information and knowledge management, pages 939–948. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S W Litvin</author>
<author>R E Goldsmith</author>
<author>B Pan</author>
</authors>
<title>Electronic word-of-mouth in hospitality and tourism management. Tourism management,</title>
<date>2008</date>
<pages>29--3</pages>
<marker>Litvin, Goldsmith, Pan, 2008</marker>
<rawString>S.W. Litvin, R.E. Goldsmith, and B. Pan. 2008. Electronic word-of-mouth in hospitality and tourism management. Tourism management, 29(3):458–468.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Mairesse</author>
<author>M A Walker</author>
<author>M R Mehl</author>
<author>R K Moore</author>
</authors>
<title>Using linguistic cues for the automatic recognition of personality in conversation and text.</title>
<date>2007</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="20767" citStr="Mairesse et al., 2007" startWordPosition="3254" endWordPosition="3257">elationship exists for truthful and deceptive reviews by constructing, for each review, features based on the frequencies of each POS tag.15 These features are also intended to provide a good baseline with which to compare our other automated approaches. 4.2 Psycholinguistic deception detection The Linguistic Inquiry and Word Count (LIWC) software (Pennebaker et al., 2007) is a popular automated text analysis tool used widely in the social sciences. It has been used to detect personality 15We use the Stanford Parser (Klein and Manning, 2003) to obtain the relative POS frequencies. 313 traits (Mairesse et al., 2007), to study tutoring dynamics (Cade et al., 2010), and, most relevantly, to analyze deception (Hancock et al., 2008; Mihalcea and Strapparava, 2009; Vrij et al., 2007). While LIWC does not include a text classifier, we can create one with features derived from the LIWC output. In particular, LIWC counts and groups the number of instances of nearly 4,500 keywords into 80 psychologically meaningful dimensions. We construct one feature for each of the 80 LIWC dimensions, which can be summarized broadly under the following four categories: 1. Linguistic processes: Functional aspects of text (e.g., </context>
</contexts>
<marker>Mairesse, Walker, Mehl, Moore, 2007</marker>
<rawString>F. Mairesse, M.A. Walker, M.R. Mehl, and R.K. Moore. 2007. Using linguistic cues for the automatic recognition of personality in conversation and text. Journal of Artificial Intelligence Research, 30(1):457–500.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>C Strapparava</author>
</authors>
<title>The lie detector: Explorations in the automatic recognition of deceptive language.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,</booktitle>
<pages>309--312</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8961" citStr="Mihalcea and Strapparava (2009)" startWordPosition="1368" endWordPosition="1371">ankings. Both of these heuristic evaluation approaches are unnecessary in our work, since we compare gold-standard deceptive and truthful opinions. Yoo and Gretzel (2009) gather 40 truthful and 42 deceptive hotel reviews and, using a standard statistical test, manually compare the psychologically relevant linguistic differences between them. In contrast, we create a much larger dataset of 800 opinions that we use to develop and evaluate automated deception classifiers. Research has also been conducted on the related task of psycholinguistic deception detection. Newman et al. (2003), and later Mihalcea and Strapparava (2009), ask participants to give both their true and untrue views on personal issues (e.g., their stance on the death penalty). Zhou et al. (2004; 2008) consider computer-mediated deception in role-playing games designed to be played over instant messaging and e-mail. However, while these studies compare n-gram–based deception classifiers to a random guess baseline of 50%, we additionally evaluate and compare two other computational approaches (described in Section 4), as well as the performance of human judges (described in Section 3.3). Lastly, automatic approaches to determining review quality ha</context>
<context position="15795" citStr="Mihalcea and Strapparava, 2009" startWordPosition="2459" endWordPosition="2462">009) suggests that a log-normal distribution is appropriate for modeling document lengths. Thus, for each of the 20 chosen hotels, we select 20 truthful reviews from a log-normal (lefttruncated at 150 characters) distribution fit to the lengths of the deceptive reviews.14 Combined with the 400 deceptive reviews gathered in Section 3.1 this yields our final dataset of 800 reviews. 3.3 Human performance Assessing human deception detection performance is important for several reasons. First, there are few other baselines for our classification task; indeed, related studies (Jindal and Liu, 2008; Mihalcea and Strapparava, 2009) have only considered a random guess baseline. Second, assessing human performance is necessary to validate the deceptive opinions gathered in Section 3.1. If human performance is low, then our deceptive opinions are convincing, and therefore, deserving of further attention. Our initial approach to assessing human performance on this task was with Mechanical Turk. Unfortunately, we found that some Turkers selected among the choices seemingly at random, presumably to maximize their hourly earnings by obviating the need to read the review. While a similar effect has been observed previously (Akk</context>
<context position="20913" citStr="Mihalcea and Strapparava, 2009" startWordPosition="3277" endWordPosition="3280">15 These features are also intended to provide a good baseline with which to compare our other automated approaches. 4.2 Psycholinguistic deception detection The Linguistic Inquiry and Word Count (LIWC) software (Pennebaker et al., 2007) is a popular automated text analysis tool used widely in the social sciences. It has been used to detect personality 15We use the Stanford Parser (Klein and Manning, 2003) to obtain the relative POS frequencies. 313 traits (Mairesse et al., 2007), to study tutoring dynamics (Cade et al., 2010), and, most relevantly, to analyze deception (Hancock et al., 2008; Mihalcea and Strapparava, 2009; Vrij et al., 2007). While LIWC does not include a text classifier, we can create one with features derived from the LIWC output. In particular, LIWC counts and groups the number of instances of nearly 4,500 keywords into 80 psychologically meaningful dimensions. We construct one feature for each of the 80 LIWC dimensions, which can be summarized broadly under the following four categories: 1. Linguistic processes: Functional aspects of text (e.g., the average number of words per sentence, the rate of misspelling, swearing, etc.) 2. Psychological processes: Includes all social, emotional, cog</context>
<context position="22796" citStr="Mihalcea and Strapparava, 2009" startWordPosition="3561" endWordPosition="3564">gies just discussed, our text categorization approach to deception detection allows us to model both content and context with n-gram features. Specifically, we consider the following three n-gram feature sets, with the corresponding features lowercased and unstemmed: UNIGRAMS, BIGRAMS+, TRIGRAMS+, where the superscript + indicates that the feature set subsumes the preceding feature set. 4.4 Classifiers Features from the three approaches just introduced are used to train Naive Bayes and Support Vector Machine classifiers, both of which have performed well in related work (Jindal and Liu, 2008; Mihalcea and Strapparava, 2009; Zhou et al., 2008). For a document x, with label y, the Naive Bayes (NB) classifier gives us the following decision rule: y� = arg max Pr(y = c) · Pr(x |y = c) (1) c When the class prior is uniform, for example when the classes are balanced (as in our case), (1) can be simplified to the maximum likelihood classifier (Peng and Schuurmans, 2003): Pr(x |y = c) (2) Under (2), both the NB classifier used by Mihalcea and Strapparava (2009) and the language model classifier used by Zhou et al. (2008) are equivalent. Thus, following Zhou et al. (2008), we use the SRI Language Modeling Toolkit (Stolc</context>
</contexts>
<marker>Mihalcea, Strapparava, 2009</marker>
<rawString>R. Mihalcea and C. Strapparava. 2009. The lie detector: Explorations in the automatic recognition of deceptive language. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 309–312. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M L Newman</author>
<author>J W Pennebaker</author>
<author>D S Berry</author>
<author>J M Richards</author>
</authors>
<title>Lying words: Predicting deception from linguistic styles.</title>
<date>2003</date>
<journal>Personality and Social Psychology Bulletin,</journal>
<volume>29</volume>
<issue>5</issue>
<contexts>
<context position="4885" citStr="Newman et al., 2003" startWordPosition="750" endWordPosition="753"> obtain a deeper understanding of the nature of deceptive opinion spam, we explore the relative utility of three potentially complementary framings of our problem. Specifically, we view the task as: (a) a standard text categorization task, in which we use n-gram–based classifiers to label opinions as either deceptive or truthful (Joachims, 1998; Sebastiani, 2002); (b) an instance of psycholinguistic deception detection, in which we expect deceptive statements to exemplify the psychological effects of lying, such as increased negative emotion and psychological distancing (Hancock et al., 2008; Newman et al., 2003); and, (c) a problem of genre identification, in which we view deceptive and truthful writing as sub-genres of imaginative and informative writing, respectively (Biber et al., 1999; Rayson et al., 2001). We compare the performance of each approach on our novel dataset. Particularly, we find that machine learning classifiers trained on features traditionally employed in (a) psychological studies of deception and (b) genre identification are both outperformed at statistically significant levels by ngram–based text categorization techniques. Notably, a combined classifier with both n-gram and psy</context>
<context position="8918" citStr="Newman et al. (2003)" startWordPosition="1362" endWordPosition="1365">n the distortion of popularity rankings. Both of these heuristic evaluation approaches are unnecessary in our work, since we compare gold-standard deceptive and truthful opinions. Yoo and Gretzel (2009) gather 40 truthful and 42 deceptive hotel reviews and, using a standard statistical test, manually compare the psychologically relevant linguistic differences between them. In contrast, we create a much larger dataset of 800 opinions that we use to develop and evaluate automated deception classifiers. Research has also been conducted on the related task of psycholinguistic deception detection. Newman et al. (2003), and later Mihalcea and Strapparava (2009), ask participants to give both their true and untrue views on personal issues (e.g., their stance on the death penalty). Zhou et al. (2004; 2008) consider computer-mediated deception in role-playing games designed to be played over instant messaging and e-mail. However, while these studies compare n-gram–based deception classifiers to a random guess baseline of 50%, we additionally evaluate and compare two other computational approaches (described in Section 4), as well as the performance of human judges (described in Section 3.3). Lastly, automatic </context>
<context position="32484" citStr="Newman et al., 2003" startWordPosition="5032" endWordPosition="5035">ttp://liwc.net. particular, truthful opinions are more specific about spatial configurations (e.g., small, bathroom, on, location). This finding is also supported by recent work by Vrij et al. (2009) suggesting that liars have considerable difficultly encoding spatial information into their lies. Accordingly, we observe an increased focus in deceptive opinions on aspects external to the hotel being reviewed (e.g., husband, business, 316 vacation). We also acknowledge several findings that, on the surface, are in contrast to previous psycholinguistic studies of deception (Hancock et al., 2008; Newman et al., 2003). For instance, while deception is often associated with negative emotion terms, our deceptive reviews have more positive and fewer negative emotion terms. This pattern makes sense when one considers the goal of our deceivers, namely to create a positive review (Buller and Burgoon, 1996). Deception has also previously been associated with decreased usage of first person singular, an effect attributed to psychological distancing (Newman et al., 2003). In contrast, we find increased first person singular to be among the largest indicators of deception, which we speculate is due to our deceivers </context>
</contexts>
<marker>Newman, Pennebaker, Berry, Richards, 2003</marker>
<rawString>M.L. Newman, J.W. Pennebaker, D.S. Berry, and J.M. Richards. 2003. Lying words: Predicting deception from linguistic styles. Personality and Social Psychology Bulletin, 29(5):665.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ntoulas</author>
<author>M Najork</author>
<author>M Manasse</author>
<author>D Fetterly</author>
</authors>
<title>Detecting spam web pages through content analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of the 15th international conference on World Wide Web,</booktitle>
<pages>83--92</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="7282" citStr="Ntoulas et al., 2006" startWordPosition="1107" endWordPosition="1110">hful opinions and informative writing. The rest of this paper is organized as follows: in Section 2, we summarize related work; in Section 3, we explain our methodology for gathering data and evaluate human performance; in Section 4, we describe the features and classifiers employed by our three automated detection approaches; in Section 5, we present and discuss experimental results; finally, conclusions and directions for future work are given in Section 6. 2 Related Work Spam has historically been studied in the contexts of e-mail (Drucker et al., 2002), and the Web (Gy¨ongyi et al., 2004; Ntoulas et al., 2006). Recently, researchers have began to look at opinion spam as well (Jindal and Liu, 2008; Wu et al., 2010; Yoo and Gretzel, 2009). Jindal and Liu (2008) find that opinion spam is both widespread and different in nature from either e-mail or Web spam. Using product review data, and in the absence of gold-standard deceptive opinions, they train models using features based on the review text, reviewer, and product, to distinguish between duplicate opinions7 (considered deceptive spam) and non-duplicate opinions (considered truthful). Wu et al. (2010) propose an alternative strategy for detecting </context>
</contexts>
<marker>Ntoulas, Najork, Manasse, Fetterly, 2006</marker>
<rawString>A. Ntoulas, M. Najork, M. Manasse, and D. Fetterly. 2006. Detecting spam web pages through content analysis. In Proceedings of the 15th international conference on World Wide Web, pages 83–92. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P O’Mahony</author>
<author>B Smyth</author>
</authors>
<title>Learning to recommend helpful hotel reviews.</title>
<date>2009</date>
<booktitle>In Proceedings of the third ACM conference on Recommender systems,</booktitle>
<pages>305--308</pages>
<publisher>ACM.</publisher>
<marker>O’Mahony, Smyth, 2009</marker>
<rawString>M.P. O’Mahony and B. Smyth. 2009. Learning to recommend helpful hotel reviews. In Proceedings of the third ACM conference on Recommender systems, pages 305–308. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Peng</author>
<author>D Schuurmans</author>
</authors>
<title>Combining naive Bayes and n-gram language models for text classification.</title>
<date>2003</date>
<booktitle>Advances in Information Retrieval,</booktitle>
<pages>547--547</pages>
<contexts>
<context position="23143" citStr="Peng and Schuurmans, 2003" startWordPosition="3628" endWordPosition="3631">et subsumes the preceding feature set. 4.4 Classifiers Features from the three approaches just introduced are used to train Naive Bayes and Support Vector Machine classifiers, both of which have performed well in related work (Jindal and Liu, 2008; Mihalcea and Strapparava, 2009; Zhou et al., 2008). For a document x, with label y, the Naive Bayes (NB) classifier gives us the following decision rule: y� = arg max Pr(y = c) · Pr(x |y = c) (1) c When the class prior is uniform, for example when the classes are balanced (as in our case), (1) can be simplified to the maximum likelihood classifier (Peng and Schuurmans, 2003): Pr(x |y = c) (2) Under (2), both the NB classifier used by Mihalcea and Strapparava (2009) and the language model classifier used by Zhou et al. (2008) are equivalent. Thus, following Zhou et al. (2008), we use the SRI Language Modeling Toolkit (Stolcke, 2002) to estimate individual language models, Pr(x |y = c), for truthful and deceptive opinions. We consider all three n-gram feature sets, namely UNIGRAMS, BIGRAMS+, and TRIGRAMS+, with corresponding language models smoothed using the interpolated Kneser-Ney method (Chen and Goodman, 1996). We also train Support Vector Machine (SVM) classif</context>
</contexts>
<marker>Peng, Schuurmans, 2003</marker>
<rawString>F. Peng and D. Schuurmans. 2003. Combining naive Bayes and n-gram language models for text classification. Advances in Information Retrieval, pages 547– 547.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J W Pennebaker</author>
<author>C K Chung</author>
<author>M Ireland</author>
<author>A Gonzales</author>
<author>R J Booth</author>
</authors>
<title>The development and psychometric properties of LIWC2007.</title>
<date>2007</date>
<location>Austin, TX, LIWC. Net.</location>
<contexts>
<context position="20520" citStr="Pennebaker et al., 2007" startWordPosition="3211" endWordPosition="3214">at the frequency distribution of part-of-speech (POS) tags in a text is often dependent on the genre of the text (Biber et al., 1999; Rayson et al., 2001). In our genre identification approach to deceptive opinion spam detection, we test if such a relationship exists for truthful and deceptive reviews by constructing, for each review, features based on the frequencies of each POS tag.15 These features are also intended to provide a good baseline with which to compare our other automated approaches. 4.2 Psycholinguistic deception detection The Linguistic Inquiry and Word Count (LIWC) software (Pennebaker et al., 2007) is a popular automated text analysis tool used widely in the social sciences. It has been used to detect personality 15We use the Stanford Parser (Klein and Manning, 2003) to obtain the relative POS frequencies. 313 traits (Mairesse et al., 2007), to study tutoring dynamics (Cade et al., 2010), and, most relevantly, to analyze deception (Hancock et al., 2008; Mihalcea and Strapparava, 2009; Vrij et al., 2007). While LIWC does not include a text classifier, we can create one with features derived from the LIWC output. In particular, LIWC counts and groups the number of instances of nearly 4,50</context>
</contexts>
<marker>Pennebaker, Chung, Ireland, Gonzales, Booth, 2007</marker>
<rawString>J.W. Pennebaker, C.K. Chung, M. Ireland, A. Gonzales, and R.J. Booth. 2007. The development and psychometric properties of LIWC2007. Austin, TX, LIWC. Net.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Quadrianto</author>
<author>A J Smola</author>
<author>T S Caetano</author>
<author>Q V Le</author>
</authors>
<title>Estimating labels from label proportions.</title>
<date>2009</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>10</volume>
<pages>2374</pages>
<contexts>
<context position="25964" citStr="Quadrianto et al., 2009" startWordPosition="4079" endWordPosition="4082">on experiments. Reported precision, recall and F-score are computed using a micro-average, i.e., from the aggregate true positive, false positive and false negative rates, as suggested by Forman and Scholz (2009). Human performance is repeated here for JUDGE 1, JUDGE 2 and the SKEPTIC meta-judge, although they cannot be directly compared since the 160-opinion subset on which they are assessed only corresponds to the first cross-validation fold. 5 Results and Discussion The deception detection strategies described in Section 4 are evaluated using a 5-fold nested crossvalidation (CV) procedure (Quadrianto et al., 2009), where model parameters are selected for each test fold based on standard CV experiments on the training folds. Folds are selected so that each contains all reviews from four hotels; thus, learned models are always evaluated on reviews from unseen hotels. Results appear in Table 3. We observe that automated classifiers outperform human judges for every metric, except truthful recall where JUDGE 2 performs best.16 However, this is expected given that untrained humans often focus on unreliable cues to deception (Vrij, 2008). For example, one study examining deception in online dating found that</context>
</contexts>
<marker>Quadrianto, Smola, Caetano, Le, 2009</marker>
<rawString>N. Quadrianto, A.J. Smola, T.S. Caetano, and Q.V. Le. 2009. Estimating labels from label proportions. The Journal of Machine Learning Research, 10:2349– 2374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Rayson</author>
<author>A Wilson</author>
<author>G Leech</author>
</authors>
<title>Grammatical word class variation within the British National Corpus sampler.</title>
<date>2001</date>
<journal>Language and Computers,</journal>
<volume>36</volume>
<issue>1</issue>
<pages>306</pages>
<contexts>
<context position="5087" citStr="Rayson et al., 2001" startWordPosition="781" endWordPosition="784"> a standard text categorization task, in which we use n-gram–based classifiers to label opinions as either deceptive or truthful (Joachims, 1998; Sebastiani, 2002); (b) an instance of psycholinguistic deception detection, in which we expect deceptive statements to exemplify the psychological effects of lying, such as increased negative emotion and psychological distancing (Hancock et al., 2008; Newman et al., 2003); and, (c) a problem of genre identification, in which we view deceptive and truthful writing as sub-genres of imaginative and informative writing, respectively (Biber et al., 1999; Rayson et al., 2001). We compare the performance of each approach on our novel dataset. Particularly, we find that machine learning classifiers trained on features traditionally employed in (a) psychological studies of deception and (b) genre identification are both outperformed at statistically significant levels by ngram–based text categorization techniques. Notably, a combined classifier with both n-gram and psychological deception features achieves nearly 90% cross-validated accuracy on this task. In contrast, we find deceptive opinion spam detection to be well beyond the capabilities of most human judges, wh</context>
<context position="20050" citStr="Rayson et al., 2001" startWordPosition="3141" endWordPosition="3144">dges of deception (Vrij, 2008), and therefore they perform nearly at-chance respective to one another. 4 Automated Approaches to Deceptive Opinion Spam Detection We consider three automated approaches to detecting deceptive opinion spam, each of which utilizes classifiers (described in Section 4.4) trained on the dataset of Section 3. The features employed by each strategy are outlined here. 4.1 Genre identification Work in computational linguistics has shown that the frequency distribution of part-of-speech (POS) tags in a text is often dependent on the genre of the text (Biber et al., 1999; Rayson et al., 2001). In our genre identification approach to deceptive opinion spam detection, we test if such a relationship exists for truthful and deceptive reviews by constructing, for each review, features based on the frequencies of each POS tag.15 These features are also intended to provide a good baseline with which to compare our other automated approaches. 4.2 Psycholinguistic deception detection The Linguistic Inquiry and Word Count (LIWC) software (Pennebaker et al., 2007) is a popular automated text analysis tool used widely in the social sciences. It has been used to detect personality 15We use the</context>
<context position="27567" citStr="Rayson et al. (2001)" startWordPosition="4332" endWordPosition="4335">tioned in Section 3.3, JUDGE 2 classified fewer than 12% of opinions as deceptive. While achieving 95% truthful recall, this judge’s corresponding precision was not significantly better than chance (two-tailed binomial p = 0.4). mated classifier outperforms most human judges (one-tailed sign test p = 0.06, 0.01, 0.001 for the three judges, respectively, on the first fold). This result is best explained by theories of reality monitoring (Johnson and Raye, 1981), which suggest that truthful and deceptive opinions might be classified into informative and imaginative genres, respectively. Work by Rayson et al. (2001) has found strong distributional differences between informative and imaginative writing, namely that the former typically consists of more nouns, adjectives, prepositions, determiners, and coordinating conjunctions, while the latter consists of more verbs,17 adverbs,18 pronouns, and pre-determiners. Indeed, we find that the weights learned by POSSVM (found in Table 4) are largely in agreement with these findings, notably except for adjective and adverb superlatives, the latter of which was found to be an exception by Rayson et al. (2001). However, that deceptive opinions contain more superlat</context>
<context position="29242" citStr="Rayson et al. (2001)" startWordPosition="4563" endWordPosition="4566">ight NOUNS Singular 0.008 VERBS Base -0.057 Plural 0.002 Past tense 0.041 Proper, singular -0.041 Present participle -0.089 Proper, plural 0.091 Singular, present -0.031 ADJECTIVES General 0.002 Third person 0.026 singular, present Comparative 0.058 Superlative -0.164 Modal -0.063 PREPOSITIONS General 0.064 ADVERBS General 0.001 0.009 DETERMINERS General Comparative -0.035 COORD. CONJ. General 0.094 PRONOUNS Personal -0.098 0.053 VERBS Past participle Possessive -0.303 ADVERBS Superlative -0.094 PRE-DETERMINERS General 0.017 Table 4: Average feature weights learned by POSSVM. Based on work by Rayson et al. (2001), we expect weights on the left to be positive (predictive of truthful opinions), and weights on the right to be negative (predictive of deceptive opinions). Boldface entries are at odds with these expectations. We report average feature weights of unit-normalized weight vectors, rather than raw weights vectors, to account for potential differences in magnitude between the folds. genre identification baseline just discussed. Specifically, the psycholinguistic approach (LIWCSVM) proposed in Section 4.2 performs 3.8% more accurately (one-tailed sign test p = 0.02), and the standard text categori</context>
</contexts>
<marker>Rayson, Wilson, Leech, 2001</marker>
<rawString>P. Rayson, A. Wilson, and G. Leech. 2001. Grammatical word class variation within the British National Corpus sampler. Language and Computers, 36(1):295– 306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R A Rigby</author>
<author>D M Stasinopoulos</author>
</authors>
<title>Generalized additive models for location, scale and shape.</title>
<date>2005</date>
<journal>Journal of the Royal Statistical Society: Series C (Applied Statistics),</journal>
<volume>54</volume>
<issue>3</issue>
<contexts>
<context position="16715" citStr="Rigby and Stasinopoulos, 2005" startWordPosition="2604" endWordPosition="2607">proach to assessing human performance on this task was with Mechanical Turk. Unfortunately, we found that some Turkers selected among the choices seemingly at random, presumably to maximize their hourly earnings by obviating the need to read the review. While a similar effect has been observed previously (Akkaya et al., 2010), there remains no universal solution. Instead, we solicit the help of three volunteer undergraduate university students to make judgments on a subset of our data. This balanced subset, corresponding to the first fold of our cross-validation 14We use the R package GAMLSS (Rigby and Stasinopoulos, 2005) to fit the left-truncated log-normal distribution. 312 TRUTHFUL DECEPTIVE Accuracy P R F P R F HUMAN JUDGE 1 61.9% 57.9 87.5 69.7 74.4 36.3 48.7 JUDGE 2 56.9% 53.9 95.0 68.8 78.9 18.8 30.3 JUDGE 3 53.1% 52.3 70.0 59.9 54.7 36.3 43.6 META MAJORITY 58.1% 54.8 92.5 68.8 76.0 23.8 36.2 SKEPTIC 60.6% 60.8 60.0 60.4 60.5 61.3 60.9 Table 2: Performance of three human judges and two meta-judges on a subset of 160 opinions, corresponding to the first fold of our cross-validation experiments in Section 5. Boldface indicates the largest value for each column. experiments described in Section 5, contains</context>
</contexts>
<marker>Rigby, Stasinopoulos, 2005</marker>
<rawString>R.A. Rigby and D.M. Stasinopoulos. 2005. Generalized additive models for location, scale and shape. Journal of the Royal Statistical Society: Series C (Applied Statistics), 54(3):507–554.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sebastiani</author>
</authors>
<title>Machine learning in automated text categorization.</title>
<date>2002</date>
<journal>ACM computing surveys (CSUR),</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="4630" citStr="Sebastiani, 2002" startWordPosition="712" endWordPosition="713">dures for evaluation. In contrast, one contribution of the work presented here is the creation of the first largescale, publicly available6 dataset for deceptive opinion spam research, containing 400 truthful and 400 gold-standard deceptive reviews. To obtain a deeper understanding of the nature of deceptive opinion spam, we explore the relative utility of three potentially complementary framings of our problem. Specifically, we view the task as: (a) a standard text categorization task, in which we use n-gram–based classifiers to label opinions as either deceptive or truthful (Joachims, 1998; Sebastiani, 2002); (b) an instance of psycholinguistic deception detection, in which we expect deceptive statements to exemplify the psychological effects of lying, such as increased negative emotion and psychological distancing (Hancock et al., 2008; Newman et al., 2003); and, (c) a problem of genre identification, in which we view deceptive and truthful writing as sub-genres of imaginative and informative writing, respectively (Biber et al., 1999; Rayson et al., 2001). We compare the performance of each approach on our novel dataset. Particularly, we find that machine learning classifiers trained on features</context>
</contexts>
<marker>Sebastiani, 2002</marker>
<rawString>F. Sebastiani. 2002. Machine learning in automated text categorization. ACM computing surveys (CSUR), 34(1):1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M ´A Serrano</author>
<author>A Flammini</author>
<author>F Menczer</author>
</authors>
<title>Modeling statistical properties of written text.</title>
<date>2009</date>
<journal>PloS one,</journal>
<pages>4--4</pages>
<contexts>
<context position="15168" citStr="Serrano et al. (2009)" startWordPosition="2365" endWordPosition="2368">sing http://tagthe.net. at least 150 characters long (see footnote 11 in Section 3.1); • 1,607 reviews written by first-time authors— new users who have not previously posted an opinion on TripAdvisor—since these opinions are more likely to contain opinion spam, which would reduce the integrity of our truthful review data (Wu et al., 2010). Finally, we balance the number of truthful and deceptive opinions by selecting 400 of the remaining 2,124 truthful reviews, such that the document lengths of the selected truthful reviews are similarly distributed to those of the deceptive reviews. Work by Serrano et al. (2009) suggests that a log-normal distribution is appropriate for modeling document lengths. Thus, for each of the 20 chosen hotels, we select 20 truthful reviews from a log-normal (lefttruncated at 150 characters) distribution fit to the lengths of the deceptive reviews.14 Combined with the 400 deceptive reviews gathered in Section 3.1 this yields our final dataset of 800 reviews. 3.3 Human performance Assessing human deception detection performance is important for several reasons. First, there are few other baselines for our classification task; indeed, related studies (Jindal and Liu, 2008; Miha</context>
</contexts>
<marker>Serrano, Flammini, Menczer, 2009</marker>
<rawString>M. ´A. Serrano, A. Flammini, and F. Menczer. 2009. Modeling statistical properties of written text. PloS one, 4(4):5372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM-an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Seventh International Conference on Spoken Language Processing,</booktitle>
<volume>3</volume>
<pages>901--904</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="23405" citStr="Stolcke, 2002" startWordPosition="3676" endWordPosition="3677"> 2009; Zhou et al., 2008). For a document x, with label y, the Naive Bayes (NB) classifier gives us the following decision rule: y� = arg max Pr(y = c) · Pr(x |y = c) (1) c When the class prior is uniform, for example when the classes are balanced (as in our case), (1) can be simplified to the maximum likelihood classifier (Peng and Schuurmans, 2003): Pr(x |y = c) (2) Under (2), both the NB classifier used by Mihalcea and Strapparava (2009) and the language model classifier used by Zhou et al. (2008) are equivalent. Thus, following Zhou et al. (2008), we use the SRI Language Modeling Toolkit (Stolcke, 2002) to estimate individual language models, Pr(x |y = c), for truthful and deceptive opinions. We consider all three n-gram feature sets, namely UNIGRAMS, BIGRAMS+, and TRIGRAMS+, with corresponding language models smoothed using the interpolated Kneser-Ney method (Chen and Goodman, 1996). We also train Support Vector Machine (SVM) classifiers, which find a high-dimensional separating hyperplane between two groups of data. To simplify feature analysis in Section 5, we restrict our evaluation to linear SVMs, which learn a weight vector w and bias term b, such that a document x can be classified by</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM-an extensible language modeling toolkit. In Seventh International Conference on Spoken Language Processing, volume 3, pages 901– 904. Citeseer.</rawString>
</citation>
<citation valid="false">
<authors>
<author>C Toma</author>
<author>J T Hancock</author>
</authors>
<title>In Press. What Lies Beneath: The Linguistic Traces of Deception in Online Dating Profiles.</title>
<journal>Journal of Communication.</journal>
<marker>Toma, Hancock, </marker>
<rawString>C. Toma and J.T. Hancock. In Press. What Lies Beneath: The Linguistic Traces of Deception in Online Dating Profiles. Journal of Communication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Vrij</author>
<author>S Mann</author>
<author>S Kristen</author>
<author>R P Fisher</author>
</authors>
<title>Cues to deception and ability to detect lies as a function of police interview styles. Law and human behavior,</title>
<date>2007</date>
<pages>31--5</pages>
<contexts>
<context position="20933" citStr="Vrij et al., 2007" startWordPosition="3281" endWordPosition="3284">ded to provide a good baseline with which to compare our other automated approaches. 4.2 Psycholinguistic deception detection The Linguistic Inquiry and Word Count (LIWC) software (Pennebaker et al., 2007) is a popular automated text analysis tool used widely in the social sciences. It has been used to detect personality 15We use the Stanford Parser (Klein and Manning, 2003) to obtain the relative POS frequencies. 313 traits (Mairesse et al., 2007), to study tutoring dynamics (Cade et al., 2010), and, most relevantly, to analyze deception (Hancock et al., 2008; Mihalcea and Strapparava, 2009; Vrij et al., 2007). While LIWC does not include a text classifier, we can create one with features derived from the LIWC output. In particular, LIWC counts and groups the number of instances of nearly 4,500 keywords into 80 psychologically meaningful dimensions. We construct one feature for each of the 80 LIWC dimensions, which can be summarized broadly under the following four categories: 1. Linguistic processes: Functional aspects of text (e.g., the average number of words per sentence, the rate of misspelling, swearing, etc.) 2. Psychological processes: Includes all social, emotional, cognitive, perceptual a</context>
</contexts>
<marker>Vrij, Mann, Kristen, Fisher, 2007</marker>
<rawString>A. Vrij, S. Mann, S. Kristen, and R.P. Fisher. 2007. Cues to deception and ability to detect lies as a function of police interview styles. Law and human behavior, 31(5):499–518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Vrij</author>
<author>S Leal</author>
<author>P A Granhag</author>
<author>S Mann</author>
<author>R P Fisher</author>
<author>J Hillman</author>
<author>K Sperry</author>
</authors>
<title>Outsmarting the liars: The benefit of asking unanticipated questions. Law and human behavior,</title>
<date>2009</date>
<pages>33--2</pages>
<contexts>
<context position="6481" citStr="Vrij et al., 2009" startWordPosition="982" endWordPosition="985">ive opinion spam. 6Available by request at: http://www.cs.cornell. edu/˜myleott/op_spam Additionally, we make several theoretical contributions based on an examination of the feature weights learned by our machine learning classifiers. Specifically, we shed light on an ongoing debate in the deception literature regarding the importance of considering the context and motivation of a deception, rather than simply identifying a universal set of deception cues. We also present findings that are consistent with recent work highlighting the difficulties that liars have encoding spatial information (Vrij et al., 2009). Lastly, our study of deceptive opinion spam detection as a genre identification problem reveals relationships between deceptive opinions and imaginative writing, and between truthful opinions and informative writing. The rest of this paper is organized as follows: in Section 2, we summarize related work; in Section 3, we explain our methodology for gathering data and evaluate human performance; in Section 4, we describe the features and classifiers employed by our three automated detection approaches; in Section 5, we present and discuss experimental results; finally, conclusions and directi</context>
<context position="32063" citStr="Vrij et al. (2009)" startWordPosition="4972" endWordPosition="4975">uman auxverb $ while past future hotel. husband inhibition perceptual other my husband assent feel Table 5: Top 15 highest weighted truthful and deceptive features learned by LIWC+BIGRAMSVM and LIWCSVM. Ambiguous features are subscripted to indicate the source of the feature. LIWC features correspond to groups of keywords as explained in Section 4.2; more details about LIWC and the LIWC categories are available at http://liwc.net. particular, truthful opinions are more specific about spatial configurations (e.g., small, bathroom, on, location). This finding is also supported by recent work by Vrij et al. (2009) suggesting that liars have considerable difficultly encoding spatial information into their lies. Accordingly, we observe an increased focus in deceptive opinions on aspects external to the hotel being reviewed (e.g., husband, business, 316 vacation). We also acknowledge several findings that, on the surface, are in contrast to previous psycholinguistic studies of deception (Hancock et al., 2008; Newman et al., 2003). For instance, while deception is often associated with negative emotion terms, our deceptive reviews have more positive and fewer negative emotion terms. This pattern makes sens</context>
</contexts>
<marker>Vrij, Leal, Granhag, Mann, Fisher, Hillman, Sperry, 2009</marker>
<rawString>A. Vrij, S. Leal, P.A. Granhag, S. Mann, R.P. Fisher, J. Hillman, and K. Sperry. 2009. Outsmarting the liars: The benefit of asking unanticipated questions. Law and human behavior, 33(2):159–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Vrij</author>
</authors>
<title>Detecting lies and deceit: Pitfalls and opportunities.</title>
<date>2008</date>
<publisher>Wiley-Interscience.</publisher>
<contexts>
<context position="18309" citStr="Vrij, 2008" startWordPosition="2874" endWordPosition="2875">AJORITY meta-judge predicts “deceptive” when at least two out of three human judges believe the review to be deceptive, and the SKEPTIC meta-judge predicts “deceptive” when any human judge believes the review to be deceptive. Human and meta-judge performance is given in Table 2. It is clear from the results that human judges are not particularly effective at this task. Indeed, a two-tailed binomial test fails to reject the null hypothesis that JUDGE 2 and JUDGE 3 perform at-chance (p = 0.003, 0.10, 0.48 for the three judges, respectively). Furthermore, all three judges suffer from truth-bias (Vrij, 2008), a common finding in deception detection research in which human judges are more likely to classify an opinion as truthful than deceptive. In fact, JUDGE 2 classified fewer than 12% of the opinions as deceptive! Interestingly, this bias is effectively smoothed by the SKEPTIC meta-judge, which produces nearly perfectly class-balanced predictions. A subsequent reevaluation of human performance on this task suggests that the truth-bias can be reduced if judges are given the class-proportions in advance, although such prior knowledge is unrealistic; and ultimately, performance remains similar to </context>
<context position="26492" citStr="Vrij, 2008" startWordPosition="4166" endWordPosition="4167">ated using a 5-fold nested crossvalidation (CV) procedure (Quadrianto et al., 2009), where model parameters are selected for each test fold based on standard CV experiments on the training folds. Folds are selected so that each contains all reviews from four hotels; thus, learned models are always evaluated on reviews from unseen hotels. Results appear in Table 3. We observe that automated classifiers outperform human judges for every metric, except truthful recall where JUDGE 2 performs best.16 However, this is expected given that untrained humans often focus on unreliable cues to deception (Vrij, 2008). For example, one study examining deception in online dating found that humans perform at-chance detecting deceptive profiles because they rely on text-based cues that are unrelated to deception, such as second-person pronouns (Toma and Hancock, In Press). Among the automated classifiers, baseline performance is given by the simple genre identification approach (POSSVM) proposed in Section 4.1. Surprisingly, we find that even this simple auto16As mentioned in Section 3.3, JUDGE 2 classified fewer than 12% of opinions as deceptive. While achieving 95% truthful recall, this judge’s correspondin</context>
</contexts>
<marker>Vrij, 2008</marker>
<rawString>A. Vrij. 2008. Detecting lies and deceit: Pitfalls and opportunities. Wiley-Interscience.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Weerkamp</author>
<author>M De Rijke</author>
</authors>
<title>Credibility improves topical blog post retrieval. ACL-08: HLT,</title>
<date>2008</date>
<pages>923--931</pages>
<marker>Weerkamp, De Rijke, 2008</marker>
<rawString>W. Weerkamp and M. De Rijke. 2008. Credibility improves topical blog post retrieval. ACL-08: HLT, pages 923–931.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Weimer</author>
<author>I Gurevych</author>
<author>M M¨uhlh¨auser</author>
</authors>
<title>Automatically assessing the post quality in online discussions on software.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,</booktitle>
<pages>125--128</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Weimer, Gurevych, M¨uhlh¨auser, 2007</marker>
<rawString>M. Weimer, I. Gurevych, and M. M¨uhlh¨auser. 2007. Automatically assessing the post quality in online discussions on software. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 125–128. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Wu</author>
<author>D Greene</author>
<author>B Smyth</author>
<author>P Cunningham</author>
</authors>
<title>Distortion as a validation criterion in the identification of suspicious reviews.</title>
<date>2010</date>
<tech>Technical report, UCD-CSI2010-04,</tech>
<institution>University College Dublin.</institution>
<contexts>
<context position="7387" citStr="Wu et al., 2010" startWordPosition="1127" endWordPosition="1130">ze related work; in Section 3, we explain our methodology for gathering data and evaluate human performance; in Section 4, we describe the features and classifiers employed by our three automated detection approaches; in Section 5, we present and discuss experimental results; finally, conclusions and directions for future work are given in Section 6. 2 Related Work Spam has historically been studied in the contexts of e-mail (Drucker et al., 2002), and the Web (Gy¨ongyi et al., 2004; Ntoulas et al., 2006). Recently, researchers have began to look at opinion spam as well (Jindal and Liu, 2008; Wu et al., 2010; Yoo and Gretzel, 2009). Jindal and Liu (2008) find that opinion spam is both widespread and different in nature from either e-mail or Web spam. Using product review data, and in the absence of gold-standard deceptive opinions, they train models using features based on the review text, reviewer, and product, to distinguish between duplicate opinions7 (considered deceptive spam) and non-duplicate opinions (considered truthful). Wu et al. (2010) propose an alternative strategy for detecting deceptive opinion spam in the absence 7Duplicate (or near-duplicate) opinions are opinions that appear mo</context>
<context position="10181" citStr="Wu et al., 2010" startWordPosition="1558" endWordPosition="1561">een studied—directly (Weimer et al., 2007), and in the contexts of helpfulness (Danescu-Niculescu-Mizil et al., 2009; Kim et al., 2006; O’Mahony and Smyth, 2009) and credibility (Weerkamp and De Rijke, 2008). Unfortunately, most measures of quality employed in those works are based exclusively on human judgments, which we find in Section 3 to be poorly calibrated to detecting deceptive opinion spam. 3 Dataset Construction and Human Performance While truthful opinions are ubiquitous online, deceptive opinions are difficult to obtain without resorting to heuristic methods (Jindal and Liu, 2008; Wu et al., 2010). In this section, we report our efforts to gather (and validate with human judgments) the first publicly available opinion spam dataset with gold-standard deceptive opinions. Following the work of Yoo and Gretzel (2009), we compare truthful and deceptive positive reviews for hotels found on TripAdvisor. Specifically, we mine all 5-star truthful reviews from the 20 most popular hotels on TripAdvisor8 in the Chicago area.9 Deceptive opinions are gathered for those same 20 hotels using Amazon Mechanical Turk10 (AMT). Below, we provide details of the collection methodologies for deceptive (Sectio</context>
<context position="14888" citStr="Wu et al., 2010" startWordPosition="2320" endWordPosition="2323">with fewer than 150 characters since, by construction, deceptive opinions are 11A submission is considered unreasonably short if it contains fewer than 150 characters. 12Submissions are individually checked for plagiarism at http://plagiarisma.net. 13Language is determined using http://tagthe.net. at least 150 characters long (see footnote 11 in Section 3.1); • 1,607 reviews written by first-time authors— new users who have not previously posted an opinion on TripAdvisor—since these opinions are more likely to contain opinion spam, which would reduce the integrity of our truthful review data (Wu et al., 2010). Finally, we balance the number of truthful and deceptive opinions by selecting 400 of the remaining 2,124 truthful reviews, such that the document lengths of the selected truthful reviews are similarly distributed to those of the deceptive reviews. Work by Serrano et al. (2009) suggests that a log-normal distribution is appropriate for modeling document lengths. Thus, for each of the 20 chosen hotels, we select 20 truthful reviews from a log-normal (lefttruncated at 150 characters) distribution fit to the lengths of the deceptive reviews.14 Combined with the 400 deceptive reviews gathered in</context>
</contexts>
<marker>Wu, Greene, Smyth, Cunningham, 2010</marker>
<rawString>G. Wu, D. Greene, B. Smyth, and P. Cunningham. 2010. Distortion as a validation criterion in the identification of suspicious reviews. Technical report, UCD-CSI2010-04, University College Dublin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K H Yoo</author>
<author>U Gretzel</author>
</authors>
<title>Comparison of Deceptive and Truthful Travel Reviews. Information and Communication Technologies in Tourism</title>
<date>2009</date>
<pages>37--47</pages>
<contexts>
<context position="7411" citStr="Yoo and Gretzel, 2009" startWordPosition="1131" endWordPosition="1134">in Section 3, we explain our methodology for gathering data and evaluate human performance; in Section 4, we describe the features and classifiers employed by our three automated detection approaches; in Section 5, we present and discuss experimental results; finally, conclusions and directions for future work are given in Section 6. 2 Related Work Spam has historically been studied in the contexts of e-mail (Drucker et al., 2002), and the Web (Gy¨ongyi et al., 2004; Ntoulas et al., 2006). Recently, researchers have began to look at opinion spam as well (Jindal and Liu, 2008; Wu et al., 2010; Yoo and Gretzel, 2009). Jindal and Liu (2008) find that opinion spam is both widespread and different in nature from either e-mail or Web spam. Using product review data, and in the absence of gold-standard deceptive opinions, they train models using features based on the review text, reviewer, and product, to distinguish between duplicate opinions7 (considered deceptive spam) and non-duplicate opinions (considered truthful). Wu et al. (2010) propose an alternative strategy for detecting deceptive opinion spam in the absence 7Duplicate (or near-duplicate) opinions are opinions that appear more than once in the corp</context>
<context position="10401" citStr="Yoo and Gretzel (2009)" startWordPosition="1592" endWordPosition="1595">tunately, most measures of quality employed in those works are based exclusively on human judgments, which we find in Section 3 to be poorly calibrated to detecting deceptive opinion spam. 3 Dataset Construction and Human Performance While truthful opinions are ubiquitous online, deceptive opinions are difficult to obtain without resorting to heuristic methods (Jindal and Liu, 2008; Wu et al., 2010). In this section, we report our efforts to gather (and validate with human judgments) the first publicly available opinion spam dataset with gold-standard deceptive opinions. Following the work of Yoo and Gretzel (2009), we compare truthful and deceptive positive reviews for hotels found on TripAdvisor. Specifically, we mine all 5-star truthful reviews from the 20 most popular hotels on TripAdvisor8 in the Chicago area.9 Deceptive opinions are gathered for those same 20 hotels using Amazon Mechanical Turk10 (AMT). Below, we provide details of the collection methodologies for deceptive (Section 3.1) and truthful opinions (Section 3.2). Ultimately, we collect 20 truthful and 20 deceptive opinions for each of the 20 chosen hotels (800 opinions total). 3.1 Deceptive opinions via Mechanical Turk Crowdsourcing ser</context>
</contexts>
<marker>Yoo, Gretzel, 2009</marker>
<rawString>K.H. Yoo and U. Gretzel. 2009. Comparison of Deceptive and Truthful Travel Reviews. Information and Communication Technologies in Tourism 2009, pages 37–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zhou</author>
<author>J K Burgoon</author>
<author>D P Twitchell</author>
<author>T Qin</author>
<author>J F Nunamaker Jr</author>
</authors>
<title>A comparison of classification methods for predicting deception in computermediated communication.</title>
<date>2004</date>
<journal>Journal of Management Information Systems,</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="9100" citStr="Zhou et al. (2004" startWordPosition="1392" endWordPosition="1395">nd Gretzel (2009) gather 40 truthful and 42 deceptive hotel reviews and, using a standard statistical test, manually compare the psychologically relevant linguistic differences between them. In contrast, we create a much larger dataset of 800 opinions that we use to develop and evaluate automated deception classifiers. Research has also been conducted on the related task of psycholinguistic deception detection. Newman et al. (2003), and later Mihalcea and Strapparava (2009), ask participants to give both their true and untrue views on personal issues (e.g., their stance on the death penalty). Zhou et al. (2004; 2008) consider computer-mediated deception in role-playing games designed to be played over instant messaging and e-mail. However, while these studies compare n-gram–based deception classifiers to a random guess baseline of 50%, we additionally evaluate and compare two other computational approaches (described in Section 4), as well as the performance of human judges (described in Section 3.3). Lastly, automatic approaches to determining review quality have been studied—directly (Weimer et al., 2007), and in the contexts of helpfulness (Danescu-Niculescu-Mizil et al., 2009; Kim et al., 2006;</context>
<context position="21852" citStr="Zhou et al. (2004)" startWordPosition="3422" endWordPosition="3425">ons, which can be summarized broadly under the following four categories: 1. Linguistic processes: Functional aspects of text (e.g., the average number of words per sentence, the rate of misspelling, swearing, etc.) 2. Psychological processes: Includes all social, emotional, cognitive, perceptual and biological processes, as well as anything related to time or space. 3. Personal concerns: Any references to work, leisure, money, religion, etc. 4. Spoken categories: Primarily filler and agreement words. While other features have been considered in past deception detection work, notably those of Zhou et al. (2004), early experiments found LIWC features to perform best. Indeed, the LIWC2007 software used in our experiments subsumes most of the features introduced in other work. Thus, we focus our psycholinguistic approach to deception detection on LIWC-based features. 4.3 Text categorization In contrast to the other strategies just discussed, our text categorization approach to deception detection allows us to model both content and context with n-gram features. Specifically, we consider the following three n-gram feature sets, with the corresponding features lowercased and unstemmed: UNIGRAMS, BIGRAMS+</context>
</contexts>
<marker>Zhou, Burgoon, Twitchell, Qin, Jr, 2004</marker>
<rawString>L. Zhou, J.K. Burgoon, D.P. Twitchell, T. Qin, and J.F. Nunamaker Jr. 2004. A comparison of classification methods for predicting deception in computermediated communication. Journal of Management Information Systems, 20(4):139–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zhou</author>
<author>Y Shi</author>
<author>D Zhang</author>
</authors>
<title>A Statistical Language Modeling Approach to Online Deception Detection.</title>
<date>2008</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>20</volume>
<issue>8</issue>
<contexts>
<context position="22816" citStr="Zhou et al., 2008" startWordPosition="3565" endWordPosition="3568">tegorization approach to deception detection allows us to model both content and context with n-gram features. Specifically, we consider the following three n-gram feature sets, with the corresponding features lowercased and unstemmed: UNIGRAMS, BIGRAMS+, TRIGRAMS+, where the superscript + indicates that the feature set subsumes the preceding feature set. 4.4 Classifiers Features from the three approaches just introduced are used to train Naive Bayes and Support Vector Machine classifiers, both of which have performed well in related work (Jindal and Liu, 2008; Mihalcea and Strapparava, 2009; Zhou et al., 2008). For a document x, with label y, the Naive Bayes (NB) classifier gives us the following decision rule: y� = arg max Pr(y = c) · Pr(x |y = c) (1) c When the class prior is uniform, for example when the classes are balanced (as in our case), (1) can be simplified to the maximum likelihood classifier (Peng and Schuurmans, 2003): Pr(x |y = c) (2) Under (2), both the NB classifier used by Mihalcea and Strapparava (2009) and the language model classifier used by Zhou et al. (2008) are equivalent. Thus, following Zhou et al. (2008), we use the SRI Language Modeling Toolkit (Stolcke, 2002) to estimat</context>
</contexts>
<marker>Zhou, Shi, Zhang, 2008</marker>
<rawString>L. Zhou, Y. Shi, and D. Zhang. 2008. A Statistical Language Modeling Approach to Online Deception Detection. IEEE Transactions on Knowledge and Data Engineering, 20(8):1077–1081.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>