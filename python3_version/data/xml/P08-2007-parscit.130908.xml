<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001610">
<title confidence="0.828924">
The Complexity of Phrase Alignment Problems
</title>
<author confidence="0.99534">
John DeNero and Dan Klein
</author>
<affiliation confidence="0.99769">
Computer Science Division, EECS Department
University of California at Berkeley
</affiliation>
<email confidence="0.99759">
{denero, klein}@cs.berkeley.edu
</email>
<sectionHeader confidence="0.994765" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999077090909091">
Many phrase alignment models operate over
the combinatorial space of bijective phrase
alignments. We prove that finding an optimal
alignment in this space is NP-hard, while com-
puting alignment expectations is #P-hard. On
the other hand, we show that the problem of
finding an optimal alignment can be cast as
an integer linear program, which provides a
simple, declarative approach to Viterbi infer-
ence for phrase alignment models that is em-
pirically quite efficient.
</bodyText>
<sectionHeader confidence="0.998429" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999959173913044">
Learning in phrase alignment models generally re-
quires computing either Viterbi phrase alignments
or expectations of alignment links. For some re-
stricted combinatorial spaces of alignments—those
that arise in ITG-based phrase models (Cherry and
Lin, 2007) or local distortion models (Zens et al.,
2004)—inference can be accomplished using poly-
nomial time dynamic programs. However, for more
permissive models such as Marcu and Wong (2002)
and DeNero et al. (2006), which operate over the full
space of bijective phrase alignments (see below), no
polynomial time algorithms for exact inference have
been exhibited. Indeed, Marcu and Wong (2002)
conjectures that none exist. In this paper, we show
that Viterbi inference in this full space is NP-hard,
while computing expectations is #P-hard.
On the other hand, we give a compact formula-
tion of Viterbi inference as an integer linear program
(ILP). Using this formulation, exact solutions to the
Viterbi search problem can be found by highly op-
timized, general purpose ILP solvers. While ILP
is of course also NP-hard, we show that, empir-
ically, exact solutions are found very quickly for
</bodyText>
<page confidence="0.968938">
25
</page>
<bodyText confidence="0.9994535">
most problem instances. In an experiment intended
to illustrate the practicality of the ILP approach, we
show speed and search accuracy results for aligning
phrases under a standard phrase translation model.
</bodyText>
<sectionHeader confidence="0.973973" genericHeader="method">
2 Phrase Alignment Problems
</sectionHeader>
<bodyText confidence="0.999516">
Rather than focus on a particular model, we describe
four problems that arise in training phrase alignment
models.
</bodyText>
<subsectionHeader confidence="0.994335">
2.1 Weighted Sentence Pairs
</subsectionHeader>
<bodyText confidence="0.9998303">
A sentence pair consists of two word sequences, e
and f. A set of phrases {eij} contains all spans eij
from between-word positions i to j of e. A link is an
aligned pair of phrases, denoted (eij, fkl).&apos;
Let a weighted sentence pair additionally include
a real-valued function 0 : {eij}x{fkl} —* R, which
scores links. 0(eij, fkl) can be sentence-specific, for
example encoding the product of a translation model
and a distortion model for (eij, fkl). We impose no
additional restrictions on 0 for our analysis.
</bodyText>
<subsectionHeader confidence="0.9996">
2.2 Bijective Phrase Alignments
</subsectionHeader>
<bodyText confidence="0.999966142857143">
An alignment is a set of links. Given a weighted
sentence pair, we will consider the space of bijective
phrase alignments A: those a C {eij} x {fkl} that
use each word token in exactly one link. We first
define the notion of a partition: UiSi = T means Si
are pairwise disjoint and cover T. Then, we can for-
mally define the set of bijective phrase alignments:
</bodyText>
<footnote confidence="0.564909">
&apos;As in parsing, the position between each word is assigned
an index, where 0 is to the left of the first word. In this paper,
we assume all phrases have length at least one: j &gt; i and l &gt; k.
</footnote>
<figure confidence="0.533987333333333">
A = { Ua : Ueij = e ; fkl = f }
(eij,fkl)Ea (eij,fkl)Ea
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 25–28,
</figure>
<page confidence="0.418206">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.8087025">
Both the conditional model of DeNero et al.
(2006) and the joint model of Marcu and Wong
(2002) operate in A, as does the phrase-based de-
coding framework of Koehn et al. (2003).
</bodyText>
<subsectionHeader confidence="0.998233">
2.3 Problem Definitions
</subsectionHeader>
<bodyText confidence="0.9991705">
For a weighted sentence pair (e, f, φ), let the score
of an alignment be the product of its link scores:
</bodyText>
<equation confidence="0.9760385">
φ(a) = rl φ(eij, fkl).
(eij,fkl)Ea
</equation>
<bodyText confidence="0.950863904761905">
Four related problems involving scored alignments
arise when training phrase alignment models.
OPTIMIZATION, O: Given (e, f, φ), find the high-
est scoring alignment a.
DECISION, D: Given (e, f, φ), decide if there is an
alignment a with φ(a) &gt; 1.
O arises in the popular Viterbi approximation to
EM (Hard EM) that assumes probability mass is
concentrated at the mode of the posterior distribu-
tion over alignments. D is the corresponding deci-
sion problem for O, useful in analysis.
EXPECTATION, £: Given a weighted sentence pair
(e, f, φ) and indices i, j, k,l, compute Ea φ(a)
over all a E A such that (eij, fkl) E a.
SUM, S: Given (e, f, φ), compute EaEA φ(a).
£ arises in computing sufficient statistics for
re-estimating phrase translation probabilities (E-
step) when training models. The existence of a
polynomial time algorithm for £ implies a poly-
nomial time algorithm for S, because A =
U;e1 1 Ukf |0 Ulf=k+1 {a : (e0j, fkl) E a, a E A}.
</bodyText>
<sectionHeader confidence="0.559847" genericHeader="method">
3 Complexity of Inference in A
</sectionHeader>
<bodyText confidence="0.9999804">
For the space A of bijective alignments, problems £
and O have long been suspected of being NP-hard,
first asserted but not proven in Marcu and Wong
(2002). We give a novel proof that O is NP-hard,
showing that D is NP-complete by reduction from
SAT, the boolean satisfiability problem. This re-
sult holds despite the fact that the related problem of
finding an optimal matching in a weighted bipartite
graph (the ASSIGNMENT problem) is polynomial-
time solvable using the Hungarian algorithm.
</bodyText>
<subsectionHeader confidence="0.999575">
3.1 Reducing Satisfiability to D
</subsectionHeader>
<bodyText confidence="0.999889047619048">
A reduction proof of NP-completeness gives a con-
struction by which a known NP-complete problem
can be solved via a newly proposed problem. From a
SAT instance, we construct a weighted sentence pair
for which alignments with positive score correspond
exactly to the SAT solutions. Since SAT is NP-
complete and our construction requires only poly-
nomial time, we conclude that D is NP-complete.2
SAT: Given vectors of boolean variables v = (v)
and propositional clauses3 C = (C), decide
whether there exists an assignment to v that si-
multaneously satisfies each clause in C.
For a SAT instance (v, C), we construct f to con-
tain one word for each clause, and e to contain sev-
eral copies of the literals that appear in those clauses.
φ scores only alignments from clauses to literals that
satisfy the clauses. The crux of the construction lies
in ensuring that no variable is assigned both true and
false. The details of constructing such a weighted
sentence pair wsp(v, C) = (e, f, φ), described be-
low, are also depicted in figure 1.
</bodyText>
<listItem confidence="0.903027">
1. f contains a word for each C, followed by an
assignment word for each variable, assign(v).
2. e contains c(`) consecutive words for each lit-
eral `, where c(`) is the number of times that `
appears in the clauses.
</listItem>
<bodyText confidence="0.991264">
Then, we set φ(·, ·) = 0 everywhere except:
</bodyText>
<listItem confidence="0.999570666666667">
3. For all clauses C and each satisfying literal `,
and each one-word phrase e in e containing `,
φ(e, fC) = 1. fC is the one-word phrase con-
taining C in f.
4. The assign(v) words in f align to longer phrases
of literals and serve to consistently assign each
variable by using up inconsistent literals. They
also align to unused literals to yield a bijection.
Let ek[`] be the phrase in e containing all literals
` and k negations of `. fassign(v) is the one-word
phrase for assign(v). Then, φ(ek[`], fassign(v)) =
1 for ` E {v, v} and all applicable k.
</listItem>
<footnote confidence="0.9608415">
2Note that D is trivially in NP: given an alignment a, it is
easy to determine whether or not 0(a) &gt; 1.
3A clause is a disjunction of literals. A literal is a bare vari-
able v. or its negation v,,,. For instance, v2 V v7 V vy is a clause.
</footnote>
<page confidence="0.983437">
26
</page>
<figure confidence="0.953550357142857">
v1 v1 V1 V1 v2 v2 V2 V2 v3 v3 V3 v3 v1 v1 V1 �v1 v2 v2 V2 &apos;U2 v3 v3 V3 v3
�v3
v1 v2 v3
�v1 v2
v1
v2 v3
a g (vi)
a g (v2)
a g (v3)
vl true
V2 false
V3 false
�v1 v2 v3
(a) ( ) ( )
</figure>
<figureCaption confidence="0.995171">
Figure 1: (a) The clauses of an example SAT instance with v = (v1, v2, v3). (b) The weighted sentence pair wsp(v, C)
constructed from the SAT instance. All links that have 0 = 1 are marked with a blue horizontal stripe. Stripes in the
last three rows demarcate the alignment options for each assign(v,,,), which consume all words for some literal. (c) A
bijective alignment with score 1. (d) The corresponding satisfying assignment for the original SAT instance.
</figureCaption>
<equation confidence="0.7038875">
Claim 1. If wsp(v, C) has an alignment a with
0(a) &gt; 1, then (v, C) is satisfiable.
</equation>
<bodyText confidence="0.999046166666667">
Proof. The score implies that f aligns using all one-
word phrases and Vai E a, 0(ai) = 1. By condition
4, each fassign(v) aligns to all v� or all v in e. Then,
assign each v to true if fassign(v) aligns to all v, and
false otherwise. By condition 3, each C must align
to a satisfying literal, while condition 4 assures that
all available literals are consistent with this assign-
ment to v, which therefore satisfies C.
Claim 2. If (v, C) is satisfiable, then wsp(v, C) has
an alignment a with 0(a) = 1.
Proof. We construct such an alignment a from the
satisfying assignment v. For each C, we choose a
satisfying literal E consistent with the assignment.
Align fC to the first available E token in e if the cor-
responding v is true, or the last if v is false. Align
each fassign(v) to all remaining literals for v.
Claims 1 and 2 together show that D is NP-
complete, and therefore that O is NP-hard.
</bodyText>
<subsectionHeader confidence="0.999413">
3.2 Reducing Perfect Matching to S
</subsectionHeader>
<bodyText confidence="0.9999864">
With another construction, we can show that S is #P-
hard, meaning that it is at least as hard as any #P-
complete problem. #P is a class of counting prob-
lems related to NP, and #P-hard problems are NP-
hard as well.
</bodyText>
<sectionHeader confidence="0.806197" genericHeader="method">
COUNTING PERFECT MATCHINGS, CPM
</sectionHeader>
<bodyText confidence="0.995157">
Given a bipartite graph G with 2n vertices,
count the number of matchings of size n.
For a bipartite graph G with edge set E = {(vj, vl)},
we construct e and f with n words each, and set
0(ej−1 j, fl−1 l) = 1 and 0 otherwise. The num-
ber of perfect matchings in G is the sum S for
this weighted sentence pair. CPM is #P-complete
(Valiant, 1979), so S (and hence £) is #P-hard.
</bodyText>
<sectionHeader confidence="0.893721" genericHeader="method">
4 Solving the Optimization Problem
</sectionHeader>
<bodyText confidence="0.997318">
Although O is NP-hard, we present an approach to
solving it using integer linear programming (ILP).
</bodyText>
<subsectionHeader confidence="0.998368">
4.1 Previous Inference Approaches
</subsectionHeader>
<bodyText confidence="0.999942470588235">
Marcu and Wong (2002) describes an approximation
to O. Given a weighted sentence pair, high scoring
phrases are linked together greedily to reach an ini-
tial alignment. Then, local operators are applied to
hill-climb A in search of the maximum a. This pro-
cedure also approximates £ by collecting weighted
counts as the space is traversed.
DeNero et al. (2006) instead proposes an
exponential-time dynamic program to systemati-
cally explore A, which can in principle solve either
O or £. In practice, however, the space of align-
ments has to be pruned severely using word align-
ments to control the running time of EM.
Notably, neither of these inference approaches of-
fers any test to know if the optimal alignment is ever
found. Furthermore, they both require small data
sets due to computational expense.
</bodyText>
<subsectionHeader confidence="0.994015">
4.2 Alignment via an Integer Program
</subsectionHeader>
<bodyText confidence="0.9998495">
We cast O as an ILP problem, for which many opti-
mization techniques are well known. First, we in-
</bodyText>
<page confidence="0.995192">
27
</page>
<bodyText confidence="0.953401416666667">
troduce binary indicator variables ai,j,k,l denoting
whether (eij, fkl) ∈ a. Furthermore, we introduce
binary indicators ei,j and fk,l that denote whether
some (eij, ·) or (·, fkl) appears in a, respectively. Fi-
nally, we represent the weight function 0 as a weight
vector in the program: wi,j,k,l = log 0(eij, fkl).
Now, we can express an integer program that,
when optimized, will yield the optimal alignment of
our weighted sentence pair.
Sentences per hour on a four-core server
Frequency of optimal solutions found
Frequency of e-optimal solutions found
</bodyText>
<tableCaption confidence="0.9957045">
Table 1: The solver, tuned for speed, regularly reports
solutions that are within 10−5 of optimal.
</tableCaption>
<bodyText confidence="0.999337125">
Using an off-the-shelf ILP solver,4 we were able
to quickly and reliably find the globally optimal
phrase alignment under 0(eij, fkl) derived from the
Moses pipeline (Koehn et al., 2007).5 Table 1 shows
that finding the optimal phrase alignment is accurate
and efficient.6 Hence, this simple search technique
effectively addresses the intractability challenges in-
herent in evaluating new phrase alignment ideas.
</bodyText>
<figure confidence="0.997713846153846">
Emax
s.t. i,j,k,l E
i,j:i&lt;x&lt;j
ei,j = 1 ∀x : 1 ≤ x ≤ |e |(1)
wi,j,k,l · ai,j,k,l
20,000
93.4%
99.2%
E fk,l = 1 ∀y : 1 ≤ y ≤ |f |(2)
k,l:k&lt;y&lt;l References
with the following constraints on index variables:
0 ≤ i &lt; |e|, 0 &lt; j ≤ |e|, i &lt; j
0 ≤ k &lt; |f|, 0 &lt; l ≤ |f|, k &lt; l .
</figure>
<bodyText confidence="0.999733375">
The objective function is log 0(a) for a implied
by {ai,j,k,l = 1}. Constraint equation 1 ensures that
the English phrases form a partition of e – each word
in e appears in exactly one phrase – as does equa-
tion 2 for f. Constraint equation 3 ensures that each
phrase in the chosen partition of e appears in exactly
one link, and that phrases not in the partition are not
aligned (and likewise constraint 4 for f).
</bodyText>
<sectionHeader confidence="0.997708" genericHeader="method">
5 Applications
</sectionHeader>
<bodyText confidence="0.992074461538462">
The need to find an optimal phrase alignment for a
weighted sentence pair arises in at least two appli-
cations. First, a generative phrase alignment model
can be trained with Viterbi EM by finding optimal
phrase alignments of a training corpus (approximate
E-step), then re-estimating phrase translation param-
eters from those alignments (M-step).
Second, this is an algorithm for forced decoding:
finding the optimal phrase-based derivation of a par-
ticular target sentence. Forced decoding arises in
online discriminative training, where model updates
are made toward the most likely derivation of a gold
translation (Liang et al., 2006).
</bodyText>
<reference confidence="0.997754428571428">
Colin Cherry and Dekang Lin. 2007. Inversion transduc-
tion grammar for joint phrasal translation modeling.
In NAACL-HLT Workshop on Syntax and Structure in
Statistical Translation.
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why generative phrase models underperform
surface heuristics. In NAACL Workshop on Statistical
Machine Translation.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In HLT-
NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL.
Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative ap-
proach to machine translation. In ACL.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In EMNLP.
Leslie G. Valiant. 1979. The complexity of computing
the permanent. In Theoretical Computer Science 8.
Richard Zens, Hermann Ney, Taro Watanabeand, and
E. Sumita. 2004. Reordering constraints for phrase
based statistical machine translation. In Coling.
</reference>
<footnote confidence="0.979809857142857">
4We used Mosek: www.mosek.com.
50(eij, fki) was estimated using the relative frequency of
phrases extracted by the default Moses training script. We eval-
uated on English-Spanish Europarl, sentences up to length 25.
6ILP solvers include many parameters that trade off speed
for accuracy. Substantial speed gains also follow from explicitly
pruning the values of ILP variables based on prior information.
</footnote>
<equation confidence="0.897944">
Eei,j = ai,j,k,l ∀i, j (3)
k,l
Efk,l = ai,j,k,l ∀k, l (4)
i,j
</equation>
<page confidence="0.996654">
28
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.948241">
<title confidence="0.999957">The Complexity of Phrase Alignment Problems</title>
<author confidence="0.999931">John DeNero</author>
<author confidence="0.999931">Dan Klein</author>
<affiliation confidence="0.9999105">Computer Science Division, EECS Department University of California at Berkeley</affiliation>
<abstract confidence="0.99542225">Many phrase alignment models operate over combinatorial space of phrase We prove that finding an optimal alignment in this space is NP-hard, while computing alignment expectations is #P-hard. On the other hand, we show that the problem of finding an optimal alignment can be cast as an integer linear program, which provides a simple, declarative approach to Viterbi inference for phrase alignment models that is empirically quite efficient.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Dekang Lin</author>
</authors>
<title>Inversion transduction grammar for joint phrasal translation modeling.</title>
<date>2007</date>
<booktitle>In NAACL-HLT Workshop on Syntax and Structure in Statistical Translation.</booktitle>
<contexts>
<context position="929" citStr="Cherry and Lin, 2007" startWordPosition="132" endWordPosition="135">ng an optimal alignment in this space is NP-hard, while computing alignment expectations is #P-hard. On the other hand, we show that the problem of finding an optimal alignment can be cast as an integer linear program, which provides a simple, declarative approach to Viterbi inference for phrase alignment models that is empirically quite efficient. 1 Introduction Learning in phrase alignment models generally requires computing either Viterbi phrase alignments or expectations of alignment links. For some restricted combinatorial spaces of alignments—those that arise in ITG-based phrase models (Cherry and Lin, 2007) or local distortion models (Zens et al., 2004)—inference can be accomplished using polynomial time dynamic programs. However, for more permissive models such as Marcu and Wong (2002) and DeNero et al. (2006), which operate over the full space of bijective phrase alignments (see below), no polynomial time algorithms for exact inference have been exhibited. Indeed, Marcu and Wong (2002) conjectures that none exist. In this paper, we show that Viterbi inference in this full space is NP-hard, while computing expectations is #P-hard. On the other hand, we give a compact formulation of Viterbi infe</context>
</contexts>
<marker>Cherry, Lin, 2007</marker>
<rawString>Colin Cherry and Dekang Lin. 2007. Inversion transduction grammar for joint phrasal translation modeling. In NAACL-HLT Workshop on Syntax and Structure in Statistical Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Dan Gillick</author>
<author>James Zhang</author>
<author>Dan Klein</author>
</authors>
<title>Why generative phrase models underperform surface heuristics.</title>
<date>2006</date>
<booktitle>In NAACL Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="1137" citStr="DeNero et al. (2006)" startWordPosition="165" endWordPosition="168"> program, which provides a simple, declarative approach to Viterbi inference for phrase alignment models that is empirically quite efficient. 1 Introduction Learning in phrase alignment models generally requires computing either Viterbi phrase alignments or expectations of alignment links. For some restricted combinatorial spaces of alignments—those that arise in ITG-based phrase models (Cherry and Lin, 2007) or local distortion models (Zens et al., 2004)—inference can be accomplished using polynomial time dynamic programs. However, for more permissive models such as Marcu and Wong (2002) and DeNero et al. (2006), which operate over the full space of bijective phrase alignments (see below), no polynomial time algorithms for exact inference have been exhibited. Indeed, Marcu and Wong (2002) conjectures that none exist. In this paper, we show that Viterbi inference in this full space is NP-hard, while computing expectations is #P-hard. On the other hand, we give a compact formulation of Viterbi inference as an integer linear program (ILP). Using this formulation, exact solutions to the Viterbi search problem can be found by highly optimized, general purpose ILP solvers. While ILP is of course also NP-ha</context>
<context position="3548" citStr="DeNero et al. (2006)" startWordPosition="577" endWordPosition="580">k. We first define the notion of a partition: UiSi = T means Si are pairwise disjoint and cover T. Then, we can formally define the set of bijective phrase alignments: &apos;As in parsing, the position between each word is assigned an index, where 0 is to the left of the first word. In this paper, we assume all phrases have length at least one: j &gt; i and l &gt; k. A = { Ua : Ueij = e ; fkl = f } (eij,fkl)Ea (eij,fkl)Ea Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 25–28, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics Both the conditional model of DeNero et al. (2006) and the joint model of Marcu and Wong (2002) operate in A, as does the phrase-based decoding framework of Koehn et al. (2003). 2.3 Problem Definitions For a weighted sentence pair (e, f, φ), let the score of an alignment be the product of its link scores: φ(a) = rl φ(eij, fkl). (eij,fkl)Ea Four related problems involving scored alignments arise when training phrase alignment models. OPTIMIZATION, O: Given (e, f, φ), find the highest scoring alignment a. DECISION, D: Given (e, f, φ), decide if there is an alignment a with φ(a) &gt; 1. O arises in the popular Viterbi approximation to EM (Hard EM) </context>
<context position="10223" citStr="DeNero et al. (2006)" startWordPosition="1819" endWordPosition="1822">s weighted sentence pair. CPM is #P-complete (Valiant, 1979), so S (and hence £) is #P-hard. 4 Solving the Optimization Problem Although O is NP-hard, we present an approach to solving it using integer linear programming (ILP). 4.1 Previous Inference Approaches Marcu and Wong (2002) describes an approximation to O. Given a weighted sentence pair, high scoring phrases are linked together greedily to reach an initial alignment. Then, local operators are applied to hill-climb A in search of the maximum a. This procedure also approximates £ by collecting weighted counts as the space is traversed. DeNero et al. (2006) instead proposes an exponential-time dynamic program to systematically explore A, which can in principle solve either O or £. In practice, however, the space of alignments has to be pruned severely using word alignments to control the running time of EM. Notably, neither of these inference approaches offers any test to know if the optimal alignment is ever found. Furthermore, they both require small data sets due to computational expense. 4.2 Alignment via an Integer Program We cast O as an ILP problem, for which many optimization techniques are well known. First, we in27 troduce binary indic</context>
</contexts>
<marker>DeNero, Gillick, Zhang, Klein, 2006</marker>
<rawString>John DeNero, Dan Gillick, James Zhang, and Dan Klein. 2006. Why generative phrase models underperform surface heuristics. In NAACL Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In HLTNAACL.</booktitle>
<contexts>
<context position="3674" citStr="Koehn et al. (2003)" startWordPosition="601" endWordPosition="604"> the set of bijective phrase alignments: &apos;As in parsing, the position between each word is assigned an index, where 0 is to the left of the first word. In this paper, we assume all phrases have length at least one: j &gt; i and l &gt; k. A = { Ua : Ueij = e ; fkl = f } (eij,fkl)Ea (eij,fkl)Ea Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 25–28, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics Both the conditional model of DeNero et al. (2006) and the joint model of Marcu and Wong (2002) operate in A, as does the phrase-based decoding framework of Koehn et al. (2003). 2.3 Problem Definitions For a weighted sentence pair (e, f, φ), let the score of an alignment be the product of its link scores: φ(a) = rl φ(eij, fkl). (eij,fkl)Ea Four related problems involving scored alignments arise when training phrase alignment models. OPTIMIZATION, O: Given (e, f, φ), find the highest scoring alignment a. DECISION, D: Given (e, f, φ), decide if there is an alignment a with φ(a) &gt; 1. O arises in the popular Viterbi approximation to EM (Hard EM) that assumes probability mass is concentrated at the mode of the posterior distribution over alignments. D is the correspondin</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="11646" citStr="Koehn et al., 2007" startWordPosition="2054" endWordPosition="2057">resent the weight function 0 as a weight vector in the program: wi,j,k,l = log 0(eij, fkl). Now, we can express an integer program that, when optimized, will yield the optimal alignment of our weighted sentence pair. Sentences per hour on a four-core server Frequency of optimal solutions found Frequency of e-optimal solutions found Table 1: The solver, tuned for speed, regularly reports solutions that are within 10−5 of optimal. Using an off-the-shelf ILP solver,4 we were able to quickly and reliably find the globally optimal phrase alignment under 0(eij, fkl) derived from the Moses pipeline (Koehn et al., 2007).5 Table 1 shows that finding the optimal phrase alignment is accurate and efficient.6 Hence, this simple search technique effectively addresses the intractability challenges inherent in evaluating new phrase alignment ideas. Emax s.t. i,j,k,l E i,j:i&lt;x&lt;j ei,j = 1 ∀x : 1 ≤ x ≤ |e |(1) wi,j,k,l · ai,j,k,l 20,000 93.4% 99.2% E fk,l = 1 ∀y : 1 ≤ y ≤ |f |(2) k,l:k&lt;y&lt;l References with the following constraints on index variables: 0 ≤ i &lt; |e|, 0 &lt; j ≤ |e|, i &lt; j 0 ≤ k &lt; |f|, 0 &lt; l ≤ |f|, k &lt; l . The objective function is log 0(a) for a implied by {ai,j,k,l = 1}. Constraint equation 1 ensures that th</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Dan Klein</author>
<author>Ben Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<marker>Liang, Bouchard-Cˆot´e, Klein, Taskar, 2006</marker>
<rawString>Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein, and Ben Taskar. 2006. An end-to-end discriminative approach to machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>William Wong</author>
</authors>
<title>A phrase-based, joint probability model for statistical machine translation.</title>
<date>2002</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1112" citStr="Marcu and Wong (2002)" startWordPosition="160" endWordPosition="163"> cast as an integer linear program, which provides a simple, declarative approach to Viterbi inference for phrase alignment models that is empirically quite efficient. 1 Introduction Learning in phrase alignment models generally requires computing either Viterbi phrase alignments or expectations of alignment links. For some restricted combinatorial spaces of alignments—those that arise in ITG-based phrase models (Cherry and Lin, 2007) or local distortion models (Zens et al., 2004)—inference can be accomplished using polynomial time dynamic programs. However, for more permissive models such as Marcu and Wong (2002) and DeNero et al. (2006), which operate over the full space of bijective phrase alignments (see below), no polynomial time algorithms for exact inference have been exhibited. Indeed, Marcu and Wong (2002) conjectures that none exist. In this paper, we show that Viterbi inference in this full space is NP-hard, while computing expectations is #P-hard. On the other hand, we give a compact formulation of Viterbi inference as an integer linear program (ILP). Using this formulation, exact solutions to the Viterbi search problem can be found by highly optimized, general purpose ILP solvers. While IL</context>
<context position="3593" citStr="Marcu and Wong (2002)" startWordPosition="586" endWordPosition="589">: UiSi = T means Si are pairwise disjoint and cover T. Then, we can formally define the set of bijective phrase alignments: &apos;As in parsing, the position between each word is assigned an index, where 0 is to the left of the first word. In this paper, we assume all phrases have length at least one: j &gt; i and l &gt; k. A = { Ua : Ueij = e ; fkl = f } (eij,fkl)Ea (eij,fkl)Ea Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 25–28, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics Both the conditional model of DeNero et al. (2006) and the joint model of Marcu and Wong (2002) operate in A, as does the phrase-based decoding framework of Koehn et al. (2003). 2.3 Problem Definitions For a weighted sentence pair (e, f, φ), let the score of an alignment be the product of its link scores: φ(a) = rl φ(eij, fkl). (eij,fkl)Ea Four related problems involving scored alignments arise when training phrase alignment models. OPTIMIZATION, O: Given (e, f, φ), find the highest scoring alignment a. DECISION, D: Given (e, f, φ), decide if there is an alignment a with φ(a) &gt; 1. O arises in the popular Viterbi approximation to EM (Hard EM) that assumes probability mass is concentrated</context>
<context position="4969" citStr="Marcu and Wong (2002)" startWordPosition="834" endWordPosition="837">hted sentence pair (e, f, φ) and indices i, j, k,l, compute Ea φ(a) over all a E A such that (eij, fkl) E a. SUM, S: Given (e, f, φ), compute EaEA φ(a). £ arises in computing sufficient statistics for re-estimating phrase translation probabilities (Estep) when training models. The existence of a polynomial time algorithm for £ implies a polynomial time algorithm for S, because A = U;e1 1 Ukf |0 Ulf=k+1 {a : (e0j, fkl) E a, a E A}. 3 Complexity of Inference in A For the space A of bijective alignments, problems £ and O have long been suspected of being NP-hard, first asserted but not proven in Marcu and Wong (2002). We give a novel proof that O is NP-hard, showing that D is NP-complete by reduction from SAT, the boolean satisfiability problem. This result holds despite the fact that the related problem of finding an optimal matching in a weighted bipartite graph (the ASSIGNMENT problem) is polynomialtime solvable using the Hungarian algorithm. 3.1 Reducing Satisfiability to D A reduction proof of NP-completeness gives a construction by which a known NP-complete problem can be solved via a newly proposed problem. From a SAT instance, we construct a weighted sentence pair for which alignments with positiv</context>
<context position="9886" citStr="Marcu and Wong (2002)" startWordPosition="1763" endWordPosition="1766">ms are NPhard as well. COUNTING PERFECT MATCHINGS, CPM Given a bipartite graph G with 2n vertices, count the number of matchings of size n. For a bipartite graph G with edge set E = {(vj, vl)}, we construct e and f with n words each, and set 0(ej−1 j, fl−1 l) = 1 and 0 otherwise. The number of perfect matchings in G is the sum S for this weighted sentence pair. CPM is #P-complete (Valiant, 1979), so S (and hence £) is #P-hard. 4 Solving the Optimization Problem Although O is NP-hard, we present an approach to solving it using integer linear programming (ILP). 4.1 Previous Inference Approaches Marcu and Wong (2002) describes an approximation to O. Given a weighted sentence pair, high scoring phrases are linked together greedily to reach an initial alignment. Then, local operators are applied to hill-climb A in search of the maximum a. This procedure also approximates £ by collecting weighted counts as the space is traversed. DeNero et al. (2006) instead proposes an exponential-time dynamic program to systematically explore A, which can in principle solve either O or £. In practice, however, the space of alignments has to be pruned severely using word alignments to control the running time of EM. Notably</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>Daniel Marcu and William Wong. 2002. A phrase-based, joint probability model for statistical machine translation. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leslie G Valiant</author>
</authors>
<title>The complexity of computing the permanent.</title>
<date>1979</date>
<journal>In Theoretical Computer Science</journal>
<volume>8</volume>
<contexts>
<context position="9663" citStr="Valiant, 1979" startWordPosition="1729" endWordPosition="1730">ucing Perfect Matching to S With another construction, we can show that S is #Phard, meaning that it is at least as hard as any #Pcomplete problem. #P is a class of counting problems related to NP, and #P-hard problems are NPhard as well. COUNTING PERFECT MATCHINGS, CPM Given a bipartite graph G with 2n vertices, count the number of matchings of size n. For a bipartite graph G with edge set E = {(vj, vl)}, we construct e and f with n words each, and set 0(ej−1 j, fl−1 l) = 1 and 0 otherwise. The number of perfect matchings in G is the sum S for this weighted sentence pair. CPM is #P-complete (Valiant, 1979), so S (and hence £) is #P-hard. 4 Solving the Optimization Problem Although O is NP-hard, we present an approach to solving it using integer linear programming (ILP). 4.1 Previous Inference Approaches Marcu and Wong (2002) describes an approximation to O. Given a weighted sentence pair, high scoring phrases are linked together greedily to reach an initial alignment. Then, local operators are applied to hill-climb A in search of the maximum a. This procedure also approximates £ by collecting weighted counts as the space is traversed. DeNero et al. (2006) instead proposes an exponential-time dy</context>
</contexts>
<marker>Valiant, 1979</marker>
<rawString>Leslie G. Valiant. 1979. The complexity of computing the permanent. In Theoretical Computer Science 8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
<author>Taro Watanabeand</author>
<author>E Sumita</author>
</authors>
<title>Reordering constraints for phrase based statistical machine translation. In Coling.</title>
<date>2004</date>
<contexts>
<context position="976" citStr="Zens et al., 2004" startWordPosition="140" endWordPosition="143">while computing alignment expectations is #P-hard. On the other hand, we show that the problem of finding an optimal alignment can be cast as an integer linear program, which provides a simple, declarative approach to Viterbi inference for phrase alignment models that is empirically quite efficient. 1 Introduction Learning in phrase alignment models generally requires computing either Viterbi phrase alignments or expectations of alignment links. For some restricted combinatorial spaces of alignments—those that arise in ITG-based phrase models (Cherry and Lin, 2007) or local distortion models (Zens et al., 2004)—inference can be accomplished using polynomial time dynamic programs. However, for more permissive models such as Marcu and Wong (2002) and DeNero et al. (2006), which operate over the full space of bijective phrase alignments (see below), no polynomial time algorithms for exact inference have been exhibited. Indeed, Marcu and Wong (2002) conjectures that none exist. In this paper, we show that Viterbi inference in this full space is NP-hard, while computing expectations is #P-hard. On the other hand, we give a compact formulation of Viterbi inference as an integer linear program (ILP). Using</context>
</contexts>
<marker>Zens, Ney, Watanabeand, Sumita, 2004</marker>
<rawString>Richard Zens, Hermann Ney, Taro Watanabeand, and E. Sumita. 2004. Reordering constraints for phrase based statistical machine translation. In Coling.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>