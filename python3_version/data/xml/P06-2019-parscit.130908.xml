<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9992605">
Constraint-based Sentence Compression
An Integer Programming Approach
</title>
<author confidence="0.993096">
James Clarke and Mirella Lapata
</author>
<affiliation confidence="0.828759">
School of Informatics, University of Edinburgh
2 Bucclecuch Place, Edinburgh EH8 9LW, UK
</affiliation>
<email confidence="0.997844">
jclarke@ed.ac.uk,mlap@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.99738" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998868">
The ability to compress sentences while
preserving their grammaticality and most
of their meaning has recently received
much attention. Our work views sentence
compression as an optimisation problem.
We develop an integer programming for-
mulation and infer globally optimal com-
pressions in the face of linguistically moti-
vated constraints. We show that such a for-
mulation allows for relatively simple and
knowledge-lean compression models that
do not require parallel corpora or large-
scale resources. The proposed approach
yields results comparable and in some
cases superior to state-of-the-art.
</bodyText>
<sectionHeader confidence="0.99939" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999863235294118">
A mechanism for automatically compressing sen-
tences while preserving their grammaticality and
most important information would greatly bene-
fit a wide range of applications. Examples include
text summarisation (Jing 2000), subtitle genera-
tion from spoken transcripts (Vandeghinste and
Pan 2004) and information retrieval (Olivers and
Dolan 1999). Sentence compression is a complex
paraphrasing task with information loss involv-
ing substitution, deletion, insertion, and reordering
operations. Recent years have witnessed increased
interest on a simpler instantiation of the compres-
sion problem, namely word deletion (Knight and
Marcu 2002; Riezler et al. 2003; Turner and Char-
niak 2005). More formally, given an input sen-
tence of words W = w1,w2,...,wn, a compression
is formed by removing any subset of these words.
Sentence compression has received both gener-
ative and discriminative formulations in the liter-
ature. Generative approaches (Knight and Marcu
2002; Turner and Charniak 2005) are instantia-
tions of the noisy-channel model: given a long sen-
tence l, the aim is to find the corresponding short
sentences which maximises the conditional prob-
ability P(s1l). In a discriminative setting (Knight
and Marcu 2002; Riezler et al. 2003; McDonald
2006), sentences are represented by a rich fea-
ture space (typically induced from parse trees) and
the goal is to learn rewrite rules indicating which
words should be deleted in a given context. Both
modelling paradigms assume access to a training
corpus consisting of original sentences and their
compressions.
Unsupervised approaches to the compression
problem are few and far between (see Hori and Fu-
rui 2004 and Turner and Charniak 2005 for excep-
tions). This is surprising considering that parallel
corpora of original-compressed sentences are not
naturally available in the way multilingual corpora
are. The scarcity of such data is demonstrated by
the fact that most work to date has focused on a
single parallel corpus, namely the Ziff-Davis cor-
pus (Knight and Marcu 2002). And some effort
into developing appropriate training data would be
necessary when porting existing algorithms to new
languages or domains.
In this paper we present an unsupervised model
of sentence compression that does not rely on a
parallel corpus – all that is required is a corpus
of uncompressed sentences and a parser. Given a
long sentence, our task is to form a compression
by preserving the words that maximise a scoring
function. In our case, the scoring function is an
n-gram language model, “with a few strings at-
tached”. While straightforward to estimate, a lan-
guage model is a fairly primitive scoring function:
it has no notion of the overall sentence structure,
grammaticality or underlying meaning. We thus
couple our language model with a small number
of structural and semantic constraints capturing
global properties of the compression process.
We encode the language model and linguistic
constraints as linear inequalities and use Integer
Programming (IP) to infer compressions that are
consistent with both. The IP formulation allows us
to capture global sentence properties and can be
easily manipulated to provide compressions tai-
lored for specific applications. For example, we
</bodyText>
<page confidence="0.975579">
144
</page>
<note confidence="0.7257985">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 144–151,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999829916666667">
could prevent overly long or overly short compres-
sions or generally avoid compressions that lack
a main verb or consist of repetitions of the same
word.
In the following section we provide an overview
of previous approaches to sentence compression.
In Section 3 we motivate the treatment of sentence
compression as an optimisation problem and for-
mulate our language model and constraints in the
IP framework. Section 4 discusses our experimen-
tal set-up and Section 5 presents our results. Dis-
cussion of future work concludes the paper.
</bodyText>
<sectionHeader confidence="0.998459" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999818684210527">
Jing (2000) was perhaps the first to tackle the sen-
tence compression problem. Her approach uses
multiple knowledge sources to determine which
phrases in a sentence to remove. Central to her
system is a grammar checking module that spec-
ifies which sentential constituents are grammati-
cally obligatory and should therefore be present
in the compression. This is achieved using sim-
ple rules and a large-scale lexicon. Other knowl-
edge sources include WordNet and corpus evi-
dence gathered from a parallel corpus of original-
compressed sentence pairs. A phrase is removed
only if it is not grammatically obligatory, not the
focus of the local context and has a reasonable
deletion probability (estimated from the parallel
corpus).
In contrast to Jing (2000), the bulk of the re-
search on sentence compression relies exclusively
on corpus data for modelling the compression
process without recourse to extensive knowledge
sources (e.g., WordNet). Approaches based on the
noisy-channel model (Knight and Marcu 2002;
Turner and Charniak 2005) consist of a source
model P(s) (whose role is to guarantee that the
generated compression is grammatical), a chan-
nel model P(l|s) (capturing the probability that
the long sentence l is an expansion of the com-
pressed sentences), and a decoder (which searches
for the compressions that maximises P(s)P(l|s)).
The channel model is typically estimated using
a parallel corpus, although Turner and Charniak
(2005) also present semi-supervised and unsu-
pervised variants of the channel model that esti-
mate P(l|s) without parallel data.
Discriminative formulations of the compres-
sion task include decision-tree learning (Knight
and Marcu 2002), maximum entropy (Riezler
et al. 2003), support vector machines (Nguyen
et al. 2004), and large-margin learning (McDonald
2006). We describe here the decision-tree model
in more detail since we will use it as a basis for
comparison when evaluating our own models (see
Section 4). According to this model, compression
is performed through a tree rewriting process in-
spired by the shift-reduce parsing paradigm. A se-
quence of shift-reduce-drop actions are performed
on a long parse tree, l, to create a smaller tree, s.
The compression process begins with an input
list generated from the leaves of the original sen-
tence’s parse tree and an empty stack. ‘Shift’ oper-
ations move leaves from the input list to the stack
while ‘drop’ operations delete from the input list.
Reduce operations are used to build trees from the
leaves on the stack. A decision-tree is trained on a
set of automatically generated learning cases from
a parallel corpus. Each learning case has a target
action associated with it and is decomposed into a
set of indicative features. The decision-tree learns
which action to perform given this set of features.
The final model is applied in a deterministic fash-
ion in which the features for the current state are
extracted and the decision-tree is queried. This is
repeated until the input list is empty and the final
compression is recovered by traversing the leaves
of resulting tree on the stack.
While most compression models operate over
constituents, Hori and Furui (2004) propose a
model which generates compressions through
word deletion. The model does not utilise parallel
data or syntactic information in any form. Given a
prespecified compression rate, it searches for the
compression with the highest score according to a
function measuring the importance of each word
and the linguistic likelihood of the resulting com-
pressions (language model probability). The score
is maximised through a dynamic programming al-
gorithm.
Although sentence compression has not been
explicitly formulated as an optimisation problem,
previous approaches have treated it in these terms.
The decoding process in the noisy-channel model
searches for the best compression given the source
and channel models. However, the compression
found is usually sub-optimal as heuristics are used
to reduce the search space or is only locally op-
timal due to the search method employed. The
decoding process used in Turner and Charniak’s
(2005) model first searches for the best combina-
tion of rules to apply. As they traverse their list
of compression rules they remove sentences out-
side the 100 best compressions (according to their
channel model). This list is eventually truncated
to 25 compressions.
In other models (Hori and Furui 2004; McDon-
ald 2006) the compression score is maximised
</bodyText>
<page confidence="0.998283">
145
</page>
<bodyText confidence="0.999947785714286">
using dynamic programming. The latter guaran-
tees we will find the global optimum provided the
principle of optimality holds. This principle states
that given the current state, the optimal decision
for each of the remaining stages does not depend
on previously reached stages or previously made
decisions (Winston and Venkataramanan 2003).
However, we know this to be false in the case of
sentence compression. For example, if we have
included modifiers to the left of a head noun in
the compression then it makes sense that we must
include the head also. With a dynamic program-
ming approach we cannot easily guarantee such
constraints hold.
</bodyText>
<sectionHeader confidence="0.992843" genericHeader="method">
3 Problem Formulation
</sectionHeader>
<bodyText confidence="0.999953065217392">
Our work models sentence compression explicitly
as an optimisation problem. There are 2n possible
compressions for each sentence and while many
of these will be unreasonable (Knight and Marcu
2002), it is unlikely that only one compression
will be satisfactory. Ideally, we require a func-
tion that captures the operations (or rules) that can
be performed on a sentence to create a compres-
sion while at the same time factoring how desir-
able each operation makes the resulting compres-
sion. We can then perform a search over all possi-
ble compressions and select the best one, as deter-
mined by how desirable it is.
Our formulation consists of two basic compo-
nents: a language model (scoring function) and a
small number of constraints ensuring that the re-
sulting compressions are structurally and semanti-
cally valid. Our task is to find a globally optimal
compression in the presence of these constraints.
We solve this inference problem using Integer Pro-
gramming without resorting to heuristics or ap-
proximations during the decoding process. Integer
programming has been recently applied to several
classification tasks, including relation extraction
(Roth and Yih 2004), semantic role labelling (Pun-
yakanok et al. 2004), and the generation of route
directions (Marciniak and Strube 2005).
Before describing our model in detail, we in-
troduce some of the concepts and terms used in
Linear Programming and Integer Programming
(see Winston and Venkataramanan 2003 for an in-
troduction). Linear Programming (LP) is a tool
for solving optimisation problems in which the
aim is to maximise (or minimise) a given function
with respect to a set of constraints. The function
to be maximised (or minimised) is referred to as
the objective function. Both the objective function
and constraints must be linear. A number of deci-
sion variables are under our control which exert
influence on the objective function. Specifically,
they have to be optimised in order to maximise
(or minimise) the objective function. Finally, a set
of constraints restrict the values that the decision
variables can take. Integer Programming is an ex-
tension of linear programming where all decision
variables must take integer values.
</bodyText>
<subsectionHeader confidence="0.996714">
3.1 Language Model
</subsectionHeader>
<bodyText confidence="0.999535285714286">
Assume we have a sentence W = w1,w2,...,wn
for which we wish to generate a compression.
We introduce a decision variable for each word
in the original sentence and constrain it to be bi-
nary; a value of 0 represents a word being dropped,
whereas a value of 1 includes the word in the com-
pression. Let:
</bodyText>
<equation confidence="0.6668">
1/2 1 if wi is in the compression
yi = 0 bi E [1...n]
otherwise
</equation>
<bodyText confidence="0.9999688">
If we were using a unigram language model,
our objective function would maximise the overall
sum of the decision variables (i.e., words) multi-
plied by their unigram probabilities (all probabili-
ties throughout this paper are log-transformed):
</bodyText>
<equation confidence="0.993554">
n
maxz = ∑ yi · P(wi)
i=1
</equation>
<bodyText confidence="0.998046461538462">
Thus if a word is selected, its corresponding yi is
given a value of 1, and its probability P(wi) ac-
cording to the language model will be counted in
our total score, z.
A unigram language model will probably gener-
ate many ungrammatical compressions. We there-
fore use a more context-aware model in our objec-
tive function, namely a trigram model. Formulat-
ing a trigram model in terms of an integer program
becomes a more involved task since we now must
make decisions based on word sequences rather
than isolated words. We first create some extra de-
cision variables:
</bodyText>
<figure confidence="0.481022125">
1/21 if wi starts the compression bi E [1... n]
pi 0 otherwise
1 if sequence wi,wj ends
the compression bi E [1...n−1]
0 otherwise bj E [i+ 1...n]
1 if sequence wi,wj,wk bi E [1...n−2]
is in the compression bj E [i + 1...n−1]
0 otherwise bk E [j +1...n]
</figure>
<bodyText confidence="0.9996028">
Our objective function is given in Equation (1).
This is the sum of all possible trigrams that can
occur in all compressions of the original sentence
where w0 represents the ‘start’ token and wi is the
ith word in sentence W. Equation (2) constrains
</bodyText>
<figure confidence="0.48452325">
⎧
⎨
⎩
qij =
⎧
⎨
⎩
xijk =
</figure>
<page confidence="0.990239">
146
</page>
<bodyText confidence="0.997704">
the decision variables to be binary.
</bodyText>
<equation confidence="0.9908142">
n
maxz = ∑ pi · P(wi|start)
i=1
subject to:
yi, pi,qij,xijk = 0 or 1 (2)
</equation>
<bodyText confidence="0.997191444444445">
The objective function in (1) allows any combi-
nation of trigrams to be selected. This means that
invalid trigram sequences (e.g., two or more tri-
grams containing the symbol ‘end’) could appear
in the output compression. We avoid this situation
by introducing sequential constraints (on the de-
cision variables yi,xijk, pi, and qij) that restrict the
set of allowable trigram combinations.
Constraint 1 Exactly one word can begin a
</bodyText>
<equation confidence="0.9021815">
sentence.
n
∑ pi = 1 (3)
i=1
</equation>
<bodyText confidence="0.8994905">
Constraint 2 If a word is included in the sen-
tence it must either start the sentence or be pre-
ceded by two other words or one other word and
the ‘start’ token w0.
</bodyText>
<equation confidence="0.998501">
xijk = 0 (4)
∀k : k ∈ [1...n]
</equation>
<bodyText confidence="0.959934">
Constraint 3 If a word is included in the sen-
tence it must either be preceded by one word and
followed by another or it must be preceded by one
word and end the sentence.
</bodyText>
<equation confidence="0.999773">
qij = 0 (5)
∀j : j ∈ [1...n]
</equation>
<bodyText confidence="0.94132425">
Constraint 4 If a word is in the sentence it
must be followed by two words or followed by one
word and then the end of the sentence or it must be
preceded by one word and end the sentence.
</bodyText>
<equation confidence="0.9982655">
qhi = 0 (6)
∀i : i ∈ [1...n]
</equation>
<bodyText confidence="0.9464435">
Constraint 5 Exactly one word pair can end
the sentence.
</bodyText>
<equation confidence="0.816964">
qij = 1 (7)
</equation>
<bodyText confidence="0.949955">
Example compressions using the trigram model
just described are given in Table 1. The model in
</bodyText>
<table confidence="0.93945975">
O: He became a power player in Greek Politics in
1974, when he founded the socialist Pasok Party.
LM: He became a player in the Pasok.
Mod: He became a player in the Pasok Party.
Sen: He became a player in politics.
Sig: He became a player in politics when he founded
the Pasok Party.
O: Finally, AppleShare Printer Server, formerly a
separate package, is now bundled with Apple-
Share File Server.
LM: Finally, AppleShare, a separate, AppleShare.
Mod: Finally, AppleShare Server, is bundled.
Sen: Finally, AppleShare Server, is bundled with
Server.
Sig: AppleShare Printer Server package is now bun-
dled with AppleShare File Server.
</table>
<tableCaption confidence="0.9824755">
Table 1: Compression examples (O: original sen-
tence, LM: compression with the trigram model,
Mod: compression with LM and modifier con-
straints, Sen: compression with LM, Mod and
sentential constraints, Sig: compression with LM,
Mod, Sen, and significance score)
</tableCaption>
<bodyText confidence="0.990429">
its current state does a reasonable job of modelling
local word dependencies, but is unable to capture
syntactic dependencies that could potentially al-
low more meaningful compressions. For example,
it does not know that Pasok Party is the object
of founded or that Appleshare modifies Printer
Server.
</bodyText>
<subsectionHeader confidence="0.998901">
3.2 Linguistic Constraints
</subsectionHeader>
<bodyText confidence="0.999959529411765">
In this section we propose a set of global con-
straints that extend the basic language model pre-
sented in Equations (1)–(7). Our aim is to bring
some syntactic knowledge into the compression
model and to preserve the meaning of the original
sentence as much as possible. Our constraints are
linguistically and semantically motivated in a sim-
ilar fashion to the grammar checking component
of Jing (2000). Importantly, we do not require any
additional knowledge sources (such as a lexicon)
beyond the parse and grammatical relations of the
original sentence. This is provided in our experi-
ments by the Robust Accurate Statistical Parsing
(RASP) toolkit (Briscoe and Carroll 2002). How-
ever, there is nothing inherent in our formulation
that restricts us to RASP; any other parser with
similar output could serve our purposes.
</bodyText>
<subsectionHeader confidence="0.980061">
Modifier Constraints Modifier constraints
</subsectionHeader>
<bodyText confidence="0.999467333333333">
ensure that relationships between head words and
their modifiers remain grammatical in the com-
pression:
</bodyText>
<equation confidence="0.890598583333333">
yi −yj ≥ 0 (8)
∀i, j : wj ∈ wi’s ncmods
yi −yj ≥ 0 (9)
∀i, j : wj ∈ wi’s detmods
xijk · P(wk|wi,wj)
qij ·P(end|wi,wj) (1)
n
∑
k=j+1
n−2
+ ∑
i=1
</equation>
<figure confidence="0.805862833333333">
n−1
∑
j=i+1
n−1
+ ∑
i=0
n
∑
j=i+1
k−1
∑
j=1
yk − pk −
k−2
∑
i=0
yj − j−1 n xijk − j−1
∑ ∑ ∑
i=0 k=j+1 i=0
yi −
n−1
∑
j=i+1
n
∑
j=i+1
n
∑
k=j+1
xijk −
i−1
qij −∑
h=0
n−1 n
∑ ∑
i=0 j=i+1
</figure>
<page confidence="0.988451">
147
</page>
<bodyText confidence="0.999810357142857">
Equation (8) guarantees that if we include a non-
clausal modifier (ncmod) in the compression then
the head of the modifier must also be included; this
is repeated for determiners (detmod) in (9).
We also want to ensure that the meaning of the
original sentence is preserved in the compression,
particularly in the face of negation. Equation (10)
implements this by forcing not in the compression
when the head is included. A similar constraint
is added for possessive modifiers (e.g., his, our),
as shown in Equation (11). Genitives (e.g., John’s
gift) are treated separately, mainly because they
are encoded as different relations in the parser (see
Equation (12)).
</bodyText>
<equation confidence="0.997204428571429">
yi −yj = 0 (10)
bi, j : wj E wi’s ncmods nwj = not
yi −yj = 0 (11)
bi, j : wj E wi’s possessive detmods
yi −yj = 0 (12)
bi, j : wi E possessive ncmods
nwj = possessive
</equation>
<bodyText confidence="0.996515466666667">
Compression examples with the addition of the
modifier constraints are shown in Table 1. Al-
though the compressions are grammatical (see the
inclusion of Party due to the modifier Pasok and
Server due to AppleShare), they are not entirely
meaning preserving.
Sentential Constraints We also define a few
intuitive constraints that take the overall sentence
structure into account. The first constraint (Equa-
tion (13)) ensures that if a verb is present in the
compression then so are its arguments, and if any
of the arguments are included in the compression
then the verb must also be included. We thus force
the program to make the same decision on the
verb, its subject, and object.
</bodyText>
<equation confidence="0.954574">
yi −yj = 0 (13)
bi, j : wj E subject/object of verb wi
</equation>
<bodyText confidence="0.999894666666667">
Our second constraint forces the compression to
contain at least one verb provided the original sen-
tence contains one as well:
</bodyText>
<equation confidence="0.905512">
∑ yi &gt; 1 (14)
icverbs
</equation>
<bodyText confidence="0.9785046">
Other sentential constraints include Equa-
tions (15) and (16) which apply to prepositional
phrases, wh-phrases and complements. These con-
straints force the introducing term (i.e., the prepo-
sition, complement or wh-word) to be included in
the compression if any word from within the syn-
tactic constituent is also included. The reverse is
also true, i.e., if the introducing term is included at
least one other word from the syntactic constituent
should also be included.
</bodyText>
<equation confidence="0.998650833333333">
yi −yj &gt; 0 (15)
bi, j : wj E PP/COMP/WH-P
nwi starts PP/COMP/WH-P
∑ yi −yj &gt; 0 (16)
iEPP/COMP/WH-P
bj : wj starts PP/COMP/WH-P
</equation>
<bodyText confidence="0.999979">
We also wish to handle coordination. If two head
words are conjoined in the original sentence, then
if they are included in the compression the coordi-
nating conjunction must also be included:
</bodyText>
<equation confidence="0.99016625">
(1−yi)+yj &gt; 1 (17)
(1−yi)+yk &gt; 1 (18)
yi +(1−yj)+(1−yk) &gt; 1 (19)
bi, j,k : wj n wk conjoined by wi
</equation>
<bodyText confidence="0.9991535">
Table 1 illustrates the compression output when
sentential constraints are added to the model. We
see that politics is forced into the compression due
to the presence of in; furthermore, since bundled
is in the compression, its object with Server is in-
cluded too.
</bodyText>
<subsectionHeader confidence="0.633231">
Compression-related Constraints Finally,
</subsectionHeader>
<bodyText confidence="0.999960363636364">
we impose some hard constraints on the com-
pression output. First, Equation (20) disallows
anything within brackets in the original sentence
from being included in the compression. This
is a somewhat superficial attempt at excluding
parenthetical and potentially unimportant material
from the compression. Second, Equation (21)
forces personal pronouns to be included in the
compression. The constraint is important for
generating coherent document as opposed to
sentence compressions.
</bodyText>
<equation confidence="0.978134">
yi = 0 (20)
bi : wi E brackets
yi = 1 (21)
bi : wi E personal pronouns
</equation>
<bodyText confidence="0.999811714285714">
It is also possible to influence the length of the
compressed sentence. For example, Equation (22)
forces the compression to contain at least b tokens.
Alternatively, we could force the compression to
be exactly b tokens (by substituting &gt; with =
in (22)) or to be less than b tokens (by replacing &gt;
with G).1
</bodyText>
<equation confidence="0.988847">
n
∑ yi &gt; b (22)
i=1
</equation>
<subsectionHeader confidence="0.998953">
3.3 Significance Score
</subsectionHeader>
<bodyText confidence="0.9048775">
While the constraint-based language model pro-
duces more grammatical output than a regular lan-
&apos;Compression rate can be also limited to a range by in-
cluding two inequality constraints.
</bodyText>
<page confidence="0.996945">
148
</page>
<bodyText confidence="0.999946">
guage model, the sentences are typically not great
compressions. The language model has no notion
of which content words to include in the compres-
sion and thus prefers words it has seen before. But
words or constituents will be of different relative
importance in different documents or even sen-
tences.
Inspired by Hori and Furui (2004), we add to
our objective function (see Equation (1)) a signif-
icance score designed to highlight important con-
tent words. Specifically, we modify Hori and Fu-
rui’s significance score to give more weight to con-
tent words that appear in the deepest level of em-
bedding in the syntactic tree. The latter usually
contains the gist of the original sentence:
</bodyText>
<equation confidence="0.972593">
l Fa
I(wi) = N · fi log (23)
Fi
</equation>
<bodyText confidence="0.999955454545455">
The significance score above is computed using a
large corpus where wi is a topic word (i.e., a noun
or verb), fi and Fi are the frequency of wi in the
document and corpus respectively, and Fa is the
sum of all topic words in the corpus. l is the num-
ber of clause constituents above wi, and N is the
deepest level of embedding. The modified objec-
tive function is given below:
A weighting factor could be also added to the ob-
jective function, to counterbalance the importance
of the language model and the significance score.
</bodyText>
<sectionHeader confidence="0.996676" genericHeader="method">
4 Evaluation Set-up
</sectionHeader>
<bodyText confidence="0.999671333333333">
We evaluated the approach presented in the pre-
vious sections against Knight and Marcu’s (2002)
decision-tree model. This model is a good basis for
comparison as it operates on parse trees and there-
fore is aware of syntactic structure (as our models
are) but requires a large parallel corpus for training
whereas our models do not; and it yields compara-
ble performance to the noisy-channel model.2 The
decision-tree model was compared against two
variants of our IP model. Both variants employed
the constraints described in Section 3.2 but dif-
fered in that one variant included the significance
</bodyText>
<tableCaption confidence="0.665033">
2Turner and Charniak (2005) argue that the noisy-channel
model is not an appropriate compression model since it uses
a source model trained on uncompressed sentences and as a
result tends to consider compressed sentences less likely than
uncompressed ones.
</tableCaption>
<bodyText confidence="0.999634185185185">
score in its objective function (see (24)), whereas
the other one did not (see (1)). In both cases the
sequential constraints from Section 3.1 were ap-
plied to ensure that the language model was well-
formed. We give details below on the corpora we
used and explain how the different model parame-
ters were estimated. We also discuss how evalua-
tion was carried out using human judgements.
Corpora We evaluate our systems on two dif-
ferent corpora. The first is the compression corpus
of Knight and Marcu (2002) derived automatically
from document-abstract pairs of the Ziff-Davis
corpus. This corpus has been used in most pre-
vious compression work. We also created a com-
pression corpus from the HUB-4 1996 English
Broadcast News corpus (provided by the LDC).
We asked annotators to produce compressions for
50 broadcast news stories (1,370 sentences).3
The Ziff-Davis corpus is partitioned into train-
ing (1,035 sentences) and test set (32 sentences).
We held out 50 sentences from the training for de-
velopment purposes. We also split the Broadcast
News corpus into a training and test set (1,237/133
sentences). Forty sentences were randomly se-
lected for evaluation purposes, 20 from the test
portion of the Ziff-Davis corpus and 20 from the
Broadcast News corpus test set.
</bodyText>
<subsectionHeader confidence="0.593693">
Parameter Estimation The decision-tree
</subsectionHeader>
<bodyText confidence="0.99988105">
model was trained, using the same feature set
as Knight and Marcu (2002) on the Ziff-Davis
corpus and used to obtain compressions for both
test corpora.4 For our IP models, we used a
language model trained on 25 million tokens from
the North American News corpus using the CMU-
Cambridge Language Modeling Toolkit (Clarkson
and Rosenfeld 1997) with a vocabulary size of
50,000 tokens and Good-Turing discounting.
The significance score used in our second model
was calculated using 25 million tokens from the
Broadcast News Corpus (for the spoken data) and
25 million tokens from the American News Text
Corpus (for the written data). Finally, the model
that includes the significance score was optimised
against a loss function similar to McDonald
(2006) to bring the language model and the score
into harmony. We used Powell’s method (Press
et al. 1992) and 50 sentences (randomly selected
from the training set).
</bodyText>
<footnote confidence="0.853798857142857">
3The corpus is available from http://homepages.inf.
ed.ac.uk/s0460084/data/.
4We found that the decision-tree was unable to produce
meaningful compressions when trained on the Broadcast
News corpus (in most cases it recreated the original sen-
tence). Thus we used the decision model trained on Ziff-
Davis to generate Broadcast News compressions.
</footnote>
<equation confidence="0.8960683">
n n
maxz = ∑ yi ·I(wi)+ ∑ pi · P(wi|start)
i=1 i=1
xijk ·P(wk|wi,wj)
qij ·P(end|wi,wj) (24)
n−2
+ ∑
i=1
n−1
∑
j=i+1
n
∑
k=j+1
n−1
+∑
i=0
n
∑
j=i+1
</equation>
<page confidence="0.996229">
149
</page>
<bodyText confidence="0.991248787234043">
We also set a minimum compression length (us-
ing the constraint in Equation (22)) in both our
models to avoid overly short compressions. The
length was set at 40% of the original sentence
length or five tokens, whichever was larger. Sen-
tences under five tokens were not compressed.
In our modeling framework, we generate and
solve an IP for every sentence we wish to com-
press. We employed lp solve for this purpose, an
efficient Mixed Integer Programming solver.5 Sen-
tences typically take less than a few seconds to
compress on a 2 GHz Pentium IV machine.
Human Evaluation As mentioned earlier, the
output of our models is evaluated on 40 exam-
ples. Although the size of our test set is compa-
rable to previous studies (which are typically as-
sessed on 32 sentences from the Ziff-Davis cor-
pus), the sample is too small to conduct signif-
icance testing. To counteract this, human judge-
ments are often collected on compression out-
put; however the evaluations are limited to small
subject pools (often four judges; Knight and
Marcu 2002; Turner and Charniak 2005; McDon-
ald 2006) which makes difficult to apply inferen-
tial statistics on the data. We overcome this prob-
lem by conducting our evaluation using a larger
sample of subjects.
Specifically, we elicited human judgements
from 56 unpaid volunteers, all self reported na-
tive English speakers. The elicitation study was
conducted over the Internet. Participants were pre-
sented with a set of instructions that explained the
sentence compression task with examples. They
were asked to judge 160 compressions in to-
tal. These included the output of the three au-
tomatic systems on the 40 test sentences paired
with their gold standard compressions. Partici-
pants were asked to read the original sentence and
then reveal its compression by pressing a button.
They were told that all compressions were gen-
erated automatically. A Latin square design en-
sured that subjects did not see two different com-
pressions of the same sentence. The order of the
sentences was randomised. Participants rated each
compression on a five point scale based on the in-
formation retained and its grammaticality. Exam-
ples of our experimental items are given in Table 2.
</bodyText>
<sectionHeader confidence="0.999986" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.9978615">
Our results are summarised in Table 3 which de-
tails the compression rates6 and average human
</bodyText>
<footnote confidence="0.99054275">
5The software is available from http://www.
geocities.com/lpsolve/.
6We follow previous work (see references) in using the
term “compression rate” to refer to the percentage of words
</footnote>
<table confidence="0.992942166666667">
O: Apparently Fergie very much wants to have a ca-
reer in television.
G: Fergie wants a career in television.
D: A career in television.
LM: Fergie wants to have a career.
Sig: Fergie wants to have a career in television.
</table>
<tableCaption confidence="0.9858069375">
O: The SCAMP module, designed and built by
Unisys and based on an Intel process, contains the
entire 48-bit A-series processor.
G: The SCAMP module contains the entire 48-bit A-
series processor.
D: The SCAMP module designed Unisys and based
on an Intel process.
LM: The SCAMP module, contains the 48-bit A-series
processor.
Sig: The SCAMP module, designed and built by
Unisys and based on process, contains the A-
series processor.
Table 2: Compression examples (O: original sen-
tence, G: Gold standard, D: Decision-tree, LM: IP
language model, Sig: IP language model with sig-
nificance score)
</tableCaption>
<table confidence="0.9997894">
Model CompR Rating
Decision-tree 56.1% 2.22*†
LangModel 49.0% 2.23*†
LangModel+Significance 73.6% 2.83*
Gold Standard 62.3% 3.68†
</table>
<tableCaption confidence="0.95311075">
Table 3: Compression results; compression rate
(CompR) and average human judgements (Rat-
ing); *: sig. diff. from gold standard; †: sig. diff.
from LangModel+Significance
</tableCaption>
<bodyText confidence="0.991549791666667">
ratings (Rating) for the three systems and the gold
standard. As can be seen, the IP language model
(LangModel) is most aggressive in terms of com-
pression rate as it reduces the original sentences
on average by half (49%). Recall that we enforce a
minimum compression rate of 40% (see (22)). The
fact that the resulting compressions are longer, in-
dicates that our constraints instill some linguistic
knowledge into the language model, thus enabling
it to prefer longer sentences over extremely short
ones. The decision-tree model compresses slightly
less than our IP language model at 56.1% but still
below the gold standard rate. We see a large com-
pression rate increase from 49% to 73.6% when
we introduce the significance score into the objec-
tive function. This is around 10% higher than the
gold standard compression rate.
We now turn to the results of our elicitation
study. We performed an Analysis of Variance
(ANOVA) to examine the effect of different system
compressions. Statistical tests were carried out on
the mean of the ratings shown in Table 3. We ob-
serve a reliable effect of compression type by sub-
retained in the compression.
</bodyText>
<page confidence="0.993222">
150
</page>
<bodyText confidence="0.99972">
jects (F1(3,165) = 132.74, p &lt; 0.01) and items
(F2(3,117) = 18.94, p &lt; 0.01). Post-hoc Tukey
tests revealed that gold standard compressions are
perceived as significantly better than those gener-
ated by all automatic systems (α &lt; 0.05). There is
no significant difference between the IP language
model and decision-tree systems. However, the IP
model with the significance score delivers a sig-
nificant increase in performance over the language
model and the decision tree (α &lt; 0.05).
These results indicate that reasonable compres-
sions can be obtained with very little supervision.
Our constraint-based language model does not
make use of a parallel corpus, whereas our second
variant uses only 50 parallel sentences for tuning
the weights of the objective function. The models
described in this paper could be easily adapted to
other domains or languages provided that syntac-
tic analysis tools are to some extent available.
</bodyText>
<sectionHeader confidence="0.999634" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.998852416666667">
In this paper we have presented a novel method
for automatic sentence compression. A key aspect
of our approach is the use of integer program-
ming for inferring globally optimal compressions
in the presence of linguistically motivated con-
straints. We have shown that such a formulation
allows for a relatively simple and knowledge-lean
compression model that does not require parallel
corpora or access to large-scale knowledge bases.
Our results demonstrate that the IP model yields
performance comparable to state-of-the-art with-
out any supervision. We also observe significant
performance gains when a small amount of train-
ing data is employed (50 parallel sentences). Be-
yond the systems discussed in this paper, the ap-
proach holds promise for other models using de-
coding algorithms for searching the space of pos-
sible compressions. The search process could be
framed as an integer program in a similar fashion
to our work here.
We obtain our best results using a model whose
objective function includes a significance score.
The significance score relies mainly on syntactic
and lexical information for determining whether
a word is important or not. An appealing future
direction is the incorporation of discourse-based
constraints into our models. The latter would high-
light topical words at the document-level instead
of considering each sentence in isolation. An-
other important issue concerns the portability of
the models presented here to other languages and
domains. We plan to apply our method to lan-
guages with more flexible word order than English
(e.g., German) and more challenging spoken do-
mains (e.g., meeting data) where parsing technol-
ogy may be less reliable.
</bodyText>
<sectionHeader confidence="0.99851" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<reference confidence="0.578936">
Thanks to Jean Carletta, Amit Dubey, Frank Keller, Steve
Renals, and Sebastian Riedel for helpful comments and sug-
gestions. Lapata acknowledges the support of EPSRC (grant
GR/T04540/01).
</reference>
<sectionHeader confidence="0.97802" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999482767857143">
Briscoe, E. J. and J. Carroll. 2002. Robust accurate statisti-
cal annotation of general text. In Proceedings of the 3rd
LREC. Las Palmas, Gran Canaria, pages 1499–1504.
Clarkson, Philip and Ronald Rosenfeld. 1997. Statistical lan-
guage modeling using the CMU–cambridge toolkit. In
Proceedings ofEurospeech. Rhodes, Greece, pages 2707–
2710.
Hori, Chiori and Sadaoki Furui. 2004. Speech summariza-
tion: an approach through word extraction and a method
for evaluation. IEICE Transactions on Information and
Systems E87-D(1):15–25.
Jing, Hongyan. 2000. Sentence reduction for automatic text
summarization. In Proceedings of the 6th ANLP. Seattle,
WA, pages 310–315.
Knight, Kevin and Daniel Marcu. 2002. Summarization be-
yond sentence extraction: a probabilistic approach to sen-
tence compression. Artificial Intelligence 139(1):91–107.
Marciniak, Tomasz and Michael Strube. 2005. Beyond the
pipeline: Discrete optimization in NLP. In Proceedings of
the 9th CoNLL. Ann Arbor, MI, pages 136–143.
McDonald, Ryan. 2006. Discriminative sentence compres-
sion with soft syntactic constraints. In Proceedings of the
11th EACL. Trento, Italy, pages 297–304.
Nguyen, Minh Le, Akira Shimazu, Susumu Horiguchi,
Tu Bao Ho, and Masaru Fukushi. 2004. Probabilistic sen-
tence reduction using support vector machines. In Pro-
ceedings of the 20th COLING. Geneva, Switzerland, pages
743–749.
Olivers, S. H. and W. B. Dolan. 1999. Less is more; eliminat-
ing index terms from subordinate clauses. In Proceedings
of the 37th ACL. College Park, MD, pages 349–356.
Press, William H., Saul A. Teukolsky, William T. Vetterling,
and Brian P. Flannery. 1992. Numerical Recipes in C: The
Art of Scientific Computing. Cambridge University Press.
Punyakanok, Vasin, Dan Roth, Wen-tau Yih, and Dav Zimak.
2004. Semantic role labeling via integer linear program-
ming inference. In Proceedings of the 20th COLING.
Geneva, Switzerland, pages 1346–1352.
Riezler, Stefan, Tracy H. King, Richard Crouch, and Annie
Zaenen. 2003. Statistical sentence condensation using
ambiguity packing and stochastic disambiguation meth-
ods for lexical-functional grammar. In Proceedings of
the HLT/NAACL. Edmonton, Canada, pages 118–125.
Roth, Dan and Wen-tau Yih. 2004. A linear programming
formulation for global inference in natural language tasks.
In Proceedings of the 8th CoNLL. Boston, MA, pages 1–8.
Turner, Jenine and Eugene Charniak. 2005. Supervised and
unsupervised learning for sentence compression. In Pro-
ceedings of the 43rd ACL. Ann Arbor, MI, pages 290–297.
Vandeghinste, Vincent and Yi Pan. 2004. Sentence compres-
sion for automated subtitling: A hybrid approach. In Pro-
ceedings of the ACL Workshop on Text Summarization.
Barcelona, Spain, pages 89–95.
Winston, Wayne L. and Munirpallam Venkataramanan.
2003. Introduction to Mathematical Programming.
Brooks/Cole.
</reference>
<page confidence="0.998341">
151
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.906780">
<title confidence="0.998472">Constraint-based Sentence Compression An Integer Programming Approach</title>
<author confidence="0.998418">Clarke Lapata</author>
<affiliation confidence="0.999958">School of Informatics, University of Edinburgh</affiliation>
<address confidence="0.964715">2 Bucclecuch Place, Edinburgh EH8 9LW, UK</address>
<abstract confidence="0.9965054375">The ability to compress sentences while preserving their grammaticality and most of their meaning has recently received much attention. Our work views sentence compression as an optimisation problem. We develop an integer programming formulation and infer globally optimal compressions in the face of linguistically motivated constraints. We show that such a formulation allows for relatively simple and knowledge-lean compression models that do not require parallel corpora or largescale resources. The proposed approach yields results comparable and in some cases superior to state-of-the-art.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Thanks to Jean Carletta</author>
<author>Amit Dubey</author>
<author>Frank Keller</author>
</authors>
<title>Steve Renals, and Sebastian Riedel for helpful comments and suggestions. Lapata acknowledges the support of EPSRC</title>
<marker>Carletta, Dubey, Keller, </marker>
<rawString>Thanks to Jean Carletta, Amit Dubey, Frank Keller, Steve Renals, and Sebastian Riedel for helpful comments and suggestions. Lapata acknowledges the support of EPSRC (grant GR/T04540/01).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E J Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Robust accurate statistical annotation of general text.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd LREC. Las Palmas, Gran Canaria,</booktitle>
<pages>1499--1504</pages>
<contexts>
<context position="17099" citStr="Briscoe and Carroll 2002" startWordPosition="2779" endWordPosition="2782">nts that extend the basic language model presented in Equations (1)–(7). Our aim is to bring some syntactic knowledge into the compression model and to preserve the meaning of the original sentence as much as possible. Our constraints are linguistically and semantically motivated in a similar fashion to the grammar checking component of Jing (2000). Importantly, we do not require any additional knowledge sources (such as a lexicon) beyond the parse and grammatical relations of the original sentence. This is provided in our experiments by the Robust Accurate Statistical Parsing (RASP) toolkit (Briscoe and Carroll 2002). However, there is nothing inherent in our formulation that restricts us to RASP; any other parser with similar output could serve our purposes. Modifier Constraints Modifier constraints ensure that relationships between head words and their modifiers remain grammatical in the compression: yi −yj ≥ 0 (8) ∀i, j : wj ∈ wi’s ncmods yi −yj ≥ 0 (9) ∀i, j : wj ∈ wi’s detmods xijk · P(wk|wi,wj) qij ·P(end|wi,wj) (1) n ∑ k=j+1 n−2 + ∑ i=1 n−1 ∑ j=i+1 n−1 + ∑ i=0 n ∑ j=i+1 k−1 ∑ j=1 yk − pk − k−2 ∑ i=0 yj − j−1 n xijk − j−1 ∑ ∑ ∑ i=0 k=j+1 i=0 yi − n−1 ∑ j=i+1 n ∑ j=i+1 n ∑ k=j+1 xijk − i−1 qij −∑ h=0</context>
</contexts>
<marker>Briscoe, Carroll, 2002</marker>
<rawString>Briscoe, E. J. and J. Carroll. 2002. Robust accurate statistical annotation of general text. In Proceedings of the 3rd LREC. Las Palmas, Gran Canaria, pages 1499–1504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Clarkson</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>Statistical language modeling using the CMU–cambridge toolkit.</title>
<date>1997</date>
<booktitle>In Proceedings ofEurospeech. Rhodes, Greece,</booktitle>
<pages>2707--2710</pages>
<contexts>
<context position="25498" citStr="Clarkson and Rosenfeld 1997" startWordPosition="4237" endWordPosition="4240">o split the Broadcast News corpus into a training and test set (1,237/133 sentences). Forty sentences were randomly selected for evaluation purposes, 20 from the test portion of the Ziff-Davis corpus and 20 from the Broadcast News corpus test set. Parameter Estimation The decision-tree model was trained, using the same feature set as Knight and Marcu (2002) on the Ziff-Davis corpus and used to obtain compressions for both test corpora.4 For our IP models, we used a language model trained on 25 million tokens from the North American News corpus using the CMUCambridge Language Modeling Toolkit (Clarkson and Rosenfeld 1997) with a vocabulary size of 50,000 tokens and Good-Turing discounting. The significance score used in our second model was calculated using 25 million tokens from the Broadcast News Corpus (for the spoken data) and 25 million tokens from the American News Text Corpus (for the written data). Finally, the model that includes the significance score was optimised against a loss function similar to McDonald (2006) to bring the language model and the score into harmony. We used Powell’s method (Press et al. 1992) and 50 sentences (randomly selected from the training set). 3The corpus is available fro</context>
</contexts>
<marker>Clarkson, Rosenfeld, 1997</marker>
<rawString>Clarkson, Philip and Ronald Rosenfeld. 1997. Statistical language modeling using the CMU–cambridge toolkit. In Proceedings ofEurospeech. Rhodes, Greece, pages 2707– 2710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chiori Hori</author>
<author>Sadaoki Furui</author>
</authors>
<title>Speech summarization: an approach through word extraction and a method for evaluation.</title>
<date>2004</date>
<booktitle>IEICE Transactions on Information and Systems E87-D(1):15–25.</booktitle>
<contexts>
<context position="2504" citStr="Hori and Furui 2004" startWordPosition="362" endWordPosition="366">en a long sentence l, the aim is to find the corresponding short sentences which maximises the conditional probability P(s1l). In a discriminative setting (Knight and Marcu 2002; Riezler et al. 2003; McDonald 2006), sentences are represented by a rich feature space (typically induced from parse trees) and the goal is to learn rewrite rules indicating which words should be deleted in a given context. Both modelling paradigms assume access to a training corpus consisting of original sentences and their compressions. Unsupervised approaches to the compression problem are few and far between (see Hori and Furui 2004 and Turner and Charniak 2005 for exceptions). This is surprising considering that parallel corpora of original-compressed sentences are not naturally available in the way multilingual corpora are. The scarcity of such data is demonstrated by the fact that most work to date has focused on a single parallel corpus, namely the Ziff-Davis corpus (Knight and Marcu 2002). And some effort into developing appropriate training data would be necessary when porting existing algorithms to new languages or domains. In this paper we present an unsupervised model of sentence compression that does not rely o</context>
<context position="7949" citStr="Hori and Furui (2004)" startWordPosition="1226" endWordPosition="1229">of automatically generated learning cases from a parallel corpus. Each learning case has a target action associated with it and is decomposed into a set of indicative features. The decision-tree learns which action to perform given this set of features. The final model is applied in a deterministic fashion in which the features for the current state are extracted and the decision-tree is queried. This is repeated until the input list is empty and the final compression is recovered by traversing the leaves of resulting tree on the stack. While most compression models operate over constituents, Hori and Furui (2004) propose a model which generates compressions through word deletion. The model does not utilise parallel data or syntactic information in any form. Given a prespecified compression rate, it searches for the compression with the highest score according to a function measuring the importance of each word and the linguistic likelihood of the resulting compressions (language model probability). The score is maximised through a dynamic programming algorithm. Although sentence compression has not been explicitly formulated as an optimisation problem, previous approaches have treated it in these term</context>
<context position="9191" citStr="Hori and Furui 2004" startWordPosition="1418" endWordPosition="1421"> in the noisy-channel model searches for the best compression given the source and channel models. However, the compression found is usually sub-optimal as heuristics are used to reduce the search space or is only locally optimal due to the search method employed. The decoding process used in Turner and Charniak’s (2005) model first searches for the best combination of rules to apply. As they traverse their list of compression rules they remove sentences outside the 100 best compressions (according to their channel model). This list is eventually truncated to 25 compressions. In other models (Hori and Furui 2004; McDonald 2006) the compression score is maximised 145 using dynamic programming. The latter guarantees we will find the global optimum provided the principle of optimality holds. This principle states that given the current state, the optimal decision for each of the remaining stages does not depend on previously reached stages or previously made decisions (Winston and Venkataramanan 2003). However, we know this to be false in the case of sentence compression. For example, if we have included modifiers to the left of a head noun in the compression then it makes sense that we must include the</context>
<context position="22069" citStr="Hori and Furui (2004)" startWordPosition="3661" endWordPosition="3664"> or to be less than b tokens (by replacing &gt; with G).1 n ∑ yi &gt; b (22) i=1 3.3 Significance Score While the constraint-based language model produces more grammatical output than a regular lan&apos;Compression rate can be also limited to a range by including two inequality constraints. 148 guage model, the sentences are typically not great compressions. The language model has no notion of which content words to include in the compression and thus prefers words it has seen before. But words or constituents will be of different relative importance in different documents or even sentences. Inspired by Hori and Furui (2004), we add to our objective function (see Equation (1)) a significance score designed to highlight important content words. Specifically, we modify Hori and Furui’s significance score to give more weight to content words that appear in the deepest level of embedding in the syntactic tree. The latter usually contains the gist of the original sentence: l Fa I(wi) = N · fi log (23) Fi The significance score above is computed using a large corpus where wi is a topic word (i.e., a noun or verb), fi and Fi are the frequency of wi in the document and corpus respectively, and Fa is the sum of all topic </context>
</contexts>
<marker>Hori, Furui, 2004</marker>
<rawString>Hori, Chiori and Sadaoki Furui. 2004. Speech summarization: an approach through word extraction and a method for evaluation. IEICE Transactions on Information and Systems E87-D(1):15–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
</authors>
<title>Sentence reduction for automatic text summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th ANLP.</booktitle>
<pages>310--315</pages>
<location>Seattle, WA,</location>
<contexts>
<context position="1066" citStr="Jing 2000" startWordPosition="144" endWordPosition="145"> programming formulation and infer globally optimal compressions in the face of linguistically motivated constraints. We show that such a formulation allows for relatively simple and knowledge-lean compression models that do not require parallel corpora or largescale resources. The proposed approach yields results comparable and in some cases superior to state-of-the-art. 1 Introduction A mechanism for automatically compressing sentences while preserving their grammaticality and most important information would greatly benefit a wide range of applications. Examples include text summarisation (Jing 2000), subtitle generation from spoken transcripts (Vandeghinste and Pan 2004) and information retrieval (Olivers and Dolan 1999). Sentence compression is a complex paraphrasing task with information loss involving substitution, deletion, insertion, and reordering operations. Recent years have witnessed increased interest on a simpler instantiation of the compression problem, namely word deletion (Knight and Marcu 2002; Riezler et al. 2003; Turner and Charniak 2005). More formally, given an input sentence of words W = w1,w2,...,wn, a compression is formed by removing any subset of these words. Sent</context>
<context position="4795" citStr="Jing (2000)" startWordPosition="725" endWordPosition="726">�2006 Association for Computational Linguistics could prevent overly long or overly short compressions or generally avoid compressions that lack a main verb or consist of repetitions of the same word. In the following section we provide an overview of previous approaches to sentence compression. In Section 3 we motivate the treatment of sentence compression as an optimisation problem and formulate our language model and constraints in the IP framework. Section 4 discusses our experimental set-up and Section 5 presents our results. Discussion of future work concludes the paper. 2 Previous Work Jing (2000) was perhaps the first to tackle the sentence compression problem. Her approach uses multiple knowledge sources to determine which phrases in a sentence to remove. Central to her system is a grammar checking module that specifies which sentential constituents are grammatically obligatory and should therefore be present in the compression. This is achieved using simple rules and a large-scale lexicon. Other knowledge sources include WordNet and corpus evidence gathered from a parallel corpus of originalcompressed sentence pairs. A phrase is removed only if it is not grammatically obligatory, no</context>
<context position="16824" citStr="Jing (2000)" startWordPosition="2739" endWordPosition="2740">dencies that could potentially allow more meaningful compressions. For example, it does not know that Pasok Party is the object of founded or that Appleshare modifies Printer Server. 3.2 Linguistic Constraints In this section we propose a set of global constraints that extend the basic language model presented in Equations (1)–(7). Our aim is to bring some syntactic knowledge into the compression model and to preserve the meaning of the original sentence as much as possible. Our constraints are linguistically and semantically motivated in a similar fashion to the grammar checking component of Jing (2000). Importantly, we do not require any additional knowledge sources (such as a lexicon) beyond the parse and grammatical relations of the original sentence. This is provided in our experiments by the Robust Accurate Statistical Parsing (RASP) toolkit (Briscoe and Carroll 2002). However, there is nothing inherent in our formulation that restricts us to RASP; any other parser with similar output could serve our purposes. Modifier Constraints Modifier constraints ensure that relationships between head words and their modifiers remain grammatical in the compression: yi −yj ≥ 0 (8) ∀i, j : wj ∈ wi’s </context>
</contexts>
<marker>Jing, 2000</marker>
<rawString>Jing, Hongyan. 2000. Sentence reduction for automatic text summarization. In Proceedings of the 6th ANLP. Seattle, WA, pages 310–315.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Summarization beyond sentence extraction: a probabilistic approach to sentence compression.</title>
<date>2002</date>
<journal>Artificial Intelligence</journal>
<volume>139</volume>
<issue>1</issue>
<contexts>
<context position="1483" citStr="Knight and Marcu 2002" startWordPosition="199" endWordPosition="202">automatically compressing sentences while preserving their grammaticality and most important information would greatly benefit a wide range of applications. Examples include text summarisation (Jing 2000), subtitle generation from spoken transcripts (Vandeghinste and Pan 2004) and information retrieval (Olivers and Dolan 1999). Sentence compression is a complex paraphrasing task with information loss involving substitution, deletion, insertion, and reordering operations. Recent years have witnessed increased interest on a simpler instantiation of the compression problem, namely word deletion (Knight and Marcu 2002; Riezler et al. 2003; Turner and Charniak 2005). More formally, given an input sentence of words W = w1,w2,...,wn, a compression is formed by removing any subset of these words. Sentence compression has received both generative and discriminative formulations in the literature. Generative approaches (Knight and Marcu 2002; Turner and Charniak 2005) are instantiations of the noisy-channel model: given a long sentence l, the aim is to find the corresponding short sentences which maximises the conditional probability P(s1l). In a discriminative setting (Knight and Marcu 2002; Riezler et al. 2003</context>
<context position="2872" citStr="Knight and Marcu 2002" startWordPosition="422" endWordPosition="425">should be deleted in a given context. Both modelling paradigms assume access to a training corpus consisting of original sentences and their compressions. Unsupervised approaches to the compression problem are few and far between (see Hori and Furui 2004 and Turner and Charniak 2005 for exceptions). This is surprising considering that parallel corpora of original-compressed sentences are not naturally available in the way multilingual corpora are. The scarcity of such data is demonstrated by the fact that most work to date has focused on a single parallel corpus, namely the Ziff-Davis corpus (Knight and Marcu 2002). And some effort into developing appropriate training data would be necessary when porting existing algorithms to new languages or domains. In this paper we present an unsupervised model of sentence compression that does not rely on a parallel corpus – all that is required is a corpus of uncompressed sentences and a parser. Given a long sentence, our task is to form a compression by preserving the words that maximise a scoring function. In our case, the scoring function is an n-gram language model, “with a few strings attached”. While straightforward to estimate, a language model is a fairly </context>
<context position="5788" citStr="Knight and Marcu 2002" startWordPosition="878" endWordPosition="881">ules and a large-scale lexicon. Other knowledge sources include WordNet and corpus evidence gathered from a parallel corpus of originalcompressed sentence pairs. A phrase is removed only if it is not grammatically obligatory, not the focus of the local context and has a reasonable deletion probability (estimated from the parallel corpus). In contrast to Jing (2000), the bulk of the research on sentence compression relies exclusively on corpus data for modelling the compression process without recourse to extensive knowledge sources (e.g., WordNet). Approaches based on the noisy-channel model (Knight and Marcu 2002; Turner and Charniak 2005) consist of a source model P(s) (whose role is to guarantee that the generated compression is grammatical), a channel model P(l|s) (capturing the probability that the long sentence l is an expansion of the compressed sentences), and a decoder (which searches for the compressions that maximises P(s)P(l|s)). The channel model is typically estimated using a parallel corpus, although Turner and Charniak (2005) also present semi-supervised and unsupervised variants of the channel model that estimate P(l|s) without parallel data. Discriminative formulations of the compress</context>
<context position="10108" citStr="Knight and Marcu 2002" startWordPosition="1565" endWordPosition="1568"> depend on previously reached stages or previously made decisions (Winston and Venkataramanan 2003). However, we know this to be false in the case of sentence compression. For example, if we have included modifiers to the left of a head noun in the compression then it makes sense that we must include the head also. With a dynamic programming approach we cannot easily guarantee such constraints hold. 3 Problem Formulation Our work models sentence compression explicitly as an optimisation problem. There are 2n possible compressions for each sentence and while many of these will be unreasonable (Knight and Marcu 2002), it is unlikely that only one compression will be satisfactory. Ideally, we require a function that captures the operations (or rules) that can be performed on a sentence to create a compression while at the same time factoring how desirable each operation makes the resulting compression. We can then perform a search over all possible compressions and select the best one, as determined by how desirable it is. Our formulation consists of two basic components: a language model (scoring function) and a small number of constraints ensuring that the resulting compressions are structurally and sema</context>
<context position="24354" citStr="Knight and Marcu (2002)" startWordPosition="4055" endWordPosition="4058"> on uncompressed sentences and as a result tends to consider compressed sentences less likely than uncompressed ones. score in its objective function (see (24)), whereas the other one did not (see (1)). In both cases the sequential constraints from Section 3.1 were applied to ensure that the language model was wellformed. We give details below on the corpora we used and explain how the different model parameters were estimated. We also discuss how evaluation was carried out using human judgements. Corpora We evaluate our systems on two different corpora. The first is the compression corpus of Knight and Marcu (2002) derived automatically from document-abstract pairs of the Ziff-Davis corpus. This corpus has been used in most previous compression work. We also created a compression corpus from the HUB-4 1996 English Broadcast News corpus (provided by the LDC). We asked annotators to produce compressions for 50 broadcast news stories (1,370 sentences).3 The Ziff-Davis corpus is partitioned into training (1,035 sentences) and test set (32 sentences). We held out 50 sentences from the training for development purposes. We also split the Broadcast News corpus into a training and test set (1,237/133 sentences)</context>
<context position="27593" citStr="Knight and Marcu 2002" startWordPosition="4591" endWordPosition="4594">urpose, an efficient Mixed Integer Programming solver.5 Sentences typically take less than a few seconds to compress on a 2 GHz Pentium IV machine. Human Evaluation As mentioned earlier, the output of our models is evaluated on 40 examples. Although the size of our test set is comparable to previous studies (which are typically assessed on 32 sentences from the Ziff-Davis corpus), the sample is too small to conduct significance testing. To counteract this, human judgements are often collected on compression output; however the evaluations are limited to small subject pools (often four judges; Knight and Marcu 2002; Turner and Charniak 2005; McDonald 2006) which makes difficult to apply inferential statistics on the data. We overcome this problem by conducting our evaluation using a larger sample of subjects. Specifically, we elicited human judgements from 56 unpaid volunteers, all self reported native English speakers. The elicitation study was conducted over the Internet. Participants were presented with a set of instructions that explained the sentence compression task with examples. They were asked to judge 160 compressions in total. These included the output of the three automatic systems on the 40</context>
</contexts>
<marker>Knight, Marcu, 2002</marker>
<rawString>Knight, Kevin and Daniel Marcu. 2002. Summarization beyond sentence extraction: a probabilistic approach to sentence compression. Artificial Intelligence 139(1):91–107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomasz Marciniak</author>
<author>Michael Strube</author>
</authors>
<title>Beyond the pipeline: Discrete optimization in NLP.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th CoNLL.</booktitle>
<pages>136--143</pages>
<location>Ann Arbor, MI,</location>
<contexts>
<context position="11196" citStr="Marciniak and Strube 2005" startWordPosition="1740" endWordPosition="1743">: a language model (scoring function) and a small number of constraints ensuring that the resulting compressions are structurally and semantically valid. Our task is to find a globally optimal compression in the presence of these constraints. We solve this inference problem using Integer Programming without resorting to heuristics or approximations during the decoding process. Integer programming has been recently applied to several classification tasks, including relation extraction (Roth and Yih 2004), semantic role labelling (Punyakanok et al. 2004), and the generation of route directions (Marciniak and Strube 2005). Before describing our model in detail, we introduce some of the concepts and terms used in Linear Programming and Integer Programming (see Winston and Venkataramanan 2003 for an introduction). Linear Programming (LP) is a tool for solving optimisation problems in which the aim is to maximise (or minimise) a given function with respect to a set of constraints. The function to be maximised (or minimised) is referred to as the objective function. Both the objective function and constraints must be linear. A number of decision variables are under our control which exert influence on the objectiv</context>
</contexts>
<marker>Marciniak, Strube, 2005</marker>
<rawString>Marciniak, Tomasz and Michael Strube. 2005. Beyond the pipeline: Discrete optimization in NLP. In Proceedings of the 9th CoNLL. Ann Arbor, MI, pages 136–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>Discriminative sentence compression with soft syntactic constraints.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th EACL.</booktitle>
<pages>297--304</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="2099" citStr="McDonald 2006" startWordPosition="300" endWordPosition="301">Riezler et al. 2003; Turner and Charniak 2005). More formally, given an input sentence of words W = w1,w2,...,wn, a compression is formed by removing any subset of these words. Sentence compression has received both generative and discriminative formulations in the literature. Generative approaches (Knight and Marcu 2002; Turner and Charniak 2005) are instantiations of the noisy-channel model: given a long sentence l, the aim is to find the corresponding short sentences which maximises the conditional probability P(s1l). In a discriminative setting (Knight and Marcu 2002; Riezler et al. 2003; McDonald 2006), sentences are represented by a rich feature space (typically induced from parse trees) and the goal is to learn rewrite rules indicating which words should be deleted in a given context. Both modelling paradigms assume access to a training corpus consisting of original sentences and their compressions. Unsupervised approaches to the compression problem are few and far between (see Hori and Furui 2004 and Turner and Charniak 2005 for exceptions). This is surprising considering that parallel corpora of original-compressed sentences are not naturally available in the way multilingual corpora ar</context>
<context position="6579" citStr="McDonald 2006" startWordPosition="998" endWordPosition="999">ability that the long sentence l is an expansion of the compressed sentences), and a decoder (which searches for the compressions that maximises P(s)P(l|s)). The channel model is typically estimated using a parallel corpus, although Turner and Charniak (2005) also present semi-supervised and unsupervised variants of the channel model that estimate P(l|s) without parallel data. Discriminative formulations of the compression task include decision-tree learning (Knight and Marcu 2002), maximum entropy (Riezler et al. 2003), support vector machines (Nguyen et al. 2004), and large-margin learning (McDonald 2006). We describe here the decision-tree model in more detail since we will use it as a basis for comparison when evaluating our own models (see Section 4). According to this model, compression is performed through a tree rewriting process inspired by the shift-reduce parsing paradigm. A sequence of shift-reduce-drop actions are performed on a long parse tree, l, to create a smaller tree, s. The compression process begins with an input list generated from the leaves of the original sentence’s parse tree and an empty stack. ‘Shift’ operations move leaves from the input list to the stack while ‘drop</context>
<context position="9207" citStr="McDonald 2006" startWordPosition="1422" endWordPosition="1424"> model searches for the best compression given the source and channel models. However, the compression found is usually sub-optimal as heuristics are used to reduce the search space or is only locally optimal due to the search method employed. The decoding process used in Turner and Charniak’s (2005) model first searches for the best combination of rules to apply. As they traverse their list of compression rules they remove sentences outside the 100 best compressions (according to their channel model). This list is eventually truncated to 25 compressions. In other models (Hori and Furui 2004; McDonald 2006) the compression score is maximised 145 using dynamic programming. The latter guarantees we will find the global optimum provided the principle of optimality holds. This principle states that given the current state, the optimal decision for each of the remaining stages does not depend on previously reached stages or previously made decisions (Winston and Venkataramanan 2003). However, we know this to be false in the case of sentence compression. For example, if we have included modifiers to the left of a head noun in the compression then it makes sense that we must include the head also. With</context>
<context position="25909" citStr="McDonald (2006)" startWordPosition="4304" endWordPosition="4305">h test corpora.4 For our IP models, we used a language model trained on 25 million tokens from the North American News corpus using the CMUCambridge Language Modeling Toolkit (Clarkson and Rosenfeld 1997) with a vocabulary size of 50,000 tokens and Good-Turing discounting. The significance score used in our second model was calculated using 25 million tokens from the Broadcast News Corpus (for the spoken data) and 25 million tokens from the American News Text Corpus (for the written data). Finally, the model that includes the significance score was optimised against a loss function similar to McDonald (2006) to bring the language model and the score into harmony. We used Powell’s method (Press et al. 1992) and 50 sentences (randomly selected from the training set). 3The corpus is available from http://homepages.inf. ed.ac.uk/s0460084/data/. 4We found that the decision-tree was unable to produce meaningful compressions when trained on the Broadcast News corpus (in most cases it recreated the original sentence). Thus we used the decision model trained on ZiffDavis to generate Broadcast News compressions. n n maxz = ∑ yi ·I(wi)+ ∑ pi · P(wi|start) i=1 i=1 xijk ·P(wk|wi,wj) qij ·P(end|wi,wj) (24) n−2</context>
<context position="27635" citStr="McDonald 2006" startWordPosition="4599" endWordPosition="4601">lver.5 Sentences typically take less than a few seconds to compress on a 2 GHz Pentium IV machine. Human Evaluation As mentioned earlier, the output of our models is evaluated on 40 examples. Although the size of our test set is comparable to previous studies (which are typically assessed on 32 sentences from the Ziff-Davis corpus), the sample is too small to conduct significance testing. To counteract this, human judgements are often collected on compression output; however the evaluations are limited to small subject pools (often four judges; Knight and Marcu 2002; Turner and Charniak 2005; McDonald 2006) which makes difficult to apply inferential statistics on the data. We overcome this problem by conducting our evaluation using a larger sample of subjects. Specifically, we elicited human judgements from 56 unpaid volunteers, all self reported native English speakers. The elicitation study was conducted over the Internet. Participants were presented with a set of instructions that explained the sentence compression task with examples. They were asked to judge 160 compressions in total. These included the output of the three automatic systems on the 40 test sentences paired with their gold sta</context>
</contexts>
<marker>McDonald, 2006</marker>
<rawString>McDonald, Ryan. 2006. Discriminative sentence compression with soft syntactic constraints. In Proceedings of the 11th EACL. Trento, Italy, pages 297–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minh Le Nguyen</author>
</authors>
<title>Akira Shimazu, Susumu Horiguchi, Tu Bao Ho, and Masaru Fukushi.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th COLING.</booktitle>
<pages>743--749</pages>
<location>Geneva, Switzerland,</location>
<marker>Nguyen, 2004</marker>
<rawString>Nguyen, Minh Le, Akira Shimazu, Susumu Horiguchi, Tu Bao Ho, and Masaru Fukushi. 2004. Probabilistic sentence reduction using support vector machines. In Proceedings of the 20th COLING. Geneva, Switzerland, pages 743–749.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S H Olivers</author>
<author>W B Dolan</author>
</authors>
<title>Less is more; eliminating index terms from subordinate clauses.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th ACL. College Park, MD,</booktitle>
<pages>349--356</pages>
<contexts>
<context position="1190" citStr="Olivers and Dolan 1999" startWordPosition="159" endWordPosition="162">aints. We show that such a formulation allows for relatively simple and knowledge-lean compression models that do not require parallel corpora or largescale resources. The proposed approach yields results comparable and in some cases superior to state-of-the-art. 1 Introduction A mechanism for automatically compressing sentences while preserving their grammaticality and most important information would greatly benefit a wide range of applications. Examples include text summarisation (Jing 2000), subtitle generation from spoken transcripts (Vandeghinste and Pan 2004) and information retrieval (Olivers and Dolan 1999). Sentence compression is a complex paraphrasing task with information loss involving substitution, deletion, insertion, and reordering operations. Recent years have witnessed increased interest on a simpler instantiation of the compression problem, namely word deletion (Knight and Marcu 2002; Riezler et al. 2003; Turner and Charniak 2005). More formally, given an input sentence of words W = w1,w2,...,wn, a compression is formed by removing any subset of these words. Sentence compression has received both generative and discriminative formulations in the literature. Generative approaches (Knig</context>
</contexts>
<marker>Olivers, Dolan, 1999</marker>
<rawString>Olivers, S. H. and W. B. Dolan. 1999. Less is more; eliminating index terms from subordinate clauses. In Proceedings of the 37th ACL. College Park, MD, pages 349–356.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William H Press</author>
<author>Saul A Teukolsky</author>
<author>William T Vetterling</author>
<author>Brian P Flannery</author>
</authors>
<title>Numerical Recipes in C: The Art of Scientific Computing.</title>
<date>1992</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="26009" citStr="Press et al. 1992" startWordPosition="4320" endWordPosition="4323">e North American News corpus using the CMUCambridge Language Modeling Toolkit (Clarkson and Rosenfeld 1997) with a vocabulary size of 50,000 tokens and Good-Turing discounting. The significance score used in our second model was calculated using 25 million tokens from the Broadcast News Corpus (for the spoken data) and 25 million tokens from the American News Text Corpus (for the written data). Finally, the model that includes the significance score was optimised against a loss function similar to McDonald (2006) to bring the language model and the score into harmony. We used Powell’s method (Press et al. 1992) and 50 sentences (randomly selected from the training set). 3The corpus is available from http://homepages.inf. ed.ac.uk/s0460084/data/. 4We found that the decision-tree was unable to produce meaningful compressions when trained on the Broadcast News corpus (in most cases it recreated the original sentence). Thus we used the decision model trained on ZiffDavis to generate Broadcast News compressions. n n maxz = ∑ yi ·I(wi)+ ∑ pi · P(wi|start) i=1 i=1 xijk ·P(wk|wi,wj) qij ·P(end|wi,wj) (24) n−2 + ∑ i=1 n−1 ∑ j=i+1 n ∑ k=j+1 n−1 +∑ i=0 n ∑ j=i+1 149 We also set a minimum compression length (us</context>
</contexts>
<marker>Press, Teukolsky, Vetterling, Flannery, 1992</marker>
<rawString>Press, William H., Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. 1992. Numerical Recipes in C: The Art of Scientific Computing. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
<author>Dav Zimak</author>
</authors>
<title>Semantic role labeling via integer linear programming inference.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th COLING.</booktitle>
<pages>1346--1352</pages>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="11128" citStr="Punyakanok et al. 2004" startWordPosition="1729" endWordPosition="1733">desirable it is. Our formulation consists of two basic components: a language model (scoring function) and a small number of constraints ensuring that the resulting compressions are structurally and semantically valid. Our task is to find a globally optimal compression in the presence of these constraints. We solve this inference problem using Integer Programming without resorting to heuristics or approximations during the decoding process. Integer programming has been recently applied to several classification tasks, including relation extraction (Roth and Yih 2004), semantic role labelling (Punyakanok et al. 2004), and the generation of route directions (Marciniak and Strube 2005). Before describing our model in detail, we introduce some of the concepts and terms used in Linear Programming and Integer Programming (see Winston and Venkataramanan 2003 for an introduction). Linear Programming (LP) is a tool for solving optimisation problems in which the aim is to maximise (or minimise) a given function with respect to a set of constraints. The function to be maximised (or minimised) is referred to as the objective function. Both the objective function and constraints must be linear. A number of decision v</context>
</contexts>
<marker>Punyakanok, Roth, Yih, Zimak, 2004</marker>
<rawString>Punyakanok, Vasin, Dan Roth, Wen-tau Yih, and Dav Zimak. 2004. Semantic role labeling via integer linear programming inference. In Proceedings of the 20th COLING. Geneva, Switzerland, pages 1346–1352.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Tracy H King</author>
<author>Richard Crouch</author>
<author>Annie Zaenen</author>
</authors>
<title>Statistical sentence condensation using ambiguity packing and stochastic disambiguation methods for lexical-functional grammar.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT/NAACL.</booktitle>
<pages>118--125</pages>
<location>Edmonton, Canada,</location>
<contexts>
<context position="1504" citStr="Riezler et al. 2003" startWordPosition="203" endWordPosition="206">ng sentences while preserving their grammaticality and most important information would greatly benefit a wide range of applications. Examples include text summarisation (Jing 2000), subtitle generation from spoken transcripts (Vandeghinste and Pan 2004) and information retrieval (Olivers and Dolan 1999). Sentence compression is a complex paraphrasing task with information loss involving substitution, deletion, insertion, and reordering operations. Recent years have witnessed increased interest on a simpler instantiation of the compression problem, namely word deletion (Knight and Marcu 2002; Riezler et al. 2003; Turner and Charniak 2005). More formally, given an input sentence of words W = w1,w2,...,wn, a compression is formed by removing any subset of these words. Sentence compression has received both generative and discriminative formulations in the literature. Generative approaches (Knight and Marcu 2002; Turner and Charniak 2005) are instantiations of the noisy-channel model: given a long sentence l, the aim is to find the corresponding short sentences which maximises the conditional probability P(s1l). In a discriminative setting (Knight and Marcu 2002; Riezler et al. 2003; McDonald 2006), sen</context>
<context position="6490" citStr="Riezler et al. 2003" startWordPosition="984" endWordPosition="987">ntee that the generated compression is grammatical), a channel model P(l|s) (capturing the probability that the long sentence l is an expansion of the compressed sentences), and a decoder (which searches for the compressions that maximises P(s)P(l|s)). The channel model is typically estimated using a parallel corpus, although Turner and Charniak (2005) also present semi-supervised and unsupervised variants of the channel model that estimate P(l|s) without parallel data. Discriminative formulations of the compression task include decision-tree learning (Knight and Marcu 2002), maximum entropy (Riezler et al. 2003), support vector machines (Nguyen et al. 2004), and large-margin learning (McDonald 2006). We describe here the decision-tree model in more detail since we will use it as a basis for comparison when evaluating our own models (see Section 4). According to this model, compression is performed through a tree rewriting process inspired by the shift-reduce parsing paradigm. A sequence of shift-reduce-drop actions are performed on a long parse tree, l, to create a smaller tree, s. The compression process begins with an input list generated from the leaves of the original sentence’s parse tree and an</context>
</contexts>
<marker>Riezler, King, Crouch, Zaenen, 2003</marker>
<rawString>Riezler, Stefan, Tracy H. King, Richard Crouch, and Annie Zaenen. 2003. Statistical sentence condensation using ambiguity packing and stochastic disambiguation methods for lexical-functional grammar. In Proceedings of the HLT/NAACL. Edmonton, Canada, pages 118–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>A linear programming formulation for global inference in natural language tasks.</title>
<date>2004</date>
<booktitle>In Proceedings of the 8th CoNLL.</booktitle>
<pages>1--8</pages>
<location>Boston, MA,</location>
<contexts>
<context position="11078" citStr="Roth and Yih 2004" startWordPosition="1722" endWordPosition="1725">nd select the best one, as determined by how desirable it is. Our formulation consists of two basic components: a language model (scoring function) and a small number of constraints ensuring that the resulting compressions are structurally and semantically valid. Our task is to find a globally optimal compression in the presence of these constraints. We solve this inference problem using Integer Programming without resorting to heuristics or approximations during the decoding process. Integer programming has been recently applied to several classification tasks, including relation extraction (Roth and Yih 2004), semantic role labelling (Punyakanok et al. 2004), and the generation of route directions (Marciniak and Strube 2005). Before describing our model in detail, we introduce some of the concepts and terms used in Linear Programming and Integer Programming (see Winston and Venkataramanan 2003 for an introduction). Linear Programming (LP) is a tool for solving optimisation problems in which the aim is to maximise (or minimise) a given function with respect to a set of constraints. The function to be maximised (or minimised) is referred to as the objective function. Both the objective function and </context>
</contexts>
<marker>Roth, Yih, 2004</marker>
<rawString>Roth, Dan and Wen-tau Yih. 2004. A linear programming formulation for global inference in natural language tasks. In Proceedings of the 8th CoNLL. Boston, MA, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenine Turner</author>
<author>Eugene Charniak</author>
</authors>
<title>Supervised and unsupervised learning for sentence compression.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd ACL. Ann Arbor, MI,</booktitle>
<pages>290--297</pages>
<contexts>
<context position="1531" citStr="Turner and Charniak 2005" startWordPosition="207" endWordPosition="211">eserving their grammaticality and most important information would greatly benefit a wide range of applications. Examples include text summarisation (Jing 2000), subtitle generation from spoken transcripts (Vandeghinste and Pan 2004) and information retrieval (Olivers and Dolan 1999). Sentence compression is a complex paraphrasing task with information loss involving substitution, deletion, insertion, and reordering operations. Recent years have witnessed increased interest on a simpler instantiation of the compression problem, namely word deletion (Knight and Marcu 2002; Riezler et al. 2003; Turner and Charniak 2005). More formally, given an input sentence of words W = w1,w2,...,wn, a compression is formed by removing any subset of these words. Sentence compression has received both generative and discriminative formulations in the literature. Generative approaches (Knight and Marcu 2002; Turner and Charniak 2005) are instantiations of the noisy-channel model: given a long sentence l, the aim is to find the corresponding short sentences which maximises the conditional probability P(s1l). In a discriminative setting (Knight and Marcu 2002; Riezler et al. 2003; McDonald 2006), sentences are represented by a</context>
<context position="5815" citStr="Turner and Charniak 2005" startWordPosition="882" endWordPosition="885">lexicon. Other knowledge sources include WordNet and corpus evidence gathered from a parallel corpus of originalcompressed sentence pairs. A phrase is removed only if it is not grammatically obligatory, not the focus of the local context and has a reasonable deletion probability (estimated from the parallel corpus). In contrast to Jing (2000), the bulk of the research on sentence compression relies exclusively on corpus data for modelling the compression process without recourse to extensive knowledge sources (e.g., WordNet). Approaches based on the noisy-channel model (Knight and Marcu 2002; Turner and Charniak 2005) consist of a source model P(s) (whose role is to guarantee that the generated compression is grammatical), a channel model P(l|s) (capturing the probability that the long sentence l is an expansion of the compressed sentences), and a decoder (which searches for the compressions that maximises P(s)P(l|s)). The channel model is typically estimated using a parallel corpus, although Turner and Charniak (2005) also present semi-supervised and unsupervised variants of the channel model that estimate P(l|s) without parallel data. Discriminative formulations of the compression task include decision-t</context>
<context position="23619" citStr="Turner and Charniak (2005)" startWordPosition="3932" endWordPosition="3935">-up We evaluated the approach presented in the previous sections against Knight and Marcu’s (2002) decision-tree model. This model is a good basis for comparison as it operates on parse trees and therefore is aware of syntactic structure (as our models are) but requires a large parallel corpus for training whereas our models do not; and it yields comparable performance to the noisy-channel model.2 The decision-tree model was compared against two variants of our IP model. Both variants employed the constraints described in Section 3.2 but differed in that one variant included the significance 2Turner and Charniak (2005) argue that the noisy-channel model is not an appropriate compression model since it uses a source model trained on uncompressed sentences and as a result tends to consider compressed sentences less likely than uncompressed ones. score in its objective function (see (24)), whereas the other one did not (see (1)). In both cases the sequential constraints from Section 3.1 were applied to ensure that the language model was wellformed. We give details below on the corpora we used and explain how the different model parameters were estimated. We also discuss how evaluation was carried out using hum</context>
<context position="27619" citStr="Turner and Charniak 2005" startWordPosition="4595" endWordPosition="4598">xed Integer Programming solver.5 Sentences typically take less than a few seconds to compress on a 2 GHz Pentium IV machine. Human Evaluation As mentioned earlier, the output of our models is evaluated on 40 examples. Although the size of our test set is comparable to previous studies (which are typically assessed on 32 sentences from the Ziff-Davis corpus), the sample is too small to conduct significance testing. To counteract this, human judgements are often collected on compression output; however the evaluations are limited to small subject pools (often four judges; Knight and Marcu 2002; Turner and Charniak 2005; McDonald 2006) which makes difficult to apply inferential statistics on the data. We overcome this problem by conducting our evaluation using a larger sample of subjects. Specifically, we elicited human judgements from 56 unpaid volunteers, all self reported native English speakers. The elicitation study was conducted over the Internet. Participants were presented with a set of instructions that explained the sentence compression task with examples. They were asked to judge 160 compressions in total. These included the output of the three automatic systems on the 40 test sentences paired wit</context>
</contexts>
<marker>Turner, Charniak, 2005</marker>
<rawString>Turner, Jenine and Eugene Charniak. 2005. Supervised and unsupervised learning for sentence compression. In Proceedings of the 43rd ACL. Ann Arbor, MI, pages 290–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Vandeghinste</author>
<author>Yi Pan</author>
</authors>
<title>Sentence compression for automated subtitling: A hybrid approach.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL Workshop on Text Summarization.</booktitle>
<pages>89--95</pages>
<location>Barcelona,</location>
<contexts>
<context position="1139" citStr="Vandeghinste and Pan 2004" startWordPosition="152" endWordPosition="155">essions in the face of linguistically motivated constraints. We show that such a formulation allows for relatively simple and knowledge-lean compression models that do not require parallel corpora or largescale resources. The proposed approach yields results comparable and in some cases superior to state-of-the-art. 1 Introduction A mechanism for automatically compressing sentences while preserving their grammaticality and most important information would greatly benefit a wide range of applications. Examples include text summarisation (Jing 2000), subtitle generation from spoken transcripts (Vandeghinste and Pan 2004) and information retrieval (Olivers and Dolan 1999). Sentence compression is a complex paraphrasing task with information loss involving substitution, deletion, insertion, and reordering operations. Recent years have witnessed increased interest on a simpler instantiation of the compression problem, namely word deletion (Knight and Marcu 2002; Riezler et al. 2003; Turner and Charniak 2005). More formally, given an input sentence of words W = w1,w2,...,wn, a compression is formed by removing any subset of these words. Sentence compression has received both generative and discriminative formulat</context>
</contexts>
<marker>Vandeghinste, Pan, 2004</marker>
<rawString>Vandeghinste, Vincent and Yi Pan. 2004. Sentence compression for automated subtitling: A hybrid approach. In Proceedings of the ACL Workshop on Text Summarization. Barcelona, Spain, pages 89–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wayne L Winston</author>
<author>Munirpallam Venkataramanan</author>
</authors>
<date>2003</date>
<booktitle>Introduction to Mathematical Programming. Brooks/Cole.</booktitle>
<contexts>
<context position="9585" citStr="Winston and Venkataramanan 2003" startWordPosition="1478" endWordPosition="1481">apply. As they traverse their list of compression rules they remove sentences outside the 100 best compressions (according to their channel model). This list is eventually truncated to 25 compressions. In other models (Hori and Furui 2004; McDonald 2006) the compression score is maximised 145 using dynamic programming. The latter guarantees we will find the global optimum provided the principle of optimality holds. This principle states that given the current state, the optimal decision for each of the remaining stages does not depend on previously reached stages or previously made decisions (Winston and Venkataramanan 2003). However, we know this to be false in the case of sentence compression. For example, if we have included modifiers to the left of a head noun in the compression then it makes sense that we must include the head also. With a dynamic programming approach we cannot easily guarantee such constraints hold. 3 Problem Formulation Our work models sentence compression explicitly as an optimisation problem. There are 2n possible compressions for each sentence and while many of these will be unreasonable (Knight and Marcu 2002), it is unlikely that only one compression will be satisfactory. Ideally, we </context>
<context position="11368" citStr="Winston and Venkataramanan 2003" startWordPosition="1767" endWordPosition="1770">o find a globally optimal compression in the presence of these constraints. We solve this inference problem using Integer Programming without resorting to heuristics or approximations during the decoding process. Integer programming has been recently applied to several classification tasks, including relation extraction (Roth and Yih 2004), semantic role labelling (Punyakanok et al. 2004), and the generation of route directions (Marciniak and Strube 2005). Before describing our model in detail, we introduce some of the concepts and terms used in Linear Programming and Integer Programming (see Winston and Venkataramanan 2003 for an introduction). Linear Programming (LP) is a tool for solving optimisation problems in which the aim is to maximise (or minimise) a given function with respect to a set of constraints. The function to be maximised (or minimised) is referred to as the objective function. Both the objective function and constraints must be linear. A number of decision variables are under our control which exert influence on the objective function. Specifically, they have to be optimised in order to maximise (or minimise) the objective function. Finally, a set of constraints restrict the values that the de</context>
</contexts>
<marker>Winston, Venkataramanan, 2003</marker>
<rawString>Winston, Wayne L. and Munirpallam Venkataramanan. 2003. Introduction to Mathematical Programming. Brooks/Cole.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>