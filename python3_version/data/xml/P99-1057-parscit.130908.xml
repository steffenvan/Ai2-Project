<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.547369">
Learning to Recognize Tables in Free Text
</title>
<author confidence="0.580955666666667">
Hwee Tou Ng
Chung Yong Lim
Jessica Li Teng Koo
</author>
<affiliation confidence="0.761504">
DSO National Laboratories
</affiliation>
<address confidence="0.415454">
20 Science Park Drive, Singapore 118230
</address>
<email confidence="0.547389">
Inhweetou,lchungyo,klitenglOdso.org.sg
</email>
<sectionHeader confidence="0.980546" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999813846153846">
Many real-world texts contain tables. In order to
process these texts correctly and extract the infor-
mation contained within the tables, it is important
to identify the presence and structure of tables. In
this paper, we present a new approach that learns
to recognize tables in free text, including the bound-
ary, rows and columns of tables. When tested on
Wall Street Journal news documents, our learning
approach outperforms a deterministic table recogni-
tion algorithm that identifies tables based on a fixed
set of conditions. Our learning approach is also more
flexible and easily adaptable to texts in different do-
mains with different table characteristics.
</bodyText>
<sectionHeader confidence="0.998428" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999971244897959">
Tables are present in many real-world texts. Some
information such as statistical data is best presented
in tabular form. A check on the more than 100,000
Wall Street Journal (WSJ) documents collected in
the ACL/DCI CD-ROM reveals that at least an es-
timated one in 30 documents contains tables.
Tables present a unique challenge to information
extraction systems. At the very least, the presence of
tables must be detected so that they can be skipped
over. Otherwise, processing the lines that consti-
tute tables as if they are normal &amp;quot;sentences&amp;quot; is at
best misleading and at worst may lead to erroneous
analysis of the text.
As tables contain important data and information,
it is critical for an information extraction system to
be able to extract the information embodied in ta-
bles. This can be accomplished only if the structure
of a table, including its rows and columns, are iden-
tified.
That table recognition is an important step in in-
formation extraction has been recognized in (Appelt
and Israel, 1997). Recently, there is also a greater
realization within the computational linguistics com-
munity that the layout and types of information
(such as tables) contained in a document are im-
portant considerations in text processing (see the
call for participation (Power and Scott, 1999) for
the 1999 AAAI Fall Symposium Series).
However, despite the omnipresence of tables and
their importance, there is surprisingly very little
work in computational linguistics on algorithms to
recognize tables. The only research that we are
aware of is the work of (Hurst and Douglas, 1997;
Douglas and Hurst, 1996; Douglas et al., 1995).
Their method is essentially a deterministic algorithm
that relies on spaces and special punctuation sym-
bols to identify the presence and structure of tables.
However, tables are notoriously idiosyncratic. The
main difficulty in table recognition is that there are
so many different and varied ways in which tables
can show up in real-world texts. Any deterministic
algorithm based on a fixed set of conditions is bound
to fail on tables with unforeseen layout and structure
in some domains.
In contrast, we present a new approach in this pa-
per that learns to recognize tables in free text. As
our approach is adaptive and trainable, it is more
flexible and easily adapted to texts in different do-
mains with different table characteristics.
</bodyText>
<sectionHeader confidence="0.990734" genericHeader="method">
2 Task Definition
</sectionHeader>
<bodyText confidence="0.999866947368421">
The input to our table recognition program consists
of plain texts in ASCII characters. Examples of in-
put texts are shown in Figure 1 to 3. They are docu-
ment fragments that contain tables. Figure 1 and 2
are taken from the Wall Street Journal documents in
the ACL/DCI CD-ROM, whereas Figure 3 is taken
from the patent documents in the TIPSTER IR Text
Research Collection Volume 3 CD-ROM.&apos;
In Figure 1, we added horizontal 2-digit line num-
bers &amp;quot;Line nn:&amp;quot; and vertical single-digit line num-
bers &amp;quot;n&amp;quot; for ease of reference to any line in this doc-
ument. We will use this document to illustrate the
details of our learning approach throughout this pa-
per. We refer to a horizontal line as Mine and a
vertical line as vline in the rest of this paper.
Each input text may contain zero, one or more
tables. A table consists of one or more hlines. For
example, in Figure 1, hlines 13-18 constitute a ta-
ble. Each table is subdivided into columns and rows.
</bodyText>
<footnote confidence="0.9947345">
1The extracted document fragments appear in a slightly
edited form in this paper due to space constraint.
</footnote>
<page confidence="0.996984">
443
</page>
<table confidence="0.938326736842105">
1234567890123456789012345678901234567890123456789012345678901234567890
Line 01: Raw-steel production by the nation&apos;s mills increased 4% last week to
Line 02: 1,633,000 tons from 1,570,000 tons the previous week, the American
Line 03: Iron and Steel Institute said.
Line 04:
Line 05: Last week&apos;s output fell 9.5% from the 1,804,000 tons produced a year
Line 06: earlier.
Line 07:
Line 08: The industry used 75.8% of its capability last week, compared with
Line 09: 71.9% the previous week and 72.3% a year earlier.
Line 10:
Line 11: The American Iron and Steel Institute reported:
Line 12:
Line 13: Net tons Capability
Line 14: produced utilization
Line 15: Week to March 14 1,633,000 75.8%
Line 16: Week to March 7 1,570,000 71.9%
Line 17: Year to date 15,029,000 66.9%
Line 18: Year earlier to date 18,431,000 70.8%
</table>
<construct confidence="0.632724333333333">
Line 19: The capability utilization rate is a calculation designed
Line 20:to indicate at what percent of its production capability the
Line 21:industry is operating in a given week.
</construct>
<figureCaption confidence="0.995694">
Figure 1: Wall Street Journal document fragment
</figureCaption>
<subsectionHeader confidence="0.9721906">
How Some Highly Conditional &apos;Bids&apos; Fared
Stock&apos;s
&apos;Bid&apos;* Initial
Date** Reaction*** Outcome
Bidder (Target Company)
</subsectionHeader>
<tableCaption confidence="0.695912454545455">
TWA/Carl Icahn (USAir Group)
$52 +5 3/8 to 49 1/8 Bid, seen a ploy to get
3/4/87 USAir to buy TWA, is
shelved Monday with
USAir at 45 1/4; closed
Wed. at 44 1/2
Columbia Ventures (Harnischfeger)
$19 +1/2 to 18 1/4 Harnischfeger rejects
2/23/87 bid Feb. 26 with stock
at 18 3/8; closed Wed.
at 17 5/8
</tableCaption>
<figureCaption confidence="0.996547">
Figure 2: Wall Street Journal document fragment
</figureCaption>
<bodyText confidence="0.850057833333333">
Each column of a table consists of one or more vlines.
For example, there are three columns in the table in
Figure 1: vlines 4-23, 36-45, and 48-58. Each row
of a table consists of one or more hlines. For ex-
ample, there are five rows in the table in Figure 1:
hlines 13-14, 15, 16, 17, and 18.
More specifically, the task of table recognition is
to identify the boundaries, columns and rows of ta-
bles within an input text. For example, given the in-
put text in Figure 1, our table recognition program
will identify one table with the following boundary,
columns and rows:
</bodyText>
<listItem confidence="0.999697">
1. Boundary: hlines 13-18
2. Columns: vlines 4-23, 36-45, and 48-58
3. Rows: hlines 13-14, 15, 16, 17, and 18
</listItem>
<figureCaption confidence="0.868219625">
Figure 1 to 3 illustrate some of the difficulties of
table recognition. The table in Figure 1 uses a string
of contiguous punctuation symbols &amp;quot;.&amp;quot; instead of
blank space characters in between two columns. In
Figure 2, the rows of the table can contain caption
or title information, like &amp;quot;How Some Highly Con-
ditional &apos;Bids&apos; Fared&amp;quot;, or header information like
&amp;quot;Stock&apos;s Initial Reaction***&amp;quot; and &amp;quot;Outcome&amp;quot;, or
</figureCaption>
<page confidence="0.999215">
444
</page>
<tableCaption confidence="0.967411">
side walls of the tray to provide even greater protection from convective
heat transfer. Preferred construction materials are shown in Table 1:
TABLE 1
</tableCaption>
<subsectionHeader confidence="0.72958">
Component
Material
Stiffener
</subsectionHeader>
<bodyText confidence="0.659941166666667">
Paperboard having a thickness of about 6 and 30
mil (between about 6 and 30 point chip board).
Insulation
Mineral wool, having a density of between 2.5
and 6.0 pounds per cubic foot and a thickness of
between 1/4 and 1 and 1/4 inch.
</bodyText>
<subsectionHeader confidence="0.70109">
Plastic sheets
</subsectionHeader>
<bodyText confidence="0.979898625">
Polyethylene, having a thickness of between 1
and 4 mil; coated with a reflective finish on the
exterior surfaces, such as aluminum having a
thickness of between 90 and 110 Angstroms
applied using a standard technique such as
vacuum deposition.
The stiffener 96 makes a smaller contribution to the insulation properties
of the blanket 92, than does the insulator 98. As stated above, the
</bodyText>
<figureCaption confidence="0.996044">
Figure 3: Patent document fragment
</figureCaption>
<bodyText confidence="0.999948272727273">
body content information like 152&amp;quot; and &amp;quot;+5 3/8
to 49 1/8&amp;quot;. Each row containing body content infor-
mation consists of several hlines — information on
&amp;quot;Outcome&amp;quot; spans several hlines. In Figure 3, strings
of contiguous dashes &amp;quot;-&amp;quot; occur within the table. Fur-
thermore, the two columns within the table appear
right next to each other — there are no blank vlines
separating the two columns. Worse still, some words
from the first column like &amp;quot;Insulation&amp;quot; and &amp;quot;Plastic
sheets&amp;quot; spill over to the second column. Notice that
there may or may not be any blank lines or delimiters
that immediately precede or follow a table within an
input text.
In this paper, we assume that our input texts are
plain texts that do not contain any formatting codes,
such as those found in an SGML or HTML docu-
ment. There is a large number of documents that
fall under the plain text category, and these are the
kinds of texts that our approach to table recognition
handles. The work of (Hurst and Douglas, 1997;
Douglas and Hurst, 1996; Douglas et al., 1995) also
deals with plain texts.
</bodyText>
<sectionHeader confidence="0.996632" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.998016032258065">
A table appearing in plain text is essentially a two
dimensional entity. Typically, the author of the text
uses the &lt;newline&gt; character to separate adjacent
hlines and a row is formed from one or more of such
hlines. Similarly, blank space characters or some
special punctuation characters are used to delimit
the columns.2 However, the specifics of how exactly
this is done can vary widely across texts, as exem-
plified by the tables in Figure 1 to 3.
Instead of resorting to an ad-hoc method to rec-
ognize tables, we present a new approach in this pa-
per that learns to recognize tables in plain text. Our
learning method uses purely surface features like the
proportion of the kinds of characters and their rela-
tive locations in a line and across lines to recognize
tables. It is domain independent and does not rely
on any domain-specific knowledge. We want to in-
vestigate how high an accuracy we can achieve based
purely on such surface characteristics.
The problem of table recognition is broken down
into 3 subproblems: recognizing table boundary, col-
umn, and row, in that order. Our learning approach
treats each subproblem as a separate classification
problem and relies on sample training texts in which
the table boundaries, columns, and rows have been
correctly identified. We built a graphical user inter-
face in which such markup by human annotators can
be readily done. With our X-window based GUI, a
typical table can be annotated with its boundary,
column, and row demarcation within a minute.
From these sample annotated texts, training ex-
</bodyText>
<footnote confidence="0.986272666666667">
2We assume that any &lt;tab&gt; character has been replaced
by the appropriate number of blank space characters in the
input text.
</footnote>
<page confidence="0.998485">
445
</page>
<bodyText confidence="0.998937933333333">
amples in the form of feature-value vectors with
correctly assigned classes are generated. One set
of training examples is generated for each subprob-
lem of recognizing table boundary, column, and row.
Machine learning algorithms are used to build clas-
sifiers from the training examples, one classifier per
subproblem. After training is completed, the table
recognition program will use the learned classifiers
to recognize tables in new, previously unseen input
texts.
We now describe in detail the feature extraction
process, the learning algorithms, and how tables in
new texts are recognized. The following classes of
characters are referred to throughout the rest of this
section:
</bodyText>
<listItem confidence="0.984049909090909">
• Space character: the character &amp;quot; &amp;quot; (i.e., the
character obtained by typing the space bar on
the keyboard).
• Alphanumeric character: one of the following
characters: &amp;quot;A&amp;quot; to &amp;quot;Z&amp;quot;, &amp;quot;a&amp;quot; to &amp;quot;z&amp;quot;, and &amp;quot;0&amp;quot; to
‘,9”.
• Special character: any character that is not a
space character and not an alphanumeric char-
acter.
• Separator character: one of the following char-
acters: &amp;quot;.&amp;quot;, , and &amp;quot;-&amp;quot;.
</listItem>
<subsectionHeader confidence="0.995412">
3.1 Feature Extraction
3.1.1 Boundary
</subsectionHeader>
<bodyText confidence="0.999960846153846">
Every hline in an input text generates one train-
ing example for the subproblem of table boundary
recognition. Every hline H within (outside) a table
generates a positive (negative) example. Each train-
ing example consists of a set of 27 feature values.
The first nine feature values are derived from the
immediately preceding hline H_1, the second nine
from the current hline Ho, and the last nine from
the immediately following H1.3
For a given hline H, its nine features and their
associated values are given in Table 1.
To illustrate, the feature values of the training ex-
ample generated by line 16 in Figure 1 are:
</bodyText>
<equation confidence="0.7126925">
f, 3,N, %,N,4, 3, 1, 1,
f, 3,N, %,N,4, 3, 1, 1,
1, 3,N, %,N,3, 3, 1, 1
Line 16 generated the feature values
</equation>
<bodyText confidence="0.991533">
f, 3,N,%,N, 4, 3, 1, 1. Since line 16 does not
consist of only space characters, the value of Fl is
f. There are three space characters before the word
</bodyText>
<footnote confidence="0.777049">
3For the purpose of generating the feature values for the
first and last hline in a text, we assume that the text is padded
with a line of blank space characters before the first line and
after the last line.
</footnote>
<bodyText confidence="0.999862733333334">
&amp;quot;Week&amp;quot; in line 16, so the value of F2 is 3. Since the
first non-space character in line 16 is &amp;quot;W&amp;quot; and it is
not one of the listed special characters, the value
of F3 is &amp;quot;N&amp;quot;. The last non-space character in line
16 is &amp;quot;%&amp;quot;, which becomes the value of F4. Since
line 16 does not consist of only special characters,
the value of F5 is &amp;quot;N&amp;quot;. There are four segments
in line 16 such that each segment consists of two
or more contiguous space characters: a segment
of three contiguous space characters before the
word &amp;quot;Week&amp;quot;; a segment of two contiguous space
characters after the punctuation characters &amp;quot;...&amp;quot;
and before the number &amp;quot;1,570,000&amp;quot;; a segment of
three contiguous space characters between the two
numbers &amp;quot;1,570,000&amp;quot; and &amp;quot;71.9%&amp;quot;; and the last
segment of contiguous space characters trailing
the number &amp;quot;71.9%&amp;quot;. The values of the remaining
features of line 16 are similarly determined. Fi-
nally, line 15 and 17 generated the feature values
f, 3, N, %, N, 4, 3, 1, 1 and f, 3,N, %,N, 3, 3,1, 1,
respectively.
The features attempt to capture some recurring
characteristics of lines that constitute tables. Lines
with only space characters or special characters tend
to delimit tables or are part of tables. Lines within
a table tend to begin with some number of leading
space characters. Since columns within a table are
separated by contiguous space characters or special
characters, we use segments of such contiguous char-
acters as features indicative of the presence of tables.
</bodyText>
<subsectionHeader confidence="0.577148">
3.1.2 Column
</subsectionHeader>
<bodyText confidence="0.99781925">
Every vline within a table generates one training ex-
ample for the subproblem of table column recogni-
tion. Each vline can belong to exactly one of five
classes:
</bodyText>
<listItem confidence="0.997815">
1. Outside any column
2. First line of a column
3. Within a column (but neither the first nor last
line)
4. Last line of a column
5. First and last line of a column (i.e., the column
consists of only one line)
</listItem>
<bodyText confidence="0.998280375">
Note that it is possible for one column to imme-
diately follow another (as is the case in Figure 3).
Thus a two-class representation is not adequate here,
since there would be no way to distinguish between
two adjoining columns versus one contiguous column
using only two classes.4
The start and end of a column in a table is typ-
ically characterized by a transition from a vline of
</bodyText>
<footnote confidence="0.999872333333333">
4For the identification of table boundary, we assume in
this paper that there is some hline separating any two tables,
and so a two-class representation for table boundary suffices.
</footnote>
<page confidence="0.984764">
446
</page>
<table confidence="0.8730222">
Feature Description
Fl Whether H consists of only space characters. Possible values are t (if H is a blank
line) or f (otherwise).
F2 The number of leading (or initial) space characters in H.
F3 The first non-space character in H. Possible values are one of the following special
characters: 0[](}&lt;&gt; +—*/=!@#$%A&amp; or N (if the first non-space character is
not one of the above special characters).
F4 The last non-space character in H. Possible values are the same as F3.
F5 Whether H consists entirely of one special character only. Possible values are either
one of the special characters listed in F3 (if H only consists of that special character)
or N (if H does not consist of one special character only).
F6 The number of segments in H with two or more contiguous space characters.
F7 The number of segments in H with three or more contiguous space characters.
F8 The number of segments in H with two or more contiguous separator characters.
F9 The number of segments in H with three or more contiguous separator characters.
</table>
<tableCaption confidence="0.999508">
Table 1: Feature values for table boundary
</tableCaption>
<bodyText confidence="0.999040617647059">
space (or special) characters to a vline with mixed al-
phanumeric and space characters. That is, the tran-
sition of character types across adjacent vlines gives
an indication of the demarcation of table columns.
Thus, we use character type transition as the fea-
tures to identify table columns.
Each training example consists of a set of six fea-
ture values. The first three feature values are derived
from comparing the immediately preceding vline V-1
and the current vline Vo, while the last three feature
values are derived from comparing Vo with the im-
mediately following vline V1.5
Let Vi and Vj+1 be any two adjacent vlines.
Suppose Vi = ... ... cm,i, and Vi+1 =
cid+i . • • Ci,j+1 • • • Cm,j+1 where m is the number of
hlines that constitute a table.
Then the three feature values that are derived
from the two vlines Vi and Vj+1 are determined
by counting the proportion of two horizontally ad-
jacent characters cid and cid+i (1 &lt; i &lt; m) that
satisfy some condition on the type of the two char-
acters. The precise conditions on the three features
are given in Table 2.
To illustrate, the feature values of vline 4 in Fig-
ure 1 are:
0.333,0, 0.667,0.333, 0, 0
and its class is 2 (first line of a column). In de-
riving the feature values, only hlines 13-18, the
lines that constitute the table, are considered (i.e.,
m = 6). For the first three feature values, Fl =
2/6 since there are two space-character-to-space-
character transitions from vline 3 to 4 (namely, on
hlines 13 and 14); F2 = 0 since there is no al-
phanumeric character or special character in vline
</bodyText>
<footnote confidence="0.8142225">
5For the purpose of generating the feature values for the
first and last vline in a table, we assume that the table is
padded with a vline of blank space characters before the first
vline and after the last vline.
</footnote>
<bodyText confidence="0.987398">
3; F3 = 4/6, since there are four space-character-to-
alphanumeric-character transitions from vline 3 to 4
(namely, on hlines 15-18). Similarly, the last 3 fea-
ture values are derived by examining the character
transitions from vline 4 to 5.
</bodyText>
<subsectionHeader confidence="0.894326">
3.1.3 Row
</subsectionHeader>
<bodyText confidence="0.999980666666667">
Every hline within a table generates one training ex-
ample for the subproblem of table row recognition.
Unlike table columns, every hline within a table be-
longs to some row in our formulation of the row
recognition problem. As such, each hline belongs
to exactly one of two classes:
</bodyText>
<listItem confidence="0.9985615">
1. First hline of a row
2. Subsequent hline of a row (not the first line)
</listItem>
<bodyText confidence="0.989067045454545">
The layout of a typical table is such that its rows
tend to record repetitive or similar data or informa-
tion. We use this clue in designing the features for
table row recognition. Since the information within
a row may span multiple hlines, as the &amp;quot;Outcome&amp;quot;
information in Figure 2 illustrates, we use the first
hline of a row as the basis for comparison across
rows. If two hlines are similar, then they belong
to two separate rows; otherwise, they belong to the
same row. Similarity is measured by character type
transitions, as in the case of table column recogni-
tion.
More specifically, to generate a training example
for a hline H, we compare H with H&apos;, where H&apos; is
the first hline of the immediately preceding row if
H is the first hline of the current row, and H&apos; is
the first hline of the current row if H is not the first
hline of the current row.6
Each training example consists of a set of four
feature values Fl, , F4. Fl, F2, and F3 are de-
termined by comparing H and H&apos; while F4 is de-
termined solely from H. Let H = cid • • • ci,n
</bodyText>
<page confidence="0.696478">
6 111 = H for the very first hline within a table.
447
</page>
<bodyText confidence="0.587906142857143">
Feature Description
Fl cid is a space character and cid+1 is a space character; or cid is a special character
and cid+i is a special character
F2 cid is an alphanumeric character or a special character, and c,+1 is a space char-
acter
F3 cid is a space character, and cid+1 is an alphanumeric character or a special char-
acter
</bodyText>
<tableCaption confidence="0.988574">
Table 2: Feature values for table column
</tableCaption>
<bodyText confidence="0.999640217391305">
and H&apos; = . ce,„, where n is the number
of vlines of the table. The values of Fl, , F3 are
determined by counting the proportion of the pairs
of characters ciid and cid (1 &lt; j &lt; n) that satisfy
some condition on the type of the two characters,
as listed in Table 3. Let ci,k be the first non-space
character in H. Then the value of F4 is kin.
To illustrate, the feature values of hline 16 in Fig-
ure 1 are:
0.236, 0.018, 0.018, 0.018
and its class is 1 (first line of a row). There are 55
vlines in the table, so n = 55.7 Since hline 16 is the
first line of a row, it is compared with hline 15, the
first hline of the immediately preceding row, to gen-
erate Fl, F2, and F3. Fl = 13/55 since there are 13
space-character-to-space-character transitions from
hline 15 to 16. F2 = F3 = 1/55 since there is
only one alphanumeric-character-to-space-character
transition (&amp;quot;4&amp;quot; to space character in vline 19) and
one space-character-to-special-character transition
(space character to &amp;quot;.&amp;quot; in vline 20). The first non-
space character is &amp;quot;W&amp;quot; in the first vline within the
table, so k =1.
</bodyText>
<subsectionHeader confidence="0.999945">
3.2 Learning Algorithms
</subsectionHeader>
<bodyText confidence="0.999958571428571">
We used the C4.5 decision tree induction algorithm
(Quinlan, 1993) and the backpropagation algorithm
for artificial neural nets (Rumelhart et al., 1986) as
the learning algorithms to generate the classifiers.
Both algorithms are representative state-of-the-art
learning algorithms for symbolic and connectionist
learning.
We used all the default learning parameters in the
C4.5 package. For backpropagation, the learning
parameters are: hidden units = 2, epochs = 1000,
learning rate = 0.35 and momentum term = 0.5. We
also used log n-bit encoding for the symbolic features
and normalized the numeric features to [0 ... 1] for
backpropagation.
</bodyText>
<subsectionHeader confidence="0.9963225">
3.3 Recognizing Tables in New Texts
3.3.1 Boundary
</subsectionHeader>
<bodyText confidence="0.899851666666667">
Every hline generates a test example and a classi-
fier assigns the example as either positive (within a
7In generating the feature values for table row recognition,
only the vlines enclosed within the identified first and last
column of the table are considered.
table) or negative (outside a table).
</bodyText>
<subsectionHeader confidence="0.965362">
3.3.2 Column
</subsectionHeader>
<bodyText confidence="0.9999814">
After the table boundary has been identified, clas-
sification proceeds from the first (leftmost) vline to
the last (rightmost) vline in a table. For each vline,
a classifier will return one of five classes for the test
example generated from the current vline.
Sometimes, the class assigned by a classifier to the
current vline may not be logically consistent with
the classes assigned up to that point. For instance,
it is not logically consistent if the previous vline is of
class 1 (outside any column) and the current vline
is assigned class 4 (last line of a column). When
this happens, for the backpropagation algorithm, the
class which is logically consistent and has the highest
score is assigned to the current vline; for C4.5, one of
the logically consistent classes is randomly chosen.
</bodyText>
<subsectionHeader confidence="0.965949">
3.3.3 Row
</subsectionHeader>
<bodyText confidence="0.999974555555556">
The first hline of a table always starts a new active
row (class 1). Thereafter, for a given hline, it is
compared with the first hline of the current active
row. If the classifier returns class 1 (first hline of
a row), then a new active row is started and the
current hline is the first hline of this new row. If
the classifier returns class 2 (subsequent hline of a
row), then the current active row grows to include
the current hline.
</bodyText>
<sectionHeader confidence="0.996727" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999829666666667">
To determine how well our learning approach per-
forms on the task of table recognition, we selected
100 Wall Street Journal (WSJ) news documents
from the ACL/DCI CD-ROM. After removing the
SGML markups on the original documents, we man-
ually annotated the plain-text documents with table
boundary, column, and row information. The docu-
ments shown in Figure 1 and 2 are part of the 100
documents used for evaluation.
</bodyText>
<subsectionHeader confidence="0.999103">
4.1 Accuracy Definition
</subsectionHeader>
<bodyText confidence="0.9999615">
To measure the accuracy of recognizing table bound-
ary of a new text, we compare the class assigned by
the human annotator to the class assigned by our ta-
ble recognition program on every hline of the text.
Let A be the number of hlines identified by the hu-
man annotator as being part of some table. Let B
</bodyText>
<page confidence="0.997613">
448
</page>
<note confidence="0.8280432">
Feature Description
Fl c&apos; ,j is a space character and cid is a space character
F2 ci,d is an alphanumeric character or a special character, and cid is a space character
F3 ci, d is a space character, and cid is an alphanumeric character or a special character
F4 k/n
</note>
<tableCaption confidence="0.998821">
Table 3: Feature values for table row
</tableCaption>
<bodyText confidence="0.999727909090909">
be the number of hlines identified by the program as
being part of some table. Let C be the number of
hlines identified by both the human annotator and
the program as being part of some table. Then recall
R= CIA and precision P = CIB. The accuracy of
table boundary recognition is defined as the F mea-
sure, where F = 2RPI(R + P). The accuracy of
recognizing table column (row) is defined similarly,
by comparing the class assigned by the human anno-
tator and the program to every vline (hline) within
a table.
</bodyText>
<subsectionHeader confidence="0.93722">
4.2 Deterministic Algorithms
</subsectionHeader>
<bodyText confidence="0.999988285714286">
To determine how well our learning approach per-
forms, we also implemented deterministic algorithms
for recognizing table boundary, column, and row.
The intent is to compare the accuracy achieved by
our learning approach to that of the baseline deter-
ministic algorithms. These deterministic algorithms
are described below.
</bodyText>
<subsectionHeader confidence="0.629214">
4.2.1 Boundary
</subsectionHeader>
<bodyText confidence="0.870866833333333">
A hline is considered part of a table if at least one
character of hline is not a space character and if any
of the following conditions is met:
• The ratio of the position of the first non-space
character in hline to the length of hline exceeds
some pre-determined threshold (0.25)
</bodyText>
<listItem confidence="0.982075571428571">
• Hline consists entirely of one special character.
• Hline contains three or more segments, each
consisting of two or more contiguous space char-
acters.
• Mine contains two or more segments, each con-
sisting of two or more contiguous separator
characters.
</listItem>
<subsubsectionHeader confidence="0.499552">
4.2.2 Column
</subsubsectionHeader>
<bodyText confidence="0.999958">
All vlines within a table that consist of entirely
space characters are considered not part of any col-
umn. The remaining vlines within the table are then
grouped together to form the columns.
</bodyText>
<subsectionHeader confidence="0.839064">
4.2.3 Row
</subsectionHeader>
<bodyText confidence="0.999996090909091">
The deterministic algorithm to recognize table row
is similar to the recognition algorithm of the learn-
ing approach given in Section 3.3.3, except that the
classifier is replaced by one that computes the pro-
portion of character type transitions. All characters
in the two hlines under consideration are grouped
into four types: space characters, special characters,
alphabetic characters, or digits. If the proportion
of characters that change type exceeds some pre-
determined threshold (0.5), then the two hlines be-
long to the same row.
</bodyText>
<subsectionHeader confidence="0.846592">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.999996805555555">
We evaluated the accuracy of our learning approach
on each subproblem of table boundary, column, and
row recognition. For each subproblem, we conducted
ten random trials and then averaged the accuracy
over the ten trials. In each random trial, 20% of the
texts are randomly chosen to serve as the texts for
testing, and the remaining 80% texts are used for
training. We plot the learning curve as each clas-
sifier is given increasing number of training texts.
Figure 4 to 6 summarize the average accuracy over
ten random trials for each subproblem. Besides the
accuracy for the C4.5 and backpropagation classi-
fiers, we also show the accuracy of the deterministic
algorithms.
The results indicate that our learning approach
outperforms the deterministic algorithms for all sub-
problems. The accuracy of the deterministic algo-
rithms is about 70%, whereas the maximum accu-
racy achieved by the learning approach ranges over
85% — 95%. No one learning algorithm clearly out-
performs the other, with C4.5 giving higher accu-
racy on recognizing table boundary and column, and
backpropagation performing better at recognizing
table row.
To test the generality of our learning approach,
we also evaluated it on 50 technical patent docu-
ments from the TIPSTER Volume 3 CD-ROM. To
test how well a classifier that is trained on one do-
main of texts will generalize to work on a different
domain, we also tested the accuracy of our learn-
ing approach on patent texts after training on WSJ
texts only, and vice versa. Space constraint does not
permit us to present the detailed empirical results in
this paper, but suffice to say that we found that our
learning approach is able to generalize well to work
on different domains of texts.
</bodyText>
<sectionHeader confidence="0.999866" genericHeader="method">
5 Future Work
</sectionHeader>
<bodyText confidence="0.99997175">
Currently, our table row recognition does not dis-
tinguish among the different types of rows, such as
title (or caption) row, header row, and content row.
We would like to extend our method to make such
</bodyText>
<page confidence="0.996144">
449
</page>
<figure confidence="0.9873182">
95
90
85
80
St
75
§ 70
6.5
90
60
55
e C4.5
Bp
Det
5
e 7
8
4c 65
80
55
60
85
50 0 50 o
10 20 30 40 50 60 70 so 10 20 30 40 50 60 70 80
Number of training examples Number of training examples
</figure>
<figureCaption confidence="0.86557">
Figure 4: Learning curve of boundary identification
accuracy on WSJ texts
</figureCaption>
<figure confidence="0.997157">
•
70 so
</figure>
<figureCaption confidence="0.924587">
Figure 5: Learning curve of column identification
accuracy on WSJ texts
</figureCaption>
<bodyText confidence="0.99693825">
distinction. We would also like to investigate the
effectiveness of other learning algorithms, such as
exemplar-based methods, on the task of table recog-
nition.
</bodyText>
<sectionHeader confidence="0.999679" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999263444444444">
In this paper, we present a new approach that learns
to recognize tables in free text, including the bound-
ary, rows and columns of tables. When tested on
Wall Street Journal news documents, our learning
approach outperforms a deterministic table recogni-
tion algorithm that identifies tables based on a fixed
set of conditions. Our learning approach is also more
flexible and easily adaptable to texts in different do-
mains with different table characteristics.
</bodyText>
<figureCaption confidence="0.9083505">
Figure 6: Learning curve of row identification accu-
racy on WSJ texts
</figureCaption>
<sectionHeader confidence="0.997479" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99900259375">
Douglas Appelt and David Israel. 1997. Tutorial
notes on building information extraction systems.
Tutorial held at the Fifth Conference on Applied
Natural Language Processing.
Shona Douglas and Matthew Hurst. 1996. Layout
&amp; language: Lists and tables in technical doc-
uments. In Proceedings of the ACL SIGPARSE
Workshop on Punctuation in Computational Lin-
guistics, pages 19-24.
Shona Douglas, Matthew Hurst, and David Quinn.
1995. Using natural language processing for iden-
tifying and interpreting tables in plain text. In
Fourth Annual Symposium on Document Analy-
sis and Information Retrieval, pages 535-545.
Matthew Hurst and Shona Douglas. 1997. Layout
&amp; language: Preliminary experiments in assigning
logical structure to table cells. In Proceedings of
the Fifth Conference on Applied Natural Language
Processing, pages 217-220.
Richard Power and Donia Scott. 1999. Using lay-
out for the generation, understanding or retrieval
of documents. Call for participation at the 1999
AAAI Fall Symposium Series.
John Ross Quinlan. 1993. C4.5: Programs for Ma-
chine Learning. Morgan Kaufinann, San Fran-
cisco, CA.
David E. Rumelhart, Geoffrey E. Hinton, and
Ronald J. Williams. 1986. Learning internal rep-
resentation by error propagation. In David E.
Rumelhart and James L. McClelland, editors,
Parallel Distributed Processing, Volume 1, pages
318-362. MIT Press, Cambridge, MA.
</reference>
<figure confidence="0.998974333333333">
10 20 30 40 50 60
Number of training examples
90
85
80
75
70
65
60
55
50
45 o
</figure>
<page confidence="0.956378">
450
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.399695">
<title confidence="0.8872315">Learning to Recognize Tables in Free Text Hwee Tou Ng</title>
<author confidence="0.943775">Chung Yong Lim Jessica Li Teng Koo</author>
<affiliation confidence="0.995639">DSO National Laboratories</affiliation>
<address confidence="0.959138">20 Science Park Drive, Singapore 118230</address>
<email confidence="0.644185">Inhweetou,lchungyo,klitenglOdso.org.sg</email>
<abstract confidence="0.996416928571429">Many real-world texts contain tables. In order to process these texts correctly and extract the information contained within the tables, it is important to identify the presence and structure of tables. In this paper, we present a new approach that learns to recognize tables in free text, including the boundary, rows and columns of tables. When tested on Wall Street Journal news documents, our learning approach outperforms a deterministic table recognition algorithm that identifies tables based on a fixed set of conditions. Our learning approach is also more flexible and easily adaptable to texts in different domains with different table characteristics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Douglas Appelt</author>
<author>David Israel</author>
</authors>
<title>Tutorial notes on building information extraction systems.</title>
<date>1997</date>
<booktitle>Tutorial held at the Fifth Conference on Applied Natural Language Processing.</booktitle>
<contexts>
<context position="1891" citStr="Appelt and Israel, 1997" startWordPosition="300" endWordPosition="303"> presence of tables must be detected so that they can be skipped over. Otherwise, processing the lines that constitute tables as if they are normal &amp;quot;sentences&amp;quot; is at best misleading and at worst may lead to erroneous analysis of the text. As tables contain important data and information, it is critical for an information extraction system to be able to extract the information embodied in tables. This can be accomplished only if the structure of a table, including its rows and columns, are identified. That table recognition is an important step in information extraction has been recognized in (Appelt and Israel, 1997). Recently, there is also a greater realization within the computational linguistics community that the layout and types of information (such as tables) contained in a document are important considerations in text processing (see the call for participation (Power and Scott, 1999) for the 1999 AAAI Fall Symposium Series). However, despite the omnipresence of tables and their importance, there is surprisingly very little work in computational linguistics on algorithms to recognize tables. The only research that we are aware of is the work of (Hurst and Douglas, 1997; Douglas and Hurst, 1996; Dou</context>
</contexts>
<marker>Appelt, Israel, 1997</marker>
<rawString>Douglas Appelt and David Israel. 1997. Tutorial notes on building information extraction systems. Tutorial held at the Fifth Conference on Applied Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shona Douglas</author>
<author>Matthew Hurst</author>
</authors>
<title>Layout &amp; language: Lists and tables in technical documents.</title>
<date>1996</date>
<booktitle>In Proceedings of the ACL SIGPARSE Workshop on Punctuation in Computational Linguistics,</booktitle>
<pages>19--24</pages>
<contexts>
<context position="2486" citStr="Douglas and Hurst, 1996" startWordPosition="393" endWordPosition="396">in (Appelt and Israel, 1997). Recently, there is also a greater realization within the computational linguistics community that the layout and types of information (such as tables) contained in a document are important considerations in text processing (see the call for participation (Power and Scott, 1999) for the 1999 AAAI Fall Symposium Series). However, despite the omnipresence of tables and their importance, there is surprisingly very little work in computational linguistics on algorithms to recognize tables. The only research that we are aware of is the work of (Hurst and Douglas, 1997; Douglas and Hurst, 1996; Douglas et al., 1995). Their method is essentially a deterministic algorithm that relies on spaces and special punctuation symbols to identify the presence and structure of tables. However, tables are notoriously idiosyncratic. The main difficulty in table recognition is that there are so many different and varied ways in which tables can show up in real-world texts. Any deterministic algorithm based on a fixed set of conditions is bound to fail on tables with unforeseen layout and structure in some domains. In contrast, we present a new approach in this paper that learns to recognize tables</context>
<context position="8782" citStr="Douglas and Hurst, 1996" startWordPosition="1471" endWordPosition="1474"> Worse still, some words from the first column like &amp;quot;Insulation&amp;quot; and &amp;quot;Plastic sheets&amp;quot; spill over to the second column. Notice that there may or may not be any blank lines or delimiters that immediately precede or follow a table within an input text. In this paper, we assume that our input texts are plain texts that do not contain any formatting codes, such as those found in an SGML or HTML document. There is a large number of documents that fall under the plain text category, and these are the kinds of texts that our approach to table recognition handles. The work of (Hurst and Douglas, 1997; Douglas and Hurst, 1996; Douglas et al., 1995) also deals with plain texts. 3 Approach A table appearing in plain text is essentially a two dimensional entity. Typically, the author of the text uses the &lt;newline&gt; character to separate adjacent hlines and a row is formed from one or more of such hlines. Similarly, blank space characters or some special punctuation characters are used to delimit the columns.2 However, the specifics of how exactly this is done can vary widely across texts, as exemplified by the tables in Figure 1 to 3. Instead of resorting to an ad-hoc method to recognize tables, we present a new appro</context>
</contexts>
<marker>Douglas, Hurst, 1996</marker>
<rawString>Shona Douglas and Matthew Hurst. 1996. Layout &amp; language: Lists and tables in technical documents. In Proceedings of the ACL SIGPARSE Workshop on Punctuation in Computational Linguistics, pages 19-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shona Douglas</author>
<author>Matthew Hurst</author>
<author>David Quinn</author>
</authors>
<title>Using natural language processing for identifying and interpreting tables in plain text.</title>
<date>1995</date>
<booktitle>In Fourth Annual Symposium on Document Analysis and Information Retrieval,</booktitle>
<pages>535--545</pages>
<contexts>
<context position="2509" citStr="Douglas et al., 1995" startWordPosition="397" endWordPosition="400">97). Recently, there is also a greater realization within the computational linguistics community that the layout and types of information (such as tables) contained in a document are important considerations in text processing (see the call for participation (Power and Scott, 1999) for the 1999 AAAI Fall Symposium Series). However, despite the omnipresence of tables and their importance, there is surprisingly very little work in computational linguistics on algorithms to recognize tables. The only research that we are aware of is the work of (Hurst and Douglas, 1997; Douglas and Hurst, 1996; Douglas et al., 1995). Their method is essentially a deterministic algorithm that relies on spaces and special punctuation symbols to identify the presence and structure of tables. However, tables are notoriously idiosyncratic. The main difficulty in table recognition is that there are so many different and varied ways in which tables can show up in real-world texts. Any deterministic algorithm based on a fixed set of conditions is bound to fail on tables with unforeseen layout and structure in some domains. In contrast, we present a new approach in this paper that learns to recognize tables in free text. As our a</context>
<context position="8805" citStr="Douglas et al., 1995" startWordPosition="1475" endWordPosition="1478">from the first column like &amp;quot;Insulation&amp;quot; and &amp;quot;Plastic sheets&amp;quot; spill over to the second column. Notice that there may or may not be any blank lines or delimiters that immediately precede or follow a table within an input text. In this paper, we assume that our input texts are plain texts that do not contain any formatting codes, such as those found in an SGML or HTML document. There is a large number of documents that fall under the plain text category, and these are the kinds of texts that our approach to table recognition handles. The work of (Hurst and Douglas, 1997; Douglas and Hurst, 1996; Douglas et al., 1995) also deals with plain texts. 3 Approach A table appearing in plain text is essentially a two dimensional entity. Typically, the author of the text uses the &lt;newline&gt; character to separate adjacent hlines and a row is formed from one or more of such hlines. Similarly, blank space characters or some special punctuation characters are used to delimit the columns.2 However, the specifics of how exactly this is done can vary widely across texts, as exemplified by the tables in Figure 1 to 3. Instead of resorting to an ad-hoc method to recognize tables, we present a new approach in this paper that </context>
</contexts>
<marker>Douglas, Hurst, Quinn, 1995</marker>
<rawString>Shona Douglas, Matthew Hurst, and David Quinn. 1995. Using natural language processing for identifying and interpreting tables in plain text. In Fourth Annual Symposium on Document Analysis and Information Retrieval, pages 535-545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Hurst</author>
<author>Shona Douglas</author>
</authors>
<title>Layout &amp; language: Preliminary experiments in assigning logical structure to table cells.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing,</booktitle>
<pages>217--220</pages>
<contexts>
<context position="2461" citStr="Hurst and Douglas, 1997" startWordPosition="389" endWordPosition="392">tion has been recognized in (Appelt and Israel, 1997). Recently, there is also a greater realization within the computational linguistics community that the layout and types of information (such as tables) contained in a document are important considerations in text processing (see the call for participation (Power and Scott, 1999) for the 1999 AAAI Fall Symposium Series). However, despite the omnipresence of tables and their importance, there is surprisingly very little work in computational linguistics on algorithms to recognize tables. The only research that we are aware of is the work of (Hurst and Douglas, 1997; Douglas and Hurst, 1996; Douglas et al., 1995). Their method is essentially a deterministic algorithm that relies on spaces and special punctuation symbols to identify the presence and structure of tables. However, tables are notoriously idiosyncratic. The main difficulty in table recognition is that there are so many different and varied ways in which tables can show up in real-world texts. Any deterministic algorithm based on a fixed set of conditions is bound to fail on tables with unforeseen layout and structure in some domains. In contrast, we present a new approach in this paper that l</context>
<context position="8757" citStr="Hurst and Douglas, 1997" startWordPosition="1467" endWordPosition="1470">parating the two columns. Worse still, some words from the first column like &amp;quot;Insulation&amp;quot; and &amp;quot;Plastic sheets&amp;quot; spill over to the second column. Notice that there may or may not be any blank lines or delimiters that immediately precede or follow a table within an input text. In this paper, we assume that our input texts are plain texts that do not contain any formatting codes, such as those found in an SGML or HTML document. There is a large number of documents that fall under the plain text category, and these are the kinds of texts that our approach to table recognition handles. The work of (Hurst and Douglas, 1997; Douglas and Hurst, 1996; Douglas et al., 1995) also deals with plain texts. 3 Approach A table appearing in plain text is essentially a two dimensional entity. Typically, the author of the text uses the &lt;newline&gt; character to separate adjacent hlines and a row is formed from one or more of such hlines. Similarly, blank space characters or some special punctuation characters are used to delimit the columns.2 However, the specifics of how exactly this is done can vary widely across texts, as exemplified by the tables in Figure 1 to 3. Instead of resorting to an ad-hoc method to recognize table</context>
</contexts>
<marker>Hurst, Douglas, 1997</marker>
<rawString>Matthew Hurst and Shona Douglas. 1997. Layout &amp; language: Preliminary experiments in assigning logical structure to table cells. In Proceedings of the Fifth Conference on Applied Natural Language Processing, pages 217-220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Power</author>
<author>Donia Scott</author>
</authors>
<title>Using layout for the generation, understanding or retrieval of documents. Call for participation at the 1999 AAAI Fall Symposium Series.</title>
<date>1999</date>
<contexts>
<context position="2171" citStr="Power and Scott, 1999" startWordPosition="343" endWordPosition="346">ation, it is critical for an information extraction system to be able to extract the information embodied in tables. This can be accomplished only if the structure of a table, including its rows and columns, are identified. That table recognition is an important step in information extraction has been recognized in (Appelt and Israel, 1997). Recently, there is also a greater realization within the computational linguistics community that the layout and types of information (such as tables) contained in a document are important considerations in text processing (see the call for participation (Power and Scott, 1999) for the 1999 AAAI Fall Symposium Series). However, despite the omnipresence of tables and their importance, there is surprisingly very little work in computational linguistics on algorithms to recognize tables. The only research that we are aware of is the work of (Hurst and Douglas, 1997; Douglas and Hurst, 1996; Douglas et al., 1995). Their method is essentially a deterministic algorithm that relies on spaces and special punctuation symbols to identify the presence and structure of tables. However, tables are notoriously idiosyncratic. The main difficulty in table recognition is that there </context>
</contexts>
<marker>Power, Scott, 1999</marker>
<rawString>Richard Power and Donia Scott. 1999. Using layout for the generation, understanding or retrieval of documents. Call for participation at the 1999 AAAI Fall Symposium Series.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Ross Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning. Morgan Kaufinann,</title>
<date>1993</date>
<location>San Francisco, CA.</location>
<contexts>
<context position="21230" citStr="Quinlan, 1993" startWordPosition="3682" endWordPosition="3683">ne of a row, it is compared with hline 15, the first hline of the immediately preceding row, to generate Fl, F2, and F3. Fl = 13/55 since there are 13 space-character-to-space-character transitions from hline 15 to 16. F2 = F3 = 1/55 since there is only one alphanumeric-character-to-space-character transition (&amp;quot;4&amp;quot; to space character in vline 19) and one space-character-to-special-character transition (space character to &amp;quot;.&amp;quot; in vline 20). The first nonspace character is &amp;quot;W&amp;quot; in the first vline within the table, so k =1. 3.2 Learning Algorithms We used the C4.5 decision tree induction algorithm (Quinlan, 1993) and the backpropagation algorithm for artificial neural nets (Rumelhart et al., 1986) as the learning algorithms to generate the classifiers. Both algorithms are representative state-of-the-art learning algorithms for symbolic and connectionist learning. We used all the default learning parameters in the C4.5 package. For backpropagation, the learning parameters are: hidden units = 2, epochs = 1000, learning rate = 0.35 and momentum term = 0.5. We also used log n-bit encoding for the symbolic features and normalized the numeric features to [0 ... 1] for backpropagation. 3.3 Recognizing Tables</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>John Ross Quinlan. 1993. C4.5: Programs for Machine Learning. Morgan Kaufinann, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David E Rumelhart</author>
<author>Geoffrey E Hinton</author>
<author>Ronald J Williams</author>
</authors>
<title>Learning internal representation by error propagation.</title>
<date>1986</date>
<booktitle>Parallel Distributed Processing,</booktitle>
<volume>1</volume>
<pages>318--362</pages>
<editor>In David E. Rumelhart and James L. McClelland, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="21316" citStr="Rumelhart et al., 1986" startWordPosition="3692" endWordPosition="3695"> preceding row, to generate Fl, F2, and F3. Fl = 13/55 since there are 13 space-character-to-space-character transitions from hline 15 to 16. F2 = F3 = 1/55 since there is only one alphanumeric-character-to-space-character transition (&amp;quot;4&amp;quot; to space character in vline 19) and one space-character-to-special-character transition (space character to &amp;quot;.&amp;quot; in vline 20). The first nonspace character is &amp;quot;W&amp;quot; in the first vline within the table, so k =1. 3.2 Learning Algorithms We used the C4.5 decision tree induction algorithm (Quinlan, 1993) and the backpropagation algorithm for artificial neural nets (Rumelhart et al., 1986) as the learning algorithms to generate the classifiers. Both algorithms are representative state-of-the-art learning algorithms for symbolic and connectionist learning. We used all the default learning parameters in the C4.5 package. For backpropagation, the learning parameters are: hidden units = 2, epochs = 1000, learning rate = 0.35 and momentum term = 0.5. We also used log n-bit encoding for the symbolic features and normalized the numeric features to [0 ... 1] for backpropagation. 3.3 Recognizing Tables in New Texts 3.3.1 Boundary Every hline generates a test example and a classifier ass</context>
</contexts>
<marker>Rumelhart, Hinton, Williams, 1986</marker>
<rawString>David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. 1986. Learning internal representation by error propagation. In David E. Rumelhart and James L. McClelland, editors, Parallel Distributed Processing, Volume 1, pages 318-362. MIT Press, Cambridge, MA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>