<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.990506">
Knowledge-Free Induction of Inflectional Morphologies
</title>
<author confidence="0.987715">
Patrick SCHONE Daniel JURAFSKY
</author>
<affiliation confidence="0.9074195">
University of Colorado at Boulder University of Colorado at Boulder
Boulder, Colorado 80309 Boulder, Colorado 80309
</affiliation>
<email confidence="0.99013">
schone@cs.colorado.edu jurafsky@cs.colorado.edu
</email>
<sectionHeader confidence="0.993621" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999966222222222">
We propose an algorithm to automatically induce
the morphology of inflectional languages using only
text corpora and no human input. Our algorithm
combines cues from orthography, semantics, and
syntactic distributions to induce morphological
relationships in German, Dutch, and English. Using
CELEX as a gold standard for evaluation, we show
our algorithm to be an improvement over any
knowledge-free algorithm yet proposed.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999800621621622">
Many NLP tasks, such as building machine-readable
dictionaries, are dependent on the results of
morphological analysis. While morphological
analyzers have existed since the early 1960s, current
algorithms require human labor to build rules for
morphological structure. In an attempt to avoid this
labor-intensive process, recent work has focused on
machine-learning approaches to induce
morphological structure using large corpora.
In this paper, we propose a knowledge-free
algorithm to automatically induce the morphology
structures of a language. Our algorithm takes as
input a large corpus and produces as output a set of
conflation sets indicating the various inflected and
derived forms for each word in the language. As an
example, the conflation set of the word “abuse”
would contain “abuse”, “abused”, “abuses”,
“abusive”, “abusively”, and so forth. Our algorithm
extends earlier approaches to morphology induction
by combining various induced information sources:
the semantic relatedness of the affixed forms using
a Latent Semantic Analysis approach to corpus-
based semantics (Schone and Jurafsky, 2000), affix
frequency, syntactic context, and transitive closure.
Using the hand-labeled CELEX lexicon (Baayen, et
al., 1993) as our gold standard, the current version
of our algorithm achieves an F-score of 88.1% on
the task of identifying conflation sets in English,
outperforming earlier algorithms. Our algorithm is
also applied to German and Dutch and evaluated on
its ability to find prefixes, suffixes, and circumfixes
in these languages. To our knowledge, this serves
as the first evaluation of complete regular
morphological induction of German or Dutch
(although researchers such as Nakisa and Hahn
(1996) have evaluated induction algorithms on
morphological sub-problems in German).
</bodyText>
<sectionHeader confidence="0.987175" genericHeader="method">
2 Previous Approaches
</sectionHeader>
<bodyText confidence="0.999435666666667">
Previous morphology induction approaches have
fallen into three categories. These categories differ
depending on whether human input is provided and
on whether the goal is to obtain affixes or complete
morphological analysis. We here briefly describe
work in each category.
</bodyText>
<subsectionHeader confidence="0.999431">
2.1 Using a Knowledge Source to Bootstrap
</subsectionHeader>
<bodyText confidence="0.9999859375">
Some researchers begin with some initial human-
labeled source from which they induce other
morphological components. In particular, Xu and
Croft (1998) use word context derived from a
corpus to refine Porter stemmer output. Gaussier
(1999) induces derivational morphology using an
inflectional lexicon which includes part of speech
information. Grabar and Zweigenbaum (1999) use
the SNOMED corpus of semantically-arranged
medical terms to find semantically-motivated
morphological relationships. Also, Yarowsky and
Wicentowski (2000) obtained outstanding results at
inducing English past tense after beginning with a
list of the open class roots in the language, a table of
a language’s inflectional parts of speech, and the
canonical suffixes for each part of speech.
</bodyText>
<subsectionHeader confidence="0.994808">
2.2 Affix Inventories
</subsectionHeader>
<bodyText confidence="0.997382545454545">
A second, knowledge-free category of research has
focused on obtaining affix inventories. Brent, et al.
(1995) used minimum description length (MDL) to
find the most data-compressing suffixes. Kazakov
(1997) does something akin to this using MDL as a
fitness metric for evolutionary computing. DéJean
(1998) uses a strategy similar to that of Harris
(1951). He declares that a stem has ended when the
number of characters following it exceed some
given threshold and identifies any residual following semantic relations, we identified those word pairs
the stems as suffixes. that have strong semantic correlations as being
</bodyText>
<subsectionHeader confidence="0.998109">
2.3 Complete morphological analysis
</subsectionHeader>
<bodyText confidence="0.999987">
Due to the existence of morphological ambiguity
(such as with the word “caring” whose stem is
“care” rather than “car”), finding affixes alone does
not constitute a complete morphological analysis.
Hence, the last category of research is also
knowledge-free but attempts to induce, for each
morphological variants of each other. With the
exception of word segmentation, we provided no
human information to our system. We applied our
system to an English corpus and evaluated by
comparing each word’s conflation set as produced
by our algorithm to those derivable from CELEX.
</bodyText>
<subsectionHeader confidence="0.99983">
2.4 Problems with earlier approaches
</subsectionHeader>
<bodyText confidence="0.99637875">
word of a corpus, a complete analysis. Since our Most of the existing algorithms described focus on
approach falls into this category (expanding upon suffixing in inflectional languages (though
our earlier approach (Schone and Jurafsky, 2000)), Jacquemin and DéJean describe work on prefixes).
we describe work in this area in more detail. None of these algorithms consider the general
</bodyText>
<subsubsectionHeader confidence="0.930864">
2.3.1 Jacquemin’s multiword approach
</subsubsectionHeader>
<bodyText confidence="0.999775111111111">
Jacquemin (1997) deems pairs of word n-grams as
morphologically related if two words in the first n-
gram have the same first few letters (or stem) as two
words in the second n-gram and if there is a suffix
for each stem whose length is less than k. He also
clusters groups of words having the same kinds of
word endings, which gives an added performance
boost. He applies his algorithm to a French term list
and scores based on sampled, by-hand evaluation.
</bodyText>
<subsubsectionHeader confidence="0.5436">
2.3.2. Goldsmith: EM and MDLs
</subsubsectionHeader>
<bodyText confidence="0.999930142857143">
Goldsmith (1997/2000) tries to automatically sever
each word in exactly one place in order to establish
a potential set of stems and suffixes. He uses the
expectation-maximization algorithm (EM) and MDL
as well as some triage procedures to help eliminate
inappropriate parses for every word in a corpus. He
collects the possible suffixes for each stem and calls
these signatures which give clues about word
classes. With the exceptions of capitalization
removal and some word segmentation, Goldsmith&apos;s
algorithm is otherwise knowledge-free. His
algorithm, Linguistica, is freely available on the
Internet. Goldsmith applies his algorithm to various
languages but evaluates in English and French.
</bodyText>
<subsubsectionHeader confidence="0.889439">
2.3.3 Schone and Jurafsky: induced semantics
</subsubsectionHeader>
<bodyText confidence="0.994634803921568">
In our earlier work, we (Schone and Jurafsky
(2000)) generated a list of N candidate suffixes and
used this list to identify word pairs which share the
same stem but conclude with distinct candidate
suffixes. We then applied Latent Semantic
Analysis (Deerwester, et al., 1990) as a method of
automatically determining semantic relatedness
between word pairs. Using statistics from the
conditions of circumfixing or infixing, nor are they
applicable to other language types such as
agglutinative languages (Sproat, 1992).
Additionally, most approaches have centered
around statistics of orthographic properties. We had
noted previously (Schone and Jurafsky, 2000),
however, that errors can arise from strictly
orthographic systems. We had observed in other
systems such errors as inappropriate removal of
valid affixes (“ally”—“all”), failure to resolve
morphological ambiguities (“hated”—“hat”), and
pruning of semi-productive affixes (“dirty”-A“dirt”).
Yet we illustrated that induced semantics can help
overcome some of these errors.
However, we have since observed that induced
semantics can give rise to different kinds of
problems. For instance, morphological variants may
be semantically opaque such that the meaning of
one variant cannot be readily determined by the
other (“reusability”-A“use”). Additionally, high-
frequency function words may be conflated due to
having weak semantic information (“as”—“a”).
Coupling semantic and orthographic statistics, as
well as introducing induced syntactic information
and relational transitivity can help in overcoming
these problems. Therefore, we begin with an
approach similar to our previous algorithm. Yet we
build upon this algorithm in several ways in that we:
[1] consider circumfixes, [2] automatically identify
capitalizations by treating them similar to prefixes
[3] incorporate frequency information, [4] use
distributional information to help identify syntactic
properties, and [5] use transitive closure to help find
variants that may not have been found to be
semantically related but which are related to mutual
variants. We then apply these strategies to English,
German, and Dutch. We evaluate our algorithm Figure 2). Yet using this approach, there may be
against the human-labeled CELEX lexicon in all circumfixes whose endings will be overlooked in
three languages and compare our results to those the search for suffixes unless we first remove all
that the Goldsmith and Schone/Jurafsky algorithms candidate prefixes. Therefore, we build a lexicon
would have obtained on our same data. We show consisting of all words in our corpus and identify all
how each of our additions result in progressively word beginnings with frequencies in excess of some
better overall solutions. threshold (T ). We call these pseudo-prefixes. We
</bodyText>
<figure confidence="0.910639">
1
3 Current Approach
</figure>
<figureCaption confidence="0.999805">
Figure 1: Strategy and evaluation
</figureCaption>
<subsectionHeader confidence="0.999693">
3.1 Finding Candidate Circumfix Pairings
</subsectionHeader>
<bodyText confidence="0.9999824375">
As in our earlier approach (Schone and Jurafsky,
2000), we begin by generating, from an untagged
corpus, a list of word pairs that might be
morphological variants. Our algorithm has changed
somewhat, though, since we previously sought word
pairs that vary only by a prefix or a suffix, yet we
now wish to generalize to those with circumfixing
differences. We use “circumfix” to mean true
circumfixes like the German ge-/-t as well as
combinations of prefixes and suffixes. It should be
mentioned also that we assume the existence of
languages having valid circumfixes that are not
composed merely of a prefix and a suffix that
appear independently elsewhere.
To find potential morphological variants, our first
goal is to find word endings which could serve as
suffixes. We had shown in our earlier work how one
might do this using a character tree, or trie (as in
strip all pseudo-prefixes from each word in our
lexicon and add the word residuals back into the
lexicon as if they were also words. Using this final
lexicon, we can now seek for suffixes in a manner
equivalent to what we had done before (Schone and
Jurafsky, 2000).
To demonstrate how this is done, suppose our
initial lexicon SC, contained the words “align,”
“real,” “aligns,” “realign”, “realigned”, “react”,
“reacts,” and “reacted.” Due to the high frequency
occurrence of “re-” suppose it is identified as a
pseudo-prefix. If we strip off “re-” from all words,
and add all residuals to a trie, the branch of the trie
of words beginning with “a” is depicted in Figure 2.
</bodyText>
<figureCaption confidence="0.988039">
Figure 2: Inserting the residual lexicon into a trie
</figureCaption>
<bodyText confidence="0.9999914">
In our earlier work, we showed that a majority of
the regular suffixes in the corpus can be found by
identifying trie branches that appear repetitively.
By “branch” we mean those places in the trie where
some splitting occurs. In the case of Figure 2, for
example, the branches NULL (empty circle), “-s”
and “-ed” each appear twice. We assemble a list of
all trie branches that occur some minimum number
of times (T2) and refer to such as potential suffixes.
Given this list, we can now find potential prefixes
using a similar strategy. Using our original lexicon,
we can now strip off all potential suffixes from each
word and form a new augmented lexicon. Then, (as
we had proposed before) if we reverse the ordering
on the words and insert them into a trie, the
branches that are formed will be potential prefixes
(in reverse order).
Before describing the last steps of this procedure,
it is beneficial to define a few terms (some of which
appeared in our previous work):
</bodyText>
<listItem confidence="0.744226">
[a] potential circumfix: A pair B/E where B and E
occur respectively in potential prefix and suffix lists
[b] pseudo-stem: the residue of a word after its
potential circumfix is removed
[c] candidate circumfix: a potential circumfix which
appears affixed to at least T3 pseudo-stems that are
shared by other potential circumfixes
[d] rule: a pair of candidate circumfixes sharing at
least T4 pseudo-stems
[e] pair of potential morphological variants
(PPMV): two words sharing the same rule but
distinct candidate circumfixes
[f] ruleset: the set of all PPMVs for a common rule
</listItem>
<bodyText confidence="0.999883121212121">
Our final goal in this first stage of induction is to
find all of the possible rules and their corresponding
rulesets. We therefore re-evaluate each word in the
original lexicon to identify all potential circumfixes
that could have been valid for the word. For
example, suppose that the lists of potential suffixes
and prefixes contained “-ed” and “re-” respectively.
Note also that NULL exists by default in both lists
as well. If we consider the word “realigned” from
our lexicon SC,, we would find that its potential
circumfixes would be NULL/ed, re/NULL, and
re/ed and the corresponding pseudo-stems would be
“realign,” “aligned,” and “align,” respectively,
From SC,, we also note that circumfixes re/ed and
NULL/ing share the pseudo-stems “us,” “align,” and
“view” so a rule could be created: re/ed&lt;NULL/ing.
This means that word pairs such as “reused/using”
and “realigned/aligning” would be deemed PPMVs.
Although the choices in T1 through T4 is
somewhat arbitrary, we chose T1=T2=T 3=10 and
T4=3. In English, for example, this yielded 30535
possible rules. Table 1 gives a sampling of these
potential rules in each of the three languages in
terms of frequency-sorted rank. Notice that several
“rules” are quite valid, such as the indication of an
English suffix -s. There are also valid circumfixes
like the ge-/-t circumfix of German. Capitalization
also appears (as a ‘prefix’), such as C&lt; c in English,
D&lt;d in German, and V&lt;v in Dutch. Likewise,there
are also some rules that may only be true in certain
circumstances, such as -d&lt;-r in English (such as
worked/worker, but certainly not for steed/steer.)
However, there are some rules that are
</bodyText>
<tableCaption confidence="0.998902">
Table 1: Outputs of the trie stage: potential rules
</tableCaption>
<table confidence="0.999617">
Rank ENGLISH GERMAN DUTCH
1 -s&lt; L -n&lt; L -en&lt; L
2 -ed&lt; -ing -en&lt; L -e&lt; L
4 -ing&lt; L -s&lt; L -n&lt; L
8 -ly&lt; L -en&lt; -t de-&lt; L
12 C-&lt; c- -en&lt; -te -er&lt; L
16 re-&lt; L 1-&lt; L -r&lt; L
20 -ers&lt; -ing er-&lt; L V-&lt; v-
24 1-&lt; L 1-&lt; 2- -ingen &lt; -e
28 -d&lt; -r ge-/-t &lt; -en ge-&lt; -e
32 s-&lt; L D-&lt; d- -n&lt; -rs
</table>
<bodyText confidence="0.9966008">
wrong: the potential ‘s-’ prefix of English is never
valid although word combinations like stick/tick
spark/park, and slap/lap happen frequently in
English. Incorporating semantics can help determine
the validity of each rule.
</bodyText>
<subsectionHeader confidence="0.999806">
3.2 Computing Semantics
</subsectionHeader>
<bodyText confidence="0.9999775">
Deerwester, et al. (1990) introduced an algorithm
called Latent Semantic Analysis (LSA) which
showed that valid semantic relationships between
words and documents in a corpus can be induced
with virtually no human intervention. To do this,
one typically begins by applying singular value
decomposition (SVD) to a matrix, M, whose entries
M(i,j) contains the frequency of word i as seen in
document j of the corpus. The SVD decomposes M
into the product of three matrices, U, D, and V such
</bodyText>
<equation confidence="0.891596666666667">
T
that U and V are orthogonal matrices and D is a
T
</equation>
<bodyText confidence="0.980051615384615">
diagonal matrix whose entries are the singular
values of M. The LSA approach then zeros out all
but the top k singular values of the SVD, which has
the effect of projecting vectors into an optimal k-
dimensional subspace. This methodology is
well-described in the literature (Landauer, et al.,
1998; Manning and Schütze, 1999).
In order to obtain semantic representations of each
word, we apply our previous strategy (Schone and
Jurafsky (2000)). Rather than using a term-
document matrix, we had followed an approach akin
to that of Schütze (1993), who performed SVD on
a Nx2N term-term matrix. The N here represents
the N-1 most-frequent words as well as a glob
position to account for all other words not in the top
N-1. The matrix is structured such that for a given
word w’s row, the first N columns denote words that
precede w by up to 50 words, and the second N �
columns represent those words that follow by up to NCS(µ,) =f NCS exp[ ((x-µ)/)2]dx
50 words. Since SVDs are more designed to work then, if there were nR items in the ruleset, the
with normally-distributed data (Manning and probability that a NCS is non-random is
Schütze, 1999, p. 565), we fill each entry with a Pr(NCS)_ nTNCS(µT,T)
normalized count (or Z-score) rather than straight
frequency. We then compute the SVD and keep the
(nR-nT)NCS(0,1)  nTNCS(µT,T) .
top 300 singular values to form semantic vectors for We define Pr (w —w )=Pr(NCS(w ,w )). We
</bodyText>
<equation confidence="0.524485">
sem 1 2 1 2
</equation>
<bodyText confidence="0.996062428571429">
each word. Word w would be assigned the semantic choose to accept as valid relationships only those
vector fIW=UwDk, where Uw represents the row of
U corresponding to w and Dk indicates that only the
top k diagonal entries of D have been preserved.
As a last comment, one would like to be able to
obtain a separate semantic vector for every word
(not just those in the top N). SVD computations can
be expensive and impractical for large values of N.
Yet due to the fact that U and VT are orthogonal
matrices, we can start with a matrix of reasonable-
sized N and “fold in” the remaining terms, which is
the approach we have followed. For details about
folding in terms, the reader is referred to Manning
and Schütze (1999, p. 563).
</bodyText>
<subsectionHeader confidence="0.999743">
3.3 Correlating Semantic Vectors
</subsectionHeader>
<bodyText confidence="0.99745275">
To correlate these semantic vectors, we use
normalized cosine scores (NCSs) as we had
illustrated before (Schone and Jurafsky (2000)).
The normalized cosine score between two words w1
and w2 is determined by first computing cosine
values between each word’s semantic vector and
200 other randomly selected semantic vectors. This
provides a mean (µ) and variance (� ) of correlation
</bodyText>
<equation confidence="0.97974875">
2
for each word. The NCS is given to be
NCS w w2 - min cos(S2W]w S2w2)-µk 1
( 1, ) ke(1,2) ak ( )
</equation>
<bodyText confidence="0.999966538461539">
We had previously illustrated NCS values on
various PPMVs and showed that this type of score
seems to be appropriately identifying semantic
relationships. (For example, the PPMVs of car/cars
and ally/allies had NCS values of 5.6 and 6.5
respectively, whereas car/cares and ally/all had
scored only -0.14 and -1.3.) Further, we showed
that by performing this normalizing process, one can
estimate the probability that an NCS is random or
not. We expect that random NCSs will be
approximately normally distributed according to
N(0,1). We can also estimate the distribution
N(µT,�T ) of true correlations and number of terms
</bodyText>
<page confidence="0.975303">
2
</page>
<bodyText confidence="0.9969134">
in that distribution (nT). If we define a function
PPMVs with Prsem&gt;_T5, where T5 is an acceptance
threshold. We showed in our earlier work that
T5=85% affords high overall precision while still
identifying most valid morphological relationships.
</bodyText>
<subsectionHeader confidence="0.999264">
3.4 Augmenting with Affix Frequencies
</subsectionHeader>
<bodyText confidence="0.994843552631579">
The first major change to our previous algorithm is
an attempt to overcome some of the weaknesses of
purely semantic-based morphology induction by
incorporating information about affix frequencies.
As validated by Kazakov (1997), high frequency
word endings and beginnings in inflectional
languages are very likely to be legitimate affixes. In
English, for example, the highest frequency rule is
-s—e. CELEX suggests that 99.7% of our PPMVs
for this rule would be true. However, since the
purely semantic-based approach tends to select only
relationships with contextually similar meanings,
only 92% of the PPMVs are retained. This suggests
that one might improve the analysis by
supplementing semantic probabilities with
orthographic-based probabilities (Prorth).
Our approach to obtaining Prorth is motivated by
an appeal to minimum edit distance (MED). MED
has been applied to the morphology induction
problem by other researchers (such as Yarowsky
and Wicentowski, 2000). MED determines the
minimum-weighted set of insertions, substitutions,
and deletions required to transform one word into
another. For example, only a single deletion is
required to transform “rates” into “rate” whereas
two substitutions and an insertion are required to
transform it into “rating.” Effectively, if Cost(-) is
transforming cost, Cost(rates—rate) = Cost(s—e)
whereas Cost(rates—rating)=Cost(es—ing). More
generally, suppose word X has circumfix C1=B1/E1
and pseudo-stem -S-, and word Y has circumfix
C2 =B2/E2 also with pseudo-stem -S-. Then,
Cost(X—Y)=Cost(B1SE1—B2SE2)=Cost(C 1 —C 2).
Since we are free to choose whatever cost function
we desire, we can equally choose one whose range
lies in the interval of [0,1]. Hence, we can assign Consider Table 2 which is a sample of PPMVs
Prorth(X&lt;Y) = 1-Cost(X&lt;Y). This calculation implies from the ruleset for “-s&lt;L” along with their
that the orthographic probability that X and Y are probabilities of validity. A validity threshold (T ) of
</bodyText>
<page confidence="0.346188">
5
</page>
<bodyText confidence="0.97099">
morphological variants is directly derivable from the 85% would mean that the four bottom PPMVs
cost of transforming C1 into C2. would be deemed invalid. Yet if we find that the
The only question remaining is how to determine local contexts of these low-scoring word pairs
Cost(C1&lt;C2). This cost should depend on a number match the contexts of other PPMVs having high
of factors: the frequency of the rule f(C1&lt;C2), the scores (i.e., those whose scores exceed T ), then
</bodyText>
<page confidence="0.433752">
5
</page>
<bodyText confidence="0.9633165">
reliability of the metric in comparison to that of their probabilities of validity should increase. If we
semantics (a, where a e [0,1]), and the frequencies could compute a syntax-based probability for these
of other rules involving C1 and C2. We define the words, namely Pr syntax, then assuming independence
orthographic probability of validity as we would have:
</bodyText>
<equation confidence="0.8580835">
Cost(C1&lt;C2)=1-
max f(C1&lt;Z) +
~Z
max f(W&lt;C2)
~W
2 a f(C1&lt;C2)
</equation>
<bodyText confidence="0.73333825">
algorithm to compute Prsyntax. Essentially, the
algorithm has two major components. First, for left
Pr (valid) = Pr +Pr - (Pr Pr )
s-o syntax s-o syntax
Figure 3 describes the pseudo-code for an
We suppose that orthographic information is less (L) and right-hand (R) sides of each valid PPMV of
reliable than semantic information, so we arbitrarily a given ruleset, try to find a collection of words
set a=0.5. Now since Prorth(X&lt;Y)=1-Cost(C &lt;C ), from the corpus that are collocated with L and R but
</bodyText>
<equation confidence="0.941325">
1 2
</equation>
<bodyText confidence="0.886099">
we can readily combine it with Prsem if we assume which occur statistically too many or too few times
independence using the “noisy or” formulation: in these collocations. Such word sets form
Prs-o (valid) = Prsem +Prorth - (Prsem Prorth ). (2) signatures. Then, determine similar signatures for
By using this formula, we obtain 3% (absolute)
more of the correct PPMVs than semantics alone
had provided for the -s&lt;L rule and, as will be
shown later, gives reasonable improvements overall.
</bodyText>
<subsectionHeader confidence="0.970167">
3.5 Local Syntactic Context
</subsectionHeader>
<bodyText confidence="0.996222444444444">
Since a primary role of morphology — inflectional
morphology in particular — is to convey syntactic
information, there is no guarantee that two words
that are morphological variants need to share similar
semantic properties. This suggests that performance
could improve if the induction process took
advantage of local, syntactic contexts around words
in addition to the more global, large-window
contexts used in semantic processing.
</bodyText>
<tableCaption confidence="0.980996">
Table 2: Sample probabilities for “-s&lt;L”
</tableCaption>
<table confidence="0.999678875">
Word+s Word Pr Word+s Word Pr
agendas agenda .968 legends legend .981
ideas idea .974 militias militia 1.00
pleas plea 1.00 guerrillas guerrilla 1.00
seas sea 1.00 formulas formula 1.00
areas area 1.00 railroads railroad 1.00
Areas Area .721 pads pad .731
Vegas Vega .641 feeds feed .543
</table>
<bodyText confidence="0.998766642857143">
a randomly-chosen set of words from the corpus as
well as for each of the PPMVs of the ruleset that are
not yet validated. Lastly, compute the NCS and
their corresponding probabilities (see equation 1)
between the ruleset’s signatures and those of the to-
be-validated PPMVs to see if they can be validated.
Table 3 gives an example of the kinds of
contextual words one might expect for the “-s&lt;L”
rule. In fact, the syntactic signature for “-s&lt;L” does
indeed include such words as are, other, these, two,
were, and have as indicators of words that occur on
the left-hand side of the ruleset, and a, an, this, is,
has, and A as indicators of the right-hand side.
These terms help distinguish plurals from singulars.
</bodyText>
<tableCaption confidence="0.99011">
Table 3: Examples of “-s&lt;L” contexts
</tableCaption>
<bodyText confidence="0.992234090909091">
Context for L Context for R
agendas are seas were a legend this formula
two red pads pleas have militia is an area
these ideas other areas railroad has A guerrilla
There is an added benefit from following this
approach: it can also be used to find rules that,
though different, seem to convey similar
information . Table 4 illustrates a number of such
agreements. We have yet to take advantage of this
feature, but it clearly could be of use for part-of-
speech induction.
</bodyText>
<figureCaption confidence="0.981842">
Figure 3: Pseudo-code to find Probabilitysyntax Figure 4: Semantic strengths
</figureCaption>
<equation confidence="0.790796705882353">
procedure SyntaxProb(ruleset,corpus)
leftSig =GetSignature(ruleset,corpus,left)
rightSig=GetSignature(ruleset,corpus,right)
Qruleset =Concatenate(leftSig, rightSig)
(µruleset ,� ruleset )=ComparetoRandom(Q )
ruleset
foreach PPMV in ruleset
if (PrS-O(PPMV) &gt;_ T5 ) continue
wLSig=GetSignature(PPMV,corpus,left)
wRSig=GetSignature(PPMV,corpus,right)
QPPMV =Concatenate(wLSig, wRSig)
(µ PPMV,� PPMV)=ComparetoRandom(Q )
PPMV
prob[PPMV]=Pr(NCS(PPMV,ruleset))
end procedure
function GetSignature(ruleset,corpus,side)
foreach PPMV in ruleset
if (PrS-O(PPMV) &lt; T5 ) continue
if (side=left) X = LeftWordOf(PPMV)
else X = RightWordOf(PPMV)
CountNeighbors(corpus,colloc,X)
colloc =SortWordsByFreq(colloc)
for i = 1 to 100 signature[i]=colloc[i]
return signature
end function
procedure CountNeighbors(corpus,colloc,X)
foreach W in Corpus
push(lexicon,W)
if (PositionalDistanceBetween(X,W)&lt;2)
count[W] = count[W]+1
foreach W in lexicon
if ( Zscore(count[W])&gt;_ 3.0 or
Zscore(count[W])&lt; -3.0)
colloc[W]=colloc[W]+1
</equation>
<tableCaption confidence="0.6169495">
end procedure
Table 4: Relations amongst rules
</tableCaption>
<table confidence="0.988043">
Rule Relative Cos Rule Relative Cos
-s&lt;L -ies&lt;y 83.8 -ed&lt;L -d&lt;L 95.5
-s&lt;L -es&lt;L 79.5 -ing&lt;L -e&lt;L 94.3
-ed&lt;L -ied&lt;y 81.9 -ing&lt;L -ting&lt;L 70.7
</table>
<subsectionHeader confidence="0.998006">
3.6 Branching Transitive Closure
</subsectionHeader>
<bodyText confidence="0.991566452830189">
Despite the semantic, orthographic, and syntactic
components of the algorithm, there are still valid
PPMVs, (X&lt;Y), that may seem unrelated due to
corpus choice or weak distributional properties.
However, X and Y may appear as members of other
valid PPMVs such as (X&lt;Z) and (Z&lt;Y) containing
variants (Z, in this case) which are either
semantically or syntactically related to both of the
other words. Figure 4 demonstrates this property in
greater detail. The words conveyed in Figure 4 are
all words from the corpus that have potential
relationships between variants of the word “abuse.”
Links between two words, such as “abuse” and
“Abuse,” are labeled with a weight which is the
semantic correlation derived by LSA. Solid lines
represent valid relationships with Prsem&gt;0.85 and
dashed lines indicate relationships with lower-than-
threshold scores. The absence of a link suggests that
either the potential relationship was never identified
or discarded at an earlier stage. Self loops are
assumed for each node since clearly each word
should be related morphologically to itself. Since
there are seven words that are valid morphological
relationships of “abuse,” we would like to see a
complete graph containing 21 solid edges. Yet, only
eight connections can be found by semantics alone
(Abuse&lt;abuse, abusers&lt;abusing, etc.).
However, note that there is a path that can be
followed along solid edges from every correct word
to every other correct variant. This suggests that
taking into consideration link transitivity (i.e., if
X&lt;Y1, Y1&lt;Y2, Y2&lt;Y3,... and Yt&lt;Z, then X&lt;Z)
may drastically reduce the number of deletions.
There are two caveats that need to be considered
for transitivity to be properly pursued. The first
caveat: if no rule exists that would transform X into
Z, we will assume that despite the fact that there
may be a probabilistic path between the two, we
will disregard such a path. The second caveat is that the algorithms we test against. Furthermore, since
we will say that paths can only consist of solid CELEX has limited coverage, many of these lower-
edges, namely each Pr(Y&lt;Yi i+1) on every path must frequency words could not be scored anyway. This
exceed the specified threshold. cut-off also helps each of the algorithms to obtain
Given these constraints, suppose now there is a stronger statistical information on the words they do
transitive relation from X to Z by way of some process which means that any observed failures
intermediate path Œi={Y1,Y2,.. Yt}. That is, assume cannot be attributed to weak statistics.
there is a path X&lt;Y1, Y1&lt;Y2,...,Yt&lt;Z. Suppose Morphological relationships can be represented as
also that the probabilities of these relationships are directed graphs. Figure 6, for instance, illustrates
respectively p0, p1, p2,...,pt. If � is a decay factor in the directed graph, according to CELEX, of words
the unit interval accounting for the number of link associated with “conduct.” We will call the words
separations, then we will say that the Pr(X&lt;Z) of such a directed graph the conflation set for any of
along path Œi has probability Pr &amp;quot;t = P&apos; IT-6 p.. We the words in the graph. Due to the difficulty in
combine the probabilities of all independent paths developing a scoring algorithm to compare directed
between X and Z according to Figure 5: graphs, we will follow our earlier approach and only
</bodyText>
<figureCaption confidence="0.839392">
Figure 5: Pseudocode for Branching Probability
</figureCaption>
<bodyText confidence="0.885968333333333">
function BranchProbBetween(X,Z)
prob=0
foreach independent path Œj
</bodyText>
<equation confidence="0.727532">
prob = prob+Pr�j(X&lt;Z) - (prob*Pr (X&lt;Z) )
�j
</equation>
<bodyText confidence="0.910644666666667">
return prob
If the returned probability exceeds T5, we declare X
and Z to be morphological variants of each other.
</bodyText>
<sectionHeader confidence="0.999467" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999883">
We compare this improved algorithm to our former
algorithm (Schone and Jurafsky (2000)) as well as
to Goldsmith&apos;s Linguistica (2000). We use as input
to our system 6.7 million words of English
newswire, 2.3 million of German, and 6.7 million of
Dutch. Our gold standards are the hand-tagged
morphologically-analyzed CELEX lexicon in each
of these languages (Baayen, et al., 1993). We apply
the algorithms only to those words of our corpora
with frequencies of 10 or more. Obviously this cut-
off slightly limits the generality of our results, but
it also greatly decreases processing time for all of
</bodyText>
<figureCaption confidence="0.959594">
Figure 6: Morphologic relations of “conduct”
</figureCaption>
<bodyText confidence="0.999146714285714">
compare induced conflation sets to those of
CELEX. To evaluate, we compute the number of
correct (C), inserted (I), and deleted (D) words each
algorithm predicts for each hypothesized conflation
set. If Xw represents word w&apos;s conflation set
according to an algorithm, and if Yw represents its
CELEX-based conflation set, then,
</bodyText>
<equation confidence="0.998063333333333">
C = E~w(|XwnYw|/|Yw|),
D = Y-~w(|Yw -(XwnYw)|/|Yw|), and
I = Y_~w (|Xw -(XwnYw)|/|Yw|),
</equation>
<bodyText confidence="0.999959607142857">
In making these computations, we disregard any
CELEX words absent from our data set and vice
versa. Most capital words are not in CELEX so this
process also discards them. Hence, we also make an
augmented CELEX to incorporate capitalized forms.
Table 5 uses the above scoring mechanism to
compare the F-Scores (product of precision and
recall divided by average of the two ) of our system
at a cutoff threshold of 85% to those of our earlier
algorithm (“S/J2000”) at the same threshold;
Goldsmith; and a baseline system which performs
no analysis (claiming that for any word, its
conflation set only consists of itself). The “S” and
“C” columns respectively indicate performance of
systems when scoring for suffixing and
circumfixing (using the unaugmented CELEX). The
“A” column shows circumfixing performance using
the augmented CELEX. Space limitations required
that we illustrate “A” scores for one language only,
but performance in the other two language is
similarly degraded. Boxes are shaded out for
algorithms not designed to produce circumfixes.
Note that each of our additions resulted in an
overall improvement which held true across each of
the three languages. Furthermore, using ten-fold
cross validation on the English data, we find that F-
score differences of the S column are each
statistically significant at least at the 95% level.
</bodyText>
<tableCaption confidence="0.997222">
Table 5: Computation of F-Scores
</tableCaption>
<table confidence="0.99947275">
Algorithms English German Dutch
S C A S C S C
None 62.8 59.9 51.7 75.8 63.0 74.2 70.0
Goldsmith 81.8 84.0 75.8
S/J2000 85.2 88.3 82.2
+orthogrph 85.7 82.2 76.9 89.3 76.1 84.5 78.9
+ syntax 87.5 84.0 79.0 91.6 78.2 85.6 79.4
+ transitive 88.1 84.5 79.7 92.3 78.9 85.8 79.6
</table>
<sectionHeader confidence="0.999478" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999995">
We have illustrated three extensions to our earlier
morphology induction work (Schone and Jurafsky
(2000)). In addition to induced semantics, we
incorporated induced orthographic, syntactic, and
transitive information resulting in almost a 20%
relative reduction in overall induction error. We
have also extended the work by illustrating
performance in German and Dutch where, to our
knowledge, complete morphology induction
performance measures have not previously been
obtained. Lastly, we showed a mechanism whereby
circumfixes as well as combinations of prefixing
and suffixing can be induced in lieu of the suffix-
only strategies prevailing in most previous research.
For the future, we expect improvements could be
derived by coupling this work, which focuses
primarily on inducing regular morphology, with that
of Yarowsky and Wicentowski (2000), who assume
some information about regular morphology in order
to induce irregular morphology. We also believe
that some findings of this work can benefit other
areas of linguistic induction, such as part of speech.
</bodyText>
<sectionHeader confidence="0.998843" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999943">
The authors wish to thank the anonymous reviewers
for their thorough review and insightful comments.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99919125">
Baayen, R.H., R. Piepenbrock, and H. van Rijn. (1993)
The CELEX lexical database (CD-ROM), LDC, Univ.
of Pennsylvania, Philadelphia, PA.
Brent, M., S. K. Murthy, A. Lundberg. (1995).
Discovering morphemic suffixes: A case study in
MDL induction. Proc. Of 5 Int’l Workshop on
th
Artificial Intelligence and Statistics
DéJean, H. (1998) Morphemes as necessary concepts for
structures: Discovery from untagged corpora.
Workshop on paradigms and Grounding in Natural
Language Learning, pp. 295-299.Adelaide, Australia
Deerwester, S., S.T. Dumais, G.W. Furnas, T.K.
Landauer, and R. Harshman. (1990) Indexing by
Latent Semantic Analysis. Journal of the American
Society of Information Science, Vol. 41, pp.391-407.
Gaussier, É. (1999) Unsupervised learning of derivational
morphology from inflectional lexicons. ACL &apos;99
Workshop: Unsupervised Learning in Natural
Language Processing, Univ. of Maryland.
Goldsmith, J. (1997/2000) Unsupervised learning of the
morphology of a natural language. Univ. of Chicago.
http://humanities.uchicago.edu/faculty/goldsmith.
Grabar, N. and P. Zweigenbaum. (1999) Acquisition
automatique de connaissances morphologiques sur le
vocabulaire médical, TALN, Cargése, France.
Harris, Z. (1951) Structural Linguistics. University of
Chicago Press.
Jacquemin, C. (1997) Guessing morphology from terms
and corpora. SIGIR&apos;97, pp. 156-167, Philadelphia, PA.
Kazakov, D. (1997) Unsupervised learning of naïve
morphology with genetic algorithms. In W.
Daelemans, et al., eds., ECML/Mlnet Workshop on
Empirical Learning of Natural Language Processing
Tasks, Prague, pp. 105-111.
Landauer, T.K., P.W. Foltz, and D. Laham. (1998)
Introduction to Latent Semantic Analysis. Discourse
Processes. Vol. 25, pp. 259-284.
Manning, C.D. and H. Schütze. (1999) Foundations of
Statistical Natural Language Processing, MIT Press,
Cambridge, MA.
Nakisa, R.C., U.Hahn. (1996) Where defaults don&apos;t help:
the case of the German plural system. Proc. of the
18th Conference of the Cognitive Science Society.
Schone, P. and D. Jurafsky. (2000) Knowledge-free
induction of morphology using latent semantic
analysis. Proc. of the Computational Natural
Language Learning Conference, Lisbon, pp. 67-72.
Schütze, H. (1993) Distributed syntactic representations
with an application to part-of-speech tagging.
Proceedings of the IEEE International Conference on
Neural Networks, pp. 1504-1509.
Sproat, R. (1992) Morphology and Computation. MIT
Press, Cambridge, MA.
Xu, J., B.W. Croft. (1998) Corpus-based stemming using
co-occurrence of word variants. ACM Transactions on
Information Systems, 16 (1), pp. 61-81.
Yarowsky, D. and R. Wicentowski. (2000) Minimally
supervised morphological analysis by multimodal
alignment. Proc. of the ACL 2000, Hong Kong.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.987243">
<title confidence="0.999954">Knowledge-Free Induction of Inflectional Morphologies</title>
<author confidence="0.999938">Patrick SCHONE Daniel JURAFSKY</author>
<affiliation confidence="0.99991">University of Colorado at Boulder University of Colorado at Boulder</affiliation>
<address confidence="0.999739">Boulder, Colorado 80309 Boulder, Colorado 80309</address>
<email confidence="0.999806">schone@cs.colorado.edujurafsky@cs.colorado.edu</email>
<abstract confidence="0.9987829">We propose an algorithm to automatically induce the morphology of inflectional languages using only text corpora and no human input. Our algorithm combines cues from orthography, semantics, and syntactic distributions to induce morphological relationships in German, Dutch, and English. Using CELEX as a gold standard for evaluation, we show our algorithm to be an improvement over any knowledge-free algorithm yet proposed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R H Baayen</author>
<author>R Piepenbrock</author>
<author>H van Rijn</author>
</authors>
<date>1993</date>
<booktitle>The CELEX lexical database (CD-ROM), LDC, Univ. of</booktitle>
<location>Pennsylvania, Philadelphia, PA.</location>
<marker>Baayen, Piepenbrock, van Rijn, 1993</marker>
<rawString>Baayen, R.H., R. Piepenbrock, and H. van Rijn. (1993) The CELEX lexical database (CD-ROM), LDC, Univ. of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Brent</author>
<author>S K Murthy</author>
<author>A Lundberg</author>
</authors>
<title>Discovering morphemic suffixes: A case study in MDL induction.</title>
<date>1995</date>
<booktitle>Proc. Of 5 Int’l Workshop on th Artificial Intelligence and Statistics</booktitle>
<contexts>
<context position="3740" citStr="Brent, et al. (1995)" startWordPosition="527" endWordPosition="530">inflectional lexicon which includes part of speech information. Grabar and Zweigenbaum (1999) use the SNOMED corpus of semantically-arranged medical terms to find semantically-motivated morphological relationships. Also, Yarowsky and Wicentowski (2000) obtained outstanding results at inducing English past tense after beginning with a list of the open class roots in the language, a table of a language’s inflectional parts of speech, and the canonical suffixes for each part of speech. 2.2 Affix Inventories A second, knowledge-free category of research has focused on obtaining affix inventories. Brent, et al. (1995) used minimum description length (MDL) to find the most data-compressing suffixes. Kazakov (1997) does something akin to this using MDL as a fitness metric for evolutionary computing. DéJean (1998) uses a strategy similar to that of Harris (1951). He declares that a stem has ended when the number of characters following it exceed some given threshold and identifies any residual following semantic relations, we identified those word pairs the stems as suffixes. that have strong semantic correlations as being 2.3 Complete morphological analysis Due to the existence of morphological ambiguity (su</context>
</contexts>
<marker>Brent, Murthy, Lundberg, 1995</marker>
<rawString>Brent, M., S. K. Murthy, A. Lundberg. (1995). Discovering morphemic suffixes: A case study in MDL induction. Proc. Of 5 Int’l Workshop on th Artificial Intelligence and Statistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>H DéJean</author>
</authors>
<title>Morphemes as necessary concepts for structures: Discovery from untagged corpora.</title>
<date>1998</date>
<booktitle>Workshop on paradigms and Grounding in Natural Language Learning,</booktitle>
<pages>295--299</pages>
<contexts>
<context position="3937" citStr="DéJean (1998)" startWordPosition="558" endWordPosition="559">tionships. Also, Yarowsky and Wicentowski (2000) obtained outstanding results at inducing English past tense after beginning with a list of the open class roots in the language, a table of a language’s inflectional parts of speech, and the canonical suffixes for each part of speech. 2.2 Affix Inventories A second, knowledge-free category of research has focused on obtaining affix inventories. Brent, et al. (1995) used minimum description length (MDL) to find the most data-compressing suffixes. Kazakov (1997) does something akin to this using MDL as a fitness metric for evolutionary computing. DéJean (1998) uses a strategy similar to that of Harris (1951). He declares that a stem has ended when the number of characters following it exceed some given threshold and identifies any residual following semantic relations, we identified those word pairs the stems as suffixes. that have strong semantic correlations as being 2.3 Complete morphological analysis Due to the existence of morphological ambiguity (such as with the word “caring” whose stem is “care” rather than “car”), finding affixes alone does not constitute a complete morphological analysis. Hence, the last category of research is also knowl</context>
</contexts>
<marker>DéJean, 1998</marker>
<rawString>DéJean, H. (1998) Morphemes as necessary concepts for structures: Discovery from untagged corpora. Workshop on paradigms and Grounding in Natural Language Learning, pp. 295-299.Adelaide, Australia</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S T Dumais</author>
<author>G W Furnas</author>
<author>T K Landauer</author>
<author>R Harshman</author>
</authors>
<title>Indexing by Latent Semantic Analysis.</title>
<date>1990</date>
<journal>Journal of the American Society of Information Science,</journal>
<volume>41</volume>
<pages>391--407</pages>
<contexts>
<context position="6827" citStr="Deerwester, et al., 1990" startWordPosition="1010" endWordPosition="1013">ues about word classes. With the exceptions of capitalization removal and some word segmentation, Goldsmith&apos;s algorithm is otherwise knowledge-free. His algorithm, Linguistica, is freely available on the Internet. Goldsmith applies his algorithm to various languages but evaluates in English and French. 2.3.3 Schone and Jurafsky: induced semantics In our earlier work, we (Schone and Jurafsky (2000)) generated a list of N candidate suffixes and used this list to identify word pairs which share the same stem but conclude with distinct candidate suffixes. We then applied Latent Semantic Analysis (Deerwester, et al., 1990) as a method of automatically determining semantic relatedness between word pairs. Using statistics from the conditions of circumfixing or infixing, nor are they applicable to other language types such as agglutinative languages (Sproat, 1992). Additionally, most approaches have centered around statistics of orthographic properties. We had noted previously (Schone and Jurafsky, 2000), however, that errors can arise from strictly orthographic systems. We had observed in other systems such errors as inappropriate removal of valid affixes (“ally”—“all”), failure to resolve morphological ambiguiti</context>
<context position="14845" citStr="Deerwester, et al. (1990)" startWordPosition="2313" endWordPosition="2316">e some rules that are Table 1: Outputs of the trie stage: potential rules Rank ENGLISH GERMAN DUTCH 1 -s&lt; L -n&lt; L -en&lt; L 2 -ed&lt; -ing -en&lt; L -e&lt; L 4 -ing&lt; L -s&lt; L -n&lt; L 8 -ly&lt; L -en&lt; -t de-&lt; L 12 C-&lt; c- -en&lt; -te -er&lt; L 16 re-&lt; L 1-&lt; L -r&lt; L 20 -ers&lt; -ing er-&lt; L V-&lt; v24 1-&lt; L 1-&lt; 2- -ingen &lt; -e 28 -d&lt; -r ge-/-t &lt; -en ge-&lt; -e 32 s-&lt; L D-&lt; d- -n&lt; -rs wrong: the potential ‘s-’ prefix of English is never valid although word combinations like stick/tick spark/park, and slap/lap happen frequently in English. Incorporating semantics can help determine the validity of each rule. 3.2 Computing Semantics Deerwester, et al. (1990) introduced an algorithm called Latent Semantic Analysis (LSA) which showed that valid semantic relationships between words and documents in a corpus can be induced with virtually no human intervention. To do this, one typically begins by applying singular value decomposition (SVD) to a matrix, M, whose entries M(i,j) contains the frequency of word i as seen in document j of the corpus. The SVD decomposes M into the product of three matrices, U, D, and V such T that U and V are orthogonal matrices and D is a T diagonal matrix whose entries are the singular values of M. The LSA approach then ze</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Deerwester, S., S.T. Dumais, G.W. Furnas, T.K. Landauer, and R. Harshman. (1990) Indexing by Latent Semantic Analysis. Journal of the American Society of Information Science, Vol. 41, pp.391-407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>É Gaussier</author>
</authors>
<title>Unsupervised learning of derivational morphology from inflectional lexicons.</title>
<date>1999</date>
<booktitle>ACL &apos;99 Workshop: Unsupervised Learning in Natural Language Processing,</booktitle>
<institution>Univ. of Maryland.</institution>
<contexts>
<context position="3078" citStr="Gaussier (1999)" startWordPosition="436" endWordPosition="437">morphological sub-problems in German). 2 Previous Approaches Previous morphology induction approaches have fallen into three categories. These categories differ depending on whether human input is provided and on whether the goal is to obtain affixes or complete morphological analysis. We here briefly describe work in each category. 2.1 Using a Knowledge Source to Bootstrap Some researchers begin with some initial humanlabeled source from which they induce other morphological components. In particular, Xu and Croft (1998) use word context derived from a corpus to refine Porter stemmer output. Gaussier (1999) induces derivational morphology using an inflectional lexicon which includes part of speech information. Grabar and Zweigenbaum (1999) use the SNOMED corpus of semantically-arranged medical terms to find semantically-motivated morphological relationships. Also, Yarowsky and Wicentowski (2000) obtained outstanding results at inducing English past tense after beginning with a list of the open class roots in the language, a table of a language’s inflectional parts of speech, and the canonical suffixes for each part of speech. 2.2 Affix Inventories A second, knowledge-free category of research ha</context>
</contexts>
<marker>Gaussier, 1999</marker>
<rawString>Gaussier, É. (1999) Unsupervised learning of derivational morphology from inflectional lexicons. ACL &apos;99 Workshop: Unsupervised Learning in Natural Language Processing, Univ. of Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goldsmith</author>
</authors>
<title>Unsupervised learning of the morphology of a natural language.</title>
<date>1997</date>
<journal>Univ. of Chicago. http://humanities.uchicago.edu/faculty/goldsmith.</journal>
<contexts>
<context position="5825" citStr="Goldsmith (1997" startWordPosition="860" endWordPosition="861">k in this area in more detail. None of these algorithms consider the general 2.3.1 Jacquemin’s multiword approach Jacquemin (1997) deems pairs of word n-grams as morphologically related if two words in the first ngram have the same first few letters (or stem) as two words in the second n-gram and if there is a suffix for each stem whose length is less than k. He also clusters groups of words having the same kinds of word endings, which gives an added performance boost. He applies his algorithm to a French term list and scores based on sampled, by-hand evaluation. 2.3.2. Goldsmith: EM and MDLs Goldsmith (1997/2000) tries to automatically sever each word in exactly one place in order to establish a potential set of stems and suffixes. He uses the expectation-maximization algorithm (EM) and MDL as well as some triage procedures to help eliminate inappropriate parses for every word in a corpus. He collects the possible suffixes for each stem and calls these signatures which give clues about word classes. With the exceptions of capitalization removal and some word segmentation, Goldsmith&apos;s algorithm is otherwise knowledge-free. His algorithm, Linguistica, is freely available on the Internet. Goldsmith</context>
</contexts>
<marker>Goldsmith, 1997</marker>
<rawString>Goldsmith, J. (1997/2000) Unsupervised learning of the morphology of a natural language. Univ. of Chicago. http://humanities.uchicago.edu/faculty/goldsmith.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Grabar</author>
<author>P Zweigenbaum</author>
</authors>
<title>Acquisition automatique de connaissances morphologiques sur le vocabulaire médical,</title>
<date>1999</date>
<location>TALN, Cargése, France.</location>
<contexts>
<context position="3213" citStr="Grabar and Zweigenbaum (1999)" startWordPosition="451" endWordPosition="454">ree categories. These categories differ depending on whether human input is provided and on whether the goal is to obtain affixes or complete morphological analysis. We here briefly describe work in each category. 2.1 Using a Knowledge Source to Bootstrap Some researchers begin with some initial humanlabeled source from which they induce other morphological components. In particular, Xu and Croft (1998) use word context derived from a corpus to refine Porter stemmer output. Gaussier (1999) induces derivational morphology using an inflectional lexicon which includes part of speech information. Grabar and Zweigenbaum (1999) use the SNOMED corpus of semantically-arranged medical terms to find semantically-motivated morphological relationships. Also, Yarowsky and Wicentowski (2000) obtained outstanding results at inducing English past tense after beginning with a list of the open class roots in the language, a table of a language’s inflectional parts of speech, and the canonical suffixes for each part of speech. 2.2 Affix Inventories A second, knowledge-free category of research has focused on obtaining affix inventories. Brent, et al. (1995) used minimum description length (MDL) to find the most data-compressing </context>
</contexts>
<marker>Grabar, Zweigenbaum, 1999</marker>
<rawString>Grabar, N. and P. Zweigenbaum. (1999) Acquisition automatique de connaissances morphologiques sur le vocabulaire médical, TALN, Cargése, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Harris</author>
</authors>
<title>Structural Linguistics.</title>
<date>1951</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="3986" citStr="Harris (1951)" startWordPosition="567" endWordPosition="568">obtained outstanding results at inducing English past tense after beginning with a list of the open class roots in the language, a table of a language’s inflectional parts of speech, and the canonical suffixes for each part of speech. 2.2 Affix Inventories A second, knowledge-free category of research has focused on obtaining affix inventories. Brent, et al. (1995) used minimum description length (MDL) to find the most data-compressing suffixes. Kazakov (1997) does something akin to this using MDL as a fitness metric for evolutionary computing. DéJean (1998) uses a strategy similar to that of Harris (1951). He declares that a stem has ended when the number of characters following it exceed some given threshold and identifies any residual following semantic relations, we identified those word pairs the stems as suffixes. that have strong semantic correlations as being 2.3 Complete morphological analysis Due to the existence of morphological ambiguity (such as with the word “caring” whose stem is “care” rather than “car”), finding affixes alone does not constitute a complete morphological analysis. Hence, the last category of research is also knowledge-free but attempts to induce, for each morpho</context>
</contexts>
<marker>Harris, 1951</marker>
<rawString>Harris, Z. (1951) Structural Linguistics. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Jacquemin</author>
</authors>
<title>Guessing morphology from terms and corpora.</title>
<date>1997</date>
<booktitle>SIGIR&apos;97,</booktitle>
<pages>156--167</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="5340" citStr="Jacquemin (1997)" startWordPosition="772" endWordPosition="773">stem to an English corpus and evaluated by comparing each word’s conflation set as produced by our algorithm to those derivable from CELEX. 2.4 Problems with earlier approaches word of a corpus, a complete analysis. Since our Most of the existing algorithms described focus on approach falls into this category (expanding upon suffixing in inflectional languages (though our earlier approach (Schone and Jurafsky, 2000)), Jacquemin and DéJean describe work on prefixes). we describe work in this area in more detail. None of these algorithms consider the general 2.3.1 Jacquemin’s multiword approach Jacquemin (1997) deems pairs of word n-grams as morphologically related if two words in the first ngram have the same first few letters (or stem) as two words in the second n-gram and if there is a suffix for each stem whose length is less than k. He also clusters groups of words having the same kinds of word endings, which gives an added performance boost. He applies his algorithm to a French term list and scores based on sampled, by-hand evaluation. 2.3.2. Goldsmith: EM and MDLs Goldsmith (1997/2000) tries to automatically sever each word in exactly one place in order to establish a potential set of stems a</context>
</contexts>
<marker>Jacquemin, 1997</marker>
<rawString>Jacquemin, C. (1997) Guessing morphology from terms and corpora. SIGIR&apos;97, pp. 156-167, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Kazakov</author>
</authors>
<title>Unsupervised learning of naïve morphology with genetic algorithms.</title>
<date>1997</date>
<booktitle>ECML/Mlnet Workshop on Empirical Learning of Natural Language Processing Tasks, Prague,</booktitle>
<pages>105--111</pages>
<editor>In W. Daelemans, et al., eds.,</editor>
<contexts>
<context position="3837" citStr="Kazakov (1997)" startWordPosition="542" endWordPosition="543">NOMED corpus of semantically-arranged medical terms to find semantically-motivated morphological relationships. Also, Yarowsky and Wicentowski (2000) obtained outstanding results at inducing English past tense after beginning with a list of the open class roots in the language, a table of a language’s inflectional parts of speech, and the canonical suffixes for each part of speech. 2.2 Affix Inventories A second, knowledge-free category of research has focused on obtaining affix inventories. Brent, et al. (1995) used minimum description length (MDL) to find the most data-compressing suffixes. Kazakov (1997) does something akin to this using MDL as a fitness metric for evolutionary computing. DéJean (1998) uses a strategy similar to that of Harris (1951). He declares that a stem has ended when the number of characters following it exceed some given threshold and identifies any residual following semantic relations, we identified those word pairs the stems as suffixes. that have strong semantic correlations as being 2.3 Complete morphological analysis Due to the existence of morphological ambiguity (such as with the word “caring” whose stem is “care” rather than “car”), finding affixes alone does </context>
<context position="19187" citStr="Kazakov (1997)" startWordPosition="3056" endWordPosition="3057">o N(0,1). We can also estimate the distribution N(µT,�T ) of true correlations and number of terms 2 in that distribution (nT). If we define a function PPMVs with Prsem&gt;_T5, where T5 is an acceptance threshold. We showed in our earlier work that T5=85% affords high overall precision while still identifying most valid morphological relationships. 3.4 Augmenting with Affix Frequencies The first major change to our previous algorithm is an attempt to overcome some of the weaknesses of purely semantic-based morphology induction by incorporating information about affix frequencies. As validated by Kazakov (1997), high frequency word endings and beginnings in inflectional languages are very likely to be legitimate affixes. In English, for example, the highest frequency rule is -s—e. CELEX suggests that 99.7% of our PPMVs for this rule would be true. However, since the purely semantic-based approach tends to select only relationships with contextually similar meanings, only 92% of the PPMVs are retained. This suggests that one might improve the analysis by supplementing semantic probabilities with orthographic-based probabilities (Prorth). Our approach to obtaining Prorth is motivated by an appeal to m</context>
</contexts>
<marker>Kazakov, 1997</marker>
<rawString>Kazakov, D. (1997) Unsupervised learning of naïve morphology with genetic algorithms. In W. Daelemans, et al., eds., ECML/Mlnet Workshop on Empirical Learning of Natural Language Processing Tasks, Prague, pp. 105-111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>P W Foltz</author>
<author>D Laham</author>
</authors>
<title>Introduction to Latent Semantic Analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes.</booktitle>
<volume>25</volume>
<pages>259--284</pages>
<contexts>
<context position="15657" citStr="Landauer, et al., 1998" startWordPosition="2453" endWordPosition="2456">ntervention. To do this, one typically begins by applying singular value decomposition (SVD) to a matrix, M, whose entries M(i,j) contains the frequency of word i as seen in document j of the corpus. The SVD decomposes M into the product of three matrices, U, D, and V such T that U and V are orthogonal matrices and D is a T diagonal matrix whose entries are the singular values of M. The LSA approach then zeros out all but the top k singular values of the SVD, which has the effect of projecting vectors into an optimal kdimensional subspace. This methodology is well-described in the literature (Landauer, et al., 1998; Manning and Schütze, 1999). In order to obtain semantic representations of each word, we apply our previous strategy (Schone and Jurafsky (2000)). Rather than using a termdocument matrix, we had followed an approach akin to that of Schütze (1993), who performed SVD on a Nx2N term-term matrix. The N here represents the N-1 most-frequent words as well as a glob position to account for all other words not in the top N-1. The matrix is structured such that for a given word w’s row, the first N columns denote words that precede w by up to 50 words, and the second N � columns represent those words</context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>Landauer, T.K., P.W. Foltz, and D. Laham. (1998) Introduction to Latent Semantic Analysis. Discourse Processes. Vol. 25, pp. 259-284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Schütze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing,</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="15685" citStr="Manning and Schütze, 1999" startWordPosition="2457" endWordPosition="2460"> one typically begins by applying singular value decomposition (SVD) to a matrix, M, whose entries M(i,j) contains the frequency of word i as seen in document j of the corpus. The SVD decomposes M into the product of three matrices, U, D, and V such T that U and V are orthogonal matrices and D is a T diagonal matrix whose entries are the singular values of M. The LSA approach then zeros out all but the top k singular values of the SVD, which has the effect of projecting vectors into an optimal kdimensional subspace. This methodology is well-described in the literature (Landauer, et al., 1998; Manning and Schütze, 1999). In order to obtain semantic representations of each word, we apply our previous strategy (Schone and Jurafsky (2000)). Rather than using a termdocument matrix, we had followed an approach akin to that of Schütze (1993), who performed SVD on a Nx2N term-term matrix. The N here represents the N-1 most-frequent words as well as a glob position to account for all other words not in the top N-1. The matrix is structured such that for a given word w’s row, the first N columns denote words that precede w by up to 50 words, and the second N � columns represent those words that follow by up to NCS(µ</context>
<context position="17526" citStr="Manning and Schütze (1999" startWordPosition="2789" endWordPosition="2792">those vector fIW=UwDk, where Uw represents the row of U corresponding to w and Dk indicates that only the top k diagonal entries of D have been preserved. As a last comment, one would like to be able to obtain a separate semantic vector for every word (not just those in the top N). SVD computations can be expensive and impractical for large values of N. Yet due to the fact that U and VT are orthogonal matrices, we can start with a matrix of reasonablesized N and “fold in” the remaining terms, which is the approach we have followed. For details about folding in terms, the reader is referred to Manning and Schütze (1999, p. 563). 3.3 Correlating Semantic Vectors To correlate these semantic vectors, we use normalized cosine scores (NCSs) as we had illustrated before (Schone and Jurafsky (2000)). The normalized cosine score between two words w1 and w2 is determined by first computing cosine values between each word’s semantic vector and 200 other randomly selected semantic vectors. This provides a mean (µ) and variance (� ) of correlation 2 for each word. The NCS is given to be NCS w w2 - min cos(S2W]w S2w2)-µk 1 ( 1, ) ke(1,2) ak ( ) We had previously illustrated NCS values on various PPMVs and showed that th</context>
</contexts>
<marker>Manning, Schütze, 1999</marker>
<rawString>Manning, C.D. and H. Schütze. (1999) Foundations of Statistical Natural Language Processing, MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Nakisa</author>
<author>U Hahn</author>
</authors>
<title>Where defaults don&apos;t help: the case of the German plural system.</title>
<date>1996</date>
<booktitle>Proc. of the 18th Conference of the Cognitive Science Society.</booktitle>
<contexts>
<context position="2423" citStr="Nakisa and Hahn (1996)" startWordPosition="339" endWordPosition="342">fsky, 2000), affix frequency, syntactic context, and transitive closure. Using the hand-labeled CELEX lexicon (Baayen, et al., 1993) as our gold standard, the current version of our algorithm achieves an F-score of 88.1% on the task of identifying conflation sets in English, outperforming earlier algorithms. Our algorithm is also applied to German and Dutch and evaluated on its ability to find prefixes, suffixes, and circumfixes in these languages. To our knowledge, this serves as the first evaluation of complete regular morphological induction of German or Dutch (although researchers such as Nakisa and Hahn (1996) have evaluated induction algorithms on morphological sub-problems in German). 2 Previous Approaches Previous morphology induction approaches have fallen into three categories. These categories differ depending on whether human input is provided and on whether the goal is to obtain affixes or complete morphological analysis. We here briefly describe work in each category. 2.1 Using a Knowledge Source to Bootstrap Some researchers begin with some initial humanlabeled source from which they induce other morphological components. In particular, Xu and Croft (1998) use word context derived from a </context>
</contexts>
<marker>Nakisa, Hahn, 1996</marker>
<rawString>Nakisa, R.C., U.Hahn. (1996) Where defaults don&apos;t help: the case of the German plural system. Proc. of the 18th Conference of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Schone</author>
<author>D Jurafsky</author>
</authors>
<title>Knowledge-free induction of morphology using latent semantic analysis.</title>
<date>2000</date>
<booktitle>Proc. of the Computational Natural Language Learning Conference, Lisbon,</booktitle>
<pages>67--72</pages>
<contexts>
<context position="1812" citStr="Schone and Jurafsky, 2000" startWordPosition="247" endWordPosition="250">automatically induce the morphology structures of a language. Our algorithm takes as input a large corpus and produces as output a set of conflation sets indicating the various inflected and derived forms for each word in the language. As an example, the conflation set of the word “abuse” would contain “abuse”, “abused”, “abuses”, “abusive”, “abusively”, and so forth. Our algorithm extends earlier approaches to morphology induction by combining various induced information sources: the semantic relatedness of the affixed forms using a Latent Semantic Analysis approach to corpusbased semantics (Schone and Jurafsky, 2000), affix frequency, syntactic context, and transitive closure. Using the hand-labeled CELEX lexicon (Baayen, et al., 1993) as our gold standard, the current version of our algorithm achieves an F-score of 88.1% on the task of identifying conflation sets in English, outperforming earlier algorithms. Our algorithm is also applied to German and Dutch and evaluated on its ability to find prefixes, suffixes, and circumfixes in these languages. To our knowledge, this serves as the first evaluation of complete regular morphological induction of German or Dutch (although researchers such as Nakisa and </context>
<context position="5143" citStr="Schone and Jurafsky, 2000" startWordPosition="741" endWordPosition="744">search is also knowledge-free but attempts to induce, for each morphological variants of each other. With the exception of word segmentation, we provided no human information to our system. We applied our system to an English corpus and evaluated by comparing each word’s conflation set as produced by our algorithm to those derivable from CELEX. 2.4 Problems with earlier approaches word of a corpus, a complete analysis. Since our Most of the existing algorithms described focus on approach falls into this category (expanding upon suffixing in inflectional languages (though our earlier approach (Schone and Jurafsky, 2000)), Jacquemin and DéJean describe work on prefixes). we describe work in this area in more detail. None of these algorithms consider the general 2.3.1 Jacquemin’s multiword approach Jacquemin (1997) deems pairs of word n-grams as morphologically related if two words in the first ngram have the same first few letters (or stem) as two words in the second n-gram and if there is a suffix for each stem whose length is less than k. He also clusters groups of words having the same kinds of word endings, which gives an added performance boost. He applies his algorithm to a French term list and scores b</context>
<context position="6602" citStr="Schone and Jurafsky (2000)" startWordPosition="974" endWordPosition="977">-maximization algorithm (EM) and MDL as well as some triage procedures to help eliminate inappropriate parses for every word in a corpus. He collects the possible suffixes for each stem and calls these signatures which give clues about word classes. With the exceptions of capitalization removal and some word segmentation, Goldsmith&apos;s algorithm is otherwise knowledge-free. His algorithm, Linguistica, is freely available on the Internet. Goldsmith applies his algorithm to various languages but evaluates in English and French. 2.3.3 Schone and Jurafsky: induced semantics In our earlier work, we (Schone and Jurafsky (2000)) generated a list of N candidate suffixes and used this list to identify word pairs which share the same stem but conclude with distinct candidate suffixes. We then applied Latent Semantic Analysis (Deerwester, et al., 1990) as a method of automatically determining semantic relatedness between word pairs. Using statistics from the conditions of circumfixing or infixing, nor are they applicable to other language types such as agglutinative languages (Sproat, 1992). Additionally, most approaches have centered around statistics of orthographic properties. We had noted previously (Schone and Jura</context>
<context position="9497" citStr="Schone and Jurafsky, 2000" startWordPosition="1397" endWordPosition="1400">ree languages and compare our results to those the search for suffixes unless we first remove all that the Goldsmith and Schone/Jurafsky algorithms candidate prefixes. Therefore, we build a lexicon would have obtained on our same data. We show consisting of all words in our corpus and identify all how each of our additions result in progressively word beginnings with frequencies in excess of some better overall solutions. threshold (T ). We call these pseudo-prefixes. We 1 3 Current Approach Figure 1: Strategy and evaluation 3.1 Finding Candidate Circumfix Pairings As in our earlier approach (Schone and Jurafsky, 2000), we begin by generating, from an untagged corpus, a list of word pairs that might be morphological variants. Our algorithm has changed somewhat, though, since we previously sought word pairs that vary only by a prefix or a suffix, yet we now wish to generalize to those with circumfixing differences. We use “circumfix” to mean true circumfixes like the German ge-/-t as well as combinations of prefixes and suffixes. It should be mentioned also that we assume the existence of languages having valid circumfixes that are not composed merely of a prefix and a suffix that appear independently elsewh</context>
<context position="15803" citStr="Schone and Jurafsky (2000)" startWordPosition="2475" endWordPosition="2478"> frequency of word i as seen in document j of the corpus. The SVD decomposes M into the product of three matrices, U, D, and V such T that U and V are orthogonal matrices and D is a T diagonal matrix whose entries are the singular values of M. The LSA approach then zeros out all but the top k singular values of the SVD, which has the effect of projecting vectors into an optimal kdimensional subspace. This methodology is well-described in the literature (Landauer, et al., 1998; Manning and Schütze, 1999). In order to obtain semantic representations of each word, we apply our previous strategy (Schone and Jurafsky (2000)). Rather than using a termdocument matrix, we had followed an approach akin to that of Schütze (1993), who performed SVD on a Nx2N term-term matrix. The N here represents the N-1 most-frequent words as well as a glob position to account for all other words not in the top N-1. The matrix is structured such that for a given word w’s row, the first N columns denote words that precede w by up to 50 words, and the second N � columns represent those words that follow by up to NCS(µ,) =f NCS exp[ ((x-µ)/)2]dx 50 words. Since SVDs are more designed to work then, if there were nR items in the rules</context>
<context position="17702" citStr="Schone and Jurafsky (2000)" startWordPosition="2815" endWordPosition="2818">ne would like to be able to obtain a separate semantic vector for every word (not just those in the top N). SVD computations can be expensive and impractical for large values of N. Yet due to the fact that U and VT are orthogonal matrices, we can start with a matrix of reasonablesized N and “fold in” the remaining terms, which is the approach we have followed. For details about folding in terms, the reader is referred to Manning and Schütze (1999, p. 563). 3.3 Correlating Semantic Vectors To correlate these semantic vectors, we use normalized cosine scores (NCSs) as we had illustrated before (Schone and Jurafsky (2000)). The normalized cosine score between two words w1 and w2 is determined by first computing cosine values between each word’s semantic vector and 200 other randomly selected semantic vectors. This provides a mean (µ) and variance (� ) of correlation 2 for each word. The NCS is given to be NCS w w2 - min cos(S2W]w S2w2)-µk 1 ( 1, ) ke(1,2) ak ( ) We had previously illustrated NCS values on various PPMVs and showed that this type of score seems to be appropriately identifying semantic relationships. (For example, the PPMVs of car/cars and ally/allies had NCS values of 5.6 and 6.5 respectively, w</context>
<context position="29871" citStr="Schone and Jurafsky (2000)" startWordPosition="4714" endWordPosition="4717">T-6 p.. We the words in the graph. Due to the difficulty in combine the probabilities of all independent paths developing a scoring algorithm to compare directed between X and Z according to Figure 5: graphs, we will follow our earlier approach and only Figure 5: Pseudocode for Branching Probability function BranchProbBetween(X,Z) prob=0 foreach independent path Œj prob = prob+Pr�j(X&lt;Z) - (prob*Pr (X&lt;Z) ) �j return prob If the returned probability exceeds T5, we declare X and Z to be morphological variants of each other. 4 Evaluation We compare this improved algorithm to our former algorithm (Schone and Jurafsky (2000)) as well as to Goldsmith&apos;s Linguistica (2000). We use as input to our system 6.7 million words of English newswire, 2.3 million of German, and 6.7 million of Dutch. Our gold standards are the hand-tagged morphologically-analyzed CELEX lexicon in each of these languages (Baayen, et al., 1993). We apply the algorithms only to those words of our corpora with frequencies of 10 or more. Obviously this cutoff slightly limits the generality of our results, but it also greatly decreases processing time for all of Figure 6: Morphologic relations of “conduct” compare induced conflation sets to those of</context>
<context position="32618" citStr="Schone and Jurafsky (2000)" startWordPosition="5161" endWordPosition="5164">h of the three languages. Furthermore, using ten-fold cross validation on the English data, we find that Fscore differences of the S column are each statistically significant at least at the 95% level. Table 5: Computation of F-Scores Algorithms English German Dutch S C A S C S C None 62.8 59.9 51.7 75.8 63.0 74.2 70.0 Goldsmith 81.8 84.0 75.8 S/J2000 85.2 88.3 82.2 +orthogrph 85.7 82.2 76.9 89.3 76.1 84.5 78.9 + syntax 87.5 84.0 79.0 91.6 78.2 85.6 79.4 + transitive 88.1 84.5 79.7 92.3 78.9 85.8 79.6 5 Conclusions We have illustrated three extensions to our earlier morphology induction work (Schone and Jurafsky (2000)). In addition to induced semantics, we incorporated induced orthographic, syntactic, and transitive information resulting in almost a 20% relative reduction in overall induction error. We have also extended the work by illustrating performance in German and Dutch where, to our knowledge, complete morphology induction performance measures have not previously been obtained. Lastly, we showed a mechanism whereby circumfixes as well as combinations of prefixing and suffixing can be induced in lieu of the suffixonly strategies prevailing in most previous research. For the future, we expect improve</context>
</contexts>
<marker>Schone, Jurafsky, 2000</marker>
<rawString>Schone, P. and D. Jurafsky. (2000) Knowledge-free induction of morphology using latent semantic analysis. Proc. of the Computational Natural Language Learning Conference, Lisbon, pp. 67-72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schütze</author>
</authors>
<title>Distributed syntactic representations with an application to part-of-speech tagging.</title>
<date>1993</date>
<booktitle>Proceedings of the IEEE International Conference on Neural Networks,</booktitle>
<pages>1504--1509</pages>
<contexts>
<context position="15905" citStr="Schütze (1993)" startWordPosition="2495" endWordPosition="2496"> U, D, and V such T that U and V are orthogonal matrices and D is a T diagonal matrix whose entries are the singular values of M. The LSA approach then zeros out all but the top k singular values of the SVD, which has the effect of projecting vectors into an optimal kdimensional subspace. This methodology is well-described in the literature (Landauer, et al., 1998; Manning and Schütze, 1999). In order to obtain semantic representations of each word, we apply our previous strategy (Schone and Jurafsky (2000)). Rather than using a termdocument matrix, we had followed an approach akin to that of Schütze (1993), who performed SVD on a Nx2N term-term matrix. The N here represents the N-1 most-frequent words as well as a glob position to account for all other words not in the top N-1. The matrix is structured such that for a given word w’s row, the first N columns denote words that precede w by up to 50 words, and the second N � columns represent those words that follow by up to NCS(µ,) =f NCS exp[ ((x-µ)/)2]dx 50 words. Since SVDs are more designed to work then, if there were nR items in the ruleset, the with normally-distributed data (Manning and probability that a NCS is non-random is Schütze, 1</context>
</contexts>
<marker>Schütze, 1993</marker>
<rawString>Schütze, H. (1993) Distributed syntactic representations with an application to part-of-speech tagging. Proceedings of the IEEE International Conference on Neural Networks, pp. 1504-1509.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sproat</author>
</authors>
<title>Morphology and Computation.</title>
<date>1992</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="7070" citStr="Sproat, 1992" startWordPosition="1046" endWordPosition="1047">s languages but evaluates in English and French. 2.3.3 Schone and Jurafsky: induced semantics In our earlier work, we (Schone and Jurafsky (2000)) generated a list of N candidate suffixes and used this list to identify word pairs which share the same stem but conclude with distinct candidate suffixes. We then applied Latent Semantic Analysis (Deerwester, et al., 1990) as a method of automatically determining semantic relatedness between word pairs. Using statistics from the conditions of circumfixing or infixing, nor are they applicable to other language types such as agglutinative languages (Sproat, 1992). Additionally, most approaches have centered around statistics of orthographic properties. We had noted previously (Schone and Jurafsky, 2000), however, that errors can arise from strictly orthographic systems. We had observed in other systems such errors as inappropriate removal of valid affixes (“ally”—“all”), failure to resolve morphological ambiguities (“hated”—“hat”), and pruning of semi-productive affixes (“dirty”-A“dirt”). Yet we illustrated that induced semantics can help overcome some of these errors. However, we have since observed that induced semantics can give rise to different k</context>
</contexts>
<marker>Sproat, 1992</marker>
<rawString>Sproat, R. (1992) Morphology and Computation. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Xu</author>
<author>B W Croft</author>
</authors>
<title>Corpus-based stemming using co-occurrence of word variants.</title>
<date>1998</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>16</volume>
<issue>1</issue>
<pages>61--81</pages>
<contexts>
<context position="2990" citStr="Xu and Croft (1998)" startWordPosition="420" endWordPosition="423">(although researchers such as Nakisa and Hahn (1996) have evaluated induction algorithms on morphological sub-problems in German). 2 Previous Approaches Previous morphology induction approaches have fallen into three categories. These categories differ depending on whether human input is provided and on whether the goal is to obtain affixes or complete morphological analysis. We here briefly describe work in each category. 2.1 Using a Knowledge Source to Bootstrap Some researchers begin with some initial humanlabeled source from which they induce other morphological components. In particular, Xu and Croft (1998) use word context derived from a corpus to refine Porter stemmer output. Gaussier (1999) induces derivational morphology using an inflectional lexicon which includes part of speech information. Grabar and Zweigenbaum (1999) use the SNOMED corpus of semantically-arranged medical terms to find semantically-motivated morphological relationships. Also, Yarowsky and Wicentowski (2000) obtained outstanding results at inducing English past tense after beginning with a list of the open class roots in the language, a table of a language’s inflectional parts of speech, and the canonical suffixes for eac</context>
</contexts>
<marker>Xu, Croft, 1998</marker>
<rawString>Xu, J., B.W. Croft. (1998) Corpus-based stemming using co-occurrence of word variants. ACM Transactions on Information Systems, 16 (1), pp. 61-81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
<author>R Wicentowski</author>
</authors>
<title>Minimally supervised morphological analysis by multimodal alignment.</title>
<date>2000</date>
<booktitle>Proc. of the ACL</booktitle>
<location>Hong Kong.</location>
<contexts>
<context position="3372" citStr="Yarowsky and Wicentowski (2000)" startWordPosition="469" endWordPosition="472">nalysis. We here briefly describe work in each category. 2.1 Using a Knowledge Source to Bootstrap Some researchers begin with some initial humanlabeled source from which they induce other morphological components. In particular, Xu and Croft (1998) use word context derived from a corpus to refine Porter stemmer output. Gaussier (1999) induces derivational morphology using an inflectional lexicon which includes part of speech information. Grabar and Zweigenbaum (1999) use the SNOMED corpus of semantically-arranged medical terms to find semantically-motivated morphological relationships. Also, Yarowsky and Wicentowski (2000) obtained outstanding results at inducing English past tense after beginning with a list of the open class roots in the language, a table of a language’s inflectional parts of speech, and the canonical suffixes for each part of speech. 2.2 Affix Inventories A second, knowledge-free category of research has focused on obtaining affix inventories. Brent, et al. (1995) used minimum description length (MDL) to find the most data-compressing suffixes. Kazakov (1997) does something akin to this using MDL as a fitness metric for evolutionary computing. DéJean (1998) uses a strategy similar to that of</context>
<context position="19933" citStr="Yarowsky and Wicentowski, 2000" startWordPosition="3164" endWordPosition="3167"> English, for example, the highest frequency rule is -s—e. CELEX suggests that 99.7% of our PPMVs for this rule would be true. However, since the purely semantic-based approach tends to select only relationships with contextually similar meanings, only 92% of the PPMVs are retained. This suggests that one might improve the analysis by supplementing semantic probabilities with orthographic-based probabilities (Prorth). Our approach to obtaining Prorth is motivated by an appeal to minimum edit distance (MED). MED has been applied to the morphology induction problem by other researchers (such as Yarowsky and Wicentowski, 2000). MED determines the minimum-weighted set of insertions, substitutions, and deletions required to transform one word into another. For example, only a single deletion is required to transform “rates” into “rate” whereas two substitutions and an insertion are required to transform it into “rating.” Effectively, if Cost(-) is transforming cost, Cost(rates—rate) = Cost(s—e) whereas Cost(rates—rating)=Cost(es—ing). More generally, suppose word X has circumfix C1=B1/E1 and pseudo-stem -S-, and word Y has circumfix C2 =B2/E2 also with pseudo-stem -S-. Then, Cost(X—Y)=Cost(B1SE1—B2SE2)=Cost(C 1 —C 2)</context>
</contexts>
<marker>Yarowsky, Wicentowski, 2000</marker>
<rawString>Yarowsky, D. and R. Wicentowski. (2000) Minimally supervised morphological analysis by multimodal alignment. Proc. of the ACL 2000, Hong Kong.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>