<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001063">
<title confidence="0.985998">
Learning Cross-lingual Word Embeddings via Matrix Co-factorization
</title>
<author confidence="0.989819">
Tianze Shi Zhiyuan Liu Yang Liu Maosong Sun
</author>
<affiliation confidence="0.9765155">
State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Science and Technology
Tsinghua University, Beijing 100084, China
</affiliation>
<email confidence="0.965221">
stz11@mails.tsinghua.edu.cn
{liuzy, liuyang2011, sms}@tsinghua.edu.cn
</email>
<sectionHeader confidence="0.99382" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999733166666667">
A joint-space model for cross-lingual
distributed representations generalizes
language-invariant semantic features.
In this paper, we present a matrix co-
factorization framework for learning
cross-lingual word embeddings. We
explicitly define monolingual training
objectives in the form of matrix de-
composition, and induce cross-lingual
constraints for simultaneously factorizing
monolingual matrices. The cross-lingual
constraints can be derived from parallel
corpora, with or without word alignments.
Empirical results on a task of cross-lingual
document classification show that our
method is effective to encode cross-lingual
knowledge as constraints for cross-lingual
word embeddings.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99971493442623">
Word embeddings allow one to represent words in
a continuous vector space, which characterizes the
lexico-semanic relations among words. In many
NLP tasks, they prove to be high-quality features,
successful applications of which include language
modelling (Bengio et al., 2003), sentiment analy-
sis (Socher et al., 2011) and word sense discrimi-
nation (Huang et al., 2012).
Like words having synonyms in the same lan-
guage, there are also word pairs across lan-
guages which share resembling semantic proper-
ties. Mikolov et al. (2013a) observed a strong
similarity of the geometric arrangements of cor-
responding concepts between the vector spaces of
different languages, and suggested that a cross-
lingual mapping between the two vector spaces is
technically plausible. In the meantime, the joint-
space models for cross-lingual word embeddings
are very desirable, as language-invariant seman-
tic features can be generalized to make it easy to
transfer models across languages. This is espe-
cially important for those low-resource languages,
where it allows one to develop accurate word rep-
resentations of one language by exploiting the
abundant textual resources in another language,
e.g., English, which has a high resource density.
The joint-space models are not only technically
plausible, but also useful for cross-lingual model
transfer. Further, studies have shown that using
cross-lingual correlation can improve the quality
of word representations trained solely with mono-
lingual corpora (Faruqui and Dyer, 2014).
Defining a cross-lingual learning objective is
crucial at the core of the joint-space model. Her-
mann and Blunsom (2014) and Chandar A P et
al. (2014) tried to calculate parallel sentence (or
document) representations and to minimize the
differences between the semantically equivalen-
t pairs. These methods are useful in capturing
semantic information carried by high-level units
(such as phrases and beyond) and usually do not
rely on word alignments. However, they suffer
from reduced accuracy for representing rare to-
kens, whose semantic information may not be well
generalized. In these cases, finer-grained informa-
tion at lexical level, such as aligned word pairs,
dictionaries, and word translation probabilities, is
considered to be helpful.
Koˇcisk`y et al. (2014) integrated word aligning
process and word embedding in machine transla-
tion models. This method makes full use of paral-
lel corpora and produces high-quality word align-
ments. However, it is unable to exploit the richer
monolingual corpora. On the other hand, Zou et al.
(2013) and Faruqui and Dyer (2014) learnt word
embeddings of different languages in separate s-
paces with monolingual corpora and projected the
embeddings into a joint space, but they can only
capture linear transformation.
In this paper, we address the above challenges
with a framework of matrix co-factorization. We
</bodyText>
<page confidence="0.868191">
567
</page>
<bodyText confidence="0.93041375">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 567–572,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
simultaneously learn word embeddings in multi-
ple languages via matrix factorization, with in-
duced constraints to assure cross-lingual seman-
tic relations. It provides the flexibility of con-
structing learning objectives from separate mono-
lingual and cross-lingual corpora. Intricate rela-
tions across languages, rather than simple linear
projections, are automatically captured. Addition-
ally, our method is efficient as it learns from global
statistics. The cross-lingual constraints can be de-
rived both with or without word alignments, given
that there is a valid measure of cross-lingual co-
occurrences or similarities.
We test the performance in a task of cross-
lingual document classification. Empirical result-
s and a visualization of the joint semantic space
demonstrate the validity of our model.
</bodyText>
<sectionHeader confidence="0.993685" genericHeader="introduction">
2 Framework
</sectionHeader>
<bodyText confidence="0.9987278125">
Without loss of generality, here we only consider
bilingual embedding learning of the two languages
l1 and l2. Given monolingual corpora Dli and
sentence-aligned parallel data Dbi, our task is to
find word embedding matrices of the size |V li|xd
where each line corresponds to the embedding of
a single word. We also define vocabularies of con-
texts Uli and we learn context embedding matrices
Cli of the size |Uli |x d at the same time. 1
These matrices are obtained by simultaneous
matrix factorization of the monolingual word-
context PMI (point-wise mutual information) ma-
trices Mli. During monolingual factorization, we
put a cross-lingual constraint (cost) on it, ensuring
cross-lingual semantic relations. We formalize the
global loss function as
</bodyText>
<equation confidence="0.998288">
ELtotal = ωi · Lmono(Wli, Cli)
i∈{1,2} (1)
+ωc · Lcross(Wl1, Cl1, Wl2, Cl2),
</equation>
<bodyText confidence="0.9999426">
where Lmono and Lcross are the monolingual and
cross-lingual objectives respectively. Wi and Wc
weigh the contribution of the different parts to the
total objective. An overview of our algorithm is
illustrated in Figure 1.
</bodyText>
<sectionHeader confidence="0.988634" genericHeader="method">
3 Monolingual Objective
</sectionHeader>
<bodyText confidence="0.997788">
Our monolingual objective follows the GloVe
model (Pennington et al., 2014), which learns
from global word co-occurrence statistics. For a
word-context pair (j, k) in language li, we try to
</bodyText>
<footnote confidence="0.994809">
1In this paper, we let Uli = V li.
</footnote>
<figureCaption confidence="0.992396">
Figure 1: The framework of cross-lingual word embedding
via matrix co-factorization.
</figureCaption>
<bodyText confidence="0.991911">
minimize the difference between the dot produc-
t of the embeddings wlij · clik and their PMI value
</bodyText>
<equation confidence="0.973852666666667">
Mli Mli = X&apos;&apos;`·Ej k X&apos;&apos;` where Xli is the
jk jk li · li ,
jkkjk
</equation>
<bodyText confidence="0.999375125">
matrix of word-context co-occurrence counts. As
Pennington et al. (2014), we add separate terms
bli wj, bli ck for each word and context to absorb the
effect of any possible word-specific biases. We al-
so add an additional matrix bias bli for the ease
of sharing embeddings among matrices. The loss
function is written as the sum of the weighted
square error,
</bodyText>
<equation confidence="0.7806175">
f(Xjk) (wji · clik + bliwj + bli + bli − M�i)2,
(2)
</equation>
<bodyText confidence="0.999904666666667">
where we choose the same weighting function as
the GloVe model to place less confidence on those
word-context pairs with rare occurrences,
</bodyText>
<equation confidence="0.52358725">
W _ J (x/xmax)α if x &lt; xmax. 3
f O 1 otherwise ( )
Notice that we only have to optimize those Xli
jk =�
</equation>
<bodyText confidence="0.9996445">
0, which can be solved efficiently since the matrix
of co-occurrence counts is usually sparse.
</bodyText>
<sectionHeader confidence="0.997986" genericHeader="method">
4 Cross-lingual Objectives
</sectionHeader>
<bodyText confidence="0.999917666666667">
As the most important part in our model, the cross-
lingual objective describes the cross-lingual word
relations and sets constraints when we factorize
monolingual co-occurrence matrices. It can be de-
rived from either cross-lingual co-occurrences or
similarities between cross-lingual word pairs.
</bodyText>
<subsectionHeader confidence="0.921422">
4.1 Cross-lingual Contexts
</subsectionHeader>
<bodyText confidence="0.999477">
The monolingual objective stems from the distri-
butional hypothesis (Harris, 1954) and optimizes
</bodyText>
<figure confidence="0.99754935">
Monolingual PMI
corpora matrices
L1
L2
L1-L2
𝑴𝒍𝟐
𝑴𝒍𝟏
Bilingual corpus
≈
≈
bilingual relations
and constraints
𝑾𝒍𝟏
𝑾𝒍𝟐
⋅
⋅
𝑪𝒍𝟏
𝑪𝒍𝟐
ELli mono =
j,k
</figure>
<page confidence="0.987427">
568
</page>
<bodyText confidence="0.978517666666667">
words in similar contexts into similar embeddings.
It is natural to further extend this idea to define
cross-lingual contexts, for which we have multi-
ple choices.
For the definition of cross-lingual contexts, we
have multiple choices. A straightforward option
is to count all the word co-occurrences in aligned
sentence pairs, which is equivalent to a uniform
word alignment model adopted by Gouws et al.
(2015). For the sentence-aligned bilingual corpus
Dbi = {(Sl1, Sl2)}, where each Slz is a monolin-
gual sentence, we count the co-occurrences as
</bodyText>
<equation confidence="0.882368">
#(j, Sl1) x #(k, Sl2), (4)
</equation>
<bodyText confidence="0.9997956">
where Xbi is the matrix of cross-lingual co-
occurrence counts, and #(j, S) is a function
counting the number of j’s in the sequence S. We
then use a similar loss function as Equation 2, with
the exception that we optimize for the dot product-
</bodyText>
<subsectionHeader confidence="0.477687">
s of wl1
</subsectionHeader>
<bodyText confidence="0.9999198">
j wl2k . This method works without word
alignments and we denote it as CLC-WA (Cross-
lingual context without word alignments).
We can also leverage word alignments and de-
fine CLC+WA (Cross-lingual context with word
alignments). The idea is to count those word-
s co-occurring with k as the context of j, where
k E V l2 is the translationally equivalent word
of j E V l1. An example is shown in Figure 2.
CLC+WA is expected to contain more precise in-
formation than CLC-WA, and we will compare the
two definitions in the following experiments.
Once we have counted the co-occurrences, a
naive solution is to concatenate the bilingual vo-
cabularies and perform matrix factorization as a
whole. To induce additional flexibility, such as
separate weighting, we divide the matrix into three
parts. It is also more reasonable to calculate PMI
values without mixing the monolingual and bilin-
gual corpora.
</bodyText>
<subsectionHeader confidence="0.96471">
4.2 Cross-lingual Similarities
</subsectionHeader>
<bodyText confidence="0.974961272727273">
An alternative way to set cross-lingual constraints
is to minimize the distances between similar word
pairs. Here the semantic similarities can be mea-
sured by equivalence in translation, sim(j, k),
which is produced by a machine translation sys-
tem. In this paper, we use the translation proba-
bilities produced by a machine translation system.
Minimizing the distances of related words in the
two languages weighted by their similarities gives
us the cross-lingual objective
... we must do all we can, not just to ...
</bodyText>
<figure confidence="0.594138">
... wir alles daran setzen müssen, nicht nur ...
</figure>
<figureCaption confidence="0.991133333333333">
Figure 2: An example of CLC+WA, where we show the
cross-lingual context of the German word “m¨ussen” in the
dashed box.
</figureCaption>
<tableCaption confidence="0.95327">
Table 1: Accuracy for cross-lingual classification.
</tableCaption>
<table confidence="0.9993108">
Model en—*de de—*en
Machine translation 68.1 67.4
Majority class 46.8 46.8
Klementiev et al. 77.6 71.1
BiCVM 83.7 71.4
BAE 91.8 74.2
BilBOWA 86.5 75.0
CLC-WA 91.3 77.2
CLC+WA 90.0 75.0
CLSim 92.7 80.2
</table>
<equation confidence="0.666499">
sim(j, k) · distance(wl1
j , wl2k ), (5)
</equation>
<bodyText confidence="0.930026363636363">
where wl1
j and wl2k are the embeddings of j and k
in l1 and l2 respectively. In this paper, we choose
the distance function to be the Euclidean distance,
distance(wl1
j , wl2k ) = ||wl1
j − wl2k ||2. Notice that
similar to the monolingual objective, we may op-
timize for only those sim(j, k) =� 0, which is ef-
ficient as the matrix of translation probabilities or
dictionary is sparse. We call this method CLSim.
</bodyText>
<sectionHeader confidence="0.998109" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999405142857143">
To evaluate the quality of the relatedness between
words in different languages, we induce the task
of cross-lingual document classification for the
English-German language pair, where a classifier
is trained in one language and later used to classi-
fy documents in another. We exactly replicated the
experiment settings of Klementiev et al. (2012).
</bodyText>
<subsectionHeader confidence="0.957347">
5.1 Data and Training
</subsectionHeader>
<bodyText confidence="0.999930555555556">
For optimizing the monolingual objectives, We
used exactly the same subset of RCV1/RCV2 cor-
pora (Lewis et al., 2004) as by Klementiev et al.
(2012), which were sampled to balance the num-
ber of tokens between languages. Our preprocess-
ing strategy followed Chandar A P et al. (2014),
where we lowercased all words, removed punctu-
ations and used the same vocabularies (|Ven |=
43, 614 and |V de |= 50, 110). When counting
</bodyText>
<equation confidence="0.918977">
�
Xbi
jk =
(Sl1,Sl2)∈Dbi
�Lcross =
j∈V l1,k∈V l2
</equation>
<page confidence="0.935101">
569
</page>
<figure confidence="0.999869923076923">
Accuray
0.95
0.85
0.75
0.65
0.55
0.9
0.8
0.7
0.6
0.5
en→de
de→en
Accuray
0.9
0.8
0.7
0.6
0.5
0.4
0.3
en→de
de→en
Accuray
0.95
0.85
0.75
0.65
0.55
0.9
0.8
0.7
0.6
0.5
en→de
de→en
1 10 100 1 10 100 1 10 100
Weight of cross-lingual objective Percentage of RCV used for training (%) Percentage of Europarl used for training (%)
(a) (b) (c)
</figure>
<figureCaption confidence="0.9995145">
Figure 3: Cross-lingual document classification accuracy, with (a) varying weighting of cross-lingual objective (b) varying size
of training monolingual corpora, and (c) varying size of training bilingual corpus.
</figureCaption>
<bodyText confidence="0.999901">
word co-occurrences, we use a decreasing weight-
ing function as Pennington et al. (2014), where d-
word-apart word pairs contribute 1/d to the total
count. We used a symmetric window size of 10
words for all our experiments.
The cross-lingual constraints were derived us-
ing the English and German sections of the Eu-
roparl v7 parallel corpus (Koehn, 2005), which
were similarly preprocessed. For CLC+WA and
CLSim, we obtained word alignments and trans-
lation probabilities with SyMGIZA++ (Junczys-
Dowmunt and Szał, 2012). We did not use Eu-
roparl for monolingual training.
The documents for classification were ran-
domly selected by Klementiev et al. (2012)
from those in RCV1/RCV2 that are assigned
to only one single topic among the four:
CCAT (Corporate/Industrial), ECAT (Economics),
GCAT (Government/Social), and MCAT (Market-
s). 1,000/5,000 documents in each language were
used as a train/test set and we kept another 1,000
documents as a development set for hyperparame-
ter tuning. Each document was represented as an
idf-weighted average embedding of all its tokens,
and a multi-class document classifier was trained
for 10 epochs with an averaged perceptron algo-
rithm as by Klementiev et al. (2012). A classifier
trained with English documents is used to classify
German documents and vice versa.
We trained our models using stochastic gradient
descent. We run 50 iterations for all of our exper-
iments and the dimensionality of the embeddings
is 40. We set xmax to be 100 for cross-lingual co-
occurrences and 30 for monolingual ones, while
α is fixed to 3/4. Other parameters are chosen
according to the performance on the development
set.
</bodyText>
<subsectionHeader confidence="0.782888">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999959861111111">
We present the empirical results on the task of
cross-lingual document classification in Table 1,
where the performance of our models is compared
with some baselines and previous work. The effec-
t of weighting between parts of the total objective
and the amount of training data on the quality of
the embeddings is demonstrated in Figure 3.
The baseline systems are Majority class where
test documents are simply classified as the class
with the most training samples, and Machine
translation where a phrased-based machine trans-
lation system is used to translate test documents
into the same language as the training documents.
We also summarize the classification accuracy
reported in some previous work, including Multi-
task learning (Klementiev et al., 2012), Bilingual
compositional vector model (BiCVM) (Herman-
n and Blunsom, 2014), Bilingual autoencoder for
bags-of-words (BAE) (Chandar A P et al., 2014),
and BilBOWA (Gouws et al., 2015). A more re-
cent work of Soyer et al. (2015) developed a com-
positional approach and reported an accuracy of
90.8% (en→de) and 80.1% (de→en) when using
full RCV and Europarl corpora.
Our method outperforms the previous work and
we observe improvements when we exploit word
translation probabilities (CLSim) over the mod-
el without word-level information (CLC-WA).
The best result is achieved with CLSim. It
is interesting to notice that CLC+WA, which
makes use of word alignments in defining cross-
lingual contexts, does not provide better perfor-
mance than CLC-WA. We guess that sentence-
level co-occurrence is more suitable for captur-
ing sentence-level semantic relations in the task of
document classification.
</bodyText>
<page confidence="0.995565">
570
</page>
<figureCaption confidence="0.99974">
Figure 4: A visualization of the joint vector space.
</figureCaption>
<subsectionHeader confidence="0.993924">
5.3 Visualization
</subsectionHeader>
<bodyText confidence="0.992100142857143">
Figure 4 gives a visualization of some selected
words using t-SNE (Van der Maaten and Hin-
ton, 2008) where we observe the topical nature of
word embeddings. Regardless of their source lan-
guages, words sharing a common topic, e.g. econ-
omy, are closely aligned with each other, revealing
the semantic validity of the joint vector space.
</bodyText>
<sectionHeader confidence="0.999881" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999951181818182">
Matrix factorization has been successfully applied
to learn word representations, which use several
low-rank matrices to approximate the original ma-
trix with extracted statistical information, usually
word co-occurrence counts or PMI. Singular value
decomposition (SVD) (Eckart and Young, 1936),
SVD-based latent semantic analysis (LSA) (Lan-
dauer et al., 1998), latent semantic indexing (LSI)
(Deerwester et al., 1990), and the more recently-
proposed global vectors for word representation
(GloVe) (Pennington et al., 2014) find their wide
applications in the area of NLP and information
retrieval (Berry et al., 1995). Additionally, there is
evidence that some neural-network-based models,
such as Skip-gram (Mikolov et al., 2013b) which
exhibits state-of-the-art performance, are also im-
plicitly factorizing a PMI-based matrix (Levy and
Goldberg, 2014). The strategy for matrix factor-
ization in this paper, as Pennington et al. (2014),
is in a stochastic fashion, which better handles un-
observed data and allows one to weigh samples ac-
cording to their importance and confidence.
Joint matrix factorization allows one to decom-
pose matrices with some correlational constraints.
Collective matrix factorization has been develope-
d to handle pairwise relations (Singh and Gordon,
2008). Chang et al. (2013) generalized LSA to
Multi-Relational LSA, which constructs a 3-way
tensor to combine the multiple relations between
words. While matrix factorization is widely used
in recommender systems, matrix co-factorization
helps to handle multiple aspects of the data and
improves in predicting individual decisions (Hong
et al., 2013). Multiple sources of information,
such as content and linkage, can also be connected
with matrix co-factorization to derive high-quality
webpage representations (Zhu et al., 2007). The
advantage of this approach is that it automatical-
ly finds optimal parameters to optimize both sin-
gle matrix factorization and relational alignments,
which avoids manually defining a projection ma-
trix or transfer function. To the best of our knowl-
edge, we are the first to introduce this technique to
learn cross-lingual word embeddings.
</bodyText>
<sectionHeader confidence="0.999574" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999984695652174">
In this paper, we introduced a framework of matrix
co-factorization to learn cross-lingual word em-
beddings. It is capable of capturing the lexico-
semantic similarities of different languages in a
unified vector space, where the embeddings are
jointly learnt instead of projected from separate
vector spaces. The overall objective is divided into
monolingual parts and a cross-lingual one, which
enables one to use different weighting and learn-
ing strategies, and to develop models either with
or without word alignments. Exploiting global
context and similarity information instead of local
ones, our proposed models are computationally ef-
ficient and effective.
With matrix co-factorization, it allows one to
integrate external information, such as syntactic
contexts and morphology, which is not discussed
in this paper. Its application in statistical ma-
chine translation and cross-lingual model transfer
remains to be explored. Learning multiple em-
beddings per word and compositional embeddings
with matrix factorization are also interesting fu-
ture directions.
</bodyText>
<sectionHeader confidence="0.997488" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999505333333333">
This research is supported by the 973 Program
(No. 2014CB340501) and the National Natu-
ral Science Foundation of China (NSFC No.
61133012, 61170196 &amp; 61202140). We thank the
anonymous reviewers for the valuable comments.
We also thank Ivan Titov and Alexandre Klemen-
tiev for kindly offering their evaluation package,
which allowed us to replicate their experiment set-
tings exactly.
</bodyText>
<figure confidence="0.99910375">
consultant
nationalpark
finance
bank
cathedral
towers
arzt doktor
physician
theorie
methoden
wisdom
weisheit
learning
development company
business
industrie
branche
wirtschaft
English
German
</figure>
<page confidence="0.988672">
571
</page>
<sectionHeader confidence="0.989104" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99992336">
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. JMLR, 3:1137–1155.
Michael W Berry, Susan T Dumais, and Gavin W
O’Brien. 1995. Using linear algebra for intelligent
information retrieval. SIAM review, 37(4):573–595.
Sarath Chandar A P, Stanislas Lauly, Hugo Larochelle,
Mitesh Khapra, Balaraman Ravindran, Vikas C
Raykar, and Amrita Saha. 2014. An autoencoder
approach to learning bilingual word representations.
In Proceedings of NIPS, pages 1853–1861.
Kai-Wei Chang, Wen-tau Yih, and Christopher Meek.
2013. Multi-relational latent semantic analysis. In
Proceedings of EMNLP, pages 1602–1612.
Scott C. Deerwester, Susan T Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. JAsIs,
41(6):391–407.
Carl Eckart and Gale Young. 1936. The approximation
of one matrix by another of lower rank. Psychome-
trika, 1(3):211–218.
Manaal Faruqui and Chris Dyer. 2014. Improving
vector space word representations using multilingual
correlation. In Proceedings of EACL, pages 462–
471.
Stephan Gouws, Yoshua Bengio, and Greg Corrado.
2015. Bilbowa: Fast bilingual distributed represen-
tations without word alignments. In ICML, pages
748–756.
Zellig S Harris. 1954. Distributional structure. Word,
10(23):146–162.
Karl Moritz Hermann and Phil Blunsom. 2014. Multi-
lingual models for compositional distributed seman-
tics. In Proceedings of ACL, pages 58–68. ACL.
Liangjie Hong, Aziz S Doumith, and Brian D Davison.
2013. Co-factorization machines: modeling user in-
terests and predicting individual decisions in twitter.
In Proceedings of WSDM, pages 557–566. ACM.
Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of ACL, pages 873–882.
ACL.
Marcin Junczys-Dowmunt and Arkadiusz Szał. 2012.
Symgiza++: symmetrized word alignment model-
s for statistical machine translation. In Security
and Intelligent Information Systems, pages 379–390.
Springer.
Alexandre Klementiev, Ivan Titov, and Binod Bhat-
tarai. 2012. Inducing crosslingual distributed rep-
resentations of words. In Proceedings of COLING.
ICCL.
Tom´aˇs Koˇcisk`y, Karl Moritz Hermann, and Phil Blun-
som. 2014. Learning bilingual word representations
by marginalizing alignments. In Proceedings of A-
CL, pages 224–229. ACL.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT summit, vol-
ume 5, pages 79–86.
Thomas K Landauer, Peter W Foltz, and Darrell La-
ham. 1998. An introduction to latent semantic anal-
ysis. Discourse processes, 25(2-3):259–284.
Omer Levy and Yoav Goldberg. 2014. Neural word
embedding as implicit matrix factorization. In Pro-
ceedings of NIPS, pages 2177–2185.
David D Lewis, Yiming Yang, Tony G Rose, and Fan
Li. 2004. Rcv1: A new benchmark collection for
text categorization research. JMLR, 5:361–397.
Tomas Mikolov, Quoc V Le, and Ilya Sutskever.
2013a. Exploiting similarities among languages for
machine translation. arXiv:1309.4168.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proceedings of NIPS, pages 3111–3119.
Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for word
representation. pages 1532–1543.
Ajit P Singh and Geoffrey J Gordon. 2008. Relational
learning via collective matrix factorization. In Pro-
ceedings of SIGKDD, pages 650–658. ACM.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of
EMNLP, pages 151–161. ACL.
Hubert Soyer, Pontus Stenetorp, and Akiko Aizawa.
2015. Leveraging monolingual data for crosslingual
compositional word representations. In Proceedings
of ICLR.
Laurens Van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. JMLR, 9:2579–2605.
Shenghuo Zhu, Kai Yu, Yun Chi, and Yihong Gong.
2007. Combining content and link for classification
using matrix factorization. In Proceedings of SIGIR,
pages 487–494. ACM.
Will Y Zou, Richard Socher, Daniel M Cer, and
Christopher D Manning. 2013. Bilingual word em-
beddings for phrase-based machine translation. In
Proceedings of EMNLP, pages 1393–1398.
</reference>
<page confidence="0.99741">
572
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.602276">
<title confidence="0.999969">Learning Cross-lingual Word Embeddings via Matrix Co-factorization</title>
<author confidence="0.975218">Tianze Shi Zhiyuan Liu Yang Liu Maosong Sun</author>
<affiliation confidence="0.9425385">State Key Laboratory of Intelligent Technology and Tsinghua National Laboratory for Information Science and Department of Computer Science and Tsinghua University, Beijing 100084,</affiliation>
<email confidence="0.752419">liuyang2011,</email>
<abstract confidence="0.998872736842105">A joint-space model for cross-lingual distributed representations generalizes language-invariant semantic features. In this paper, we present a matrix cofactorization framework for learning cross-lingual word embeddings. explicitly define monolingual training objectives in the form of matrix decomposition, and induce cross-lingual constraints for simultaneously factorizing monolingual matrices. The cross-lingual constraints can be derived from parallel corpora, with or without word alignments. Empirical results on a task of cross-lingual document classification show that our method is effective to encode cross-lingual knowledge as constraints for cross-lingual word embeddings.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>JMLR,</journal>
<pages>3--1137</pages>
<contexts>
<context position="1387" citStr="Bengio et al., 2003" startWordPosition="172" endWordPosition="175">ously factorizing monolingual matrices. The cross-lingual constraints can be derived from parallel corpora, with or without word alignments. Empirical results on a task of cross-lingual document classification show that our method is effective to encode cross-lingual knowledge as constraints for cross-lingual word embeddings. 1 Introduction Word embeddings allow one to represent words in a continuous vector space, which characterizes the lexico-semanic relations among words. In many NLP tasks, they prove to be high-quality features, successful applications of which include language modelling (Bengio et al., 2003), sentiment analysis (Socher et al., 2011) and word sense discrimination (Huang et al., 2012). Like words having synonyms in the same language, there are also word pairs across languages which share resembling semantic properties. Mikolov et al. (2013a) observed a strong similarity of the geometric arrangements of corresponding concepts between the vector spaces of different languages, and suggested that a crosslingual mapping between the two vector spaces is technically plausible. In the meantime, the jointspace models for cross-lingual word embeddings are very desirable, as language-invarian</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. JMLR, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael W Berry</author>
<author>Susan T Dumais</author>
<author>Gavin W O’Brien</author>
</authors>
<title>Using linear algebra for intelligent information retrieval.</title>
<date>1995</date>
<journal>SIAM review,</journal>
<volume>37</volume>
<issue>4</issue>
<marker>Berry, Dumais, O’Brien, 1995</marker>
<rawString>Michael W Berry, Susan T Dumais, and Gavin W O’Brien. 1995. Using linear algebra for intelligent information retrieval. SIAM review, 37(4):573–595.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarath Chandar A P</author>
<author>Stanislas Lauly</author>
<author>Hugo Larochelle</author>
<author>Mitesh Khapra</author>
<author>Balaraman Ravindran</author>
<author>Vikas C Raykar</author>
<author>Amrita Saha</author>
</authors>
<title>An autoencoder approach to learning bilingual word representations.</title>
<date>2014</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>1853--1861</pages>
<contexts>
<context position="2772" citStr="P et al. (2014)" startWordPosition="385" endWordPosition="388"> one to develop accurate word representations of one language by exploiting the abundant textual resources in another language, e.g., English, which has a high resource density. The joint-space models are not only technically plausible, but also useful for cross-lingual model transfer. Further, studies have shown that using cross-lingual correlation can improve the quality of word representations trained solely with monolingual corpora (Faruqui and Dyer, 2014). Defining a cross-lingual learning objective is crucial at the core of the joint-space model. Hermann and Blunsom (2014) and Chandar A P et al. (2014) tried to calculate parallel sentence (or document) representations and to minimize the differences between the semantically equivalent pairs. These methods are useful in capturing semantic information carried by high-level units (such as phrases and beyond) and usually do not rely on word alignments. However, they suffer from reduced accuracy for representing rare tokens, whose semantic information may not be well generalized. In these cases, finer-grained information at lexical level, such as aligned word pairs, dictionaries, and word translation probabilities, is considered to be helpful. K</context>
<context position="11767" citStr="P et al. (2014)" startWordPosition="1850" endWordPosition="1853">edness between words in different languages, we induce the task of cross-lingual document classification for the English-German language pair, where a classifier is trained in one language and later used to classify documents in another. We exactly replicated the experiment settings of Klementiev et al. (2012). 5.1 Data and Training For optimizing the monolingual objectives, We used exactly the same subset of RCV1/RCV2 corpora (Lewis et al., 2004) as by Klementiev et al. (2012), which were sampled to balance the number of tokens between languages. Our preprocessing strategy followed Chandar A P et al. (2014), where we lowercased all words, removed punctuations and used the same vocabularies (|Ven |= 43, 614 and |V de |= 50, 110). When counting � Xbi jk = (Sl1,Sl2)∈Dbi �Lcross = j∈V l1,k∈V l2 569 Accuray 0.95 0.85 0.75 0.65 0.55 0.9 0.8 0.7 0.6 0.5 en→de de→en Accuray 0.9 0.8 0.7 0.6 0.5 0.4 0.3 en→de de→en Accuray 0.95 0.85 0.75 0.65 0.55 0.9 0.8 0.7 0.6 0.5 en→de de→en 1 10 100 1 10 100 1 10 100 Weight of cross-lingual objective Percentage of RCV used for training (%) Percentage of Europarl used for training (%) (a) (b) (c) Figure 3: Cross-lingual document classification accuracy, with (a) varyi</context>
<context position="15065" citStr="P et al., 2014" startWordPosition="2386" endWordPosition="2389"> of the embeddings is demonstrated in Figure 3. The baseline systems are Majority class where test documents are simply classified as the class with the most training samples, and Machine translation where a phrased-based machine translation system is used to translate test documents into the same language as the training documents. We also summarize the classification accuracy reported in some previous work, including Multitask learning (Klementiev et al., 2012), Bilingual compositional vector model (BiCVM) (Hermann and Blunsom, 2014), Bilingual autoencoder for bags-of-words (BAE) (Chandar A P et al., 2014), and BilBOWA (Gouws et al., 2015). A more recent work of Soyer et al. (2015) developed a compositional approach and reported an accuracy of 90.8% (en→de) and 80.1% (de→en) when using full RCV and Europarl corpora. Our method outperforms the previous work and we observe improvements when we exploit word translation probabilities (CLSim) over the model without word-level information (CLC-WA). The best result is achieved with CLSim. It is interesting to notice that CLC+WA, which makes use of word alignments in defining crosslingual contexts, does not provide better performance than CLC-WA. We gu</context>
</contexts>
<marker>P, Lauly, Larochelle, Khapra, Ravindran, Raykar, Saha, 2014</marker>
<rawString>Sarath Chandar A P, Stanislas Lauly, Hugo Larochelle, Mitesh Khapra, Balaraman Ravindran, Vikas C Raykar, and Amrita Saha. 2014. An autoencoder approach to learning bilingual word representations. In Proceedings of NIPS, pages 1853–1861.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai-Wei Chang</author>
<author>Wen-tau Yih</author>
<author>Christopher Meek</author>
</authors>
<title>Multi-relational latent semantic analysis.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1602--1612</pages>
<contexts>
<context position="17533" citStr="Chang et al. (2013)" startWordPosition="2763" endWordPosition="2766">odels, such as Skip-gram (Mikolov et al., 2013b) which exhibits state-of-the-art performance, are also implicitly factorizing a PMI-based matrix (Levy and Goldberg, 2014). The strategy for matrix factorization in this paper, as Pennington et al. (2014), is in a stochastic fashion, which better handles unobserved data and allows one to weigh samples according to their importance and confidence. Joint matrix factorization allows one to decompose matrices with some correlational constraints. Collective matrix factorization has been developed to handle pairwise relations (Singh and Gordon, 2008). Chang et al. (2013) generalized LSA to Multi-Relational LSA, which constructs a 3-way tensor to combine the multiple relations between words. While matrix factorization is widely used in recommender systems, matrix co-factorization helps to handle multiple aspects of the data and improves in predicting individual decisions (Hong et al., 2013). Multiple sources of information, such as content and linkage, can also be connected with matrix co-factorization to derive high-quality webpage representations (Zhu et al., 2007). The advantage of this approach is that it automatically finds optimal parameters to optimize </context>
</contexts>
<marker>Chang, Yih, Meek, 2013</marker>
<rawString>Kai-Wei Chang, Wen-tau Yih, and Christopher Meek. 2013. Multi-relational latent semantic analysis. In Proceedings of EMNLP, pages 1602–1612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott C Deerwester</author>
<author>Susan T Dumais</author>
<author>Thomas K Landauer</author>
<author>George W Furnas</author>
<author>Richard A Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>JAsIs,</journal>
<pages>41--6</pages>
<contexts>
<context position="16649" citStr="Deerwester et al., 1990" startWordPosition="2631" endWordPosition="2634"> embeddings. Regardless of their source languages, words sharing a common topic, e.g. economy, are closely aligned with each other, revealing the semantic validity of the joint vector space. 6 Related Work Matrix factorization has been successfully applied to learn word representations, which use several low-rank matrices to approximate the original matrix with extracted statistical information, usually word co-occurrence counts or PMI. Singular value decomposition (SVD) (Eckart and Young, 1936), SVD-based latent semantic analysis (LSA) (Landauer et al., 1998), latent semantic indexing (LSI) (Deerwester et al., 1990), and the more recentlyproposed global vectors for word representation (GloVe) (Pennington et al., 2014) find their wide applications in the area of NLP and information retrieval (Berry et al., 1995). Additionally, there is evidence that some neural-network-based models, such as Skip-gram (Mikolov et al., 2013b) which exhibits state-of-the-art performance, are also implicitly factorizing a PMI-based matrix (Levy and Goldberg, 2014). The strategy for matrix factorization in this paper, as Pennington et al. (2014), is in a stochastic fashion, which better handles unobserved data and allows one t</context>
</contexts>
<marker>Deerwester, Dumais, Landauer, Furnas, Harshman, 1990</marker>
<rawString>Scott C. Deerwester, Susan T Dumais, Thomas K. Landauer, George W. Furnas, and Richard A. Harshman. 1990. Indexing by latent semantic analysis. JAsIs, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Eckart</author>
<author>Gale Young</author>
</authors>
<title>The approximation of one matrix by another of lower rank.</title>
<date>1936</date>
<journal>Psychometrika,</journal>
<volume>1</volume>
<issue>3</issue>
<contexts>
<context position="16525" citStr="Eckart and Young, 1936" startWordPosition="2613" endWordPosition="2616">ualization of some selected words using t-SNE (Van der Maaten and Hinton, 2008) where we observe the topical nature of word embeddings. Regardless of their source languages, words sharing a common topic, e.g. economy, are closely aligned with each other, revealing the semantic validity of the joint vector space. 6 Related Work Matrix factorization has been successfully applied to learn word representations, which use several low-rank matrices to approximate the original matrix with extracted statistical information, usually word co-occurrence counts or PMI. Singular value decomposition (SVD) (Eckart and Young, 1936), SVD-based latent semantic analysis (LSA) (Landauer et al., 1998), latent semantic indexing (LSI) (Deerwester et al., 1990), and the more recentlyproposed global vectors for word representation (GloVe) (Pennington et al., 2014) find their wide applications in the area of NLP and information retrieval (Berry et al., 1995). Additionally, there is evidence that some neural-network-based models, such as Skip-gram (Mikolov et al., 2013b) which exhibits state-of-the-art performance, are also implicitly factorizing a PMI-based matrix (Levy and Goldberg, 2014). The strategy for matrix factorization i</context>
</contexts>
<marker>Eckart, Young, 1936</marker>
<rawString>Carl Eckart and Gale Young. 1936. The approximation of one matrix by another of lower rank. Psychometrika, 1(3):211–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manaal Faruqui</author>
<author>Chris Dyer</author>
</authors>
<title>Improving vector space word representations using multilingual correlation.</title>
<date>2014</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>462--471</pages>
<contexts>
<context position="2621" citStr="Faruqui and Dyer, 2014" startWordPosition="359" endWordPosition="362">features can be generalized to make it easy to transfer models across languages. This is especially important for those low-resource languages, where it allows one to develop accurate word representations of one language by exploiting the abundant textual resources in another language, e.g., English, which has a high resource density. The joint-space models are not only technically plausible, but also useful for cross-lingual model transfer. Further, studies have shown that using cross-lingual correlation can improve the quality of word representations trained solely with monolingual corpora (Faruqui and Dyer, 2014). Defining a cross-lingual learning objective is crucial at the core of the joint-space model. Hermann and Blunsom (2014) and Chandar A P et al. (2014) tried to calculate parallel sentence (or document) representations and to minimize the differences between the semantically equivalent pairs. These methods are useful in capturing semantic information carried by high-level units (such as phrases and beyond) and usually do not rely on word alignments. However, they suffer from reduced accuracy for representing rare tokens, whose semantic information may not be well generalized. In these cases, f</context>
</contexts>
<marker>Faruqui, Dyer, 2014</marker>
<rawString>Manaal Faruqui and Chris Dyer. 2014. Improving vector space word representations using multilingual correlation. In Proceedings of EACL, pages 462– 471.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Gouws</author>
<author>Yoshua Bengio</author>
<author>Greg Corrado</author>
</authors>
<title>Bilbowa: Fast bilingual distributed representations without word alignments.</title>
<date>2015</date>
<booktitle>In ICML,</booktitle>
<pages>748--756</pages>
<contexts>
<context position="8373" citStr="Gouws et al. (2015)" startWordPosition="1272" endWordPosition="1275">he distributional hypothesis (Harris, 1954) and optimizes Monolingual PMI corpora matrices L1 L2 L1-L2 𝑴𝒍𝟐 𝑴𝒍𝟏 Bilingual corpus ≈ ≈ bilingual relations and constraints 𝑾𝒍𝟏 𝑾𝒍𝟐 ⋅ ⋅ 𝑪𝒍𝟏 𝑪𝒍𝟐 ELli mono = j,k 568 words in similar contexts into similar embeddings. It is natural to further extend this idea to define cross-lingual contexts, for which we have multiple choices. For the definition of cross-lingual contexts, we have multiple choices. A straightforward option is to count all the word co-occurrences in aligned sentence pairs, which is equivalent to a uniform word alignment model adopted by Gouws et al. (2015). For the sentence-aligned bilingual corpus Dbi = {(Sl1, Sl2)}, where each Slz is a monolingual sentence, we count the co-occurrences as #(j, Sl1) x #(k, Sl2), (4) where Xbi is the matrix of cross-lingual cooccurrence counts, and #(j, S) is a function counting the number of j’s in the sequence S. We then use a similar loss function as Equation 2, with the exception that we optimize for the dot products of wl1 j wl2k . This method works without word alignments and we denote it as CLC-WA (Crosslingual context without word alignments). We can also leverage word alignments and define CLC+WA (Cross</context>
<context position="15099" citStr="Gouws et al., 2015" startWordPosition="2392" endWordPosition="2395">ated in Figure 3. The baseline systems are Majority class where test documents are simply classified as the class with the most training samples, and Machine translation where a phrased-based machine translation system is used to translate test documents into the same language as the training documents. We also summarize the classification accuracy reported in some previous work, including Multitask learning (Klementiev et al., 2012), Bilingual compositional vector model (BiCVM) (Hermann and Blunsom, 2014), Bilingual autoencoder for bags-of-words (BAE) (Chandar A P et al., 2014), and BilBOWA (Gouws et al., 2015). A more recent work of Soyer et al. (2015) developed a compositional approach and reported an accuracy of 90.8% (en→de) and 80.1% (de→en) when using full RCV and Europarl corpora. Our method outperforms the previous work and we observe improvements when we exploit word translation probabilities (CLSim) over the model without word-level information (CLC-WA). The best result is achieved with CLSim. It is interesting to notice that CLC+WA, which makes use of word alignments in defining crosslingual contexts, does not provide better performance than CLC-WA. We guess that sentencelevel co-occurren</context>
</contexts>
<marker>Gouws, Bengio, Corrado, 2015</marker>
<rawString>Stephan Gouws, Yoshua Bengio, and Greg Corrado. 2015. Bilbowa: Fast bilingual distributed representations without word alignments. In ICML, pages 748–756.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig S Harris</author>
</authors>
<date>1954</date>
<booktitle>Distributional structure. Word,</booktitle>
<volume>10</volume>
<issue>23</issue>
<contexts>
<context position="7797" citStr="Harris, 1954" startWordPosition="1179" endWordPosition="1180">if x &lt; xmax. 3 f O 1 otherwise ( ) Notice that we only have to optimize those Xli jk =� 0, which can be solved efficiently since the matrix of co-occurrence counts is usually sparse. 4 Cross-lingual Objectives As the most important part in our model, the crosslingual objective describes the cross-lingual word relations and sets constraints when we factorize monolingual co-occurrence matrices. It can be derived from either cross-lingual co-occurrences or similarities between cross-lingual word pairs. 4.1 Cross-lingual Contexts The monolingual objective stems from the distributional hypothesis (Harris, 1954) and optimizes Monolingual PMI corpora matrices L1 L2 L1-L2 𝑴𝒍𝟐 𝑴𝒍𝟏 Bilingual corpus ≈ ≈ bilingual relations and constraints 𝑾𝒍𝟏 𝑾𝒍𝟐 ⋅ ⋅ 𝑪𝒍𝟏 𝑪𝒍𝟐 ELli mono = j,k 568 words in similar contexts into similar embeddings. It is natural to further extend this idea to define cross-lingual contexts, for which we have multiple choices. For the definition of cross-lingual contexts, we have multiple choices. A straightforward option is to count all the word co-occurrences in aligned sentence pairs, which is equivalent to a uniform word alignment model adopted by Gouws et al. (2015). For the sentence-align</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig S Harris. 1954. Distributional structure. Word, 10(23):146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Moritz Hermann</author>
<author>Phil Blunsom</author>
</authors>
<title>Multilingual models for compositional distributed semantics.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>58--68</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="2742" citStr="Hermann and Blunsom (2014)" startWordPosition="377" endWordPosition="381">e low-resource languages, where it allows one to develop accurate word representations of one language by exploiting the abundant textual resources in another language, e.g., English, which has a high resource density. The joint-space models are not only technically plausible, but also useful for cross-lingual model transfer. Further, studies have shown that using cross-lingual correlation can improve the quality of word representations trained solely with monolingual corpora (Faruqui and Dyer, 2014). Defining a cross-lingual learning objective is crucial at the core of the joint-space model. Hermann and Blunsom (2014) and Chandar A P et al. (2014) tried to calculate parallel sentence (or document) representations and to minimize the differences between the semantically equivalent pairs. These methods are useful in capturing semantic information carried by high-level units (such as phrases and beyond) and usually do not rely on word alignments. However, they suffer from reduced accuracy for representing rare tokens, whose semantic information may not be well generalized. In these cases, finer-grained information at lexical level, such as aligned word pairs, dictionaries, and word translation probabilities, </context>
<context position="14991" citStr="Hermann and Blunsom, 2014" startWordPosition="2374" endWordPosition="2378">g between parts of the total objective and the amount of training data on the quality of the embeddings is demonstrated in Figure 3. The baseline systems are Majority class where test documents are simply classified as the class with the most training samples, and Machine translation where a phrased-based machine translation system is used to translate test documents into the same language as the training documents. We also summarize the classification accuracy reported in some previous work, including Multitask learning (Klementiev et al., 2012), Bilingual compositional vector model (BiCVM) (Hermann and Blunsom, 2014), Bilingual autoencoder for bags-of-words (BAE) (Chandar A P et al., 2014), and BilBOWA (Gouws et al., 2015). A more recent work of Soyer et al. (2015) developed a compositional approach and reported an accuracy of 90.8% (en→de) and 80.1% (de→en) when using full RCV and Europarl corpora. Our method outperforms the previous work and we observe improvements when we exploit word translation probabilities (CLSim) over the model without word-level information (CLC-WA). The best result is achieved with CLSim. It is interesting to notice that CLC+WA, which makes use of word alignments in defining cro</context>
</contexts>
<marker>Hermann, Blunsom, 2014</marker>
<rawString>Karl Moritz Hermann and Phil Blunsom. 2014. Multilingual models for compositional distributed semantics. In Proceedings of ACL, pages 58–68. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liangjie Hong</author>
<author>Aziz S Doumith</author>
<author>Brian D Davison</author>
</authors>
<title>Co-factorization machines: modeling user interests and predicting individual decisions in twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of WSDM,</booktitle>
<pages>557--566</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="17858" citStr="Hong et al., 2013" startWordPosition="2809" endWordPosition="2812">ows one to weigh samples according to their importance and confidence. Joint matrix factorization allows one to decompose matrices with some correlational constraints. Collective matrix factorization has been developed to handle pairwise relations (Singh and Gordon, 2008). Chang et al. (2013) generalized LSA to Multi-Relational LSA, which constructs a 3-way tensor to combine the multiple relations between words. While matrix factorization is widely used in recommender systems, matrix co-factorization helps to handle multiple aspects of the data and improves in predicting individual decisions (Hong et al., 2013). Multiple sources of information, such as content and linkage, can also be connected with matrix co-factorization to derive high-quality webpage representations (Zhu et al., 2007). The advantage of this approach is that it automatically finds optimal parameters to optimize both single matrix factorization and relational alignments, which avoids manually defining a projection matrix or transfer function. To the best of our knowledge, we are the first to introduce this technique to learn cross-lingual word embeddings. 7 Conclusions In this paper, we introduced a framework of matrix co-factoriza</context>
</contexts>
<marker>Hong, Doumith, Davison, 2013</marker>
<rawString>Liangjie Hong, Aziz S Doumith, and Brian D Davison. 2013. Co-factorization machines: modeling user interests and predicting individual decisions in twitter. In Proceedings of WSDM, pages 557–566. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving word representations via global context and multiple word prototypes.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>873--882</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="1480" citStr="Huang et al., 2012" startWordPosition="188" endWordPosition="191">llel corpora, with or without word alignments. Empirical results on a task of cross-lingual document classification show that our method is effective to encode cross-lingual knowledge as constraints for cross-lingual word embeddings. 1 Introduction Word embeddings allow one to represent words in a continuous vector space, which characterizes the lexico-semanic relations among words. In many NLP tasks, they prove to be high-quality features, successful applications of which include language modelling (Bengio et al., 2003), sentiment analysis (Socher et al., 2011) and word sense discrimination (Huang et al., 2012). Like words having synonyms in the same language, there are also word pairs across languages which share resembling semantic properties. Mikolov et al. (2013a) observed a strong similarity of the geometric arrangements of corresponding concepts between the vector spaces of different languages, and suggested that a crosslingual mapping between the two vector spaces is technically plausible. In the meantime, the jointspace models for cross-lingual word embeddings are very desirable, as language-invariant semantic features can be generalized to make it easy to transfer models across languages. T</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. 2012. Improving word representations via global context and multiple word prototypes. In Proceedings of ACL, pages 873–882. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcin Junczys-Dowmunt</author>
<author>Arkadiusz Szał</author>
</authors>
<title>Symgiza++: symmetrized word alignment models for statistical machine translation.</title>
<date>2012</date>
<booktitle>In Security and Intelligent Information Systems,</booktitle>
<pages>379--390</pages>
<publisher>Springer.</publisher>
<marker>Junczys-Dowmunt, Szał, 2012</marker>
<rawString>Marcin Junczys-Dowmunt and Arkadiusz Szał. 2012. Symgiza++: symmetrized word alignment models for statistical machine translation. In Security and Intelligent Information Systems, pages 379–390. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Klementiev</author>
<author>Ivan Titov</author>
<author>Binod Bhattarai</author>
</authors>
<title>Inducing crosslingual distributed representations of words.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING. ICCL.</booktitle>
<contexts>
<context position="11463" citStr="Klementiev et al. (2012)" startWordPosition="1798" endWordPosition="1801">ance, distance(wl1 j , wl2k ) = ||wl1 j − wl2k ||2. Notice that similar to the monolingual objective, we may optimize for only those sim(j, k) =� 0, which is efficient as the matrix of translation probabilities or dictionary is sparse. We call this method CLSim. 5 Experiments To evaluate the quality of the relatedness between words in different languages, we induce the task of cross-lingual document classification for the English-German language pair, where a classifier is trained in one language and later used to classify documents in another. We exactly replicated the experiment settings of Klementiev et al. (2012). 5.1 Data and Training For optimizing the monolingual objectives, We used exactly the same subset of RCV1/RCV2 corpora (Lewis et al., 2004) as by Klementiev et al. (2012), which were sampled to balance the number of tokens between languages. Our preprocessing strategy followed Chandar A P et al. (2014), where we lowercased all words, removed punctuations and used the same vocabularies (|Ven |= 43, 614 and |V de |= 50, 110). When counting � Xbi jk = (Sl1,Sl2)∈Dbi �Lcross = j∈V l1,k∈V l2 569 Accuray 0.95 0.85 0.75 0.65 0.55 0.9 0.8 0.7 0.6 0.5 en→de de→en Accuray 0.9 0.8 0.7 0.6 0.5 0.4 0.3 en→</context>
<context position="13157" citStr="Klementiev et al. (2012)" startWordPosition="2084" endWordPosition="2087">se a decreasing weighting function as Pennington et al. (2014), where dword-apart word pairs contribute 1/d to the total count. We used a symmetric window size of 10 words for all our experiments. The cross-lingual constraints were derived using the English and German sections of the Europarl v7 parallel corpus (Koehn, 2005), which were similarly preprocessed. For CLC+WA and CLSim, we obtained word alignments and translation probabilities with SyMGIZA++ (JunczysDowmunt and Szał, 2012). We did not use Europarl for monolingual training. The documents for classification were randomly selected by Klementiev et al. (2012) from those in RCV1/RCV2 that are assigned to only one single topic among the four: CCAT (Corporate/Industrial), ECAT (Economics), GCAT (Government/Social), and MCAT (Markets). 1,000/5,000 documents in each language were used as a train/test set and we kept another 1,000 documents as a development set for hyperparameter tuning. Each document was represented as an idf-weighted average embedding of all its tokens, and a multi-class document classifier was trained for 10 epochs with an averaged perceptron algorithm as by Klementiev et al. (2012). A classifier trained with English documents is use</context>
<context position="14917" citStr="Klementiev et al., 2012" startWordPosition="2365" endWordPosition="2368">s compared with some baselines and previous work. The effect of weighting between parts of the total objective and the amount of training data on the quality of the embeddings is demonstrated in Figure 3. The baseline systems are Majority class where test documents are simply classified as the class with the most training samples, and Machine translation where a phrased-based machine translation system is used to translate test documents into the same language as the training documents. We also summarize the classification accuracy reported in some previous work, including Multitask learning (Klementiev et al., 2012), Bilingual compositional vector model (BiCVM) (Hermann and Blunsom, 2014), Bilingual autoencoder for bags-of-words (BAE) (Chandar A P et al., 2014), and BilBOWA (Gouws et al., 2015). A more recent work of Soyer et al. (2015) developed a compositional approach and reported an accuracy of 90.8% (en→de) and 80.1% (de→en) when using full RCV and Europarl corpora. Our method outperforms the previous work and we observe improvements when we exploit word translation probabilities (CLSim) over the model without word-level information (CLC-WA). The best result is achieved with CLSim. It is interesting</context>
</contexts>
<marker>Klementiev, Titov, Bhattarai, 2012</marker>
<rawString>Alexandre Klementiev, Ivan Titov, and Binod Bhattarai. 2012. Inducing crosslingual distributed representations of words. In Proceedings of COLING. ICCL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom´aˇs Koˇcisk`y</author>
<author>Karl Moritz Hermann</author>
<author>Phil Blunsom</author>
</authors>
<title>Learning bilingual word representations by marginalizing alignments.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>224--229</pages>
<publisher>ACL.</publisher>
<marker>Koˇcisk`y, Hermann, Blunsom, 2014</marker>
<rawString>Tom´aˇs Koˇcisk`y, Karl Moritz Hermann, and Phil Blunsom. 2014. Learning bilingual word representations by marginalizing alignments. In Proceedings of ACL, pages 224–229. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In MT summit,</booktitle>
<volume>5</volume>
<pages>79--86</pages>
<contexts>
<context position="12859" citStr="Koehn, 2005" startWordPosition="2041" endWordPosition="2042">e of Europarl used for training (%) (a) (b) (c) Figure 3: Cross-lingual document classification accuracy, with (a) varying weighting of cross-lingual objective (b) varying size of training monolingual corpora, and (c) varying size of training bilingual corpus. word co-occurrences, we use a decreasing weighting function as Pennington et al. (2014), where dword-apart word pairs contribute 1/d to the total count. We used a symmetric window size of 10 words for all our experiments. The cross-lingual constraints were derived using the English and German sections of the Europarl v7 parallel corpus (Koehn, 2005), which were similarly preprocessed. For CLC+WA and CLSim, we obtained word alignments and translation probabilities with SyMGIZA++ (JunczysDowmunt and Szał, 2012). We did not use Europarl for monolingual training. The documents for classification were randomly selected by Klementiev et al. (2012) from those in RCV1/RCV2 that are assigned to only one single topic among the four: CCAT (Corporate/Industrial), ECAT (Economics), GCAT (Government/Social), and MCAT (Markets). 1,000/5,000 documents in each language were used as a train/test set and we kept another 1,000 documents as a development set</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT summit, volume 5, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Peter W Foltz</author>
<author>Darrell Laham</author>
</authors>
<title>An introduction to latent semantic analysis.</title>
<date>1998</date>
<booktitle>Discourse processes,</booktitle>
<pages>25--2</pages>
<contexts>
<context position="16591" citStr="Landauer et al., 1998" startWordPosition="2622" endWordPosition="2626">inton, 2008) where we observe the topical nature of word embeddings. Regardless of their source languages, words sharing a common topic, e.g. economy, are closely aligned with each other, revealing the semantic validity of the joint vector space. 6 Related Work Matrix factorization has been successfully applied to learn word representations, which use several low-rank matrices to approximate the original matrix with extracted statistical information, usually word co-occurrence counts or PMI. Singular value decomposition (SVD) (Eckart and Young, 1936), SVD-based latent semantic analysis (LSA) (Landauer et al., 1998), latent semantic indexing (LSI) (Deerwester et al., 1990), and the more recentlyproposed global vectors for word representation (GloVe) (Pennington et al., 2014) find their wide applications in the area of NLP and information retrieval (Berry et al., 1995). Additionally, there is evidence that some neural-network-based models, such as Skip-gram (Mikolov et al., 2013b) which exhibits state-of-the-art performance, are also implicitly factorizing a PMI-based matrix (Levy and Goldberg, 2014). The strategy for matrix factorization in this paper, as Pennington et al. (2014), is in a stochastic fash</context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>Thomas K Landauer, Peter W Foltz, and Darrell Laham. 1998. An introduction to latent semantic analysis. Discourse processes, 25(2-3):259–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Neural word embedding as implicit matrix factorization.</title>
<date>2014</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>2177--2185</pages>
<contexts>
<context position="17084" citStr="Levy and Goldberg, 2014" startWordPosition="2693" endWordPosition="2696"> PMI. Singular value decomposition (SVD) (Eckart and Young, 1936), SVD-based latent semantic analysis (LSA) (Landauer et al., 1998), latent semantic indexing (LSI) (Deerwester et al., 1990), and the more recentlyproposed global vectors for word representation (GloVe) (Pennington et al., 2014) find their wide applications in the area of NLP and information retrieval (Berry et al., 1995). Additionally, there is evidence that some neural-network-based models, such as Skip-gram (Mikolov et al., 2013b) which exhibits state-of-the-art performance, are also implicitly factorizing a PMI-based matrix (Levy and Goldberg, 2014). The strategy for matrix factorization in this paper, as Pennington et al. (2014), is in a stochastic fashion, which better handles unobserved data and allows one to weigh samples according to their importance and confidence. Joint matrix factorization allows one to decompose matrices with some correlational constraints. Collective matrix factorization has been developed to handle pairwise relations (Singh and Gordon, 2008). Chang et al. (2013) generalized LSA to Multi-Relational LSA, which constructs a 3-way tensor to combine the multiple relations between words. While matrix factorization i</context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014. Neural word embedding as implicit matrix factorization. In Proceedings of NIPS, pages 2177–2185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>Yiming Yang</author>
<author>Tony G Rose</author>
<author>Fan Li</author>
</authors>
<title>Rcv1: A new benchmark collection for text categorization research.</title>
<date>2004</date>
<journal>JMLR,</journal>
<pages>5--361</pages>
<contexts>
<context position="11603" citStr="Lewis et al., 2004" startWordPosition="1821" endWordPosition="1824"> 0, which is efficient as the matrix of translation probabilities or dictionary is sparse. We call this method CLSim. 5 Experiments To evaluate the quality of the relatedness between words in different languages, we induce the task of cross-lingual document classification for the English-German language pair, where a classifier is trained in one language and later used to classify documents in another. We exactly replicated the experiment settings of Klementiev et al. (2012). 5.1 Data and Training For optimizing the monolingual objectives, We used exactly the same subset of RCV1/RCV2 corpora (Lewis et al., 2004) as by Klementiev et al. (2012), which were sampled to balance the number of tokens between languages. Our preprocessing strategy followed Chandar A P et al. (2014), where we lowercased all words, removed punctuations and used the same vocabularies (|Ven |= 43, 614 and |V de |= 50, 110). When counting � Xbi jk = (Sl1,Sl2)∈Dbi �Lcross = j∈V l1,k∈V l2 569 Accuray 0.95 0.85 0.75 0.65 0.55 0.9 0.8 0.7 0.6 0.5 en→de de→en Accuray 0.9 0.8 0.7 0.6 0.5 0.4 0.3 en→de de→en Accuray 0.95 0.85 0.75 0.65 0.55 0.9 0.8 0.7 0.6 0.5 en→de de→en 1 10 100 1 10 100 1 10 100 Weight of cross-lingual objective Perce</context>
</contexts>
<marker>Lewis, Yang, Rose, Li, 2004</marker>
<rawString>David D Lewis, Yiming Yang, Tony G Rose, and Fan Li. 2004. Rcv1: A new benchmark collection for text categorization research. JMLR, 5:361–397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Quoc V Le</author>
<author>Ilya Sutskever</author>
</authors>
<title>Exploiting similarities among languages for machine translation.</title>
<date>2013</date>
<pages>1309--4168</pages>
<contexts>
<context position="1638" citStr="Mikolov et al. (2013" startWordPosition="215" endWordPosition="218">de cross-lingual knowledge as constraints for cross-lingual word embeddings. 1 Introduction Word embeddings allow one to represent words in a continuous vector space, which characterizes the lexico-semanic relations among words. In many NLP tasks, they prove to be high-quality features, successful applications of which include language modelling (Bengio et al., 2003), sentiment analysis (Socher et al., 2011) and word sense discrimination (Huang et al., 2012). Like words having synonyms in the same language, there are also word pairs across languages which share resembling semantic properties. Mikolov et al. (2013a) observed a strong similarity of the geometric arrangements of corresponding concepts between the vector spaces of different languages, and suggested that a crosslingual mapping between the two vector spaces is technically plausible. In the meantime, the jointspace models for cross-lingual word embeddings are very desirable, as language-invariant semantic features can be generalized to make it easy to transfer models across languages. This is especially important for those low-resource languages, where it allows one to develop accurate word representations of one language by exploiting the a</context>
<context position="16960" citStr="Mikolov et al., 2013" startWordPosition="2677" endWordPosition="2680">matrices to approximate the original matrix with extracted statistical information, usually word co-occurrence counts or PMI. Singular value decomposition (SVD) (Eckart and Young, 1936), SVD-based latent semantic analysis (LSA) (Landauer et al., 1998), latent semantic indexing (LSI) (Deerwester et al., 1990), and the more recentlyproposed global vectors for word representation (GloVe) (Pennington et al., 2014) find their wide applications in the area of NLP and information retrieval (Berry et al., 1995). Additionally, there is evidence that some neural-network-based models, such as Skip-gram (Mikolov et al., 2013b) which exhibits state-of-the-art performance, are also implicitly factorizing a PMI-based matrix (Levy and Goldberg, 2014). The strategy for matrix factorization in this paper, as Pennington et al. (2014), is in a stochastic fashion, which better handles unobserved data and allows one to weigh samples according to their importance and confidence. Joint matrix factorization allows one to decompose matrices with some correlational constraints. Collective matrix factorization has been developed to handle pairwise relations (Singh and Gordon, 2008). Chang et al. (2013) generalized LSA to Multi-R</context>
</contexts>
<marker>Mikolov, Le, Sutskever, 2013</marker>
<rawString>Tomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013a. Exploiting similarities among languages for machine translation. arXiv:1309.4168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="1638" citStr="Mikolov et al. (2013" startWordPosition="215" endWordPosition="218">de cross-lingual knowledge as constraints for cross-lingual word embeddings. 1 Introduction Word embeddings allow one to represent words in a continuous vector space, which characterizes the lexico-semanic relations among words. In many NLP tasks, they prove to be high-quality features, successful applications of which include language modelling (Bengio et al., 2003), sentiment analysis (Socher et al., 2011) and word sense discrimination (Huang et al., 2012). Like words having synonyms in the same language, there are also word pairs across languages which share resembling semantic properties. Mikolov et al. (2013a) observed a strong similarity of the geometric arrangements of corresponding concepts between the vector spaces of different languages, and suggested that a crosslingual mapping between the two vector spaces is technically plausible. In the meantime, the jointspace models for cross-lingual word embeddings are very desirable, as language-invariant semantic features can be generalized to make it easy to transfer models across languages. This is especially important for those low-resource languages, where it allows one to develop accurate word representations of one language by exploiting the a</context>
<context position="16960" citStr="Mikolov et al., 2013" startWordPosition="2677" endWordPosition="2680">matrices to approximate the original matrix with extracted statistical information, usually word co-occurrence counts or PMI. Singular value decomposition (SVD) (Eckart and Young, 1936), SVD-based latent semantic analysis (LSA) (Landauer et al., 1998), latent semantic indexing (LSI) (Deerwester et al., 1990), and the more recentlyproposed global vectors for word representation (GloVe) (Pennington et al., 2014) find their wide applications in the area of NLP and information retrieval (Berry et al., 1995). Additionally, there is evidence that some neural-network-based models, such as Skip-gram (Mikolov et al., 2013b) which exhibits state-of-the-art performance, are also implicitly factorizing a PMI-based matrix (Levy and Goldberg, 2014). The strategy for matrix factorization in this paper, as Pennington et al. (2014), is in a stochastic fashion, which better handles unobserved data and allows one to weigh samples according to their importance and confidence. Joint matrix factorization allows one to decompose matrices with some correlational constraints. Collective matrix factorization has been developed to handle pairwise relations (Singh and Gordon, 2008). Chang et al. (2013) generalized LSA to Multi-R</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Proceedings of NIPS, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Glove: Global vectors for word representation.</title>
<date>2014</date>
<pages>1532--1543</pages>
<contexts>
<context position="6221" citStr="Pennington et al., 2014" startWordPosition="907" endWordPosition="910">dcontext PMI (point-wise mutual information) matrices Mli. During monolingual factorization, we put a cross-lingual constraint (cost) on it, ensuring cross-lingual semantic relations. We formalize the global loss function as ELtotal = ωi · Lmono(Wli, Cli) i∈{1,2} (1) +ωc · Lcross(Wl1, Cl1, Wl2, Cl2), where Lmono and Lcross are the monolingual and cross-lingual objectives respectively. Wi and Wc weigh the contribution of the different parts to the total objective. An overview of our algorithm is illustrated in Figure 1. 3 Monolingual Objective Our monolingual objective follows the GloVe model (Pennington et al., 2014), which learns from global word co-occurrence statistics. For a word-context pair (j, k) in language li, we try to 1In this paper, we let Uli = V li. Figure 1: The framework of cross-lingual word embedding via matrix co-factorization. minimize the difference between the dot product of the embeddings wlij · clik and their PMI value Mli Mli = X&apos;&apos;`·Ej k X&apos;&apos;` where Xli is the jk jk li · li , jkkjk matrix of word-context co-occurrence counts. As Pennington et al. (2014), we add separate terms bli wj, bli ck for each word and context to absorb the effect of any possible word-specific biases. We also</context>
<context position="12595" citStr="Pennington et al. (2014)" startWordPosition="1994" endWordPosition="1997"> 0.85 0.75 0.65 0.55 0.9 0.8 0.7 0.6 0.5 en→de de→en Accuray 0.9 0.8 0.7 0.6 0.5 0.4 0.3 en→de de→en Accuray 0.95 0.85 0.75 0.65 0.55 0.9 0.8 0.7 0.6 0.5 en→de de→en 1 10 100 1 10 100 1 10 100 Weight of cross-lingual objective Percentage of RCV used for training (%) Percentage of Europarl used for training (%) (a) (b) (c) Figure 3: Cross-lingual document classification accuracy, with (a) varying weighting of cross-lingual objective (b) varying size of training monolingual corpora, and (c) varying size of training bilingual corpus. word co-occurrences, we use a decreasing weighting function as Pennington et al. (2014), where dword-apart word pairs contribute 1/d to the total count. We used a symmetric window size of 10 words for all our experiments. The cross-lingual constraints were derived using the English and German sections of the Europarl v7 parallel corpus (Koehn, 2005), which were similarly preprocessed. For CLC+WA and CLSim, we obtained word alignments and translation probabilities with SyMGIZA++ (JunczysDowmunt and Szał, 2012). We did not use Europarl for monolingual training. The documents for classification were randomly selected by Klementiev et al. (2012) from those in RCV1/RCV2 that are assi</context>
<context position="16753" citStr="Pennington et al., 2014" startWordPosition="2646" endWordPosition="2649">ly aligned with each other, revealing the semantic validity of the joint vector space. 6 Related Work Matrix factorization has been successfully applied to learn word representations, which use several low-rank matrices to approximate the original matrix with extracted statistical information, usually word co-occurrence counts or PMI. Singular value decomposition (SVD) (Eckart and Young, 1936), SVD-based latent semantic analysis (LSA) (Landauer et al., 1998), latent semantic indexing (LSI) (Deerwester et al., 1990), and the more recentlyproposed global vectors for word representation (GloVe) (Pennington et al., 2014) find their wide applications in the area of NLP and information retrieval (Berry et al., 1995). Additionally, there is evidence that some neural-network-based models, such as Skip-gram (Mikolov et al., 2013b) which exhibits state-of-the-art performance, are also implicitly factorizing a PMI-based matrix (Levy and Goldberg, 2014). The strategy for matrix factorization in this paper, as Pennington et al. (2014), is in a stochastic fashion, which better handles unobserved data and allows one to weigh samples according to their importance and confidence. Joint matrix factorization allows one to d</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. pages 1532–1543.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ajit P Singh</author>
<author>Geoffrey J Gordon</author>
</authors>
<title>Relational learning via collective matrix factorization.</title>
<date>2008</date>
<booktitle>In Proceedings of SIGKDD,</booktitle>
<pages>650--658</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="17512" citStr="Singh and Gordon, 2008" startWordPosition="2759" endWordPosition="2762">me neural-network-based models, such as Skip-gram (Mikolov et al., 2013b) which exhibits state-of-the-art performance, are also implicitly factorizing a PMI-based matrix (Levy and Goldberg, 2014). The strategy for matrix factorization in this paper, as Pennington et al. (2014), is in a stochastic fashion, which better handles unobserved data and allows one to weigh samples according to their importance and confidence. Joint matrix factorization allows one to decompose matrices with some correlational constraints. Collective matrix factorization has been developed to handle pairwise relations (Singh and Gordon, 2008). Chang et al. (2013) generalized LSA to Multi-Relational LSA, which constructs a 3-way tensor to combine the multiple relations between words. While matrix factorization is widely used in recommender systems, matrix co-factorization helps to handle multiple aspects of the data and improves in predicting individual decisions (Hong et al., 2013). Multiple sources of information, such as content and linkage, can also be connected with matrix co-factorization to derive high-quality webpage representations (Zhu et al., 2007). The advantage of this approach is that it automatically finds optimal pa</context>
</contexts>
<marker>Singh, Gordon, 2008</marker>
<rawString>Ajit P Singh and Geoffrey J Gordon. 2008. Relational learning via collective matrix factorization. In Proceedings of SIGKDD, pages 650–658. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>151--161</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="1429" citStr="Socher et al., 2011" startWordPosition="179" endWordPosition="182">e cross-lingual constraints can be derived from parallel corpora, with or without word alignments. Empirical results on a task of cross-lingual document classification show that our method is effective to encode cross-lingual knowledge as constraints for cross-lingual word embeddings. 1 Introduction Word embeddings allow one to represent words in a continuous vector space, which characterizes the lexico-semanic relations among words. In many NLP tasks, they prove to be high-quality features, successful applications of which include language modelling (Bengio et al., 2003), sentiment analysis (Socher et al., 2011) and word sense discrimination (Huang et al., 2012). Like words having synonyms in the same language, there are also word pairs across languages which share resembling semantic properties. Mikolov et al. (2013a) observed a strong similarity of the geometric arrangements of corresponding concepts between the vector spaces of different languages, and suggested that a crosslingual mapping between the two vector spaces is technically plausible. In the meantime, the jointspace models for cross-lingual word embeddings are very desirable, as language-invariant semantic features can be generalized to </context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning. 2011. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of EMNLP, pages 151–161. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hubert Soyer</author>
</authors>
<title>Pontus Stenetorp, and Akiko Aizawa.</title>
<date>2015</date>
<booktitle>In Proceedings of ICLR.</booktitle>
<marker>Soyer, 2015</marker>
<rawString>Hubert Soyer, Pontus Stenetorp, and Akiko Aizawa. 2015. Leveraging monolingual data for crosslingual compositional word representations. In Proceedings of ICLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurens Van der Maaten</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Visualizing data using t-sne.</title>
<date>2008</date>
<journal>JMLR,</journal>
<pages>9--2579</pages>
<marker>Van der Maaten, Hinton, 2008</marker>
<rawString>Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-sne. JMLR, 9:2579–2605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shenghuo Zhu</author>
<author>Kai Yu</author>
<author>Yun Chi</author>
<author>Yihong Gong</author>
</authors>
<title>Combining content and link for classification using matrix factorization.</title>
<date>2007</date>
<booktitle>In Proceedings of SIGIR,</booktitle>
<pages>487--494</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="18038" citStr="Zhu et al., 2007" startWordPosition="2834" endWordPosition="2837">trix factorization has been developed to handle pairwise relations (Singh and Gordon, 2008). Chang et al. (2013) generalized LSA to Multi-Relational LSA, which constructs a 3-way tensor to combine the multiple relations between words. While matrix factorization is widely used in recommender systems, matrix co-factorization helps to handle multiple aspects of the data and improves in predicting individual decisions (Hong et al., 2013). Multiple sources of information, such as content and linkage, can also be connected with matrix co-factorization to derive high-quality webpage representations (Zhu et al., 2007). The advantage of this approach is that it automatically finds optimal parameters to optimize both single matrix factorization and relational alignments, which avoids manually defining a projection matrix or transfer function. To the best of our knowledge, we are the first to introduce this technique to learn cross-lingual word embeddings. 7 Conclusions In this paper, we introduced a framework of matrix co-factorization to learn cross-lingual word embeddings. It is capable of capturing the lexicosemantic similarities of different languages in a unified vector space, where the embeddings are j</context>
</contexts>
<marker>Zhu, Yu, Chi, Gong, 2007</marker>
<rawString>Shenghuo Zhu, Kai Yu, Yun Chi, and Yihong Gong. 2007. Combining content and link for classification using matrix factorization. In Proceedings of SIGIR, pages 487–494. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Will Y Zou</author>
<author>Richard Socher</author>
<author>Daniel M Cer</author>
<author>Christopher D Manning</author>
</authors>
<title>Bilingual word embeddings for phrase-based machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1393--1398</pages>
<contexts>
<context position="3669" citStr="Zou et al. (2013)" startWordPosition="521" endWordPosition="524">ly on word alignments. However, they suffer from reduced accuracy for representing rare tokens, whose semantic information may not be well generalized. In these cases, finer-grained information at lexical level, such as aligned word pairs, dictionaries, and word translation probabilities, is considered to be helpful. Koˇcisk`y et al. (2014) integrated word aligning process and word embedding in machine translation models. This method makes full use of parallel corpora and produces high-quality word alignments. However, it is unable to exploit the richer monolingual corpora. On the other hand, Zou et al. (2013) and Faruqui and Dyer (2014) learnt word embeddings of different languages in separate spaces with monolingual corpora and projected the embeddings into a joint space, but they can only capture linear transformation. In this paper, we address the above challenges with a framework of matrix co-factorization. We 567 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 567–572, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics simultan</context>
</contexts>
<marker>Zou, Socher, Cer, Manning, 2013</marker>
<rawString>Will Y Zou, Richard Socher, Daniel M Cer, and Christopher D Manning. 2013. Bilingual word embeddings for phrase-based machine translation. In Proceedings of EMNLP, pages 1393–1398.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>