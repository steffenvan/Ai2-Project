<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007254">
<title confidence="0.9371675">
Collective Annotation of Linguistic Resources:
Basic Principles and a Formal Model
</title>
<author confidence="0.972516">
Ulle Endriss and Raquel Fern´andez
</author>
<affiliation confidence="0.993526">
Institute for Logic, Language &amp; Computation
University of Amsterdam
</affiliation>
<email confidence="0.996138">
{ulle.endriss|raquel.fernandez}@uva.nl
</email>
<sectionHeader confidence="0.994735" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999784263157895">
Crowdsourcing, which offers new ways
of cheaply and quickly gathering large
amounts of information contributed by
volunteers online, has revolutionised the
collection of labelled data. Yet, to create
annotated linguistic resources from this
data, we face the challenge of having to
combine the judgements of a potentially
large group of annotators. In this paper
we investigate how to aggregate individual
annotations into a single collective anno-
tation, taking inspiration from the field of
social choice theory. We formulate a gen-
eral formal model for collective annotation
and propose several aggregation methods
that go beyond the commonly used major-
ity rule. We test some of our methods on
data from a crowdsourcing experiment on
textual entailment annotation.
</bodyText>
<sectionHeader confidence="0.998786" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999981075471698">
In recent years, the possibility to undertake large-
scale annotation projects with hundreds or thou-
sands of annotators has become a reality thanks to
online crowdsourcing methods such as Amazon’s
Mechanical Turk and Games with a Purpose. Al-
though these techniques open the door to a true
revolution for the creation of annotated corpora,
within the computational linguistics community
there so far is no clear understanding of how the
so-called “wisdom of the crowds” could or should
be used to develop useful annotated linguistic re-
sources. Those who have looked into this increas-
ingly important issue have mostly concentrated on
validating the quality of multiple non-expert an-
notations in terms of how they compare to ex-
pert gold standards; but they have only used sim-
ple aggregation methods based on majority voting
to combine the judgments of individual annotators
(Snow et al., 2008; Venhuizen et al., 2013).
In this paper, we take a different perspective and
instead focus on investigating different aggrega-
tion methods for deriving a single collective an-
notation from a diverse set of judgments. For this
we draw inspiration from the field of social choice
theory, a theoretical framework for combining the
preferences or choices of several individuals into
a collective decision (Arrow et al., 2002). Our aim
is to explore the parallels between the task of ag-
gregating the preferences of the citizens participat-
ing in an election and the task of combining the
expertise of speakers taking part in an annotation
project. Our contribution consists in the formula-
tion of a general formal model for collective an-
notation and, in particular, the introduction of sev-
eral families of aggregation methods that go be-
yond the commonly used majority rule.
The remainder of this paper is organised as fol-
lows. In Section 2 we introduce some basic termi-
nology and argue that there are four natural forms
of collective annotation. We then focus on one of
them and present a formal model for it in Sec-
tion 3. We also formulate some basic principles
of aggregation within this model in the same sec-
tion. Section 4 introduces three families of ag-
gregation methods: bias-correcting majority rules,
greedy methods for identifying (near-)consensual
coalitions of annotators, and distance-based aggre-
gators. We test the former two families of aggrega-
tors, as well as the simple majority rule commonly
used in similar studies, in a case study on data ex-
tracted from a crowdsourcing experiment on tex-
tual entailment in Section 5. Section 6 discusses
related work and Section 7 concludes.
</bodyText>
<sectionHeader confidence="0.971133" genericHeader="method">
2 Four Types of Collective Annotation
</sectionHeader>
<bodyText confidence="0.9998756">
An annotation task consists of a set of items, each
of which is associated with a set of possible cate-
gories (Artstein and Poesio, 2008). The categories
may be the same for all items or they may be item-
specific. For instance, dialogue act annotation
</bodyText>
<page confidence="0.977941">
539
</page>
<note confidence="0.914547">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 539–549,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999470659574468">
(Allen and Core, 1997; Carletta et al., 1997) and
word similarity rating (Miller and Charles, 1991;
Finkelstein et al., 2002) involve choosing from
amongst a set of categories—acts in a dialogue
act taxonomy or values on a scale, respectively—
which remains fixed for all items in the annotation
task. In contrast, in tasks such as word sense la-
belling (Kilgarriff and Palmer, 2000; Palmer et al.,
2007; Venhuizen et al., 2013) and PP-attachment
annotation (Rosenthal et al., 2010; Jha et al., 2010)
coders need to choose a category amongst a set of
options specific to each item—the possible senses
of each word or the possible attachment points in
each sentence with a prepositional phrase.
In either case (one set of categories for all items
vs. item-specific sets of categories), annotators are
typically asked to identify, for each item, the cat-
egory they consider the best match. In addition,
they may be given the opportunity to indicate that
they cannot judge (the “don’t know” or “unclear”
category). For large-scale annotation projects run
over the Internet it is furthermore very likely that
an annotator will not be confronted with every sin-
gle item, and it makes sense to distinguish items
not seen by the annotator from items labelled as
“don’t know”. We refer to this form of annotation,
i.e., an annotation task where coders have the op-
tion to (i) label items with one of the available cat-
egories, to (ii) choose “don’t know”, or to (iii) not
label an item at all, as plain annotation.
Plain annotation is the most common form of
annotation and it is the one we shall focus on in
this paper. However, other, more complex, forms
of annotation are also possible and of interest. For
instance, we may ask coders to rank the avail-
able categories (resulting in, say, a weak or par-
tial order over the categories); we may ask them to
provide a qualitative ratings of the available cat-
egories for each item (e.g., excellent match, good
match, etc.); or we may ask for quantitative rat-
ings (e.g., numbers from 1 to 100).1 We refer to
these forms of annotation as complex annotation.
We want to investigate how to aggregate the
information available for each item once annota-
tions by multiple annotators have been collected.
In line with the terminology used in social choice
theory and particularly judgment aggregation (Ar-
</bodyText>
<footnote confidence="0.9632512">
1Some authors have combined qualitative and quantitative
ratings; e.g., for the Graded Word Sense dataset of Erk et al.
(2009) coders were asked to classify each relevant WordNet
sense for a given item on a 5-point scale: 1 completely differ-
ent, 2 mostly different, 3 similar, 4 very similar, 5 identical.
</footnote>
<bodyText confidence="0.996436375">
row, 1963; List and Pettit, 2002), let us call an ag-
gregation method independent if the outcome re-
garding a given item j only depends on the cate-
gories provided by the annotators regarding j it-
self (but not on, say, the categories assigned to a
different item j&apos;). Independent aggregation meth-
ods are attractive due to their simplicity. They also
have some conceptual appeal: when deciding on
j maybe we should only concern ourselves with
what people have to say regarding j? On the other
hand, insisting on independence prevents us from
exploiting potentially useful information that cuts
across items. For instance, if a particular anno-
tator almost always chooses category c, then we
should maybe give less weight to her selecting c
for the item j at hand than when some other anno-
tator chooses c for j. This would call for methods
that do not respect independence, which we shall
refer to as general aggregation. Note that when
studying independent aggregation methods, with-
out loss of generality, we may assume that each
annotation task consists of just a single item.
In view of our discussion above, there are four
classes of approaches to collective annotation:
</bodyText>
<listItem confidence="0.987432925925926">
(1) Independent aggregation of plain annota-
tions. This is the simplest case, resulting in a
fairly limited design space. When, for a given
item, each annotator has to choose between
k categories (or abstain) and we do not per-
mit ourselves to use any other information,
then the only reasonable choice is to imple-
ment the plurality rule (Taylor, 2005), under
which the winning category is the category
chosen by the largest number of annotators.
In case there are exactly two categories avail-
able, the plurality rule is also called the ma-
jority rule. The only additional consideration
to make here (besides how to deal with ties)
is whether or not we may want to declare no
winner at all in case the plurality winner does
not win by a sufficiently significant margin or
does not make a particular quota. This is the
most common approach in the literature (see,
e.g., Venhuizen et al., 2013).
(2) Independent aggregation of complex annota-
tions. This is a natural generalisation of the
first approach, resulting in a wider range of
possible methods. We shall not explore it
here, but only point out that in case annotators
provide linear orders over categories, there is
a close resemblance to classical voting the-
</listItem>
<page confidence="0.993959">
540
</page>
<bodyText confidence="0.984816111111111">
ory (Taylor, 2005); in case only partial orders
can be elicited, recent work in computational
social choice on the generalisation of classi-
cal voting rules may prove helpful (Pini et al.,
2009; Endriss et al., 2009); and in case an-
notators rate categories using qualitative ex-
pressions such as excellent match, the method
of majority judgment of Balinski and Laraki
(2011) should be considered.
</bodyText>
<listItem confidence="0.929212461538462">
(3) General aggregation of plain annotations.
This is the approach we shall discuss be-
low. It is related to voting in combinato-
rial domains studied in computational social
choice (Chevaleyre et al., 2008), and to both
binary aggregation (Dokow and Holzman,
2010; Grandi and Endriss, 2011) and judg-
ment aggregation (List and Pettit, 2002).
(4) General aggregation of complex annotations.
While appealing due to its great level of gen-
erality, this approach can only be tackled suc-
cessfully once approaches (2) and (3) are suf-
ficiently well understood.
</listItem>
<sectionHeader confidence="0.999505" genericHeader="method">
3 Formal Model
</sectionHeader>
<bodyText confidence="0.9999675">
Next we present our model for general aggregation
of plain annotations into a collective annotation.
</bodyText>
<subsectionHeader confidence="0.997729">
3.1 Terminology and Notation
</subsectionHeader>
<bodyText confidence="0.99869452631579">
An annotation task is defined in terms of m items,
with each item j E {1, ... , m} being associated
with a finite set of possible categories Cj. Anno-
tators are asked to provide an answer for each of
the items of the annotation task. In the context of
plain annotations, a valid answer for item j is an
element of the set Aj = Cj U {?,1}.2 Here ?
represents the answer “don’t know” and we use 1
to indicate that the annotator has not answered (or
even seen) the item at all. An annotation is a vec-
tor of answers by one annotator, one answer for
each item of the annotation task at hand, i.e., an
annotation is an element of the Cartesian product
A = A1 x A2 x · · · x Am. A typical element of
A will be denoted as A = (a1, ... , am).
Let N = {1, ... , n} be a finite set of n anno-
tators (or coders). A profile A = (A1,... , An) E
An, for a given annotation task, is a vector of an-
notations, one for each annotator. That is, A is an
</bodyText>
<footnote confidence="0.899039333333333">
2As discussed earlier, in the context of complex annota-
tions, an answer could also be, say, a partial order on C; or a
function associating elements of C; with numerical ratings.
</footnote>
<table confidence="0.9824244">
Item 1 Item 2 Item 3
Annotator 1 B A A
Annotator 2 B B B
Annotator 3 A B A
Majority B B A
</table>
<tableCaption confidence="0.999785">
Table 1: A profile with a collective annotation.
</tableCaption>
<bodyText confidence="0.99935352">
n x m-matrix; e.g., a3,7 is the answer that the 3rd
annotator provides for the 7th item.
We want to aggregate the information provided
by the annotators into a (single) collective anno-
tation. For the sake of simplicity, we use A also
as the domain of possible collective annotations
(even though the distinction between ? and 1 may
not be strictly needed here; they both indicate that
we do not want to commit to any particular cate-
gory). An aggregator is a function F : An → A,
mapping any given profile into a collective annota-
tion, i.e., a labelling of the items in the annotation
task with corresponding categories (or ? or 1). An
example is the plurality rule (also known as the
majority rule for binary tasks with |Cj |= 2 for
all items j), which annotates each item with the
category chosen most often.
Note that the collective annotation need not
coincide with any of the individual annotations.
Take, for example, a binary annotation task in
which three coders label three items with category
A or B as shown in Table 1. Here using the major-
ity rule to aggregate the annotations would result
in a collective annotation that does not fully match
any annotation by an individual coder.
</bodyText>
<subsectionHeader confidence="0.999364">
3.2 Basic Properties
</subsectionHeader>
<bodyText confidence="0.999835272727273">
A typical task in social choice theory is to formu-
late axioms that formalise specific desirable prop-
erties of an aggregator F (Arrow et al., 2002). Be-
low we adapt three of the most basic axioms that
have been considered in the social choice litera-
ture to our setting and we briefly discuss their rel-
evance to collective annotation tasks.
We will require some additional notation: for
any profile A, item j, and possible answer a E Aj,
let NAj:a denote the set of annotators who chose
answer a for item j under profile A.
</bodyText>
<listItem confidence="0.996934">
• F is anonymous if it treats coders symmetri-
cally, i.e., if for every permutation π : N → N,
F(A1, ... , An) = F(Aπ(1), ... , Aπ(n)). In so-
cial choice theory, this is a fairness constraint.
For us, fairness per se is not a desideratum,
</listItem>
<page confidence="0.99144">
541
</page>
<bodyText confidence="0.960980583333333">
but when we do not have any a priori informa-
tion regarding the expertise of annotators, then
anonymity is a natural axiom to adopt.
• F is neutral if it treats all items symmetri-
cally, i.e., if for every two items j and j&apos; with
the same set of possible categories (i.e., with
Cj = Cj,) and for every profile A, it is the case
that whenever N!j.a = Nj!,.a for all answers
a E Aj = Aj1, then F(A)j = F(A)j,. That
is, if the patterns of individual annotations of j
and j&apos; are the same, then also their collective
annotation should coincide. In social choice
theory, neutrality is also considered a basic fair-
ness requirement (avoiding preferential treat-
ment one candidate in an election). In the con-
text of collective annotation there may be good
reasons to violate neutrality: e.g., we may use
an aggregator that assigns different default cat-
egories to different items and that can override
such a default decision only in the presence of
a significant majority (note that this is different
from anonymity: we will often not have any in-
formation on our annotators, but we may have
tangible information on items).3
• F is independent if the collective annotation of
any given item j only depends on the individual
annotations of j. Formally, F is independent if,
for every item j and every two profiles A and
A&apos;, it is the case that whenever N!j.a = N!�
j.a for
all answers a E Aj, then F(A)j = F(A&apos;)j.
In social choice theory, independence is often
seen as a desirable albeit hard (or even impos-
sible) to achieve property (Arrow, 1963). For
collective annotation, we strongly believe that
it is not a desirable property: by considering
how annotators label other items we can learn
about their biases and we should try to exploit
this information to obtain the best possible an-
notation for the item at hand.
Note that the plurality/majority rule is indepen-
dent. All of the methods we shall propose in Sec-
tion 4 are both anonymous and neutral—except to
the extent to which we have to violate basic sym-
metry requirements in order to break ties between
categories chosen equally often for a given item.
None of our aggregators is independent.
3It would also be of interest to formulate a neutrality ax-
iom w.r.t. categories (rather than items). For two categories,
this idea has been discussed under the name of domain-
neutrality in the literature (Grandi and Endriss, 2011), but
for larger sets of categories it has not yet been explored.
Some annotation tasks might be subject to in-
tegrity constraints that determine the internal con-
sistency of an annotation. For example, if our
items are pairs of words and the possible cate-
gories include synonymous and antonymous, then
if item 1 is about words A and B, item 2 about
words B and C, and item 3 about words A and
C, then any annotation that labels items 1 and 2
as synonymous should not label item 3 as antony-
mous. Thus, a further desirable property that will
play a role for some annotation tasks is collective
rationality (Grandi and Endriss, 2011): if all in-
dividual annotations respect a given integrity con-
straint, then so should the collective annotation.
We can think of integrity constraints as impos-
ing top-down expert knowledge on an annotation.
However, for some annotation tasks, no integrity
constraints may be known to us in advance, even
though we may have reasons to believe that the
individual annotators do respect some such con-
straints. In that case, selecting one of the indi-
vidual annotations in the profile as the collective
annotation is the only way to ensure that these in-
tegrity constraints will be satisfied by the collec-
tive annotation (Grandi and Endriss, 2011). Of
course, to do so we would need to assume that
there is at least one annotator who has labelled all
items (and to be able to design a high-quality ag-
gregator in this way we should have a sufficiently
large number of such annotators to choose from),
which may not always be possible, particularly in
the context of crowdsourcing.
</bodyText>
<sectionHeader confidence="0.978689" genericHeader="method">
4 Three Families of Aggregators
</sectionHeader>
<bodyText confidence="0.999983916666667">
In this section we instantiate our formal model by
proposing three families of methods for aggrega-
tion. Each of them is inspired, in part, by standard
approaches to desigining aggregation rules devel-
oped in social choice theory and, in part, by the
specific needs of collective annotation. Regard-
ing the latter point, we specifically emphasise the
fact that not all annotators can be expected to be
equally reliable (in general or w.r.t. certain items)
and we try to integrate the process of aggregation
with a process whereby less reliable annotators are
either given less weight or are excluded altogether.
</bodyText>
<subsectionHeader confidence="0.999838">
4.1 Bias-Correcting Majority Rules
</subsectionHeader>
<bodyText confidence="0.999954666666667">
We first want to explore the following idea: If a
given annotator annotates most items with 0, then
we might want to assign less significance to that
</bodyText>
<page confidence="0.983844">
542
</page>
<bodyText confidence="0.999622428571429">
choice for any particular item.4 That is, if an an-
notator appears to be biased towards a particular
category, then we might want to try to correct for
this bias during aggregation.
What follows applies only to annotation tasks
where every item is associated with the same set of
categories. For ease of exposition, let us further-
more assume that there are only two categories, 0
and 1, and that annotators do not make use of the
option to annotate with ? (“don’t know”).
For every annotator i E N and every cate-
gory X E 10, 11, fix a weight wXi E R. The
bias-correcting majority (BCM) rule for this fam-
ily of weights is defined as follows. Given profile
A, the collective category for item j will be 1 in
case �ai,j=1 w1i &gt; &amp;i,j=0 w0i , and 0 otherwise.5
That is, we compute the overall weight for cate-
gory 1 by adding up the corresponding weights for
those coders that chose 1 for item j, and we do
accordingly for the overall weight for category 0;
finally, we choose as collective category that cate-
gory with the larger overall weight. Note that for
wXi - 1 we obtain the simple majority rule.
Below we define three intuitively appealing
families of weights, and thereby three BCM rules.
However, before we do so, we first require some
additional notation. Fix a profile of annotations.
For X E 10, 11, let Freqi(X) denote the relative
frequency with which annotator i has chosen cat-
egory X. For instance, if i has annotated 20 items
and has chosen 1 in five cases, then Freqi(1) =
0.25. Similarly, let Freq(X) denote the frequency
of X across the entire profile.
Here are three ways of making the intuitive idea
of bias correction concrete:
</bodyText>
<listItem confidence="0.489728454545454">
(1) The complement-based BCM rule (ComBCM)
is defined by weights wXi = Freqi(1−X).
That is, the weight of annotator i for cate-
gory X is equal to her relative frequency of
having chosen the other category 1−X. For
example, if you annotate two items with 1 and
eight with 0, then each of your 1-annotations
will have weight 0.8, while each of your
0-annotations will only have weight 0.2.
(2) The difference-based BCM rule (DiffBCM) is
defined by weights wXi = 1 + Freq(X) −
</listItem>
<footnote confidence="0.9717095">
4A similar idea is at the heart of cumulative voting, which
requires a voter to distribute a fixed number of points amongst
the candidates (Glasser, 1959; Brams and Fishburn, 2002).
5For the sake of simplicity, our description here presup-
poses that ties are always broken in favour of 0. Other tie-
breaking rules (e.g., random tie-breaking) are possible.
</footnote>
<bodyText confidence="0.995817547619048">
Freqi(X). Recall that Freq(X) is the rela-
tive frequency of X in the entire profile, while
Freqi(X) is the relative frequency of X in
the annotation of i. Hence, if i assigns cat-
egory X less often than the general popula-
tion, then her weight on X-choices will be in-
creased by the difference (and vice versa in
case she assigns X more often than the popu-
lation at large). For example, if you assign 1
in two out of ten cases, while in general cat-
egory 1 appears in exactly 50% of all annota-
tions, then your weight for a choice of 1 will
be 1 + 0.5 − 0.2 = 1.3, while you weight for
a choice of 0 will only be 0.7.
(3) The relative BCM rule (RelBCM) is defined
by weights wX i = Freq(X)
Freqi(X). The idea is very
similar to the DiffBCM rule. For the exam-
ple given above, your weight for a choice of
1 would be 0.5/0.2 = 2.5, while your weight
for a choice of 0 would be 0.5/0.8 = 0.625.
The main difference between the ComBCM rule
and the other two rules is that the former only takes
into account the possible bias of individual anno-
tators, while the latter two factor in as well the
possible skewness of the data (as reflected by the
labelling behaviour of the full set of annotators).
In addition, while ComBCM is specific to the
case of two categories, DiffBCM and RelBCM
immediately generalise to any number of cate-
gories. In this case, we add up the category-
specific weights as before and then choose the cat-
egory with maximal support (i.e., we generalise
the majority rule underlying the family of BCM
rules to the plurality rule).
We stress that our bias-correcting majority rules
do not violate anonymity (nor neutrality for that
matter). If we were to give less weight to a given
annotator based on, say, her name, this would con-
stitute a violation of anonymity; if we do so due to
properties of the profile at hand and if we do so in
a symmetric manner, then it does not.
</bodyText>
<subsectionHeader confidence="0.972056">
4.2 Greedy Consensus Rules
</subsectionHeader>
<bodyText confidence="0.999861375">
Now consider the following idea: If for a given
item there is almost complete consensus amongst
those coders that annotated it with a proper cate-
gory (i.e., those who did not choose ? or +), then
we should probably adopt their choice for the col-
lective annotation. Indeed, most aggregators will
make this recommendation. Furthermore, the fact
that there is almost full consensus for one item
</bodyText>
<page confidence="0.993358">
543
</page>
<bodyText confidence="0.999660461538461">
may cast doubts on the reliability of coders who
disagree with this near-consensus choice and we
might want to disregard their views not only w.r.t.
that item but also as far as the annotation of other
items is concerned. Next we propose a family of
aggregators that implement this idea.
For simplicity, suppose that the only proper cat-
egories available are 0 and 1 and that annotators
do not make use of ? (but it is easy to generalise
to arbitrary numbers of categories and scenarios
where different items are associated with different
categories). Fix a tolerance value t ∈ {0, ... , m}.
The greedy consensus rule GreedyCRt works as
follows. First, initialise the set N* with the full
population of annotators N. Then iterate the fol-
lowing two steps:
(1) Find the item with the strongest majority for
either 0 or 1 amongst coders in N* and lock
in that value for the collective annotation.
annotators need not be the largest such coalition
(due to the greedy nature of our rule).
Note that greedy consensus rules, as defined
here, are both anonymous and neutral. Specifi-
cally, it is important not to confuse possible skew-
ness of the data with a violation of neutrality of the
aggregator.
</bodyText>
<subsectionHeader confidence="0.981936">
4.3 Distance-based Aggregation
</subsectionHeader>
<bodyText confidence="0.983598729166666">
Our third approach is based on the notion of dis-
tance. We first define a metric on choices to be
able to say how distant two choices are. This in-
duces an aggregator that, for a given profile, re-
turns a collective choice that minimises the sum
of distances to the individual choices in the pro-
file.7 This opens up a wide range of possibilities;
we only sketch some of them here.
A natural choice is the adjusted Hamming dis-
tance H : A×A → R_&gt;O, which counts how many
items two annotations differ on:
(2) Eliminate all coders from N* who disagree H(A, A&apos;) = �m S(aj, a&apos;j)
on more than t items with the values locked j=1
in for the collective annotation so far.
Repeat this process until the categories for all m
items have been settled.6 We may think of this as
a “greedy” way of identifying a coalition N* with
high inter-annotator agreement and then applying
the majority rule to this coalition to obtain the col-
lective annotation.
To be precise, the above is a description of an
entire family of aggregators: Whenever there is
more than one item with a majority of maximal
strength, we could choose to lock in any one of
them. Also, when there is a split majority between
annotators in N* voting 0 and those voting 1, we
have to use a tie-breaking rule to make a decision.
Additional heuristics may be used to make these
local decisions, or they may be left to chance.
Note that in case t = m, GreedyCRt is sim-
ply the majority rule (as no annotator will ever get
eliminated). In case t = 0, we end up with a coali-
tion of annotators that unanimously agree with all
of the categories chosen for the collective annota-
tion. However, this coalition of perfectly aligned
6There are some similarities to Tideman’s Ranked Pairs
method for preference aggregation (Tideman, 1987), which
works by fixing the relative rankings of pairs of alternatives
in order of the strength of the supporting majorities. In pref-
erence aggregation (unlike here), the population of voters is
not reduced in the process; instead, decisions against the ma-
jority are taken whenever this is necessary to guarantee the
transitivity of the resulting collective preference order.
Here S is the adjusted discrete distance defined as
S(x, y) = 0 if x = y or x ∈ {?,⊥} or y ∈ {?, ⊥},
and as S(x, y) = 1 in all other cases.8
Once we have fixed a distance d on A (such
as H), this induces an aggregator Fd:
</bodyText>
<equation confidence="0.973495">
n
Fd(A) = argmin d(A, AZ)
AEA Z=1
</equation>
<bodyText confidence="0.999924857142857">
To be precise, Fd is an irresolute aggregator that
might return a set of best annotations with minimal
distance to the profile.
Note that FH is simply the plurality rule. This
is so because every element of the Cartesian prod-
uct is a possible annotation. In the presence of in-
tegrity constraints excluding some combinations,
however, a distance-based rule allows for more so-
phisticated forms of aggregation (by choosing the
optimal annotation w.r.t. all feasible annotations).
We may also try to restrict the computation of
distances to a subset of “reliable” annotators. Con-
sider the following idea: If a group of annota-
tors is (fairly) reliable, then they should have a
</bodyText>
<footnote confidence="0.999625">
7This idea has been used in voting (Kemeny, 1959), belief
merging (Konieczny and Pino P´erez, 2002), and judgment
aggregation (Miller and Osherson, 2009).
8This δ, divided by m, is the same thing as what Artstein
and Poesio (2008) call the agreement value agr, for item j.
</footnote>
<page confidence="0.997854">
544
</page>
<bodyText confidence="0.9999248">
(fairly) high inter-annotator agreement. By this
reasoning, we should choose a group of annota-
tors ANN ⊆ JV that maximises inter-annotator
agreement in ANN and work with the aggrega-
tor argminAEA EiEANN d(A, Ai). But this is too
simplistic: any singleton ANN = {i} will result
in perfect agreement. That is, while we can eas-
ily maximise agreement, doing so in a naive way
means ignoring most of the information collected.
In other words, we face the following dilemma:
</bodyText>
<listItem confidence="0.882607111111111">
• On the one hand, we should choose a small set
ANN (i.e., select few annotators to base our col-
lective annotation on), as that will allow us to
increase the (average) reliability of the annota-
tors taken into account.
• On the other hand, we should choose a large set
ANN (i.e., select many annotators to base our
collective annotation on), as that will increase
the amount of information exploited.
</listItem>
<bodyText confidence="0.997857">
One pragmatic approach is to fix a minimum qual-
ity threshold regarding one of the two dimensions
and optimise in view of the other.9
</bodyText>
<sectionHeader confidence="0.993852" genericHeader="method">
5 A Case Study
</sectionHeader>
<bodyText confidence="0.99992805">
In this section, we report on a case study in
which we have tested our bias-correcting major-
ity and greedy consensus rules.10 We have used
the dataset created by Snow et al. (2008) for
the task of recognising textual entailment, orig-
inally proposed by Dagan et al. (2006) in the
PASCAL Recognizing Textual Entailment (RTE)
Challenge. RTE is a binary classification task con-
sisting in judging whether the meaning of a piece
of text (the so-called hypothesis) can be inferred
from another piece of text (the entailing text).
The original RTE1 Challenge testset consists of
800 text-hypothesis pairs (such as T: “Chr´etien
visited Peugeot’s newly renovated car factory”,
H: “Peugeot manufactures cars”) with a gold
standard annotation that classifies each item as ei-
ther true (1)—in case H can be inferred from T—
or false (0). Exactly 400 items are annotated as
0 and exactly 400 as 1. Bos and Markert (2006)
performed an independent expert annotation of
</bodyText>
<footnote confidence="0.929778">
9GreedyCRt is a greedy (rather than optimal) implemen-
tation of this basic idea, with the tolerance value t fixing a
threshold on (a particular form of) inter-annotator agreement.
10Since the annotation task and dataset used for our case
study do not involve any interesting integrity constraints, we
have not tested any distance-based aggregation rules.
</footnote>
<bodyText confidence="0.999053269230769">
this testset, obtaining 95% agreement between the
RTE1 gold standard and their own annotation.
The dataset of Snow et al. (2008) includes 10
non-expert annotations for each of the 800 items
in the RTE1 testset, collected with Amazon’s Me-
chanical Turk. A quick examination of the dataset
shows that there are a total of 164 annotators who
have annotated between 20 items (124 annotators)
and 800 items each (only one annotator). Non-
expert annotations with category 1 (rather than 0)
are slightly more frequent (Freq(1) Pz 0.57).
We have applied our aggregators to this data and
compared the outcomes with each other and to the
gold standard. The results are summarised in Ta-
ble 2 and discussed in the sequel. For each pair
we report the observed agreement Ao (proportion
of items on which two annotations agree) and, in
brackets, Cohen’s kappa n = Ao−Ae
1−Ae , with Ae be-
ing the expected agreement for independent anno-
tators (Cohen, 1960; Artstein and Poesio, 2008).
Note that there are several variants of the major-
ity rule, depending on how we break ties. In Ta-
ble 2, Maj1r0 is the majority rule that chooses 1 in
case the number of annotators choosing 1 is equal
to the number of annotators choosing 0 (and ac-
cordingly for Maj0r1). For 65 out of the 800 items
there has been a tie (i.e., five annotators choose 0
and another five choose 1). This means that the tie-
breaking rule used can have a significant impact
on results. Snow et al. (2008) work with a major-
ity rule where ties are broken uniformly at random
and report an observed agreement (accuracy) be-
tween the majority rule and the gold standard of
89.7%. This is confirmed by our results: 89.7%
is the mean of 87.5% (our result for Maj1r0) and
91.9% (our result for Maj0r1). If we break ties
in the optimal way (in view of approximating the
gold standard (which of course would not actu-
ally be possible without having access to that gold
standard), then we obtain an observed agreement
of 93.8%, but if we are unlucky and ties happen to
get broken in the worst possible way, we obtain an
observed agreement of only 85.6%.
For none of our bias-correcting majority rules
did we encounter any ties. Hence, for these ag-
gregators the somewhat arbitrary choices we have
to make when breaking ties are of no significance,
which is an important point in their favour. Ob-
serve that all of the bias-correcting majority rules
approximate the gold standard better than the ma-
jority rule with uniformly random tie-breaking.
</bodyText>
<page confidence="0.995104">
545
</page>
<table confidence="0.99988825">
Annotation Maj1r0 Maj0r1 ComBCM DiffBCM RelBCM GreedyCR0 GreedyCR15
Gold Standard 87.5% (.75) 91.9% (.84) 91.1% (.80) 91.5% (.81) 90.8% (.80) 86.6% (.73) 92.5% (.85)
Maj1r0 91.9% (.84) 88.9% (.76) 94.3% (.87) 94.0% (.87) 87.6% (.75) 91.5% (.83)
Maj0r1 96.0% (.91) 97.6% (.95) 96.9% (.93) 89.0% (.78) 96.1% (.92)
ComBCM 94.6% (.86) 94.4% (.86) 88.8% (.75) 93.9% (.86)
DiffBCM 98.8% (.97) 88.6% (.75) 94.8% (.88)
RelBCM 88.4% (.74) 93.8% (.86)
GreedyCR0 90.6% (.81)
</table>
<tableCaption confidence="0.999769">
Table 2: Observed agreement (and κ) between collective annotations and the gold standard.
</tableCaption>
<bodyText confidence="0.998752615384616">
Recall that the greedy consensus rule is in fact
a family of aggregators: whenever there is more
than one item with a maximal majority, we may
lock in any one of them. Furthermore, when there
is a split majority, then ties may be broken either
way. The results reported here refer to an imple-
mentation that always chooses the lexicographi-
cally first item amongst all those with a maximal
majority and that breaks ties in favour of 1. These
parameters yield neither the best or the worst ap-
proximations of the gold standard. We tested a
range of tolerance values. As an example, Table 2
includes results for tolerance values 0 and 15. The
coalition found for tolerance 0 consists of 46 an-
notators who all completely agree with the col-
lective annotation; the coalition found for toler-
ance 15 consists of 156 annotators who all dis-
agree with the collective annotation on at most
15 items. While GreedyCR0 appears to perform
rather poorly, GreedyCR15 approximates the gold
standard particularly well. This is surprising and
suggests, on the one hand, that eliminating only
the most extreme outlier annotators is a useful
strategy, and on the other hand, that a high-quality
collective annotation can be obtained from a group
of annotators that disagree substantially.11
</bodyText>
<sectionHeader confidence="0.999865" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.998456530612245">
There is an increasing number of projects using
crowdsourcing methods for labelling data. On-
line Games with a Purpose, originally conceived
by von Ahn and Dabbish (2004) to annotate im-
ages, have been used for a variety of linguis-
tic tasks: Lafourcade (2007) created JeuxDeMots
to develop a semantic network by asking players
to label words with semantically related words;
Phrase Detectives (Chamberlain et al., 2008) has
been used to gather annotations on anaphoric co-
reference; and more recently Basile et al. (2012)
11Recall that 124 out of 164 coders only annotated 20 items
each; a tolerance value of 15 thus is fairly lenient.
have developed the Wordrobe set of games for
annotating named entities, word senses, homo-
graphs, and pronouns. Similarly, crowdsourcing
via microworking sites like Amazon’s Mechanical
Turk has been used in several annotation experi-
ments related to tasks such as affect analysis, event
annotation, sense definition and word sense disam-
biguation (Snow et al., 2008; Rumshisky, 2011;
Rumshisky et al., 2012), amongst others.12
All these efforts face the problem of how to ag-
gregate the information provided by a group of
volunteers into a collective annotation. However,
by and large, the emphasis so far has been on is-
sues such as experiment design, data quality, and
costs, with little attention being paid to the aggre-
gation methods used, which are typically limited
to some form of majority vote (or taking averages
if the categories are numeric). In contrast, our fo-
cus has been on investigating different aggregation
methods for arriving at a collective annotation.
Our work has connections with the literature on
inter-annotator agreement. Agreement scores such
as kappa are used to assess the quality of an anno-
tation but do not play a direct role in constructing
one single annotation from the labellings of sev-
eral coders.13 The methods we have proposed, in
contrast, do precisely that. Still, agreement plays
a prominent role in some of these methods. In our
discussion of distance-based aggregation, we sug-
gested how agreement can be used to select a sub-
set of annotators whose individual annotations are
minimally distant from the resulting collective an-
notation. Our greedy consensus rule also makes
use of agreement to ensure a minimum level of
consensus. In both cases, the aggregators have the
effect of disregarding some outlier annotators.
</bodyText>
<footnote confidence="0.979238857142857">
12See also the papers presented at the NAACL 2010 Work-
shop on Creating Speech and Language Data with Amazon’s
Mechanical Turk (tinyurl.com/amtworkshop2010).
13Creating a gold standard often involves adjudication of
disagreements by experts, or even the removal of cases with
disagreement from the dataset. See, e.g., the papers cited by
Beigman Klebanov and Beigman (2009).
</footnote>
<page confidence="0.99716">
546
</page>
<bodyText confidence="0.999941142857143">
Other researchers have explored ways to di-
rectly identify “low-quality” annotators. For in-
stance, Snow et al. (2008) and Raykar et al. (2010)
propose Bayesian methods for identifying and cor-
recting annotators’ biases, while Ipeirotis et al.
(2010) propose an algorithm for assigning a qual-
ity score to annotators that distinguishes intrinsic
error rate from an annotator’s bias. In our ap-
proach, we do not directly rate annotators or re-
calibrate their annotations—rather, some outlier
annotators get to play a marginal role in the re-
sulting collective annotation as a side effect of the
aggregation methods themselves.
Although in our case study we have tested our
aggregators by comparing their outcomes to a gold
standard, our approach to collective annotation it-
self does not assume that there is in fact a ground
truth. Instead, we view collective annotations as
reflecting the views of a community of speakers.14
This contrasts significantly with, for instance, the
machine learning literature, where there is a fo-
cus on estimating the hidden true label from a set
of noisy labels using maximum-likelihood estima-
tors (Dawid and Skene, 1979; Smyth et al., 1995;
Raykar et al., 2010).
In application domains where it is reasonable to
assume the existence of a ground truth and where
we are able to model the manner in which individ-
ual judgments are being distorted relative to this
ground truth, social choice theory provides tools
(using again maximum-likelihood estimators) for
the design of aggregators that maximise chances
of recovering the ground truth for a given model of
distortion (Young, 1995; Conitzer and Sandholm,
2005). In recent work, Mao et al. (2013) have dis-
cussed the use of these methods in the context of
crowdsourcing. Specifically, they have designed
an experiment in which the ground truth is defined
unambiguously and known to the experiment de-
signer, so as to be able to extract realistic models
of distortion from the data collected in a crowd-
sourcing exercise.
</bodyText>
<sectionHeader confidence="0.999225" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999733305084746">
We have presented a framework for combining
the expertise of speakers taking part in large-scale
14In some domains, such as medical diagnosis, it makes
perfect sense to assume that there is a ground truth. However,
in tasks related to linguistic knowledge and language use such
an assumption seems far less justified. Hence, a collective
annotation may be the closest we can get to a representation
of the linguistic knowledge/use of a linguistic community.
annotation projects. Such projects are becoming
more and more common, due to the availability
of online crowdsourcing methods for data annota-
tion. Our work is novel in several respects. We
have drawn inspiration from the field of social
choice theory to formulate a general formal model
for aggregation problems, which we believe sheds
light on the kind of issues that arise when trying
to build annotated linguistic resources from a po-
tentially large group of annotators; and we have
proposed several families of concrete methods for
aggregating individual annotations that are more
fine-grained that the standard majority rule that so
far has been used across the board. We have tested
some of our methods on a gold standard testset for
the task of recognising textual entailment.
Our aim has been conceptual, namely to point
out that it is important for computational linguists
to reflect on the methods used when aggregat-
ing annotation information. We believe that so-
cial choice theory offers an appropriate general
methodology for supporting this reflection. Im-
portantly, this does not mean that the concrete ag-
gregation methods developed in social choice the-
ory are immediately applicable or that all the ax-
ioms typically studied in social choice theory are
necessarily relevant to aggregating linguistic an-
notations. Rather, what we claim is that it is the
methodology of social choice theory which is use-
ful: to formally state desirable properties of ag-
gregators as axioms and then to investigate which
specific aggregators satisfy them. To put it dif-
ferently: at the moment, researchers in compu-
tational linguistics simply use some given aggre-
gation methods (almost always the majority rule)
and judge their quality on how they fare in specific
experiments—but there is no principled reflection
on the methods themselves. We believe that this
should change and hope that the framework out-
lined here can provide a suitable starting point.
In future work, the framework we have pre-
sented here should be tested more extensively, not
only against a gold standard but also in terms of
the usefulness of the derived collective annotations
for training supervised learning systems. On the
theoretial side, it would be interesting to study the
axiomatic properties of the methods of aggrega-
tion we have proposed here in more depth and to
define axiomatic properties of aggregators that are
specifically tailored to the task of collective anno-
tation of linguistic resources.
</bodyText>
<page confidence="0.995704">
547
</page>
<sectionHeader confidence="0.982378" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999574660377359">
James Allen and Mark Core, 1997. DAMSL: Dialogue
Act Markup in Several Layers. Discourse Resource
Initiative.
Kenneth J. Arrow, Armatya K. Sen, and Kotaro Suzu-
mura, editors. 2002. Handbook of Social Choice
and Welfare. North-Holland.
Kenneth J. Arrow. 1963. Social Choice and Individual
Values. John Wiley and Sons, 2nd edition. First
edition published in 1951.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555–596.
Michel Balinski and Rida Laraki. 2011. Majority
Judgment: Measuring, Ranking, and Electing. MIT
Press.
Valerio Basile, Johan Bos, Kilian Evang, and Noortje
Venhuizen. 2012. A platform for collaborative se-
mantic annotation. In Proc. 13th Conference of the
European Chapter of the Association for Computa-
tional Linguistics (EACL-2012), pages 92–96.
Beata Beigman Klebanov and Eyal Beigman. 2009.
From annotator agreement to noise models. Com-
putational Linguistics, 35(4):495–503.
Johan Bos and Katja Markert. 2006. Recognising tex-
tual entailment with robust logical inference. In Ma-
chine Learning Challenges, volume 3944 of LNCS,
pages 404–426. Springer-Verlag.
Steven J. Brams and Peter C. Fishburn. 2002. Voting
procedures. In Kenneth J. Arrow, Armartya K. Sen,
and Kotaro Suzumura, editors, Handbook of Social
Choice and Welfare. North-Holland.
Jean Carletta, Stephen Isard, Anne H. Anderson,
Gwyneth Doherty-Sneddon, Amy Isard, and Jacque-
line C. Kowtko. 1997. The reliability of a dialogue
structure coding scheme. Computational Linguis-
tics, 23:13–31.
Jon Chamberlain, Massimo Poesio, and Udo Kr-
uschwitz. 2008. Addressing the resource bottleneck
to create large-scale annotated texts. In Semantics
in Text Processing. STEP 2008 Conference Proceed-
ings, volume 1 of Research in Computational Se-
mantics, pages 375–380. College Publications.
Yann Chevaleyre, Ulle Endriss, J´erˆome Lang, and
Nicolas Maudet. 2008. Preference handling in com-
binatorial domains: From AI to social choice. AI
Magazine, 29(4):37–46.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20:37–46.
Vincent Conitzer and Tuomas Sandholm. 2005. Com-
mon voting rules as maximum likelihood estimators.
In Proc. 21st Conference on Uncertainty in Artificial
Intelligence (UAI-2005).
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entail-
ment challenge. In Machine Learning Challenges,
volume 3944 of LNCS, pages 177–190. Springer-
Verlag.
Alexander Philip Dawid and Allan M. Skene. 1979.
Maximum likelihood estimation of observer error-
rates using the EM algorithm. Applied Statistics,
28(1):20–28.
Elad Dokow and Ron Holzman. 2010. Aggregation
of binary evaluations. Journal of Economic Theory,
145(2):495–511.
Ulle Endriss, Maria Silvia Pini, Francesca Rossi, and
K. Brent Venable. 2009. Preference aggrega-
tion over restricted ballot languages: Sincerity and
strategy-proofness. In Proc. 21st International Joint
Conference on Artificial Intelligence (IJCAI-2009).
Katrin Erk, Diana McCarthy, and Nicholas Gaylord.
2009. Investigations on word senses and word us-
ages. In Proc. 47th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL-2009),
pages 10–18.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions on Informa-
tion Systems, 20(1):116–131.
Gerald J. Glasser. 1959. Game theory and cumula-
tive voting for corporate directors. Management Sci-
ence, 5(2):151–156.
Umberto Grandi and Ulle Endriss. 2011. Binary ag-
gregation with integrity constraints. In Proc. 22nd
International Joint Conference on Artificial Intelli-
gence (IJCAI-2011).
Panagiotis G. Ipeirotis, Foster Provost, and Jing Wang.
2010. Quality Management on Amazon Mechanical
Turk. In Proc. 2nd Human Computation Workshop
(HCOMP-2010).
Mukund Jha, Jacob Andreas, Kapil Thadani, Sara
Rosenthal, and Kathleen McKeown. 2010. Corpus
creation for new genres: A crowdsourced approach
to PP attachment. In Proc. NAACL-HLT Workshop
on Creating Speech and Language Data with Ama-
zon’s Mechanical Turk, pages 13–20.
John Kemeny. 1959. Mathematics without numbers.
Daedalus, 88:577–591.
Adam Kilgarriff and Martha Palmer. 2000. Introduc-
tion to the special issue on senseval. Computers and
the Humanities, 34(1):1–13.
S´ebastien Konieczny and Ram´on Pino P´erez. 2002.
Merging information under constraints: A logical
framework. Journal of Logic and Computation,
12(5):773–808.
</reference>
<page confidence="0.973367">
548
</page>
<reference confidence="0.999703527777778">
Mathieu Lafourcade. 2007. Making people play for
lexical acquisition with the JeuxDeMots prototype.
In Proc. 7th International Symposium on Natural
Language Processing.
Christian List and Philip Pettit. 2002. Aggregating sets
of judgments: An impossibility result. Economics
and Philosophy, 18(1):89–110.
Andrew Mao, Ariel D. Procaccia, and Yiling Chen.
2013. Better human computation through principled
voting. In Proc. 27th AAAI Conference on Artificial
Intelligence.
George A. Miller and Walter G. Charles. 1991. Con-
textual correlates of semantic similarity. Language
and Cognitive Processes, 6(1):1–28.
Michael K. Miller and Daniel Osherson. 2009. Meth-
ods for distance-based judgment aggregation. Social
Choice and Welfare, 32(4):575–601.
Martha Palmer, Hoa Trang Dang, and Christiane
Fellbaum. 2007. Making fine-grained and
coarse-grained sense distinctions, both manually
and automatically. Natural Language Engineering,
13(2):137–163.
Maria Silvia Pini, Francesca Rossi, K. Brent Venable,
and Toby Walsh. 2009. Aggregating partially or-
dered preferences. Journal of Logic and Computa-
tion, 19(3):475–502.
Vikas Raykar, Shipeng Yu, Linda Zhao, Gerardo Her-
mosillo Valadez, Charles Florin, Luca Bogoni, and
Linda Moy. 2010. Learning from crowds. The Jour-
nal of Machine Learning Research, 11:1297–1322.
Sara Rosenthal, William Lipovsky, Kathleen McKe-
own, Kapil Thadani, and Jacob Andreas. 2010. To-
wards semi-automated annotation for prepositional
phrase attachment. In Proc. 7th International Con-
ference on Language Resources and Evaluation
(LREC-2010).
Anna Rumshisky, Nick Botchan, Sophie Kushkuley,
and James Pustejovsky. 2012. Word sense inven-
tories by non-experts. In Proc. 8th International
Conference on Language Resources and Evaluation
(LREC-2012).
Anna Rumshisky. 2011. Crowdsourcing word sense
definition. In Proc. ACL-HLT 5th Linguistic Anno-
tation Workshop (LAW-V).
Padhraic Smyth, Usama Fayyad, Michael Burl, Pietro
Perona, and Pierre Baldi. 1995. Inferring ground
truth from subjective labelling of venus images. Ad-
vances in Neural Information Processing Systems,
pages 1085–1092.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast—but is it
good? Evaluating non-expert annotations for natural
language tasks. In Proc. Conference on Empirical
Methods in Natural Language Processing (EMNLP-
2008), pages 254–263.
Alan D. Taylor. 2005. Social Choice and the Math-
ematics of Manipulation. Cambridge University
Press.
T. Nicolaus Tideman. 1987. Independence of clones
as a criterion for voting rules. Social Choice and
Welfare, 4(3):185–206.
Noortje Venhuizen, Valerio Basile, Kilian Evang, and
Johan Bos. 2013. Gamification for word sense la-
beling. In Proc. 10th International Conference on
Computational Semantics (IWCS-2013), pages 397–
403.
Luis von Ahn and Laura Dabbish. 2004. Labeling im-
ages with a computer game. In Proc. SIGCHI Con-
ference on Human Factors in Computing Systems,
pages 319–326. ACM.
H. Peyton Young. 1995. Optimal voting rules. Journal
of Economic Perspectives, 9(1):51–64.
</reference>
<page confidence="0.998739">
549
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.256395">
<title confidence="0.898811">Collective Annotation of Linguistic Basic Principles and a Formal Model</title>
<affiliation confidence="0.800959">Endriss Institute for Logic, Language &amp; University of</affiliation>
<abstract confidence="0.9994503">Crowdsourcing, which offers new ways of cheaply and quickly gathering large amounts of information contributed by volunteers online, has revolutionised the collection of labelled data. Yet, to create annotated linguistic resources from this data, we face the challenge of having to combine the judgements of a potentially large group of annotators. In this paper we investigate how to aggregate individual annotations into a single collective annotation, taking inspiration from the field of social choice theory. We formulate a general formal model for collective annotation and propose several aggregation methods that go beyond the commonly used majority rule. We test some of our methods on data from a crowdsourcing experiment on textual entailment annotation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allen</author>
<author>Mark Core</author>
</authors>
<title>DAMSL: Dialogue Act Markup in Several Layers. Discourse Resource Initiative.</title>
<date>1997</date>
<contexts>
<context position="4088" citStr="Allen and Core, 1997" startWordPosition="642" endWordPosition="645"> crowdsourcing experiment on textual entailment in Section 5. Section 6 discusses related work and Section 7 concludes. 2 Four Types of Collective Annotation An annotation task consists of a set of items, each of which is associated with a set of possible categories (Artstein and Poesio, 2008). The categories may be the same for all items or they may be itemspecific. For instance, dialogue act annotation 539 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 539–549, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics (Allen and Core, 1997; Carletta et al., 1997) and word similarity rating (Miller and Charles, 1991; Finkelstein et al., 2002) involve choosing from amongst a set of categories—acts in a dialogue act taxonomy or values on a scale, respectively— which remains fixed for all items in the annotation task. In contrast, in tasks such as word sense labelling (Kilgarriff and Palmer, 2000; Palmer et al., 2007; Venhuizen et al., 2013) and PP-attachment annotation (Rosenthal et al., 2010; Jha et al., 2010) coders need to choose a category amongst a set of options specific to each item—the possible senses of each word or the p</context>
</contexts>
<marker>Allen, Core, 1997</marker>
<rawString>James Allen and Mark Core, 1997. DAMSL: Dialogue Act Markup in Several Layers. Discourse Resource Initiative.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kenneth J Arrow</author>
<author>Armatya K Sen</author>
</authors>
<booktitle>and Kotaro Suzumura, editors. 2002. Handbook of Social Choice and Welfare.</booktitle>
<publisher>North-Holland.</publisher>
<marker>Arrow, Sen, </marker>
<rawString>Kenneth J. Arrow, Armatya K. Sen, and Kotaro Suzumura, editors. 2002. Handbook of Social Choice and Welfare. North-Holland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth J Arrow</author>
</authors>
<title>Social Choice and Individual Values.</title>
<date>1963</date>
<publisher>John Wiley and</publisher>
<note>Sons, 2nd edition. First edition published in</note>
<contexts>
<context position="14935" citStr="Arrow, 1963" startWordPosition="2566" endWordPosition="2567">the presence of a significant majority (note that this is different from anonymity: we will often not have any information on our annotators, but we may have tangible information on items).3 • F is independent if the collective annotation of any given item j only depends on the individual annotations of j. Formally, F is independent if, for every item j and every two profiles A and A&apos;, it is the case that whenever N!j.a = N!� j.a for all answers a E Aj, then F(A)j = F(A&apos;)j. In social choice theory, independence is often seen as a desirable albeit hard (or even impossible) to achieve property (Arrow, 1963). For collective annotation, we strongly believe that it is not a desirable property: by considering how annotators label other items we can learn about their biases and we should try to exploit this information to obtain the best possible annotation for the item at hand. Note that the plurality/majority rule is independent. All of the methods we shall propose in Section 4 are both anonymous and neutral—except to the extent to which we have to violate basic symmetry requirements in order to break ties between categories chosen equally often for a given item. None of our aggregators is independ</context>
</contexts>
<marker>Arrow, 1963</marker>
<rawString>Kenneth J. Arrow. 1963. Social Choice and Individual Values. John Wiley and Sons, 2nd edition. First edition published in 1951.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ron Artstein</author>
<author>Massimo Poesio</author>
</authors>
<title>Inter-coder agreement for computational linguistics.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="3762" citStr="Artstein and Poesio, 2008" startWordPosition="593" endWordPosition="596">e families of aggregation methods: bias-correcting majority rules, greedy methods for identifying (near-)consensual coalitions of annotators, and distance-based aggregators. We test the former two families of aggregators, as well as the simple majority rule commonly used in similar studies, in a case study on data extracted from a crowdsourcing experiment on textual entailment in Section 5. Section 6 discusses related work and Section 7 concludes. 2 Four Types of Collective Annotation An annotation task consists of a set of items, each of which is associated with a set of possible categories (Artstein and Poesio, 2008). The categories may be the same for all items or they may be itemspecific. For instance, dialogue act annotation 539 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 539–549, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics (Allen and Core, 1997; Carletta et al., 1997) and word similarity rating (Miller and Charles, 1991; Finkelstein et al., 2002) involve choosing from amongst a set of categories—acts in a dialogue act taxonomy or values on a scale, respectively— which remains fixed for all items in the annotation</context>
<context position="27503" citStr="Artstein and Poesio (2008)" startWordPosition="4793" endWordPosition="4796">tegrity constraints excluding some combinations, however, a distance-based rule allows for more sophisticated forms of aggregation (by choosing the optimal annotation w.r.t. all feasible annotations). We may also try to restrict the computation of distances to a subset of “reliable” annotators. Consider the following idea: If a group of annotators is (fairly) reliable, then they should have a 7This idea has been used in voting (Kemeny, 1959), belief merging (Konieczny and Pino P´erez, 2002), and judgment aggregation (Miller and Osherson, 2009). 8This δ, divided by m, is the same thing as what Artstein and Poesio (2008) call the agreement value agr, for item j. 544 (fairly) high inter-annotator agreement. By this reasoning, we should choose a group of annotators ANN ⊆ JV that maximises inter-annotator agreement in ANN and work with the aggregator argminAEA EiEANN d(A, Ai). But this is too simplistic: any singleton ANN = {i} will result in perfect agreement. That is, while we can easily maximise agreement, doing so in a naive way means ignoring most of the information collected. In other words, we face the following dilemma: • On the one hand, we should choose a small set ANN (i.e., select few annotators to b</context>
<context position="30837" citStr="Artstein and Poesio, 2008" startWordPosition="5356" endWordPosition="5359">have annotated between 20 items (124 annotators) and 800 items each (only one annotator). Nonexpert annotations with category 1 (rather than 0) are slightly more frequent (Freq(1) Pz 0.57). We have applied our aggregators to this data and compared the outcomes with each other and to the gold standard. The results are summarised in Table 2 and discussed in the sequel. For each pair we report the observed agreement Ao (proportion of items on which two annotations agree) and, in brackets, Cohen’s kappa n = Ao−Ae 1−Ae , with Ae being the expected agreement for independent annotators (Cohen, 1960; Artstein and Poesio, 2008). Note that there are several variants of the majority rule, depending on how we break ties. In Table 2, Maj1r0 is the majority rule that chooses 1 in case the number of annotators choosing 1 is equal to the number of annotators choosing 0 (and accordingly for Maj0r1). For 65 out of the 800 items there has been a tie (i.e., five annotators choose 0 and another five choose 1). This means that the tiebreaking rule used can have a significant impact on results. Snow et al. (2008) work with a majority rule where ties are broken uniformly at random and report an observed agreement (accuracy) betwee</context>
</contexts>
<marker>Artstein, Poesio, 2008</marker>
<rawString>Ron Artstein and Massimo Poesio. 2008. Inter-coder agreement for computational linguistics. Computational Linguistics, 34(4):555–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Balinski</author>
<author>Rida Laraki</author>
</authors>
<title>Majority Judgment: Measuring, Ranking, and Electing.</title>
<date>2011</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="9459" citStr="Balinski and Laraki (2011)" startWordPosition="1552" endWordPosition="1555">generalisation of the first approach, resulting in a wider range of possible methods. We shall not explore it here, but only point out that in case annotators provide linear orders over categories, there is a close resemblance to classical voting the540 ory (Taylor, 2005); in case only partial orders can be elicited, recent work in computational social choice on the generalisation of classical voting rules may prove helpful (Pini et al., 2009; Endriss et al., 2009); and in case annotators rate categories using qualitative expressions such as excellent match, the method of majority judgment of Balinski and Laraki (2011) should be considered. (3) General aggregation of plain annotations. This is the approach we shall discuss below. It is related to voting in combinatorial domains studied in computational social choice (Chevaleyre et al., 2008), and to both binary aggregation (Dokow and Holzman, 2010; Grandi and Endriss, 2011) and judgment aggregation (List and Pettit, 2002). (4) General aggregation of complex annotations. While appealing due to its great level of generality, this approach can only be tackled successfully once approaches (2) and (3) are sufficiently well understood. 3 Formal Model Next we pres</context>
</contexts>
<marker>Balinski, Laraki, 2011</marker>
<rawString>Michel Balinski and Rida Laraki. 2011. Majority Judgment: Measuring, Ranking, and Electing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valerio Basile</author>
<author>Johan Bos</author>
<author>Kilian Evang</author>
<author>Noortje Venhuizen</author>
</authors>
<title>A platform for collaborative semantic annotation.</title>
<date>2012</date>
<booktitle>In Proc. 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL-2012),</booktitle>
<pages>92--96</pages>
<contexts>
<context position="34702" citStr="Basile et al. (2012)" startWordPosition="6011" endWordPosition="6014">e annotation can be obtained from a group of annotators that disagree substantially.11 6 Related Work There is an increasing number of projects using crowdsourcing methods for labelling data. Online Games with a Purpose, originally conceived by von Ahn and Dabbish (2004) to annotate images, have been used for a variety of linguistic tasks: Lafourcade (2007) created JeuxDeMots to develop a semantic network by asking players to label words with semantically related words; Phrase Detectives (Chamberlain et al., 2008) has been used to gather annotations on anaphoric coreference; and more recently Basile et al. (2012) 11Recall that 124 out of 164 coders only annotated 20 items each; a tolerance value of 15 thus is fairly lenient. have developed the Wordrobe set of games for annotating named entities, word senses, homographs, and pronouns. Similarly, crowdsourcing via microworking sites like Amazon’s Mechanical Turk has been used in several annotation experiments related to tasks such as affect analysis, event annotation, sense definition and word sense disambiguation (Snow et al., 2008; Rumshisky, 2011; Rumshisky et al., 2012), amongst others.12 All these efforts face the problem of how to aggregate the in</context>
</contexts>
<marker>Basile, Bos, Evang, Venhuizen, 2012</marker>
<rawString>Valerio Basile, Johan Bos, Kilian Evang, and Noortje Venhuizen. 2012. A platform for collaborative semantic annotation. In Proc. 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL-2012), pages 92–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beata Beigman Klebanov</author>
<author>Eyal Beigman</author>
</authors>
<title>From annotator agreement to noise models.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>4</issue>
<contexts>
<context position="36945" citStr="Klebanov and Beigman (2009)" startWordPosition="6367" endWordPosition="6370">otations are minimally distant from the resulting collective annotation. Our greedy consensus rule also makes use of agreement to ensure a minimum level of consensus. In both cases, the aggregators have the effect of disregarding some outlier annotators. 12See also the papers presented at the NAACL 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk (tinyurl.com/amtworkshop2010). 13Creating a gold standard often involves adjudication of disagreements by experts, or even the removal of cases with disagreement from the dataset. See, e.g., the papers cited by Beigman Klebanov and Beigman (2009). 546 Other researchers have explored ways to directly identify “low-quality” annotators. For instance, Snow et al. (2008) and Raykar et al. (2010) propose Bayesian methods for identifying and correcting annotators’ biases, while Ipeirotis et al. (2010) propose an algorithm for assigning a quality score to annotators that distinguishes intrinsic error rate from an annotator’s bias. In our approach, we do not directly rate annotators or recalibrate their annotations—rather, some outlier annotators get to play a marginal role in the resulting collective annotation as a side effect of the aggrega</context>
</contexts>
<marker>Klebanov, Beigman, 2009</marker>
<rawString>Beata Beigman Klebanov and Eyal Beigman. 2009. From annotator agreement to noise models. Computational Linguistics, 35(4):495–503.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Katja Markert</author>
</authors>
<title>Recognising textual entailment with robust logical inference.</title>
<date>2006</date>
<booktitle>In Machine Learning Challenges,</booktitle>
<volume>3944</volume>
<pages>404--426</pages>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="29472" citStr="Bos and Markert (2006)" startWordPosition="5132" endWordPosition="5135">L Recognizing Textual Entailment (RTE) Challenge. RTE is a binary classification task consisting in judging whether the meaning of a piece of text (the so-called hypothesis) can be inferred from another piece of text (the entailing text). The original RTE1 Challenge testset consists of 800 text-hypothesis pairs (such as T: “Chr´etien visited Peugeot’s newly renovated car factory”, H: “Peugeot manufactures cars”) with a gold standard annotation that classifies each item as either true (1)—in case H can be inferred from T— or false (0). Exactly 400 items are annotated as 0 and exactly 400 as 1. Bos and Markert (2006) performed an independent expert annotation of 9GreedyCRt is a greedy (rather than optimal) implementation of this basic idea, with the tolerance value t fixing a threshold on (a particular form of) inter-annotator agreement. 10Since the annotation task and dataset used for our case study do not involve any interesting integrity constraints, we have not tested any distance-based aggregation rules. this testset, obtaining 95% agreement between the RTE1 gold standard and their own annotation. The dataset of Snow et al. (2008) includes 10 non-expert annotations for each of the 800 items in the RT</context>
</contexts>
<marker>Bos, Markert, 2006</marker>
<rawString>Johan Bos and Katja Markert. 2006. Recognising textual entailment with robust logical inference. In Machine Learning Challenges, volume 3944 of LNCS, pages 404–426. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven J Brams</author>
<author>Peter C Fishburn</author>
</authors>
<title>Voting procedures.</title>
<date>2002</date>
<booktitle>Handbook of Social Choice and Welfare.</booktitle>
<editor>In Kenneth J. Arrow, Armartya K. Sen, and Kotaro Suzumura, editors,</editor>
<publisher>North-Holland.</publisher>
<contexts>
<context position="20499" citStr="Brams and Fishburn, 2002" startWordPosition="3534" endWordPosition="3537"> rule (ComBCM) is defined by weights wXi = Freqi(1−X). That is, the weight of annotator i for category X is equal to her relative frequency of having chosen the other category 1−X. For example, if you annotate two items with 1 and eight with 0, then each of your 1-annotations will have weight 0.8, while each of your 0-annotations will only have weight 0.2. (2) The difference-based BCM rule (DiffBCM) is defined by weights wXi = 1 + Freq(X) − 4A similar idea is at the heart of cumulative voting, which requires a voter to distribute a fixed number of points amongst the candidates (Glasser, 1959; Brams and Fishburn, 2002). 5For the sake of simplicity, our description here presupposes that ties are always broken in favour of 0. Other tiebreaking rules (e.g., random tie-breaking) are possible. Freqi(X). Recall that Freq(X) is the relative frequency of X in the entire profile, while Freqi(X) is the relative frequency of X in the annotation of i. Hence, if i assigns category X less often than the general population, then her weight on X-choices will be increased by the difference (and vice versa in case she assigns X more often than the population at large). For example, if you assign 1 in two out of ten cases, wh</context>
</contexts>
<marker>Brams, Fishburn, 2002</marker>
<rawString>Steven J. Brams and Peter C. Fishburn. 2002. Voting procedures. In Kenneth J. Arrow, Armartya K. Sen, and Kotaro Suzumura, editors, Handbook of Social Choice and Welfare. North-Holland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
<author>Stephen Isard</author>
<author>Anne H Anderson</author>
<author>Gwyneth Doherty-Sneddon</author>
<author>Amy Isard</author>
<author>Jacqueline C Kowtko</author>
</authors>
<title>The reliability of a dialogue structure coding scheme.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--13</pages>
<contexts>
<context position="4112" citStr="Carletta et al., 1997" startWordPosition="646" endWordPosition="649">ent on textual entailment in Section 5. Section 6 discusses related work and Section 7 concludes. 2 Four Types of Collective Annotation An annotation task consists of a set of items, each of which is associated with a set of possible categories (Artstein and Poesio, 2008). The categories may be the same for all items or they may be itemspecific. For instance, dialogue act annotation 539 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 539–549, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics (Allen and Core, 1997; Carletta et al., 1997) and word similarity rating (Miller and Charles, 1991; Finkelstein et al., 2002) involve choosing from amongst a set of categories—acts in a dialogue act taxonomy or values on a scale, respectively— which remains fixed for all items in the annotation task. In contrast, in tasks such as word sense labelling (Kilgarriff and Palmer, 2000; Palmer et al., 2007; Venhuizen et al., 2013) and PP-attachment annotation (Rosenthal et al., 2010; Jha et al., 2010) coders need to choose a category amongst a set of options specific to each item—the possible senses of each word or the possible attachment point</context>
</contexts>
<marker>Carletta, Isard, Anderson, Doherty-Sneddon, Isard, Kowtko, 1997</marker>
<rawString>Jean Carletta, Stephen Isard, Anne H. Anderson, Gwyneth Doherty-Sneddon, Amy Isard, and Jacqueline C. Kowtko. 1997. The reliability of a dialogue structure coding scheme. Computational Linguistics, 23:13–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon Chamberlain</author>
<author>Massimo Poesio</author>
<author>Udo Kruschwitz</author>
</authors>
<title>Addressing the resource bottleneck to create large-scale annotated texts.</title>
<date>2008</date>
<booktitle>In Semantics in Text Processing. STEP 2008 Conference Proceedings,</booktitle>
<volume>1</volume>
<pages>375--380</pages>
<publisher>College Publications.</publisher>
<contexts>
<context position="34601" citStr="Chamberlain et al., 2008" startWordPosition="5994" endWordPosition="5997">most extreme outlier annotators is a useful strategy, and on the other hand, that a high-quality collective annotation can be obtained from a group of annotators that disagree substantially.11 6 Related Work There is an increasing number of projects using crowdsourcing methods for labelling data. Online Games with a Purpose, originally conceived by von Ahn and Dabbish (2004) to annotate images, have been used for a variety of linguistic tasks: Lafourcade (2007) created JeuxDeMots to develop a semantic network by asking players to label words with semantically related words; Phrase Detectives (Chamberlain et al., 2008) has been used to gather annotations on anaphoric coreference; and more recently Basile et al. (2012) 11Recall that 124 out of 164 coders only annotated 20 items each; a tolerance value of 15 thus is fairly lenient. have developed the Wordrobe set of games for annotating named entities, word senses, homographs, and pronouns. Similarly, crowdsourcing via microworking sites like Amazon’s Mechanical Turk has been used in several annotation experiments related to tasks such as affect analysis, event annotation, sense definition and word sense disambiguation (Snow et al., 2008; Rumshisky, 2011; Rum</context>
</contexts>
<marker>Chamberlain, Poesio, Kruschwitz, 2008</marker>
<rawString>Jon Chamberlain, Massimo Poesio, and Udo Kruschwitz. 2008. Addressing the resource bottleneck to create large-scale annotated texts. In Semantics in Text Processing. STEP 2008 Conference Proceedings, volume 1 of Research in Computational Semantics, pages 375–380. College Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yann Chevaleyre</author>
<author>Ulle Endriss</author>
<author>J´erˆome Lang</author>
<author>Nicolas Maudet</author>
</authors>
<title>Preference handling in combinatorial domains: From AI to social choice.</title>
<date>2008</date>
<journal>AI Magazine,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="9686" citStr="Chevaleyre et al., 2008" startWordPosition="1588" endWordPosition="1591"> classical voting the540 ory (Taylor, 2005); in case only partial orders can be elicited, recent work in computational social choice on the generalisation of classical voting rules may prove helpful (Pini et al., 2009; Endriss et al., 2009); and in case annotators rate categories using qualitative expressions such as excellent match, the method of majority judgment of Balinski and Laraki (2011) should be considered. (3) General aggregation of plain annotations. This is the approach we shall discuss below. It is related to voting in combinatorial domains studied in computational social choice (Chevaleyre et al., 2008), and to both binary aggregation (Dokow and Holzman, 2010; Grandi and Endriss, 2011) and judgment aggregation (List and Pettit, 2002). (4) General aggregation of complex annotations. While appealing due to its great level of generality, this approach can only be tackled successfully once approaches (2) and (3) are sufficiently well understood. 3 Formal Model Next we present our model for general aggregation of plain annotations into a collective annotation. 3.1 Terminology and Notation An annotation task is defined in terms of m items, with each item j E {1, ... , m} being associated with a fi</context>
</contexts>
<marker>Chevaleyre, Endriss, Lang, Maudet, 2008</marker>
<rawString>Yann Chevaleyre, Ulle Endriss, J´erˆome Lang, and Nicolas Maudet. 2008. Preference handling in combinatorial domains: From AI to social choice. AI Magazine, 29(4):37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measurement,</booktitle>
<pages>20--37</pages>
<contexts>
<context position="30809" citStr="Cohen, 1960" startWordPosition="5354" endWordPosition="5355">notators who have annotated between 20 items (124 annotators) and 800 items each (only one annotator). Nonexpert annotations with category 1 (rather than 0) are slightly more frequent (Freq(1) Pz 0.57). We have applied our aggregators to this data and compared the outcomes with each other and to the gold standard. The results are summarised in Table 2 and discussed in the sequel. For each pair we report the observed agreement Ao (proportion of items on which two annotations agree) and, in brackets, Cohen’s kappa n = Ao−Ae 1−Ae , with Ae being the expected agreement for independent annotators (Cohen, 1960; Artstein and Poesio, 2008). Note that there are several variants of the majority rule, depending on how we break ties. In Table 2, Maj1r0 is the majority rule that chooses 1 in case the number of annotators choosing 1 is equal to the number of annotators choosing 0 (and accordingly for Maj0r1). For 65 out of the 800 items there has been a tie (i.e., five annotators choose 0 and another five choose 1). This means that the tiebreaking rule used can have a significant impact on results. Snow et al. (2008) work with a majority rule where ties are broken uniformly at random and report an observed</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20:37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Conitzer</author>
<author>Tuomas Sandholm</author>
</authors>
<title>Common voting rules as maximum likelihood estimators.</title>
<date>2005</date>
<booktitle>In Proc. 21st Conference on Uncertainty in Artificial Intelligence (UAI-2005).</booktitle>
<contexts>
<context position="38587" citStr="Conitzer and Sandholm, 2005" startWordPosition="6630" endWordPosition="6633">ere is a focus on estimating the hidden true label from a set of noisy labels using maximum-likelihood estimators (Dawid and Skene, 1979; Smyth et al., 1995; Raykar et al., 2010). In application domains where it is reasonable to assume the existence of a ground truth and where we are able to model the manner in which individual judgments are being distorted relative to this ground truth, social choice theory provides tools (using again maximum-likelihood estimators) for the design of aggregators that maximise chances of recovering the ground truth for a given model of distortion (Young, 1995; Conitzer and Sandholm, 2005). In recent work, Mao et al. (2013) have discussed the use of these methods in the context of crowdsourcing. Specifically, they have designed an experiment in which the ground truth is defined unambiguously and known to the experiment designer, so as to be able to extract realistic models of distortion from the data collected in a crowdsourcing exercise. 7 Conclusions We have presented a framework for combining the expertise of speakers taking part in large-scale 14In some domains, such as medical diagnosis, it makes perfect sense to assume that there is a ground truth. However, in tasks relat</context>
</contexts>
<marker>Conitzer, Sandholm, 2005</marker>
<rawString>Vincent Conitzer and Tuomas Sandholm. 2005. Common voting rules as maximum likelihood estimators. In Proc. 21st Conference on Uncertainty in Artificial Intelligence (UAI-2005).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The PASCAL recognising textual entailment challenge.</title>
<date>2006</date>
<booktitle>In Machine Learning Challenges,</booktitle>
<volume>3944</volume>
<pages>177--190</pages>
<publisher>SpringerVerlag.</publisher>
<contexts>
<context position="28837" citStr="Dagan et al. (2006)" startWordPosition="5028" endWordPosition="5031">ken into account. • On the other hand, we should choose a large set ANN (i.e., select many annotators to base our collective annotation on), as that will increase the amount of information exploited. One pragmatic approach is to fix a minimum quality threshold regarding one of the two dimensions and optimise in view of the other.9 5 A Case Study In this section, we report on a case study in which we have tested our bias-correcting majority and greedy consensus rules.10 We have used the dataset created by Snow et al. (2008) for the task of recognising textual entailment, originally proposed by Dagan et al. (2006) in the PASCAL Recognizing Textual Entailment (RTE) Challenge. RTE is a binary classification task consisting in judging whether the meaning of a piece of text (the so-called hypothesis) can be inferred from another piece of text (the entailing text). The original RTE1 Challenge testset consists of 800 text-hypothesis pairs (such as T: “Chr´etien visited Peugeot’s newly renovated car factory”, H: “Peugeot manufactures cars”) with a gold standard annotation that classifies each item as either true (1)—in case H can be inferred from T— or false (0). Exactly 400 items are annotated as 0 and exact</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The PASCAL recognising textual entailment challenge. In Machine Learning Challenges, volume 3944 of LNCS, pages 177–190. SpringerVerlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Philip Dawid</author>
<author>Allan M Skene</author>
</authors>
<title>Maximum likelihood estimation of observer errorrates using the EM algorithm.</title>
<date>1979</date>
<journal>Applied Statistics,</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="38095" citStr="Dawid and Skene, 1979" startWordPosition="6551" endWordPosition="6554"> in the resulting collective annotation as a side effect of the aggregation methods themselves. Although in our case study we have tested our aggregators by comparing their outcomes to a gold standard, our approach to collective annotation itself does not assume that there is in fact a ground truth. Instead, we view collective annotations as reflecting the views of a community of speakers.14 This contrasts significantly with, for instance, the machine learning literature, where there is a focus on estimating the hidden true label from a set of noisy labels using maximum-likelihood estimators (Dawid and Skene, 1979; Smyth et al., 1995; Raykar et al., 2010). In application domains where it is reasonable to assume the existence of a ground truth and where we are able to model the manner in which individual judgments are being distorted relative to this ground truth, social choice theory provides tools (using again maximum-likelihood estimators) for the design of aggregators that maximise chances of recovering the ground truth for a given model of distortion (Young, 1995; Conitzer and Sandholm, 2005). In recent work, Mao et al. (2013) have discussed the use of these methods in the context of crowdsourcing.</context>
</contexts>
<marker>Dawid, Skene, 1979</marker>
<rawString>Alexander Philip Dawid and Allan M. Skene. 1979. Maximum likelihood estimation of observer errorrates using the EM algorithm. Applied Statistics, 28(1):20–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elad Dokow</author>
<author>Ron Holzman</author>
</authors>
<title>Aggregation of binary evaluations.</title>
<date>2010</date>
<journal>Journal of Economic Theory,</journal>
<volume>145</volume>
<issue>2</issue>
<contexts>
<context position="9743" citStr="Dokow and Holzman, 2010" startWordPosition="1597" endWordPosition="1600">partial orders can be elicited, recent work in computational social choice on the generalisation of classical voting rules may prove helpful (Pini et al., 2009; Endriss et al., 2009); and in case annotators rate categories using qualitative expressions such as excellent match, the method of majority judgment of Balinski and Laraki (2011) should be considered. (3) General aggregation of plain annotations. This is the approach we shall discuss below. It is related to voting in combinatorial domains studied in computational social choice (Chevaleyre et al., 2008), and to both binary aggregation (Dokow and Holzman, 2010; Grandi and Endriss, 2011) and judgment aggregation (List and Pettit, 2002). (4) General aggregation of complex annotations. While appealing due to its great level of generality, this approach can only be tackled successfully once approaches (2) and (3) are sufficiently well understood. 3 Formal Model Next we present our model for general aggregation of plain annotations into a collective annotation. 3.1 Terminology and Notation An annotation task is defined in terms of m items, with each item j E {1, ... , m} being associated with a finite set of possible categories Cj. Annotators are asked </context>
</contexts>
<marker>Dokow, Holzman, 2010</marker>
<rawString>Elad Dokow and Ron Holzman. 2010. Aggregation of binary evaluations. Journal of Economic Theory, 145(2):495–511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulle Endriss</author>
<author>Maria Silvia Pini</author>
<author>Francesca Rossi</author>
<author>K Brent Venable</author>
</authors>
<title>Preference aggregation over restricted ballot languages: Sincerity and strategy-proofness.</title>
<date>2009</date>
<booktitle>In Proc. 21st International Joint Conference on Artificial Intelligence (IJCAI-2009).</booktitle>
<contexts>
<context position="9302" citStr="Endriss et al., 2009" startWordPosition="1527" endWordPosition="1530">s the most common approach in the literature (see, e.g., Venhuizen et al., 2013). (2) Independent aggregation of complex annotations. This is a natural generalisation of the first approach, resulting in a wider range of possible methods. We shall not explore it here, but only point out that in case annotators provide linear orders over categories, there is a close resemblance to classical voting the540 ory (Taylor, 2005); in case only partial orders can be elicited, recent work in computational social choice on the generalisation of classical voting rules may prove helpful (Pini et al., 2009; Endriss et al., 2009); and in case annotators rate categories using qualitative expressions such as excellent match, the method of majority judgment of Balinski and Laraki (2011) should be considered. (3) General aggregation of plain annotations. This is the approach we shall discuss below. It is related to voting in combinatorial domains studied in computational social choice (Chevaleyre et al., 2008), and to both binary aggregation (Dokow and Holzman, 2010; Grandi and Endriss, 2011) and judgment aggregation (List and Pettit, 2002). (4) General aggregation of complex annotations. While appealing due to its great </context>
</contexts>
<marker>Endriss, Pini, Rossi, Venable, 2009</marker>
<rawString>Ulle Endriss, Maria Silvia Pini, Francesca Rossi, and K. Brent Venable. 2009. Preference aggregation over restricted ballot languages: Sincerity and strategy-proofness. In Proc. 21st International Joint Conference on Artificial Intelligence (IJCAI-2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Diana McCarthy</author>
<author>Nicholas Gaylord</author>
</authors>
<title>Investigations on word senses and word usages.</title>
<date>2009</date>
<booktitle>In Proc. 47th Annual Meeting of the Association for Computational Linguistics (ACL-2009),</booktitle>
<pages>10--18</pages>
<contexts>
<context position="6523" citStr="Erk et al. (2009)" startWordPosition="1054" endWordPosition="1057"> to provide a qualitative ratings of the available categories for each item (e.g., excellent match, good match, etc.); or we may ask for quantitative ratings (e.g., numbers from 1 to 100).1 We refer to these forms of annotation as complex annotation. We want to investigate how to aggregate the information available for each item once annotations by multiple annotators have been collected. In line with the terminology used in social choice theory and particularly judgment aggregation (Ar1Some authors have combined qualitative and quantitative ratings; e.g., for the Graded Word Sense dataset of Erk et al. (2009) coders were asked to classify each relevant WordNet sense for a given item on a 5-point scale: 1 completely different, 2 mostly different, 3 similar, 4 very similar, 5 identical. row, 1963; List and Pettit, 2002), let us call an aggregation method independent if the outcome regarding a given item j only depends on the categories provided by the annotators regarding j itself (but not on, say, the categories assigned to a different item j&apos;). Independent aggregation methods are attractive due to their simplicity. They also have some conceptual appeal: when deciding on j maybe we should only conc</context>
</contexts>
<marker>Erk, McCarthy, Gaylord, 2009</marker>
<rawString>Katrin Erk, Diana McCarthy, and Nicholas Gaylord. 2009. Investigations on word senses and word usages. In Proc. 47th Annual Meeting of the Association for Computational Linguistics (ACL-2009), pages 10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="4192" citStr="Finkelstein et al., 2002" startWordPosition="658" endWordPosition="661">Section 7 concludes. 2 Four Types of Collective Annotation An annotation task consists of a set of items, each of which is associated with a set of possible categories (Artstein and Poesio, 2008). The categories may be the same for all items or they may be itemspecific. For instance, dialogue act annotation 539 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 539–549, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics (Allen and Core, 1997; Carletta et al., 1997) and word similarity rating (Miller and Charles, 1991; Finkelstein et al., 2002) involve choosing from amongst a set of categories—acts in a dialogue act taxonomy or values on a scale, respectively— which remains fixed for all items in the annotation task. In contrast, in tasks such as word sense labelling (Kilgarriff and Palmer, 2000; Palmer et al., 2007; Venhuizen et al., 2013) and PP-attachment annotation (Rosenthal et al., 2010; Jha et al., 2010) coders need to choose a category amongst a set of options specific to each item—the possible senses of each word or the possible attachment points in each sentence with a prepositional phrase. In either case (one set of categ</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2002</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2002. Placing search in context: The concept revisited. ACM Transactions on Information Systems, 20(1):116–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald J Glasser</author>
</authors>
<title>Game theory and cumulative voting for corporate directors.</title>
<date>1959</date>
<journal>Management Science,</journal>
<volume>5</volume>
<issue>2</issue>
<contexts>
<context position="20472" citStr="Glasser, 1959" startWordPosition="3532" endWordPosition="3533">ement-based BCM rule (ComBCM) is defined by weights wXi = Freqi(1−X). That is, the weight of annotator i for category X is equal to her relative frequency of having chosen the other category 1−X. For example, if you annotate two items with 1 and eight with 0, then each of your 1-annotations will have weight 0.8, while each of your 0-annotations will only have weight 0.2. (2) The difference-based BCM rule (DiffBCM) is defined by weights wXi = 1 + Freq(X) − 4A similar idea is at the heart of cumulative voting, which requires a voter to distribute a fixed number of points amongst the candidates (Glasser, 1959; Brams and Fishburn, 2002). 5For the sake of simplicity, our description here presupposes that ties are always broken in favour of 0. Other tiebreaking rules (e.g., random tie-breaking) are possible. Freqi(X). Recall that Freq(X) is the relative frequency of X in the entire profile, while Freqi(X) is the relative frequency of X in the annotation of i. Hence, if i assigns category X less often than the general population, then her weight on X-choices will be increased by the difference (and vice versa in case she assigns X more often than the population at large). For example, if you assign 1 </context>
</contexts>
<marker>Glasser, 1959</marker>
<rawString>Gerald J. Glasser. 1959. Game theory and cumulative voting for corporate directors. Management Science, 5(2):151–156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Umberto Grandi</author>
<author>Ulle Endriss</author>
</authors>
<title>Binary aggregation with integrity constraints.</title>
<date>2011</date>
<booktitle>In Proc. 22nd International Joint Conference on Artificial Intelligence (IJCAI-2011).</booktitle>
<contexts>
<context position="9770" citStr="Grandi and Endriss, 2011" startWordPosition="1601" endWordPosition="1604">cited, recent work in computational social choice on the generalisation of classical voting rules may prove helpful (Pini et al., 2009; Endriss et al., 2009); and in case annotators rate categories using qualitative expressions such as excellent match, the method of majority judgment of Balinski and Laraki (2011) should be considered. (3) General aggregation of plain annotations. This is the approach we shall discuss below. It is related to voting in combinatorial domains studied in computational social choice (Chevaleyre et al., 2008), and to both binary aggregation (Dokow and Holzman, 2010; Grandi and Endriss, 2011) and judgment aggregation (List and Pettit, 2002). (4) General aggregation of complex annotations. While appealing due to its great level of generality, this approach can only be tackled successfully once approaches (2) and (3) are sufficiently well understood. 3 Formal Model Next we present our model for general aggregation of plain annotations into a collective annotation. 3.1 Terminology and Notation An annotation task is defined in terms of m items, with each item j E {1, ... , m} being associated with a finite set of possible categories Cj. Annotators are asked to provide an answer for ea</context>
<context position="15769" citStr="Grandi and Endriss, 2011" startWordPosition="2705" endWordPosition="2708">on to obtain the best possible annotation for the item at hand. Note that the plurality/majority rule is independent. All of the methods we shall propose in Section 4 are both anonymous and neutral—except to the extent to which we have to violate basic symmetry requirements in order to break ties between categories chosen equally often for a given item. None of our aggregators is independent. 3It would also be of interest to formulate a neutrality axiom w.r.t. categories (rather than items). For two categories, this idea has been discussed under the name of domainneutrality in the literature (Grandi and Endriss, 2011), but for larger sets of categories it has not yet been explored. Some annotation tasks might be subject to integrity constraints that determine the internal consistency of an annotation. For example, if our items are pairs of words and the possible categories include synonymous and antonymous, then if item 1 is about words A and B, item 2 about words B and C, and item 3 about words A and C, then any annotation that labels items 1 and 2 as synonymous should not label item 3 as antonymous. Thus, a further desirable property that will play a role for some annotation tasks is collective rationali</context>
<context position="17038" citStr="Grandi and Endriss, 2011" startWordPosition="2924" endWordPosition="2927">dual annotations respect a given integrity constraint, then so should the collective annotation. We can think of integrity constraints as imposing top-down expert knowledge on an annotation. However, for some annotation tasks, no integrity constraints may be known to us in advance, even though we may have reasons to believe that the individual annotators do respect some such constraints. In that case, selecting one of the individual annotations in the profile as the collective annotation is the only way to ensure that these integrity constraints will be satisfied by the collective annotation (Grandi and Endriss, 2011). Of course, to do so we would need to assume that there is at least one annotator who has labelled all items (and to be able to design a high-quality aggregator in this way we should have a sufficiently large number of such annotators to choose from), which may not always be possible, particularly in the context of crowdsourcing. 4 Three Families of Aggregators In this section we instantiate our formal model by proposing three families of methods for aggregation. Each of them is inspired, in part, by standard approaches to desigining aggregation rules developed in social choice theory and, in</context>
</contexts>
<marker>Grandi, Endriss, 2011</marker>
<rawString>Umberto Grandi and Ulle Endriss. 2011. Binary aggregation with integrity constraints. In Proc. 22nd International Joint Conference on Artificial Intelligence (IJCAI-2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Panagiotis G Ipeirotis</author>
<author>Foster Provost</author>
<author>Jing Wang</author>
</authors>
<title>Quality Management on Amazon Mechanical Turk.</title>
<date>2010</date>
<booktitle>In Proc. 2nd Human Computation Workshop (HCOMP-2010).</booktitle>
<contexts>
<context position="37198" citStr="Ipeirotis et al. (2010)" startWordPosition="6406" endWordPosition="6409">See also the papers presented at the NAACL 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk (tinyurl.com/amtworkshop2010). 13Creating a gold standard often involves adjudication of disagreements by experts, or even the removal of cases with disagreement from the dataset. See, e.g., the papers cited by Beigman Klebanov and Beigman (2009). 546 Other researchers have explored ways to directly identify “low-quality” annotators. For instance, Snow et al. (2008) and Raykar et al. (2010) propose Bayesian methods for identifying and correcting annotators’ biases, while Ipeirotis et al. (2010) propose an algorithm for assigning a quality score to annotators that distinguishes intrinsic error rate from an annotator’s bias. In our approach, we do not directly rate annotators or recalibrate their annotations—rather, some outlier annotators get to play a marginal role in the resulting collective annotation as a side effect of the aggregation methods themselves. Although in our case study we have tested our aggregators by comparing their outcomes to a gold standard, our approach to collective annotation itself does not assume that there is in fact a ground truth. Instead, we view collec</context>
</contexts>
<marker>Ipeirotis, Provost, Wang, 2010</marker>
<rawString>Panagiotis G. Ipeirotis, Foster Provost, and Jing Wang. 2010. Quality Management on Amazon Mechanical Turk. In Proc. 2nd Human Computation Workshop (HCOMP-2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mukund Jha</author>
<author>Jacob Andreas</author>
<author>Kapil Thadani</author>
<author>Sara Rosenthal</author>
<author>Kathleen McKeown</author>
</authors>
<title>Corpus creation for new genres: A crowdsourced approach to PP attachment.</title>
<date>2010</date>
<booktitle>In Proc. NAACL-HLT Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk,</booktitle>
<pages>13--20</pages>
<contexts>
<context position="4566" citStr="Jha et al., 2010" startWordPosition="720" endWordPosition="723">tional Linguistics, pages 539–549, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics (Allen and Core, 1997; Carletta et al., 1997) and word similarity rating (Miller and Charles, 1991; Finkelstein et al., 2002) involve choosing from amongst a set of categories—acts in a dialogue act taxonomy or values on a scale, respectively— which remains fixed for all items in the annotation task. In contrast, in tasks such as word sense labelling (Kilgarriff and Palmer, 2000; Palmer et al., 2007; Venhuizen et al., 2013) and PP-attachment annotation (Rosenthal et al., 2010; Jha et al., 2010) coders need to choose a category amongst a set of options specific to each item—the possible senses of each word or the possible attachment points in each sentence with a prepositional phrase. In either case (one set of categories for all items vs. item-specific sets of categories), annotators are typically asked to identify, for each item, the category they consider the best match. In addition, they may be given the opportunity to indicate that they cannot judge (the “don’t know” or “unclear” category). For large-scale annotation projects run over the Internet it is furthermore very likely t</context>
</contexts>
<marker>Jha, Andreas, Thadani, Rosenthal, McKeown, 2010</marker>
<rawString>Mukund Jha, Jacob Andreas, Kapil Thadani, Sara Rosenthal, and Kathleen McKeown. 2010. Corpus creation for new genres: A crowdsourced approach to PP attachment. In Proc. NAACL-HLT Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 13–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Kemeny</author>
</authors>
<title>Mathematics without numbers.</title>
<date>1959</date>
<tech>Daedalus, 88:577–591.</tech>
<contexts>
<context position="27322" citStr="Kemeny, 1959" startWordPosition="4766" endWordPosition="4767">nce to the profile. Note that FH is simply the plurality rule. This is so because every element of the Cartesian product is a possible annotation. In the presence of integrity constraints excluding some combinations, however, a distance-based rule allows for more sophisticated forms of aggregation (by choosing the optimal annotation w.r.t. all feasible annotations). We may also try to restrict the computation of distances to a subset of “reliable” annotators. Consider the following idea: If a group of annotators is (fairly) reliable, then they should have a 7This idea has been used in voting (Kemeny, 1959), belief merging (Konieczny and Pino P´erez, 2002), and judgment aggregation (Miller and Osherson, 2009). 8This δ, divided by m, is the same thing as what Artstein and Poesio (2008) call the agreement value agr, for item j. 544 (fairly) high inter-annotator agreement. By this reasoning, we should choose a group of annotators ANN ⊆ JV that maximises inter-annotator agreement in ANN and work with the aggregator argminAEA EiEANN d(A, Ai). But this is too simplistic: any singleton ANN = {i} will result in perfect agreement. That is, while we can easily maximise agreement, doing so in a naive way m</context>
</contexts>
<marker>Kemeny, 1959</marker>
<rawString>John Kemeny. 1959. Mathematics without numbers. Daedalus, 88:577–591.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Martha Palmer</author>
</authors>
<title>Introduction to the special issue on senseval.</title>
<date>2000</date>
<booktitle>Computers and the Humanities,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="4448" citStr="Kilgarriff and Palmer, 2000" startWordPosition="701" endWordPosition="704">be itemspecific. For instance, dialogue act annotation 539 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 539–549, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics (Allen and Core, 1997; Carletta et al., 1997) and word similarity rating (Miller and Charles, 1991; Finkelstein et al., 2002) involve choosing from amongst a set of categories—acts in a dialogue act taxonomy or values on a scale, respectively— which remains fixed for all items in the annotation task. In contrast, in tasks such as word sense labelling (Kilgarriff and Palmer, 2000; Palmer et al., 2007; Venhuizen et al., 2013) and PP-attachment annotation (Rosenthal et al., 2010; Jha et al., 2010) coders need to choose a category amongst a set of options specific to each item—the possible senses of each word or the possible attachment points in each sentence with a prepositional phrase. In either case (one set of categories for all items vs. item-specific sets of categories), annotators are typically asked to identify, for each item, the category they consider the best match. In addition, they may be given the opportunity to indicate that they cannot judge (the “don’t k</context>
</contexts>
<marker>Kilgarriff, Palmer, 2000</marker>
<rawString>Adam Kilgarriff and Martha Palmer. 2000. Introduction to the special issue on senseval. Computers and the Humanities, 34(1):1–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S´ebastien Konieczny</author>
<author>Ram´on Pino P´erez</author>
</authors>
<title>Merging information under constraints: A logical framework.</title>
<date>2002</date>
<journal>Journal of Logic and Computation,</journal>
<volume>12</volume>
<issue>5</issue>
<marker>Konieczny, P´erez, 2002</marker>
<rawString>S´ebastien Konieczny and Ram´on Pino P´erez. 2002. Merging information under constraints: A logical framework. Journal of Logic and Computation, 12(5):773–808.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathieu Lafourcade</author>
</authors>
<title>Making people play for lexical acquisition with the JeuxDeMots prototype.</title>
<date>2007</date>
<booktitle>In Proc. 7th International Symposium on Natural Language Processing.</booktitle>
<contexts>
<context position="34441" citStr="Lafourcade (2007)" startWordPosition="5973" endWordPosition="5974">rather poorly, GreedyCR15 approximates the gold standard particularly well. This is surprising and suggests, on the one hand, that eliminating only the most extreme outlier annotators is a useful strategy, and on the other hand, that a high-quality collective annotation can be obtained from a group of annotators that disagree substantially.11 6 Related Work There is an increasing number of projects using crowdsourcing methods for labelling data. Online Games with a Purpose, originally conceived by von Ahn and Dabbish (2004) to annotate images, have been used for a variety of linguistic tasks: Lafourcade (2007) created JeuxDeMots to develop a semantic network by asking players to label words with semantically related words; Phrase Detectives (Chamberlain et al., 2008) has been used to gather annotations on anaphoric coreference; and more recently Basile et al. (2012) 11Recall that 124 out of 164 coders only annotated 20 items each; a tolerance value of 15 thus is fairly lenient. have developed the Wordrobe set of games for annotating named entities, word senses, homographs, and pronouns. Similarly, crowdsourcing via microworking sites like Amazon’s Mechanical Turk has been used in several annotation</context>
</contexts>
<marker>Lafourcade, 2007</marker>
<rawString>Mathieu Lafourcade. 2007. Making people play for lexical acquisition with the JeuxDeMots prototype. In Proc. 7th International Symposium on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian List</author>
<author>Philip Pettit</author>
</authors>
<title>Aggregating sets of judgments: An impossibility result.</title>
<date>2002</date>
<journal>Economics and Philosophy,</journal>
<volume>18</volume>
<issue>1</issue>
<contexts>
<context position="6736" citStr="List and Pettit, 2002" startWordPosition="1091" endWordPosition="1094">forms of annotation as complex annotation. We want to investigate how to aggregate the information available for each item once annotations by multiple annotators have been collected. In line with the terminology used in social choice theory and particularly judgment aggregation (Ar1Some authors have combined qualitative and quantitative ratings; e.g., for the Graded Word Sense dataset of Erk et al. (2009) coders were asked to classify each relevant WordNet sense for a given item on a 5-point scale: 1 completely different, 2 mostly different, 3 similar, 4 very similar, 5 identical. row, 1963; List and Pettit, 2002), let us call an aggregation method independent if the outcome regarding a given item j only depends on the categories provided by the annotators regarding j itself (but not on, say, the categories assigned to a different item j&apos;). Independent aggregation methods are attractive due to their simplicity. They also have some conceptual appeal: when deciding on j maybe we should only concern ourselves with what people have to say regarding j? On the other hand, insisting on independence prevents us from exploiting potentially useful information that cuts across items. For instance, if a particular</context>
<context position="9819" citStr="List and Pettit, 2002" startWordPosition="1609" endWordPosition="1612"> the generalisation of classical voting rules may prove helpful (Pini et al., 2009; Endriss et al., 2009); and in case annotators rate categories using qualitative expressions such as excellent match, the method of majority judgment of Balinski and Laraki (2011) should be considered. (3) General aggregation of plain annotations. This is the approach we shall discuss below. It is related to voting in combinatorial domains studied in computational social choice (Chevaleyre et al., 2008), and to both binary aggregation (Dokow and Holzman, 2010; Grandi and Endriss, 2011) and judgment aggregation (List and Pettit, 2002). (4) General aggregation of complex annotations. While appealing due to its great level of generality, this approach can only be tackled successfully once approaches (2) and (3) are sufficiently well understood. 3 Formal Model Next we present our model for general aggregation of plain annotations into a collective annotation. 3.1 Terminology and Notation An annotation task is defined in terms of m items, with each item j E {1, ... , m} being associated with a finite set of possible categories Cj. Annotators are asked to provide an answer for each of the items of the annotation task. In the co</context>
</contexts>
<marker>List, Pettit, 2002</marker>
<rawString>Christian List and Philip Pettit. 2002. Aggregating sets of judgments: An impossibility result. Economics and Philosophy, 18(1):89–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Mao</author>
<author>Ariel D Procaccia</author>
<author>Yiling Chen</author>
</authors>
<title>Better human computation through principled voting.</title>
<date>2013</date>
<booktitle>In Proc. 27th AAAI Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="38622" citStr="Mao et al. (2013)" startWordPosition="6637" endWordPosition="6640">abel from a set of noisy labels using maximum-likelihood estimators (Dawid and Skene, 1979; Smyth et al., 1995; Raykar et al., 2010). In application domains where it is reasonable to assume the existence of a ground truth and where we are able to model the manner in which individual judgments are being distorted relative to this ground truth, social choice theory provides tools (using again maximum-likelihood estimators) for the design of aggregators that maximise chances of recovering the ground truth for a given model of distortion (Young, 1995; Conitzer and Sandholm, 2005). In recent work, Mao et al. (2013) have discussed the use of these methods in the context of crowdsourcing. Specifically, they have designed an experiment in which the ground truth is defined unambiguously and known to the experiment designer, so as to be able to extract realistic models of distortion from the data collected in a crowdsourcing exercise. 7 Conclusions We have presented a framework for combining the expertise of speakers taking part in large-scale 14In some domains, such as medical diagnosis, it makes perfect sense to assume that there is a ground truth. However, in tasks related to linguistic knowledge and lang</context>
</contexts>
<marker>Mao, Procaccia, Chen, 2013</marker>
<rawString>Andrew Mao, Ariel D. Procaccia, and Yiling Chen. 2013. Better human computation through principled voting. In Proc. 27th AAAI Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Walter G Charles</author>
</authors>
<title>Contextual correlates of semantic similarity.</title>
<date>1991</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>6--1</pages>
<contexts>
<context position="4165" citStr="Miller and Charles, 1991" startWordPosition="654" endWordPosition="657">iscusses related work and Section 7 concludes. 2 Four Types of Collective Annotation An annotation task consists of a set of items, each of which is associated with a set of possible categories (Artstein and Poesio, 2008). The categories may be the same for all items or they may be itemspecific. For instance, dialogue act annotation 539 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 539–549, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics (Allen and Core, 1997; Carletta et al., 1997) and word similarity rating (Miller and Charles, 1991; Finkelstein et al., 2002) involve choosing from amongst a set of categories—acts in a dialogue act taxonomy or values on a scale, respectively— which remains fixed for all items in the annotation task. In contrast, in tasks such as word sense labelling (Kilgarriff and Palmer, 2000; Palmer et al., 2007; Venhuizen et al., 2013) and PP-attachment annotation (Rosenthal et al., 2010; Jha et al., 2010) coders need to choose a category amongst a set of options specific to each item—the possible senses of each word or the possible attachment points in each sentence with a prepositional phrase. In ei</context>
</contexts>
<marker>Miller, Charles, 1991</marker>
<rawString>George A. Miller and Walter G. Charles. 1991. Contextual correlates of semantic similarity. Language and Cognitive Processes, 6(1):1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael K Miller</author>
<author>Daniel Osherson</author>
</authors>
<title>Methods for distance-based judgment aggregation.</title>
<date>2009</date>
<journal>Social Choice and Welfare,</journal>
<volume>32</volume>
<issue>4</issue>
<contexts>
<context position="27426" citStr="Miller and Osherson, 2009" startWordPosition="4778" endWordPosition="4781">ment of the Cartesian product is a possible annotation. In the presence of integrity constraints excluding some combinations, however, a distance-based rule allows for more sophisticated forms of aggregation (by choosing the optimal annotation w.r.t. all feasible annotations). We may also try to restrict the computation of distances to a subset of “reliable” annotators. Consider the following idea: If a group of annotators is (fairly) reliable, then they should have a 7This idea has been used in voting (Kemeny, 1959), belief merging (Konieczny and Pino P´erez, 2002), and judgment aggregation (Miller and Osherson, 2009). 8This δ, divided by m, is the same thing as what Artstein and Poesio (2008) call the agreement value agr, for item j. 544 (fairly) high inter-annotator agreement. By this reasoning, we should choose a group of annotators ANN ⊆ JV that maximises inter-annotator agreement in ANN and work with the aggregator argminAEA EiEANN d(A, Ai). But this is too simplistic: any singleton ANN = {i} will result in perfect agreement. That is, while we can easily maximise agreement, doing so in a naive way means ignoring most of the information collected. In other words, we face the following dilemma: • On the</context>
</contexts>
<marker>Miller, Osherson, 2009</marker>
<rawString>Michael K. Miller and Daniel Osherson. 2009. Methods for distance-based judgment aggregation. Social Choice and Welfare, 32(4):575–601.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Hoa Trang Dang</author>
<author>Christiane Fellbaum</author>
</authors>
<title>Making fine-grained and coarse-grained sense distinctions, both manually and automatically.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="4469" citStr="Palmer et al., 2007" startWordPosition="705" endWordPosition="708">, dialogue act annotation 539 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 539–549, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics (Allen and Core, 1997; Carletta et al., 1997) and word similarity rating (Miller and Charles, 1991; Finkelstein et al., 2002) involve choosing from amongst a set of categories—acts in a dialogue act taxonomy or values on a scale, respectively— which remains fixed for all items in the annotation task. In contrast, in tasks such as word sense labelling (Kilgarriff and Palmer, 2000; Palmer et al., 2007; Venhuizen et al., 2013) and PP-attachment annotation (Rosenthal et al., 2010; Jha et al., 2010) coders need to choose a category amongst a set of options specific to each item—the possible senses of each word or the possible attachment points in each sentence with a prepositional phrase. In either case (one set of categories for all items vs. item-specific sets of categories), annotators are typically asked to identify, for each item, the category they consider the best match. In addition, they may be given the opportunity to indicate that they cannot judge (the “don’t know” or “unclear” cat</context>
</contexts>
<marker>Palmer, Dang, Fellbaum, 2007</marker>
<rawString>Martha Palmer, Hoa Trang Dang, and Christiane Fellbaum. 2007. Making fine-grained and coarse-grained sense distinctions, both manually and automatically. Natural Language Engineering, 13(2):137–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Silvia Pini</author>
<author>Francesca Rossi</author>
<author>K Brent Venable</author>
<author>Toby Walsh</author>
</authors>
<title>Aggregating partially ordered preferences.</title>
<date>2009</date>
<journal>Journal of Logic and Computation,</journal>
<volume>19</volume>
<issue>3</issue>
<contexts>
<context position="9279" citStr="Pini et al., 2009" startWordPosition="1523" endWordPosition="1526">cular quota. This is the most common approach in the literature (see, e.g., Venhuizen et al., 2013). (2) Independent aggregation of complex annotations. This is a natural generalisation of the first approach, resulting in a wider range of possible methods. We shall not explore it here, but only point out that in case annotators provide linear orders over categories, there is a close resemblance to classical voting the540 ory (Taylor, 2005); in case only partial orders can be elicited, recent work in computational social choice on the generalisation of classical voting rules may prove helpful (Pini et al., 2009; Endriss et al., 2009); and in case annotators rate categories using qualitative expressions such as excellent match, the method of majority judgment of Balinski and Laraki (2011) should be considered. (3) General aggregation of plain annotations. This is the approach we shall discuss below. It is related to voting in combinatorial domains studied in computational social choice (Chevaleyre et al., 2008), and to both binary aggregation (Dokow and Holzman, 2010; Grandi and Endriss, 2011) and judgment aggregation (List and Pettit, 2002). (4) General aggregation of complex annotations. While appe</context>
</contexts>
<marker>Pini, Rossi, Venable, Walsh, 2009</marker>
<rawString>Maria Silvia Pini, Francesca Rossi, K. Brent Venable, and Toby Walsh. 2009. Aggregating partially ordered preferences. Journal of Logic and Computation, 19(3):475–502.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vikas Raykar</author>
<author>Shipeng Yu</author>
<author>Linda Zhao</author>
<author>Gerardo Hermosillo Valadez</author>
<author>Charles Florin</author>
<author>Luca Bogoni</author>
<author>Linda Moy</author>
</authors>
<title>Learning from crowds.</title>
<date>2010</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>11--1297</pages>
<contexts>
<context position="37092" citStr="Raykar et al. (2010)" startWordPosition="6391" endWordPosition="6394">f consensus. In both cases, the aggregators have the effect of disregarding some outlier annotators. 12See also the papers presented at the NAACL 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk (tinyurl.com/amtworkshop2010). 13Creating a gold standard often involves adjudication of disagreements by experts, or even the removal of cases with disagreement from the dataset. See, e.g., the papers cited by Beigman Klebanov and Beigman (2009). 546 Other researchers have explored ways to directly identify “low-quality” annotators. For instance, Snow et al. (2008) and Raykar et al. (2010) propose Bayesian methods for identifying and correcting annotators’ biases, while Ipeirotis et al. (2010) propose an algorithm for assigning a quality score to annotators that distinguishes intrinsic error rate from an annotator’s bias. In our approach, we do not directly rate annotators or recalibrate their annotations—rather, some outlier annotators get to play a marginal role in the resulting collective annotation as a side effect of the aggregation methods themselves. Although in our case study we have tested our aggregators by comparing their outcomes to a gold standard, our approach to </context>
</contexts>
<marker>Raykar, Yu, Zhao, Valadez, Florin, Bogoni, Moy, 2010</marker>
<rawString>Vikas Raykar, Shipeng Yu, Linda Zhao, Gerardo Hermosillo Valadez, Charles Florin, Luca Bogoni, and Linda Moy. 2010. Learning from crowds. The Journal of Machine Learning Research, 11:1297–1322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>William Lipovsky</author>
<author>Kathleen McKeown</author>
<author>Kapil Thadani</author>
<author>Jacob Andreas</author>
</authors>
<title>Towards semi-automated annotation for prepositional phrase attachment.</title>
<date>2010</date>
<booktitle>In Proc. 7th International Conference on Language Resources and Evaluation (LREC-2010).</booktitle>
<contexts>
<context position="4547" citStr="Rosenthal et al., 2010" startWordPosition="716" endWordPosition="719"> Association for Computational Linguistics, pages 539–549, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics (Allen and Core, 1997; Carletta et al., 1997) and word similarity rating (Miller and Charles, 1991; Finkelstein et al., 2002) involve choosing from amongst a set of categories—acts in a dialogue act taxonomy or values on a scale, respectively— which remains fixed for all items in the annotation task. In contrast, in tasks such as word sense labelling (Kilgarriff and Palmer, 2000; Palmer et al., 2007; Venhuizen et al., 2013) and PP-attachment annotation (Rosenthal et al., 2010; Jha et al., 2010) coders need to choose a category amongst a set of options specific to each item—the possible senses of each word or the possible attachment points in each sentence with a prepositional phrase. In either case (one set of categories for all items vs. item-specific sets of categories), annotators are typically asked to identify, for each item, the category they consider the best match. In addition, they may be given the opportunity to indicate that they cannot judge (the “don’t know” or “unclear” category). For large-scale annotation projects run over the Internet it is furthe</context>
</contexts>
<marker>Rosenthal, Lipovsky, McKeown, Thadani, Andreas, 2010</marker>
<rawString>Sara Rosenthal, William Lipovsky, Kathleen McKeown, Kapil Thadani, and Jacob Andreas. 2010. Towards semi-automated annotation for prepositional phrase attachment. In Proc. 7th International Conference on Language Resources and Evaluation (LREC-2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Rumshisky</author>
<author>Nick Botchan</author>
<author>Sophie Kushkuley</author>
<author>James Pustejovsky</author>
</authors>
<title>Word sense inventories by non-experts.</title>
<date>2012</date>
<booktitle>In Proc. 8th International Conference on Language Resources and Evaluation (LREC-2012).</booktitle>
<contexts>
<context position="35221" citStr="Rumshisky et al., 2012" startWordPosition="6092" endWordPosition="6095">08) has been used to gather annotations on anaphoric coreference; and more recently Basile et al. (2012) 11Recall that 124 out of 164 coders only annotated 20 items each; a tolerance value of 15 thus is fairly lenient. have developed the Wordrobe set of games for annotating named entities, word senses, homographs, and pronouns. Similarly, crowdsourcing via microworking sites like Amazon’s Mechanical Turk has been used in several annotation experiments related to tasks such as affect analysis, event annotation, sense definition and word sense disambiguation (Snow et al., 2008; Rumshisky, 2011; Rumshisky et al., 2012), amongst others.12 All these efforts face the problem of how to aggregate the information provided by a group of volunteers into a collective annotation. However, by and large, the emphasis so far has been on issues such as experiment design, data quality, and costs, with little attention being paid to the aggregation methods used, which are typically limited to some form of majority vote (or taking averages if the categories are numeric). In contrast, our focus has been on investigating different aggregation methods for arriving at a collective annotation. Our work has connections with the l</context>
</contexts>
<marker>Rumshisky, Botchan, Kushkuley, Pustejovsky, 2012</marker>
<rawString>Anna Rumshisky, Nick Botchan, Sophie Kushkuley, and James Pustejovsky. 2012. Word sense inventories by non-experts. In Proc. 8th International Conference on Language Resources and Evaluation (LREC-2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Rumshisky</author>
</authors>
<title>Crowdsourcing word sense definition.</title>
<date>2011</date>
<booktitle>In Proc. ACL-HLT 5th Linguistic Annotation Workshop (LAW-V).</booktitle>
<contexts>
<context position="35196" citStr="Rumshisky, 2011" startWordPosition="6090" endWordPosition="6091">erlain et al., 2008) has been used to gather annotations on anaphoric coreference; and more recently Basile et al. (2012) 11Recall that 124 out of 164 coders only annotated 20 items each; a tolerance value of 15 thus is fairly lenient. have developed the Wordrobe set of games for annotating named entities, word senses, homographs, and pronouns. Similarly, crowdsourcing via microworking sites like Amazon’s Mechanical Turk has been used in several annotation experiments related to tasks such as affect analysis, event annotation, sense definition and word sense disambiguation (Snow et al., 2008; Rumshisky, 2011; Rumshisky et al., 2012), amongst others.12 All these efforts face the problem of how to aggregate the information provided by a group of volunteers into a collective annotation. However, by and large, the emphasis so far has been on issues such as experiment design, data quality, and costs, with little attention being paid to the aggregation methods used, which are typically limited to some form of majority vote (or taking averages if the categories are numeric). In contrast, our focus has been on investigating different aggregation methods for arriving at a collective annotation. Our work h</context>
</contexts>
<marker>Rumshisky, 2011</marker>
<rawString>Anna Rumshisky. 2011. Crowdsourcing word sense definition. In Proc. ACL-HLT 5th Linguistic Annotation Workshop (LAW-V).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Padhraic Smyth</author>
<author>Usama Fayyad</author>
<author>Michael Burl</author>
<author>Pietro Perona</author>
<author>Pierre Baldi</author>
</authors>
<title>Inferring ground truth from subjective labelling of venus images.</title>
<date>1995</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>1085--1092</pages>
<contexts>
<context position="38115" citStr="Smyth et al., 1995" startWordPosition="6555" endWordPosition="6558">ctive annotation as a side effect of the aggregation methods themselves. Although in our case study we have tested our aggregators by comparing their outcomes to a gold standard, our approach to collective annotation itself does not assume that there is in fact a ground truth. Instead, we view collective annotations as reflecting the views of a community of speakers.14 This contrasts significantly with, for instance, the machine learning literature, where there is a focus on estimating the hidden true label from a set of noisy labels using maximum-likelihood estimators (Dawid and Skene, 1979; Smyth et al., 1995; Raykar et al., 2010). In application domains where it is reasonable to assume the existence of a ground truth and where we are able to model the manner in which individual judgments are being distorted relative to this ground truth, social choice theory provides tools (using again maximum-likelihood estimators) for the design of aggregators that maximise chances of recovering the ground truth for a given model of distortion (Young, 1995; Conitzer and Sandholm, 2005). In recent work, Mao et al. (2013) have discussed the use of these methods in the context of crowdsourcing. Specifically, they </context>
</contexts>
<marker>Smyth, Fayyad, Burl, Perona, Baldi, 1995</marker>
<rawString>Padhraic Smyth, Usama Fayyad, Michael Burl, Pietro Perona, and Pierre Baldi. 1995. Inferring ground truth from subjective labelling of venus images. Advances in Neural Information Processing Systems, pages 1085–1092.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast—but is it good? Evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proc. Conference on Empirical Methods in Natural Language Processing (EMNLP2008),</booktitle>
<pages>254--263</pages>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast—but is it good? Evaluating non-expert annotations for natural language tasks. In Proc. Conference on Empirical Methods in Natural Language Processing (EMNLP2008), pages 254–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan D Taylor</author>
</authors>
<title>Social Choice and the Mathematics of Manipulation.</title>
<date>2005</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="8222" citStr="Taylor, 2005" startWordPosition="1345" endWordPosition="1346">regation. Note that when studying independent aggregation methods, without loss of generality, we may assume that each annotation task consists of just a single item. In view of our discussion above, there are four classes of approaches to collective annotation: (1) Independent aggregation of plain annotations. This is the simplest case, resulting in a fairly limited design space. When, for a given item, each annotator has to choose between k categories (or abstain) and we do not permit ourselves to use any other information, then the only reasonable choice is to implement the plurality rule (Taylor, 2005), under which the winning category is the category chosen by the largest number of annotators. In case there are exactly two categories available, the plurality rule is also called the majority rule. The only additional consideration to make here (besides how to deal with ties) is whether or not we may want to declare no winner at all in case the plurality winner does not win by a sufficiently significant margin or does not make a particular quota. This is the most common approach in the literature (see, e.g., Venhuizen et al., 2013). (2) Independent aggregation of complex annotations. This is</context>
</contexts>
<marker>Taylor, 2005</marker>
<rawString>Alan D. Taylor. 2005. Social Choice and the Mathematics of Manipulation. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Nicolaus Tideman</author>
</authors>
<title>Independence of clones as a criterion for voting rules.</title>
<date>1987</date>
<journal>Social Choice and Welfare,</journal>
<volume>4</volume>
<issue>3</issue>
<contexts>
<context position="25969" citStr="Tideman, 1987" startWordPosition="4527" endWordPosition="4528">majority between annotators in N* voting 0 and those voting 1, we have to use a tie-breaking rule to make a decision. Additional heuristics may be used to make these local decisions, or they may be left to chance. Note that in case t = m, GreedyCRt is simply the majority rule (as no annotator will ever get eliminated). In case t = 0, we end up with a coalition of annotators that unanimously agree with all of the categories chosen for the collective annotation. However, this coalition of perfectly aligned 6There are some similarities to Tideman’s Ranked Pairs method for preference aggregation (Tideman, 1987), which works by fixing the relative rankings of pairs of alternatives in order of the strength of the supporting majorities. In preference aggregation (unlike here), the population of voters is not reduced in the process; instead, decisions against the majority are taken whenever this is necessary to guarantee the transitivity of the resulting collective preference order. Here S is the adjusted discrete distance defined as S(x, y) = 0 if x = y or x ∈ {?,⊥} or y ∈ {?, ⊥}, and as S(x, y) = 1 in all other cases.8 Once we have fixed a distance d on A (such as H), this induces an aggregator Fd: n </context>
</contexts>
<marker>Tideman, 1987</marker>
<rawString>T. Nicolaus Tideman. 1987. Independence of clones as a criterion for voting rules. Social Choice and Welfare, 4(3):185–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noortje Venhuizen</author>
<author>Valerio Basile</author>
<author>Kilian Evang</author>
<author>Johan Bos</author>
</authors>
<title>Gamification for word sense labeling.</title>
<date>2013</date>
<booktitle>In Proc. 10th International Conference on Computational Semantics (IWCS-2013),</booktitle>
<pages>397--403</pages>
<contexts>
<context position="1927" citStr="Venhuizen et al., 2013" startWordPosition="288" endWordPosition="291">volution for the creation of annotated corpora, within the computational linguistics community there so far is no clear understanding of how the so-called “wisdom of the crowds” could or should be used to develop useful annotated linguistic resources. Those who have looked into this increasingly important issue have mostly concentrated on validating the quality of multiple non-expert annotations in terms of how they compare to expert gold standards; but they have only used simple aggregation methods based on majority voting to combine the judgments of individual annotators (Snow et al., 2008; Venhuizen et al., 2013). In this paper, we take a different perspective and instead focus on investigating different aggregation methods for deriving a single collective annotation from a diverse set of judgments. For this we draw inspiration from the field of social choice theory, a theoretical framework for combining the preferences or choices of several individuals into a collective decision (Arrow et al., 2002). Our aim is to explore the parallels between the task of aggregating the preferences of the citizens participating in an election and the task of combining the expertise of speakers taking part in an anno</context>
<context position="4494" citStr="Venhuizen et al., 2013" startWordPosition="709" endWordPosition="712">tion 539 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 539–549, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics (Allen and Core, 1997; Carletta et al., 1997) and word similarity rating (Miller and Charles, 1991; Finkelstein et al., 2002) involve choosing from amongst a set of categories—acts in a dialogue act taxonomy or values on a scale, respectively— which remains fixed for all items in the annotation task. In contrast, in tasks such as word sense labelling (Kilgarriff and Palmer, 2000; Palmer et al., 2007; Venhuizen et al., 2013) and PP-attachment annotation (Rosenthal et al., 2010; Jha et al., 2010) coders need to choose a category amongst a set of options specific to each item—the possible senses of each word or the possible attachment points in each sentence with a prepositional phrase. In either case (one set of categories for all items vs. item-specific sets of categories), annotators are typically asked to identify, for each item, the category they consider the best match. In addition, they may be given the opportunity to indicate that they cannot judge (the “don’t know” or “unclear” category). For large-scale a</context>
<context position="8761" citStr="Venhuizen et al., 2013" startWordPosition="1438" endWordPosition="1441">n, then the only reasonable choice is to implement the plurality rule (Taylor, 2005), under which the winning category is the category chosen by the largest number of annotators. In case there are exactly two categories available, the plurality rule is also called the majority rule. The only additional consideration to make here (besides how to deal with ties) is whether or not we may want to declare no winner at all in case the plurality winner does not win by a sufficiently significant margin or does not make a particular quota. This is the most common approach in the literature (see, e.g., Venhuizen et al., 2013). (2) Independent aggregation of complex annotations. This is a natural generalisation of the first approach, resulting in a wider range of possible methods. We shall not explore it here, but only point out that in case annotators provide linear orders over categories, there is a close resemblance to classical voting the540 ory (Taylor, 2005); in case only partial orders can be elicited, recent work in computational social choice on the generalisation of classical voting rules may prove helpful (Pini et al., 2009; Endriss et al., 2009); and in case annotators rate categories using qualitative </context>
</contexts>
<marker>Venhuizen, Basile, Evang, Bos, 2013</marker>
<rawString>Noortje Venhuizen, Valerio Basile, Kilian Evang, and Johan Bos. 2013. Gamification for word sense labeling. In Proc. 10th International Conference on Computational Semantics (IWCS-2013), pages 397– 403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis von Ahn</author>
<author>Laura Dabbish</author>
</authors>
<title>Labeling images with a computer game.</title>
<date>2004</date>
<booktitle>In Proc. SIGCHI Conference on Human Factors in Computing Systems,</booktitle>
<pages>319--326</pages>
<publisher>ACM.</publisher>
<marker>von Ahn, Dabbish, 2004</marker>
<rawString>Luis von Ahn and Laura Dabbish. 2004. Labeling images with a computer game. In Proc. SIGCHI Conference on Human Factors in Computing Systems, pages 319–326. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Peyton Young</author>
</authors>
<title>Optimal voting rules.</title>
<date>1995</date>
<journal>Journal of Economic Perspectives,</journal>
<volume>9</volume>
<issue>1</issue>
<contexts>
<context position="38557" citStr="Young, 1995" startWordPosition="6628" endWordPosition="6629">ure, where there is a focus on estimating the hidden true label from a set of noisy labels using maximum-likelihood estimators (Dawid and Skene, 1979; Smyth et al., 1995; Raykar et al., 2010). In application domains where it is reasonable to assume the existence of a ground truth and where we are able to model the manner in which individual judgments are being distorted relative to this ground truth, social choice theory provides tools (using again maximum-likelihood estimators) for the design of aggregators that maximise chances of recovering the ground truth for a given model of distortion (Young, 1995; Conitzer and Sandholm, 2005). In recent work, Mao et al. (2013) have discussed the use of these methods in the context of crowdsourcing. Specifically, they have designed an experiment in which the ground truth is defined unambiguously and known to the experiment designer, so as to be able to extract realistic models of distortion from the data collected in a crowdsourcing exercise. 7 Conclusions We have presented a framework for combining the expertise of speakers taking part in large-scale 14In some domains, such as medical diagnosis, it makes perfect sense to assume that there is a ground </context>
</contexts>
<marker>Young, 1995</marker>
<rawString>H. Peyton Young. 1995. Optimal voting rules. Journal of Economic Perspectives, 9(1):51–64.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>