<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006543">
<title confidence="0.994252">
Simple semi-supervised training of part-of-speech taggers
</title>
<author confidence="0.994708">
Anders Søgaard
</author>
<affiliation confidence="0.9975365">
Center for Language Technology
University of Copenhagen
</affiliation>
<email confidence="0.989457">
soegaard@hum.ku.dk
</email>
<sectionHeader confidence="0.993704" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999582416666667">
Most attempts to train part-of-speech tag-
gers on a mixture of labeled and unlabeled
data have failed. In this work stacked
learning is used to reduce tagging to a
classification task. This simplifies semi-
supervised training considerably. Our
prefered semi-supervised method com-
bines tri-training (Li and Zhou, 2005) and
disagreement-based co-training. On the
Wall Street Journal, we obtain an error re-
duction of 4.2% with SVMTool (Gimenez
and Marquez, 2004).
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.986341844444445">
Semi-supervised part-of-speech (POS) tagging is
relatively rare, and the main reason seems to be
that results have mostly been negative. Meri-
aldo (1994), in a now famous negative result, at-
tempted to improve HMM POS tagging by expec-
tation maximization with unlabeled data. Clark
et al. (2003) reported positive results with little
labeled training data but negative results when
the amount of labeled training data increased; the
same seems to be the case in Wang et al. (2007)
who use co-training of two diverse POS taggers.
Huang et al. (2009) present positive results for
self-training a simple bigram POS tagger, but re-
sults are considerably below state-of-the-art.
Recently researchers have explored alternative
methods. Suzuki and Isozaki (2008) introduce
a semi-supervised extension of conditional ran-
dom fields that combines supervised and unsuper-
vised probability models by so-called MDF pa-
rameter estimation, which reduces error on Wall
Street Journal (WSJ) standard splits by about 7%
relative to their supervised baseline. Spoustova
et al. (2009) use a new pool of unlabeled data
tagged by an ensemble of state-of-the-art taggers
in every training step of an averaged perceptron
POS tagger with 4–5% error reduction. Finally,
Søgaard (2009) stacks a POS tagger on an un-
supervised clustering algorithm trained on large
amounts of unlabeled data with mixed results.
This work combines a new semi-supervised
learning method to POS tagging, namely tri-
training (Li and Zhou, 2005), with stacking on un-
supervised clustering. It is shown that this method
can be used to improve a state-of-the-art POS tag-
ger, SVMTool (Gimenez and Marquez, 2004). Fi-
nally, we introduce a variant of tri-training called
tri-training with disagreement, which seems to
perform equally well, but which imports much less
unlabeled data and is therefore more efficient.
2 Tagging as classification
This section describes our dataset and our input
tagger. We also describe how stacking is used to
reduce POS tagging to a classification task. Fi-
nally, we introduce the supervised learning algo-
rithms used in our experiments.
</bodyText>
<subsectionHeader confidence="0.973938">
2.1 Data
</subsectionHeader>
<bodyText confidence="0.999440666666667">
We use the POS-tagged WSJ from the Penn Tree-
bank Release 3 (Marcus et al., 1993) with the
standard split: Sect. 0–18 is used for training,
Sect. 19–21 for development, and Sect. 22–24 for
testing. Since we need to train our classifiers on
material distinct from the training material for our
input POS tagger, we save Sect. 19 for training our
classifiers. Finally, we use the (untagged) Brown
corpus as our unlabeled data. The number of to-
kens we use for training, developing and testing
the classifiers, and the amount of unlabeled data
available to it, are thus:
</bodyText>
<footnote confidence="0.7730082">
tokens
train 44,472
development 103,686
test 129,281
unlabeled 1,170,811
</footnote>
<page confidence="0.965044">
205
</page>
<note confidence="0.5139675">
Proceedings of the ACL 2010 Conference Short Papers, pages 205–208,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.998722333333333">
The amount of unlabeled data available to our
classifiers is thus a bit more than 25 times the
amount of labeled data.
</bodyText>
<subsectionHeader confidence="0.999055">
2.2 Input tagger
</subsectionHeader>
<bodyText confidence="0.999996571428571">
In our experiments we use SVMTool (Gimenez
and Marquez, 2004) with model type 4 run incre-
mentally in both directions. SVMTool has an ac-
curacy of 97.15% on WSJ Sect. 22-24 with this
parameter setting. Gimenez and Marquez (2004)
report that SVMTool has an accuracy of 97.16%
with an optimized parameter setting.
</bodyText>
<subsectionHeader confidence="0.995501">
2.3 Classifier input
</subsectionHeader>
<bodyText confidence="0.978382875">
The way classifiers are constructed in our experi-
ments is very simple. We train SVMTool and an
unsupervised tagger, Unsupos (Biemann, 2006),
on our training sections and apply them to the de-
velopment, test and unlabeled sections. The re-
sults are combined in tables that will be the input
of our classifiers. Here is an excerpt:1
Gold standard SVMTool Unsupos
</bodyText>
<equation confidence="0.699103">
DT DT 17
NNP NNP 27
NNP NNS 17*
NNP NNP 17
VBD VBD 26
</equation>
<bodyText confidence="0.998876764705882">
Each row represents a word and lists the gold
standard POS tag, the predicted POS tag and the
word cluster selected by Unsupos. For example,
the first word is labeled ’DT’, which SVMTool
correctly predicts, and it belongs to cluster 17 of
about 500 word clusters. The first column is blank
in the table for the unlabeled section.
Generally, the idea is that a classifier will learn
to trust SVMTool in some cases, but that it may
also learn that if SVMTool predicts a certain tag
for some word cluster the correct label is another
tag. This way of combining taggers into a single
end classifier can be seen as a form of stacking
(Wolpert, 1992). It has the advantage that it re-
duces POS tagging to a classification task. This
may simplify semi-supervised learning consider-
ably.
</bodyText>
<subsectionHeader confidence="0.99948">
2.4 Learning algorithms
</subsectionHeader>
<bodyText confidence="0.999848666666667">
We assume some knowledge of supervised learn-
ing algorithms. Most of our experiments are im-
plementations of wrapper methods that call off-
</bodyText>
<footnote confidence="0.8685975">
1The numbers provided by Unsupos refer to clusters; ”*”
marks out-of-vocabulary words.
</footnote>
<bodyText confidence="0.998707928571429">
the-shelf implementations of supervised learning
algorithms. Specifically we have experimented
with support vector machines (SVMs), decision
trees, bagging and random forests. Tri-training,
explained below, is a semi-supervised learning
method which requires large amounts of data.
Consequently, we only used very fast learning al-
gorithms in the context of tri-training. On the de-
velopment section, decisions trees performed bet-
ter than bagging and random forests. The de-
cision tree algorithm is the C4.5 algorithm first
introduced in Quinlan (1993). We used SVMs
with polynomial kernels of degree 2 to provide a
stronger stacking-only baseline.
</bodyText>
<sectionHeader confidence="0.983292" genericHeader="introduction">
3 Tri-training
</sectionHeader>
<bodyText confidence="0.999955558823529">
This section first presents the tri-training algo-
rithm originally proposed by Li and Zhou (2005)
and then considers a novel variant: tri-training
with disagreement.
Let L denote the labeled data and U the unla-
beled data. Assume that three classifiers c1, c2, c3
(same learning algorithm) have been trained on
three bootstrap samples of L. In tri-training, an
unlabeled datapoint in U is now labeled for a clas-
sifier, say c1, if the other two classifiers agree on
its label, i.e. c2 and c3. Two classifiers inform
the third. If the two classifiers agree on a label-
ing, there is a good chance that they are right.
The algorithm stops when the classifiers no longer
change. The three classifiers are combined by ma-
jority voting. Li and Zhou (2005) show that un-
der certain conditions the increase in classification
noise rate is compensated by the amount of newly
labeled data points.
The most important condition is that the three
classifiers are diverse. If the three classifiers are
identical, tri-training degenerates to self-training.
Diversity is obtained in Li and Zhou (2005) by
training classifiers on bootstrap samples. In their
experiments, they consider classifiers based on the
C4.5 algorithm, BP neural networks and naive
Bayes classifiers. The algorithm is sketched
in a simplified form in Figure 1; see Li and
Zhou (2005) for all the details.
Tri-training has to the best of our knowledge not
been applied to POS tagging before, but it has been
applied to other NLP classification tasks, incl. Chi-
nese chunking (Chen et al., 2006) and question
classification (Nguyen et al., 2008).
</bodyText>
<page confidence="0.995345">
206
</page>
<listItem confidence="0.9874008125">
1: for i E {1..3} do
2: Si , bootstrap sample(L)
3: ci , train classifier(Si)
4: end for
5: repeat
6: for i E {1..3} do
7: for x E U do
8: Li , 0
9: if cj(x) = ck(x)(j, k =� i) then
10: Li , Li U {(x,cj(x)}
11: end if
12: end for
13: ci , train classifier(L U Li)
14: end for
15: until none of ci changes
16: apply majority vote over ci
</listItem>
<figureCaption confidence="0.994593">
Figure 1: Tri-training (Li and Zhou, 2005).
</figureCaption>
<subsectionHeader confidence="0.999693">
3.1 Tri-training with disagreement
</subsectionHeader>
<bodyText confidence="0.983229882352941">
We introduce a possible improvement of the tri-
training algorithm: If we change lines 9–10 in the
algorithm in Figure 1 with the lines:
if cj(x) = ck(x) =� ci(x)(j, k =� i) then
Li , Li U {(x, cj(x)}
end if
two classifiers, say c1 and c2, only label a data-
point for the third classifier, c3, if c1 and c2 agree
on its label, but c3 disagrees. The intuition is
that we only want to strengthen a classifier in its
weak points, and we want to avoid skewing our
labeled data by easy data points. Finally, since tri-
training with disagreement imports less unlabeled
data, it is much more efficient than tri-training. No
one has to the best of our knowledge applied tri-
training with disagreement to real-life classifica-
tion tasks before.
</bodyText>
<sectionHeader confidence="0.999953" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.977958318181818">
Our results are presented in Figure 2. The stacking
result was obtained by training a SVM on top of
the predictions of SVMTool and the word clusters
of Unsupos. SVMs performed better than deci-
sion trees, bagging and random forests on our de-
velopment section, but improvements on test data
were modest. Tri-training refers to the original al-
gorithm sketched in Figure 1 with C4.5 as learn-
ing algorithm. Since tri-training degenerates to
self-training if the three classifiers are trained on
the same sample, we used our implementation of
tri-training to obtain self-training results and vali-
dated our results by a simpler implementation. We
varied poolsize to optimize self-training. Finally,
we list results for a technique called co-forests (Li
and Zhou, 2007), which is a recent alternative to
tri-training presented by the same authors, and for
tri-training with disagreement (tri-disagr). The p-
values are computed using 10,000 stratified shuf-
fles.
Tri-training and tri-training with disagreement
gave the best results. Note that since tri-training
leads to much better results than stacking alone,
it is unlabeled data that gives us most of the im-
provement, not the stacking itself. The differ-
ence between tri-training and self-training is near-
significant (p &lt;0.0150). It seems that tri-training
with disagreement is a competitive technique in
terms of accuracy. The main advantage of tri-
training with disagreement compared to ordinary
tri-training, however, is that it is very efficient.
This is reflected by the average number of tokens
in Li over the three learners in the worst round of
learning:
av. tokens in Li
tri-training
tri-disagr
Note also that self-training gave very good re-
sults. Self-training was, again, much slower than
tri-training with disagreement since we had to
train on a large pool of unlabeled data (but only
once). Of course this is not a standard self-training
set-up, but self-training informed by unsupervised
word clusters.
</bodyText>
<subsectionHeader confidence="0.985928">
4.1 Follow-up experiments
</subsectionHeader>
<bodyText confidence="0.999852076923077">
SVMTool is one of the most accurate POS tag-
gers available. This means that the predictions
that are added to the labeled data are of very
high quality. To test if our semi-supervised learn-
ing methods were sensitive to the quality of the
input taggers we repeated the self-training and
tri-training experiments with a less competitive
POS tagger, namely the maximum entropy-based
POS tagger first described in (Ratnaparkhi, 1998)
that comes with the maximum entropy library in
(Zhang, 2004). Results are presented as the sec-
ond line in Figure 2. Note that error reduction is
much lower in this case.
</bodyText>
<page confidence="0.668017666666667">
1,170,811
173
207
</page>
<table confidence="0.813937">
BL stacking tri-tr. self-tr. co-forests tri-disagr error red. p-value
SVMTool 97.15% 97.19% 97.27% 97.26% 97.13% 97.27% 4.21% &lt;0.0001
MaxEnt 96.31% - 96.36% 96.36% 96.28% 96.36% 1.36% &lt;0.0001
</table>
<figureCaption confidence="0.940056">
Figure 2: Results on Wall Street Journal Sect. 22-24 with different semi-supervised methods.
</figureCaption>
<sectionHeader confidence="0.997288" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999972071428572">
This paper first shows how stacking can be used to
reduce POS tagging to a classification task. This
reduction seems to enable robust semi-supervised
learning. The technique was used to improve the
accuracy of a state-of-the-art POS tagger, namely
SVMTool. Four semi-supervised learning meth-
ods were tested, incl. self-training, tri-training, co-
forests and tri-training with disagreement. All
methods increased the accuracy of SVMTool sig-
nificantly. Error reduction on Wall Street Jour-
nal Sect. 22-24 was 4.2%, which is comparable
to related work in the literature, e.g. Suzuki and
Isozaki (2008) (7%) and Spoustova et al. (2009)
(4–5%).
</bodyText>
<sectionHeader confidence="0.998918" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999665847457627">
Chris Biemann. 2006. Unsupervised part-of-speech
tagging employing efficient graph clustering. In
COLING-ACL Student Session, Sydney, Australia.
Wenliang Chen, Yujie Zhang, and Hitoshi Isahara.
2006. Chinese chunking with tri-training learn-
ing. In Computer processing of oriental languages,
pages 466–473. Springer, Berlin, Germany.
Stephen Clark, James Curran, and Mike Osborne.
2003. Bootstrapping POS taggers using unlabeled
data. In CONLL, Edmonton, Canada.
Jesus Gimenez and Lluis Marquez. 2004. SVMTool: a
general POS tagger generator based on support vec-
tor machines. In LREC, Lisbon, Portugal.
Zhongqiang Huang, Vladimir Eidelman, and Mary
Harper. 2009. Improving a simple bigram HMM
part-of-speech tagger by latent annotation and self-
training. In NAACL-HLT, Boulder, CO.
Ming Li and Zhi-Hua Zhou. 2005. Tri-training: ex-
ploiting unlabeled data using three classifiers. IEEE
Transactions on Knowledge and Data Engineering,
17(11):1529–1541.
Ming Li and Zhi-Hua Zhou. 2007. Improve computer-
aided diagnosis with machine learning techniques
using undiagnosed samples. IEEE Transactions on
Systems, Man and Cybernetics, 37(6):1088–1098.
Mitchell Marcus, Mary Marcinkiewicz, and Beatrice
Santorini. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313–330.
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational Linguistics,
20(2):155–171.
Tri Nguyen, Le Nguyen, and Akira Shimazu. 2008.
Using semi-supervised learning for question classi-
fication. Journal of Natural Language Processing,
15:3–21.
Ross Quinlan. 1993. Programs for machine learning.
Morgan Kaufmann.
Adwait Ratnaparkhi. 1998. Maximum entropy mod-
els for natural language ambiguity resolution. Ph.D.
thesis, University of Pennsylvania.
Anders Søgaard. 2009. Ensemble-based POS tagging
of italian. In IAAI-EVALITA, Reggio Emilia, Italy.
Drahomira Spoustova, Jan Hajic, Jan Raab, and
Miroslav Spousta. 2009. Semi-supervised training
for the averaged perceptron POS tagger. In EACL,
Athens, Greece.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-
word scale unlabeled data. In ACL, pages 665–673,
Columbus, Ohio.
Wen Wang, Zhongqiang Huang, and Mary Harper.
2007. Semi-supervised learning for part-of-speech
tagging of Mandarin transcribed speech. In ICASSP,
Hawaii.
David Wolpert. 1992. Stacked generalization. Neural
Networks, 5:241–259.
Le Zhang. 2004. Maximum entropy modeling toolkit
for Python and C++. University of Edinburgh.
</reference>
<page confidence="0.99778">
208
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.461838">
<title confidence="0.996389">Simple semi-supervised training of part-of-speech taggers</title>
<author confidence="0.995743">Anders Søgaard</author>
<affiliation confidence="0.998888">Center for Language Technology University of Copenhagen</affiliation>
<email confidence="0.988052">soegaard@hum.ku.dk</email>
<abstract confidence="0.9799615">Most attempts to train part-of-speech taggers on a mixture of labeled and unlabeled data have failed. In this work stacked learning is used to reduce tagging to a classification task. This simplifies semisupervised training considerably. Our prefered semi-supervised method combines tri-training (Li and Zhou, 2005) and disagreement-based co-training. On the Wall Street Journal, we obtain an error reduction of 4.2% with SVMTool (Gimenez</abstract>
<note confidence="0.632645">and Marquez, 2004).</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
</authors>
<title>Unsupervised part-of-speech tagging employing efficient graph clustering.</title>
<date>2006</date>
<booktitle>In COLING-ACL Student Session,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="4144" citStr="Biemann, 2006" startWordPosition="648" endWordPosition="649">uistics The amount of unlabeled data available to our classifiers is thus a bit more than 25 times the amount of labeled data. 2.2 Input tagger In our experiments we use SVMTool (Gimenez and Marquez, 2004) with model type 4 run incrementally in both directions. SVMTool has an accuracy of 97.15% on WSJ Sect. 22-24 with this parameter setting. Gimenez and Marquez (2004) report that SVMTool has an accuracy of 97.16% with an optimized parameter setting. 2.3 Classifier input The way classifiers are constructed in our experiments is very simple. We train SVMTool and an unsupervised tagger, Unsupos (Biemann, 2006), on our training sections and apply them to the development, test and unlabeled sections. The results are combined in tables that will be the input of our classifiers. Here is an excerpt:1 Gold standard SVMTool Unsupos DT DT 17 NNP NNP 27 NNP NNS 17* NNP NNP 17 VBD VBD 26 Each row represents a word and lists the gold standard POS tag, the predicted POS tag and the word cluster selected by Unsupos. For example, the first word is labeled ’DT’, which SVMTool correctly predicts, and it belongs to cluster 17 of about 500 word clusters. The first column is blank in the table for the unlabeled secti</context>
</contexts>
<marker>Biemann, 2006</marker>
<rawString>Chris Biemann. 2006. Unsupervised part-of-speech tagging employing efficient graph clustering. In COLING-ACL Student Session, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenliang Chen</author>
<author>Yujie Zhang</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Chinese chunking with tri-training learning.</title>
<date>2006</date>
<booktitle>In Computer processing of oriental languages,</booktitle>
<pages>466--473</pages>
<publisher>Springer,</publisher>
<location>Berlin, Germany.</location>
<contexts>
<context position="7646" citStr="Chen et al., 2006" startWordPosition="1225" endWordPosition="1228">ree classifiers are diverse. If the three classifiers are identical, tri-training degenerates to self-training. Diversity is obtained in Li and Zhou (2005) by training classifiers on bootstrap samples. In their experiments, they consider classifiers based on the C4.5 algorithm, BP neural networks and naive Bayes classifiers. The algorithm is sketched in a simplified form in Figure 1; see Li and Zhou (2005) for all the details. Tri-training has to the best of our knowledge not been applied to POS tagging before, but it has been applied to other NLP classification tasks, incl. Chinese chunking (Chen et al., 2006) and question classification (Nguyen et al., 2008). 206 1: for i E {1..3} do 2: Si , bootstrap sample(L) 3: ci , train classifier(Si) 4: end for 5: repeat 6: for i E {1..3} do 7: for x E U do 8: Li , 0 9: if cj(x) = ck(x)(j, k =� i) then 10: Li , Li U {(x,cj(x)} 11: end if 12: end for 13: ci , train classifier(L U Li) 14: end for 15: until none of ci changes 16: apply majority vote over ci Figure 1: Tri-training (Li and Zhou, 2005). 3.1 Tri-training with disagreement We introduce a possible improvement of the tritraining algorithm: If we change lines 9–10 in the algorithm in Figure 1 with the </context>
</contexts>
<marker>Chen, Zhang, Isahara, 2006</marker>
<rawString>Wenliang Chen, Yujie Zhang, and Hitoshi Isahara. 2006. Chinese chunking with tri-training learning. In Computer processing of oriental languages, pages 466–473. Springer, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James Curran</author>
<author>Mike Osborne</author>
</authors>
<title>Bootstrapping POS taggers using unlabeled data. In CONLL,</title>
<date>2003</date>
<location>Edmonton, Canada.</location>
<contexts>
<context position="923" citStr="Clark et al. (2003)" startWordPosition="132" endWordPosition="135">ce tagging to a classification task. This simplifies semisupervised training considerably. Our prefered semi-supervised method combines tri-training (Li and Zhou, 2005) and disagreement-based co-training. On the Wall Street Journal, we obtain an error reduction of 4.2% with SVMTool (Gimenez and Marquez, 2004). 1 Introduction Semi-supervised part-of-speech (POS) tagging is relatively rare, and the main reason seems to be that results have mostly been negative. Merialdo (1994), in a now famous negative result, attempted to improve HMM POS tagging by expectation maximization with unlabeled data. Clark et al. (2003) reported positive results with little labeled training data but negative results when the amount of labeled training data increased; the same seems to be the case in Wang et al. (2007) who use co-training of two diverse POS taggers. Huang et al. (2009) present positive results for self-training a simple bigram POS tagger, but results are considerably below state-of-the-art. Recently researchers have explored alternative methods. Suzuki and Isozaki (2008) introduce a semi-supervised extension of conditional random fields that combines supervised and unsupervised probability models by so-called</context>
</contexts>
<marker>Clark, Curran, Osborne, 2003</marker>
<rawString>Stephen Clark, James Curran, and Mike Osborne. 2003. Bootstrapping POS taggers using unlabeled data. In CONLL, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jesus Gimenez</author>
<author>Lluis Marquez</author>
</authors>
<title>SVMTool: a general POS tagger generator based on support vector machines.</title>
<date>2004</date>
<booktitle>In LREC,</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="2280" citStr="Gimenez and Marquez, 2004" startWordPosition="343" endWordPosition="346">d baseline. Spoustova et al. (2009) use a new pool of unlabeled data tagged by an ensemble of state-of-the-art taggers in every training step of an averaged perceptron POS tagger with 4–5% error reduction. Finally, Søgaard (2009) stacks a POS tagger on an unsupervised clustering algorithm trained on large amounts of unlabeled data with mixed results. This work combines a new semi-supervised learning method to POS tagging, namely tritraining (Li and Zhou, 2005), with stacking on unsupervised clustering. It is shown that this method can be used to improve a state-of-the-art POS tagger, SVMTool (Gimenez and Marquez, 2004). Finally, we introduce a variant of tri-training called tri-training with disagreement, which seems to perform equally well, but which imports much less unlabeled data and is therefore more efficient. 2 Tagging as classification This section describes our dataset and our input tagger. We also describe how stacking is used to reduce POS tagging to a classification task. Finally, we introduce the supervised learning algorithms used in our experiments. 2.1 Data We use the POS-tagged WSJ from the Penn Treebank Release 3 (Marcus et al., 1993) with the standard split: Sect. 0–18 is used for trainin</context>
<context position="3735" citStr="Gimenez and Marquez, 2004" startWordPosition="579" endWordPosition="582"> we use the (untagged) Brown corpus as our unlabeled data. The number of tokens we use for training, developing and testing the classifiers, and the amount of unlabeled data available to it, are thus: tokens train 44,472 development 103,686 test 129,281 unlabeled 1,170,811 205 Proceedings of the ACL 2010 Conference Short Papers, pages 205–208, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics The amount of unlabeled data available to our classifiers is thus a bit more than 25 times the amount of labeled data. 2.2 Input tagger In our experiments we use SVMTool (Gimenez and Marquez, 2004) with model type 4 run incrementally in both directions. SVMTool has an accuracy of 97.15% on WSJ Sect. 22-24 with this parameter setting. Gimenez and Marquez (2004) report that SVMTool has an accuracy of 97.16% with an optimized parameter setting. 2.3 Classifier input The way classifiers are constructed in our experiments is very simple. We train SVMTool and an unsupervised tagger, Unsupos (Biemann, 2006), on our training sections and apply them to the development, test and unlabeled sections. The results are combined in tables that will be the input of our classifiers. Here is an excerpt:1 G</context>
</contexts>
<marker>Gimenez, Marquez, 2004</marker>
<rawString>Jesus Gimenez and Lluis Marquez. 2004. SVMTool: a general POS tagger generator based on support vector machines. In LREC, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Vladimir Eidelman</author>
<author>Mary Harper</author>
</authors>
<title>Improving a simple bigram HMM part-of-speech tagger by latent annotation and selftraining.</title>
<date>2009</date>
<booktitle>In NAACL-HLT,</booktitle>
<location>Boulder, CO.</location>
<contexts>
<context position="1176" citStr="Huang et al. (2009)" startWordPosition="175" endWordPosition="178">eduction of 4.2% with SVMTool (Gimenez and Marquez, 2004). 1 Introduction Semi-supervised part-of-speech (POS) tagging is relatively rare, and the main reason seems to be that results have mostly been negative. Merialdo (1994), in a now famous negative result, attempted to improve HMM POS tagging by expectation maximization with unlabeled data. Clark et al. (2003) reported positive results with little labeled training data but negative results when the amount of labeled training data increased; the same seems to be the case in Wang et al. (2007) who use co-training of two diverse POS taggers. Huang et al. (2009) present positive results for self-training a simple bigram POS tagger, but results are considerably below state-of-the-art. Recently researchers have explored alternative methods. Suzuki and Isozaki (2008) introduce a semi-supervised extension of conditional random fields that combines supervised and unsupervised probability models by so-called MDF parameter estimation, which reduces error on Wall Street Journal (WSJ) standard splits by about 7% relative to their supervised baseline. Spoustova et al. (2009) use a new pool of unlabeled data tagged by an ensemble of state-of-the-art taggers in </context>
</contexts>
<marker>Huang, Eidelman, Harper, 2009</marker>
<rawString>Zhongqiang Huang, Vladimir Eidelman, and Mary Harper. 2009. Improving a simple bigram HMM part-of-speech tagger by latent annotation and selftraining. In NAACL-HLT, Boulder, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming Li</author>
<author>Zhi-Hua Zhou</author>
</authors>
<title>Tri-training: exploiting unlabeled data using three classifiers.</title>
<date>2005</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>17</volume>
<issue>11</issue>
<contexts>
<context position="2118" citStr="Li and Zhou, 2005" startWordPosition="316" endWordPosition="319">ity models by so-called MDF parameter estimation, which reduces error on Wall Street Journal (WSJ) standard splits by about 7% relative to their supervised baseline. Spoustova et al. (2009) use a new pool of unlabeled data tagged by an ensemble of state-of-the-art taggers in every training step of an averaged perceptron POS tagger with 4–5% error reduction. Finally, Søgaard (2009) stacks a POS tagger on an unsupervised clustering algorithm trained on large amounts of unlabeled data with mixed results. This work combines a new semi-supervised learning method to POS tagging, namely tritraining (Li and Zhou, 2005), with stacking on unsupervised clustering. It is shown that this method can be used to improve a state-of-the-art POS tagger, SVMTool (Gimenez and Marquez, 2004). Finally, we introduce a variant of tri-training called tri-training with disagreement, which seems to perform equally well, but which imports much less unlabeled data and is therefore more efficient. 2 Tagging as classification This section describes our dataset and our input tagger. We also describe how stacking is used to reduce POS tagging to a classification task. Finally, we introduce the supervised learning algorithms used in </context>
<context position="6200" citStr="Li and Zhou (2005)" startWordPosition="983" endWordPosition="986">n trees, bagging and random forests. Tri-training, explained below, is a semi-supervised learning method which requires large amounts of data. Consequently, we only used very fast learning algorithms in the context of tri-training. On the development section, decisions trees performed better than bagging and random forests. The decision tree algorithm is the C4.5 algorithm first introduced in Quinlan (1993). We used SVMs with polynomial kernels of degree 2 to provide a stronger stacking-only baseline. 3 Tri-training This section first presents the tri-training algorithm originally proposed by Li and Zhou (2005) and then considers a novel variant: tri-training with disagreement. Let L denote the labeled data and U the unlabeled data. Assume that three classifiers c1, c2, c3 (same learning algorithm) have been trained on three bootstrap samples of L. In tri-training, an unlabeled datapoint in U is now labeled for a classifier, say c1, if the other two classifiers agree on its label, i.e. c2 and c3. Two classifiers inform the third. If the two classifiers agree on a labeling, there is a good chance that they are right. The algorithm stops when the classifiers no longer change. The three classifiers are</context>
<context position="7437" citStr="Li and Zhou (2005)" startWordPosition="1188" endWordPosition="1191">ty voting. Li and Zhou (2005) show that under certain conditions the increase in classification noise rate is compensated by the amount of newly labeled data points. The most important condition is that the three classifiers are diverse. If the three classifiers are identical, tri-training degenerates to self-training. Diversity is obtained in Li and Zhou (2005) by training classifiers on bootstrap samples. In their experiments, they consider classifiers based on the C4.5 algorithm, BP neural networks and naive Bayes classifiers. The algorithm is sketched in a simplified form in Figure 1; see Li and Zhou (2005) for all the details. Tri-training has to the best of our knowledge not been applied to POS tagging before, but it has been applied to other NLP classification tasks, incl. Chinese chunking (Chen et al., 2006) and question classification (Nguyen et al., 2008). 206 1: for i E {1..3} do 2: Si , bootstrap sample(L) 3: ci , train classifier(Si) 4: end for 5: repeat 6: for i E {1..3} do 7: for x E U do 8: Li , 0 9: if cj(x) = ck(x)(j, k =� i) then 10: Li , Li U {(x,cj(x)} 11: end if 12: end for 13: ci , train classifier(L U Li) 14: end for 15: until none of ci changes 16: apply majority vote over c</context>
</contexts>
<marker>Li, Zhou, 2005</marker>
<rawString>Ming Li and Zhi-Hua Zhou. 2005. Tri-training: exploiting unlabeled data using three classifiers. IEEE Transactions on Knowledge and Data Engineering, 17(11):1529–1541.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming Li</author>
<author>Zhi-Hua Zhou</author>
</authors>
<title>Improve computeraided diagnosis with machine learning techniques using undiagnosed samples.</title>
<date>2007</date>
<journal>IEEE Transactions on Systems, Man and Cybernetics,</journal>
<volume>37</volume>
<issue>6</issue>
<contexts>
<context position="9619" citStr="Li and Zhou, 2007" startWordPosition="1582" endWordPosition="1585">sters of Unsupos. SVMs performed better than decision trees, bagging and random forests on our development section, but improvements on test data were modest. Tri-training refers to the original algorithm sketched in Figure 1 with C4.5 as learning algorithm. Since tri-training degenerates to self-training if the three classifiers are trained on the same sample, we used our implementation of tri-training to obtain self-training results and validated our results by a simpler implementation. We varied poolsize to optimize self-training. Finally, we list results for a technique called co-forests (Li and Zhou, 2007), which is a recent alternative to tri-training presented by the same authors, and for tri-training with disagreement (tri-disagr). The pvalues are computed using 10,000 stratified shuffles. Tri-training and tri-training with disagreement gave the best results. Note that since tri-training leads to much better results than stacking alone, it is unlabeled data that gives us most of the improvement, not the stacking itself. The difference between tri-training and self-training is nearsignificant (p &lt;0.0150). It seems that tri-training with disagreement is a competitive technique in terms of accu</context>
</contexts>
<marker>Li, Zhou, 2007</marker>
<rawString>Ming Li and Zhi-Hua Zhou. 2007. Improve computeraided diagnosis with machine learning techniques using undiagnosed samples. IEEE Transactions on Systems, Man and Cybernetics, 37(6):1088–1098.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Mary Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="2824" citStr="Marcus et al., 1993" startWordPosition="432" endWordPosition="435">o improve a state-of-the-art POS tagger, SVMTool (Gimenez and Marquez, 2004). Finally, we introduce a variant of tri-training called tri-training with disagreement, which seems to perform equally well, but which imports much less unlabeled data and is therefore more efficient. 2 Tagging as classification This section describes our dataset and our input tagger. We also describe how stacking is used to reduce POS tagging to a classification task. Finally, we introduce the supervised learning algorithms used in our experiments. 2.1 Data We use the POS-tagged WSJ from the Penn Treebank Release 3 (Marcus et al., 1993) with the standard split: Sect. 0–18 is used for training, Sect. 19–21 for development, and Sect. 22–24 for testing. Since we need to train our classifiers on material distinct from the training material for our input POS tagger, we save Sect. 19 for training our classifiers. Finally, we use the (untagged) Brown corpus as our unlabeled data. The number of tokens we use for training, developing and testing the classifiers, and the amount of unlabeled data available to it, are thus: tokens train 44,472 development 103,686 test 129,281 unlabeled 1,170,811 205 Proceedings of the ACL 2010 Conferenc</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell Marcus, Mary Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Merialdo</author>
</authors>
<title>Tagging English text with a probabilistic model.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="783" citStr="Merialdo (1994)" startWordPosition="109" endWordPosition="111">ts to train part-of-speech taggers on a mixture of labeled and unlabeled data have failed. In this work stacked learning is used to reduce tagging to a classification task. This simplifies semisupervised training considerably. Our prefered semi-supervised method combines tri-training (Li and Zhou, 2005) and disagreement-based co-training. On the Wall Street Journal, we obtain an error reduction of 4.2% with SVMTool (Gimenez and Marquez, 2004). 1 Introduction Semi-supervised part-of-speech (POS) tagging is relatively rare, and the main reason seems to be that results have mostly been negative. Merialdo (1994), in a now famous negative result, attempted to improve HMM POS tagging by expectation maximization with unlabeled data. Clark et al. (2003) reported positive results with little labeled training data but negative results when the amount of labeled training data increased; the same seems to be the case in Wang et al. (2007) who use co-training of two diverse POS taggers. Huang et al. (2009) present positive results for self-training a simple bigram POS tagger, but results are considerably below state-of-the-art. Recently researchers have explored alternative methods. Suzuki and Isozaki (2008) </context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>Bernard Merialdo. 1994. Tagging English text with a probabilistic model. Computational Linguistics, 20(2):155–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tri Nguyen</author>
<author>Le Nguyen</author>
<author>Akira Shimazu</author>
</authors>
<title>Using semi-supervised learning for question classification.</title>
<date>2008</date>
<journal>Journal of Natural Language Processing,</journal>
<pages>15--3</pages>
<marker>Nguyen, Le Nguyen, Shimazu, 2008</marker>
<rawString>Tri Nguyen, Le Nguyen, and Akira Shimazu. 2008. Using semi-supervised learning for question classification. Journal of Natural Language Processing, 15:3–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ross Quinlan</author>
</authors>
<title>Programs for machine learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="5992" citStr="Quinlan (1993)" startWordPosition="953" endWordPosition="954">by Unsupos refer to clusters; ”*” marks out-of-vocabulary words. the-shelf implementations of supervised learning algorithms. Specifically we have experimented with support vector machines (SVMs), decision trees, bagging and random forests. Tri-training, explained below, is a semi-supervised learning method which requires large amounts of data. Consequently, we only used very fast learning algorithms in the context of tri-training. On the development section, decisions trees performed better than bagging and random forests. The decision tree algorithm is the C4.5 algorithm first introduced in Quinlan (1993). We used SVMs with polynomial kernels of degree 2 to provide a stronger stacking-only baseline. 3 Tri-training This section first presents the tri-training algorithm originally proposed by Li and Zhou (2005) and then considers a novel variant: tri-training with disagreement. Let L denote the labeled data and U the unlabeled data. Assume that three classifiers c1, c2, c3 (same learning algorithm) have been trained on three bootstrap samples of L. In tri-training, an unlabeled datapoint in U is now labeled for a classifier, say c1, if the other two classifiers agree on its label, i.e. c2 and c3</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>Ross Quinlan. 1993. Programs for machine learning. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>Maximum entropy models for natural language ambiguity resolution.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="11271" citStr="Ratnaparkhi, 1998" startWordPosition="1842" endWordPosition="1843">on a large pool of unlabeled data (but only once). Of course this is not a standard self-training set-up, but self-training informed by unsupervised word clusters. 4.1 Follow-up experiments SVMTool is one of the most accurate POS taggers available. This means that the predictions that are added to the labeled data are of very high quality. To test if our semi-supervised learning methods were sensitive to the quality of the input taggers we repeated the self-training and tri-training experiments with a less competitive POS tagger, namely the maximum entropy-based POS tagger first described in (Ratnaparkhi, 1998) that comes with the maximum entropy library in (Zhang, 2004). Results are presented as the second line in Figure 2. Note that error reduction is much lower in this case. 1,170,811 173 207 BL stacking tri-tr. self-tr. co-forests tri-disagr error red. p-value SVMTool 97.15% 97.19% 97.27% 97.26% 97.13% 97.27% 4.21% &lt;0.0001 MaxEnt 96.31% - 96.36% 96.36% 96.28% 96.36% 1.36% &lt;0.0001 Figure 2: Results on Wall Street Journal Sect. 22-24 with different semi-supervised methods. 5 Conclusion This paper first shows how stacking can be used to reduce POS tagging to a classification task. This reduction se</context>
</contexts>
<marker>Ratnaparkhi, 1998</marker>
<rawString>Adwait Ratnaparkhi. 1998. Maximum entropy models for natural language ambiguity resolution. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Søgaard</author>
</authors>
<title>Ensemble-based POS tagging of italian.</title>
<date>2009</date>
<booktitle>In IAAI-EVALITA, Reggio Emilia,</booktitle>
<location>Italy.</location>
<contexts>
<context position="1883" citStr="Søgaard (2009)" startWordPosition="280" endWordPosition="281">derably below state-of-the-art. Recently researchers have explored alternative methods. Suzuki and Isozaki (2008) introduce a semi-supervised extension of conditional random fields that combines supervised and unsupervised probability models by so-called MDF parameter estimation, which reduces error on Wall Street Journal (WSJ) standard splits by about 7% relative to their supervised baseline. Spoustova et al. (2009) use a new pool of unlabeled data tagged by an ensemble of state-of-the-art taggers in every training step of an averaged perceptron POS tagger with 4–5% error reduction. Finally, Søgaard (2009) stacks a POS tagger on an unsupervised clustering algorithm trained on large amounts of unlabeled data with mixed results. This work combines a new semi-supervised learning method to POS tagging, namely tritraining (Li and Zhou, 2005), with stacking on unsupervised clustering. It is shown that this method can be used to improve a state-of-the-art POS tagger, SVMTool (Gimenez and Marquez, 2004). Finally, we introduce a variant of tri-training called tri-training with disagreement, which seems to perform equally well, but which imports much less unlabeled data and is therefore more efficient. 2</context>
</contexts>
<marker>Søgaard, 2009</marker>
<rawString>Anders Søgaard. 2009. Ensemble-based POS tagging of italian. In IAAI-EVALITA, Reggio Emilia, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Drahomira Spoustova</author>
<author>Jan Hajic</author>
<author>Jan Raab</author>
<author>Miroslav Spousta</author>
</authors>
<title>Semi-supervised training for the averaged perceptron POS tagger. In EACL,</title>
<date>2009</date>
<location>Athens, Greece.</location>
<contexts>
<context position="1689" citStr="Spoustova et al. (2009)" startWordPosition="247" endWordPosition="250">seems to be the case in Wang et al. (2007) who use co-training of two diverse POS taggers. Huang et al. (2009) present positive results for self-training a simple bigram POS tagger, but results are considerably below state-of-the-art. Recently researchers have explored alternative methods. Suzuki and Isozaki (2008) introduce a semi-supervised extension of conditional random fields that combines supervised and unsupervised probability models by so-called MDF parameter estimation, which reduces error on Wall Street Journal (WSJ) standard splits by about 7% relative to their supervised baseline. Spoustova et al. (2009) use a new pool of unlabeled data tagged by an ensemble of state-of-the-art taggers in every training step of an averaged perceptron POS tagger with 4–5% error reduction. Finally, Søgaard (2009) stacks a POS tagger on an unsupervised clustering algorithm trained on large amounts of unlabeled data with mixed results. This work combines a new semi-supervised learning method to POS tagging, namely tritraining (Li and Zhou, 2005), with stacking on unsupervised clustering. It is shown that this method can be used to improve a state-of-the-art POS tagger, SVMTool (Gimenez and Marquez, 2004). Finally</context>
</contexts>
<marker>Spoustova, Hajic, Raab, Spousta, 2009</marker>
<rawString>Drahomira Spoustova, Jan Hajic, Jan Raab, and Miroslav Spousta. 2009. Semi-supervised training for the averaged perceptron POS tagger. In EACL, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
</authors>
<title>Semi-supervised sequential labeling and segmentation using gigaword scale unlabeled data. In</title>
<date>2008</date>
<booktitle>ACL,</booktitle>
<pages>665--673</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="1382" citStr="Suzuki and Isozaki (2008)" startWordPosition="202" endWordPosition="205"> negative. Merialdo (1994), in a now famous negative result, attempted to improve HMM POS tagging by expectation maximization with unlabeled data. Clark et al. (2003) reported positive results with little labeled training data but negative results when the amount of labeled training data increased; the same seems to be the case in Wang et al. (2007) who use co-training of two diverse POS taggers. Huang et al. (2009) present positive results for self-training a simple bigram POS tagger, but results are considerably below state-of-the-art. Recently researchers have explored alternative methods. Suzuki and Isozaki (2008) introduce a semi-supervised extension of conditional random fields that combines supervised and unsupervised probability models by so-called MDF parameter estimation, which reduces error on Wall Street Journal (WSJ) standard splits by about 7% relative to their supervised baseline. Spoustova et al. (2009) use a new pool of unlabeled data tagged by an ensemble of state-of-the-art taggers in every training step of an averaged perceptron POS tagger with 4–5% error reduction. Finally, Søgaard (2009) stacks a POS tagger on an unsupervised clustering algorithm trained on large amounts of unlabeled </context>
</contexts>
<marker>Suzuki, Isozaki, 2008</marker>
<rawString>Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised sequential labeling and segmentation using gigaword scale unlabeled data. In ACL, pages 665–673, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen Wang</author>
<author>Zhongqiang Huang</author>
<author>Mary Harper</author>
</authors>
<title>Semi-supervised learning for part-of-speech tagging of Mandarin transcribed speech.</title>
<date>2007</date>
<booktitle>In ICASSP,</booktitle>
<location>Hawaii.</location>
<contexts>
<context position="1108" citStr="Wang et al. (2007)" startWordPosition="163" endWordPosition="166">based co-training. On the Wall Street Journal, we obtain an error reduction of 4.2% with SVMTool (Gimenez and Marquez, 2004). 1 Introduction Semi-supervised part-of-speech (POS) tagging is relatively rare, and the main reason seems to be that results have mostly been negative. Merialdo (1994), in a now famous negative result, attempted to improve HMM POS tagging by expectation maximization with unlabeled data. Clark et al. (2003) reported positive results with little labeled training data but negative results when the amount of labeled training data increased; the same seems to be the case in Wang et al. (2007) who use co-training of two diverse POS taggers. Huang et al. (2009) present positive results for self-training a simple bigram POS tagger, but results are considerably below state-of-the-art. Recently researchers have explored alternative methods. Suzuki and Isozaki (2008) introduce a semi-supervised extension of conditional random fields that combines supervised and unsupervised probability models by so-called MDF parameter estimation, which reduces error on Wall Street Journal (WSJ) standard splits by about 7% relative to their supervised baseline. Spoustova et al. (2009) use a new pool of </context>
</contexts>
<marker>Wang, Huang, Harper, 2007</marker>
<rawString>Wen Wang, Zhongqiang Huang, and Mary Harper. 2007. Semi-supervised learning for part-of-speech tagging of Mandarin transcribed speech. In ICASSP, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Wolpert</author>
</authors>
<title>Stacked generalization.</title>
<date>1992</date>
<journal>Neural Networks,</journal>
<pages>5--241</pages>
<contexts>
<context position="5062" citStr="Wolpert, 1992" startWordPosition="817" endWordPosition="818"> lists the gold standard POS tag, the predicted POS tag and the word cluster selected by Unsupos. For example, the first word is labeled ’DT’, which SVMTool correctly predicts, and it belongs to cluster 17 of about 500 word clusters. The first column is blank in the table for the unlabeled section. Generally, the idea is that a classifier will learn to trust SVMTool in some cases, but that it may also learn that if SVMTool predicts a certain tag for some word cluster the correct label is another tag. This way of combining taggers into a single end classifier can be seen as a form of stacking (Wolpert, 1992). It has the advantage that it reduces POS tagging to a classification task. This may simplify semi-supervised learning considerably. 2.4 Learning algorithms We assume some knowledge of supervised learning algorithms. Most of our experiments are implementations of wrapper methods that call off1The numbers provided by Unsupos refer to clusters; ”*” marks out-of-vocabulary words. the-shelf implementations of supervised learning algorithms. Specifically we have experimented with support vector machines (SVMs), decision trees, bagging and random forests. Tri-training, explained below, is a semi-su</context>
</contexts>
<marker>Wolpert, 1992</marker>
<rawString>David Wolpert. 1992. Stacked generalization. Neural Networks, 5:241–259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Le Zhang</author>
</authors>
<title>Maximum entropy modeling toolkit for Python and C++.</title>
<date>2004</date>
<institution>University of Edinburgh.</institution>
<marker>Le Zhang, 2004</marker>
<rawString>Le Zhang. 2004. Maximum entropy modeling toolkit for Python and C++. University of Edinburgh.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>