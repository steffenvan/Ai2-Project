<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.979436">
String Extension Learning
</title>
<author confidence="0.998907">
Jeffrey Heinz
</author>
<affiliation confidence="0.999025">
University of Delaware
</affiliation>
<address confidence="0.776623">
Newark, Delaware, USA
</address>
<email confidence="0.995745">
heinz@udel.edu
</email>
<sectionHeader confidence="0.997353" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999977181818182">
This paper provides a unified, learning-
theoretic analysis of several learnable
classes of languages discussed previously
in the literature. The analysis shows that
for these classes an incremental, globally
consistent, locally conservative, set-driven
learner always exists. Additionally, the
analysis provides a recipe for constructing
new learnable classes. Potential applica-
tions include learnable models for aspects
of natural language and cognition.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998565666666667">
The problem of generalizing from examples to
patterns is an important one in linguistics and
computer science. This paper shows that many
disparate language classes, many previously dis-
cussed in the literature, have a simple, natural
and interesting (because non-enumerative) learner
which exactly identifies the class in the limit from
distribution-free, positive evidence in the sense of
Gold (Gold, 1967).1 These learners are called
String Extension Learners because each string in
the language can be mapped (extended) to an ele-
ment of the grammar, which in every case, is con-
ceived as a finite set of elements. These learners
have desirable properties: they are incremental,
globally consistent, and locally conservative.
Classes previously discussed in the litera-
ture which are string extension learnable in-
clude the Locally Testable (LT) languages, the
Locally Testable Languages in the Strict Sense
1The allowance of negative evidence (Gold, 1967) or re-
stricting the kinds of texts the learner is required to succeed
on (i.e. non-distribution-free evidence) (Gold, 1967; Horn-
ing, 1969; Angluin, 1988) admits the learnability of the class
of recursively enumerable languages. Classes of languages
learnable in the harder, distribution-free, positive-evidence-
only settings are due to structural properties of the language
classes that permit generalization (Angluin, 1980b; Blumer
et al., 1989). That is the central interest here.
(Strictly Local, SL) (McNaughton and Papert,
1971; Rogers and Pullum, to appear), the Piece-
wise Testable (PT) languages (Simon, 1975), the
Piecewise Testable languages in the Strict Sense
(Strictly Piecewise, SP) (Rogers et al., 2009), the
Strongly Testable languages (Beauquier and Pin,
1991), the Definite languages (Brzozowski, 1962),
and the Finite languages, among others. To our
knowledge, this is the first analysis which identi-
fies the common structural elements of these lan-
guage classes which allows them to be identifiable
in the limit from positive data: each language class
induces a natural partition over all logically possi-
ble strings and each language in the class is the
union of finitely many blocks of this partition.
One consequence of this analysis is a recipe
for constructing new learnable classes. One no-
table case is the Strictly Piecewise (SP) languages,
which was originally motivated for two reasons:
the learnability properties discussed here and its
ability to describe long-distance dependencies in
natural language phonology (Heinz, 2007; Heinz,
to appear). Later this class was discovered to have
several independent characterizations and form
the basis of another subregular hierarchy (Rogers
et al., 2009).
It is expected string extension learning will have
applications in linguistic and cognitive models. As
mentioned, the SP languages already provide a
novel hypothesis of how long-distance dependen-
cies in sound patterns are learned. Another exam-
ple is the Strictly Local (SL) languages which are
the categorical, symbolic version of n-gram mod-
els, which are widely used in natural language pro-
cessing (Jurafsky and Martin, 2008). Since the SP
languages also admit a probabilistic variant which
describe an efficiently estimable class of distribu-
tions (Heinz and Rogers, 2010), it is plausible to
expect the other classes will as well, though this is
left for future research.
String extension learners are also simple, mak-
</bodyText>
<page confidence="0.969475">
897
</page>
<note confidence="0.943706">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 897–906,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.998804538461538">
ing them accessible to linguists without a rigorous
mathematical background.
This paper is organized as follow. §2 goes
over basic notation and definitions. §3 defines
string extension grammars, languages, and lan-
guage classes and proves some of their fundamen-
tal properties. §4 defines string extension learn-
ers and proves their behavior. §5 shows how im-
portant subregular classes are string extension lan-
guage classes. §6 gives examples of nonregular
and infinite language classes which are string ex-
tension learnable. §7 summarizes the results, and
discusses lines of inquiry for future research.
</bodyText>
<sectionHeader confidence="0.997162" genericHeader="introduction">
2 Preliminaries
</sectionHeader>
<bodyText confidence="0.9998874">
This section establishes notation and recalls basic
definitions for formal languages, the paradigm of
identification in the limit from positive data (Gold,
1967). Familiarity with the basic concepts of sets,
functions, and sequences is assumed.
</bodyText>
<listItem confidence="0.5521656">
For some set A, P(A) denotes the set of all
subsets of A and Pfin(A) denotes the set of all
finite subsets of A. If f is a function such that
f : A → B then let f°(a) = {f(a)}. Thus,
f° : A → P(B) (note f° is not surjective). A
</listItem>
<bodyText confidence="0.98750308">
set 7r of nonempty subsets of S is a partition of S
iff the elements of 7r (called blocks) are pairwise
disjoint and their union equals S.
E denotes a fixed finite set of symbols, the al-
phabet. Let En, Ern, E*, E+ denote all strings
formed over this alphabet of length n, of length
less than or equal to n, of any finite length, and
of any finite length strictly greater than zero, re-
spectively. The term word is used interchangeably
with string. The range of a string w is the set
of symbols which are in w. The empty string is
the unique string of length zero denoted λ. Thus
range(λ) = ∅. The length of a string u is de-
noted by |u|, e.g. |λ |= 0. A language L is
some subset of E*. The reverse of a language
Lr = {wr : w ∈ L}.
Gold (1967) establishes a learning paradigm
known as identification in the limit from positive
data. A text is an infinite sequence whose ele-
ments are drawn from E* ∪ {#} where # rep-
resents a non-expression. The ith element of t is
denoted t(i), and t[i] denotes the finite sequence
t(0), t(1), ... t(i). Following Jain et al. (1999),
let SEQ denote the set of all possible finite se-
quences:
</bodyText>
<equation confidence="0.8158685">
SEQ = {t[i] : t is a text and i ∈N}
The content of a text is defined below.
content(t) =
{w ∈ E* : ∃n ∈Nsuch that t(n) = w}
</equation>
<bodyText confidence="0.979971857142857">
A text t is a positive text for a language L iff
content(t) = L. Thus there is only one text t for
the empty language: for all i, t(i) = #.
A learner is a function 0 which maps ini-
tial finite sequences of texts to grammars,
i.e. 0 : SEQ → G. The elements of G (the gram-
mars) generate languages in some well-defined
way. A learner converges on a text t iff there exists
i ∈Nand a grammar G such that for all j &gt; i,
0(t[j]) = G.
For any grammar G, the language it generates is
denoted L(G). A learner 0 identifies a language
L in the limit iff for any positive text t for L, 0
converges on t to grammar G and L(G) = L. Fi-
nally, a learner 0 identifies a class of languages L
in the limit iff for any L ∈ L, 0 identifies L in
the limit. Angluin (1980b) provides necessary and
sufficient properties of language classes which are
identifiable in the limit from positive data.
A learner 0 of language class L is globally con-
sistent iff for each i and for all texts t for some
L ∈ L, content(t[i]) ⊆ L(0(t[i])). A learner 0 is
locally conservative iff for each i and for all texts
t for some L ∈ L, whenever 0(t[i]) =6 0(t[i − 1]),
it is the case that t(i) ∈6 L(0([i−1])). These terms
are from Jain et al. (2007). Also, learners which
do not depend on the order of the text are called
set-driven (Jain et al., 1999, p. 99).
</bodyText>
<sectionHeader confidence="0.974693" genericHeader="method">
3 Grammars and Languages
</sectionHeader>
<bodyText confidence="0.9501919">
Consider some set A. A string extension function
is a total function f : E* → Pfin(A). It is not
required that f be onto. Denote the class of func-
tions which have this general form SEF.
Each string extension function is naturally as-
sociated with some formal class of grammars and
languages. These functions, grammars, and lan-
guages are called string extension functions, gram-
mars, and languages, respectively.
Definition 1 Let f ∈ SEF.
</bodyText>
<listItem confidence="0.9808495">
1. A grammar is a finite subset of A.
2. The language ofgrammar G is
</listItem>
<equation confidence="0.948569">
Lf(G) = {w ∈ E* : f(w) ⊆ G}
</equation>
<page confidence="0.926071">
898
</page>
<bodyText confidence="0.537558">
3. The class of languages obtained by all possi-
ble grammars is
</bodyText>
<equation confidence="0.929176">
Lf = {Lf(G) : G E Pfin(A)I
</equation>
<bodyText confidence="0.995346363636364">
The subscript f is omitted when it is understood
from context.
A function f E S£F naturally induces a par-
tition πf over Enn&apos;*. Strings u and v are equivalent
(u —f v) iff f (W) = f(v).
Theorem 1 Every language L E Lf is a finite
union of blocks of πf.
Proof: Follows directly from the definition of —f
and the finiteness of string extension grammars. ❑
We return to this result in §6.
Theorem 2 Lf is closed under intersection.
Proof: We show L1nL2 = L(G1nG2). Consider
any word w belonging to L1 and L2. Then f(w)
is a subset of G1 and of G2. Thus f(w) C G1 n
G2, and therefore w E L(G1 n G2). The other
inclusion follows similarly. ❑
String extension language classes are not in gen-
eral closed under union or reversal (counterexam-
ples to union closure are given in §5.1 and to re-
versal closure in §6.)
It is useful to extend the domain of the function
f from strings to languages.
</bodyText>
<equation confidence="0.9976285">
f(L) = U f(w) (1)
wEL
</equation>
<bodyText confidence="0.92345675">
An element g of grammar G for language L =
Lf(G) is useful iff g E f(L). An element is use-
less if it is not useful. A grammar with no useless
elements is called canonical.
</bodyText>
<equation confidence="0.700749">
Remark 1 Fix a function f E S£F. For every
L E Lf, there is a canonical grammar, namely
f(L). In other words, L = L(f(L)).
Lemma 1 Let L, L′ E Lf. L C L′ iff f (L) C
f (L′) — —
Proof: (==&gt;-) Suppose L C L′ and consider any
g E f(L). Since g is useful, there is a w E L such
that g E f(w). But f(w) C f(L′) since w E L′.
(.&lt;--) Suppose f(L) C f(L′) and consider any
w E L. Then f(w) C f(L) so by transitivity,
f(w) C f(L′). Therefore w E L′. ❑
</equation>
<bodyText confidence="0.996369125">
The significance of this result is that as the gram-
mar G monotonically increases, the language
L(G) monotonically increases too. The following
result can now be proved, used in the next section
on learning.2
Theorem 3 For any finite L0 C E*, L =
L(f(L0)) is the smallest language in Lf contain-
ing L0.
</bodyText>
<figure confidence="0.749224666666667">
Proof: Clearly L0 C L. Suppose L′ E Lf and
L0 C L′. It follows directly from Lemma 1 that
L C L′ (since f(L) = f(L0) C f(L′)). ❑
</figure>
<sectionHeader confidence="0.815427" genericHeader="method">
4 String Extension Learning
</sectionHeader>
<bodyText confidence="0.998923285714286">
Learning string extension classes is simple. The
initial hypothesis of the learner is the empty gram-
mar. The learner’s next hypothesis is obtained by
applying function f to the current observation and
taking the union of that set with the previous one.
Definition 2 For all f E S£F and for all t E
SEQ, define φf as follows:
</bodyText>
<equation confidence="0.990645">
φf(t[i]) = { 0 if i = −1
φf (t[i − 1]) if t(i) = #
φf (t[i − 1]) U f(t(i)) otherwise
</equation>
<bodyText confidence="0.979604586206897">
By convention, the initial state of the grammar
is given by φ(t[−1]) = 0. The learner φf exem-
plifies string extension learning. Each individual
string in the text reveals, by extension with f, as-
pects of the canonical grammar for L E Lf.
Theorem 4 φf is globally consistent, locally con-
servative, and set-driven.
Proof: Global consistness and local conservative-
ness follow immediately from Definition 2. For
set-drivenness, witness (by Definition 2) it is the
case that for any text t and any i EN, φ(t[i]) =
f(content(t[i])). ❑
The key to the proof that φf identifies Lf in the
limit from positive data is the finiteness of G for
all L(G) E L. The idea is that there is a point
in the text in which every element of the grammar
has been seen because (1) there are only finitely
many useful elements of G, and (2) the learner is
guaranteed to see a word in L which yields (via f)
each element of G at some point (since the learner
receives a positive text for L). Thus at this point
2The requirement in Theorem 3 that Lo be finite can be
dropped if the qualifier “in Lf ” be dropped as well. This
can be seen when one considers the identity function and the
class of finite languages. (The identity function is a string
extension function, see §6.) In this case, id(E∗) = E∗, but
E∗ is not a member of Lfin. However since the interest here
is learners which generalize on the basis of finite experience,
Theorem 3 is sufficient as is.
</bodyText>
<page confidence="0.994023">
899
</page>
<bodyText confidence="0.9926866">
the learner O is guaranteed to have converged to
the target G as no additional words will add any
more elements to the learner’s grammar.
Lemma 2 For all L E Lf, there is a finite sample
S such that L is the smallest language in Lf con-
taining S. S is called a characteristic sample of L
in Lf (S is also called a tell-tale).
Proof: For L E Lf, construct the sample S as
follows. For each g E f(L), choose some word
w E L such that g E f(w). Since f(L) is finite
(Remark 1), S is finite. Clearly f(S) = f(L) and
thus L = L(f(S)). Therefore, by Theorem 3, L is
the smallest language in Lf containing S. ❑
Theorem 5 Fix f E SEF. Then Of identifies Lf
in the limit.
Proof: For any L E Lf , there is a characteristic fi-
nite sample S for L (Lemma 2). Thus for any text t
for L, there is i such that S C content(t[i]). Thus
for any j &gt; i, O(t(j)) is the smallest language
in Lf containing S by Theorem 3 and Lemma 2.
</bodyText>
<equation confidence="0.618148">
Thus, O(t(j)) = f(S) = f(L). ❑
</equation>
<bodyText confidence="0.999941727272727">
An immediate corollary is the efficiency of Of
in the length of the sample, provided f is efficient
in the length of the string (de la Higuera, 1997).
Corollary 1 Of is efficient in the length of the
sample iff f is efficiently computable in the length
of a string.
To summarize: string extension grammars are
finite subsets of some set A. The class of lan-
guages they generate are determined by a func-
tion f which maps strings to finite subsets of A
(chunks of grammars). Since the size of the canon-
ical grammars is finite, a learner which develops a
grammar on the basis of the observed words and
the function f identifies this class exactly in the
limit from positive data. It also follows that if f
is efficient in the length of the string then Of is ef-
ficient in the length of the sample and that Of is
globally consistent, locally conservative, and set-
driven. It is striking that such a natural and gen-
eral framework for generalization exists and that,
as will be shown, a variety of language classes can
be expressed given the choice of f.
</bodyText>
<sectionHeader confidence="0.992117" genericHeader="method">
5 Subregular examples
</sectionHeader>
<bodyText confidence="0.999728142857143">
This section shows how classes which make up
the subregular hierarchies (McNaughton and Pa-
pert, 1971) are string extension language classes.
Readers are referred to Rogers and Pullum (2007)
and Rogers et al. (2009) for an introduction to the
subregular hierarchies, as well as their relevance
to linguistics and cognition.
</bodyText>
<subsectionHeader confidence="0.906238">
5.1 K-factor languages
</subsectionHeader>
<bodyText confidence="0.8681945">
The k-factors of a word are the contiguous subse-
quences of length k in w. Consider the following
string extension function.
Definition 3 For some k EN, let
</bodyText>
<equation confidence="0.9953225">
fack(w) =
{x E Ek : ∃u,v E E*
</equation>
<bodyText confidence="0.96068721875">
such that w = uxv} when k ≤ |w |and
{w} otherwise
Following the earlier definitions, for some k, a
grammar G is a subset of E≤k and a word w be-
longs to the language of G iff fack(w) C G.
Example 1 Let E = {a, b} and consider gram-
mars G = {λ, a, aa, ab, ba}. Then L(G) =
{λ, a} U {w : |w |≥ 2 and w E� E*bbE*}. The 2-
factor bb is aprohibited 2-factor for L(G). Clearly,
L(G) E Lfac2.
Languages in Lfack make distinctions based on
which k-factors are permitted or prohibited. Since
fack E SEF, it follows immediately from the
results in §§3-4 that the k-factor languages are
closed under intersection, and each has a char-
acteristic sample. For example, a characteristic
sample for the 2-factor language in Example 1 is
{λ, a, ab, ba, aa}; i.e. the canonical grammar it-
self. It follows from Theorem 5 that the class of
k-factor languages is identifiable in the limit by
Ofack. The learner Ofac2 with a text from the lan-
guage in Example 1 is illustrated in Table 1.
The class Lfack is not closed under
union. For example for k = 2, con-
sider L1 = L({λ, a, b, aa, bb, ba}) and
L2 = L({λ, a, b, aa, ab, bb}). Then L1 U L2
excludes string aba, but includes ab and ba, which
is not possible for any L E Lfack.
K-factors are used to define other language
classes, such as the Strictly Local and Lo-
cally Testable languages (McNaughton and Pa-
pert, 1971), discussed in §5.4 and §5.5.
</bodyText>
<subsectionHeader confidence="0.999288">
5.2 Strictly k-Piecewise languages
</subsectionHeader>
<bodyText confidence="0.99985">
The Strictly k-Piecewise (SPk) languages (Rogers
et al., 2009) can be defined with a function whose
co-domain is P(E≤k). However unlike the func-
tion fack, the function SPk, does not require that
the k-length subsequences be contiguous.
</bodyText>
<page confidence="0.952284">
900
</page>
<table confidence="0.999485">
i t(i) fac2(t(i)) Grammar G L(G)
-1 ∅ ∅
0 aaaa {aa} {aa} aaa∗
1 aab {aa, ab} {aa, ab} aaa∗ ∪ aaa∗b
2 a {a} {a, aa, ab} aa∗ ∪ aa∗b
. . .
</table>
<tableCaption confidence="0.8836535">
Table 1: The learner φfac2 with a text from the language in Example 1. Boldtype indicates newly added
elements to the grammar.
</tableCaption>
<bodyText confidence="0.9911698">
A string u = a1 ... ak is a subsequence of
string w iff ∃ v0, v1, ... vk ∈ E∗ such that w =
v0a1v1 ... akvk. The empty string λ is a subse-
quence of every string. When u is a subsequence
of w we write u ⊑ w.
</bodyText>
<equation confidence="0.783682">
Definition 4 For some k ∈N,
SPk(w) = {u ∈ E≤k : u ⊑ w}
</equation>
<bodyText confidence="0.990865041666667">
In other words, SPk(w) returns all subse-
quences, contiguous or not, in w up to length k.
Thus, for some k, a grammar G is a subset of E≤k.
Following Definition 1, a word w belongs to the
language of G only if SP2(w) ⊆ G.3
Example 2 Let E = {a, b} and consider the
grammar G = {λ, a, b, aa, ab, ba}. Then L(G) =
E∗\(E∗bE∗bE∗).
As seen from Example 2, SP languages encode
long-distance dependencies. In Example 2, L pro-
hibits a b from following another b in a word, no
matter how distant. Table 2 illustrates φSP2 learn-
ing the language in Example 2.
Heinz (2007,2009a) shows that consonantal
harmony patterns in natural language are describ-
able by such SP2 languages and hypothesizes
that humans learn them in the way suggested by
φSP2. Strictly 2-Piecewise languages have also
been used in models of reading comprehension
(Whitney, 2001; Grainger and Whitney, 2004;
Whitney and Cornelissen, 2008) as well as text
classification(Lodhi et al., 2002; Cancedda et al.,
2003) (see also (Shawe-Taylor and Christianini,
2005, chap. 11)).
</bodyText>
<subsectionHeader confidence="0.997551">
5.3 K-Piecewise Testable languages
</subsectionHeader>
<bodyText confidence="0.999257">
A language L is k-Piecewise Testable iff when-
ever strings u and v have the same subsequences
</bodyText>
<footnote confidence="0.6018115">
3In earlier work, the function SP2 has been described
as returning the set of precedence relations in w, and the
language class LSP2 was called the precedence languages
(Heinz, 2007; Heinz, to appear).
</footnote>
<bodyText confidence="0.998818444444444">
of length at most k and u is in L, then v is in L as
well (Simon, 1975; Simon, 1993; Lothaire, 2005).
A language L is said to be Piecewise-Testable
(PT) if it is k-Piecewise Testable for some k ∈N.
If k is fixed, the k-Piecewise Testable languages
are identifiable in the limit from positive data
(Garcia and Ruiz, 1996; Garcia and Ruiz, 2004).
More recently, the Piecewise Testable languages
has been shown to be linearly separable with a
subsequence kernel (Kontorovich et al., 2008).
The k-Piecewise Testable languages can also
be described with the function SPk⋄ . Recall that
f⋄(a) = {f(a)}. Thus functions SPk⋄ define
grammars as a finite list of sets of subsequences
up to length k that may occur in words in the lan-
guage. This reflects the fact that the k-Piecewise
Testable languages are the boolean closure of the
Strictly k-Piecewise languages.4
</bodyText>
<subsectionHeader confidence="0.994623">
5.4 Strictly k-Local languages
</subsectionHeader>
<bodyText confidence="0.989249666666667">
To define the Strictly k-Local languages, it is nec-
essary to make a pointwise extension to the defini-
tions in §3.
Definition 5 For sets A1,... , An, suppose for
(each i, fi : E∗ → Pfin(Ai), and let f =
(f1,..., fn).
</bodyText>
<listItem confidence="0.996436222222222">
1. A grammar G is a tuple (G1, ... , Gn) where
G1 ∈ Pfin(A1), ..., Gn ∈ Pfin(An).
2. If for any w ∈ E∗, each fi(w) ⊆ Gi for all
1 ≤ i ≤ n, then f(w) is a pointwise subset
of G, written f(w) ⊆· G.
3. The language ofgrammar G is
Lf(G) = {w : f(w) ⊆· G}
4. The class of languages obtained by all such
possible grammars G is Lf.
</listItem>
<footnote confidence="0.9476515">
4More generally, it is not hard to show that Lf⋄ is the
boolean closure of Lf.
</footnote>
<page confidence="0.961564">
901
</page>
<table confidence="0.998984285714286">
i t(i) SP2(t(i)) Grammar G Language of G
-1 0 0
0 aaaa {A, a, aa} {A, a, aa} a*
1 aab {A, a, b, aa, ab} {A, a, aa, b, ab} a* U a*b
2 baa {A, a, b, aa, ba} {A, a, b, aa, ab, ba} E*\(E*bE*bE*)
3 aba {A, a, b, ab, ba} {A, a, b, aa, ab, ba} E*\(E*bE*bE*)
. . .
</table>
<tableCaption confidence="0.862641">
Table 2: The learner OSP2 with a text from the language in Example 2. Boldtype indicates newly added
elements to the grammar.
</tableCaption>
<bodyText confidence="0.9981795">
These definitions preserve the learning results
of §4. Note that the characteristic sample of L E
Lf will be the union of the characteristic samples
of each fZ and the language Lf (G) is the intersec-
tion of Lf,(GZ).
Locally k-Testable Languages in the Strict
Sense (Strictly k-Local) have been studied by sev-
eral researchers (McNaughton and Papert, 1971;
Garcia et al., 1990; Caron, 2000; Rogers and Pul-
lum, to appear), among others. We follow the
definitions from (McNaughton and Papert, 1971,
p. 14), effectively encoded in the following func-
tions.
Definition 6 Fix k EN. Then the (left-edge) pre-
fix of length k, the (right-edge) suffix of length k,
and the interior k-factors of a word w are
</bodyText>
<equation confidence="0.996651555555556">
Lk(w) = {u E Ek : ]v E E* such that w = uv}
Rk(w) = {u E Ek : ]v E E* such that w = vu}
Ik(w) = fack(w)\(Lk(w) U Rk(w))
Example 3 Suppose w = abcba. Then L2(w) =
{ab}, R2(w) = {ba} and I2(w) = {bc, cb}.
Example 4 Suppose |w |k. Then Lk(w) =
Rk(w) = {w} and Ik(w) = 0.
Example 5 Suppose |w |is less than k. Then
Lk(w) = Rk(w) = 0 and Ik(w) = {w}.
</equation>
<bodyText confidence="0.990448625">
A language L is k-Strictly Local (k-SL) iff for
all w E L, there exist sets L, R, and I such
that w E L iff Lk(w) C L, Rk(w) C R, and
Ik(w) C I. McNaughton and Papert note that if
w is of length less than k than L may be perfectly
arbitrary about w.
This can now be expressed as the string exten-
sion function:
</bodyText>
<equation confidence="0.988034">
LRIk(w) = (Lk(w),Rk(w),Ik(w))
</equation>
<bodyText confidence="0.994195428571429">
Thus for some k, a grammar G is triple formed
by taking subsets of Ek, Ek, and E&lt;k, respec-
tively. A word w belongs to the language of G
only if LRIk(w) C� G. Clearly, LLRIk = k-
SL, and henceforth we refer to this class as k-SL.
Since, for fixed k, LRIk E SEF, all of the learn-
ing results in §4 apply.
</bodyText>
<subsectionHeader confidence="0.991925">
5.5 Locally k-Testable languages
</subsectionHeader>
<bodyText confidence="0.999506625">
The Locally k-testable languages (k-LT) are orig-
inally defined in McNaughton and Papert (1971)
and are the subject of several studies (Brzozowski
and Simon, 1973; McNaughton, 1974; Kim et
al., 1991; Caron, 2000; Garcia and Ruiz, 2004;
Rogers and Pullum, to appear).
A language L is k-testable iff for all w1, w2 E
E* such that |w1 |≥ k and |w2 |≥ k, and
LRIk(w1) = LRIk(w2) then either both w1, w2
belong to L or neither do. Clearly, every language
in k-SL belongs to k-LT. However k-LT prop-
erly include k-SL because a k-testable language
only distinguishes words whenever LRIk(w1) =�
LRIk(w2). It is known that the k-LT languages
are the boolean closure of the k-SL (McNaughton
and Papert, 1971).
The function LRIk exactly expresses k-testable
languages. Informally, each word w is mapped
to a set containing a single element, this element
is the triple LRIk(w). Thus a grammar G is a
subset of the triples used to define k-SL. Clearly,
LLRI⋄k = k-LT since it is the boolean closure of
LLRIk. Henceforth we refer to LLRI⋄k as the k-
Locally Testable (k-LT) languages.
</bodyText>
<subsectionHeader confidence="0.971381">
5.6 Generalized subsequence languages
</subsectionHeader>
<bodyText confidence="0.999981875">
Here we introduce generalized subsequence func-
tions, a general class of functions to which the
SPk and fack functions belong. Like those
functions, generalized subsequence functions map
words to a set of subsequences found within the
words. These functions are instantiated by a vec-
tor whose number of coordinates determine how
many times a subsequence may be discontiguous
</bodyText>
<page confidence="0.995204">
902
</page>
<bodyText confidence="0.9918942">
and whose coordinate values determine the length
of each contiguous part of the subsequence.
Definition 7 For some n EN, let v~ =
(v0, v1, ... , vn), where each vi EN. Let k be
the length of the subsequences; i.e. k = En
</bodyText>
<equation confidence="0.9963885">
0 vi.
f;v(w) =
</equation>
<bodyText confidence="0.982260533333333">
{u E Σk : ]x0, ... , xn, u0, ... , un+1 E Σ*
such that w = u0x0u1x1, ... , unxnun+1
and |xi |= vi for all 0 &lt; i &lt; n}
when k &lt; |w|, and{w} otherwise
The following examples help make the general-
ized subsequence functions clear.
Example 6 Let v~ = (2). Then f(2) = fac2. Gen-
erally, f(k) = fack.
Example 7 Let v~ = (1, 1). Then f(1,1) = SP2.
Generally, if v~ = (1,... 1) with |~v |= k. Then
f;v = SPk.
Example 8 Let v~ = (3, 2,1) and a, b, c, d, e, fE
Σ. Then Lf(,,,2,1) includes languages which
prohibit strings w which contain subsequences
abcdef where abc and de must be contiguous in
w and abcdef is a subsequence of w.
Generalized subsequence languages make dif-
ferent kinds of distinctions to be made than PT and
LT languages. For example, the language in Ex-
ample 8 is neither k-LT nor k�-PT for any values
k, k�. Generalized subsequence languages prop-
erly include the k-SP and k-SL classes (Exam-
ples 6 and 7), and the boolean closure of the sub-
sequence languages (fv) properly includes the LT
and PT classes.
Since for any ~v, f;v and fv are string extension
functions the learning results in §4 apply. Note
that f;v(w) is computable in time O(|w|k) where k
is the length of the maximal subsequences deter-
mined by ~v.
</bodyText>
<sectionHeader confidence="0.996545" genericHeader="method">
6 Other examples
</sectionHeader>
<bodyText confidence="0.999714285714286">
This section provides examples of infinite and
nonregular language classes that are string exten-
sion learnable. Recall from Theorem 1 that string
extension languages are finite unions of blocks of
the partition of Σ* induced by f. Assuming the
blocks of this partition can be enumerated, the
range of f can be construed as Pfin(N).
</bodyText>
<table confidence="0.8711948">
grammar G Language of G
0 0
{0} anbn
{1} Σ*�anbn
{0, 1} Σ*
</table>
<tableCaption confidence="0.998806">
Table 3: The language class Lf from Example 9
</tableCaption>
<bodyText confidence="0.989801268292683">
In the examples considered so far, the enumera-
tion of the blocks is essentially encoded in partic-
ular substrings (or tuples of substrings). However,
much less clever enumerations are available.
Example 9 Let A = {0,1} and consider the fol-
lowing function:
f (w)=( 0 iff w E anbn
Sl 1 otherwise
The function f belongs to S£F because it is maps
strings to a finite co-domain. Lf has four lan-
guages shown in Table 3.
The language class in Example 9 is not regular be-
cause it includes the well-known context-free lan-
guage anbn. This collection of languages is also
not closed under reversal.
There are also infinite language classes that are
string extension language classes. Arguably the
simplest example is the class of finite languages,
denoted Lfin.
Example 10 Consider the function id which
maps words in Σ* to their singleton sets, i.e.
id(w) = {w}.5 A grammar G is then a finite
subset of Σ*, and so L(G) is just a finite set of
words in Σ*; in fact, L(G) = G. It follows that
Lid = Lfin.
It can be easily seen that the function id induces
the trivial partition over Σ*, and languages are
just finite unions of these blocks. The learner φid
makes no generalizations at all, and only remem-
bers what it has observed.
There are other more interesting infinite string
extension classes. Here is one relating to the
Parikh map (Parikh, 1966). For all a E Σ, let
fa(w) be the set containing n where n is the num-
ber of times the letter a occurs in the string w. For
5Strictly speaking, this is not the identity function per
se, but it is as close to the identity function as one can get
since string extension functions are defined as mappings from
strings to sets. However, once the domain of the function is
extended (Equation 1), then it follows that id is the identity
function when its argument is a set of strings.
</bodyText>
<page confidence="0.997319">
903
</page>
<bodyText confidence="0.99993816">
example fa(babab) = {2}. Thus fa is atotal func-
tion mapping strings to singleton sets of natural
numbers, so it is a string extension function. This
function induces an infinite partition of E*, where
the words in any particular block have the same
number of letters a. It is convenient to enumerate
the blocks according to how many occurrences of
the letter a may occur in words within the block.
Hence, B0 is the block whose words have no oc-
currences of a, B1 is the block whose words have
one occurrence of a, and so on.
In this case, a grammar G is a finite subset ofN,
e.g. {2, 3, 4}. L(G) is simply those words which
have either 2, 3, or 4, occurrences of the letter a.
Thus Lf. is an infinite class, which contains lan-
guages of infinite size, which is easily identified in
the limit from positive data by Of..
This section gave examples of nonregular and
nonfinite string extension classes by pursuing the
implications of Theorem 1, which established that
f E SEF partition E* into blocks of which lan-
guages are finite unions thereof. The string exten-
sion function f provides an effective way of en-
coding all languages L in Lf because f(L) en-
codes a finite set, the grammar.
</bodyText>
<sectionHeader confidence="0.922849" genericHeader="conclusions">
7 Conclusion and open questions
</sectionHeader>
<bodyText confidence="0.99994946875">
One contribution of this paper is a unified way of
thinking about many formal language classes, all
of which have been shown to be identifiable in
the limit from positive data by a string extension
learner. Another contribution is a recipe for defin-
ing classes of languages identifiable in the limit
from positive data by this kind of learner.
As shown, these learners have many desirable
properties. In particular, they are globally consis-
tent, locally conservative, and set-driven. Addi-
tionally, the learner is guaranteed to be efficient
in the size of the sample, provided the function f
itself is efficient in the length of the string.
Several additional questions of interest remain
open for theoretical linguistics, theoretical com-
puter science, and computational linguistics.
For theoretical linguistics, it appears that the
string extension function f = (LRI3i P2), which
defines a class of languages which obey restric-
tions on both contiguous subsequences of length
3 and on discontiguous subsequences of length 2,
provides a good first approximation to the seg-
mental phonotactic patterns in natural languages
(Heinz, 2007). The string extension learner for
this class is essentially two learners: OLxz3 and
OPt, operating simultaneously.6 The learners
make predictions about generalizations, which can
be tested in artificial language learning experi-
ments on adults and infants (Rogers and Pullum, to
appear; Chambers et al., 2002; Onishi et al., 2003;
Cristi´a and Seidl, 2008).7
For theoretical computer science, it remains an
open question what property holds of functions
f in SEF to ensure that Lf is regular, context-
free, or context-sensitive. For known subregular
classes, there are constructions that provide deter-
ministic automata that suggest the relevant prop-
erties. (See, for example, Garcia et al. (1990) and
Garica and Ruiz (1996).)
Also, Timo K¨otzing and Samuel Moelius (p.c.)
suggest that the results here may be generalized
along the following lines. Instead of defining the
function f as a map from strings to finite subsets,
let f be a function from strings to elements of a
lattice. A grammar G is an element of the lattice
and the language of the G are all strings w such
that f maps w to a grammar less than G. Learners
Of are defined as the least upper bound of its cur-
rent hypothesis and the grammar to which f maps
the current word.8 Kasprzik and K¨otzing (2010)
develop this idea and demonstrate additional prop-
erties of string extension classes and learning, and
show that the pattern languages (Angluin, 1980a)
form a string extension class.9
Also, hyperplane learning (Clark et al., 2006a;
Clark et al., 2006b) and function-distinguishable
learning (Fernau, 2003) similarly associate lan-
guage classes with functions. How those analyses
relate to the current one remains open.
Finally, since the stochastic counterpart of k-
SL class is the n-gram model, it is plausible that
probabilistic string extension language classes can
form the basis of new natural language process-
ing techniques. (Heinz and Rogers, 2010) show
</bodyText>
<footnote confidence="0.991892066666667">
6This learner resembles what learning theorists call par-
allel learning (Case and Moelius, 2007) and what cognitive
scientists call modular learning (Gallistel and King, 2009).
7I conjecture that morphological and syntactic patterns
are generally not amenable to a string extension learning
analysis because these patterns appear to require a paradigm,
i.e. a set of data points, before any conclusion can be confi-
dently drawn about the generating grammar. Stress patterns
also do not appear to be amenable to a string extension learn-
ing (Heinz, 2007; Edlefsen et al., 2008; Heinz, 2009).
8See also Lange et al. (2008, Theorem 15) and Case et al.
(1999, pp.101-103).
9The basic idea is to consider the lattice L = (Lf in,
Each element of L is a finite set of strings representing the ⊇�.
intersection of all pattern languages consistent with this set.
</footnote>
<page confidence="0.995412">
904
</page>
<bodyText confidence="0.999798">
how to efficiently estimate k-SP distributions, and
it is conjectured that the other string extension lan-
guage classes can be recast as classes of distri-
butions, which can also be successfully estimated
from positive evidence.
</bodyText>
<sectionHeader confidence="0.99775" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.988099666666667">
This work was supported by a University of
Delaware Research Fund grant during the 2008-
2009 academic year. I would like to thank John
Case, Alexander Clark, Timo K¨otzing, Samuel
Moelius, James Rogers, and Edward Stabler for
valuable discussion. I would also like to thank
Timo K¨otzing for careful reading of an earlier
draft and for catching some errors. Remaining er-
rors are my responsibility.
</bodyText>
<sectionHeader confidence="0.999365" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999678079545455">
Dana Angluin. 1980a. Finding patterns common to
a set of strings. Journal of Computer and System
Sciences, 21:46–62.
Dana Angluin. 1980b. Inductive inference of formal
languages from positive data. Information Control,
45:117–135.
Dana Angluin. 1988. Identifying languages from
stochastic examples. Technical Report 614, Yale
University, New Haven, CT.
D. Beauquier and J.E. Pin. 1991. Languages and scan-
ners. Theoretical Computer Science, 84:3–21.
Anselm Blumer, Andrzej Ehrenfeucht, David Haus-
sler, and Manfred K. Warmuth. 1989. Learnability
and the Vapnik-Chervonenkis dimension. J. ACM,
36(4):929–965.
J.A. Brzozowski and I. Simon. 1973. Characterization
of locally testable events. Discrete Math, 4:243–
271.
J.A. Brzozowski. 1962. Canonical regular expres-
sions and minimal state graphs for definite events. In
Mathematical Theory of Automata, pages 529–561.
New York.
Nicola Cancedda, Eric Gaussier, Cyril Goutte, and
Jean-Michel Renders. 2003. Word-sequence ker-
nels. Journal of Machine Learning Research,
3:1059–1082.
Pascal Caron. 2000. Families of locally testable lan-
guages. Theoretical Computer Science, 242:361–
376.
John Case and Sam Moelius. 2007. Parallelism
increases iterative learning power. In 18th An-
nual Conference on Algorithmic Learning Theory
(ALT07), volume 4754 of Lecture Notes in Artificial
Intelligence, pages 49–63. Springer-Verlag, Berlin.
John Case, Sanjay Jain, Steffen Lange, and Thomas
Zeugmann. 1999. Incremental concept learning for
bounded data mining. Information and Computa-
tion, 152:74–110.
Kyle E. Chambers, Kristine H. Onishi, and Cynthia
Fisher. 2002. Learning phonotactic constraints from
brief auditory experience. Cognition, 83:B13–B23.
Alexander Clark, Christophe Costa Florˆencio, and
Chris Watkins. 2006a. Languages as hyperplanes:
grammatical inference with string kernels. In Pro-
ceedings of the European Conference on Machine
Learning (ECML), pages 90–101.
Alexander Clark, Christophe Costa Florˆencio, Chris
Watkins, and Mariette Serayet. 2006b. Planar
languages and learnability. In Proceedings of the
8th International Colloquium on GrammaticalInfer-
ence (ICGI), pages 148–160.
Alejandrina Cristi´a and Amanda Seidl. 2008. Phono-
logical features in infants phonotactic learning: Ev-
idence from artificial grammar learning. Language,
Learning, and Development, 4(3):203–227.
Colin de la Higuera. 1997. Characteristic sets for poly-
nomial grammatical inference. Machine Learning,
27:125–138.
Matt Edlefsen, Dylan Leeman, Nathan Myers,
Nathaniel Smith, Molly Visscher, and David Well-
come. 2008. Deciding strictly local (SL) lan-
guages. In Jon Breitenbucher, editor, Proceedings
of the Midstates Conference for Undergraduate Re-
search in Computer Science and Mathematics, pages
66–73.
Henning Fernau. 2003. Identification of function dis-
tinguishable languages. Theoretical Computer Sci-
ence, 290:1679–1711.
C.R. Gallistel and Adam Philip King. 2009. Memory
and the Computational Brain. Wiley-Blackwell.
Pedro Garcia and Jos´e Ruiz. 1996. Learning k-
piecewise testable languages from positive data. In
Laurent Miclet and Colin de la Higuera, editors,
Grammatical Interference: Learning Syntax from
Sentences, volume 1147 of Lecture Notes in Com-
puter Science, pages 203–210. Springer.
Pedro Garcia and Jos´e Ruiz. 2004. Learning k-testable
and k-piecewise testable languages from positive
data. Grammars, 7:125–140.
Pedro Garcia, Enrique Vidal, and Jos´e Oncina. 1990.
Learning locally testable languages in the strict
sense. In Proceedings of the Workshop on Algorith-
mic Learning Theory, pages 325–338.
E.M. Gold. 1967. Language identification in the limit.
Information and Control, 10:447–474.
J. Grainger and C. Whitney. 2004. Does the huamn
mnid raed wrods as a wlohe? Trends in Cognitive
Science, 8:58–59.
</reference>
<page confidence="0.999235">
905
</page>
<bodyText confidence="0.971887285714286">
Kristine H. Onishi, Kyle E. Chambers, and Cynthia
Fisher. 2003. Infants learn phonotactic regularities
from brief auditory experience. Cognition, 87:B69–
B77.
Jeffrey Heinz and James Rogers. 2010. Estimating
strictly piecewise distributions. In Proceedings of
the ACL.
</bodyText>
<reference confidence="0.999801141025641">
Jeffrey Heinz. 2007. The Inductive Learning of
Phonotactic Patterns. Ph.D. thesis, University of
California, Los Angeles.
Jeffrey Heinz. 2009. On the role of locality in learning
stress patterns. Phonology, 26(2):303–351.
Jeffrey Heinz. to appear. Learning long distance
phonotactics. Linguistic Inquiry.
J. J. Horning. 1969. A Study of Grammatical Infer-
ence. Ph.D. thesis, Stanford University.
Sanjay Jain, Daniel Osherson, James S. Royer, and
Arun Sharma. 1999. Systems That Learn: An In-
troduction to Learning Theory (Learning, Develop-
ment and Conceptual Change). The MIT Press, 2nd
edition.
Sanjay Jain, Steffen Lange, and Sandra Zilles. 2007.
Some natural conditions on incremental learning.
Information and Computation, 205(11):1671–1684.
Daniel Jurafsky and James Martin. 2008. Speech
and Language Processing: An Introduction to Nat-
ural Language Processing, Speech Recognition, and
Computational Linguistics. Prentice-Hall, Upper
Saddle River, NJ, 2nd edition.
Anna Kasprzik and Timo K¨otzing. to appear. String
extension learning using lattices. In Proceedings of
the 4th International Conference on Language and
Automata Theory and Applications (LATA 2010),
Trier, Germany.
S.M. Kim, R. McNaughton, and R. McCloskey. 1991.
A polynomial time algorithm for the local testabil-
ity problem of deterministic finite automata. IEEE
Trans. Comput., 40(10):1087–1093.
Leonid (Aryeh) Kontorovich, Corinna Cortes, and
Mehryar Mohri. 2008. Kernel methods for learn-
ing languages. Theoretical Computer Science,
405(3):223 – 236. Algorithmic Learning Theory.
Steffen Lange, Thomas Zeugmann, and Sandra Zilles.
2008. Learning indexed families of recursive lan-
guages from positive data: A survey. Theoretical
Computer Science, 397:194–232.
H. Lodhi, N. Cristianini, J. Shawe-Taylor, and
C. Watkins. 2002. Text classification using string
kernels. Journal of Machine Language Research,
2:419–444.
M. Lothaire, editor. 2005. Applied Combinatorics on
Words. Cmbridge University Press, 2nd edition.
Robert McNaughton and Seymour Papert. 1971.
Counter-Free Automata. MIT Press.
R. McNaughton. 1974. Algebraic decision procedures
for local testability. Math. Systems Theory, 8:60–76.
R. J. Parikh. 1966. On context-free languages. Journal
of the ACM, 13, 570581., 13:570–581.
James Rogers and Geoffrey Pullum. to appear. Aural
pattern recognition experiments and the subregular
hierarchy. Journal of Logic, Language and Infor-
mation.
James Rogers, Jeffrey Heinz, Gil Bailey, Matt Edlef-
sen, Molly Visscher, David Wellcome, and Sean
Wibel. 2009. On languages piecewise testable in
the strict sense. In Proceedings of the 11th Meeting
of the Assocation for Mathematics ofLanguage.
John Shawe-Taylor and Nello Christianini. 2005. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press.
Imre Simon. 1975. Piecewise testable events. In Au-
tomata Theory and Formal Languages, pages 214–
222.
Imre Simon. 1993. The product of rational lan-
guages. In ICALP ’93: Proceedings of the 20th
International Colloquium on Automata, Languages
and Programming, pages 430–444, London, UK.
Springer-Verlag.
Carol Whitney and Piers Cornelissen. 2008. SE-
RIOL reading. Language and Cognitive Processes,
23:143–164.
Carol Whitney. 2001. How the brain encodes the or-
der of letters in a printed word: the SERIOL model
and selective literature review. Psychonomic Bul-
letin Review, 8:221–243.
</reference>
<page confidence="0.998604">
906
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.961696">
<title confidence="0.999676">String Extension Learning</title>
<author confidence="0.999992">Jeffrey Heinz</author>
<affiliation confidence="0.999982">University of Delaware</affiliation>
<address confidence="0.989471">Newark, Delaware, USA</address>
<email confidence="0.99977">heinz@udel.edu</email>
<abstract confidence="0.997619416666667">This paper provides a unified, learningtheoretic analysis of several learnable classes of languages discussed previously in the literature. The analysis shows that for these classes an incremental, globally consistent, locally conservative, set-driven learner always exists. Additionally, the analysis provides a recipe for constructing new learnable classes. Potential applications include learnable models for aspects of natural language and cognition.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dana Angluin</author>
</authors>
<title>Finding patterns common to a set of strings.</title>
<date>1980</date>
<journal>Journal of Computer and System Sciences,</journal>
<pages>21--46</pages>
<contexts>
<context position="1956" citStr="Angluin, 1980" startWordPosition="276" endWordPosition="277">iterature which are string extension learnable include the Locally Testable (LT) languages, the Locally Testable Languages in the Strict Sense 1The allowance of negative evidence (Gold, 1967) or restricting the kinds of texts the learner is required to succeed on (i.e. non-distribution-free evidence) (Gold, 1967; Horning, 1969; Angluin, 1988) admits the learnability of the class of recursively enumerable languages. Classes of languages learnable in the harder, distribution-free, positive-evidenceonly settings are due to structural properties of the language classes that permit generalization (Angluin, 1980b; Blumer et al., 1989). That is the central interest here. (Strictly Local, SL) (McNaughton and Papert, 1971; Rogers and Pullum, to appear), the Piecewise Testable (PT) languages (Simon, 1975), the Piecewise Testable languages in the Strict Sense (Strictly Piecewise, SP) (Rogers et al., 2009), the Strongly Testable languages (Beauquier and Pin, 1991), the Definite languages (Brzozowski, 1962), and the Finite languages, among others. To our knowledge, this is the first analysis which identifies the common structural elements of these language classes which allows them to be identifiable in the</context>
<context position="7251" citStr="Angluin (1980" startWordPosition="1204" endWordPosition="1205">) = #. A learner is a function 0 which maps initial finite sequences of texts to grammars, i.e. 0 : SEQ → G. The elements of G (the grammars) generate languages in some well-defined way. A learner converges on a text t iff there exists i ∈Nand a grammar G such that for all j &gt; i, 0(t[j]) = G. For any grammar G, the language it generates is denoted L(G). A learner 0 identifies a language L in the limit iff for any positive text t for L, 0 converges on t to grammar G and L(G) = L. Finally, a learner 0 identifies a class of languages L in the limit iff for any L ∈ L, 0 identifies L in the limit. Angluin (1980b) provides necessary and sufficient properties of language classes which are identifiable in the limit from positive data. A learner 0 of language class L is globally consistent iff for each i and for all texts t for some L ∈ L, content(t[i]) ⊆ L(0(t[i])). A learner 0 is locally conservative iff for each i and for all texts t for some L ∈ L, whenever 0(t[i]) =6 0(t[i − 1]), it is the case that t(i) ∈6 L(0([i−1])). These terms are from Jain et al. (2007). Also, learners which do not depend on the order of the text are called set-driven (Jain et al., 1999, p. 99). 3 Grammars and Languages Consi</context>
<context position="30957" citStr="Angluin, 1980" startWordPosition="5646" endWordPosition="5647">ay be generalized along the following lines. Instead of defining the function f as a map from strings to finite subsets, let f be a function from strings to elements of a lattice. A grammar G is an element of the lattice and the language of the G are all strings w such that f maps w to a grammar less than G. Learners Of are defined as the least upper bound of its current hypothesis and the grammar to which f maps the current word.8 Kasprzik and K¨otzing (2010) develop this idea and demonstrate additional properties of string extension classes and learning, and show that the pattern languages (Angluin, 1980a) form a string extension class.9 Also, hyperplane learning (Clark et al., 2006a; Clark et al., 2006b) and function-distinguishable learning (Fernau, 2003) similarly associate language classes with functions. How those analyses relate to the current one remains open. Finally, since the stochastic counterpart of kSL class is the n-gram model, it is plausible that probabilistic string extension language classes can form the basis of new natural language processing techniques. (Heinz and Rogers, 2010) show 6This learner resembles what learning theorists call parallel learning (Case and Moelius, </context>
</contexts>
<marker>Angluin, 1980</marker>
<rawString>Dana Angluin. 1980a. Finding patterns common to a set of strings. Journal of Computer and System Sciences, 21:46–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dana Angluin</author>
</authors>
<title>Inductive inference of formal languages from positive data. Information Control,</title>
<date>1980</date>
<pages>45--117</pages>
<contexts>
<context position="1956" citStr="Angluin, 1980" startWordPosition="276" endWordPosition="277">iterature which are string extension learnable include the Locally Testable (LT) languages, the Locally Testable Languages in the Strict Sense 1The allowance of negative evidence (Gold, 1967) or restricting the kinds of texts the learner is required to succeed on (i.e. non-distribution-free evidence) (Gold, 1967; Horning, 1969; Angluin, 1988) admits the learnability of the class of recursively enumerable languages. Classes of languages learnable in the harder, distribution-free, positive-evidenceonly settings are due to structural properties of the language classes that permit generalization (Angluin, 1980b; Blumer et al., 1989). That is the central interest here. (Strictly Local, SL) (McNaughton and Papert, 1971; Rogers and Pullum, to appear), the Piecewise Testable (PT) languages (Simon, 1975), the Piecewise Testable languages in the Strict Sense (Strictly Piecewise, SP) (Rogers et al., 2009), the Strongly Testable languages (Beauquier and Pin, 1991), the Definite languages (Brzozowski, 1962), and the Finite languages, among others. To our knowledge, this is the first analysis which identifies the common structural elements of these language classes which allows them to be identifiable in the</context>
<context position="7251" citStr="Angluin (1980" startWordPosition="1204" endWordPosition="1205">) = #. A learner is a function 0 which maps initial finite sequences of texts to grammars, i.e. 0 : SEQ → G. The elements of G (the grammars) generate languages in some well-defined way. A learner converges on a text t iff there exists i ∈Nand a grammar G such that for all j &gt; i, 0(t[j]) = G. For any grammar G, the language it generates is denoted L(G). A learner 0 identifies a language L in the limit iff for any positive text t for L, 0 converges on t to grammar G and L(G) = L. Finally, a learner 0 identifies a class of languages L in the limit iff for any L ∈ L, 0 identifies L in the limit. Angluin (1980b) provides necessary and sufficient properties of language classes which are identifiable in the limit from positive data. A learner 0 of language class L is globally consistent iff for each i and for all texts t for some L ∈ L, content(t[i]) ⊆ L(0(t[i])). A learner 0 is locally conservative iff for each i and for all texts t for some L ∈ L, whenever 0(t[i]) =6 0(t[i − 1]), it is the case that t(i) ∈6 L(0([i−1])). These terms are from Jain et al. (2007). Also, learners which do not depend on the order of the text are called set-driven (Jain et al., 1999, p. 99). 3 Grammars and Languages Consi</context>
<context position="30957" citStr="Angluin, 1980" startWordPosition="5646" endWordPosition="5647">ay be generalized along the following lines. Instead of defining the function f as a map from strings to finite subsets, let f be a function from strings to elements of a lattice. A grammar G is an element of the lattice and the language of the G are all strings w such that f maps w to a grammar less than G. Learners Of are defined as the least upper bound of its current hypothesis and the grammar to which f maps the current word.8 Kasprzik and K¨otzing (2010) develop this idea and demonstrate additional properties of string extension classes and learning, and show that the pattern languages (Angluin, 1980a) form a string extension class.9 Also, hyperplane learning (Clark et al., 2006a; Clark et al., 2006b) and function-distinguishable learning (Fernau, 2003) similarly associate language classes with functions. How those analyses relate to the current one remains open. Finally, since the stochastic counterpart of kSL class is the n-gram model, it is plausible that probabilistic string extension language classes can form the basis of new natural language processing techniques. (Heinz and Rogers, 2010) show 6This learner resembles what learning theorists call parallel learning (Case and Moelius, </context>
</contexts>
<marker>Angluin, 1980</marker>
<rawString>Dana Angluin. 1980b. Inductive inference of formal languages from positive data. Information Control, 45:117–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dana Angluin</author>
</authors>
<title>Identifying languages from stochastic examples.</title>
<date>1988</date>
<tech>Technical Report 614,</tech>
<institution>Yale University,</institution>
<location>New Haven, CT.</location>
<contexts>
<context position="1687" citStr="Angluin, 1988" startWordPosition="241" endWordPosition="242">age can be mapped (extended) to an element of the grammar, which in every case, is conceived as a finite set of elements. These learners have desirable properties: they are incremental, globally consistent, and locally conservative. Classes previously discussed in the literature which are string extension learnable include the Locally Testable (LT) languages, the Locally Testable Languages in the Strict Sense 1The allowance of negative evidence (Gold, 1967) or restricting the kinds of texts the learner is required to succeed on (i.e. non-distribution-free evidence) (Gold, 1967; Horning, 1969; Angluin, 1988) admits the learnability of the class of recursively enumerable languages. Classes of languages learnable in the harder, distribution-free, positive-evidenceonly settings are due to structural properties of the language classes that permit generalization (Angluin, 1980b; Blumer et al., 1989). That is the central interest here. (Strictly Local, SL) (McNaughton and Papert, 1971; Rogers and Pullum, to appear), the Piecewise Testable (PT) languages (Simon, 1975), the Piecewise Testable languages in the Strict Sense (Strictly Piecewise, SP) (Rogers et al., 2009), the Strongly Testable languages (Be</context>
</contexts>
<marker>Angluin, 1988</marker>
<rawString>Dana Angluin. 1988. Identifying languages from stochastic examples. Technical Report 614, Yale University, New Haven, CT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Beauquier</author>
<author>J E Pin</author>
</authors>
<title>Languages and scanners.</title>
<date>1991</date>
<journal>Theoretical Computer Science,</journal>
<pages>84--3</pages>
<contexts>
<context position="2309" citStr="Beauquier and Pin, 1991" startWordPosition="327" endWordPosition="330">8) admits the learnability of the class of recursively enumerable languages. Classes of languages learnable in the harder, distribution-free, positive-evidenceonly settings are due to structural properties of the language classes that permit generalization (Angluin, 1980b; Blumer et al., 1989). That is the central interest here. (Strictly Local, SL) (McNaughton and Papert, 1971; Rogers and Pullum, to appear), the Piecewise Testable (PT) languages (Simon, 1975), the Piecewise Testable languages in the Strict Sense (Strictly Piecewise, SP) (Rogers et al., 2009), the Strongly Testable languages (Beauquier and Pin, 1991), the Definite languages (Brzozowski, 1962), and the Finite languages, among others. To our knowledge, this is the first analysis which identifies the common structural elements of these language classes which allows them to be identifiable in the limit from positive data: each language class induces a natural partition over all logically possible strings and each language in the class is the union of finitely many blocks of this partition. One consequence of this analysis is a recipe for constructing new learnable classes. One notable case is the Strictly Piecewise (SP) languages, which was o</context>
</contexts>
<marker>Beauquier, Pin, 1991</marker>
<rawString>D. Beauquier and J.E. Pin. 1991. Languages and scanners. Theoretical Computer Science, 84:3–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anselm Blumer</author>
<author>Andrzej Ehrenfeucht</author>
<author>David Haussler</author>
<author>Manfred K Warmuth</author>
</authors>
<title>Learnability and the Vapnik-Chervonenkis dimension.</title>
<date>1989</date>
<journal>J. ACM,</journal>
<volume>36</volume>
<issue>4</issue>
<contexts>
<context position="1979" citStr="Blumer et al., 1989" startWordPosition="278" endWordPosition="281">are string extension learnable include the Locally Testable (LT) languages, the Locally Testable Languages in the Strict Sense 1The allowance of negative evidence (Gold, 1967) or restricting the kinds of texts the learner is required to succeed on (i.e. non-distribution-free evidence) (Gold, 1967; Horning, 1969; Angluin, 1988) admits the learnability of the class of recursively enumerable languages. Classes of languages learnable in the harder, distribution-free, positive-evidenceonly settings are due to structural properties of the language classes that permit generalization (Angluin, 1980b; Blumer et al., 1989). That is the central interest here. (Strictly Local, SL) (McNaughton and Papert, 1971; Rogers and Pullum, to appear), the Piecewise Testable (PT) languages (Simon, 1975), the Piecewise Testable languages in the Strict Sense (Strictly Piecewise, SP) (Rogers et al., 2009), the Strongly Testable languages (Beauquier and Pin, 1991), the Definite languages (Brzozowski, 1962), and the Finite languages, among others. To our knowledge, this is the first analysis which identifies the common structural elements of these language classes which allows them to be identifiable in the limit from positive da</context>
</contexts>
<marker>Blumer, Ehrenfeucht, Haussler, Warmuth, 1989</marker>
<rawString>Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. 1989. Learnability and the Vapnik-Chervonenkis dimension. J. ACM, 36(4):929–965.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Brzozowski</author>
<author>I Simon</author>
</authors>
<title>Characterization of locally testable events.</title>
<date>1973</date>
<journal>Discrete Math,</journal>
<volume>4</volume>
<pages>271</pages>
<contexts>
<context position="22143" citStr="Brzozowski and Simon, 1973" startWordPosition="4086" endWordPosition="4089">han L may be perfectly arbitrary about w. This can now be expressed as the string extension function: LRIk(w) = (Lk(w),Rk(w),Ik(w)) Thus for some k, a grammar G is triple formed by taking subsets of Ek, Ek, and E&lt;k, respectively. A word w belongs to the language of G only if LRIk(w) C� G. Clearly, LLRIk = kSL, and henceforth we refer to this class as k-SL. Since, for fixed k, LRIk E SEF, all of the learning results in §4 apply. 5.5 Locally k-Testable languages The Locally k-testable languages (k-LT) are originally defined in McNaughton and Papert (1971) and are the subject of several studies (Brzozowski and Simon, 1973; McNaughton, 1974; Kim et al., 1991; Caron, 2000; Garcia and Ruiz, 2004; Rogers and Pullum, to appear). A language L is k-testable iff for all w1, w2 E E* such that |w1 |≥ k and |w2 |≥ k, and LRIk(w1) = LRIk(w2) then either both w1, w2 belong to L or neither do. Clearly, every language in k-SL belongs to k-LT. However k-LT properly include k-SL because a k-testable language only distinguishes words whenever LRIk(w1) =� LRIk(w2). It is known that the k-LT languages are the boolean closure of the k-SL (McNaughton and Papert, 1971). The function LRIk exactly expresses k-testable languages. Infor</context>
</contexts>
<marker>Brzozowski, Simon, 1973</marker>
<rawString>J.A. Brzozowski and I. Simon. 1973. Characterization of locally testable events. Discrete Math, 4:243– 271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Brzozowski</author>
</authors>
<title>Canonical regular expressions and minimal state graphs for definite events.</title>
<date>1962</date>
<booktitle>In Mathematical Theory of Automata,</booktitle>
<pages>529--561</pages>
<location>New York.</location>
<contexts>
<context position="2352" citStr="Brzozowski, 1962" startWordPosition="334" endWordPosition="335">vely enumerable languages. Classes of languages learnable in the harder, distribution-free, positive-evidenceonly settings are due to structural properties of the language classes that permit generalization (Angluin, 1980b; Blumer et al., 1989). That is the central interest here. (Strictly Local, SL) (McNaughton and Papert, 1971; Rogers and Pullum, to appear), the Piecewise Testable (PT) languages (Simon, 1975), the Piecewise Testable languages in the Strict Sense (Strictly Piecewise, SP) (Rogers et al., 2009), the Strongly Testable languages (Beauquier and Pin, 1991), the Definite languages (Brzozowski, 1962), and the Finite languages, among others. To our knowledge, this is the first analysis which identifies the common structural elements of these language classes which allows them to be identifiable in the limit from positive data: each language class induces a natural partition over all logically possible strings and each language in the class is the union of finitely many blocks of this partition. One consequence of this analysis is a recipe for constructing new learnable classes. One notable case is the Strictly Piecewise (SP) languages, which was originally motivated for two reasons: the le</context>
</contexts>
<marker>Brzozowski, 1962</marker>
<rawString>J.A. Brzozowski. 1962. Canonical regular expressions and minimal state graphs for definite events. In Mathematical Theory of Automata, pages 529–561. New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Cancedda</author>
</authors>
<title>Eric Gaussier, Cyril Goutte, and Jean-Michel Renders.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1059</pages>
<marker>Cancedda, 2003</marker>
<rawString>Nicola Cancedda, Eric Gaussier, Cyril Goutte, and Jean-Michel Renders. 2003. Word-sequence kernels. Journal of Machine Learning Research, 3:1059–1082.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Caron</author>
</authors>
<title>Families of locally testable languages.</title>
<date>2000</date>
<journal>Theoretical Computer Science,</journal>
<volume>242</volume>
<pages>376</pages>
<contexts>
<context position="20655" citStr="Caron, 2000" startWordPosition="3786" endWordPosition="3787">a, b, aa, ab, ba} E*\(E*bE*bE*) 3 aba {A, a, b, ab, ba} {A, a, b, aa, ab, ba} E*\(E*bE*bE*) . . . Table 2: The learner OSP2 with a text from the language in Example 2. Boldtype indicates newly added elements to the grammar. These definitions preserve the learning results of §4. Note that the characteristic sample of L E Lf will be the union of the characteristic samples of each fZ and the language Lf (G) is the intersection of Lf,(GZ). Locally k-Testable Languages in the Strict Sense (Strictly k-Local) have been studied by several researchers (McNaughton and Papert, 1971; Garcia et al., 1990; Caron, 2000; Rogers and Pullum, to appear), among others. We follow the definitions from (McNaughton and Papert, 1971, p. 14), effectively encoded in the following functions. Definition 6 Fix k EN. Then the (left-edge) prefix of length k, the (right-edge) suffix of length k, and the interior k-factors of a word w are Lk(w) = {u E Ek : ]v E E* such that w = uv} Rk(w) = {u E Ek : ]v E E* such that w = vu} Ik(w) = fack(w)\(Lk(w) U Rk(w)) Example 3 Suppose w = abcba. Then L2(w) = {ab}, R2(w) = {ba} and I2(w) = {bc, cb}. Example 4 Suppose |w |k. Then Lk(w) = Rk(w) = {w} and Ik(w) = 0. Example 5 Suppose |w |is</context>
<context position="22192" citStr="Caron, 2000" startWordPosition="4096" endWordPosition="4097">sed as the string extension function: LRIk(w) = (Lk(w),Rk(w),Ik(w)) Thus for some k, a grammar G is triple formed by taking subsets of Ek, Ek, and E&lt;k, respectively. A word w belongs to the language of G only if LRIk(w) C� G. Clearly, LLRIk = kSL, and henceforth we refer to this class as k-SL. Since, for fixed k, LRIk E SEF, all of the learning results in §4 apply. 5.5 Locally k-Testable languages The Locally k-testable languages (k-LT) are originally defined in McNaughton and Papert (1971) and are the subject of several studies (Brzozowski and Simon, 1973; McNaughton, 1974; Kim et al., 1991; Caron, 2000; Garcia and Ruiz, 2004; Rogers and Pullum, to appear). A language L is k-testable iff for all w1, w2 E E* such that |w1 |≥ k and |w2 |≥ k, and LRIk(w1) = LRIk(w2) then either both w1, w2 belong to L or neither do. Clearly, every language in k-SL belongs to k-LT. However k-LT properly include k-SL because a k-testable language only distinguishes words whenever LRIk(w1) =� LRIk(w2). It is known that the k-LT languages are the boolean closure of the k-SL (McNaughton and Papert, 1971). The function LRIk exactly expresses k-testable languages. Informally, each word w is mapped to a set containing </context>
</contexts>
<marker>Caron, 2000</marker>
<rawString>Pascal Caron. 2000. Families of locally testable languages. Theoretical Computer Science, 242:361– 376.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Case</author>
<author>Sam Moelius</author>
</authors>
<title>Parallelism increases iterative learning power.</title>
<date>2007</date>
<booktitle>In 18th Annual Conference on Algorithmic Learning Theory (ALT07),</booktitle>
<volume>4754</volume>
<pages>49--63</pages>
<publisher>Springer-Verlag,</publisher>
<location>Berlin.</location>
<contexts>
<context position="31562" citStr="Case and Moelius, 2007" startWordPosition="5734" endWordPosition="5737">ges (Angluin, 1980a) form a string extension class.9 Also, hyperplane learning (Clark et al., 2006a; Clark et al., 2006b) and function-distinguishable learning (Fernau, 2003) similarly associate language classes with functions. How those analyses relate to the current one remains open. Finally, since the stochastic counterpart of kSL class is the n-gram model, it is plausible that probabilistic string extension language classes can form the basis of new natural language processing techniques. (Heinz and Rogers, 2010) show 6This learner resembles what learning theorists call parallel learning (Case and Moelius, 2007) and what cognitive scientists call modular learning (Gallistel and King, 2009). 7I conjecture that morphological and syntactic patterns are generally not amenable to a string extension learning analysis because these patterns appear to require a paradigm, i.e. a set of data points, before any conclusion can be confidently drawn about the generating grammar. Stress patterns also do not appear to be amenable to a string extension learning (Heinz, 2007; Edlefsen et al., 2008; Heinz, 2009). 8See also Lange et al. (2008, Theorem 15) and Case et al. (1999, pp.101-103). 9The basic idea is to conside</context>
</contexts>
<marker>Case, Moelius, 2007</marker>
<rawString>John Case and Sam Moelius. 2007. Parallelism increases iterative learning power. In 18th Annual Conference on Algorithmic Learning Theory (ALT07), volume 4754 of Lecture Notes in Artificial Intelligence, pages 49–63. Springer-Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Case</author>
<author>Sanjay Jain</author>
<author>Steffen Lange</author>
<author>Thomas Zeugmann</author>
</authors>
<title>Incremental concept learning for bounded data mining.</title>
<date>1999</date>
<journal>Information and Computation,</journal>
<pages>152--74</pages>
<contexts>
<context position="32118" citStr="Case et al. (1999" startWordPosition="5824" endWordPosition="5827">ing theorists call parallel learning (Case and Moelius, 2007) and what cognitive scientists call modular learning (Gallistel and King, 2009). 7I conjecture that morphological and syntactic patterns are generally not amenable to a string extension learning analysis because these patterns appear to require a paradigm, i.e. a set of data points, before any conclusion can be confidently drawn about the generating grammar. Stress patterns also do not appear to be amenable to a string extension learning (Heinz, 2007; Edlefsen et al., 2008; Heinz, 2009). 8See also Lange et al. (2008, Theorem 15) and Case et al. (1999, pp.101-103). 9The basic idea is to consider the lattice L = (Lf in, Each element of L is a finite set of strings representing the ⊇�. intersection of all pattern languages consistent with this set. 904 how to efficiently estimate k-SP distributions, and it is conjectured that the other string extension language classes can be recast as classes of distributions, which can also be successfully estimated from positive evidence. Acknowledgments This work was supported by a University of Delaware Research Fund grant during the 2008- 2009 academic year. I would like to thank John Case, Alexander C</context>
</contexts>
<marker>Case, Jain, Lange, Zeugmann, 1999</marker>
<rawString>John Case, Sanjay Jain, Steffen Lange, and Thomas Zeugmann. 1999. Incremental concept learning for bounded data mining. Information and Computation, 152:74–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyle E Chambers</author>
<author>Kristine H Onishi</author>
<author>Cynthia Fisher</author>
</authors>
<title>Learning phonotactic constraints from brief auditory experience.</title>
<date>2002</date>
<tech>Cognition, 83:B13–B23.</tech>
<contexts>
<context position="29850" citStr="Chambers et al., 2002" startWordPosition="5454" endWordPosition="5457">ars that the string extension function f = (LRI3i P2), which defines a class of languages which obey restrictions on both contiguous subsequences of length 3 and on discontiguous subsequences of length 2, provides a good first approximation to the segmental phonotactic patterns in natural languages (Heinz, 2007). The string extension learner for this class is essentially two learners: OLxz3 and OPt, operating simultaneously.6 The learners make predictions about generalizations, which can be tested in artificial language learning experiments on adults and infants (Rogers and Pullum, to appear; Chambers et al., 2002; Onishi et al., 2003; Cristi´a and Seidl, 2008).7 For theoretical computer science, it remains an open question what property holds of functions f in SEF to ensure that Lf is regular, contextfree, or context-sensitive. For known subregular classes, there are constructions that provide deterministic automata that suggest the relevant properties. (See, for example, Garcia et al. (1990) and Garica and Ruiz (1996).) Also, Timo K¨otzing and Samuel Moelius (p.c.) suggest that the results here may be generalized along the following lines. Instead of defining the function f as a map from strings to f</context>
</contexts>
<marker>Chambers, Onishi, Fisher, 2002</marker>
<rawString>Kyle E. Chambers, Kristine H. Onishi, and Cynthia Fisher. 2002. Learning phonotactic constraints from brief auditory experience. Cognition, 83:B13–B23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
<author>Christophe Costa Florˆencio</author>
<author>Chris Watkins</author>
</authors>
<title>Languages as hyperplanes: grammatical inference with string kernels.</title>
<date>2006</date>
<booktitle>In Proceedings of the European Conference on Machine Learning (ECML),</booktitle>
<pages>90--101</pages>
<marker>Clark, Florˆencio, Watkins, 2006</marker>
<rawString>Alexander Clark, Christophe Costa Florˆencio, and Chris Watkins. 2006a. Languages as hyperplanes: grammatical inference with string kernels. In Proceedings of the European Conference on Machine Learning (ECML), pages 90–101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
<author>Christophe Costa Florˆencio</author>
<author>Chris Watkins</author>
<author>Mariette Serayet</author>
</authors>
<title>Planar languages and learnability.</title>
<date>2006</date>
<booktitle>In Proceedings of the 8th International Colloquium on GrammaticalInference (ICGI),</booktitle>
<pages>148--160</pages>
<marker>Clark, Florˆencio, Watkins, Serayet, 2006</marker>
<rawString>Alexander Clark, Christophe Costa Florˆencio, Chris Watkins, and Mariette Serayet. 2006b. Planar languages and learnability. In Proceedings of the 8th International Colloquium on GrammaticalInference (ICGI), pages 148–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alejandrina Cristi´a</author>
<author>Amanda Seidl</author>
</authors>
<title>Phonological features in infants phonotactic learning:</title>
<date>2008</date>
<booktitle>Evidence from artificial grammar learning. Language, Learning, and Development,</booktitle>
<pages>4--3</pages>
<marker>Cristi´a, Seidl, 2008</marker>
<rawString>Alejandrina Cristi´a and Amanda Seidl. 2008. Phonological features in infants phonotactic learning: Evidence from artificial grammar learning. Language, Learning, and Development, 4(3):203–227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin de la Higuera</author>
</authors>
<title>Characteristic sets for polynomial grammatical inference.</title>
<date>1997</date>
<booktitle>Machine Learning,</booktitle>
<pages>27--125</pages>
<contexts>
<context position="13390" citStr="Higuera, 1997" startWordPosition="2418" endWordPosition="2419">is finite. Clearly f(S) = f(L) and thus L = L(f(S)). Therefore, by Theorem 3, L is the smallest language in Lf containing S. ❑ Theorem 5 Fix f E SEF. Then Of identifies Lf in the limit. Proof: For any L E Lf , there is a characteristic finite sample S for L (Lemma 2). Thus for any text t for L, there is i such that S C content(t[i]). Thus for any j &gt; i, O(t(j)) is the smallest language in Lf containing S by Theorem 3 and Lemma 2. Thus, O(t(j)) = f(S) = f(L). ❑ An immediate corollary is the efficiency of Of in the length of the sample, provided f is efficient in the length of the string (de la Higuera, 1997). Corollary 1 Of is efficient in the length of the sample iff f is efficiently computable in the length of a string. To summarize: string extension grammars are finite subsets of some set A. The class of languages they generate are determined by a function f which maps strings to finite subsets of A (chunks of grammars). Since the size of the canonical grammars is finite, a learner which develops a grammar on the basis of the observed words and the function f identifies this class exactly in the limit from positive data. It also follows that if f is efficient in the length of the string then O</context>
</contexts>
<marker>Higuera, 1997</marker>
<rawString>Colin de la Higuera. 1997. Characteristic sets for polynomial grammatical inference. Machine Learning, 27:125–138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Edlefsen</author>
<author>Dylan Leeman</author>
<author>Nathan Myers</author>
<author>Nathaniel Smith</author>
<author>Molly Visscher</author>
<author>David Wellcome</author>
</authors>
<title>Deciding strictly local (SL) languages.</title>
<date>2008</date>
<booktitle>Proceedings of the Midstates Conference for Undergraduate Research in Computer Science and Mathematics,</booktitle>
<pages>66--73</pages>
<editor>In Jon Breitenbucher, editor,</editor>
<contexts>
<context position="32039" citStr="Edlefsen et al., 2008" startWordPosition="5809" endWordPosition="5812">essing techniques. (Heinz and Rogers, 2010) show 6This learner resembles what learning theorists call parallel learning (Case and Moelius, 2007) and what cognitive scientists call modular learning (Gallistel and King, 2009). 7I conjecture that morphological and syntactic patterns are generally not amenable to a string extension learning analysis because these patterns appear to require a paradigm, i.e. a set of data points, before any conclusion can be confidently drawn about the generating grammar. Stress patterns also do not appear to be amenable to a string extension learning (Heinz, 2007; Edlefsen et al., 2008; Heinz, 2009). 8See also Lange et al. (2008, Theorem 15) and Case et al. (1999, pp.101-103). 9The basic idea is to consider the lattice L = (Lf in, Each element of L is a finite set of strings representing the ⊇�. intersection of all pattern languages consistent with this set. 904 how to efficiently estimate k-SP distributions, and it is conjectured that the other string extension language classes can be recast as classes of distributions, which can also be successfully estimated from positive evidence. Acknowledgments This work was supported by a University of Delaware Research Fund grant du</context>
</contexts>
<marker>Edlefsen, Leeman, Myers, Smith, Visscher, Wellcome, 2008</marker>
<rawString>Matt Edlefsen, Dylan Leeman, Nathan Myers, Nathaniel Smith, Molly Visscher, and David Wellcome. 2008. Deciding strictly local (SL) languages. In Jon Breitenbucher, editor, Proceedings of the Midstates Conference for Undergraduate Research in Computer Science and Mathematics, pages 66–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henning Fernau</author>
</authors>
<title>Identification of function distinguishable languages.</title>
<date>2003</date>
<journal>Theoretical Computer Science,</journal>
<pages>290--1679</pages>
<contexts>
<context position="31113" citStr="Fernau, 2003" startWordPosition="5667" endWordPosition="5668"> elements of a lattice. A grammar G is an element of the lattice and the language of the G are all strings w such that f maps w to a grammar less than G. Learners Of are defined as the least upper bound of its current hypothesis and the grammar to which f maps the current word.8 Kasprzik and K¨otzing (2010) develop this idea and demonstrate additional properties of string extension classes and learning, and show that the pattern languages (Angluin, 1980a) form a string extension class.9 Also, hyperplane learning (Clark et al., 2006a; Clark et al., 2006b) and function-distinguishable learning (Fernau, 2003) similarly associate language classes with functions. How those analyses relate to the current one remains open. Finally, since the stochastic counterpart of kSL class is the n-gram model, it is plausible that probabilistic string extension language classes can form the basis of new natural language processing techniques. (Heinz and Rogers, 2010) show 6This learner resembles what learning theorists call parallel learning (Case and Moelius, 2007) and what cognitive scientists call modular learning (Gallistel and King, 2009). 7I conjecture that morphological and syntactic patterns are generally </context>
</contexts>
<marker>Fernau, 2003</marker>
<rawString>Henning Fernau. 2003. Identification of function distinguishable languages. Theoretical Computer Science, 290:1679–1711.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C R Gallistel</author>
<author>Adam Philip King</author>
</authors>
<date>2009</date>
<journal>Memory and the Computational Brain. Wiley-Blackwell.</journal>
<contexts>
<context position="31641" citStr="Gallistel and King, 2009" startWordPosition="5745" endWordPosition="5748">g (Clark et al., 2006a; Clark et al., 2006b) and function-distinguishable learning (Fernau, 2003) similarly associate language classes with functions. How those analyses relate to the current one remains open. Finally, since the stochastic counterpart of kSL class is the n-gram model, it is plausible that probabilistic string extension language classes can form the basis of new natural language processing techniques. (Heinz and Rogers, 2010) show 6This learner resembles what learning theorists call parallel learning (Case and Moelius, 2007) and what cognitive scientists call modular learning (Gallistel and King, 2009). 7I conjecture that morphological and syntactic patterns are generally not amenable to a string extension learning analysis because these patterns appear to require a paradigm, i.e. a set of data points, before any conclusion can be confidently drawn about the generating grammar. Stress patterns also do not appear to be amenable to a string extension learning (Heinz, 2007; Edlefsen et al., 2008; Heinz, 2009). 8See also Lange et al. (2008, Theorem 15) and Case et al. (1999, pp.101-103). 9The basic idea is to consider the lattice L = (Lf in, Each element of L is a finite set of strings represen</context>
</contexts>
<marker>Gallistel, King, 2009</marker>
<rawString>C.R. Gallistel and Adam Philip King. 2009. Memory and the Computational Brain. Wiley-Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro Garcia</author>
<author>Jos´e Ruiz</author>
</authors>
<title>Learning kpiecewise testable languages from positive data.</title>
<date>1996</date>
<booktitle>In Laurent Miclet and Colin de la Higuera, editors, Grammatical Interference: Learning Syntax from Sentences,</booktitle>
<volume>1147</volume>
<pages>203--210</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="18692" citStr="Garcia and Ruiz, 1996" startWordPosition="3404" endWordPosition="3407">guages A language L is k-Piecewise Testable iff whenever strings u and v have the same subsequences 3In earlier work, the function SP2 has been described as returning the set of precedence relations in w, and the language class LSP2 was called the precedence languages (Heinz, 2007; Heinz, to appear). of length at most k and u is in L, then v is in L as well (Simon, 1975; Simon, 1993; Lothaire, 2005). A language L is said to be Piecewise-Testable (PT) if it is k-Piecewise Testable for some k ∈N. If k is fixed, the k-Piecewise Testable languages are identifiable in the limit from positive data (Garcia and Ruiz, 1996; Garcia and Ruiz, 2004). More recently, the Piecewise Testable languages has been shown to be linearly separable with a subsequence kernel (Kontorovich et al., 2008). The k-Piecewise Testable languages can also be described with the function SPk⋄ . Recall that f⋄(a) = {f(a)}. Thus functions SPk⋄ define grammars as a finite list of sets of subsequences up to length k that may occur in words in the language. This reflects the fact that the k-Piecewise Testable languages are the boolean closure of the Strictly k-Piecewise languages.4 5.4 Strictly k-Local languages To define the Strictly k-Local </context>
</contexts>
<marker>Garcia, Ruiz, 1996</marker>
<rawString>Pedro Garcia and Jos´e Ruiz. 1996. Learning kpiecewise testable languages from positive data. In Laurent Miclet and Colin de la Higuera, editors, Grammatical Interference: Learning Syntax from Sentences, volume 1147 of Lecture Notes in Computer Science, pages 203–210. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro Garcia</author>
<author>Jos´e Ruiz</author>
</authors>
<title>Learning k-testable and k-piecewise testable languages from positive data. Grammars,</title>
<date>2004</date>
<pages>7--125</pages>
<contexts>
<context position="18716" citStr="Garcia and Ruiz, 2004" startWordPosition="3408" endWordPosition="3411">k-Piecewise Testable iff whenever strings u and v have the same subsequences 3In earlier work, the function SP2 has been described as returning the set of precedence relations in w, and the language class LSP2 was called the precedence languages (Heinz, 2007; Heinz, to appear). of length at most k and u is in L, then v is in L as well (Simon, 1975; Simon, 1993; Lothaire, 2005). A language L is said to be Piecewise-Testable (PT) if it is k-Piecewise Testable for some k ∈N. If k is fixed, the k-Piecewise Testable languages are identifiable in the limit from positive data (Garcia and Ruiz, 1996; Garcia and Ruiz, 2004). More recently, the Piecewise Testable languages has been shown to be linearly separable with a subsequence kernel (Kontorovich et al., 2008). The k-Piecewise Testable languages can also be described with the function SPk⋄ . Recall that f⋄(a) = {f(a)}. Thus functions SPk⋄ define grammars as a finite list of sets of subsequences up to length k that may occur in words in the language. This reflects the fact that the k-Piecewise Testable languages are the boolean closure of the Strictly k-Piecewise languages.4 5.4 Strictly k-Local languages To define the Strictly k-Local languages, it is necessa</context>
<context position="22215" citStr="Garcia and Ruiz, 2004" startWordPosition="4098" endWordPosition="4101">ring extension function: LRIk(w) = (Lk(w),Rk(w),Ik(w)) Thus for some k, a grammar G is triple formed by taking subsets of Ek, Ek, and E&lt;k, respectively. A word w belongs to the language of G only if LRIk(w) C� G. Clearly, LLRIk = kSL, and henceforth we refer to this class as k-SL. Since, for fixed k, LRIk E SEF, all of the learning results in §4 apply. 5.5 Locally k-Testable languages The Locally k-testable languages (k-LT) are originally defined in McNaughton and Papert (1971) and are the subject of several studies (Brzozowski and Simon, 1973; McNaughton, 1974; Kim et al., 1991; Caron, 2000; Garcia and Ruiz, 2004; Rogers and Pullum, to appear). A language L is k-testable iff for all w1, w2 E E* such that |w1 |≥ k and |w2 |≥ k, and LRIk(w1) = LRIk(w2) then either both w1, w2 belong to L or neither do. Clearly, every language in k-SL belongs to k-LT. However k-LT properly include k-SL because a k-testable language only distinguishes words whenever LRIk(w1) =� LRIk(w2). It is known that the k-LT languages are the boolean closure of the k-SL (McNaughton and Papert, 1971). The function LRIk exactly expresses k-testable languages. Informally, each word w is mapped to a set containing a single element, this </context>
</contexts>
<marker>Garcia, Ruiz, 2004</marker>
<rawString>Pedro Garcia and Jos´e Ruiz. 2004. Learning k-testable and k-piecewise testable languages from positive data. Grammars, 7:125–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro Garcia</author>
<author>Enrique Vidal</author>
<author>Jos´e Oncina</author>
</authors>
<title>Learning locally testable languages in the strict sense.</title>
<date>1990</date>
<booktitle>In Proceedings of the Workshop on Algorithmic Learning Theory,</booktitle>
<pages>325--338</pages>
<contexts>
<context position="20642" citStr="Garcia et al., 1990" startWordPosition="3782" endWordPosition="3785">A, a, b, aa, ba} {A, a, b, aa, ab, ba} E*\(E*bE*bE*) 3 aba {A, a, b, ab, ba} {A, a, b, aa, ab, ba} E*\(E*bE*bE*) . . . Table 2: The learner OSP2 with a text from the language in Example 2. Boldtype indicates newly added elements to the grammar. These definitions preserve the learning results of §4. Note that the characteristic sample of L E Lf will be the union of the characteristic samples of each fZ and the language Lf (G) is the intersection of Lf,(GZ). Locally k-Testable Languages in the Strict Sense (Strictly k-Local) have been studied by several researchers (McNaughton and Papert, 1971; Garcia et al., 1990; Caron, 2000; Rogers and Pullum, to appear), among others. We follow the definitions from (McNaughton and Papert, 1971, p. 14), effectively encoded in the following functions. Definition 6 Fix k EN. Then the (left-edge) prefix of length k, the (right-edge) suffix of length k, and the interior k-factors of a word w are Lk(w) = {u E Ek : ]v E E* such that w = uv} Rk(w) = {u E Ek : ]v E E* such that w = vu} Ik(w) = fack(w)\(Lk(w) U Rk(w)) Example 3 Suppose w = abcba. Then L2(w) = {ab}, R2(w) = {ba} and I2(w) = {bc, cb}. Example 4 Suppose |w |k. Then Lk(w) = Rk(w) = {w} and Ik(w) = 0. Example 5 S</context>
<context position="30237" citStr="Garcia et al. (1990)" startWordPosition="5514" endWordPosition="5517">OLxz3 and OPt, operating simultaneously.6 The learners make predictions about generalizations, which can be tested in artificial language learning experiments on adults and infants (Rogers and Pullum, to appear; Chambers et al., 2002; Onishi et al., 2003; Cristi´a and Seidl, 2008).7 For theoretical computer science, it remains an open question what property holds of functions f in SEF to ensure that Lf is regular, contextfree, or context-sensitive. For known subregular classes, there are constructions that provide deterministic automata that suggest the relevant properties. (See, for example, Garcia et al. (1990) and Garica and Ruiz (1996).) Also, Timo K¨otzing and Samuel Moelius (p.c.) suggest that the results here may be generalized along the following lines. Instead of defining the function f as a map from strings to finite subsets, let f be a function from strings to elements of a lattice. A grammar G is an element of the lattice and the language of the G are all strings w such that f maps w to a grammar less than G. Learners Of are defined as the least upper bound of its current hypothesis and the grammar to which f maps the current word.8 Kasprzik and K¨otzing (2010) develop this idea and demons</context>
</contexts>
<marker>Garcia, Vidal, Oncina, 1990</marker>
<rawString>Pedro Garcia, Enrique Vidal, and Jos´e Oncina. 1990. Learning locally testable languages in the strict sense. In Proceedings of the Workshop on Algorithmic Learning Theory, pages 325–338.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Gold</author>
</authors>
<title>Language identification in the limit.</title>
<date>1967</date>
<journal>Information and Control,</journal>
<pages>10--447</pages>
<contexts>
<context position="986" citStr="Gold, 1967" startWordPosition="133" endWordPosition="134"> exists. Additionally, the analysis provides a recipe for constructing new learnable classes. Potential applications include learnable models for aspects of natural language and cognition. 1 Introduction The problem of generalizing from examples to patterns is an important one in linguistics and computer science. This paper shows that many disparate language classes, many previously discussed in the literature, have a simple, natural and interesting (because non-enumerative) learner which exactly identifies the class in the limit from distribution-free, positive evidence in the sense of Gold (Gold, 1967).1 These learners are called String Extension Learners because each string in the language can be mapped (extended) to an element of the grammar, which in every case, is conceived as a finite set of elements. These learners have desirable properties: they are incremental, globally consistent, and locally conservative. Classes previously discussed in the literature which are string extension learnable include the Locally Testable (LT) languages, the Locally Testable Languages in the Strict Sense 1The allowance of negative evidence (Gold, 1967) or restricting the kinds of texts the learner is re</context>
<context position="4947" citStr="Gold, 1967" startWordPosition="728" endWordPosition="729">s string extension grammars, languages, and language classes and proves some of their fundamental properties. §4 defines string extension learners and proves their behavior. §5 shows how important subregular classes are string extension language classes. §6 gives examples of nonregular and infinite language classes which are string extension learnable. §7 summarizes the results, and discusses lines of inquiry for future research. 2 Preliminaries This section establishes notation and recalls basic definitions for formal languages, the paradigm of identification in the limit from positive data (Gold, 1967). Familiarity with the basic concepts of sets, functions, and sequences is assumed. For some set A, P(A) denotes the set of all subsets of A and Pfin(A) denotes the set of all finite subsets of A. If f is a function such that f : A → B then let f°(a) = {f(a)}. Thus, f° : A → P(B) (note f° is not surjective). A set 7r of nonempty subsets of S is a partition of S iff the elements of 7r (called blocks) are pairwise disjoint and their union equals S. E denotes a fixed finite set of symbols, the alphabet. Let En, Ern, E*, E+ denote all strings formed over this alphabet of length n, of length less t</context>
</contexts>
<marker>Gold, 1967</marker>
<rawString>E.M. Gold. 1967. Language identification in the limit. Information and Control, 10:447–474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Grainger</author>
<author>C Whitney</author>
</authors>
<title>Does the huamn mnid raed wrods as a wlohe? Trends in Cognitive Science,</title>
<date>2004</date>
<pages>8--58</pages>
<contexts>
<context position="17876" citStr="Grainger and Whitney, 2004" startWordPosition="3265" endWordPosition="3268"> = {a, b} and consider the grammar G = {λ, a, b, aa, ab, ba}. Then L(G) = E∗\(E∗bE∗bE∗). As seen from Example 2, SP languages encode long-distance dependencies. In Example 2, L prohibits a b from following another b in a word, no matter how distant. Table 2 illustrates φSP2 learning the language in Example 2. Heinz (2007,2009a) shows that consonantal harmony patterns in natural language are describable by such SP2 languages and hypothesizes that humans learn them in the way suggested by φSP2. Strictly 2-Piecewise languages have also been used in models of reading comprehension (Whitney, 2001; Grainger and Whitney, 2004; Whitney and Cornelissen, 2008) as well as text classification(Lodhi et al., 2002; Cancedda et al., 2003) (see also (Shawe-Taylor and Christianini, 2005, chap. 11)). 5.3 K-Piecewise Testable languages A language L is k-Piecewise Testable iff whenever strings u and v have the same subsequences 3In earlier work, the function SP2 has been described as returning the set of precedence relations in w, and the language class LSP2 was called the precedence languages (Heinz, 2007; Heinz, to appear). of length at most k and u is in L, then v is in L as well (Simon, 1975; Simon, 1993; Lothaire, 2005). A</context>
</contexts>
<marker>Grainger, Whitney, 2004</marker>
<rawString>J. Grainger and C. Whitney. 2004. Does the huamn mnid raed wrods as a wlohe? Trends in Cognitive Science, 8:58–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Heinz</author>
</authors>
<title>The Inductive Learning of Phonotactic Patterns.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California,</institution>
<location>Los Angeles.</location>
<contexts>
<context position="3086" citStr="Heinz, 2007" startWordPosition="449" endWordPosition="450">al elements of these language classes which allows them to be identifiable in the limit from positive data: each language class induces a natural partition over all logically possible strings and each language in the class is the union of finitely many blocks of this partition. One consequence of this analysis is a recipe for constructing new learnable classes. One notable case is the Strictly Piecewise (SP) languages, which was originally motivated for two reasons: the learnability properties discussed here and its ability to describe long-distance dependencies in natural language phonology (Heinz, 2007; Heinz, to appear). Later this class was discovered to have several independent characterizations and form the basis of another subregular hierarchy (Rogers et al., 2009). It is expected string extension learning will have applications in linguistic and cognitive models. As mentioned, the SP languages already provide a novel hypothesis of how long-distance dependencies in sound patterns are learned. Another example is the Strictly Local (SL) languages which are the categorical, symbolic version of n-gram models, which are widely used in natural language processing (Jurafsky and Martin, 2008).</context>
<context position="17572" citStr="Heinz (2007" startWordPosition="3222" endWordPosition="3223">inition 4 For some k ∈N, SPk(w) = {u ∈ E≤k : u ⊑ w} In other words, SPk(w) returns all subsequences, contiguous or not, in w up to length k. Thus, for some k, a grammar G is a subset of E≤k. Following Definition 1, a word w belongs to the language of G only if SP2(w) ⊆ G.3 Example 2 Let E = {a, b} and consider the grammar G = {λ, a, b, aa, ab, ba}. Then L(G) = E∗\(E∗bE∗bE∗). As seen from Example 2, SP languages encode long-distance dependencies. In Example 2, L prohibits a b from following another b in a word, no matter how distant. Table 2 illustrates φSP2 learning the language in Example 2. Heinz (2007,2009a) shows that consonantal harmony patterns in natural language are describable by such SP2 languages and hypothesizes that humans learn them in the way suggested by φSP2. Strictly 2-Piecewise languages have also been used in models of reading comprehension (Whitney, 2001; Grainger and Whitney, 2004; Whitney and Cornelissen, 2008) as well as text classification(Lodhi et al., 2002; Cancedda et al., 2003) (see also (Shawe-Taylor and Christianini, 2005, chap. 11)). 5.3 K-Piecewise Testable languages A language L is k-Piecewise Testable iff whenever strings u and v have the same subsequences 3</context>
<context position="29542" citStr="Heinz, 2007" startWordPosition="5411" endWordPosition="5412">eed to be efficient in the size of the sample, provided the function f itself is efficient in the length of the string. Several additional questions of interest remain open for theoretical linguistics, theoretical computer science, and computational linguistics. For theoretical linguistics, it appears that the string extension function f = (LRI3i P2), which defines a class of languages which obey restrictions on both contiguous subsequences of length 3 and on discontiguous subsequences of length 2, provides a good first approximation to the segmental phonotactic patterns in natural languages (Heinz, 2007). The string extension learner for this class is essentially two learners: OLxz3 and OPt, operating simultaneously.6 The learners make predictions about generalizations, which can be tested in artificial language learning experiments on adults and infants (Rogers and Pullum, to appear; Chambers et al., 2002; Onishi et al., 2003; Cristi´a and Seidl, 2008).7 For theoretical computer science, it remains an open question what property holds of functions f in SEF to ensure that Lf is regular, contextfree, or context-sensitive. For known subregular classes, there are constructions that provide deter</context>
<context position="32016" citStr="Heinz, 2007" startWordPosition="5807" endWordPosition="5808">language processing techniques. (Heinz and Rogers, 2010) show 6This learner resembles what learning theorists call parallel learning (Case and Moelius, 2007) and what cognitive scientists call modular learning (Gallistel and King, 2009). 7I conjecture that morphological and syntactic patterns are generally not amenable to a string extension learning analysis because these patterns appear to require a paradigm, i.e. a set of data points, before any conclusion can be confidently drawn about the generating grammar. Stress patterns also do not appear to be amenable to a string extension learning (Heinz, 2007; Edlefsen et al., 2008; Heinz, 2009). 8See also Lange et al. (2008, Theorem 15) and Case et al. (1999, pp.101-103). 9The basic idea is to consider the lattice L = (Lf in, Each element of L is a finite set of strings representing the ⊇�. intersection of all pattern languages consistent with this set. 904 how to efficiently estimate k-SP distributions, and it is conjectured that the other string extension language classes can be recast as classes of distributions, which can also be successfully estimated from positive evidence. Acknowledgments This work was supported by a University of Delaware</context>
</contexts>
<marker>Heinz, 2007</marker>
<rawString>Jeffrey Heinz. 2007. The Inductive Learning of Phonotactic Patterns. Ph.D. thesis, University of California, Los Angeles.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Heinz</author>
</authors>
<title>On the role of locality in learning stress patterns.</title>
<date>2009</date>
<journal>Phonology,</journal>
<volume>26</volume>
<issue>2</issue>
<contexts>
<context position="32053" citStr="Heinz, 2009" startWordPosition="5813" endWordPosition="5814">nz and Rogers, 2010) show 6This learner resembles what learning theorists call parallel learning (Case and Moelius, 2007) and what cognitive scientists call modular learning (Gallistel and King, 2009). 7I conjecture that morphological and syntactic patterns are generally not amenable to a string extension learning analysis because these patterns appear to require a paradigm, i.e. a set of data points, before any conclusion can be confidently drawn about the generating grammar. Stress patterns also do not appear to be amenable to a string extension learning (Heinz, 2007; Edlefsen et al., 2008; Heinz, 2009). 8See also Lange et al. (2008, Theorem 15) and Case et al. (1999, pp.101-103). 9The basic idea is to consider the lattice L = (Lf in, Each element of L is a finite set of strings representing the ⊇�. intersection of all pattern languages consistent with this set. 904 how to efficiently estimate k-SP distributions, and it is conjectured that the other string extension language classes can be recast as classes of distributions, which can also be successfully estimated from positive evidence. Acknowledgments This work was supported by a University of Delaware Research Fund grant during the 2008-</context>
</contexts>
<marker>Heinz, 2009</marker>
<rawString>Jeffrey Heinz. 2009. On the role of locality in learning stress patterns. Phonology, 26(2):303–351.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jeffrey Heinz</author>
</authors>
<title>to appear. Learning long distance phonotactics. Linguistic Inquiry.</title>
<marker>Heinz, </marker>
<rawString>Jeffrey Heinz. to appear. Learning long distance phonotactics. Linguistic Inquiry.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Horning</author>
</authors>
<title>A Study of Grammatical Inference.</title>
<date>1969</date>
<tech>Ph.D. thesis,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="1671" citStr="Horning, 1969" startWordPosition="238" endWordPosition="240">ng in the language can be mapped (extended) to an element of the grammar, which in every case, is conceived as a finite set of elements. These learners have desirable properties: they are incremental, globally consistent, and locally conservative. Classes previously discussed in the literature which are string extension learnable include the Locally Testable (LT) languages, the Locally Testable Languages in the Strict Sense 1The allowance of negative evidence (Gold, 1967) or restricting the kinds of texts the learner is required to succeed on (i.e. non-distribution-free evidence) (Gold, 1967; Horning, 1969; Angluin, 1988) admits the learnability of the class of recursively enumerable languages. Classes of languages learnable in the harder, distribution-free, positive-evidenceonly settings are due to structural properties of the language classes that permit generalization (Angluin, 1980b; Blumer et al., 1989). That is the central interest here. (Strictly Local, SL) (McNaughton and Papert, 1971; Rogers and Pullum, to appear), the Piecewise Testable (PT) languages (Simon, 1975), the Piecewise Testable languages in the Strict Sense (Strictly Piecewise, SP) (Rogers et al., 2009), the Strongly Testab</context>
</contexts>
<marker>Horning, 1969</marker>
<rawString>J. J. Horning. 1969. A Study of Grammatical Inference. Ph.D. thesis, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjay Jain</author>
<author>Daniel Osherson</author>
<author>James S Royer</author>
<author>Arun Sharma</author>
</authors>
<title>Systems That Learn: An Introduction to Learning Theory (Learning, Development and Conceptual Change).</title>
<date>1999</date>
<publisher>The MIT Press,</publisher>
<note>2nd edition.</note>
<contexts>
<context position="6322" citStr="Jain et al. (1999)" startWordPosition="1000" endWordPosition="1003">. The range of a string w is the set of symbols which are in w. The empty string is the unique string of length zero denoted λ. Thus range(λ) = ∅. The length of a string u is denoted by |u|, e.g. |λ |= 0. A language L is some subset of E*. The reverse of a language Lr = {wr : w ∈ L}. Gold (1967) establishes a learning paradigm known as identification in the limit from positive data. A text is an infinite sequence whose elements are drawn from E* ∪ {#} where # represents a non-expression. The ith element of t is denoted t(i), and t[i] denotes the finite sequence t(0), t(1), ... t(i). Following Jain et al. (1999), let SEQ denote the set of all possible finite sequences: SEQ = {t[i] : t is a text and i ∈N} The content of a text is defined below. content(t) = {w ∈ E* : ∃n ∈Nsuch that t(n) = w} A text t is a positive text for a language L iff content(t) = L. Thus there is only one text t for the empty language: for all i, t(i) = #. A learner is a function 0 which maps initial finite sequences of texts to grammars, i.e. 0 : SEQ → G. The elements of G (the grammars) generate languages in some well-defined way. A learner converges on a text t iff there exists i ∈Nand a grammar G such that for all j &gt; i, 0(t</context>
<context position="7811" citStr="Jain et al., 1999" startWordPosition="1308" endWordPosition="1311">for any L ∈ L, 0 identifies L in the limit. Angluin (1980b) provides necessary and sufficient properties of language classes which are identifiable in the limit from positive data. A learner 0 of language class L is globally consistent iff for each i and for all texts t for some L ∈ L, content(t[i]) ⊆ L(0(t[i])). A learner 0 is locally conservative iff for each i and for all texts t for some L ∈ L, whenever 0(t[i]) =6 0(t[i − 1]), it is the case that t(i) ∈6 L(0([i−1])). These terms are from Jain et al. (2007). Also, learners which do not depend on the order of the text are called set-driven (Jain et al., 1999, p. 99). 3 Grammars and Languages Consider some set A. A string extension function is a total function f : E* → Pfin(A). It is not required that f be onto. Denote the class of functions which have this general form SEF. Each string extension function is naturally associated with some formal class of grammars and languages. These functions, grammars, and languages are called string extension functions, grammars, and languages, respectively. Definition 1 Let f ∈ SEF. 1. A grammar is a finite subset of A. 2. The language ofgrammar G is Lf(G) = {w ∈ E* : f(w) ⊆ G} 898 3. The class of languages ob</context>
</contexts>
<marker>Jain, Osherson, Royer, Sharma, 1999</marker>
<rawString>Sanjay Jain, Daniel Osherson, James S. Royer, and Arun Sharma. 1999. Systems That Learn: An Introduction to Learning Theory (Learning, Development and Conceptual Change). The MIT Press, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjay Jain</author>
<author>Steffen Lange</author>
<author>Sandra Zilles</author>
</authors>
<title>Some natural conditions on incremental learning.</title>
<date>2007</date>
<journal>Information and Computation,</journal>
<volume>205</volume>
<issue>11</issue>
<contexts>
<context position="7709" citStr="Jain et al. (2007)" startWordPosition="1289" endWordPosition="1292">n t to grammar G and L(G) = L. Finally, a learner 0 identifies a class of languages L in the limit iff for any L ∈ L, 0 identifies L in the limit. Angluin (1980b) provides necessary and sufficient properties of language classes which are identifiable in the limit from positive data. A learner 0 of language class L is globally consistent iff for each i and for all texts t for some L ∈ L, content(t[i]) ⊆ L(0(t[i])). A learner 0 is locally conservative iff for each i and for all texts t for some L ∈ L, whenever 0(t[i]) =6 0(t[i − 1]), it is the case that t(i) ∈6 L(0([i−1])). These terms are from Jain et al. (2007). Also, learners which do not depend on the order of the text are called set-driven (Jain et al., 1999, p. 99). 3 Grammars and Languages Consider some set A. A string extension function is a total function f : E* → Pfin(A). It is not required that f be onto. Denote the class of functions which have this general form SEF. Each string extension function is naturally associated with some formal class of grammars and languages. These functions, grammars, and languages are called string extension functions, grammars, and languages, respectively. Definition 1 Let f ∈ SEF. 1. A grammar is a finite su</context>
</contexts>
<marker>Jain, Lange, Zilles, 2007</marker>
<rawString>Sanjay Jain, Steffen Lange, and Sandra Zilles. 2007. Some natural conditions on incremental learning. Information and Computation, 205(11):1671–1684.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>James Martin</author>
</authors>
<title>Speech and Language Processing: An Introduction to Natural Language Processing, Speech Recognition, and Computational Linguistics. Prentice-Hall, Upper Saddle River, NJ, 2nd edition.</title>
<date>2008</date>
<contexts>
<context position="3685" citStr="Jurafsky and Martin, 2008" startWordPosition="538" endWordPosition="541">uage phonology (Heinz, 2007; Heinz, to appear). Later this class was discovered to have several independent characterizations and form the basis of another subregular hierarchy (Rogers et al., 2009). It is expected string extension learning will have applications in linguistic and cognitive models. As mentioned, the SP languages already provide a novel hypothesis of how long-distance dependencies in sound patterns are learned. Another example is the Strictly Local (SL) languages which are the categorical, symbolic version of n-gram models, which are widely used in natural language processing (Jurafsky and Martin, 2008). Since the SP languages also admit a probabilistic variant which describe an efficiently estimable class of distributions (Heinz and Rogers, 2010), it is plausible to expect the other classes will as well, though this is left for future research. String extension learners are also simple, mak897 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 897–906, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics ing them accessible to linguists without a rigorous mathematical background. This paper is organized as follow. §2 </context>
</contexts>
<marker>Jurafsky, Martin, 2008</marker>
<rawString>Daniel Jurafsky and James Martin. 2008. Speech and Language Processing: An Introduction to Natural Language Processing, Speech Recognition, and Computational Linguistics. Prentice-Hall, Upper Saddle River, NJ, 2nd edition.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Anna Kasprzik</author>
<author>Timo K¨otzing</author>
</authors>
<title>to appear. String extension learning using lattices.</title>
<booktitle>In Proceedings of the 4th International Conference on Language and Automata Theory and Applications (LATA 2010),</booktitle>
<location>Trier, Germany.</location>
<marker>Kasprzik, K¨otzing, </marker>
<rawString>Anna Kasprzik and Timo K¨otzing. to appear. String extension learning using lattices. In Proceedings of the 4th International Conference on Language and Automata Theory and Applications (LATA 2010), Trier, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Kim</author>
<author>R McNaughton</author>
<author>R McCloskey</author>
</authors>
<title>A polynomial time algorithm for the local testability problem of deterministic finite automata.</title>
<date>1991</date>
<journal>IEEE Trans. Comput.,</journal>
<volume>40</volume>
<issue>10</issue>
<contexts>
<context position="22179" citStr="Kim et al., 1991" startWordPosition="4092" endWordPosition="4095"> can now be expressed as the string extension function: LRIk(w) = (Lk(w),Rk(w),Ik(w)) Thus for some k, a grammar G is triple formed by taking subsets of Ek, Ek, and E&lt;k, respectively. A word w belongs to the language of G only if LRIk(w) C� G. Clearly, LLRIk = kSL, and henceforth we refer to this class as k-SL. Since, for fixed k, LRIk E SEF, all of the learning results in §4 apply. 5.5 Locally k-Testable languages The Locally k-testable languages (k-LT) are originally defined in McNaughton and Papert (1971) and are the subject of several studies (Brzozowski and Simon, 1973; McNaughton, 1974; Kim et al., 1991; Caron, 2000; Garcia and Ruiz, 2004; Rogers and Pullum, to appear). A language L is k-testable iff for all w1, w2 E E* such that |w1 |≥ k and |w2 |≥ k, and LRIk(w1) = LRIk(w2) then either both w1, w2 belong to L or neither do. Clearly, every language in k-SL belongs to k-LT. However k-LT properly include k-SL because a k-testable language only distinguishes words whenever LRIk(w1) =� LRIk(w2). It is known that the k-LT languages are the boolean closure of the k-SL (McNaughton and Papert, 1971). The function LRIk exactly expresses k-testable languages. Informally, each word w is mapped to a se</context>
</contexts>
<marker>Kim, McNaughton, McCloskey, 1991</marker>
<rawString>S.M. Kim, R. McNaughton, and R. McCloskey. 1991. A polynomial time algorithm for the local testability problem of deterministic finite automata. IEEE Trans. Comput., 40(10):1087–1093.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonid Kontorovich</author>
<author>Corinna Cortes</author>
<author>Mehryar Mohri</author>
</authors>
<title>Kernel methods for learning languages.</title>
<date>2008</date>
<journal>Theoretical Computer Science,</journal>
<volume>405</volume>
<issue>3</issue>
<contexts>
<context position="18858" citStr="Kontorovich et al., 2008" startWordPosition="3429" endWordPosition="3432">ning the set of precedence relations in w, and the language class LSP2 was called the precedence languages (Heinz, 2007; Heinz, to appear). of length at most k and u is in L, then v is in L as well (Simon, 1975; Simon, 1993; Lothaire, 2005). A language L is said to be Piecewise-Testable (PT) if it is k-Piecewise Testable for some k ∈N. If k is fixed, the k-Piecewise Testable languages are identifiable in the limit from positive data (Garcia and Ruiz, 1996; Garcia and Ruiz, 2004). More recently, the Piecewise Testable languages has been shown to be linearly separable with a subsequence kernel (Kontorovich et al., 2008). The k-Piecewise Testable languages can also be described with the function SPk⋄ . Recall that f⋄(a) = {f(a)}. Thus functions SPk⋄ define grammars as a finite list of sets of subsequences up to length k that may occur in words in the language. This reflects the fact that the k-Piecewise Testable languages are the boolean closure of the Strictly k-Piecewise languages.4 5.4 Strictly k-Local languages To define the Strictly k-Local languages, it is necessary to make a pointwise extension to the definitions in §3. Definition 5 For sets A1,... , An, suppose for (each i, fi : E∗ → Pfin(Ai), and let</context>
</contexts>
<marker>Kontorovich, Cortes, Mohri, 2008</marker>
<rawString>Leonid (Aryeh) Kontorovich, Corinna Cortes, and Mehryar Mohri. 2008. Kernel methods for learning languages. Theoretical Computer Science, 405(3):223 – 236. Algorithmic Learning Theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steffen Lange</author>
<author>Thomas Zeugmann</author>
<author>Sandra Zilles</author>
</authors>
<title>Learning indexed families of recursive languages from positive data: A survey.</title>
<date>2008</date>
<journal>Theoretical Computer Science,</journal>
<pages>397--194</pages>
<contexts>
<context position="32083" citStr="Lange et al. (2008" startWordPosition="5817" endWordPosition="5820">w 6This learner resembles what learning theorists call parallel learning (Case and Moelius, 2007) and what cognitive scientists call modular learning (Gallistel and King, 2009). 7I conjecture that morphological and syntactic patterns are generally not amenable to a string extension learning analysis because these patterns appear to require a paradigm, i.e. a set of data points, before any conclusion can be confidently drawn about the generating grammar. Stress patterns also do not appear to be amenable to a string extension learning (Heinz, 2007; Edlefsen et al., 2008; Heinz, 2009). 8See also Lange et al. (2008, Theorem 15) and Case et al. (1999, pp.101-103). 9The basic idea is to consider the lattice L = (Lf in, Each element of L is a finite set of strings representing the ⊇�. intersection of all pattern languages consistent with this set. 904 how to efficiently estimate k-SP distributions, and it is conjectured that the other string extension language classes can be recast as classes of distributions, which can also be successfully estimated from positive evidence. Acknowledgments This work was supported by a University of Delaware Research Fund grant during the 2008- 2009 academic year. I would l</context>
</contexts>
<marker>Lange, Zeugmann, Zilles, 2008</marker>
<rawString>Steffen Lange, Thomas Zeugmann, and Sandra Zilles. 2008. Learning indexed families of recursive languages from positive data: A survey. Theoretical Computer Science, 397:194–232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Lodhi</author>
<author>N Cristianini</author>
<author>J Shawe-Taylor</author>
<author>C Watkins</author>
</authors>
<title>Text classification using string kernels.</title>
<date>2002</date>
<journal>Journal of Machine Language Research,</journal>
<pages>2--419</pages>
<contexts>
<context position="17958" citStr="Lodhi et al., 2002" startWordPosition="3277" endWordPosition="3280">s seen from Example 2, SP languages encode long-distance dependencies. In Example 2, L prohibits a b from following another b in a word, no matter how distant. Table 2 illustrates φSP2 learning the language in Example 2. Heinz (2007,2009a) shows that consonantal harmony patterns in natural language are describable by such SP2 languages and hypothesizes that humans learn them in the way suggested by φSP2. Strictly 2-Piecewise languages have also been used in models of reading comprehension (Whitney, 2001; Grainger and Whitney, 2004; Whitney and Cornelissen, 2008) as well as text classification(Lodhi et al., 2002; Cancedda et al., 2003) (see also (Shawe-Taylor and Christianini, 2005, chap. 11)). 5.3 K-Piecewise Testable languages A language L is k-Piecewise Testable iff whenever strings u and v have the same subsequences 3In earlier work, the function SP2 has been described as returning the set of precedence relations in w, and the language class LSP2 was called the precedence languages (Heinz, 2007; Heinz, to appear). of length at most k and u is in L, then v is in L as well (Simon, 1975; Simon, 1993; Lothaire, 2005). A language L is said to be Piecewise-Testable (PT) if it is k-Piecewise Testable fo</context>
</contexts>
<marker>Lodhi, Cristianini, Shawe-Taylor, Watkins, 2002</marker>
<rawString>H. Lodhi, N. Cristianini, J. Shawe-Taylor, and C. Watkins. 2002. Text classification using string kernels. Journal of Machine Language Research, 2:419–444.</rawString>
</citation>
<citation valid="true">
<title>Applied Combinatorics on Words.</title>
<date>2005</date>
<editor>M. Lothaire, editor.</editor>
<publisher>Cmbridge University Press,</publisher>
<note>2nd edition.</note>
<marker>2005</marker>
<rawString>M. Lothaire, editor. 2005. Applied Combinatorics on Words. Cmbridge University Press, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert McNaughton</author>
<author>Seymour Papert</author>
</authors>
<title>Counter-Free Automata.</title>
<date>1971</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2065" citStr="McNaughton and Papert, 1971" startWordPosition="291" endWordPosition="294"> Locally Testable Languages in the Strict Sense 1The allowance of negative evidence (Gold, 1967) or restricting the kinds of texts the learner is required to succeed on (i.e. non-distribution-free evidence) (Gold, 1967; Horning, 1969; Angluin, 1988) admits the learnability of the class of recursively enumerable languages. Classes of languages learnable in the harder, distribution-free, positive-evidenceonly settings are due to structural properties of the language classes that permit generalization (Angluin, 1980b; Blumer et al., 1989). That is the central interest here. (Strictly Local, SL) (McNaughton and Papert, 1971; Rogers and Pullum, to appear), the Piecewise Testable (PT) languages (Simon, 1975), the Piecewise Testable languages in the Strict Sense (Strictly Piecewise, SP) (Rogers et al., 2009), the Strongly Testable languages (Beauquier and Pin, 1991), the Definite languages (Brzozowski, 1962), and the Finite languages, among others. To our knowledge, this is the first analysis which identifies the common structural elements of these language classes which allows them to be identifiable in the limit from positive data: each language class induces a natural partition over all logically possible string</context>
<context position="14410" citStr="McNaughton and Papert, 1971" startWordPosition="2599" endWordPosition="2603">evelops a grammar on the basis of the observed words and the function f identifies this class exactly in the limit from positive data. It also follows that if f is efficient in the length of the string then Of is efficient in the length of the sample and that Of is globally consistent, locally conservative, and setdriven. It is striking that such a natural and general framework for generalization exists and that, as will be shown, a variety of language classes can be expressed given the choice of f. 5 Subregular examples This section shows how classes which make up the subregular hierarchies (McNaughton and Papert, 1971) are string extension language classes. Readers are referred to Rogers and Pullum (2007) and Rogers et al. (2009) for an introduction to the subregular hierarchies, as well as their relevance to linguistics and cognition. 5.1 K-factor languages The k-factors of a word are the contiguous subsequences of length k in w. Consider the following string extension function. Definition 3 For some k EN, let fack(w) = {x E Ek : ∃u,v E E* such that w = uxv} when k ≤ |w |and {w} otherwise Following the earlier definitions, for some k, a grammar G is a subset of E≤k and a word w belongs to the language of G</context>
<context position="16183" citStr="McNaughton and Papert, 1971" startWordPosition="2938" endWordPosition="2942"> is {λ, a, ab, ba, aa}; i.e. the canonical grammar itself. It follows from Theorem 5 that the class of k-factor languages is identifiable in the limit by Ofack. The learner Ofac2 with a text from the language in Example 1 is illustrated in Table 1. The class Lfack is not closed under union. For example for k = 2, consider L1 = L({λ, a, b, aa, bb, ba}) and L2 = L({λ, a, b, aa, ab, bb}). Then L1 U L2 excludes string aba, but includes ab and ba, which is not possible for any L E Lfack. K-factors are used to define other language classes, such as the Strictly Local and Locally Testable languages (McNaughton and Papert, 1971), discussed in §5.4 and §5.5. 5.2 Strictly k-Piecewise languages The Strictly k-Piecewise (SPk) languages (Rogers et al., 2009) can be defined with a function whose co-domain is P(E≤k). However unlike the function fack, the function SPk, does not require that the k-length subsequences be contiguous. 900 i t(i) fac2(t(i)) Grammar G L(G) -1 ∅ ∅ 0 aaaa {aa} {aa} aaa∗ 1 aab {aa, ab} {aa, ab} aaa∗ ∪ aaa∗b 2 a {a} {a, aa, ab} aa∗ ∪ aa∗b . . . Table 1: The learner φfac2 with a text from the language in Example 1. Boldtype indicates newly added elements to the grammar. A string u = a1 ... ak is a subs</context>
<context position="20621" citStr="McNaughton and Papert, 1971" startWordPosition="3778" endWordPosition="3781">, aa, b, ab} a* U a*b 2 baa {A, a, b, aa, ba} {A, a, b, aa, ab, ba} E*\(E*bE*bE*) 3 aba {A, a, b, ab, ba} {A, a, b, aa, ab, ba} E*\(E*bE*bE*) . . . Table 2: The learner OSP2 with a text from the language in Example 2. Boldtype indicates newly added elements to the grammar. These definitions preserve the learning results of §4. Note that the characteristic sample of L E Lf will be the union of the characteristic samples of each fZ and the language Lf (G) is the intersection of Lf,(GZ). Locally k-Testable Languages in the Strict Sense (Strictly k-Local) have been studied by several researchers (McNaughton and Papert, 1971; Garcia et al., 1990; Caron, 2000; Rogers and Pullum, to appear), among others. We follow the definitions from (McNaughton and Papert, 1971, p. 14), effectively encoded in the following functions. Definition 6 Fix k EN. Then the (left-edge) prefix of length k, the (right-edge) suffix of length k, and the interior k-factors of a word w are Lk(w) = {u E Ek : ]v E E* such that w = uv} Rk(w) = {u E Ek : ]v E E* such that w = vu} Ik(w) = fack(w)\(Lk(w) U Rk(w)) Example 3 Suppose w = abcba. Then L2(w) = {ab}, R2(w) = {ba} and I2(w) = {bc, cb}. Example 4 Suppose |w |k. Then Lk(w) = Rk(w) = {w} and I</context>
<context position="22076" citStr="McNaughton and Papert (1971)" startWordPosition="4075" endWordPosition="4078"> C I. McNaughton and Papert note that if w is of length less than k than L may be perfectly arbitrary about w. This can now be expressed as the string extension function: LRIk(w) = (Lk(w),Rk(w),Ik(w)) Thus for some k, a grammar G is triple formed by taking subsets of Ek, Ek, and E&lt;k, respectively. A word w belongs to the language of G only if LRIk(w) C� G. Clearly, LLRIk = kSL, and henceforth we refer to this class as k-SL. Since, for fixed k, LRIk E SEF, all of the learning results in §4 apply. 5.5 Locally k-Testable languages The Locally k-testable languages (k-LT) are originally defined in McNaughton and Papert (1971) and are the subject of several studies (Brzozowski and Simon, 1973; McNaughton, 1974; Kim et al., 1991; Caron, 2000; Garcia and Ruiz, 2004; Rogers and Pullum, to appear). A language L is k-testable iff for all w1, w2 E E* such that |w1 |≥ k and |w2 |≥ k, and LRIk(w1) = LRIk(w2) then either both w1, w2 belong to L or neither do. Clearly, every language in k-SL belongs to k-LT. However k-LT properly include k-SL because a k-testable language only distinguishes words whenever LRIk(w1) =� LRIk(w2). It is known that the k-LT languages are the boolean closure of the k-SL (McNaughton and Papert, 197</context>
</contexts>
<marker>McNaughton, Papert, 1971</marker>
<rawString>Robert McNaughton and Seymour Papert. 1971. Counter-Free Automata. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McNaughton</author>
</authors>
<title>Algebraic decision procedures for local testability.</title>
<date>1974</date>
<journal>Math. Systems Theory,</journal>
<pages>8--60</pages>
<contexts>
<context position="22161" citStr="McNaughton, 1974" startWordPosition="4090" endWordPosition="4091">rary about w. This can now be expressed as the string extension function: LRIk(w) = (Lk(w),Rk(w),Ik(w)) Thus for some k, a grammar G is triple formed by taking subsets of Ek, Ek, and E&lt;k, respectively. A word w belongs to the language of G only if LRIk(w) C� G. Clearly, LLRIk = kSL, and henceforth we refer to this class as k-SL. Since, for fixed k, LRIk E SEF, all of the learning results in §4 apply. 5.5 Locally k-Testable languages The Locally k-testable languages (k-LT) are originally defined in McNaughton and Papert (1971) and are the subject of several studies (Brzozowski and Simon, 1973; McNaughton, 1974; Kim et al., 1991; Caron, 2000; Garcia and Ruiz, 2004; Rogers and Pullum, to appear). A language L is k-testable iff for all w1, w2 E E* such that |w1 |≥ k and |w2 |≥ k, and LRIk(w1) = LRIk(w2) then either both w1, w2 belong to L or neither do. Clearly, every language in k-SL belongs to k-LT. However k-LT properly include k-SL because a k-testable language only distinguishes words whenever LRIk(w1) =� LRIk(w2). It is known that the k-LT languages are the boolean closure of the k-SL (McNaughton and Papert, 1971). The function LRIk exactly expresses k-testable languages. Informally, each word w</context>
</contexts>
<marker>McNaughton, 1974</marker>
<rawString>R. McNaughton. 1974. Algebraic decision procedures for local testability. Math. Systems Theory, 8:60–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Parikh</author>
</authors>
<title>On context-free languages.</title>
<date>1966</date>
<journal>Journal of the ACM,</journal>
<volume>13</volume>
<pages>13--570</pages>
<contexts>
<context position="26714" citStr="Parikh, 1966" startWordPosition="4919" endWordPosition="4920">anguages, denoted Lfin. Example 10 Consider the function id which maps words in Σ* to their singleton sets, i.e. id(w) = {w}.5 A grammar G is then a finite subset of Σ*, and so L(G) is just a finite set of words in Σ*; in fact, L(G) = G. It follows that Lid = Lfin. It can be easily seen that the function id induces the trivial partition over Σ*, and languages are just finite unions of these blocks. The learner φid makes no generalizations at all, and only remembers what it has observed. There are other more interesting infinite string extension classes. Here is one relating to the Parikh map (Parikh, 1966). For all a E Σ, let fa(w) be the set containing n where n is the number of times the letter a occurs in the string w. For 5Strictly speaking, this is not the identity function per se, but it is as close to the identity function as one can get since string extension functions are defined as mappings from strings to sets. However, once the domain of the function is extended (Equation 1), then it follows that id is the identity function when its argument is a set of strings. 903 example fa(babab) = {2}. Thus fa is atotal function mapping strings to singleton sets of natural numbers, so it is a s</context>
</contexts>
<marker>Parikh, 1966</marker>
<rawString>R. J. Parikh. 1966. On context-free languages. Journal of the ACM, 13, 570581., 13:570–581.</rawString>
</citation>
<citation valid="false">
<authors>
<author>James Rogers</author>
<author>Geoffrey Pullum</author>
</authors>
<title>to appear. Aural pattern recognition experiments and the subregular hierarchy.</title>
<journal>Journal of Logic, Language and Information.</journal>
<marker>Rogers, Pullum, </marker>
<rawString>James Rogers and Geoffrey Pullum. to appear. Aural pattern recognition experiments and the subregular hierarchy. Journal of Logic, Language and Information.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Rogers</author>
<author>Jeffrey Heinz</author>
<author>Gil Bailey</author>
<author>Matt Edlefsen</author>
<author>Molly Visscher</author>
<author>David Wellcome</author>
<author>Sean Wibel</author>
</authors>
<title>On languages piecewise testable in the strict sense.</title>
<date>2009</date>
<booktitle>In Proceedings of the 11th Meeting of the Assocation for Mathematics ofLanguage.</booktitle>
<contexts>
<context position="2250" citStr="Rogers et al., 2009" startWordPosition="319" endWordPosition="322">free evidence) (Gold, 1967; Horning, 1969; Angluin, 1988) admits the learnability of the class of recursively enumerable languages. Classes of languages learnable in the harder, distribution-free, positive-evidenceonly settings are due to structural properties of the language classes that permit generalization (Angluin, 1980b; Blumer et al., 1989). That is the central interest here. (Strictly Local, SL) (McNaughton and Papert, 1971; Rogers and Pullum, to appear), the Piecewise Testable (PT) languages (Simon, 1975), the Piecewise Testable languages in the Strict Sense (Strictly Piecewise, SP) (Rogers et al., 2009), the Strongly Testable languages (Beauquier and Pin, 1991), the Definite languages (Brzozowski, 1962), and the Finite languages, among others. To our knowledge, this is the first analysis which identifies the common structural elements of these language classes which allows them to be identifiable in the limit from positive data: each language class induces a natural partition over all logically possible strings and each language in the class is the union of finitely many blocks of this partition. One consequence of this analysis is a recipe for constructing new learnable classes. One notable</context>
<context position="14523" citStr="Rogers et al. (2009)" startWordPosition="2618" endWordPosition="2621">ositive data. It also follows that if f is efficient in the length of the string then Of is efficient in the length of the sample and that Of is globally consistent, locally conservative, and setdriven. It is striking that such a natural and general framework for generalization exists and that, as will be shown, a variety of language classes can be expressed given the choice of f. 5 Subregular examples This section shows how classes which make up the subregular hierarchies (McNaughton and Papert, 1971) are string extension language classes. Readers are referred to Rogers and Pullum (2007) and Rogers et al. (2009) for an introduction to the subregular hierarchies, as well as their relevance to linguistics and cognition. 5.1 K-factor languages The k-factors of a word are the contiguous subsequences of length k in w. Consider the following string extension function. Definition 3 For some k EN, let fack(w) = {x E Ek : ∃u,v E E* such that w = uxv} when k ≤ |w |and {w} otherwise Following the earlier definitions, for some k, a grammar G is a subset of E≤k and a word w belongs to the language of G iff fack(w) C G. Example 1 Let E = {a, b} and consider grammars G = {λ, a, aa, ab, ba}. Then L(G) = {λ, a} U {w </context>
<context position="16310" citStr="Rogers et al., 2009" startWordPosition="2957" endWordPosition="2960">le in the limit by Ofack. The learner Ofac2 with a text from the language in Example 1 is illustrated in Table 1. The class Lfack is not closed under union. For example for k = 2, consider L1 = L({λ, a, b, aa, bb, ba}) and L2 = L({λ, a, b, aa, ab, bb}). Then L1 U L2 excludes string aba, but includes ab and ba, which is not possible for any L E Lfack. K-factors are used to define other language classes, such as the Strictly Local and Locally Testable languages (McNaughton and Papert, 1971), discussed in §5.4 and §5.5. 5.2 Strictly k-Piecewise languages The Strictly k-Piecewise (SPk) languages (Rogers et al., 2009) can be defined with a function whose co-domain is P(E≤k). However unlike the function fack, the function SPk, does not require that the k-length subsequences be contiguous. 900 i t(i) fac2(t(i)) Grammar G L(G) -1 ∅ ∅ 0 aaaa {aa} {aa} aaa∗ 1 aab {aa, ab} {aa, ab} aaa∗ ∪ aaa∗b 2 a {a} {a, aa, ab} aa∗ ∪ aa∗b . . . Table 1: The learner φfac2 with a text from the language in Example 1. Boldtype indicates newly added elements to the grammar. A string u = a1 ... ak is a subsequence of string w iff ∃ v0, v1, ... vk ∈ E∗ such that w = v0a1v1 ... akvk. The empty string λ is a subsequence of every strin</context>
</contexts>
<marker>Rogers, Heinz, Bailey, Edlefsen, Visscher, Wellcome, Wibel, 2009</marker>
<rawString>James Rogers, Jeffrey Heinz, Gil Bailey, Matt Edlefsen, Molly Visscher, David Wellcome, and Sean Wibel. 2009. On languages piecewise testable in the strict sense. In Proceedings of the 11th Meeting of the Assocation for Mathematics ofLanguage.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Shawe-Taylor</author>
<author>Nello Christianini</author>
</authors>
<title>Kernel Methods for Pattern Analysis.</title>
<date>2005</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="18029" citStr="Shawe-Taylor and Christianini, 2005" startWordPosition="3287" endWordPosition="3290">ce dependencies. In Example 2, L prohibits a b from following another b in a word, no matter how distant. Table 2 illustrates φSP2 learning the language in Example 2. Heinz (2007,2009a) shows that consonantal harmony patterns in natural language are describable by such SP2 languages and hypothesizes that humans learn them in the way suggested by φSP2. Strictly 2-Piecewise languages have also been used in models of reading comprehension (Whitney, 2001; Grainger and Whitney, 2004; Whitney and Cornelissen, 2008) as well as text classification(Lodhi et al., 2002; Cancedda et al., 2003) (see also (Shawe-Taylor and Christianini, 2005, chap. 11)). 5.3 K-Piecewise Testable languages A language L is k-Piecewise Testable iff whenever strings u and v have the same subsequences 3In earlier work, the function SP2 has been described as returning the set of precedence relations in w, and the language class LSP2 was called the precedence languages (Heinz, 2007; Heinz, to appear). of length at most k and u is in L, then v is in L as well (Simon, 1975; Simon, 1993; Lothaire, 2005). A language L is said to be Piecewise-Testable (PT) if it is k-Piecewise Testable for some k ∈N. If k is fixed, the k-Piecewise Testable languages are iden</context>
</contexts>
<marker>Shawe-Taylor, Christianini, 2005</marker>
<rawString>John Shawe-Taylor and Nello Christianini. 2005. Kernel Methods for Pattern Analysis. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Imre Simon</author>
</authors>
<title>Piecewise testable events.</title>
<date>1975</date>
<booktitle>In Automata Theory and Formal Languages,</booktitle>
<pages>214--222</pages>
<contexts>
<context position="2149" citStr="Simon, 1975" startWordPosition="306" endWordPosition="307"> restricting the kinds of texts the learner is required to succeed on (i.e. non-distribution-free evidence) (Gold, 1967; Horning, 1969; Angluin, 1988) admits the learnability of the class of recursively enumerable languages. Classes of languages learnable in the harder, distribution-free, positive-evidenceonly settings are due to structural properties of the language classes that permit generalization (Angluin, 1980b; Blumer et al., 1989). That is the central interest here. (Strictly Local, SL) (McNaughton and Papert, 1971; Rogers and Pullum, to appear), the Piecewise Testable (PT) languages (Simon, 1975), the Piecewise Testable languages in the Strict Sense (Strictly Piecewise, SP) (Rogers et al., 2009), the Strongly Testable languages (Beauquier and Pin, 1991), the Definite languages (Brzozowski, 1962), and the Finite languages, among others. To our knowledge, this is the first analysis which identifies the common structural elements of these language classes which allows them to be identifiable in the limit from positive data: each language class induces a natural partition over all logically possible strings and each language in the class is the union of finitely many blocks of this partit</context>
<context position="18443" citStr="Simon, 1975" startWordPosition="3364" endWordPosition="3365">n (Whitney, 2001; Grainger and Whitney, 2004; Whitney and Cornelissen, 2008) as well as text classification(Lodhi et al., 2002; Cancedda et al., 2003) (see also (Shawe-Taylor and Christianini, 2005, chap. 11)). 5.3 K-Piecewise Testable languages A language L is k-Piecewise Testable iff whenever strings u and v have the same subsequences 3In earlier work, the function SP2 has been described as returning the set of precedence relations in w, and the language class LSP2 was called the precedence languages (Heinz, 2007; Heinz, to appear). of length at most k and u is in L, then v is in L as well (Simon, 1975; Simon, 1993; Lothaire, 2005). A language L is said to be Piecewise-Testable (PT) if it is k-Piecewise Testable for some k ∈N. If k is fixed, the k-Piecewise Testable languages are identifiable in the limit from positive data (Garcia and Ruiz, 1996; Garcia and Ruiz, 2004). More recently, the Piecewise Testable languages has been shown to be linearly separable with a subsequence kernel (Kontorovich et al., 2008). The k-Piecewise Testable languages can also be described with the function SPk⋄ . Recall that f⋄(a) = {f(a)}. Thus functions SPk⋄ define grammars as a finite list of sets of subsequen</context>
</contexts>
<marker>Simon, 1975</marker>
<rawString>Imre Simon. 1975. Piecewise testable events. In Automata Theory and Formal Languages, pages 214– 222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Imre Simon</author>
</authors>
<title>The product of rational languages.</title>
<date>1993</date>
<booktitle>In ICALP ’93: Proceedings of the 20th International Colloquium on Automata, Languages and Programming,</booktitle>
<pages>430--444</pages>
<publisher>Springer-Verlag.</publisher>
<location>London, UK.</location>
<contexts>
<context position="18456" citStr="Simon, 1993" startWordPosition="3366" endWordPosition="3367">001; Grainger and Whitney, 2004; Whitney and Cornelissen, 2008) as well as text classification(Lodhi et al., 2002; Cancedda et al., 2003) (see also (Shawe-Taylor and Christianini, 2005, chap. 11)). 5.3 K-Piecewise Testable languages A language L is k-Piecewise Testable iff whenever strings u and v have the same subsequences 3In earlier work, the function SP2 has been described as returning the set of precedence relations in w, and the language class LSP2 was called the precedence languages (Heinz, 2007; Heinz, to appear). of length at most k and u is in L, then v is in L as well (Simon, 1975; Simon, 1993; Lothaire, 2005). A language L is said to be Piecewise-Testable (PT) if it is k-Piecewise Testable for some k ∈N. If k is fixed, the k-Piecewise Testable languages are identifiable in the limit from positive data (Garcia and Ruiz, 1996; Garcia and Ruiz, 2004). More recently, the Piecewise Testable languages has been shown to be linearly separable with a subsequence kernel (Kontorovich et al., 2008). The k-Piecewise Testable languages can also be described with the function SPk⋄ . Recall that f⋄(a) = {f(a)}. Thus functions SPk⋄ define grammars as a finite list of sets of subsequences up to len</context>
</contexts>
<marker>Simon, 1993</marker>
<rawString>Imre Simon. 1993. The product of rational languages. In ICALP ’93: Proceedings of the 20th International Colloquium on Automata, Languages and Programming, pages 430–444, London, UK. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carol Whitney</author>
<author>Piers Cornelissen</author>
</authors>
<date>2008</date>
<booktitle>SERIOL reading. Language and Cognitive Processes,</booktitle>
<pages>23--143</pages>
<contexts>
<context position="17908" citStr="Whitney and Cornelissen, 2008" startWordPosition="3269" endWordPosition="3272">rammar G = {λ, a, b, aa, ab, ba}. Then L(G) = E∗\(E∗bE∗bE∗). As seen from Example 2, SP languages encode long-distance dependencies. In Example 2, L prohibits a b from following another b in a word, no matter how distant. Table 2 illustrates φSP2 learning the language in Example 2. Heinz (2007,2009a) shows that consonantal harmony patterns in natural language are describable by such SP2 languages and hypothesizes that humans learn them in the way suggested by φSP2. Strictly 2-Piecewise languages have also been used in models of reading comprehension (Whitney, 2001; Grainger and Whitney, 2004; Whitney and Cornelissen, 2008) as well as text classification(Lodhi et al., 2002; Cancedda et al., 2003) (see also (Shawe-Taylor and Christianini, 2005, chap. 11)). 5.3 K-Piecewise Testable languages A language L is k-Piecewise Testable iff whenever strings u and v have the same subsequences 3In earlier work, the function SP2 has been described as returning the set of precedence relations in w, and the language class LSP2 was called the precedence languages (Heinz, 2007; Heinz, to appear). of length at most k and u is in L, then v is in L as well (Simon, 1975; Simon, 1993; Lothaire, 2005). A language L is said to be Piecew</context>
</contexts>
<marker>Whitney, Cornelissen, 2008</marker>
<rawString>Carol Whitney and Piers Cornelissen. 2008. SERIOL reading. Language and Cognitive Processes, 23:143–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carol Whitney</author>
</authors>
<title>How the brain encodes the order of letters in a printed word: the SERIOL model and selective literature review. Psychonomic Bulletin Review,</title>
<date>2001</date>
<pages>8--221</pages>
<contexts>
<context position="17848" citStr="Whitney, 2001" startWordPosition="3263" endWordPosition="3264">Example 2 Let E = {a, b} and consider the grammar G = {λ, a, b, aa, ab, ba}. Then L(G) = E∗\(E∗bE∗bE∗). As seen from Example 2, SP languages encode long-distance dependencies. In Example 2, L prohibits a b from following another b in a word, no matter how distant. Table 2 illustrates φSP2 learning the language in Example 2. Heinz (2007,2009a) shows that consonantal harmony patterns in natural language are describable by such SP2 languages and hypothesizes that humans learn them in the way suggested by φSP2. Strictly 2-Piecewise languages have also been used in models of reading comprehension (Whitney, 2001; Grainger and Whitney, 2004; Whitney and Cornelissen, 2008) as well as text classification(Lodhi et al., 2002; Cancedda et al., 2003) (see also (Shawe-Taylor and Christianini, 2005, chap. 11)). 5.3 K-Piecewise Testable languages A language L is k-Piecewise Testable iff whenever strings u and v have the same subsequences 3In earlier work, the function SP2 has been described as returning the set of precedence relations in w, and the language class LSP2 was called the precedence languages (Heinz, 2007; Heinz, to appear). of length at most k and u is in L, then v is in L as well (Simon, 1975; Sim</context>
</contexts>
<marker>Whitney, 2001</marker>
<rawString>Carol Whitney. 2001. How the brain encodes the order of letters in a printed word: the SERIOL model and selective literature review. Psychonomic Bulletin Review, 8:221–243.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>