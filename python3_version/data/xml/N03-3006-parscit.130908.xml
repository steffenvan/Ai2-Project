<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001647">
<note confidence="0.932773333333333">
Proceedings of HLT-NAACL 2003
Student Research Workshop , pp. 31-36
Edmonton, May-June 2003
</note>
<title confidence="0.960311">
A low-complexity, broad-coverage probabilistic Dependency Parser for
English
</title>
<author confidence="0.998338">
Gerold Schneider
</author>
<affiliation confidence="0.999779">
Institute of Computational Linguistics, University of Zurich
Department of Linguistics, University of Geneva
</affiliation>
<email confidence="0.994657">
gerold.schneider@lettres.unige.ch
</email>
<sectionHeader confidence="0.995599" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999749272727273">
Large-scale parsing is still a complex and time-
consuming process, often so much that it is in-
feasible in real-world applications. The parsing
system described here addresses this problem
by combining finite-state approaches, statisti-
cal parsing techniques and engineering knowl-
edge, thus keeping parsing complexity as low
as possible at the cost of a slight decrease in
performance. The parser is robust and fast
and at the same time based on strong linguis-
tic foundations.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999884256410257">
Many extensions to text-based, data-intensive knowledge
management approaches, such as Information Retrieval
or Data Mining, focus on integrating the impressive re-
cent advances in language technology. For this, they need
fast, robust parsers that deliver linguistic data which is
meaningful for the subsequent processing stages. This
paper presents such a parsing system. Its output is a hi-
erarchical structure of syntactic relations, functional de-
pendency structures, which are discussed in section 2.
The parser differs on the one hand from successful De-
pendency Grammar implementations (e.g. (Lin, 1998),
(Tapanainen and J¨arvinen, 1997)) by using a statistical
base, and on the other hand from state-of-the-art statisti-
cal approaches (e.g. (Collins, 1999)) by carefully follow-
ing an established formal grammar theory, Dependency
Grammar (DG). It combines two probabilistic models of
language, similar to (Collins, 1999), which are discussed
in section 3. Both are supervised and based on Maximum
Likelihood Estimation (MLE). The first one is based on
the lexical probabilities of the heads of phrases, similar
to (Collins and Brooks, 1995). It calculates the probabil-
ity of finding specific syntactic relations (such as subject,
sentential object, etc.) between given lexical heads. Two
simple extensions for the interaction between several de-
pendents of the same mother node are also used. The
second probability model is a PCFG for the production
of the VP. Although traditional CFGs are not part of DG,
VP PCFG rules can model verb subcategorization frames,
an important DG component.
The parser has been trained, developed and tested on
a large collection of syntactically analyzed sentences, the
Penn Treebank (Marcus et al., 1993). It is broad-coverage
and robust and returns an optimal set of partial structures
when it fails to find a complete structure for a sentence. It
has been designed to keep complexity as low as possible
during the parsing process in order to be fast enough to be
useful for parsing large amounts of unrestricted text. This
has been achieved by observing the following constraints,
discussed in section 4:
</bodyText>
<listItem confidence="0.9994435">
• using a syntactic theory known for its relatively flat
structures and lack of empty nodes (see also subsec-
tion 2.4)
• relying on finite-state preprocessing
• discarding unlikely readings with a beam search
• using the fast Cocke-Younger-Kasami (CYK) pars-
ing algorithm
• using a restrictive hand-written linguistic grammar
</listItem>
<bodyText confidence="0.99982075">
The parsing system uses a divide-and-conquer approach.
Low-level linguistic tasks that can be reliably solved by
finite-state techniques are handed over to them. These
low-level tasks are the recognition of part-of-speech by
means of tagging, and the recognition of base NPs and
verbal groups by means of chunking. The parser then re-
lies on the disambiguation decisions of the tagging and
chunking stage and can profit from a reduced search
space, at the cost of a slightly decreased performance due
to tagging and chunking errors.
The paper ends with a preliminary evaluation of this
work in progress.
</bodyText>
<equation confidence="0.980211346153846">
Det
Y
Y
Subj
Rel
Subj
Y
PP
PObj
Obj
Det
V
V
V
Y
V
the/D man/N that/IN came/V eats/V bananas/N with/IN a/D fork/N
eats/V
✭✭✭✭✭✭✭✭ ✭ ❤❤❤❤❤❤❤❤❤
man/N
✘✘ ✘✘✘ � ❳❳❳❳❳
eats/V
✥ ✥✥✥✥✥ ✱ ❵❵❵❵❵❵
✱
came/V
✧ ❜❜
</equation>
<figure confidence="0.7737042">
✧
eats/V
bananas/N
with/IN
✟✟ ✟ ❍❍❍
the/D man/N
the man that/IN came/V eats bananas with/IN fork/N
✚ ✚ ❩❩
that came with a/D fork/N
a fork
</figure>
<figureCaption confidence="0.999887">
Figure 1: A dependency representation and its typically unlabeled constituency counterpart
</figureCaption>
<sectionHeader confidence="0.940756" genericHeader="method">
2 Dependency Grammar
</sectionHeader>
<bodyText confidence="0.9997365">
This system quite strictly follows DG assumptions. De-
pendency Grammar (DG) is essentially a valency gram-
mar in which the valency concept is extended from verbs
to nouns and adjectives and finally to all word classes.
</bodyText>
<subsectionHeader confidence="0.98572">
2.1 Relation to Constituency
</subsectionHeader>
<bodyText confidence="0.999916333333333">
In its simplest definition, a projective DG is a binary ver-
sion (except for valency, see 2.2) of a constituent gram-
mar which only knows lexical items, which entails that
</bodyText>
<listItem confidence="0.998584875">
• for every mother node, the mother node and exactly
one of its daughters, the so-called head, are isomor-
phic
• projection is deterministic, endocentric and can thus
not fail, which gives DG a robustness advantage
• equivalent constituency CFG trees can be derived
• it is in Chomsky Normal Form (CNF), the efficient
CYK parsing algorithm can thus be used
</listItem>
<bodyText confidence="0.991453333333333">
Any DG has an equivalent constituency counterpart
(Covington, 1994). Figure 1 shows a dependency struc-
ture and its unlabeled constituency counterpart.
</bodyText>
<subsectionHeader confidence="0.998618">
2.2 Valency as an isomorphism constraint
</subsectionHeader>
<bodyText confidence="0.999305714285714">
Total equivalence between mother and head daughter
could not prevent a verb from taking an infinite number of
subjects or objects. Therefore, valency theory is as vital
a part of DG as is its constituency counterpart, subcat-
egorization. The manually written rules check the most
obvious valency constraints. Verbal valency is modeled
by a PCFG for VP production.
</bodyText>
<figure confidence="0.6807668">
Obj
Sentobj
Subj
V
What did you think Mary said
</figure>
<figureCaption confidence="0.994962">
Figure 2: Non-projective analysis of a WH-question
</figureCaption>
<subsectionHeader confidence="0.990592">
2.3 Functionalism
</subsectionHeader>
<bodyText confidence="0.999501">
DG was originally conceived to be a deep-syntactic,
proto-semantic theory (Tesni`ere, 1959). The version of
DG used here retains syntactic functions as dependency
labels, like in LFG, which means that the dependency
analyses returned by the parser are also a simple ver-
sion of LFG f-structures, a hierarchy of syntactic rela-
tions between lexical heads which serves as a bridgehead
to semantics. Functional DG only accepts content words
as heads. This has the advantage that no empty heads
(for example empty complementizers for zero-relatives)
are needed. It also means that its syntactical structures
are closer to argument-structure representation than tra-
ditional constituency-based structures such as those of
GB or the Treebank. The closeness to argument-structure
makes them especially useful for subsequent stages of
knowledge management processing.
A restricted use of Tesni`ere-style translations is also
made. Adjectives outside a noun chunk may function as
a nominal constituent (the poor/JJ are the saint/JJ). Par-
ticiples may function as adjectives (Western industrial-
ized/VBN countries). Present participles may also func-
tion as nouns (after winning/VBG the race).
Traditional constituency analyses such as those in the
</bodyText>
<equation confidence="0.749873333333333">
Y
Y
aux
Y
Y
Subj
</equation>
<bodyText confidence="0.9997720625">
Treebank contain many discontinuous constituents, also
known as long-distance dependencies, expressed by the
use of structure-copying methods. This parser deals
with them by allowing non-projectivity in specific, well-
defined situations, such as in WH-questions (Figure 2).
But in order to keep complexity low, discontinuity is re-
stricted to a minimum. Many long-distance dependen-
cies are not strictly necessary. For example, the analy-
sis of passive clauses does not need to involve discon-
tinuity, in which a subordinate VP whose absent object
is structure-shared with the subject of the superordinate
VP. Because the verb form allows a clear identification of
passive clauses, a surface analysis is sufficient, as long as
an appropriate probability model is used. In this parser,
passive subjects use their own probability model, which
is completely distinct from active subjects.
</bodyText>
<subsectionHeader confidence="0.9709505">
2.4 Mapping the Treebank to Functional
Dependency
</subsectionHeader>
<bodyText confidence="0.99979971875">
A popular query tool for the extraction of tree structures
from Treebanks, tgrep, has been used for the mapping
to dependencies. The mapping from a configurational
paradigm to a functional one turns out to be non-trivial
(Musillo and Sima’an, 2002). A relatively simple exam-
ple, the verb-object (obj) relation is discussed now.
In a first approximation, a verb–object relation holds
between the head of a VP and the head of the NP im-
mediately under the VP. In most cases, the VP head is the
lowest verb and the NP head is the lowest rightmost noun.
As tgrep seriously overgenerates, a large number of
highly specific subqueries had to be used, specifying
all possible configurations of arbitrarily nested NPs and
VPs. Since hundreds of possible configurations are thus
mapped onto one dependency relation, statistical mod-
els based on them are much less sparse than lexicalized
PCFGs, which is an advantage as lexicalized models of-
ten suffer from sparseness. In order to extract relations
compatible to the parser’s treatment of conjunction and
apposition, the queries had to be further specified, thereby
missing few structures that should match.
In order to restrict discontinuity to where it is strictly
necessary, copular verb complements and small clause
complements are also treated as objects. Since the func-
tion of such objects can be unambiguously derived from a
verb’s lexical entry this is a linguistically viable decision.
The mapping from the Penn treebank to dependencies
by means of tgrep is a close approximation but not a
complete mapping. A few structures corresponding to
a certain dependency are almost certain to be missed or
doubled. Also, structures involving genuine discontinuity
like the verb–object relation in figure 2 are not extracted.
</bodyText>
<sectionHeader confidence="0.994386" genericHeader="method">
3 Probabilistic Models of Language
</sectionHeader>
<bodyText confidence="0.999968805555556">
Writing grammar rules is an easy task for a linguist, par-
ticularly when using a framework that is close to tra-
ditional school grammar assumptions, such as DG. Ac-
knowledged facts such as the one that a verb has typically
one but never two subjects are expressed in hand-written
declarative rules. The rules of this parser are based on
the Treebank tags of heads of chunks. Since the tagset is
limited and dependency rules are binary, even a broad-
coverage set of rules can be written in relatively little
time.
What is much more difficult, also for a linguist, is to as-
sess the scope of application of a rule and the amount of
ambiguity it creates. Long real-world sentences typically
have dozens to hundreds of syntactically correct complete
analyses and thousands of partial analyses, although most
of them are semantically so odd that one would never
think of them. Here, machine-learning approaches, such
as probabilizing the manually written rules, are vital to
any parser, for two reasons: first, the syntactically possi-
ble analyses can be ranked according to their probabili-
ties. For subsequent processing stages like semantic in-
terpretation or document classification it then often suf-
fices to take the first ranked or the n first ranked readings.
Second, in the course of the parsing process, very im-
probable analyses can be abandoned, which greatly im-
proves parsing efficiency (see section 4).
The parser uses two linguistic probability models. The
first one is based on the lexical probabilities of the heads
of phrases. Two simple extensions for the interaction be-
tween several dependents of the same mother node are
also used. The second probability model is a PCFG for
the expansion of the VP.
Since the parser aims at a global disambiguation, all
local probabilities are stored in the parsing chart. The
global probability of a parse is the product of all its local
probabilities, a product of disambiguation decisions.
</bodyText>
<subsectionHeader confidence="0.996656">
3.1 Lexical Dependencies
</subsectionHeader>
<bodyText confidence="0.994857076923077">
Given two adjacent lexical heads (say a and b), the prob-
abilities of the possible dependency relations between
them are calculated as Maximum Likelihood (MLE) esti-
mates. In a binary CFG, constituents which are adjacent
at some stage in the parsing process are candidates for the
right-hand side (RHS) of a rewrite rule. If a rule exists for
these constituents (say A and B), then in a DG or in Bare
Phrase Structure, one of these is isomorphic to the LHS,
i.e. the head. DG rules additionally use a syntactic re-
lation label R, for which the probabilities are calculated
in this probability model. The dependency rules used are
based on Treebank tags, the relation probabilities are con-
ditioned on them and on the lexical heads.
</bodyText>
<equation confidence="0.978225">
p(R|A → AB, a, b) = #(R, A → AB, a, b)
#(A → AB, a, b) (1)
</equation>
<bodyText confidence="0.990456666666667">
All that A → AB expresses is that in the dependency
relation the dependency is towards the right, it is therefore
rewritten as right.
</bodyText>
<equation confidence="0.8971625">
p(R|right, a, b) = #(R, right, a, b) (2)
#(right, a, b)
</equation>
<bodyText confidence="0.999824285714286">
Such a probability model is used to model the local
competition between object and adjunct relation (he left
town vs. he left yesterday), in which the verb is always
the left RHS constituent. But in some cases, the direc-
tion is also a parameter, for example in the subject–verb
relation (she said versus said she). There, the probability
space is divided into two equal sections.
</bodyText>
<equation confidence="0.94443">
1 #(R, right, a, b)
</equation>
<bodyText confidence="0.9426256875">
p(R, right|a, b) = 2 ∗ #(right, a, b) +#(left, a, b)
(3)
The PP-attachment model probabilities are condi-
tioned on three lexical heads – the verb, the preposition
and the description noun (Collins and Brooks, 1995). The
probability model is backed off across several levels. In
addition to backing off to only partly lexicalized counts
(ibid.), semantic classes are also used in all the modeled
relations, for verbs the Levin classes (Levin, 1993), for
nouns the top Wordnet class (Fellbaum, 1998) of the most
frequent sense. As an alternative to backing-off, linear in-
terpolation with the back-off models has also been tried,
but the difference in performance is very small.
A large subset of syntactic relations, the ones which are
considered to be most relevant for argument structure, are
modeled, specifically:
</bodyText>
<sectionHeader confidence="0.4235" genericHeader="method">
Relation Label Example
</sectionHeader>
<bodyText confidence="0.955226333333333">
verb–subject subj he sleeps
verb–direct object obj sees it
verb–indirect object obj2 gave (her) kisses
verb–adjunct adj ate yesterday
verb–subord. clause sentobj saw (they) came
verb–prep. phrase pobj slept in bed
noun–prep. phrase modpp draft of paper
noun–participle modpart report written
verb–complementizer compl to eat apples
noun–preposition prep to the house
Until now one relation has two distinct probability
models: verb–subject is different for active and passive
verbs, henceforth referred to as asubj and psubj, where
needed. The disambiguation between complementizer
and preposition is necessary as the Treebank tagset unfor-
tunately uses the same tag (IN) for both. Many relations
have slightly individualized models. As an example the
modpart relation will be discussed in detail.
</bodyText>
<subsectionHeader confidence="0.889222">
3.1.1 An Example: Modification by Participle
</subsectionHeader>
<bodyText confidence="0.999960576923077">
The noun–participle relation is also known as reduced
relative clause. In the Treebank, reduced relative clauses
are adjoined to the NP they modify, and under certain
conditions also have an explicit RRC label. Reduced rel-
ative clauses are frequent enough to warrant a probabilis-
tic treatment, but considerably sparser than verb–non-
passive-subject or verb–object relations. They are in di-
rect competition with the subject–verb relation, because
its candidates are also a NP followed by a VP. We prob-
ably have a subject-verb relation in the report announced
the deal and a noun-participle relation in the report an-
nounced yesterday. The majority of modification by par-
ticiple relations, if the participle is a past participle, func-
tionally correspond to passive constructions (the report
written ∼= the report which has been written). In order to
reduce data sparseness, which could lead to giving pref-
erence to a verb–non-passive-subject reading (asubj),
the verb–passive-subject counts (psubj) are added to the
noun–participle counts. Some past participles also ex-
press adjunct readings (the week ended Friday); there-
fore the converse, i.e. adding noun–participle counts to
verb–passive-subject counts, is not recommended.
The next back-off step maps the noun a to its Wordnet-
class a˚ and the verb b to its Levin-class ˚b. If the counts
are still zero, counts on only the verb and then only the
noun are used.
</bodyText>
<equation confidence="0.999419888888889">
p(modpart|a, b) = (4)
#(modpart,right,a,b)+#(psubj,left,a,b)
#(modpart,right,a,b)+#(psubj,left,a,b)+#(asubj,left,a,b)
if&gt;0,else
#(modpart,right,˚a,˚b)+#(psubj,left,˚a,˚b)
#(modpart,right,˚a,˚b)+#(psubj,left,˚a,˚b)+#(asubj,left,˚a,˚b)
if&gt;0,else
#(modpart,right,b)+#(psubj,left,b)
#(modpart,right,a)+#(psubj,left,a)+#(asubj,left,a)
</equation>
<bodyText confidence="0.9998398">
As the last backoff, a low non-zero probability is as-
signed. In the verb–adjunct relation, which drastically in-
creases complexity but can only occur with a closed class
of nouns (mostly adverbial expressions of time), this last
backoff is not used.
</bodyText>
<subsectionHeader confidence="0.999444">
3.2 Interaction between Several Dependents
</subsectionHeader>
<bodyText confidence="0.999982">
For the verb–prepositional-phrase relation, two models
that take the interaction between the several PPs of the
same verb into account have been implemented. They
are based on the verbal head and the prepositions.
The first one estimates the probability of attaching a PP
introduced by preposition p2, given that the verb to which
it could be attached already has another PP introduced by
the preposition p1. Back-offs using the verb-class v˚ and
</bodyText>
<equation confidence="0.981307545454545">
#(modpart,right,b)+#(psubj,left,b)+#(asubj,left,b)
if&gt;0,else
#(modpart,right,a)+#(psubj,left,a)
then the preposition(s) only are used.
p(p2|v,p1) = #(p2,v,p1) if &gt; 0, else (5)
#(v,p1)
#(p2,˚v,p1)
#(˚v,p1) if &gt; 0, else
#(p2,S v,p1) #(S v,p1) if &gt; 0, else
#(p2,S v)
#(S v)
</equation>
<bodyText confidence="0.998824">
The second model estimates the probability of attach-
ing a PP introduced by preposition p2 as a non-first PP.
The usual backoffs are not printed here.
</bodyText>
<equation confidence="0.9996105">
�p1) = #(p2,v,S p1)
p(p2|v, #(v,S p1) (6)
</equation>
<bodyText confidence="0.9996365">
As prepositions are a closed class, a zero probability is
assigned if the last back-offs fail.
</bodyText>
<subsectionHeader confidence="0.6730795">
3.3 PCFG for Verbal Subcategoriation and VP
Production
</subsectionHeader>
<bodyText confidence="0.999913363636364">
Verbs often have several dependents. Ditransive verbs,
for example, have up to three NP complements, the sub-
ject, the direct object and the indirect object. An inde-
terminate number of adjuncts can be added. Transitivity,
expressed by a verb’s subcategorization, is strongly lex-
icalized. But because the Treebank does not distinguish
arguments and complements, and because a standard lex-
icon does not contain probabilistic subcategorization, a
probabilistic model has advantages. Dependency mod-
els as discussed hitherto fail to model complex depen-
dencies between the dependents of the same mother, un-
like PCFGs. A simple PCFG model for the production of
the VP rule which is lexicalized on the VP head and has
a non-lexicalized backoff, is therefore used. RHS con-
stituents C, for the time being, are unlexicalized phrasal
categories like NP,PP, Comma, etc. At some stage
in the parsing process, given an attachment candidate
Cn and a verbal head v which already has attached con-
stituents C1 to Cn−1, the probability of attaching Cn is
estimated. This probability can also be seen as the prob-
ability of continuing versus ending the VP under produc-
tion.
</bodyText>
<equation confidence="0.9766534">
p(attach|Cn, v, C1..Cn−1) = (7)
#(vp → v, C1, ...Cn) if &gt; 0, else
#(vp → v, C1, ...Cn−1)
#(vp → U v, C1, ...Cn)
#(vp → U v, C1, ...Cn−1)
</equation>
<sectionHeader confidence="0.984274" genericHeader="method">
4 Implementation
</sectionHeader>
<bodyText confidence="0.915787666666667">
The parser has been implemented in Prolog, it runs in
SWI-Prolog and Sicstus Prolog. For SWI-Prolog, a
graphical interface has also been programmed in XPCE1.
</bodyText>
<footnote confidence="0.85327">
1For more information, see
http://www.ifi.unizh.ch/CL/gschneid/parser
</footnote>
<bodyText confidence="0.99977038">
If no analysis spanning the entire length of the sentence
can be found, an optimal path of partial structures span-
ning as much of the sentence as possible is searched. The
algorithm devised for this accepts the first-ranked of the
longest of all the partial analyses found, say S. Then, it
recursively searches for the first-ranked of the longest of
the partial analyses found to the left and to the right of S,
and so on, until all or most of the sentence is spanned.
The parser uses the preprocessed input of a finite-state
tagger-chunker. Finite-state technology is fast enough
for unlimited amounts of data, taggers and chunkers are
known to be reliable but not error-free, with typical er-
ror rates between 2 and 5 %. Tagging and chunking is
done by a standard tagger and chunker, LTPos (Mikheev,
1997). Heads are extracted from the chunks and lem-
matized (Minnen et al., 2000). Parsing takes place only
between the heads of phrases, and only using the best tag
suggested by the tagger, which leads to a reduction in
complexity. The parser uses the CYK algorithm, which
has parsing complexity of O(n3), where n is the number
of words in a word-based, but only chunks in a head-
of-chunk-based model. The chunk to word relation is
1.52 for Treebank section 0. In a test with a toy NP and
verb-group grammar parsing was about 4 times slower
when using unchunked input. Due to the insufficiency
of the toy grammar the lingusitic quality and the number
of complete parses decreased. The average number of
tags per token is 2.11 for the entire Treebank. With un-
tagged input, every possible tag would have to be taken
into consideration. Although untested, at least a similar
slowdown as for unchunked input can be expected.
In a hand-written grammar, some typical parsing er-
rors can be corrected by the grammar engineer, or rules
can explicitly ignore particularly error-prone distinctions.
Examples of rules that can correct tagging errors with-
out introducing many new errors are allowing VBD to
act as a participle or the possible translation of VBG to
an adjective. As an example of ignoring error-prone dis-
tinctions, the disambiguation between prepositions and
verbal particles is unreliable. The grammar therefore
makes no distinction and treats all verbal particles as
prepositions, which leads to an incorrect but consistent
analysis for phrasal verbs. A hand-written grammar al-
lows to model complex but important phenomena which
overstep manageable ML search spaces, such as discon-
tinous analysis of questions can be expressed, while on
the other hand rare and marginal rules can be left out
to free resources. For tagging, (Samuelsson and Vouti-
lainen, 1997) have shown that a manually built tagger can
equal a statistical tagger.
</bodyText>
<sectionHeader confidence="0.994734" genericHeader="conclusions">
5 Preliminary Evaluation
</sectionHeader>
<bodyText confidence="0.9987285">
The probabilistic language models have been trained on
section 2 to 24 and the parser tested on section 0. The
</bodyText>
<table confidence="0.9760365">
Percentage Values for IN
Subject Object PP-attach
Precision 77 72 67 80
Recall 70 75 49 78
</table>
<tableCaption confidence="0.999783">
Table 1: Provisional precision and recall values
</tableCaption>
<bodyText confidence="0.999922763157894">
held out training data and the first-ranked reading for each
sentence of section 0 are compared for evaluation (Lin,
1995). Parsing the 46527 words of section 0 takes 30
minutes on a 800 MHz Pentium 3 PC, including about 3
minutes for tagging and chunking. Current precision and
recall values for subject, object and PP-attachment rela-
tions, and for the disambiguation between prepositions
and complements are in table 1.
These results, slightly lower than state-of-the-art ((Lin,
1998), (Preiss, 2003)), are least merit figures or a proof
of concept rather than accurate figures. On the one hand,
the performance of the parser suffers from mistaggings
and mischunkings or a limited grammar, the price for the
speed increase. On the other hand, different grammatical
assumptions both between the Treebank and the chunker,
and between the Treebank and functional dependency, se-
riously affect the evaluation. For example, the chunker
often recognizes units longer than base-NPs like [many
of the people], or smaller or longer than verbal groups
[has] for a long time [been], [likely to bring] – correct
chunks which are currently considered as errors.
In addition, it is very difficult to avoid tgrep overgen-
erating or missing. It turns out that the mapping is accu-
rate enough for a statistical model but not for a reliable
evaluation. Some possible configurations are missed by
the current extraction queries. For example, extraposed
PPs such as the one starting this sentence, have escaped
unmapped until now. For the future, the use of a stan-
dardized DG test suite is envisaged (Carroll et al., 1999).
The grammar explicitly excludes a number of gram-
matical phenomena which cannot currently be treated re-
liably. For example, since no PP-interaction model such
as PCFG rules for NP-attached PPs exists yet, the current
grammar does not allow a NP to take several PPs, which
affects the analysis of relational nouns. The statistical
models, the dependency extraction, the grammar, the tag-
ger and chunker approach and the evaluation method will
continue to be improved.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999853259259259">
John Carroll, Guido Minnen, and Ted Briscoe. 1999.
Corpus annotation for parser evaluation. In Proceed-
ings of the EACL-99 Post-Conference Workshop on
Linguistically Interpreted Corpora, Bergen, Norway.
Michael Collins and James Brooks. 1995. Prepositional
attachment through a backed-off model. In Proceed-
ings of the Third Workshop on Very Large Corpora,
Cambridge, MA.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.d. dissertation,
University of Pennsylvania, Philadelphia, PA.
Michael A. Covington. 1994. An empirically motivated
reinterpretation of Dependency Grammar. Techni-
cal Report AI1994-01, University of Georgia, Athens,
Georgia.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, MA.
Beth C. Levin. 1993. English Verb Classes and Alter-
nations: a Preliminary Investigation. University of
Chicago Press, Chicago, IL.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In Proceedings of
IJCAI-95, Montreal.
Dekang Lin. 1998. Dependency-based evaluation of
MINIPAR. In Workshop on the Evaluation of Parsing
Systems, Granada, Spain.
Mitch Marcus, Beatrice Santorini, and M.A.
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computational
Linguistics, 19:313–330.
Andrei Mikheev. 1997. Automatic rule induction for
unknown word guessing. Computational Linguistics,
23(3):405–423.
Guido Minnen, John Carroll, and Darren Pearce. 2000.
Applied morphological generation. In Proceedings
of the 1st International Natural Language Generation
Conference (INLG), Mitzpe Ramon, Israel.
Gabriele Musillo and Khalil Sima’an. 2002. Towards
comparing parsers from different linguistic frame-
works. In Proceedings of LREC 2002 Beyond PAR-
SEVAL Workshop, Las Palmas, Spain.
Judita Preiss. 2003. Using grammatical relations to com-
pare parsers. In Proceedings of EACL 03, Budapest,
Hungary.
Christer Samuelsson and Atro Voutilainen. 1997. Com-
paring a linguistic and a stochastic tagger. In Proceed-
ings of ofACL/EACL Joint Conference, Madrid.
Pasi Tapanainen and Timo J¨arvinen. 1997. A non-
projective dependency parser. In Proceedings of the
5th Conference on Applied Natural Language Process-
ing, pages 64–71. Association for Computational Lin-
guistics.
Lucien Tesni`ere. 1959. El´ements de Syntaxe Structurale.
Librairie Klincksieck, Paris.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.285417">
<note confidence="0.905347666666667">Proceedings of HLT-NAACL 2003 Student Research Workshop , pp. 31-36 Edmonton, May-June 2003</note>
<title confidence="0.7414455">A low-complexity, broad-coverage probabilistic Dependency Parser for English</title>
<author confidence="0.714053">Gerold</author>
<affiliation confidence="0.9924495">Institute of Computational Linguistics, University of Department of Linguistics, University of</affiliation>
<email confidence="0.850742">gerold.schneider@lettres.unige.ch</email>
<abstract confidence="0.994646916666666">Large-scale parsing is still a complex and timeconsuming process, often so much that it is infeasible in real-world applications. The parsing system described here addresses this problem by combining finite-state approaches, statistical parsing techniques and engineering knowledge, thus keeping parsing complexity as low as possible at the cost of a slight decrease in performance. The parser is robust and fast and at the same time based on strong linguistic foundations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Guido Minnen</author>
<author>Ted Briscoe</author>
</authors>
<title>Corpus annotation for parser evaluation.</title>
<date>1999</date>
<booktitle>In Proceedings of the EACL-99 Post-Conference Workshop on Linguistically Interpreted Corpora,</booktitle>
<location>Bergen, Norway.</location>
<marker>Carroll, Minnen, Briscoe, 1999</marker>
<rawString>John Carroll, Guido Minnen, and Ted Briscoe. 1999. Corpus annotation for parser evaluation. In Proceedings of the EACL-99 Post-Conference Workshop on Linguistically Interpreted Corpora, Bergen, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>James Brooks</author>
</authors>
<title>Prepositional attachment through a backed-off model.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third Workshop on Very Large Corpora,</booktitle>
<location>Cambridge, MA.</location>
<contexts>
<context position="1970" citStr="Collins and Brooks, 1995" startWordPosition="280" endWordPosition="283">ers on the one hand from successful Dependency Grammar implementations (e.g. (Lin, 1998), (Tapanainen and J¨arvinen, 1997)) by using a statistical base, and on the other hand from state-of-the-art statistical approaches (e.g. (Collins, 1999)) by carefully following an established formal grammar theory, Dependency Grammar (DG). It combines two probabilistic models of language, similar to (Collins, 1999), which are discussed in section 3. Both are supervised and based on Maximum Likelihood Estimation (MLE). The first one is based on the lexical probabilities of the heads of phrases, similar to (Collins and Brooks, 1995). It calculates the probability of finding specific syntactic relations (such as subject, sentential object, etc.) between given lexical heads. Two simple extensions for the interaction between several dependents of the same mother node are also used. The second probability model is a PCFG for the production of the VP. Although traditional CFGs are not part of DG, VP PCFG rules can model verb subcategorization frames, an important DG component. The parser has been trained, developed and tested on a large collection of syntactically analyzed sentences, the Penn Treebank (Marcus et al., 1993). I</context>
<context position="13321" citStr="Collins and Brooks, 1995" startWordPosition="2133" endWordPosition="2136">ight, a, b) Such a probability model is used to model the local competition between object and adjunct relation (he left town vs. he left yesterday), in which the verb is always the left RHS constituent. But in some cases, the direction is also a parameter, for example in the subject–verb relation (she said versus said she). There, the probability space is divided into two equal sections. 1 #(R, right, a, b) p(R, right|a, b) = 2 ∗ #(right, a, b) +#(left, a, b) (3) The PP-attachment model probabilities are conditioned on three lexical heads – the verb, the preposition and the description noun (Collins and Brooks, 1995). The probability model is backed off across several levels. In addition to backing off to only partly lexicalized counts (ibid.), semantic classes are also used in all the modeled relations, for verbs the Levin classes (Levin, 1993), for nouns the top Wordnet class (Fellbaum, 1998) of the most frequent sense. As an alternative to backing-off, linear interpolation with the back-off models has also been tried, but the difference in performance is very small. A large subset of syntactic relations, the ones which are considered to be most relevant for argument structure, are modeled, specifically</context>
</contexts>
<marker>Collins, Brooks, 1995</marker>
<rawString>Michael Collins and James Brooks. 1995. Prepositional attachment through a backed-off model. In Proceedings of the Third Workshop on Very Large Corpora, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<note>Ph.d. dissertation,</note>
<contexts>
<context position="1586" citStr="Collins, 1999" startWordPosition="223" endWordPosition="224"> the impressive recent advances in language technology. For this, they need fast, robust parsers that deliver linguistic data which is meaningful for the subsequent processing stages. This paper presents such a parsing system. Its output is a hierarchical structure of syntactic relations, functional dependency structures, which are discussed in section 2. The parser differs on the one hand from successful Dependency Grammar implementations (e.g. (Lin, 1998), (Tapanainen and J¨arvinen, 1997)) by using a statistical base, and on the other hand from state-of-the-art statistical approaches (e.g. (Collins, 1999)) by carefully following an established formal grammar theory, Dependency Grammar (DG). It combines two probabilistic models of language, similar to (Collins, 1999), which are discussed in section 3. Both are supervised and based on Maximum Likelihood Estimation (MLE). The first one is based on the lexical probabilities of the heads of phrases, similar to (Collins and Brooks, 1995). It calculates the probability of finding specific syntactic relations (such as subject, sentential object, etc.) between given lexical heads. Two simple extensions for the interaction between several dependents of </context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.d. dissertation, University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A Covington</author>
</authors>
<title>An empirically motivated reinterpretation of Dependency Grammar.</title>
<date>1994</date>
<tech>Technical Report AI1994-01,</tech>
<institution>University of Georgia,</institution>
<location>Athens,</location>
<contexts>
<context position="5187" citStr="Covington, 1994" startWordPosition="815" endWordPosition="816">lation to Constituency In its simplest definition, a projective DG is a binary version (except for valency, see 2.2) of a constituent grammar which only knows lexical items, which entails that • for every mother node, the mother node and exactly one of its daughters, the so-called head, are isomorphic • projection is deterministic, endocentric and can thus not fail, which gives DG a robustness advantage • equivalent constituency CFG trees can be derived • it is in Chomsky Normal Form (CNF), the efficient CYK parsing algorithm can thus be used Any DG has an equivalent constituency counterpart (Covington, 1994). Figure 1 shows a dependency structure and its unlabeled constituency counterpart. 2.2 Valency as an isomorphism constraint Total equivalence between mother and head daughter could not prevent a verb from taking an infinite number of subjects or objects. Therefore, valency theory is as vital a part of DG as is its constituency counterpart, subcategorization. The manually written rules check the most obvious valency constraints. Verbal valency is modeled by a PCFG for VP production. Obj Sentobj Subj V What did you think Mary said Figure 2: Non-projective analysis of a WH-question 2.3 Functiona</context>
</contexts>
<marker>Covington, 1994</marker>
<rawString>Michael A. Covington. 1994. An empirically motivated reinterpretation of Dependency Grammar. Technical Report AI1994-01, University of Georgia, Athens, Georgia.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth C Levin</author>
</authors>
<title>English Verb Classes and Alternations: a Preliminary Investigation.</title>
<date>1993</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago, IL.</location>
<contexts>
<context position="13554" citStr="Levin, 1993" startWordPosition="2172" endWordPosition="2173"> parameter, for example in the subject–verb relation (she said versus said she). There, the probability space is divided into two equal sections. 1 #(R, right, a, b) p(R, right|a, b) = 2 ∗ #(right, a, b) +#(left, a, b) (3) The PP-attachment model probabilities are conditioned on three lexical heads – the verb, the preposition and the description noun (Collins and Brooks, 1995). The probability model is backed off across several levels. In addition to backing off to only partly lexicalized counts (ibid.), semantic classes are also used in all the modeled relations, for verbs the Levin classes (Levin, 1993), for nouns the top Wordnet class (Fellbaum, 1998) of the most frequent sense. As an alternative to backing-off, linear interpolation with the back-off models has also been tried, but the difference in performance is very small. A large subset of syntactic relations, the ones which are considered to be most relevant for argument structure, are modeled, specifically: Relation Label Example verb–subject subj he sleeps verb–direct object obj sees it verb–indirect object obj2 gave (her) kisses verb–adjunct adj ate yesterday verb–subord. clause sentobj saw (they) came verb–prep. phrase pobj slept i</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth C. Levin. 1993. English Verb Classes and Alternations: a Preliminary Investigation. University of Chicago Press, Chicago, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>A dependency-based method for evaluating broad-coverage parsers.</title>
<date>1995</date>
<booktitle>In Proceedings of IJCAI-95,</booktitle>
<location>Montreal.</location>
<contexts>
<context position="22547" citStr="Lin, 1995" startWordPosition="3574" endWordPosition="3575">pressed, while on the other hand rare and marginal rules can be left out to free resources. For tagging, (Samuelsson and Voutilainen, 1997) have shown that a manually built tagger can equal a statistical tagger. 5 Preliminary Evaluation The probabilistic language models have been trained on section 2 to 24 and the parser tested on section 0. The Percentage Values for IN Subject Object PP-attach Precision 77 72 67 80 Recall 70 75 49 78 Table 1: Provisional precision and recall values held out training data and the first-ranked reading for each sentence of section 0 are compared for evaluation (Lin, 1995). Parsing the 46527 words of section 0 takes 30 minutes on a 800 MHz Pentium 3 PC, including about 3 minutes for tagging and chunking. Current precision and recall values for subject, object and PP-attachment relations, and for the disambiguation between prepositions and complements are in table 1. These results, slightly lower than state-of-the-art ((Lin, 1998), (Preiss, 2003)), are least merit figures or a proof of concept rather than accurate figures. On the one hand, the performance of the parser suffers from mistaggings and mischunkings or a limited grammar, the price for the speed increa</context>
</contexts>
<marker>Lin, 1995</marker>
<rawString>Dekang Lin. 1995. A dependency-based method for evaluating broad-coverage parsers. In Proceedings of IJCAI-95, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Dependency-based evaluation of MINIPAR.</title>
<date>1998</date>
<booktitle>In Workshop on the Evaluation of Parsing Systems,</booktitle>
<location>Granada,</location>
<contexts>
<context position="1433" citStr="Lin, 1998" startWordPosition="201" endWordPosition="202">ion Many extensions to text-based, data-intensive knowledge management approaches, such as Information Retrieval or Data Mining, focus on integrating the impressive recent advances in language technology. For this, they need fast, robust parsers that deliver linguistic data which is meaningful for the subsequent processing stages. This paper presents such a parsing system. Its output is a hierarchical structure of syntactic relations, functional dependency structures, which are discussed in section 2. The parser differs on the one hand from successful Dependency Grammar implementations (e.g. (Lin, 1998), (Tapanainen and J¨arvinen, 1997)) by using a statistical base, and on the other hand from state-of-the-art statistical approaches (e.g. (Collins, 1999)) by carefully following an established formal grammar theory, Dependency Grammar (DG). It combines two probabilistic models of language, similar to (Collins, 1999), which are discussed in section 3. Both are supervised and based on Maximum Likelihood Estimation (MLE). The first one is based on the lexical probabilities of the heads of phrases, similar to (Collins and Brooks, 1995). It calculates the probability of finding specific syntactic r</context>
<context position="22911" citStr="Lin, 1998" startWordPosition="3631" endWordPosition="3632">s for IN Subject Object PP-attach Precision 77 72 67 80 Recall 70 75 49 78 Table 1: Provisional precision and recall values held out training data and the first-ranked reading for each sentence of section 0 are compared for evaluation (Lin, 1995). Parsing the 46527 words of section 0 takes 30 minutes on a 800 MHz Pentium 3 PC, including about 3 minutes for tagging and chunking. Current precision and recall values for subject, object and PP-attachment relations, and for the disambiguation between prepositions and complements are in table 1. These results, slightly lower than state-of-the-art ((Lin, 1998), (Preiss, 2003)), are least merit figures or a proof of concept rather than accurate figures. On the one hand, the performance of the parser suffers from mistaggings and mischunkings or a limited grammar, the price for the speed increase. On the other hand, different grammatical assumptions both between the Treebank and the chunker, and between the Treebank and functional dependency, seriously affect the evaluation. For example, the chunker often recognizes units longer than base-NPs like [many of the people], or smaller or longer than verbal groups [has] for a long time [been], [likely to br</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Dependency-based evaluation of MINIPAR. In Workshop on the Evaluation of Parsing Systems, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitch Marcus</author>
<author>Beatrice Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="2567" citStr="Marcus et al., 1993" startWordPosition="374" endWordPosition="377">llins and Brooks, 1995). It calculates the probability of finding specific syntactic relations (such as subject, sentential object, etc.) between given lexical heads. Two simple extensions for the interaction between several dependents of the same mother node are also used. The second probability model is a PCFG for the production of the VP. Although traditional CFGs are not part of DG, VP PCFG rules can model verb subcategorization frames, an important DG component. The parser has been trained, developed and tested on a large collection of syntactically analyzed sentences, the Penn Treebank (Marcus et al., 1993). It is broad-coverage and robust and returns an optimal set of partial structures when it fails to find a complete structure for a sentence. It has been designed to keep complexity as low as possible during the parsing process in order to be fast enough to be useful for parsing large amounts of unrestricted text. This has been achieved by observing the following constraints, discussed in section 4: • using a syntactic theory known for its relatively flat structures and lack of empty nodes (see also subsection 2.4) • relying on finite-state preprocessing • discarding unlikely readings with a b</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitch Marcus, Beatrice Santorini, and M.A. Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrei Mikheev</author>
</authors>
<title>Automatic rule induction for unknown word guessing.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="20220" citStr="Mikheev, 1997" startWordPosition="3190" endWordPosition="3191">r this accepts the first-ranked of the longest of all the partial analyses found, say S. Then, it recursively searches for the first-ranked of the longest of the partial analyses found to the left and to the right of S, and so on, until all or most of the sentence is spanned. The parser uses the preprocessed input of a finite-state tagger-chunker. Finite-state technology is fast enough for unlimited amounts of data, taggers and chunkers are known to be reliable but not error-free, with typical error rates between 2 and 5 %. Tagging and chunking is done by a standard tagger and chunker, LTPos (Mikheev, 1997). Heads are extracted from the chunks and lemmatized (Minnen et al., 2000). Parsing takes place only between the heads of phrases, and only using the best tag suggested by the tagger, which leads to a reduction in complexity. The parser uses the CYK algorithm, which has parsing complexity of O(n3), where n is the number of words in a word-based, but only chunks in a headof-chunk-based model. The chunk to word relation is 1.52 for Treebank section 0. In a test with a toy NP and verb-group grammar parsing was about 4 times slower when using unchunked input. Due to the insufficiency of the toy gr</context>
</contexts>
<marker>Mikheev, 1997</marker>
<rawString>Andrei Mikheev. 1997. Automatic rule induction for unknown word guessing. Computational Linguistics, 23(3):405–423.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guido Minnen</author>
<author>John Carroll</author>
<author>Darren Pearce</author>
</authors>
<title>Applied morphological generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st International Natural Language Generation Conference (INLG),</booktitle>
<location>Mitzpe Ramon,</location>
<contexts>
<context position="20294" citStr="Minnen et al., 2000" startWordPosition="3201" endWordPosition="3204">alyses found, say S. Then, it recursively searches for the first-ranked of the longest of the partial analyses found to the left and to the right of S, and so on, until all or most of the sentence is spanned. The parser uses the preprocessed input of a finite-state tagger-chunker. Finite-state technology is fast enough for unlimited amounts of data, taggers and chunkers are known to be reliable but not error-free, with typical error rates between 2 and 5 %. Tagging and chunking is done by a standard tagger and chunker, LTPos (Mikheev, 1997). Heads are extracted from the chunks and lemmatized (Minnen et al., 2000). Parsing takes place only between the heads of phrases, and only using the best tag suggested by the tagger, which leads to a reduction in complexity. The parser uses the CYK algorithm, which has parsing complexity of O(n3), where n is the number of words in a word-based, but only chunks in a headof-chunk-based model. The chunk to word relation is 1.52 for Treebank section 0. In a test with a toy NP and verb-group grammar parsing was about 4 times slower when using unchunked input. Due to the insufficiency of the toy grammar the lingusitic quality and the number of complete parses decreased. </context>
</contexts>
<marker>Minnen, Carroll, Pearce, 2000</marker>
<rawString>Guido Minnen, John Carroll, and Darren Pearce. 2000. Applied morphological generation. In Proceedings of the 1st International Natural Language Generation Conference (INLG), Mitzpe Ramon, Israel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriele Musillo</author>
<author>Khalil Sima’an</author>
</authors>
<title>Towards comparing parsers from different linguistic frameworks.</title>
<date>2002</date>
<booktitle>In Proceedings of LREC 2002 Beyond PARSEVAL Workshop,</booktitle>
<location>Las Palmas,</location>
<marker>Musillo, Sima’an, 2002</marker>
<rawString>Gabriele Musillo and Khalil Sima’an. 2002. Towards comparing parsers from different linguistic frameworks. In Proceedings of LREC 2002 Beyond PARSEVAL Workshop, Las Palmas, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judita Preiss</author>
</authors>
<title>Using grammatical relations to compare parsers.</title>
<date>2003</date>
<booktitle>In Proceedings of EACL 03,</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="22927" citStr="Preiss, 2003" startWordPosition="3633" endWordPosition="3634">ect Object PP-attach Precision 77 72 67 80 Recall 70 75 49 78 Table 1: Provisional precision and recall values held out training data and the first-ranked reading for each sentence of section 0 are compared for evaluation (Lin, 1995). Parsing the 46527 words of section 0 takes 30 minutes on a 800 MHz Pentium 3 PC, including about 3 minutes for tagging and chunking. Current precision and recall values for subject, object and PP-attachment relations, and for the disambiguation between prepositions and complements are in table 1. These results, slightly lower than state-of-the-art ((Lin, 1998), (Preiss, 2003)), are least merit figures or a proof of concept rather than accurate figures. On the one hand, the performance of the parser suffers from mistaggings and mischunkings or a limited grammar, the price for the speed increase. On the other hand, different grammatical assumptions both between the Treebank and the chunker, and between the Treebank and functional dependency, seriously affect the evaluation. For example, the chunker often recognizes units longer than base-NPs like [many of the people], or smaller or longer than verbal groups [has] for a long time [been], [likely to bring] – correct c</context>
</contexts>
<marker>Preiss, 2003</marker>
<rawString>Judita Preiss. 2003. Using grammatical relations to compare parsers. In Proceedings of EACL 03, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christer Samuelsson</author>
<author>Atro Voutilainen</author>
</authors>
<title>Comparing a linguistic and a stochastic tagger.</title>
<date>1997</date>
<booktitle>In Proceedings of ofACL/EACL Joint Conference,</booktitle>
<location>Madrid.</location>
<contexts>
<context position="22076" citStr="Samuelsson and Voutilainen, 1997" startWordPosition="3492" endWordPosition="3496">translation of VBG to an adjective. As an example of ignoring error-prone distinctions, the disambiguation between prepositions and verbal particles is unreliable. The grammar therefore makes no distinction and treats all verbal particles as prepositions, which leads to an incorrect but consistent analysis for phrasal verbs. A hand-written grammar allows to model complex but important phenomena which overstep manageable ML search spaces, such as discontinous analysis of questions can be expressed, while on the other hand rare and marginal rules can be left out to free resources. For tagging, (Samuelsson and Voutilainen, 1997) have shown that a manually built tagger can equal a statistical tagger. 5 Preliminary Evaluation The probabilistic language models have been trained on section 2 to 24 and the parser tested on section 0. The Percentage Values for IN Subject Object PP-attach Precision 77 72 67 80 Recall 70 75 49 78 Table 1: Provisional precision and recall values held out training data and the first-ranked reading for each sentence of section 0 are compared for evaluation (Lin, 1995). Parsing the 46527 words of section 0 takes 30 minutes on a 800 MHz Pentium 3 PC, including about 3 minutes for tagging and chun</context>
</contexts>
<marker>Samuelsson, Voutilainen, 1997</marker>
<rawString>Christer Samuelsson and Atro Voutilainen. 1997. Comparing a linguistic and a stochastic tagger. In Proceedings of ofACL/EACL Joint Conference, Madrid.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pasi Tapanainen</author>
<author>Timo J¨arvinen</author>
</authors>
<title>A nonprojective dependency parser.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th Conference on Applied Natural Language Processing,</booktitle>
<pages>64--71</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Tapanainen, J¨arvinen, 1997</marker>
<rawString>Pasi Tapanainen and Timo J¨arvinen. 1997. A nonprojective dependency parser. In Proceedings of the 5th Conference on Applied Natural Language Processing, pages 64–71. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucien Tesni`ere</author>
</authors>
<title>El´ements de Syntaxe Structurale. Librairie Klincksieck,</title>
<date>1959</date>
<location>Paris.</location>
<marker>Tesni`ere, 1959</marker>
<rawString>Lucien Tesni`ere. 1959. El´ements de Syntaxe Structurale. Librairie Klincksieck, Paris.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>