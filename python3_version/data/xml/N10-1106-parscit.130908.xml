<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001706">
<title confidence="0.959512">
Arabic Mention Detection: Toward Better Unit of Analysis
</title>
<author confidence="0.934541">
Yassine Benajiba Imed Zitouni
</author>
<affiliation confidence="0.9123575">
Center for Computational Learning Systems IBM T. J. Watson Research Center
Columbia University izitouni@us.ibm.com
</affiliation>
<email confidence="0.996555">
ybenajiba@ccls.columbia.edu
</email>
<sectionHeader confidence="0.998589" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999716692307692">
We investigate in this paper the adequate unit
of analysis for Arabic Mention Detection. We
experiment different segmentation schemes
with various feature-sets. Results show that
when limited resources are available, models
built on morphologically segmented data out-
perform other models by up to 4F points. On
the other hand, when more resources extracted
from morphologically segmented data become
available, models built with Arabic TreeBank
style segmentation yield to better results. We
also show additional improvement by combin-
ing different segmentation schemes.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999825736842105">
This paper addresses an important and basic task of
information extraction: Mention Detection (MD)1:
the identification and classification of textual refer-
ences to objects/abstractions (i.e., mentions). These
mentions can be either named (e.g. Mohammed,
John), nominal (city, president) or pronominal (e.g.
he, she). For instance, in the sentence “President
Obama said he will visit ...” there are three men-
tions: President, Obama and he. This is similar
to the Named Entity Recognition (NER) task with
the additional twist of also identifying nominal and
pronominal mentions. We formulate the mention de-
tection problem as a classification problem, by as-
signing to each token in the text a label, indicating
whether it starts a specific mention, is inside a spe-
cific mention, or is outside all mentions. The se-
lection of the unit of analysis is an important step
toward a better classification. When processing lan-
guages, such as English, using the word itself as the
</bodyText>
<footnote confidence="0.9383135">
1We adopt here the ACE nomenclature:
http://www.nist.gov/speech/tests/ace/index.html
</footnote>
<bodyText confidence="0.9927069375">
unit of analysis (after separating punctuations) leads
to a good performance (Florian et al., 2004). For
other languages, such as Chinese, character is con-
sidered as the adequate unit of analysis (Jing et al.,
2003). In this paper, we investigate different seg-
mentation schemes in order to define the best unit of
analysis for Arabic MD. Arabic adopts a very com-
plex morphology, i.e. each word is composed of zero
or more prefixes, one stem and zero or more suffixes.
Consequently, the Arabic data is sparser than other
languages, such as English, and it is necessary to
“segment” the words into several units of analysis in
order to achieve a good performance.
(Zitouni et al., 2005) used Arabic morphologically
segmented data and claimed to have very competi-
tive results in ACE 2003 and ACE 2004 data. On the
other hand, (Benajiba et al., 2008) report good re-
sults for Arabic NER on ACE 2003, 2004 and 2005
data using Arabic TreeBank (ATB) segmentation. In
all published works, authors do not mention a spe-
cific motivation for the segmentation scheme they
have adopted. Only for the Machine Translation
task, (Habash and Sadat, 2006) report several results
using different Arabic segmentation schemes. They
report that the best results were obtained when the
ATB-like segmentation was used. We explore here
the four known and linguistically-motivated sorts of
segmentation: punctuation separation, ATB, mor-
phological and character-level segmentations. To
our knowledge, this is the first paper which inves-
tigates different segmentation schemes to define the
unit of analysis which best fits Arabic MD.
</bodyText>
<sectionHeader confidence="0.885315" genericHeader="method">
2 Arabic Segmentation Schemes
</sectionHeader>
<footnote confidence="0.524134333333333">
Character-level Segmentation: considers that each
character is a separate token.
Morphological Segmentation: aims at segmenting
</footnote>
<page confidence="0.939575">
709
</page>
<subsubsectionHeader confidence="0.565035">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 709–712,
</subsubsectionHeader>
<subsectionHeader confidence="0.271473">
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.959586307692308">
all affixes of a word. The morphological segmenta-
tion for the wordÖÏ @ð (wAlmktb — and the of-
fice)2 could be: “I.� È@+ ð” (w +Al +mktb).
Arabic TreeBank (ATB) segmentation: This seg-
mentation considers splitting the word into affixes
only if it projects an independent phrasal constituent
in the parse tree. As an example, in the word shown
above I. �JºÖÏ@ð, the phrasal independent constituents
are: the conjunction ð (w — and) and the noun
I.JºÖÏ@ (Almktb — the office). The morphological
�
segmentation of this word would lead to the follow-
ing parse tree:
</bodyText>
<equation confidence="0.995540833333333">
S
� � ��
CONJ NP
� ��
�
w Al +mktb
</equation>
<bodyText confidence="0.999614333333333">
Since the È@ (Al, the definite article) is not an in-
dependent constituent, it is not considered for ATB
segmentation. Hence, for I. �JºÖÏ@ð, the ATB segmen-
tation would be I. �JºÖÏ@+ ð (w +Almktb).
Punctuation separation : it consists of separating
the punctuation marks from the word.
Both ATB and morphological segmentation sys-
tems are based on weighted finite state transducers
(WFST). The decoder implements a general Bell-
man dynamic programming search for the best path
on a lattice of segmentation hypotheses that match
the input characters (Benajiba and Zitouni, 2009).
ATB and morphological segmentation systems have
a performance of 99.4 and 98.1 F-measure respec-
tively on ATB data.
The unit of analysis when doing classification de-
pends on the used segmentation. When using the
punctuation separation or character-based segmen-
tations, the unit of analysis is the word itself (with-
out the punctuation marks attached) or the character,
respectively. The ATB and morphological segmen-
tations are language specific and are based on dif-
ferent linguistic viewpoint. When using one of these
two segmentation schemes, the unit of analysis is the
morph (i.e. prefix, stem or suffix). Our goal in this
paper is to find the unit of analysis that fits best Ara-
bic MD.
</bodyText>
<footnote confidence="0.992768333333333">
2Throughout the paper, for each Arabic example we show
between parenthesis its transliteration and English translation
separated by “—”.
</footnote>
<sectionHeader confidence="0.861911" genericHeader="method">
3 Mention Detection System
</sectionHeader>
<bodyText confidence="0.999135935483871">
As explained earlier, we consider the MD task as a
sequence classification problem where the class we
predict for each unit of analysis (i.e., token) is the
type of the entity which it refers to. We chose the
maximum entropy (MaxEnt) classifier that can in-
tegrate arbitrary types of information and make a
classification decision by aggregating all informa-
tion available for a given classification. For more
details about the system architecture, reader may re-
fer to (Zitouni et al., 2009). The features used in our
MD system can be divided into four categories:
Lexical Features: n-grams spanning the current to-
ken; both preceding and following it. A number of
n equal to 3 turned out to be a good choice.
Stem n-gram Features: stem trigram spanning the
current stem; both preceding and following it (Zi-
touni et al., 2005).
Syntactic Features: POS tags and shallow parsing
information in a f2 window.
Features From Other Classifiers: outputs of MD
and NER taggers trained on other data-sets different
from the one we used here. They may identify types
of mentions different from the mentions of interest
in our task. For instance, such a tagger may identify
dates or occupation references (not used in our task),
among other types. Our hypothesis is that combin-
ing classifiers from diverse sources will boost per-
formance by injecting complementary information
into the mention detection models. We also use the
two previously assigned classification tags as addi-
tional feature.
</bodyText>
<sectionHeader confidence="0.999167" genericHeader="method">
4 Data
</sectionHeader>
<bodyText confidence="0.841858055555556">
Experiments are conducted on the Arabic ACE 2007
data. Since the evaluation tests set are not publicly
available, we have split the publicly available train-
ing corpus into an 85%/15% data split. We use 323
documents (80, 000 words, 17,634 mentions) for
training and 56 documents (18, 000 words, 3, 566
mentions) as a test set. We are interested in 7 types
of mentions: facility, Geo-Political Entity (GPE),
location, organization, person, vehicle and weapon.
We segmented the training and test set with four dif-
ferent styles building the following corpora:
Words: a corpus which is the result of running
punctuation separation;
ATBs: a corpus obtained by running punctuation
separation and ATB segmentation;
Mophs: a corpus where we conduct punctuation
separation and morphological segmentation;
Chars: a corpus where the original text is separated
</bodyText>
<page confidence="0.980468">
710
</page>
<bodyText confidence="0.994768">
into a sequence of characters.
When building MD systems on Words, ATBs,
Morphs and Chars, the unit of analysis is the word,
the ATB token, the morph and the character, respec-
tively.
</bodyText>
<sectionHeader confidence="0.999563" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9932274">
We show in this section the experimental results
when using Arabic MD system with different seg-
mentation schemes and different feature sets. We
explore in this paper four categories of features (c.f.
Section 3):
</bodyText>
<table confidence="0.565617666666667">
Lexf: lexical features;
Stemf: Lexf + morphological features;
Syntf: Stemf + syntactic features;
</table>
<bodyText confidence="0.9724185625">
Semf: Syntf + output of other MD classifiers.
Lexf and Stemf features are directly extracted
from the appropriate corpus based on the used seg-
mentation style. This is different for Semf: we first
run classifiers on the morphologically segmented
data. Thereafter, we project those labels to other
corpora. This is because, we use classifiers initially
trained on morphologically segmented data such as
ACE 2003, 2004 and 2005 data. In such data, two
morphs belonging to the same word or ATB token
may have 2 different mentions. During transfer, a
token will have the label of the corresponding stem
in the morphologically segmented data. One moti-
vation to not re-train classifiers on each corpus sep-
arately is to be able to extract Semf features from
classifiers with similar performance.
</bodyText>
<tableCaption confidence="0.9603625">
Table 1: Results in terms of F-measure per feature-set and
segmentation scheme
</tableCaption>
<table confidence="0.9995594">
Lexf Stemf Syntf Semf
Word, 66.4 66.6 69.0 77.1
ATB, 70.1 69.8 72.1 79.0
Morph, 74.1 74.5 75.5 78.3
Char, 22.3 22.4 22.5 22.6
</table>
<bodyText confidence="0.994597682539683">
Results in Table 1 show that classifiers built on
ATBs and Morphs have shown to perform better
than classifiers trained on data with other segmenta-
tion styles. When the system uses character as the
unit of analysis, performance is poor. This is be-
cause the token itself becomes insignificant informa-
tion to the classifier. On the other hand, when only
punctuation separation is performed (Words), the
data is significantly sparse and the obtained results
achieves high F-measure (77.1) only when outputs
of other classifiers are used. As mentioned earlier,
classifiers used to extract those features are trained
on Morphs (less sparse), which explains their re-
markable positive impact since they resolve part of
the data sparseness problem in Words. When us-
ing full morphological segmentation, the data is less
sparse, which leads to less Out-Of-Vocabulary to-
kens (OOVs): the number of OOVs in the Morphs
As an example, the word �é�JJ�ëQË@ (Alrhynp — the
data is 1,518 whereas it is 2,464 in the ATBs.
hostage), which is person mention in the training
data. This word is kept unchanged after ATB seg-
mentation and is segmented to ” o+ WiëP + È @” (Al+
rhyn +p) in Morphs. In the development set the
same word appears in its dual form without defi-
nite article, i.e. gy���J��J�ëP. This word is unchanged in
ATBs and is segmented to ” �K�+ &amp;quot;+ WiëP” (rhyn
+p +yn) in Morphs. For the model built on ATBs,
this word is an OOV, whereas for the model built
on Morphs the stem has been seen as part of a per-
son mention and consequently has a better chance
to tag it correctly. These phenomena are frequent,
which make the classifier trained on Morphs more
robust for such cases. Also, we observed that mod-
els trained on ATBs perform better on long span
mentions. We think this is because a model trained
on ATBs has access to larger context. One may
argue that a similar behavior of the model built on
the Morphs might be obtained if we use a wider
context window than the one used for ATBs in or-
der to have similar contextual information. In or-
der to confirm this statement, we have carried out a
set of experiments using all features over Morphs
data for a context window up to −5/ + 5, the ob-
tained results show no improvement. Similar behav-
ior is observed when looking to results on identi-
fied named (Nam.), nominal (Nom.) and pronomi-
nal (Pro.) mentions on ATBs and Morphs (c.f. Ta-
ble 2); we remind the reader that NER is about rec-
ognizing named mentions. When limited resources
are available (e.g. Lexf, Stemf or Syntf), we be-
lieve that it is more effective to morphologically seg-
ment the text (Morphs) as a pre-processing step.
The use of morph as a unit of analysis reduces the
data sparseness issue and at the same time allows
better context handling when compared to character.
On the other hand, when a larger set of resources
are available (e.g., Semf), the use of the ATB to-
ken as a unit of analysis combined with morph-
based features leads to better performance (79.0 vs.
78.3 on Morphs). This is because (1) classifiers
trained on ATBs handle better the context and (2)
the use of morph-based features (output of classi-
</bodyText>
<page confidence="0.994139">
711
</page>
<bodyText confidence="0.999724">
fiers trained on morphologically segmented data) re-
moves some of the data sparseness from which clas-
sifiers trained on ATB, suffer. The obtained im-
provement in performance is statistically significant
when using the stratified bootstrap re-sampling sig-
nificance test (Noreen, 1989). We consider results
as statistically significant when p &lt; 0.02, which is
the case in this paper. For an accurate MD system,
we think it is appropriate to benefit from ATB, to-
kens and Morph,. We investigate in the following
the combination of these two segmentation styles.
</bodyText>
<tableCaption confidence="0.866182">
Table 2: Performance in terms of F-measure per level on
ATBs and Morphs
</tableCaption>
<table confidence="0.999453714285714">
Seg. Lexf Stemf Syntf Semf
Nam. ATBs 68.2 69.0 72.8 79.1
Morphs 73.4 73.8 75.3 78.7
Nom. ATBs 65.6 64.6 66.9 75.8
Morphs 71.7 72.2 72.9 75.4
Pro. ATBs 60.7 60.1 59.9 66.3
Morphs 63.0 67.2 65.7 65.1
</table>
<subsectionHeader confidence="0.996247">
5.1 Combination of ATB and Morph
</subsectionHeader>
<bodyText confidence="0.999613714285714">
We trained a model on ATB, that uses output of the
model trained on Morph, as additional information
(M2Af feature). We proceed similarly by training a
model on Morph, using output of the model trained
on ATB, (A2Mf feature). We have obtained the
features by a 15-way round-robin. Table 3 shows
the obtained results.
</bodyText>
<tableCaption confidence="0.976072">
Table 3: Results in terms of F-measure of the combina-
tion experiments
</tableCaption>
<table confidence="0.9991896">
Lexf Stemf Syntf Semf
ATBs 70.1 69.8 72.1 79.0
ATBs+M2Af 70.7 70.8 73.1 79.1
Morphs 74.1 74.5 75.5 78.3
Morphs+A2Mf 74.9 75.2 75.4 78.6
</table>
<bodyText confidence="0.986599266666667">
Results show a significant improvement for mod-
els that are trained on ATB, using information from
Morph, in addition to Lexf, Stemf and Syntf
features. This again confirms our claim that the use
of features from morphologically segmented text re-
duces the data sparseness and consequently leads to
better performance. For Semf features, only a 0.1
F-measure points have been gained. This is because
we are already using output of classifiers trained
on morphologically segmented data, which resolve
some of the data sparseness issue. The Morph,
side shows that the obtained performance when the
ATB, output is employed together with the Stemf
(75.2) is only 0.3 points below the performance of
the system using Syntf (75.5).
</bodyText>
<sectionHeader confidence="0.999449" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999981833333333">
We have shown a comparative study aiming at defin-
ing the adequate unit of analysis for Arabic MD.
We conducted our study using four segmentation
schemes with four different feature-sets. Results
show that when only limited resources are available,
using morphological segmentation leads to the best
results. On the other hand, model trained on ATB
segmented data become more powerful and effective
when data sparseness is reduced by the use of other
classifier outputs trained on morphologically seg-
mented data. More improvement is obtained when
both segmentation styles are combined.
</bodyText>
<sectionHeader confidence="0.999661" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999611424242424">
Y. Benajiba and I. Zitouni. 2009. Morphology-
based segmentation combination for arabic men-
tion detection. Special Issue on Arabic Nat-
ural Language Processing of ACM Transac-
tions on Asian Language Information Processing
(TALIP), 8(4).
Y. Benajiba, M. Diab, and P. Rosso. 2008. Arabic
named entity recognition using optimized feature
sets. In Proc. of EMNLP’08, pages 284–293.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing,
N. Kambhatla, X. Luo, N. Nicolov, and
S. Roukos. 2004. A statistical model for
multilingual entity detection and tracking. In
Proc.eedings of HLT-NAACL’04, pages 1–8.
N. Habash and F. Sadat. 2006. Combination of ara-
bic preprocessing schemes for statistical machine
translation. In Proceedings ofACL’06, pages 1–8.
H. Jing, R. Florian, X. Luo, T. Zhang, and A. Itty-
cheriah. 2003. HowtogetaChineseName(Entity):
Segmentation and combination issues. In Pro-
ceedings of EMNLP’03, pages 200–207.
E. W. Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses. John Wiley Sons.
I. Zitouni, J. Sorensen, X. Luo, and R. Florian.
2005. The impact of morphological stemming on
arabic mention detection and coreference resolu-
tion. In Proc. of the ACL Workshop on Compu-
tational Approaches to Semitic Languages, pages
63–70.
I. Zitouni, X. Luo, and R. Florian. 2009. A cascaded
approach to mention detection and chaining in
arabic. IEEE Transactions on Audio, Speech and
Language Processing, 17:935–944.
</reference>
<page confidence="0.997312">
712
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.974917">
<title confidence="0.998487">Arabic Mention Detection: Toward Better Unit of Analysis</title>
<author confidence="0.986414">Yassine Benajiba Imed Zitouni</author>
<affiliation confidence="0.9971935">Center for Computational Learning Systems IBM T. J. Watson Research Center University</affiliation>
<email confidence="0.99917">ybenajiba@ccls.columbia.edu</email>
<abstract confidence="0.999685857142857">We investigate in this paper the adequate unit of analysis for Arabic Mention Detection. We experiment different segmentation schemes with various feature-sets. Results show that when limited resources are available, models built on morphologically segmented data outperform other models by up to 4F points. On the other hand, when more resources extracted from morphologically segmented data become available, models built with Arabic TreeBank style segmentation yield to better results. We also show additional improvement by combining different segmentation schemes.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Benajiba</author>
<author>I Zitouni</author>
</authors>
<title>Morphologybased segmentation combination for arabic mention detection.</title>
<date>2009</date>
<journal>Special Issue on Arabic Natural Language Processing of ACM Transactions on Asian Language Information Processing (TALIP),</journal>
<volume>8</volume>
<issue>4</issue>
<contexts>
<context position="5008" citStr="Benajiba and Zitouni, 2009" startWordPosition="773" endWordPosition="776">owing parse tree: S � � �� CONJ NP � �� � w Al +mktb Since the È@ (Al, the definite article) is not an independent constituent, it is not considered for ATB segmentation. Hence, for I. �JºÖÏ@ð, the ATB segmentation would be I. �JºÖÏ@+ ð (w +Almktb). Punctuation separation : it consists of separating the punctuation marks from the word. Both ATB and morphological segmentation systems are based on weighted finite state transducers (WFST). The decoder implements a general Bellman dynamic programming search for the best path on a lattice of segmentation hypotheses that match the input characters (Benajiba and Zitouni, 2009). ATB and morphological segmentation systems have a performance of 99.4 and 98.1 F-measure respectively on ATB data. The unit of analysis when doing classification depends on the used segmentation. When using the punctuation separation or character-based segmentations, the unit of analysis is the word itself (without the punctuation marks attached) or the character, respectively. The ATB and morphological segmentations are language specific and are based on different linguistic viewpoint. When using one of these two segmentation schemes, the unit of analysis is the morph (i.e. prefix, stem or </context>
</contexts>
<marker>Benajiba, Zitouni, 2009</marker>
<rawString>Y. Benajiba and I. Zitouni. 2009. Morphologybased segmentation combination for arabic mention detection. Special Issue on Arabic Natural Language Processing of ACM Transactions on Asian Language Information Processing (TALIP), 8(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Benajiba</author>
<author>M Diab</author>
<author>P Rosso</author>
</authors>
<title>Arabic named entity recognition using optimized feature sets.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP’08,</booktitle>
<pages>284--293</pages>
<contexts>
<context position="2723" citStr="Benajiba et al., 2008" startWordPosition="412" endWordPosition="415"> paper, we investigate different segmentation schemes in order to define the best unit of analysis for Arabic MD. Arabic adopts a very complex morphology, i.e. each word is composed of zero or more prefixes, one stem and zero or more suffixes. Consequently, the Arabic data is sparser than other languages, such as English, and it is necessary to “segment” the words into several units of analysis in order to achieve a good performance. (Zitouni et al., 2005) used Arabic morphologically segmented data and claimed to have very competitive results in ACE 2003 and ACE 2004 data. On the other hand, (Benajiba et al., 2008) report good results for Arabic NER on ACE 2003, 2004 and 2005 data using Arabic TreeBank (ATB) segmentation. In all published works, authors do not mention a specific motivation for the segmentation scheme they have adopted. Only for the Machine Translation task, (Habash and Sadat, 2006) report several results using different Arabic segmentation schemes. They report that the best results were obtained when the ATB-like segmentation was used. We explore here the four known and linguistically-motivated sorts of segmentation: punctuation separation, ATB, morphological and character-level segment</context>
</contexts>
<marker>Benajiba, Diab, Rosso, 2008</marker>
<rawString>Y. Benajiba, M. Diab, and P. Rosso. 2008. Arabic named entity recognition using optimized feature sets. In Proc. of EMNLP’08, pages 284–293.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Florian</author>
<author>H Hassan</author>
<author>A Ittycheriah</author>
<author>H Jing</author>
<author>N Kambhatla</author>
<author>X Luo</author>
<author>N Nicolov</author>
<author>S Roukos</author>
</authors>
<title>A statistical model for multilingual entity detection and tracking.</title>
<date>2004</date>
<booktitle>In Proc.eedings of HLT-NAACL’04,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="1976" citStr="Florian et al., 2004" startWordPosition="283" endWordPosition="286"> identifying nominal and pronominal mentions. We formulate the mention detection problem as a classification problem, by assigning to each token in the text a label, indicating whether it starts a specific mention, is inside a specific mention, or is outside all mentions. The selection of the unit of analysis is an important step toward a better classification. When processing languages, such as English, using the word itself as the 1We adopt here the ACE nomenclature: http://www.nist.gov/speech/tests/ace/index.html unit of analysis (after separating punctuations) leads to a good performance (Florian et al., 2004). For other languages, such as Chinese, character is considered as the adequate unit of analysis (Jing et al., 2003). In this paper, we investigate different segmentation schemes in order to define the best unit of analysis for Arabic MD. Arabic adopts a very complex morphology, i.e. each word is composed of zero or more prefixes, one stem and zero or more suffixes. Consequently, the Arabic data is sparser than other languages, such as English, and it is necessary to “segment” the words into several units of analysis in order to achieve a good performance. (Zitouni et al., 2005) used Arabic mo</context>
</contexts>
<marker>Florian, Hassan, Ittycheriah, Jing, Kambhatla, Luo, Nicolov, Roukos, 2004</marker>
<rawString>R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos. 2004. A statistical model for multilingual entity detection and tracking. In Proc.eedings of HLT-NAACL’04, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Habash</author>
<author>F Sadat</author>
</authors>
<title>Combination of arabic preprocessing schemes for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings ofACL’06,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="3012" citStr="Habash and Sadat, 2006" startWordPosition="460" endWordPosition="463">n other languages, such as English, and it is necessary to “segment” the words into several units of analysis in order to achieve a good performance. (Zitouni et al., 2005) used Arabic morphologically segmented data and claimed to have very competitive results in ACE 2003 and ACE 2004 data. On the other hand, (Benajiba et al., 2008) report good results for Arabic NER on ACE 2003, 2004 and 2005 data using Arabic TreeBank (ATB) segmentation. In all published works, authors do not mention a specific motivation for the segmentation scheme they have adopted. Only for the Machine Translation task, (Habash and Sadat, 2006) report several results using different Arabic segmentation schemes. They report that the best results were obtained when the ATB-like segmentation was used. We explore here the four known and linguistically-motivated sorts of segmentation: punctuation separation, ATB, morphological and character-level segmentations. To our knowledge, this is the first paper which investigates different segmentation schemes to define the unit of analysis which best fits Arabic MD. 2 Arabic Segmentation Schemes Character-level Segmentation: considers that each character is a separate token. Morphological Segmen</context>
</contexts>
<marker>Habash, Sadat, 2006</marker>
<rawString>N. Habash and F. Sadat. 2006. Combination of arabic preprocessing schemes for statistical machine translation. In Proceedings ofACL’06, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Jing</author>
<author>R Florian</author>
<author>X Luo</author>
<author>T Zhang</author>
<author>A Ittycheriah</author>
</authors>
<title>HowtogetaChineseName(Entity): Segmentation and combination issues.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP’03,</booktitle>
<pages>200--207</pages>
<contexts>
<context position="2092" citStr="Jing et al., 2003" startWordPosition="303" endWordPosition="306">y assigning to each token in the text a label, indicating whether it starts a specific mention, is inside a specific mention, or is outside all mentions. The selection of the unit of analysis is an important step toward a better classification. When processing languages, such as English, using the word itself as the 1We adopt here the ACE nomenclature: http://www.nist.gov/speech/tests/ace/index.html unit of analysis (after separating punctuations) leads to a good performance (Florian et al., 2004). For other languages, such as Chinese, character is considered as the adequate unit of analysis (Jing et al., 2003). In this paper, we investigate different segmentation schemes in order to define the best unit of analysis for Arabic MD. Arabic adopts a very complex morphology, i.e. each word is composed of zero or more prefixes, one stem and zero or more suffixes. Consequently, the Arabic data is sparser than other languages, such as English, and it is necessary to “segment” the words into several units of analysis in order to achieve a good performance. (Zitouni et al., 2005) used Arabic morphologically segmented data and claimed to have very competitive results in ACE 2003 and ACE 2004 data. On the othe</context>
</contexts>
<marker>Jing, Florian, Luo, Zhang, Ittycheriah, 2003</marker>
<rawString>H. Jing, R. Florian, X. Luo, T. Zhang, and A. Ittycheriah. 2003. HowtogetaChineseName(Entity): Segmentation and combination issues. In Proceedings of EMNLP’03, pages 200–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E W Noreen</author>
</authors>
<title>Computer-Intensive Methods for Testing Hypotheses.</title>
<date>1989</date>
<publisher>John Wiley Sons.</publisher>
<contexts>
<context position="13106" citStr="Noreen, 1989" startWordPosition="2130" endWordPosition="2131">, when a larger set of resources are available (e.g., Semf), the use of the ATB token as a unit of analysis combined with morphbased features leads to better performance (79.0 vs. 78.3 on Morphs). This is because (1) classifiers trained on ATBs handle better the context and (2) the use of morph-based features (output of classi711 fiers trained on morphologically segmented data) removes some of the data sparseness from which classifiers trained on ATB, suffer. The obtained improvement in performance is statistically significant when using the stratified bootstrap re-sampling significance test (Noreen, 1989). We consider results as statistically significant when p &lt; 0.02, which is the case in this paper. For an accurate MD system, we think it is appropriate to benefit from ATB, tokens and Morph,. We investigate in the following the combination of these two segmentation styles. Table 2: Performance in terms of F-measure per level on ATBs and Morphs Seg. Lexf Stemf Syntf Semf Nam. ATBs 68.2 69.0 72.8 79.1 Morphs 73.4 73.8 75.3 78.7 Nom. ATBs 65.6 64.6 66.9 75.8 Morphs 71.7 72.2 72.9 75.4 Pro. ATBs 60.7 60.1 59.9 66.3 Morphs 63.0 67.2 65.7 65.1 5.1 Combination of ATB and Morph We trained a model on </context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>E. W. Noreen. 1989. Computer-Intensive Methods for Testing Hypotheses. John Wiley Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Zitouni</author>
<author>J Sorensen</author>
<author>X Luo</author>
<author>R Florian</author>
</authors>
<title>The impact of morphological stemming on arabic mention detection and coreference resolution.</title>
<date>2005</date>
<booktitle>In Proc. of the ACL Workshop on Computational Approaches to Semitic Languages,</booktitle>
<pages>63--70</pages>
<contexts>
<context position="2561" citStr="Zitouni et al., 2005" startWordPosition="384" endWordPosition="387">d performance (Florian et al., 2004). For other languages, such as Chinese, character is considered as the adequate unit of analysis (Jing et al., 2003). In this paper, we investigate different segmentation schemes in order to define the best unit of analysis for Arabic MD. Arabic adopts a very complex morphology, i.e. each word is composed of zero or more prefixes, one stem and zero or more suffixes. Consequently, the Arabic data is sparser than other languages, such as English, and it is necessary to “segment” the words into several units of analysis in order to achieve a good performance. (Zitouni et al., 2005) used Arabic morphologically segmented data and claimed to have very competitive results in ACE 2003 and ACE 2004 data. On the other hand, (Benajiba et al., 2008) report good results for Arabic NER on ACE 2003, 2004 and 2005 data using Arabic TreeBank (ATB) segmentation. In all published works, authors do not mention a specific motivation for the segmentation scheme they have adopted. Only for the Machine Translation task, (Habash and Sadat, 2006) report several results using different Arabic segmentation schemes. They report that the best results were obtained when the ATB-like segmentation w</context>
<context position="6685" citStr="Zitouni et al., 2005" startWordPosition="1048" endWordPosition="1052">se the maximum entropy (MaxEnt) classifier that can integrate arbitrary types of information and make a classification decision by aggregating all information available for a given classification. For more details about the system architecture, reader may refer to (Zitouni et al., 2009). The features used in our MD system can be divided into four categories: Lexical Features: n-grams spanning the current token; both preceding and following it. A number of n equal to 3 turned out to be a good choice. Stem n-gram Features: stem trigram spanning the current stem; both preceding and following it (Zitouni et al., 2005). Syntactic Features: POS tags and shallow parsing information in a f2 window. Features From Other Classifiers: outputs of MD and NER taggers trained on other data-sets different from the one we used here. They may identify types of mentions different from the mentions of interest in our task. For instance, such a tagger may identify dates or occupation references (not used in our task), among other types. Our hypothesis is that combining classifiers from diverse sources will boost performance by injecting complementary information into the mention detection models. We also use the two previou</context>
</contexts>
<marker>Zitouni, Sorensen, Luo, Florian, 2005</marker>
<rawString>I. Zitouni, J. Sorensen, X. Luo, and R. Florian. 2005. The impact of morphological stemming on arabic mention detection and coreference resolution. In Proc. of the ACL Workshop on Computational Approaches to Semitic Languages, pages 63–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Zitouni</author>
<author>X Luo</author>
<author>R Florian</author>
</authors>
<title>A cascaded approach to mention detection and chaining in arabic.</title>
<date>2009</date>
<journal>IEEE Transactions on Audio, Speech and Language Processing,</journal>
<pages>17--935</pages>
<contexts>
<context position="6351" citStr="Zitouni et al., 2009" startWordPosition="990" endWordPosition="993">Arabic example we show between parenthesis its transliteration and English translation separated by “—”. 3 Mention Detection System As explained earlier, we consider the MD task as a sequence classification problem where the class we predict for each unit of analysis (i.e., token) is the type of the entity which it refers to. We chose the maximum entropy (MaxEnt) classifier that can integrate arbitrary types of information and make a classification decision by aggregating all information available for a given classification. For more details about the system architecture, reader may refer to (Zitouni et al., 2009). The features used in our MD system can be divided into four categories: Lexical Features: n-grams spanning the current token; both preceding and following it. A number of n equal to 3 turned out to be a good choice. Stem n-gram Features: stem trigram spanning the current stem; both preceding and following it (Zitouni et al., 2005). Syntactic Features: POS tags and shallow parsing information in a f2 window. Features From Other Classifiers: outputs of MD and NER taggers trained on other data-sets different from the one we used here. They may identify types of mentions different from the menti</context>
</contexts>
<marker>Zitouni, Luo, Florian, 2009</marker>
<rawString>I. Zitouni, X. Luo, and R. Florian. 2009. A cascaded approach to mention detection and chaining in arabic. IEEE Transactions on Audio, Speech and Language Processing, 17:935–944.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>