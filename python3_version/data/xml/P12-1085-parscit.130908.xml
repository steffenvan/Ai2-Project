<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.025857">
<title confidence="0.988433">
Structuring E-Commerce Inventory
</title>
<author confidence="0.990559">
Karin Mauge
</author>
<affiliation confidence="0.976488">
eBay Research Labs
</affiliation>
<address confidence="0.8234575">
2145 Hamilton Avenue
San Jose, CA 95125
</address>
<email confidence="0.998536">
kmauge@ebay.com
</email>
<author confidence="0.949961">
Khash Rohanimanesh
</author>
<affiliation confidence="0.951287">
eBay Research Labs
</affiliation>
<address confidence="0.8314605">
2145 Hamilton Avenue
San Jose, CA 95125
</address>
<email confidence="0.998889">
krohanimanesh@ebay.com
</email>
<author confidence="0.954477">
Jean-David Ruvini
</author>
<affiliation confidence="0.952745">
eBay Research Labs
</affiliation>
<address confidence="0.8317125">
2145 Hamilton Avenue
San Jose, CA 95125
</address>
<email confidence="0.999364">
jruvini@ebay.com
</email>
<sectionHeader confidence="0.995644" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999754533333333">
Large e-commerce enterprises feature mil-
lions of items entered daily by a large vari-
ety of sellers. While some sellers provide
rich, structured descriptions of their items, a
vast majority of them provide unstructured
natural language descriptions. In the paper
we present a 2 steps method for structuring
items into descriptive properties. The first step
consists in unsupervised property discovery
and extraction. The second step involves su-
pervised property synonym discovery using a
maximum entropy based clustering algorithm.
We evaluate our method on a year worth of e-
commerce data and show that it achieves ex-
cellent precision with good recall.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999894">
Online commerce has gained a lot of popularity over
the past decade. Large on-line C2C marketplaces
like eBay and Amazon, feature a very large and
long-tail inventory with millions of items (product
offers) entered into the marketplace every day by a
large variety of sellers. While some sellers (gener-
ally large professional ones) provide rich, structured
description of their products (using schemas or via
a global trade item number), the vast majority only
provide unstructured natural language descriptions.
To manage items effectively and provide the best
user experience, it is critical for these marketplaces
to structure their inventory into descriptive name-
value pairs (called properties) and ensure that items
of the same kind (digital cameras for instance) are
described using a unique set of property names
(brand, model, zoom, resolution, etc.) and values.
For example, this is important for measuring item
similarity and complementarity in merchandising,
providing faceted navigation and various business
intelligence applications. Note that structuring items
does not necessarily mean identifying products as
not all e-commerce inventory is manufactured (an-
imals for examples).
Structuring inventory in the e-commerce domain
raises several challenges. First, one needs to iden-
tify and extract the names and the values used by
individual sellers from unstructured textual descrip-
tions. Second, different sellers may describe the
same product in very different ways, using differ-
ent terminologies. For example, Figure 1 shows
3 item descriptions of hard drives from 3 different
sellers. The left description mentions ”rotational
speed” in a specification table while the other two
descriptions use the synonym ”spindle speed” in a
bulleted list (top right) or natural language speci-
fications (bottom right). This requires discovering
semantically equivalent property names and values
across inventories from multiple sellers. Third, the
scale at which on-line marketplaces operate makes
impractical to solve any of these problems manually.
For instance, eBay reported 99 million active users
in 2011, many of whom are sellers, which may trans-
late into thousands or even millions of synonyms to
discover accross more than 20,000 categories rang-
ing from consumer electronics to collectible and art.
This paper describes a two step process for struc-
turing items in the e-commerce domain. The first
step consists in an unsupervised property extrac-
tion technique which allows discovering name-value
</bodyText>
<page confidence="0.980544">
805
</page>
<note confidence="0.9857635">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 805–814,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999641428571429">
pairs from unstructured item descriptions. The sec-
ond step consists in identifying semantically equiv-
alent property names amongst these extracted prop-
erties. This is accomplished using supervised max-
imum entropy based clustering. Note that, although
value synonym discovery is an equally important
task for structuring items, this is still an area of on-
going research and is not addressed in this paper.
The remainder of this paper is structured as fol-
lows. We first review related work. We then describe
the two steps of our approach: 1) unsupervised prop-
erty discovery and extraction and 2) property name
synonym discovery. Finally, we present experimen-
tal results on real large-scale e-commerce data.
</bodyText>
<sectionHeader confidence="0.999814" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99982425">
This section reviews related work for the two com-
ponents of our method, namely unsupervised prop-
erty extraction and supervised property name syn-
onym discovery.
</bodyText>
<subsectionHeader confidence="0.98455">
2.1 Unsupervised Property Extraction
</subsectionHeader>
<bodyText confidence="0.999909574468085">
A lot of progress has been accomplished in the area
of property discovery from product reviews since the
pioneering work by (Hu and Liu, 2004). Most of
this work is based on the observation, later formal-
ized as double propagation by (Qiu et al., 2009),
that in reviews, opinion words are usually asso-
ciated with product properties in some ways, and
thus product properties can be identified from opin-
ion words and opinion words from properties alter-
nately and iteratively. While (Hu and Liu, 2004) ini-
tially used association mining techniques; (Liu et al.,
2005) used Part-Of-Speech and supervised rule min-
ing to generate language patterns and identify prod-
uct properties; (Popescu and Etzioni, 2005) used
point wise mutual information between candidate
properties and meronymy discriminators; (Zhuang
et al., 2006; Qiu et al., 2009) improved on previous
work by using dependency parsing; (Kobayashi et
al., 2007) mined property-opinion patterns using sta-
tistical and contextual cues; (Wang and Wang, 2008)
leveraged property-opinion mutual information and
linguistic rules to identify infrequent properties; and
(Zhang et al., 2010) proposed a ranking scheme to
improve double propagation precision. In this pa-
per, we are focusing on extracting properties from
product descriptions which do not contain opinion
words.
In a sense, item properties can be viewed as slots
of product templates and our work bears similari-
ties with template induction methods. (Chambers
and Jurafsky, 2011) proposed a method for inferring
event templates based on word clustering according
to their proximity in the corpus and syntactic func-
tion clustering. Unfortunately, this technique can-
not be applied to our problem due to the lack of dis-
course redundancy within item descriptions.
(Putthividhya and Hu, 2011) and (Sachan et al.,
2011) also addressed the problem of structuring
items in the e-commerce domain. However, these
works assume that property names are known in
advance and focus on discovering values for these
properties from very short product titles.
Although we are primarily concerned with unsu-
pervised property discovery, it is worth mentioning
(Peng and McCallum, 2004) and (Ghani et al., 2006)
who approached the problem using supervised ma-
chine learning techniques and require labeled data.
</bodyText>
<subsectionHeader confidence="0.99964">
2.2 Property Name Synonym Discovery
</subsectionHeader>
<bodyText confidence="0.999925958333333">
Our work is related to the synonym discovery re-
search which aims at identifying groups of words
that are semantically identical based on some de-
fined similarity metric. The body of work on
this problem can be divided into two major ap-
proaches (Agirre et al., 2009): methods that are
based on the available knowledge resources (e.g.,
WordNet, or available taxonomies) (Yang and Pow-
ers, 2005; Alvarez and Lim, 2007; Hughes and Ra-
mage, ), and methods that use contextual/property
distribution around the words (Pereira et al., 1993;
Chen et al., 2006; Sahami and Heilman, 2006; Pan-
tel et al., 2009). (Zhai et al., 2010) propose a con-
strained semi-supervised learning method using a
naive Bayes formulation of EM seeded by a small
set of labeled data and a set of soft constraints based
on the prior knowledge of the problem. There has
been also some recent work on applying topic mod-
eling (e.g., LDA) for solving this problem (Guo et
al., 2009).
Our work is also related to the existing research
on schema matching problem where the objective is
to identify objects that are semantically related cross
schemas. There has been an extensive study on the
</bodyText>
<page confidence="0.99893">
806
</page>
<figureCaption confidence="0.9965445">
Figure 1: Three examples of item descriptions containing a specification table (left image), a bulleted list (top right)
and natural language specifications (bottom right).
</figureCaption>
<bodyText confidence="0.999894369565217">
problem of schema matching (for a comprehensive
survey see (Rahm and Bernstein, 2001; Bellahsene
et al., 2011; Bernstein et al., 2011)). In general the
work can be classified into rule-based and learning-
based approaches. Rule-based systems (Castano
and de Antonellis, 1999; Milo and Zohar, 1998;
L. Palopol and Ursino, 1998) often utilize only the
schema information (e.g., elements, domain types
of schema elements, and schema structure) to define
a similarity metric for performing matching among
the schema elements in a hard coded fashion. In
contrast learning based approaches learn a similar-
ity metric based on both the schema information
and the data. Earlier learning based systems (Li
and Clifton, 2000; Perkowitz and Etzioni, 1995;
Clifton et al., 1997) often rely on one type of learn-
ing (e.g., schema meta-data, statistics of the data
content, properties of the objects shared between
the schemas, etc). These systems do not exploit
the complete textual information in the data con-
tent therefore have limited applicability. Most re-
cent systems attempt to incorporate the textual con-
tents of the data sources into the system. Doan et
al. (2001) introduce LSD which is a semi-automatic
machine learning based matching framework that
trains a set of base learners using a set of user pro-
vided semantic mappings over a small data sources.
Each base learner exploits a different type of in-
formation, e.g. source schema information and in-
formation in the data source. Given a new data
source, the base learners are used to discover se-
mantic mappings and their prediction is combined
using a meta-learner. Similar to LSD, GLUE (Doan
et al., 2003) also uses a set of base learners com-
bined into a meta-learner for solving the match-
ing problem between two ontologies. Our work is
mostly related to (Wick et al., 2008) where they
propose a general framework for performing jointly
schema matching, co-reference and canonicalization
using a supervised machine learning approach. In
this approach the matching problem is treated as
a clustering problem in the schema attribute space,
where a cluster captures a matched set of attributes.
A conditional random field (CRF) (Lafferty et al.,
2001) is trained using user provided mappings be-
tween example schemas, or ontologies. CRF bene-
</bodyText>
<page confidence="0.983255">
807
</page>
<bodyText confidence="0.999954333333334">
fits from first order logic features that capture both
schema/ontology information as well as textual fea-
tures in the related data sources.
</bodyText>
<sectionHeader confidence="0.985392" genericHeader="method">
3 Unsupervised Property Extraction
</sectionHeader>
<bodyText confidence="0.999963571428572">
The first step of our solution to structuring e-
commerce inventory aims at discovering and ex-
tracting relevant properties from items.
Our method is unsupervised and requires no prior
knowledge of relevant properties or any domain
knowledge as it operates the exact same way for
all items and categories. It maintains a set of pre-
viously discovered properties called known proper-
ties with popularity information. The popularity of
a given property name N (resp. value V ) is defined
as the number of sellers who are using N (resp. V ).
A seller is said to use a name or a value if we are
able to extract the property name or value from at
least one of its item descriptions. The method is
incremental in that it starts with an empty set of
known properties, mines individual items indepen-
dently and incrementally builds and updates the set
of known properties.
The key intuition is that the abundance of data
in e-commerce allows simple and scalable heuris-
tic to perform very well. For property extraction this
translates into the following observation: although
we may need complex natural language processing
for extracting properties from each and every item,
simple patterns can extract most of the relevant prop-
erties from a subset of the items due to redundancy
between sellers. In other words, popular properties
are used by many sellers and some of them write
their descriptions in a manner that makes these prop-
erties easy to extract. For example one pattern that
some sellers use to describe product properties often
starts by a property name followed by a colon and
then the property value (we refer to this pattern as
the colon pattern). Using this pattern we can mine
colon separated short strings like ”size : 20 inches”
or ”color : light blue” which enables us to discover
most relevant property names. However, such a pat-
tern extracts properties from a fraction of the inven-
tory only and does not suffice. We are using 4 pat-
terns which are formally defined in Table 1.
All patterns run on the entire item description.
Pattern 1 skips the html markers and scripts and
applies only to the content sentences. It ignores
any candidate property which name is longer than
30 characters and values longer than 80 characters.
These length thresholds may be domain dependent.
They have been chosen empirically. Pattern 2, 3 and
4 search for known property names. Pattern 2 ex-
tracts the closest value to the right of the name. It al-
lows the name and the value to be separated by spe-
cial characters or some html markups (like ”&lt;TR&gt;”,
”&lt;TD&gt;”, etc.). It captures a wide range of name
value pair occurrences including rows of specifica-
tion tables.
Syntactic cleaning and validation is performed
on all the extracted properties. Cleaning consists
mainly in removing bullets from the beginning of
names and punctuation at the end of names and val-
ues. Validation rejects properties which names are
pure numbers, properties that contain some special
characters and names which are less than 3 charac-
ters long. All discovered properties are added to the
set of known properties and their popularity counts
are updated.
Note that for efficiency reasons, Part-Of-Speech
(POS) tagging is performed only on sentences con-
taining the anchor of a pattern. The anchor of pat-
tern 1 is the colon sign while the anchor of the other
patterns is the known property name KN. We use
(Toutanova et al., 2003) for POS tagging.
</bodyText>
<sectionHeader confidence="0.982032" genericHeader="method">
4 Property Synonym Discovery
</sectionHeader>
<bodyText confidence="0.999444">
In this section we briefly overview a probabilistic
pairwise property synonym model inspired by (Cu-
lotta et al., 2007).
</bodyText>
<subsectionHeader confidence="0.966813">
4.1 Probabilistic Model
</subsectionHeader>
<bodyText confidence="0.9999505">
Given a category C, let Xc = {x1, x2,... , x,} be
the raw set of n property names (prior to synonym
discovery) extracted from a corpus of data associ-
ated with that category. Every property name is as-
sociated with pairs of values and popularity count
(as defined in Section 3) Vxi = {(vij,ci(vij))} j�1,
where vij is the jth value associated for the prop-
erty name xi and ci(vij) is the popularity of value vij.
Given a pair of property names xij = {xi, xj}, let
the binary random variable yij be 1 if xi and xj are
synonyms. Let T = {fk(xij, y)} be a set of fea-
tures over xij. For example, fk(xij, y) may indicate
</bodyText>
<page confidence="0.983398">
808
</page>
<table confidence="0.9998742">
# Pattern Example
1 [NP][:][optional DT][NP] ”color : light blue”
2 [KN][optional html][NP] ”size&lt;/TD&gt;&lt;TD&gt;&lt;FONT COLOR=&amp;quot;red&amp;quot;&gt;20 inches”
3 [!IN][KN][&amp;quot;is&amp;quot; or &amp;quot;are&amp;quot;][NP] ”color is red”
4 [NP][KN] ”red color”
</table>
<tableCaption confidence="0.981316666666667">
Table 1: Patterns used to extract properties from item description. The macro tag NP denotes any of the tags NN,
NNP, NNS, NNPS, JJ, JJS or CD. The KN tag is defined as a NP tag over a known property name. Pattern 1 only can
discover new names; patterns 2 to 4 aim at capturing values for known property names.
</tableCaption>
<bodyText confidence="0.999719333333333">
whether xi and xj have both numerical values. Each
feature fk has an associated real-valued parameter
Ak. The pairwise model is given by:
</bodyText>
<equation confidence="0.997013">
P(yij  |xij) = Z1.. exp E Akfk(xij, yij) (1)
xz�
k
</equation>
<bodyText confidence="0.999734595238095">
where Zxij is a normalizer that sums over the two
settings of yij. This is a maximum-entropy classifier
(i.e. logistic regression) in which P(yij|xij) is the
probability that xi and xj are synonyms. To estimate
A = {Ak} from labeled training data, we perform
gradient ascent to maximize the log-likelihood of the
labeled data.
Given a data set in which property names are
manually clustered, the training data can be cre-
ated by simply enumerating over each pair of syn-
onym property names xij, where yij is true if xi
and xj are in the same cluster. More practically,
given the raw set of extracted properties, first we
manually cluster them. Positive examples are then
pairs of property names from the same cluster. Neg-
ative examples are pairs of names cross two dif-
ferent clusters randomly selected. For example,
let assume that the following four property name
clusters have been constructed: {color, shade},
{size, dimension}, {weight}, {features}. These
clusters implies that ”color” and ”shade” are syn-
onym; that ”size” and ”dimension” are synonym and
that ”weight” and ”features” don’t have any syn-
onym. The pair (color, shade) is a positive exam-
ples, while (size, shade) and (weight, features)
are negative examples.
Now, given an unseen category C&apos; and the set of
raw properties (property names and values) mined
from that category, we can construct an undirected-
weighted graph in which vertices correspond to the
property names NCs and edge weights are propor-
tional to P(yij|xij). The problem is now reduced to
finding the maximum a posteriori (MAP) setting of
yijs in the new graph. The inference in such mod-
els is generally intractable, therefore we apply ap-
proximate graph partitioning methods where we par-
tition the graph into clusters with high intra-cluster
edge weights and low inter-cluster edge weights. In
this work we employ the standard greedy agglom-
erative clustering, in which each noun phrase would
be assigned to the most probable cluster according
to P(yij|xij).
</bodyText>
<subsectionHeader confidence="0.658017">
4.2 Features
</subsectionHeader>
<bodyText confidence="0.998076">
Given a pair of property names xij = {xi, xj} we
have designed a set of features as follows:
Property name string similarity/distance: This
measures string similarity between two names. We
have included various string edit distances such as
Jaccard distance over n-grams extracted from the
property names, and also Levenstein distance. We
have also included a feature that compares the two
property names after their commoner morphologi-
cal and inflectional endings have been removed us-
ing the Porter Stemmer algorithm.
Property value set coverage: We compute a
weighted Jaccard measure given the values and the
value frequencies associated with a property name.
</bodyText>
<equation confidence="0.992888">
EvE(VxinVxj) min(ci(v), cj(v))
EvE(VxinVxj) max(ci(v), cj(v))
</equation>
<bodyText confidence="0.9987068">
This feature essentially computes how many prop-
erty values are common between the two property
names, weighted by their popularity.
Property name co-occurrence: This is an inter-
esting feature which is based on the observation that
</bodyText>
<equation confidence="0.676144">
J(Vxi, Vxj) =
</equation>
<page confidence="0.98207">
809
</page>
<bodyText confidence="0.999868157894737">
two property names that are synonyms, rarely oc-
cur together within the same description. This is
based on the assumption that sellers are consistent
when using property names throughout a single de-
scription. For example when they are specifying the
size of an item, they either use size or dimensions
exclusively in a single description. However, it is
more likely that two property names that are not syn-
onyms appear together within a single description.
To conform this assumption, we ran a separate ex-
periment that measures the co-occurrence frequency
of the property names in a single category. Table 2
shows a measurement of pairwise co-occurrence of
a few example property names computed over the
Audio books eBay category. Given a property name
x let Z(x) be the total number of descriptions that
contain the name x. Now, given two property names
xi and xj, we define a measure of co-occurrence of
these names as:
</bodyText>
<equation confidence="0.998044333333333">
Z(xi) n Z(xj)
CO(xi, xj) =
Z(xi) U Z(xj)
</equation>
<bodyText confidence="0.9998904">
In Table 2 it can be seen that synonym prop-
erty names such as ”author” and ”by” have a zero
co-occurrence measure, while semantically different
property names such as ”format” and ”read by” have
a non-zero co-occurrence measure.
</bodyText>
<sectionHeader confidence="0.996818" genericHeader="evaluation">
5 Experimental results
</sectionHeader>
<bodyText confidence="0.99994025">
This section presents experimental results on a real
dataset. We first describe the dataset used for these
experiments and then provide results for property
extraction and property name synonym discovery.
</bodyText>
<subsectionHeader confidence="0.999307">
5.1 Data set and methodology
</subsectionHeader>
<bodyText confidence="0.999973161290323">
All the results we are reporting in this paper were ob-
tained from a dataset of several billion descriptions
corresponding to a year worth of eBay item (no sam-
pling was performed).
For listing an item on eBay, a seller must pro-
vide a short descriptive title (up to 80 characters) and
can optionally provide a few descriptive name value
pairs called item specifics, and a free-form html de-
scription. Contrary to item specifics, a vast majority
of sellers provide a rich description containing very
useful information about the property of their item.
Figure 1 shows 3 examples of eBay descriptions.
eBay organizes items into a six-level category
structure similar to a topic hierarchy comprising
20,000 leaf categories and covering most of the
goods in the world. An item is typically listed in
one category but some items may be suitable for and
listed in two categories.
Although this dataset is not publicly available,
very similar data can be obtained from the eBay web
site and through eBay Developers API 1.
In the following, we report precision and recall
results. Evaluation was performed by two annota-
tors (non expert of the domain). For property ex-
traction, they were asked to decide whether or not an
extracted property is relevant for the corresponding
items; for synonym discovery to decide whether or
not sellers refer to the same semantic entity. Anno-
tators were asked to reject the null hypothesis only
beyond reasonable doubt and we found the annotator
agreement to be extremely high.
</bodyText>
<subsectionHeader confidence="0.99862">
5.2 Property Extraction Results
</subsectionHeader>
<bodyText confidence="0.999984333333333">
We have been running the property extraction
method described in Section 3 on our entire dataset.
The properties extracted have been aggregated at the
leaf category level and ranked by popularity (as de-
fined in Section 3). Because no gold standard data
is available for this task, evaluation has to be per-
formed manually. However, it is impractical to re-
view results for 20,000 categories and we uniformly
sampled 20 categories randomly.
Precision. Table 3 shows the weighted (by cat-
egory size) average precision of the extracted prop-
erty names up to rank 20. Precision at rank k for a
given category is defined as the number of relevant
properties in the top k properties of that categories,
divided by k. Table 4 shows the top 15 properties
extracted for five eBay categories.
Although we did not formally evaluate the preci-
sion of the discovered values, informal reviews have
shown that this method extracts good quality val-
ues. Examples are ”n/a”, ”well”, ”storage or well”,
”would be by well” and ”by well” for the prop-
erty name ”Water” in the Land category; ”metal”,
”plastic”, ”nylon”, ”acetate” and ”durable o matter”
for ”Frame material” in Sunglasses; or ”acrylic”,
</bodyText>
<footnote confidence="0.9944775">
1See https://www.x.com/developers/ebay/ for
details.
</footnote>
<page confidence="0.988061">
810
</page>
<table confidence="0.627952166666667">
author by read by format narrated by
author 0 0.06 0.06 0.006
by 0 0.17 0.005 0.013
read by 0.06 0.17 0.035 0
format 0.06 0.005 0.035 0.006
narrated by 0.006 0.013 0 0.006
</table>
<tableCaption confidence="0.994941333333333">
Table 2: Co-occurrence measure computed over a subset of property names in the Audio books category. Some
synonym property names such as author and by have zero co-occurrence frequency, while semantically different
property names such as format and read by sometimes appear together in some of the item descriptions.
</tableCaption>
<table confidence="0.999888">
Rank 1 2 3 4 5 6 7 8 9 10
Precision 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.992 0.992 0.986
Rank 11 12 13 14 15 16 17 18 19 20
Precision 0.986 0.997 0.986 1 0.998 1 1 0.959 0.722 0.747
</table>
<tableCaption confidence="0.999889">
Table 3: Weighted average precision of the top 20 extracted property names.
</tableCaption>
<bodyText confidence="0.997711">
”oil”, ”acrylic on canvas” and ”oil on canvas” for
”Medium” in Paintings.
Sets of values tend to contain more synonyms
than names. Also, we observed that some names
exhibit polysemy issues in that their values clearly
belong to several semantic clusters. An example
of polysemy is the name ”Postmark” in the ”Post-
cards” categories which contains values like ”none,
postally used, no, unused” and years (”1909, 1908,
1910...”). Cleaning and normalizing values is on-
going research effort.
Recall. Evaluating recall of our method requires
comparing for each category, the number of relevant
properties extracted to the number of relevant prop-
erties the descriptions in this category contain. It
is dauntingly expensive. As a proxy for name re-
call, we examined 20 categories and found that our
method discovered all the relevant popular property
names.
It is quite remarkable that an unsupervised
method like ours achieves results of that quality and
is able to cover most of the good of the world with
descriptive properties. To our knowledge, this has
never been accomplished before in the e-commerce
domain.
</bodyText>
<subsectionHeader confidence="0.999429">
5.3 Synonym discovery results
</subsectionHeader>
<bodyText confidence="0.99797364516129">
To train our name synonym discovery algorithm, we
manually clustered properties from 27 randomly se-
lected categories as described in Section 4. This re-
sulted in 178 clusters, 113 of them containing a sin-
gle property (no synonym) and 65 containing 2 or
more properties and capturing actual synonym in-
formation. Note that although estimating the co-
occurrence table (see Table 2) can be computation-
ally expensive, it is very manageable for such a small
set of clusters. Scalability issues due to the large
number of eBay categories (nearly 20,000) made im-
practical to use the solutions proposed in the past to
solve that problem as baselines.
Results were produced by applying the trained
model to the top 20 discovered properties for each
and every eBay categories. The algorithm discov-
ered 10672 synonyms spanning 2957 categories.
Precision. To measure the precision of our algo-
rithm, we manually labeled 6618 synonyms as cor-
rect or incorrect. 6076 synonyms were found to be
correct and 542 incorrect, a precision of 91.8%. Ta-
ble 5 shows examples of synonyms and one of the
categories where they have been discovered. Some
of them are very category specific. For instance,
while ”hp” means ”horsepower” for air compres-
sors, it is an acronym of a well known brand in con-
sumer electronics.
Recall. Evaluating recall is a more labor inten-
sive task as it involves comparing, for each of the
2957 categories, the number of synonyms discov-
ered to the number of synonyms the category con-
</bodyText>
<page confidence="0.994076">
811
</page>
<table confidence="0.9997995">
Land Aquariums iPod &amp; MP3 Players Acoustic Guitars Postcards
State Dimensions Weight Top Condition
Zoning Height Width Scale length Publisher
County Size Depth Neck Size
Water Width Height Bridge Postmark
Location Includes Color Finish Postally used
Taxes Weight Battery type Rosette Type
Size Depth Dimensions Binding Age
Sewer Capacity Frequency response Fingerboard Stamp
Power Color Storage capacity Tuning machines Date
Roads Power Display Case Title
Lot size LCD size Capacity Pickguard Postmarked
Utilities Length Screen size Tuners Subject
Parcel number Material Battery Nut width Location
Cable length Length Corners
Condition Thickness Era
</table>
<tableCaption confidence="0.995922">
Table 4: Examples of discovered properties for 5 eBay categories.
</tableCaption>
<table confidence="0.97757725">
Category Synonyms
Rechargeable Batteries {Battery type, Chemical composition}
Lodging {Check-in, Check-in time}
Flower seeds {Bloom time, Flowering season}
Doors &amp; Door Hardware {Colour,Color, Main color}
Gemstone {Cut, Shape}
Air Compressors {Hp, Horsepower}
Decorative Collectibles {Item no, Item sku, Item number}
Router Memory {Memory (ram), Memory size}
Equestrian Clothing {Bust, Chest}
Traiding Cards {Rarity, Availability}
Paper Calendar {Time period, Calendars era}
</table>
<tableCaption confidence="0.999728">
Table 5: Examples of discovered property name synonyms.
</tableCaption>
<bodyText confidence="0.999943333333333">
tains. As a proxy we labeled 40 randomly selected
categories. For these categories, we found the recall
to be 51%. As explained in Section 4, the overlap
of values between two names is an important feature
for our algorithm. The fact that we are not cleaning
and normalizing the values discovered by our prop-
erty extraction algorithm clearly impacts recall. This
is definitively an important direction for further im-
provements.
</bodyText>
<sectionHeader confidence="0.99671" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999969">
We presented a method for structuring e-commerce
inventory into descriptive properties. This method
is based on unsupervised property discovery and ex-
traction from unstructured item descriptions, and on
property name synonym discovery achieved using
a supervised maximum entropy based clustering al-
gorithm. Experiments on a large real e-commerce
dataset showed that both techniques achieve very
good results. However, we did not address the issue
of property value cleaning and normalization. This
is an important direction for future work.
</bodyText>
<page confidence="0.996209">
812
</page>
<sectionHeader confidence="0.918025" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99404816981132">
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009. A
study on similarity and relatedness using distributional
and wordnet-based approaches. In Proceedings of Hu-
man Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics, NAACL ’09,
pages 19–27, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Marco A. Alvarez and SeungJin Lim. 2007. A graph
modeling of semantic similarity between words. In
Proceedings of the International Conference on Se-
mantic Computing, pages 355–362, Washington, DC,
USA. IEEE Computer Society.
Zohra Bellahsene, Angela Bonifati, and Erhard Rahm,
editors. 2011. Schema Matching and Mapping.
Springer.
Philip A. Bernstein, Jayant Madhavan, and Erhard Rahm.
2011. Generic schema matching, ten years later. Pro-
ceedings of the VLDB Endowment, 4(11):695–701.
Silvana Castano and Valeria de Antonellis. 1999. A
schema analysis and reconciliation tool environment
for heterogeneous databases. In Proceedings of the
1999 International Symposium on Database Engineer-
ing &amp; Applications, IDEAS ’99, pages 53–, Washing-
ton, DC, USA. IEEE Computer Society.
Nathanael Chambers and Dan Jurafsky. 2011. Template-
based information extraction without the templates. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1, HLT ’11, pages 976–
986, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Hsin-Hsi Chen, Ming-Shun Lin, and Yu-Chuan Wei.
2006. Novel association measures using web search
with double checking. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th annual meeting of the Association
for Computational Linguistics, ACL-44, pages 1009–
1016, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Chris Clifton, Ed Housman, and Arnon Rosenthal. 1997.
Experience with a combined approach to attribute-
matching across heterogeneous databases. In In Proc.
of the IFIP Working Conference on Data Semantics
(DS-7.
Aron Culotta, Michael Wick, Robert Hall, and Andrew
Mccallum. 2007. First-order probabilistic models
for coreference resolution. In In Proceedings of HLT-
NAACL 2007.
AnHai Doan, Pedro Domingos, and Alon Y. Halevy.
2001. Reconciling schemas of disparate data sources:
a machine-learning approach. In Proceedings of
the 2001 ACM SIGMOD international conference on
Management of data, SIGMOD ’01, pages 509–520,
New York, NY, USA. ACM.
AnHai Doan, Jayant Madhavan, Robin Dhamankar, Pe-
dro Domingos, and Alon Halevy. 2003. Learning
to match ontologies on the semantic web. The VLDB
Journal, 12:303–319, November.
Rayid Ghani, Katharina Probst, Yan Liu, Marko Krema,
and Andrew Fano. 2006. Text mining for product at-
tribute extraction. SIGKDD Explor. Newsl., 8:41–48,
June.
Honglei Guo, Huijia Zhu, Zhili Guo, XiaoXun Zhang,
and Zhong Su. 2009. Product feature categorization
with multilevel latent semantic association. In Pro-
ceedings of the 18th ACM conference on Information
and knowledge management, CIKM ’09, pages 1087–
1096, New York, NY, USA. ACM.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ’04, pages 168–
177, New York, NY, USA. ACM.
Thad Hughes and Daniel Ramage. Lexical semantic re-
latedness with random graph walks. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
581–589.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-of re-
lations in opinion mining. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL.
D. Sacc`a L. Palopol and D. Ursino. 1998. Semi-
automatic, semantic discovery of properties from
database schemes. In Proceedings of the 1998 Inter-
national Symposium on Database Engineering &amp; Ap-
plications, pages 244–, Washington, DC, USA. IEEE
Computer Society.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling sequence
data. In Proceedings of the Eighteenth International
Conference on Machine Learning, ICML ’01, pages
282–289, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Wen-Syan Li and Chris Clifton. 2000. Semint: a tool
for identifying attribute correspondences in heteroge-
neous databases using neural networks. Data Knowl.
Eng., 33:49–84, April.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: analyzing and comparing opinions
</reference>
<page confidence="0.99576">
813
</page>
<reference confidence="0.998252528301887">
on the web. In Proceedings of the 14th international
conference on World Wide Web, WWW ’05, pages
342–351, New York, NY, USA. ACM.
Tova Milo and Sagit Zohar. 1998. Using schema match-
ing to simplify heterogeneous data translation. In Pro-
ceedings of the 24rd International Conference on Very
Large Data Bases, VLDB ’98, pages 122–133, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume 2 -
Volume 2, EMNLP ’09, pages 938–947, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Fuchun Peng and Andrew McCallum. 2004. Accu-
rate information extraction from research papers using
conditional random fields. In HLT-NAACL04, pages
329–336.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of english words. In Pro-
ceedings of the 31st annual meeting on Association for
Computational Linguistics, ACL ’93, pages 183–190,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Mike Perkowitz and Oren Etzioni. 1995. Category trans-
lation: learning to understand information on the in-
ternet. In Proceedings of the 14th international joint
conference on Artificial intelligence - Volume 1, pages
930–936, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural Lan-
guage Processing, HLT ’05, pages 339–346, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Duangmanee Putthividhya and Junling Hu. 2011. Boot-
strapped named entity recognition for product attribute
extraction. In EMNLP, pages 1557–1567.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2009.
Expanding domain sentiment lexicon through double
propagation. In Proceedings of the 21st international
jont conference on Artifical intelligence, IJCAI’09,
pages 1199–1204, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
Erhard Rahm and Philip A. Bernstein. 2001. A survey of
approaches to automatic schema matching. The VLDB
Journal, 10:334–350.
Mrinmaya Sachan, Tanveer Faruquie, L. V. Subrama-
niam, and Mukesh Mohania. 2011. Using text reviews
for product entity completion. In Poster at the 5th
International Joint Conference on Natural Language
Processing, IJCNLP’11, pages 983–991.
Mehran Sahami and Timothy D. Heilman. 2006. A web-
based kernel function for measuring the similarity of
short text snippets. In Proceedings of the 15th inter-
national conference on World Wide Web, WWW ’06,
pages 377–386, New York, NY, USA. ACM.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ’03, pages 173–180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Bo Wang and Houfeng Wang. 2008. Bootstrapping both
product features and opinion words from chinese cus-
tomer reviews with cross-inducing. In Proceedings of
the Third International Joint Conference on Natural
Language Processing.
Michael L. Wick, Khashayar Rohanimanesh, Karl
Schultz, and Andrew McCallum. 2008. A unified ap-
proach for schema matching, coreference and canoni-
calization. In Proceedings of the 14th ACM SIGKDD
international conference on Knowledge discovery and
data mining, KDD ’08, pages 722–730, New York,
NY, USA. ACM.
Dongqiang Yang and David M. W. Powers. 2005. Mea-
suring semantic similarity in the taxonomy of word-
net. In Proceedings of the Twenty-eighth Australasian
conference on Computer Science - Volume 38, ACSC
’05, pages 315–322, Darlinghurst, Australia, Aus-
tralia. Australian Computer Society, Inc.
Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia. 2010.
Grouping product features using semi-supervised
learning with soft-constraints. In Proceedings of the
23rd International Conference on Computational Lin-
guistics, COLING ’10, pages 1272–1280, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn
O’Brien-Strain. 2010. Extracting and ranking prod-
uct features in opinion documents. In Proceedings of
the 23rd International Conference on Computational
Linguistics: Posters, COLING ’10, pages 1462–1470,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006. Movie
review mining and summarization. In CIKM ’06: Pro-
ceedings of the 15th ACM international conference on
Information and knowledge management, pages 43–
50, New York, NY, USA. ACM.
</reference>
<page confidence="0.998535">
814
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.898032">
<title confidence="0.999547">Structuring E-Commerce Inventory</title>
<author confidence="0.999057">Karin Mauge</author>
<affiliation confidence="0.997517">eBay Research</affiliation>
<address confidence="0.9792165">2145 Hamilton Avenue San Jose, CA</address>
<email confidence="0.999423">kmauge@ebay.com</email>
<author confidence="0.985826">Khash Rohanimanesh</author>
<affiliation confidence="0.9981">eBay Research</affiliation>
<address confidence="0.999828">2145 Hamilton Avenue San Jose, CA 95125</address>
<email confidence="0.999577">krohanimanesh@ebay.com</email>
<author confidence="0.997458">Jean-David Ruvini</author>
<affiliation confidence="0.999483">eBay Research Labs</affiliation>
<address confidence="0.9998625">2145 Hamilton Avenue San Jose, CA 95125</address>
<email confidence="0.999764">jruvini@ebay.com</email>
<abstract confidence="0.9973883125">Large e-commerce enterprises feature millions of items entered daily by a large variety of sellers. While some sellers provide rich, structured descriptions of their items, a vast majority of them provide unstructured natural language descriptions. In the paper we present a 2 steps method for structuring items into descriptive properties. The first step consists in unsupervised property discovery and extraction. The second step involves supervised property synonym discovery using a maximum entropy based clustering algorithm. We evaluate our method on a year worth of ecommerce data and show that it achieves excellent precision with good recall.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Eneko Agirre</author>
<author>Enrique Alfonseca</author>
<author>Keith Hall</author>
<author>Jana Kravalova</author>
<author>Marius Pas¸ca</author>
<author>Aitor Soroa</author>
</authors>
<title>A study on similarity and relatedness using distributional and wordnet-based approaches.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09,</booktitle>
<pages>pages</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pas¸ca, Soroa, 2009</marker>
<rawString>Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pas¸ca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and wordnet-based approaches. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09, pages 19–27, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco A Alvarez</author>
<author>SeungJin Lim</author>
</authors>
<title>A graph modeling of semantic similarity between words.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on Semantic Computing,</booktitle>
<pages>355--362</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="7355" citStr="Alvarez and Lim, 2007" startWordPosition="1114" endWordPosition="1117"> it is worth mentioning (Peng and McCallum, 2004) and (Ghani et al., 2006) who approached the problem using supervised machine learning techniques and require labeled data. 2.2 Property Name Synonym Discovery Our work is related to the synonym discovery research which aims at identifying groups of words that are semantically identical based on some defined similarity metric. The body of work on this problem can be divided into two major approaches (Agirre et al., 2009): methods that are based on the available knowledge resources (e.g., WordNet, or available taxonomies) (Yang and Powers, 2005; Alvarez and Lim, 2007; Hughes and Ramage, ), and methods that use contextual/property distribution around the words (Pereira et al., 1993; Chen et al., 2006; Sahami and Heilman, 2006; Pantel et al., 2009). (Zhai et al., 2010) propose a constrained semi-supervised learning method using a naive Bayes formulation of EM seeded by a small set of labeled data and a set of soft constraints based on the prior knowledge of the problem. There has been also some recent work on applying topic modeling (e.g., LDA) for solving this problem (Guo et al., 2009). Our work is also related to the existing research on schema matching </context>
</contexts>
<marker>Alvarez, Lim, 2007</marker>
<rawString>Marco A. Alvarez and SeungJin Lim. 2007. A graph modeling of semantic similarity between words. In Proceedings of the International Conference on Semantic Computing, pages 355–362, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<title>Schema Matching and Mapping.</title>
<date>2011</date>
<editor>Zohra Bellahsene, Angela Bonifati, and Erhard Rahm, editors.</editor>
<publisher>Springer.</publisher>
<marker>2011</marker>
<rawString>Zohra Bellahsene, Angela Bonifati, and Erhard Rahm, editors. 2011. Schema Matching and Mapping. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip A Bernstein</author>
<author>Jayant Madhavan</author>
<author>Erhard Rahm</author>
</authors>
<title>Generic schema matching, ten years later.</title>
<date>2011</date>
<booktitle>Proceedings of the VLDB Endowment,</booktitle>
<volume>4</volume>
<issue>11</issue>
<contexts>
<context position="8403" citStr="Bernstein et al., 2011" startWordPosition="1287" endWordPosition="1290">also some recent work on applying topic modeling (e.g., LDA) for solving this problem (Guo et al., 2009). Our work is also related to the existing research on schema matching problem where the objective is to identify objects that are semantically related cross schemas. There has been an extensive study on the 806 Figure 1: Three examples of item descriptions containing a specification table (left image), a bulleted list (top right) and natural language specifications (bottom right). problem of schema matching (for a comprehensive survey see (Rahm and Bernstein, 2001; Bellahsene et al., 2011; Bernstein et al., 2011)). In general the work can be classified into rule-based and learningbased approaches. Rule-based systems (Castano and de Antonellis, 1999; Milo and Zohar, 1998; L. Palopol and Ursino, 1998) often utilize only the schema information (e.g., elements, domain types of schema elements, and schema structure) to define a similarity metric for performing matching among the schema elements in a hard coded fashion. In contrast learning based approaches learn a similarity metric based on both the schema information and the data. Earlier learning based systems (Li and Clifton, 2000; Perkowitz and Etzioni</context>
</contexts>
<marker>Bernstein, Madhavan, Rahm, 2011</marker>
<rawString>Philip A. Bernstein, Jayant Madhavan, and Erhard Rahm. 2011. Generic schema matching, ten years later. Proceedings of the VLDB Endowment, 4(11):695–701.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silvana Castano</author>
<author>Valeria de Antonellis</author>
</authors>
<title>A schema analysis and reconciliation tool environment for heterogeneous databases.</title>
<date>1999</date>
<booktitle>In Proceedings of the 1999 International Symposium on Database Engineering &amp; Applications, IDEAS ’99,</booktitle>
<pages>53</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<marker>Castano, de Antonellis, 1999</marker>
<rawString>Silvana Castano and Valeria de Antonellis. 1999. A schema analysis and reconciliation tool environment for heterogeneous databases. In Proceedings of the 1999 International Symposium on Database Engineering &amp; Applications, IDEAS ’99, pages 53–, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Templatebased information extraction without the templates.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>976--986</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6098" citStr="Chambers and Jurafsky, 2011" startWordPosition="914" endWordPosition="917">rk by using dependency parsing; (Kobayashi et al., 2007) mined property-opinion patterns using statistical and contextual cues; (Wang and Wang, 2008) leveraged property-opinion mutual information and linguistic rules to identify infrequent properties; and (Zhang et al., 2010) proposed a ranking scheme to improve double propagation precision. In this paper, we are focusing on extracting properties from product descriptions which do not contain opinion words. In a sense, item properties can be viewed as slots of product templates and our work bears similarities with template induction methods. (Chambers and Jurafsky, 2011) proposed a method for inferring event templates based on word clustering according to their proximity in the corpus and syntactic function clustering. Unfortunately, this technique cannot be applied to our problem due to the lack of discourse redundancy within item descriptions. (Putthividhya and Hu, 2011) and (Sachan et al., 2011) also addressed the problem of structuring items in the e-commerce domain. However, these works assume that property names are known in advance and focus on discovering values for these properties from very short product titles. Although we are primarily concerned w</context>
</contexts>
<marker>Chambers, Jurafsky, 2011</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2011. Templatebased information extraction without the templates. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 976– 986, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hsin-Hsi Chen</author>
<author>Ming-Shun Lin</author>
<author>Yu-Chuan Wei</author>
</authors>
<title>Novel association measures using web search with double checking.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44,</booktitle>
<pages>1009--1016</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7490" citStr="Chen et al., 2006" startWordPosition="1136" endWordPosition="1139">hniques and require labeled data. 2.2 Property Name Synonym Discovery Our work is related to the synonym discovery research which aims at identifying groups of words that are semantically identical based on some defined similarity metric. The body of work on this problem can be divided into two major approaches (Agirre et al., 2009): methods that are based on the available knowledge resources (e.g., WordNet, or available taxonomies) (Yang and Powers, 2005; Alvarez and Lim, 2007; Hughes and Ramage, ), and methods that use contextual/property distribution around the words (Pereira et al., 1993; Chen et al., 2006; Sahami and Heilman, 2006; Pantel et al., 2009). (Zhai et al., 2010) propose a constrained semi-supervised learning method using a naive Bayes formulation of EM seeded by a small set of labeled data and a set of soft constraints based on the prior knowledge of the problem. There has been also some recent work on applying topic modeling (e.g., LDA) for solving this problem (Guo et al., 2009). Our work is also related to the existing research on schema matching problem where the objective is to identify objects that are semantically related cross schemas. There has been an extensive study on th</context>
</contexts>
<marker>Chen, Lin, Wei, 2006</marker>
<rawString>Hsin-Hsi Chen, Ming-Shun Lin, and Yu-Chuan Wei. 2006. Novel association measures using web search with double checking. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44, pages 1009– 1016, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Clifton</author>
<author>Ed Housman</author>
<author>Arnon Rosenthal</author>
</authors>
<title>Experience with a combined approach to attributematching across heterogeneous databases. In</title>
<date>1997</date>
<booktitle>In Proc. of the IFIP Working Conference on Data Semantics (DS-7.</booktitle>
<contexts>
<context position="9032" citStr="Clifton et al., 1997" startWordPosition="1384" endWordPosition="1387">neral the work can be classified into rule-based and learningbased approaches. Rule-based systems (Castano and de Antonellis, 1999; Milo and Zohar, 1998; L. Palopol and Ursino, 1998) often utilize only the schema information (e.g., elements, domain types of schema elements, and schema structure) to define a similarity metric for performing matching among the schema elements in a hard coded fashion. In contrast learning based approaches learn a similarity metric based on both the schema information and the data. Earlier learning based systems (Li and Clifton, 2000; Perkowitz and Etzioni, 1995; Clifton et al., 1997) often rely on one type of learning (e.g., schema meta-data, statistics of the data content, properties of the objects shared between the schemas, etc). These systems do not exploit the complete textual information in the data content therefore have limited applicability. Most recent systems attempt to incorporate the textual contents of the data sources into the system. Doan et al. (2001) introduce LSD which is a semi-automatic machine learning based matching framework that trains a set of base learners using a set of user provided semantic mappings over a small data sources. Each base learne</context>
</contexts>
<marker>Clifton, Housman, Rosenthal, 1997</marker>
<rawString>Chris Clifton, Ed Housman, and Arnon Rosenthal. 1997. Experience with a combined approach to attributematching across heterogeneous databases. In In Proc. of the IFIP Working Conference on Data Semantics (DS-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Michael Wick</author>
<author>Robert Hall</author>
<author>Andrew Mccallum</author>
</authors>
<title>First-order probabilistic models for coreference resolution. In</title>
<date>2007</date>
<booktitle>In Proceedings of HLTNAACL</booktitle>
<contexts>
<context position="14283" citStr="Culotta et al., 2007" startWordPosition="2265" endWordPosition="2269">n some special characters and names which are less than 3 characters long. All discovered properties are added to the set of known properties and their popularity counts are updated. Note that for efficiency reasons, Part-Of-Speech (POS) tagging is performed only on sentences containing the anchor of a pattern. The anchor of pattern 1 is the colon sign while the anchor of the other patterns is the known property name KN. We use (Toutanova et al., 2003) for POS tagging. 4 Property Synonym Discovery In this section we briefly overview a probabilistic pairwise property synonym model inspired by (Culotta et al., 2007). 4.1 Probabilistic Model Given a category C, let Xc = {x1, x2,... , x,} be the raw set of n property names (prior to synonym discovery) extracted from a corpus of data associated with that category. Every property name is associated with pairs of values and popularity count (as defined in Section 3) Vxi = {(vij,ci(vij))} j�1, where vij is the jth value associated for the property name xi and ci(vij) is the popularity of value vij. Given a pair of property names xij = {xi, xj}, let the binary random variable yij be 1 if xi and xj are synonyms. Let T = {fk(xij, y)} be a set of features over xij</context>
</contexts>
<marker>Culotta, Wick, Hall, Mccallum, 2007</marker>
<rawString>Aron Culotta, Michael Wick, Robert Hall, and Andrew Mccallum. 2007. First-order probabilistic models for coreference resolution. In In Proceedings of HLTNAACL 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>AnHai Doan</author>
<author>Pedro Domingos</author>
<author>Alon Y Halevy</author>
</authors>
<title>Reconciling schemas of disparate data sources:</title>
<date>2001</date>
<contexts>
<context position="9424" citStr="Doan et al. (2001)" startWordPosition="1449" endWordPosition="1452">shion. In contrast learning based approaches learn a similarity metric based on both the schema information and the data. Earlier learning based systems (Li and Clifton, 2000; Perkowitz and Etzioni, 1995; Clifton et al., 1997) often rely on one type of learning (e.g., schema meta-data, statistics of the data content, properties of the objects shared between the schemas, etc). These systems do not exploit the complete textual information in the data content therefore have limited applicability. Most recent systems attempt to incorporate the textual contents of the data sources into the system. Doan et al. (2001) introduce LSD which is a semi-automatic machine learning based matching framework that trains a set of base learners using a set of user provided semantic mappings over a small data sources. Each base learner exploits a different type of information, e.g. source schema information and information in the data source. Given a new data source, the base learners are used to discover semantic mappings and their prediction is combined using a meta-learner. Similar to LSD, GLUE (Doan et al., 2003) also uses a set of base learners combined into a meta-learner for solving the matching problem between </context>
</contexts>
<marker>Doan, Domingos, Halevy, 2001</marker>
<rawString>AnHai Doan, Pedro Domingos, and Alon Y. Halevy. 2001. Reconciling schemas of disparate data sources:</rawString>
</citation>
<citation valid="false">
<title>a machine-learning approach.</title>
<booktitle>In Proceedings of the 2001 ACM SIGMOD international conference on Management of data, SIGMOD ’01,</booktitle>
<pages>509--520</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker></marker>
<rawString>a machine-learning approach. In Proceedings of the 2001 ACM SIGMOD international conference on Management of data, SIGMOD ’01, pages 509–520, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>AnHai Doan</author>
<author>Jayant Madhavan</author>
<author>Robin Dhamankar</author>
<author>Pedro Domingos</author>
<author>Alon Halevy</author>
</authors>
<title>Learning to match ontologies on the semantic web.</title>
<date>2003</date>
<journal>The VLDB Journal,</journal>
<pages>12--303</pages>
<contexts>
<context position="9920" citStr="Doan et al., 2003" startWordPosition="1533" endWordPosition="1536">y. Most recent systems attempt to incorporate the textual contents of the data sources into the system. Doan et al. (2001) introduce LSD which is a semi-automatic machine learning based matching framework that trains a set of base learners using a set of user provided semantic mappings over a small data sources. Each base learner exploits a different type of information, e.g. source schema information and information in the data source. Given a new data source, the base learners are used to discover semantic mappings and their prediction is combined using a meta-learner. Similar to LSD, GLUE (Doan et al., 2003) also uses a set of base learners combined into a meta-learner for solving the matching problem between two ontologies. Our work is mostly related to (Wick et al., 2008) where they propose a general framework for performing jointly schema matching, co-reference and canonicalization using a supervised machine learning approach. In this approach the matching problem is treated as a clustering problem in the schema attribute space, where a cluster captures a matched set of attributes. A conditional random field (CRF) (Lafferty et al., 2001) is trained using user provided mappings between example </context>
</contexts>
<marker>Doan, Madhavan, Dhamankar, Domingos, Halevy, 2003</marker>
<rawString>AnHai Doan, Jayant Madhavan, Robin Dhamankar, Pedro Domingos, and Alon Halevy. 2003. Learning to match ontologies on the semantic web. The VLDB Journal, 12:303–319, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rayid Ghani</author>
<author>Katharina Probst</author>
<author>Yan Liu</author>
<author>Marko Krema</author>
<author>Andrew Fano</author>
</authors>
<title>Text mining for product attribute extraction.</title>
<date>2006</date>
<journal>SIGKDD Explor. Newsl.,</journal>
<volume>8</volume>
<contexts>
<context position="6808" citStr="Ghani et al., 2006" startWordPosition="1025" endWordPosition="1028">r proximity in the corpus and syntactic function clustering. Unfortunately, this technique cannot be applied to our problem due to the lack of discourse redundancy within item descriptions. (Putthividhya and Hu, 2011) and (Sachan et al., 2011) also addressed the problem of structuring items in the e-commerce domain. However, these works assume that property names are known in advance and focus on discovering values for these properties from very short product titles. Although we are primarily concerned with unsupervised property discovery, it is worth mentioning (Peng and McCallum, 2004) and (Ghani et al., 2006) who approached the problem using supervised machine learning techniques and require labeled data. 2.2 Property Name Synonym Discovery Our work is related to the synonym discovery research which aims at identifying groups of words that are semantically identical based on some defined similarity metric. The body of work on this problem can be divided into two major approaches (Agirre et al., 2009): methods that are based on the available knowledge resources (e.g., WordNet, or available taxonomies) (Yang and Powers, 2005; Alvarez and Lim, 2007; Hughes and Ramage, ), and methods that use contextu</context>
</contexts>
<marker>Ghani, Probst, Liu, Krema, Fano, 2006</marker>
<rawString>Rayid Ghani, Katharina Probst, Yan Liu, Marko Krema, and Andrew Fano. 2006. Text mining for product attribute extraction. SIGKDD Explor. Newsl., 8:41–48, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Honglei Guo</author>
<author>Huijia Zhu</author>
<author>Zhili Guo</author>
<author>XiaoXun Zhang</author>
<author>Zhong Su</author>
</authors>
<title>Product feature categorization with multilevel latent semantic association.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th ACM conference on Information and knowledge management, CIKM ’09,</booktitle>
<pages>1087--1096</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="7884" citStr="Guo et al., 2009" startWordPosition="1207" endWordPosition="1210">s (e.g., WordNet, or available taxonomies) (Yang and Powers, 2005; Alvarez and Lim, 2007; Hughes and Ramage, ), and methods that use contextual/property distribution around the words (Pereira et al., 1993; Chen et al., 2006; Sahami and Heilman, 2006; Pantel et al., 2009). (Zhai et al., 2010) propose a constrained semi-supervised learning method using a naive Bayes formulation of EM seeded by a small set of labeled data and a set of soft constraints based on the prior knowledge of the problem. There has been also some recent work on applying topic modeling (e.g., LDA) for solving this problem (Guo et al., 2009). Our work is also related to the existing research on schema matching problem where the objective is to identify objects that are semantically related cross schemas. There has been an extensive study on the 806 Figure 1: Three examples of item descriptions containing a specification table (left image), a bulleted list (top right) and natural language specifications (bottom right). problem of schema matching (for a comprehensive survey see (Rahm and Bernstein, 2001; Bellahsene et al., 2011; Bernstein et al., 2011)). In general the work can be classified into rule-based and learningbased approa</context>
</contexts>
<marker>Guo, Zhu, Guo, Zhang, Su, 2009</marker>
<rawString>Honglei Guo, Huijia Zhu, Zhili Guo, XiaoXun Zhang, and Zhong Su. 2009. Product feature categorization with multilevel latent semantic association. In Proceedings of the 18th ACM conference on Information and knowledge management, CIKM ’09, pages 1087– 1096, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’04,</booktitle>
<pages>168--177</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="4755" citStr="Hu and Liu, 2004" startWordPosition="709" endWordPosition="712">ured as follows. We first review related work. We then describe the two steps of our approach: 1) unsupervised property discovery and extraction and 2) property name synonym discovery. Finally, we present experimental results on real large-scale e-commerce data. 2 Related Work This section reviews related work for the two components of our method, namely unsupervised property extraction and supervised property name synonym discovery. 2.1 Unsupervised Property Extraction A lot of progress has been accomplished in the area of property discovery from product reviews since the pioneering work by (Hu and Liu, 2004). Most of this work is based on the observation, later formalized as double propagation by (Qiu et al., 2009), that in reviews, opinion words are usually associated with product properties in some ways, and thus product properties can be identified from opinion words and opinion words from properties alternately and iteratively. While (Hu and Liu, 2004) initially used association mining techniques; (Liu et al., 2005) used Part-Of-Speech and supervised rule mining to generate language patterns and identify product properties; (Popescu and Etzioni, 2005) used point wise mutual information betwee</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’04, pages 168– 177, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Thad Hughes</author>
<author>Daniel Ramage</author>
</authors>
<title>Lexical semantic relatedness with random graph walks.</title>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>581--589</pages>
<marker>Hughes, Ramage, </marker>
<rawString>Thad Hughes and Daniel Ramage. Lexical semantic relatedness with random graph walks. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 581–589.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nozomi Kobayashi</author>
<author>Kentaro Inui</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Extracting aspect-evaluation and aspect-of relations in opinion mining.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL.</booktitle>
<contexts>
<context position="5526" citStr="Kobayashi et al., 2007" startWordPosition="830" endWordPosition="833">ually associated with product properties in some ways, and thus product properties can be identified from opinion words and opinion words from properties alternately and iteratively. While (Hu and Liu, 2004) initially used association mining techniques; (Liu et al., 2005) used Part-Of-Speech and supervised rule mining to generate language patterns and identify product properties; (Popescu and Etzioni, 2005) used point wise mutual information between candidate properties and meronymy discriminators; (Zhuang et al., 2006; Qiu et al., 2009) improved on previous work by using dependency parsing; (Kobayashi et al., 2007) mined property-opinion patterns using statistical and contextual cues; (Wang and Wang, 2008) leveraged property-opinion mutual information and linguistic rules to identify infrequent properties; and (Zhang et al., 2010) proposed a ranking scheme to improve double propagation precision. In this paper, we are focusing on extracting properties from product descriptions which do not contain opinion words. In a sense, item properties can be viewed as slots of product templates and our work bears similarities with template induction methods. (Chambers and Jurafsky, 2011) proposed a method for infer</context>
</contexts>
<marker>Kobayashi, Inui, Matsumoto, 2007</marker>
<rawString>Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto. 2007. Extracting aspect-evaluation and aspect-of relations in opinion mining. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Sacc`a L Palopol</author>
<author>D Ursino</author>
</authors>
<title>Semiautomatic, semantic discovery of properties from database schemes.</title>
<date>1998</date>
<booktitle>In Proceedings of the 1998 International Symposium on Database Engineering &amp; Applications,</booktitle>
<pages>244</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="8593" citStr="Palopol and Ursino, 1998" startWordPosition="1316" endWordPosition="1319"> the objective is to identify objects that are semantically related cross schemas. There has been an extensive study on the 806 Figure 1: Three examples of item descriptions containing a specification table (left image), a bulleted list (top right) and natural language specifications (bottom right). problem of schema matching (for a comprehensive survey see (Rahm and Bernstein, 2001; Bellahsene et al., 2011; Bernstein et al., 2011)). In general the work can be classified into rule-based and learningbased approaches. Rule-based systems (Castano and de Antonellis, 1999; Milo and Zohar, 1998; L. Palopol and Ursino, 1998) often utilize only the schema information (e.g., elements, domain types of schema elements, and schema structure) to define a similarity metric for performing matching among the schema elements in a hard coded fashion. In contrast learning based approaches learn a similarity metric based on both the schema information and the data. Earlier learning based systems (Li and Clifton, 2000; Perkowitz and Etzioni, 1995; Clifton et al., 1997) often rely on one type of learning (e.g., schema meta-data, statistics of the data content, properties of the objects shared between the schemas, etc). These sy</context>
</contexts>
<marker>Palopol, Ursino, 1998</marker>
<rawString>D. Sacc`a L. Palopol and D. Ursino. 1998. Semiautomatic, semantic discovery of properties from database schemes. In Proceedings of the 1998 International Symposium on Database Engineering &amp; Applications, pages 244–, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01,</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="10463" citStr="Lafferty et al., 2001" startWordPosition="1619" endWordPosition="1622">tion is combined using a meta-learner. Similar to LSD, GLUE (Doan et al., 2003) also uses a set of base learners combined into a meta-learner for solving the matching problem between two ontologies. Our work is mostly related to (Wick et al., 2008) where they propose a general framework for performing jointly schema matching, co-reference and canonicalization using a supervised machine learning approach. In this approach the matching problem is treated as a clustering problem in the schema attribute space, where a cluster captures a matched set of attributes. A conditional random field (CRF) (Lafferty et al., 2001) is trained using user provided mappings between example schemas, or ontologies. CRF bene807 fits from first order logic features that capture both schema/ontology information as well as textual features in the related data sources. 3 Unsupervised Property Extraction The first step of our solution to structuring ecommerce inventory aims at discovering and extracting relevant properties from items. Our method is unsupervised and requires no prior knowledge of relevant properties or any domain knowledge as it operates the exact same way for all items and categories. It maintains a set of previou</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML ’01, pages 282–289, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-Syan Li</author>
<author>Chris Clifton</author>
</authors>
<title>Semint: a tool for identifying attribute correspondences in heterogeneous databases using neural networks.</title>
<date>2000</date>
<journal>Data Knowl. Eng.,</journal>
<volume>33</volume>
<contexts>
<context position="8980" citStr="Li and Clifton, 2000" startWordPosition="1376" endWordPosition="1379">hsene et al., 2011; Bernstein et al., 2011)). In general the work can be classified into rule-based and learningbased approaches. Rule-based systems (Castano and de Antonellis, 1999; Milo and Zohar, 1998; L. Palopol and Ursino, 1998) often utilize only the schema information (e.g., elements, domain types of schema elements, and schema structure) to define a similarity metric for performing matching among the schema elements in a hard coded fashion. In contrast learning based approaches learn a similarity metric based on both the schema information and the data. Earlier learning based systems (Li and Clifton, 2000; Perkowitz and Etzioni, 1995; Clifton et al., 1997) often rely on one type of learning (e.g., schema meta-data, statistics of the data content, properties of the objects shared between the schemas, etc). These systems do not exploit the complete textual information in the data content therefore have limited applicability. Most recent systems attempt to incorporate the textual contents of the data sources into the system. Doan et al. (2001) introduce LSD which is a semi-automatic machine learning based matching framework that trains a set of base learners using a set of user provided semantic </context>
</contexts>
<marker>Li, Clifton, 2000</marker>
<rawString>Wen-Syan Li and Chris Clifton. 2000. Semint: a tool for identifying attribute correspondences in heterogeneous databases using neural networks. Data Knowl. Eng., 33:49–84, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
<author>Minqing Hu</author>
<author>Junsheng Cheng</author>
</authors>
<title>Opinion observer: analyzing and comparing opinions on the web.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th international conference on World Wide Web, WWW ’05,</booktitle>
<pages>342--351</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="5175" citStr="Liu et al., 2005" startWordPosition="779" endWordPosition="782">ynonym discovery. 2.1 Unsupervised Property Extraction A lot of progress has been accomplished in the area of property discovery from product reviews since the pioneering work by (Hu and Liu, 2004). Most of this work is based on the observation, later formalized as double propagation by (Qiu et al., 2009), that in reviews, opinion words are usually associated with product properties in some ways, and thus product properties can be identified from opinion words and opinion words from properties alternately and iteratively. While (Hu and Liu, 2004) initially used association mining techniques; (Liu et al., 2005) used Part-Of-Speech and supervised rule mining to generate language patterns and identify product properties; (Popescu and Etzioni, 2005) used point wise mutual information between candidate properties and meronymy discriminators; (Zhuang et al., 2006; Qiu et al., 2009) improved on previous work by using dependency parsing; (Kobayashi et al., 2007) mined property-opinion patterns using statistical and contextual cues; (Wang and Wang, 2008) leveraged property-opinion mutual information and linguistic rules to identify infrequent properties; and (Zhang et al., 2010) proposed a ranking scheme to</context>
</contexts>
<marker>Liu, Hu, Cheng, 2005</marker>
<rawString>Bing Liu, Minqing Hu, and Junsheng Cheng. 2005. Opinion observer: analyzing and comparing opinions on the web. In Proceedings of the 14th international conference on World Wide Web, WWW ’05, pages 342–351, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tova Milo</author>
<author>Sagit Zohar</author>
</authors>
<title>Using schema matching to simplify heterogeneous data translation.</title>
<date>1998</date>
<booktitle>In Proceedings of the 24rd International Conference on Very Large Data Bases, VLDB ’98,</booktitle>
<pages>122--133</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="8563" citStr="Milo and Zohar, 1998" startWordPosition="1311" endWordPosition="1314">ma matching problem where the objective is to identify objects that are semantically related cross schemas. There has been an extensive study on the 806 Figure 1: Three examples of item descriptions containing a specification table (left image), a bulleted list (top right) and natural language specifications (bottom right). problem of schema matching (for a comprehensive survey see (Rahm and Bernstein, 2001; Bellahsene et al., 2011; Bernstein et al., 2011)). In general the work can be classified into rule-based and learningbased approaches. Rule-based systems (Castano and de Antonellis, 1999; Milo and Zohar, 1998; L. Palopol and Ursino, 1998) often utilize only the schema information (e.g., elements, domain types of schema elements, and schema structure) to define a similarity metric for performing matching among the schema elements in a hard coded fashion. In contrast learning based approaches learn a similarity metric based on both the schema information and the data. Earlier learning based systems (Li and Clifton, 2000; Perkowitz and Etzioni, 1995; Clifton et al., 1997) often rely on one type of learning (e.g., schema meta-data, statistics of the data content, properties of the objects shared betwe</context>
</contexts>
<marker>Milo, Zohar, 1998</marker>
<rawString>Tova Milo and Sagit Zohar. 1998. Using schema matching to simplify heterogeneous data translation. In Proceedings of the 24rd International Conference on Very Large Data Bases, VLDB ’98, pages 122–133, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
</authors>
<title>Eric Crestan, Arkady Borkovsky, AnaMaria Popescu, and Vishnu Vyas.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 -Volume 2, EMNLP ’09,</booktitle>
<pages>938--947</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Pantel, 2009</marker>
<rawString>Patrick Pantel, Eric Crestan, Arkady Borkovsky, AnaMaria Popescu, and Vishnu Vyas. 2009. Web-scale distributional similarity and entity set expansion. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 -Volume 2, EMNLP ’09, pages 938–947, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Andrew McCallum</author>
</authors>
<title>Accurate information extraction from research papers using conditional random fields.</title>
<date>2004</date>
<booktitle>In HLT-NAACL04,</booktitle>
<pages>329--336</pages>
<contexts>
<context position="6783" citStr="Peng and McCallum, 2004" startWordPosition="1020" endWordPosition="1023">d clustering according to their proximity in the corpus and syntactic function clustering. Unfortunately, this technique cannot be applied to our problem due to the lack of discourse redundancy within item descriptions. (Putthividhya and Hu, 2011) and (Sachan et al., 2011) also addressed the problem of structuring items in the e-commerce domain. However, these works assume that property names are known in advance and focus on discovering values for these properties from very short product titles. Although we are primarily concerned with unsupervised property discovery, it is worth mentioning (Peng and McCallum, 2004) and (Ghani et al., 2006) who approached the problem using supervised machine learning techniques and require labeled data. 2.2 Property Name Synonym Discovery Our work is related to the synonym discovery research which aims at identifying groups of words that are semantically identical based on some defined similarity metric. The body of work on this problem can be divided into two major approaches (Agirre et al., 2009): methods that are based on the available knowledge resources (e.g., WordNet, or available taxonomies) (Yang and Powers, 2005; Alvarez and Lim, 2007; Hughes and Ramage, ), and </context>
</contexts>
<marker>Peng, McCallum, 2004</marker>
<rawString>Fuchun Peng and Andrew McCallum. 2004. Accurate information extraction from research papers using conditional random fields. In HLT-NAACL04, pages 329–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Naftali Tishby</author>
<author>Lillian Lee</author>
</authors>
<title>Distributional clustering of english words.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st annual meeting on Association for Computational Linguistics, ACL ’93,</booktitle>
<pages>183--190</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7471" citStr="Pereira et al., 1993" startWordPosition="1132" endWordPosition="1135">d machine learning techniques and require labeled data. 2.2 Property Name Synonym Discovery Our work is related to the synonym discovery research which aims at identifying groups of words that are semantically identical based on some defined similarity metric. The body of work on this problem can be divided into two major approaches (Agirre et al., 2009): methods that are based on the available knowledge resources (e.g., WordNet, or available taxonomies) (Yang and Powers, 2005; Alvarez and Lim, 2007; Hughes and Ramage, ), and methods that use contextual/property distribution around the words (Pereira et al., 1993; Chen et al., 2006; Sahami and Heilman, 2006; Pantel et al., 2009). (Zhai et al., 2010) propose a constrained semi-supervised learning method using a naive Bayes formulation of EM seeded by a small set of labeled data and a set of soft constraints based on the prior knowledge of the problem. There has been also some recent work on applying topic modeling (e.g., LDA) for solving this problem (Guo et al., 2009). Our work is also related to the existing research on schema matching problem where the objective is to identify objects that are semantically related cross schemas. There has been an ex</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993. Distributional clustering of english words. In Proceedings of the 31st annual meeting on Association for Computational Linguistics, ACL ’93, pages 183–190, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Perkowitz</author>
<author>Oren Etzioni</author>
</authors>
<title>Category translation: learning to understand information on the internet.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th international joint conference on Artificial intelligence -</booktitle>
<volume>1</volume>
<pages>930--936</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="9009" citStr="Perkowitz and Etzioni, 1995" startWordPosition="1380" endWordPosition="1383">rnstein et al., 2011)). In general the work can be classified into rule-based and learningbased approaches. Rule-based systems (Castano and de Antonellis, 1999; Milo and Zohar, 1998; L. Palopol and Ursino, 1998) often utilize only the schema information (e.g., elements, domain types of schema elements, and schema structure) to define a similarity metric for performing matching among the schema elements in a hard coded fashion. In contrast learning based approaches learn a similarity metric based on both the schema information and the data. Earlier learning based systems (Li and Clifton, 2000; Perkowitz and Etzioni, 1995; Clifton et al., 1997) often rely on one type of learning (e.g., schema meta-data, statistics of the data content, properties of the objects shared between the schemas, etc). These systems do not exploit the complete textual information in the data content therefore have limited applicability. Most recent systems attempt to incorporate the textual contents of the data sources into the system. Doan et al. (2001) introduce LSD which is a semi-automatic machine learning based matching framework that trains a set of base learners using a set of user provided semantic mappings over a small data so</context>
</contexts>
<marker>Perkowitz, Etzioni, 1995</marker>
<rawString>Mike Perkowitz and Oren Etzioni. 1995. Category translation: learning to understand information on the internet. In Proceedings of the 14th international joint conference on Artificial intelligence - Volume 1, pages 930–936, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Popescu</author>
<author>Oren Etzioni</author>
</authors>
<title>Extracting product features and opinions from reviews.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05,</booktitle>
<pages>339--346</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5313" citStr="Popescu and Etzioni, 2005" startWordPosition="799" endWordPosition="802">from product reviews since the pioneering work by (Hu and Liu, 2004). Most of this work is based on the observation, later formalized as double propagation by (Qiu et al., 2009), that in reviews, opinion words are usually associated with product properties in some ways, and thus product properties can be identified from opinion words and opinion words from properties alternately and iteratively. While (Hu and Liu, 2004) initially used association mining techniques; (Liu et al., 2005) used Part-Of-Speech and supervised rule mining to generate language patterns and identify product properties; (Popescu and Etzioni, 2005) used point wise mutual information between candidate properties and meronymy discriminators; (Zhuang et al., 2006; Qiu et al., 2009) improved on previous work by using dependency parsing; (Kobayashi et al., 2007) mined property-opinion patterns using statistical and contextual cues; (Wang and Wang, 2008) leveraged property-opinion mutual information and linguistic rules to identify infrequent properties; and (Zhang et al., 2010) proposed a ranking scheme to improve double propagation precision. In this paper, we are focusing on extracting properties from product descriptions which do not cont</context>
</contexts>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>Ana-Maria Popescu and Oren Etzioni. 2005. Extracting product features and opinions from reviews. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05, pages 339–346, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duangmanee Putthividhya</author>
<author>Junling Hu</author>
</authors>
<title>Bootstrapped named entity recognition for product attribute extraction.</title>
<date>2011</date>
<booktitle>In EMNLP,</booktitle>
<pages>1557--1567</pages>
<contexts>
<context position="6406" citStr="Putthividhya and Hu, 2011" startWordPosition="962" endWordPosition="965">ove double propagation precision. In this paper, we are focusing on extracting properties from product descriptions which do not contain opinion words. In a sense, item properties can be viewed as slots of product templates and our work bears similarities with template induction methods. (Chambers and Jurafsky, 2011) proposed a method for inferring event templates based on word clustering according to their proximity in the corpus and syntactic function clustering. Unfortunately, this technique cannot be applied to our problem due to the lack of discourse redundancy within item descriptions. (Putthividhya and Hu, 2011) and (Sachan et al., 2011) also addressed the problem of structuring items in the e-commerce domain. However, these works assume that property names are known in advance and focus on discovering values for these properties from very short product titles. Although we are primarily concerned with unsupervised property discovery, it is worth mentioning (Peng and McCallum, 2004) and (Ghani et al., 2006) who approached the problem using supervised machine learning techniques and require labeled data. 2.2 Property Name Synonym Discovery Our work is related to the synonym discovery research which aim</context>
</contexts>
<marker>Putthividhya, Hu, 2011</marker>
<rawString>Duangmanee Putthividhya and Junling Hu. 2011. Bootstrapped named entity recognition for product attribute extraction. In EMNLP, pages 1557–1567.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guang Qiu</author>
<author>Bing Liu</author>
<author>Jiajun Bu</author>
<author>Chun Chen</author>
</authors>
<title>Expanding domain sentiment lexicon through double propagation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 21st international jont conference on Artifical intelligence, IJCAI’09,</booktitle>
<pages>1199--1204</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="4864" citStr="Qiu et al., 2009" startWordPosition="729" endWordPosition="732">d property discovery and extraction and 2) property name synonym discovery. Finally, we present experimental results on real large-scale e-commerce data. 2 Related Work This section reviews related work for the two components of our method, namely unsupervised property extraction and supervised property name synonym discovery. 2.1 Unsupervised Property Extraction A lot of progress has been accomplished in the area of property discovery from product reviews since the pioneering work by (Hu and Liu, 2004). Most of this work is based on the observation, later formalized as double propagation by (Qiu et al., 2009), that in reviews, opinion words are usually associated with product properties in some ways, and thus product properties can be identified from opinion words and opinion words from properties alternately and iteratively. While (Hu and Liu, 2004) initially used association mining techniques; (Liu et al., 2005) used Part-Of-Speech and supervised rule mining to generate language patterns and identify product properties; (Popescu and Etzioni, 2005) used point wise mutual information between candidate properties and meronymy discriminators; (Zhuang et al., 2006; Qiu et al., 2009) improved on previ</context>
</contexts>
<marker>Qiu, Liu, Bu, Chen, 2009</marker>
<rawString>Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2009. Expanding domain sentiment lexicon through double propagation. In Proceedings of the 21st international jont conference on Artifical intelligence, IJCAI’09, pages 1199–1204, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erhard Rahm</author>
<author>Philip A Bernstein</author>
</authors>
<title>A survey of approaches to automatic schema matching.</title>
<date>2001</date>
<journal>The VLDB Journal,</journal>
<pages>10--334</pages>
<contexts>
<context position="8353" citStr="Rahm and Bernstein, 2001" startWordPosition="1279" endWordPosition="1282">the prior knowledge of the problem. There has been also some recent work on applying topic modeling (e.g., LDA) for solving this problem (Guo et al., 2009). Our work is also related to the existing research on schema matching problem where the objective is to identify objects that are semantically related cross schemas. There has been an extensive study on the 806 Figure 1: Three examples of item descriptions containing a specification table (left image), a bulleted list (top right) and natural language specifications (bottom right). problem of schema matching (for a comprehensive survey see (Rahm and Bernstein, 2001; Bellahsene et al., 2011; Bernstein et al., 2011)). In general the work can be classified into rule-based and learningbased approaches. Rule-based systems (Castano and de Antonellis, 1999; Milo and Zohar, 1998; L. Palopol and Ursino, 1998) often utilize only the schema information (e.g., elements, domain types of schema elements, and schema structure) to define a similarity metric for performing matching among the schema elements in a hard coded fashion. In contrast learning based approaches learn a similarity metric based on both the schema information and the data. Earlier learning based sy</context>
</contexts>
<marker>Rahm, Bernstein, 2001</marker>
<rawString>Erhard Rahm and Philip A. Bernstein. 2001. A survey of approaches to automatic schema matching. The VLDB Journal, 10:334–350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mrinmaya Sachan</author>
<author>Tanveer Faruquie</author>
<author>L V Subramaniam</author>
<author>Mukesh Mohania</author>
</authors>
<title>Using text reviews for product entity completion.</title>
<date>2011</date>
<booktitle>In Poster at the 5th International Joint Conference on Natural Language Processing, IJCNLP’11,</booktitle>
<pages>983--991</pages>
<contexts>
<context position="6432" citStr="Sachan et al., 2011" startWordPosition="967" endWordPosition="970">. In this paper, we are focusing on extracting properties from product descriptions which do not contain opinion words. In a sense, item properties can be viewed as slots of product templates and our work bears similarities with template induction methods. (Chambers and Jurafsky, 2011) proposed a method for inferring event templates based on word clustering according to their proximity in the corpus and syntactic function clustering. Unfortunately, this technique cannot be applied to our problem due to the lack of discourse redundancy within item descriptions. (Putthividhya and Hu, 2011) and (Sachan et al., 2011) also addressed the problem of structuring items in the e-commerce domain. However, these works assume that property names are known in advance and focus on discovering values for these properties from very short product titles. Although we are primarily concerned with unsupervised property discovery, it is worth mentioning (Peng and McCallum, 2004) and (Ghani et al., 2006) who approached the problem using supervised machine learning techniques and require labeled data. 2.2 Property Name Synonym Discovery Our work is related to the synonym discovery research which aims at identifying groups of</context>
</contexts>
<marker>Sachan, Faruquie, Subramaniam, Mohania, 2011</marker>
<rawString>Mrinmaya Sachan, Tanveer Faruquie, L. V. Subramaniam, and Mukesh Mohania. 2011. Using text reviews for product entity completion. In Poster at the 5th International Joint Conference on Natural Language Processing, IJCNLP’11, pages 983–991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehran Sahami</author>
<author>Timothy D Heilman</author>
</authors>
<title>A webbased kernel function for measuring the similarity of short text snippets.</title>
<date>2006</date>
<booktitle>In Proceedings of the 15th international conference on World Wide Web, WWW ’06,</booktitle>
<pages>377--386</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="7516" citStr="Sahami and Heilman, 2006" startWordPosition="1140" endWordPosition="1143"> labeled data. 2.2 Property Name Synonym Discovery Our work is related to the synonym discovery research which aims at identifying groups of words that are semantically identical based on some defined similarity metric. The body of work on this problem can be divided into two major approaches (Agirre et al., 2009): methods that are based on the available knowledge resources (e.g., WordNet, or available taxonomies) (Yang and Powers, 2005; Alvarez and Lim, 2007; Hughes and Ramage, ), and methods that use contextual/property distribution around the words (Pereira et al., 1993; Chen et al., 2006; Sahami and Heilman, 2006; Pantel et al., 2009). (Zhai et al., 2010) propose a constrained semi-supervised learning method using a naive Bayes formulation of EM seeded by a small set of labeled data and a set of soft constraints based on the prior knowledge of the problem. There has been also some recent work on applying topic modeling (e.g., LDA) for solving this problem (Guo et al., 2009). Our work is also related to the existing research on schema matching problem where the objective is to identify objects that are semantically related cross schemas. There has been an extensive study on the 806 Figure 1: Three exam</context>
</contexts>
<marker>Sahami, Heilman, 2006</marker>
<rawString>Mehran Sahami and Timothy D. Heilman. 2006. A webbased kernel function for measuring the similarity of short text snippets. In Proceedings of the 15th international conference on World Wide Web, WWW ’06, pages 377–386, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03,</booktitle>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="14118" citStr="Toutanova et al., 2003" startWordPosition="2240" endWordPosition="2243"> bullets from the beginning of names and punctuation at the end of names and values. Validation rejects properties which names are pure numbers, properties that contain some special characters and names which are less than 3 characters long. All discovered properties are added to the set of known properties and their popularity counts are updated. Note that for efficiency reasons, Part-Of-Speech (POS) tagging is performed only on sentences containing the anchor of a pattern. The anchor of pattern 1 is the colon sign while the anchor of the other patterns is the known property name KN. We use (Toutanova et al., 2003) for POS tagging. 4 Property Synonym Discovery In this section we briefly overview a probabilistic pairwise property synonym model inspired by (Culotta et al., 2007). 4.1 Probabilistic Model Given a category C, let Xc = {x1, x2,... , x,} be the raw set of n property names (prior to synonym discovery) extracted from a corpus of data associated with that category. Every property name is associated with pairs of values and popularity count (as defined in Section 3) Vxi = {(vij,ci(vij))} j�1, where vij is the jth value associated for the property name xi and ci(vij) is the popularity of value vij.</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03, pages 173–180, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Wang</author>
<author>Houfeng Wang</author>
</authors>
<title>Bootstrapping both product features and opinion words from chinese customer reviews with cross-inducing.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third International Joint Conference on Natural Language Processing.</booktitle>
<contexts>
<context position="5619" citStr="Wang and Wang, 2008" startWordPosition="843" endWordPosition="846">fied from opinion words and opinion words from properties alternately and iteratively. While (Hu and Liu, 2004) initially used association mining techniques; (Liu et al., 2005) used Part-Of-Speech and supervised rule mining to generate language patterns and identify product properties; (Popescu and Etzioni, 2005) used point wise mutual information between candidate properties and meronymy discriminators; (Zhuang et al., 2006; Qiu et al., 2009) improved on previous work by using dependency parsing; (Kobayashi et al., 2007) mined property-opinion patterns using statistical and contextual cues; (Wang and Wang, 2008) leveraged property-opinion mutual information and linguistic rules to identify infrequent properties; and (Zhang et al., 2010) proposed a ranking scheme to improve double propagation precision. In this paper, we are focusing on extracting properties from product descriptions which do not contain opinion words. In a sense, item properties can be viewed as slots of product templates and our work bears similarities with template induction methods. (Chambers and Jurafsky, 2011) proposed a method for inferring event templates based on word clustering according to their proximity in the corpus and </context>
</contexts>
<marker>Wang, Wang, 2008</marker>
<rawString>Bo Wang and Houfeng Wang. 2008. Bootstrapping both product features and opinion words from chinese customer reviews with cross-inducing. In Proceedings of the Third International Joint Conference on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael L Wick</author>
<author>Khashayar Rohanimanesh</author>
<author>Karl Schultz</author>
<author>Andrew McCallum</author>
</authors>
<title>A unified approach for schema matching, coreference and canonicalization.</title>
<date>2008</date>
<booktitle>In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’08,</booktitle>
<pages>722--730</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="10089" citStr="Wick et al., 2008" startWordPosition="1564" endWordPosition="1567">e learning based matching framework that trains a set of base learners using a set of user provided semantic mappings over a small data sources. Each base learner exploits a different type of information, e.g. source schema information and information in the data source. Given a new data source, the base learners are used to discover semantic mappings and their prediction is combined using a meta-learner. Similar to LSD, GLUE (Doan et al., 2003) also uses a set of base learners combined into a meta-learner for solving the matching problem between two ontologies. Our work is mostly related to (Wick et al., 2008) where they propose a general framework for performing jointly schema matching, co-reference and canonicalization using a supervised machine learning approach. In this approach the matching problem is treated as a clustering problem in the schema attribute space, where a cluster captures a matched set of attributes. A conditional random field (CRF) (Lafferty et al., 2001) is trained using user provided mappings between example schemas, or ontologies. CRF bene807 fits from first order logic features that capture both schema/ontology information as well as textual features in the related data so</context>
</contexts>
<marker>Wick, Rohanimanesh, Schultz, McCallum, 2008</marker>
<rawString>Michael L. Wick, Khashayar Rohanimanesh, Karl Schultz, and Andrew McCallum. 2008. A unified approach for schema matching, coreference and canonicalization. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’08, pages 722–730, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dongqiang Yang</author>
<author>David M W Powers</author>
</authors>
<title>Measuring semantic similarity in the taxonomy of wordnet.</title>
<date>2005</date>
<booktitle>In Proceedings of the Twenty-eighth Australasian conference on Computer Science - Volume 38, ACSC ’05,</booktitle>
<pages>315--322</pages>
<publisher>Australian Computer Society, Inc.</publisher>
<location>Darlinghurst, Australia, Australia.</location>
<contexts>
<context position="7332" citStr="Yang and Powers, 2005" startWordPosition="1109" endWordPosition="1113">sed property discovery, it is worth mentioning (Peng and McCallum, 2004) and (Ghani et al., 2006) who approached the problem using supervised machine learning techniques and require labeled data. 2.2 Property Name Synonym Discovery Our work is related to the synonym discovery research which aims at identifying groups of words that are semantically identical based on some defined similarity metric. The body of work on this problem can be divided into two major approaches (Agirre et al., 2009): methods that are based on the available knowledge resources (e.g., WordNet, or available taxonomies) (Yang and Powers, 2005; Alvarez and Lim, 2007; Hughes and Ramage, ), and methods that use contextual/property distribution around the words (Pereira et al., 1993; Chen et al., 2006; Sahami and Heilman, 2006; Pantel et al., 2009). (Zhai et al., 2010) propose a constrained semi-supervised learning method using a naive Bayes formulation of EM seeded by a small set of labeled data and a set of soft constraints based on the prior knowledge of the problem. There has been also some recent work on applying topic modeling (e.g., LDA) for solving this problem (Guo et al., 2009). Our work is also related to the existing resea</context>
</contexts>
<marker>Yang, Powers, 2005</marker>
<rawString>Dongqiang Yang and David M. W. Powers. 2005. Measuring semantic similarity in the taxonomy of wordnet. In Proceedings of the Twenty-eighth Australasian conference on Computer Science - Volume 38, ACSC ’05, pages 315–322, Darlinghurst, Australia, Australia. Australian Computer Society, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongwu Zhai</author>
<author>Bing Liu</author>
<author>Hua Xu</author>
<author>Peifa Jia</author>
</authors>
<title>Grouping product features using semi-supervised learning with soft-constraints.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10,</booktitle>
<pages>1272--1280</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7559" citStr="Zhai et al., 2010" startWordPosition="1149" endWordPosition="1152">y Our work is related to the synonym discovery research which aims at identifying groups of words that are semantically identical based on some defined similarity metric. The body of work on this problem can be divided into two major approaches (Agirre et al., 2009): methods that are based on the available knowledge resources (e.g., WordNet, or available taxonomies) (Yang and Powers, 2005; Alvarez and Lim, 2007; Hughes and Ramage, ), and methods that use contextual/property distribution around the words (Pereira et al., 1993; Chen et al., 2006; Sahami and Heilman, 2006; Pantel et al., 2009). (Zhai et al., 2010) propose a constrained semi-supervised learning method using a naive Bayes formulation of EM seeded by a small set of labeled data and a set of soft constraints based on the prior knowledge of the problem. There has been also some recent work on applying topic modeling (e.g., LDA) for solving this problem (Guo et al., 2009). Our work is also related to the existing research on schema matching problem where the objective is to identify objects that are semantically related cross schemas. There has been an extensive study on the 806 Figure 1: Three examples of item descriptions containing a spec</context>
</contexts>
<marker>Zhai, Liu, Xu, Jia, 2010</marker>
<rawString>Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia. 2010. Grouping product features using semi-supervised learning with soft-constraints. In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10, pages 1272–1280, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Zhang</author>
<author>Bing Liu</author>
<author>Suk Hwan Lim</author>
<author>Eamonn O’Brien-Strain</author>
</authors>
<title>Extracting and ranking product features in opinion documents.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10,</booktitle>
<pages>1462--1470</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Zhang, Liu, Lim, O’Brien-Strain, 2010</marker>
<rawString>Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn O’Brien-Strain. 2010. Extracting and ranking product features in opinion documents. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10, pages 1462–1470, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Li Zhuang</author>
<author>Feng Jing</author>
<author>Xiao-Yan Zhu</author>
</authors>
<title>Movie review mining and summarization.</title>
<date>2006</date>
<booktitle>In CIKM ’06: Proceedings of the 15th ACM international conference on Information and knowledge management,</booktitle>
<pages>43--50</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="5427" citStr="Zhuang et al., 2006" startWordPosition="814" endWordPosition="817">r formalized as double propagation by (Qiu et al., 2009), that in reviews, opinion words are usually associated with product properties in some ways, and thus product properties can be identified from opinion words and opinion words from properties alternately and iteratively. While (Hu and Liu, 2004) initially used association mining techniques; (Liu et al., 2005) used Part-Of-Speech and supervised rule mining to generate language patterns and identify product properties; (Popescu and Etzioni, 2005) used point wise mutual information between candidate properties and meronymy discriminators; (Zhuang et al., 2006; Qiu et al., 2009) improved on previous work by using dependency parsing; (Kobayashi et al., 2007) mined property-opinion patterns using statistical and contextual cues; (Wang and Wang, 2008) leveraged property-opinion mutual information and linguistic rules to identify infrequent properties; and (Zhang et al., 2010) proposed a ranking scheme to improve double propagation precision. In this paper, we are focusing on extracting properties from product descriptions which do not contain opinion words. In a sense, item properties can be viewed as slots of product templates and our work bears simi</context>
</contexts>
<marker>Zhuang, Jing, Zhu, 2006</marker>
<rawString>Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006. Movie review mining and summarization. In CIKM ’06: Proceedings of the 15th ACM international conference on Information and knowledge management, pages 43– 50, New York, NY, USA. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>