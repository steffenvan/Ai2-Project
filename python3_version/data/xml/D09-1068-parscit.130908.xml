<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000644">
<title confidence="0.971967">
Improving Web Search Relevance with Semantic Features
</title>
<author confidence="0.900101">
Yumao Lu Fuchun Peng Gilad Mishne Xing Wei Benoit Dumoulin
</author>
<affiliation confidence="0.607415">
Yahoo! Inc.
</affiliation>
<address confidence="0.629424">
701 First Avenue
Sunnyvale, CA, 94089
</address>
<email confidence="0.894123">
yumaol,fuchun,gilad,xwei,benoitd@yahoo-inc.com
</email>
<sectionHeader confidence="0.99103" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999269">
Most existing information retrieval (IR)
systems do not take much advantage of
natural language processing (NLP) tech-
niques due to the complexity and limited
observed effectiveness of applying NLP
to IR. In this paper, we demonstrate that
substantial gains can be obtained over a
strong baseline using NLP techniques, if
properly handled. We propose a frame-
work for deriving semantic text matching
features from named entities identified in
Web queries; we then utilize these features
in a supervised machine-learned ranking
approach, applying a set of emerging ma-
chine learning techniques. Our approach
is especially useful for queries that contain
multiple types of concepts. Comparing to
a major commercial Web search engine,
we observe a substantial 4% DCG5 gain
over the affected queries.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999804137931035">
Most existing IR models score documents pri-
marily based on various term statistics. In tra-
ditional models—from classic probabilistic mod-
els (Croft and Harper, 1979; Fuhr, 1992), through
vector space models (Salton et al., 1975; Narita
and Ogawa, 2000), to well studied statistical lan-
guage models (Ponte and Croft, 2000; Lafferty
and Zhai, 2001)—these term statistics have been
captured directly in the ranking formula. More re-
cently, learning to rank approaches to IR (Fried-
man, 2002) have become prominent; in these
frameworks, that aim at learning a ranking func-
tion from data, term statistics are often modeled
as term matching features in a machine learning
process.
Traditional text matching features are mainly
based on frequencies of n-grams of the user’s
query in a variety of document sections, such as
the document title, body text, anchor text, and so
on. Global information such as frequency of term
or term group in the corpus may also be used, as
well as its combination with local statistics – pro-
ducing relative scores such as tf · idf or BM25
scores (Robertson et al., 1995). Matching may
be restricted to certain window sizes to enforce
proximity, or may be more lenient, allowing un-
ordered sequences and nonconsecutive sequences
for a higher recall.
Even before machine learning was applied to
IR, NLP techniques such as Named Entity Recog-
nition (NER), Part-of-Speech (POS) tagging, and
parsing have been applied to both query model-
ing and document indexing (Smeaton and van Ri-
jsbergen, 1988; Narita and Ogawa, 2000; Sparck-
Jones, 1999). For example, statistical concept
language models generalize classic n-gram mod-
els to concept n-gram model by enforcing query
term proximity within each concept (Srikanth and
Srihari, 2003). However, researchers have of-
ten reported limited gains or even decreased per-
formance when applying NLP to IR (Voorhees,
1999).
Typically, concepts detected through NLP tech-
niques either in the query or in documents are
used as proximity constraints for text match-
ing (Sparck-Jones, 1999), ignoring the actual con-
cept type. The machine learned approach to docu-
ment ranking provides us with an opportunity to
revisit the manner in which NLP information is
used for ranking. Using knowledge gained from
NLP application as features rather than heuris-
tically allows us much greater flexibility in the
amount and variability of information used – e.g.,
incorporating knowledge about the actual entity
types. This has several benefits: first, entity types
appearing in queries are an indicator of the user’s
intent. A query consisting of a business category
and a location (e.g., hotels Palo Alto) appears to be
</bodyText>
<page confidence="0.965459">
648
</page>
<note confidence="0.9966095">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 648–657,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999647098039216">
informational, and perhaps is best answered with
a page containing a list of hotels in Palo Alto.
Queries containing a business name and a location
(e.g., Fuki Sushi Palo Alto) are more navigational
in nature – for many users, the intent is finding the
home page of a specific business. Similarly, entity
types appearing in documents are an indicator of
the document type. For example, if “Palo Alto”
appears ten times in document’s body text, it is
more likely to be a local listing page than a home
page. For the query hotels Palo Alto, a local listing
page may be a good page, while for the query Fuki
Sushi Palo Alto a listing page is not a good page.
In addition, knowledge of the particular entities
in queries allows us to incorporate external knowl-
edge about these entities, such as entity-specific
stopwords (“inc.” as in Yahoo Inc. or “services”
as in kaiser medical service), and so on.
Finally, even when using named entities only
for deriving proximity-related features, we can
benefit from applying different levels of proxim-
ity for different entities. For example, for enti-
ties like cities (e.g., “River Side”), the proximity
requirement is fairly strict: we should not allow
extra words between the original terms, and pre-
serve their order. For other entities the proximity
constraint can be relaxed—for example, for per-
son names, due to the middle name convention:
Hillary Clinton vs. Hillary R. Clinton.
In this paper, we propose a systematic approach
to modeling semantic features, incorporating con-
cept types extracted from query analysis. Ver-
tical attributes, such as city-state relationships,
metropolitan definition, or idf scores from a do-
main specific corpus, are extracted for each con-
cept type from vertical database. The vertical at-
tributes, together with the concept attributes, are
used to compose a set of semantic features for ma-
chine learning based IR models. A few machine
learning techniques are discussed to further im-
prove relevance for subclass of difficult queries
such as queries containing multiple types of con-
cepts. Figure 1 shows an overview of our ap-
proach; after discussing related work in Section 2,
we spend Sections 3 to 5 of the paper describing
the components of our system. We then evaluate
the effectiveness of our approach both using gen-
eral queries and with a set of “difficult” queries;
our results show that the techniques are robust, and
particularly effective for this type of queries. We
conclude in Section 7.
</bodyText>
<figureCaption confidence="0.991834">
Figure 1: Ranking with Semantic Features
</figureCaption>
<sectionHeader confidence="0.999201" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9996398">
There is substantial body of work involving us-
age of NLP techniques to improve information re-
trieval (Brants, 2003; Strzalkowski et al., 1996).
Allan and Ragahavan (Allan and Raghavan, 2002)
use Part-of-Speech tagging to reduce ambiguity
of difficult queries by converting short queries
to questions. In other POS-tags work, Aram-
patzis et al. (Arampatzis et al., 1990) observed
an improvement when using nouns only for re-
trieval. Croft et al. (Croft et al., 1991) and Tong
et al. (Buckley et al., 1993; Tong et al., 1996) ex-
plored phrases and structured queries and found
phrases are effective in improving retrieval per-
formance. Voorhees (Voohees, 1993) uses word
sense disambiguation to improve retrieval perfor-
mance. One IR domain that consistently benefits
from usage of various NLP techniques is question
answering, where queries are formed in natural
language format; e.g., (Peng et al., 2005).
In general, however, researchers often observe
limited gains or even degraded performance when
applying NLP to IR (Voorhees, 1999). Having
said this, most past studies use small datasets and
a modest baseline; it is unclear whether a similar
conclusion would be reached when using a state-
of-art system such as a commercial web search
engine as a baseline, and a full-web corpus – as
we do in this paper. This leads to another differ-
ence between this work and existing work involv-
ing named entity recognition for retrieval. Most
</bodyText>
<figure confidence="0.9993911">
Location
DB
Business Vertical Attribute Query
DB with
Annotations
... Vertical Attribute
Name
DB
Semantic Text Matching Document
Index
Semantic Features
Specialized
Ranking
Module
Specialized
Ranking
Module
Specialized
Ranking
Module
Query
Linguistic
Analysis
Tagger1
Resolution Module
Tagger2
...
Query
Tagger n
Vertical Attribute
</figure>
<page confidence="0.997454">
649
</page>
<bodyText confidence="0.999874888888889">
previous research on usage of named entities in
IR combines entity detection in documents and
queries (Prager et al., 2000). Entity detection in
document has a high indexing cost that is often
overlooked, but cannot be ignored in the case of
commercial search engines. For this reason, we
restrict NLP processing to queries only – although
we believe that document-side NLP processing
will provide additional useful information.
</bodyText>
<sectionHeader confidence="0.958572" genericHeader="method">
3 Query Analysis
</sectionHeader>
<bodyText confidence="0.999989444444445">
We begin by briefly describing our approach to
named entity recognition in web queries, which
serves as the basis for deriving the semantic text
matching features.
Named entity recognition (NER) is the task of
identifying and classifying entities, such as per-
son names or locations, in text. The majority of
state-of-the-art NER methods utilize a statistical
approach, attempting to learn a mapping between
a sequence of observations (words) and a sequence
of tags (entity types). In these methods, the se-
quential nature of the data is often central to the
model, as named entities tend to appear in particu-
lar context in text. For example, for most types of
text, in the two sequences met with X and buy the
Y, the likelihood of X being a person name is sub-
stantially higher than the corresponding likelihood
of Y . Indeed, many named entity taggers perform
well when applied to grammatical text with suf-
ficient contexts, such as newswire text (Sang and
Meulder, 2003).
Web queries, however, tend to be short, with
most queries consisting of 1–3 words, and lack
context – posing a particular challenge for iden-
tifying named entities in them. Existing work on
NER in web queries focuses on tailoring a solu-
tion for a particular entity type and its usage in
web search (Wang et al., 2005; Shen et al., 2008);
in contrast, we aim at identifying a large range
of possible entities in web queries, and using a
generic solution for all of them.
In web queries, different entity types may bene-
fit from different detection techniques. For exam-
ple, an entity type with a large variability among
instances as well as existence of external resources
like product name calls for an approach that can
make use of many features, such as a conditional
random field; for entity types that are more struc-
tured like person names, a grammar-based ap-
proach can be more effective (Shen et al., 2008).
To this end, we utilize multiple approaches for en-
tity detection and combine them into a single, co-
herent “interpretation” of the query.
Given a query, we use several entity recogniz-
ers in parallel, one for each of the common en-
tity types found in web queries. The modeling
types may differ between the recognizers: some
are Markovian models, while others are just dic-
tionary lookups; the accuracy of each recognizer
is also different. We then have a machine-learned
disambiguation module that combines output from
different taggers, ranking the tagging sequences.
The details of scoring is out of the scope of this
paper, and we omit it for simplicity.
</bodyText>
<sectionHeader confidence="0.98562" genericHeader="method">
4 Semantic Text Matching Features
</sectionHeader>
<bodyText confidence="0.9979394">
Our proposed semantic features operate at the
semantic type level rather than at the term level:
instead of matching a term (or set of terms) in doc-
uments, we match their semantic type. Given the
query San Francisco colleges and the annotation
[San Francisco]CityName [colleges]BusinessCategory,
the semantic text matching features would de-
scribe how relevant a document section is for a en-
tity of type CityName, for BusinessCategory,
and for their combination.
Concretely, we exploit a set of features that
attempts to capture proximity, general relevance,
and vertical relevance for each type of semantic
tag and for each section of the document. We now
review these feature by their broad types.
</bodyText>
<subsectionHeader confidence="0.995255">
4.1 Semantic Proximity Features
</subsectionHeader>
<bodyText confidence="0.999914944444444">
Proximity features—features that capture the de-
gree to which search terms appear close to each
other in a document—are among the most impor-
tant feature sets in ranking functions. Traditional
proximity features are typically designed for all
query terms (Metzler and Croft, 2005) and may
suffer from wrong segmentations of the query. For
example, for the query New York city bus char-
ter, a traditional proximity feature may treat “city
bus” similarly to “York city.” But given detailed
information about the entities in the query in their
types, we can enforce proximity for “New York
city” and “bus charter” more accurately. Different
types of entities usually have different proximity
characteristics in relevant documents. Strongly-
bound entities such as city names typically have
very high proximity in relevant documents, while
entities such as business names may have much
</bodyText>
<page confidence="0.987857">
650
</page>
<bodyText confidence="0.999925083333333">
lower proximity: a search for Kaiser medical of-
fice, for example, may be well-served with docu-
ments referring to Kaiser Permanente medical of-
fice, and as we mentioned before, person names
matches may also benefit from lenient proximity
enforcement. This is naturally addressed by treat-
ing each entity type differently.
We propose a set of semantic proximity fea-
tures that associate each semantic tag type with
generic proximity measures. We also consider tag-
ging confidence together with term group proxim-
ity; we discuss these two approaches next.
</bodyText>
<subsectionHeader confidence="0.602098">
4.1.1 Semantic Minimum Coverage (SMC)
</subsectionHeader>
<bodyText confidence="0.97773695">
Minimum Coverage (MC) is a popular span based
proximity distance measure, which is defined as
the length of the shortest document segment that
cover the query term at least once in a docu-
ment (Tao and Zhai, 2007). We extend this mea-
sure to Semantic Minimum Coverage (SMC) for
each semantic type t in document section s and
define it as
where wi is a weight for tagged term group i,
MCi,s is the the minimum coverage of term group
i in document section s, {k|Tk = t} denotes the
set of all concepts having type t, and |{k|Tk = t}|
is the size of the set. The definition of the weight
w is flexible. We list a few candidate weight-
ing schemes in this paper: uniform weights (wu),
weights based on idf scores (widf) and “strength”-
based weight (ws), which we define as follows:
fq
where c is a constant and fq is the frequency of the
term group in a large query log;
</bodyText>
<equation confidence="0.725447">
ws =min
l
</equation>
<bodyText confidence="0.950520666666667">
where MIl is the point-wise mutual information of
the l-th consecutive pair within the semantic tag.
We can also combine strength and idf scores such
that the weight reflects both relative importance
and constraints in proximity. In this paper, we use
df
</bodyText>
<equation confidence="0.57932">
wsi = wswi.
</equation>
<bodyText confidence="0.898159">
In Section 6, we use all four weighting schemes
mentioned above in the semantic feature set.
4.1.2 Semantic Moving Average BM25
(SMABM25)
BM25, a commonly-used bag-of-words relevance
estimation method (Robertson et al., 1995), is de-
fined (when applied to document sections) as
</bodyText>
<equation confidence="0.9992718">
fj,s(c1 + 1)
idfj
ls
fi,s + c1 (1 − c2 + c2 �ls )
dj + c5
</equation>
<bodyText confidence="0.999954833333333">
where dj is the number of sections in all collec-
tions that contains term j and c4, c5 are constants.
To characterize proximity, we could use a fixed
length sliding window and calculate the average
BM25. We further associate each sliding average
BM25 with each type of semantic term groups.
This results in a Semantic Moving Average BM25
(SMABM25) of type t, which we define as fol-
lows:
where m is a fixed length sliding window m and
M is the total number of sliding windows (that de-
pends on the length of the section window size).
</bodyText>
<subsectionHeader confidence="0.997099">
4.2 Semantic Vertical Relevance Features
</subsectionHeader>
<bodyText confidence="0.999989578947369">
Vertical databases contain a large amount of struc-
tured domain knowledge typically discarded by
traditional web relevance features. Having access
to the semantic types in queries, we can tap into
that knowledge to improve accuracy. For exam-
ple, term frequencies in different corpora can as-
sist in determining relevance given an entity type.
As we mentioned in Section 1, we observe that
term frequency in a database of business names
provides an indication of the business brand, the
key part of the business name phrase. While both
“yahoo” and “inc” are very common terms on the
web, in a database of businesses only “inc” is com-
mon enough to be considered a stopword in the
context of business names.
We propose a Vertical Moving Average BM25
(VMABM25) as a feature aiming at quantifying
the vertical knowledge for web search. The ba-
sic idea here is to replace the idf score idfj of
</bodyText>
<equation confidence="0.9851345">
{ l V, wiMCi,s
`k |Tk _ tf  |iE{kjTk=t}
SMCt,s =
1
wu = 1;
widf = c
MIl
�
BM25 =
j
</equation>
<bodyText confidence="0.9991965">
where fj,s is the frequency of term j in section s,
ls is the length of section s, �ls is the average length
of document section s, c1, c2, c3 are constants and
the idf score of term j is defined as
</bodyText>
<equation confidence="0.995323166666667">
c4 − dj + c5
idfj = log ,
|{k|Tk t} |iE{kjTk=t}
1 2_1 (1/M)�
BM25m
m
</equation>
<page confidence="0.976133">
651
</page>
<bodyText confidence="0.997972">
SMABM25 with an idf score calculated from a
vertical database for type t, namely idftj:
where
</bodyText>
<equation confidence="0.991578">
fj,s(c1 + 1)
idft
j fi,s + c1(1 − c2 + c2 ls�ls )
</equation>
<bodyText confidence="0.999880714285714">
where the idftj is associated with the semantic type
t and calculated from the corpus associated with
that type.
VMABM25 links vertical knowledge, proxim-
ity, and page relevance together; we show later that
it is one of most salient features among all seman-
tic features.
</bodyText>
<subsectionHeader confidence="0.988019">
4.3 Generalized Semantic Features
</subsectionHeader>
<bodyText confidence="0.9999373">
Finally, we develop a generalized feature based on
the previous features by removing tags. Semantic
features are often sparse, as many queries contain
one entity or no entities at all; generalized features
increase their coverage by combining the basic se-
mantic features. An entity without tag is essen-
tially a segment.
A segment feature xi for query i does not have
entity type and can be expressed as
where Ki is the number of segments in the query
and T (k) is the semantic type associated with kth
concept.
Although these features are less informative
than type-specific features, one advantage of using
them is that they have substantially higher cover-
age. In our experiments, more than 40% of the
queries have some identified entity. Another rel-
atively subtle advantage is that segment features
have no type related errors: the only possible error
is a mistake in entity boundaries.
</bodyText>
<sectionHeader confidence="0.990328" genericHeader="method">
5 Ranking Function Optimization
</sectionHeader>
<bodyText confidence="0.99985175">
The ultimate goal of the machine learning ap-
proach to web search is to learn a ranking func-
tion h(xi), where xi is a feature vector of a query-
document pair i, such that the error
</bodyText>
<equation confidence="0.9913685">
L(h) _ XN (yi − h(xi))2 (1)
i=1
</equation>
<bodyText confidence="0.999786310344828">
is minimized. Here, yi is the actual relevance score
for the query-document pair i (typically assigned
by a human) and N is the number of training sam-
ples.
As mentioned in the previous Section, an inher-
ent issue with semantic features is their sparse-
ness. User queries are usually short, with an av-
erage length of less than 3 words. Text matching
features that are associated with the semantic type
of query term or term groups are clearly sparse
comparing with traditional, non-entity text match-
ing features – that can be derived for any query.
When a feature is very sparse, it is unlikely that
it would play a very meaningful role in a machine
learned ranking function, since the error L would
largely depend on other samples that do not con-
tain the specific semantic features at all. To over-
come the spareness issue and take advantage of
semantic features, we suggested generalizing our
features; but we also exploit a few ranking func-
tion modeling techniques.
First, we use a “divide-and-conquer” approach.
Long queries usually contain multiple concepts
and could be difficult to retrieve relevant docu-
ments. Semantic features, however, are rich in
this set of queries. We may train special models
to further optimize our ranking function for those
queries. The loss function over ranking function h
becomes
</bodyText>
<equation confidence="0.9842845">
LC(h) _ X (yi − h(xi))2 (2)
iEC
</equation>
<bodyText confidence="0.999977263157895">
where C is the training set that falls into a pre-
defined subclass. For example, queries containing
both location and business name, queries contains
both location and business category, etc, are good
candidates to apply semantic features.
To this end, we first classify queries into several
classes, each of which has multiple types of enti-
ties. The semantic features of those types would
be dense for this subclass of queries. We then
train models that may rank the specific class of
queries well. This approach, however, may suf-
fer from significantly less training samples due to
training data partition resulted from the query clas-
sification. Increasing the modeling accuracy, then,
comes at a cost of reduced data available for train-
ing. We apply two techniques to address this is-
sue. The first approach is to over-weight subclass
training samples such that the subclass of queries
plays a more important role in modeling while still
</bodyText>
<equation confidence="0.986375333333333">
1 Ki
X
k=1
xi = Ki
xT(k)
1 X X
(1/M)
BM25m,t
m
`
|Tk|Tk
=tf  |iE{kjTk=t}
X
BM25m,t =
j
</equation>
<page confidence="0.985482">
652
</page>
<bodyText confidence="0.99997425">
keeping a large pool of the overall training sam-
ples. The second approach is model adaptation:
a generalized incremental learning method. Here,
instead of being over-weighted in a joint optimiza-
tion, the subclass of training data is used to mod-
ify an existing model such that the new model is
“adapted” to the subclass problem. We elaborate
on our approaches as follows.
</bodyText>
<subsectionHeader confidence="0.99523">
5.1 Weighted Training Samples
</subsectionHeader>
<bodyText confidence="0.999955">
To take advantage of both large a pool of training
samples and sparse related semantic features for
a subclass of queries, we could modify the loss
function as follows
</bodyText>
<equation confidence="0.995269444444444">
LwC(h) ≡ w � (yi − h(xi))2 + � (yi − h(xi))2,
iEC iE C¯
(3)
where C is the complement of set C. Here, the
weight w is a compromise between loss function
(1) and (2). When w = 1, we have
L1C(h) ≡ L(h);
when w− &gt; ∞
Lc (h) ≡ LC(h).
</equation>
<bodyText confidence="0.999967">
A large weight may help optimize the training for
a special subclass of queries, and a small weight
may help to preserve good generality of the ranker.
We could use cross-validation to select the weight
w to optimize a the ranking function for a sub-
class of queries. In practice, a small w is desired
to avoid overfitting.
</bodyText>
<subsectionHeader confidence="0.999332">
5.2 Model Adaptation
</subsectionHeader>
<bodyText confidence="0.999831083333333">
Model adaptation is an emerging machine learn-
ing technique that is used for information retrieval
applications with limited amount of training data.
In this paper, we apply Trada, proposed by Chen
et al. (Chen et al., 2008), as our adaptation algo-
rithm.
The Trada algorithm aims at adapting tree-
based models. A popular tree based regression ap-
proach is Gradient Boosting Trees (GBT) , which
is an additive model h(x) = EKk=1 γkhk(x),
where each regression tree hk is sequentially op-
timized with a hill-climbing procedure. As with
other decision trees, a binary regression tree hk(x)
consists of a set of decision nodes; each node is
associated with a feature variable and a splitting
value that partition the data into two parts, with the
corresponding predicted value defined in the leave
node. The basic idea of Trada is to apply piece-
wise linear transformation to the base model based
on the new training data. A set of linear transfor-
mations are applied to each decision node, either
predict or split point or both, such that the new pre-
dict or the split point of a node in a decision tree
satisfies
</bodyText>
<equation confidence="0.809653">
v = (1 − pC)v + pCvC
</equation>
<bodyText confidence="0.999008571428571">
where v� denotes predict or split point of that node
in the base mode and vC denotes predict or split
point of that node using new data set C, and
the weight pC depends on the number of origi-
nal training data and new training data that fall
through the node. For each node, the split or pre-
dict can be estimated by
</bodyText>
<equation confidence="0.988651">
βnC
,
n + βnC
</equation>
<bodyText confidence="0.999938">
where n is the number of training sample of the
base model that fall through the node, nC is the
number of new training sample that fall through
the node, and β is a parameter that can be deter-
mined using cross validation. The parameter β
is used to over-weight new training data, an ap-
proach that is very effective in practice. For new
features that are not included in the base model,
more trees are allowed to be added to incorporate
them.
</bodyText>
<sectionHeader confidence="0.999618" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999996133333333">
We now measure the effectiveness of our proposal,
and answer related questions, through extensive
experimental evaluation. We begin by examining
the effectiveness of features as well as the model-
ing approaches introduced in Section 5 on a par-
ticular class of queries—those with a local intent.
We proceed by evaluating whether if the type asso-
ciated with each entity really matters by compar-
ing results with type dependent semantic features
and segment features. Finally, we examine the ro-
bustness of our features by measuring the change
in the accuracy of our resulting ranking function
when the query analysis is wrong; we do this by
introducing simulated noise into the query analy-
sis results.
</bodyText>
<subsectionHeader confidence="0.927561">
6.1 Dataset
</subsectionHeader>
<bodyText confidence="0.997065">
Our training, validation and test sets are human-
labeled query-document pairs. Each item in the
</bodyText>
<equation confidence="0.97145">
pC =
</equation>
<page confidence="0.990411">
653
</page>
<bodyText confidence="0.999918033333333">
sets consists of a feature vector xi represent-
ing the query and the document, and a judg-
ment score yi assigned by a human. There are
around 600 features in each vector, including both
the newly introduced semantic features and exist-
ing features; features are either query-dependent
ones, document-dependent ones, or query-and-
document-dependent features.
The training set is based on uniformly sampled
Web queries from our query log, and top ranked
documents returned by commercial search engines
for these queries; this set consists of 1.24M query-
document pairs.
We use two additional sets for validation and
testing. One set is based on uniformly sampled
Web queries, and contains 42790 validation sam-
ples and 70320 test samples. The second set is
based on uniformly sampled local queries. By lo-
cal queries, we mean queries that contain at least
two types of semantic tags: a location tag (such
as street, city or state name) and a business tag (a
business name or business category). We refer to
this class of queries “local queries,” as users often
type this kind of queries in local vertical search.
The local query set consists of 11040 validation
samples and 39169 test samples. In the training
set we described above, there are 56299 training
samples out of the 1.24M total number of training
samples that satisfy the definition of local queries.
We call this set local training subset.
</bodyText>
<subsectionHeader confidence="0.9987">
6.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999981428571429">
To evaluate the effectiveness of our semantic
features we use Discounted Cumulative Gain
(DCG) (Jarvelin and Kekalainen, 2000), a widely-
used metric for measuring Web search relevance.
(Jarvelin and Kekalainen, 2000). Given a query
and a ranked list of K documents (K = 5 in our
experiments), the DCG for this query is defined as
</bodyText>
<equation confidence="0.999975">
yi (4)
lo92 (1 + i)
</equation>
<bodyText confidence="0.999951">
where yi E [0,10] is a relevance score for the
document at position i, typically assigned by a hu-
man, where 10 is assigned to the most relevant
documents and 0 to the least relevant ones.
To measure statistical significance, we use the
Wilcoxon test (Wilcoxon, 1945); when the p-value
is below 0.01 we consider a difference to be statis-
tically significant and mark it with a bold font in
the result table.
</bodyText>
<subsectionHeader confidence="0.998993">
6.3 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999976555555556">
We use Stochastic Gradient Boosting Trees
(SGBT) (Friedman, 2002), a robust none linear
regression algorithm, for training ranking func-
tions and, as mentioned earlier, Trada (Chen et al.,
2008) for model adaptation.
Training parameters are selected to optimize the
relevance on a separated validation set. The best
resulting is evaluated against the test set; all results
presented here use the test set for evaluation.
</bodyText>
<subsectionHeader confidence="0.967563">
6.3.1 Feature Effectiveness with Ranking
Function Modeling
</subsectionHeader>
<bodyText confidence="0.999672652173913">
We apply the modeling approaches introduced in
Section 5 to improve feature effectiveness on “dif-
ficult” queries—those more than one entity type;
we evaluate these approaches with the semantic-
feature-rich set, the local query test set. We split
training sets into two parts: one set belongs to the
local queries, the other is the rest. We first weight
the local queries and use the combined dataset as
training data to learn the ranking functions; we
train functions with and without the semantic fea-
tures. We evaluate these functions against the lo-
cal query test set. The results are summarized in
Table 1, where w denotes the weight assigned to
the local training set, bolded numbers are statis-
tically significant result compared to the baseline,
uniformly weighted training data without seman-
tic features (with superscript b). It is interesting
to observe that without semantic features, over-
weighted local training data does not have statis-
tically significant impact on the test performance;
with semantic features, a proper weight over train-
ing samples does improve test performance sub-
stantially.
</bodyText>
<tableCaption confidence="0.828901666666667">
Table 1: Evaluation of Ranking Models Trained
Against Over-weighted Local Queries with Se-
mantic Features on the Local Query Test Set
</tableCaption>
<table confidence="0.814356875">
Weight w/o semantic features w/ semantic features
DCG(5) Impr. DCG(5) Impr.
w = 0 8.09&apos; - 8.25 2.0%
w = 2 8.09 0.02% 8.26 2.1%
w = 4 8.13 0.49% 8.34 3.1%
w = 8 8.13 0.49% 8.42 4.1%
w = 16 8.13 0.49% 8.30 2.6%
w = 32 8.04 −0.60% 8.27 2.2%
</table>
<bodyText confidence="0.99969225">
Next, we use the local query training set as “new
data” in the tree adaptation approach. In tree adap-
tations, all parameters are set to optimize the per-
formance over the local validation set. We com-
</bodyText>
<equation confidence="0.980957">
K
DCG(K) =
i��
</equation>
<page confidence="0.991865">
654
</page>
<bodyText confidence="0.9936782">
pare two major adaptation approaches proposed
in (Chen et al., 2008): adapting predict only and
adapting both predict and split. We use the model
trained with the combined training and uniform
weights as the baseline; results are summarized in
</bodyText>
<tableCaption confidence="0.974381333333333">
Table 2.
Table 3: Type-dependent Semantic Features vs.
Segment Features
</tableCaption>
<table confidence="0.97362325">
Feature set DCG(5)
base + type dependent semantic features 8.23
base + segment features 8.19
base + all semantic features 8.25
</table>
<tableCaption confidence="0.96041">
Table 2: Trada Algorithms with Semantic Features
on Local Query Test Set
</tableCaption>
<table confidence="0.995239333333333">
Ada. Appr. w/o semantic feat. w/ semantic feat.
DCG(5) Impr. DCG(5) Impr.
Combined data 8.09&apos; - 8.25 2.0%
Ada. predict 8.02 −0.1% 8.14 0.6%
Ada. predict 8.00 −0.1% 8.17 1.0%
&amp; split
</table>
<bodyText confidence="0.9991256875">
Comparing Tables 1 and 2, note that using the
combined training data with local query training
samples over-weighted achieves better results than
tree adaption. The latter approach, however, has
the advantage of far less training time, since the
adaptation is over a much smaller local query
training set. With the same hardware, it takes
just a few minutes to train an adaptation model,
while it takes days to train a model over the entire,
combined training data. Considering that massive
model validation tasks are required to select good
training parameters, training many different mod-
els with over a million training samples becomes
prohibitly costly. Applying tree adaptation tech-
niques makes research and prototyping of these
models feasible.
</bodyText>
<subsectionHeader confidence="0.9796765">
6.3.2 Type Dependent Semantic Features vs.
Segment Features
</subsectionHeader>
<bodyText confidence="0.999963692307692">
Our next experiment compares type-dependent
features and segment features, evaluating models
trained with these features against the local query
test set. No special modeling approach is applied
here; results are summarized in Table 3. We ob-
serve that by using type-dependent semantic fea-
tures only, we can achieve as much as by using
all semantic features. Since segment features only
convey proximity information while the base fea-
ture set already contain a systematic set of prox-
imity measures, the improvement through segment
features is not as significant as the the type depen-
dent ones.
</bodyText>
<subsectionHeader confidence="0.943876">
6.3.3 Robustness of Semantic Features
</subsectionHeader>
<bodyText confidence="0.999944375">
Our final set of experiments aims at evaluating the
robustness of our semantic features by introducing
simulated errors to the output of our query analy-
sis. Concretely, we manipulate the precision and
the recall of a specific type of entity tagger, t, on
the training and test set. To decrease the recall of
type t, we uniformly remove a set of a% tags of
type t – preserving precision. To decrease preci-
sion, we uniformly select a set of query segments
(viewing the entity detection as simple segmenta-
tion, as detailed earlier) and assign the semantic
type t to those segments. Since the newly added
term group are selected from query segmentation
results, the introduced errors are rather semantic
type error than boundary error or proximity error.
The total number of newly assigned type t tags are
b% of the original number of type t tags in the
training set. By doing this, we decrease the preci-
sion of type t while keeping the recall of it at the
same level.
Suppose the original tagger achieves precision
p and recall r. By removing a% of tags, we have
estimated precision pˆ and recall rˆ defined as fol-
lows:
</bodyText>
<equation confidence="0.918595">
100r − ar
100 ,
pˆ = p.
</equation>
<bodyText confidence="0.9998055">
By adding b% more term group to this type, we
have estimated precision and recall as
</bodyText>
<equation confidence="0.935280333333333">
100p
100 + bp,
rˆ= r.
</equation>
<bodyText confidence="0.985038857142857">
In the experiment reported here we use BUSI-
NESS NAME as the target semantic type for this ro-
bustness experiment. An editorial test shows that
our tagger achieves 74% precision and 66% recall
based on a random set of human labeled queries
for this entity type. We train ranking models with
various values of a and b. When we reduce the
estimated recall, we evaluate these models against
the local test set since other data are not affected.
The results are summarized in Table 4.
When we reduce the precision, we evaluate the
resulting models against the general test set as
rˆ=
pˆ=
</bodyText>
<page confidence="0.999132">
655
</page>
<tableCaption confidence="0.978015">
Table 4: Relevance with simulated error on local
query test set
</tableCaption>
<table confidence="0.7768114">
a b p r DCG(5) Impr.
0 0 0.74 0.66 8.25 −
10 0 0.74 0.594 8.21 0.48%
20 0 0.74 0.528 8.19 0.72%
40 0 0.74 0.396 8.18 0.85%
</table>
<tableCaption confidence="0.9875585">
Table 5: Search relevance with simulated error for
semantic features on general test set
</tableCaption>
<figure confidence="0.951876125">
a b p r DCG(5) Impr.
0 0 0.74 0.66 10.11 -
0 10 0.689 0.66 10.11 0.00%
0 20 0.645 0.66 10.12 0.10%
0 40 0.571 0.66 10.12 0.10%
0 60 0.513 0.66 10.12 0.10%
0 80 0.465 0.66 10.11 0.00%
0 100 0.425 0.66 10.10 −0.10%
</figure>
<bodyText confidence="0.990426076923077">
simulated errors would virtually affect any sam-
ples with certain probability. Results appear in
Table 5. The results are quite interesting: when
the recall of business name entity decreases, we
observe statistically significant relevance degrada-
tion: if less entities are discovered, search rele-
vance is hit. The experiments with simulated pre-
cision error, however, are less conclusive. One
may note the experiments are conducted over the
general test set. Therefore, it is not clear if the pre-
cision of the NER system really has insignificant
impact on the IR relevance or just the impact is
diluted in a larger test set.
</bodyText>
<subsectionHeader confidence="0.997555">
6.4 Case Analysis
</subsectionHeader>
<bodyText confidence="0.999996897959184">
In this section, we take a close look at a few
cases where our new semantic features help
most and where they fail. For the query sil-
verado ranch in irving texas, with no semantic
features, the ranking function ranks a local
listing page for this business, http://local.
yahoo.com/info-28646193, as the top
document. With semantic features, the ranking
function ranks the business home page: http:
//www.silveradoranchparties.com/
as top URL. Examining the two documents, the
local listing page actually contains much more rel-
evant anchor text, which are the among the most
salient features in traditional ranking models. The
home page, however, contains almost no relevant
anchor text: for a small business home page, this
is not a rare situation. Looking at the semantic
features of these two pages, the highest resolution
of location, the city name “Irving,” appears in the
document body text 19 times in the local listing
page body text, and only 2 times in the home page
body text. The training process learns, then, that
for a query for a local business name (rather than
a business category), home pages—even with
fewer location terms in them—are likely to be
more relevant than a local listing page that usually
contain high frequency location terms.
In some cases, however, our new features do
hurt performance. For the query pa treasur-
ers office, the ranking function with no seman-
tic features ranks the document http://www.
patreasury.org highest, while the one with
semantic features ranks the page http://www.
pikepa.org/treasurer.htm higher. The
latter page is somewhat relevant: it is a treasurer’s
office in Pennsylvania. However, it belongs to a
specific county, which makes it less relevant than
the former page. This is a classic error that we ob-
serve: a mismatch of the intended location area.
While users are looking for state level business,
we provide results of county level. To resolve
this type of error, query analysis and semantic text
matching are no longer enough: here, the rank-
ing function needs to know that Pike County is a
county in Pennsylvania, Milford is a city in Pike
County, and neither are referred to by the user.
Document-side entity recognition, however, may
provide this type of information, helping to ad-
dress this type of errors.
</bodyText>
<sectionHeader confidence="0.98909" genericHeader="conclusions">
7 Conclusion and Future Research
</sectionHeader>
<bodyText confidence="0.999857578947369">
In this paper, we investigate how semantic features
can improve search relevance in a large-scale in-
formation retrieval setting; to our knowledge, it is
the first study of this approach on a web scale. We
present a set of features that incorporate semantic
and vertical knowledge into the retrieval process,
propose techniques to handle the sparseness prob-
lem for these features, and describe how they fit
in the learning process. We demonstrate that these
carefully designed features significantly improve
relevance, particularly for difficult queries – long
queries with multiple entities.
The work reported here focuses on query-side
processing, avoiding the indexing cost of docu-
ment processing. We are currently investigating
document-side analysis to complement the query-
side work, and believe that this will further boost
the retrieval accuracy; we hope to report on this in
a follow-up study.
</bodyText>
<page confidence="0.998819">
656
</page>
<sectionHeader confidence="0.99585" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999961846938776">
J. Allan and H. Raghavan. 2002. Using Part-of-Speech
Patterns to Reduce Query Ambiguity. In Proceed-
ings of SIGIR.
A. T. Arampatzis, Th. P. Weide, C. H. A. Koster,
and P. Bommel. 1990. Text Filtering using
Linguistically-motivated Indexing Terms. Techni-
cal Report CSI-R9901, Computing Science Institute,
University of Nijmegen, Nijmegen,The Netherlands.
Thorsten Brants. 2003. Natural Language Processing
in Information Retrieval. In Proceedings of CLIN.
C. Buckley, J. Allan, and G. Salton. 1993. Auto-
matic Routing and Ad-hoc Retrieval Using SMART:
TREC 2. In Proceedings of TREC-2.
Keke Chen, Rongqing Lu, C.K. Wong, Gordon Sun,
Larry Heck, and Belle Tseng. 2008. Trada: tree
based ranking function adaptation. In Proceedings
ofCIKM.
W.B. Croft and D.J. Harper. 1979. Using Probabilistic
Models of Document Retrieval without Relevance
Information. Journal of Documentation, 37:285–
295.
W. Bruce Croft, Howard R. Turtle, and David D. Lewis.
1991. The Use of Phrases and Structured queries in
Information Retrieval. In Proceedings of SIGIR.
J. H. Friedman. 2002. Stochastic Gradient Boost-
ing. Computational Statistics and Data Analysis,
38(4):367–378.
N. Fuhr. 1992. Probabilistic Models in Information
Retrieval. The Computer Journal, 35:243–255.
K. Jarvelin and J. Kekalainen. 2000. IR Evalua-
tion Methods for Retrieving Highly Relevant Doc-
uments. In Proceedings of SIGIR.
J. Lafferty and C. Zhai. 2001. Document Language
Models, Query Models and Risk Minimization for
Information Retrieval. In Proceedings of SIGIR.
Donald Metzler and W. Bruce Croft. 2005. A markov
random field model for term dependencies. In Pro-
ceedings of SIGIR.
M. Narita and Y. Ogawa. 2000. The Use of Phrases
from Query Texts in Information Retrieval. In Pro-
ceedings of SIGIR.
Fuchun Peng, Ralph Weischedel, Ana Licuanan, and
Jinxi Xu. 2005. Combining Deep Linguistics
Analysis and Surface Pattern Learning: A Hybrid
Approach to Chinese Definitional Question Answer-
ing. In In Proceedings of the HLT-EMNLP.
J. Ponte and W.B. Croft. 2000. A Language Modeling
Approach to Informaiton Retrieval. In Proceedings
of SIGIR.
John Prager, Eric Brown, Anni Coden, and Dragomir
Radev. 2000. Question-answering by Predictive
Annotation. In Proceedings of SIGIR.
Stephen Robertson, Steve Walker, Susan Jones, Miche-
line Hancock-Beaulieu, and Mike Gatford. 1995.
Okapi at TREC-3. In Proceedings of TREC-3.
G. Salton, C. S. Yang, and C. T. Yu. 1975. A Theory of
Term Importance in Automatic Text Analysis. Jour-
nal of the Ameican Society of Information Science,
26:33–44.
Erik Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the CoNLL-2003 Shared Task:
Language-Independent Named Entity Recognition.
In Proceedings of CoNLL.
Dou Shen, Toby Walkery, Zijian Zheng, Qiang Yang,
and Ying Li. 2008. Personal Name Classification in
Web Queries. In Proceedings of WSDM.
A.F. Smeaton and C.J. van Rijsbergen. 1988. Experi-
ments on Incorporating Syntactic Processing of User
Queries into a Document Retrieval Stragegy. In Pro-
ceedings of SIGIR.
K. Sparck-Jones, 1999. What is the Role ofNLP in Text
Retrieval, pages 1–25. Kluwer.
M. Srikanth and R.K. Srihari. 2003. Incorporating
Query Term Dependencies in Language Models for
Document Retrieval. In Proceedings of SIGIR.
Tomek Strzalkowski, Jose Perez-Carballo, Jussi Karl-
gren, Anette Hulth, Pasi Tapanainen, and Timo
Lahtinen. 1996. Natural Language Information Re-
trieval: TREC-8 Report. In Proceedings of TREC-8.
Tao Tao and ChengXiang Zhai. 2007. An exploration
of proximity measures in information retrieval. In
Proceedings of SIGIR.
X. Tong, C. Zhai, N. Millic-Frayling, and D Evans.
1996. Evaluation of Syntactic Phrase Indexing –
CLARIT NLP Track Report. In Proceedings of
TREC-5.
E. Voohees. 1993. Using WordNet to Disambiguate
Word Senses for Text Retrieval. In Proceedings of
SIGIR.
Ellen Voorhees. 1999. Natural Language Processing
and Information Retrieval. Lecture Notes in Com-
puter Science, 1714:32–48.
Lee Wang, Chuang Wang, Xing Xie, Josh Forman,
Yansheng Lu, Wei-Ying Ma, and Ying Li. 2005.
Detecting dominant locations from search queries.
In Proceedings of SIGIR.
F. Wilcoxon. 1945. Individual Comparisons by Rank-
ing Methods. Biometrics, 1:80–83.
</reference>
<page confidence="0.998029">
657
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.587659">
<title confidence="0.999915">Improving Web Search Relevance with Semantic Features</title>
<author confidence="0.993394">Yumao Lu Fuchun Peng Gilad Mishne Xing Wei Benoit</author>
<affiliation confidence="0.882718">Yahoo!</affiliation>
<address confidence="0.8116905">701 First Sunnyvale, CA,</address>
<email confidence="0.999365">yumaol,fuchun,gilad,xwei,benoitd@yahoo-inc.com</email>
<abstract confidence="0.999724523809524">Most existing information retrieval (IR) systems do not take much advantage of natural language processing (NLP) techniques due to the complexity and limited observed effectiveness of applying NLP to IR. In this paper, we demonstrate that substantial gains can be obtained over a strong baseline using NLP techniques, if properly handled. We propose a framework for deriving semantic text matching features from named entities identified in Web queries; we then utilize these features in a supervised machine-learned ranking approach, applying a set of emerging machine learning techniques. Our approach is especially useful for queries that contain multiple types of concepts. Comparing to a major commercial Web search engine, we observe a substantial 4% DCG5 gain over the affected queries.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allan</author>
<author>H Raghavan</author>
</authors>
<title>Using Part-of-Speech Patterns to Reduce Query Ambiguity.</title>
<date>2002</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="6553" citStr="Allan and Raghavan, 2002" startWordPosition="1044" endWordPosition="1047"> approach; after discussing related work in Section 2, we spend Sections 3 to 5 of the paper describing the components of our system. We then evaluate the effectiveness of our approach both using general queries and with a set of “difficult” queries; our results show that the techniques are robust, and particularly effective for this type of queries. We conclude in Section 7. Figure 1: Ranking with Semantic Features 2 Related Work There is substantial body of work involving usage of NLP techniques to improve information retrieval (Brants, 2003; Strzalkowski et al., 1996). Allan and Ragahavan (Allan and Raghavan, 2002) use Part-of-Speech tagging to reduce ambiguity of difficult queries by converting short queries to questions. In other POS-tags work, Arampatzis et al. (Arampatzis et al., 1990) observed an improvement when using nouns only for retrieval. Croft et al. (Croft et al., 1991) and Tong et al. (Buckley et al., 1993; Tong et al., 1996) explored phrases and structured queries and found phrases are effective in improving retrieval performance. Voorhees (Voohees, 1993) uses word sense disambiguation to improve retrieval performance. One IR domain that consistently benefits from usage of various NLP tec</context>
</contexts>
<marker>Allan, Raghavan, 2002</marker>
<rawString>J. Allan and H. Raghavan. 2002. Using Part-of-Speech Patterns to Reduce Query Ambiguity. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Weide</author>
<author>C H A Koster</author>
<author>P Bommel</author>
</authors>
<title>Text Filtering using Linguistically-motivated Indexing Terms.</title>
<date>1990</date>
<tech>Technical Report CSI-R9901,</tech>
<institution>Computing Science Institute, University of Nijmegen, Nijmegen,The Netherlands.</institution>
<marker>Weide, Koster, Bommel, 1990</marker>
<rawString>A. T. Arampatzis, Th. P. Weide, C. H. A. Koster, and P. Bommel. 1990. Text Filtering using Linguistically-motivated Indexing Terms. Technical Report CSI-R9901, Computing Science Institute, University of Nijmegen, Nijmegen,The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>Natural Language Processing in Information Retrieval.</title>
<date>2003</date>
<booktitle>In Proceedings of CLIN.</booktitle>
<contexts>
<context position="6477" citStr="Brants, 2003" startWordPosition="1035" endWordPosition="1036">g multiple types of concepts. Figure 1 shows an overview of our approach; after discussing related work in Section 2, we spend Sections 3 to 5 of the paper describing the components of our system. We then evaluate the effectiveness of our approach both using general queries and with a set of “difficult” queries; our results show that the techniques are robust, and particularly effective for this type of queries. We conclude in Section 7. Figure 1: Ranking with Semantic Features 2 Related Work There is substantial body of work involving usage of NLP techniques to improve information retrieval (Brants, 2003; Strzalkowski et al., 1996). Allan and Ragahavan (Allan and Raghavan, 2002) use Part-of-Speech tagging to reduce ambiguity of difficult queries by converting short queries to questions. In other POS-tags work, Arampatzis et al. (Arampatzis et al., 1990) observed an improvement when using nouns only for retrieval. Croft et al. (Croft et al., 1991) and Tong et al. (Buckley et al., 1993; Tong et al., 1996) explored phrases and structured queries and found phrases are effective in improving retrieval performance. Voorhees (Voohees, 1993) uses word sense disambiguation to improve retrieval perform</context>
</contexts>
<marker>Brants, 2003</marker>
<rawString>Thorsten Brants. 2003. Natural Language Processing in Information Retrieval. In Proceedings of CLIN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Buckley</author>
<author>J Allan</author>
<author>G Salton</author>
</authors>
<title>Automatic Routing and Ad-hoc Retrieval Using SMART: TREC 2.</title>
<date>1993</date>
<booktitle>In Proceedings of TREC-2.</booktitle>
<contexts>
<context position="6864" citStr="Buckley et al., 1993" startWordPosition="1096" endWordPosition="1099"> effective for this type of queries. We conclude in Section 7. Figure 1: Ranking with Semantic Features 2 Related Work There is substantial body of work involving usage of NLP techniques to improve information retrieval (Brants, 2003; Strzalkowski et al., 1996). Allan and Ragahavan (Allan and Raghavan, 2002) use Part-of-Speech tagging to reduce ambiguity of difficult queries by converting short queries to questions. In other POS-tags work, Arampatzis et al. (Arampatzis et al., 1990) observed an improvement when using nouns only for retrieval. Croft et al. (Croft et al., 1991) and Tong et al. (Buckley et al., 1993; Tong et al., 1996) explored phrases and structured queries and found phrases are effective in improving retrieval performance. Voorhees (Voohees, 1993) uses word sense disambiguation to improve retrieval performance. One IR domain that consistently benefits from usage of various NLP techniques is question answering, where queries are formed in natural language format; e.g., (Peng et al., 2005). In general, however, researchers often observe limited gains or even degraded performance when applying NLP to IR (Voorhees, 1999). Having said this, most past studies use small datasets and a modest </context>
</contexts>
<marker>Buckley, Allan, Salton, 1993</marker>
<rawString>C. Buckley, J. Allan, and G. Salton. 1993. Automatic Routing and Ad-hoc Retrieval Using SMART: TREC 2. In Proceedings of TREC-2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keke Chen</author>
<author>Rongqing Lu</author>
<author>C K Wong</author>
<author>Gordon Sun</author>
<author>Larry Heck</author>
<author>Belle Tseng</author>
</authors>
<title>Trada: tree based ranking function adaptation.</title>
<date>2008</date>
<booktitle>In Proceedings ofCIKM.</booktitle>
<contexts>
<context position="21946" citStr="Chen et al., 2008" startWordPosition="3679" endWordPosition="3682">we have L1C(h) ≡ L(h); when w− &gt; ∞ Lc (h) ≡ LC(h). A large weight may help optimize the training for a special subclass of queries, and a small weight may help to preserve good generality of the ranker. We could use cross-validation to select the weight w to optimize a the ranking function for a subclass of queries. In practice, a small w is desired to avoid overfitting. 5.2 Model Adaptation Model adaptation is an emerging machine learning technique that is used for information retrieval applications with limited amount of training data. In this paper, we apply Trada, proposed by Chen et al. (Chen et al., 2008), as our adaptation algorithm. The Trada algorithm aims at adapting treebased models. A popular tree based regression approach is Gradient Boosting Trees (GBT) , which is an additive model h(x) = EKk=1 γkhk(x), where each regression tree hk is sequentially optimized with a hill-climbing procedure. As with other decision trees, a binary regression tree hk(x) consists of a set of decision nodes; each node is associated with a feature variable and a splitting value that partition the data into two parts, with the corresponding predicted value defined in the leave node. The basic idea of Trada is </context>
<context position="26837" citStr="Chen et al., 2008" startWordPosition="4526" endWordPosition="4529">yi E [0,10] is a relevance score for the document at position i, typically assigned by a human, where 10 is assigned to the most relevant documents and 0 to the least relevant ones. To measure statistical significance, we use the Wilcoxon test (Wilcoxon, 1945); when the p-value is below 0.01 we consider a difference to be statistically significant and mark it with a bold font in the result table. 6.3 Experimental Results We use Stochastic Gradient Boosting Trees (SGBT) (Friedman, 2002), a robust none linear regression algorithm, for training ranking functions and, as mentioned earlier, Trada (Chen et al., 2008) for model adaptation. Training parameters are selected to optimize the relevance on a separated validation set. The best resulting is evaluated against the test set; all results presented here use the test set for evaluation. 6.3.1 Feature Effectiveness with Ranking Function Modeling We apply the modeling approaches introduced in Section 5 to improve feature effectiveness on “difficult” queries—those more than one entity type; we evaluate these approaches with the semanticfeature-rich set, the local query test set. We split training sets into two parts: one set belongs to the local queries, t</context>
<context position="28882" citStr="Chen et al., 2008" startWordPosition="4870" endWordPosition="4873">Trained Against Over-weighted Local Queries with Semantic Features on the Local Query Test Set Weight w/o semantic features w/ semantic features DCG(5) Impr. DCG(5) Impr. w = 0 8.09&apos; - 8.25 2.0% w = 2 8.09 0.02% 8.26 2.1% w = 4 8.13 0.49% 8.34 3.1% w = 8 8.13 0.49% 8.42 4.1% w = 16 8.13 0.49% 8.30 2.6% w = 32 8.04 −0.60% 8.27 2.2% Next, we use the local query training set as “new data” in the tree adaptation approach. In tree adaptations, all parameters are set to optimize the performance over the local validation set. We comK DCG(K) = i�� 654 pare two major adaptation approaches proposed in (Chen et al., 2008): adapting predict only and adapting both predict and split. We use the model trained with the combined training and uniform weights as the baseline; results are summarized in Table 2. Table 3: Type-dependent Semantic Features vs. Segment Features Feature set DCG(5) base + type dependent semantic features 8.23 base + segment features 8.19 base + all semantic features 8.25 Table 2: Trada Algorithms with Semantic Features on Local Query Test Set Ada. Appr. w/o semantic feat. w/ semantic feat. DCG(5) Impr. DCG(5) Impr. Combined data 8.09&apos; - 8.25 2.0% Ada. predict 8.02 −0.1% 8.14 0.6% Ada. predict</context>
</contexts>
<marker>Chen, Lu, Wong, Sun, Heck, Tseng, 2008</marker>
<rawString>Keke Chen, Rongqing Lu, C.K. Wong, Gordon Sun, Larry Heck, and Belle Tseng. 2008. Trada: tree based ranking function adaptation. In Proceedings ofCIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W B Croft</author>
<author>D J Harper</author>
</authors>
<title>Using Probabilistic Models of Document Retrieval without Relevance Information.</title>
<date>1979</date>
<journal>Journal of Documentation,</journal>
<volume>37</volume>
<pages>295</pages>
<contexts>
<context position="1191" citStr="Croft and Harper, 1979" startWordPosition="172" endWordPosition="175">a framework for deriving semantic text matching features from named entities identified in Web queries; we then utilize these features in a supervised machine-learned ranking approach, applying a set of emerging machine learning techniques. Our approach is especially useful for queries that contain multiple types of concepts. Comparing to a major commercial Web search engine, we observe a substantial 4% DCG5 gain over the affected queries. 1 Introduction Most existing IR models score documents primarily based on various term statistics. In traditional models—from classic probabilistic models (Croft and Harper, 1979; Fuhr, 1992), through vector space models (Salton et al., 1975; Narita and Ogawa, 2000), to well studied statistical language models (Ponte and Croft, 2000; Lafferty and Zhai, 2001)—these term statistics have been captured directly in the ranking formula. More recently, learning to rank approaches to IR (Friedman, 2002) have become prominent; in these frameworks, that aim at learning a ranking function from data, term statistics are often modeled as term matching features in a machine learning process. Traditional text matching features are mainly based on frequencies of n-grams of the user’s</context>
</contexts>
<marker>Croft, Harper, 1979</marker>
<rawString>W.B. Croft and D.J. Harper. 1979. Using Probabilistic Models of Document Retrieval without Relevance Information. Journal of Documentation, 37:285– 295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Bruce Croft</author>
<author>Howard R Turtle</author>
<author>David D Lewis</author>
</authors>
<title>The Use of Phrases and Structured queries in Information Retrieval.</title>
<date>1991</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="6826" citStr="Croft et al., 1991" startWordPosition="1088" endWordPosition="1091">chniques are robust, and particularly effective for this type of queries. We conclude in Section 7. Figure 1: Ranking with Semantic Features 2 Related Work There is substantial body of work involving usage of NLP techniques to improve information retrieval (Brants, 2003; Strzalkowski et al., 1996). Allan and Ragahavan (Allan and Raghavan, 2002) use Part-of-Speech tagging to reduce ambiguity of difficult queries by converting short queries to questions. In other POS-tags work, Arampatzis et al. (Arampatzis et al., 1990) observed an improvement when using nouns only for retrieval. Croft et al. (Croft et al., 1991) and Tong et al. (Buckley et al., 1993; Tong et al., 1996) explored phrases and structured queries and found phrases are effective in improving retrieval performance. Voorhees (Voohees, 1993) uses word sense disambiguation to improve retrieval performance. One IR domain that consistently benefits from usage of various NLP techniques is question answering, where queries are formed in natural language format; e.g., (Peng et al., 2005). In general, however, researchers often observe limited gains or even degraded performance when applying NLP to IR (Voorhees, 1999). Having said this, most past st</context>
</contexts>
<marker>Croft, Turtle, Lewis, 1991</marker>
<rawString>W. Bruce Croft, Howard R. Turtle, and David D. Lewis. 1991. The Use of Phrases and Structured queries in Information Retrieval. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Friedman</author>
</authors>
<date>2002</date>
<booktitle>Stochastic Gradient Boosting. Computational Statistics and Data Analysis,</booktitle>
<pages>38--4</pages>
<contexts>
<context position="1513" citStr="Friedman, 2002" startWordPosition="224" endWordPosition="226">omparing to a major commercial Web search engine, we observe a substantial 4% DCG5 gain over the affected queries. 1 Introduction Most existing IR models score documents primarily based on various term statistics. In traditional models—from classic probabilistic models (Croft and Harper, 1979; Fuhr, 1992), through vector space models (Salton et al., 1975; Narita and Ogawa, 2000), to well studied statistical language models (Ponte and Croft, 2000; Lafferty and Zhai, 2001)—these term statistics have been captured directly in the ranking formula. More recently, learning to rank approaches to IR (Friedman, 2002) have become prominent; in these frameworks, that aim at learning a ranking function from data, term statistics are often modeled as term matching features in a machine learning process. Traditional text matching features are mainly based on frequencies of n-grams of the user’s query in a variety of document sections, such as the document title, body text, anchor text, and so on. Global information such as frequency of term or term group in the corpus may also be used, as well as its combination with local statistics – producing relative scores such as tf · idf or BM25 scores (Robertson et al.</context>
<context position="26709" citStr="Friedman, 2002" startWordPosition="4508" endWordPosition="4509"> and a ranked list of K documents (K = 5 in our experiments), the DCG for this query is defined as yi (4) lo92 (1 + i) where yi E [0,10] is a relevance score for the document at position i, typically assigned by a human, where 10 is assigned to the most relevant documents and 0 to the least relevant ones. To measure statistical significance, we use the Wilcoxon test (Wilcoxon, 1945); when the p-value is below 0.01 we consider a difference to be statistically significant and mark it with a bold font in the result table. 6.3 Experimental Results We use Stochastic Gradient Boosting Trees (SGBT) (Friedman, 2002), a robust none linear regression algorithm, for training ranking functions and, as mentioned earlier, Trada (Chen et al., 2008) for model adaptation. Training parameters are selected to optimize the relevance on a separated validation set. The best resulting is evaluated against the test set; all results presented here use the test set for evaluation. 6.3.1 Feature Effectiveness with Ranking Function Modeling We apply the modeling approaches introduced in Section 5 to improve feature effectiveness on “difficult” queries—those more than one entity type; we evaluate these approaches with the se</context>
</contexts>
<marker>Friedman, 2002</marker>
<rawString>J. H. Friedman. 2002. Stochastic Gradient Boosting. Computational Statistics and Data Analysis, 38(4):367–378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Fuhr</author>
</authors>
<title>Probabilistic Models in Information Retrieval.</title>
<date>1992</date>
<journal>The Computer Journal,</journal>
<pages>35--243</pages>
<contexts>
<context position="1204" citStr="Fuhr, 1992" startWordPosition="176" endWordPosition="177"> semantic text matching features from named entities identified in Web queries; we then utilize these features in a supervised machine-learned ranking approach, applying a set of emerging machine learning techniques. Our approach is especially useful for queries that contain multiple types of concepts. Comparing to a major commercial Web search engine, we observe a substantial 4% DCG5 gain over the affected queries. 1 Introduction Most existing IR models score documents primarily based on various term statistics. In traditional models—from classic probabilistic models (Croft and Harper, 1979; Fuhr, 1992), through vector space models (Salton et al., 1975; Narita and Ogawa, 2000), to well studied statistical language models (Ponte and Croft, 2000; Lafferty and Zhai, 2001)—these term statistics have been captured directly in the ranking formula. More recently, learning to rank approaches to IR (Friedman, 2002) have become prominent; in these frameworks, that aim at learning a ranking function from data, term statistics are often modeled as term matching features in a machine learning process. Traditional text matching features are mainly based on frequencies of n-grams of the user’s query in a v</context>
</contexts>
<marker>Fuhr, 1992</marker>
<rawString>N. Fuhr. 1992. Probabilistic Models in Information Retrieval. The Computer Journal, 35:243–255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Jarvelin</author>
<author>J Kekalainen</author>
</authors>
<title>IR Evaluation Methods for Retrieving Highly Relevant Documents.</title>
<date>2000</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="25990" citStr="Jarvelin and Kekalainen, 2000" startWordPosition="4378" endWordPosition="4381">r state name) and a business tag (a business name or business category). We refer to this class of queries “local queries,” as users often type this kind of queries in local vertical search. The local query set consists of 11040 validation samples and 39169 test samples. In the training set we described above, there are 56299 training samples out of the 1.24M total number of training samples that satisfy the definition of local queries. We call this set local training subset. 6.2 Evaluation Metrics To evaluate the effectiveness of our semantic features we use Discounted Cumulative Gain (DCG) (Jarvelin and Kekalainen, 2000), a widelyused metric for measuring Web search relevance. (Jarvelin and Kekalainen, 2000). Given a query and a ranked list of K documents (K = 5 in our experiments), the DCG for this query is defined as yi (4) lo92 (1 + i) where yi E [0,10] is a relevance score for the document at position i, typically assigned by a human, where 10 is assigned to the most relevant documents and 0 to the least relevant ones. To measure statistical significance, we use the Wilcoxon test (Wilcoxon, 1945); when the p-value is below 0.01 we consider a difference to be statistically significant and mark it with a bo</context>
</contexts>
<marker>Jarvelin, Kekalainen, 2000</marker>
<rawString>K. Jarvelin and J. Kekalainen. 2000. IR Evaluation Methods for Retrieving Highly Relevant Documents. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>C Zhai</author>
</authors>
<title>Document Language Models, Query Models and Risk Minimization for Information Retrieval.</title>
<date>2001</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="1373" citStr="Lafferty and Zhai, 2001" startWordPosition="201" endWordPosition="204">ach, applying a set of emerging machine learning techniques. Our approach is especially useful for queries that contain multiple types of concepts. Comparing to a major commercial Web search engine, we observe a substantial 4% DCG5 gain over the affected queries. 1 Introduction Most existing IR models score documents primarily based on various term statistics. In traditional models—from classic probabilistic models (Croft and Harper, 1979; Fuhr, 1992), through vector space models (Salton et al., 1975; Narita and Ogawa, 2000), to well studied statistical language models (Ponte and Croft, 2000; Lafferty and Zhai, 2001)—these term statistics have been captured directly in the ranking formula. More recently, learning to rank approaches to IR (Friedman, 2002) have become prominent; in these frameworks, that aim at learning a ranking function from data, term statistics are often modeled as term matching features in a machine learning process. Traditional text matching features are mainly based on frequencies of n-grams of the user’s query in a variety of document sections, such as the document title, body text, anchor text, and so on. Global information such as frequency of term or term group in the corpus may </context>
</contexts>
<marker>Lafferty, Zhai, 2001</marker>
<rawString>J. Lafferty and C. Zhai. 2001. Document Language Models, Query Models and Risk Minimization for Information Retrieval. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Metzler</author>
<author>W Bruce Croft</author>
</authors>
<title>A markov random field model for term dependencies.</title>
<date>2005</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="12152" citStr="Metzler and Croft, 2005" startWordPosition="1955" endWordPosition="1958"> a entity of type CityName, for BusinessCategory, and for their combination. Concretely, we exploit a set of features that attempts to capture proximity, general relevance, and vertical relevance for each type of semantic tag and for each section of the document. We now review these feature by their broad types. 4.1 Semantic Proximity Features Proximity features—features that capture the degree to which search terms appear close to each other in a document—are among the most important feature sets in ranking functions. Traditional proximity features are typically designed for all query terms (Metzler and Croft, 2005) and may suffer from wrong segmentations of the query. For example, for the query New York city bus charter, a traditional proximity feature may treat “city bus” similarly to “York city.” But given detailed information about the entities in the query in their types, we can enforce proximity for “New York city” and “bus charter” more accurately. Different types of entities usually have different proximity characteristics in relevant documents. Stronglybound entities such as city names typically have very high proximity in relevant documents, while entities such as business names may have much 6</context>
</contexts>
<marker>Metzler, Croft, 2005</marker>
<rawString>Donald Metzler and W. Bruce Croft. 2005. A markov random field model for term dependencies. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Narita</author>
<author>Y Ogawa</author>
</authors>
<title>The Use of Phrases from Query Texts in Information Retrieval.</title>
<date>2000</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="1279" citStr="Narita and Ogawa, 2000" startWordPosition="186" endWordPosition="189"> in Web queries; we then utilize these features in a supervised machine-learned ranking approach, applying a set of emerging machine learning techniques. Our approach is especially useful for queries that contain multiple types of concepts. Comparing to a major commercial Web search engine, we observe a substantial 4% DCG5 gain over the affected queries. 1 Introduction Most existing IR models score documents primarily based on various term statistics. In traditional models—from classic probabilistic models (Croft and Harper, 1979; Fuhr, 1992), through vector space models (Salton et al., 1975; Narita and Ogawa, 2000), to well studied statistical language models (Ponte and Croft, 2000; Lafferty and Zhai, 2001)—these term statistics have been captured directly in the ranking formula. More recently, learning to rank approaches to IR (Friedman, 2002) have become prominent; in these frameworks, that aim at learning a ranking function from data, term statistics are often modeled as term matching features in a machine learning process. Traditional text matching features are mainly based on frequencies of n-grams of the user’s query in a variety of document sections, such as the document title, body text, anchor </context>
<context position="2563" citStr="Narita and Ogawa, 2000" startWordPosition="398" endWordPosition="401">or term group in the corpus may also be used, as well as its combination with local statistics – producing relative scores such as tf · idf or BM25 scores (Robertson et al., 1995). Matching may be restricted to certain window sizes to enforce proximity, or may be more lenient, allowing unordered sequences and nonconsecutive sequences for a higher recall. Even before machine learning was applied to IR, NLP techniques such as Named Entity Recognition (NER), Part-of-Speech (POS) tagging, and parsing have been applied to both query modeling and document indexing (Smeaton and van Rijsbergen, 1988; Narita and Ogawa, 2000; SparckJones, 1999). For example, statistical concept language models generalize classic n-gram models to concept n-gram model by enforcing query term proximity within each concept (Srikanth and Srihari, 2003). However, researchers have often reported limited gains or even decreased performance when applying NLP to IR (Voorhees, 1999). Typically, concepts detected through NLP techniques either in the query or in documents are used as proximity constraints for text matching (Sparck-Jones, 1999), ignoring the actual concept type. The machine learned approach to document ranking provides us with</context>
</contexts>
<marker>Narita, Ogawa, 2000</marker>
<rawString>M. Narita and Y. Ogawa. 2000. The Use of Phrases from Query Texts in Information Retrieval. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Ralph Weischedel</author>
<author>Ana Licuanan</author>
<author>Jinxi Xu</author>
</authors>
<title>Combining Deep Linguistics Analysis and Surface Pattern Learning: A Hybrid Approach to Chinese Definitional Question Answering. In</title>
<date>2005</date>
<booktitle>In Proceedings of the HLT-EMNLP.</booktitle>
<contexts>
<context position="7262" citStr="Peng et al., 2005" startWordPosition="1156" endWordPosition="1159">eries to questions. In other POS-tags work, Arampatzis et al. (Arampatzis et al., 1990) observed an improvement when using nouns only for retrieval. Croft et al. (Croft et al., 1991) and Tong et al. (Buckley et al., 1993; Tong et al., 1996) explored phrases and structured queries and found phrases are effective in improving retrieval performance. Voorhees (Voohees, 1993) uses word sense disambiguation to improve retrieval performance. One IR domain that consistently benefits from usage of various NLP techniques is question answering, where queries are formed in natural language format; e.g., (Peng et al., 2005). In general, however, researchers often observe limited gains or even degraded performance when applying NLP to IR (Voorhees, 1999). Having said this, most past studies use small datasets and a modest baseline; it is unclear whether a similar conclusion would be reached when using a stateof-art system such as a commercial web search engine as a baseline, and a full-web corpus – as we do in this paper. This leads to another difference between this work and existing work involving named entity recognition for retrieval. Most Location DB Business Vertical Attribute Query DB with Annotations ... </context>
</contexts>
<marker>Peng, Weischedel, Licuanan, Xu, 2005</marker>
<rawString>Fuchun Peng, Ralph Weischedel, Ana Licuanan, and Jinxi Xu. 2005. Combining Deep Linguistics Analysis and Surface Pattern Learning: A Hybrid Approach to Chinese Definitional Question Answering. In In Proceedings of the HLT-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ponte</author>
<author>W B Croft</author>
</authors>
<title>A Language Modeling Approach to Informaiton Retrieval.</title>
<date>2000</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="1347" citStr="Ponte and Croft, 2000" startWordPosition="197" endWordPosition="200">e-learned ranking approach, applying a set of emerging machine learning techniques. Our approach is especially useful for queries that contain multiple types of concepts. Comparing to a major commercial Web search engine, we observe a substantial 4% DCG5 gain over the affected queries. 1 Introduction Most existing IR models score documents primarily based on various term statistics. In traditional models—from classic probabilistic models (Croft and Harper, 1979; Fuhr, 1992), through vector space models (Salton et al., 1975; Narita and Ogawa, 2000), to well studied statistical language models (Ponte and Croft, 2000; Lafferty and Zhai, 2001)—these term statistics have been captured directly in the ranking formula. More recently, learning to rank approaches to IR (Friedman, 2002) have become prominent; in these frameworks, that aim at learning a ranking function from data, term statistics are often modeled as term matching features in a machine learning process. Traditional text matching features are mainly based on frequencies of n-grams of the user’s query in a variety of document sections, such as the document title, body text, anchor text, and so on. Global information such as frequency of term or ter</context>
</contexts>
<marker>Ponte, Croft, 2000</marker>
<rawString>J. Ponte and W.B. Croft. 2000. A Language Modeling Approach to Informaiton Retrieval. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Prager</author>
<author>Eric Brown</author>
<author>Anni Coden</author>
<author>Dragomir Radev</author>
</authors>
<title>Question-answering by Predictive Annotation.</title>
<date>2000</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="8251" citStr="Prager et al., 2000" startWordPosition="1309" endWordPosition="1312">web corpus – as we do in this paper. This leads to another difference between this work and existing work involving named entity recognition for retrieval. Most Location DB Business Vertical Attribute Query DB with Annotations ... Vertical Attribute Name DB Semantic Text Matching Document Index Semantic Features Specialized Ranking Module Specialized Ranking Module Specialized Ranking Module Query Linguistic Analysis Tagger1 Resolution Module Tagger2 ... Query Tagger n Vertical Attribute 649 previous research on usage of named entities in IR combines entity detection in documents and queries (Prager et al., 2000). Entity detection in document has a high indexing cost that is often overlooked, but cannot be ignored in the case of commercial search engines. For this reason, we restrict NLP processing to queries only – although we believe that document-side NLP processing will provide additional useful information. 3 Query Analysis We begin by briefly describing our approach to named entity recognition in web queries, which serves as the basis for deriving the semantic text matching features. Named entity recognition (NER) is the task of identifying and classifying entities, such as person names or locat</context>
</contexts>
<marker>Prager, Brown, Coden, Radev, 2000</marker>
<rawString>John Prager, Eric Brown, Anni Coden, and Dragomir Radev. 2000. Question-answering by Predictive Annotation. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Robertson</author>
<author>Steve Walker</author>
<author>Susan Jones</author>
<author>Micheline Hancock-Beaulieu</author>
<author>Mike Gatford</author>
</authors>
<title>Okapi at TREC-3.</title>
<date>1995</date>
<booktitle>In Proceedings of TREC-3.</booktitle>
<contexts>
<context position="2120" citStr="Robertson et al., 1995" startWordPosition="328" endWordPosition="331">(Friedman, 2002) have become prominent; in these frameworks, that aim at learning a ranking function from data, term statistics are often modeled as term matching features in a machine learning process. Traditional text matching features are mainly based on frequencies of n-grams of the user’s query in a variety of document sections, such as the document title, body text, anchor text, and so on. Global information such as frequency of term or term group in the corpus may also be used, as well as its combination with local statistics – producing relative scores such as tf · idf or BM25 scores (Robertson et al., 1995). Matching may be restricted to certain window sizes to enforce proximity, or may be more lenient, allowing unordered sequences and nonconsecutive sequences for a higher recall. Even before machine learning was applied to IR, NLP techniques such as Named Entity Recognition (NER), Part-of-Speech (POS) tagging, and parsing have been applied to both query modeling and document indexing (Smeaton and van Rijsbergen, 1988; Narita and Ogawa, 2000; SparckJones, 1999). For example, statistical concept language models generalize classic n-gram models to concept n-gram model by enforcing query term proxi</context>
<context position="14707" citStr="Robertson et al., 1995" startWordPosition="2388" endWordPosition="2391">s), which we define as follows: fq where c is a constant and fq is the frequency of the term group in a large query log; ws =min l where MIl is the point-wise mutual information of the l-th consecutive pair within the semantic tag. We can also combine strength and idf scores such that the weight reflects both relative importance and constraints in proximity. In this paper, we use df wsi = wswi. In Section 6, we use all four weighting schemes mentioned above in the semantic feature set. 4.1.2 Semantic Moving Average BM25 (SMABM25) BM25, a commonly-used bag-of-words relevance estimation method (Robertson et al., 1995), is defined (when applied to document sections) as fj,s(c1 + 1) idfj ls fi,s + c1 (1 − c2 + c2 �ls ) dj + c5 where dj is the number of sections in all collections that contains term j and c4, c5 are constants. To characterize proximity, we could use a fixed length sliding window and calculate the average BM25. We further associate each sliding average BM25 with each type of semantic term groups. This results in a Semantic Moving Average BM25 (SMABM25) of type t, which we define as follows: where m is a fixed length sliding window m and M is the total number of sliding windows (that depends on</context>
</contexts>
<marker>Robertson, Walker, Jones, Hancock-Beaulieu, Gatford, 1995</marker>
<rawString>Stephen Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford. 1995. Okapi at TREC-3. In Proceedings of TREC-3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C S Yang</author>
<author>C T Yu</author>
</authors>
<title>A Theory of Term Importance in Automatic Text Analysis.</title>
<date>1975</date>
<journal>Journal of the Ameican Society of Information Science,</journal>
<pages>26--33</pages>
<contexts>
<context position="1254" citStr="Salton et al., 1975" startWordPosition="182" endWordPosition="185">d entities identified in Web queries; we then utilize these features in a supervised machine-learned ranking approach, applying a set of emerging machine learning techniques. Our approach is especially useful for queries that contain multiple types of concepts. Comparing to a major commercial Web search engine, we observe a substantial 4% DCG5 gain over the affected queries. 1 Introduction Most existing IR models score documents primarily based on various term statistics. In traditional models—from classic probabilistic models (Croft and Harper, 1979; Fuhr, 1992), through vector space models (Salton et al., 1975; Narita and Ogawa, 2000), to well studied statistical language models (Ponte and Croft, 2000; Lafferty and Zhai, 2001)—these term statistics have been captured directly in the ranking formula. More recently, learning to rank approaches to IR (Friedman, 2002) have become prominent; in these frameworks, that aim at learning a ranking function from data, term statistics are often modeled as term matching features in a machine learning process. Traditional text matching features are mainly based on frequencies of n-grams of the user’s query in a variety of document sections, such as the document </context>
</contexts>
<marker>Salton, Yang, Yu, 1975</marker>
<rawString>G. Salton, C. S. Yang, and C. T. Yu. 1975. A Theory of Term Importance in Automatic Text Analysis. Journal of the Ameican Society of Information Science, 26:33–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Tjong Kim Sang</author>
<author>Fien De Meulder</author>
</authors>
<title>Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<marker>Sang, De Meulder, 2003</marker>
<rawString>Erik Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dou Shen</author>
<author>Toby Walkery</author>
<author>Zijian Zheng</author>
<author>Qiang Yang</author>
<author>Ying Li</author>
</authors>
<title>Personal Name Classification in Web Queries.</title>
<date>2008</date>
<booktitle>In Proceedings of WSDM.</booktitle>
<contexts>
<context position="9880" citStr="Shen et al., 2008" startWordPosition="1584" endWordPosition="1587">t with X and buy the Y, the likelihood of X being a person name is substantially higher than the corresponding likelihood of Y . Indeed, many named entity taggers perform well when applied to grammatical text with sufficient contexts, such as newswire text (Sang and Meulder, 2003). Web queries, however, tend to be short, with most queries consisting of 1–3 words, and lack context – posing a particular challenge for identifying named entities in them. Existing work on NER in web queries focuses on tailoring a solution for a particular entity type and its usage in web search (Wang et al., 2005; Shen et al., 2008); in contrast, we aim at identifying a large range of possible entities in web queries, and using a generic solution for all of them. In web queries, different entity types may benefit from different detection techniques. For example, an entity type with a large variability among instances as well as existence of external resources like product name calls for an approach that can make use of many features, such as a conditional random field; for entity types that are more structured like person names, a grammar-based approach can be more effective (Shen et al., 2008). To this end, we utilize m</context>
</contexts>
<marker>Shen, Walkery, Zheng, Yang, Li, 2008</marker>
<rawString>Dou Shen, Toby Walkery, Zijian Zheng, Qiang Yang, and Ying Li. 2008. Personal Name Classification in Web Queries. In Proceedings of WSDM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F Smeaton</author>
<author>C J van Rijsbergen</author>
</authors>
<title>Experiments on Incorporating Syntactic Processing of User Queries into a Document Retrieval Stragegy.</title>
<date>1988</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<marker>Smeaton, van Rijsbergen, 1988</marker>
<rawString>A.F. Smeaton and C.J. van Rijsbergen. 1988. Experiments on Incorporating Syntactic Processing of User Queries into a Document Retrieval Stragegy. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sparck-Jones</author>
</authors>
<title>What is the Role ofNLP in Text Retrieval,</title>
<date>1999</date>
<pages>1--25</pages>
<publisher>Kluwer.</publisher>
<contexts>
<context position="3062" citStr="Sparck-Jones, 1999" startWordPosition="475" endWordPosition="476">have been applied to both query modeling and document indexing (Smeaton and van Rijsbergen, 1988; Narita and Ogawa, 2000; SparckJones, 1999). For example, statistical concept language models generalize classic n-gram models to concept n-gram model by enforcing query term proximity within each concept (Srikanth and Srihari, 2003). However, researchers have often reported limited gains or even decreased performance when applying NLP to IR (Voorhees, 1999). Typically, concepts detected through NLP techniques either in the query or in documents are used as proximity constraints for text matching (Sparck-Jones, 1999), ignoring the actual concept type. The machine learned approach to document ranking provides us with an opportunity to revisit the manner in which NLP information is used for ranking. Using knowledge gained from NLP application as features rather than heuristically allows us much greater flexibility in the amount and variability of information used – e.g., incorporating knowledge about the actual entity types. This has several benefits: first, entity types appearing in queries are an indicator of the user’s intent. A query consisting of a business category and a location (e.g., hotels Palo Al</context>
</contexts>
<marker>Sparck-Jones, 1999</marker>
<rawString>K. Sparck-Jones, 1999. What is the Role ofNLP in Text Retrieval, pages 1–25. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Srikanth</author>
<author>R K Srihari</author>
</authors>
<title>Incorporating Query Term Dependencies in Language Models for Document Retrieval.</title>
<date>2003</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="2773" citStr="Srikanth and Srihari, 2003" startWordPosition="428" endWordPosition="431">ed to certain window sizes to enforce proximity, or may be more lenient, allowing unordered sequences and nonconsecutive sequences for a higher recall. Even before machine learning was applied to IR, NLP techniques such as Named Entity Recognition (NER), Part-of-Speech (POS) tagging, and parsing have been applied to both query modeling and document indexing (Smeaton and van Rijsbergen, 1988; Narita and Ogawa, 2000; SparckJones, 1999). For example, statistical concept language models generalize classic n-gram models to concept n-gram model by enforcing query term proximity within each concept (Srikanth and Srihari, 2003). However, researchers have often reported limited gains or even decreased performance when applying NLP to IR (Voorhees, 1999). Typically, concepts detected through NLP techniques either in the query or in documents are used as proximity constraints for text matching (Sparck-Jones, 1999), ignoring the actual concept type. The machine learned approach to document ranking provides us with an opportunity to revisit the manner in which NLP information is used for ranking. Using knowledge gained from NLP application as features rather than heuristically allows us much greater flexibility in the am</context>
</contexts>
<marker>Srikanth, Srihari, 2003</marker>
<rawString>M. Srikanth and R.K. Srihari. 2003. Incorporating Query Term Dependencies in Language Models for Document Retrieval. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomek Strzalkowski</author>
<author>Jose Perez-Carballo</author>
<author>Jussi Karlgren</author>
</authors>
<title>Anette Hulth, Pasi Tapanainen, and Timo Lahtinen.</title>
<date>1996</date>
<booktitle>In Proceedings of TREC-8.</booktitle>
<contexts>
<context position="6505" citStr="Strzalkowski et al., 1996" startWordPosition="1037" endWordPosition="1040">es of concepts. Figure 1 shows an overview of our approach; after discussing related work in Section 2, we spend Sections 3 to 5 of the paper describing the components of our system. We then evaluate the effectiveness of our approach both using general queries and with a set of “difficult” queries; our results show that the techniques are robust, and particularly effective for this type of queries. We conclude in Section 7. Figure 1: Ranking with Semantic Features 2 Related Work There is substantial body of work involving usage of NLP techniques to improve information retrieval (Brants, 2003; Strzalkowski et al., 1996). Allan and Ragahavan (Allan and Raghavan, 2002) use Part-of-Speech tagging to reduce ambiguity of difficult queries by converting short queries to questions. In other POS-tags work, Arampatzis et al. (Arampatzis et al., 1990) observed an improvement when using nouns only for retrieval. Croft et al. (Croft et al., 1991) and Tong et al. (Buckley et al., 1993; Tong et al., 1996) explored phrases and structured queries and found phrases are effective in improving retrieval performance. Voorhees (Voohees, 1993) uses word sense disambiguation to improve retrieval performance. One IR domain that con</context>
</contexts>
<marker>Strzalkowski, Perez-Carballo, Karlgren, 1996</marker>
<rawString>Tomek Strzalkowski, Jose Perez-Carballo, Jussi Karlgren, Anette Hulth, Pasi Tapanainen, and Timo Lahtinen. 1996. Natural Language Information Retrieval: TREC-8 Report. In Proceedings of TREC-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Tao</author>
<author>ChengXiang Zhai</author>
</authors>
<title>An exploration of proximity measures in information retrieval.</title>
<date>2007</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="13553" citStr="Tao and Zhai, 2007" startWordPosition="2180" endWordPosition="2183">on names matches may also benefit from lenient proximity enforcement. This is naturally addressed by treating each entity type differently. We propose a set of semantic proximity features that associate each semantic tag type with generic proximity measures. We also consider tagging confidence together with term group proximity; we discuss these two approaches next. 4.1.1 Semantic Minimum Coverage (SMC) Minimum Coverage (MC) is a popular span based proximity distance measure, which is defined as the length of the shortest document segment that cover the query term at least once in a document (Tao and Zhai, 2007). We extend this measure to Semantic Minimum Coverage (SMC) for each semantic type t in document section s and define it as where wi is a weight for tagged term group i, MCi,s is the the minimum coverage of term group i in document section s, {k|Tk = t} denotes the set of all concepts having type t, and |{k|Tk = t}| is the size of the set. The definition of the weight w is flexible. We list a few candidate weighting schemes in this paper: uniform weights (wu), weights based on idf scores (widf) and “strength”- based weight (ws), which we define as follows: fq where c is a constant and fq is th</context>
</contexts>
<marker>Tao, Zhai, 2007</marker>
<rawString>Tao Tao and ChengXiang Zhai. 2007. An exploration of proximity measures in information retrieval. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Tong</author>
<author>C Zhai</author>
<author>N Millic-Frayling</author>
<author>D Evans</author>
</authors>
<date>1996</date>
<booktitle>Evaluation of Syntactic Phrase Indexing – CLARIT NLP Track Report. In Proceedings of TREC-5.</booktitle>
<contexts>
<context position="6884" citStr="Tong et al., 1996" startWordPosition="1100" endWordPosition="1103">pe of queries. We conclude in Section 7. Figure 1: Ranking with Semantic Features 2 Related Work There is substantial body of work involving usage of NLP techniques to improve information retrieval (Brants, 2003; Strzalkowski et al., 1996). Allan and Ragahavan (Allan and Raghavan, 2002) use Part-of-Speech tagging to reduce ambiguity of difficult queries by converting short queries to questions. In other POS-tags work, Arampatzis et al. (Arampatzis et al., 1990) observed an improvement when using nouns only for retrieval. Croft et al. (Croft et al., 1991) and Tong et al. (Buckley et al., 1993; Tong et al., 1996) explored phrases and structured queries and found phrases are effective in improving retrieval performance. Voorhees (Voohees, 1993) uses word sense disambiguation to improve retrieval performance. One IR domain that consistently benefits from usage of various NLP techniques is question answering, where queries are formed in natural language format; e.g., (Peng et al., 2005). In general, however, researchers often observe limited gains or even degraded performance when applying NLP to IR (Voorhees, 1999). Having said this, most past studies use small datasets and a modest baseline; it is uncl</context>
</contexts>
<marker>Tong, Zhai, Millic-Frayling, Evans, 1996</marker>
<rawString>X. Tong, C. Zhai, N. Millic-Frayling, and D Evans. 1996. Evaluation of Syntactic Phrase Indexing – CLARIT NLP Track Report. In Proceedings of TREC-5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Voohees</author>
</authors>
<title>Using WordNet to Disambiguate Word Senses for Text Retrieval.</title>
<date>1993</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="7017" citStr="Voohees, 1993" startWordPosition="1121" endWordPosition="1122">ing usage of NLP techniques to improve information retrieval (Brants, 2003; Strzalkowski et al., 1996). Allan and Ragahavan (Allan and Raghavan, 2002) use Part-of-Speech tagging to reduce ambiguity of difficult queries by converting short queries to questions. In other POS-tags work, Arampatzis et al. (Arampatzis et al., 1990) observed an improvement when using nouns only for retrieval. Croft et al. (Croft et al., 1991) and Tong et al. (Buckley et al., 1993; Tong et al., 1996) explored phrases and structured queries and found phrases are effective in improving retrieval performance. Voorhees (Voohees, 1993) uses word sense disambiguation to improve retrieval performance. One IR domain that consistently benefits from usage of various NLP techniques is question answering, where queries are formed in natural language format; e.g., (Peng et al., 2005). In general, however, researchers often observe limited gains or even degraded performance when applying NLP to IR (Voorhees, 1999). Having said this, most past studies use small datasets and a modest baseline; it is unclear whether a similar conclusion would be reached when using a stateof-art system such as a commercial web search engine as a baselin</context>
</contexts>
<marker>Voohees, 1993</marker>
<rawString>E. Voohees. 1993. Using WordNet to Disambiguate Word Senses for Text Retrieval. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Voorhees</author>
</authors>
<date>1999</date>
<booktitle>Natural Language Processing and Information Retrieval. Lecture Notes in Computer Science,</booktitle>
<pages>1714--32</pages>
<contexts>
<context position="2900" citStr="Voorhees, 1999" startWordPosition="450" endWordPosition="451">gher recall. Even before machine learning was applied to IR, NLP techniques such as Named Entity Recognition (NER), Part-of-Speech (POS) tagging, and parsing have been applied to both query modeling and document indexing (Smeaton and van Rijsbergen, 1988; Narita and Ogawa, 2000; SparckJones, 1999). For example, statistical concept language models generalize classic n-gram models to concept n-gram model by enforcing query term proximity within each concept (Srikanth and Srihari, 2003). However, researchers have often reported limited gains or even decreased performance when applying NLP to IR (Voorhees, 1999). Typically, concepts detected through NLP techniques either in the query or in documents are used as proximity constraints for text matching (Sparck-Jones, 1999), ignoring the actual concept type. The machine learned approach to document ranking provides us with an opportunity to revisit the manner in which NLP information is used for ranking. Using knowledge gained from NLP application as features rather than heuristically allows us much greater flexibility in the amount and variability of information used – e.g., incorporating knowledge about the actual entity types. This has several benefi</context>
<context position="7394" citStr="Voorhees, 1999" startWordPosition="1177" endWordPosition="1178">or retrieval. Croft et al. (Croft et al., 1991) and Tong et al. (Buckley et al., 1993; Tong et al., 1996) explored phrases and structured queries and found phrases are effective in improving retrieval performance. Voorhees (Voohees, 1993) uses word sense disambiguation to improve retrieval performance. One IR domain that consistently benefits from usage of various NLP techniques is question answering, where queries are formed in natural language format; e.g., (Peng et al., 2005). In general, however, researchers often observe limited gains or even degraded performance when applying NLP to IR (Voorhees, 1999). Having said this, most past studies use small datasets and a modest baseline; it is unclear whether a similar conclusion would be reached when using a stateof-art system such as a commercial web search engine as a baseline, and a full-web corpus – as we do in this paper. This leads to another difference between this work and existing work involving named entity recognition for retrieval. Most Location DB Business Vertical Attribute Query DB with Annotations ... Vertical Attribute Name DB Semantic Text Matching Document Index Semantic Features Specialized Ranking Module Specialized Ranking Mo</context>
</contexts>
<marker>Voorhees, 1999</marker>
<rawString>Ellen Voorhees. 1999. Natural Language Processing and Information Retrieval. Lecture Notes in Computer Science, 1714:32–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lee Wang</author>
<author>Chuang Wang</author>
<author>Xing Xie</author>
<author>Josh Forman</author>
<author>Yansheng Lu</author>
<author>Wei-Ying Ma</author>
<author>Ying Li</author>
</authors>
<title>Detecting dominant locations from search queries.</title>
<date>2005</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="9860" citStr="Wang et al., 2005" startWordPosition="1580" endWordPosition="1583">he two sequences met with X and buy the Y, the likelihood of X being a person name is substantially higher than the corresponding likelihood of Y . Indeed, many named entity taggers perform well when applied to grammatical text with sufficient contexts, such as newswire text (Sang and Meulder, 2003). Web queries, however, tend to be short, with most queries consisting of 1–3 words, and lack context – posing a particular challenge for identifying named entities in them. Existing work on NER in web queries focuses on tailoring a solution for a particular entity type and its usage in web search (Wang et al., 2005; Shen et al., 2008); in contrast, we aim at identifying a large range of possible entities in web queries, and using a generic solution for all of them. In web queries, different entity types may benefit from different detection techniques. For example, an entity type with a large variability among instances as well as existence of external resources like product name calls for an approach that can make use of many features, such as a conditional random field; for entity types that are more structured like person names, a grammar-based approach can be more effective (Shen et al., 2008). To th</context>
</contexts>
<marker>Wang, Wang, Xie, Forman, Lu, Ma, Li, 2005</marker>
<rawString>Lee Wang, Chuang Wang, Xing Xie, Josh Forman, Yansheng Lu, Wei-Ying Ma, and Ying Li. 2005. Detecting dominant locations from search queries. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Wilcoxon</author>
</authors>
<title>Individual Comparisons by Ranking Methods.</title>
<date>1945</date>
<journal>Biometrics,</journal>
<pages>1--80</pages>
<contexts>
<context position="26479" citStr="Wilcoxon, 1945" startWordPosition="4470" endWordPosition="4471">To evaluate the effectiveness of our semantic features we use Discounted Cumulative Gain (DCG) (Jarvelin and Kekalainen, 2000), a widelyused metric for measuring Web search relevance. (Jarvelin and Kekalainen, 2000). Given a query and a ranked list of K documents (K = 5 in our experiments), the DCG for this query is defined as yi (4) lo92 (1 + i) where yi E [0,10] is a relevance score for the document at position i, typically assigned by a human, where 10 is assigned to the most relevant documents and 0 to the least relevant ones. To measure statistical significance, we use the Wilcoxon test (Wilcoxon, 1945); when the p-value is below 0.01 we consider a difference to be statistically significant and mark it with a bold font in the result table. 6.3 Experimental Results We use Stochastic Gradient Boosting Trees (SGBT) (Friedman, 2002), a robust none linear regression algorithm, for training ranking functions and, as mentioned earlier, Trada (Chen et al., 2008) for model adaptation. Training parameters are selected to optimize the relevance on a separated validation set. The best resulting is evaluated against the test set; all results presented here use the test set for evaluation. 6.3.1 Feature E</context>
</contexts>
<marker>Wilcoxon, 1945</marker>
<rawString>F. Wilcoxon. 1945. Individual Comparisons by Ranking Methods. Biometrics, 1:80–83.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>