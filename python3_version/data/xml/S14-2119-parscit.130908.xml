<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015344">
<title confidence="0.9879955">
tucSage: Grammar Rule Induction for Spoken Dialogue Systems via
Probabilistic Candidate Selection
</title>
<note confidence="0.93398">
Arodami Chorianopoulou†, Georgia Athanasopoulou†, Elias Iosif$ †,
Ioannis Klasinas†, Alexandros Potamianos*
† School of ECE, Technical University of Crete, Chania 73100, Greece
* School of ECE, National Technical University of Athens, Zografou 15780, Greece
$ “Athena” Research Center, Marousi 15125, Greece
</note>
<email confidence="0.9839505">
{achorianopoulou,gathanasopoulou,iklasinas}@isc.tuc.gr
iosife@telecom.tuc.gr,apotam@gmail.com
</email>
<sectionHeader confidence="0.994625" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999956">
We describe the grammar induction sys-
tem for Spoken Dialogue Systems (SDS)
submitted to SemEval’14: Task 2. A sta-
tistical model is trained with a rich fea-
ture set and used for the selection of can-
didate rule fragments. Posterior probabil-
ities produced by the fragment selection
model are fused with estimates of phrase-
level similarity based on lexical and con-
textual information. Domain and language
portability are among the advantages of
the proposed system that was experimen-
tally validated for three thematically dif-
ferent domains in two languages.
</bodyText>
<sectionHeader confidence="0.998427" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998075">
A critical task for Spoken Dialogue Systems
(SDS) is the understanding of the transcribed user
input, that utilizes an underlying domain grammar.
An obstacle to the rapid deployment of SDS to
new domains and languages is the time-consuming
development of grammars that require human ex-
pertise. Machine-assisted grammar induction has
been an open research area for decades (K. Lari
and S. Young, 1990; S. F. Chen, 1995) aiming
to lower this barrier. Induction algorithms can
be broadly distinguished into resource-based, e.g.,
(A. Ranta, 2004), and data-driven, e.g., (H. Meng
and K.-C. Siu, 2002). The main drawback of
the resource-based paradigm is the requirement of
pre-existing knowledge bases. This is addressed
by the data-driven paradigm that relies (mostly)
on plain corpora. SDS grammars are built by uti-
lizing low- and high-level rules. Low-level rules
</bodyText>
<footnote confidence="0.8473426">
This work is licenced under a Creative Commons Attri-
bution 4.0 International License. Page numbers and pro-
ceedings footer are added by the organizers. License de-
tails: http://creativecommons.org/licenses/
by/4.0/
</footnote>
<bodyText confidence="0.999885">
are similar to gazetteers consisting of terminal en-
tries, e.g., list of city names. High-level rules can
be lexicalized as textual fragments (or chunks),
which are semantically defined on top of low-
level rules, e.g., ‘depart from &lt;City&gt;’.
The data-driven induction of low-level rules is a
well-researched area enabled by various technolo-
gies including web harvesting for corpora creation
(Klasinas et al., 2013), term extraction (K. Frantzi
and S. Ananiadou, 1997), word-level similarity
computation (Pargellis et al., 2004) and cluster-
ing (E. Iosif and A. Potamianos, 2007). High-level
rule induction is a less researched area that poses
two main challenges: 1) the extraction and selec-
tion of salient candidate fragments from a corpus
that convey semantics relevant to the domain of in-
terests and 2) the organization of such fragments
(e.g., via clustering) according to their semantic
similarity. Despite the recent interest on phrase (J.
Mitchell and M. Lapata, 2010) and sentence simi-
larity, each respective problem remains open.
Next, our submission1 for the Se-
mEval’14: Task2 is briefly described, which
constitutes a data-driven approach for inducing
high-level SDS grammar rules. At the system’s
core lies a statistical model for the selection of
textual fragments based on a rich set of features.
This set includes various lexical features, aug-
mented with statistics from n-gram language
models, as well as with heuristic features. The
candidate selection model posterior is fused
with a phrase-level semantic similarity metric.
Two different approaches are used for similarity
computation relying on the overlap of character
bigrams or context-based similarity according
to the distributional hypothesis of meaning.
The domain and language portability of the
proposed system is demonstrated by its successful
application across three different domains and
</bodyText>
<footnote confidence="0.99886">
1Please note that the last three authors of this submission
are among the organizers of this task.
</footnote>
<page confidence="0.782866">
668
</page>
<note confidence="0.746699">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 668–672,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.999823666666667">
two languages. All the four subtasks defined by
the organizers were completed with very good
performance that exceeds the baseline.
</bodyText>
<sectionHeader confidence="0.947331" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.9928325">
The basic functionality of the proposed system
is the mapping (assignment) of unknown textual
fragments into known high-level grammar rules.
Let E be the set of unknown fragments, while the
set of known rules is denoted by R. Each unknown
fragment f ∈ E is allowed to be mapped to a sin-
gle high-level rule rs ∈ R, where 1 ≤ s ≤ m and
m is the total number of rules in the grammar.
</bodyText>
<figureCaption confidence="0.999032">
Figure 1: Overview of system architecture.
</figureCaption>
<bodyText confidence="0.9784634">
The system consists of three major components as
shown at the system architecture diagram in Fig.
1, specifically: 1) candidate selection: a set of
classifiers is built, one for each rs to select whether
f ∈ E is a candidate member of the specific rule2,
</bodyText>
<listItem confidence="0.964285666666667">
2) similarity computation between f and rs, and
3) mapping f to a high-level rule rs (denoted as
f 7→ rs) according to the following model:
</listItem>
<equation confidence="0.7825745">
argmax {p(rs|f)wS(f,rs)} : f 7→ rs (1)
s
</equation>
<bodyText confidence="0.876488894736842">
where p(rs|f) stands for the probability of f
belonging to rule rs and it is estimated via the
respective classifier. The similarity between
f and rs is denoted by S(f|rs), while w is
a fixed weight taking values in the interval
[0 ∞). The fusion weight w controls the rela-
tive importance of the candidate selection and
semantic similarity modules, e.g., for w = 0
only the similarity metric S(f, rs) is used in the
decision. For example, consider the fragment f
‘leaving &lt;City&gt;’. Also, assume two high-
level rules, namely, &lt;ArrCity&gt;={‘arrive
2The requirement for building a classifier for each gram-
mar rule is realistic for the case of SDS, especially for the typ-
ical iterative human-in-the-loop grammar development sce-
nario.
at &lt;City&gt;’,...} and &lt;DepCity&gt;=
{‘depart &lt;City&gt;’,...}. According to (1)
f is mapped to the &lt;DepCity&gt; rule.
</bodyText>
<subsectionHeader confidence="0.994007">
2.1 Candidate Selection
</subsectionHeader>
<bodyText confidence="0.99999445">
In this section, the features used for building the
candidate selection module for each rs ∈ R are
briefly described. Given a pair (f,rs) a two-class
statistical classification model that corresponds to
rs is used for estimating p(rs|f) in (1).
Definitions. A high-level rule rs can be con-
sidered as a set of fragments, e.g.,‘depart
&lt;City&gt;’, ‘leaving &lt;City&gt;’. For each
fragment there are two types of constituents,
namely, lexical (e.g., ‘depart’,‘leaving’)
and low-level rules (e.g., ‘&lt;City&gt;’). The fol-
lowing features are extracted for rs considering its
respective fragments, as well as for f.
Shallow features. 1) the number of constituents
(i.e., tokens), 2) the count of lexical constituents
to the number of tokens, 3) the count of low-level
rules to the number of tokens, 4) the count of lex-
ical constituents that follow the right-most low-
level rule of the fragment, and 5) the count of low-
level rules that appear twice in a fragment.
</bodyText>
<listItem confidence="0.768303">
Perplexity-based features. A fragment f˜ can
be represented as a sequence of tokens as
w1 w2 ... wz. The perplexity of f˜ is defined as
</listItem>
<equation confidence="0.9104">
PP(˜f) = 2H( ˜f) , where H(˜f) = 1z log(p(˜f)). p(˜f)
</equation>
<bodyText confidence="0.991352958333333">
stands for the probability of f˜ estimated using an
n-gram language model. Two PP values were
used as features computed for n=2, 3.
Features of lexical similarity. Four scores of lex-
ical similarity computed between f and rs were
used as features. Let Ns denote the set of frag-
ments that are included in the training set of each
rule rs. The following metrics were employed
for computing the similarity between the unknown
fragment f and a fragment fs ∈ Ns: 1) the nor-
malized longest common subsequence (Stoilos et
al., 2005) denoted as SC, 2) the normalized over-
lap in character bigrams that is denoted as SB and
it is defined in (2), 3) a proposed variation of the
Levenshtein distance, SL, defined as SL(f, fs) =
l1−L(f,f3), where l1 and l2 are the lengths (in char-
l1+d
acters) of the lengthiest and the shortest fragment
between f and fs, respectively, while d = l1 − l2.
L(.) stands for the Levenshtein distance (V. I. Lev-
enshtein, 1966; R. A. Wagner and M. J. Fischer,
1974). 4) if f and fs differ by one token exactly
SL is applied, otherwise their similarity is set to
0. Regarding SC and SB, the similarity between
</bodyText>
<page confidence="0.995208">
669
</page>
<bodyText confidence="0.99866984375">
f and rs was estimated as the maximum similarity
yielded when computing the similarities between
f and each fs E Ns. For the rest metrics, the sim-
ilarity between f and rs was estimated by averag-
ing the |Ns |similarities computed between f and
each fs ENs.
Heuristic features. Considering an unknown
fragment f and the set of training fragments Ns
corresponding to rule rs, in total nine features
were used: 1) the difference between the aver-
age length (in tokens) of fragments in Ns and the
length of f, 2) the difference between the average
number of low-level rules in Ns and the number
of low-level rules in f, 3) as 2) but considering
the lexical constituents instead of low-level rules,
4) the number of low-level rules shared between
Ns and f, 5) as 4) but considering the lexical con-
stituents instead of low-level rules, 6) a boolean
function that equals 1 if f is a substring of at least
one fs E Ns, 7) a boolean function that equals 1 if
f shares the same lexical constituents at least one
fs E Ns, 8) a boolean function that equals 1 if f
is shorter by one token compared to any fs E Ns,
9) a boolean function that equals 1 if f is lengthier
by one token compared to any fs E Ns.
Selection. The aforementioned features are used
for building a binary classifier for each rs E R,
where 1 &lt; s &lt; m, for deciding whether f can
be regarded as a candidate member of rs or not.
Given an unknown fragment f these classifiers are
employed for estimating in total m probabilities
p(rs|f).
</bodyText>
<subsectionHeader confidence="0.99746">
2.2 Similarity Metrics
</subsectionHeader>
<bodyText confidence="0.995631666666667">
Here, two types of similarity metrics are defined,
which are used for estimating S(f, rs) in (1).
String-based similarity. Consider two fragments
fi and fj whose sets of character bigrams are de-
noted as Mi and Mj, respectively. Also, Mmin =
min(|Mi |,|Mj |) and Mmax = max(|Mi |,|Mj |
). The similarity between fi and fj is based on
the overlap of their respective character bigrams
defined as (Jimenez et al., 2012):
</bodyText>
<equation confidence="0.8750935">
|Mi n Mj  |SB(fi, fj) = , (2)
αMmax + (1 − α)Mmin
</equation>
<bodyText confidence="0.972886130434783">
where 0 &lt; α &lt; 1, while, here we use α = 0.5. The
similarity between a fragment f and a rule rs is
computed by averaging the similarities computed
between f and each fs ENs.
Context-based similarity. This is a corpus-based
metric relying on the distributional hypothesis of
meaning suggesting that similarity of context im-
plies similarity of meaning (Z. Harris, 1954). A
contextual window of size 2K+1 words is cen-
tered on the fragment of interest fi and lexical
features are extracted. For every instance of fi in
the corpus the K words left and right of fi for-
mulate a feature vector vi. For a given value of K
the context-based semantic similarity between two
fragments, fi and fj, is computed as the cosine of
their feature vectors: SK(fi, fj) = vi.vj
||vi  |vj||. The
elements of feature vectors can be weighted ac-
cording various schemes (E. Iosif and A. Potami-
anos, 2010), while, here we use a binary scheme.
The similarity between a fragment f and a rule
rs is computed by averaging the similarities com-
puted between f and each fs ENs.
</bodyText>
<subsectionHeader confidence="0.999952">
2.3 Mapping of Unknown Fragments
</subsectionHeader>
<bodyText confidence="0.999262571428572">
The output of the described system is the mapping
of a fragment f to a single (i.e., one-to-one assign-
ment) high-level rule rs E R, where 1 &lt; s &lt; m.
This is achieved by applying (1). The p(rs|f)
probabilities were estimated as described in Sec-
tion 2.1. The S(f, rs) similarities were estimated
using either SK or SB defined in Section 2.2.
</bodyText>
<sectionHeader confidence="0.9914" genericHeader="method">
3 Datasets and Experiments
</sectionHeader>
<bodyText confidence="0.9975948">
Datasets. The data was organized with respect to
three different domains: 1) air travel (flight book-
ing, car rental etc.), 2) tourism (information for
city guide), and 3) finance (currency exchange). In
total, there are four separate datasets: two datasets
for the air travel domain in English (EN) and
Greek (GR), one dataset for the tourism domain
in English, and one dataset for the finance domain
in English.
The number of high-level rules for each dataset
</bodyText>
<table confidence="0.998499">
Domain #rules #train frag. #test frag.
Travel:EN 32 982 284
Travel:GR 35 956 324
Tourism:EN 24 1004 285
Finance:EN 9 136 37
</table>
<tableCaption confidence="0.999788">
Table 1: Number of rules and train/test fragments.
</tableCaption>
<bodyText confidence="0.985781428571429">
are shown in Table 1, along with the number
of fragments included in training and test data.
Experiments. Regarding the computation of
perplexity-based features (defined in Section 2.1)
the SRILM toolkit (A. Stolcke, 2002) was used.
The n-gram probabilities were estimated over a
corpus that was created by aggregating all the
</bodyText>
<page confidence="0.99534">
670
</page>
<bodyText confidence="0.9992004">
valid fragments included in the training data.
For the computation of the context-based similar-
ity metric 5K (defined in Section 2.2) a corpus
of web-harvested data was created for each do-
main/language. The context window size K was
</bodyText>
<table confidence="0.9411182">
Domain # sentences
Travel:EN 5721
Travel:GR 6359
Tourism:EN 829516
Finance:EN 168380
</table>
<tableCaption confidence="0.997033">
Table 2: Size of corpora used in 5K metric.
</tableCaption>
<bodyText confidence="0.993638833333333">
set to 1. The size of the used corpora are presented
Table 2, while the process of corpus creation is
detailed in (Klasinas et al., 2013). The classifiers
used for the candidate selection module, described
in Section 2.1 were random forests with 50 trees
(L. Breiman, 2001).
</bodyText>
<sectionHeader confidence="0.953583" genericHeader="method">
4 Evaluation Metrics and Results
</sectionHeader>
<bodyText confidence="0.999966">
The proposed model defined by (1) was evaluated
in terms of weighted F-measure, (FM). Initially,
we run our system using the training and develop-
ment set provided by the task organizers, in order
to tune the w and K parameters. The tuning was
conducted on the Travel English domain, while the
respective evaluation results are shown in Table 3
in terms of FM. We observe that the best re-
</bodyText>
<table confidence="0.947588">
Weight w 0 1 50 500
FM 0.68 0.72 0.70 0.72
</table>
<tableCaption confidence="0.997833">
Table 3: Results for the tuning of w.
</tableCaption>
<bodyText confidence="0.999444428571429">
sults are achieved for w = 1 and w = 500. In
the case where w = 0 the rule mapping relies only
on the similarity metric. In addition, we exper-
imented with various values the context window
size K of the context-based similarity metric 5K:
K = 1, 3, 7. For all values of K similar perfor-
mance was obtained (0.70). Given the aforemen-
</bodyText>
<table confidence="0.999522857142857">
Domains Baseline Run 1 Run 2 Run 3
Travel:EN 0.51 0.66 0.65 0.68
Travel:GR 0.26 0.52 0.49 0.49
Tourism:EN 0.87 0.86 0.85 0.86
Finance:EN 0.60 0.70 0.63 0.58
UA 0.56 0.69 0.66 0.65
WA 0.52 0.66 0.64 0.65
</table>
<tableCaption confidence="0.999595">
Table 4: Official results.
</tableCaption>
<bodyText confidence="0.998595034482759">
tioned tuning the following values were selected
for the official runs: w = 1, w = 500 and K = 1.
In total, three system runs were submitted:
Run 1. The character bigram similarity metric was
used, while w was set to 1.
Run 2. The context-based similarity metrics was
used with K = 1, while w was set to 1.
Run 3. The character bigram similarity metric was
used, while w was set to 500.
The results for the aforementioned runs, along
with the baseline performance are shown in Ta-
ble 4. An overview of the participating systems
suggests that our submission achieved the high-
est performance for almost all domains and lan-
guages. The weighted (WA) and unweighted (UA)
average across the 4 datasets are also presented,
where the weight depends on the number of rules
in the dataset. Using these measures, our main
run (Run 1) obtained the best results. We ob-
serve that the performance is consistently worse
for Runs 2 and 3, with the exception of the Travel
English dataset. Comparing the performance of
Runs 1 and 2, we observe that the character bigram
metric consistently outperforms the context-based
one. For individual datasets, our system underper-
forms for the Finance (in Run 3) and the Tourism
domain (in all Runs). For the case of the Finance
domain this may be attributed to the relatively lim-
ited training data.
</bodyText>
<sectionHeader confidence="0.999429" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999971142857143">
We proposed a supervised grammar induction sys-
tem using the fusion of a grammar fragment se-
lection and similarity estimation modules. The
best configuration of our system was Run 1 which
achieved the highest performance compared to
other submissions, in almost all domains. To sum-
marize, 1) the selection module boost the sys-
tem’s performance significanlty, 2) the high per-
formance in different domains is a promising indi-
cator for domain and language portability. Future
work should involve the implementation of more
complex features for the candidate selection algo-
rithm and further investigation of phrase level sim-
ilarity metrics.
</bodyText>
<sectionHeader confidence="0.996178" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998748">
This work has been partially funded by the
projects: 1) SpeDial, and 2) PortDial, supported
by the EU Seventh Framework Programme (FP7),
with grant number 611396 and 296170, respec-
tively.
</bodyText>
<page confidence="0.99833">
671
</page>
<sectionHeader confidence="0.977677" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998710372340426">
Elias Iosif and Alexandros Potamianos. 2010. Un-
supervised semantic similarity computation between
terms using web documents. IEEE Transactions on
Knowledge and Data Engineering, 22(11), pp. 1637-
1647.
Sergio Jimenez, Claudia Becerra and Alexander Gel-
bukh. 2012. Soft Cardinality: A parameterized sim-
ilarity function for text comparison. In Proceedings
of the First Joint Conference on Lexical and Com-
putational Semantics (*SEM), pp. 449-453
Ioannis Klasinas, Alexandros Potamianos, Elias Iosif,
Spyros Georgiladakis and Gianluka Mameli. 2013.
Web data harvesting for speech understanding
grammar induction. in Proceedings of the Inter-
speech.
Helen M. Meng and Kai-Chung Siu 2002. Semi-
automatic acquisition of semantic structures for
understanding domain-specific natural language
queries. IEEE Transactions on Knowledge and Data
Engineering, 14(1), pp. 172-181.
PortDial Project free data deliverable D3.1.
https://sites.google.com/site/portdial2/deliverables-
publication
Andreas Stolcke 2002 Srilm-an extensible language
modeling toolkit in Proceedings of the Interspeech
2002
Karim Lari and Steve J. Young 2002. The estimation of
stochastic context-free grammars using the inside-
outside algorithm. Computer Speech and Language,
4(1), pp. 35-56.
Stanley F. Chen 1995. Bayesian grammar induction
for language modeling. in Proceedings of the 33rd
annual meeting of ACL
Zellig Harris 1954. Distributional structure. Word,
10(23), pp. 146-162.
Rebecca Hwa 1999. Supervised grammar induction
using training data with limited constituent informa-
tion. in Proceedings of the 37th annual meeting of
ACL
Matthew Lease, Eugene Charniak, and Mark Johnson
2005. Parsing and its applications for conversa-
tional speech. in Proceedings of Acoustics, Speech,
and Signal Processing (ICASSP)
Vladimir I. Levenshtein 1966. Binary codes capable
of correcting deletions, insertions and reversals. in
Soviet physics doklady, 10(8), pp. 707-710.
Leo Breiman 2001. Random forests. in Machine
Learning, 45(1), pp. 5-32.
Dan Jurafsky and James H. Martin 2009. Speech
and language processing an introduction to natural
language processing, computational linguistics, and
speech. Pearson Education Inc
Giorgos Stoilos, Giorgos Stamou, and Stefanos Kollias
2005. A string metric for ontology alignment. in
The Semantic WebISWC, pp. 624637
Robert A. Wagner and Michael J. Fisher 1974. The
string-to-string correction problem. Journal of the
ACM (JACM), 21(1), pp. 168-173
Katerina Frantzi and Sophia Ananiadou 1997. Au-
tomatic term recognition using contextual cues. in
Proceedings of International Joint Conferences on
Artificial Intelligence
Elias Iosif and Alexandros Potamianos 2007. A soft-
clustering algorithm for automatic induction of se-
mantic classes. in Proceedings of Interspeech
Jeffrey Mitchell and Mirela Lapata 2010. Composi-
tion in distributional models of semantics. Cognitive
Science, 34(8):1388-1429.
Ye-Yi Wang and Alex Acero 2006. Rapid develop-
ment of spoken language understanding grammars.
Speech Communication, 48(3), pp. 360-416.
Eric Brill 1992. A simple rule-based part of speech
tagger. in Proceedings of the workshop on Speech
and Natural Language
Alexander Clark 2001. Unsupervised induction
of stochastic context-free grammars using distribu-
tional clustering. in Proceedings of the 2001 work-
shop on Computational Natural Language Learning
Benjamin Snyder, Tahira Naseem, and Regina Barzilay
2009. Unsupervised multilingual grammar induc-
tion. in Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL
Aarne Ranta 2009. Grammatical framework: A type-
theoretical grammar formalism. Journal of Func-
tional Programming: 14(2), pp. 145-189
Andrew Pargellis, Eric Fosler-Lussier, Chin Hui Lee,
Alexandros Potamianos and Augustine Tsai 2009.
Auto-induced Semantic Classes. Speech Communi-
cation: 43(3), pp. 183-203
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre 2012. SemEval-2012 Task 6: A
Pilot on Semantic Textual Similarity. in Proceedings
of the First Joint Conference on Lexical and Com-
putational Semantics (*Sem), pp. 385-393
</reference>
<page confidence="0.998262">
672
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.263545">
<title confidence="0.992754">tucSage: Grammar Rule Induction for Spoken Dialogue Systems Probabilistic Candidate Selection</title>
<author confidence="0.925126">Georgia Elias</author>
<note confidence="0.46243925">Alexandros of ECE, Technical University of Crete, Chania 73100, of ECE, National Technical University of Athens, Zografou 15780, Research Center, Marousi 15125,</note>
<abstract confidence="0.998029">We describe the grammar induction system for Spoken Dialogue Systems (SDS) submitted to SemEval’14: Task 2. A statistical model is trained with a rich feature set and used for the selection of candidate rule fragments. Posterior probabilities produced by the fragment selection model are fused with estimates of phraselevel similarity based on lexical and contextual information. Domain and language portability are among the advantages of the proposed system that was experimentally validated for three thematically different domains in two languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Elias Iosif</author>
<author>Alexandros Potamianos</author>
</authors>
<title>Unsupervised semantic similarity computation between terms using web documents.</title>
<date>2010</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>22</volume>
<issue>11</issue>
<pages>1637--1647</pages>
<marker>Iosif, Potamianos, 2010</marker>
<rawString>Elias Iosif and Alexandros Potamianos. 2010. Unsupervised semantic similarity computation between terms using web documents. IEEE Transactions on Knowledge and Data Engineering, 22(11), pp. 1637-1647.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergio Jimenez</author>
<author>Claudia Becerra</author>
<author>Alexander Gelbukh</author>
</authors>
<title>Soft Cardinality: A parameterized similarity function for text comparison.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics (*SEM),</booktitle>
<pages>449--453</pages>
<contexts>
<context position="10316" citStr="Jimenez et al., 2012" startWordPosition="1694" endWordPosition="1697">or deciding whether f can be regarded as a candidate member of rs or not. Given an unknown fragment f these classifiers are employed for estimating in total m probabilities p(rs|f). 2.2 Similarity Metrics Here, two types of similarity metrics are defined, which are used for estimating S(f, rs) in (1). String-based similarity. Consider two fragments fi and fj whose sets of character bigrams are denoted as Mi and Mj, respectively. Also, Mmin = min(|Mi |,|Mj |) and Mmax = max(|Mi |,|Mj | ). The similarity between fi and fj is based on the overlap of their respective character bigrams defined as (Jimenez et al., 2012): |Mi n Mj |SB(fi, fj) = , (2) αMmax + (1 − α)Mmin where 0 &lt; α &lt; 1, while, here we use α = 0.5. The similarity between a fragment f and a rule rs is computed by averaging the similarities computed between f and each fs ENs. Context-based similarity. This is a corpus-based metric relying on the distributional hypothesis of meaning suggesting that similarity of context implies similarity of meaning (Z. Harris, 1954). A contextual window of size 2K+1 words is centered on the fragment of interest fi and lexical features are extracted. For every instance of fi in the corpus the K words left and rig</context>
</contexts>
<marker>Jimenez, Becerra, Gelbukh, 2012</marker>
<rawString>Sergio Jimenez, Claudia Becerra and Alexander Gelbukh. 2012. Soft Cardinality: A parameterized similarity function for text comparison. In Proceedings of the First Joint Conference on Lexical and Computational Semantics (*SEM), pp. 449-453</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Klasinas</author>
</authors>
<title>Alexandros Potamianos, Elias Iosif, Spyros Georgiladakis and Gianluka Mameli.</title>
<date>2013</date>
<booktitle>in Proceedings of the Interspeech.</booktitle>
<marker>Klasinas, 2013</marker>
<rawString>Ioannis Klasinas, Alexandros Potamianos, Elias Iosif, Spyros Georgiladakis and Gianluka Mameli. 2013. Web data harvesting for speech understanding grammar induction. in Proceedings of the Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helen M Meng</author>
<author>Kai-Chung Siu</author>
</authors>
<title>Semiautomatic acquisition of semantic structures for understanding domain-specific natural language queries.</title>
<date>2002</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>14</volume>
<issue>1</issue>
<pages>172--181</pages>
<marker>Meng, Siu, 2002</marker>
<rawString>Helen M. Meng and Kai-Chung Siu 2002. Semiautomatic acquisition of semantic structures for understanding domain-specific natural language queries. IEEE Transactions on Knowledge and Data Engineering, 14(1), pp. 172-181.</rawString>
</citation>
<citation valid="false">
<title>PortDial Project free data deliverable D3.1. https://sites.google.com/site/portdial2/deliverablespublication</title>
<marker></marker>
<rawString>PortDial Project free data deliverable D3.1. https://sites.google.com/site/portdial2/deliverablespublication</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm-an extensible language modeling toolkit</title>
<date>2002</date>
<booktitle>in Proceedings of the Interspeech</booktitle>
<contexts>
<context position="12666" citStr="Stolcke, 2002" startWordPosition="2108" endWordPosition="2109">two datasets for the air travel domain in English (EN) and Greek (GR), one dataset for the tourism domain in English, and one dataset for the finance domain in English. The number of high-level rules for each dataset Domain #rules #train frag. #test frag. Travel:EN 32 982 284 Travel:GR 35 956 324 Tourism:EN 24 1004 285 Finance:EN 9 136 37 Table 1: Number of rules and train/test fragments. are shown in Table 1, along with the number of fragments included in training and test data. Experiments. Regarding the computation of perplexity-based features (defined in Section 2.1) the SRILM toolkit (A. Stolcke, 2002) was used. The n-gram probabilities were estimated over a corpus that was created by aggregating all the 670 valid fragments included in the training data. For the computation of the context-based similarity metric 5K (defined in Section 2.2) a corpus of web-harvested data was created for each domain/language. The context window size K was Domain # sentences Travel:EN 5721 Travel:GR 6359 Tourism:EN 829516 Finance:EN 168380 Table 2: Size of corpora used in 5K metric. set to 1. The size of the used corpora are presented Table 2, while the process of corpus creation is detailed in (Klasinas et al</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke 2002 Srilm-an extensible language modeling toolkit in Proceedings of the Interspeech 2002</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karim Lari</author>
<author>Steve J Young</author>
</authors>
<title>The estimation of stochastic context-free grammars using the insideoutside algorithm.</title>
<date>2002</date>
<journal>Computer Speech and Language,</journal>
<volume>4</volume>
<issue>1</issue>
<pages>35--56</pages>
<marker>Lari, Young, 2002</marker>
<rawString>Karim Lari and Steve J. Young 2002. The estimation of stochastic context-free grammars using the insideoutside algorithm. Computer Speech and Language, 4(1), pp. 35-56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
</authors>
<title>Bayesian grammar induction for language modeling.</title>
<date>1995</date>
<booktitle>in Proceedings of the 33rd annual meeting of ACL</booktitle>
<contexts>
<context position="1495" citStr="Chen, 1995" startWordPosition="212" endWordPosition="213">ion. Domain and language portability are among the advantages of the proposed system that was experimentally validated for three thematically different domains in two languages. 1 Introduction A critical task for Spoken Dialogue Systems (SDS) is the understanding of the transcribed user input, that utilizes an underlying domain grammar. An obstacle to the rapid deployment of SDS to new domains and languages is the time-consuming development of grammars that require human expertise. Machine-assisted grammar induction has been an open research area for decades (K. Lari and S. Young, 1990; S. F. Chen, 1995) aiming to lower this barrier. Induction algorithms can be broadly distinguished into resource-based, e.g., (A. Ranta, 2004), and data-driven, e.g., (H. Meng and K.-C. Siu, 2002). The main drawback of the resource-based paradigm is the requirement of pre-existing knowledge bases. This is addressed by the data-driven paradigm that relies (mostly) on plain corpora. SDS grammars are built by utilizing low- and high-level rules. Low-level rules This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. Lice</context>
</contexts>
<marker>Chen, 1995</marker>
<rawString>Stanley F. Chen 1995. Bayesian grammar induction for language modeling. in Proceedings of the 33rd annual meeting of ACL</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<date>1954</date>
<journal>Distributional structure. Word,</journal>
<volume>10</volume>
<issue>23</issue>
<pages>146--162</pages>
<contexts>
<context position="10733" citStr="Harris, 1954" startWordPosition="1772" endWordPosition="1773">tively. Also, Mmin = min(|Mi |,|Mj |) and Mmax = max(|Mi |,|Mj | ). The similarity between fi and fj is based on the overlap of their respective character bigrams defined as (Jimenez et al., 2012): |Mi n Mj |SB(fi, fj) = , (2) αMmax + (1 − α)Mmin where 0 &lt; α &lt; 1, while, here we use α = 0.5. The similarity between a fragment f and a rule rs is computed by averaging the similarities computed between f and each fs ENs. Context-based similarity. This is a corpus-based metric relying on the distributional hypothesis of meaning suggesting that similarity of context implies similarity of meaning (Z. Harris, 1954). A contextual window of size 2K+1 words is centered on the fragment of interest fi and lexical features are extracted. For every instance of fi in the corpus the K words left and right of fi formulate a feature vector vi. For a given value of K the context-based semantic similarity between two fragments, fi and fj, is computed as the cosine of their feature vectors: SK(fi, fj) = vi.vj ||vi |vj||. The elements of feature vectors can be weighted according various schemes (E. Iosif and A. Potamianos, 2010), while, here we use a binary scheme. The similarity between a fragment f and a rule rs is </context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig Harris 1954. Distributional structure. Word, 10(23), pp. 146-162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
</authors>
<title>Supervised grammar induction using training data with limited constituent information.</title>
<date>1999</date>
<booktitle>in Proceedings of the 37th annual meeting of ACL</booktitle>
<marker>Hwa, 1999</marker>
<rawString>Rebecca Hwa 1999. Supervised grammar induction using training data with limited constituent information. in Proceedings of the 37th annual meeting of ACL</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Lease</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Parsing and its applications for conversational speech.</title>
<date>2005</date>
<booktitle>in Proceedings of Acoustics, Speech, and Signal Processing (ICASSP)</booktitle>
<marker>Lease, Charniak, Johnson, 2005</marker>
<rawString>Matthew Lease, Eugene Charniak, and Mark Johnson 2005. Parsing and its applications for conversational speech. in Proceedings of Acoustics, Speech, and Signal Processing (ICASSP)</rawString>
</citation>
<citation valid="false">
<authors>
<author>I Vladimir</author>
</authors>
<title>Levenshtein 1966. Binary codes capable of correcting deletions, insertions and reversals.</title>
<booktitle>in Soviet physics doklady,</booktitle>
<volume>10</volume>
<issue>8</issue>
<pages>707--710</pages>
<marker>Vladimir, </marker>
<rawString>Vladimir I. Levenshtein 1966. Binary codes capable of correcting deletions, insertions and reversals. in Soviet physics doklady, 10(8), pp. 707-710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Random forests.</title>
<date>2001</date>
<booktitle>in Machine Learning,</booktitle>
<volume>45</volume>
<issue>1</issue>
<pages>5--32</pages>
<contexts>
<context position="13410" citStr="Breiman, 2001" startWordPosition="2231" endWordPosition="2232">ded in the training data. For the computation of the context-based similarity metric 5K (defined in Section 2.2) a corpus of web-harvested data was created for each domain/language. The context window size K was Domain # sentences Travel:EN 5721 Travel:GR 6359 Tourism:EN 829516 Finance:EN 168380 Table 2: Size of corpora used in 5K metric. set to 1. The size of the used corpora are presented Table 2, while the process of corpus creation is detailed in (Klasinas et al., 2013). The classifiers used for the candidate selection module, described in Section 2.1 were random forests with 50 trees (L. Breiman, 2001). 4 Evaluation Metrics and Results The proposed model defined by (1) was evaluated in terms of weighted F-measure, (FM). Initially, we run our system using the training and development set provided by the task organizers, in order to tune the w and K parameters. The tuning was conducted on the Travel English domain, while the respective evaluation results are shown in Table 3 in terms of FM. We observe that the best reWeight w 0 1 50 500 FM 0.68 0.72 0.70 0.72 Table 3: Results for the tuning of w. sults are achieved for w = 1 and w = 500. In the case where w = 0 the rule mapping relies only on</context>
</contexts>
<marker>Breiman, 2001</marker>
<rawString>Leo Breiman 2001. Random forests. in Machine Learning, 45(1), pp. 5-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Jurafsky</author>
<author>James H Martin</author>
</authors>
<title>Speech and language processing an introduction to natural language processing, computational linguistics, and speech.</title>
<date>2009</date>
<publisher>Pearson Education Inc</publisher>
<marker>Jurafsky, Martin, 2009</marker>
<rawString>Dan Jurafsky and James H. Martin 2009. Speech and language processing an introduction to natural language processing, computational linguistics, and speech. Pearson Education Inc</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giorgos Stoilos</author>
</authors>
<title>Giorgos Stamou, and Stefanos Kollias</title>
<date>2005</date>
<booktitle>in The Semantic WebISWC,</booktitle>
<pages>624637</pages>
<marker>Stoilos, 2005</marker>
<rawString>Giorgos Stoilos, Giorgos Stamou, and Stefanos Kollias 2005. A string metric for ontology alignment. in The Semantic WebISWC, pp. 624637</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert A Wagner</author>
<author>Michael J Fisher</author>
</authors>
<title>The string-to-string correction problem.</title>
<date>1974</date>
<journal>Journal of the ACM (JACM),</journal>
<volume>21</volume>
<issue>1</issue>
<pages>168--173</pages>
<marker>Wagner, Fisher, 1974</marker>
<rawString>Robert A. Wagner and Michael J. Fisher 1974. The string-to-string correction problem. Journal of the ACM (JACM), 21(1), pp. 168-173</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katerina Frantzi</author>
<author>Sophia Ananiadou</author>
</authors>
<title>Automatic term recognition using contextual cues.</title>
<date>1997</date>
<booktitle>in Proceedings of International Joint Conferences on Artificial Intelligence</booktitle>
<marker>Frantzi, Ananiadou, 1997</marker>
<rawString>Katerina Frantzi and Sophia Ananiadou 1997. Automatic term recognition using contextual cues. in Proceedings of International Joint Conferences on Artificial Intelligence</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elias Iosif</author>
<author>Alexandros Potamianos</author>
</authors>
<title>A softclustering algorithm for automatic induction of semantic classes.</title>
<date>2007</date>
<booktitle>in Proceedings of Interspeech</booktitle>
<marker>Iosif, Potamianos, 2007</marker>
<rawString>Elias Iosif and Alexandros Potamianos 2007. A softclustering algorithm for automatic induction of semantic classes. in Proceedings of Interspeech</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Mitchell</author>
<author>Mirela Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<pages>34--8</pages>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeffrey Mitchell and Mirela Lapata 2010. Composition in distributional models of semantics. Cognitive Science, 34(8):1388-1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ye-Yi Wang</author>
<author>Alex Acero</author>
</authors>
<title>Rapid development of spoken language understanding grammars.</title>
<date>2006</date>
<journal>Speech Communication,</journal>
<volume>48</volume>
<issue>3</issue>
<pages>360--416</pages>
<marker>Wang, Acero, 2006</marker>
<rawString>Ye-Yi Wang and Alex Acero 2006. Rapid development of spoken language understanding grammars. Speech Communication, 48(3), pp. 360-416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>A simple rule-based part of speech tagger.</title>
<date>1992</date>
<booktitle>in Proceedings of the workshop on Speech and Natural Language</booktitle>
<marker>Brill, 1992</marker>
<rawString>Eric Brill 1992. A simple rule-based part of speech tagger. in Proceedings of the workshop on Speech and Natural Language</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Unsupervised induction of stochastic context-free grammars using distributional clustering.</title>
<date>2001</date>
<booktitle>in Proceedings of the 2001 workshop on Computational Natural Language Learning</booktitle>
<marker>Clark, 2001</marker>
<rawString>Alexander Clark 2001. Unsupervised induction of stochastic context-free grammars using distributional clustering. in Proceedings of the 2001 workshop on Computational Natural Language Learning</rawString>
</citation>
<citation valid="false">
<authors>
<author>Benjamin Snyder</author>
</authors>
<title>Tahira Naseem, and Regina Barzilay 2009. Unsupervised multilingual grammar induction.</title>
<booktitle>in Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL</booktitle>
<marker>Snyder, </marker>
<rawString>Benjamin Snyder, Tahira Naseem, and Regina Barzilay 2009. Unsupervised multilingual grammar induction. in Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aarne Ranta</author>
</authors>
<title>Grammatical framework: A typetheoretical grammar formalism.</title>
<date>2009</date>
<journal>Journal of Functional Programming:</journal>
<volume>14</volume>
<issue>2</issue>
<pages>145--189</pages>
<marker>Ranta, 2009</marker>
<rawString>Aarne Ranta 2009. Grammatical framework: A typetheoretical grammar formalism. Journal of Functional Programming: 14(2), pp. 145-189</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Pargellis</author>
</authors>
<title>Eric Fosler-Lussier, Chin Hui Lee, Alexandros Potamianos and Augustine Tsai</title>
<date>2009</date>
<journal>Speech Communication:</journal>
<volume>43</volume>
<issue>3</issue>
<pages>183--203</pages>
<marker>Pargellis, 2009</marker>
<rawString>Andrew Pargellis, Eric Fosler-Lussier, Chin Hui Lee, Alexandros Potamianos and Augustine Tsai 2009. Auto-induced Semantic Classes. Speech Communication: 43(3), pp. 183-203</rawString>
</citation>
<citation valid="false">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
</authors>
<title>and Aitor Gonzalez-Agirre 2012. SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity.</title>
<booktitle>in Proceedings of the First Joint Conference on Lexical and Computational Semantics (*Sem),</booktitle>
<pages>385--393</pages>
<marker>Agirre, Cer, Diab, </marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre 2012. SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity. in Proceedings of the First Joint Conference on Lexical and Computational Semantics (*Sem), pp. 385-393</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>