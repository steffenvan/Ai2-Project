<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.992683">
Project for production of closed-caption TV programs
for the hearing impaired
</title>
<author confidence="0.756082">
Takahiro Walcao
Telecommunications Advancement
</author>
<affiliation confidence="0.945858">
Organization of Japan
</affiliation>
<address confidence="0.710916">
Uehara Shibuya-ku, Tokyo 151-0064, Japan
</address>
<email confidence="0.930163">
walcao@shibuya.tao.or.jp
</email>
<author confidence="0.467995125">
Eiji Sawamura
TAO
Terumasa Ehara
NHK Science and Technical
Research Lab / TAO
Ichiro Maruyama
TAO
Katsuhiko Shirai
</author>
<affiliation confidence="0.9299655">
Waseda University, Department of
Information and Computer Science / TAO
</affiliation>
<sectionHeader confidence="0.994551" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999859857142857">
We describe an on-going project whose
primary aim is to establish the technology of
producing closed captions for TV news
programs efficiently using natural language
processing and speech recognition techniques
for the benefit of the hearing impaired in
Japan. The project is supported by the
Telecommunications Advancement
Organisation of Japan with the help of the
ministry of Posts and Telecommunications.
We propose natural language and speech
processing techniques should be used for
efficient closed caption production of TV
programs. They enable us to summarise TV
news texts into captions automatically, and
synchronise TV news texts with speech and
video automatically. Then the captions are
superimposed on the screen.
We propose a combination of shallow
methods for the summarisation. For all the
sentences in the original text, an importance
measure is computed based on key words in
the text to determine which sentences are
important. If some parts of the sentences
are judged unimportant, they are shortened or
deleted. We also propose keyword pair
model for the synchronisation between text
and speech.
</bodyText>
<sectionHeader confidence="0.987033" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.999896518518519">
The closed captions for TV programs are not
provided widely in Japan. Only 10 percent of
the TV programs are shown with captions, in
contrast to 70 % in the United States and more
than 30 % in Britain. Reasons why the
availability is low are firstly the characters used
in the Japanese language are complex and many.
Secondly, at the moment, the closed captions are
produced manually and it is a time-consuming
and costly task. Thus we think the natural
language and speech processing technology will
be useful for the efficient production of TV
programs with closed captions.
The Telecommunications Advancement
Organisation of Japan with the support of the
ministry of Posts and Telecommunications has
initiated a project in which an electronically
available text of TV news programs is
summarised and syncrhorinised with the speech
and video automatically, then superimposed on
the original programs.
It is a five-year project which started in 1996,
and its annual budget is about 200 million yen.
In the following chapters we describe main
research issues in detail and the project schedule,
and the results of our preliminary research on
the main research topics are presented.
</bodyText>
<page confidence="0.993762">
1340
</page>
<figureCaption confidence="0.996088">
Figure 1 System Outline
</figureCaption>
<figure confidence="0.998769266666667">
Time Code
Original ; MITA
Mg=
UMMar
AkC
ill J
Original
ARDEN
RIJKLMN
TIITIP Carl
Time Crile
gpol2A&amp;quot;
74012345
tits
Original ; umnian
Itt5ifii Pi
VTR VR
Original program
video &amp; audio
A
TV program with
closed captions
audio audio &amp; time code Or I I I 1.1110 I I I I. zing.
.1;
&amp; time code
news script closed automatic synchronisation 111rITTM111
automatic tsp—
= atiz
summarisation captions recognition syndl_z_voi
phase wad
</figure>
<sectionHeader confidence="0.830057" genericHeader="method">
1 Research Issues
</sectionHeader>
<bodyText confidence="0.9511535">
Main research issues in the project are as
follows:
</bodyText>
<listItem confidence="0.973159">
• automatic text summarisation
</listItem>
<bodyText confidence="0.991105909090909">
automatic synchronisation of text and
speech
building an efficient closed caption
production system
The outline of the system is shown in Figure 1.
Although all types of TV programs are to be
handled in the project system, the first priority is
given to TV news programs since most of the
hearing impaired people say they want to watch
closed-captioned TV news programs. The
research issues are explained briefly next.
</bodyText>
<subsectionHeader confidence="0.998997">
1.1 Text Summarisation
</subsectionHeader>
<bodyText confidence="0.999996333333333">
For most of the TV news programs today, the
scripts (written text) are available electronically
before they are read out by newscasters.
Japanese news texts are read at the speed of
between 350 and 400 characters per minute, and
if all the characters in the texts are shown on the
TV screen, there are too many of them to be
understood well (Komine et aL 1996).
Therefore we need to summarise the news
texts to some extent, and then show them on the
screen. The aim of the research on automatic
text summarisation is to summarise the text fully
or partially automatically to a proper size to
obtain closed captions. The current aim is 70%
summarisation in the number of characters.
</bodyText>
<subsectionHeader confidence="0.99936">
1.2 Synchronisation of Text and Speech
</subsectionHeader>
<bodyText confidence="0.999905636363636">
We need to synchronise the text with the sound,
or speech of the program. This is done by hand
at present and we would like to employ speech
recognition technology to assist the
synchronisation.
First, synchronising points between the
original text and the speech are determined
automatically (recognition phase in Figure!).
Then the captions are synchronised with the
speech and video (synchronisation phase in
Figurel).
</bodyText>
<subsectionHeader confidence="0.993533">
1.3 Efficient Closed Caption Production
System
</subsectionHeader>
<bodyText confidence="0.999915166666667">
We will build a system by integrating the
summarisation and synchronisation techniques
with techniques for superimposing characters on
to the screen. We have also conducted
research on how to present the captions on the
screen for the handicapped people.
</bodyText>
<sectionHeader confidence="0.900124" genericHeader="method">
2 Project Schedule
</sectionHeader>
<bodyText confidence="0.99989275">
The project has two stages: the first 3 years and
the rest 2 years. We research on the above
issues and build a prototype system in the first
stage. The prototype system will be used to
produce closed captions, and the capability and
functions of the system will be evaluated. We
will focus on improvement and evaluation of the
system in the second stage.
</bodyText>
<page confidence="0.987096">
1341
</page>
<sectionHeader confidence="0.995463" genericHeader="method">
3 Preliminary Research Results
</sectionHeader>
<bodyText confidence="0.9999186">
We describe results of our research on automatic
summarisation and automatic synchronisation of
text and speech. Then, a study on how to
present captions on TV screen to the hearing
impaired people is briefly mentioned.
</bodyText>
<subsectionHeader confidence="0.999106">
3.1 Automatic Text Summarisation
</subsectionHeader>
<bodyText confidence="0.99995275">
We have a combination of shallow processing
methods for automatic text summarisation.
The first is to compute key words in a text and
importance measures for each sentence, and then
select importanct sentences for the text. The
second is to shoten or delete unimportant parts
in a sentence using Japanese language-specific
rules.
</bodyText>
<subsubsectionHeader confidence="0.970966">
3.1.1 Sentence Extraction
</subsubsectionHeader>
<bodyText confidence="0.99976108">
Ehara found that compared with newspaper text,
TV news texts have longer sentences and each
text has a smaller number of sentences (Ehara et
al 1997). If we summarise TV news text by
selecting sentences from the orignal text, it
would be &apos;rough&apos; summarisation. On the other
hand, if we devide long sentences into smaller
units, thus increase the number of sentences in
the text, we may have finer and better
summarisation (Kim &amp; Ehara 1994).
Therefore what is done in the system is that if a
sentence in a given text is too long, it will be
partitioned into smaller units with minimun
changes made to the original sentence.
To compute importance measures for each
sentence, we need to find first key words of the
text. We tested high-frequency key word
method (Luhn 1957, Edumundson 1969) and a
TF-IDF-based (Text frequency, Inverse
Document Frequency) method. We evaluated
the two methods using ten thousand TV news
texts, and found that high-frequency key word
method showed slightly better results than the
method based on TF-IDF scores (Wakao et at
1997).
</bodyText>
<subsubsectionHeader confidence="0.976855">
3.1.2 Rules for shortening text
</subsubsectionHeader>
<bodyText confidence="0.9999716875">
Another way of reducing the number of
characters in a Japanese text, thus summarising
the text, is to shorten or delete parts of the
sentences. For example, if a sentence ends
with a sahen verb followed by its inflection, or
helping verbs or particles to express proper
politeness, it does not change the meaning
much even if we keep only the verb stem (or
sahen noun) and delete the rest of it. This is
one of the ways found in the captions to shorten
or delete unimportant parts of the sentences.
We analysed texts and captions in a TV
news program which is broadcast fully
captioned for the hearing impaired in Japan. We
complied 16 rules. The rules are devided into 5
groups. We describe them one by one below.
</bodyText>
<listItem confidence="0.395568">
1) Shotening and deletion of sentence ends
</listItem>
<bodyText confidence="0.9917112">
We find some of phrases which come at the
end of the sentence can be shortened or
deleted. If a sahen verb is used as the main
verb, we can change it to its sahen noun.
For example:
</bodyText>
<listItem confidence="0.703443">
• ... keikakushiteimasu 1, &amp;quot;C
</listItem>
<bodyText confidence="0.8514375">
---■ keikaku (itg)
(note: keikakusuru = plan, sahen verb)
If the sentence ends in a reporting style, we
may delete the verb part.
</bodyText>
<listItem confidence="0.84288">
• ... bekida to nobemashita
</listItem>
<equation confidence="0.519222">
Lt•■•11.k)
bekida
(bekida = should, nobemashita = have said)
</equation>
<sectionHeader confidence="0.417317" genericHeader="method">
2) Keeping parts of sentence
</sectionHeader>
<bodyText confidence="0.91052">
Important noun phrases are kept in captions,
and the rest of the sentence is deleted.
</bodyText>
<listItem confidence="0.793544666666667">
• taihosaretano ha Matumoto shachou
htt 0 itti`AO±R)
taiho Matumoto shachou
</listItem>
<equation confidence="0.673429">
(111 210±-A)
</equation>
<bodyText confidence="0.7861215">
(taiho = arrest, shachou = a company
president, Matumoto = name of a person)
</bodyText>
<sectionHeader confidence="0.458993" genericHeader="method">
3) Replacing with shorter phrase
</sectionHeader>
<bodyText confidence="0.997194">
Some nouns are replaced with a simpler and
shoter phrase.
</bodyText>
<listItem confidence="0.991021714285714">
• souridaijin shushou (S4f1)
(souridaijin, shushou both mean a prime
minister)
4) Conneticting phrases omitted
Connecting phrases at the beginning of the
sentence may be omitted.
• shikashi (1. b, 1, = however),
</listItem>
<equation confidence="0.554308">
ippou = on the other hand)
</equation>
<page confidence="0.965519">
1342
</page>
<sectionHeader confidence="0.488285" genericHeader="method">
5) Time expressions deleted
</sectionHeader>
<bodyText confidence="0.99995925">
Comparative time expressions such as today
(kyou 4El ), yesterday (kinou, Err fl) can be
deleted. However, the absolute time expressions
such as May, 1998 ( 1 9 9 8 5 A ) stay
unchanged in summarisation.
When we apply these rules to selected
important sentences, we can reduce the size of
text further 10 to 20 percent.
</bodyText>
<subsectionHeader confidence="0.9998675">
3.2 Automatic Synchronisation of Text
and Speech
</subsectionHeader>
<bodyText confidence="0.999949">
We next synchronise the text and speech. First,
the written TV news text is changed into a
stream of phonetic transcriptions. Second,
we try to detect the time points of the text and
their corresponding speech sections. We have
developed &apos;keyword pair model&apos; for the
synchronisation which is shown in Figure 2.
</bodyText>
<figureCaption confidence="0.868853">
Figure 2 Keyword Pair Model
</figureCaption>
<bodyText confidence="0.999905464285714">
The model consists of two sets of words
(keywordsl and keywords2) before and after the
synchronisation point (point B). Each set
contains one or two key words which are
represented by a sequence of phonetic HMMs
(Hidden Markov Models). Each 1-IMM is a
three-loop, eight-mixture-distribution HMM.
We use 39 phonetic HMMs to represent all
Japanese phonemes.
When the speech is put in the model, non-
synchronising input data travel through the
garbage arc while synchronising data go through
the two keyword sets, which makes the
likelihood at point B increase. Therefore if we
observe the likelihood at point B and it becomes
bigger than a certain threshold, we decide it is
the synchronisation point for the input data.
Thirty-four (34) keywords pairs were taken
from the data which was not used in the training
and selected for the evaluation of the model.
We used the speech of four people for the
evaluation.
The evaluation results are shown in Table 1.
They are the accuracy (detection rate) and false
alarm rate for the case that each keyword set has
two key words. The threshold is computed as
logarithm of the likelihood which is between
zero and one, thus it becomes less than zero.
</bodyText>
<table confidence="0.998718625">
Threshold Detection rate False Alarm Rate
(%) (FA/KW/Hour)
-10 34.56 0
-20 44.12 0
-30 54.41 0
-40 60.29 0
-50 64.71 0.06
-60 69.12 0.06
-70 69.85 0.06
-80 71.32 0.12
-90 78.68 0.18
-100 82.35 0.18
-150 91.18 0.54
-200 94.85 1.21
-250 95.59 1.81
-300 99.26 2.41
</table>
<tableCaption confidence="0.998596">
Table 1 Synchronisation Detection
</tableCaption>
<bodyText confidence="0.999906">
As the threshold decreases, the detection rate
increases, however, the false alarm rate
increases little (Maruyama 1998).
</bodyText>
<subsectionHeader confidence="0.999115">
3.3 Speech Database
</subsectionHeader>
<bodyText confidence="0.999985153846154">
We have been gathering TV and radio news
speech. In 1996 we collected speech data by
simulating news programs, i.e. TV news texts
were read and recorded sentence by sentence in
a studio. It has seven and a half houses of
recordings of twenty people (both male and
female). In 1997 we continued to record TV
news speech by simulation, and recorded speech
data from actual radio and TV programs. It has
now ten hours of actual radio recording and ten
hours of actual TV programs. We will
continue to record speech data and increase the
size of the database in 1998.
</bodyText>
<subsectionHeader confidence="0.937953">
3.4 Caption Presentation
</subsectionHeader>
<bodyText confidence="0.72194775">
We have conducted a study, though on small
scale, on how to present captions on TV screen
Null arc
Ke ord set1 Keywords t2
</bodyText>
<page confidence="0.85392">
1343
</page>
<bodyText confidence="0.9996006">
to the hearing impaired people. We
superimposed captions by hand on several kinds
of TV programs. They were evaluated by the
hadicapped people (hard of hearing persons) in
terms of the following points:
</bodyText>
<listItem confidence="0.997689857142857">
• characters : size, font, colour
• number of lines
• timing
• location
• methods of scrolling
• inside or outside of the picture (see two
examples below).
</listItem>
<figureCaption confidence="0.9914025">
Figure 3 Captions in the picture
Figure 4 Captions outside of the picture
</figureCaption>
<bodyText confidence="0.9999554">
Most of the subjects preferred 2-line, outside
of the picture captions without scrolling
(Tanahashi, 1998). This was still a preliminary
study, and we plan to conduct similar evaluation
by the hearing impaired people on large scale.
</bodyText>
<sectionHeader confidence="0.964385" genericHeader="conclusions">
Conclusion
</sectionHeader>
<bodyText confidence="0.999992333333333">
We have described a national project, its
research issues and schedule, as well as
preliminary research results. The project aim is
to establish language and speech processing
technology so that TV news program text is
summarised and changed into captions, and
synchronised with the speech, and superimposed
to the original program for the benefits of the
hearing impaired. We will continue to conduct
research and build a prototype TV caption
production system, and try to put it to a practical
use in the near future.
</bodyText>
<sectionHeader confidence="0.997275" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999988">
We would like to thank Nippon Television
Network Corporation for letting us use the
pictures (Figure 3, 4) of their news program for
the purpose of our research.
</bodyText>
<sectionHeader confidence="0.999406" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999542394736842">
Edmundson, H.P. (1969) New Methods in Automatic
Extracting Journal of the ACM, 16(2), pp 264-
285.
Ehara, T., Wakao, T., Sawamura, E., Maruyama I.,
Abe Y., Shirai K. (1997) Application of natural
language processing and speech processing
technology to production of closed-caption TV
programs for the hearing impaired NLPRS 1997
Kim Y.B., Ehara, T. (1994) A method of
partitioning of long Japanese sentences with
subject resolution in J/E machine translation, Proc.
of 1994 International Conference on Computer
Processing of Oriental Languages, pp.467-473.
Komine, K., Hoshino, H., Isom, H., Uchida, T.,
Iwahana, Y. (1996) Cognitive Experiments of
News Captioning for Hearing Impaired Persons
Technical Report of IECE (The Institution of
Electronics, Information and Communication
Engineers), HCS96-23, in Japanese, pp 7-12.
Luhn, H.P. (1957) A statistical approach to the
mechanized encoding and searching of literary
information IBM Journal of Research and
Development, 1(4), pp 309-317.
Maruyama, I., Abe, Y., Ehara, T., Shirai, K. (1998) A
Study on Keyword spotting using Keyword pair
models for Synchronization of Text and Speech,
Acoustical Society of Japan, Spring meeting, 2-Q-
13, in Japanese.
Tanahashi D. (1998) Study on Caption Presentation
for TV news programs for the hearing impaired
Waseda University, Department of Information and
Computer Science (master&apos;s thesis) in Japanese.
Wakao, T., Ehara, E., Sawamura, E., Abe, Y., Shirai,
K. (1997) Application of NLP technology to
production of closed-caption TV programs in
Japanese for the hearing impaired ACL 97
workshop, Natural Language Processing for
Communication Aids, pp 55-58.
</reference>
<figure confidence="0.99814025">
tr• s
• • 11* M riJ 11
Mans
ttistCYA
</figure>
<page confidence="0.956547">
1344
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.006066">
<title confidence="0.82397825">Project for production of closed-caption TV programs for the hearing impaired Takahiro Walcao Telecommunications Advancement</title>
<affiliation confidence="0.723027">Organization of Japan</affiliation>
<address confidence="0.643106">Uehara Shibuya-ku, Tokyo 151-0064, Japan</address>
<email confidence="0.944074">walcao@shibuya.tao.or.jp</email>
<title confidence="0.396527428571429">Eiji Sawamura TAO Terumasa Ehara NHK Science and Technical Research Lab / TAO Ichiro Maruyama TAO</title>
<author confidence="0.978456">Katsuhiko Shirai</author>
<affiliation confidence="0.7633105">Waseda University, Department of Information and Computer Science / TAO</affiliation>
<abstract confidence="0.999015379310345">We describe an on-going project whose primary aim is to establish the technology of producing closed captions for TV news programs efficiently using natural language processing and speech recognition techniques for the benefit of the hearing impaired in Japan. The project is supported by the Telecommunications Advancement Organisation of Japan with the help of the ministry of Posts and Telecommunications. We propose natural language and speech processing techniques should be used for efficient closed caption production of TV programs. They enable us to summarise TV news texts into captions automatically, and synchronise TV news texts with speech and video automatically. Then the captions are superimposed on the screen. We propose a combination of shallow methods for the summarisation. For all the sentences in the original text, an importance measure is computed based on key words in the text to determine which sentences are important. If some parts of the sentences are judged unimportant, they are shortened or deleted. We also propose keyword pair model for the synchronisation between text and speech.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H P Edmundson</author>
</authors>
<title>New Methods in Automatic Extracting</title>
<date>1969</date>
<journal>Journal of the ACM,</journal>
<volume>16</volume>
<issue>2</issue>
<pages>264--285</pages>
<marker>Edmundson, 1969</marker>
<rawString>Edmundson, H.P. (1969) New Methods in Automatic Extracting Journal of the ACM, 16(2), pp 264-285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Ehara</author>
<author>T Wakao</author>
<author>E Sawamura</author>
<author>I Maruyama</author>
<author>Y Abe</author>
<author>K Shirai</author>
</authors>
<title>Application of natural language processing and speech processing technology to production of closed-caption TV programs for the hearing impaired NLPRS</title>
<date>1997</date>
<contexts>
<context position="6332" citStr="Ehara et al 1997" startWordPosition="1008" endWordPosition="1011">ent captions on TV screen to the hearing impaired people is briefly mentioned. 3.1 Automatic Text Summarisation We have a combination of shallow processing methods for automatic text summarisation. The first is to compute key words in a text and importance measures for each sentence, and then select importanct sentences for the text. The second is to shoten or delete unimportant parts in a sentence using Japanese language-specific rules. 3.1.1 Sentence Extraction Ehara found that compared with newspaper text, TV news texts have longer sentences and each text has a smaller number of sentences (Ehara et al 1997). If we summarise TV news text by selecting sentences from the orignal text, it would be &apos;rough&apos; summarisation. On the other hand, if we devide long sentences into smaller units, thus increase the number of sentences in the text, we may have finer and better summarisation (Kim &amp; Ehara 1994). Therefore what is done in the system is that if a sentence in a given text is too long, it will be partitioned into smaller units with minimun changes made to the original sentence. To compute importance measures for each sentence, we need to find first key words of the text. We tested high-frequency key w</context>
</contexts>
<marker>Ehara, Wakao, Sawamura, Maruyama, Abe, Shirai, 1997</marker>
<rawString>Ehara, T., Wakao, T., Sawamura, E., Maruyama I., Abe Y., Shirai K. (1997) Application of natural language processing and speech processing technology to production of closed-caption TV programs for the hearing impaired NLPRS 1997</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y B Kim</author>
<author>T Ehara</author>
</authors>
<title>A method of partitioning of long Japanese sentences with subject resolution</title>
<date>1994</date>
<booktitle>in J/E machine translation, Proc. of 1994 International Conference on Computer Processing of Oriental Languages,</booktitle>
<pages>467--473</pages>
<contexts>
<context position="6623" citStr="Kim &amp; Ehara 1994" startWordPosition="1058" endWordPosition="1061">en select importanct sentences for the text. The second is to shoten or delete unimportant parts in a sentence using Japanese language-specific rules. 3.1.1 Sentence Extraction Ehara found that compared with newspaper text, TV news texts have longer sentences and each text has a smaller number of sentences (Ehara et al 1997). If we summarise TV news text by selecting sentences from the orignal text, it would be &apos;rough&apos; summarisation. On the other hand, if we devide long sentences into smaller units, thus increase the number of sentences in the text, we may have finer and better summarisation (Kim &amp; Ehara 1994). Therefore what is done in the system is that if a sentence in a given text is too long, it will be partitioned into smaller units with minimun changes made to the original sentence. To compute importance measures for each sentence, we need to find first key words of the text. We tested high-frequency key word method (Luhn 1957, Edumundson 1969) and a TF-IDF-based (Text frequency, Inverse Document Frequency) method. We evaluated the two methods using ten thousand TV news texts, and found that high-frequency key word method showed slightly better results than the method based on TF-IDF scores </context>
</contexts>
<marker>Kim, Ehara, 1994</marker>
<rawString>Kim Y.B., Ehara, T. (1994) A method of partitioning of long Japanese sentences with subject resolution in J/E machine translation, Proc. of 1994 International Conference on Computer Processing of Oriental Languages, pp.467-473.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Komine</author>
<author>H Hoshino</author>
<author>H Isom</author>
<author>T Uchida</author>
<author>Y Iwahana</author>
</authors>
<title>Cognitive Experiments of News Captioning for Hearing Impaired Persons</title>
<date>1996</date>
<journal>Technical Report of IECE (The Institution of Electronics, Information and Communication Engineers), HCS96-23, in Japanese,</journal>
<pages>7--12</pages>
<marker>Komine, Hoshino, Isom, Uchida, Iwahana, 1996</marker>
<rawString>Komine, K., Hoshino, H., Isom, H., Uchida, T., Iwahana, Y. (1996) Cognitive Experiments of News Captioning for Hearing Impaired Persons Technical Report of IECE (The Institution of Electronics, Information and Communication Engineers), HCS96-23, in Japanese, pp 7-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Luhn</author>
</authors>
<title>A statistical approach to the mechanized encoding and searching of literary information</title>
<date>1957</date>
<journal>IBM Journal of Research and Development,</journal>
<volume>1</volume>
<issue>4</issue>
<pages>309--317</pages>
<contexts>
<context position="6953" citStr="Luhn 1957" startWordPosition="1119" endWordPosition="1120">arise TV news text by selecting sentences from the orignal text, it would be &apos;rough&apos; summarisation. On the other hand, if we devide long sentences into smaller units, thus increase the number of sentences in the text, we may have finer and better summarisation (Kim &amp; Ehara 1994). Therefore what is done in the system is that if a sentence in a given text is too long, it will be partitioned into smaller units with minimun changes made to the original sentence. To compute importance measures for each sentence, we need to find first key words of the text. We tested high-frequency key word method (Luhn 1957, Edumundson 1969) and a TF-IDF-based (Text frequency, Inverse Document Frequency) method. We evaluated the two methods using ten thousand TV news texts, and found that high-frequency key word method showed slightly better results than the method based on TF-IDF scores (Wakao et at 1997). 3.1.2 Rules for shortening text Another way of reducing the number of characters in a Japanese text, thus summarising the text, is to shorten or delete parts of the sentences. For example, if a sentence ends with a sahen verb followed by its inflection, or helping verbs or particles to express proper politene</context>
</contexts>
<marker>Luhn, 1957</marker>
<rawString>Luhn, H.P. (1957) A statistical approach to the mechanized encoding and searching of literary information IBM Journal of Research and Development, 1(4), pp 309-317.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Maruyama</author>
<author>Y Abe</author>
<author>T Ehara</author>
<author>K Shirai</author>
</authors>
<title>A Study on Keyword spotting using Keyword pair models for Synchronization of Text and Speech, Acoustical Society of Japan, Spring meeting, 2-Q13,</title>
<date>1998</date>
<note>in Japanese.</note>
<marker>Maruyama, Abe, Ehara, Shirai, 1998</marker>
<rawString>Maruyama, I., Abe, Y., Ehara, T., Shirai, K. (1998) A Study on Keyword spotting using Keyword pair models for Synchronization of Text and Speech, Acoustical Society of Japan, Spring meeting, 2-Q13, in Japanese.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Tanahashi</author>
</authors>
<title>Study on Caption Presentation for TV news programs for the hearing impaired</title>
<date>1998</date>
<institution>Waseda University, Department of Information and Computer Science</institution>
<note>(master&apos;s thesis) in Japanese.</note>
<contexts>
<context position="12699" citStr="Tanahashi, 1998" startWordPosition="2109" endWordPosition="2110">n how to present captions on TV screen Null arc Ke ord set1 Keywords t2 1343 to the hearing impaired people. We superimposed captions by hand on several kinds of TV programs. They were evaluated by the hadicapped people (hard of hearing persons) in terms of the following points: • characters : size, font, colour • number of lines • timing • location • methods of scrolling • inside or outside of the picture (see two examples below). Figure 3 Captions in the picture Figure 4 Captions outside of the picture Most of the subjects preferred 2-line, outside of the picture captions without scrolling (Tanahashi, 1998). This was still a preliminary study, and we plan to conduct similar evaluation by the hearing impaired people on large scale. Conclusion We have described a national project, its research issues and schedule, as well as preliminary research results. The project aim is to establish language and speech processing technology so that TV news program text is summarised and changed into captions, and synchronised with the speech, and superimposed to the original program for the benefits of the hearing impaired. We will continue to conduct research and build a prototype TV caption production system,</context>
</contexts>
<marker>Tanahashi, 1998</marker>
<rawString>Tanahashi D. (1998) Study on Caption Presentation for TV news programs for the hearing impaired Waseda University, Department of Information and Computer Science (master&apos;s thesis) in Japanese.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wakao</author>
<author>E Ehara</author>
<author>E Sawamura</author>
<author>Y Abe</author>
<author>K Shirai</author>
</authors>
<title>Application of NLP technology to production of closed-caption TV programs in Japanese for the hearing impaired ACL 97 workshop, Natural Language Processing for Communication Aids,</title>
<date>1997</date>
<pages>55--58</pages>
<marker>Wakao, Ehara, Sawamura, Abe, Shirai, 1997</marker>
<rawString>Wakao, T., Ehara, E., Sawamura, E., Abe, Y., Shirai, K. (1997) Application of NLP technology to production of closed-caption TV programs in Japanese for the hearing impaired ACL 97 workshop, Natural Language Processing for Communication Aids, pp 55-58.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>