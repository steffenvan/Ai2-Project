<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015558">
<title confidence="0.990749">
Answering List Questions using Web as a corpus
</title>
<author confidence="0.997126">
Patricia Nunes Gonc¸alves, Ant´onio Branco
</author>
<affiliation confidence="0.875186333333333">
University of Lisbon
Edificio C6, Departamento de Inform´atica
Faculdade de Ciˆencias, Universidade de Lisboa
</affiliation>
<address confidence="0.66331">
Campo Grande, 1749-016 Lisboa, Portugal
</address>
<email confidence="0.997612">
{patricia.nunes, antonio.branco}@di.fc.ul.pt
</email>
<sectionHeader confidence="0.993851" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9994334">
This paper supports the demo of LX-
ListQuestion, a Web Question Answering
System that exploit the redundancy of in-
formation available in the Web to answer
List Questions in the form of Word Cloud.
</bodyText>
<sectionHeader confidence="0.999261" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99954575">
The combination of web growth and improve-
ments in Information Technology has reignited the
interest in Question Answering (QA) systems. QA
is a type of information retrieval combined with
natural language processing techniques that aims
at finding exact answers to natural language ques-
tions. In a search engine, the user inserts a few
keywords and gets as a result links and snippets.
The task of finding the desired answer among the
results that were returned then falls on the user.
From the point of view of QA, in turn, the users
use a question in natural language and the system
searches within the documents for the answers.
Questions have various levels of complexity.
When thinking about questions immediately come
to mind factual questions (eg: When did Nel-
son Mandela die?), however, the QA area has
expanded beyond factual questions towards more
complex questions. One of the most common
types of complex factual is list questions. The list
questions are questions for which there is a list
of answers, e.g., In which countries Portuguese
is an official language? List answers: Angola,
Brazil, Cape Verde, Guine Bissau, Mozambique,
Portugal, Macau, Sao Tome and Principe and East
Timor.
When the information that is needed is non-
trivial and it is found spread over several texts, a
lot of human effort is required to gather the vari-
ous separate pieces of data into the desired result,
which is not an easy task. Ideally, they would pre-
fer to quickly get a precise answer and go on to
make use of it instead of spending time searching
and compiling the answer from pieces spread over
several documents.
Our purpose is to provide better QA solutions to
users, who desire direct answers to their queries,
using approaches that deal with the complex prob-
lem of extracting answers found spread over sev-
eral documents and use them to compile a list
of answers that are the most accurate possible.
The LX-ListQuestion development is guided by
the circumstance that answers may appear redun-
dantly in many places and in many forms. Our
approach to address the problem of answering list
questions is to explore this redundancy. To build
on this redundancy, we use techniques that will be
explained in section 4.
</bodyText>
<sectionHeader confidence="0.999691" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999806260869565">
List QA is an emerging topic and few approaches
have been developed. The most common approach
is to take a QA system for factoid questions and
extend it to answer List questions. Some pioneer-
ing systems using this approach are (Gaizauskas et
al., 2005) and (Wu and Strzalkowski, 2006), which
show a low performance. Other systems explore
NLP tools and linguistic resources (Hickl et al.,
2006) (Yang et al., 2003). This approach seems
to have achieved competitive results. However the
time required for processing is very high and the
performance of these systems depend on the per-
formance of the supporting NLP tools.
Other approaches resort to statistical and ma-
chine learning approaches. The system developed
in (Whittaker et al., 2006) is based on a statisti-
cal model. The system developed by (Yang and
Chua, 2004) employ classification techniques to
find complete and distinct answers. The system
proposed by (Razmara and Kosseim, 2008) an-
swers List questions using a clustering method to
group candidate answers that co-occur more often
in the collection.
</bodyText>
<page confidence="0.988237">
81
</page>
<note confidence="0.772699">
Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 81–84,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999946">
Systems that take advantage from semantic con-
tent to answer List questions (Cardoso et al.,
2009), (Hartrumpf and Leveling, 2010), (Dor-
nescu, 2009) achieved good results although all
information should be stored in the database. This
approach seems suitable to QA system that focus
on a specific domain where the information source
can be limited and more easily stored.
</bodyText>
<sectionHeader confidence="0.952894" genericHeader="method">
3 List Question
</sectionHeader>
<bodyText confidence="0.999781857142857">
In the context of QA research, list questions may
appear in three basic forms: (1) a question starting
with an interrogative pronoun, (2) a request using
an imperative verb and (3) other forms: without
interrogative pronoun or imperative verb (usually
a complex noun phrase). Table 1 shows some ex-
amples.
</bodyText>
<table confidence="0.99039375">
Type of Example
List Question
Interrogative What European Union countries
Pronoun have national parks in the Alps?
Imperative Name rare diseases with dedicated
Form research centers in Europe.
Other Chefs born in Austria who have
received a Michelin Star.
</table>
<tableCaption confidence="0.999749">
Table 1: Examples of List questions
</tableCaption>
<bodyText confidence="0.999657346153846">
Another important topic in QA is to identify the
so called question target. Our study shows that tar-
get can be expressed by a named entity, a common
noun or be multiple targets. Most List Questions
have named entities as target question, e.g. Which
cities in Germany have more than one university?.
Common noun is not so frequent as target question
but it can show up, e.g. Typical food of the Cape
Verde cuisine. Multiples target can be of named
entities or common nouns, e.g. Newspapers, mag-
azines and other periodicals of Macau.
The list answers (for a list question) may appear
in many places and in many forms. They can be in
the same document; when the answer is already a
list, e.g., list of cities in Portugal: Lisbon, Coim-
bra, Porto e Faro; or the answers can be spread
over multiple documents; e.g., (document A): Lis-
bon is the capital of Portugal. (document B) Porto
is a very important city in Portugal. In the latter
case, a QA system able to answer List questions
has to deal with this diversity and find all answers
of several texts and compose the final list of an-
swers.
Our system focuses on answering List questions
where the answers are extracted from several doc-
uments from the Web.
</bodyText>
<sectionHeader confidence="0.987182" genericHeader="method">
4 LX-ListQuestion Architecture
</sectionHeader>
<bodyText confidence="0.967920375">
LX-ListQuestion System seeks to answer List
questions through the use of the techniques of
Question Answering running over the Web of Por-
tuguese pages, while ensuring that the Final An-
swer List is as correct and complete as possible.
This system has three main modules: Question
Processing, Passage Retrieval and Answer Extrac-
tion. Figure 1 shows its architecture.
</bodyText>
<figureCaption confidence="0.910672">
Figure 1: Question Answering System Architec-
ture
</figureCaption>
<bodyText confidence="0.999967032258065">
The Question Processing module is responsible
for converting a natural language question into a
form that a computer is capable of handling and
extracting the information that will be passed and
used by the subsequent modules. Question Anal-
ysis task is responsible to clean the questions,
i.e. removing question marks, interrogative pro-
noun and imperative verbs. Besides identifing
each meaningful element of the question, the sys-
tem annotate each word with their part-of-speech
tag. The following information are set apart: main
verb, question target, named entity.
The Passage Retrieval module is responsible for
searching web pages using the keywords into the
question and save them into local files for post-
processing. This version of the system is work-
ing with 10 downloaded files.This module is also
responsible for cleaning the HTML files and sav-
ing into local files only the content information.
We use a relevance score based on the average of
the number of words in the question to preserves
the original idea of the question. After the content
is saved into a file, the system will select the rel-
evant sentences based on matching and counting
the keywords in the sentences.
The Answer Extraction Module aims at identi-
fying and extracting relevant answers and presents
them in the form the a list. This module has
two main tasks: Candidate Answer Identification
and Building the List Answer. Candidate Answer
Identification task extracts all words tagged with
</bodyText>
<page confidence="0.993793">
82
</page>
<bodyText confidence="0.999839714285714">
the proper name tag (in this version of the sys-
tem version we are assuming that all answers are
proper names).
The process of Building the List Answers based
on frequency and word occur rules. The process
based on frequency uses two lists: Premium List
and Work List. The Premium List is composed
by candidates extracted from sentences previously
classified as highly relevant and will serve to guide
the rest of the processing. If we were to consider
only these elements, the list of answers would
probably contain correct items. However, the list
may be incomplete and lack elements. Then, con-
tinuing in the same vein of our strategy, the Work
List is build with candidates extracted from sen-
tences classified as medium and low. This list will
be used to confirm and expand the elements in
the list. The word occur rules take advantage of
the title of web page, of sentences that perfectly
matches with the question and of candidate verb
that matches with the question verb.
</bodyText>
<subsectionHeader confidence="0.527325">
4.1 Results
</subsectionHeader>
<bodyText confidence="0.9204825">
For the experiments we used a set of 10 questions1
that require List Answers:
</bodyText>
<listItem confidence="0.992183708333333">
Q1: Instrumentos musicais de origem africana
comuns no Brasil. African musical instruments
common in Brazil.
Q2: Parques do Rio de Janeiro que tˆem ca-
choeiras. Parks of Rio de Janeiro that have
waterfalls.
Q3: Igrejas em Macau. Churches in Macau.
Q4: Cidades que fizeram parte do dominio por-
tuguˆes na ´India. Cities in India that were under
Portuguese rule.
Q5: Parques nacionais de Moc¸ambique. National
parks in Mozambique
Q6: Ilhas de Moc¸ambique. Islands of Mozam-
bique
Q7: Movimentos culturais surgidos no nordeste
do Brasil. Cultural movements that emerged n the
northeast of Brazil.
Q8: Dioceses cat´olicas de Moc¸ambique. Catholic
dioceses in Mozambique.
Q9: Candidatos a alguma das eleic¸˜oes presiden-
ciais na Guin´e-Bissau. Candidates for any of the
presidential elections in Guinea-Bissau.
Q10: Capitais das provincias de Angola. Capitals
of the provinces in Angola.
</listItem>
<footnote confidence="0.9847105">
1These questions were based on Pagico:
www.linguateca.pt/Pagico
</footnote>
<bodyText confidence="0.999754222222222">
Table 2 shows the metric evaluation of the LX-
ListQuestion. The metrics used are: recall, preci-
sion and F-measure. These metrics take into con-
sideration two lists: a reference list (correct an-
swers expected) and the system list (answers re-
turned by the QA system). Precision: C is the
number of common elements between reference
and system lists and S is the number of elements
given by the system.
</bodyText>
<equation confidence="0.777499">
Precision = CS
Recall: C is the number of common elements be-
</equation>
<bodyText confidence="0.911852">
tween reference and system lists and L is the num-
ber of elements in reference list.
</bodyText>
<equation confidence="0.734916">
Recall = i
</equation>
<bodyText confidence="0.808174">
F-measure: its the combination between Recall
and Precision.
</bodyText>
<equation confidence="0.7829755">
F − measure = 2∗Recall∗Precision
Recall+Precision
</equation>
<tableCaption confidence="0.990669">
Table 2: Metric Evaluation.
</tableCaption>
<table confidence="0.999889583333333">
Question Precision Recall F-Measure
Q1 0.15 0.36 0.21
Q2 0.08 0.50 0.14
Q3 0.13 0.35 0.18
Q4 0.14 0.23 0.17
Q5 0.10 0.75 0.18
Q6 0.13 0.42 0.20
Q7 0.05 0.20 0.08
Q8 0.15 0.71 0.24
Q9 0.05 0.25 0.09
Q10 0.38 0.42 0.40
AVERAGE 0.14 0.38 0.20
</table>
<bodyText confidence="0.9995074">
We observe from Table 2 that the system an-
swered all questions. It achieved better recall for
the questions Q5 and Q8. The Question Q10 ob-
tained better precision and also f-measure. Over-
all, the system scores 0.38 of recall, which is a
very competitive result for the current state-of-art.
Exploring the redundancy of information seems to
be a good approach to this task, but it alone cannot
handle all problems. The word occur rules imple-
mented was important step to enrich the system.
</bodyText>
<subsectionHeader confidence="0.987687">
4.2 User interface
</subsectionHeader>
<bodyText confidence="0.919166285714286">
LX-ListQuestion is available on the web:
http://nlxserv.di.fc.ul.pt/lxlistquestion/
In our tests, the response time ranged between 16
and 28 seconds of processing from submitting the
question and getting the list of answers. We chose
to use word cloud instead of a traditional list as
presentation of results because the final list an-
</bodyText>
<page confidence="0.996045">
83
</page>
<bodyText confidence="0.998062">
swers evidence not only the correct answers but
of the possibly relevant words related to the ques-
tion. The confidence that the system has a given
answer is based on the frequency of words found
in the texts collected from the web. The most fre-
quent words are represented in the word cloud us-
ing a greater font size. The word cloud also helps
the user to understand the context in which the
answers may be embedded. Figure 2 shows LX-
ListQuestion online GUI.
</bodyText>
<figureCaption confidence="0.997759">
Figure 2: LX-ListQuestion online GUI
</figureCaption>
<sectionHeader confidence="0.978005" genericHeader="conclusions">
5 Concluding remarks
</sectionHeader>
<bodyText confidence="0.999982454545455">
LX-ListQuestion is a fully-fledged Web based QA
system that generates answers to list questions and
presents them in a word cloud. The system ex-
ploits the redundancy of information available in
the Web and combine with word occur rules to
improve QA accuracy. This version handles Por-
tuguese Language. The next version will be ex-
tended to provide answers to other languages as
well. This work has being developed as the sub-
ject of a progressing doctoral thesis and new im-
provements will be implemented.
</bodyText>
<sectionHeader confidence="0.998973" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99953940625">
Nuno Cardoso, David Batista, Francisco J. L´opez-
Pellicer, and M´ario J. Silva. 2009. Where in the
wikipedia is that answer? the xldb at the giki-
clef 2009 task. In Carol Peters, Giorgio Maria Di
Nunzio, Mikko Kurimo, Djamel Mostefa, Anselmo
Pe˜nas, and Giovanna Roda, editors, CLEF, volume
6241 of Lecture Notes in Computer Science, pages
305–309. Springer.
Iustin Dornescu. 2009. Semantic qa for encyclopaedic
questions: Equal in gikiclef. In Carol Peters,
Giorgio Maria Di Nunzio, Mikko Kurimo, Djamel
Mostefa, Anselmo Pe˜nas, and Giovanna Roda, edi-
tors, CLEF, volume 6241 of Lecture Notes in Com-
puter Science, pages 326–333. Springer.
Robert J. Gaizauskas, Mark A. Greenwood, Henk
Harkema, Mark Hepple, Horacio Saggion, and
Atheesh Sanka. 2005. The university of sheffield s
trec 2005 q&amp;a experiments. In Ellen M. Voorhees
and Lori P. Buckland, editors, Proceedings of the
Fourteenth Text REtrieval Conference, TREC 2005,
Gaithersburg, Maryland, November 15-18, 2005,
volume Special Publication 500-266. National Insti-
tute of Standards and Technology (NIST).
Sven Hartrumpf and Johannes Leveling. 2010.
GIRSA-WP at GikiCLEF: Integration of structured
information and decomposition of questions. In
10th Workshop of the Cross-Language Evaluation
Forum, CLEF 2009, Corfu, Greece, September 30-
October 2, Revised Selected Papers, Lecture Notes
in Computer Science (LNCS). Springer. (to appear).
Andrew Hickl, John Williams, Jeremy Bensley, Kirk
Roberts, Ying Shi, and Bryan Rink. 2006. Question
answering with lcc’s chaucer at trec 2006. In TREC.
Majid Razmara and Leila Kosseim. 2008. Answer-
ing list questions using co-occurrence and cluster-
ing. In Proceedings of the International Confer-
ence on Language Resources and Evaluation, LREC
2008, 26 May - 1 June 2008, Marrakech, Morocco.
European Language Resources Association.
Edward W. D. Whittaker, Josef R. Novak, Pierre
Chatain, and Sadaoki Furui. 2006. Trec 2006
question answering experiments at tokyo institute
of technology. In Ellen M. Voorhees and Lori P.
Buckland, editors, Proceedings of the Fifteenth Text
REtrieval Conference, TREC 2006, Gaithersburg,
Maryland, November 14-17, 2006, volume Special
Publication 500-272. National Institute of Standards
and Technology (NIST).
Min Wu and Tomek Strzalkowski. 2006. Utilizing co-
occurrence of answers in question answering. In
ACL 2006, 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics, Pro-
ceedings of the Conference, Sydney, Australia, 17-21
July 2006. The Association for Computer Linguis-
tics.
Hui Yang and Tat-Seng Chua. 2004. Web-based list
question answering. In COLING ’04: Proceed-
ings of the 20th international conference on Com-
putational Linguistics, page 1277, Morristown, NJ,
USA. Association for Computational Linguistics.
Hui Yang, Hang Cui, Mstislav Maslennikov, Long Qiu,
Min-Yen Kan, and Tat-Seng Chua. 2003. Qualifier
in trec-12 qa main task. In TREC, pages 480–488.
</reference>
<page confidence="0.999229">
84
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.128865">
<title confidence="0.996197">Answering List Questions using Web as a corpus</title>
<author confidence="0.996882">Nunes Ant´onio</author>
<affiliation confidence="0.992388">University of</affiliation>
<note confidence="0.302859666666667">Edificio C6, Departamento de Faculdade de Ciˆencias, Universidade de Campo Grande, 1749-016 Lisboa,</note>
<abstract confidence="0.927637333333333">This paper supports the demo of LX- ListQuestion, a Web Question Answering System that exploit the redundancy of information available in the Web to answer List Questions in the form of Word Cloud.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nuno Cardoso</author>
<author>David Batista</author>
<author>Francisco J L´opezPellicer</author>
<author>M´ario J Silva</author>
</authors>
<title>Where in the wikipedia is that answer? the xldb at the gikiclef 2009 task.</title>
<date>2009</date>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>6241</volume>
<pages>305--309</pages>
<editor>In Carol Peters, Giorgio Maria Di Nunzio, Mikko Kurimo, Djamel Mostefa, Anselmo Pe˜nas, and Giovanna Roda, editors, CLEF,</editor>
<publisher>Springer.</publisher>
<marker>Cardoso, Batista, L´opezPellicer, Silva, 2009</marker>
<rawString>Nuno Cardoso, David Batista, Francisco J. L´opezPellicer, and M´ario J. Silva. 2009. Where in the wikipedia is that answer? the xldb at the gikiclef 2009 task. In Carol Peters, Giorgio Maria Di Nunzio, Mikko Kurimo, Djamel Mostefa, Anselmo Pe˜nas, and Giovanna Roda, editors, CLEF, volume 6241 of Lecture Notes in Computer Science, pages 305–309. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iustin Dornescu</author>
</authors>
<title>Semantic qa for encyclopaedic questions: Equal in gikiclef.</title>
<date>2009</date>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>6241</volume>
<pages>326--333</pages>
<editor>In Carol Peters, Giorgio Maria Di Nunzio, Mikko Kurimo, Djamel Mostefa, Anselmo Pe˜nas, and Giovanna Roda, editors, CLEF,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="4177" citStr="Dornescu, 2009" startWordPosition="670" endWordPosition="672">assification techniques to find complete and distinct answers. The system proposed by (Razmara and Kosseim, 2008) answers List questions using a clustering method to group candidate answers that co-occur more often in the collection. 81 Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 81–84, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Systems that take advantage from semantic content to answer List questions (Cardoso et al., 2009), (Hartrumpf and Leveling, 2010), (Dornescu, 2009) achieved good results although all information should be stored in the database. This approach seems suitable to QA system that focus on a specific domain where the information source can be limited and more easily stored. 3 List Question In the context of QA research, list questions may appear in three basic forms: (1) a question starting with an interrogative pronoun, (2) a request using an imperative verb and (3) other forms: without interrogative pronoun or imperative verb (usually a complex noun phrase). Table 1 shows some examples. Type of Example List Question Interrogative What Europe</context>
</contexts>
<marker>Dornescu, 2009</marker>
<rawString>Iustin Dornescu. 2009. Semantic qa for encyclopaedic questions: Equal in gikiclef. In Carol Peters, Giorgio Maria Di Nunzio, Mikko Kurimo, Djamel Mostefa, Anselmo Pe˜nas, and Giovanna Roda, editors, CLEF, volume 6241 of Lecture Notes in Computer Science, pages 326–333. Springer.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Robert J Gaizauskas</author>
<author>Mark A Greenwood</author>
</authors>
<title>Henk Harkema, Mark Hepple, Horacio Saggion, and Atheesh Sanka.</title>
<date>2005</date>
<booktitle>Proceedings of the Fourteenth Text REtrieval Conference, TREC 2005,</booktitle>
<volume>volume</volume>
<editor>In Ellen M. Voorhees and Lori P. Buckland, editors,</editor>
<location>Gaithersburg, Maryland,</location>
<marker>Gaizauskas, Greenwood, 2005</marker>
<rawString>Robert J. Gaizauskas, Mark A. Greenwood, Henk Harkema, Mark Hepple, Horacio Saggion, and Atheesh Sanka. 2005. The university of sheffield s trec 2005 q&amp;a experiments. In Ellen M. Voorhees and Lori P. Buckland, editors, Proceedings of the Fourteenth Text REtrieval Conference, TREC 2005, Gaithersburg, Maryland, November 15-18, 2005, volume Special Publication 500-266. National Institute of Standards and Technology (NIST).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sven Hartrumpf</author>
<author>Johannes Leveling</author>
</authors>
<title>GIRSA-WP at GikiCLEF: Integration of structured information and decomposition of questions.</title>
<date>2010</date>
<booktitle>In 10th Workshop of the Cross-Language Evaluation Forum, CLEF 2009, Corfu, Greece, September 30-October 2, Revised Selected Papers, Lecture Notes in Computer Science (LNCS).</booktitle>
<publisher>Springer.</publisher>
<note>(to appear).</note>
<contexts>
<context position="4159" citStr="Hartrumpf and Leveling, 2010" startWordPosition="666" endWordPosition="669"> (Yang and Chua, 2004) employ classification techniques to find complete and distinct answers. The system proposed by (Razmara and Kosseim, 2008) answers List questions using a clustering method to group candidate answers that co-occur more often in the collection. 81 Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 81–84, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Systems that take advantage from semantic content to answer List questions (Cardoso et al., 2009), (Hartrumpf and Leveling, 2010), (Dornescu, 2009) achieved good results although all information should be stored in the database. This approach seems suitable to QA system that focus on a specific domain where the information source can be limited and more easily stored. 3 List Question In the context of QA research, list questions may appear in three basic forms: (1) a question starting with an interrogative pronoun, (2) a request using an imperative verb and (3) other forms: without interrogative pronoun or imperative verb (usually a complex noun phrase). Table 1 shows some examples. Type of Example List Question Interro</context>
</contexts>
<marker>Hartrumpf, Leveling, 2010</marker>
<rawString>Sven Hartrumpf and Johannes Leveling. 2010. GIRSA-WP at GikiCLEF: Integration of structured information and decomposition of questions. In 10th Workshop of the Cross-Language Evaluation Forum, CLEF 2009, Corfu, Greece, September 30-October 2, Revised Selected Papers, Lecture Notes in Computer Science (LNCS). Springer. (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Hickl</author>
<author>John Williams</author>
<author>Jeremy Bensley</author>
<author>Kirk Roberts</author>
<author>Ying Shi</author>
<author>Bryan Rink</author>
</authors>
<title>Question answering with lcc’s chaucer at trec</title>
<date>2006</date>
<booktitle>In TREC.</booktitle>
<contexts>
<context position="3127" citStr="Hickl et al., 2006" startWordPosition="509" endWordPosition="512"> many places and in many forms. Our approach to address the problem of answering list questions is to explore this redundancy. To build on this redundancy, we use techniques that will be explained in section 4. 2 Related Work List QA is an emerging topic and few approaches have been developed. The most common approach is to take a QA system for factoid questions and extend it to answer List questions. Some pioneering systems using this approach are (Gaizauskas et al., 2005) and (Wu and Strzalkowski, 2006), which show a low performance. Other systems explore NLP tools and linguistic resources (Hickl et al., 2006) (Yang et al., 2003). This approach seems to have achieved competitive results. However the time required for processing is very high and the performance of these systems depend on the performance of the supporting NLP tools. Other approaches resort to statistical and machine learning approaches. The system developed in (Whittaker et al., 2006) is based on a statistical model. The system developed by (Yang and Chua, 2004) employ classification techniques to find complete and distinct answers. The system proposed by (Razmara and Kosseim, 2008) answers List questions using a clustering method to</context>
</contexts>
<marker>Hickl, Williams, Bensley, Roberts, Shi, Rink, 2006</marker>
<rawString>Andrew Hickl, John Williams, Jeremy Bensley, Kirk Roberts, Ying Shi, and Bryan Rink. 2006. Question answering with lcc’s chaucer at trec 2006. In TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Majid Razmara</author>
<author>Leila Kosseim</author>
</authors>
<title>Answering list questions using co-occurrence and clustering.</title>
<date>2008</date>
<journal>European Language Resources Association.</journal>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation, LREC 2008, 26 May - 1</booktitle>
<location>Marrakech,</location>
<contexts>
<context position="3675" citStr="Razmara and Kosseim, 2008" startWordPosition="596" endWordPosition="599"> Other systems explore NLP tools and linguistic resources (Hickl et al., 2006) (Yang et al., 2003). This approach seems to have achieved competitive results. However the time required for processing is very high and the performance of these systems depend on the performance of the supporting NLP tools. Other approaches resort to statistical and machine learning approaches. The system developed in (Whittaker et al., 2006) is based on a statistical model. The system developed by (Yang and Chua, 2004) employ classification techniques to find complete and distinct answers. The system proposed by (Razmara and Kosseim, 2008) answers List questions using a clustering method to group candidate answers that co-occur more often in the collection. 81 Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 81–84, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Systems that take advantage from semantic content to answer List questions (Cardoso et al., 2009), (Hartrumpf and Leveling, 2010), (Dornescu, 2009) achieved good results although all information should be stored in the database. This approach se</context>
</contexts>
<marker>Razmara, Kosseim, 2008</marker>
<rawString>Majid Razmara and Leila Kosseim. 2008. Answering list questions using co-occurrence and clustering. In Proceedings of the International Conference on Language Resources and Evaluation, LREC 2008, 26 May - 1 June 2008, Marrakech, Morocco. European Language Resources Association.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Edward W D Whittaker</author>
<author>Josef R Novak</author>
<author>Pierre Chatain</author>
<author>Sadaoki Furui</author>
</authors>
<title>Trec</title>
<date>2006</date>
<booktitle>Proceedings of the Fifteenth Text REtrieval Conference, TREC 2006,</booktitle>
<volume>volume</volume>
<editor>In Ellen M. Voorhees and Lori P. Buckland, editors,</editor>
<location>Gaithersburg, Maryland,</location>
<contexts>
<context position="3473" citStr="Whittaker et al., 2006" startWordPosition="564" endWordPosition="567"> for factoid questions and extend it to answer List questions. Some pioneering systems using this approach are (Gaizauskas et al., 2005) and (Wu and Strzalkowski, 2006), which show a low performance. Other systems explore NLP tools and linguistic resources (Hickl et al., 2006) (Yang et al., 2003). This approach seems to have achieved competitive results. However the time required for processing is very high and the performance of these systems depend on the performance of the supporting NLP tools. Other approaches resort to statistical and machine learning approaches. The system developed in (Whittaker et al., 2006) is based on a statistical model. The system developed by (Yang and Chua, 2004) employ classification techniques to find complete and distinct answers. The system proposed by (Razmara and Kosseim, 2008) answers List questions using a clustering method to group candidate answers that co-occur more often in the collection. 81 Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 81–84, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Systems that take advantage from semantic c</context>
</contexts>
<marker>Whittaker, Novak, Chatain, Furui, 2006</marker>
<rawString>Edward W. D. Whittaker, Josef R. Novak, Pierre Chatain, and Sadaoki Furui. 2006. Trec 2006 question answering experiments at tokyo institute of technology. In Ellen M. Voorhees and Lori P. Buckland, editors, Proceedings of the Fifteenth Text REtrieval Conference, TREC 2006, Gaithersburg, Maryland, November 14-17, 2006, volume Special Publication 500-272. National Institute of Standards and Technology (NIST).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Wu</author>
<author>Tomek Strzalkowski</author>
</authors>
<title>Utilizing cooccurrence of answers in question answering.</title>
<date>2006</date>
<booktitle>In ACL 2006, 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference,</booktitle>
<location>Sydney, Australia,</location>
<contexts>
<context position="3018" citStr="Wu and Strzalkowski, 2006" startWordPosition="492" endWordPosition="495">urate possible. The LX-ListQuestion development is guided by the circumstance that answers may appear redundantly in many places and in many forms. Our approach to address the problem of answering list questions is to explore this redundancy. To build on this redundancy, we use techniques that will be explained in section 4. 2 Related Work List QA is an emerging topic and few approaches have been developed. The most common approach is to take a QA system for factoid questions and extend it to answer List questions. Some pioneering systems using this approach are (Gaizauskas et al., 2005) and (Wu and Strzalkowski, 2006), which show a low performance. Other systems explore NLP tools and linguistic resources (Hickl et al., 2006) (Yang et al., 2003). This approach seems to have achieved competitive results. However the time required for processing is very high and the performance of these systems depend on the performance of the supporting NLP tools. Other approaches resort to statistical and machine learning approaches. The system developed in (Whittaker et al., 2006) is based on a statistical model. The system developed by (Yang and Chua, 2004) employ classification techniques to find complete and distinct an</context>
</contexts>
<marker>Wu, Strzalkowski, 2006</marker>
<rawString>Min Wu and Tomek Strzalkowski. 2006. Utilizing cooccurrence of answers in question answering. In ACL 2006, 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, Sydney, Australia, 17-21 July 2006. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Yang</author>
<author>Tat-Seng Chua</author>
</authors>
<title>Web-based list question answering.</title>
<date>2004</date>
<booktitle>In COLING ’04: Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>1277</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="3552" citStr="Yang and Chua, 2004" startWordPosition="579" endWordPosition="582">tems using this approach are (Gaizauskas et al., 2005) and (Wu and Strzalkowski, 2006), which show a low performance. Other systems explore NLP tools and linguistic resources (Hickl et al., 2006) (Yang et al., 2003). This approach seems to have achieved competitive results. However the time required for processing is very high and the performance of these systems depend on the performance of the supporting NLP tools. Other approaches resort to statistical and machine learning approaches. The system developed in (Whittaker et al., 2006) is based on a statistical model. The system developed by (Yang and Chua, 2004) employ classification techniques to find complete and distinct answers. The system proposed by (Razmara and Kosseim, 2008) answers List questions using a clustering method to group candidate answers that co-occur more often in the collection. 81 Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 81–84, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Systems that take advantage from semantic content to answer List questions (Cardoso et al., 2009), (Hartrumpf and Leveling</context>
</contexts>
<marker>Yang, Chua, 2004</marker>
<rawString>Hui Yang and Tat-Seng Chua. 2004. Web-based list question answering. In COLING ’04: Proceedings of the 20th international conference on Computational Linguistics, page 1277, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Yang</author>
<author>Hang Cui</author>
<author>Mstislav Maslennikov</author>
<author>Long Qiu</author>
<author>Min-Yen Kan</author>
<author>Tat-Seng Chua</author>
</authors>
<date>2003</date>
<booktitle>Qualifier in trec-12 qa main task. In TREC,</booktitle>
<pages>480--488</pages>
<contexts>
<context position="3147" citStr="Yang et al., 2003" startWordPosition="513" endWordPosition="516">any forms. Our approach to address the problem of answering list questions is to explore this redundancy. To build on this redundancy, we use techniques that will be explained in section 4. 2 Related Work List QA is an emerging topic and few approaches have been developed. The most common approach is to take a QA system for factoid questions and extend it to answer List questions. Some pioneering systems using this approach are (Gaizauskas et al., 2005) and (Wu and Strzalkowski, 2006), which show a low performance. Other systems explore NLP tools and linguistic resources (Hickl et al., 2006) (Yang et al., 2003). This approach seems to have achieved competitive results. However the time required for processing is very high and the performance of these systems depend on the performance of the supporting NLP tools. Other approaches resort to statistical and machine learning approaches. The system developed in (Whittaker et al., 2006) is based on a statistical model. The system developed by (Yang and Chua, 2004) employ classification techniques to find complete and distinct answers. The system proposed by (Razmara and Kosseim, 2008) answers List questions using a clustering method to group candidate ans</context>
</contexts>
<marker>Yang, Cui, Maslennikov, Qiu, Kan, Chua, 2003</marker>
<rawString>Hui Yang, Hang Cui, Mstislav Maslennikov, Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2003. Qualifier in trec-12 qa main task. In TREC, pages 480–488.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>