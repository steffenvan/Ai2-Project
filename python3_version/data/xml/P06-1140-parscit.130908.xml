<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005261">
<title confidence="0.971722">
Learning to Say It Well:
Reranking Realizations by Predicted Synthesis Quality
</title>
<author confidence="0.993362">
Crystal Nakatsu and Michael White
</author>
<affiliation confidence="0.993491">
Department of Linguistics
The Ohio State University
</affiliation>
<address confidence="0.848429">
Columbus, OH 43210 USA
</address>
<email confidence="0.999677">
❢cnakatsu,mwhite❣@ling.ohio-state.edu
</email>
<sectionHeader confidence="0.993919" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998926">
This paper presents a method for adapting
a language generator to the strengths and
weaknesses of a synthetic voice, thereby
improving the naturalness of synthetic
speech in a spoken language dialogue sys-
tem. The method trains a discriminative
reranker to select paraphrases that are pre-
dicted to sound natural when synthesized.
The ranker is trained on realizer and syn-
thesizer features in supervised fashion, us-
ing human judgements of synthetic voice
quality on a sample of the paraphrases rep-
resentative of the generator’s capability.
Results from a cross-validation study indi-
cate that discriminative paraphrase rerank-
ing can achieve substantial improvements
in naturalness on average, ameliorating the
problem of highly variable synthesis qual-
ity typically encountered with today’s unit
selection synthesizers.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999174181818182">
Unit selection synthesis1—a technique which con-
catenates segments of natural speech selected from
a database—has been found to be capable of pro-
ducing high quality synthetic speech, especially
for utterances that are similar to the speech in the
database in terms of style, delivery, and coverage
(Black and Lenzo, 2001). In particular, in the lim-
ited domain of a spoken language dialogue sys-
tem, it is possible to achieve highly natural synthe-
sis with a purpose-built voice (Black and Lenzo,
2000). However, it can be difficult to develop
</bodyText>
<note confidence="0.6333345">
1See e.g. (Hunt and Black, 1996; Black and Taylor, 1997;
Beutnagel et al., 1999).
</note>
<bodyText confidence="0.999307485714286">
a synthetic voice for a dialogue system that pro-
duces natural speech completely reliably, and thus
in practice output quality can be quite variable.
Two important factors in this regard are the label-
ing process for the speech database and the direc-
tion of the dialogue system’s further development,
after the voice has been built: when labels are as-
signed fully automatically to the recorded speech,
label boundaries may be inaccurate, leading to un-
natural sounding joins in speech output; and when
further system development leads to the genera-
tion of utterances that are less like those in the
recording script, such utterances must be synthe-
sized using smaller units with more joins between
them, which can lead to a considerable dropoff in
quality.
As suggested by Bulyko and Ostendorf (2002),
one avenue for improving synthesis quality in a di-
alogue system is to have the system choose what
to say in part by taking into account what is likely
to sound natural when synthesized. The idea is
to take advantage of the generator’s periphrastic
ability:2 given a set of generated paraphrases that
suitably express the desired content in the dialogue
context, the system can select the specific para-
phrase to use as its response according to the pre-
dicted quality of the speech synthesized for that
paraphrase. In this way, if there are significant
differences in the predicted synthesis quality for
the various paraphrases—and if these predictions
are generally borne out—then, by selecting para-
phrases with high predicted synthesis quality, the
dialogue system (as a whole) can more reliably
produce natural sounding speech.
In this paper, we present an application of dis-
</bodyText>
<footnote confidence="0.971220666666667">
2See e.g. (Iordanskaja et al., 1991; Langkilde and Knight,
1998; Barzilay and McKeown, 2001; Pang et al., 2003) for
discussion of paraphrase in generation.
</footnote>
<page confidence="0.815435">
1113
</page>
<note confidence="0.597045">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1113–1120,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.99993728">
criminative reranking to the task of adapting a lan-
guage generator to the strengths and weaknesses
of a particular synthetic voice. Our method in-
volves training a reranker to select paraphrases
that are predicted to sound natural when synthe-
sized, from the N-best realizations produced by
the generator. The ranker is trained in super-
vised fashion, using human judgements of syn-
thetic voice quality on a representative sample of
the paraphrases. In principle, the method can be
employed with any speech synthesizer. Addition-
ally, when features derived from the synthesizer’s
unit selection search can be made available, fur-
ther quality improvements become possible.
The paper is organized as follows. In Section 2,
we review previous work on integrating choice in
language generation and speech synthesis, and on
learning discriminative rerankers for generation.
In Section 3, we present our method. In Section 4,
we describe a cross-validation study whose results
indicate that discriminative paraphrase reranking
can achieve substantial improvements in natural-
ness on average. Finally, in Section 5, we con-
clude with a summary and a discussion of future
work.
</bodyText>
<sectionHeader confidence="0.993593" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999975906666667">
Most previous work on integrating language gen-
eration and synthesis, e.g. (Davis and Hirschberg,
1988; Prevost and Steedman, 1994; Hitzeman et
al., 1998; Pan et al., 2002), has focused on how
to use the information present in the language
generation component in order to specify contex-
tually appropriate intonation for the speech syn-
thesizer to target. For example, syntactic struc-
ture, information structure and dialogue context
have all been argued to play a role in improving
prosody prediction, compared to unrestricted text-
to-speech synthesis. While this topic remains an
important area of research, our focus is instead
on a different opportunity that arises in a dialogue
system, namely, the possibility of choosing the ex-
act wording and prosody of a response according
to how natural it is likely to sound when synthe-
sized.
To our knowledge, Bulyko and Ostendorf
(2002) were the first to propose allowing the
choice of wording and prosody to be jointly deter-
mined by the language generator and speech syn-
thesizer. In their approach, a template-based gen-
erator passes a prosodically annotated word net-
work to the speech synthesizer, rather than a single
text string (or prosodically annotated text string).
To perform the unit selection search on this ex-
panded input efficiently, they employ weighted
finite-state transducers, where each step of net-
work expansion is then followed by minimiza-
tion. The weights are determined by concatena-
tion (join) costs, relative frequencies (negative log
probabilities) of the word sequences, and prosodic
prediction costs, for cases where the prosody is
not determined by the templates. In a perception
experiment, they demonstrated that by expand-
ing the space of candidate responses, their system
achieved higher quality speech output.
Following (Bulyko and Ostendorf, 2002), Stone
et al. (2004) developed a method for jointly de-
termining wording, speech and gesture. In their
approach, a template-based generator produces
a word lattice with intonational phrase breaks.
A unit selection algorithm then searches for a
low-cost way of realizing a path through this
lattice that combines captured motion samples
with recorded speech samples to create coherent
phrases, blending segments of speech and mo-
tion together phrase-by-phrase into extended ut-
terances. Video demonstrations indicate that natu-
ral and highly expressive results can be achieved,
though no human evaluations are reported.
In an alternative approach, Pan and Weng
(2002) proposed integrating instance-based real-
ization and synthesis. In their framework, sen-
tence structure, wording, prosody and speech
waveforms from a domain-specific corpus are si-
multaneously reused. To do so, they add prosodic
and acoustic costs to the insertion, deletion and
replacement costs used for instance-based surface
realization. Their contribution focuses on how to
design an appropriate speech corpus to facilitate
an integrated approach to instance-based realiza-
tion and synthesis, and does not report evaluation
results.
A drawback of these approaches to integrating
choice in language generation and synthesis is that
they cannot be used with most existing speech syn-
thesizers, which do not accept (annotated) word
lattices as input. In contrast, the approach we in-
troduce here can be employed with any speech
synthesizer in principle. All that is required is
that the language generator be capable of produc-
ing N-best outputs; that is, the generator must be
able to construct a set of suitable paraphrases ex-
</bodyText>
<page confidence="0.992542">
1114
</page>
<bodyText confidence="0.999978878787879">
pressing the desired content, from which the top
N realizations can be selected for reranking ac-
cording to their predicted synthesis quality. Once
the realizations have been reranked, the top scor-
ing realization can be sent to the synthesizer as
usual. Alternatively, when features derived from
the synthesizer’s unit selection search can be made
available—and if the time demands of the dia-
logue system permit—several of the top scoring
reranked realizations can be sent to the synthe-
sizer, and the resulting utterances can be rescored
with the extended feature set.
Our reranking approach has been inspired by
previous work on reranking in parsing and gen-
eration, especially (Collins, 2000) and (Walker et
al., 2002). As in Walker et al.’s (2002) method for
training a sentence plan ranker, we use our gen-
erator to produce a representative sample of para-
phrases and then solicit human judgements of their
naturalness to use as data for training the ranker.
This method is attractive when there is no suit-
able corpus of naturally occurring dialogues avail-
able for training purposes, as is often the case for
systems that engage in human-computer dialogues
that differ substantially from human-human ones.
The primary difference between Walker et al.’s
work and ours is that theirs examines the impact
on text quality of sentence planning decisions such
as aggregation, whereas ours focuses on the im-
pact of the lexical and syntactic choice at the sur-
face realization level on speech synthesis quality,
according to the strengths and weaknesses of a
particular synthetic voice.
</bodyText>
<sectionHeader confidence="0.748244" genericHeader="method">
3 Reranking Realizations by Predicted
Synthesis Quality
</sectionHeader>
<subsectionHeader confidence="0.999891">
3.1 Generating Alternatives
</subsectionHeader>
<bodyText confidence="0.9999335">
Our experiments with integrating language gener-
ation and synthesis have been carried out in the
context of the COMIC3 multimodal dialogue sys-
tem (den Os and Boves, 2003). The COMIC sys-
tem adds a dialogue interface to a CAD-like ap-
plication used in sales situations to help clients re-
design their bathrooms. The input to the system
includes speech, handwriting, and pen gestures;
the output combines synthesized speech, an ani-
mated talking head, deictic gestures at on-screen
objects, and direct control of the underlying appli-
cation.
</bodyText>
<footnote confidence="0.35506">
3COnversational Multimodal Interaction with Computers,
http://www.hcrc.ed.ac.uk/comic/.
</footnote>
<bodyText confidence="0.999967352941177">
Drawing on the materials used in (Foster and
White, 2005) to evaluate adaptive generation in
COMIC, we selected a sample of 104 sentences
from 38 different output turns across three dia-
logues. For each sentence in the set, a variant was
included that expressed the same content adapted
to a different user model or adapted to a differ-
ent dialogue history. For example, a description
of a certain design’s colour scheme for one user
might be phrased as As you can see, the tiles have
a blue and green colour scheme, whereas a vari-
ant expression of the same content for a different
user could be Although the tiles have a blue colour
scheme, the design does also feature green, if the
user disprefers blue.
In COMIC, the sentence planner uses XSLT to
generate disjunctive logical forms (LFs), which
specify a range of possible paraphrases in a nested
free-choice form (Foster and White, 2004). Such
disjunctive LFs can be efficiently realized us-
ing the OpenCCG realizer (White, 2004; White,
2006b; White, 2006a). Note that for the experi-
ments reported here, we manually augmented the
disjunctive LFs for the 104 sentences in our sam-
ple to make greater use of the periphrastic capa-
bilities of the COMIC grammar; it remains for fu-
ture work to augment the COMIC sentence plan-
ner produce these more richly disjunctive LFs au-
tomatically.
OpenCCG includes an extensible API for inte-
grating language modeling and realization. To se-
lect preferred word orders, from among all those
allowed by the grammar for the input LF, we used
a backoff trigram model trained on approximately
750 example target sentences, where certain words
were replaced with their semantic classes (e.g.
MANUFACTURER, COLOUR) for better general-
ization. For each of the 104 sentences in our sam-
ple, we performed 25-best realization from the dis-
junctive LF, and then randomly selected up to 12
different realizations to include in our experiments
based on a simulated coin flip for each realization,
starting with the top-scoring one. We used this
procedure to sample from a larger portion of the
N-best realizations, while keeping the sample size
manageable.
Figure 1 shows an example of 12 paraphrases
for a sentence chosen for inclusion in our sample.
Note that the realizations include words with pitch
accent annotations as well as boundary tones as
separate, punctuation-like words. Generally the
</bodyText>
<page confidence="0.944">
1115
</page>
<listItem confidence="0.99900832">
• thisH✄ designH✄ uses tiles from Villeroy and BochH✄
’s Funny DayH✄ collection LL% .
• thisH✄ designH✄ is based on the Funny DayH✄ collec-
tion by Villeroy and BochH✄ LL% .
• thisH✄ designH✄ is based on Funny DayH✄ LL% , by
Villeroy and BochH✄ LL% .
• thisH✄ designH✄ draws from the Funny DayH✄ collec-
tion by Villeroy and BochH✄ LL% .
• thisH✄ one draws from Funny DayH✄ LL% , by
Villeroy and BochH✄ LL% .
• hereL+H✄ LH% we have a design that is based on
the Funny DayH✄ collection by Villeroy and BochH✄
LL% .
• thisH✄ designH✄ draws from Villeroy and BochH✄ ’s
Funny DayH✄ series LL% .
• here is a design that draws from Funny DayH✄ LL% ,
by Villeroy and BochH✄ LL% .
• thisH✄ one draws from Villeroy and BochH✄ ’s
Funny DayH✄ collection LL% .
• thisH✄ draws from the Funny DayH✄ collection by
Villeroy and BochH✄ LL% .
• thisH✄ one draws from the Funny DayH✄ collection by
Villeroy and BochH✄ LL% .
• here is a design that draws from Villeroy and BochH✄
’s Funny DayH✄ collection LL% .
</listItem>
<figureCaption confidence="0.9867565">
Figure 1: Example of sampled periphrastic alter-
natives for a sentence.
</figureCaption>
<bodyText confidence="0.998953">
quality of the sampled paraphrases is very high,
only occasionally including dispreferred word or-
ders such as We here have a design in the family
style, where here is in medial position rather than
fronted.4
</bodyText>
<subsectionHeader confidence="0.999664">
3.2 Synthesizing Utterances
</subsectionHeader>
<bodyText confidence="0.9999909">
For synthesis, OpenCCG’s output realizations are
converted to APML,5 a markup language which
allows pitch accents and boundary tones to be
specified, and then passed to the Festival speech
synthesis system (Taylor et al., 1998; Clark et al.,
2004). Festival uses the prosodic markup in the
text analysis phase of synthesis in place of the
structures that it would otherwise have to predict
from the text. The synthesiser then uses the con-
text provided by the markup to enforce the selec-
</bodyText>
<footnote confidence="0.9830968">
4In other examples medial position is preferred, e.g. This
design here is in the family style.
5Affective Presentation Markup Language; see
http://www.cstr.ed.ac.uk/projects/
festival/apml.html.
</footnote>
<bodyText confidence="0.999832083333334">
tion of suitable units from the database.
A custom synthetic voice for the COMIC sys-
tem was developed, as follows. First, a domain-
specific recording script was prepared by select-
ing about 150 sentences from the larger set of tar-
get sentences used to train the system’s n-gram
model. The sentences were greedily selected with
the goals of ensuring that (i) all words (including
proper names) in the target sentences appeared at
least once in the record script, and (ii) all bigrams
at the level of semantic classes (e.g. MANUFAC-
TURER, COLOUR) were covered as well. For the
cross-validation study reported in the next section,
we also built a trigram model on the words in the
domain-specific recording script, without replac-
ing any words with semantic classes, so that we
could examine whether the more frequent occur-
rence of the specific words and phrases in this part
of the script is predictive of synthesis quality.
The domain-specific script was augmented with
a set of 600 newspaper sentences selected for di-
phone coverage. The newspaper sentences make
it possible for the voice to synthesize words out-
side of the domain-specific script, though not
necessarily with the same quality. Once these
scripts were in place, an amateur voice talent was
recorded reading the sentences in the scripts dur-
ing two recording sessions. Finally, after the
speech files were semi-automatically segmented
into individual sentences, the speech database was
constructed, using fully automatic labeling.
We have found that the utterances synthesized
with the COMIC voice vary considerably in their
naturalness, due to two main factors. First, the
system underwent further development after the
voice was built, leading to the addition of a va-
riety of new phrases to the system’s repertoire, as
well as many extra proper names (and their pro-
nunciations); since these names and phrases usu-
ally require going outside of the domain-specific
part of the speech database, they often (though not
always) exhibit a considerable dropoff in synthe-
sis quality.6 And second, the boundaries of the au-
tomatically assigned unit labels were not always
accurate, leading to problems with unnatural joins
and reduced intelligibility. To improve the reliabil-
ity of the COMIC voice, we could have recorded
more speech, or manually corrected label bound-
</bodyText>
<footnote confidence="0.98904475">
6Note that in the current version of the system, proper
names are always required parts of the output, and thus the
discriminative reranker cannot learn to simply choose para-
phrases that leave out problematic names.
</footnote>
<page confidence="0.991402">
1116
</page>
<bodyText confidence="0.999103333333333">
aries; the goal of this paper is to examine whether
the naturalness of a dialogue system’s output can
be improved in a less labor-intensive way.
</bodyText>
<subsectionHeader confidence="0.999348">
3.3 Rating Synthesis Quality
</subsectionHeader>
<bodyText confidence="0.999996090909091">
To obtain data for training our realization reranker,
we solicited judgements of the naturalness of the
synthesized speech produced by Festival for the
utterances in our sample COMIC corpus. Two
judges (the first two authors) provided judgements
on a 1–7 point scale, with higher scores represent-
ing more natural synthesis. Ratings were gathered
using WebExp2,7 with the periphrastic alternatives
for each sentence presented as a group in a ran-
domized order. Note that for practical reasons,
the utterances were presented out of the dialogue
context, though both judges were familiar with the
kinds of dialogues that the COMIC system is ca-
pable of.
Though the numbers on the seven point scale
were not assigned labels, they were roughly taken
to be “horrible,” “poor,” “fair,” “ok,” “good,” “very
good” and “perfect.” The average assigned rating
across all utterances was 4.05 (“ok”), with a stan-
dard deviation of 1.56. The correlation between
the two judges’ ratings was 0.45, with one judge’s
ratings consistently higher than the other’s.
Some common problems noted by the judges
included slurred words, especially the sometimes
sounding like ther or even their; clipped words,
such as has shortened at times to the point of
sounding like is, or though clipped to unintelligi-
bility; unnatural phrasing or emphasis, e.g. occa-
sional pauses before a possessive ’s, or words such
as style sounding emphasized when they should
be deaccented; unnatural rate changes; “choppy”
speech from poor joins; and some unintelligible
proper names.
</bodyText>
<subsectionHeader confidence="0.975022">
3.4 Ranking
</subsectionHeader>
<bodyText confidence="0.99995525">
While Collins (2000) and Walker et al. (2002)
develop their rankers using the RankBoost algo-
rithm (Freund et al., 1998), we have instead cho-
sen to use Joachims’ (2002) method of formu-
lating ranking tasks as Support Vector Machine
(SVM) constraint optimization problems.8 This
choice has been motivated primarily by conve-
nience, as Joachims’ SVMiZght package is easy to
</bodyText>
<footnote confidence="0.9564975">
7http://www.hcrc.ed.ac.uk/web exp/
8See (Barzilay and Lapata, 2005) for another application
of SVM ranking in generation, namely to the task of ranking
alternative text orderings for local coherence.
</footnote>
<bodyText confidence="0.999557644444445">
use; we leave it for future work to compare the
performance of RankBoost and SVMiZght on our
ranking task.
The ranker takes as input a set of paraphrases
that express the desired content of each sentence,
optionally together with synthesized utterances
for each paraphrase. The output is a ranking of
the paraphrases according to the predicted natu-
ralness of their corresponding synthesized utter-
ances. Ranking is more appropriate than classifi-
cation for our purposes, as naturalnesss is a graded
assessment rather than a categorical one.
To encode the ranking task as an SVM con-
straint optimization problem, each paraphrase j
of a sentence i is represented by a feature vector
✟✭8✐❥✮ ❂ (f✶✭8✐❥✮, ... , f♠✭8✐❥✮), where rn is the
number of features. In the training data, the fea-
ture vectors are paired with the average value of
their corresponding human judgements of natural-
ness. From this data, ordered pairs of paraphrases
✭8✐❥, 8✐❦✮ are derived, where 8✐❥ has a higher nat-
uralness rating than 8✐❦. The constraint optimiza-
tion problem is then to derive a parameter vector
w that yields a ranking score function w • ✟✭8✐❥✮
which minimizes the number of pairwise rank-
ing violations. Ideally, for every ordered pair
✭8✐❥, 8✐❦✮, we would have w • ✟✭8✐❥✮ &gt; w • ✟✭8✐❦✮;
in practice, it is often impossible or intractable to
find such a parameter vector, and thus slack vari-
ables are introduced that allow for training errors.
A parameter to the algorithm controls the trade-off
between ranking margin and training error.
In testing, the ranker’s accuracy can be deter-
mined by comparing the ranking scores for ev-
ery ordered pair ✭8✐❥, 8✐❦✮ in the test data, and
determining whether the actual preferences are
borne out by the predicted preference, i.e. whether
w • ✟✭8✐❥✮ &gt; w • ✟✭8✐❦✮ as desired. Note that
the ranking scores, unlike the original ratings, do
not have any meaning in the absolute sense; their
import is only to order alternative paraphrases by
their predicted naturalness.
In our ranking experiments, we have used
SVMiZght with all parameters set to their default
values.
</bodyText>
<sectionHeader confidence="0.718799" genericHeader="method">
3.5 Features
</sectionHeader>
<bodyText confidence="0.9999195">
Table 1 shows the feature sets we have investigated
for reranking, distinguished by the availability of
the features and the need for discriminative train-
ing. The first row shows the feature sets that are
</bodyText>
<page confidence="0.999014">
1117
</page>
<tableCaption confidence="0.999294">
Table 1: Feature sets for reranking.
</tableCaption>
<table confidence="0.93253775">
Discriminative
Availability no yes
Realizer NGRAMS WORDS
Synthesizer COSTS ALL
</table>
<bodyText confidence="0.999863424242424">
available to the realizer. There are two n-gram
models that can be used to directly rank alterna-
tive realizations: NGRAM-1, the language model
used in COMIC, and NGRAM-2, the language
model derived from the domain-specific recording
script; for feature values, the negative logarithms
are used. There are also two WORDS feature
sets (shown in the second column): WORDS-BI,
which includes NGRAMS plus a feature for every
possible unigram and bigram, where the value of
the feature is the count of the unigram or bigram
in a given realization; and WORDS-TRI, which
includes all the features in WORDS-BI, plus a
feature for every possible trigram. The second
row shows the feature sets that require informa-
tion from the synthesizer. The COSTS feature set
includes NGRAMS plus the total join and target
costs from the unit selection search. Note that a
weighted sum of these costs could be used to di-
rectly rerank realizations, in much the same way
as relative frequencies and concatenation costs are
used in (Bulyko and Ostendorf, 2002); in our
experiments, we let SVM&amp;quot;ght determine how to
weight these costs. Finally, there are two ALL fea-
ture sets: ALL-BI includes NGRAMS, WORDS-
BI and COSTS, plus features for every possi-
ble phone and diphone, and features for every
specific unit in the database; ALL-TRI includes
NGRAMS, WORDS-TRI, COSTS, and a feature
for every phone, diphone and triphone, as well as
specific units in the database. As with WORDS,
the value of a feature is the count of that feature in
a given synthesized utterance.
</bodyText>
<sectionHeader confidence="0.972554" genericHeader="method">
4 Cross-Validation Study
</sectionHeader>
<bodyText confidence="0.999803888888889">
To train and test our ranker on our feature sets,
we partitioned the corpus into 10 folds and per-
formed 10-fold cross-validation. For each fold,
90% of the examples were used for training the
ranker and the remaining unseen 10% were used
for testing. The folds were created by randomly
choosing from among the sentence groups, result-
ing in all of the paraphrases for a given sentence
occurring in the same fold, and each occurring ex-
</bodyText>
<tableCaption confidence="0.8578415">
Table 2: Comparison of results for differing fea-
ture sets, topline and baseline.
</tableCaption>
<table confidence="0.9989859">
Features Mean Score SD Accuracy (%)
BEST 5.38 1.11 100.0
WORDS-TRI 4.95 1.24 77.3
ALL-BI 4.95 1.24 77.9
ALL-TRI 4.90 1.25 78.0
WORDS-BI 4.86 1.28 76.8
COSTS 4.69 1.27 68.2
NGRAM-2 4.34 1.38 56.2
NGRAM-1 4.30 1.29 53.3
RANDOM 4.11 1.22 50.0
</table>
<bodyText confidence="0.995178">
actly once in the testing set as a whole.
We evaluated the performance of our ranker
by determining the average score of the best
ranked paraphrase for each sentence, under each
of the following feature combinations: NGRAM-
1, NGRAM-2, COSTS, WORDS-BI, WORDS-
TRI, ALL-BI, and ALL-TRI. Note that since we
used the human ratings to calculate the score of
the highest ranked utterance, the score of the high-
est ranked utterance cannot be higher than that
of the highest human-rated utterance. Therefore,
we effectively set the human ratings as the topline
(BEST). For the baseline, we randomly chose an
utterance from among the alternatives, and used
its associated score. In 15 tests generating the ran-
dom scores, our average scores ranged from 3.88–
4.18. We report the median score of 4.11 as the
average for the baseline, along with the mean of
the topline and each of the feature subsets, in Ta-
ble 2.
We also report the ordering accuracy of each
feature set used by the ranker in Table 2. As men-
tioned in Section 3.4, the ordering accuracy of the
ranker using a given feature set is determined by
clN, where c is the number of correctly ordered
pairs (of each paraphrase, not just the top ranked
one) produced by the ranker, and N is the total
number of human-ranked ordered pairs.
As Table 2 indicates, the mean of BEST is 5.38,
whereas our ranker using WORDS-TRI features
achieves a mean score of 4.95. This is a difference
of 0.42 on a seven point scale, or only a 6% dif-
ference. The ordering accuracy of WORDS-TRI
is 77.3%.
We also measured the improvement of our
ranker with each feature set over the random base-
line as a percentage of the maximum possible
gain (which would be to reproduce the human
topline). The results appear in Figure 2. As the
</bodyText>
<page confidence="0.993367">
1118
</page>
<figureCaption confidence="0.962608">
Figure 2: Improvement as a percentage of the
maximum possible gain over the random baseline.
</figureCaption>
<bodyText confidence="0.998270869565217">
figure indicates, the maximum possible gain our
ranker achieves over the baseline is 66% (using the
WORDS-TRI or ALL-BI feature set) . By com-
parison, NGRAM-1 and NGRAM-2 achieve less
than 20% of the possible gain.
To verify our main hypothesis that our ranker
would significantly outperform the baselines,
we computed paired one-tailed t-tests between
WORDS-TRI and RANDOM (t = 2.4, p &lt;
8.9x10-13), and WORDS-TRI and NGRAM-1
(t = 1.4, p &lt; 4.5x10-8). Both differences were
highly significant. We also performed seven post-
hoc comparisons using two-tailed t-tests, as we
did not have an a priori expectation as to which
feature set would work better. Using the Bonfer-
roni adjustment for multiple comparisons, the p-
value required to achieve an overall level of signif-
icance of 0.05 is 0.007. In the first post-hoc test,
we found a significant difference between BEST
and WORDS-TRI (t = 8.0,p &lt; 1.86x10-12),
indicating that there is room for improvement of
our ranker. However, in considering the top scor-
ing feature sets, we did not find a significant dif-
ference between WORDS-TRI and WORDS-BI
(t = 2.3, p &lt; 0.022), from which we infer that the
difference among all of WORDS-TRI, ALL-BI,
ALL-TRI and WORDS-BI is not significant also.
This suggests that the synthesizer features have
no substantial impact on our ranker, as we would
expect ALL-TRI to be significantly higher than
WORDS-TRI if so. However, since COSTS does
significantly improve upon NGRAM2 (t = 3.5,
p &lt; 0.001), there is some value to the use of syn-
thesizer features in the absence of WORDS. We
also looked at the comparison for the WORDS
models and COSTS. While WORDS-BI did not
perform significantly better than COSTS ( t =
2.3, p &lt; 0.025), the added trigrams in WORDS-
TRI did improve ranker performance significantly
over COSTS (t = 3.7, p &lt; 3.29x10-4). Since
COSTS ranks realizations in the much the same
way as (Bulyko and Ostendorf, 2002), the fact that
WORDS-TRI outperforms COSTS indicates that
our discriminative reranking method can signifi-
cantly improve upon their non-discriminative ap-
proach.
</bodyText>
<sectionHeader confidence="0.999407" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999963666666667">
In this paper, we have presented a method for
adapting a language generator to the strengths
and weaknesses of a particular synthetic voice by
training a discriminative reranker to select para-
phrases that are predicted to sound natural when
synthesized. In contrast to previous work on
this topic, our method can be employed with any
speech synthesizer in principle, so long as fea-
tures derived from the synthesizer’s unit selec-
tion search can be made available. In a case
study with the COMIC dialogue system, we have
demonstrated substantial improvements in the nat-
uralness of the resulting synthetic speech, achiev-
ing two-thirds of the maximum possible gain, and
raising the average rating from “ok” to “good.” We
have also shown that in this study, our discrimina-
tive method significantly outperforms an approach
that performs selection based solely on corpus fre-
quencies together with target and join costs.
In future work, we intend to verify the results
of our cross-validation study in a perception ex-
periment with naive subjects. We also plan to in-
vestigate whether additional features derived from
the synthesizer can better detect unnatural pauses
or changes in speech rate, as well as F0 contours
that fail to exhibit the targeting accenting pattern.
Finally, we plan to examine whether gains in qual-
ity can be achieved with an off-the-shelf, general
purpose voice that are similar to those we have ob-
served using COMIC’s limited domain voice.
</bodyText>
<sectionHeader confidence="0.996515" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999821333333333">
We thank Mary Ellen Foster, Eric Fosler-Lussier
and the anonymous reviewers for helpful com-
ments and discussion.
</bodyText>
<sectionHeader confidence="0.993423" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.76744">
Regina Barzilay and Mirella Lapata. 2005. Modeling
local coherence: An entity-based approach. In Pro-
</reference>
<page confidence="0.977604">
1119
</page>
<reference confidence="0.996898704761904">
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, Ann Arbor.
Regina Barzilay and Kathleen McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Proc.
ACL/EACL.
M. Beutnagel, A. Conkie, J. Schroeter, Y. Stylianou,
and A. Syrdal. 1999. The AT&amp;T Next-Gen TTS
system. In Joint Meeting ofASA, EAA, and DAGA.
Alan Black and Kevin Lenzo. 2000. Limited domain
synthesis. In Proceedings of ICSLP2000, Beijing,
China.
Alan Black and Kevin Lenzo. 2001. Optimal data
selection for unit selection synthesis. In 4th ISCA
Speech Synthesis Workshop, Pitlochry, Scotland.
Alan Black and Paul Taylor. 1997. Automatically clus-
tering similar units for unit selection in speech syn-
thesis. In Eurospeech ’97.
Ivan Bulyko and Mari Ostendorf. 2002. Efficient in-
tegrated response generation from multiple targets
using weighted finite state transducers. Computer
Speech and Language, 16:533–550.
Robert A.J. Clark, Korin Richmond, and Simon King.
2004. Festival 2 – build your own general pur-
pose unit selection speech synthesiser. In 5th ISCA
Speech Synthesis Workshop, pages 173–178, Pitts-
burgh, PA.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In Proc. ICML.
James Raymond Davis and Julia Hirschberg. 1988.
Assigning intonational features in synthesized spo-
ken directions. In Proc. ACL.
Els den Os and Lou Boves. 2003. Towards ambient
intelligence: Multimodal computers that understand
our intentions. In Proc. eChallenges-03.
Mary Ellen Foster and Michael White. 2004. Tech-
niques for Text Planning with XSLT. In Proc. 4th
NLPXML Workshop.
Mary Ellen Foster and Michael White. 2005. As-
sessing the impact of adaptive generation in the
COMIC multimodal dialogue system. In Proc.
IJCAI-05 Workshop on Knowledge and Representa-
tion in Practical Dialogue Systems.
Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. 1998.
An efficient boosting algorithm for combining pref-
erences. In Machine Learning: Proc. of the Fif-
teenth International Conference.
Janet Hitzeman, Alan W. Black, Chris Mellish, Jon
Oberlander, and Paul Taylor. 1998. On the use of
automatically generated discourse-level information
in a concept-to-speech synthesis system. In Proc.
ICSLP-98.
A. Hunt and A. Black. 1996. Unit selection in a
concatenative speech synthesis system using a large
speech database. In Proc. ICASSP-96, Atlanta,
Georgia.
Lidija Iordanskaja, Richard Kittredge, and Alain
Polg´uere. 1991. Lexical selection and paraphrase
in a meaning-text generation model. In C´ecile L.
Paris, William R. Swartout, and William C. Mann,
editors, Natural Language Generation in Artificial
Intelligence and Computational Linguistics, pages
293–312. Kluwer.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proc. KDD.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
Proc. COLING-ACL.
Shimei Pan and Wubin Weng. 2002. Designing a
speech corpus for instance-based spoken language
generation. In Proc. of the International Natural
Language Generation Conference (INLG-02).
Shimei Pan, Kathleen McKeown, and Julia Hirschberg.
2002. Exploring features from natural language
generation for prosody modeling. Computer Speech
and Language, 16:457–490.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations:
Extracting paraphrases and generating new sen-
tences. In Proc. HLT/NAACL.
Scott Prevost and Mark Steedman. 1994. Specify-
ing intonation from context for speech synthesis.
Speech Communication, 15:139–153.
Matthew Stone, Doug DeCarlo, Insuk Oh, Christian
Rodriguez, Adrian Stere, Alyssa Lees, and Chris
Bregler. 2004. Speaking with hands: Creating ani-
mated conversational characters from recordings of
human performance. ACM Transactions on Graph-
ics (SIGGRAPH), 23(3).
P. Taylor, A. Black, and R. Caley. 1998. The architec-
ture of the the Festival speech synthesis system. In
Third International Workshop on Speech Synthesis,
Sydney, Australia.
Marilyn A. Walker, Owen C. Rambow, and Monica Ro-
gati. 2002. Training a sentence planner for spo-
ken dialogue using boosting. Computer Speech and
Language, 16:409–433.
Michael White. 2004. Reining in CCG Chart Realiza-
tion. In Proc. INLG-04.
Michael White. 2006a. CCG chart realization from
disjunctive logical forms. In Proc. INLG-06. To ap-
pear.
Michael White. 2006b. Efficient Realization of Coor-
dinate Structures in Combinatory Categorial Gram-
mar. Research on Language &amp; Computation, on-
line first, March.
</reference>
<page confidence="0.992317">
1120
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.985416">
<title confidence="0.997896">Learning to Say It Well: Reranking Realizations by Predicted Synthesis Quality</title>
<author confidence="0.999033">Nakatsu White</author>
<affiliation confidence="0.9997815">Department of Linguistics The Ohio State University</affiliation>
<address confidence="0.999972">Columbus, OH 43210 USA</address>
<abstract confidence="0.999557904761905">This paper presents a method for adapting a language generator to the strengths and weaknesses of a synthetic voice, thereby improving the naturalness of synthetic speech in a spoken language dialogue system. The method trains a discriminative reranker to select paraphrases that are predicted to sound natural when synthesized. The ranker is trained on realizer and synthesizer features in supervised fashion, using human judgements of synthetic voice quality on a sample of the paraphrases representative of the generator’s capability. Results from a cross-validation study indicate that discriminative paraphrase reranking can achieve substantial improvements in naturalness on average, ameliorating the problem of highly variable synthesis quality typically encountered with today’s unit selection synthesizers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Modeling local coherence: An entity-based approach.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Ann Arbor.</location>
<contexts>
<context position="19736" citStr="Barzilay and Lapata, 2005" startWordPosition="3138" endWordPosition="3141">ive ’s, or words such as style sounding emphasized when they should be deaccented; unnatural rate changes; “choppy” speech from poor joins; and some unintelligible proper names. 3.4 Ranking While Collins (2000) and Walker et al. (2002) develop their rankers using the RankBoost algorithm (Freund et al., 1998), we have instead chosen to use Joachims’ (2002) method of formulating ranking tasks as Support Vector Machine (SVM) constraint optimization problems.8 This choice has been motivated primarily by convenience, as Joachims’ SVMiZght package is easy to 7http://www.hcrc.ed.ac.uk/web exp/ 8See (Barzilay and Lapata, 2005) for another application of SVM ranking in generation, namely to the task of ranking alternative text orderings for local coherence. use; we leave it for future work to compare the performance of RankBoost and SVMiZght on our ranking task. The ranker takes as input a set of paraphrases that express the desired content of each sentence, optionally together with synthesized utterances for each paraphrase. The output is a ranking of the paraphrases according to the predicted naturalness of their corresponding synthesized utterances. Ranking is more appropriate than classification for our purposes</context>
</contexts>
<marker>Barzilay, Lapata, 2005</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2005. Modeling local coherence: An entity-based approach. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, Ann Arbor.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen McKeown</author>
</authors>
<title>Extracting paraphrases from a parallel corpus. In</title>
<date>2001</date>
<booktitle>Proc. ACL/EACL.</booktitle>
<contexts>
<context position="3453" citStr="Barzilay and McKeown, 2001" startWordPosition="536" endWordPosition="539">he dialogue context, the system can select the specific paraphrase to use as its response according to the predicted quality of the speech synthesized for that paraphrase. In this way, if there are significant differences in the predicted synthesis quality for the various paraphrases—and if these predictions are generally borne out—then, by selecting paraphrases with high predicted synthesis quality, the dialogue system (as a whole) can more reliably produce natural sounding speech. In this paper, we present an application of dis2See e.g. (Iordanskaja et al., 1991; Langkilde and Knight, 1998; Barzilay and McKeown, 2001; Pang et al., 2003) for discussion of paraphrase in generation. 1113 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1113–1120, Sydney, July 2006. c�2006 Association for Computational Linguistics criminative reranking to the task of adapting a language generator to the strengths and weaknesses of a particular synthetic voice. Our method involves training a reranker to select paraphrases that are predicted to sound natural when synthesized, from the N-best realizations produced by the generator. The ranker is trained in su</context>
</contexts>
<marker>Barzilay, McKeown, 2001</marker>
<rawString>Regina Barzilay and Kathleen McKeown. 2001. Extracting paraphrases from a parallel corpus. In Proc. ACL/EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Beutnagel</author>
<author>A Conkie</author>
<author>J Schroeter</author>
<author>Y Stylianou</author>
<author>A Syrdal</author>
</authors>
<date>1999</date>
<booktitle>The AT&amp;T Next-Gen TTS system. In Joint Meeting ofASA, EAA, and DAGA.</booktitle>
<contexts>
<context position="1686" citStr="Beutnagel et al., 1999" startWordPosition="249" endWordPosition="252">on Unit selection synthesis1—a technique which concatenates segments of natural speech selected from a database—has been found to be capable of producing high quality synthetic speech, especially for utterances that are similar to the speech in the database in terms of style, delivery, and coverage (Black and Lenzo, 2001). In particular, in the limited domain of a spoken language dialogue system, it is possible to achieve highly natural synthesis with a purpose-built voice (Black and Lenzo, 2000). However, it can be difficult to develop 1See e.g. (Hunt and Black, 1996; Black and Taylor, 1997; Beutnagel et al., 1999). a synthetic voice for a dialogue system that produces natural speech completely reliably, and thus in practice output quality can be quite variable. Two important factors in this regard are the labeling process for the speech database and the direction of the dialogue system’s further development, after the voice has been built: when labels are assigned fully automatically to the recorded speech, label boundaries may be inaccurate, leading to unnatural sounding joins in speech output; and when further system development leads to the generation of utterances that are less like those in the re</context>
</contexts>
<marker>Beutnagel, Conkie, Schroeter, Stylianou, Syrdal, 1999</marker>
<rawString>M. Beutnagel, A. Conkie, J. Schroeter, Y. Stylianou, and A. Syrdal. 1999. The AT&amp;T Next-Gen TTS system. In Joint Meeting ofASA, EAA, and DAGA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Black</author>
<author>Kevin Lenzo</author>
</authors>
<title>Limited domain synthesis.</title>
<date>2000</date>
<booktitle>In Proceedings of ICSLP2000,</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="1564" citStr="Black and Lenzo, 2000" startWordPosition="228" endWordPosition="231">problem of highly variable synthesis quality typically encountered with today’s unit selection synthesizers. 1 Introduction Unit selection synthesis1—a technique which concatenates segments of natural speech selected from a database—has been found to be capable of producing high quality synthetic speech, especially for utterances that are similar to the speech in the database in terms of style, delivery, and coverage (Black and Lenzo, 2001). In particular, in the limited domain of a spoken language dialogue system, it is possible to achieve highly natural synthesis with a purpose-built voice (Black and Lenzo, 2000). However, it can be difficult to develop 1See e.g. (Hunt and Black, 1996; Black and Taylor, 1997; Beutnagel et al., 1999). a synthetic voice for a dialogue system that produces natural speech completely reliably, and thus in practice output quality can be quite variable. Two important factors in this regard are the labeling process for the speech database and the direction of the dialogue system’s further development, after the voice has been built: when labels are assigned fully automatically to the recorded speech, label boundaries may be inaccurate, leading to unnatural sounding joins in s</context>
</contexts>
<marker>Black, Lenzo, 2000</marker>
<rawString>Alan Black and Kevin Lenzo. 2000. Limited domain synthesis. In Proceedings of ICSLP2000, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Black</author>
<author>Kevin Lenzo</author>
</authors>
<title>Optimal data selection for unit selection synthesis.</title>
<date>2001</date>
<booktitle>In 4th ISCA Speech Synthesis Workshop,</booktitle>
<location>Pitlochry, Scotland.</location>
<contexts>
<context position="1386" citStr="Black and Lenzo, 2001" startWordPosition="197" endWordPosition="200">ability. Results from a cross-validation study indicate that discriminative paraphrase reranking can achieve substantial improvements in naturalness on average, ameliorating the problem of highly variable synthesis quality typically encountered with today’s unit selection synthesizers. 1 Introduction Unit selection synthesis1—a technique which concatenates segments of natural speech selected from a database—has been found to be capable of producing high quality synthetic speech, especially for utterances that are similar to the speech in the database in terms of style, delivery, and coverage (Black and Lenzo, 2001). In particular, in the limited domain of a spoken language dialogue system, it is possible to achieve highly natural synthesis with a purpose-built voice (Black and Lenzo, 2000). However, it can be difficult to develop 1See e.g. (Hunt and Black, 1996; Black and Taylor, 1997; Beutnagel et al., 1999). a synthetic voice for a dialogue system that produces natural speech completely reliably, and thus in practice output quality can be quite variable. Two important factors in this regard are the labeling process for the speech database and the direction of the dialogue system’s further development,</context>
</contexts>
<marker>Black, Lenzo, 2001</marker>
<rawString>Alan Black and Kevin Lenzo. 2001. Optimal data selection for unit selection synthesis. In 4th ISCA Speech Synthesis Workshop, Pitlochry, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Black</author>
<author>Paul Taylor</author>
</authors>
<title>Automatically clustering similar units for unit selection in speech synthesis.</title>
<date>1997</date>
<booktitle>In Eurospeech ’97.</booktitle>
<contexts>
<context position="1661" citStr="Black and Taylor, 1997" startWordPosition="245" endWordPosition="248">nthesizers. 1 Introduction Unit selection synthesis1—a technique which concatenates segments of natural speech selected from a database—has been found to be capable of producing high quality synthetic speech, especially for utterances that are similar to the speech in the database in terms of style, delivery, and coverage (Black and Lenzo, 2001). In particular, in the limited domain of a spoken language dialogue system, it is possible to achieve highly natural synthesis with a purpose-built voice (Black and Lenzo, 2000). However, it can be difficult to develop 1See e.g. (Hunt and Black, 1996; Black and Taylor, 1997; Beutnagel et al., 1999). a synthetic voice for a dialogue system that produces natural speech completely reliably, and thus in practice output quality can be quite variable. Two important factors in this regard are the labeling process for the speech database and the direction of the dialogue system’s further development, after the voice has been built: when labels are assigned fully automatically to the recorded speech, label boundaries may be inaccurate, leading to unnatural sounding joins in speech output; and when further system development leads to the generation of utterances that are </context>
</contexts>
<marker>Black, Taylor, 1997</marker>
<rawString>Alan Black and Paul Taylor. 1997. Automatically clustering similar units for unit selection in speech synthesis. In Eurospeech ’97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Bulyko</author>
<author>Mari Ostendorf</author>
</authors>
<title>Efficient integrated response generation from multiple targets using weighted finite state transducers. Computer Speech and Language,</title>
<date>2002</date>
<pages>16--533</pages>
<contexts>
<context position="2484" citStr="Bulyko and Ostendorf (2002)" startWordPosition="380" endWordPosition="383">s in this regard are the labeling process for the speech database and the direction of the dialogue system’s further development, after the voice has been built: when labels are assigned fully automatically to the recorded speech, label boundaries may be inaccurate, leading to unnatural sounding joins in speech output; and when further system development leads to the generation of utterances that are less like those in the recording script, such utterances must be synthesized using smaller units with more joins between them, which can lead to a considerable dropoff in quality. As suggested by Bulyko and Ostendorf (2002), one avenue for improving synthesis quality in a dialogue system is to have the system choose what to say in part by taking into account what is likely to sound natural when synthesized. The idea is to take advantage of the generator’s periphrastic ability:2 given a set of generated paraphrases that suitably express the desired content in the dialogue context, the system can select the specific paraphrase to use as its response according to the predicted quality of the speech synthesized for that paraphrase. In this way, if there are significant differences in the predicted synthesis quality </context>
<context position="5778" citStr="Bulyko and Ostendorf (2002)" startWordPosition="895" endWordPosition="898">omponent in order to specify contextually appropriate intonation for the speech synthesizer to target. For example, syntactic structure, information structure and dialogue context have all been argued to play a role in improving prosody prediction, compared to unrestricted textto-speech synthesis. While this topic remains an important area of research, our focus is instead on a different opportunity that arises in a dialogue system, namely, the possibility of choosing the exact wording and prosody of a response according to how natural it is likely to sound when synthesized. To our knowledge, Bulyko and Ostendorf (2002) were the first to propose allowing the choice of wording and prosody to be jointly determined by the language generator and speech synthesizer. In their approach, a template-based generator passes a prosodically annotated word network to the speech synthesizer, rather than a single text string (or prosodically annotated text string). To perform the unit selection search on this expanded input efficiently, they employ weighted finite-state transducers, where each step of network expansion is then followed by minimization. The weights are determined by concatenation (join) costs, relative frequ</context>
<context position="23325" citStr="Bulyko and Ostendorf, 2002" startWordPosition="3730" endWordPosition="3733">ure for every possible unigram and bigram, where the value of the feature is the count of the unigram or bigram in a given realization; and WORDS-TRI, which includes all the features in WORDS-BI, plus a feature for every possible trigram. The second row shows the feature sets that require information from the synthesizer. The COSTS feature set includes NGRAMS plus the total join and target costs from the unit selection search. Note that a weighted sum of these costs could be used to directly rerank realizations, in much the same way as relative frequencies and concatenation costs are used in (Bulyko and Ostendorf, 2002); in our experiments, we let SVM&amp;quot;ght determine how to weight these costs. Finally, there are two ALL feature sets: ALL-BI includes NGRAMS, WORDSBI and COSTS, plus features for every possible phone and diphone, and features for every specific unit in the database; ALL-TRI includes NGRAMS, WORDS-TRI, COSTS, and a feature for every phone, diphone and triphone, as well as specific units in the database. As with WORDS, the value of a feature is the count of that feature in a given synthesized utterance. 4 Cross-Validation Study To train and test our ranker on our feature sets, we partitioned the co</context>
<context position="28365" citStr="Bulyko and Ostendorf, 2002" startWordPosition="4600" endWordPosition="4603"> no substantial impact on our ranker, as we would expect ALL-TRI to be significantly higher than WORDS-TRI if so. However, since COSTS does significantly improve upon NGRAM2 (t = 3.5, p &lt; 0.001), there is some value to the use of synthesizer features in the absence of WORDS. We also looked at the comparison for the WORDS models and COSTS. While WORDS-BI did not perform significantly better than COSTS ( t = 2.3, p &lt; 0.025), the added trigrams in WORDSTRI did improve ranker performance significantly over COSTS (t = 3.7, p &lt; 3.29x10-4). Since COSTS ranks realizations in the much the same way as (Bulyko and Ostendorf, 2002), the fact that WORDS-TRI outperforms COSTS indicates that our discriminative reranking method can significantly improve upon their non-discriminative approach. 5 Conclusions In this paper, we have presented a method for adapting a language generator to the strengths and weaknesses of a particular synthetic voice by training a discriminative reranker to select paraphrases that are predicted to sound natural when synthesized. In contrast to previous work on this topic, our method can be employed with any speech synthesizer in principle, so long as features derived from the synthesizer’s unit se</context>
</contexts>
<marker>Bulyko, Ostendorf, 2002</marker>
<rawString>Ivan Bulyko and Mari Ostendorf. 2002. Efficient integrated response generation from multiple targets using weighted finite state transducers. Computer Speech and Language, 16:533–550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert A J Clark</author>
<author>Korin Richmond</author>
<author>Simon King</author>
</authors>
<title>Festival 2 – build your own general purpose unit selection speech synthesiser.</title>
<date>2004</date>
<booktitle>In 5th ISCA Speech Synthesis Workshop,</booktitle>
<pages>173--178</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="14607" citStr="Clark et al., 2004" startWordPosition="2325" endWordPosition="2328">t draws from Villeroy and BochH✄ ’s Funny DayH✄ collection LL% . Figure 1: Example of sampled periphrastic alternatives for a sentence. quality of the sampled paraphrases is very high, only occasionally including dispreferred word orders such as We here have a design in the family style, where here is in medial position rather than fronted.4 3.2 Synthesizing Utterances For synthesis, OpenCCG’s output realizations are converted to APML,5 a markup language which allows pitch accents and boundary tones to be specified, and then passed to the Festival speech synthesis system (Taylor et al., 1998; Clark et al., 2004). Festival uses the prosodic markup in the text analysis phase of synthesis in place of the structures that it would otherwise have to predict from the text. The synthesiser then uses the context provided by the markup to enforce the selec4In other examples medial position is preferred, e.g. This design here is in the family style. 5Affective Presentation Markup Language; see http://www.cstr.ed.ac.uk/projects/ festival/apml.html. tion of suitable units from the database. A custom synthetic voice for the COMIC system was developed, as follows. First, a domainspecific recording script was prepar</context>
</contexts>
<marker>Clark, Richmond, King, 2004</marker>
<rawString>Robert A.J. Clark, Korin Richmond, and Simon King. 2004. Festival 2 – build your own general purpose unit selection speech synthesiser. In 5th ISCA Speech Synthesis Workshop, pages 173–178, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In Proc. ICML.</booktitle>
<contexts>
<context position="9104" citStr="Collins, 2000" startWordPosition="1407" endWordPosition="1408">d for reranking according to their predicted synthesis quality. Once the realizations have been reranked, the top scoring realization can be sent to the synthesizer as usual. Alternatively, when features derived from the synthesizer’s unit selection search can be made available—and if the time demands of the dialogue system permit—several of the top scoring reranked realizations can be sent to the synthesizer, and the resulting utterances can be rescored with the extended feature set. Our reranking approach has been inspired by previous work on reranking in parsing and generation, especially (Collins, 2000) and (Walker et al., 2002). As in Walker et al.’s (2002) method for training a sentence plan ranker, we use our generator to produce a representative sample of paraphrases and then solicit human judgements of their naturalness to use as data for training the ranker. This method is attractive when there is no suitable corpus of naturally occurring dialogues available for training purposes, as is often the case for systems that engage in human-computer dialogues that differ substantially from human-human ones. The primary difference between Walker et al.’s work and ours is that theirs examines t</context>
<context position="19320" citStr="Collins (2000)" startWordPosition="3077" endWordPosition="3078">judges’ ratings was 0.45, with one judge’s ratings consistently higher than the other’s. Some common problems noted by the judges included slurred words, especially the sometimes sounding like ther or even their; clipped words, such as has shortened at times to the point of sounding like is, or though clipped to unintelligibility; unnatural phrasing or emphasis, e.g. occasional pauses before a possessive ’s, or words such as style sounding emphasized when they should be deaccented; unnatural rate changes; “choppy” speech from poor joins; and some unintelligible proper names. 3.4 Ranking While Collins (2000) and Walker et al. (2002) develop their rankers using the RankBoost algorithm (Freund et al., 1998), we have instead chosen to use Joachims’ (2002) method of formulating ranking tasks as Support Vector Machine (SVM) constraint optimization problems.8 This choice has been motivated primarily by convenience, as Joachims’ SVMiZght package is easy to 7http://www.hcrc.ed.ac.uk/web exp/ 8See (Barzilay and Lapata, 2005) for another application of SVM ranking in generation, namely to the task of ranking alternative text orderings for local coherence. use; we leave it for future work to compare the per</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative reranking for natural language parsing. In Proc. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Raymond Davis</author>
<author>Julia Hirschberg</author>
</authors>
<title>Assigning intonational features in synthesized spoken directions.</title>
<date>1988</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="5001" citStr="Davis and Hirschberg, 1988" startWordPosition="770" endWordPosition="773">me possible. The paper is organized as follows. In Section 2, we review previous work on integrating choice in language generation and speech synthesis, and on learning discriminative rerankers for generation. In Section 3, we present our method. In Section 4, we describe a cross-validation study whose results indicate that discriminative paraphrase reranking can achieve substantial improvements in naturalness on average. Finally, in Section 5, we conclude with a summary and a discussion of future work. 2 Previous Work Most previous work on integrating language generation and synthesis, e.g. (Davis and Hirschberg, 1988; Prevost and Steedman, 1994; Hitzeman et al., 1998; Pan et al., 2002), has focused on how to use the information present in the language generation component in order to specify contextually appropriate intonation for the speech synthesizer to target. For example, syntactic structure, information structure and dialogue context have all been argued to play a role in improving prosody prediction, compared to unrestricted textto-speech synthesis. While this topic remains an important area of research, our focus is instead on a different opportunity that arises in a dialogue system, namely, the p</context>
</contexts>
<marker>Davis, Hirschberg, 1988</marker>
<rawString>James Raymond Davis and Julia Hirschberg. 1988. Assigning intonational features in synthesized spoken directions. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Els den Os</author>
<author>Lou Boves</author>
</authors>
<title>Towards ambient intelligence: Multimodal computers that understand our intentions.</title>
<date>2003</date>
<booktitle>In Proc. eChallenges-03.</booktitle>
<marker>den Os, Boves, 2003</marker>
<rawString>Els den Os and Lou Boves. 2003. Towards ambient intelligence: Multimodal computers that understand our intentions. In Proc. eChallenges-03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Ellen Foster</author>
<author>Michael White</author>
</authors>
<title>Techniques for Text Planning with XSLT.</title>
<date>2004</date>
<booktitle>In Proc. 4th NLPXML Workshop.</booktitle>
<contexts>
<context position="11586" citStr="Foster and White, 2004" startWordPosition="1806" endWordPosition="1809">me content adapted to a different user model or adapted to a different dialogue history. For example, a description of a certain design’s colour scheme for one user might be phrased as As you can see, the tiles have a blue and green colour scheme, whereas a variant expression of the same content for a different user could be Although the tiles have a blue colour scheme, the design does also feature green, if the user disprefers blue. In COMIC, the sentence planner uses XSLT to generate disjunctive logical forms (LFs), which specify a range of possible paraphrases in a nested free-choice form (Foster and White, 2004). Such disjunctive LFs can be efficiently realized using the OpenCCG realizer (White, 2004; White, 2006b; White, 2006a). Note that for the experiments reported here, we manually augmented the disjunctive LFs for the 104 sentences in our sample to make greater use of the periphrastic capabilities of the COMIC grammar; it remains for future work to augment the COMIC sentence planner produce these more richly disjunctive LFs automatically. OpenCCG includes an extensible API for integrating language modeling and realization. To select preferred word orders, from among all those allowed by the gram</context>
</contexts>
<marker>Foster, White, 2004</marker>
<rawString>Mary Ellen Foster and Michael White. 2004. Techniques for Text Planning with XSLT. In Proc. 4th NLPXML Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Ellen Foster</author>
<author>Michael White</author>
</authors>
<title>Assessing the impact of adaptive generation in the COMIC multimodal dialogue system.</title>
<date>2005</date>
<booktitle>In Proc. IJCAI-05 Workshop on Knowledge and Representation in Practical Dialogue Systems.</booktitle>
<contexts>
<context position="10753" citStr="Foster and White, 2005" startWordPosition="1662" endWordPosition="1665"> synthesis have been carried out in the context of the COMIC3 multimodal dialogue system (den Os and Boves, 2003). The COMIC system adds a dialogue interface to a CAD-like application used in sales situations to help clients redesign their bathrooms. The input to the system includes speech, handwriting, and pen gestures; the output combines synthesized speech, an animated talking head, deictic gestures at on-screen objects, and direct control of the underlying application. 3COnversational Multimodal Interaction with Computers, http://www.hcrc.ed.ac.uk/comic/. Drawing on the materials used in (Foster and White, 2005) to evaluate adaptive generation in COMIC, we selected a sample of 104 sentences from 38 different output turns across three dialogues. For each sentence in the set, a variant was included that expressed the same content adapted to a different user model or adapted to a different dialogue history. For example, a description of a certain design’s colour scheme for one user might be phrased as As you can see, the tiles have a blue and green colour scheme, whereas a variant expression of the same content for a different user could be Although the tiles have a blue colour scheme, the design does a</context>
</contexts>
<marker>Foster, White, 2005</marker>
<rawString>Mary Ellen Foster and Michael White. 2005. Assessing the impact of adaptive generation in the COMIC multimodal dialogue system. In Proc. IJCAI-05 Workshop on Knowledge and Representation in Practical Dialogue Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R Iyer</author>
<author>R E Schapire</author>
<author>Y Singer</author>
</authors>
<title>An efficient boosting algorithm for combining preferences.</title>
<date>1998</date>
<booktitle>In Machine Learning: Proc. of the Fifteenth International Conference.</booktitle>
<contexts>
<context position="19419" citStr="Freund et al., 1998" startWordPosition="3092" endWordPosition="3095"> common problems noted by the judges included slurred words, especially the sometimes sounding like ther or even their; clipped words, such as has shortened at times to the point of sounding like is, or though clipped to unintelligibility; unnatural phrasing or emphasis, e.g. occasional pauses before a possessive ’s, or words such as style sounding emphasized when they should be deaccented; unnatural rate changes; “choppy” speech from poor joins; and some unintelligible proper names. 3.4 Ranking While Collins (2000) and Walker et al. (2002) develop their rankers using the RankBoost algorithm (Freund et al., 1998), we have instead chosen to use Joachims’ (2002) method of formulating ranking tasks as Support Vector Machine (SVM) constraint optimization problems.8 This choice has been motivated primarily by convenience, as Joachims’ SVMiZght package is easy to 7http://www.hcrc.ed.ac.uk/web exp/ 8See (Barzilay and Lapata, 2005) for another application of SVM ranking in generation, namely to the task of ranking alternative text orderings for local coherence. use; we leave it for future work to compare the performance of RankBoost and SVMiZght on our ranking task. The ranker takes as input a set of paraphra</context>
</contexts>
<marker>Freund, Iyer, Schapire, Singer, 1998</marker>
<rawString>Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. 1998. An efficient boosting algorithm for combining preferences. In Machine Learning: Proc. of the Fifteenth International Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janet Hitzeman</author>
<author>Alan W Black</author>
<author>Chris Mellish</author>
<author>Jon Oberlander</author>
<author>Paul Taylor</author>
</authors>
<title>On the use of automatically generated discourse-level information in a concept-to-speech synthesis system.</title>
<date>1998</date>
<booktitle>In Proc. ICSLP-98.</booktitle>
<contexts>
<context position="5052" citStr="Hitzeman et al., 1998" startWordPosition="778" endWordPosition="781">on 2, we review previous work on integrating choice in language generation and speech synthesis, and on learning discriminative rerankers for generation. In Section 3, we present our method. In Section 4, we describe a cross-validation study whose results indicate that discriminative paraphrase reranking can achieve substantial improvements in naturalness on average. Finally, in Section 5, we conclude with a summary and a discussion of future work. 2 Previous Work Most previous work on integrating language generation and synthesis, e.g. (Davis and Hirschberg, 1988; Prevost and Steedman, 1994; Hitzeman et al., 1998; Pan et al., 2002), has focused on how to use the information present in the language generation component in order to specify contextually appropriate intonation for the speech synthesizer to target. For example, syntactic structure, information structure and dialogue context have all been argued to play a role in improving prosody prediction, compared to unrestricted textto-speech synthesis. While this topic remains an important area of research, our focus is instead on a different opportunity that arises in a dialogue system, namely, the possibility of choosing the exact wording and prosod</context>
</contexts>
<marker>Hitzeman, Black, Mellish, Oberlander, Taylor, 1998</marker>
<rawString>Janet Hitzeman, Alan W. Black, Chris Mellish, Jon Oberlander, and Paul Taylor. 1998. On the use of automatically generated discourse-level information in a concept-to-speech synthesis system. In Proc. ICSLP-98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hunt</author>
<author>A Black</author>
</authors>
<title>Unit selection in a concatenative speech synthesis system using a large speech database.</title>
<date>1996</date>
<booktitle>In Proc. ICASSP-96,</booktitle>
<location>Atlanta, Georgia.</location>
<contexts>
<context position="1637" citStr="Hunt and Black, 1996" startWordPosition="241" endWordPosition="244">ay’s unit selection synthesizers. 1 Introduction Unit selection synthesis1—a technique which concatenates segments of natural speech selected from a database—has been found to be capable of producing high quality synthetic speech, especially for utterances that are similar to the speech in the database in terms of style, delivery, and coverage (Black and Lenzo, 2001). In particular, in the limited domain of a spoken language dialogue system, it is possible to achieve highly natural synthesis with a purpose-built voice (Black and Lenzo, 2000). However, it can be difficult to develop 1See e.g. (Hunt and Black, 1996; Black and Taylor, 1997; Beutnagel et al., 1999). a synthetic voice for a dialogue system that produces natural speech completely reliably, and thus in practice output quality can be quite variable. Two important factors in this regard are the labeling process for the speech database and the direction of the dialogue system’s further development, after the voice has been built: when labels are assigned fully automatically to the recorded speech, label boundaries may be inaccurate, leading to unnatural sounding joins in speech output; and when further system development leads to the generation</context>
</contexts>
<marker>Hunt, Black, 1996</marker>
<rawString>A. Hunt and A. Black. 1996. Unit selection in a concatenative speech synthesis system using a large speech database. In Proc. ICASSP-96, Atlanta, Georgia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lidija Iordanskaja</author>
<author>Richard Kittredge</author>
<author>Alain Polg´uere</author>
</authors>
<title>Lexical selection and paraphrase in a meaning-text generation model.</title>
<date>1991</date>
<booktitle>Natural Language Generation in Artificial Intelligence and Computational Linguistics,</booktitle>
<pages>293--312</pages>
<editor>In C´ecile L. Paris, William R. Swartout, and William C. Mann, editors,</editor>
<publisher>Kluwer.</publisher>
<marker>Iordanskaja, Kittredge, Polg´uere, 1991</marker>
<rawString>Lidija Iordanskaja, Richard Kittredge, and Alain Polg´uere. 1991. Lexical selection and paraphrase in a meaning-text generation model. In C´ecile L. Paris, William R. Swartout, and William C. Mann, editors, Natural Language Generation in Artificial Intelligence and Computational Linguistics, pages 293–312. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Optimizing search engines using clickthrough data. In</title>
<date>2002</date>
<booktitle>Proc. KDD.</booktitle>
<marker>Joachims, 2002</marker>
<rawString>Thorsten Joachims. 2002. Optimizing search engines using clickthrough data. In Proc. KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde</author>
<author>Kevin Knight</author>
</authors>
<title>Generation that exploits corpus-based statistical knowledge.</title>
<date>1998</date>
<booktitle>In Proc. COLING-ACL.</booktitle>
<contexts>
<context position="3425" citStr="Langkilde and Knight, 1998" startWordPosition="532" endWordPosition="535">ess the desired content in the dialogue context, the system can select the specific paraphrase to use as its response according to the predicted quality of the speech synthesized for that paraphrase. In this way, if there are significant differences in the predicted synthesis quality for the various paraphrases—and if these predictions are generally borne out—then, by selecting paraphrases with high predicted synthesis quality, the dialogue system (as a whole) can more reliably produce natural sounding speech. In this paper, we present an application of dis2See e.g. (Iordanskaja et al., 1991; Langkilde and Knight, 1998; Barzilay and McKeown, 2001; Pang et al., 2003) for discussion of paraphrase in generation. 1113 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1113–1120, Sydney, July 2006. c�2006 Association for Computational Linguistics criminative reranking to the task of adapting a language generator to the strengths and weaknesses of a particular synthetic voice. Our method involves training a reranker to select paraphrases that are predicted to sound natural when synthesized, from the N-best realizations produced by the generator.</context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>Irene Langkilde and Kevin Knight. 1998. Generation that exploits corpus-based statistical knowledge. In Proc. COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shimei Pan</author>
<author>Wubin Weng</author>
</authors>
<title>Designing a speech corpus for instance-based spoken language generation.</title>
<date>2002</date>
<booktitle>In Proc. of the International Natural Language Generation Conference (INLG-02).</booktitle>
<contexts>
<context position="7383" citStr="Pan and Weng (2002)" startWordPosition="1138" endWordPosition="1141"> jointly determining wording, speech and gesture. In their approach, a template-based generator produces a word lattice with intonational phrase breaks. A unit selection algorithm then searches for a low-cost way of realizing a path through this lattice that combines captured motion samples with recorded speech samples to create coherent phrases, blending segments of speech and motion together phrase-by-phrase into extended utterances. Video demonstrations indicate that natural and highly expressive results can be achieved, though no human evaluations are reported. In an alternative approach, Pan and Weng (2002) proposed integrating instance-based realization and synthesis. In their framework, sentence structure, wording, prosody and speech waveforms from a domain-specific corpus are simultaneously reused. To do so, they add prosodic and acoustic costs to the insertion, deletion and replacement costs used for instance-based surface realization. Their contribution focuses on how to design an appropriate speech corpus to facilitate an integrated approach to instance-based realization and synthesis, and does not report evaluation results. A drawback of these approaches to integrating choice in language </context>
</contexts>
<marker>Pan, Weng, 2002</marker>
<rawString>Shimei Pan and Wubin Weng. 2002. Designing a speech corpus for instance-based spoken language generation. In Proc. of the International Natural Language Generation Conference (INLG-02).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shimei Pan</author>
<author>Kathleen McKeown</author>
<author>Julia Hirschberg</author>
</authors>
<title>Exploring features from natural language generation for prosody modeling. Computer Speech and Language,</title>
<date>2002</date>
<pages>16--457</pages>
<contexts>
<context position="5071" citStr="Pan et al., 2002" startWordPosition="782" endWordPosition="785">s work on integrating choice in language generation and speech synthesis, and on learning discriminative rerankers for generation. In Section 3, we present our method. In Section 4, we describe a cross-validation study whose results indicate that discriminative paraphrase reranking can achieve substantial improvements in naturalness on average. Finally, in Section 5, we conclude with a summary and a discussion of future work. 2 Previous Work Most previous work on integrating language generation and synthesis, e.g. (Davis and Hirschberg, 1988; Prevost and Steedman, 1994; Hitzeman et al., 1998; Pan et al., 2002), has focused on how to use the information present in the language generation component in order to specify contextually appropriate intonation for the speech synthesizer to target. For example, syntactic structure, information structure and dialogue context have all been argued to play a role in improving prosody prediction, compared to unrestricted textto-speech synthesis. While this topic remains an important area of research, our focus is instead on a different opportunity that arises in a dialogue system, namely, the possibility of choosing the exact wording and prosody of a response acc</context>
</contexts>
<marker>Pan, McKeown, Hirschberg, 2002</marker>
<rawString>Shimei Pan, Kathleen McKeown, and Julia Hirschberg. 2002. Exploring features from natural language generation for prosody modeling. Computer Speech and Language, 16:457–490.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Syntax-based alignment of multiple translations: Extracting paraphrases and generating new sentences.</title>
<date>2003</date>
<booktitle>In Proc. HLT/NAACL.</booktitle>
<contexts>
<context position="3473" citStr="Pang et al., 2003" startWordPosition="540" endWordPosition="543">tem can select the specific paraphrase to use as its response according to the predicted quality of the speech synthesized for that paraphrase. In this way, if there are significant differences in the predicted synthesis quality for the various paraphrases—and if these predictions are generally borne out—then, by selecting paraphrases with high predicted synthesis quality, the dialogue system (as a whole) can more reliably produce natural sounding speech. In this paper, we present an application of dis2See e.g. (Iordanskaja et al., 1991; Langkilde and Knight, 1998; Barzilay and McKeown, 2001; Pang et al., 2003) for discussion of paraphrase in generation. 1113 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1113–1120, Sydney, July 2006. c�2006 Association for Computational Linguistics criminative reranking to the task of adapting a language generator to the strengths and weaknesses of a particular synthetic voice. Our method involves training a reranker to select paraphrases that are predicted to sound natural when synthesized, from the N-best realizations produced by the generator. The ranker is trained in supervised fashion, us</context>
</contexts>
<marker>Pang, Knight, Marcu, 2003</marker>
<rawString>Bo Pang, Kevin Knight, and Daniel Marcu. 2003. Syntax-based alignment of multiple translations: Extracting paraphrases and generating new sentences. In Proc. HLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Prevost</author>
<author>Mark Steedman</author>
</authors>
<title>Specifying intonation from context for speech synthesis.</title>
<date>1994</date>
<journal>Speech Communication,</journal>
<pages>15--139</pages>
<contexts>
<context position="5029" citStr="Prevost and Steedman, 1994" startWordPosition="774" endWordPosition="777">ganized as follows. In Section 2, we review previous work on integrating choice in language generation and speech synthesis, and on learning discriminative rerankers for generation. In Section 3, we present our method. In Section 4, we describe a cross-validation study whose results indicate that discriminative paraphrase reranking can achieve substantial improvements in naturalness on average. Finally, in Section 5, we conclude with a summary and a discussion of future work. 2 Previous Work Most previous work on integrating language generation and synthesis, e.g. (Davis and Hirschberg, 1988; Prevost and Steedman, 1994; Hitzeman et al., 1998; Pan et al., 2002), has focused on how to use the information present in the language generation component in order to specify contextually appropriate intonation for the speech synthesizer to target. For example, syntactic structure, information structure and dialogue context have all been argued to play a role in improving prosody prediction, compared to unrestricted textto-speech synthesis. While this topic remains an important area of research, our focus is instead on a different opportunity that arises in a dialogue system, namely, the possibility of choosing the e</context>
</contexts>
<marker>Prevost, Steedman, 1994</marker>
<rawString>Scott Prevost and Mark Steedman. 1994. Specifying intonation from context for speech synthesis. Speech Communication, 15:139–153.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Stone</author>
<author>Doug DeCarlo</author>
<author>Insuk Oh</author>
<author>Christian Rodriguez</author>
<author>Adrian Stere</author>
<author>Alyssa Lees</author>
<author>Chris Bregler</author>
</authors>
<title>Speaking with hands: Creating animated conversational characters from recordings of human performance.</title>
<date>2004</date>
<journal>ACM Transactions on Graphics (SIGGRAPH),</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="6741" citStr="Stone et al. (2004)" startWordPosition="1043" endWordPosition="1046">orm the unit selection search on this expanded input efficiently, they employ weighted finite-state transducers, where each step of network expansion is then followed by minimization. The weights are determined by concatenation (join) costs, relative frequencies (negative log probabilities) of the word sequences, and prosodic prediction costs, for cases where the prosody is not determined by the templates. In a perception experiment, they demonstrated that by expanding the space of candidate responses, their system achieved higher quality speech output. Following (Bulyko and Ostendorf, 2002), Stone et al. (2004) developed a method for jointly determining wording, speech and gesture. In their approach, a template-based generator produces a word lattice with intonational phrase breaks. A unit selection algorithm then searches for a low-cost way of realizing a path through this lattice that combines captured motion samples with recorded speech samples to create coherent phrases, blending segments of speech and motion together phrase-by-phrase into extended utterances. Video demonstrations indicate that natural and highly expressive results can be achieved, though no human evaluations are reported. In an</context>
</contexts>
<marker>Stone, DeCarlo, Oh, Rodriguez, Stere, Lees, Bregler, 2004</marker>
<rawString>Matthew Stone, Doug DeCarlo, Insuk Oh, Christian Rodriguez, Adrian Stere, Alyssa Lees, and Chris Bregler. 2004. Speaking with hands: Creating animated conversational characters from recordings of human performance. ACM Transactions on Graphics (SIGGRAPH), 23(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Taylor</author>
<author>A Black</author>
<author>R Caley</author>
</authors>
<title>The architecture of the the Festival speech synthesis system.</title>
<date>1998</date>
<booktitle>In Third International Workshop on Speech Synthesis,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="14586" citStr="Taylor et al., 1998" startWordPosition="2321" endWordPosition="2324"> here is a design that draws from Villeroy and BochH✄ ’s Funny DayH✄ collection LL% . Figure 1: Example of sampled periphrastic alternatives for a sentence. quality of the sampled paraphrases is very high, only occasionally including dispreferred word orders such as We here have a design in the family style, where here is in medial position rather than fronted.4 3.2 Synthesizing Utterances For synthesis, OpenCCG’s output realizations are converted to APML,5 a markup language which allows pitch accents and boundary tones to be specified, and then passed to the Festival speech synthesis system (Taylor et al., 1998; Clark et al., 2004). Festival uses the prosodic markup in the text analysis phase of synthesis in place of the structures that it would otherwise have to predict from the text. The synthesiser then uses the context provided by the markup to enforce the selec4In other examples medial position is preferred, e.g. This design here is in the family style. 5Affective Presentation Markup Language; see http://www.cstr.ed.ac.uk/projects/ festival/apml.html. tion of suitable units from the database. A custom synthetic voice for the COMIC system was developed, as follows. First, a domainspecific record</context>
</contexts>
<marker>Taylor, Black, Caley, 1998</marker>
<rawString>P. Taylor, A. Black, and R. Caley. 1998. The architecture of the the Festival speech synthesis system. In Third International Workshop on Speech Synthesis, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Owen C Rambow</author>
<author>Monica Rogati</author>
</authors>
<title>Training a sentence planner for spoken dialogue using boosting. Computer Speech and Language,</title>
<date>2002</date>
<pages>16--409</pages>
<contexts>
<context position="9130" citStr="Walker et al., 2002" startWordPosition="1410" endWordPosition="1413">rding to their predicted synthesis quality. Once the realizations have been reranked, the top scoring realization can be sent to the synthesizer as usual. Alternatively, when features derived from the synthesizer’s unit selection search can be made available—and if the time demands of the dialogue system permit—several of the top scoring reranked realizations can be sent to the synthesizer, and the resulting utterances can be rescored with the extended feature set. Our reranking approach has been inspired by previous work on reranking in parsing and generation, especially (Collins, 2000) and (Walker et al., 2002). As in Walker et al.’s (2002) method for training a sentence plan ranker, we use our generator to produce a representative sample of paraphrases and then solicit human judgements of their naturalness to use as data for training the ranker. This method is attractive when there is no suitable corpus of naturally occurring dialogues available for training purposes, as is often the case for systems that engage in human-computer dialogues that differ substantially from human-human ones. The primary difference between Walker et al.’s work and ours is that theirs examines the impact on text quality </context>
<context position="19345" citStr="Walker et al. (2002)" startWordPosition="3080" endWordPosition="3083"> 0.45, with one judge’s ratings consistently higher than the other’s. Some common problems noted by the judges included slurred words, especially the sometimes sounding like ther or even their; clipped words, such as has shortened at times to the point of sounding like is, or though clipped to unintelligibility; unnatural phrasing or emphasis, e.g. occasional pauses before a possessive ’s, or words such as style sounding emphasized when they should be deaccented; unnatural rate changes; “choppy” speech from poor joins; and some unintelligible proper names. 3.4 Ranking While Collins (2000) and Walker et al. (2002) develop their rankers using the RankBoost algorithm (Freund et al., 1998), we have instead chosen to use Joachims’ (2002) method of formulating ranking tasks as Support Vector Machine (SVM) constraint optimization problems.8 This choice has been motivated primarily by convenience, as Joachims’ SVMiZght package is easy to 7http://www.hcrc.ed.ac.uk/web exp/ 8See (Barzilay and Lapata, 2005) for another application of SVM ranking in generation, namely to the task of ranking alternative text orderings for local coherence. use; we leave it for future work to compare the performance of RankBoost and</context>
</contexts>
<marker>Walker, Rambow, Rogati, 2002</marker>
<rawString>Marilyn A. Walker, Owen C. Rambow, and Monica Rogati. 2002. Training a sentence planner for spoken dialogue using boosting. Computer Speech and Language, 16:409–433.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
</authors>
<title>Reining in CCG Chart Realization. In</title>
<date>2004</date>
<booktitle>Proc. INLG-04.</booktitle>
<contexts>
<context position="11586" citStr="White, 2004" startWordPosition="1808" endWordPosition="1809">adapted to a different user model or adapted to a different dialogue history. For example, a description of a certain design’s colour scheme for one user might be phrased as As you can see, the tiles have a blue and green colour scheme, whereas a variant expression of the same content for a different user could be Although the tiles have a blue colour scheme, the design does also feature green, if the user disprefers blue. In COMIC, the sentence planner uses XSLT to generate disjunctive logical forms (LFs), which specify a range of possible paraphrases in a nested free-choice form (Foster and White, 2004). Such disjunctive LFs can be efficiently realized using the OpenCCG realizer (White, 2004; White, 2006b; White, 2006a). Note that for the experiments reported here, we manually augmented the disjunctive LFs for the 104 sentences in our sample to make greater use of the periphrastic capabilities of the COMIC grammar; it remains for future work to augment the COMIC sentence planner produce these more richly disjunctive LFs automatically. OpenCCG includes an extensible API for integrating language modeling and realization. To select preferred word orders, from among all those allowed by the gram</context>
</contexts>
<marker>White, 2004</marker>
<rawString>Michael White. 2004. Reining in CCG Chart Realization. In Proc. INLG-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
</authors>
<title>CCG chart realization from disjunctive logical forms.</title>
<date>2006</date>
<booktitle>In Proc. INLG-06.</booktitle>
<note>To appear.</note>
<contexts>
<context position="11689" citStr="White, 2006" startWordPosition="1824" endWordPosition="1825"> of a certain design’s colour scheme for one user might be phrased as As you can see, the tiles have a blue and green colour scheme, whereas a variant expression of the same content for a different user could be Although the tiles have a blue colour scheme, the design does also feature green, if the user disprefers blue. In COMIC, the sentence planner uses XSLT to generate disjunctive logical forms (LFs), which specify a range of possible paraphrases in a nested free-choice form (Foster and White, 2004). Such disjunctive LFs can be efficiently realized using the OpenCCG realizer (White, 2004; White, 2006b; White, 2006a). Note that for the experiments reported here, we manually augmented the disjunctive LFs for the 104 sentences in our sample to make greater use of the periphrastic capabilities of the COMIC grammar; it remains for future work to augment the COMIC sentence planner produce these more richly disjunctive LFs automatically. OpenCCG includes an extensible API for integrating language modeling and realization. To select preferred word orders, from among all those allowed by the grammar for the input LF, we used a backoff trigram model trained on approximately 750 example target sente</context>
</contexts>
<marker>White, 2006</marker>
<rawString>Michael White. 2006a. CCG chart realization from disjunctive logical forms. In Proc. INLG-06. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
</authors>
<title>Efficient Realization of Coordinate Structures</title>
<date>2006</date>
<booktitle>in Combinatory Categorial Grammar. Research on Language &amp; Computation, online first,</booktitle>
<contexts>
<context position="11689" citStr="White, 2006" startWordPosition="1824" endWordPosition="1825"> of a certain design’s colour scheme for one user might be phrased as As you can see, the tiles have a blue and green colour scheme, whereas a variant expression of the same content for a different user could be Although the tiles have a blue colour scheme, the design does also feature green, if the user disprefers blue. In COMIC, the sentence planner uses XSLT to generate disjunctive logical forms (LFs), which specify a range of possible paraphrases in a nested free-choice form (Foster and White, 2004). Such disjunctive LFs can be efficiently realized using the OpenCCG realizer (White, 2004; White, 2006b; White, 2006a). Note that for the experiments reported here, we manually augmented the disjunctive LFs for the 104 sentences in our sample to make greater use of the periphrastic capabilities of the COMIC grammar; it remains for future work to augment the COMIC sentence planner produce these more richly disjunctive LFs automatically. OpenCCG includes an extensible API for integrating language modeling and realization. To select preferred word orders, from among all those allowed by the grammar for the input LF, we used a backoff trigram model trained on approximately 750 example target sente</context>
</contexts>
<marker>White, 2006</marker>
<rawString>Michael White. 2006b. Efficient Realization of Coordinate Structures in Combinatory Categorial Grammar. Research on Language &amp; Computation, online first, March.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>