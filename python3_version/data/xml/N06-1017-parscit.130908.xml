<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000050">
<title confidence="0.98461">
Unknown word sense detection as outlier detection
</title>
<author confidence="0.966507">
Katrin Erk
</author>
<affiliation confidence="0.966592">
Computational Linguistics
Saarland University
</affiliation>
<address confidence="0.502337">
Saarbr¨ucken, Germany
</address>
<email confidence="0.996661">
erk@coli.uni-sb.de
</email>
<sectionHeader confidence="0.995615" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999981666666667">
We address the problem of unknown word
sense detection: the identification of cor-
pus occurrences that are not covered by
a given sense inventory. We model this
as an instance of outlier detection, using
a simple nearest neighbor-based approach
to measuring the resemblance of a new
item to a training set. In combination with
a method that alleviates data sparseness by
sharing training data across lemmas, the
approach achieves a precision of 0.77 and
recall of 0.82.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99807">
If a system has seen only positive examples, how
does it recognize a negative example? This is
the problem addressed by outlier detection, also
called novelty detection1 (Markou and Singh, 2003a;
Markou and Singh, 2003b; Marsland, 2003): to de-
tect novel or unknown items that differ from all the
seen training data. Outlier detection approaches typ-
ically derive some model of “normal” objects from
the training set and use a distance measure and a
threshold to detect abnormal items.
In this paper, we apply outlier detection tech-
niques to the task of unknown sense detection: the
identification of corpus occurrences that are not cov-
ered by a given sense inventory. The training set
</bodyText>
<footnote confidence="0.872009666666667">
1The term novelty detection is also used for the distinction
of novel and repeated information in information retrieval, a
different if related topic.
</footnote>
<figureCaption confidence="0.993634">
Figure 1: Wrong assignment due to missing sense:
from the Hound of the Baskervilles, Ch. 14
</figureCaption>
<bodyText confidence="0.999548789473685">
against which new occurrences are compared will
consist of sense-annotated text.
Unknown sense detection is related to word sense
disambiguation (WSD) and to word sense discrim-
ination (Sch¨utze, 1998), but differs from both. In
WSD all senses are assumed known, and the task is
to select one of them, while in unknown sense detec-
tion the task is to decide whether a given occurrence
matches any of the known senses or none of them,
and all training instances, regardless of the sense to
which they belong, are modeled as one group of
known data. Unknown sense detection also differs
from word sense discrimination, where no sense in-
ventory is given and the task is to group occurrences
into senses. In unknown sense detection the model
respects the given word senses.
The main motivation for this study comes from
shallow semantic parsing, by which we mean a com-
bination of WSD and the automatic assignment of
</bodyText>
<page confidence="0.960905">
128
</page>
<note confidence="0.9951295">
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 128–135,
New York, June 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.997552803571429">
semantic roles to free text. In cases where a sense
is missing from the inventory, WSD will wrongly
assign one of the existing senses. Figure 1 shows
an example, a sentence from the Hound of the
Baskervilles, analyzed by the SHALMANESER (Erk
and Pado, 2006) shallow semantic parser. The anal-
ysis is based on FrameNet (Baker et al., 1998), a
resource that lists senses and semantic roles for En-
glish expressions. FrameNet is lacking a sense of
“expectation” or “being mentally prepared” for the
verb prepare, so prepared has been assigned the
sense COOKING CREATION, a possible but improb-
able analysis2. Such erroneous labels can be fa-
tal when further processing builds on the results of
shallow semantic parsing, e.g. for drawing infer-
ences. Unknown sense detection can prevent such
mistakes.
All sense inventories face the problem of missing
senses, either because of their small overall size (as
is the case for some non-English WordNets) or when
they encounter domain-specific senses. Our study
will be evaluated on FrameNet because of our main
aim of improving shallow semantic parsing, but the
method we propose is applicable to any sense inven-
tory that has annotated data; in particular, it is also
applicable to WordNet.
In this paper we model unknown sense detec-
tion as outlier detection, using a simple Nearest
Neighbor-based method (Tax and Duin, 2000) that
compares the local probability density at each test
item with that of its nearest training item.
To our knowledge, there exists no other approach
to date to the problem of detecting unknown senses.
There are, however, approaches to the complemen-
tary problem of determining the closest known sense
for unknown words (Widdows, 2003; Curran, 2005;
Burchardt et al., 2005), which can be viewed as the
logical next step after unknown sense detection.
Plan of the paper. After a brief sketch of
FrameNet in Section 2, we describe the experimen-
tal setup used throughout this paper in Section 3.
Section 4 tests whether a very simple model suffices
for detecting unknown senses: a threshold on confi-
dence scores returned by the SHALMANESER WSD
2Unfortunately, the semantic roles have been mis-assigned
by the system. The word I should fill the FOOD role, while for
a hound could be assigned the optional RECEIVER role.
system. The result is that recall is much too low.
Section 5 introduces the NN-based outlier detection
approach that we use in section 6 for unknown sense
detection, with better results than in the first experi-
ment but still low recall. Section 7 repeats the exper-
iment of section 6 with added training data, making
use of the fact that one semantic class in FrameNet
typically pertains to several lemmas and achieving a
marked improvement in results.
</bodyText>
<sectionHeader confidence="0.989287" genericHeader="introduction">
2 FrameNet
</sectionHeader>
<bodyText confidence="0.999602125">
Frame Semantics (Fillmore, 1982) models the mean-
ings of a word or expression by reference to
frames which describe the background and situa-
tional knowledge necessary for understanding what
the predicate is “about”. Each frame provides its
specific set of semantic roles.
The Berkeley FrameNet project (Baker et al.,
1998) is building a semantic lexicon for English de-
scribing the frames and linking them to the words
and expressions that can evoke them. These can
be verbs as well as nouns, adjectives, preposi-
tions, adverbs, and multiword expressions. Frames
are linked by IS-A and other relations. Currently,
FrameNet contains 609 frames with 8,755 lemma-
frame pairs, of which 5,308 are exemplified in an-
notated sentences from the British National Corpus.
The annotation comprises 133,846 sentences.
As FrameNet is a growing resource, many lem-
mas are still lacking senses, and many senses are still
lacking annotation. This is problematic for the use
of FrameNet analyses as a basis for inferences over
text, as e.g. in Tatu and Moldovan (2005).
For example, the verb prepare from Figure 1 is
associated with the frames
</bodyText>
<sectionHeader confidence="0.400955" genericHeader="method">
COOKING CREATION: prepare food
ACTIVITY PREPARE: get ready for an activity
ACTIVITY READY STATE: be ready for an activity
WILLINGNESS: be willing
</sectionHeader>
<bodyText confidence="0.9766875">
of which only the COOKING CREATION sense has
been annotated. The sense in Figure 1 is not cov-
ered yet: ACTIVITY READY STATE would be more
appropriate than COOKING CREATION, but still not
optimal, since the sentence refers to a mental state
rather than the preparation of an activity.
</bodyText>
<page confidence="0.999129">
129
</page>
<sectionHeader confidence="0.981329" genericHeader="method">
3 Experimental setup and data
</sectionHeader>
<bodyText confidence="0.999939193548387">
Experimental setup. To evaluate an unknown
sense detection system, we need occurrences that are
guaranteed not to belong to any of the seen senses.
To that end we use sense-annotated data, in our case
the FrameNet annotated sentences, simulating un-
known senses by designating one sense of each am-
biguous lemma as unknown. All occurrences of that
sense are placed in the test set, while occurrences
of all other senses are split randomly between train-
ing and test set, using 5-fold cross-validation. We
repeat the experiment with each of the senses of an
ambiguous lemma playing the part of the unknown
sense once. Viewing each cross-validation run for
each unknown sense as a separate experiment, we
then report precision and recall averaged over un-
known senses and cross-validation runs.
It may seem questionable that in this experimen-
tal setup, the unknown sense occurrences of each
lemma all belong to the same sense. However, this
does not bias the experiment since none of the mod-
els we study take advantage of the shape of the test
set in any way. Rather, each test item is classified in-
dividually, without recourse to the other test items.
Data. All experiments in this paper were per-
formed on the FrameNet 1.2 annotated data per-
taining to ambiguous lemmas. After removal of
instances that were annotated with more than one
sense, we obtain 26,496 annotated sentences for the
1,031 ambiguous lemmas. They were parsed with
Minipar (Lin, 1993); named entities were computed
using Heart of Gold (Callmeier et al., 2004).
</bodyText>
<sectionHeader confidence="0.877002" genericHeader="method">
4 Experiment 1: WSD confidence scores
</sectionHeader>
<subsectionHeader confidence="0.684981">
for unknown sense detection
</subsectionHeader>
<bodyText confidence="0.999772727272727">
In this section we test a very simple model of un-
known sense detection: Classifiers often return a
confidence score along with the assigned label. We
will try to detect unknown senses by a threshold
on confidence scores, declaring anything below the
threshold as unknown. Note that this method can
only be applied to lemmas that have more than one
sense, since for single-sense lemmas the system will
always return the maximum confidence score.
Data. While the approach that we follow in this
section is applicable to all lemmas with at least two
</bodyText>
<figure confidence="0.576244">
wave
s subj obj mod
She
She
her and upwards
(1): subj, obj, mod (since s and subj corefer,
we use only one of them)
(2): she, hand, outwards
(3): subj-she, obj-hand,mod-outwards
(4): mod-obj-subj
</figure>
<figureCaption confidence="0.994109">
Figure 2: Sample Minipar parse and extracted gram-
matical function features
</figureCaption>
<bodyText confidence="0.98713725">
senses, we need lemmas with at least three senses
to evaluate it: One of the senses of each lemma is
treated as unknown, which for lemmas with three or
more senses leaves at least two senses for the train-
ing set. This reduces our data set to 125 lemmas
with 7,435 annotated sentences.
Modeling. We test whether the WSD system built
into SHALMANESER (Erk, 2005) can distinguish
known sense items from unknown sense items reli-
ably by its confidence scores. The system extracts
a rich feature set, which forms the basis of all three
experiments in this paper:
</bodyText>
<listItem confidence="0.996935666666667">
• a bag-of-words context, with a window size of
one sentence;
• bi- and trigrams centered on the target word;
• grammatical function information: for each de-
pendent of the target, (1) its function label, (2)
its headword, and (3) a combination of both are
used as features. (4) The concatenation of all
function labels constitutes another feature. For
PPs, function labels are extended by the prepo-
sition. As an example, Figure 2 shows a BNC
sentence and its grammatical function features.
• for verb targets, the target voice.
</listItem>
<bodyText confidence="0.99982">
The feature set is based on Florian et al. (2002) but
contains additional syntax-related features. Each
word-related feature is represented as four features
for word, lemma, part of speech, and named entity.
SHALMANESER trains one Naive Bayes classifier
per lemma to be disambiguated. For this experiment,
</bodyText>
<figure confidence="0.905153666666667">
hand outwards
gen
punc conj
</figure>
<page confidence="0.505102">
130
</page>
<table confidence="0.5414714">
0 Precision Recall
0.5 0.6524 (v 0.115) 0.0011 (v 0.0004)
0.75 0.7855 (v 0.0086) 0.0527 (v 0.0013)
0.9 0.7855 (v 0.0093) 0.1006 (v 0.0021)
0.98 0.7847 (v 0.0073) 0.1744 (v 0.0025)
</table>
<tableCaption confidence="0.684952666666667">
Table 1: Experiment 1: Results for label unknown
sense, WSD confidence level approach. 0: confi-
dence threshold. Q: std. dev.
</tableCaption>
<figureCaption confidence="0.931858">
Figure 3: Outlier detection by comparing distances
between nearest neighbors
</figureCaption>
<equation confidence="0.9463518">
t‘
dtt´
t
x
dxt
</equation>
<bodyText confidence="0.999868777777778">
all system parameters were set to their default set-
tings. To detect unknown senses building on this
WSD system, we use a fixed confidence threshold
and label all items below the threshold as unknown.
Results and discussion. Table 1 shows precision
and recall for labeling instances as unknown using
different confidence thresholds 0, averaged over un-
known senses and 5-fold cross-validation3. We see
that while the precision of this method is acceptable
at 0.74 to 0.765, recall is extremely low, i.e. almost
no items were labeled unknown, even at a threshold
of 0.98. However, SHALMANESER has very high
confidence values overall: Only 14.5% of all in-
stances in this study had a confidence value of 0.98
or below (7,697 of 53,206).
We conclude that with the given WSD system and
(rather standard) features, this simple method cannot
detect items with an unknown sense reliably. This
may be due to the indiscriminately high confidence
scores; or it could indicate that classifiers, which
are geared at distinguishing between known classes
rather than detecting objects that differ from all seen
data, are not optimally suited to the task. However,
one further disadvantage of this approach is that, as
mentioned above, it can only be applied to lemmas
with more than one annotated sense. For FrameNet
1.2, this comprises only 19% of the lemmas.
</bodyText>
<sectionHeader confidence="0.9476615" genericHeader="method">
5 A nearest neighbor-based method for
outlier detection
</sectionHeader>
<bodyText confidence="0.982017351351352">
In the previous section we have tested a simple ap-
proach to unknown sense detection using WSD con-
fidence scores. Our conclusion was that it was not a
viable approach, given its low recall and given that
3Note that the minimum confidence score is 0.5 if 2 senses
are present in the training set, 0.33 for 3 present senses etc.
it is only applicable to lemmas with more than one
known sense. In this section we introduce an al-
ternative approach, which uses distances to nearest
neighbors to detect outliers.
In general, the task of outlier detection is to de-
cide whether a new object belongs to a given training
set or not. Typically, outlier detection approaches
derive some boundary around the training set, or
they derive from the set some model of “normal-
ity” to which new objects are compared (Markou
and Singh, 2003a; Markou and Singh, 2003b; Mars-
land, 2003). Applications of outlier detection in-
clude fault detection (Hickinbotham and Austin,
2000), hand writing deciphering (Tax and Duin,
1998; Sch¨olkopf et al., 2000), and network intru-
sion detection (Yeung and Chow, 2002; Dasgupta
and Forrest, 1999). One standard approach to out-
lier detection estimates the probability density of the
training set, such that a test object can be classified
as an outlier or non-outlier according to its probabil-
ity of belonging to the set.
Rather than estimating the complete density func-
tion, Tax and Duin (2000) approximate local density
at the test object by comparing distances between
nearest neighbors. Given a test object x, the ap-
proach considers the training object t nearest to x
and compares the distance dxt between x and t to the
distance dtt, between t and its own nearest training
data neighbor t&apos;. Then the quotient between the dis-
tances is used as an indicator of the (ab-)normality
of the test object x:
</bodyText>
<equation confidence="0.832905">
dxt
pNN�x� _
</equation>
<bodyText confidence="0.9152916">
When the distance dxt is much larger than dtt,, x is
considered an outlier. Figure 3 illustrates the idea.
The normality or abnormality of test objects is de-
cided by a fixed threshold 0 on pNN. The lowest
dtt,
</bodyText>
<page confidence="0.965789">
131
</page>
<bodyText confidence="0.9992425">
threshold that makes sense is 1.0, which rejects any
x that is further apart from its nearest training neigh-
bor t than t is from its neighbor. Tax and Duin use
Euclidean distance, i.e.
</bodyText>
<equation confidence="0.971688">
:dxt = (xi − ti)2
i
</equation>
<bodyText confidence="0.999663">
Applied to feature vectors with entries either 0 or 1,
this corresponds to the size of the symmetric differ-
ence of the two feature sets.
</bodyText>
<sectionHeader confidence="0.9932775" genericHeader="method">
6 Experiment 2: NN-based outlier
detection
</sectionHeader>
<bodyText confidence="0.993685451612903">
In this section we use the NN-based outlier detection
approach of the previous section for an experiment
in unknown sense detection. Experimental setup and
data are as described in Section 3.
Modeling. We model unknown sense detection as
an outlier detection task, using Tax and Duin’s out-
lier detection approach that we have outlined in
the previous section. Nearest neighbors (by Eu-
clidean distance) were computed using the ANN
tool (Mount and Arya, 2005). We compute one out-
lier detection model per lemma. With training and
test sets constructed as described in Section 3, the
average training set comprises 22.5 sentences.
We use the same features as in Section 4, with fea-
ture vector entries of 1 for present and 0 for absent
features. For a more detailed analysis of the contri-
bution of different feature types, we test on reduced
as well as full feature vectors:
All: full feature vectors
Cx: only bag-of-word context features (words, lem-
mas, POS, NE)
Syn: function labels of dependents
Syn-hw: Syn plus headwords of dependents
We compare the NN-based model to that of
Experiment 1, but not to any simpler baseline.
While for WSD it is possible to formulate simple
frequency-based methods that can serve as a base-
line, this is not so in unknown sense detection be-
cause the frequency of unknown senses is, by def-
inition, unknown. Furthermore, the number of an-
notated sentences per sense in FrameNet depends
</bodyText>
<table confidence="0.999312">
Features Precision Recall
All 0.7072 (v 0.0088) 0.2683 (v 0.0043)
Cx 0.7016 (v 0.0041) 0.3511 (v 0.0035)
Syn 0.8333 (v 0.0085) 0.2099 (v 0.0042)
Syn-hw 0.7784 (v 0.0029) 0.2368 (v 0.0022)
</table>
<tableCaption confidence="0.988439">
Table 2: Experiment 2: Results for label unknown
sense, NN-based outlier detection, 0 = 1.0. Q: stan-
dard deviation
</tableCaption>
<table confidence="0.9999695">
Precision Recall
Features all &gt; 10 &gt; 20 all &gt; 10 &gt; 20
All 0.71 0.70 0.67 0.27 0.35 0.45
Cx 0.70 0.70 0.67 0.35 0.47 0.58
Syn 0.83 0.81 0.77 0.21 0.22 0.21
Syn-hw 0.78 0.76 0.73 0.24 0.28 0.31
</table>
<tableCaption confidence="0.999826">
Table 3: Experiment 2: Results by training set size,
</tableCaption>
<equation confidence="0.703045">
0 = 1.0
</equation>
<bodyText confidence="0.999528">
on the number of subcategorization frames of the
lemma rather than the frequency of the sense, which
makes frequency calculations meaningless.
Results. Table 2 shows precision and recall for la-
beling instances as unknown using a distance quo-
tient threshold of 0=1.0, averaged over unknown
senses and over 5-fold cross-validation. We see that
recall is markedly higher than in Experiment 1, es-
pecially for the two conditions that include context
words, All and Cx. The syntax-based conditions
Syn and Syn-hw show a higher precision, with a
less pronounced increase in recall.
Raising the distance quotient threshold results in
little change in precision, but a large drop in recall.
For example, All vectors with a threshold of 0 =
1.1 achieve a recall of 0.14 in comparison to 0.27
for 0 = 1.0 .
Training set size is an important factor in sys-
tem results. Table 3 lists precision and recall for all
training sets, for training sets of size &gt; 10, and for
training sets of size &gt; 20. Especially in conditions
All and Cx, recall rises steeply when we only con-
sider cases with larger training sets. However note
that precision does not rise with larger training sets,
rather it shows a slight decline.
Another important factor is the number of senses
that a lemma has, as the upper part of Table 7 shows.
For lemmas with a higher number of senses, preci-
</bodyText>
<page confidence="0.99635">
132
</page>
<figureCaption confidence="0.911977">
Figure 4: “Acceptance radius” of an outlier within
the training set (left) and a more “normal” training
set object (right)
</figureCaption>
<bodyText confidence="0.997770125">
sion is much lower, while recall is much higher.
Discussion. While results in this experiment are
better than in Experiment 1 – in particular recall has
risen by 19 points for Cx –, system performance is
still not high enough to be usable in practice.
The uniformity of the training set has a large in-
fluence on performance, as Table 7 shows. The more
senses a lemma has, the harder it seems to be for the
model to identify known sense occurrences. Preci-
sion for the assignment of the unknown label drops,
while recall rises. We see a tradeoff between preci-
sion and recall, in this table as well as in Table 3.
There, we see that many more unknown test objects
are identified when training sets are larger, but a
larger training set does not translate into universally
higher results.
One possible explanation for this lies in a prop-
erty of Tax and Duin’s approach. If a training item t
is situated at distance d from its nearest neighbor in
the training set, then any test item within a radius of
d around t will be considered known. Thus we could
term d the “acceptance radius” of t. Now if t is an
outlier within the training set, then d will be large, as
illustrated in Figure 4. The sparser the training set is,
the more training outliers we are likely to find, with
large acceptance radii that assign a label of known
even to more distanced test items. Thus a sparse
training set could lead to lower recall of unknown
sense assignment and at the same time higher pre-
cision, as the items labeled unknown would be the
ones at great distance from any items on the training
set – conforming to the pattern in Tables 3 and 7.
</bodyText>
<sectionHeader confidence="0.5987065" genericHeader="method">
7 Experiment 3: NN-based outlier
detection with added training data
</sectionHeader>
<bodyText confidence="0.860464">
While the NN-based outlier detection model we
used in the previous experiment showed better re-
</bodyText>
<table confidence="0.995772142857143">
Target lemma: put
Senses: ENCODING, PLACING
Sense currently treated as unknown: PLACING
Extend training set by: all annotated sentences for
lemmas other than put in the sense ENCODING:
couch.v, expression.n, formulate.v, formulation.n,
frame.v, phrase.v, word.v, wording.n
</table>
<tableCaption confidence="0.997502">
Table 4: Extending training sets: an example
</tableCaption>
<table confidence="0.999917571428571">
Features Precision Recall
All 0.7709 (v 0.001) 0.7243 (v 0.0018)
Cx 0.7727 (v 0.0027) 0.8172 (v 0.0035)
Syn 0.8571 (v 0.0045) 0.1694 (v 0.0012)
Syn-hw 0.8025 (v 0.0041) 0.3383 (v 0.0025)
Syn 0.8587 (v 0.0081) 0.1748 (v 0.0015)
Syn-hw 0.8055 (v 0.0056) 0.3516 (v 0.0015)
</table>
<tableCaption confidence="0.843824666666667">
Table 5: Experiment 3: Results for label unknown
sense, NN-based outlier detection, 0 = 1.0. Q: stan-
dard deviation
</tableCaption>
<bodyText confidence="0.999941541666667">
sults than the WSD confidence model, its recall is
still low. We have suggested that data sparseness
may be responsible for the low performance. Con-
sequently, we repeat the experiment of the previous
section with more, but less specific, training data.
Like WordNet synsets, FrameNet frames are se-
mantic classes that typically comprise several lem-
mas or expressions. So, assuming that words with
similar meaning occur in similar contexts, the con-
text features for lemmas in the same frame should
be similar. Following this idea, we supplement the
training data for a lemma by all the other annotated
data for the senses that are present in the training
set, where by “other data” we mean data with other
target lemmas. Table 4 shows an example4.
Modeling. Again, we use Tax and Duin’s outlier
detection approach for unknown sense detection.
The experimental design and evaluation are the same
as in Experiment 2, the only difference being the
training set extension. Training set extension raises
the average training set size from 22.5 to 374.
Results. Table 5 shows precision and recall for la-
beling instances as unknown, with a distance quo-
tient threshold of 1.0, averaged over unknown senses
</bodyText>
<footnote confidence="0.985213">
4Conditions Syn and Syn-hw were also tested using only
other target lemmas with the same part of speech. Results were
virtually unchanged.
</footnote>
<page confidence="0.992925">
133
</page>
<table confidence="0.99869475">
Features all Precision &gt; 200 all Recall &gt; 200
&gt; 50 &gt; 50
All 0.77 0.77 0.73 0.72 0.80 0.87
Cx 0.77 0.77 0.73 0.82 0.89 0.94
Syn 0.86 0.85 0.82 0.17 0.16 0.13
Syn-hw 0.80 0.79 0.76 0.38 0.36 0.38
Syn 0.86 0.85 0.82 0.17 0.17 0.14
Syn-hw 0.81 0.80 0.76 0.35 0.37 0.38
</table>
<tableCaption confidence="0.991827">
Table 6: Experiment 3: Results by training set size,
</tableCaption>
<table confidence="0.916157142857143">
0 = 1.0
Number of senses
2 3 4 5
Exp. 2 Prec. 0.78 0.68 0.59 0.55
Rec. 0.21 0.38 0.47 0.59
Exp. 3 Prec. 0.83 0.71 0.63 0.56
Rec. 0.68 0.81 0.89 0.88
</table>
<tableCaption confidence="0.9204885">
Table 7: Experiments 2 and 3: Results by the num-
ber of senses of a lemma, condition All, 0 = 1.0
</tableCaption>
<bodyText confidence="0.999812142857143">
and 5-fold cross-validation. In comparison to Exper-
iment 2, precision has risen slightly, and for condi-
tions All, Cx and Syn-hw, recall has risen steeply;
the maximum recall is achieved by Cx at 0.82.
As before, increasing the distance quotient thresh-
old leads to little change in precision but a sharp
drop in recall. For All vectors, recall is 0.72 for
threshold 1.0, 0.56 for 0 = 1.1, and 0.41 for 0 = 1.2.
Table 6 shows system performance by training set
size. As the average training set in this experiment
is much larger than in Experiment 2, we are now
inspecting sets of minimum size 50 and 200 rather
than 10 and 20. We find the same effect as in Ex-
periment 2, with noticeably higher recall for lemmas
with larger training sets, but slightly lower precision.
Table 7 breaks down system performance by the
degree of ambiguity of a lemma. Here, too, we see
the same effect as in Experiment 2: the more senses
a lemma has, the lower the precision and the higher
the recall of unknown label assignment.
Discussion. In comparison to Experiment 2, Ex-
periment 3 shows a dramatic increase in recall, and
even some increase in precision. Precision and re-
call for conditions All and Cx are good enough for
the system to be usable in practice.
Of the four conditions, the three that involve con-
text words, All, Cx and Syn-hw, show consid-
erably higher recall than Syn. Furthermore, the
two conditions that do not involve syntactic fea-
tures, All and Cx, have markedly higher results
than Syn-hw. This could mean that syntactic fea-
tures are not as helpful as context features in detect-
ing unknown senses; however in Experiment 2 the
performance difference between Syn and the other
conditions was not by far as large as in this experi-
ment. It could also mean that frames are not as uni-
form in their syntactic structure as they are in their
context words. This seems plausible as FrameNet
frames are constructed mostly on semantic grounds,
without recourse to similarity in syntactic structure.
Table 6 points to a sparse data problem, even with
training sets extended by additional items. It also
shows that the more a test condition relies on context
word information, the more it profits from additional
data. So it may be worthwhile to explore methods
for a further alleviation of data sparseness, e.g. by
generalizing over context words.
Table 7 underscores the large influence of train-
ing set uniformity: the more senses a lemma has, the
more likely the model is to classify a test instance as
unknown. This is the case even for extended training
sets. One possible way of addressing this problem
would be to take into account more than a single
nearest neighbor in NN-based outlier detection in
order to compute more precise boundaries between
known and unknown instances.
</bodyText>
<sectionHeader confidence="0.935892" genericHeader="conclusions">
8 Conclusion and outlook
</sectionHeader>
<bodyText confidence="0.9998854375">
We have defined and addressed the problem of
unknown word sense detection: the identification
of corpus occurrences that are not covered by a
given sense inventory, using a training set of sense-
annotated data as a basis. We have modeled this
problem as an instance of outlier detection, using
the simple nearest neighbor-based approach of Tax
and Duin to measure the resemblance of a new oc-
currence to the training data. In combination with
a method that alleviates data sparseness by sharing
training data across lemmas, the approach achieves
good results that make it usable in practice: With
items represented as vectors of context words (in-
cluding lemma, POS and NE), the system achieves
0.77 precision and 0.82 recall in an evaluation on
FrameNet 1.2. The training set extension method,
</bodyText>
<page confidence="0.995913">
134
</page>
<bodyText confidence="0.999953307692308">
which proved crucial to our approach, relies solely
on a grouping of annotated data by semantic simi-
larity. As such, the method is applicable to any re-
source that groups words into semantic classes, for
example WordNet.
For this first study on unknown sense detection,
we have chosen a maximally simple outlier detec-
tion method; many extensions are possible. One ob-
vious possibility is the extension of Tax and Duin’s
method to more than one nearest training neigh-
bor for a more accurate estimate of local density.
Furthermore, more sophisticated feature vectors can
be employed to generalize over context words, and
other outlier detection approaches (Markou and
Singh, 2003a; Markou and Singh, 2003b; Marsland,
2003) can be tested on this task.
Our immediate goal is to use unknown sense de-
tection in combination with WSD, to filter out items
that the WSD system cannot handle due to missing
senses. Once items have been identified as unknown,
they are available for further processing: If possible
one would like to assign some measure of sense in-
formation even to these items. Possibilities include
associating items with similar existing senses (Wid-
dows, 2003; Curran, 2005; Burchardt et al., 2005) or
clustering them into approximate senses.
</bodyText>
<sectionHeader confidence="0.999243" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999899031746032">
C. Baker, C. Fillmore, and J. Lowe. 1998. The Berkeley
FrameNet Project. In Proc. ACL-98, Montreal.
A. Burchardt, K. Erk, and A. Frank. 2005. A WordNet
detour to FrameNet. In Proc. GLDV 2005 Workshop
GermaNet II, Bonn.
U. Callmeier, A. Eisele, U. Sch¨afer, and M. Siegel. 2004.
The DeepThought core architecture framework. In
Proc. LREC-04, Lisbon.
James Curran. 2005. Supersense tagging of unknown
nouns using semantic similarity. In Proc. ACL-05,
Ann Arbor.
D. Dasgupta and S. Forrest. 1999. Novelty detection
in time series data using ideas from immunology. In
Proc. International Conference on Intelligent Systems.
Katrin Erk and Sebastian Pado. 2006. Shalmaneser -
a toolchain for shallow semantic parsing. In Proc.
LREC-06, Genoa.
K. Erk. 2005. Frame assignment as word sense disam-
biguation. In Proc. IWCS 2005, Tilburg.
C. Fillmore. 1982. Frame Semantics. Linguistics in the
Morning Calm.
R. Florian, S. Cucerzan, C. Schafer, and D. Yarowsky.
2002. Combining classifiers for word sense disam-
biguation. Journal of Natural Language Engineering,
8(4):327–431.
S. Hickinbotham and J. Austin. 2000. Neural networks
for novelty detection in airframe strain data. In Proc.
International Joint Conference on Neural Networks.
D. Lin. 1993. Principle-based parsing without overgen-
eration. In Proc. ACL-93, Columbus, OH.
M. Markou and S. Singh. 2003a. Novelty detection:
A review. part 1: Statistical approaches. ACM Signal
Processing, 83(12):2481 – 2497.
M. Markou and S. Singh. 2003b. Novelty detection:
A review. part 2: Neural network based approaches.
ACMSignal Processing, 83(12):2499 – 2521.
S. Marsland. 2003. Novelty detection in learning sys-
tems. Neural computing surveys, 3:157–195.
D. Mount and S. Arya. 2005. ANN: A library for approx-
imate nearest neighbor searching. Download from
http://www.cs.umd.edu/˜mount/ANN/.
B. Sch¨olkopf, R. Williamson, A. Smola, J. Shawe-Taylor,
and J. Platt. 2000. Support vector method for novelty
detection. Advances in neural information processing
systems, 12.
H. Sch¨utze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97 – 123.
M. Tatu and D. Moldovan. 2005. A semantic approach to
recognizing textual entailment. In Proc. HLT/EMNLP
2005, Vancouver.
D. Tax and R. Duin. 1998. Outlier detection using clas-
sifier instability. In Advances in Pattern Recognition:
the Joint IAPR International Workshops.
D. Tax and R. Duin. 2000. Data description in sub-
spaces. In International Conference on Pattern recog-
nition, volume 2, Barcelona.
Dominic Widdows. 2003. Unsupervised methods for de-
veloping taxonomies by combining syntactic and sta-
tistical information. In Proc. HLT/NAACL-03, Ed-
monton.
D. Yeung and C. Chow. 2002. Parzen-window network
intrusion detectors. In Proc. International Conference
on Pattern Recognition.
</reference>
<page confidence="0.998784">
135
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.178341">
<title confidence="0.979856">Unknown word sense detection as outlier detection</title>
<author confidence="0.782849">Katrin</author>
<affiliation confidence="0.663794">Computational Saarland</affiliation>
<address confidence="0.363314">Saarbr¨ucken,</address>
<email confidence="0.996402">erk@coli.uni-sb.de</email>
<abstract confidence="0.987119692307692">address the problem of word the identification of corpus occurrences that are not covered by a given sense inventory. We model this an instance of using a simple nearest neighbor-based approach to measuring the resemblance of a new item to a training set. In combination with a method that alleviates data sparseness by sharing training data across lemmas, the approach achieves a precision of 0.77 and recall of 0.82.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Baker</author>
<author>C Fillmore</author>
<author>J Lowe</author>
</authors>
<title>The Berkeley FrameNet Project.</title>
<date>1998</date>
<booktitle>In Proc. ACL-98,</booktitle>
<location>Montreal.</location>
<contexts>
<context position="3002" citStr="Baker et al., 1998" startWordPosition="485" endWordPosition="488">hallow semantic parsing, by which we mean a combination of WSD and the automatic assignment of 128 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 128–135, New York, June 2006. c�2006 Association for Computational Linguistics semantic roles to free text. In cases where a sense is missing from the inventory, WSD will wrongly assign one of the existing senses. Figure 1 shows an example, a sentence from the Hound of the Baskervilles, analyzed by the SHALMANESER (Erk and Pado, 2006) shallow semantic parser. The analysis is based on FrameNet (Baker et al., 1998), a resource that lists senses and semantic roles for English expressions. FrameNet is lacking a sense of “expectation” or “being mentally prepared” for the verb prepare, so prepared has been assigned the sense COOKING CREATION, a possible but improbable analysis2. Such erroneous labels can be fatal when further processing builds on the results of shallow semantic parsing, e.g. for drawing inferences. Unknown sense detection can prevent such mistakes. All sense inventories face the problem of missing senses, either because of their small overall size (as is the case for some non-English WordNe</context>
<context position="5728" citStr="Baker et al., 1998" startWordPosition="931" endWordPosition="934"> with better results than in the first experiment but still low recall. Section 7 repeats the experiment of section 6 with added training data, making use of the fact that one semantic class in FrameNet typically pertains to several lemmas and achieving a marked improvement in results. 2 FrameNet Frame Semantics (Fillmore, 1982) models the meanings of a word or expression by reference to frames which describe the background and situational knowledge necessary for understanding what the predicate is “about”. Each frame provides its specific set of semantic roles. The Berkeley FrameNet project (Baker et al., 1998) is building a semantic lexicon for English describing the frames and linking them to the words and expressions that can evoke them. These can be verbs as well as nouns, adjectives, prepositions, adverbs, and multiword expressions. Frames are linked by IS-A and other relations. Currently, FrameNet contains 609 frames with 8,755 lemmaframe pairs, of which 5,308 are exemplified in annotated sentences from the British National Corpus. The annotation comprises 133,846 sentences. As FrameNet is a growing resource, many lemmas are still lacking senses, and many senses are still lacking annotation. T</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>C. Baker, C. Fillmore, and J. Lowe. 1998. The Berkeley FrameNet Project. In Proc. ACL-98, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Burchardt</author>
<author>K Erk</author>
<author>A Frank</author>
</authors>
<title>A WordNet detour to FrameNet. In</title>
<date>2005</date>
<booktitle>Proc. GLDV 2005 Workshop GermaNet II,</booktitle>
<location>Bonn.</location>
<contexts>
<context position="4401" citStr="Burchardt et al., 2005" startWordPosition="710" endWordPosition="713">ose is applicable to any sense inventory that has annotated data; in particular, it is also applicable to WordNet. In this paper we model unknown sense detection as outlier detection, using a simple Nearest Neighbor-based method (Tax and Duin, 2000) that compares the local probability density at each test item with that of its nearest training item. To our knowledge, there exists no other approach to date to the problem of detecting unknown senses. There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection. Plan of the paper. After a brief sketch of FrameNet in Section 2, we describe the experimental setup used throughout this paper in Section 3. Section 4 tests whether a very simple model suffices for detecting unknown senses: a threshold on confidence scores returned by the SHALMANESER WSD 2Unfortunately, the semantic roles have been mis-assigned by the system. The word I should fill the FOOD role, while for a hound could be assigned the optional RECEIVER role. system. The result is that recall is much too low. Sectio</context>
</contexts>
<marker>Burchardt, Erk, Frank, 2005</marker>
<rawString>A. Burchardt, K. Erk, and A. Frank. 2005. A WordNet detour to FrameNet. In Proc. GLDV 2005 Workshop GermaNet II, Bonn.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Callmeier</author>
<author>A Eisele</author>
<author>U Sch¨afer</author>
<author>M Siegel</author>
</authors>
<title>The DeepThought core architecture framework.</title>
<date>2004</date>
<booktitle>In Proc. LREC-04,</booktitle>
<location>Lisbon.</location>
<marker>Callmeier, Eisele, Sch¨afer, Siegel, 2004</marker>
<rawString>U. Callmeier, A. Eisele, U. Sch¨afer, and M. Siegel. 2004. The DeepThought core architecture framework. In Proc. LREC-04, Lisbon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Curran</author>
</authors>
<title>Supersense tagging of unknown nouns using semantic similarity.</title>
<date>2005</date>
<booktitle>In Proc. ACL-05,</booktitle>
<location>Ann Arbor.</location>
<contexts>
<context position="4376" citStr="Curran, 2005" startWordPosition="708" endWordPosition="709">method we propose is applicable to any sense inventory that has annotated data; in particular, it is also applicable to WordNet. In this paper we model unknown sense detection as outlier detection, using a simple Nearest Neighbor-based method (Tax and Duin, 2000) that compares the local probability density at each test item with that of its nearest training item. To our knowledge, there exists no other approach to date to the problem of detecting unknown senses. There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection. Plan of the paper. After a brief sketch of FrameNet in Section 2, we describe the experimental setup used throughout this paper in Section 3. Section 4 tests whether a very simple model suffices for detecting unknown senses: a threshold on confidence scores returned by the SHALMANESER WSD 2Unfortunately, the semantic roles have been mis-assigned by the system. The word I should fill the FOOD role, while for a hound could be assigned the optional RECEIVER role. system. The result is that recal</context>
</contexts>
<marker>Curran, 2005</marker>
<rawString>James Curran. 2005. Supersense tagging of unknown nouns using semantic similarity. In Proc. ACL-05, Ann Arbor.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Dasgupta</author>
<author>S Forrest</author>
</authors>
<title>Novelty detection in time series data using ideas from immunology.</title>
<date>1999</date>
<booktitle>In Proc. International Conference on Intelligent Systems.</booktitle>
<contexts>
<context position="13731" citStr="Dasgupta and Forrest, 1999" startWordPosition="2263" endWordPosition="2266">ors to detect outliers. In general, the task of outlier detection is to decide whether a new object belongs to a given training set or not. Typically, outlier detection approaches derive some boundary around the training set, or they derive from the set some model of “normality” to which new objects are compared (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003). Applications of outlier detection include fault detection (Hickinbotham and Austin, 2000), hand writing deciphering (Tax and Duin, 1998; Sch¨olkopf et al., 2000), and network intrusion detection (Yeung and Chow, 2002; Dasgupta and Forrest, 1999). One standard approach to outlier detection estimates the probability density of the training set, such that a test object can be classified as an outlier or non-outlier according to its probability of belonging to the set. Rather than estimating the complete density function, Tax and Duin (2000) approximate local density at the test object by comparing distances between nearest neighbors. Given a test object x, the approach considers the training object t nearest to x and compares the distance dxt between x and t to the distance dtt, between t and its own nearest training data neighbor t&apos;. T</context>
</contexts>
<marker>Dasgupta, Forrest, 1999</marker>
<rawString>D. Dasgupta and S. Forrest. 1999. Novelty detection in time series data using ideas from immunology. In Proc. International Conference on Intelligent Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pado</author>
</authors>
<title>Shalmaneser -a toolchain for shallow semantic parsing.</title>
<date>2006</date>
<booktitle>In Proc. LREC-06,</booktitle>
<location>Genoa.</location>
<contexts>
<context position="2922" citStr="Erk and Pado, 2006" startWordPosition="471" endWordPosition="474"> respects the given word senses. The main motivation for this study comes from shallow semantic parsing, by which we mean a combination of WSD and the automatic assignment of 128 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 128–135, New York, June 2006. c�2006 Association for Computational Linguistics semantic roles to free text. In cases where a sense is missing from the inventory, WSD will wrongly assign one of the existing senses. Figure 1 shows an example, a sentence from the Hound of the Baskervilles, analyzed by the SHALMANESER (Erk and Pado, 2006) shallow semantic parser. The analysis is based on FrameNet (Baker et al., 1998), a resource that lists senses and semantic roles for English expressions. FrameNet is lacking a sense of “expectation” or “being mentally prepared” for the verb prepare, so prepared has been assigned the sense COOKING CREATION, a possible but improbable analysis2. Such erroneous labels can be fatal when further processing builds on the results of shallow semantic parsing, e.g. for drawing inferences. Unknown sense detection can prevent such mistakes. All sense inventories face the problem of missing senses, either</context>
</contexts>
<marker>Erk, Pado, 2006</marker>
<rawString>Katrin Erk and Sebastian Pado. 2006. Shalmaneser -a toolchain for shallow semantic parsing. In Proc. LREC-06, Genoa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Erk</author>
</authors>
<title>Frame assignment as word sense disambiguation.</title>
<date>2005</date>
<booktitle>In Proc. IWCS 2005,</booktitle>
<location>Tilburg. C. Fillmore.</location>
<contexts>
<context position="9761" citStr="Erk, 2005" startWordPosition="1609" endWordPosition="1610">he She her and upwards (1): subj, obj, mod (since s and subj corefer, we use only one of them) (2): she, hand, outwards (3): subj-she, obj-hand,mod-outwards (4): mod-obj-subj Figure 2: Sample Minipar parse and extracted grammatical function features senses, we need lemmas with at least three senses to evaluate it: One of the senses of each lemma is treated as unknown, which for lemmas with three or more senses leaves at least two senses for the training set. This reduces our data set to 125 lemmas with 7,435 annotated sentences. Modeling. We test whether the WSD system built into SHALMANESER (Erk, 2005) can distinguish known sense items from unknown sense items reliably by its confidence scores. The system extracts a rich feature set, which forms the basis of all three experiments in this paper: • a bag-of-words context, with a window size of one sentence; • bi- and trigrams centered on the target word; • grammatical function information: for each dependent of the target, (1) its function label, (2) its headword, and (3) a combination of both are used as features. (4) The concatenation of all function labels constitutes another feature. For PPs, function labels are extended by the prepositio</context>
</contexts>
<marker>Erk, 2005</marker>
<rawString>K. Erk. 2005. Frame assignment as word sense disambiguation. In Proc. IWCS 2005, Tilburg. C. Fillmore. 1982. Frame Semantics. Linguistics in the Morning Calm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Florian</author>
<author>S Cucerzan</author>
<author>C Schafer</author>
<author>D Yarowsky</author>
</authors>
<title>Combining classifiers for word sense disambiguation.</title>
<date>2002</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>8</volume>
<issue>4</issue>
<contexts>
<context position="10535" citStr="Florian et al. (2002)" startWordPosition="1739" endWordPosition="1742">sis of all three experiments in this paper: • a bag-of-words context, with a window size of one sentence; • bi- and trigrams centered on the target word; • grammatical function information: for each dependent of the target, (1) its function label, (2) its headword, and (3) a combination of both are used as features. (4) The concatenation of all function labels constitutes another feature. For PPs, function labels are extended by the preposition. As an example, Figure 2 shows a BNC sentence and its grammatical function features. • for verb targets, the target voice. The feature set is based on Florian et al. (2002) but contains additional syntax-related features. Each word-related feature is represented as four features for word, lemma, part of speech, and named entity. SHALMANESER trains one Naive Bayes classifier per lemma to be disambiguated. For this experiment, hand outwards gen punc conj 130 0 Precision Recall 0.5 0.6524 (v 0.115) 0.0011 (v 0.0004) 0.75 0.7855 (v 0.0086) 0.0527 (v 0.0013) 0.9 0.7855 (v 0.0093) 0.1006 (v 0.0021) 0.98 0.7847 (v 0.0073) 0.1744 (v 0.0025) Table 1: Experiment 1: Results for label unknown sense, WSD confidence level approach. 0: confidence threshold. Q: std. dev. Figure</context>
</contexts>
<marker>Florian, Cucerzan, Schafer, Yarowsky, 2002</marker>
<rawString>R. Florian, S. Cucerzan, C. Schafer, and D. Yarowsky. 2002. Combining classifiers for word sense disambiguation. Journal of Natural Language Engineering, 8(4):327–431.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hickinbotham</author>
<author>J Austin</author>
</authors>
<title>Neural networks for novelty detection in airframe strain data. In</title>
<date>2000</date>
<booktitle>Proc. International Joint Conference on Neural Networks.</booktitle>
<contexts>
<context position="13575" citStr="Hickinbotham and Austin, 2000" startWordPosition="2239" endWordPosition="2242">c. it is only applicable to lemmas with more than one known sense. In this section we introduce an alternative approach, which uses distances to nearest neighbors to detect outliers. In general, the task of outlier detection is to decide whether a new object belongs to a given training set or not. Typically, outlier detection approaches derive some boundary around the training set, or they derive from the set some model of “normality” to which new objects are compared (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003). Applications of outlier detection include fault detection (Hickinbotham and Austin, 2000), hand writing deciphering (Tax and Duin, 1998; Sch¨olkopf et al., 2000), and network intrusion detection (Yeung and Chow, 2002; Dasgupta and Forrest, 1999). One standard approach to outlier detection estimates the probability density of the training set, such that a test object can be classified as an outlier or non-outlier according to its probability of belonging to the set. Rather than estimating the complete density function, Tax and Duin (2000) approximate local density at the test object by comparing distances between nearest neighbors. Given a test object x, the approach considers the </context>
</contexts>
<marker>Hickinbotham, Austin, 2000</marker>
<rawString>S. Hickinbotham and J. Austin. 2000. Neural networks for novelty detection in airframe strain data. In Proc. International Joint Conference on Neural Networks.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Principle-based parsing without overgeneration.</title>
<date>1993</date>
<booktitle>In Proc. ACL-93,</booktitle>
<location>Columbus, OH.</location>
<contexts>
<context position="8440" citStr="Lin, 1993" startWordPosition="1384" endWordPosition="1385">l setup, the unknown sense occurrences of each lemma all belong to the same sense. However, this does not bias the experiment since none of the models we study take advantage of the shape of the test set in any way. Rather, each test item is classified individually, without recourse to the other test items. Data. All experiments in this paper were performed on the FrameNet 1.2 annotated data pertaining to ambiguous lemmas. After removal of instances that were annotated with more than one sense, we obtain 26,496 annotated sentences for the 1,031 ambiguous lemmas. They were parsed with Minipar (Lin, 1993); named entities were computed using Heart of Gold (Callmeier et al., 2004). 4 Experiment 1: WSD confidence scores for unknown sense detection In this section we test a very simple model of unknown sense detection: Classifiers often return a confidence score along with the assigned label. We will try to detect unknown senses by a threshold on confidence scores, declaring anything below the threshold as unknown. Note that this method can only be applied to lemmas that have more than one sense, since for single-sense lemmas the system will always return the maximum confidence score. Data. While </context>
</contexts>
<marker>Lin, 1993</marker>
<rawString>D. Lin. 1993. Principle-based parsing without overgeneration. In Proc. ACL-93, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Markou</author>
<author>S Singh</author>
</authors>
<title>Novelty detection: A review. part 1: Statistical approaches.</title>
<date>2003</date>
<journal>ACM Signal Processing,</journal>
<volume>83</volume>
<issue>12</issue>
<pages>2497</pages>
<contexts>
<context position="834" citStr="Markou and Singh, 2003" startWordPosition="123" endWordPosition="126"> the identification of corpus occurrences that are not covered by a given sense inventory. We model this as an instance of outlier detection, using a simple nearest neighbor-based approach to measuring the resemblance of a new item to a training set. In combination with a method that alleviates data sparseness by sharing training data across lemmas, the approach achieves a precision of 0.77 and recall of 0.82. 1 Introduction If a system has seen only positive examples, how does it recognize a negative example? This is the problem addressed by outlier detection, also called novelty detection1 (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003): to detect novel or unknown items that differ from all the seen training data. Outlier detection approaches typically derive some model of “normal” objects from the training set and use a distance measure and a threshold to detect abnormal items. In this paper, we apply outlier detection techniques to the task of unknown sense detection: the identification of corpus occurrences that are not covered by a given sense inventory. The training set 1The term novelty detection is also used for the distinction of novel and repeated information in information</context>
<context position="13441" citStr="Markou and Singh, 2003" startWordPosition="2220" endWordPosition="2223"> that 3Note that the minimum confidence score is 0.5 if 2 senses are present in the training set, 0.33 for 3 present senses etc. it is only applicable to lemmas with more than one known sense. In this section we introduce an alternative approach, which uses distances to nearest neighbors to detect outliers. In general, the task of outlier detection is to decide whether a new object belongs to a given training set or not. Typically, outlier detection approaches derive some boundary around the training set, or they derive from the set some model of “normality” to which new objects are compared (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003). Applications of outlier detection include fault detection (Hickinbotham and Austin, 2000), hand writing deciphering (Tax and Duin, 1998; Sch¨olkopf et al., 2000), and network intrusion detection (Yeung and Chow, 2002; Dasgupta and Forrest, 1999). One standard approach to outlier detection estimates the probability density of the training set, such that a test object can be classified as an outlier or non-outlier according to its probability of belonging to the set. Rather than estimating the complete density function, Tax and Duin (2000) approximate</context>
</contexts>
<marker>Markou, Singh, 2003</marker>
<rawString>M. Markou and S. Singh. 2003a. Novelty detection: A review. part 1: Statistical approaches. ACM Signal Processing, 83(12):2481 – 2497.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Markou</author>
<author>S Singh</author>
</authors>
<title>Novelty detection: A review. part 2: Neural network based approaches.</title>
<date>2003</date>
<booktitle>ACMSignal Processing,</booktitle>
<volume>83</volume>
<issue>12</issue>
<pages>2521</pages>
<contexts>
<context position="834" citStr="Markou and Singh, 2003" startWordPosition="123" endWordPosition="126"> the identification of corpus occurrences that are not covered by a given sense inventory. We model this as an instance of outlier detection, using a simple nearest neighbor-based approach to measuring the resemblance of a new item to a training set. In combination with a method that alleviates data sparseness by sharing training data across lemmas, the approach achieves a precision of 0.77 and recall of 0.82. 1 Introduction If a system has seen only positive examples, how does it recognize a negative example? This is the problem addressed by outlier detection, also called novelty detection1 (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003): to detect novel or unknown items that differ from all the seen training data. Outlier detection approaches typically derive some model of “normal” objects from the training set and use a distance measure and a threshold to detect abnormal items. In this paper, we apply outlier detection techniques to the task of unknown sense detection: the identification of corpus occurrences that are not covered by a given sense inventory. The training set 1The term novelty detection is also used for the distinction of novel and repeated information in information</context>
<context position="13441" citStr="Markou and Singh, 2003" startWordPosition="2220" endWordPosition="2223"> that 3Note that the minimum confidence score is 0.5 if 2 senses are present in the training set, 0.33 for 3 present senses etc. it is only applicable to lemmas with more than one known sense. In this section we introduce an alternative approach, which uses distances to nearest neighbors to detect outliers. In general, the task of outlier detection is to decide whether a new object belongs to a given training set or not. Typically, outlier detection approaches derive some boundary around the training set, or they derive from the set some model of “normality” to which new objects are compared (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003). Applications of outlier detection include fault detection (Hickinbotham and Austin, 2000), hand writing deciphering (Tax and Duin, 1998; Sch¨olkopf et al., 2000), and network intrusion detection (Yeung and Chow, 2002; Dasgupta and Forrest, 1999). One standard approach to outlier detection estimates the probability density of the training set, such that a test object can be classified as an outlier or non-outlier according to its probability of belonging to the set. Rather than estimating the complete density function, Tax and Duin (2000) approximate</context>
</contexts>
<marker>Markou, Singh, 2003</marker>
<rawString>M. Markou and S. Singh. 2003b. Novelty detection: A review. part 2: Neural network based approaches. ACMSignal Processing, 83(12):2499 – 2521.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Marsland</author>
</authors>
<title>Novelty detection in learning systems. Neural computing surveys,</title>
<date>2003</date>
<pages>3--157</pages>
<contexts>
<context position="877" citStr="Marsland, 2003" startWordPosition="131" endWordPosition="132"> not covered by a given sense inventory. We model this as an instance of outlier detection, using a simple nearest neighbor-based approach to measuring the resemblance of a new item to a training set. In combination with a method that alleviates data sparseness by sharing training data across lemmas, the approach achieves a precision of 0.77 and recall of 0.82. 1 Introduction If a system has seen only positive examples, how does it recognize a negative example? This is the problem addressed by outlier detection, also called novelty detection1 (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003): to detect novel or unknown items that differ from all the seen training data. Outlier detection approaches typically derive some model of “normal” objects from the training set and use a distance measure and a threshold to detect abnormal items. In this paper, we apply outlier detection techniques to the task of unknown sense detection: the identification of corpus occurrences that are not covered by a given sense inventory. The training set 1The term novelty detection is also used for the distinction of novel and repeated information in information retrieval, a different if related topic. F</context>
<context position="13484" citStr="Marsland, 2003" startWordPosition="2228" endWordPosition="2230">.5 if 2 senses are present in the training set, 0.33 for 3 present senses etc. it is only applicable to lemmas with more than one known sense. In this section we introduce an alternative approach, which uses distances to nearest neighbors to detect outliers. In general, the task of outlier detection is to decide whether a new object belongs to a given training set or not. Typically, outlier detection approaches derive some boundary around the training set, or they derive from the set some model of “normality” to which new objects are compared (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003). Applications of outlier detection include fault detection (Hickinbotham and Austin, 2000), hand writing deciphering (Tax and Duin, 1998; Sch¨olkopf et al., 2000), and network intrusion detection (Yeung and Chow, 2002; Dasgupta and Forrest, 1999). One standard approach to outlier detection estimates the probability density of the training set, such that a test object can be classified as an outlier or non-outlier according to its probability of belonging to the set. Rather than estimating the complete density function, Tax and Duin (2000) approximate local density at the test object by compar</context>
</contexts>
<marker>Marsland, 2003</marker>
<rawString>S. Marsland. 2003. Novelty detection in learning systems. Neural computing surveys, 3:157–195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Mount</author>
<author>S Arya</author>
</authors>
<title>ANN: A library for approximate nearest neighbor searching. Download from http://www.cs.umd.edu/˜mount/ANN/.</title>
<date>2005</date>
<contexts>
<context position="15507" citStr="Mount and Arya, 2005" startWordPosition="2573" endWordPosition="2576">re vectors with entries either 0 or 1, this corresponds to the size of the symmetric difference of the two feature sets. 6 Experiment 2: NN-based outlier detection In this section we use the NN-based outlier detection approach of the previous section for an experiment in unknown sense detection. Experimental setup and data are as described in Section 3. Modeling. We model unknown sense detection as an outlier detection task, using Tax and Duin’s outlier detection approach that we have outlined in the previous section. Nearest neighbors (by Euclidean distance) were computed using the ANN tool (Mount and Arya, 2005). We compute one outlier detection model per lemma. With training and test sets constructed as described in Section 3, the average training set comprises 22.5 sentences. We use the same features as in Section 4, with feature vector entries of 1 for present and 0 for absent features. For a more detailed analysis of the contribution of different feature types, we test on reduced as well as full feature vectors: All: full feature vectors Cx: only bag-of-word context features (words, lemmas, POS, NE) Syn: function labels of dependents Syn-hw: Syn plus headwords of dependents We compare the NN-base</context>
</contexts>
<marker>Mount, Arya, 2005</marker>
<rawString>D. Mount and S. Arya. 2005. ANN: A library for approximate nearest neighbor searching. Download from http://www.cs.umd.edu/˜mount/ANN/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Sch¨olkopf</author>
<author>R Williamson</author>
<author>A Smola</author>
<author>J Shawe-Taylor</author>
<author>J Platt</author>
</authors>
<title>Support vector method for novelty detection. Advances in neural information processing systems,</title>
<date>2000</date>
<pages>12</pages>
<marker>Sch¨olkopf, Williamson, Smola, Shawe-Taylor, Platt, 2000</marker>
<rawString>B. Sch¨olkopf, R. Williamson, A. Smola, J. Shawe-Taylor, and J. Platt. 2000. Support vector method for novelty detection. Advances in neural information processing systems, 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<pages>123</pages>
<marker>Sch¨utze, 1998</marker>
<rawString>H. Sch¨utze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97 – 123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tatu</author>
<author>D Moldovan</author>
</authors>
<title>A semantic approach to recognizing textual entailment.</title>
<date>2005</date>
<booktitle>In Proc. HLT/EMNLP 2005,</booktitle>
<location>Vancouver.</location>
<contexts>
<context position="6452" citStr="Tatu and Moldovan (2005)" startWordPosition="1049" endWordPosition="1052">expressions that can evoke them. These can be verbs as well as nouns, adjectives, prepositions, adverbs, and multiword expressions. Frames are linked by IS-A and other relations. Currently, FrameNet contains 609 frames with 8,755 lemmaframe pairs, of which 5,308 are exemplified in annotated sentences from the British National Corpus. The annotation comprises 133,846 sentences. As FrameNet is a growing resource, many lemmas are still lacking senses, and many senses are still lacking annotation. This is problematic for the use of FrameNet analyses as a basis for inferences over text, as e.g. in Tatu and Moldovan (2005). For example, the verb prepare from Figure 1 is associated with the frames COOKING CREATION: prepare food ACTIVITY PREPARE: get ready for an activity ACTIVITY READY STATE: be ready for an activity WILLINGNESS: be willing of which only the COOKING CREATION sense has been annotated. The sense in Figure 1 is not covered yet: ACTIVITY READY STATE would be more appropriate than COOKING CREATION, but still not optimal, since the sentence refers to a mental state rather than the preparation of an activity. 129 3 Experimental setup and data Experimental setup. To evaluate an unknown sense detection s</context>
</contexts>
<marker>Tatu, Moldovan, 2005</marker>
<rawString>M. Tatu and D. Moldovan. 2005. A semantic approach to recognizing textual entailment. In Proc. HLT/EMNLP 2005, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Tax</author>
<author>R Duin</author>
</authors>
<title>Outlier detection using classifier instability.</title>
<date>1998</date>
<booktitle>In Advances in Pattern Recognition: the Joint IAPR International Workshops.</booktitle>
<contexts>
<context position="13621" citStr="Tax and Duin, 1998" startWordPosition="2246" endWordPosition="2249">n sense. In this section we introduce an alternative approach, which uses distances to nearest neighbors to detect outliers. In general, the task of outlier detection is to decide whether a new object belongs to a given training set or not. Typically, outlier detection approaches derive some boundary around the training set, or they derive from the set some model of “normality” to which new objects are compared (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003). Applications of outlier detection include fault detection (Hickinbotham and Austin, 2000), hand writing deciphering (Tax and Duin, 1998; Sch¨olkopf et al., 2000), and network intrusion detection (Yeung and Chow, 2002; Dasgupta and Forrest, 1999). One standard approach to outlier detection estimates the probability density of the training set, such that a test object can be classified as an outlier or non-outlier according to its probability of belonging to the set. Rather than estimating the complete density function, Tax and Duin (2000) approximate local density at the test object by comparing distances between nearest neighbors. Given a test object x, the approach considers the training object t nearest to x and compares th</context>
</contexts>
<marker>Tax, Duin, 1998</marker>
<rawString>D. Tax and R. Duin. 1998. Outlier detection using classifier instability. In Advances in Pattern Recognition: the Joint IAPR International Workshops.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Tax</author>
<author>R Duin</author>
</authors>
<title>Data description in subspaces.</title>
<date>2000</date>
<booktitle>In International Conference on Pattern recognition,</booktitle>
<volume>2</volume>
<location>Barcelona.</location>
<contexts>
<context position="4027" citStr="Tax and Duin, 2000" startWordPosition="650" endWordPosition="653">nknown sense detection can prevent such mistakes. All sense inventories face the problem of missing senses, either because of their small overall size (as is the case for some non-English WordNets) or when they encounter domain-specific senses. Our study will be evaluated on FrameNet because of our main aim of improving shallow semantic parsing, but the method we propose is applicable to any sense inventory that has annotated data; in particular, it is also applicable to WordNet. In this paper we model unknown sense detection as outlier detection, using a simple Nearest Neighbor-based method (Tax and Duin, 2000) that compares the local probability density at each test item with that of its nearest training item. To our knowledge, there exists no other approach to date to the problem of detecting unknown senses. There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection. Plan of the paper. After a brief sketch of FrameNet in Section 2, we describe the experimental setup used throughout this paper in Section 3. Sectio</context>
<context position="14029" citStr="Tax and Duin (2000)" startWordPosition="2313" endWordPosition="2316">ompared (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003). Applications of outlier detection include fault detection (Hickinbotham and Austin, 2000), hand writing deciphering (Tax and Duin, 1998; Sch¨olkopf et al., 2000), and network intrusion detection (Yeung and Chow, 2002; Dasgupta and Forrest, 1999). One standard approach to outlier detection estimates the probability density of the training set, such that a test object can be classified as an outlier or non-outlier according to its probability of belonging to the set. Rather than estimating the complete density function, Tax and Duin (2000) approximate local density at the test object by comparing distances between nearest neighbors. Given a test object x, the approach considers the training object t nearest to x and compares the distance dxt between x and t to the distance dtt, between t and its own nearest training data neighbor t&apos;. Then the quotient between the distances is used as an indicator of the (ab-)normality of the test object x: dxt pNN�x� _ When the distance dxt is much larger than dtt,, x is considered an outlier. Figure 3 illustrates the idea. The normality or abnormality of test objects is decided by a fixed thre</context>
</contexts>
<marker>Tax, Duin, 2000</marker>
<rawString>D. Tax and R. Duin. 2000. Data description in subspaces. In International Conference on Pattern recognition, volume 2, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
</authors>
<title>Unsupervised methods for developing taxonomies by combining syntactic and statistical information.</title>
<date>2003</date>
<booktitle>In Proc. HLT/NAACL-03,</booktitle>
<location>Edmonton.</location>
<contexts>
<context position="4362" citStr="Widdows, 2003" startWordPosition="706" endWordPosition="707">rsing, but the method we propose is applicable to any sense inventory that has annotated data; in particular, it is also applicable to WordNet. In this paper we model unknown sense detection as outlier detection, using a simple Nearest Neighbor-based method (Tax and Duin, 2000) that compares the local probability density at each test item with that of its nearest training item. To our knowledge, there exists no other approach to date to the problem of detecting unknown senses. There are, however, approaches to the complementary problem of determining the closest known sense for unknown words (Widdows, 2003; Curran, 2005; Burchardt et al., 2005), which can be viewed as the logical next step after unknown sense detection. Plan of the paper. After a brief sketch of FrameNet in Section 2, we describe the experimental setup used throughout this paper in Section 3. Section 4 tests whether a very simple model suffices for detecting unknown senses: a threshold on confidence scores returned by the SHALMANESER WSD 2Unfortunately, the semantic roles have been mis-assigned by the system. The word I should fill the FOOD role, while for a hound could be assigned the optional RECEIVER role. system. The result</context>
</contexts>
<marker>Widdows, 2003</marker>
<rawString>Dominic Widdows. 2003. Unsupervised methods for developing taxonomies by combining syntactic and statistical information. In Proc. HLT/NAACL-03, Edmonton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yeung</author>
<author>C Chow</author>
</authors>
<title>Parzen-window network intrusion detectors.</title>
<date>2002</date>
<booktitle>In Proc. International Conference on Pattern Recognition.</booktitle>
<contexts>
<context position="13702" citStr="Yeung and Chow, 2002" startWordPosition="2259" endWordPosition="2262">nces to nearest neighbors to detect outliers. In general, the task of outlier detection is to decide whether a new object belongs to a given training set or not. Typically, outlier detection approaches derive some boundary around the training set, or they derive from the set some model of “normality” to which new objects are compared (Markou and Singh, 2003a; Markou and Singh, 2003b; Marsland, 2003). Applications of outlier detection include fault detection (Hickinbotham and Austin, 2000), hand writing deciphering (Tax and Duin, 1998; Sch¨olkopf et al., 2000), and network intrusion detection (Yeung and Chow, 2002; Dasgupta and Forrest, 1999). One standard approach to outlier detection estimates the probability density of the training set, such that a test object can be classified as an outlier or non-outlier according to its probability of belonging to the set. Rather than estimating the complete density function, Tax and Duin (2000) approximate local density at the test object by comparing distances between nearest neighbors. Given a test object x, the approach considers the training object t nearest to x and compares the distance dxt between x and t to the distance dtt, between t and its own nearest</context>
</contexts>
<marker>Yeung, Chow, 2002</marker>
<rawString>D. Yeung and C. Chow. 2002. Parzen-window network intrusion detectors. In Proc. International Conference on Pattern Recognition.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>