<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000007">
<note confidence="0.987088333333333">
Proceedings of the Workshop on Effective Tools and Methodologies for Teaching
Natural Language Processing and Computational Linguistics, Philadelphia,
July 2002, pp. 77-84. Association for Computational Linguistics.
</note>
<title confidence="0.983511">
Lexicalized Grammar 101
</title>
<author confidence="0.996794">
Matthew Stone
</author>
<affiliation confidence="0.883317333333333">
Department of Computer Science and Center for Cognitive Science
Rutgers, the State University of New Jersey
Piscataway NJ 08854-8019 USA
</affiliation>
<email confidence="0.8610815">
http://www.cs.rutgers.edu/˜mdstone
mdstone@cs.rutgers.edu
</email>
<sectionHeader confidence="0.995585" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999295">
This paper presents a simple and ver-
satile tree-rewriting lexicalized grammar
formalism, TAGLET, that provides an ef-
fective scaffold for introducing advanced
topics in a survey course on natural lan-
guage processing (NLP). Students who
implement a strong competence TAGLET
parser and generator simultaneously get
experience with central computer science
ideas and develop an effective starting
point for their own subsequent projects in
data-intensive and interactive NLP.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99997685106383">
This paper is particularly addressed to readers at in-
stitutions whose resources and organization rule out
extensive formal course-work in natural language
processing (NLP). This is typical at universities in
North America. In such places, NLP teaching must
be ambitious but focused; courses must quickly ac-
quaint a broad range of students to the essential
concepts of the field and sell them on its current
research opportunities and challenges. This paper
presents one resource that may help. Specifically,
I outline a simple and versatile lexicalized formal-
ism for natural language syntax, semantics and prag-
matics, called TAGLET, and draw on my experi-
ence with CS 533 (NLP) at Rutgers to motivate the
potential role for TAGLET in a broad NLP class
whose emphasis is to introduce topics of current re-
search. Notes, assignments and implementations for
TAGLET are available on the web.
I begin in Section 2 by describing CS 533—
situating the course within the university and outlin-
ing its topics, audience and goals. I then describe the
specific goals for teaching and implementing gram-
mar formalisms within such a course, in Section 3.
Section 4 gives an informal overview of TAGLET,
and the algorithms, specifications and assignments
that fit TAGLET into a broad general NLP class.
In brief, TAGLET is a context-free tree-rewriting
formalism, defined by the usual complementation
operation and the simplest imaginable modifica-
tion operation. By implementing a strong compe-
tence TAGLET parser and generator students simul-
taneously get experience with central computer sci-
ence ideas—data structures, unification, recursion
and abstraction—and develop an effective starting
point for their own subsequent projects. Two note-
worthy directions are the construction of interac-
tive applications, where TAGLET’s relatively scal-
able and reversible processing lets students easily
explore cutting-edge issues in dialogue semantics
and pragmatics, and the development of linguistic
specifications, where TAGLET’s ability to lexical-
ize tree-bank parses introduces a modern perspec-
tive of linguistic intuitions and annotations as pro-
grams. Section 5 briefly summarizes the advantages
of TAGLET over the many alternative formalisms
that are available; an appendix to the paper provides
more extensive technical details.
</bodyText>
<sectionHeader confidence="0.981176" genericHeader="method">
2 CS 533
</sectionHeader>
<bodyText confidence="0.99994646511628">
NLP at Rutgers is taught as part of the graduate ar-
tificial intelligence (AI) sequence in the computer
science department. As a prerequisite, computer sci-
ence students are expected to be familiar with prob-
abilistic and decision-theoretic modeling (including
statistical classification, hidden Markov models and
Markov decision processes) from the graduate-level
AI foundations class. They might take NLP as a pre-
liminary to research in dialogue systems or in learn-
ing for language and information—or simply to ful-
fill the breadth requirement of MS and PhD degrees.
Students from a number of other departments fre-
quently get involved in natural language research,
however, and are also welcome in 533; on average,
only about half the students in 533 come from com-
puter science. Students from the linguistics depart-
ment frequently undertake computational work as
a way of exploring practical learnability as a con-
straint on universal grammar, or practical reasoning
as a constraint on formal semantics and pragmatics.
The course also attracts students from Rutgers’s li-
brary and information science department, its pri-
mary locus for research in information retrieval and
human-computer interaction. Ambitious undergrad-
uates can also take 533 their senior year; most par-
ticipate in the interdisciplinary cognitive science un-
dergraduate major. 533 is the only computational
course in natural language at Rutgers.
Overall, the course is structured into three mod-
ules, each of which represents about fifteen hours of
in-class lecture time.
The first module gives a general overview of lan-
guage use and dialogue applications. Lectures fol-
low (Clark, 1996), but instill the practical method-
ology for specifying and constructing knowledge-
based systems, in the style of (Brachman et al.,
1990), into the treatment of communication. Con-
currently, students explore precise descriptions of
their intuitions about language and communication
through a series of short homework exercises.
The second module focuses on general techniques
for linguistic representation and implementation, us-
ing TAGLET. With an extended TAGLET project,
conveniently implemented in stages, we use basic
tree operations to introduce Prolog programming,
including data structures, recursion and abstraction
much as outlined in (Sterling and Shapiro, 1994);
then we write a simple chart parser with incremental
interpretation, and a simple communicative-intent
generator scaled down after (Stone et al., 2001).
The third module explores the distinctive prob-
lems of specific applications in NLP, including spo-
ken dialogue systems, information retrieval and text
classification, spelling correction and shallow tag-
ging applications, and machine translation. Jurafsky
and Martin (2000) is our source-book. Concurrently,
students pursue a final project, singly or in cross-
disciplinary teams, involving a more substantial and
potentially innovative implementation.
In its overall structure, the course seems quite
successful. The initial emphasis on clarifying in-
tuitions about communication puts students on an
even footing, as it highlights important ideas about
language use without too much dependence on spe-
cialized training in language or computation. By the
end of the class, students are able to build on the
more specifically computational material to come up
with substantial and interesting final projects. In
Spring 2002 (the first time this version of 533 was
taught), some students looked at utterance interpre-
tation, response generation and graphics generation
in dialogue interaction; explored statistical methods
for word-sense disambiguation, summarization and
generation; and quantified the potential impact of
NLP techniques on information tasks. Many of these
results represented fruitful collaborations between
students from different departments.
Naturally, there is always room for improvement,
and the course is evolving. My presentation of
TAGLET here, for example, represents as much a
project for the next run of 533 as a report of this
year’s materials; in many respects, TAGLET actu-
ally emerged during the semester as a dynamic reac-
tion to the requirements and opportunities of a six-
week module on general techniques for linguistic
representation and implementation.
</bodyText>
<sectionHeader confidence="0.959778" genericHeader="method">
3 Language and Computation in NLP
</sectionHeader>
<bodyText confidence="0.999914438356165">
In a survey course for a broad, research-oriented au-
dience, like CS 533 at Rutgers, a module on linguis-
tic representation must orient itself to central ideas
about computation. 533 may be the first and last
place linguistics or information science students en-
counter concepts of specification, abstraction, com-
plexity and search in class-work. The students who
attack interdisciplinary research with success will be
the ones who internalize and draw on these concepts,
not those who merely hack proficiently. At the same
time, computer scientists also can benefit from an
emphasis on computational fundamentals; it means
that they are building on and reinforcing their ex-
pertise in computation in exploring its application to
language. Nevertheless, NLP is not compiler con-
struction. Programming assignments should always
underline a worthwhile linguistic lesson, not indulge
in implementation for its own sake.
This perspective suggests a number of desiderata
for the grammar formalism for a survey course in
NLP.
Tree rewriting. Students need to master recur-
sive data-structures and programming. NLP directs
our attention to the recursive structures of linguistic
syntax. In fact, by adopting a grammar formalism
whose primitives operate on these structures as first-
class objects, we can introduce a rich set of relatively
straightforward operations to implement, and moti-
vate them by their role in subsequent programs.
Lexicalization. Students need to distinguish be-
tween specification and implementation, and to un-
derstand the barriers of abstraction that underlie
the distinction. Lexicalized grammars come with a
ready notion of abstraction. From the outside, ab-
stractly, a lexicalized grammar analyzes each sen-
tence as a simple combination of atomic elements
from a lexicon of options. Simultaneously, a con-
crete implementation can assign complex structures
to the atomic elements (elementary trees) and imple-
ment complex combinatory operations.
Strong competence implementation. Students
need to understand how natural language must and
does respond to the practical logic of physical re-
alization, like all AI (Agre, 1997). Mechanisms that
use grammars face inherent computational problems
and natural grammars in particular must respond to
these problems: students should undertake imple-
mentations which directly realize the operations of
the grammar in parsing and generation. But these
must be effective programs that students can build
on—our time and interest is too scarce for extensive
reimplementations.
Simplicity. Where possible, linguistic proposals
should translate readily to the formalism. At the
same time, students should be able to adapt aspects
of the formalism to explore their own judgments
and ideas. Where possible, students should get in-
tuitive and satisfying results from straightforward
algorithms implemented with minimal bookkeeping
and case analysis. At the same time, there is no rea-
son why the formalism should not offer opportuni-
ties for meaningful optimization.
We cannot expect any formalism to fare perfectly
by all these criteria—if any does, it is a deep fact
about natural language! Still, it is worth remark-
ing just how badly these criteria judge traditional
unification-based context-free grammars (CFGs), as
presented in say (Pereira and Shieber, 1987). Data-
structures are an afterthought in CFGs; CFGs can-
not in principle be lexicalized; and, whatever their
merits in parsing or recognition, CFGs set up a pos-
itively abysmal search space for meaningful genera-
tion tasks.
</bodyText>
<sectionHeader confidence="0.999421" genericHeader="method">
4 TAGLET
</sectionHeader>
<bodyText confidence="0.998970857142857">
TAGLET1 is my response to the objectives mo-
tivated in Section 2 and outlined in Section 3.
TAGLET represents my way of distilling the essen-
tial linguistic and computational insights of lexical-
ized tree-adjoining grammar—LTAG (Joshi et al.,
1975; Schabes, 1990)—into a form that students can
easily realize in end-to-end implementations.
</bodyText>
<subsectionHeader confidence="0.975525">
4.1 Overview
</subsectionHeader>
<bodyText confidence="0.9999384">
Like LTAG, TAGLET analyzes sentences as a com-
plex of atomic elements combined by two kinds of
operations, complementation and modification. Ab-
stractly, complementation combines a head with an
argument which is syntactically obligatory and se-
mantically dependent on the head. Abstractly, mod-
ification combines a head with an adjunct which is
syntactically optional and need not involve any spe-
cial semantic dependence. Crucially for generation,
in a derivation, modification and complementation
operations can apply to a head in any order, often
yielding identical structures in surface syntax. This
means the generator can provide required material
first, then elaborate it, enabling use of grammar in
high-level tasks such as the planning of referring ex-
pressions or the “aggregation” of related semantic
material into a single complex sentence.
Concretely, TAGLET operations are implemented
by operations that rewrite trees. Each lexical el-
ement is associated with a fragmentary phrase-
</bodyText>
<footnote confidence="0.6648485">
1If the acronym must stand for something, “Tree Assembly
Grammar for LExicalized Teaching” will do.
</footnote>
<figure confidence="0.998962458333334">
NP
NP
Chris
NP
Sandy
S
�� � �
NP VP
�� ��
V
loves
VP*
\
ADVP
madly
+
T
C
C
T’
�
T
C
T’
</figure>
<figureCaption confidence="0.999997">
Figure 1: Substitution (complementation).
Figure 3: Parallel analysis in TAGLET and TAG.
Figure 2: Forward sister-adjunction (modification.)
</figureCaption>
<bodyText confidence="0.999938956521739">
structure tree containing a distinguished word called
the anchor. For complementation, TAGLET adopts
TAG’s substitution operation; substitution replaces
a leaf node in the head tree with the phrase struc-
ture tree associated with the complement. See Fig-
ure 1. For modification, TAGLET adopts the the
sister-adjunction operation defined in (Rambow et
al., 1995); sister-adjunction just adds the modifier
subtree as a child of an existing node in the head
tree—either on the left of the head (forward sister-
adjunction) as in Figure 2, or on the right of the head
(backward sister-adjunction). I describe TAGLET
formally in Appendix A.
TAGLET is equivalent in weak generative power
to context-free grammar. That is, any language de-
fined by a TAGLET also has a CFG, and any lan-
guage defined by a CFG also has a TAGLET. On the
other hand context-free languages can have deriva-
tions in which all lexical items are arbitrarily far
from the root; TAGLET derived structures always
have an anchor whose path to the root of the sen-
tence has a fixed length given by a grammatical ele-
ment. See Appendix B. The restriction seems of lit-
tle linguistic significance, since any tree-bank parse
induces a unique TAGLET grammar once you la-
bel which child of each node is the head, which are
complements and which are modifiers. Indeed, since
TAGLET thus induces bigram dependency struc-
tures from trees, this invites the estimation of proba-
bility distributions on TAGLET derivations based on
observed bigram dependencies; see (Chiang, 2000).
To implement an effective TAGLET generator,
you can perform a greedy head-first search of deriva-
tions guided by heuristic progress toward achieving
communicative goals (Stone et al., 2001). Mean-
while, because TAGLET is context-free, you can
easily write a CKY-style dynamic programming
parser that stores structures recognized for spans of
text in a chart, and iteratively combines structures
in adjacent spans until the analyses span the entire
sentence. (More complexity would be required for
multiply-anchored trees, as they induce discontinu-
ous constituents.) The simple requirement that op-
erations never apply inside complements or modi-
fiers, and apply left-to-right within a head, suffices
to avoid spurious ambiguity. See Appendix C.
</bodyText>
<subsectionHeader confidence="0.913759">
4.2 Examples
</subsectionHeader>
<bodyText confidence="0.9999898">
With TAGLET, two kinds of examples are instruc-
tive: those where TAGLET can mirror TAG, and
those where it cannot. For the first case, consider
an analysis of Chris loves Sandy madly by the trees
of Figure 3. The final structure is:
</bodyText>
<equation confidence="0.903945428571428">
S
���� � � � �
VP
�
���� � � �
V NP ADVP
loves Sandy madly
</equation>
<bodyText confidence="0.999145666666667">
For the second case, consider the embedded ques-
tion who Chris thinks Sandy likes. The usual TAG
analysis uses the full power of adjunction. TAGLET
requires the use of one of the familiar context-free
filler-gap analyses, as perhaps that suggested by the
trees in Figure 4, and their composition:
</bodyText>
<figure confidence="0.780547071428571">
T
C
+
C*
C’ �
T’
C’
T’
T
C
NP
Chris
NP
S/NP
</figure>
<equation confidence="0.956968888888889">
��� �
��
NP VP/NP
�� � �
V S/NP
thinks
S/NP
�� � �
NP VP/NP
</equation>
<bodyText confidence="0.9751955">
singular; we can immediately unify the case X of the
pronoun with the nominative assigned by the verb:
</bodyText>
<figure confidence="0.977418277777778">
Q
�� �� NP
NP S/NP
Chris
who
NM SG 
NP
CS N
/he/
S
�
���� � � �
VP
V NM SG 
/know/
Sandy V The feature values will be preserved by further steps
of derivation.
likes
</figure>
<figureCaption confidence="0.965758">
Figure 4: TAGLET requires a gap-threading analy-
sis of extraction (or another context-free analysis).
</figureCaption>
<equation confidence="0.950102545454546">
Q
��� � � �
S/NP
��� � � �
VP/NP
��� � � �
V S/NP
�� �
�
thinks NP VP/NP
Sandy V
</equation>
<subsectionHeader confidence="0.979066">
4.3 Building on TAGLET
</subsectionHeader>
<bodyText confidence="0.9748199">
Semantics and pragmatics are crucial to NLP.
TAGLET lets students explore meaty issues in se-
mantics and pragmatics, using the unification-based
semantics proposed in (Stone and Doran, 1997). We
view constituents as referential, or better, indexical;
we link elementary trees with constraints on these
indices and conjoin the constraints in the meaning
of a compound structure. This example shows how
the strategy depends on a rich ontology:
S:e
</bodyText>
<equation confidence="0.711318923076923">
�
���� � � �
VP:e
�
����� �� � �
NP
who
NP
Chris
NP:c
Chris
V:e NP:s ADVP:e
likes
</equation>
<bodyText confidence="0.999638444444444">
The use of syntactic features amounts to an in-
termediate case. In TAGLET derivations (unlike in
TAG) nodes accrete children during the course of a
derivation but are never rewritten or split. Thus, we
can decorate any TAGLET node with a single set
of syntactic features that is preserved throughout the
derivation. Consider the trees for he knows below:
When these trees combine, we can immediately
unify the number Y of the verb with the pronoun’s
</bodyText>
<subsectionHeader confidence="0.470227">
loves Sandy madly
</subsectionHeader>
<bodyText confidence="0.928573166666667">
chris(c) n sandy(s) n love(e,c,s) n mad(e)
The example also shows how the strategy lets us
quickly implement, say, the constraint-satisfaction
approaches to reference resolution or the plan-
recognition approaches to discourse integration de-
scribed in (Stone and Webber, 1998).
</bodyText>
<subsectionHeader confidence="0.996068">
4.4 Lectures and Assignments
</subsectionHeader>
<bodyText confidence="0.99993425">
Here is a plan for a six-week TAGLET module. The
first two weeks introduce data structures and recur-
sive programming in Prolog, with examples drawn
from phrase structure trees and syntactic combi-
nation; and discuss dynamic-programming parsers,
with an aside on convenient implementation using
Prolog assertion. As homework, students implement
simple tree operations, and build up to definitions of
</bodyText>
<equation confidence="0.993195818181818">
 NM SG 
NP
CS X
/he/
 NM Y VP
NP
CS N
V NM Y 
/know/
S
���� � � � �
</equation>
<bodyText confidence="0.999938133333333">
substitution and modification for parsing and gener-
ation; they use these combinatory operations to write
a CKY TAGLET parser.
The next two weeks begin with lectures on the
lexicon, emphasizing abstraction on the computa-
tional side and the idiosyncrasy of lexical syntax and
the indexicality of lexical semantics on the linguis-
tic side; and continue with lectures on semantics and
interpretation. Meanwhile, students add reference
resolution to the parser, and implement routines to
construct grammars from tree-bank parses.
The final two weeks cover generation as problem-
solving, and search through the grammar. Students
reuse the grammar and interpretation model they al-
ready have to construct a generator.
</bodyText>
<sectionHeader confidence="0.99386" genericHeader="method">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999969953488372">
Important as they are, lexicalized grammars can
be forbidding. Versions of TAG and combinatory
categorial grammars (CCG) (Steedman, 2000), as
presented in the literature, require complex book-
keeping for effective computation. When I wrote
a CCG parser as an undergraduate, it took me a
whole semester to get an implemented handle on
the metatheory that governs the interaction of (cross-
ing) composition or type-raising with spurious am-
biguity; I still have never written a TAG parser or a
CCG generator. Variants of TAG like TIG (Schabes
and Waters, 1995) or D-Tree grammars (Rambow
et al., 1995) are motivated by linguistic or formal
considerations rather than pedagogical or computa-
tional ones. Other formalisms come with linguistic
assumptions that are hard to manage. Link gram-
mar (Sleator and Temperley, 1993) and other pure
dependency formalisms can make it difficult to ex-
plore rich hierarchical syntax and the flexibility of
modification; HPSG (Pollard and Sag, 1994) comes
with a commitment to its complex, rather bewilder-
ing regime for formalizing linguistic information as
feature structures. Of course, you probably could
refine any of these theories to a simple core—and
would get something very like TAGLET.
I strongly believe that this distillation is worth
the trouble, because lexicalization ties grammar for-
malisms so closely to the motivations for studying
language in the first place. For linguistics, this phi-
losophy invites a fine-grained description of sen-
tence syntax, in which researchers document the di-
versity of linguistic constructions within and across
languages, and at the same time uncover impor-
tant generalizations among them. For computation,
this philosophy suggests a particularly concrete ap-
proach to language processing, in which the infor-
mation a system maintains and the decisions it takes
ultimately always just concern words. In taking
TAGLET as a starting point for teaching implemen-
tation in NLP, I aim to expose a broad range of stu-
dents to a lexicalized approach to the cognitive sci-
ence of human language that respects and integrates
both linguistic and computational advantages.
</bodyText>
<sectionHeader confidence="0.992251" genericHeader="method">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999319">
Thanks to the students of CS 533 and four anony-
mous reviewers for helping to disabuse me of nu-
merous preconceptions.
</bodyText>
<sectionHeader confidence="0.988051" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.992513">
Philip E. Agre. 1997. Computation and Human Experi-
ence. Cambridge.
Ronald Brachman, Deborah McGuinness, Peter Pa-
tel Schneider, Lori Alperin Resnick, and Alexander
Borgida. 1990. Living with CLASSIC: when and how
to use a KL-ONE-like language. In J. Sowa, editor,
Principles of Semantic Networks. Morgan Kaufmann.
David Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
ACL, pages 456–463.
Herbert H. Clark. 1996. Using Language. Cambridge
University Press, Cambridge, UK.
John E. Hopcroft, Rajeev Motwani, and Jeffrey D. Ull-
man. 2000. Introduction to automata theory, lan-
guages and computation. Addison-Wesley, second
edition.
Aravind K. Joshi, L. Levy, and M. Takahashi. 1975. Tree
adjunct grammars. Journal of the Computer and Sys-
tem Sciences, 10:136–163.
Daniel Jurafsky and James H. Martin. 2000. Speech
and Language Processing: An introduction to nat-
ural language processing, computational linguistics
and speech recognition. Prentice-Hall.
Fernando C. N. Pereira and Stuart M. Shieber. 1987.
Prolog and Natural Language Analysis. CSLI, Stan-
ford CA.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press,
Chicago.
Owen Rambow, K. Vijay-Shanker, and David Weir.
1995. D-Tree grammars. In ACL, pages 151–158.
Yves Schabes and Richard C. Waters. 1995. Tree-
insertion grammar: A cubic-time parsable formalism
that lexicalizes context-free grammar without chang-
ing the trees produced. Computational Linguistics,
21:479–513.
Yves Schabes. 1990. Mathematical and Computational
Aspects ofLexicalized Grammars. Ph.D. thesis, Com-
puter Science Department, University of Pennsylva-
nia.
Daniel Sleator and Davy Temperley. 1993. Parsing
English with a link grammar. In Third International
Workshop on Parsing Technologies.
Mark Steedman. 2000. The Syntactic Process. MIT.
Leon Sterling and Ehud Shapiro. 1994. The Art of Pro-
log. MIT, second edition.
Matthew Stone and Christine Doran. 1997. Sentence
planning as description using tree-adjoining grammar.
In Proceedings ofACL, pages 198–205.
Matthew Stone and Bonnie Webber. 1998. Textual
economy through close coupling of syntax and seman-
tics. In Proceedings of International Natural Lan-
guage Generation Workshop, pages 178–187.
Matthew Stone, Christine Doran, Bonnie Webber, Tonia
Bleam, and Martha Palmer. 2001. Microplanning
with communicative intentions: The SPUD system.
Under review.
</reference>
<sectionHeader confidence="0.65275" genericHeader="method">
A Definitions
</sectionHeader>
<bodyText confidence="0.999961625">
I define TAGLET in terms of primitive trees. The
definitions require a set VT of terminal categories,
corresponding to our lexical items, and a disjoint set
VN of nonterminal categories, corresponding to con-
stituent categories. TAGLET uses trees labeled by
these categories both as representations of the syn-
tactic structure of sentences and as representations
of the grammatical properties of words:
</bodyText>
<listItem confidence="0.9996485">
• A syntactic tree is a tree whose nodes are each
assigned a unique label in VN UVT, such that
only leaf nodes are assigned a label in VT.
• A lexical tree is a syntactic tree in which ex-
</listItem>
<bodyText confidence="0.943103368421053">
actly one node, called the anchor, is assigned a
label in VT. The path through such a tree from
the root to the anchor is called the spine.
A primitive tree is lexical tree in which every leaf is
the child of a node on the spine. See Figures 3 and 4.
A TAGLET element is a pair (T,O) consisting of
primitive tree together with the specification of the
operation for the tree; the allowable operations are
complementation, indicated by α; premodification
at a specified category C E VN, indicated by β&apos;(C)
and postmodification at a specified category C E VN,
indicated by β&apos;(C).
Formally, then, a TAGLET grammar is a tuple
G = (VT,VN,Γ) where VT gives the set of termi-
nal categories, VN gives the set of nonterminal cat-
egories, and Γ gives a set of TAGLET elements for
VT and VN. Given a TAGLET grammar G, the set
of derived trees for G is defined as the smallest set
closed under the following operations:
</bodyText>
<listItem confidence="0.906200583333333">
• (Initial) Suppose (T,O) E Γ. Then (T,O) is a
derived tree for G.
• (Substitution) Suppose (T,O) is a derived tree
for G where T contains leaf node n with label
C EVN; and suppose (T&apos;,α) is a derived tree for
G where the root of T&apos; also has label C. Then
(T&apos;&apos;,O) is a derived tree for G where T&apos;&apos; is ob-
tained from T by identifying node n with the
root of T&apos;.
• (Premodification) Suppose (T,O) is a derived
tree for G where T contains node n with label
C E VN, and suppose (T&apos;,β&apos;(C)) is a derived
</listItem>
<bodyText confidence="0.792324666666667">
tree for G. Then (T&apos;&apos;,O) is a derived tree for G
where T&apos;&apos; is obtained from T by adding T&apos; as
the first child of node n.
</bodyText>
<listItem confidence="0.793934666666667">
• (Postmodification) Suppose (T,O) is a derived
tree for G where T contains node n with label
C E VN, and suppose (T&apos;,β&apos;(C)) is a derived
</listItem>
<bodyText confidence="0.983372111111111">
tree for G. Then (T&apos;&apos;,O) is a derived tree for G
where T&apos;&apos; is obtained from T by adding T&apos; as
the last child of node n.
A derivation for G is a derived tree (T,α) for G, in
which all the leaves of T are elements of VT. The
yield of a derivation (T,α) is the string consisting of
the leaves of T in order. A string σ is in the language
generated by G just in case σ is the yield of some
derivation for G.
</bodyText>
<subsectionHeader confidence="0.546814">
B Properties
</subsectionHeader>
<bodyText confidence="0.994407785714286">
Each node in a TAGLET derived tree T is first con-
tributed by a specific TAGLET element, and so in-
directly by a particular anchor. Accordingly, we can
construct a lexicalized derivation tree corresponding
to T. Nodes in the derivation tree are labeled by the
elements used in deriving T. An edge leads from
parent E to child E if T includes a step of deriva-
tion in which E is substituted or sister-adjoined at a
node first contributed by E. To make the derivation
unambiguous, we record the address of the node in
E at which the operation applies, and we order the
edges in the derivation tree in the same order that
the corresponding operations are applied in T. For
Figure 3, we have:
</bodyText>
<equation confidence="0.932422">
a2:loves

       
a1:Chris (0) a3:Sandy (1.1) b4 :madly (1.1)
</equation>
<bodyText confidence="0.984599142857143">
Let L be a CFL. Then there is a grammar G for
L in Greibach normal form (Hopcroft et al., 2000),
where each production has the form
A  xB1...Bn
where x VT and Bi VN. For each such production,
create the TAGLET element which allows comple-
mentation with a tree as below:
</bodyText>
<equation confidence="0.853915666666667">
A
 
x B1 Bn
</equation>
<bodyText confidence="0.99991925">
An easy induction transforms any derivation in G
to a derivation in this TAGLET grammar, and vice
versa. So both generate the same language L.
Conversely, we can build a CFG for a TAGLET by
creating nonterminals and productions for each node
in a TAGLET elementary structure, taking into ac-
count the possibilities for optional premodification
and postmodification as well as complementation.
</bodyText>
<sectionHeader confidence="0.699154" genericHeader="method">
C Parsing
</sectionHeader>
<bodyText confidence="0.968832770833334">
Suppose we make a bottom-up traversal of a
TAGLET derivation tree to construct the derived
tree. After we finish with each node (and all its chil-
dren), we obtain a subtree of the final derived tree.
This subtree represents a complete constituent that
must appear as a subsequence of the final sentence.
A CKY TAGLET parser just reproduces this hier-
archical discovery of constituents, by adding com-
pleted constituents for complements and modifiers
into an open constituent for a head.
The only trick is to preserve linear order; this
means adding each new complement and modifier at
a possible “next place”, without skipping past miss-
ing complements or slipping under existing modi-
fiers. To do that, we only apply operations that add
completed constituents T2 along what is known as
the frontier of the head tree T1, further away from
the head than previously incorporated material. This
concept, though complex, is essential in any account
of incremental structure-building. To avoid spuri-
ous ambiguities, we also require that operations to
the left frontier must precede operations to the right
frontier. This gives a relation COMBINE(T1,T2,T3).
The parser analyses a string of length N using a
dynamic-programming procedure to enumerate all
the analyses that span contiguous substrings, short-
est substrings first. We write T  (i, j) to indicate
that object T spans position i to j. The start of the
string is position 0; the end is position N. So we
have:
for word w  (i,i + 1), T with anchor w
add T  (i,i + 1)
for k  2 up to N
for i  k− 2 down to 0
for j  i+1 up to k−1
for T1  (i, j) and T2  (j,k)
for T3 with COMBINE(T1,T2,T3)
add T3  (i,k)
Now, any parser that delivers possible analyses ex-
haustively will be prohibitively expensive in the
worst-case; analyses of ambiguities multiply expo-
nentially. At the cost of a strong-competence imple-
mentation, one can imagine avoiding the complexity
by maintaining TAGLET derivation forests. This en-
ables O(N3) recognition, since TAGLET parsing op-
erations apply within spans of the spine of single ele-
mentary trees and therefore the number of COMBINE
results for T1 and T2 is independent of N.
</bodyText>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.255974">
<title confidence="0.660552">Proceedings of the Workshop on Effective Tools and Methodologies for Teaching</title>
<note confidence="0.798516333333333">Natural Language Processing and Computational Linguistics, Philadelphia, July 2002, pp. 77-84. Association for Computational Linguistics. Lexicalized Grammar 101</note>
<author confidence="0.920814">Matthew</author>
<affiliation confidence="0.9976815">Department of Computer Science and Center for Cognitive Rutgers, the State University of New</affiliation>
<address confidence="0.89246">Piscataway NJ 08854-8019</address>
<email confidence="0.999852">mdstone@cs.rutgers.edu</email>
<abstract confidence="0.993570076923077">This paper presents a simple and versatile tree-rewriting lexicalized grammar formalism, TAGLET, that provides an effective scaffold for introducing advanced topics in a survey course on natural language processing (NLP). Students who implement a strong competence TAGLET parser and generator simultaneously get experience with central computer science ideas and develop an effective starting point for their own subsequent projects in data-intensive and interactive NLP.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Philip E Agre</author>
</authors>
<title>Computation and Human Experience.</title>
<date>1997</date>
<location>Cambridge.</location>
<contexts>
<context position="9679" citStr="Agre, 1997" startWordPosition="1425" endWordPosition="1426">erstand the barriers of abstraction that underlie the distinction. Lexicalized grammars come with a ready notion of abstraction. From the outside, abstractly, a lexicalized grammar analyzes each sentence as a simple combination of atomic elements from a lexicon of options. Simultaneously, a concrete implementation can assign complex structures to the atomic elements (elementary trees) and implement complex combinatory operations. Strong competence implementation. Students need to understand how natural language must and does respond to the practical logic of physical realization, like all AI (Agre, 1997). Mechanisms that use grammars face inherent computational problems and natural grammars in particular must respond to these problems: students should undertake implementations which directly realize the operations of the grammar in parsing and generation. But these must be effective programs that students can build on—our time and interest is too scarce for extensive reimplementations. Simplicity. Where possible, linguistic proposals should translate readily to the formalism. At the same time, students should be able to adapt aspects of the formalism to explore their own judgments and ideas. </context>
</contexts>
<marker>Agre, 1997</marker>
<rawString>Philip E. Agre. 1997. Computation and Human Experience. Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Brachman</author>
</authors>
<title>Deborah McGuinness, Peter Patel Schneider, Lori Alperin Resnick, and Alexander Borgida.</title>
<date>1990</date>
<booktitle>Principles of Semantic Networks.</booktitle>
<editor>J. Sowa, editor,</editor>
<publisher>Morgan Kaufmann.</publisher>
<marker>Brachman, 1990</marker>
<rawString>Ronald Brachman, Deborah McGuinness, Peter Patel Schneider, Lori Alperin Resnick, and Alexander Borgida. 1990. Living with CLASSIC: when and how to use a KL-ONE-like language. In J. Sowa, editor, Principles of Semantic Networks. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Statistical parsing with an automatically-extracted tree adjoining grammar.</title>
<date>2000</date>
<booktitle>In ACL,</booktitle>
<pages>456--463</pages>
<contexts>
<context position="14261" citStr="Chiang, 2000" startWordPosition="2147" endWordPosition="2148">e arbitrarily far from the root; TAGLET derived structures always have an anchor whose path to the root of the sentence has a fixed length given by a grammatical element. See Appendix B. The restriction seems of little linguistic significance, since any tree-bank parse induces a unique TAGLET grammar once you label which child of each node is the head, which are complements and which are modifiers. Indeed, since TAGLET thus induces bigram dependency structures from trees, this invites the estimation of probability distributions on TAGLET derivations based on observed bigram dependencies; see (Chiang, 2000). To implement an effective TAGLET generator, you can perform a greedy head-first search of derivations guided by heuristic progress toward achieving communicative goals (Stone et al., 2001). Meanwhile, because TAGLET is context-free, you can easily write a CKY-style dynamic programming parser that stores structures recognized for spans of text in a chart, and iteratively combines structures in adjacent spans until the analyses span the entire sentence. (More complexity would be required for multiply-anchored trees, as they induce discontinuous constituents.) The simple requirement that operat</context>
</contexts>
<marker>Chiang, 2000</marker>
<rawString>David Chiang. 2000. Statistical parsing with an automatically-extracted tree adjoining grammar. In ACL, pages 456–463.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
</authors>
<title>Using Language.</title>
<date>1996</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="4910" citStr="Clark, 1996" startWordPosition="726" endWordPosition="727">acts students from Rutgers’s library and information science department, its primary locus for research in information retrieval and human-computer interaction. Ambitious undergraduates can also take 533 their senior year; most participate in the interdisciplinary cognitive science undergraduate major. 533 is the only computational course in natural language at Rutgers. Overall, the course is structured into three modules, each of which represents about fifteen hours of in-class lecture time. The first module gives a general overview of language use and dialogue applications. Lectures follow (Clark, 1996), but instill the practical methodology for specifying and constructing knowledgebased systems, in the style of (Brachman et al., 1990), into the treatment of communication. Concurrently, students explore precise descriptions of their intuitions about language and communication through a series of short homework exercises. The second module focuses on general techniques for linguistic representation and implementation, using TAGLET. With an extended TAGLET project, conveniently implemented in stages, we use basic tree operations to introduce Prolog programming, including data structures, recur</context>
</contexts>
<marker>Clark, 1996</marker>
<rawString>Herbert H. Clark. 1996. Using Language. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John E Hopcroft</author>
<author>Rajeev Motwani</author>
<author>Jeffrey D Ullman</author>
</authors>
<title>Introduction to automata theory, languages and computation.</title>
<date>2000</date>
<publisher>Addison-Wesley,</publisher>
<note>second edition.</note>
<marker>Hopcroft, Motwani, Ullman, 2000</marker>
<rawString>John E. Hopcroft, Rajeev Motwani, and Jeffrey D. Ullman. 2000. Introduction to automata theory, languages and computation. Addison-Wesley, second edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>L Levy</author>
<author>M Takahashi</author>
</authors>
<title>Tree adjunct grammars.</title>
<date>1975</date>
<journal>Journal of the Computer and System Sciences,</journal>
<pages>10--136</pages>
<contexts>
<context position="11322" citStr="Joshi et al., 1975" startWordPosition="1674" endWordPosition="1677">g just how badly these criteria judge traditional unification-based context-free grammars (CFGs), as presented in say (Pereira and Shieber, 1987). Datastructures are an afterthought in CFGs; CFGs cannot in principle be lexicalized; and, whatever their merits in parsing or recognition, CFGs set up a positively abysmal search space for meaningful generation tasks. 4 TAGLET TAGLET1 is my response to the objectives motivated in Section 2 and outlined in Section 3. TAGLET represents my way of distilling the essential linguistic and computational insights of lexicalized tree-adjoining grammar—LTAG (Joshi et al., 1975; Schabes, 1990)—into a form that students can easily realize in end-to-end implementations. 4.1 Overview Like LTAG, TAGLET analyzes sentences as a complex of atomic elements combined by two kinds of operations, complementation and modification. Abstractly, complementation combines a head with an argument which is syntactically obligatory and semantically dependent on the head. Abstractly, modification combines a head with an adjunct which is syntactically optional and need not involve any special semantic dependence. Crucially for generation, in a derivation, modification and complementation </context>
</contexts>
<marker>Joshi, Levy, Takahashi, 1975</marker>
<rawString>Aravind K. Joshi, L. Levy, and M. Takahashi. 1975. Tree adjunct grammars. Journal of the Computer and System Sciences, 10:136–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>James H Martin</author>
</authors>
<title>Speech and Language Processing: An introduction to natural language processing, computational linguistics and speech recognition.</title>
<date>2000</date>
<publisher>Prentice-Hall.</publisher>
<contexts>
<context position="6005" citStr="Jurafsky and Martin (2000)" startWordPosition="876" endWordPosition="879"> conveniently implemented in stages, we use basic tree operations to introduce Prolog programming, including data structures, recursion and abstraction much as outlined in (Sterling and Shapiro, 1994); then we write a simple chart parser with incremental interpretation, and a simple communicative-intent generator scaled down after (Stone et al., 2001). The third module explores the distinctive problems of specific applications in NLP, including spoken dialogue systems, information retrieval and text classification, spelling correction and shallow tagging applications, and machine translation. Jurafsky and Martin (2000) is our source-book. Concurrently, students pursue a final project, singly or in crossdisciplinary teams, involving a more substantial and potentially innovative implementation. In its overall structure, the course seems quite successful. The initial emphasis on clarifying intuitions about communication puts students on an even footing, as it highlights important ideas about language use without too much dependence on specialized training in language or computation. By the end of the class, students are able to build on the more specifically computational material to come up with substantial a</context>
</contexts>
<marker>Jurafsky, Martin, 2000</marker>
<rawString>Daniel Jurafsky and James H. Martin. 2000. Speech and Language Processing: An introduction to natural language processing, computational linguistics and speech recognition. Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C N Pereira</author>
<author>Stuart M Shieber</author>
</authors>
<title>Prolog and Natural Language Analysis. CSLI,</title>
<date>1987</date>
<location>Stanford CA.</location>
<contexts>
<context position="10849" citStr="Pereira and Shieber, 1987" startWordPosition="1597" endWordPosition="1600">the formalism to explore their own judgments and ideas. Where possible, students should get intuitive and satisfying results from straightforward algorithms implemented with minimal bookkeeping and case analysis. At the same time, there is no reason why the formalism should not offer opportunities for meaningful optimization. We cannot expect any formalism to fare perfectly by all these criteria—if any does, it is a deep fact about natural language! Still, it is worth remarking just how badly these criteria judge traditional unification-based context-free grammars (CFGs), as presented in say (Pereira and Shieber, 1987). Datastructures are an afterthought in CFGs; CFGs cannot in principle be lexicalized; and, whatever their merits in parsing or recognition, CFGs set up a positively abysmal search space for meaningful generation tasks. 4 TAGLET TAGLET1 is my response to the objectives motivated in Section 2 and outlined in Section 3. TAGLET represents my way of distilling the essential linguistic and computational insights of lexicalized tree-adjoining grammar—LTAG (Joshi et al., 1975; Schabes, 1990)—into a form that students can easily realize in end-to-end implementations. 4.1 Overview Like LTAG, TAGLET ana</context>
</contexts>
<marker>Pereira, Shieber, 1987</marker>
<rawString>Fernando C. N. Pereira and Stuart M. Shieber. 1987. Prolog and Natural Language Analysis. CSLI, Stanford CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="19653" citStr="Pollard and Sag, 1994" startWordPosition="3058" endWordPosition="3061">governs the interaction of (crossing) composition or type-raising with spurious ambiguity; I still have never written a TAG parser or a CCG generator. Variants of TAG like TIG (Schabes and Waters, 1995) or D-Tree grammars (Rambow et al., 1995) are motivated by linguistic or formal considerations rather than pedagogical or computational ones. Other formalisms come with linguistic assumptions that are hard to manage. Link grammar (Sleator and Temperley, 1993) and other pure dependency formalisms can make it difficult to explore rich hierarchical syntax and the flexibility of modification; HPSG (Pollard and Sag, 1994) comes with a commitment to its complex, rather bewildering regime for formalizing linguistic information as feature structures. Of course, you probably could refine any of these theories to a simple core—and would get something very like TAGLET. I strongly believe that this distillation is worth the trouble, because lexicalization ties grammar formalisms so closely to the motivations for studying language in the first place. For linguistics, this philosophy invites a fine-grained description of sentence syntax, in which researchers document the diversity of linguistic constructions within and</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase Structure Grammar. University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Owen Rambow</author>
<author>K Vijay-Shanker</author>
<author>David Weir</author>
</authors>
<title>D-Tree grammars.</title>
<date>1995</date>
<booktitle>In ACL,</booktitle>
<pages>151--158</pages>
<contexts>
<context position="13102" citStr="Rambow et al., 1995" startWordPosition="1948" endWordPosition="1951">r LExicalized Teaching” will do. NP NP Chris NP Sandy S �� � � NP VP �� �� V loves VP* \ ADVP madly + T C C T’ � T C T’ Figure 1: Substitution (complementation). Figure 3: Parallel analysis in TAGLET and TAG. Figure 2: Forward sister-adjunction (modification.) structure tree containing a distinguished word called the anchor. For complementation, TAGLET adopts TAG’s substitution operation; substitution replaces a leaf node in the head tree with the phrase structure tree associated with the complement. See Figure 1. For modification, TAGLET adopts the the sister-adjunction operation defined in (Rambow et al., 1995); sister-adjunction just adds the modifier subtree as a child of an existing node in the head tree—either on the left of the head (forward sisteradjunction) as in Figure 2, or on the right of the head (backward sister-adjunction). I describe TAGLET formally in Appendix A. TAGLET is equivalent in weak generative power to context-free grammar. That is, any language defined by a TAGLET also has a CFG, and any language defined by a CFG also has a TAGLET. On the other hand context-free languages can have derivations in which all lexical items are arbitrarily far from the root; TAGLET derived struct</context>
<context position="19274" citStr="Rambow et al., 1995" startWordPosition="3001" endWordPosition="3004">generator. 5 Conclusion Important as they are, lexicalized grammars can be forbidding. Versions of TAG and combinatory categorial grammars (CCG) (Steedman, 2000), as presented in the literature, require complex bookkeeping for effective computation. When I wrote a CCG parser as an undergraduate, it took me a whole semester to get an implemented handle on the metatheory that governs the interaction of (crossing) composition or type-raising with spurious ambiguity; I still have never written a TAG parser or a CCG generator. Variants of TAG like TIG (Schabes and Waters, 1995) or D-Tree grammars (Rambow et al., 1995) are motivated by linguistic or formal considerations rather than pedagogical or computational ones. Other formalisms come with linguistic assumptions that are hard to manage. Link grammar (Sleator and Temperley, 1993) and other pure dependency formalisms can make it difficult to explore rich hierarchical syntax and the flexibility of modification; HPSG (Pollard and Sag, 1994) comes with a commitment to its complex, rather bewildering regime for formalizing linguistic information as feature structures. Of course, you probably could refine any of these theories to a simple core—and would get so</context>
</contexts>
<marker>Rambow, Vijay-Shanker, Weir, 1995</marker>
<rawString>Owen Rambow, K. Vijay-Shanker, and David Weir. 1995. D-Tree grammars. In ACL, pages 151–158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
<author>Richard C Waters</author>
</authors>
<title>Treeinsertion grammar: A cubic-time parsable formalism that lexicalizes context-free grammar without changing the trees produced.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--479</pages>
<contexts>
<context position="19233" citStr="Schabes and Waters, 1995" startWordPosition="2994" endWordPosition="2997">tation model they already have to construct a generator. 5 Conclusion Important as they are, lexicalized grammars can be forbidding. Versions of TAG and combinatory categorial grammars (CCG) (Steedman, 2000), as presented in the literature, require complex bookkeeping for effective computation. When I wrote a CCG parser as an undergraduate, it took me a whole semester to get an implemented handle on the metatheory that governs the interaction of (crossing) composition or type-raising with spurious ambiguity; I still have never written a TAG parser or a CCG generator. Variants of TAG like TIG (Schabes and Waters, 1995) or D-Tree grammars (Rambow et al., 1995) are motivated by linguistic or formal considerations rather than pedagogical or computational ones. Other formalisms come with linguistic assumptions that are hard to manage. Link grammar (Sleator and Temperley, 1993) and other pure dependency formalisms can make it difficult to explore rich hierarchical syntax and the flexibility of modification; HPSG (Pollard and Sag, 1994) comes with a commitment to its complex, rather bewildering regime for formalizing linguistic information as feature structures. Of course, you probably could refine any of these t</context>
</contexts>
<marker>Schabes, Waters, 1995</marker>
<rawString>Yves Schabes and Richard C. Waters. 1995. Treeinsertion grammar: A cubic-time parsable formalism that lexicalizes context-free grammar without changing the trees produced. Computational Linguistics, 21:479–513.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
</authors>
<title>Mathematical and Computational Aspects ofLexicalized Grammars.</title>
<date>1990</date>
<tech>Ph.D. thesis,</tech>
<institution>Computer Science Department, University of Pennsylvania.</institution>
<contexts>
<context position="11338" citStr="Schabes, 1990" startWordPosition="1678" endWordPosition="1679">se criteria judge traditional unification-based context-free grammars (CFGs), as presented in say (Pereira and Shieber, 1987). Datastructures are an afterthought in CFGs; CFGs cannot in principle be lexicalized; and, whatever their merits in parsing or recognition, CFGs set up a positively abysmal search space for meaningful generation tasks. 4 TAGLET TAGLET1 is my response to the objectives motivated in Section 2 and outlined in Section 3. TAGLET represents my way of distilling the essential linguistic and computational insights of lexicalized tree-adjoining grammar—LTAG (Joshi et al., 1975; Schabes, 1990)—into a form that students can easily realize in end-to-end implementations. 4.1 Overview Like LTAG, TAGLET analyzes sentences as a complex of atomic elements combined by two kinds of operations, complementation and modification. Abstractly, complementation combines a head with an argument which is syntactically obligatory and semantically dependent on the head. Abstractly, modification combines a head with an adjunct which is syntactically optional and need not involve any special semantic dependence. Crucially for generation, in a derivation, modification and complementation operations can a</context>
</contexts>
<marker>Schabes, 1990</marker>
<rawString>Yves Schabes. 1990. Mathematical and Computational Aspects ofLexicalized Grammars. Ph.D. thesis, Computer Science Department, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Sleator</author>
<author>Davy Temperley</author>
</authors>
<title>Parsing English with a link grammar.</title>
<date>1993</date>
<booktitle>In Third International Workshop on Parsing Technologies. Mark Steedman.</booktitle>
<publisher>MIT.</publisher>
<contexts>
<context position="19492" citStr="Sleator and Temperley, 1993" startWordPosition="3033" endWordPosition="3036"> bookkeeping for effective computation. When I wrote a CCG parser as an undergraduate, it took me a whole semester to get an implemented handle on the metatheory that governs the interaction of (crossing) composition or type-raising with spurious ambiguity; I still have never written a TAG parser or a CCG generator. Variants of TAG like TIG (Schabes and Waters, 1995) or D-Tree grammars (Rambow et al., 1995) are motivated by linguistic or formal considerations rather than pedagogical or computational ones. Other formalisms come with linguistic assumptions that are hard to manage. Link grammar (Sleator and Temperley, 1993) and other pure dependency formalisms can make it difficult to explore rich hierarchical syntax and the flexibility of modification; HPSG (Pollard and Sag, 1994) comes with a commitment to its complex, rather bewildering regime for formalizing linguistic information as feature structures. Of course, you probably could refine any of these theories to a simple core—and would get something very like TAGLET. I strongly believe that this distillation is worth the trouble, because lexicalization ties grammar formalisms so closely to the motivations for studying language in the first place. For lingu</context>
</contexts>
<marker>Sleator, Temperley, 1993</marker>
<rawString>Daniel Sleator and Davy Temperley. 1993. Parsing English with a link grammar. In Third International Workshop on Parsing Technologies. Mark Steedman. 2000. The Syntactic Process. MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leon Sterling</author>
<author>Ehud Shapiro</author>
</authors>
<title>The Art of Prolog.</title>
<date>1994</date>
<note>MIT, second edition.</note>
<contexts>
<context position="5579" citStr="Sterling and Shapiro, 1994" startWordPosition="817" endWordPosition="820">r specifying and constructing knowledgebased systems, in the style of (Brachman et al., 1990), into the treatment of communication. Concurrently, students explore precise descriptions of their intuitions about language and communication through a series of short homework exercises. The second module focuses on general techniques for linguistic representation and implementation, using TAGLET. With an extended TAGLET project, conveniently implemented in stages, we use basic tree operations to introduce Prolog programming, including data structures, recursion and abstraction much as outlined in (Sterling and Shapiro, 1994); then we write a simple chart parser with incremental interpretation, and a simple communicative-intent generator scaled down after (Stone et al., 2001). The third module explores the distinctive problems of specific applications in NLP, including spoken dialogue systems, information retrieval and text classification, spelling correction and shallow tagging applications, and machine translation. Jurafsky and Martin (2000) is our source-book. Concurrently, students pursue a final project, singly or in crossdisciplinary teams, involving a more substantial and potentially innovative implementati</context>
</contexts>
<marker>Sterling, Shapiro, 1994</marker>
<rawString>Leon Sterling and Ehud Shapiro. 1994. The Art of Prolog. MIT, second edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Stone</author>
<author>Christine Doran</author>
</authors>
<title>Sentence planning as description using tree-adjoining grammar.</title>
<date>1997</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>198--205</pages>
<contexts>
<context position="16375" citStr="Stone and Doran, 1997" startWordPosition="2527" endWordPosition="2530"> of the pronoun with the nominative assigned by the verb: Q �� �� NP NP S/NP Chris who NM SG  NP CS N /he/ S � ���� � � � VP V NM SG  /know/ Sandy V The feature values will be preserved by further steps of derivation. likes Figure 4: TAGLET requires a gap-threading analysis of extraction (or another context-free analysis). Q ��� � � � S/NP ��� � � � VP/NP ��� � � � V S/NP �� � � thinks NP VP/NP Sandy V 4.3 Building on TAGLET Semantics and pragmatics are crucial to NLP. TAGLET lets students explore meaty issues in semantics and pragmatics, using the unification-based semantics proposed in (Stone and Doran, 1997). We view constituents as referential, or better, indexical; we link elementary trees with constraints on these indices and conjoin the constraints in the meaning of a compound structure. This example shows how the strategy depends on a rich ontology: S:e � ���� � � � VP:e � ����� �� � � NP who NP Chris NP:c Chris V:e NP:s ADVP:e likes The use of syntactic features amounts to an intermediate case. In TAGLET derivations (unlike in TAG) nodes accrete children during the course of a derivation but are never rewritten or split. Thus, we can decorate any TAGLET node with a single set of syntactic f</context>
</contexts>
<marker>Stone, Doran, 1997</marker>
<rawString>Matthew Stone and Christine Doran. 1997. Sentence planning as description using tree-adjoining grammar. In Proceedings ofACL, pages 198–205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Stone</author>
<author>Bonnie Webber</author>
</authors>
<title>Textual economy through close coupling of syntax and semantics.</title>
<date>1998</date>
<booktitle>In Proceedings of International Natural Language Generation Workshop,</booktitle>
<pages>178--187</pages>
<contexts>
<context position="17454" citStr="Stone and Webber, 1998" startWordPosition="2707" endWordPosition="2710">ldren during the course of a derivation but are never rewritten or split. Thus, we can decorate any TAGLET node with a single set of syntactic features that is preserved throughout the derivation. Consider the trees for he knows below: When these trees combine, we can immediately unify the number Y of the verb with the pronoun’s loves Sandy madly chris(c) n sandy(s) n love(e,c,s) n mad(e) The example also shows how the strategy lets us quickly implement, say, the constraint-satisfaction approaches to reference resolution or the planrecognition approaches to discourse integration described in (Stone and Webber, 1998). 4.4 Lectures and Assignments Here is a plan for a six-week TAGLET module. The first two weeks introduce data structures and recursive programming in Prolog, with examples drawn from phrase structure trees and syntactic combination; and discuss dynamic-programming parsers, with an aside on convenient implementation using Prolog assertion. As homework, students implement simple tree operations, and build up to definitions of  NM SG  NP CS X /he/  NM Y VP NP CS N V NM Y  /know/ S ���� � � � � substitution and modification for parsing and generation; they use these combinatory operations t</context>
</contexts>
<marker>Stone, Webber, 1998</marker>
<rawString>Matthew Stone and Bonnie Webber. 1998. Textual economy through close coupling of syntax and semantics. In Proceedings of International Natural Language Generation Workshop, pages 178–187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Stone</author>
<author>Christine Doran</author>
<author>Bonnie Webber</author>
<author>Tonia Bleam</author>
<author>Martha Palmer</author>
</authors>
<title>Microplanning with communicative intentions: The SPUD system. Under review.</title>
<date>2001</date>
<contexts>
<context position="5732" citStr="Stone et al., 2001" startWordPosition="839" endWordPosition="842">e precise descriptions of their intuitions about language and communication through a series of short homework exercises. The second module focuses on general techniques for linguistic representation and implementation, using TAGLET. With an extended TAGLET project, conveniently implemented in stages, we use basic tree operations to introduce Prolog programming, including data structures, recursion and abstraction much as outlined in (Sterling and Shapiro, 1994); then we write a simple chart parser with incremental interpretation, and a simple communicative-intent generator scaled down after (Stone et al., 2001). The third module explores the distinctive problems of specific applications in NLP, including spoken dialogue systems, information retrieval and text classification, spelling correction and shallow tagging applications, and machine translation. Jurafsky and Martin (2000) is our source-book. Concurrently, students pursue a final project, singly or in crossdisciplinary teams, involving a more substantial and potentially innovative implementation. In its overall structure, the course seems quite successful. The initial emphasis on clarifying intuitions about communication puts students on an ev</context>
<context position="14451" citStr="Stone et al., 2001" startWordPosition="2173" endWordPosition="2176">B. The restriction seems of little linguistic significance, since any tree-bank parse induces a unique TAGLET grammar once you label which child of each node is the head, which are complements and which are modifiers. Indeed, since TAGLET thus induces bigram dependency structures from trees, this invites the estimation of probability distributions on TAGLET derivations based on observed bigram dependencies; see (Chiang, 2000). To implement an effective TAGLET generator, you can perform a greedy head-first search of derivations guided by heuristic progress toward achieving communicative goals (Stone et al., 2001). Meanwhile, because TAGLET is context-free, you can easily write a CKY-style dynamic programming parser that stores structures recognized for spans of text in a chart, and iteratively combines structures in adjacent spans until the analyses span the entire sentence. (More complexity would be required for multiply-anchored trees, as they induce discontinuous constituents.) The simple requirement that operations never apply inside complements or modifiers, and apply left-to-right within a head, suffices to avoid spurious ambiguity. See Appendix C. 4.2 Examples With TAGLET, two kinds of examples</context>
</contexts>
<marker>Stone, Doran, Webber, Bleam, Palmer, 2001</marker>
<rawString>Matthew Stone, Christine Doran, Bonnie Webber, Tonia Bleam, and Martha Palmer. 2001. Microplanning with communicative intentions: The SPUD system. Under review.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>