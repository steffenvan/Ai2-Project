<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000094">
<title confidence="0.998556">
Event Clustering on Streaming News
Using Co-Reference Chains and Event Words
</title>
<author confidence="0.997736">
June-Jei Kuo
</author>
<affiliation confidence="0.988317666666667">
Department of Computer Science and
Information Engineering
National Taiwan University, Taipei, Taiwan
</affiliation>
<email confidence="0.978599">
jjkuo@nlg.csie.ntu.edu.tw
</email>
<author confidence="0.973128">
Hsin-Hsi Chen
</author>
<affiliation confidence="0.98756">
Department of Computer Science and
Information Engineering
National Taiwan University, Taipei, Taiwan
</affiliation>
<email confidence="0.985108">
hh_chen@csie.ntu.edu.tw
</email>
<sectionHeader confidence="0.992233" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999655411764706">
Event clustering on streaming news aims to
group documents by events automatically.
This paper employs co-reference chains to
extract the most representative sentences, and
then uses them to select the most informative
features for clustering. Due to the long span
of events, a fixed threshold approach prohibits
the latter documents to be clustered and thus
decreases the performance. A dynamic
threshold using time decay function and
spanning window is proposed. Besides the
noun phrases in co-reference chains, event
words in each sentence are also introduced to
improve the related performance. Two models
are proposed. The experimental results show
that both event words and co-reference chains
are useful on event clustering.
</bodyText>
<sectionHeader confidence="0.99869" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999333605633803">
News, which is an important information source,
is reported anytime and anywhere, and is
disseminated across geographic barriers through
Internet. Detecting the occurrences of new events
and tracking the processes of the events (Allan,
Carbonell, and Yamron, 2002) are useful for
decision-making in this fast-changing network era.
Event clustering automatically groups documents
by events that are specified in the documents in a
temporal order. The research issues behind event
clustering include: how many features can be used
to determine event clusters, which cue patterns can
be employed to relate news stories in the same
event, how the clustering strategies affect the
clustering performance using retrospective data or
on-line data, how the time factor affects clustering
performance, and how multilingual data is
clustered.
Chen and Ku (2002) considered named entities,
other nouns and verbs as cue patterns to relate
news stories describing the same event. A
centroid-based approach with a two-threshold
scheme determines relevance (irrelevance)
between a news story and a topic cluster. A least-
recently-used removal strategy models the time
factor in such a way that older and unimportant
terms will have no effect on clustering. Chen, Kuo
and Su (2003) touched on event clustering in
multilingual multi-document summarization. They
showed that translation after clustering is better
than translation before clustering, and translation
deferred to sentence clustering, which reduces the
propagation of translation errors, is most promising.
Fukumoto and Suzuki (2000) proposed concepts
of topic words and event words for event tracking.
They introduced more semantic approach for
feature selection than the approach of parts of
speech. Wong, Kuo and Chen (2001) employed
these concepts to select informative words for
headline generation, and to rank the extracted
sentences in multi-document summarization (Kuo,
Wong, Lin, and Chen, 2002).
Bagga and Baldwin (1998) proposed entity-
based cross-document co-referencing which uses
co-reference chains of each document to generate
its summary and then use the summary rather than
the whole article to select informative words to be
the features of the document. Azzam, Humphreys,
and Gaizauskas (1999) proposed a primitive model
for text summarization using co-reference chains
as well. Silber and McCoy (2002) proposed a text
summarization model using lexical chains and
showed that proper nouns and anaphora resolution
is indispensable.
The two semantics-based feature selection
approaches, i.e., co-reference chains and event
words, are complementary in some sense. The
former denotes equivalence classes of noun
phrases, and the latter considers both nominal and
verbal features, which appear across paragraphs.
This paper will employ both co-reference chains
and event words for temporal event clustering. An
event clustering system using co-reference chains
is described in Section 2. The evaluation method
and the related experimental results are described
in Section 3. The event words are introduced and
discussed in Section 4. Section 5 proposes a
summation model and a two-level model,
respectively for event clustering using both co-
reference chains and event words. Section 6
concludes the remarks.
</bodyText>
<sectionHeader confidence="0.9319835" genericHeader="method">
2 Event Clustering using Co-Reference
Chains
</sectionHeader>
<bodyText confidence="0.999434527777778">
A co-reference chain in a document denotes an
equivalence class of noun phrases. (Cardie and
Wagstaff, 1999) A co-reference resolution
procedure is first to find all the possible NP
candidates. It includes word segmentation, named
entity extraction, part of speech tagging, and noun
phrase chunking. Then the candidates are
partitioned into equivalence classes using the
attributes such as word/phrase itself, parts of
speech of head nouns, named entities, positions in
a document, numbers (singular, plural, or
unknown), pronouns, gender (female, male, or
unknown), and semantics of head nouns. As the
best F-measure of automatic co-reference
resolution in English documents in MUC-7 was
61.8% (MUC, 1998), a corpus hand-tagged with
named entities, and co-reference chains are
prepared and employed to examine the real effects
of co-reference chains in event clustering r.
Headlines of a news story can be regarded as its
short summary. That is, the words in the headline
represent the content of a document in some sense.
The co-reference chains that are initiated by the
words in the headlines are assumed to have higher
weights. A sentence which contains any words in
a given co-reference chain is said to “cover” that
chain. Those sentences which cover more co-
reference chains contain more information, and are
selected to represent a document. Each sentence in
a document is ranked according to the number of
co-reference chains that it covers. Five scores
shown below are computed. Sentences are sorted
by the five scores in sequence and the sentences of
the highest score are selected. The selection
procedure is repeated until the designated number
of sentences, e.g., 4 in this paper, is obtained.
</bodyText>
<listItem confidence="0.999776625">
(1) For each sentence that is not selected, count
the number of noun co-reference chains from
the headline, which are covered by this
sentence and have not been covered by the
previously selected sentences.
(2) For each sentence that is not selected, count
the number of noun co-reference chains from
the headline, which are covered by this
sentence, and add the count to the number of
verbal terms in this sentence which also appear
in the headline.
(3) For each sentence that is not selected, count
the number of noun co-reference chains, which
are covered by this sentence and have not been
covered by the previously selected sentences.
(4) For each sentence that is not selected, count
</listItem>
<bodyText confidence="0.97283576">
the number of noun co-reference chains, which
are covered by this sentence, and add the count
to the number of verbal terms in this sentence
which also appear in the headline.
(5) The position of a sentence
Score 1 only considers nominal features.
Comparatively, Score 2 considers both nominal
and verbal features together. Both scores are
initiated by headlines. Scores 3 and 4 consider all
the co-reference chains no matter whether these
chains are initiated by headlines or not. These two
scores ranks those sentences of the same scores 1
and 2. Besides, they can assign scores to news
stories without headlines. Scores 1 and 3 are
recomputed in the iteration. Finally, since news
stories tend to contain more information in the
leading paragraphs, Score 5 determines which
sentence will be selected according to position of
sentences, when sentences are of the same scores
(1)-(4). The smaller the position number of a
sentence is, the more it will be preferred.
The sentences extracted from a document form a
summary for this document. It is in terms of a
term vector with weights defined below. It is a
normalized TF-IDF.
</bodyText>
<equation confidence="0.998422333333333">
(1)
+ 2 s2
si2 +⋅⋅⋅ + in
</equation>
<bodyText confidence="0.996744714285714">
where tfij is frequency of term tj in summary i, N
is total number of summaries in the
collection being examined, dfj is number
of summaries that term tj occurs, and sij
denotes the TF-IDF value of term tj in
summary i.
A single-pass complete link clustering algorithm
incrementally divides the documents into several
event clusters. We compute the similarities of the
summary of an incoming news story with each
summary in a cluster. Let V1 and V2 be the vectors
for the two summaries extracted from documents
D1 and D2. The similarity between V1 and V2 is
computed as follows.
∑ w1j × w2 j
If all the similarities are larger than a fixed
threshold, the news story is assigned to the cluster.
Otherwise, it forms a new cluster itself. Life span
is a typical phenomenon for an event. It may be
very long. Figure 1 shows the life span of an air
crash event is more than 100 days. To tackle the
</bodyText>
<equation confidence="0.952982785714286">
(V1 , V2 ) = j (2) ∑
j =1 w1 j ∑
Sim
common
term t
2
m
w
j =1 2j
wij
= tfij × log N
dfj
2
si1
</equation>
<bodyText confidence="0.999952142857143">
long life span of an event, a dynamic threshold
(d_th) shown below is introduced, where th is an
initial threshold. In other words, the earlier the
documents are put in a cluster, the smaller their
thresholds are. Assume the published day of
document D2 is later than that of document D1.
where dist (day distance) denotes the number of
days away from the day at which the
event happens, and w_size (window size)
keeps the threshold unchanged within the
same window.
Moreover, we use square root function to
prevent the dynamic threshold from downgrading
too fast.
</bodyText>
<sectionHeader confidence="0.963331" genericHeader="method">
3 Test Collection
</sectionHeader>
<bodyText confidence="0.9983636875">
In our experiment, we used the knowledge base
provided by the United Daily News
(http://udndata.com/), which has collected
6,270,000 Chinese news articles from 6 Taiwan
local newspaper companies since 1975/1/1. To
prepare a test corpus, we first set the topic to be
“華航空難&amp;quot; (Air Accident of China Airlines),
and the range of searching date from 2002/5/26 to
2002/9/4 (stopping all rescue activities). Total 964
related news articles, which have published date,
news source, headline and content, respectively,
are returned from search engine. All are in SGML
format. After reading those news articles, we
deleted 5 news articles which have headlines but
without any content. The average length of a news
article is 15.6 sentences. Figure 1 depicts the
distribution of the document number within the
event life span, where the x-axis denotes the day
from the start of the year. For example, “146”
denotes the day of ‘2002/5/26’, which is the 146th
day of year 2002.
Then, we identify thirteen focus events, e.g.,
rescue status. Meanwhile, two annotators are
asked to read all the 959 news articles and classify
these articles into 13 events. If a news article can
not be classified, the article is marked as “other”
type. A news article which reports more than one
event may be classified into more than one event
cluster. We compare the classification results of
annotators and consider those consistent results as
our answer set. Table 1 shows the distribution of
the 13 focus events.
</bodyText>
<table confidence="0.999886166666667">
Event Name Number of
Documents
Fly right negotiation between 20
Taiwan and Hong Kong
Cause of air accident 57
Confirmation of air accident 6
Influence on stock market 27
Influence on insurance fee 11
Influence on China Airline 8
Influence on Peng-Hu 26
archipelagoes
Punishment for persons in charge 10
News reporting 18
Wreckage found 28
Remains found 57
Rescue status 65
Solatium 34
Other 664
</table>
<tableCaption confidence="0.999468">
Table 1: Focus Events
</tableCaption>
<bodyText confidence="0.9999355">
We adopt the metric used in Topic Detection
and Tracking (Fiscus and Doddington, 2002). The
evaluation is based on miss and false alarm rates.
Both miss and false alarm are penalties. They can
</bodyText>
<figure confidence="0.9106085">
d _th(D1,D2) = 1 x th (3)dist(D)/w_size+ 1
2
dist(P)/w size+
—A—fd�./dw
160
140
120
100
40
20
80
60
0
145 155 165 175 185 195 205 215 225 235 245
</figure>
<figureCaption confidence="0.999959">
Figure 1. Event Evolution of China Airlines Air Accident (2002/5/26 ~ 2002/9/4)
</figureCaption>
<bodyText confidence="0.950270285714286">
measure more accurately the behavior of users who
try to retrieve news stories. If miss or false alarm
is too high, users will not be satisfied with the
clustering results. The performance is
characterized by a detection cost, , in terms of
Cdet
the probability of miss and false alarm:
</bodyText>
<equation confidence="0.742278">
CDet = CMiss X PMiss X Parget + CFA X PFA X P n1arget (4)
</equation>
<bodyText confidence="0.988639">
where and are costs of a miss and a
</bodyText>
<subsubsectionHeader confidence="0.275805">
CMiss CFA
</subsubsectionHeader>
<bodyText confidence="0.823578">
false alarm, respectively, and
</bodyText>
<subsubsectionHeader confidence="0.496834">
PMiss PFA
</subsubsectionHeader>
<bodyText confidence="0.993054666666667">
are the conditional probabilities of a miss
and a false alarm, and tParget and
Pnon−t arget(= 1 − Ptarget) are the prior target
probabilities.
Manmatha, Feng and Allan (2002) indicated that
the standard TDT cost function used for all
evaluations in TDT is Cdet = 0.02PMiss + 0.098PFA ,
when CMiss, CFA and Ptarget are set to 1, 0.1 and 0.02,
respectively. The less the detection cost is, the
higher the performance is.
For comparison, the centroid-based approach
and single pass clustering is regarded as a baseline
model. Conventional TF-IDF scheme selects 20
features for each incoming news articles and each
cluster uses 30 features to be its centroid.
Whenever an article is assigned to a cluster, the 30
words of the higher TF-IDFs are regarded as the
new centroid of that cluster. The experimental
results with various thresholds are shown in Table
2. The best result is 0.012990 when the threshold
is set to 0.05.
</bodyText>
<table confidence="0.986208428571428">
Fixed Threshold Cdet
0.01 0.024644
0.05 0.012990
0.10 0.013736
0.15 0.014331
0.20 0.015480
0.25 0.015962
</table>
<tableCaption confidence="0.997893">
Table 2: Detection Costs Using Centroid Approach
</tableCaption>
<bodyText confidence="0.998584285714286">
Kuo, Wong, Lin and Chen (2002) indicated that
near 26% of compression rate is suitable for a
normal reader in multi-document summarization.
Recall that the average length of a news story is
15.6 sentences. Following their postulation, total 4
sentences, i.e., 16/4, are selected using co-
reference chains. Table 3 shows the detection cost
with various threshold settings. We found that the
best result could be obtained using threshold 0.05,
however, it was lower than the result of baseline
(i.e., 0.013137 &gt; 0.012990).
Next, we study the effects of dynamic thresholds.
Three dynamic threshold functions are
experimented under the window size 1. A linear
decay approach removes the square root function
in Formula (3). A slow decay approach adds a
constant (0.05) to Formula (3) to keep the
minimum threshold to be 0.05 and degrades the
threshold slowly. Table 4 shows that Formula (3)
obtained the best result, and the dynamic threshold
approach is better than the baseline model.
</bodyText>
<figure confidence="0.964446285714286">
Fixed Threshold Cdet
0.01 0.015960
0.05 0.013137
0.10 0.015309
0.15 0.016507
0.20 0.016736
0.25 0.017360
</figure>
<tableCaption confidence="0.949518">
Table 3. Detection Costs Using Co-Reference
Chains
</tableCaption>
<table confidence="0.999073">
Function Linear Formula Slow
Type decaying (3) Decaying
Cdet 0.013196 0.012657 0.016344
</table>
<tableCaption confidence="0.9743065">
Table 4. Detection Costs with Various Dynamic
Threshold Functions (Initial Threshold = 0.05)
</tableCaption>
<bodyText confidence="0.999925666666667">
Additionally, we evaluate the effect of the
window size. Table 5 shows the results using
various window sizes in Formula (3). The best
detection cost, i.e., 0.012647, is achieved under
window size 2. It also shows the efficiency of
dynamic threshold and window size.
</bodyText>
<table confidence="0.810792333333333">
Window 1 2 3 4
size
Cdet 0.012657 0.012647 0.012809 0.012942
</table>
<tableCaption confidence="0.8704145">
Table 5. Detection Costs with Various Window
Sizes Using Formula (3) (Initial Threshold = 0.05)
</tableCaption>
<sectionHeader confidence="0.967027" genericHeader="method">
4 Event Clustering Using Event Words
</sectionHeader>
<bodyText confidence="0.999943297297297">
The co-reference chains in the above approach
considered those features, such as person name,
organization name, location, temporal expression
and number expression. However, the important
words “black box” or “rescue” in an air crash event
are never shown in any co-reference chain. This
section introduces the concepts of event words.
Topic and event words were applied to topic
tracking successfully (Fukumoto and Suzuki,
2000). The basic hypothesis is that an event word
associated with a news article appears across
paragraphs, but a topic word does not. In contrast,
a topic word frequently appears across all news
documents. Because the goal of event clustering is
to extract all the events associated with a topic,
those documents belonging to the same topic, e.g.,
China Airlines Air Accident, always have the
similar topic words like “China Airlines”, “flight
611”, “air accident”, “Pen-Hu”, “Taiwan strait”,
“rescue boats”, etc. Topic words seem to have no
help in event clustering. Comparatively, each
news article has different event words, e.g.,
“emergency command center”, “set up”,
“17:10PM”, “CKS airport”, “Commander Lin”,
“stock market”, “body recovery”, and so on.
Extracting such keywords is useful to understand
the events, and distinguish one document from
another.
The postulation by Fukumoto and Suzuki (2002)
is that the domain dependency among words is a
key clue to distinguish a topic and an event. This
can be captured by dispersion value and deviation
value. The former tells if a word appears across
paragraphs (documents), and the latter tells if a
word appears frequently. Event words are
extracted by using these two values. Formula (5)
defines a weight of term t in the i-th story.
</bodyText>
<equation confidence="0.98876">
Wsit = TFsit x log N
Maxj (TFsij) Nst
</equation>
<bodyText confidence="0.99482275">
where TFsit denotes term frequency of term t in
the i-th story, N is total number of stories,
and Nst is the number of stories where
term t occurs.
Besides term weight in story level, Wpit defines
a weight of term t in the i-th paragraph. Formulas
(6) and (7) define dispersion value and deviation
value, respectively.
</bodyText>
<equation confidence="0.950586">
) 2 (6)
m
DevSit = (Wsit − meant) x α + β (7)
DispSt
</equation>
<bodyText confidence="0.997818764705882">
Where, meant is average weight of term t in
story level. Similarly, DispPt and DevPjt are
defined in the paragraph level. The dispersion
value of term t in the story level denotes how
frequently term t appears across m stories. The
deviation value of term t in the i-th story denotes
how frequently it appears in a particular story.
Coefficients α and R are used to adjust the number
of event words. In our experiments, 20 event
words are extracted for each document. In such a
case, (α, R) is set to (10, 50) in story level and set
to (10, 25) in paragraph level, respectively.
Formula (8) shows that term t frequently appears
across paragraphs rather than stories. Formula (9)
shows that term t frequently appears in the i-th
story rather than paragraph Pj. An event word is
extracted if it satisfies both formulas (8) and (9).
</bodyText>
<equation confidence="0.508639">
DispPt &lt; DispSt (8)
DevPjt &lt; DevSit for all Pj such that Pj ∈ Si (9)
</equation>
<bodyText confidence="0.935355366666667">
Below shows the event clustering using event
words only. At first, we extract the event words of
each news article using the whole news collection.
For each sentence, we then compute the number of
event words in it. After sorting all sentences, the
designated number of sentences are extracted
according to their number of event words. In the
experiments, we use different window sizes to
study the change of detection cost after introducing
event words. Table 6 shows the experimental
results under the same threshold (0.005) and test
collection mentioned in Section 3.
Window 1 2 3 4
size
Cdet 0.011918 0.011842 0.011747 0.011923
Table 6. Detection Costs with Event Words and
Various Window Sizes
The results in Table 6 are much better than those
in Table 5, because inclusion of event words
selects more informative or representative
sentences or paragraphs. The more informative
feature words documents have, the more
effectively documents of one event can be
distinguished from those of another. In other
words, the similarities of documents among
different events become smaller, so that the
documents cannot be assigned to the same cluster
easily under the higher threshold, and the best
performance is shifted from window size 2 to
window size 3.
</bodyText>
<sectionHeader confidence="0.9397775" genericHeader="method">
5 Event Clustering Using Both Co-reference
Chains and Event Words
</sectionHeader>
<bodyText confidence="0.999786071428572">
According to the above experimental results, it is
evident that either co-reference chains or event
words are useful for event clustering on streaming
news. As co-reference chains and event words are
complementary in some sense, we further examine
the effect on event clustering using both of them.
Thus, two models called summation model and
two-level model, respectively, are proposed. The
summation model is used to observe the
summation effect using both the co-reference
chains and the event words on event clustering.
On the other hand, the two-level model is used to
observe the interaction between co-reference
chains and event words.
</bodyText>
<equation confidence="0.752871666666667">
(5)
DispSt
m 1(Wsit −mean
</equation>
<bodyText confidence="0.482553">
=
</bodyText>
<subsectionHeader confidence="0.966885">
5.1 Summation Model
</subsectionHeader>
<bodyText confidence="0.999706578947368">
In summation model, we simply add the scores
for both co-reference chains and event words,
which are described above respectively to be the
score for each sentence in the news document. At
first, we extract the event words of each news
article using the whole news collection described
in Section 3. For each sentence, we then compute
the number of event words in it, and add this count
to the number of co-reference chains it covers. The
iterative procedure specified in Section 2 extracts
the designated number of sentences according to
the number of event words and co-reference chains.
Table 7 summarizes the experimental results
under the same test collection mentioned in
Section 3. The experiments of summation model
show that the best detection cost is 0.011603.
Comparing the best result with those in Tables 5
and 6, the detection costs are decreased 9% and 2%,
respectively.
</bodyText>
<table confidence="0.929560666666667">
Window 1 2 3 4
size
Cdet 0.112233 0.011603 0.013109 0.013109
</table>
<tableCaption confidence="0.992169">
Table 7. Detection Costs Using Summation Model
</tableCaption>
<subsectionHeader confidence="0.87051">
5.2 Two-level model
</subsectionHeader>
<bodyText confidence="0.999990678571428">
By comparing the experimental results described
in Section 3 and 4, we noticed that the event word
factor seems more important than the co-reference
factor on event clustering of news document.
Moreover, from the summation model we only
know that both factors are useful on event
clustering. In order to make clear which factor is
more important during event clustering of news
documents, a two-level model is designed in such a
way that the co-reference chains or the event words
are used separately rather than simultaneously. For
example, we use the score function and the
sentence selection algorithm described in Section 3
first, when there is a tie during sentence selection.
Then we use the score function described in
Section 4 to decide which sentence is selected from
those candidate sentences, and vice versa. Thus,
two alternatives are considered. Type 1 model
uses the event words sentence selection algorithm
described in Section 4 to select the representative
sentences from each document, the co-reference
chains are used to solve the tie issue. In contrast,
type 2 model uses the co-reference chains sentence
selection algorithm described in Section 3 to select
the representative sentences for each documents
and use event words to solve the tie issue. Table 8
shows the experimental result under the same test
collection as described in previous sections.
</bodyText>
<table confidence="0.9365205">
Window 2 3 4 5
size
Type 1 0.012116 0.011987 0.011662 0.012266
Type 2 0.012789 0.012674 0.012854 0.012941
</table>
<tableCaption confidence="0.992785">
Table 8. Detection Costs Using Two level Models
</tableCaption>
<bodyText confidence="0.999042466666667">
The performance of type 1 outperforms that of
type 2. This result conforms to those shown in
Table 5 and Table 6. We can say that the effect of
event words is better than the co-reference chains
in event clustering. Furthermore, the best score of
type 1 is also better than the best score of Table 6.
Thus, the introduction of co-reference chains can
really improve the performance of event clustering
using event words. On the other hand, the
introduction of event words in type 2 does not have
such an effect. Moreover, to further examine the
use of co-reference chain information and the
event words in event clustering, a more elaborate
combination, e.g., using mutual information or
entropy, of the two approaches is needed.
</bodyText>
<sectionHeader confidence="0.991664" genericHeader="conclusions">
6 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999894913043478">
This paper presented an approach for event
clustering on streaming news based on both co-
reference chains and event words. The
experimental results using event words only
outperform the results using the co-reference
chains only. Nevertheless, as to the combination
of co-reference chains and event words in event
clustering, the experimental results show that the
introduction of co-reference chains can improve
the performance of event clustering using event
words much. To model the temporal behavior of
event clustering of streaming news, a dynamic
threshold setting using time decay function and
spanning window size is proposed. The
experimental results, using TDT’s evaluation
metric – say, detection cost, show that the dynamic
threshold is useful. .
We believe that the improvement of multi-
document co-reference resolution will have great
impact on temporal event clustering. In order to
further improve our performance in even clustering
on streaming news, there are still future works
needed to be studied:
</bodyText>
<listItem confidence="0.7074294">
(1) In order to verify the significance of the
experimental results, statistical test is needed.
(2) Instead of hand-tagging method, we will
introduce automatic co-reference resolution
tools to create large scale test corpus and
conduct large scale experiments.
(3) When the length of document is variable, the
fixed number of representative sentences may
lose many important sentences to degrade the
performance of event clustering. The dynamic
</listItem>
<bodyText confidence="0.738822888888889">
number of representative sentences for each
document according to its length is introduced.
(4) As the news stories are reported incrementally
instead of being given totally in the on-line
event clustering, the computation of event
words is an important issue.
(5) Apply the extracted sentences for each
document to generate event-based short
summary.
</bodyText>
<sectionHeader confidence="0.996677" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999867471428571">
Allan, James; Carbonell, Jaime; and Yamron,
Jonathan (Eds) (2002) Topic Detection and
Tracking: Event-Based Information Organization,
Kluwer.
Azzam, S.; Humphreys, K; and Gaizauskas, R.
(1999) “Using Coreference Chains for Text
Summarization,” Proceedings of the ACL
Workshop on Coreference and Its Applications,
Maryland.
Bagga, A. and Baldwin, B. (1998) “Entity-Based
Cross-Document Coreferencing Using the
Vector Space Model,” Proceedings of the 36th
Annual Meeting of ACL and the 17th
International Conference on Computational
Linguistics.
Cardie, Claire and Wagstaff, Kiri (1999) “Noun
Phrase Co-reference as Clustering,” Proceeding
of the Joint Coreference on EMNLP and VLC
Chen, Hsin-Hsi and Ku, Lun-Wei (2002) “An NLP
&amp; IR Approach to Topic Detection,” Topic
Detection and Tracking: Event-Based
Information Organization, James Allan, Jaime
Carbonell, and Jonathan Yamron (Editors),
Kluwer, pp. 243-264.
Chen, Hsin-Hsi; Kuo, June-Jei and Su, Tsei-Chun
(2003) “Clustering and Visualization in a Multi-
Lingual Multi-Document Summarization
System,” Proceedings of 25th European
Conference on Information Retrieval Research,
Lecture Notes in Computer Science, LNCS 2633,
pp. 266-280.
Fiscus, Jonathan G. and Doddington, George R.
(2002) “Topic Detection and Tracking
Evaluation Overview,” Topic Detection and
Tracking: Event-Based Information Organization,
James Allan, Jaime Carbonell, and Jonathan
Yamron (Eds), Kluwer, pp. 17-32.
Fukumoto, F. and Suzuki, Y. (2000) “Event
Tracking based on Domain Dependency,”
Proceedings of the 23rd ACM SIGIR 2000
Conference, pp. 57-64
Kuo, June-Jei; Wong, Hung-Chia; Lin, Chuan-Jie
and Chen, Hsin-Hsi (2002) “Multi-Document
Summarization Using Informative Words and Its
Evaluation with a QA System,” Proceedings of
The Third International Conference on
Intelligent Text Processing and Computational
Linguistics, Lecture Notes in Computer Science,
LNCS 2276, pp. 391-401.
Manmatha, R.; Feng, A. and Allan, James (2002)
“A Critical Examination of TDT’s Cost
Function,” Proceedings of the 25th ACM SIGIR
Conference, pp. 403-404.
MUC (1998) Proceedings of 7th Message
Understanding Conference, Fairfax, VA, 29
April - 1 May, 1998,
http://www.itl.nist.gov/iaui/894.02/
related_projects/muc/ index.html.
Silber, H. Gregory and McCoy, Kathleen F. (2002)
“Eficiently Computed Lexical Chains As an
Intermediate Representation for Automatic Text
Summarization.” Journal of Association for
Computational Linguistics, Vol.28, No.4, pp.
487-496.
Wong, Hong-Jia; Kuo, June-Jei and Chen, Hsin-
Hsi (2001) “Headline Generation for Summaries
from Multiple Online Sources.” Proceedings of
6th Natural Language Processing Pacific Rim
Symposium, November 27-29 2001, Tokyo,
Japan, pp. 653-660.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.441117">
<title confidence="0.999658">Event Clustering on Streaming Using Co-Reference Chains and Event Words</title>
<author confidence="0.999869">June-Jei Kuo</author>
<affiliation confidence="0.98444">Department of Computer Science and Information Engineering</affiliation>
<address confidence="0.741766">National Taiwan University, Taipei, Taiwan</address>
<email confidence="0.971714">jjkuo@nlg.csie.ntu.edu.tw</email>
<author confidence="0.996871">Hsin-Hsi Chen</author>
<affiliation confidence="0.9853175">Department of Computer Science and Information Engineering</affiliation>
<address confidence="0.65337">National Taiwan University, Taipei, Taiwan</address>
<email confidence="0.966063">hh_chen@csie.ntu.edu.tw</email>
<abstract confidence="0.999487777777778">Event clustering on streaming news aims to group documents by events automatically. This paper employs co-reference chains to extract the most representative sentences, and then uses them to select the most informative features for clustering. Due to the long span of events, a fixed threshold approach prohibits the latter documents to be clustered and thus decreases the performance. A threshold using time decay function and spanning window is proposed. Besides the noun phrases in co-reference chains, event words in each sentence are also introduced to improve the related performance. Two models are proposed. The experimental results show that both event words and co-reference chains are useful on event clustering.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allan</author>
<author>Jaime Carbonell</author>
<author>Jonathan Yamron</author>
</authors>
<title>Topic Detection and Tracking: Event-Based Information Organization,</title>
<date>2002</date>
<publisher>Kluwer.</publisher>
<contexts>
<context position="1377" citStr="Allan, Carbonell, and Yamron, 2002" startWordPosition="187" endWordPosition="191">formance. A dynamic threshold using time decay function and spanning window is proposed. Besides the noun phrases in co-reference chains, event words in each sentence are also introduced to improve the related performance. Two models are proposed. The experimental results show that both event words and co-reference chains are useful on event clustering. 1 Introduction News, which is an important information source, is reported anytime and anywhere, and is disseminated across geographic barriers through Internet. Detecting the occurrences of new events and tracking the processes of the events (Allan, Carbonell, and Yamron, 2002) are useful for decision-making in this fast-changing network era. Event clustering automatically groups documents by events that are specified in the documents in a temporal order. The research issues behind event clustering include: how many features can be used to determine event clusters, which cue patterns can be employed to relate news stories in the same event, how the clustering strategies affect the clustering performance using retrospective data or on-line data, how the time factor affects clustering performance, and how multilingual data is clustered. Chen and Ku (2002) considered </context>
</contexts>
<marker>Allan, Carbonell, Yamron, 2002</marker>
<rawString>Allan, James; Carbonell, Jaime; and Yamron, Jonathan (Eds) (2002) Topic Detection and Tracking: Event-Based Information Organization, Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Azzam</author>
<author>K Humphreys</author>
<author>R Gaizauskas</author>
</authors>
<title>Using Coreference Chains for Text Summarization,”</title>
<date>1999</date>
<booktitle>Proceedings of the ACL Workshop on Coreference and Its Applications,</booktitle>
<location>Maryland.</location>
<marker>Azzam, Humphreys, Gaizauskas, 1999</marker>
<rawString>Azzam, S.; Humphreys, K; and Gaizauskas, R. (1999) “Using Coreference Chains for Text Summarization,” Proceedings of the ACL Workshop on Coreference and Its Applications, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bagga</author>
<author>B Baldwin</author>
</authors>
<title>Entity-Based Cross-Document Coreferencing Using the Vector Space Model,”</title>
<date>1998</date>
<booktitle>Proceedings of the 36th Annual Meeting of ACL and the 17th International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="3090" citStr="Bagga and Baldwin (1998)" startWordPosition="441" endWordPosition="444">hat translation after clustering is better than translation before clustering, and translation deferred to sentence clustering, which reduces the propagation of translation errors, is most promising. Fukumoto and Suzuki (2000) proposed concepts of topic words and event words for event tracking. They introduced more semantic approach for feature selection than the approach of parts of speech. Wong, Kuo and Chen (2001) employed these concepts to select informative words for headline generation, and to rank the extracted sentences in multi-document summarization (Kuo, Wong, Lin, and Chen, 2002). Bagga and Baldwin (1998) proposed entitybased cross-document co-referencing which uses co-reference chains of each document to generate its summary and then use the summary rather than the whole article to select informative words to be the features of the document. Azzam, Humphreys, and Gaizauskas (1999) proposed a primitive model for text summarization using co-reference chains as well. Silber and McCoy (2002) proposed a text summarization model using lexical chains and showed that proper nouns and anaphora resolution is indispensable. The two semantics-based feature selection approaches, i.e., co-reference chains </context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Bagga, A. and Baldwin, B. (1998) “Entity-Based Cross-Document Coreferencing Using the Vector Space Model,” Proceedings of the 36th Annual Meeting of ACL and the 17th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Cardie</author>
<author>Kiri Wagstaff</author>
</authors>
<date>1999</date>
<booktitle>Noun Phrase Co-reference as Clustering,” Proceeding of the Joint Coreference on EMNLP and VLC</booktitle>
<contexts>
<context position="4535" citStr="Cardie and Wagstaff, 1999" startWordPosition="654" endWordPosition="657"> both co-reference chains and event words for temporal event clustering. An event clustering system using co-reference chains is described in Section 2. The evaluation method and the related experimental results are described in Section 3. The event words are introduced and discussed in Section 4. Section 5 proposes a summation model and a two-level model, respectively for event clustering using both coreference chains and event words. Section 6 concludes the remarks. 2 Event Clustering using Co-Reference Chains A co-reference chain in a document denotes an equivalence class of noun phrases. (Cardie and Wagstaff, 1999) A co-reference resolution procedure is first to find all the possible NP candidates. It includes word segmentation, named entity extraction, part of speech tagging, and noun phrase chunking. Then the candidates are partitioned into equivalence classes using the attributes such as word/phrase itself, parts of speech of head nouns, named entities, positions in a document, numbers (singular, plural, or unknown), pronouns, gender (female, male, or unknown), and semantics of head nouns. As the best F-measure of automatic co-reference resolution in English documents in MUC-7 was 61.8% (MUC, 1998), </context>
</contexts>
<marker>Cardie, Wagstaff, 1999</marker>
<rawString>Cardie, Claire and Wagstaff, Kiri (1999) “Noun Phrase Co-reference as Clustering,” Proceeding of the Joint Coreference on EMNLP and VLC</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hsin-Hsi Chen</author>
<author>Lun-Wei Ku</author>
</authors>
<title>An NLP &amp; IR Approach to Topic Detection,” Topic Detection and Tracking: Event-Based Information Organization,</title>
<date>2002</date>
<pages>243--264</pages>
<publisher>Kluwer,</publisher>
<location>James</location>
<contexts>
<context position="1965" citStr="Chen and Ku (2002)" startWordPosition="276" endWordPosition="279">n, Carbonell, and Yamron, 2002) are useful for decision-making in this fast-changing network era. Event clustering automatically groups documents by events that are specified in the documents in a temporal order. The research issues behind event clustering include: how many features can be used to determine event clusters, which cue patterns can be employed to relate news stories in the same event, how the clustering strategies affect the clustering performance using retrospective data or on-line data, how the time factor affects clustering performance, and how multilingual data is clustered. Chen and Ku (2002) considered named entities, other nouns and verbs as cue patterns to relate news stories describing the same event. A centroid-based approach with a two-threshold scheme determines relevance (irrelevance) between a news story and a topic cluster. A leastrecently-used removal strategy models the time factor in such a way that older and unimportant terms will have no effect on clustering. Chen, Kuo and Su (2003) touched on event clustering in multilingual multi-document summarization. They showed that translation after clustering is better than translation before clustering, and translation defe</context>
</contexts>
<marker>Chen, Ku, 2002</marker>
<rawString>Chen, Hsin-Hsi and Ku, Lun-Wei (2002) “An NLP &amp; IR Approach to Topic Detection,” Topic Detection and Tracking: Event-Based Information Organization, James Allan, Jaime Carbonell, and Jonathan Yamron (Editors), Kluwer, pp. 243-264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hsin-Hsi Chen</author>
<author>June-Jei Kuo</author>
<author>Tsei-Chun Su</author>
</authors>
<title>Clustering and Visualization in a MultiLingual Multi-Document Summarization System,”</title>
<date>2003</date>
<booktitle>Proceedings of 25th European Conference on Information Retrieval Research, Lecture Notes in Computer Science, LNCS 2633,</booktitle>
<pages>266--280</pages>
<marker>Chen, Kuo, Su, 2003</marker>
<rawString>Chen, Hsin-Hsi; Kuo, June-Jei and Su, Tsei-Chun (2003) “Clustering and Visualization in a MultiLingual Multi-Document Summarization System,” Proceedings of 25th European Conference on Information Retrieval Research, Lecture Notes in Computer Science, LNCS 2633, pp. 266-280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan G Fiscus</author>
<author>George R Doddington</author>
</authors>
<title>Topic Detection and Tracking Evaluation Overview,” Topic Detection and Tracking: Event-Based Information Organization,</title>
<date>2002</date>
<pages>17--32</pages>
<publisher>Kluwer,</publisher>
<location>James</location>
<contexts>
<context position="11536" citStr="Fiscus and Doddington, 2002" startWordPosition="1843" endWordPosition="1846">f annotators and consider those consistent results as our answer set. Table 1 shows the distribution of the 13 focus events. Event Name Number of Documents Fly right negotiation between 20 Taiwan and Hong Kong Cause of air accident 57 Confirmation of air accident 6 Influence on stock market 27 Influence on insurance fee 11 Influence on China Airline 8 Influence on Peng-Hu 26 archipelagoes Punishment for persons in charge 10 News reporting 18 Wreckage found 28 Remains found 57 Rescue status 65 Solatium 34 Other 664 Table 1: Focus Events We adopt the metric used in Topic Detection and Tracking (Fiscus and Doddington, 2002). The evaluation is based on miss and false alarm rates. Both miss and false alarm are penalties. They can d _th(D1,D2) = 1 x th (3)dist(D)/w_size+ 1 2 dist(P)/w size+ —A—fd�./dw 160 140 120 100 40 20 80 60 0 145 155 165 175 185 195 205 215 225 235 245 Figure 1. Event Evolution of China Airlines Air Accident (2002/5/26 ~ 2002/9/4) measure more accurately the behavior of users who try to retrieve news stories. If miss or false alarm is too high, users will not be satisfied with the clustering results. The performance is characterized by a detection cost, , in terms of Cdet the probability of mi</context>
</contexts>
<marker>Fiscus, Doddington, 2002</marker>
<rawString>Fiscus, Jonathan G. and Doddington, George R. (2002) “Topic Detection and Tracking Evaluation Overview,” Topic Detection and Tracking: Event-Based Information Organization, James Allan, Jaime Carbonell, and Jonathan Yamron (Eds), Kluwer, pp. 17-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Fukumoto</author>
<author>Y Suzuki</author>
</authors>
<title>Event Tracking based on Domain Dependency,”</title>
<date>2000</date>
<booktitle>Proceedings of the 23rd ACM SIGIR 2000 Conference,</booktitle>
<pages>57--64</pages>
<contexts>
<context position="2692" citStr="Fukumoto and Suzuki (2000)" startWordPosition="381" endWordPosition="384">he same event. A centroid-based approach with a two-threshold scheme determines relevance (irrelevance) between a news story and a topic cluster. A leastrecently-used removal strategy models the time factor in such a way that older and unimportant terms will have no effect on clustering. Chen, Kuo and Su (2003) touched on event clustering in multilingual multi-document summarization. They showed that translation after clustering is better than translation before clustering, and translation deferred to sentence clustering, which reduces the propagation of translation errors, is most promising. Fukumoto and Suzuki (2000) proposed concepts of topic words and event words for event tracking. They introduced more semantic approach for feature selection than the approach of parts of speech. Wong, Kuo and Chen (2001) employed these concepts to select informative words for headline generation, and to rank the extracted sentences in multi-document summarization (Kuo, Wong, Lin, and Chen, 2002). Bagga and Baldwin (1998) proposed entitybased cross-document co-referencing which uses co-reference chains of each document to generate its summary and then use the summary rather than the whole article to select informative w</context>
<context position="15586" citStr="Fukumoto and Suzuki, 2000" startWordPosition="2512" endWordPosition="2515">. Window 1 2 3 4 size Cdet 0.012657 0.012647 0.012809 0.012942 Table 5. Detection Costs with Various Window Sizes Using Formula (3) (Initial Threshold = 0.05) 4 Event Clustering Using Event Words The co-reference chains in the above approach considered those features, such as person name, organization name, location, temporal expression and number expression. However, the important words “black box” or “rescue” in an air crash event are never shown in any co-reference chain. This section introduces the concepts of event words. Topic and event words were applied to topic tracking successfully (Fukumoto and Suzuki, 2000). The basic hypothesis is that an event word associated with a news article appears across paragraphs, but a topic word does not. In contrast, a topic word frequently appears across all news documents. Because the goal of event clustering is to extract all the events associated with a topic, those documents belonging to the same topic, e.g., China Airlines Air Accident, always have the similar topic words like “China Airlines”, “flight 611”, “air accident”, “Pen-Hu”, “Taiwan strait”, “rescue boats”, etc. Topic words seem to have no help in event clustering. Comparatively, each news article has</context>
</contexts>
<marker>Fukumoto, Suzuki, 2000</marker>
<rawString>Fukumoto, F. and Suzuki, Y. (2000) “Event Tracking based on Domain Dependency,” Proceedings of the 23rd ACM SIGIR 2000 Conference, pp. 57-64</rawString>
</citation>
<citation valid="true">
<authors>
<author>June-Jei Kuo</author>
<author>Hung-Chia Wong</author>
<author>Chuan-Jie Lin</author>
<author>Hsin-Hsi Chen</author>
</authors>
<title>Multi-Document Summarization Using Informative Words and Its Evaluation with a QA System,”</title>
<date>2002</date>
<booktitle>Proceedings of The Third International Conference on Intelligent Text Processing and Computational Linguistics, Lecture Notes in Computer Science, LNCS 2276,</booktitle>
<pages>391--401</pages>
<contexts>
<context position="3063" citStr="Kuo, Wong, Lin, and Chen, 2002" startWordPosition="435" endWordPosition="440">ment summarization. They showed that translation after clustering is better than translation before clustering, and translation deferred to sentence clustering, which reduces the propagation of translation errors, is most promising. Fukumoto and Suzuki (2000) proposed concepts of topic words and event words for event tracking. They introduced more semantic approach for feature selection than the approach of parts of speech. Wong, Kuo and Chen (2001) employed these concepts to select informative words for headline generation, and to rank the extracted sentences in multi-document summarization (Kuo, Wong, Lin, and Chen, 2002). Bagga and Baldwin (1998) proposed entitybased cross-document co-referencing which uses co-reference chains of each document to generate its summary and then use the summary rather than the whole article to select informative words to be the features of the document. Azzam, Humphreys, and Gaizauskas (1999) proposed a primitive model for text summarization using co-reference chains as well. Silber and McCoy (2002) proposed a text summarization model using lexical chains and showed that proper nouns and anaphora resolution is indispensable. The two semantics-based feature selection approaches,</context>
</contexts>
<marker>Kuo, Wong, Lin, Chen, 2002</marker>
<rawString>Kuo, June-Jei; Wong, Hung-Chia; Lin, Chuan-Jie and Chen, Hsin-Hsi (2002) “Multi-Document Summarization Using Informative Words and Its Evaluation with a QA System,” Proceedings of The Third International Conference on Intelligent Text Processing and Computational Linguistics, Lecture Notes in Computer Science, LNCS 2276, pp. 391-401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Manmatha</author>
<author>A Feng</author>
<author>James Allan</author>
</authors>
<title>A Critical Examination of TDT’s Cost Function,”</title>
<date>2002</date>
<booktitle>Proceedings of the 25th ACM SIGIR Conference,</booktitle>
<pages>403--404</pages>
<marker>Manmatha, Feng, Allan, 2002</marker>
<rawString>Manmatha, R.; Feng, A. and Allan, James (2002) “A Critical Examination of TDT’s Cost Function,” Proceedings of the 25th ACM SIGIR Conference, pp. 403-404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MUC</author>
</authors>
<date>1998</date>
<booktitle>Proceedings of 7th Message Understanding Conference,</booktitle>
<location>Fairfax, VA, 29</location>
<contexts>
<context position="5133" citStr="MUC, 1998" startWordPosition="744" endWordPosition="745">staff, 1999) A co-reference resolution procedure is first to find all the possible NP candidates. It includes word segmentation, named entity extraction, part of speech tagging, and noun phrase chunking. Then the candidates are partitioned into equivalence classes using the attributes such as word/phrase itself, parts of speech of head nouns, named entities, positions in a document, numbers (singular, plural, or unknown), pronouns, gender (female, male, or unknown), and semantics of head nouns. As the best F-measure of automatic co-reference resolution in English documents in MUC-7 was 61.8% (MUC, 1998), a corpus hand-tagged with named entities, and co-reference chains are prepared and employed to examine the real effects of co-reference chains in event clustering r. Headlines of a news story can be regarded as its short summary. That is, the words in the headline represent the content of a document in some sense. The co-reference chains that are initiated by the words in the headlines are assumed to have higher weights. A sentence which contains any words in a given co-reference chain is said to “cover” that chain. Those sentences which cover more coreference chains contain more information</context>
</contexts>
<marker>MUC, 1998</marker>
<rawString>MUC (1998) Proceedings of 7th Message Understanding Conference, Fairfax, VA, 29</rawString>
</citation>
<citation valid="true">
<authors>
<author>April</author>
</authors>
<date>1998</date>
<volume>1</volume>
<note>http://www.itl.nist.gov/iaui/894.02/ related_projects/muc/ index.html.</note>
<marker>April, 1998</marker>
<rawString>April - 1 May, 1998, http://www.itl.nist.gov/iaui/894.02/ related_projects/muc/ index.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Gregory Silber</author>
<author>Kathleen F McCoy</author>
</authors>
<title>Eficiently Computed Lexical Chains As an Intermediate Representation for Automatic Text Summarization.”</title>
<date>2002</date>
<journal>Journal of Association for Computational Linguistics,</journal>
<volume>28</volume>
<pages>487--496</pages>
<contexts>
<context position="3481" citStr="Silber and McCoy (2002)" startWordPosition="499" endWordPosition="502">h. Wong, Kuo and Chen (2001) employed these concepts to select informative words for headline generation, and to rank the extracted sentences in multi-document summarization (Kuo, Wong, Lin, and Chen, 2002). Bagga and Baldwin (1998) proposed entitybased cross-document co-referencing which uses co-reference chains of each document to generate its summary and then use the summary rather than the whole article to select informative words to be the features of the document. Azzam, Humphreys, and Gaizauskas (1999) proposed a primitive model for text summarization using co-reference chains as well. Silber and McCoy (2002) proposed a text summarization model using lexical chains and showed that proper nouns and anaphora resolution is indispensable. The two semantics-based feature selection approaches, i.e., co-reference chains and event words, are complementary in some sense. The former denotes equivalence classes of noun phrases, and the latter considers both nominal and verbal features, which appear across paragraphs. This paper will employ both co-reference chains and event words for temporal event clustering. An event clustering system using co-reference chains is described in Section 2. The evaluation meth</context>
</contexts>
<marker>Silber, McCoy, 2002</marker>
<rawString>Silber, H. Gregory and McCoy, Kathleen F. (2002) “Eficiently Computed Lexical Chains As an Intermediate Representation for Automatic Text Summarization.” Journal of Association for Computational Linguistics, Vol.28, No.4, pp. 487-496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong-Jia Wong</author>
<author>June-Jei Kuo</author>
<author>HsinHsi Chen</author>
</authors>
<title>Headline Generation for Summaries from Multiple Online Sources.”</title>
<date>2001</date>
<booktitle>Proceedings of 6th Natural Language Processing Pacific Rim Symposium,</booktitle>
<pages>653--660</pages>
<location>Tokyo,</location>
<marker>Wong, Kuo, Chen, 2001</marker>
<rawString>Wong, Hong-Jia; Kuo, June-Jei and Chen, HsinHsi (2001) “Headline Generation for Summaries from Multiple Online Sources.” Proceedings of 6th Natural Language Processing Pacific Rim Symposium, November 27-29 2001, Tokyo, Japan, pp. 653-660.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>