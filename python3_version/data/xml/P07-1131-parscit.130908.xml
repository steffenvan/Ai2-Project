<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000134">
<title confidence="0.99608">
Extracting Social Networks and Biographical Facts From Conversational
Speech Transcripts
</title>
<author confidence="0.830219">
Nanda Kambhatla
</author>
<affiliation confidence="0.781421">
IBM India Research Lab
</affiliation>
<address confidence="0.874023">
EGL, Domlur Ring Road
Bangalore - 560071, India
</address>
<email confidence="0.988849">
kambhatla@in.ibm.com
</email>
<author confidence="0.529908">
Hongyan Jing
</author>
<affiliation confidence="0.448474">
IBM T.J. Watson Research Center
</affiliation>
<address confidence="0.9687075">
1101 Kitchawan Road
Yorktown Heights, NY 10598
</address>
<email confidence="0.998901">
hjing@us.ibm.com
</email>
<author confidence="0.710442">
Salim Roukos
</author>
<affiliation confidence="0.601085">
IBM T.J. Watson Research Center
</affiliation>
<address confidence="0.9700465">
1101 Kitchawan Road
Yorktown Heights, NY 10598
</address>
<email confidence="0.999497">
roukos@us.ibm.com
</email>
<sectionHeader confidence="0.998604" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999425">
We present a general framework for
automatically extracting social networks
and biographical facts from conversational
speech. Our approach relies on fusing
the output produced by multiple informa-
tion extraction modules, including entity
recognition and detection, relation detec-
tion, and event detection modules. We
describe the specific features and algo-
rithmic refinements effective for conver-
sational speech. These cumulatively in-
crease the performance of social network
extraction from 0.06 to 0.30 for the devel-
opment set, and from 0.06 to 0.28 for the
test set, as measured by f-measure on the
ties within a network. The same frame-
work can be applied to other genres of text
— we have built an automatic biography
generation system for general domain text
using the same approach.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999899928571429">
A social network represents social relationships
between individuals or organizations. It consists
of nodes and ties. Nodes are individual actors
within the networks, generally a person or an or-
ganization. Ties are the relationships between the
nodes. Social network analysis has become a key
technique in many disciplines, including modern
sociology and information science.
In this paper, we present our system for au-
tomatically extracting social networks and bio-
graphical facts from conversational speech tran-
scripts by integrating the output of different IE
modules. The IE modules are the building blocks;
the fusing module depicts the ways of assembling
</bodyText>
<page confidence="0.965368">
1040
</page>
<bodyText confidence="0.999821961538462">
these building blocks. The final output depends on
which fundamental IE modules are used and how
their results are integrated.
The contributions of this work are two fold. We
propose a general framework for extracting social
networks and biographies from text that applies to
conversational speech as well as other genres, in-
cluding general newswire stories. Secondly, we
present specific methods that proved effective for
us for improving the performance of IE systems on
conversational speech transcripts. These improve-
ments include feature engineering and algorithmic
revisions that led to a nearly five-fold performance
increase for both development and test sets.
In the next section, we present our framework
for extracting social networks and other biograph-
ical facts from text. In Section 3, we discuss the
refinements we made to our IE modules in order
to reliably extract information from conversational
speech transcripts. In Section 4, we describe the
experiments, evaluation metrics, and the results of
social network and biography extraction. In Sec-
tion 5, we show the results of applying the frame-
work to other genres of text. Finally, we discuss
related work and conclude with lessons learned
and future work.
</bodyText>
<sectionHeader confidence="0.983122" genericHeader="introduction">
2 The General Framework
</sectionHeader>
<bodyText confidence="0.9990695">
For extraction of social networks and biographi-
cal facts, our approach relies on three standard IE
modules — entity detection and recognition, rela-
tion detection, and event detection — and a fusion
module that integrates the output from the three IE
systems.
</bodyText>
<subsectionHeader confidence="0.843471">
2.1 Entity, Relation, and Event Detection
</subsectionHeader>
<bodyText confidence="0.9999345">
We use the term entity to refer to a person, an or-
ganization, or other real world entities, as adopted
</bodyText>
<subsectionHeader confidence="0.3268655">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1040–1047,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.999701571428571">
in the Automatic Content Extraction (ACE) Work-
shops (ACE, 2005). A mention is a reference to
a real world entity. It can be named (e.g. “John
Lennon”), nominal (e.g. “mother”), or pronomi-
nal (e.g. “she”).
Entity detection is generally accomplished in
two steps: first, a mention detection module iden-
tifies all the mentions of interest; second, a co-
reference module merges mentions that refer to the
same entity into a single co-reference chain.
A relation detection system identifies (typi-
cally) binary relationships between pairs of men-
tions. For instance, for the sentence “I’m in New
York”, the following relation exists: locatedAt (I,
New York).
An event detection system identifies events of
interest and the arguments of the event. For ex-
ample, from the sentence “John married Eva in
1940”, the system should identify the marriage
event, the people who got married and the time
of the event.
The latest ACE evaluations involve all of the
above tasks. However, as shown in the next sec-
tion, our focus is quite different from ACE —
we are particularly interested in improving perfor-
mance for conversational speech and building on
top of ACE tasks to produce social networks and
biographies.
</bodyText>
<subsectionHeader confidence="0.996707">
2.2 Fusion Module
</subsectionHeader>
<bodyText confidence="0.999960285714286">
The fusion module merges the output from IE
modules to extract social networks and biographi-
cal facts. For example, if a relation detection sys-
tem has identified the relation motherOf (mother,
my) from the input sentence “my mother is a
cook”, and if an entity recognition module has
generated entities referenced by the mentions {my,
</bodyText>
<equation confidence="0.7834165">
Josh, me, I, I, } and {mother, she, her, her,
Rosa }, then by replacing my and mother with
</equation>
<bodyText confidence="0.999807166666667">
the named mentions within the same co-reference
chains, the fusion module produces the follow-
ing nodes and ties in a social network: motherOf
(Rosa, Josh).
We generate the nodes of social networks by se-
lecting all the PERSON entities produced by the
entity recognition system. Typically, we only in-
clude entities that contain at least one named men-
tion. To identify ties between nodes, we retrieve
all relations that indicate social relationships be-
tween a pair of nodes in the network.
We extract biographical profiles by selecting the
events (extracted by the event extraction module)
and corresponding relations (extracted by the rela-
tion extraction module) that involve a given indi-
vidual as an argument. When multiple documents
are used, then we employ a cross-document co-
reference system.
</bodyText>
<sectionHeader confidence="0.7115395" genericHeader="method">
3 Improving Performance for
Conversational Speech Transcripts
</sectionHeader>
<bodyText confidence="0.9991044">
Extracting information from conversational
speech transcripts is uniquely challenging. In this
section, we describe the data collection used in
our experiments, and explain specific techniques
we used to improve IE performance on this data.
</bodyText>
<subsectionHeader confidence="0.999629">
3.1 Conversational Speech Collection
</subsectionHeader>
<bodyText confidence="0.999620962962963">
We use a corpus of videotaped, digitized oral in-
terviews with Holocaust survivors in our experi-
ments. This data was collected by the USC Shoah
Foundation Institute (formerly known as the Vi-
sual History Foundation), and has been used in
many research activities under the Multilingual
Access to Large Spoken Archives (MALACH)
project (Gustman et al., 2002; Oard et al., 2004).
The collection contains oral interviews in 32 lan-
guages from 52,000 survivors, liberators, rescuers
and witnesses of the Holocaust.
This data is very challenging. Besides the usual
characteristics of conversational speech, such as
speaker turns and speech repairs, the interview
transcripts contain a large percentage of ungram-
matical, incoherent, or even incomprehensible
clauses (a sample interview segment is shown in
Figure 1). In addition, each interview covers many
people and places over a long time period, which
makes it even more difficult to extract social net-
works and biographical facts.
speaker2 in on that ninth of Novem-
ber nineteen hundred thirty eight I was
with my parents at home we heard
not through the we heard even through
the windows the crashing of glass the
crashing of and and they are our can’t
</bodyText>
<figureCaption confidence="0.994855">
Figure 1: Sample interview segment.
</figureCaption>
<subsectionHeader confidence="0.856547">
3.2 The Importance of Co-reference
Resolution
</subsectionHeader>
<bodyText confidence="0.999796">
Our initial attempts at social network extraction
for the above data set resulted in a very poor score
</bodyText>
<page confidence="0.9584">
1041
</page>
<bodyText confidence="0.999992210526316">
of 0.06 f-measure for finding the relations within
a network (as shown in Table 3 as baseline perfor-
mance).
An error analysis indicated poor co-reference
resolution to be the chief culprit for the low per-
formance. For instance, suppose we have two
clauses: “his mother’s name is Mary” and “his
brother Mark went to the army”. Further sup-
pose that “his” in the first clause refers to a
person named “John” and “his” in the second
clause refers to a person named “Tim”. If the
co-reference system works perfectly, the system
should find a social network involving four peo-
ple: {John, Tim, Mary, Mark}, and the ties: moth-
erOf (Mary, John), and brotherOf (Mark, Tim).
However, if the co-reference system mistakenly
links “John” to “his” in the second clause and links
“Tim” to “his” in the first clause, then we will still
have a network with four people, but the ties will
be: motherOf (Mary, Tim), and brotherOf (Mark,
John), which are completely wrong. This example
shows that co-reference errors involving mentions
that are relation arguments can lead to very bad
performance in social network extraction.
Our existing co-reference module is a state-of-
the-art system that produces very competitive re-
sults compared to other existing systems (Luo et
al., 2004). It traverses the document from left to
right and uses a mention-synchronous approach to
decide whether a mention should be merged with
an existing entity or start a new entity.
However, our existing system has shortcomings
for this data: the system lacks features for han-
dling conversational speech, and the system of-
ten makes mistakes in pronoun resolution. Re-
solving pronominal references is very important
for extracting social networks from conversational
speech, as illustrated in the previous example.
</bodyText>
<subsectionHeader confidence="0.999751">
3.3 Improving Co-reference for
Conversational Speech
</subsectionHeader>
<bodyText confidence="0.999990333333333">
We developed a new co-reference resolution sys-
tem for conversational speech transcripts. Simi-
lar to many previous works on co-reference (Ng,
2005), we cast the problem as a classification task
and solve it in two steps: (1) train a classifier to
determine whether two mentions are co-referent or
not, and (2) use a clustering algorithm to partition
the mentions into clusters, based on the pairwise
predictions.
We added many features to our model specifi-
cally designed for conversational speech, and sig-
nificantly improved the agglomerative clustering
used for co-reference, including integrating rela-
tions as constraints, and designing better cluster
linkage methods and clustering stopping criteria.
</bodyText>
<subsectionHeader confidence="0.9547325">
3.3.1 Adding Features for Conversational
Speech
</subsectionHeader>
<bodyText confidence="0.999176431818182">
We added many features to our model specifi-
cally designed for conversational speech:
Speaker role identification. In manual tran-
scripts, the speaker turns are given and each
speaker is labeled differently (e.g. “speaker1”,
“speaker2”), but the identity of the speaker is not
given. An interview typically involves 2 or more
speakers and it is useful to identify the roles of
each speaker (e.g. interviewer, interviewee, etc.).
For instance, ”you” spoken by the interviewer is
likely to be linked with ”I” spoken by the inter-
viewee, but ”you” spoken by the third person in
the interview is more likely to be referring to the
interviewer than to the interviewee.
We developed a program to identify the speaker
roles. The program classifies the speakers into
three categories: interviewer, interviewee, and
others. The algorithm relies on three indicators
— number of turns by each speaker, difference in
number of words spoken by each speaker, and the
ratio of first-person pronouns such as “I”, “me”,
and “we” vs. second-person pronouns such as
“you” and “your”. This speaker role identifica-
tion program works very well when we checked
the results on the development and test set — the
interviewers and survivors in all the documents in
the development set were correctly identified.
Speaker turns. Using the results from the
speaker role identification program, we enrich cer-
tain features with speaker turn information. For
example, without this information, the system can-
not distinguish “I” spoken by an interviewer from
“I” spoken by an interviewee.
Spelling features for speech transcripts. We
add additional spelling features so that mentions
such as “Cyla C Y L A Lewin” and “Cyla Lewin”
are considered as exact matches. Names with
spelled-out letters occur frequently in our data col-
lection.
Name Patterns. We add some features that
capture frequent syntactic structures that speakers
use to express names, such as “her name is Irene”,
“my cousin Mark”, and “interviewer Ellen”.
Pronoun features. To improve the perfor-
</bodyText>
<page confidence="0.979834">
1042
</page>
<bodyText confidence="0.999397235294118">
mance on pronouns, we add features such as the
speaker turns of the pronouns, whether the two
pronouns agree in person and number, whether
there exist other mentions between them, etc.
Other miscellaneous features. We also in-
clude other features such as gender, token dis-
tance, sentence distance, and mention distance.
We trained a maximum-entropy classifier using
these features. For each pair of mentions, the clas-
sifier outputs the probability that the two mentions
are co-referent.
We also modified existing features to make
them more applicable to conversational speech.
For instance, we added pronoun-distance features
taking into account the presence of other pronom-
inal references in between (if so, the types of the
pronouns), other mentions in between, etc.
</bodyText>
<subsectionHeader confidence="0.820499">
3.3.2 Improving Agglomerative Clustering
</subsectionHeader>
<bodyText confidence="0.999347075949367">
We use an agglomerative clustering approach
for partitioning mentions into entities. This is a
bottom-up approach which joins the closest pair
of clusters (i.e., entities) first. Initially, each men-
tion is placed into its own cluster. If we have N
mentions to cluster, we start with N clusters.
The intuition behind choosing the agglomera-
tive method is to merge the most confident pairs
first, and use the properties of existing clusters to
constrain future clustering. This seems to be espe-
cially important for our data collection, since con-
versational speech tends to have a lot of repetitions
or local structures that indicate co-reference. In
such cases, it is beneficial to merge these closely
related mentions first.
Cluster linkage method. In agglomerative
clustering, each cycle merges two clusters into a
single cluster, thus reducing the number of clus-
ters by one. We need to decide upon a method of
measuring the distance between two clusters.
At each cycle, the two mentions with the high-
est co-referent probability are linked first. This re-
sults in the merging of the two clusters that contain
these two mentions.
We improve upon this method by imposing min-
imal distance criteria between clusters. Two clus-
ters C1 and C2 can be combined only if the dis-
tance between all the mentions from C1 and all
the mentions from C2 is above the minimal dis-
tance threshold. For instance, suppose C1 =
{he, father}, and C2 = {he, brother}, and “he”
from C1 and “he” from C2 has the highest linkage
probability. The standard single linkage method
will combine these two clusters, despite the fact
that “father” and “brother” are very unlikely to
be linked. Imposing minimal distance criteria
can solve this problem and prevent the linkage of
clusters which contain very dissimilar mentions.
In practice, we used multiple minimal distance
thresholds, such as minimal distance between two
named mentions and minimal distance between
two nominal mentions.
We chose not to use complete or average link-
age methods. In our data collection, the narrations
contain a lot of pronouns and the focus tends to
be very local. Whereas the similarity model may
be reasonably good at predicting the distance be-
tween two pronouns that are close to each other, it
is not good at predicting the distance between pro-
nouns that are furthur apart. Therefore, it seems
more reasonable to use single linkage method with
modifications than complete or average linkage
methods.
Using relations to constrain clustering. An-
other novelty of our co-reference system is the
use of relations for constraining co-reference. The
idea is that two clusters should not be merged if
such merging will introduce contradictory rela-
tions. For instance, if we know that person entity
A is the mother of person entity B, and person en-
tity C is the sister of B, then A and C should not
be linked since the resulting entity will be both the
mother and the sister of B.
We construct co-existent relation sets from the
training data. For any two pairs of entities, we col-
lect all the types of relations that exist between
them. These types of relations are labeled as
co-existent. For instance, “motherOf” and “par-
entOf” can co-exist, but “motherOf” and “sis-
terOf” cannot. By using these relation constraints,
the system refrains from generating contradictory
relations in social networks.
Speed improvement. Suppose the number of
mentions is N, the time complexity of simple link-
age method is O(N2). With the minimal dis-
tance criteria, the complexity is O(N3). However,
N can be dramatically reduced for conversational
transcripts by first linking all the first-person pro-
nouns by each speaker.
</bodyText>
<sectionHeader confidence="0.999616" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9845265">
In this section, we describe the experimental setup
and present sample outputs and evaluation results.
</bodyText>
<page confidence="0.913888">
1043
</page>
<table confidence="0.999302">
Train Dev Test
Words 198k 73k 255k
Mentions 43k 16k 56k
Relations 7K 3k 8k
</table>
<tableCaption confidence="0.996294">
Table 2: Experimental Data Sets.
</tableCaption>
<subsectionHeader confidence="0.973051">
4.1 Data Annotation
</subsectionHeader>
<bodyText confidence="0.999935846153846">
The data used in our experiments consist of partial
or complete English interviews of Holocaust sur-
vivors. The input to our system is transcripts of
interviews.
We manually annotated manual transcripts with
entities, relations, and event categories, specifi-
cally designed for this task and the results of care-
ful data analysis. The annotation was performed
by a single annotator over a few months. The an-
notation categories for entities, events, and rela-
tions are shown in Table 1. Please note that the
event and relation definitions are slightly different
than the definitions in ACE.
</bodyText>
<subsectionHeader confidence="0.989667">
4.2 Training and Test Sets
</subsectionHeader>
<bodyText confidence="0.9999628125">
We divided the data into training, development,
and test data sets. Table 2 shows the size of each
data set. The training set includes transcripts of
partial interviews. The development set consists
of 5 complete interviews, and the test set con-
sists of 15 complete interviews. The reason that
the training set contains only partial interviews is
due to the high cost of transcription and annota-
tion. Since those partial interviews had already
been transcribed for speech recognition purpose,
we decided to reuse them in our annotation. In ad-
dition, we transcribed and annotated 20 complete
interviews (each interview is about 2 hours) for
building the development and test sets, in order
to give a more accurate assessment of extraction
performance.
</bodyText>
<subsectionHeader confidence="0.998727">
4.3 Implementation
</subsectionHeader>
<bodyText confidence="0.999966592592593">
We developed the initial entity detection, rela-
tion detection, and event detection systems using
the same techniques as our submission systems to
ACE (Florian et al., 2004). Our submission sys-
tems use statistical approaches, and have ranked
in the top tier in ACE evaluations. We easily built
the models for our application by retraining exist-
ing systems with our training set.
The entity detection task is accomplished in two
steps: mention detection and co-reference resolu-
tion. The mention detection is formulated as a la-
beling problem, and a maximum-entropy classifier
is trained to identify all the mentions.
Similarly, relation detection is also cast as a
classification problem — for each pair of men-
tions, the system decides which type of relation
exists between them. It uses a maximum-entropy
classifier and various lexical, contextual, and syn-
tactic features for such predications.
Event detection is accomplished in two steps:
first, identifying the event anchor words using an
approach similar to mention detection; then, iden-
tifying event arguments using an approach similar
to relation detection.
The co-reference resolution system for conver-
sational speech and the fusion module were devel-
oped anew.
</bodyText>
<subsectionHeader confidence="0.998221">
4.4 The Output
</subsectionHeader>
<bodyText confidence="0.9989345">
The system aims to extract the following types of
information:
</bodyText>
<listItem confidence="0.9942458">
• The social network of the survivor.
• Important biographical facts about each per-
son in the social network.
• Track the movements of the survivor and
other individuals in the social network.
</listItem>
<bodyText confidence="0.999254166666667">
Figure 2 shows a sample social network ex-
tracted by the system (only partial of the network
is shown). Figure 3 shows sample biographical
facts and movement summaries extracted by the
system. In general, we focus more on higher pre-
cision than recall.
</bodyText>
<subsectionHeader confidence="0.972769">
4.5 Evaluation
</subsectionHeader>
<bodyText confidence="0.996176">
In this paper, we focus only on the evaluation
of social network extraction. We first describe
the metrics for social network evaluation and then
present the results of the system.
</bodyText>
<figureCaption confidence="0.986732">
Figure 2: Social network extracted by the system.
</figureCaption>
<page confidence="0.904013">
1044
</page>
<table confidence="0.9999075625">
Entity (12) Event (8) Relation (34)
Social Rels (12) Event Args (8) Bio Facts (14)
AGE CUSTODY aidgiverOf affectedBy bornAt
COUNTRY DEATH auntOf agentOf bornOn
DATE HIDING cousinOf participantIn citizenOf
DATEREF LIBERATION fatherOf timeOf diedAt
DURATION MARRIAGE friendOf travelArranger diedOn
GHETTOORCAMP MIGRATION grandparentOf travelFrom employeeOf
OCCUPATION SURVIVAL motherOf travelPerson hasProperty
ORGANIZATION VIOLENCE otherRelativeOf travelTo locatedAt
OTHERLOC parentOf managerOf
PEOPLE siblingOf memberOf
PERSON spouseOf near
SALUTATION uncleOf partOf
partOfMany
resideIn
</table>
<tableCaption confidence="0.99974">
Table 1: Annotation Categories for Entities, Events, and Relations.
</tableCaption>
<subsectionHeader confidence="0.751011">
Sidonia Lax:
</subsectionHeader>
<bodyText confidence="0.900984">
date of birth: June the eighth nineteen twenty
seven
</bodyText>
<figure confidence="0.5783435">
Movements:
Moved To: Auschwitz
Moved To: United States
... ...
</figure>
<figureCaption confidence="0.915397">
Figure 3: Biographical facts and movement sum-
</figureCaption>
<bodyText confidence="0.980287863636364">
maries extracted by the system.
To compare two social networks, we first need
to match the nodes and ties between the networks.
Two nodes (i.e., entities) are matched if they have
the same canonical name. Two ties (i.e., edges or
relations) are matched if these three criteria are
met: they contain the same type of relations, the
arguments of the relation are the same, and the or-
der of the arguments are the same if the relation is
unsymmetrical.
We define the the following measurements for
social network evaluation: the precision for nodes
(or ties) is the ratio of common nodes (or ties) in
the two networks to the total number of nodes (or
ties) in the system output, the recall for nodes (or
ties) is the ratio of common nodes (or ties) in the
two networks to the total number of nodes/ties in
the reference output, and the f-measure for nodes
(or ties) is the harmonic mean of precision and re-
call for nodes (or ties). The f-measure for ties in-
dicates the overall performance of social network
extraction.
</bodyText>
<table confidence="0.99845025">
F-mea Dev Test
Baseline New Baseline New
Nodes 0.59 0.64 0.62 0.66
Ties 0.06 0.30 0.06 0.28
</table>
<tableCaption confidence="0.999871">
Table 3: Performance of social network extraction.
</tableCaption>
<bodyText confidence="0.999623851851852">
Table 3 shows the results of social network ex-
traction. The new co-reference approach improves
the performance for f-measure on ties by five-fold
on development set and by nearly five-fold for test
set.
We also tested the system using automatic tran-
scripts by our speech recognition system. Not sur-
prisingly, the result is much worse: the nodes f-
measure is 0.11 for the test set, and the system
did not find any relations. A few factors are ac-
countable for this low performance: (1) Speech
recognition is very challenging for this data set,
since the testimonies contained elderly, emotional,
accented speech. Given that the speech recogni-
tion system fails to recognize most of the person
names, extraction of social networks is difficult.
(2) The extraction systems perform worse on au-
tomatic transcripts, due to the quality of the auto-
matic transcript, and the discrepancy between the
training and test data. (3) Our measurements are
very strict, and no partial credit is given to partially
correct entities or relations.
We decided not to present the evaluation results
of the individual components since the perfor-
mance of individual components are not at all in-
dicative of the overall performance. For instance,
a single pronoun co-reference error might slighlty
</bodyText>
<page confidence="0.974445">
1045
</page>
<bodyText confidence="0.998651333333333">
change the co-reference score, but can introduce a
serious error in the social network, as shown in the
example in Section 3.2.
</bodyText>
<sectionHeader confidence="0.966223" genericHeader="method">
5 Biography Generation from General
Domain Text
</sectionHeader>
<bodyText confidence="0.999879090909091">
We have applied the same framework to biogra-
phy generation from general news articles. This
general system also contains three fundamental IE
systems and a fusion module, similar to the work
presented in the paper. The difference is that the IE
systems are trained on general news text using dif-
ferent categories of entities, relations, and events.
A sample biography output extracted from
TDT5 English documents is shown in Figure 4.
The numbers in brackets indicate the corpus count
of the facts.
</bodyText>
<sectionHeader confidence="0.7336295" genericHeader="method">
Saddam Hussein:
Basic Information:
</sectionHeader>
<bodyText confidence="0.897875">
citizenship: Iraq [203]
occupation: president [4412], leader [1792],
dictator [664],...
relative: odai [89], qusay [65], uday [65],...
Life Events:
places been to: bagdad [403], iraq [270],
palaces [149]...
Organizations associated with: manager of
baath party [1000], ...
Custody Events: Saddam was arrested [52],
Communication Events: Saddam said [3587]
... ...
</bodyText>
<figureCaption confidence="0.986079">
Figure 4: Sample biography output.
</figureCaption>
<sectionHeader confidence="0.999966" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.9999373">
While there has been previous work on extracting
social networks from emails and the web (Culotta
et al., 2004), we believe this is the first paper to
present a full-fledged system for extracting social
networks from conversational speech transcripts.
Similarly, most of the work on co-reference res-
olution has not focused on conversational speech.
(Ji et al., 2005) uses semantic relations to refine
co-reference decisions, but in a approach different
from ours.
</bodyText>
<sectionHeader confidence="0.99237" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999992375">
We have described a novel approach for extracting
social networks, biographical facts, and movement
summaries from transcripts of oral interviews with
Holocaust survivors. We have improved the per-
formance of social network extraction five-fold,
compared to a baseline system that already uses
state-of-the-art technology. In particular, we im-
proved the performance of co-reference resolution
for conversational speech, by feature engineering
and improving the clustering algorithm.
Although our application data consists of con-
versational speech transcripts in this paper, the
same extraction approach can be applied to
general-domain text as well. Extracting general,
rich social networks is very important in many ap-
plications, since it provides the knowledge of who
is connected to whom and how they are connected.
There are many interesting issues involved in
biography generation from a large data collection,
such as how to resolve contradictions. The counts
from the corpus certainly help to filter out false
information which would otherwise be difficult to
filter. But better technology at detecting and re-
solving contradictions will definitely be beneficial.
</bodyText>
<sectionHeader confidence="0.973777" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.999982375">
We would like to thank Martin Franz and Bhuvana
Ramabhadran for their help during this project.
This project is funded by NSF under the Infor-
mation Technology Research (ITR) program, NSF
IIS Award No. 0122466. Any opinions, findings
and conclusions or recommendations expressed in
this material are those of the authors and do not
necessarily reflect the views of the NSF.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999831882352941">
2005. Automatic content extraction.
http://www.nist.gov/speech/tests/ace/.
Aron Culotta, Ron Bekkerman, and Andrew McCal-
lum. 2004. Extracting social networks and con-
tact information from email and the web. In CEAS,
Mountain View, CA.
Radu Florian, Hany Hassan, Abraham Ittycheriah,
Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo,
Nicolas Nicolov, and Salim Roukos. 2004. A sta-
tistical model for multilingual entity detection and
tracking. In Proceedings of. HLT-NAACL 2004.
Samuel Gustman, Dagobert Soergeland Douglas Oard,
William Byrne, Michael Picheny, Bhuvana Ramab-
hadran, and Douglas Greenberg. 2002. Support-
ing access to large digital oral history archives. In
Proceedings of the Joint Conference on Digital Li-
braries, pages 18–27.
</reference>
<page confidence="0.75947">
1046
</page>
<reference confidence="0.99936345">
Heng Ji, David Westbrook, and Ralph Grishman. 2005.
Using semantic relations to refine coreference deci-
sions. In Proceedings of HLT/EMNLP’05, Vancou-
ver, B.C., Canada.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the bell tree. In Proceedings of the 42nd An-
nual Meeting of the Association for Computational
Linguistics (ACL2004), pages 135–142, Barcelona,
Spain.
Vincent Ng. 2005. Machine learning for coreference
resolution: From local classification to global rank-
ing. In Proceedings ofACL’04.
D. Oard, D. Soergel, D. Doermann, X. Huang, G.C.
Murray, J. Wang, B. Ramabhadran, M. Franz,
S. Gustman, J. Mayfield, L. Kharevych, and
S. Strassel. 2004. Building an information re-
trieval test collection for spontaneous conversational
speech. In Proceedings ofSIGIR’04, Sheffield, U.K.
</reference>
<page confidence="0.992027">
1047
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.646476">
<title confidence="0.9981795">Extracting Social Networks and Biographical Facts From Conversational Speech Transcripts</title>
<author confidence="0.998861">Nanda Kambhatla</author>
<affiliation confidence="0.8454685">IBM India Research Lab EGL, Domlur Ring Road</affiliation>
<address confidence="0.981855">Bangalore - 560071, India</address>
<email confidence="0.999782">kambhatla@in.ibm.com</email>
<author confidence="0.998373">Hongyan Jing</author>
<affiliation confidence="0.999973">IBM T.J. Watson Research Center</affiliation>
<address confidence="0.9905545">1101 Kitchawan Road Yorktown Heights, NY 10598</address>
<email confidence="0.998734">hjing@us.ibm.com</email>
<author confidence="0.999221">Salim Roukos</author>
<affiliation confidence="0.999951">IBM T.J. Watson Research Center</affiliation>
<address confidence="0.9917125">1101 Kitchawan Road Yorktown Heights, NY 10598</address>
<email confidence="0.999698">roukos@us.ibm.com</email>
<abstract confidence="0.999812761904762">We present a general framework for automatically extracting social networks and biographical facts from conversational speech. Our approach relies on fusing the output produced by multiple information extraction modules, including entity recognition and detection, relation detection, and event detection modules. We describe the specific features and algorithmic refinements effective for conversational speech. These cumulatively increase the performance of social network extraction from 0.06 to 0.30 for the development set, and from 0.06 to 0.28 for the test set, as measured by f-measure on the ties within a network. The same framework can be applied to other genres of text — we have built an automatic biography generation system for general domain text using the same approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>Automatic content extraction.</title>
<date>2005</date>
<note>http://www.nist.gov/speech/tests/ace/.</note>
<marker>2005</marker>
<rawString>2005. Automatic content extraction. http://www.nist.gov/speech/tests/ace/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Ron Bekkerman</author>
<author>Andrew McCallum</author>
</authors>
<title>Extracting social networks and contact information from email and the web. In CEAS,</title>
<date>2004</date>
<location>Mountain View, CA.</location>
<contexts>
<context position="25076" citStr="Culotta et al., 2004" startWordPosition="3980" endWordPosition="3983"> numbers in brackets indicate the corpus count of the facts. Saddam Hussein: Basic Information: citizenship: Iraq [203] occupation: president [4412], leader [1792], dictator [664],... relative: odai [89], qusay [65], uday [65],... Life Events: places been to: bagdad [403], iraq [270], palaces [149]... Organizations associated with: manager of baath party [1000], ... Custody Events: Saddam was arrested [52], Communication Events: Saddam said [3587] ... ... Figure 4: Sample biography output. 6 Related Work While there has been previous work on extracting social networks from emails and the web (Culotta et al., 2004), we believe this is the first paper to present a full-fledged system for extracting social networks from conversational speech transcripts. Similarly, most of the work on co-reference resolution has not focused on conversational speech. (Ji et al., 2005) uses semantic relations to refine co-reference decisions, but in a approach different from ours. 7 Conclusions and Future Work We have described a novel approach for extracting social networks, biographical facts, and movement summaries from transcripts of oral interviews with Holocaust survivors. We have improved the performance of social ne</context>
</contexts>
<marker>Culotta, Bekkerman, McCallum, 2004</marker>
<rawString>Aron Culotta, Ron Bekkerman, and Andrew McCallum. 2004. Extracting social networks and contact information from email and the web. In CEAS, Mountain View, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Florian</author>
<author>Hany Hassan</author>
<author>Abraham Ittycheriah</author>
<author>Hongyan Jing</author>
<author>Nanda Kambhatla</author>
<author>Xiaoqiang Luo</author>
<author>Nicolas Nicolov</author>
<author>Salim Roukos</author>
</authors>
<title>A statistical model for multilingual entity detection and tracking.</title>
<date>2004</date>
<booktitle>In Proceedings of. HLT-NAACL</booktitle>
<contexts>
<context position="18764" citStr="Florian et al., 2004" startWordPosition="2980" endWordPosition="2983">nly partial interviews is due to the high cost of transcription and annotation. Since those partial interviews had already been transcribed for speech recognition purpose, we decided to reuse them in our annotation. In addition, we transcribed and annotated 20 complete interviews (each interview is about 2 hours) for building the development and test sets, in order to give a more accurate assessment of extraction performance. 4.3 Implementation We developed the initial entity detection, relation detection, and event detection systems using the same techniques as our submission systems to ACE (Florian et al., 2004). Our submission systems use statistical approaches, and have ranked in the top tier in ACE evaluations. We easily built the models for our application by retraining existing systems with our training set. The entity detection task is accomplished in two steps: mention detection and co-reference resolution. The mention detection is formulated as a labeling problem, and a maximum-entropy classifier is trained to identify all the mentions. Similarly, relation detection is also cast as a classification problem — for each pair of mentions, the system decides which type of relation exists between t</context>
</contexts>
<marker>Florian, Hassan, Ittycheriah, Jing, Kambhatla, Luo, Nicolov, Roukos, 2004</marker>
<rawString>Radu Florian, Hany Hassan, Abraham Ittycheriah, Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo, Nicolas Nicolov, and Salim Roukos. 2004. A statistical model for multilingual entity detection and tracking. In Proceedings of. HLT-NAACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Gustman</author>
<author>Dagobert Soergeland Douglas Oard</author>
<author>William Byrne</author>
<author>Michael Picheny</author>
<author>Bhuvana Ramabhadran</author>
<author>Douglas Greenberg</author>
</authors>
<title>Supporting access to large digital oral history archives.</title>
<date>2002</date>
<booktitle>In Proceedings of the Joint Conference on Digital Libraries,</booktitle>
<pages>18--27</pages>
<contexts>
<context position="6865" citStr="Gustman et al., 2002" startWordPosition="1067" endWordPosition="1070">nformation from conversational speech transcripts is uniquely challenging. In this section, we describe the data collection used in our experiments, and explain specific techniques we used to improve IE performance on this data. 3.1 Conversational Speech Collection We use a corpus of videotaped, digitized oral interviews with Holocaust survivors in our experiments. This data was collected by the USC Shoah Foundation Institute (formerly known as the Visual History Foundation), and has been used in many research activities under the Multilingual Access to Large Spoken Archives (MALACH) project (Gustman et al., 2002; Oard et al., 2004). The collection contains oral interviews in 32 languages from 52,000 survivors, liberators, rescuers and witnesses of the Holocaust. This data is very challenging. Besides the usual characteristics of conversational speech, such as speaker turns and speech repairs, the interview transcripts contain a large percentage of ungrammatical, incoherent, or even incomprehensible clauses (a sample interview segment is shown in Figure 1). In addition, each interview covers many people and places over a long time period, which makes it even more difficult to extract social networks a</context>
</contexts>
<marker>Gustman, Oard, Byrne, Picheny, Ramabhadran, Greenberg, 2002</marker>
<rawString>Samuel Gustman, Dagobert Soergeland Douglas Oard, William Byrne, Michael Picheny, Bhuvana Ramabhadran, and Douglas Greenberg. 2002. Supporting access to large digital oral history archives. In Proceedings of the Joint Conference on Digital Libraries, pages 18–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>David Westbrook</author>
<author>Ralph Grishman</author>
</authors>
<title>Using semantic relations to refine coreference decisions.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP’05,</booktitle>
<location>Vancouver, B.C.,</location>
<contexts>
<context position="25331" citStr="Ji et al., 2005" startWordPosition="4019" endWordPosition="4022">bagdad [403], iraq [270], palaces [149]... Organizations associated with: manager of baath party [1000], ... Custody Events: Saddam was arrested [52], Communication Events: Saddam said [3587] ... ... Figure 4: Sample biography output. 6 Related Work While there has been previous work on extracting social networks from emails and the web (Culotta et al., 2004), we believe this is the first paper to present a full-fledged system for extracting social networks from conversational speech transcripts. Similarly, most of the work on co-reference resolution has not focused on conversational speech. (Ji et al., 2005) uses semantic relations to refine co-reference decisions, but in a approach different from ours. 7 Conclusions and Future Work We have described a novel approach for extracting social networks, biographical facts, and movement summaries from transcripts of oral interviews with Holocaust survivors. We have improved the performance of social network extraction five-fold, compared to a baseline system that already uses state-of-the-art technology. In particular, we improved the performance of co-reference resolution for conversational speech, by feature engineering and improving the clustering a</context>
</contexts>
<marker>Ji, Westbrook, Grishman, 2005</marker>
<rawString>Heng Ji, David Westbrook, and Ralph Grishman. 2005. Using semantic relations to refine coreference decisions. In Proceedings of HLT/EMNLP’05, Vancouver, B.C., Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
<author>Abe Ittycheriah</author>
<author>Hongyan Jing</author>
<author>Nanda Kambhatla</author>
<author>Salim Roukos</author>
</authors>
<title>A mentionsynchronous coreference resolution algorithm based on the bell tree.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL2004),</booktitle>
<pages>135--142</pages>
<location>Barcelona,</location>
<contexts>
<context position="9159" citStr="Luo et al., 2004" startWordPosition="1444" endWordPosition="1447">rotherOf (Mark, Tim). However, if the co-reference system mistakenly links “John” to “his” in the second clause and links “Tim” to “his” in the first clause, then we will still have a network with four people, but the ties will be: motherOf (Mary, Tim), and brotherOf (Mark, John), which are completely wrong. This example shows that co-reference errors involving mentions that are relation arguments can lead to very bad performance in social network extraction. Our existing co-reference module is a state-ofthe-art system that produces very competitive results compared to other existing systems (Luo et al., 2004). It traverses the document from left to right and uses a mention-synchronous approach to decide whether a mention should be merged with an existing entity or start a new entity. However, our existing system has shortcomings for this data: the system lacks features for handling conversational speech, and the system often makes mistakes in pronoun resolution. Resolving pronominal references is very important for extracting social networks from conversational speech, as illustrated in the previous example. 3.3 Improving Co-reference for Conversational Speech We developed a new co-reference resol</context>
</contexts>
<marker>Luo, Ittycheriah, Jing, Kambhatla, Roukos, 2004</marker>
<rawString>Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda Kambhatla, and Salim Roukos. 2004. A mentionsynchronous coreference resolution algorithm based on the bell tree. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL2004), pages 135–142, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
</authors>
<title>Machine learning for coreference resolution: From local classification to global ranking.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL’04.</booktitle>
<contexts>
<context position="9868" citStr="Ng, 2005" startWordPosition="1552" endWordPosition="1553">ther a mention should be merged with an existing entity or start a new entity. However, our existing system has shortcomings for this data: the system lacks features for handling conversational speech, and the system often makes mistakes in pronoun resolution. Resolving pronominal references is very important for extracting social networks from conversational speech, as illustrated in the previous example. 3.3 Improving Co-reference for Conversational Speech We developed a new co-reference resolution system for conversational speech transcripts. Similar to many previous works on co-reference (Ng, 2005), we cast the problem as a classification task and solve it in two steps: (1) train a classifier to determine whether two mentions are co-referent or not, and (2) use a clustering algorithm to partition the mentions into clusters, based on the pairwise predictions. We added many features to our model specifically designed for conversational speech, and significantly improved the agglomerative clustering used for co-reference, including integrating relations as constraints, and designing better cluster linkage methods and clustering stopping criteria. 3.3.1 Adding Features for Conversational Sp</context>
</contexts>
<marker>Ng, 2005</marker>
<rawString>Vincent Ng. 2005. Machine learning for coreference resolution: From local classification to global ranking. In Proceedings ofACL’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Oard</author>
<author>D Soergel</author>
<author>D Doermann</author>
<author>X Huang</author>
<author>G C Murray</author>
<author>J Wang</author>
<author>B Ramabhadran</author>
<author>M Franz</author>
<author>S Gustman</author>
<author>J Mayfield</author>
<author>L Kharevych</author>
<author>S Strassel</author>
</authors>
<title>Building an information retrieval test collection for spontaneous conversational speech.</title>
<date>2004</date>
<booktitle>In Proceedings ofSIGIR’04,</booktitle>
<location>Sheffield, U.K.</location>
<contexts>
<context position="6885" citStr="Oard et al., 2004" startWordPosition="1071" endWordPosition="1074">sational speech transcripts is uniquely challenging. In this section, we describe the data collection used in our experiments, and explain specific techniques we used to improve IE performance on this data. 3.1 Conversational Speech Collection We use a corpus of videotaped, digitized oral interviews with Holocaust survivors in our experiments. This data was collected by the USC Shoah Foundation Institute (formerly known as the Visual History Foundation), and has been used in many research activities under the Multilingual Access to Large Spoken Archives (MALACH) project (Gustman et al., 2002; Oard et al., 2004). The collection contains oral interviews in 32 languages from 52,000 survivors, liberators, rescuers and witnesses of the Holocaust. This data is very challenging. Besides the usual characteristics of conversational speech, such as speaker turns and speech repairs, the interview transcripts contain a large percentage of ungrammatical, incoherent, or even incomprehensible clauses (a sample interview segment is shown in Figure 1). In addition, each interview covers many people and places over a long time period, which makes it even more difficult to extract social networks and biographical fact</context>
</contexts>
<marker>Oard, Soergel, Doermann, Huang, Murray, Wang, Ramabhadran, Franz, Gustman, Mayfield, Kharevych, Strassel, 2004</marker>
<rawString>D. Oard, D. Soergel, D. Doermann, X. Huang, G.C. Murray, J. Wang, B. Ramabhadran, M. Franz, S. Gustman, J. Mayfield, L. Kharevych, and S. Strassel. 2004. Building an information retrieval test collection for spontaneous conversational speech. In Proceedings ofSIGIR’04, Sheffield, U.K.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>