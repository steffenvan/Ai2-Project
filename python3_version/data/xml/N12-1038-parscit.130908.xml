<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009514">
<title confidence="0.956633">
Getting More from Segmentation Evaluation
</title>
<author confidence="0.997231">
Martin Scaiano
</author>
<affiliation confidence="0.999039">
University of Ottawa
</affiliation>
<address confidence="0.939576">
Ottawa, ON, K1N 6N5, Canada
</address>
<email confidence="0.974596">
mscai056@uottawa.ca
</email>
<author confidence="0.979288">
Diana Inkpen
</author>
<affiliation confidence="0.997181">
University of Ottawa
</affiliation>
<address confidence="0.942972">
Ottawa, ON, K1N 6N5, Canada
</address>
<email confidence="0.990018">
diana@eecs.uottawa.com
</email>
<sectionHeader confidence="0.997274" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9981285">
We introduce a new segmentation evaluation
measure, WinPR, which resolves some of the
limitations of WindowDiff. WinPR distin-
guishes between false positive and false nega-
tive errors; produces more intuitive measures,
such as precision, recall, and F-measure; is in-
sensitive to window size, which allows us to
customize near miss sensitivity; and is based
on counting errors not windows, but still pro-
vides partial reward for near misses.
</bodyText>
<sectionHeader confidence="0.999396" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9997225">
WindowDiff (Pevzner and Hearst, 2002) has be-
come the most frequently used measure to evalu-
ate segmentation. Segmentation is the task of di-
viding a stream of data (text or other media) into
coherent units. These units may be motivated top-
ically (Malioutov and Barzilay, 2006), structurally
(Stokes, 2003) (Malioutov et al., 2007) (Jancsary et
al., 2008), or visually (Chen et al., 2008), depending
on the domain and task. Segmentation evaluation
is difficult because exact comparison of boundaries
is too strict; a partial reward is required for close
boundaries.
</bodyText>
<sectionHeader confidence="0.997965" genericHeader="introduction">
2 WindowDiff
</sectionHeader>
<bodyText confidence="0.99992556">
“The WindowDiff metric is a variant of the Pk mea-
sure, which penalizes false positives and near misses
equally.” (Malioutov et al., 2007). WindowDiff uses
a sliding window over the segmentation; each win-
dow is evaluated as correct or incorrect. WindowD-
iff is effectively 1 − accuracy for all windows,
but accuracy is sensitive to the balance of positive
and negative data being evaluated. The positive
and negative balance is determined by the window
size. Small windows produce more negatives, thus
WindowDiff recommends using a window size (k)
of half the average segment length. This produces
an almost equal number of positive windows (con-
taining boundaries) and negative windows (without
boundaries).
Equation 1 represents the window size (k), where
N is the total number of sentences (or content units).
Equation 2 is WindowDiff’s traditional definition,
where R is the number of reference boundaries in
the window from i to i+k, and C is the number
of computed boundaries in the same window. The
comparison (&gt; 0) is sometimes forgotten, which
produces strange values not bound between 0 and 1;
thus we prefer equation 3 to represent WindowDiff,
as it emphasizes the comparison.
</bodyText>
<equation confidence="0.9979048">
N
k = (1)
2 * number of segments
(|Ri,i+k − Ci,i+k |&gt; 0)(2)
(Ri,i+k =� Ci,i+k) (3)
</equation>
<bodyText confidence="0.995671384615385">
Figure 1 illustrates WindowDiff’s sliding win-
dow evaluation. Each rectangle represents a sen-
tence, while the shade indicates to which segment
it truly belongs (reference segmentation). The ver-
tical line represents a computed boundary. This ex-
ample contains a near miss (misaligned boundary).
In this example, we are using a window size of 5.
The columns i, R, C, W represent the window po-
sition, the number of boundaries from the reference
(true) segmentation in the window, the number of
boundaries from the computed segmentation in the
window, and whether the values agree, respectively.
Only windows up to i = 5 are shown, but to process
</bodyText>
<equation confidence="0.9980651">
1
WindowDiff =
1
WindowDiff =
N−kE
i=0
N−kE
i=0
N − k
N − k
</equation>
<page confidence="0.918222666666667">
362
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 362–366,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<figure confidence="0.407575">
the entire segmentation 8 windows are required.
</figure>
<figureCaption confidence="0.999895">
Figure 1: Illustration of counting boundaries in windows
</figureCaption>
<bodyText confidence="0.99786425">
Franz et al. (2007) note that WindowDiff does not
allow different segmentation tasks to optimize dif-
ferent aspects, or tolerate different types of errors.
Tasks requiring a uniform theme in a segment might
tolerate false positives, while tasks requiring com-
plete ideas or complete themes might accept false
negatives.
Georgescul et al. (2009) note that while Win-
dowDiff technically penalizes false positives and
false negatives equally, false positives are in fact
more likely; a false positive error occurs anywhere
were there are more computed boundaries than
boundaries in the reference, while a false negative
error can only occur when a boundary is missed.
Consider figure 1, only 3 of the 8 windows contain a
boundary; only those 3 windows may have false neg-
atives (a missed boundary), while all other windows
may contain false positives (too many boundaries).
Lamprier et al. (2008) note that errors near the
beginning and end of a segmentation are actually
counted slightly less than other errors. Lamprier of-
fers a simple correction for this problem, by adding
k −1 phantom positions, which have no boundaries,
at the beginning and at the end sequence. The ad-
dition of these phantom boundaries allows for win-
dows extending outside the segmentation to be eval-
uated, and thus allowing for each position to be
count k times. Example E in figure 4 in the next
section will illustrate this point. Consider example
D in figure 4; this error will only be accounted for in
the first window, instead of the typical k windows.
Furthermore, tasks may want to adjust sensitiv-
ity or reward for near misses. Naturally, one would
be inclined to adjust the window size, but changing
the window size will change the balance of positive
windows and negative windows. Changing this bal-
ance has a significant impact on how WindowDiff
functions.
Some researchers have questioned what the Win-
dowDiff value tells us; how do we interpret it?
</bodyText>
<sectionHeader confidence="0.986067" genericHeader="method">
3 WinPR
</sectionHeader>
<bodyText confidence="0.999985483870968">
WinPR is derived from WindowDiff, but differs on
one main point: WinPR evaluates boundary posi-
tions, while WindowDiff evaluates regions (or win-
dows). WinPR is a set of equations (4-7) (Figure 2)
producing a confusion matrix. The confusion matrix
allows for the distinction between false positive and
negative errors, and can be used with Precision, Re-
call, and F-measure. Furthermore, the window size
may be changed to adjust near-miss sensitivity with-
out affecting the the interpretation of the confusion
matrix.
N is the number of content units and k repre-
sents the window size. WinPR includes the Lam-
prier (2008) correction, thus the sum is from 1 − k
to N instead of 1 to N − k as with WindowDiff.
min and max refer to the tradition computer sci-
ence functions which select the minimal or maximal
value from a set of two values. True negatives (5)
start with a negative term, which removes the value
of the phantom positions.
Each WinPR equation is a summation over all
windows. To understand the intuition behind each
equation, consider Figure 3. R and C represent the
number of boundaries from the reference and com-
puted segmentations, respectively, in the ith win-
dow, up to a maximum of k. The overlapping region
represents the TPs. The difference is the error, while
the sign of the difference indicates whether they are
FPs or FNs. The WinPR equations select the differ-
ence using the max function, forcing negative val-
ues to 0. The remainder, up to k, represents the TNs.
</bodyText>
<equation confidence="0.4791618">
TP
error
TN
C
0 Ri Ci k
</equation>
<figureCaption confidence="0.996172">
Figure 3: WinPR within Window Counting Demostration
</figureCaption>
<bodyText confidence="0.98247425">
Consider how WindowDiff and WinPR handle
the examples in Figure 4. These examples use the
same basic representation as Figure 1 in section 2.
Each segment is 6 units long and the window size is
</bodyText>
<table confidence="0.986134">
i R C W
0 0 0 ✓
1 0 0 ✓
2 0 1 X
3 1 1 ✓
4 1 1 ✓
5 1 0 X
R
363
True Positives = N
True Negatives = TP = min(Ri,i+k, Ci,i+k)
False Positives = E
False Negatives = i=1−k
N
TN = −k(k − 1) + (k − max(Ri,i+k, Ci,i+k))
E
i=1−k
N
FP = max(0, Ci,i+k − Ri,i+k)
E
i=1−k
N
FN = max(0, Ri,i+k − Ci,i+k)
E
i=1−k
</table>
<figureCaption confidence="0.984878">
Figure 2: Equations for the WinPR confusion matrix
</figureCaption>
<figure confidence="0.891621875">
3 = (6/2). Each window contains 3 content units,
thus we consider 4 potential boundary positions (the
edges are inclusive).
A) Correct boundary
B) Missed boundary
C) Near boundary
D) Extra boundary
E) Extra boundaries
</figure>
<figureCaption confidence="0.999979">
Figure 4: Example segmentations
</figureCaption>
<bodyText confidence="0.999730111111111">
Example A provides a baseline for comparison; B
is a false negative (a missed boundary); C is a near
miss; D is an extra boundary at the beginning of the
sequence, providing an example of Lamprier’s criti-
cism. E includes two errors near each other. Notice
how the additional errors in E have have a very small
impact on the WindowDiff value. Table 1 lists the
number of correct and incorrect windows, and the
WindowDiff value for each example.
</bodyText>
<table confidence="0.999339333333333">
Example Correct Incorrect WindowDiff
A 10 0 0
B 6 4 0.4
C 8 2 0.2
D 9 1 0.1
E 4 6 0.6
</table>
<tableCaption confidence="0.999899">
Table 1: WindowDiff values for examples A to E
</tableCaption>
<bodyText confidence="0.99942119047619">
WindowDiff should penalize an error k times,
once for each window in which it appears, with the
exception of near misses which have partial reward
and penalization. D is only penalized in one win-
dow, because most of the other windows would be
outside the sequence. E contains two errors, but they
are not fully penalized because they appear in over-
lapping windows. Furthermore, using a single met-
ric does not indicate if the errors are false positives
or false negatives. This information is important to
the development of a segmentation algorithm.
If we apply WinPR to examples A-E, we get the
results in Table 2. We will calculate precision and
recall using the WinPR confusion matrix, shown un-
der WinP and WinR respectively. You will note that
we can easily see whether an error is a false posi-
tive or a false negative. As we would expect, false
positives affect precision, and false negatives affect
recall. Near misses manifest as equal parts false pos-
itive and false negative. In example E, each error is
counted, unlike WindowDiff.
</bodyText>
<table confidence="0.740602">
Example TP TN FP FN WinP WinR
a 4 40 0 0 1 1.0
b 0 40 0 4 - 0
c 3 40 1 1 0.75 0.75
d 4 36 4 0 0.5 1.0
e 4 32 8 0 0.33 1.0
</table>
<tableCaption confidence="0.995657">
Table 2: WinPR values for examples A to E
</tableCaption>
<bodyText confidence="0.9974032">
In Table 2, note that each potential boundary posi-
tion is considered k (the window size) times. Thus,
each positive or negative boundary assignment is
counted k times; near misses producing a blend of
values: TP, FP, FN. We refer to the normalized con-
</bodyText>
<page confidence="0.997403">
364
</page>
<bodyText confidence="0.9991021">
fusion matrix (or normalized WinPR), as the con-
fusion matrix divided by the window size. If near
misses are not considered, this confusion matrix
gives the exact count of boundary assignments.
What is not apparent in Table 2, is that WinPR
is insensitive to window size, with the exception of
near misses. Thus adjusting the window size can
be used to adjust the tolerance or sensitivity to near
misses. Large window sizes are more forgiving of
near misses, smaller window size are more strict.
</bodyText>
<subsectionHeader confidence="0.999764">
3.1 Near Misses and Window Size
</subsectionHeader>
<bodyText confidence="0.999992">
WinPR does not provide any particular values in-
dicating the number of near misses, their distance,
or contribution to the evaluation. Because WinPR’s
window size only affects near miss sensitivity, and
not the positive/negative balance like in WindowD-
iff, we can subtract two normalized confusion ma-
trices using different window sizes. The difference
between the confusion matrices gives the impact of
near misses under different window sizes. Choosing
a very strict window size (k = 1), and subtracting it
from another window size would effectively provide
the contribution of the near misses to the confusion
matrix. In many circumstances, using several win-
dow sizes may be desirable.
</bodyText>
<subsectionHeader confidence="0.9955955">
3.2 Variations in Segment Size: Validation by
Simulation
</subsectionHeader>
<bodyText confidence="0.999953333333333">
We ran numerous tests on artificial segmentation
data composed of 40 segments, with a mean segment
length of 40 content units, and standard deviations
varying from 10 to 120. All tests showed that a false
positive or a false negative error is always penalized
k times, as expected.
</bodyText>
<subsectionHeader confidence="0.98594">
3.3 WinPR Applied to a Complete
Segmentation
</subsectionHeader>
<bodyText confidence="0.9997144">
Using a reference segmentation of 40 segments, we
derived two flawed segments: we added 20 extra
boundaries to one, and removed 18 boundaries from
the other. Both produced WindowDiff values of
0.22, while WinPR provided WinP = 0.66 and WinR
= 1.0 for the addition of boundaries and WinP =
1.00 and WinR = 0.54 for the removal of bound-
aries. WinPR highlights the differences in the na-
ture of the two flawed segmentations, while WinDiff
masks both the number and types of errors.
</bodyText>
<sectionHeader confidence="0.999283" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.889689416666667">
We presented a new evaluation method for segmen-
tation, called WinPR because it produces a confu-
sion matrix from which Precision and Recall can be
derived. WinPR is easy to implement and provides
more detail on the types of errors in a computed seg-
mentation, as compared with the reference. Some of
the major benefits of WinPR, as opposed to Win-
dowDiff are presented below:
1. Distinct counting of false positives and false
negatives, which helps in algorithm selection
for downstream tasks and helps with analysis
and optimization of an algorithm.
</bodyText>
<listItem confidence="0.951998583333333">
2. The confusion matrix is easier to interpret than
a WindowDiff value.
3. WinPR counts errors from boundaries, not win-
dows, thus close errors are not masked
4. Precision, and Recall are easier to understand
than WindowDiff.
5. F-measure is effective when a single value is
required for comparison.
6. WinPR incorporates Lamprier (2008) correc-
tion.
7. Adjusting the window size can customize an
evaluation’s tolerance of near misses
</listItem>
<bodyText confidence="0.949196333333333">
8. WinPR provides a method of detecting the im-
pact of near misses on an evaluation
WinPR counts boundaries, not windows, which
has analytical benefits, but WindowDiff’s counting
of windows provides an evaluation of segmentation
by region. Thus WindowDiff is more appropriate
when an evaluator is less interested in the types and
the number of errors and more interested in the per-
centage of the sequence that is correct.
</bodyText>
<sectionHeader confidence="0.998137" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999936428571429">
Thanks to Dr. Stan Szpakowicz for all his help refin-
ing the arguments and the presentation of this paper.
Thanks to Anna Kazantseva for months of discus-
sions about segmentation and the evaluation prob-
lems we each faced. Thanks to Natural Sciences and
Engineering Research Council of Canada (NSERC)
for funding our research.
</bodyText>
<page confidence="0.998668">
365
</page>
<sectionHeader confidence="0.998325" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999871585365854">
L Chen, YC Lai, and H Liao. 2008. Movie scene
segmentation using background information. Pattern
Recognition, Jan.
M Franz, J McCarley, and J Xu. 2007. User-oriented
text segmentation evaluation measure. SIGIR ’07 Pro-
ceedings of the 30th annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval, Jan.
M Georgescul, A Clark, and S Armstrong. 2009. An
analysis of quantitative aspects in the evaluation of the-
matic segmentation algorithms. SigDIAL ’06 Proceed-
ings of the 7th SIGdial Workshop on Discourse and
Dialogue, Jan.
J Jancsary, J Matiasek, and H Trost. 2008. Revealing the
structure of medical dictations with conditional ran-
dom fields. EMNLP ’08 Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, Jan.
S Lamprier, T Amghar, and B Levrat. 2008. On evalu-
ation methodologies for text segmentation algorithms.
19th IEEE International Conference on Tools with Ar-
tificial Intelligence - Vol.2, Jan.
I Malioutov and R Barzilay. 2006. Minimum cut model
for spoken lecture segmentation. ACL-44 Proceedings
of the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Associ-
ation for Computational Linguistics, Jan.
I Malioutov, A Park, R Barzilay, and R Glass. 2007.
Making sense of sound: Unsupervised topic segmen-
tation over acoustic input. Proceeding of the Annual
Meeting of the Association for Computation Linguis-
tics 2007, Jan.
L Pevzner and M Hearst. 2002. A critique and improve-
ment of an evaluation metric for text segmentation.
Computational Linguistics, Jan.
N Stokes. 2003. Spoken and written news story seg-
mentation using lexical chains. Proceedings of the
2003 Conference of the North American Chapter of the
Association for Computational Linguistics on Human
Language Technology: HLT-NAACL2003 Student Re-
search Workshop, Jan.
</reference>
<page confidence="0.999084">
366
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.304577">
<title confidence="0.999782">Getting More from Segmentation Evaluation</title>
<author confidence="0.987527">Martin</author>
<affiliation confidence="0.995544">University of</affiliation>
<address confidence="0.562393">Ottawa, ON, K1N 6N5,</address>
<email confidence="0.921018">mscai056@uottawa.ca</email>
<author confidence="0.676432">Diana</author>
<affiliation confidence="0.997231">University of</affiliation>
<address confidence="0.846897">Ottawa, ON, K1N 6N5,</address>
<email confidence="0.999827">diana@eecs.uottawa.com</email>
<abstract confidence="0.998011454545454">We introduce a new segmentation evaluation measure, WinPR, which resolves some of the limitations of WindowDiff. WinPR distinguishes between false positive and false negative errors; produces more intuitive measures, such as precision, recall, and F-measure; is insensitive to window size, which allows us to customize near miss sensitivity; and is based on counting errors not windows, but still provides partial reward for near misses.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Chen</author>
<author>YC Lai</author>
<author>H Liao</author>
</authors>
<title>Movie scene segmentation using background information. Pattern Recognition,</title>
<date>2008</date>
<contexts>
<context position="1058" citStr="Chen et al., 2008" startWordPosition="157" endWordPosition="160">h as precision, recall, and F-measure; is insensitive to window size, which allows us to customize near miss sensitivity; and is based on counting errors not windows, but still provides partial reward for near misses. 1 Introduction WindowDiff (Pevzner and Hearst, 2002) has become the most frequently used measure to evaluate segmentation. Segmentation is the task of dividing a stream of data (text or other media) into coherent units. These units may be motivated topically (Malioutov and Barzilay, 2006), structurally (Stokes, 2003) (Malioutov et al., 2007) (Jancsary et al., 2008), or visually (Chen et al., 2008), depending on the domain and task. Segmentation evaluation is difficult because exact comparison of boundaries is too strict; a partial reward is required for close boundaries. 2 WindowDiff “The WindowDiff metric is a variant of the Pk measure, which penalizes false positives and near misses equally.” (Malioutov et al., 2007). WindowDiff uses a sliding window over the segmentation; each window is evaluated as correct or incorrect. WindowDiff is effectively 1 − accuracy for all windows, but accuracy is sensitive to the balance of positive and negative data being evaluated. The positive and neg</context>
</contexts>
<marker>Chen, Lai, Liao, 2008</marker>
<rawString>L Chen, YC Lai, and H Liao. 2008. Movie scene segmentation using background information. Pattern Recognition, Jan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Franz</author>
<author>J McCarley</author>
<author>J Xu</author>
</authors>
<title>User-oriented text segmentation evaluation measure.</title>
<date>2007</date>
<booktitle>SIGIR ’07 Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<contexts>
<context position="3570" citStr="Franz et al. (2007)" startWordPosition="565" endWordPosition="568">rence (true) segmentation in the window, the number of boundaries from the computed segmentation in the window, and whether the values agree, respectively. Only windows up to i = 5 are shown, but to process 1 WindowDiff = 1 WindowDiff = N−kE i=0 N−kE i=0 N − k N − k 362 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 362–366, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics the entire segmentation 8 windows are required. Figure 1: Illustration of counting boundaries in windows Franz et al. (2007) note that WindowDiff does not allow different segmentation tasks to optimize different aspects, or tolerate different types of errors. Tasks requiring a uniform theme in a segment might tolerate false positives, while tasks requiring complete ideas or complete themes might accept false negatives. Georgescul et al. (2009) note that while WindowDiff technically penalizes false positives and false negatives equally, false positives are in fact more likely; a false positive error occurs anywhere were there are more computed boundaries than boundaries in the reference, while a false negative error</context>
</contexts>
<marker>Franz, McCarley, Xu, 2007</marker>
<rawString>M Franz, J McCarley, and J Xu. 2007. User-oriented text segmentation evaluation measure. SIGIR ’07 Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, Jan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Georgescul</author>
<author>A Clark</author>
<author>S Armstrong</author>
</authors>
<title>An analysis of quantitative aspects in the evaluation of thematic segmentation algorithms.</title>
<date>2009</date>
<booktitle>SigDIAL ’06 Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<contexts>
<context position="3893" citStr="Georgescul et al. (2009)" startWordPosition="614" endWordPosition="617">f the Association for Computational Linguistics: Human Language Technologies, pages 362–366, Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics the entire segmentation 8 windows are required. Figure 1: Illustration of counting boundaries in windows Franz et al. (2007) note that WindowDiff does not allow different segmentation tasks to optimize different aspects, or tolerate different types of errors. Tasks requiring a uniform theme in a segment might tolerate false positives, while tasks requiring complete ideas or complete themes might accept false negatives. Georgescul et al. (2009) note that while WindowDiff technically penalizes false positives and false negatives equally, false positives are in fact more likely; a false positive error occurs anywhere were there are more computed boundaries than boundaries in the reference, while a false negative error can only occur when a boundary is missed. Consider figure 1, only 3 of the 8 windows contain a boundary; only those 3 windows may have false negatives (a missed boundary), while all other windows may contain false positives (too many boundaries). Lamprier et al. (2008) note that errors near the beginning and end of a seg</context>
</contexts>
<marker>Georgescul, Clark, Armstrong, 2009</marker>
<rawString>M Georgescul, A Clark, and S Armstrong. 2009. An analysis of quantitative aspects in the evaluation of thematic segmentation algorithms. SigDIAL ’06 Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, Jan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jancsary</author>
<author>J Matiasek</author>
<author>H Trost</author>
</authors>
<title>Revealing the structure of medical dictations with conditional random fields.</title>
<date>2008</date>
<booktitle>EMNLP ’08 Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<contexts>
<context position="1025" citStr="Jancsary et al., 2008" startWordPosition="151" endWordPosition="154">produces more intuitive measures, such as precision, recall, and F-measure; is insensitive to window size, which allows us to customize near miss sensitivity; and is based on counting errors not windows, but still provides partial reward for near misses. 1 Introduction WindowDiff (Pevzner and Hearst, 2002) has become the most frequently used measure to evaluate segmentation. Segmentation is the task of dividing a stream of data (text or other media) into coherent units. These units may be motivated topically (Malioutov and Barzilay, 2006), structurally (Stokes, 2003) (Malioutov et al., 2007) (Jancsary et al., 2008), or visually (Chen et al., 2008), depending on the domain and task. Segmentation evaluation is difficult because exact comparison of boundaries is too strict; a partial reward is required for close boundaries. 2 WindowDiff “The WindowDiff metric is a variant of the Pk measure, which penalizes false positives and near misses equally.” (Malioutov et al., 2007). WindowDiff uses a sliding window over the segmentation; each window is evaluated as correct or incorrect. WindowDiff is effectively 1 − accuracy for all windows, but accuracy is sensitive to the balance of positive and negative data bein</context>
</contexts>
<marker>Jancsary, Matiasek, Trost, 2008</marker>
<rawString>J Jancsary, J Matiasek, and H Trost. 2008. Revealing the structure of medical dictations with conditional random fields. EMNLP ’08 Proceedings of the Conference on Empirical Methods in Natural Language Processing, Jan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lamprier</author>
<author>T Amghar</author>
<author>B Levrat</author>
</authors>
<title>On evaluation methodologies for text segmentation algorithms.</title>
<date>2008</date>
<booktitle>19th IEEE International Conference on Tools with Artificial Intelligence - Vol.2,</booktitle>
<contexts>
<context position="4440" citStr="Lamprier et al. (2008)" startWordPosition="703" endWordPosition="706">s or complete themes might accept false negatives. Georgescul et al. (2009) note that while WindowDiff technically penalizes false positives and false negatives equally, false positives are in fact more likely; a false positive error occurs anywhere were there are more computed boundaries than boundaries in the reference, while a false negative error can only occur when a boundary is missed. Consider figure 1, only 3 of the 8 windows contain a boundary; only those 3 windows may have false negatives (a missed boundary), while all other windows may contain false positives (too many boundaries). Lamprier et al. (2008) note that errors near the beginning and end of a segmentation are actually counted slightly less than other errors. Lamprier offers a simple correction for this problem, by adding k −1 phantom positions, which have no boundaries, at the beginning and at the end sequence. The addition of these phantom boundaries allows for windows extending outside the segmentation to be evaluated, and thus allowing for each position to be count k times. Example E in figure 4 in the next section will illustrate this point. Consider example D in figure 4; this error will only be accounted for in the first windo</context>
</contexts>
<marker>Lamprier, Amghar, Levrat, 2008</marker>
<rawString>S Lamprier, T Amghar, and B Levrat. 2008. On evaluation methodologies for text segmentation algorithms. 19th IEEE International Conference on Tools with Artificial Intelligence - Vol.2, Jan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Malioutov</author>
<author>R Barzilay</author>
</authors>
<title>Minimum cut model for spoken lecture segmentation.</title>
<date>2006</date>
<booktitle>ACL-44 Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<contexts>
<context position="947" citStr="Malioutov and Barzilay, 2006" startWordPosition="140" endWordPosition="143">of WindowDiff. WinPR distinguishes between false positive and false negative errors; produces more intuitive measures, such as precision, recall, and F-measure; is insensitive to window size, which allows us to customize near miss sensitivity; and is based on counting errors not windows, but still provides partial reward for near misses. 1 Introduction WindowDiff (Pevzner and Hearst, 2002) has become the most frequently used measure to evaluate segmentation. Segmentation is the task of dividing a stream of data (text or other media) into coherent units. These units may be motivated topically (Malioutov and Barzilay, 2006), structurally (Stokes, 2003) (Malioutov et al., 2007) (Jancsary et al., 2008), or visually (Chen et al., 2008), depending on the domain and task. Segmentation evaluation is difficult because exact comparison of boundaries is too strict; a partial reward is required for close boundaries. 2 WindowDiff “The WindowDiff metric is a variant of the Pk measure, which penalizes false positives and near misses equally.” (Malioutov et al., 2007). WindowDiff uses a sliding window over the segmentation; each window is evaluated as correct or incorrect. WindowDiff is effectively 1 − accuracy for all window</context>
</contexts>
<marker>Malioutov, Barzilay, 2006</marker>
<rawString>I Malioutov and R Barzilay. 2006. Minimum cut model for spoken lecture segmentation. ACL-44 Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, Jan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Malioutov</author>
<author>A Park</author>
<author>R Barzilay</author>
<author>R Glass</author>
</authors>
<title>Making sense of sound: Unsupervised topic segmentation over acoustic input. Proceeding of the Annual Meeting of the Association for Computation Linguistics</title>
<date>2007</date>
<contexts>
<context position="1001" citStr="Malioutov et al., 2007" startWordPosition="147" endWordPosition="150">d false negative errors; produces more intuitive measures, such as precision, recall, and F-measure; is insensitive to window size, which allows us to customize near miss sensitivity; and is based on counting errors not windows, but still provides partial reward for near misses. 1 Introduction WindowDiff (Pevzner and Hearst, 2002) has become the most frequently used measure to evaluate segmentation. Segmentation is the task of dividing a stream of data (text or other media) into coherent units. These units may be motivated topically (Malioutov and Barzilay, 2006), structurally (Stokes, 2003) (Malioutov et al., 2007) (Jancsary et al., 2008), or visually (Chen et al., 2008), depending on the domain and task. Segmentation evaluation is difficult because exact comparison of boundaries is too strict; a partial reward is required for close boundaries. 2 WindowDiff “The WindowDiff metric is a variant of the Pk measure, which penalizes false positives and near misses equally.” (Malioutov et al., 2007). WindowDiff uses a sliding window over the segmentation; each window is evaluated as correct or incorrect. WindowDiff is effectively 1 − accuracy for all windows, but accuracy is sensitive to the balance of positiv</context>
</contexts>
<marker>Malioutov, Park, Barzilay, Glass, 2007</marker>
<rawString>I Malioutov, A Park, R Barzilay, and R Glass. 2007. Making sense of sound: Unsupervised topic segmentation over acoustic input. Proceeding of the Annual Meeting of the Association for Computation Linguistics 2007, Jan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Pevzner</author>
<author>M Hearst</author>
</authors>
<title>A critique and improvement of an evaluation metric for text segmentation. Computational Linguistics,</title>
<date>2002</date>
<contexts>
<context position="710" citStr="Pevzner and Hearst, 2002" startWordPosition="99" endWordPosition="102"> ON, K1N 6N5, Canada mscai056@uottawa.ca Diana Inkpen University of Ottawa Ottawa, ON, K1N 6N5, Canada diana@eecs.uottawa.com Abstract We introduce a new segmentation evaluation measure, WinPR, which resolves some of the limitations of WindowDiff. WinPR distinguishes between false positive and false negative errors; produces more intuitive measures, such as precision, recall, and F-measure; is insensitive to window size, which allows us to customize near miss sensitivity; and is based on counting errors not windows, but still provides partial reward for near misses. 1 Introduction WindowDiff (Pevzner and Hearst, 2002) has become the most frequently used measure to evaluate segmentation. Segmentation is the task of dividing a stream of data (text or other media) into coherent units. These units may be motivated topically (Malioutov and Barzilay, 2006), structurally (Stokes, 2003) (Malioutov et al., 2007) (Jancsary et al., 2008), or visually (Chen et al., 2008), depending on the domain and task. Segmentation evaluation is difficult because exact comparison of boundaries is too strict; a partial reward is required for close boundaries. 2 WindowDiff “The WindowDiff metric is a variant of the Pk measure, which </context>
</contexts>
<marker>Pevzner, Hearst, 2002</marker>
<rawString>L Pevzner and M Hearst. 2002. A critique and improvement of an evaluation metric for text segmentation. Computational Linguistics, Jan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Stokes</author>
</authors>
<title>Spoken and written news story segmentation using lexical chains.</title>
<date>2003</date>
<booktitle>Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology: HLT-NAACL2003 Student Research Workshop,</booktitle>
<contexts>
<context position="976" citStr="Stokes, 2003" startWordPosition="145" endWordPosition="146">lse positive and false negative errors; produces more intuitive measures, such as precision, recall, and F-measure; is insensitive to window size, which allows us to customize near miss sensitivity; and is based on counting errors not windows, but still provides partial reward for near misses. 1 Introduction WindowDiff (Pevzner and Hearst, 2002) has become the most frequently used measure to evaluate segmentation. Segmentation is the task of dividing a stream of data (text or other media) into coherent units. These units may be motivated topically (Malioutov and Barzilay, 2006), structurally (Stokes, 2003) (Malioutov et al., 2007) (Jancsary et al., 2008), or visually (Chen et al., 2008), depending on the domain and task. Segmentation evaluation is difficult because exact comparison of boundaries is too strict; a partial reward is required for close boundaries. 2 WindowDiff “The WindowDiff metric is a variant of the Pk measure, which penalizes false positives and near misses equally.” (Malioutov et al., 2007). WindowDiff uses a sliding window over the segmentation; each window is evaluated as correct or incorrect. WindowDiff is effectively 1 − accuracy for all windows, but accuracy is sensitive </context>
</contexts>
<marker>Stokes, 2003</marker>
<rawString>N Stokes. 2003. Spoken and written news story segmentation using lexical chains. Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology: HLT-NAACL2003 Student Research Workshop, Jan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>