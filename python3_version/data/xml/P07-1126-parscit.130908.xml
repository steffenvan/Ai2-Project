<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012235">
<title confidence="0.99611">
Text Analysis for Automatic Image Annotation
</title>
<author confidence="0.991054">
Koen Deschacht and Marie-Francine Moens
</author>
<affiliation confidence="0.933885">
Interdisciplinary Centre for Law &amp; IT
Department of Computer Science
Katholieke Universiteit Leuven
</affiliation>
<address confidence="0.956939">
Tiensestraat 41, 3000 Leuven, Belgium
</address>
<email confidence="0.999299">
{koen.deschacht,marie-france.moens}@law.kuleuven.ac.be
</email>
<sectionHeader confidence="0.998602" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999462333333334">
We present a novel approach to automati-
cally annotate images using associated text.
We detect and classify all entities (persons
and objects) in the text after which we de-
termine the salience (the importance of an
entity in a text) and visualness (the extent to
which an entity can be perceived visually)
of these entities. We combine these mea-
sures to compute the probability that an en-
tity is present in the image. The suitability
of our approach was successfully tested on
100 image-text pairs of Yahoo! News.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999952884615385">
Our society deals with a growing bulk of un-
structured information such as text, images and
video, a situation witnessed in many domains (news,
biomedical information, intelligence information,
business documents, etc.). This growth comes along
with the demand for more effective tools to search
and summarize this information. Moreover, there is
the need to mine information from texts and images
when they contribute to decision making by gov-
ernments, businesses and other institutions. The
capability to accurately recognize content in these
sources would largely contribute to improved index-
ing, classification, filtering, mining and interroga-
tion.
Algorithms and techniques for the disclosure of
information from the different media have been de-
veloped for every medium independently during the
last decennium, but only recently the interplay be-
tween these different media has become a topic of
interest. One of the possible applications is to help
analysis in one medium by employing information
from another medium. In this paper we study text
that is associated with an image, such as for instance
image captions, video transcripts or surrounding text
in a web page. We develop techniques that extract
information from these texts to help with the diffi-
cult task of accurate object recognition in images.
Although images and associated texts never contain
precisely the same information, in many situations
the associated text offers valuable information that
helps to interpret the image.
The central objective of the CLASS project1 is to
develop advanced learning methods that allow ima-
ges, video and associated text to be automatically
analyzed and structured. In this paper we test the
feasibility of automatically annotating images by us-
ing textual information in near-parallel image-text
pairs, in which most of the content of the image
corresponds to content of the text and vice versa.
We will focus on entities such as persons and ob-
jects. We will hereby take into account the text’s dis-
course structure and semantics, which allow a more
fine-grained identification of what content might be
present in the image, and will enrich our model with
world knowledge that is not present in the text.
We will first discuss the corpus on which we ap-
ply and test our techniques in section 2, after which
we outline what techniques we have developed: we
start with a baseline system to annotate images with
person names (section 3) and improve this by com-
puting the importance of the persons in the text (sec-
tion 4). We will then extend the model to include all
</bodyText>
<footnote confidence="0.987135">
1http://class.inrialpes.fr/
</footnote>
<page confidence="0.697156">
1000
</page>
<note confidence="0.9307475">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1000–1007,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.964218509803922">
other objects) that appear both in the image and in
the text. For example, the image-text pair shown in
fig. 1 is annotated with one entity, ”Hiram Myers”,
since this is the only entity that appears both in the
text and in the image. On average these texts contain
15.04 entities, of which 2.58 appear in the image.
To build the appearance model of the text, we
have combined different tools. We will evaluate
every tool separately on 100 image-text pairs. This
way we have a detailed view on the nature of the
errors in the final model.
3 Automatically annotating person names
Given a text that is associated with an image, we
want to compute a probabilistic appearance model,
i.e. a collection of entities that are visible in the
image. We will start with a model that holds the
names of the persons that appear in the image, such
as was done by (Satoh et al., 1999; Berg et al., 2004),
and extend this model in section 5 to include all
other objects.
Hiram Myers, of Edmond, Okla., walks across the
fence, attempting to deliver what he called a ’people’s
indictment’ of Halliburton CEO David Lesar, outside the
site of the annual Halliburton shareholders meeting in
Duncan, Okla., leading to his arrest, Wednesday, May 17,
2006.
Figure 1: Image-text pair with entity “Hiram Myers” 3.1 Named Entity Recognition
appearing both in the text and in the image. A logical first step to detect person names is Named
types of objects (section 5) and improve it by defin- Entity Recognition (NER). We use the OpenNLP
ing and computing the visualness measure (section package3, which detects noun phrase chunks in the
6). Finally we will combine these different tech- sentences that represent persons, locations, organi-
niques in one probabilistic model in section 7. zations and dates. To improve the recognition of
2 The parallel corpus person names, we use a dictionary of names, which
We have created a parallel corpus consisting of 1700 we have extracted from the Wikipedia4 website. We
image-text pairs, retrieved from the Yahoo! News have manually evaluated performance of NER on
website2. Every image has an accompanying text our test corpus and found that performance was sa-
which describes the content of the image. This text tisfying: we obtained a precision of 93.37% and a re-
will in general discuss one or more persons in the call of 97.69%. Precision is the percentage of iden-
image, possibly one or more other objects, the loca- tified person names by the system that corresponds
tion and the event for which the picture was taken. to correct person names, and recall is the percentage
An example of an image-text pair is given in fig. 1. of person names in the text that have been correctly
Not all persons or objects who are pictured in the identified by the system.
images are necessarily described in the texts. The The texts contain a small number of noun phrase
inverse is also true, i.e. content mentioned in the coreferents that are in the form of pronouns, we have
text may not be present in the image. resolved these using the LingPipe5 package.
We have randomly selected 100 text-pairs from
the corpus, and one annotator has labeled every
image-text pair with the entities (i.e. persons and
3.2 Baseline system
We want to annotate an image using the associated
text. We try to find the names of persons which are
</bodyText>
<footnote confidence="0.9236654">
3http://opennlp.sourceforge.net/
4http://en.wikipedia.org/
5http://www.alias-i.com/lingpipe/
2http://news.yahoo.com/
1001
</footnote>
<bodyText confidence="0.998969318181818">
both described in the text and visible in the image,
and we want to do so by relying only on an analysis
of the text. In some cases, such as the following
example, the text states explicitly whether a person
is (not) visible in the image:
President Bush [...] with Danish Prime
Minister Anders Fogh Rasmussen, not
pictured, at Camp David [...].
Developing a system that could extract this informa-
tion is not trivial, and even if we could do so, only a
very small percentage of the texts in our corpus con-
tain this kind of information. In the next section we
will look into a method that is applicable to a wide
range of (descriptive) texts and that does not rely on
specific information within the text.
To evaluate the performance of this system, we
will compare it with a simple baseline system. The
baseline system assumes that all persons in the text
are visible in the image, which results in a precision
of 71.27% and a recall of 95.56%. The (low) preci-
sion can be explained by the fact that the texts often
discuss people which are not present in the image.
</bodyText>
<sectionHeader confidence="0.827797" genericHeader="introduction">
4 Detection of the salience of a person
</sectionHeader>
<bodyText confidence="0.9998798125">
Not all persons discussed in a text are equally im-
portant. We would like to discover what persons
are in the focus of a text and what persons are only
mentioned briefly, because we presume that more
important persons in the text have a larger proba-
bility of appearing in the image than less important
persons. Because of the short lengths of the docu-
ments in our corpus, an analysis of lexical cohesion
between terms in the text will not be sufficient for
distinguishing between important and less important
entities. We define a measure, salience, which is a
number between 0 and 1 that represents the impor-
tance of an entity in a text. We present here a method
for computing this score based on an in depth ana-
lysis of the discourse of the text and of the syntactic
structure of the individual sentences.
</bodyText>
<subsectionHeader confidence="0.9936">
4.1 Discourse segmentation
</subsectionHeader>
<bodyText confidence="0.999966730769231">
The discourse segmentation module, which we de-
veloped in earlier research, hierarchically and se-
quentially segments the discourse in different topics
and subtopics resulting in a table of contents of a
text (Moens, 2006). The table shows the main en-
tities and the related subtopic entities in a tree-like
structure that also indicates the segments (by means
of character pointers) to which an entity applies. The
algorithm detects patterns of thematic progression in
texts and can thus recognize the main topic of a sen-
tence (i.e., about whom or what the sentence speaks)
and the hierarchical and sequential relationships be-
tween individual topics. A mixture model, taking
into account different discourse features, is trained
with the Expectation Maximization algorithm on an
annotated DUC-2003 corpus. We use the resulting
discourse segmentation to define the salience of in-
dividual entities that are recognized as topics of a
sentence. We compute for each noun entity er in the
discourse its salience (Sal1) in the discourse tree,
which is proportional with the depth of the entity in
the discourse tree -hereby assuming that deeper in
this tree more detailed topics of a text are described-
and normalize this value to be between zero and one.
When an entity occurs in different subtrees, its max-
imum score is chosen.
</bodyText>
<subsectionHeader confidence="0.8985875">
4.2 Refinement with sentence parse
information
</subsectionHeader>
<bodyText confidence="0.999992260869565">
Because not all entities of the text are captured in the
discourse tree, we implement an additional refine-
ment of the computation of the salience of an entity
which is inspired by (Moens et al., 2006). The seg-
mentation module already determines the main topic
of a sentence. Since the syntactic structure is often
indicative of the information distribution in a sen-
tence, we can determine the relative importance of
the other entities in a sentence by relying on the re-
lationships between entities as signaled by the parse
tree. When determining the salience of an entity, we
take into account the level of the entity mention in
the parse tree (Sal2), and the number of children for
the entity in this structure (Sal3), where the normal-
ized score is respectively inversely proportional with
the depth of the parse tree where the entity occurs,
and proportional with the number of children.
We combine the three salience values (Sal1,
Sal2 and Sal3) by using a linear weighting. We
have experimentally determined reasonable coeffi-
cients for these three values, which are respectively
0.8, 0.1 and 0.1. Eventually, we could learn these
coefficients from a training corpus (e.g., with the
</bodyText>
<page confidence="0.963736">
1002
</page>
<table confidence="0.999924666666667">
Precision Recall F-measure
NER 71.27% 95.56% 81.65%
NER+DYN 97.66% 92.59% 95.06%
</table>
<tableCaption confidence="0.999264">
Table 1: Comparison of methods to predict what per-
</tableCaption>
<bodyText confidence="0.857750333333333">
sons described in the text will appear in the image,
using Named Entity Recognition (NER), and the
salience measure with dynamic cut-off (DYN).
</bodyText>
<subsectionHeader confidence="0.666417">
Expectation Maximization algorithm).
</subsectionHeader>
<bodyText confidence="0.99994625">
We do not separately evaluate our technology
for salience detection as this technology was
already extensively evaluated in the past (Moens,
2006).
</bodyText>
<subsectionHeader confidence="0.996875">
4.3 Evaluating the improved system
</subsectionHeader>
<bodyText confidence="0.9999907">
The salience measure defines a ranking of all the
persons in a text. We will use this ranking to improve
our baseline system. We assume that it is possible
to automatically determine the number of faces that
are recognized in the image, which gives us an indi-
cation of a suitable cut-off value. This approach is
reasonable since face detection (determine whether a
face is present in the image) is significant easier than
face recognition (determine which person is present
in the image). In the improved model we assume
that persons which are ranked higher than, or equal
to, the cut-off value appear in the image. For ex-
ample, if 4 faces appear in the image, we assume
that only the 4 persons of which the names in the
text have been assigned the highest salience appear
in the image. We see from table 1 that the precision
(97.66%) has improved drastically, while the recall
remained high (92.59%). This confirms the hypoth-
esis that determining the focus of a text helps in de-
termining the persons that appear in the image.
</bodyText>
<sectionHeader confidence="0.994858" genericHeader="method">
5 Automatically annotating persons and
objects
</sectionHeader>
<bodyText confidence="0.9998905">
After having developed a reasonable successful sys-
tem to detect what persons will appear in the image,
we turn to a more difficult case : Detecting persons
and all other objects that are described in the text.
</bodyText>
<subsectionHeader confidence="0.95055">
5.1 Entity detection
</subsectionHeader>
<bodyText confidence="0.9999795">
We will first detect what words in the text refer to an
entity. For this, we perform part-of-speech tagging
(i.e., detecting the syntactic word class such as noun,
verb, etc.). We take that every noun in the text rep-
resents an entity. We have used LTPOS (Mikheev,
1997), which performed the task almost errorless
(precision of 98.144% and recall of 97.36% on the
nouns in the test corpus). Person names which were
segmented using the NER package are also marked
as entities.
</bodyText>
<subsectionHeader confidence="0.998284">
5.2 Baseline system
</subsectionHeader>
<bodyText confidence="0.999981571428571">
We want to detect the objects and the names of per-
sons which are both visible in the image and de-
scribed in the text. We start with a simple baseline
system, in which we assume that every entity in the
text appears in the image. As can be expected, this
results in a high recall (91.08%), and a very low pre-
cision (15.62%). We see that the problem here is
far more difficult compared to detecting only per-
son names. This can be explained by the fact that
many entities (such as for example August, idea and
history) will never (or only indirectly) appear in an
image. In the next section we will try to determine
what types of entities are more likely to appear in
the image.
</bodyText>
<sectionHeader confidence="0.919722" genericHeader="method">
6 Detection of the visualness of an entity
</sectionHeader>
<bodyText confidence="0.999920285714286">
The assumption that every entity in the text appears
in the image is rather crude. We will enrich our
model with external world knowledge to find enti-
ties which are not likely to appear in an image. We
define a measure called visualness, which is defined
as the extent to which an entity can be perceived vi-
sually.
</bodyText>
<subsectionHeader confidence="0.982896">
6.1 Entity classification
</subsectionHeader>
<bodyText confidence="0.999971777777778">
After we have performed entity detection, we want
to classify every entity according to a certain seman-
tic database. We use the WordNet (Fellbaum, 1998)
database, which organizes English nouns, verbs, ad-
jectives and adverbs in synsets. A synset is a col-
lection of words that have a close meaning and that
represent an underlying concept. An example of
such a synset is “person, individual, someone, some-
body, mortal, soul”. All these words refer to a hu-
</bodyText>
<page confidence="0.939434">
1003
</page>
<bodyText confidence="0.9998981875">
man being. In order to correctly assign a noun in
a text to its synset, i.e., to disambiguate the sense
of this word, we use an efficient Word Sense Dis-
ambiguation (WSD) system that was developed by
the authors and which is described in (Deschacht
and Moens, 2006). Proper names are labeled by
the Named Entity Recognizer, which recognizes per-
sons, locations and organizations. These labels in
turn allow us to assign the corresponding WordNet
synset.
The combination of the WSD system and the
NER package achieved a 75.97% accuracy in classi-
fying the entities. Apart from errors that resulted
from erroneous entity detection (32.32%), errors
were mainly due to the WSD system (60.56%) and
in a smaller amount to the NER package (8.12%).
</bodyText>
<subsectionHeader confidence="0.998462">
6.2 WordNet similarity
</subsectionHeader>
<bodyText confidence="0.9999676875">
We determine the visualness for every synset us-
ing a method that was inspired by Kamps and Marx
(2002). Kamps and Marx use a distance measure
defined on the adjectives of the WordNet database
together with two seed adjectives to determine the
emotive or affective meaning of any given adjective.
They compute the relative distance of the adjective
to the seed synsets “good” and “bad” and use this
distance to define a measure of affective meaning.
We take a similar approach to determine the visu-
alness of a given synset. We first define a similarity
measure between synsets in the WordNet database.
Then we select a set of seed synsets, i.e. synsets
with a predefined visualness, and use the similarity
of a given synset to the seed synsets to determine the
visualness.
</bodyText>
<subsectionHeader confidence="0.998081">
6.3 Distance measure
</subsectionHeader>
<bodyText confidence="0.982778083333333">
The WordNet database defines different relations be-
tween its synsets. An important relation for nouns is
the hypernym/hyponym relation. A noun X is a hy-
pernym of a noun Y if Y is a subtype or instance of
X. For example, “bird” is a hypernym of “penguin”
(and “penguin” is a hyponym of “bird”). A synset
in WordNet can have one or more hypernyms. This
relation organizes the synsets in a hierarchical tree
(Hayes, 1999).
The similarity measure defined by Lin (1998) uses
the hypernym/hyponym relation to compute a se-
mantic similarity between two WordNet synsets S1
and S2. First it finds the most specific (lowest in the
tree) synset Sp that is a parent of both S1 and S2.
Then it computes the similarity of S1 and S2 as
sim(S1, S2) = logP(S1) + logP(S2)
Here the probability P(Si) is the probability of
labeling any word in a text with synset Si or with
one of the descendants of Si in the WordNet hier-
archy. We estimate these probabilities by counting
the number of occurrences of a synset in the Sem-
cor corpus (Fellbaum, 1998; Landes et al., 1998),
where all noun chunks are labeled with their Word-
Net synset. The probability P(Si) is computed as
</bodyText>
<equation confidence="0.999279666666667">
P(Si) = N (Si) + EK=1 P(Sk)
E 5n)
n=1 C(
</equation>
<bodyText confidence="0.999910833333333">
where C(Si) is the number of occurrences of Si,
N is the total number of synsets in WordNet and
K is the number of children of Si. The Word-
Net::Similarity package (Pedersen et al., 2004) im-
plements this distance measure and was used by the
authors.
</bodyText>
<subsectionHeader confidence="0.995589">
6.4 Seed synsets
</subsectionHeader>
<bodyText confidence="0.998057888888889">
We have manually selected 25 seed synsets in Word-
Net, where we tried to cover the wide range of topics
we were likely to encounter in the test corpus. We
have set the visualness of these seed synsets to either
1 (visual) or 0 (not visual). We determine the visu-
alness of all other synsets using these seed synsets.
A synset that is close to a visual seed synset gets a
high visualness and vice versa. We choose a linear
weighting:
</bodyText>
<equation confidence="0.998148666666667">
�vis(s) =
i vis(si)sim(s, si)
C(s)
</equation>
<bodyText confidence="0.9947558">
where vis(s) returns a number between 0 and 1 de-
noting the visualness of a synset s, si are the seed
synsets, sim(s, t) returns a number between 0 and 1
denoting the similarity between synsets s and t and
C(s) is constant given a synset s:
</bodyText>
<equation confidence="0.996097333333333">
C(s) = � sim(s, si)
i
2logP(Sp)
</equation>
<page confidence="0.987597">
1004
</page>
<subsectionHeader confidence="0.976312">
6.5 Evaluation of the visualness computation
</subsectionHeader>
<bodyText confidence="0.999971785714286">
To determine the visualness, we first assign the cor-
rect WordNet synset to every entity, after which we
compute a visualness score for these synsets. Since
these scores are floating point numbers, they are
hard to evaluate manually. During evaluation, we
make the simplifying assumption that all entities
with a visualness below a certain threshold are not
visual, and all entities above this threshold are vi-
sual. We choose this threshold to be 0.5. This re-
sults in an accuracy of 79.56%. Errors are mainly
caused by erroneous entity detection and classifica-
tion (63.10%) but also because of an incorrect as-
signment of the visualness (36.90%) by the method
described above.
</bodyText>
<sectionHeader confidence="0.994124" genericHeader="method">
7 Creating an appearance model using
salience and visualness
</sectionHeader>
<bodyText confidence="0.999979310344828">
In the previous section we have created a method to
calculate a visualness score for every entity, because
we stated that removing the entities which can never
be perceived visually will improve the performance
of our baseline system. An experiment proves that
this is exactly the case. If we assume that only the
entities that have a visualness above a 0.5 thresh-
old are visible and will appear in the image, we get
a precision of 48.81% and a recall of 87.98%. We
see from table 2 that this is already a significant im-
provement over the baseline system.
In section 4 we have seen that the salience mea-
sure helps in determining what persons are visible in
the image. We have used the fact that face detection
in images is relatively easily and can thus supply a
cut-off value for the ranked person names. In the
present state-of-the-art, we are not able to exploit a
similar fact when detecting all types of entities. We
will thus use the salience measure in a different way.
We compute the salience of every entity, and we
assume that only the entities with a salience score
above a threshold of 0.5 will appear in the image.
We see that this method drastically improves preci-
sion to 66.03%, but also lowers recall until 54.26%.
We now create a last model where we combine
both the visualness and the salience measures. We
want to calculate the probability of the occurrence of
an entity eim in the image, given a text t, P(eim|t).
We assume that this probability is proportional with
</bodyText>
<table confidence="0.9999168">
Precision Recall F-measure
Ent 15.62% 91.08% 26.66%
Ent+Vis 48.81% 87.98% 62.78%
Ent+Sal 66.03% 54.26% 59.56%
Ent+Vis+Sal 70.56% 67.82% 69.39%
</table>
<tableCaption confidence="0.996921">
Table 2: Comparison of methods to predict the en-
</tableCaption>
<bodyText confidence="0.977403">
tities that appear in the image, using entity detec-
tion (Ent), and the visualness (Vis) and salience (Sal)
measures.
the degree of visualness and salience of eim in t. In
our framework, P(eim|t) is computed as the product
of the salience of the entity eim and its visualness
score, as we assume both scores to be independent.
Again, for evaluation sake, we choose a threshold
of 0.4 to transform this continuous ranking into a
binary classification. This results in a precision of
70.56% and a recall of 67.82%. This model is the
best of the 4 models for entity annotation which have
been evaluated.
</bodyText>
<sectionHeader confidence="0.99743" genericHeader="method">
8 Related Research
</sectionHeader>
<bodyText confidence="0.9997655">
Using text that accompanies the image for annotat-
ing images and for training image recognition is not
new. The earliest work (only on person names) is
by Satoh (1999) and this research can be considered
as the closest to our work. The authors make a dis-
tinction between proper names, common nouns and
other words, and detect entities based on a thesaurus
list of persons, social groups and other words, thus
exploiting already simple semantics. Also a rudi-
mentary approach to discourse analysis is followed
by taking into account the position of words in a
text. The results were not satisfactory: 752 words
were extracted from video as candidates for being in
the accompanying images, but only 94 were correct
where 658 were false positives. Mori et al. (2000)
learn textual descriptions of images from surround-
ing texts. These authors filter nouns and adjectives
from the surrounding texts when they occur above
a certain frequency and obtain a maximum hit rate
of top 3 words that is situated between 30% and
40%. Other approaches consider both the textual
and image features when building a content model
of the image. For instance, some content is selected
from the text (such as person names) and from the
</bodyText>
<page confidence="0.981548">
1005
</page>
<bodyText confidence="0.999915094339623">
image (such as faces) and both contribute in describ-
ing the content of a document. This approach was
followed by Barnard (2003).
Westerveld (2000) combines image features and
words from collateral text into one semantic space.
This author uses Latent Semantic Indexing for rep-
resenting the image/text pair content. Ayache et al.
(2005) classify video data into different topical con-
cepts. The results of these approaches are often dis-
appointing. The methods here represent the text as a
bag of words possibly augmented with a tf (term fre-
quency) x idf (inverse document frequency) weight
of the words (Amir et al., 2005). In exceptional
cases, the hierarchical XML structure of a text doc-
ument (which was manually annotated) is taken into
account (Westerveld et al., 2005). The most inter-
esting work here to mention is the work of Berg
et al. (2004) who also process the nearly parallel
image-text pairs found in the Yahoo! news corpus.
They link faces in the image with names in the text
(recognized with named entity recognition), but do
not consider other objects. They consider pairs of
person names (text) and faces (image) and use clus-
tering with the Expectation Maximization algorithm
to find all faces belonging to a certain person. In
their model they consider the probability that an en-
tity is pictured given the textual context (i.e., the
part-of-speech tags immediately prior and after the
name, the location of the name in the text and the
distance to particular symbols such as “(R)”), which
is learned with a probabilistic classifier in each step
of the EM iteration. They obtained an accuracy of
84% on person face recognition.
In the CLASS project we work together with
groups specialized in image recognition. In future
work we will combine face and object recognition
with text analysis techniques. We expect the recog-
nition and disambiguation of faces to improve if
many image-text pairs that treat the same person are
used. On the other hand our approach is also valu-
able when there are few image-text pairs that picture
a certain person or object. The approach of Berg
et al. could be augmented with the typical features
that we use, namely salience and visualness. In De-
schacht et al. (2007) we have evaluated the ranking
of persons and objects by the method we have de-
scribed here and we have shown that this ranking
correlates with the importance of persons and ob-
jects in the picture.
None of the above state-of-the-art approaches
consider salience and visualness as discriminating
factors in the entity recognition, although these as-
pects could advance the state-of-the-art.
</bodyText>
<sectionHeader confidence="0.995516" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.99984815">
Our society in the 21st century produces gigantic
amounts of data, which are a mixture of different
media. Our repositories contain texts interwoven
with images, audio and video and we need auto-
mated ways to automatically index these data and
to automatically find interrelationships between the
various media contents. This is not an easy task.
However, if we succeed in recognizing and aligning
content in near-parallel image-text pairs, we might
be able to use this acquired knowledge in index-
ing comparable image-text pairs (e.g., in video) by
aligning content in these media.
In the experiment described above, we analyze
the discourse and semantics of texts of near-parallel
image-text pairs in order to compute the probability
that an entity mentioned in the text is also present in
the accompanying image. First, we have developed
an approach for computing the salience of each en-
tity mentioned in the text. Secondly, we have used
the WordNet classification in order to detect the vi-
sualness of an entity, which is translated into a vi-
sualness probability. The combined salience and vi-
sualness provide a score that signals the probability
that the entity is present in the accompanying image.
We extensively evaluated all the different modules
of our system, pinpointing weak points that could be
improved and exposing the potential of our work in
cross-media exploitation of content.
We were able to detect the persons in the text
that are also present in the image with a (evenly
weighted) F-measure of more than 95%, and in addi-
tion were able to detect the entities that are present
in the image with a F-measure of more than 69%.
These results have been obtained by relying only on
an analysis of the text and were substantially better
than the baseline approach. Even if we can not re-
solve all ambiguity, keeping the most confident hy-
potheses generated by our textual hypotheses will
greatly assist in analyzing images.
In the future we hope to extrinsically evaluate
</bodyText>
<page confidence="0.97335">
1006
</page>
<bodyText confidence="0.999945">
the proposed technologies, e.g., by testing whether
the recognized content in the text, improves image
recognition, retrieval of multimedia sources, mining
of these sources, and cross-media retrieval. In addi-
tion, we will investigate how we can build more re-
fined appearance models that incorporate attributes
and actions of entities.
</bodyText>
<sectionHeader confidence="0.997544" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999529875">
The work reported in this paper was supported
by the EU-IST project CLASS (Cognitive-Level
Annotation using Latent Statistical Structure, IST-
027978). We acknowledge the CLASS consortium
partners for their valuable comments and we are es-
pecially grateful to Yves Gufflet from the INRIA
research team (Grenoble, France) for collecting the
Yahoo! News dataset.
</bodyText>
<sectionHeader confidence="0.999424" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999905316455697">
Arnon Amir, Janne Argillander, Murray Campbell,
Alexander Haubold, Giridharan Iyengar, Shahram
Ebadollahi, Feng Kang, Milind R. Naphade, Apostol
Natsev, John R. Smith, Jelena Teˇsi´o, and Timo Volk-
mer. 2005. IBM Research TRECVID-2005 Video
Retrieval System. In Proceedings of TRECVID 2005,
Gaithersburg, MD.
St´ephane Ayache, Gearges M. Qunot, Jrme Gensel, and
Shin’Ichi Satoh. 2005. CLIPS-LRS-NII Experiments
at TRECVID 2005. In Proceedings of TRECVID
2005, Gaithersburg, MD.
Kobus Barnard, Pinar Duygulu, Nando de Freitas, David
Forsyth, David Blei, and Michael I. Jordan. 2003.
Matching Words and Pictures. Journal of Machine
Learning Research, 3(6):1107–1135.
Tamara L. Berg, Alexander C. Berg, Jaety Edwards, and
D.A. Forsyth. 2004. Who’s in the Picture? In Neural
Information Processing Systems, pages 137–144.
Koen Deschacht and Marie-Francine Moens. 2006. Ef-
ficient Hierarchical Entity Classification Using Con-
ditional Random Fields. In Proceedings of the
2nd Workshop on Ontology Learning and Population,
pages 33–40, Sydney, July.
Koen Deschacht, Marie-Francine Moens, and
W Robeyns. 2007. Cross-media entity recogni-
tion in nearly parallel visual and textual documents.
In Proceedings of the 8th RIAO Conference on Large-
Scale Semantic Access to Content (Text, Image, Video
and Sound). Cmu. (in press).
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. The MIT Press.
Brian Hayes. 1999. The Web of Words. American Sci-
entist, 87(2):108–112, March-April.
Jaap Kamps and Maarten Marx. 2002. Words with Atti-
tude. In Proceedings of the 1st International Confer-
ence on Global WordNet, pages 332–341, India.
Shari Landes, Claudia Leacock, and Randee I. Tengi.
1998. Building Semantic Concordances. In Chris-
tiane Fellbaum, editor, WordNet: An Electronic Lex-
ical Database. The MIT Press.
Dekang Lin. 1998. An Information-Theoretic Definition
of Similarity. In Proc. 15th International Conf. on Ma-
chine Learning.
Andrei Mikheev. 1997. Automatic Rule Induction for
Unknown-Word Guessing. Computational Linguis-
tics, 23(3):405–423.
Marie-Francine Moens, Patrick Jeuniaux, Roxana
Angheluta, and Rudradeb Mitra. 2006. Measur-
ing Aboutness of an Entity in a Text. In Proceed-
ings of HLT-NAACL 2006 TextGraphs: Graph-based
Algorithms for Natural Language Processing, East
Stroudsburg. ACL.
Marie-Francine Moens. 2006. Using Patterns of The-
matic Progression for Building a Table of Content of
a Text. Journal of Natural Language Engineering,
12(3):1–28.
Yasuhide Mori, Hironobu Takahashi, and Ryuichi Oka.
2000. Automatic Word Assignment to Images Based
on Image Division and Vector Quantization. In RIAO-
2000 Content-Based Multimedia Information Access,
Paris, April 12-14.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity - Measuring the Re-
latedness of Concepts. In The Proceedings ofFifth An-
nual Meeting ofthe North American Chapter ofthe As-
sociation for Computational Linguistics (NAACL-04),
Boston, May.
Shin’ichi Satoh, Yuichi Nakamura, and Takeo Kanade.
1999. Name-It: Naming and Detecting Faces in News
Videos. IEEE MultiMedia, 6(1):22–35, January-
March.
Thijs Westerveld, Jan C. van Gemert, Roberto Cornac-
chia, Djoerd Hiemstra, and Arjen de Vries. 2005. An
Integrated Approach to Text and Image Retrieval. In
Proceedings of TRECVID 2005, Gaithersburg, MD.
Thijs Westerveld. 2000. Image Retrieval: Content versus
Context. In Content-Based Multimedia Information
Access, RIAO 2000 Conference Proceedings, pages
276–284, April.
</reference>
<page confidence="0.991437">
1007
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.307775">
<title confidence="0.9133025">Text Analysis for Automatic Image Annotation Deschacht Moens</title>
<affiliation confidence="0.878836333333333">Interdisciplinary Centre for Law &amp; IT Department of Computer Science Katholieke Universiteit Leuven</affiliation>
<address confidence="0.980957">Tiensestraat 41, 3000 Leuven, Belgium</address>
<abstract confidence="0.972543384615385">We present a novel approach to automatically annotate images using associated text. We detect and classify all entities (persons and objects) in the text after which we determine the salience (the importance of an entity in a text) and visualness (the extent to which an entity can be perceived visually) of these entities. We combine these measures to compute the probability that an entity is present in the image. The suitability of our approach was successfully tested on 100 image-text pairs of Yahoo! News.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Arnon Amir</author>
<author>Janne Argillander</author>
<author>Murray Campbell</author>
<author>Alexander Haubold</author>
<author>Giridharan Iyengar</author>
<author>Shahram Ebadollahi</author>
<author>Feng Kang</author>
<author>Milind R Naphade</author>
<author>Apostol Natsev</author>
<author>John R Smith</author>
<author>Jelena Teˇsi´o</author>
<author>Timo Volkmer</author>
</authors>
<date>2005</date>
<booktitle>IBM Research TRECVID-2005 Video Retrieval System. In Proceedings of TRECVID 2005,</booktitle>
<location>Gaithersburg, MD.</location>
<marker>Amir, Argillander, Campbell, Haubold, Iyengar, Ebadollahi, Kang, Naphade, Natsev, Smith, Teˇsi´o, Volkmer, 2005</marker>
<rawString>Arnon Amir, Janne Argillander, Murray Campbell, Alexander Haubold, Giridharan Iyengar, Shahram Ebadollahi, Feng Kang, Milind R. Naphade, Apostol Natsev, John R. Smith, Jelena Teˇsi´o, and Timo Volkmer. 2005. IBM Research TRECVID-2005 Video Retrieval System. In Proceedings of TRECVID 2005, Gaithersburg, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>St´ephane Ayache</author>
<author>Gearges M Qunot</author>
<author>Jrme Gensel</author>
<author>Shin’Ichi Satoh</author>
</authors>
<title>CLIPS-LRS-NII Experiments at TRECVID</title>
<date>2005</date>
<booktitle>In Proceedings of TRECVID 2005,</booktitle>
<location>Gaithersburg, MD.</location>
<contexts>
<context position="23747" citStr="Ayache et al. (2005)" startWordPosition="4023" endWordPosition="4026">y and obtain a maximum hit rate of top 3 words that is situated between 30% and 40%. Other approaches consider both the textual and image features when building a content model of the image. For instance, some content is selected from the text (such as person names) and from the 1005 image (such as faces) and both contribute in describing the content of a document. This approach was followed by Barnard (2003). Westerveld (2000) combines image features and words from collateral text into one semantic space. This author uses Latent Semantic Indexing for representing the image/text pair content. Ayache et al. (2005) classify video data into different topical concepts. The results of these approaches are often disappointing. The methods here represent the text as a bag of words possibly augmented with a tf (term frequency) x idf (inverse document frequency) weight of the words (Amir et al., 2005). In exceptional cases, the hierarchical XML structure of a text document (which was manually annotated) is taken into account (Westerveld et al., 2005). The most interesting work here to mention is the work of Berg et al. (2004) who also process the nearly parallel image-text pairs found in the Yahoo! news corpus</context>
</contexts>
<marker>Ayache, Qunot, Gensel, Satoh, 2005</marker>
<rawString>St´ephane Ayache, Gearges M. Qunot, Jrme Gensel, and Shin’Ichi Satoh. 2005. CLIPS-LRS-NII Experiments at TRECVID 2005. In Proceedings of TRECVID 2005, Gaithersburg, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kobus Barnard</author>
<author>Pinar Duygulu</author>
<author>Nando de Freitas</author>
<author>David Forsyth</author>
<author>David Blei</author>
<author>Michael I Jordan</author>
</authors>
<title>Matching Words and Pictures.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>3</volume>
<issue>6</issue>
<marker>Barnard, Duygulu, de Freitas, Forsyth, Blei, Jordan, 2003</marker>
<rawString>Kobus Barnard, Pinar Duygulu, Nando de Freitas, David Forsyth, David Blei, and Michael I. Jordan. 2003. Matching Words and Pictures. Journal of Machine Learning Research, 3(6):1107–1135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tamara L Berg</author>
<author>Alexander C Berg</author>
<author>Jaety Edwards</author>
<author>D A Forsyth</author>
</authors>
<title>Who’s in the Picture?</title>
<date>2004</date>
<booktitle>In Neural Information Processing Systems,</booktitle>
<pages>137--144</pages>
<contexts>
<context position="4491" citStr="Berg et al., 2004" startWordPosition="719" endWordPosition="722">4 entities, of which 2.58 appear in the image. To build the appearance model of the text, we have combined different tools. We will evaluate every tool separately on 100 image-text pairs. This way we have a detailed view on the nature of the errors in the final model. 3 Automatically annotating person names Given a text that is associated with an image, we want to compute a probabilistic appearance model, i.e. a collection of entities that are visible in the image. We will start with a model that holds the names of the persons that appear in the image, such as was done by (Satoh et al., 1999; Berg et al., 2004), and extend this model in section 5 to include all other objects. Hiram Myers, of Edmond, Okla., walks across the fence, attempting to deliver what he called a ’people’s indictment’ of Halliburton CEO David Lesar, outside the site of the annual Halliburton shareholders meeting in Duncan, Okla., leading to his arrest, Wednesday, May 17, 2006. Figure 1: Image-text pair with entity “Hiram Myers” 3.1 Named Entity Recognition appearing both in the text and in the image. A logical first step to detect person names is Named types of objects (section 5) and improve it by defin- Entity Recognition (NE</context>
<context position="24261" citStr="Berg et al. (2004)" startWordPosition="4112" endWordPosition="4115"> author uses Latent Semantic Indexing for representing the image/text pair content. Ayache et al. (2005) classify video data into different topical concepts. The results of these approaches are often disappointing. The methods here represent the text as a bag of words possibly augmented with a tf (term frequency) x idf (inverse document frequency) weight of the words (Amir et al., 2005). In exceptional cases, the hierarchical XML structure of a text document (which was manually annotated) is taken into account (Westerveld et al., 2005). The most interesting work here to mention is the work of Berg et al. (2004) who also process the nearly parallel image-text pairs found in the Yahoo! news corpus. They link faces in the image with names in the text (recognized with named entity recognition), but do not consider other objects. They consider pairs of person names (text) and faces (image) and use clustering with the Expectation Maximization algorithm to find all faces belonging to a certain person. In their model they consider the probability that an entity is pictured given the textual context (i.e., the part-of-speech tags immediately prior and after the name, the location of the name in the text and </context>
</contexts>
<marker>Berg, Berg, Edwards, Forsyth, 2004</marker>
<rawString>Tamara L. Berg, Alexander C. Berg, Jaety Edwards, and D.A. Forsyth. 2004. Who’s in the Picture? In Neural Information Processing Systems, pages 137–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koen Deschacht</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Efficient Hierarchical Entity Classification Using Conditional Random Fields.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2nd Workshop on Ontology Learning and Population,</booktitle>
<pages>33--40</pages>
<location>Sydney,</location>
<contexts>
<context position="15628" citStr="Deschacht and Moens, 2006" startWordPosition="2615" endWordPosition="2618">n semantic database. We use the WordNet (Fellbaum, 1998) database, which organizes English nouns, verbs, adjectives and adverbs in synsets. A synset is a collection of words that have a close meaning and that represent an underlying concept. An example of such a synset is “person, individual, someone, somebody, mortal, soul”. All these words refer to a hu1003 man being. In order to correctly assign a noun in a text to its synset, i.e., to disambiguate the sense of this word, we use an efficient Word Sense Disambiguation (WSD) system that was developed by the authors and which is described in (Deschacht and Moens, 2006). Proper names are labeled by the Named Entity Recognizer, which recognizes persons, locations and organizations. These labels in turn allow us to assign the corresponding WordNet synset. The combination of the WSD system and the NER package achieved a 75.97% accuracy in classifying the entities. Apart from errors that resulted from erroneous entity detection (32.32%), errors were mainly due to the WSD system (60.56%) and in a smaller amount to the NER package (8.12%). 6.2 WordNet similarity We determine the visualness for every synset using a method that was inspired by Kamps and Marx (2002).</context>
</contexts>
<marker>Deschacht, Moens, 2006</marker>
<rawString>Koen Deschacht and Marie-Francine Moens. 2006. Efficient Hierarchical Entity Classification Using Conditional Random Fields. In Proceedings of the 2nd Workshop on Ontology Learning and Population, pages 33–40, Sydney, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koen Deschacht</author>
<author>Marie-Francine Moens</author>
<author>W Robeyns</author>
</authors>
<title>Cross-media entity recognition in nearly parallel visual and textual documents.</title>
<date>2007</date>
<booktitle>In Proceedings of the 8th RIAO Conference on LargeScale Semantic Access to Content (Text, Image, Video</booktitle>
<note>(in press).</note>
<contexts>
<context position="25626" citStr="Deschacht et al. (2007)" startWordPosition="4341" endWordPosition="4345">ained an accuracy of 84% on person face recognition. In the CLASS project we work together with groups specialized in image recognition. In future work we will combine face and object recognition with text analysis techniques. We expect the recognition and disambiguation of faces to improve if many image-text pairs that treat the same person are used. On the other hand our approach is also valuable when there are few image-text pairs that picture a certain person or object. The approach of Berg et al. could be augmented with the typical features that we use, namely salience and visualness. In Deschacht et al. (2007) we have evaluated the ranking of persons and objects by the method we have described here and we have shown that this ranking correlates with the importance of persons and objects in the picture. None of the above state-of-the-art approaches consider salience and visualness as discriminating factors in the entity recognition, although these aspects could advance the state-of-the-art. 9 Conclusion Our society in the 21st century produces gigantic amounts of data, which are a mixture of different media. Our repositories contain texts interwoven with images, audio and video and we need automated</context>
</contexts>
<marker>Deschacht, Moens, Robeyns, 2007</marker>
<rawString>Koen Deschacht, Marie-Francine Moens, and W Robeyns. 2007. Cross-media entity recognition in nearly parallel visual and textual documents. In Proceedings of the 8th RIAO Conference on LargeScale Semantic Access to Content (Text, Image, Video and Sound). Cmu. (in press).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="15058" citStr="Fellbaum, 1998" startWordPosition="2516" endWordPosition="2517">ry to determine what types of entities are more likely to appear in the image. 6 Detection of the visualness of an entity The assumption that every entity in the text appears in the image is rather crude. We will enrich our model with external world knowledge to find entities which are not likely to appear in an image. We define a measure called visualness, which is defined as the extent to which an entity can be perceived visually. 6.1 Entity classification After we have performed entity detection, we want to classify every entity according to a certain semantic database. We use the WordNet (Fellbaum, 1998) database, which organizes English nouns, verbs, adjectives and adverbs in synsets. A synset is a collection of words that have a close meaning and that represent an underlying concept. An example of such a synset is “person, individual, someone, somebody, mortal, soul”. All these words refer to a hu1003 man being. In order to correctly assign a noun in a text to its synset, i.e., to disambiguate the sense of this word, we use an efficient Word Sense Disambiguation (WSD) system that was developed by the authors and which is described in (Deschacht and Moens, 2006). Proper names are labeled by </context>
<context position="17945" citStr="Fellbaum, 1998" startWordPosition="3017" endWordPosition="3018">1999). The similarity measure defined by Lin (1998) uses the hypernym/hyponym relation to compute a semantic similarity between two WordNet synsets S1 and S2. First it finds the most specific (lowest in the tree) synset Sp that is a parent of both S1 and S2. Then it computes the similarity of S1 and S2 as sim(S1, S2) = logP(S1) + logP(S2) Here the probability P(Si) is the probability of labeling any word in a text with synset Si or with one of the descendants of Si in the WordNet hierarchy. We estimate these probabilities by counting the number of occurrences of a synset in the Semcor corpus (Fellbaum, 1998; Landes et al., 1998), where all noun chunks are labeled with their WordNet synset. The probability P(Si) is computed as P(Si) = N (Si) + EK=1 P(Sk) E 5n) n=1 C( where C(Si) is the number of occurrences of Si, N is the total number of synsets in WordNet and K is the number of children of Si. The WordNet::Similarity package (Pedersen et al., 2004) implements this distance measure and was used by the authors. 6.4 Seed synsets We have manually selected 25 seed synsets in WordNet, where we tried to cover the wide range of topics we were likely to encounter in the test corpus. We have set the visu</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Hayes</author>
</authors>
<title>The Web of Words.</title>
<date>1999</date>
<journal>American Scientist,</journal>
<volume>87</volume>
<issue>2</issue>
<contexts>
<context position="17336" citStr="Hayes, 1999" startWordPosition="2906" endWordPosition="2907">. Then we select a set of seed synsets, i.e. synsets with a predefined visualness, and use the similarity of a given synset to the seed synsets to determine the visualness. 6.3 Distance measure The WordNet database defines different relations between its synsets. An important relation for nouns is the hypernym/hyponym relation. A noun X is a hypernym of a noun Y if Y is a subtype or instance of X. For example, “bird” is a hypernym of “penguin” (and “penguin” is a hyponym of “bird”). A synset in WordNet can have one or more hypernyms. This relation organizes the synsets in a hierarchical tree (Hayes, 1999). The similarity measure defined by Lin (1998) uses the hypernym/hyponym relation to compute a semantic similarity between two WordNet synsets S1 and S2. First it finds the most specific (lowest in the tree) synset Sp that is a parent of both S1 and S2. Then it computes the similarity of S1 and S2 as sim(S1, S2) = logP(S1) + logP(S2) Here the probability P(Si) is the probability of labeling any word in a text with synset Si or with one of the descendants of Si in the WordNet hierarchy. We estimate these probabilities by counting the number of occurrences of a synset in the Semcor corpus (Fellb</context>
</contexts>
<marker>Hayes, 1999</marker>
<rawString>Brian Hayes. 1999. The Web of Words. American Scientist, 87(2):108–112, March-April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaap Kamps</author>
<author>Maarten Marx</author>
</authors>
<title>Words with Attitude.</title>
<date>2002</date>
<booktitle>In Proceedings of the 1st International Conference on Global WordNet,</booktitle>
<pages>332--341</pages>
<contexts>
<context position="16227" citStr="Kamps and Marx (2002)" startWordPosition="2713" endWordPosition="2716">hacht and Moens, 2006). Proper names are labeled by the Named Entity Recognizer, which recognizes persons, locations and organizations. These labels in turn allow us to assign the corresponding WordNet synset. The combination of the WSD system and the NER package achieved a 75.97% accuracy in classifying the entities. Apart from errors that resulted from erroneous entity detection (32.32%), errors were mainly due to the WSD system (60.56%) and in a smaller amount to the NER package (8.12%). 6.2 WordNet similarity We determine the visualness for every synset using a method that was inspired by Kamps and Marx (2002). Kamps and Marx use a distance measure defined on the adjectives of the WordNet database together with two seed adjectives to determine the emotive or affective meaning of any given adjective. They compute the relative distance of the adjective to the seed synsets “good” and “bad” and use this distance to define a measure of affective meaning. We take a similar approach to determine the visualness of a given synset. We first define a similarity measure between synsets in the WordNet database. Then we select a set of seed synsets, i.e. synsets with a predefined visualness, and use the similari</context>
</contexts>
<marker>Kamps, Marx, 2002</marker>
<rawString>Jaap Kamps and Maarten Marx. 2002. Words with Attitude. In Proceedings of the 1st International Conference on Global WordNet, pages 332–341, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shari Landes</author>
<author>Claudia Leacock</author>
<author>Randee I Tengi</author>
</authors>
<title>Building Semantic Concordances.</title>
<date>1998</date>
<editor>In Christiane Fellbaum, editor,</editor>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="17967" citStr="Landes et al., 1998" startWordPosition="3019" endWordPosition="3022">arity measure defined by Lin (1998) uses the hypernym/hyponym relation to compute a semantic similarity between two WordNet synsets S1 and S2. First it finds the most specific (lowest in the tree) synset Sp that is a parent of both S1 and S2. Then it computes the similarity of S1 and S2 as sim(S1, S2) = logP(S1) + logP(S2) Here the probability P(Si) is the probability of labeling any word in a text with synset Si or with one of the descendants of Si in the WordNet hierarchy. We estimate these probabilities by counting the number of occurrences of a synset in the Semcor corpus (Fellbaum, 1998; Landes et al., 1998), where all noun chunks are labeled with their WordNet synset. The probability P(Si) is computed as P(Si) = N (Si) + EK=1 P(Sk) E 5n) n=1 C( where C(Si) is the number of occurrences of Si, N is the total number of synsets in WordNet and K is the number of children of Si. The WordNet::Similarity package (Pedersen et al., 2004) implements this distance measure and was used by the authors. 6.4 Seed synsets We have manually selected 25 seed synsets in WordNet, where we tried to cover the wide range of topics we were likely to encounter in the test corpus. We have set the visualness of these seed s</context>
</contexts>
<marker>Landes, Leacock, Tengi, 1998</marker>
<rawString>Shari Landes, Claudia Leacock, and Randee I. Tengi. 1998. Building Semantic Concordances. In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An Information-Theoretic Definition of Similarity. In</title>
<date>1998</date>
<booktitle>Proc. 15th International Conf. on Machine Learning.</booktitle>
<contexts>
<context position="17382" citStr="Lin (1998)" startWordPosition="2913" endWordPosition="2914">sets with a predefined visualness, and use the similarity of a given synset to the seed synsets to determine the visualness. 6.3 Distance measure The WordNet database defines different relations between its synsets. An important relation for nouns is the hypernym/hyponym relation. A noun X is a hypernym of a noun Y if Y is a subtype or instance of X. For example, “bird” is a hypernym of “penguin” (and “penguin” is a hyponym of “bird”). A synset in WordNet can have one or more hypernyms. This relation organizes the synsets in a hierarchical tree (Hayes, 1999). The similarity measure defined by Lin (1998) uses the hypernym/hyponym relation to compute a semantic similarity between two WordNet synsets S1 and S2. First it finds the most specific (lowest in the tree) synset Sp that is a parent of both S1 and S2. Then it computes the similarity of S1 and S2 as sim(S1, S2) = logP(S1) + logP(S2) Here the probability P(Si) is the probability of labeling any word in a text with synset Si or with one of the descendants of Si in the WordNet hierarchy. We estimate these probabilities by counting the number of occurrences of a synset in the Semcor corpus (Fellbaum, 1998; Landes et al., 1998), where all nou</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An Information-Theoretic Definition of Similarity. In Proc. 15th International Conf. on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrei Mikheev</author>
</authors>
<title>Automatic Rule Induction for Unknown-Word Guessing.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="13620" citStr="Mikheev, 1997" startWordPosition="2256" endWordPosition="2257"> a text helps in determining the persons that appear in the image. 5 Automatically annotating persons and objects After having developed a reasonable successful system to detect what persons will appear in the image, we turn to a more difficult case : Detecting persons and all other objects that are described in the text. 5.1 Entity detection We will first detect what words in the text refer to an entity. For this, we perform part-of-speech tagging (i.e., detecting the syntactic word class such as noun, verb, etc.). We take that every noun in the text represents an entity. We have used LTPOS (Mikheev, 1997), which performed the task almost errorless (precision of 98.144% and recall of 97.36% on the nouns in the test corpus). Person names which were segmented using the NER package are also marked as entities. 5.2 Baseline system We want to detect the objects and the names of persons which are both visible in the image and described in the text. We start with a simple baseline system, in which we assume that every entity in the text appears in the image. As can be expected, this results in a high recall (91.08%), and a very low precision (15.62%). We see that the problem here is far more difficult</context>
</contexts>
<marker>Mikheev, 1997</marker>
<rawString>Andrei Mikheev. 1997. Automatic Rule Induction for Unknown-Word Guessing. Computational Linguistics, 23(3):405–423.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Francine Moens</author>
<author>Patrick Jeuniaux</author>
<author>Roxana Angheluta</author>
<author>Rudradeb Mitra</author>
</authors>
<title>Measuring Aboutness of an Entity in a Text.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<publisher>ACL.</publisher>
<contexts>
<context position="10560" citStr="Moens et al., 2006" startWordPosition="1744" endWordPosition="1747">e for each noun entity er in the discourse its salience (Sal1) in the discourse tree, which is proportional with the depth of the entity in the discourse tree -hereby assuming that deeper in this tree more detailed topics of a text are describedand normalize this value to be between zero and one. When an entity occurs in different subtrees, its maximum score is chosen. 4.2 Refinement with sentence parse information Because not all entities of the text are captured in the discourse tree, we implement an additional refinement of the computation of the salience of an entity which is inspired by (Moens et al., 2006). The segmentation module already determines the main topic of a sentence. Since the syntactic structure is often indicative of the information distribution in a sentence, we can determine the relative importance of the other entities in a sentence by relying on the relationships between entities as signaled by the parse tree. When determining the salience of an entity, we take into account the level of the entity mention in the parse tree (Sal2), and the number of children for the entity in this structure (Sal3), where the normalized score is respectively inversely proportional with the depth</context>
</contexts>
<marker>Moens, Jeuniaux, Angheluta, Mitra, 2006</marker>
<rawString>Marie-Francine Moens, Patrick Jeuniaux, Roxana Angheluta, and Rudradeb Mitra. 2006. Measuring Aboutness of an Entity in a Text. In Proceedings of HLT-NAACL 2006 TextGraphs: Graph-based Algorithms for Natural Language Processing, East Stroudsburg. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Francine Moens</author>
</authors>
<title>Using Patterns of Thematic Progression for Building a Table of Content of a Text.</title>
<date>2006</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="9213" citStr="Moens, 2006" startWordPosition="1524" endWordPosition="1525">ufficient for distinguishing between important and less important entities. We define a measure, salience, which is a number between 0 and 1 that represents the importance of an entity in a text. We present here a method for computing this score based on an in depth analysis of the discourse of the text and of the syntactic structure of the individual sentences. 4.1 Discourse segmentation The discourse segmentation module, which we developed in earlier research, hierarchically and sequentially segments the discourse in different topics and subtopics resulting in a table of contents of a text (Moens, 2006). The table shows the main entities and the related subtopic entities in a tree-like structure that also indicates the segments (by means of character pointers) to which an entity applies. The algorithm detects patterns of thematic progression in texts and can thus recognize the main topic of a sentence (i.e., about whom or what the sentence speaks) and the hierarchical and sequential relationships between individual topics. A mixture model, taking into account different discourse features, is trained with the Expectation Maximization algorithm on an annotated DUC-2003 corpus. We use the resul</context>
<context position="12009" citStr="Moens, 2006" startWordPosition="1976" endWordPosition="1977">or these three values, which are respectively 0.8, 0.1 and 0.1. Eventually, we could learn these coefficients from a training corpus (e.g., with the 1002 Precision Recall F-measure NER 71.27% 95.56% 81.65% NER+DYN 97.66% 92.59% 95.06% Table 1: Comparison of methods to predict what persons described in the text will appear in the image, using Named Entity Recognition (NER), and the salience measure with dynamic cut-off (DYN). Expectation Maximization algorithm). We do not separately evaluate our technology for salience detection as this technology was already extensively evaluated in the past (Moens, 2006). 4.3 Evaluating the improved system The salience measure defines a ranking of all the persons in a text. We will use this ranking to improve our baseline system. We assume that it is possible to automatically determine the number of faces that are recognized in the image, which gives us an indication of a suitable cut-off value. This approach is reasonable since face detection (determine whether a face is present in the image) is significant easier than face recognition (determine which person is present in the image). In the improved model we assume that persons which are ranked higher than,</context>
<context position="15628" citStr="Moens, 2006" startWordPosition="2617" endWordPosition="2618">abase. We use the WordNet (Fellbaum, 1998) database, which organizes English nouns, verbs, adjectives and adverbs in synsets. A synset is a collection of words that have a close meaning and that represent an underlying concept. An example of such a synset is “person, individual, someone, somebody, mortal, soul”. All these words refer to a hu1003 man being. In order to correctly assign a noun in a text to its synset, i.e., to disambiguate the sense of this word, we use an efficient Word Sense Disambiguation (WSD) system that was developed by the authors and which is described in (Deschacht and Moens, 2006). Proper names are labeled by the Named Entity Recognizer, which recognizes persons, locations and organizations. These labels in turn allow us to assign the corresponding WordNet synset. The combination of the WSD system and the NER package achieved a 75.97% accuracy in classifying the entities. Apart from errors that resulted from erroneous entity detection (32.32%), errors were mainly due to the WSD system (60.56%) and in a smaller amount to the NER package (8.12%). 6.2 WordNet similarity We determine the visualness for every synset using a method that was inspired by Kamps and Marx (2002).</context>
</contexts>
<marker>Moens, 2006</marker>
<rawString>Marie-Francine Moens. 2006. Using Patterns of Thematic Progression for Building a Table of Content of a Text. Journal of Natural Language Engineering, 12(3):1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yasuhide Mori</author>
<author>Hironobu Takahashi</author>
<author>Ryuichi Oka</author>
</authors>
<title>Automatic Word Assignment to Images Based on Image Division and Vector Quantization.</title>
<date>2000</date>
<booktitle>In RIAO2000 Content-Based Multimedia Information Access,</booktitle>
<pages>12--14</pages>
<location>Paris,</location>
<contexts>
<context position="22956" citStr="Mori et al. (2000)" startWordPosition="3893" endWordPosition="3896">mes) is by Satoh (1999) and this research can be considered as the closest to our work. The authors make a distinction between proper names, common nouns and other words, and detect entities based on a thesaurus list of persons, social groups and other words, thus exploiting already simple semantics. Also a rudimentary approach to discourse analysis is followed by taking into account the position of words in a text. The results were not satisfactory: 752 words were extracted from video as candidates for being in the accompanying images, but only 94 were correct where 658 were false positives. Mori et al. (2000) learn textual descriptions of images from surrounding texts. These authors filter nouns and adjectives from the surrounding texts when they occur above a certain frequency and obtain a maximum hit rate of top 3 words that is situated between 30% and 40%. Other approaches consider both the textual and image features when building a content model of the image. For instance, some content is selected from the text (such as person names) and from the 1005 image (such as faces) and both contribute in describing the content of a document. This approach was followed by Barnard (2003). Westerveld (200</context>
</contexts>
<marker>Mori, Takahashi, Oka, 2000</marker>
<rawString>Yasuhide Mori, Hironobu Takahashi, and Ryuichi Oka. 2000. Automatic Word Assignment to Images Based on Image Division and Vector Quantization. In RIAO2000 Content-Based Multimedia Information Access, Paris, April 12-14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>WordNet::Similarity - Measuring the Relatedness of Concepts.</title>
<date>2004</date>
<booktitle>In The Proceedings ofFifth Annual Meeting ofthe North American Chapter ofthe Association for Computational Linguistics (NAACL-04),</booktitle>
<location>Boston,</location>
<contexts>
<context position="18294" citStr="Pedersen et al., 2004" startWordPosition="3082" endWordPosition="3085">Here the probability P(Si) is the probability of labeling any word in a text with synset Si or with one of the descendants of Si in the WordNet hierarchy. We estimate these probabilities by counting the number of occurrences of a synset in the Semcor corpus (Fellbaum, 1998; Landes et al., 1998), where all noun chunks are labeled with their WordNet synset. The probability P(Si) is computed as P(Si) = N (Si) + EK=1 P(Sk) E 5n) n=1 C( where C(Si) is the number of occurrences of Si, N is the total number of synsets in WordNet and K is the number of children of Si. The WordNet::Similarity package (Pedersen et al., 2004) implements this distance measure and was used by the authors. 6.4 Seed synsets We have manually selected 25 seed synsets in WordNet, where we tried to cover the wide range of topics we were likely to encounter in the test corpus. We have set the visualness of these seed synsets to either 1 (visual) or 0 (not visual). We determine the visualness of all other synsets using these seed synsets. A synset that is close to a visual seed synset gets a high visualness and vice versa. We choose a linear weighting: �vis(s) = i vis(si)sim(s, si) C(s) where vis(s) returns a number between 0 and 1 denoting</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. WordNet::Similarity - Measuring the Relatedness of Concepts. In The Proceedings ofFifth Annual Meeting ofthe North American Chapter ofthe Association for Computational Linguistics (NAACL-04), Boston, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shin’ichi Satoh</author>
<author>Yuichi Nakamura</author>
<author>Takeo Kanade</author>
</authors>
<title>Name-It: Naming and Detecting Faces in News Videos.</title>
<date>1999</date>
<journal>IEEE MultiMedia,</journal>
<volume>6</volume>
<issue>1</issue>
<pages>JanuaryMarch.</pages>
<contexts>
<context position="4471" citStr="Satoh et al., 1999" startWordPosition="715" endWordPosition="718">e texts contain 15.04 entities, of which 2.58 appear in the image. To build the appearance model of the text, we have combined different tools. We will evaluate every tool separately on 100 image-text pairs. This way we have a detailed view on the nature of the errors in the final model. 3 Automatically annotating person names Given a text that is associated with an image, we want to compute a probabilistic appearance model, i.e. a collection of entities that are visible in the image. We will start with a model that holds the names of the persons that appear in the image, such as was done by (Satoh et al., 1999; Berg et al., 2004), and extend this model in section 5 to include all other objects. Hiram Myers, of Edmond, Okla., walks across the fence, attempting to deliver what he called a ’people’s indictment’ of Halliburton CEO David Lesar, outside the site of the annual Halliburton shareholders meeting in Duncan, Okla., leading to his arrest, Wednesday, May 17, 2006. Figure 1: Image-text pair with entity “Hiram Myers” 3.1 Named Entity Recognition appearing both in the text and in the image. A logical first step to detect person names is Named types of objects (section 5) and improve it by defin- En</context>
</contexts>
<marker>Satoh, Nakamura, Kanade, 1999</marker>
<rawString>Shin’ichi Satoh, Yuichi Nakamura, and Takeo Kanade. 1999. Name-It: Naming and Detecting Faces in News Videos. IEEE MultiMedia, 6(1):22–35, JanuaryMarch.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thijs Westerveld</author>
<author>Jan C van Gemert</author>
<author>Roberto Cornacchia</author>
<author>Djoerd Hiemstra</author>
<author>Arjen de Vries</author>
</authors>
<title>An Integrated Approach to Text and Image Retrieval.</title>
<date>2005</date>
<booktitle>In Proceedings of TRECVID 2005,</booktitle>
<location>Gaithersburg, MD.</location>
<marker>Westerveld, van Gemert, Cornacchia, Hiemstra, de Vries, 2005</marker>
<rawString>Thijs Westerveld, Jan C. van Gemert, Roberto Cornacchia, Djoerd Hiemstra, and Arjen de Vries. 2005. An Integrated Approach to Text and Image Retrieval. In Proceedings of TRECVID 2005, Gaithersburg, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thijs Westerveld</author>
</authors>
<title>Image Retrieval: Content versus Context. In Content-Based Multimedia Information Access,</title>
<date>2000</date>
<booktitle>RIAO 2000 Conference Proceedings,</booktitle>
<pages>276--284</pages>
<contexts>
<context position="23558" citStr="Westerveld (2000)" startWordPosition="3996" endWordPosition="3997">i et al. (2000) learn textual descriptions of images from surrounding texts. These authors filter nouns and adjectives from the surrounding texts when they occur above a certain frequency and obtain a maximum hit rate of top 3 words that is situated between 30% and 40%. Other approaches consider both the textual and image features when building a content model of the image. For instance, some content is selected from the text (such as person names) and from the 1005 image (such as faces) and both contribute in describing the content of a document. This approach was followed by Barnard (2003). Westerveld (2000) combines image features and words from collateral text into one semantic space. This author uses Latent Semantic Indexing for representing the image/text pair content. Ayache et al. (2005) classify video data into different topical concepts. The results of these approaches are often disappointing. The methods here represent the text as a bag of words possibly augmented with a tf (term frequency) x idf (inverse document frequency) weight of the words (Amir et al., 2005). In exceptional cases, the hierarchical XML structure of a text document (which was manually annotated) is taken into account</context>
</contexts>
<marker>Westerveld, 2000</marker>
<rawString>Thijs Westerveld. 2000. Image Retrieval: Content versus Context. In Content-Based Multimedia Information Access, RIAO 2000 Conference Proceedings, pages 276–284, April.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>