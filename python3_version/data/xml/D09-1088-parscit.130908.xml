<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.9988885">
An Alternative to Head-Driven Approaches for
Parsing a (Relatively) Free Word-Order Language
</title>
<author confidence="0.991692">
Reut Tsarfaty Khalil Sima’an Remko Scha
</author>
<affiliation confidence="0.9962375">
Institute for Logic Language and Computation
University of Amsterdam
</affiliation>
<email confidence="0.998405">
{r.tsarfaty,k.simaan,r.scha}@uva.nl
</email>
<sectionHeader confidence="0.997384" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999912529411765">
Applying statistical parsers developed for
English to languages with freer word-
order has turned out to be harder than
expected. This paper investigates the
adequacy of different statistical parsing
models for dealing with a (relatively)
free word-order language. We show
that the recently proposed Relational-
Realizational (RR) model consistently
outperforms state-of-the-art Head-Driven
(HD) models on the Hebrew Treebank.
Our analysis reveals a weakness of HD
models: their intrinsic focus on configu-
rational information. We conclude that the
form-function separation ingrained in RR
models makes them better suited for pars-
ing nonconfigurational phenomena.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999921982758621">
Parsing technology has come a long way since
Charniak (1996) demonstrated that a simple tree-
bank PCFG performs better than any other parser
(with F175 accuracy) on parsing the WSJ Penn
treebank (Marcus et al., 1993). Treebank Gram-
mars (Scha, 1990; Charniak, 1996) trained on
large corpora nowadays present the best available
means to parse natural language text.
The performance curve for parsing the WSJ was
a steep one at first, as the incorporation of no-
tions such as head, distance, subcategorization
(Charniak, 1997; Collins, 1999) brought about
a dramatic increase in parsing accuracy to the
level of F188. Discriminative approaches, Data-
Oriented Parsing (‘all-subtrees’) approaches, and
self-training techniques brought further improve-
ments, and recent results are starting to level off at
around F192.1(McClosky et al., 2008).
As the interest of the NLP community grows
to encompass more languages, we observe efforts
towards adapting an English parser for parsing
other languages (e.g., (Collins et al., 1999)), or
towards designing a language-independent frame-
work based on principles underlying the mod-
els for parsing English (Bikel, 2002). The per-
formance curve for parsing other languages with
these models looks rather different. A case in point
is Modern Standard Arabic. Since the initial ef-
fort of (Bikel, 2002) to parse the Arabic treebank
(Maamouri et al., 2004), which yielded F175 ac-
curacy, four years and successive revisions have
led to no more than F179 (Maamouri et al., 2008).
This pattern from Arabic is not peculiar. The
level of state-of-the-art results for other languages
still lags behind those for English, even after
putting considerable effort into the adaptation.1
Given that these languages are inherently differ-
ent from English and from one another, it appears
that we cannot avoid a question concerning the ad-
equacy of the models used to parse them. That is,
given the properties of a language, which model-
ing strategy would be appropriate for parsing it?
Until recently, there has been practically
no computationally affordable alternative to the
Head-Driven (HD) approach in the development
of phrase-structure based statistical parsing mod-
els. Recently, we proposed the Relational-
Realizational (RR) approach that rests upon differ-
ent premises (Tsarfaty and Sima’an, 2008). The
question of how the RR model fares against the
HD models that have so far been predominantly
used has never been tackled. Yet, it is precisely
such a comparison that can shed new light on the
question of adequacy we posed above.
Empirically quantifying the effects of differ-
ent modeling choices has been addressed for En-
glish by, e.g., (Johnson, 1998; Klein and Manning,
2003), and for German by, e.g., (Dubey, 2004;
</bodyText>
<footnote confidence="0.922143333333333">
1Consider, e.g., “The PaGe shared task on parsing Ger-
man” (Kubler, 2008), reporting F,75, F,79, F,83 for the
participating parsers.
</footnote>
<page confidence="0.903239">
842
</page>
<note confidence="0.997092">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 842–851,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.98904784">
Rafferty and Manning, 2008). This paper provides
an empirical systematic comparison of conceptu-
ally different modeling strategies with respect to
parsing Hebrew. This comparison is intended to
provide a first answer to the question of parser ad-
equacy in the face of word-order freedom.
Our two empirical results are unequivocal.
Firstly, RR models significantly outperform HD
models (about 2 points absolute improvement in
Fi) in parsing the Modern Hebrew treebank. In
particular, RR models show better performance
in identifying the constituents for which syntactic
positions are relatively free. Secondly, we show
a novel variation of the HD model, incorporating
the Relational notions of the RR model, on the hy-
pothesis that this might bridge the gap. The RR
model remains superior.
Our post-experimental analysis shows that HD
modeling is inherently problematic for parsing a
language with freer word-order because of the
hard-wiring of notions such as left, right and dis-
tance from the head. RR models, taking a prin-
cipled approach towards capturing variable form-
function correspondence patterns, are better suited
for parsing nonconfigurational phenomena.
</bodyText>
<sectionHeader confidence="0.988954" genericHeader="introduction">
2 The Data
</sectionHeader>
<bodyText confidence="0.999822">
This section describes some properties of Modern
Hebrew (henceforth, Hebrew) that make it signifi-
cantly different from English. These properties af-
fect the syntactic representations found in the He-
brew Treebank and the kind of syntactic phenom-
ena a parser for Hebrew has to cope with.
Modern Hebrew is a Semitic language with a
canonical SVO word-order pattern,2 yet it allows
considerable freedom in the placement of syntac-
tic constituents in a clause. For example, linguistic
elements of any kind may be fronted, triggering
an inversion familiar from Germanic languages
as in (1b) (Triggered Inversion (TI) in (Shlonsky,
1997)). Under some information structuring con-
ditions, Verb Initial (VI) constructions are also al-
lowed, as in (1c) (Melnik, 2002). All sentences
in (1) thus mean “Dani gave the present to Dina”,
despite their different word-ordering.
</bodyText>
<listItem confidence="0.74819175">
(1) a. dani natan et hamatana ledina
Dani gave ACC the-present to-Dina
b. et hamatana natan dani ledina
ACC the-present gave Dani to-Dina
</listItem>
<footnote confidence="0.9982405">
2SVO is an abbreviation for the Subject-Verb-Object type
in the basic word-order typology of (Greenberg, 1963).
</footnote>
<table confidence="0.9506068">
Word Order Frequency Relative Frequency
SV 1612 41%
VS 1144 29%
No S 624 16%
No V 550 14%
</table>
<tableCaption confidence="0.946457">
Table 1: Modern Hebrew Predicative Clause-
Types in 3930 Predicative Matrix Clauses in the
Training Set of the Modern Hebrew Treebank.
</tableCaption>
<bodyText confidence="0.985202">
c. natan dani et hamatana ledina
gave Dani ACC the-present to-Dina
A corpus study we conducted on a fragment of
the Modern Hebrew treebank reveals that although
there is a significant number of subjects preceding
verbs in simple (matrix) clauses (41%), there are
also a fair number of sentences for which this or-
der is reversed (29%), and there is evidence for
other configurations, such as empty realization of
subjects (16%) and non-verbal realization of pred-
icates (14%).
In the face of such lack of consistency in its
configurational position, the grammatical function
Object in Hebrew is indicated by Differential Ob-
ject Marking (DOM) (Aissen, 2003). NP objects
in Hebrew are marked for accusativity (using the
marker et) if they are also marked for definiteness
(indicated by the prefix ha). So, in contrast with
(2a)-(2b), the indefinite object renders (2c) un-
grammatical, and the missing accusativity renders
(2d) awkward. The fact that marking NP objects
involves the joint contribution of multiple surface
elements (et, ha) contributing features to the NP
constituent is referred to as extended exponence
(Matthews, 1993, p. 182).
</bodyText>
<listItem confidence="0.8920167">
(2) a. dani natan matana ledina
Dani gave present to-Dina
“Dani gave a present to Dina”
b. dani natan et hamatana ledina
Dani gave ACC the-present to-Dina
“Dani gave the present to Dina”
c. *dani natan et matana ledina
Dani gave ACC present to-Dina
d. ??dani natan hamatana ledina
Dani gave the-present to-Dina
</listItem>
<bodyText confidence="0.99963025">
These data pose a challenge to generative pars-
ing models, as they would be required to gener-
ate alternative word-order patterns while maintain-
ing a coherent pattern of object marking, encom-
</bodyText>
<page confidence="0.99742">
843
</page>
<bodyText confidence="0.999677333333333">
passing the contribution of multiple surface expo-
nents. The question this paper addresses is there-
fore what kind of modeling approach would be ad-
equate for modeling the interplay between syntax
and morphology in marking grammatical relations
in Hebrew, as reflected by the sentence-pair (3).
They both mean, roughly, “Dani gave the present
to Dina yesterday; their word-order vary, but the
pattern of object marking is retained.
</bodyText>
<listItem confidence="0.480613">
(3) a. dani natan etmol et hamatana ledina
Dani gave yesterday ACC the-present
to-Dina
b. et hamatana natan etmol dani ledina
ACC the-present gave yesterday dani
to-dina
</listItem>
<sectionHeader confidence="0.995186" genericHeader="method">
3 The Models
</sectionHeader>
<bodyText confidence="0.999976333333333">
The different models we experiment with are all
trained on syntactic structures annotated in the
Modern Hebrew Treebank (Sima’an et al., 2001).
The native representation of clause-level cate-
gories in the Treebank employs flat structures.
This choice was made due to the lack of empirical
evidence in Hebrew for grouping freely positioned
syntactic elements to form a constituent.3 In order
to compensate for the ambiguity in the interpreta-
tion of flat structures, additional information such
as morphological marking and grammatical func-
tion labels is added to the phrase-structure trees.
</bodyText>
<subsectionHeader confidence="0.999785">
3.1 The State-Splits Approach
</subsectionHeader>
<bodyText confidence="0.999957066666667">
The simplest way to encode grammatical func-
tions information on top of the phrase-structure
representation in the treebank is by decorating
non-terminal nodes with morphological or func-
tional features, similarly to the rich representation
format of syntactic categories in GPSG. This is
the approach taken by the annotators of the He-
brew treebank in which information about mor-
phological marking appears at multiple levels of
constituency (Guthmann et al., 2009), and func-
tional features (such as subject, object, etc.) deco-
rate phrase-level constituent labels (Sima’an et al.,
2001). The S-level representation of our example
sentences (3a)–(3b) then would be as we depict
in figure 1, which can be read off as feature-rich
</bodyText>
<footnote confidence="0.757145833333333">
3Such clauses are defined formally as exocentric in for-
mal theories of syntax, and are used to describe syntactic
structures in, e.g., Tagalog, Hungarian and Warlpiri (Bres-
nan, 2001, page 110). This flat representation format is char-
acteristic of treebanks for other languages with relatively-free
word-order as well, such as German (cf. (Kubler, 2008)).
</footnote>
<bodyText confidence="0.996738666666667">
PCFG productions. We refer to this approach as
the State-Splits (SP) approach, which serves as the
baseline for the rest of our investigation.
</bodyText>
<subsectionHeader confidence="0.999836">
3.2 The Head-Driven Approach
</subsectionHeader>
<bodyText confidence="0.999985607142857">
Following the linguistic wisdom that the inter-
nal organization of syntactic constituents revolves
around their heads, Head-Driven (HD) models
have been proposed by (Magerman, 1995; Char-
niak, 1997; Collins, 1999). In a generative HD
model, the head daughter is generated first, con-
ditioned on properties of the mother node. Then,
sisters of the head daughter are generated condi-
tioned on the head, typically by left and right gen-
eration processes. Overall, HD processes have the
modeling advantage that they capture structurally-
marked positions that characterize the argument
structure of the sentence. The simplest possible
process uses unigram probabilities, but (Klein and
Manning, 2003) show that using vertical and hori-
zontal Markovization improves parsing accuracy.4
An unlexicalized generative HD model will
generate our two example sentences as we illus-
trate in figure 2. The generation of the context-
free events in figure 1 is then broken down to
seven different context-free parameters each, en-
coding head-parent and head-sister structural rela-
tionships — the latter mediated with a structurally-
marked delta function (Δi). The rich morpho-
logical representation of phrase-level NP objects
(+def/acc), for instance, is conditioned on the
head sister, its direction, and the distance from the
head (check, e.g., nodes ΔL1, ΔR2).
</bodyText>
<subsectionHeader confidence="0.999757">
3.3 The Relational-Realizational Approach
</subsectionHeader>
<bodyText confidence="0.999916272727273">
The Relational-Realizational (RR) parsing model
of (Tsarfaty and Sima’an, 2008) similarly decom-
poses the generation of the context-free events in
figure 1 into multiple independent parameters, but
does so in a conceptually different way. Instead of
decomposing a context-free event to head and sis-
ters, the RR model is best viewed as a generative
grammar that decomposes it to form and function.
The RR grammar first generates a set of gram-
matical functions depicting the Relational Net-
work (RN) (Perlmutter, 1982) of the clause. This
</bodyText>
<footnote confidence="0.978643833333333">
4The success of Head-Driven models (Charniak, 1997;
Collins, 2003) was initially attributed to the fact that they
were fully lexicalized, but (Klein and Manning, 2003) show
that an unlexicalized model combining Head-Driven Marko-
vian processes with linguistically motivated state-splits can
approach the performance of fully lexicalized models.
</footnote>
<page confidence="0.995769">
844
</page>
<figure confidence="0.9931206">
(3a) S
NP-SBJ VP-PRD ADVP NP+D+ACC-OBJ PP-COM
Dani natan etmol et-hamatana le-dina
gave yesterday the-present to-Dina
(3b) S
</figure>
<figureCaption confidence="0.66079975">
NP+D+ACC-OBJ VP-PRD ADVP NP-SBJ PP-COM
et-ha-matana natan etmol Dani le-dina
the-present gave yesterday Dani to-Dina
Figure 1: The State-Splits Approach for Ex. (3)
</figureCaption>
<equation confidence="0.794820423076923">
(3a) S
V P@S
L,ΔL1, V P@S
NP
Dani VP
Dani natan
gave
R,ΔR2, V P@S
NP+D+ACC R,ΔR3, V P@S
et-ha-matana
the-Present PP
le-dina
to-Dina
(3b) S
V P@S
L,ΔLt, V P@S
et-ha-matana
NP+D+ACC VP
natan
the-present
gave
NP R,ΔR3, V P@S
Dani
Dani PP
le-dina
to-Dina
</equation>
<figureCaption confidence="0.984503">
Figure 2: The Head-Driven Approach for Ex. (3)
</figureCaption>
<figure confidence="0.726937">
(3a) S
{SBJ, PRD, OBJ, COM}@S
SBJ@S PRD@S PRD:OBJ@S OBJ@S COM@S
NP VP ADVP NP+D+ACC PP
Dani natan etmol et-hamatana le-dina
Dani gave yesterday the-present to-Dina
(3b) S
{SBJ, PRD, OBJ, COM}@S
</figure>
<figureCaption confidence="0.487537">
OBJ@S PRD@S PRD: SBJ@S SBJ@S COM@S
NP+D+ACC VP ADVP@S NP PP
et-ha-matana natan etmol Dani le-dina
the-Present gave yesterday Dani to-Dina
Figure 3: The Relational-Realizational Approach
</figureCaption>
<bodyText confidence="0.999979741935484">
RN provides an abstract set-theoretic representa-
tion of the argument structure of the clause.5 This
is called the projection phase. Then an ordering
of the grammatical relations is generated, includ-
ing reserved contextual slots for adjunction and/or
punctuation marks. This is called the configura-
tion phase. Finally, each of the grammatical func-
tion labels and adjunction slots gets realized as a
morphosyntactic representation (a category label
plus dominated morphological features) of the re-
spective daughter constituent. This is called the
realization phase.6
Figure 3 shows the generation of sentences
(3a)–(3b) following the projection, configuration
and realization phases corresponding to the top-
down context-free layers of the tree. In both
cases, the same relational network is generated,
capturing the fact that they have the same argu-
ment structure. Then the different orderings of
the grammatical elements are generated, reserving
an adjunction slot for sentential modification (la-
beled by short context). Interestingly, the HD/RR
models for our sentences are of comparable size
(seven parameters) but the parameter types en-
code radically different notions. Illustrative of the
difference is the realization of a morphologically
marked NP object. In the RR model this is con-
ditioned on a grammatical relation (check, for in-
stance, node OBJ@S) and in the HD model it is
conditioned on linear ordering or configurational
notions such as left, right and distance.
</bodyText>
<sectionHeader confidence="0.999847" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999824692307692">
Goal We set out to compare the performance
of the different modeling approaches for pars-
ing Modern Hebrew. Considerable effort was de-
voted to making the models strictly comparable,
in terms of preparing the data, defining statistical
events, and unifying the rules determining cross-
cutting linguistic notions (e.g., heads and predi-
cates, grammatical functions and subcat sets). We
spell out some of the setup considerations below.
Data We use the Modern Hebrew treebank
(MHTB) (Sima’an et al., 2001) consisting of 6501
sentences from news-wire texts, morphologically
analyzed and syntactically annotated as phrase-
</bodyText>
<footnote confidence="0.9568484">
5Unlike in HD models or dependency grammars, the head
predicative element has no distinguished status here.
6Realization of adjunction slots (but not of function la-
bels) may generate multiple sisters adjoining at a single
position.
</footnote>
<figure confidence="0.994428545454545">
HEAD,VP@S
R,ΔR1, V P@S
ADVP
etmol
yesterday
HEAD,VP@S
R,ΔR1, V P@S
ADVP
etmol
yesterday
R,ΔR2, V P@S
</figure>
<page confidence="0.978618">
845
</page>
<table confidence="0.946003888888889">
GF Description Applicable to...
PRD Predicative Elements VP, PREDP
SBJ Grammatical Subjects NP, SBAR
OBJ Direct Objects NP
COM Indirect Objects NP, PP
FInite Complements SBAR
IC Infinitival Complements VP
CNJ A Conjunct within
a Conjunction Structure All
</table>
<tableCaption confidence="0.911159">
Table 2: Grammatical Functions in the MHTB
</tableCaption>
<table confidence="0.996643666666666">
SP-PCFG Expansion P(Cln, ... , Ch, ... , Crn|P)
HD-PCFG Head P(Ch|P)
Left Branch? P(L:Ol1, H:Oh|Ch, P)
Right Branch? P(Ch, R:Or1|Oh, Ch, P)
Left Arg/Mod P(Cli, Oli+1 |L , Oli, Ch, P)
Right Arg/Mod P(Cri, Ori+1 |R , Ori, Ch, P)
Left Final? P(C1 |L , Oln−1, Ch, P)
Right Final? P(Cn |R , Orn−1,Ch, P)
RR-PCFG Projection P({gr1, ... , grm}|P)
Configuration P(hgr1, ... , grmi|{gr1, ... , grm}P)
Realization P(Cj|grj, P)
Adjunction P(Cj1, ... , Cjn|grj : grj+1, P)
</table>
<tableCaption confidence="0.998973">
Table 3: PCFG Parameter Classes for All Models
</tableCaption>
<bodyText confidence="0.999962636363636">
structure trees. In our version of the MHTB, def-
initeness and accusativity features are percolated
from the PoS-tags level to phrase-level categories,
extending the procedure of (Guthmann et al.,
2009). For all models, we applied non-terminal
state-splits distinguishing finite from non-finite
verb forms and possessive from non-possessive
noun phrases. We head-annotated the treebank,
and based on the ‘subject’, ‘object’, ‘complement’
and ‘conjunction’ labels in the MHTB we devised
an automatic procedure to annotate all the gram-
matical functions indicated in table 2.7
Procedure For all models, we learn a PCFG by
reading off the parameters described in table 3,
in accordance with the trees depicted in figures
1–3.8 For all models, we use relative frequency
estimates. For lexical parameters, we use a sim-
ple smoothing procedure assigning probability to
unknown words using the per-tag distribution of
rare words (“rare” threshold set to &lt; 2). The in-
put to our parser consists of morphologically seg-
mented surface forms, and the parser has to as-
</bodyText>
<footnote confidence="0.98981">
7The enhanced corpus will be available at www.
science.uva.nl/˜rtsarfat/resources.htm.
8Our training procedure is strictly equivalent to the
transform-detransform methodology of (Johnson, 1998), but
we implement a tree-traverse procedure as in (Bikel, 2002)
collecting all parameters per event at once.
</footnote>
<bodyText confidence="0.999973375">
sign the syntactic as well as morphological anal-
ysis to the surface segments.9 We use the stan-
dard development/training/test split as in (Tsarfaty
and Sima’an, 2008). Since our goal is a detailed
comparison and fine-grained analysis of the results
we concentrate on the development set. We use
a general-purpose CKY parser (Schmid, 2004) to
exhaustively parse the sentences, and we strip off
all model-specific information prior to evaluation.
Evaluation We use standard Parseval measures
calculated for the original, flat, canonical repre-
sentation of the parse trees.10 We report Pre-
cision/Recall for the coarse-grained non-terminal
categories. In addition to overall Parseval scores
we report the accuracy results Per Syntactic Cate-
gory. We further report model size in terms of the
number of parameters. As is well known in Ma-
chine Learning, models with more parameters re-
quire more data to learn, and are more vulnerable
to sparseness. In our evaluation we thus follow the
rule of thumb that (all else being equal) for mod-
els of equal size the better performing model is
preferred, and for models with equal performance,
the smaller one is preferred.
</bodyText>
<sectionHeader confidence="0.999643" genericHeader="method">
5 Results and Analysis
</sectionHeader>
<subsectionHeader confidence="0.978845">
5.1 Overall Results
</subsectionHeader>
<bodyText confidence="0.997417739130435">
Table 4 shows the parsing results for the State-
Split (SP) PCFG, the Head-Driven (HD) PCFG
and the Relational-Realizational (RR) PCFG
models on parsing the Modern Hebrew Treebank,
with definiteness and accusativity marked on PoS-
tags as well as phrase-level categories. For all
models, we experiment with grandparent encod-
ing. For non-HD models, we also examine the
utility of a head-category split.11
9This setup is more difficult than, e.g., the Arabic parsing
setup of (Bikel, 2002), as they assume gold-standard pos-tags
as input. Yet it is easier than the setup of (Tsarfaty, 2006;
Goldberg and Tsarfaty, 2008) which uses unsegmented sur-
face forms as input. The decision to use segmented and un-
tagged forms was made to retain a realistic scenario. Mor-
phological analysis is known to be ambiguous, and we do
not assume that morphological features are known up front.
Morphological segmentation is also ambiguous, but for our
purposes it is unavoidable. When comparing different mod-
els on an individual sentence they may propose segmenta-
tion to sequences of different lengths, for which accuracy re-
sults cannot be faithfully compared. See (Tsarfaty, 2006) for
discussion.
</bodyText>
<footnote confidence="0.500214">
10The flat canonical representation also allows for a fair
comparison that is not biased by the differing branching fac-
tors of the different models.
11In HD models, a head-tag is already assumed in the con-
ditioning context for sister nodes (Klein and Manning, 2003).
</footnote>
<page confidence="0.995666">
846
</page>
<table confidence="0.9985054">
SP-PCFG
Grand-Parent − − + +
Head-Tag − + − +
Prec/Rec 70.05/72.40 71.14/72.03 74.66/74.35 71.99/72.17
(#Params) (4995) (8366) (7385) (11633)
HD-PCFG
Grand-Parent − − + +
Markov 0 1 0 1
Prec/Rec 66.87/71.64 70.40/74.35 73.04/71.94 73.52/74.84
(#Params) (6678) (10015) (19066) (21399)
RR-PCFG
Grand-Parent − − + +
Head Tag − + − +
Prec/Rec 69.90/73.96 72.96/75.73 74.19/75.03 76.32/76.51
(#Params) (3791) (7546) (7611) (13618)
</table>
<tableCaption confidence="0.999436">
Table 4: The Performance of Different Models
</tableCaption>
<bodyText confidence="0.985368285714286">
in Parsing Hebrew: Parsing Results Prec/Recall
for Sentences of Length G 40.
For all models, grandparent encoding is help-
ful. For HD models, a higher Markovian order im-
proves performance. This shows that even in He-
brew there are linear-precedence tendencies that
help steer the disambiguation in the right direc-
tion, which is in line with our observation that
word-order patterns in Modern Hebrew are not
completely free (cf. table 1).
The best SP model performs equally or better
than all HD models. This might be due to the
smaller size of SP grammars, resulting in more ro-
bust estimates. But it is remarkable that, given the
feature-rich representation, such a simple treebank
grammar provides better disambiguation capacity
than linguistically articulated HD models. We at-
tribute this to the fact that parent-daughter rela-
tions have a stronger association with grammati-
cal functions than relations between neighbouring
nodes. For Hebrew, such adjacency relations may
be arbitrary due to word-order variability.
Overall, RR models show the best performance
for the set of all models with parent encoding, and
for the set of all models without. Our best RR
model shows 6.6%/8.4% Prec/Rec error reduction
from the best SP model. The Recall improvement
shows that the RR model is much better in gener-
alizing, recovering successfully more of the con-
stituents found in the gold representation. The
best RR model also outperforms HD models with
8.7%/6.7% Prec/Rec error reduction from the best
In our SP or RR models, head-information is used as yet an-
other feature-value pair rather than an object with a distin-
guished status during generation.
</bodyText>
<table confidence="0.999307454545455">
Model / SP-PCFG HD-PCFG RR-PCFG
Category
NP 77.39 / 74.32 77.94 / 73.75 78.96 / 76.11
PP 71.78 / 71.14 71.83 / 69.24 74.4 / 72.02
SBAR 55.73 / 59.71 53.79 / 57.49 57.97 / 61.67
ADVP 71.37 / 77.01 72.52 / 73.56 73.57 / 77.59
ADJP 79.37 / 78.96 78.47 / 77.14 78.69 / 78.18
S 73.25 / 79.07 71.07 / 76.49 72.37 / 78.33
SQ 36.00 / 32.14 30.77 / 14.29 55.56 / 17.86
PREDP 36.31 / 39.63 44.74 / 39.63 44.51 / 46.95
VP 76.34 / 80.81 77.33 / 82.51 78.59 / 81.18
</table>
<tableCaption confidence="0.965734">
Table 5: Per-Category Evaluation of Parsing
</tableCaption>
<bodyText confidence="0.963283416666667">
Performance for Different Models: Prec/Rec
Per Category Calculated for All Sentences.
HD model. The resulting precision improvement
of the RR relative to HD is larger than the im-
provement relative to SP, and the Recall improve-
ment pattern is reversed. So it seems that the HD
model generalizes better than the SP model, but
also gets generalizations wrong more often than
the SP model.
The RR model combines the generalization
advantage of breaking down context-free events
while it maintains the coherence advantage of
learning flat trees (cf. (Johnson, 1998)). The best
RR model obtains the best performance among
all models: F176.41. To put this result in con-
text, for the setting in which the Arabic parser of
(Maamouri et al., 2008) obtains F178.1, — i.e.,
with gold standard feature-rich tags — the best
RR model obtains F183.3 accuracy which is the
best parsing result reported for a Semitic language
so far. RR models also have the advantage of re-
sulting in more compact grammars, which makes
learning and parsing with them much more com-
putationally efficient.
</bodyText>
<subsectionHeader confidence="0.996175">
5.2 Per-Category Break-Down Analysis
</subsectionHeader>
<bodyText confidence="0.9998715">
To understand better the merits of the different
models we conducted a break-down analysis of
performance-per-category for the best performing
models of each kind. The break-down results are
shown in table 5. We divided the table into three
sets of categories: those for which the RR model
gave the best performance, those for which the SP
model gave the best performance, and those for
which there is no clear trend.
The most striking outcome is that the RR model
identifies at higher accuracy precisely those syn-
tactic elements that are freely positioned with re-
</bodyText>
<page confidence="0.993898">
847
</page>
<bodyText confidence="0.999952952380952">
spect to the head: NPs, PPs, ADVPs and SBARs.
Adjectives, in contrast, have clear ordering con-
straints — they always appear after the noun. S
level elements, when embedded, always appear
immediately after a conjunction or a relativizer.
In particular, NPs and PPs realize arguments and
adjuncts that may occupy different positions rela-
tive to the head. The RR model is better than the
other models in identifying those elements partly
because morphological information helps to dis-
ambiguate syntactically relevant chunks and make
correct attachment decisions about them.
Remarkably, predicative (verb-less) phrases
(PREDP), which are characteristic of Semitic lan-
guages, are hard to parse, but here too the RR does
slightly better than the other two, as it allows for
variability in the means to realize a (verbal or verb-
less) predicate. Both RR and HD models outper-
form SP for VPs, which is due to the specific na-
ture of VPs in the MHTB – they exist only for
complement phrases with strict linear ordering.
</bodyText>
<sectionHeader confidence="0.742529" genericHeader="method">
6 Distances, Functions and
</sectionHeader>
<subsectionHeader confidence="0.718004">
Subcategorization Frames
</subsectionHeader>
<bodyText confidence="0.963626357142857">
Markovian processes to the left and to the right of
the head provide a first approximation of the pred-
icate’s argument structure, as they capture trends
in the co-occurrences of constituents reflected in
their pattern of positioning and adjacency. But
as our results so far show, such an approxima-
tion is empirically less rewarding for a language
in which grammatical relations are not tightly cor-
related with structural notions.12
Collins (2003) attempted a more abstract for-
mulation of argument-structure by articulating left
and right subcat-sets. Each set represents those
arguments that are expected to occur at each side
of the head. Argument sisters (“complements”)
are generated if and only if they are required, and
their generation ‘cancels’ the requirement in the
set. Adjuncts (“modifiers”) may be freely gener-
ated at any position.
At first glance, such a dissociation of configura-
tional positions and subcategorization sets seems
to be more adequate for parsing Hebrew, because
it allows for some variability in the order of gen-
eration. But here too, since the model uses sets of
12Conditioning based on adjacency and distance is also
common inside dependency parsing models, and we conjec-
ture that this is one of the reasons for their difficulty in coping
with freer word-order languages, a difficulty pointed out in
(Nivre et al., 2007).
</bodyText>
<figure confidence="0.971713166666667">
(3a) S
VPOS
L,{SBJ}, V POS H,VPOS
NP}D}ACC
et-ha-matana
the-present
</figure>
<figureCaption confidence="0.999984">
Figure 4: The Relational Head-Driven Approach
</figureCaption>
<bodyText confidence="0.999786423076923">
constituent labels, it disambiguates the grammati-
cal functions of an NP solely based on the direc-
tion of the head, which is adequate for English but
not for Hebrew. In order to relax this association
further, we propose to replace constituent labels
in the subcat-sets with grammatical relations iden-
tical to the functional elements in the relational
network of the RR. This provides means to medi-
ate the cancellation of constituents in the sets with
their functions and correlate it with morphology.
To get an idea of the implications of such a
modeling strategy, let us consider our example
sentences in such a Relational-HD model as de-
picted in figure 4. Both representations share
the event of generating the verbal head. Sisters
are generated conditioned on the head and the
functional elements remaining to be “cancelled”.
Each of the two trees consists of an event real-
izing an “object”, one for an NP to the right of
the head, and the other for an NP to its left. In
both cases, an object constituent will be generated
jointly with the morphological features associated
with it. Evidently, when using sets of grammatical
relations instead of constituent-labels, correlation
of morphology and grammatical functions is more
straight-forward to maintain.
</bodyText>
<figure confidence="0.997099794117647">
ADVP
etmol
yesterday
(3b) S
VPOS
L,{OBJ}, V POS H,VPOS
R,{OBJ, COM}, VPOS
NP
Dani
Dani
VP
natan
gave
R,{OBJ, COM}, V POS
NP}D}ACC
et-ha-matana
the-Present
R,{COM}, V POS
PP
le-dina
to-Dina
ADVP
etmol
yesterday
R,{SBJ, COM}, V POS
NP R,{COM}, V POS
Dani
Dani PP
le-dina
to-Dina
VP
natan
gave
R,{SBJ, COM}, V POS
</figure>
<page confidence="0.960314">
848
</page>
<table confidence="0.9994644">
Model SP-PCFG HD-PCFG HD-PCFG HD-PCFG HD-PCFG RR-PCFG
Type ofDistance A Phrase-Level Intervening Left and Right Left and Right Left and Right Subcat Sets
or Subcategorization State-Splits Verb/Punc #Constituents Constituent Labels Function Labels Configuration
Precision/Recall 70.95/70.32 72.39 / 71.97 72.70 / 74.46 72.42 / 74.29 72.84/74.62 76.32/76.51
(#Params) (13884) (11650) (18058) (16334) (16460) (13618)
</table>
<tableCaption confidence="0.977934">
Table 6: Incorporating Distance and Grammatical Functions into Head-Driven Parsing Models
</tableCaption>
<table confidence="0.593752">
Reporting Precison/Recall (#Parameters) for Sentences Length &lt; 40.
</table>
<subsectionHeader confidence="0.961294">
6.1 Results and Analysis
</subsectionHeader>
<bodyText confidence="0.975129534883721">
Table 6 reports the results of experimenting with
HD models with different instantiations of a dis-
tance function, starting from the standard notion
of (Collins, 2003) and ending with our proposed,
relational, function sets. For all HD models, we
retain the head, left and right generation cycle and
only change the conditioning context (Ai) for sis-
ter generation.
As a baseline, we show the results of adding
grammatical function information as state-splits
on top of an SP-PCFG.13 This SP model presents
much lower performance than the RR model al-
though they are almost of the same size and they
start off with the same information. This result
shows that sophisticated modeling can blunt the
claws of the sparseness problem. One may ob-
tain the same number of parameters for two dif-
ferent models, but correlate them with more pro-
found linguistic notions in one model than in the
other. In our case, there is more statistical evi-
dence in the data for, e.g., case marking patterns,
than for association of grammatical relations with
structurally-marked positions.
For all HD variations, the RR model contin-
ues to outperform HD models. The function-set
variation performs slightly (but not significantly)
better than the category-set. What seems to be
still standing in the way of getting useful dis-
ambiguation cues for HD models is the fact that
the left and right direction of realization is hard-
wired in their representation. This breaks down a
coherent distribution over morphosyntactic repre-
sentations realizing grammatical relations to arbi-
trary position-dependent fragments, which results
in larger grammars and inferior performance.14
13The startegy of adding grammatical functions as state-
splits is used in, e.g., German (Rafferty and Manning, 2008).
14Due to the difference in the size of the grammars, one
could argue that smoothing will bridge the gap between
the HD and RR modeling strategies. However, the better
size/accuracy trade-off shown here for RR models suggests
that they provide a good bias/variance balancing point, es-
pecially for feature-rich models characterizing morphologi-
</bodyText>
<sectionHeader confidence="0.989252" genericHeader="method">
7 A Typological Detour
</sectionHeader>
<bodyText confidence="0.999354976190476">
Hebrew, Arabic and other Semitic Languages are
known to be substantially different from English
in that English is strongly configurational. In
configurational languages word-order is fixed, and
information about the grammatical functions of
constituents (e.g., subject or object) is often cor-
related with structurally-marked positions inside
highly-nested constituency structures. Nonconfig-
urational languages (Hale, 1983), in contrast, al-
low for freedom in their word-ordering and infor-
mation about grammatical relations between con-
stituents is often marked by means of morphology.
Configurationality is hardly a clear-cut notion.
The difference in the configurationality level of
different languages is often conceived as depicted
in figure 7. In linguistic typology, the branch
of linguistics that studies the differences between
languages (Song, 2001), the division of labor be-
tween linear ordering and morphological marking
in the realization of grammatical relations is of-
ten viewed as a continuum. Common wisdom has
it that the lower a language is on the configura-
tionality scale, the more morphological marking
we expect to be used (Bresnan, 2001, page 6).
For a statistical parser to cope with nonconfig-
urational phenomena as observed in, for instance,
Hebrew or German, it should allow for flexibil-
ity in the form of realization of the grammati-
cal functions within the phrase-structure represen-
tation of trees. Recent morphological theories
employ Form-Function separation as a widely-
accepted practice for enhancing the adequacy of
models describing variability in the realization of
grammatical properties. Our results suggest that
the adequacy of syntactic processing models is re-
lated to such typological insights as well, and is
enhanced by adopting a similar form-function sep-
aration for expressing grammatical relations.
cally rich languages. A promising strategy then would be to
smooth or split-and-merge (Petrov et al., 2006)) RR-based
models rather than to add an elaborate smoothing component
to configurationally-based HD models.
</bodyText>
<page confidence="0.992266">
849
</page>
<figure confidence="0.9824445">
configurational nonconfigurational
Chinese&gt;English&gt;{German,Hebrew}&gt;Warlpiri
</figure>
<figureCaption confidence="0.999964">
Figure 5: The Configurationality Scale
</figureCaption>
<bodyText confidence="0.99999285">
The HD assumptions take the function of a con-
stituent to be transparently related to its formal
position, which entails word-order rigidity. Such
transparent relations between configurational po-
sitions and grammatical functions are assumed by
other kinds of parsing frameworks such as the ‘all-
subtrees’ approach of Data-Oriented Parsing, and
the distinction between left and right application
in CCG-based parsers.
The RR modeling strategy stipulates a strict
separation between form — parametrizing explic-
itly basic word-order (Greenberg, 1963) and mor-
phological realization (Greenberg, 1954) — and
function — parametrizing relational networks bor-
rowed from (Perlmutter, 1982) — which makes
it possible to statistically learn complex form-
function mapping reflected in the data. This is
an adequate means to capture, e.g., morphosyn-
tactic interactions, which characterize the less-
configurational languages on the scale.
</bodyText>
<sectionHeader confidence="0.998661" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999479391304348">
In our comparison of the HD and RR modeling
approaches, the RR approach is shown to be em-
pirically superior and typologically more adequate
for parsing a language exhibiting word-order vari-
ation interleaved with extended morphology. HD
models are less accurate and more vulnerable to
sparseness as they assume transparent mappings
between form and function, based on left and right
decompositions hard-wired in the HD representa-
tion. RR models, in contrast, employ form and
function separation which allows the statistical
model to learn complex correspondance patterns
reflected in the data. In the future we plan to in-
vestigate how the different models fare against one
another in parsing different languages. In particu-
lar we wish to examine whether parsing different
languages should be pursued by different models,
or whether the RR strategy can effectively cope
with different languages types. Finally, we wish
to explore the implications of RR modeling for
applications that consider the form of expression
in multiple languages, for instance Statistical Ma-
chine Translation (SMT).
</bodyText>
<sectionHeader confidence="0.996949" genericHeader="acknowledgments">
9 Acknowledgements
</sectionHeader>
<bodyText confidence="0.998828428571429">
We thank Jelle Zuidema, Inbal Tsarfati, David
McCloskey and Yoav Golderg for excellent com-
ments on earlier versions. We also thank Miles
Osborne and Tikitu de Jager for comments on the
camera-ready draft. All errors are our own. The
work of the first author is funded by the Dutch Sci-
ence Foundation (NWO) grant 017.001.271.
</bodyText>
<sectionHeader confidence="0.999419" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999959641025641">
J. Aissen. 2003. Differential Object Marking: Iconic-
ity vs. Economy. Natural Language and Linguistic
Theory, 21.
D. M. Bikel. 2002. Design of a Multi-lingual, Parallel-
processing Statistical Parsing Engine. In Proceed-
ings ofHLT.
J. Bresnan. 2001. Lexical-Functional Syntax. Black-
well Textbooks in Linguistics. Blackwell.
E. Charniak. 1996. Tree-Bank Grammars. In
AAAI/IAAI, Vol. 2.
E. Charniak. 1997. Statistical Parsing with a Context-
Free Grammar and Word Statistics. In AAAI/IAAI.
M. Collins, J. Hajiˇc, E. Brill, L. Ramshaw, and C. Till-
mann. 1999. A Statistical Parser of Czech. In Pro-
ceedings ACL.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
M. Collins. 2003. Head-Driven Statistical Models for
Natural Language Parsing. Computational Linguis-
tics.
A. Dubey. 2004. Statistical Parsing for German:
Modeling syntactic properties and annotation differ-
ences. Ph.D. thesis, Saarland University, Germany.
Y. Goldberg and R. Tsarfaty. 2008. A Single Frame-
work for Joint Morphological Segmentation and
Syntactic Parsing. In Proceedings ofACL.
J.H. Greenberg. 1954. A Quantitative Approach to
the Morphological Typology of Language. In R. F.
Spencer, editor, Method and Perspective in Anthro-
pology. University of Minessota Press.
J. H. Greenberg. 1963. Some Universals of Grammar
with Particular Reference to the Order of Meaning-
ful Elements. In Joseph H. Greenberg, editor, Uni-
versals ofLanguage. MIT Press.
N. Guthmann, Y. Krymolowski, A. Milea, and Y. Win-
ter. 2009. Automatic Annotation of Morpho-
Syntactic Dependencies in a Modern Hebrew Tree-
bank. In Proceedings of TLT.
</reference>
<page confidence="0.978145">
850
</page>
<reference confidence="0.996180983606558">
K. Sima’an, A. Itai, Y. Winter, A. Altman, and N. Na-
tiv. 2001. Building a Tree-Bank for Modern He-
brew Text. In Traitement Automatique des Langues.
K. L. Hale. 1983. Warlpiri and the Grammar of Non-
Configurational Languages. Natural Language and
Linguistic Theory, 1(1).
M. Johnson. 1998. PCFG Models of Linguistic Tree
Representations. Computational Linguistics, 24(4).
D. Klein and C. Manning. 2003. Accurate Unlexical-
ized Parsing. In Proceedings ofACL.
S. Kubler. 2008. The PaGe Shared task on Parsing
German. In ACL Workshop on Parsing German.
M. Maamouri, A. Bies, T. Buckwalter, and W. Mekki.
2004. The Penn Arabic Treebank: Building a Large-
Scale Annotated Arabic Corpus. In Proceedings of
NEMLAR.
M. Maamouri, A. Bies, and S. Kulick. 2008. Enhanced
Annotation and Parsing of the Arabic treebank. In
Proceedings ofINFOS.
D. M. Magerman. 1995. Statistical Decision-Tree
Models for Parsing. In Proceedings ofACL.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a Large Annotated Corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics.
P. H. Matthews. 1993. Morphology. Cambridge.
D. McClosky, E. Charniak, and M. Johnson. 2008.
When is self-training effective for parsing? In Pro-
ceedings of CoLing.
N. Melnik. 2002. Verb-Initial Constructions in Mod-
ern Hebrew. Ph.D. thesis, Berkeley, California.
Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The Shared Task on Dependency Pars-
ing. In Proceedings of the CoNLL Shared Task.
D. M. Perlmutter. 1982. Syntactic Representation,
Syntactic Levels, and the Notion of a Subject. In
Pauline Jacobson and Geoffrey Pullum, editors, The
Nature of Syntactic Representation. Springer.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning Accurate, Compact, and Interpretable Tree
Annotation. In Proceedings ofACL.
A. Rafferty and C. D. Manning. 2008. Parsing Three
German Treebanks: Lexicalized and Unlexicalized
Baselines. In ACL WorkShop on Parsing German.
R. Scha. 1990. Language Theory and Language Tech-
nology; Competence and Performance. In Q. A. M.
de Kort and G. L. J. Leerdam, editors, Computer-
toepassingen in de Neerlandistiek. Almere: LVVN.
H. Schmid. 2004. Efficient Parsing of Highly Am-
biguous Context-Free Grammars with Bit vectors.
In Proceedings of COLING.
U. Shlonsky. 1997. Clause Structure and Word Order
in Hebrew and Arabic. Oxford University Press.
J. J. Song. 2001. Linguistic Typology: Morphology
and Syntax. Pearson Education Limited, Edinbrugh.
R. Tsarfaty and K. Sima’an. 2008. Relational-
Realizational Parsing. In Proceedings of CoLing.
R. Tsarfaty. 2006. Integrated Morphological and Syn-
tactic Disambiguation for Modern Hebrew. In Pro-
ceeding ofACL-SRW.
</reference>
<page confidence="0.998666">
851
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.801197">
<title confidence="0.998967">An Alternative to Head-Driven Approaches Parsing a (Relatively) Free Word-Order Language</title>
<author confidence="0.857107">Reut Tsarfaty Khalil Sima’an Remko Scha</author>
<affiliation confidence="0.9967615">Institute for Logic Language and University of Amsterdam</affiliation>
<abstract confidence="0.996597055555556">Applying statistical parsers developed for English to languages with freer wordorder has turned out to be harder than expected. This paper investigates the adequacy of different statistical parsing models for dealing with a (relatively) free word-order language. We show the recently proposed Relational- (RR) consistently state-of-the-art on the Hebrew Treebank. Our analysis reveals a weakness of HD models: their intrinsic focus on configurational information. We conclude that the form-function separation ingrained in RR models makes them better suited for parsing nonconfigurational phenomena.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Aissen</author>
</authors>
<title>Differential Object Marking: Iconicity vs.</title>
<date>2003</date>
<journal>Economy. Natural Language and Linguistic Theory,</journal>
<volume>21</volume>
<contexts>
<context position="7103" citStr="Aissen, 2003" startWordPosition="1092" endWordPosition="1093">i ACC the-present to-Dina A corpus study we conducted on a fragment of the Modern Hebrew treebank reveals that although there is a significant number of subjects preceding verbs in simple (matrix) clauses (41%), there are also a fair number of sentences for which this order is reversed (29%), and there is evidence for other configurations, such as empty realization of subjects (16%) and non-verbal realization of predicates (14%). In the face of such lack of consistency in its configurational position, the grammatical function Object in Hebrew is indicated by Differential Object Marking (DOM) (Aissen, 2003). NP objects in Hebrew are marked for accusativity (using the marker et) if they are also marked for definiteness (indicated by the prefix ha). So, in contrast with (2a)-(2b), the indefinite object renders (2c) ungrammatical, and the missing accusativity renders (2d) awkward. The fact that marking NP objects involves the joint contribution of multiple surface elements (et, ha) contributing features to the NP constituent is referred to as extended exponence (Matthews, 1993, p. 182). (2) a. dani natan matana ledina Dani gave present to-Dina “Dani gave a present to Dina” b. dani natan et hamatana</context>
</contexts>
<marker>Aissen, 2003</marker>
<rawString>J. Aissen. 2003. Differential Object Marking: Iconicity vs. Economy. Natural Language and Linguistic Theory, 21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Bikel</author>
</authors>
<title>Design of a Multi-lingual, Parallelprocessing Statistical Parsing Engine.</title>
<date>2002</date>
<booktitle>In Proceedings ofHLT.</booktitle>
<contexts>
<context position="2071" citStr="Bikel, 2002" startWordPosition="296" endWordPosition="297">rought about a dramatic increase in parsing accuracy to the level of F188. Discriminative approaches, DataOriented Parsing (‘all-subtrees’) approaches, and self-training techniques brought further improvements, and recent results are starting to level off at around F192.1(McClosky et al., 2008). As the interest of the NLP community grows to encompass more languages, we observe efforts towards adapting an English parser for parsing other languages (e.g., (Collins et al., 1999)), or towards designing a language-independent framework based on principles underlying the models for parsing English (Bikel, 2002). The performance curve for parsing other languages with these models looks rather different. A case in point is Modern Standard Arabic. Since the initial effort of (Bikel, 2002) to parse the Arabic treebank (Maamouri et al., 2004), which yielded F175 accuracy, four years and successive revisions have led to no more than F179 (Maamouri et al., 2008). This pattern from Arabic is not peculiar. The level of state-of-the-art results for other languages still lags behind those for English, even after putting considerable effort into the adaptation.1 Given that these languages are inherently differe</context>
<context position="18358" citStr="Bikel, 2002" startWordPosition="2826" endWordPosition="2827">depicted in figures 1–3.8 For all models, we use relative frequency estimates. For lexical parameters, we use a simple smoothing procedure assigning probability to unknown words using the per-tag distribution of rare words (“rare” threshold set to &lt; 2). The input to our parser consists of morphologically segmented surface forms, and the parser has to as7The enhanced corpus will be available at www. science.uva.nl/˜rtsarfat/resources.htm. 8Our training procedure is strictly equivalent to the transform-detransform methodology of (Johnson, 1998), but we implement a tree-traverse procedure as in (Bikel, 2002) collecting all parameters per event at once. sign the syntactic as well as morphological analysis to the surface segments.9 We use the standard development/training/test split as in (Tsarfaty and Sima’an, 2008). Since our goal is a detailed comparison and fine-grained analysis of the results we concentrate on the development set. We use a general-purpose CKY parser (Schmid, 2004) to exhaustively parse the sentences, and we strip off all model-specific information prior to evaluation. Evaluation We use standard Parseval measures calculated for the original, flat, canonical representation of th</context>
<context position="20085" citStr="Bikel, 2002" startWordPosition="3100" endWordPosition="3101">el is preferred, and for models with equal performance, the smaller one is preferred. 5 Results and Analysis 5.1 Overall Results Table 4 shows the parsing results for the StateSplit (SP) PCFG, the Head-Driven (HD) PCFG and the Relational-Realizational (RR) PCFG models on parsing the Modern Hebrew Treebank, with definiteness and accusativity marked on PoStags as well as phrase-level categories. For all models, we experiment with grandparent encoding. For non-HD models, we also examine the utility of a head-category split.11 9This setup is more difficult than, e.g., the Arabic parsing setup of (Bikel, 2002), as they assume gold-standard pos-tags as input. Yet it is easier than the setup of (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008) which uses unsegmented surface forms as input. The decision to use segmented and untagged forms was made to retain a realistic scenario. Morphological analysis is known to be ambiguous, and we do not assume that morphological features are known up front. Morphological segmentation is also ambiguous, but for our purposes it is unavoidable. When comparing different models on an individual sentence they may propose segmentation to sequences of different lengths, for w</context>
</contexts>
<marker>Bikel, 2002</marker>
<rawString>D. M. Bikel. 2002. Design of a Multi-lingual, Parallelprocessing Statistical Parsing Engine. In Proceedings ofHLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bresnan</author>
</authors>
<title>Lexical-Functional Syntax. Blackwell Textbooks in Linguistics.</title>
<date>2001</date>
<publisher>Blackwell.</publisher>
<contexts>
<context position="10231" citStr="Bresnan, 2001" startWordPosition="1582" endWordPosition="1584">proach taken by the annotators of the Hebrew treebank in which information about morphological marking appears at multiple levels of constituency (Guthmann et al., 2009), and functional features (such as subject, object, etc.) decorate phrase-level constituent labels (Sima’an et al., 2001). The S-level representation of our example sentences (3a)–(3b) then would be as we depict in figure 1, which can be read off as feature-rich 3Such clauses are defined formally as exocentric in formal theories of syntax, and are used to describe syntactic structures in, e.g., Tagalog, Hungarian and Warlpiri (Bresnan, 2001, page 110). This flat representation format is characteristic of treebanks for other languages with relatively-free word-order as well, such as German (cf. (Kubler, 2008)). PCFG productions. We refer to this approach as the State-Splits (SP) approach, which serves as the baseline for the rest of our investigation. 3.2 The Head-Driven Approach Following the linguistic wisdom that the internal organization of syntactic constituents revolves around their heads, Head-Driven (HD) models have been proposed by (Magerman, 1995; Charniak, 1997; Collins, 1999). In a generative HD model, the head daught</context>
<context position="33306" citStr="Bresnan, 2001" startWordPosition="5212" endWordPosition="5213">s is often marked by means of morphology. Configurationality is hardly a clear-cut notion. The difference in the configurationality level of different languages is often conceived as depicted in figure 7. In linguistic typology, the branch of linguistics that studies the differences between languages (Song, 2001), the division of labor between linear ordering and morphological marking in the realization of grammatical relations is often viewed as a continuum. Common wisdom has it that the lower a language is on the configurationality scale, the more morphological marking we expect to be used (Bresnan, 2001, page 6). For a statistical parser to cope with nonconfigurational phenomena as observed in, for instance, Hebrew or German, it should allow for flexibility in the form of realization of the grammatical functions within the phrase-structure representation of trees. Recent morphological theories employ Form-Function separation as a widelyaccepted practice for enhancing the adequacy of models describing variability in the realization of grammatical properties. Our results suggest that the adequacy of syntactic processing models is related to such typological insights as well, and is enhanced by</context>
</contexts>
<marker>Bresnan, 2001</marker>
<rawString>J. Bresnan. 2001. Lexical-Functional Syntax. Blackwell Textbooks in Linguistics. Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Tree-Bank Grammars.</title>
<date>1996</date>
<booktitle>In AAAI/IAAI,</booktitle>
<volume>2</volume>
<contexts>
<context position="981" citStr="Charniak (1996)" startWordPosition="131" endWordPosition="132">xpected. This paper investigates the adequacy of different statistical parsing models for dealing with a (relatively) free word-order language. We show that the recently proposed RelationalRealizational (RR) model consistently outperforms state-of-the-art Head-Driven (HD) models on the Hebrew Treebank. Our analysis reveals a weakness of HD models: their intrinsic focus on configurational information. We conclude that the form-function separation ingrained in RR models makes them better suited for parsing nonconfigurational phenomena. 1 Introduction Parsing technology has come a long way since Charniak (1996) demonstrated that a simple treebank PCFG performs better than any other parser (with F175 accuracy) on parsing the WSJ Penn treebank (Marcus et al., 1993). Treebank Grammars (Scha, 1990; Charniak, 1996) trained on large corpora nowadays present the best available means to parse natural language text. The performance curve for parsing the WSJ was a steep one at first, as the incorporation of notions such as head, distance, subcategorization (Charniak, 1997; Collins, 1999) brought about a dramatic increase in parsing accuracy to the level of F188. Discriminative approaches, DataOriented Parsing</context>
</contexts>
<marker>Charniak, 1996</marker>
<rawString>E. Charniak. 1996. Tree-Bank Grammars. In AAAI/IAAI, Vol. 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Statistical Parsing with a ContextFree Grammar and Word Statistics.</title>
<date>1997</date>
<booktitle>In AAAI/IAAI.</booktitle>
<contexts>
<context position="1441" citStr="Charniak, 1997" startWordPosition="205" endWordPosition="206">n RR models makes them better suited for parsing nonconfigurational phenomena. 1 Introduction Parsing technology has come a long way since Charniak (1996) demonstrated that a simple treebank PCFG performs better than any other parser (with F175 accuracy) on parsing the WSJ Penn treebank (Marcus et al., 1993). Treebank Grammars (Scha, 1990; Charniak, 1996) trained on large corpora nowadays present the best available means to parse natural language text. The performance curve for parsing the WSJ was a steep one at first, as the incorporation of notions such as head, distance, subcategorization (Charniak, 1997; Collins, 1999) brought about a dramatic increase in parsing accuracy to the level of F188. Discriminative approaches, DataOriented Parsing (‘all-subtrees’) approaches, and self-training techniques brought further improvements, and recent results are starting to level off at around F192.1(McClosky et al., 2008). As the interest of the NLP community grows to encompass more languages, we observe efforts towards adapting an English parser for parsing other languages (e.g., (Collins et al., 1999)), or towards designing a language-independent framework based on principles underlying the models for</context>
<context position="10772" citStr="Charniak, 1997" startWordPosition="1662" endWordPosition="1664">actic structures in, e.g., Tagalog, Hungarian and Warlpiri (Bresnan, 2001, page 110). This flat representation format is characteristic of treebanks for other languages with relatively-free word-order as well, such as German (cf. (Kubler, 2008)). PCFG productions. We refer to this approach as the State-Splits (SP) approach, which serves as the baseline for the rest of our investigation. 3.2 The Head-Driven Approach Following the linguistic wisdom that the internal organization of syntactic constituents revolves around their heads, Head-Driven (HD) models have been proposed by (Magerman, 1995; Charniak, 1997; Collins, 1999). In a generative HD model, the head daughter is generated first, conditioned on properties of the mother node. Then, sisters of the head daughter are generated conditioned on the head, typically by left and right generation processes. Overall, HD processes have the modeling advantage that they capture structurallymarked positions that characterize the argument structure of the sentence. The simplest possible process uses unigram probabilities, but (Klein and Manning, 2003) show that using vertical and horizontal Markovization improves parsing accuracy.4 An unlexicalized genera</context>
<context position="12539" citStr="Charniak, 1997" startWordPosition="1933" endWordPosition="1934">ional-Realizational Approach The Relational-Realizational (RR) parsing model of (Tsarfaty and Sima’an, 2008) similarly decomposes the generation of the context-free events in figure 1 into multiple independent parameters, but does so in a conceptually different way. Instead of decomposing a context-free event to head and sisters, the RR model is best viewed as a generative grammar that decomposes it to form and function. The RR grammar first generates a set of grammatical functions depicting the Relational Network (RN) (Perlmutter, 1982) of the clause. This 4The success of Head-Driven models (Charniak, 1997; Collins, 2003) was initially attributed to the fact that they were fully lexicalized, but (Klein and Manning, 2003) show that an unlexicalized model combining Head-Driven Markovian processes with linguistically motivated state-splits can approach the performance of fully lexicalized models. 844 (3a) S NP-SBJ VP-PRD ADVP NP+D+ACC-OBJ PP-COM Dani natan etmol et-hamatana le-dina gave yesterday the-present to-Dina (3b) S NP+D+ACC-OBJ VP-PRD ADVP NP-SBJ PP-COM et-ha-matana natan etmol Dani le-dina the-present gave yesterday Dani to-Dina Figure 1: The State-Splits Approach for Ex. (3) (3a) S V P@S</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>E. Charniak. 1997. Statistical Parsing with a ContextFree Grammar and Word Statistics. In AAAI/IAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>J Hajiˇc</author>
<author>E Brill</author>
<author>L Ramshaw</author>
<author>C Tillmann</author>
</authors>
<title>A Statistical Parser of Czech.</title>
<date>1999</date>
<booktitle>In Proceedings ACL.</booktitle>
<marker>Collins, Hajiˇc, Brill, Ramshaw, Tillmann, 1999</marker>
<rawString>M. Collins, J. Hajiˇc, E. Brill, L. Ramshaw, and C. Tillmann. 1999. A Statistical Parser of Czech. In Proceedings ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1457" citStr="Collins, 1999" startWordPosition="207" endWordPosition="208">s them better suited for parsing nonconfigurational phenomena. 1 Introduction Parsing technology has come a long way since Charniak (1996) demonstrated that a simple treebank PCFG performs better than any other parser (with F175 accuracy) on parsing the WSJ Penn treebank (Marcus et al., 1993). Treebank Grammars (Scha, 1990; Charniak, 1996) trained on large corpora nowadays present the best available means to parse natural language text. The performance curve for parsing the WSJ was a steep one at first, as the incorporation of notions such as head, distance, subcategorization (Charniak, 1997; Collins, 1999) brought about a dramatic increase in parsing accuracy to the level of F188. Discriminative approaches, DataOriented Parsing (‘all-subtrees’) approaches, and self-training techniques brought further improvements, and recent results are starting to level off at around F192.1(McClosky et al., 2008). As the interest of the NLP community grows to encompass more languages, we observe efforts towards adapting an English parser for parsing other languages (e.g., (Collins et al., 1999)), or towards designing a language-independent framework based on principles underlying the models for parsing English</context>
<context position="10788" citStr="Collins, 1999" startWordPosition="1665" endWordPosition="1666"> in, e.g., Tagalog, Hungarian and Warlpiri (Bresnan, 2001, page 110). This flat representation format is characteristic of treebanks for other languages with relatively-free word-order as well, such as German (cf. (Kubler, 2008)). PCFG productions. We refer to this approach as the State-Splits (SP) approach, which serves as the baseline for the rest of our investigation. 3.2 The Head-Driven Approach Following the linguistic wisdom that the internal organization of syntactic constituents revolves around their heads, Head-Driven (HD) models have been proposed by (Magerman, 1995; Charniak, 1997; Collins, 1999). In a generative HD model, the head daughter is generated first, conditioned on properties of the mother node. Then, sisters of the head daughter are generated conditioned on the head, typically by left and right generation processes. Overall, HD processes have the modeling advantage that they capture structurallymarked positions that characterize the argument structure of the sentence. The simplest possible process uses unigram probabilities, but (Klein and Manning, 2003) show that using vertical and horizontal Markovization improves parsing accuracy.4 An unlexicalized generative HD model wi</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>M. Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing. Computational Linguistics.</title>
<date>2003</date>
<contexts>
<context position="12555" citStr="Collins, 2003" startWordPosition="1935" endWordPosition="1936">nal Approach The Relational-Realizational (RR) parsing model of (Tsarfaty and Sima’an, 2008) similarly decomposes the generation of the context-free events in figure 1 into multiple independent parameters, but does so in a conceptually different way. Instead of decomposing a context-free event to head and sisters, the RR model is best viewed as a generative grammar that decomposes it to form and function. The RR grammar first generates a set of grammatical functions depicting the Relational Network (RN) (Perlmutter, 1982) of the clause. This 4The success of Head-Driven models (Charniak, 1997; Collins, 2003) was initially attributed to the fact that they were fully lexicalized, but (Klein and Manning, 2003) show that an unlexicalized model combining Head-Driven Markovian processes with linguistically motivated state-splits can approach the performance of fully lexicalized models. 844 (3a) S NP-SBJ VP-PRD ADVP NP+D+ACC-OBJ PP-COM Dani natan etmol et-hamatana le-dina gave yesterday the-present to-Dina (3b) S NP+D+ACC-OBJ VP-PRD ADVP NP-SBJ PP-COM et-ha-matana natan etmol Dani le-dina the-present gave yesterday Dani to-Dina Figure 1: The State-Splits Approach for Ex. (3) (3a) S V P@S L,ΔL1, V P@S NP</context>
<context position="26837" citStr="Collins (2003)" startWordPosition="4211" endWordPosition="4212">due to the specific nature of VPs in the MHTB – they exist only for complement phrases with strict linear ordering. 6 Distances, Functions and Subcategorization Frames Markovian processes to the left and to the right of the head provide a first approximation of the predicate’s argument structure, as they capture trends in the co-occurrences of constituents reflected in their pattern of positioning and adjacency. But as our results so far show, such an approximation is empirically less rewarding for a language in which grammatical relations are not tightly correlated with structural notions.12 Collins (2003) attempted a more abstract formulation of argument-structure by articulating left and right subcat-sets. Each set represents those arguments that are expected to occur at each side of the head. Argument sisters (“complements”) are generated if and only if they are required, and their generation ‘cancels’ the requirement in the set. Adjuncts (“modifiers”) may be freely generated at any position. At first glance, such a dissociation of configurational positions and subcategorization sets seems to be more adequate for parsing Hebrew, because it allows for some variability in the order of generati</context>
<context position="30195" citStr="Collins, 2003" startWordPosition="4736" endWordPosition="4737"> Sets or Subcategorization State-Splits Verb/Punc #Constituents Constituent Labels Function Labels Configuration Precision/Recall 70.95/70.32 72.39 / 71.97 72.70 / 74.46 72.42 / 74.29 72.84/74.62 76.32/76.51 (#Params) (13884) (11650) (18058) (16334) (16460) (13618) Table 6: Incorporating Distance and Grammatical Functions into Head-Driven Parsing Models Reporting Precison/Recall (#Parameters) for Sentences Length &lt; 40. 6.1 Results and Analysis Table 6 reports the results of experimenting with HD models with different instantiations of a distance function, starting from the standard notion of (Collins, 2003) and ending with our proposed, relational, function sets. For all HD models, we retain the head, left and right generation cycle and only change the conditioning context (Ai) for sister generation. As a baseline, we show the results of adding grammatical function information as state-splits on top of an SP-PCFG.13 This SP model presents much lower performance than the RR model although they are almost of the same size and they start off with the same information. This result shows that sophisticated modeling can blunt the claws of the sparseness problem. One may obtain the same number of param</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>M. Collins. 2003. Head-Driven Statistical Models for Natural Language Parsing. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dubey</author>
</authors>
<title>Statistical Parsing for German: Modeling syntactic properties and annotation differences.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>Saarland University,</institution>
<contexts>
<context position="3655" citStr="Dubey, 2004" startWordPosition="552" endWordPosition="553">elopment of phrase-structure based statistical parsing models. Recently, we proposed the RelationalRealizational (RR) approach that rests upon different premises (Tsarfaty and Sima’an, 2008). The question of how the RR model fares against the HD models that have so far been predominantly used has never been tackled. Yet, it is precisely such a comparison that can shed new light on the question of adequacy we posed above. Empirically quantifying the effects of different modeling choices has been addressed for English by, e.g., (Johnson, 1998; Klein and Manning, 2003), and for German by, e.g., (Dubey, 2004; 1Consider, e.g., “The PaGe shared task on parsing German” (Kubler, 2008), reporting F,75, F,79, F,83 for the participating parsers. 842 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 842–851, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP Rafferty and Manning, 2008). This paper provides an empirical systematic comparison of conceptually different modeling strategies with respect to parsing Hebrew. This comparison is intended to provide a first answer to the question of parser adequacy in the face of word-order freedom. Our two empirical result</context>
</contexts>
<marker>Dubey, 2004</marker>
<rawString>A. Dubey. 2004. Statistical Parsing for German: Modeling syntactic properties and annotation differences. Ph.D. thesis, Saarland University, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Goldberg</author>
<author>R Tsarfaty</author>
</authors>
<title>A Single Framework for Joint Morphological Segmentation and Syntactic Parsing.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="20215" citStr="Goldberg and Tsarfaty, 2008" startWordPosition="3119" endWordPosition="3122">verall Results Table 4 shows the parsing results for the StateSplit (SP) PCFG, the Head-Driven (HD) PCFG and the Relational-Realizational (RR) PCFG models on parsing the Modern Hebrew Treebank, with definiteness and accusativity marked on PoStags as well as phrase-level categories. For all models, we experiment with grandparent encoding. For non-HD models, we also examine the utility of a head-category split.11 9This setup is more difficult than, e.g., the Arabic parsing setup of (Bikel, 2002), as they assume gold-standard pos-tags as input. Yet it is easier than the setup of (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008) which uses unsegmented surface forms as input. The decision to use segmented and untagged forms was made to retain a realistic scenario. Morphological analysis is known to be ambiguous, and we do not assume that morphological features are known up front. Morphological segmentation is also ambiguous, but for our purposes it is unavoidable. When comparing different models on an individual sentence they may propose segmentation to sequences of different lengths, for which accuracy results cannot be faithfully compared. See (Tsarfaty, 2006) for discussion. 10The flat canonical representation also</context>
</contexts>
<marker>Goldberg, Tsarfaty, 2008</marker>
<rawString>Y. Goldberg and R. Tsarfaty. 2008. A Single Framework for Joint Morphological Segmentation and Syntactic Parsing. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Greenberg</author>
</authors>
<title>A Quantitative Approach to the Morphological Typology of Language. In</title>
<date>1954</date>
<booktitle>Method and Perspective in Anthropology.</booktitle>
<editor>R. F. Spencer, editor,</editor>
<publisher>University of Minessota Press.</publisher>
<contexts>
<context position="34916" citStr="Greenberg, 1954" startWordPosition="5437" endWordPosition="5438">nfigurationality Scale The HD assumptions take the function of a constituent to be transparently related to its formal position, which entails word-order rigidity. Such transparent relations between configurational positions and grammatical functions are assumed by other kinds of parsing frameworks such as the ‘allsubtrees’ approach of Data-Oriented Parsing, and the distinction between left and right application in CCG-based parsers. The RR modeling strategy stipulates a strict separation between form — parametrizing explicitly basic word-order (Greenberg, 1963) and morphological realization (Greenberg, 1954) — and function — parametrizing relational networks borrowed from (Perlmutter, 1982) — which makes it possible to statistically learn complex formfunction mapping reflected in the data. This is an adequate means to capture, e.g., morphosyntactic interactions, which characterize the lessconfigurational languages on the scale. 8 Conclusion In our comparison of the HD and RR modeling approaches, the RR approach is shown to be empirically superior and typologically more adequate for parsing a language exhibiting word-order variation interleaved with extended morphology. HD models are less accurate</context>
</contexts>
<marker>Greenberg, 1954</marker>
<rawString>J.H. Greenberg. 1954. A Quantitative Approach to the Morphological Typology of Language. In R. F. Spencer, editor, Method and Perspective in Anthropology. University of Minessota Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Greenberg</author>
</authors>
<title>Some Universals of Grammar with Particular Reference to the Order of Meaningful Elements.</title>
<date>1963</date>
<editor>In Joseph H. Greenberg, editor, Universals ofLanguage.</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="6224" citStr="Greenberg, 1963" startWordPosition="946" endWordPosition="947">any kind may be fronted, triggering an inversion familiar from Germanic languages as in (1b) (Triggered Inversion (TI) in (Shlonsky, 1997)). Under some information structuring conditions, Verb Initial (VI) constructions are also allowed, as in (1c) (Melnik, 2002). All sentences in (1) thus mean “Dani gave the present to Dina”, despite their different word-ordering. (1) a. dani natan et hamatana ledina Dani gave ACC the-present to-Dina b. et hamatana natan dani ledina ACC the-present gave Dani to-Dina 2SVO is an abbreviation for the Subject-Verb-Object type in the basic word-order typology of (Greenberg, 1963). Word Order Frequency Relative Frequency SV 1612 41% VS 1144 29% No S 624 16% No V 550 14% Table 1: Modern Hebrew Predicative ClauseTypes in 3930 Predicative Matrix Clauses in the Training Set of the Modern Hebrew Treebank. c. natan dani et hamatana ledina gave Dani ACC the-present to-Dina A corpus study we conducted on a fragment of the Modern Hebrew treebank reveals that although there is a significant number of subjects preceding verbs in simple (matrix) clauses (41%), there are also a fair number of sentences for which this order is reversed (29%), and there is evidence for other configur</context>
<context position="34868" citStr="Greenberg, 1963" startWordPosition="5431" endWordPosition="5432">nglish&gt;{German,Hebrew}&gt;Warlpiri Figure 5: The Configurationality Scale The HD assumptions take the function of a constituent to be transparently related to its formal position, which entails word-order rigidity. Such transparent relations between configurational positions and grammatical functions are assumed by other kinds of parsing frameworks such as the ‘allsubtrees’ approach of Data-Oriented Parsing, and the distinction between left and right application in CCG-based parsers. The RR modeling strategy stipulates a strict separation between form — parametrizing explicitly basic word-order (Greenberg, 1963) and morphological realization (Greenberg, 1954) — and function — parametrizing relational networks borrowed from (Perlmutter, 1982) — which makes it possible to statistically learn complex formfunction mapping reflected in the data. This is an adequate means to capture, e.g., morphosyntactic interactions, which characterize the lessconfigurational languages on the scale. 8 Conclusion In our comparison of the HD and RR modeling approaches, the RR approach is shown to be empirically superior and typologically more adequate for parsing a language exhibiting word-order variation interleaved with </context>
</contexts>
<marker>Greenberg, 1963</marker>
<rawString>J. H. Greenberg. 1963. Some Universals of Grammar with Particular Reference to the Order of Meaningful Elements. In Joseph H. Greenberg, editor, Universals ofLanguage. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Guthmann</author>
<author>Y Krymolowski</author>
<author>A Milea</author>
<author>Y Winter</author>
</authors>
<title>Automatic Annotation of MorphoSyntactic Dependencies in a Modern Hebrew Treebank.</title>
<date>2009</date>
<booktitle>In Proceedings of TLT.</booktitle>
<contexts>
<context position="9787" citStr="Guthmann et al., 2009" startWordPosition="1510" endWordPosition="1513">res, additional information such as morphological marking and grammatical function labels is added to the phrase-structure trees. 3.1 The State-Splits Approach The simplest way to encode grammatical functions information on top of the phrase-structure representation in the treebank is by decorating non-terminal nodes with morphological or functional features, similarly to the rich representation format of syntactic categories in GPSG. This is the approach taken by the annotators of the Hebrew treebank in which information about morphological marking appears at multiple levels of constituency (Guthmann et al., 2009), and functional features (such as subject, object, etc.) decorate phrase-level constituent labels (Sima’an et al., 2001). The S-level representation of our example sentences (3a)–(3b) then would be as we depict in figure 1, which can be read off as feature-rich 3Such clauses are defined formally as exocentric in formal theories of syntax, and are used to describe syntactic structures in, e.g., Tagalog, Hungarian and Warlpiri (Bresnan, 2001, page 110). This flat representation format is characteristic of treebanks for other languages with relatively-free word-order as well, such as German (cf.</context>
<context position="17251" citStr="Guthmann et al., 2009" startWordPosition="2659" endWordPosition="2662">l1, H:Oh|Ch, P) Right Branch? P(Ch, R:Or1|Oh, Ch, P) Left Arg/Mod P(Cli, Oli+1 |L , Oli, Ch, P) Right Arg/Mod P(Cri, Ori+1 |R , Ori, Ch, P) Left Final? P(C1 |L , Oln−1, Ch, P) Right Final? P(Cn |R , Orn−1,Ch, P) RR-PCFG Projection P({gr1, ... , grm}|P) Configuration P(hgr1, ... , grmi|{gr1, ... , grm}P) Realization P(Cj|grj, P) Adjunction P(Cj1, ... , Cjn|grj : grj+1, P) Table 3: PCFG Parameter Classes for All Models structure trees. In our version of the MHTB, definiteness and accusativity features are percolated from the PoS-tags level to phrase-level categories, extending the procedure of (Guthmann et al., 2009). For all models, we applied non-terminal state-splits distinguishing finite from non-finite verb forms and possessive from non-possessive noun phrases. We head-annotated the treebank, and based on the ‘subject’, ‘object’, ‘complement’ and ‘conjunction’ labels in the MHTB we devised an automatic procedure to annotate all the grammatical functions indicated in table 2.7 Procedure For all models, we learn a PCFG by reading off the parameters described in table 3, in accordance with the trees depicted in figures 1–3.8 For all models, we use relative frequency estimates. For lexical parameters, we</context>
</contexts>
<marker>Guthmann, Krymolowski, Milea, Winter, 2009</marker>
<rawString>N. Guthmann, Y. Krymolowski, A. Milea, and Y. Winter. 2009. Automatic Annotation of MorphoSyntactic Dependencies in a Modern Hebrew Treebank. In Proceedings of TLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sima’an</author>
<author>A Itai</author>
<author>Y Winter</author>
<author>A Altman</author>
<author>N Nativ</author>
</authors>
<title>Building a Tree-Bank for Modern Hebrew Text. In Traitement Automatique des Langues.</title>
<date>2001</date>
<marker>Sima’an, Itai, Winter, Altman, Nativ, 2001</marker>
<rawString>K. Sima’an, A. Itai, Y. Winter, A. Altman, and N. Nativ. 2001. Building a Tree-Bank for Modern Hebrew Text. In Traitement Automatique des Langues.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K L Hale</author>
</authors>
<title>Warlpiri and the Grammar of NonConfigurational Languages.</title>
<date>1983</date>
<journal>Natural Language and Linguistic Theory,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="32574" citStr="Hale, 1983" startWordPosition="5099" endWordPosition="5100">racy trade-off shown here for RR models suggests that they provide a good bias/variance balancing point, especially for feature-rich models characterizing morphologi7 A Typological Detour Hebrew, Arabic and other Semitic Languages are known to be substantially different from English in that English is strongly configurational. In configurational languages word-order is fixed, and information about the grammatical functions of constituents (e.g., subject or object) is often correlated with structurally-marked positions inside highly-nested constituency structures. Nonconfigurational languages (Hale, 1983), in contrast, allow for freedom in their word-ordering and information about grammatical relations between constituents is often marked by means of morphology. Configurationality is hardly a clear-cut notion. The difference in the configurationality level of different languages is often conceived as depicted in figure 7. In linguistic typology, the branch of linguistics that studies the differences between languages (Song, 2001), the division of labor between linear ordering and morphological marking in the realization of grammatical relations is often viewed as a continuum. Common wisdom has</context>
</contexts>
<marker>Hale, 1983</marker>
<rawString>K. L. Hale. 1983. Warlpiri and the Grammar of NonConfigurational Languages. Natural Language and Linguistic Theory, 1(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>PCFG Models of Linguistic Tree Representations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="3590" citStr="Johnson, 1998" startWordPosition="541" endWordPosition="542"> affordable alternative to the Head-Driven (HD) approach in the development of phrase-structure based statistical parsing models. Recently, we proposed the RelationalRealizational (RR) approach that rests upon different premises (Tsarfaty and Sima’an, 2008). The question of how the RR model fares against the HD models that have so far been predominantly used has never been tackled. Yet, it is precisely such a comparison that can shed new light on the question of adequacy we posed above. Empirically quantifying the effects of different modeling choices has been addressed for English by, e.g., (Johnson, 1998; Klein and Manning, 2003), and for German by, e.g., (Dubey, 2004; 1Consider, e.g., “The PaGe shared task on parsing German” (Kubler, 2008), reporting F,75, F,79, F,83 for the participating parsers. 842 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 842–851, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP Rafferty and Manning, 2008). This paper provides an empirical systematic comparison of conceptually different modeling strategies with respect to parsing Hebrew. This comparison is intended to provide a first answer to the question of parser ade</context>
<context position="18294" citStr="Johnson, 1998" startWordPosition="2816" endWordPosition="2817">the parameters described in table 3, in accordance with the trees depicted in figures 1–3.8 For all models, we use relative frequency estimates. For lexical parameters, we use a simple smoothing procedure assigning probability to unknown words using the per-tag distribution of rare words (“rare” threshold set to &lt; 2). The input to our parser consists of morphologically segmented surface forms, and the parser has to as7The enhanced corpus will be available at www. science.uva.nl/˜rtsarfat/resources.htm. 8Our training procedure is strictly equivalent to the transform-detransform methodology of (Johnson, 1998), but we implement a tree-traverse procedure as in (Bikel, 2002) collecting all parameters per event at once. sign the syntactic as well as morphological analysis to the surface segments.9 We use the standard development/training/test split as in (Tsarfaty and Sima’an, 2008). Since our goal is a detailed comparison and fine-grained analysis of the results we concentrate on the development set. We use a general-purpose CKY parser (Schmid, 2004) to exhaustively parse the sentences, and we strip off all model-specific information prior to evaluation. Evaluation We use standard Parseval measures c</context>
<context position="24217" citStr="Johnson, 1998" startWordPosition="3782" endWordPosition="3783">9 / 81.18 Table 5: Per-Category Evaluation of Parsing Performance for Different Models: Prec/Rec Per Category Calculated for All Sentences. HD model. The resulting precision improvement of the RR relative to HD is larger than the improvement relative to SP, and the Recall improvement pattern is reversed. So it seems that the HD model generalizes better than the SP model, but also gets generalizations wrong more often than the SP model. The RR model combines the generalization advantage of breaking down context-free events while it maintains the coherence advantage of learning flat trees (cf. (Johnson, 1998)). The best RR model obtains the best performance among all models: F176.41. To put this result in context, for the setting in which the Arabic parser of (Maamouri et al., 2008) obtains F178.1, — i.e., with gold standard feature-rich tags — the best RR model obtains F183.3 accuracy which is the best parsing result reported for a Semitic language so far. RR models also have the advantage of resulting in more compact grammars, which makes learning and parsing with them much more computationally efficient. 5.2 Per-Category Break-Down Analysis To understand better the merits of the different model</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>M. Johnson. 1998. PCFG Models of Linguistic Tree Representations. Computational Linguistics, 24(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="3616" citStr="Klein and Manning, 2003" startWordPosition="543" endWordPosition="546">ernative to the Head-Driven (HD) approach in the development of phrase-structure based statistical parsing models. Recently, we proposed the RelationalRealizational (RR) approach that rests upon different premises (Tsarfaty and Sima’an, 2008). The question of how the RR model fares against the HD models that have so far been predominantly used has never been tackled. Yet, it is precisely such a comparison that can shed new light on the question of adequacy we posed above. Empirically quantifying the effects of different modeling choices has been addressed for English by, e.g., (Johnson, 1998; Klein and Manning, 2003), and for German by, e.g., (Dubey, 2004; 1Consider, e.g., “The PaGe shared task on parsing German” (Kubler, 2008), reporting F,75, F,79, F,83 for the participating parsers. 842 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 842–851, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP Rafferty and Manning, 2008). This paper provides an empirical systematic comparison of conceptually different modeling strategies with respect to parsing Hebrew. This comparison is intended to provide a first answer to the question of parser adequacy in the face of word-</context>
<context position="11266" citStr="Klein and Manning, 2003" startWordPosition="1736" endWordPosition="1739"> syntactic constituents revolves around their heads, Head-Driven (HD) models have been proposed by (Magerman, 1995; Charniak, 1997; Collins, 1999). In a generative HD model, the head daughter is generated first, conditioned on properties of the mother node. Then, sisters of the head daughter are generated conditioned on the head, typically by left and right generation processes. Overall, HD processes have the modeling advantage that they capture structurallymarked positions that characterize the argument structure of the sentence. The simplest possible process uses unigram probabilities, but (Klein and Manning, 2003) show that using vertical and horizontal Markovization improves parsing accuracy.4 An unlexicalized generative HD model will generate our two example sentences as we illustrate in figure 2. The generation of the contextfree events in figure 1 is then broken down to seven different context-free parameters each, encoding head-parent and head-sister structural relationships — the latter mediated with a structurallymarked delta function (Δi). The rich morphological representation of phrase-level NP objects (+def/acc), for instance, is conditioned on the head sister, its direction, and the distance</context>
<context position="12656" citStr="Klein and Manning, 2003" startWordPosition="1949" endWordPosition="1952">) similarly decomposes the generation of the context-free events in figure 1 into multiple independent parameters, but does so in a conceptually different way. Instead of decomposing a context-free event to head and sisters, the RR model is best viewed as a generative grammar that decomposes it to form and function. The RR grammar first generates a set of grammatical functions depicting the Relational Network (RN) (Perlmutter, 1982) of the clause. This 4The success of Head-Driven models (Charniak, 1997; Collins, 2003) was initially attributed to the fact that they were fully lexicalized, but (Klein and Manning, 2003) show that an unlexicalized model combining Head-Driven Markovian processes with linguistically motivated state-splits can approach the performance of fully lexicalized models. 844 (3a) S NP-SBJ VP-PRD ADVP NP+D+ACC-OBJ PP-COM Dani natan etmol et-hamatana le-dina gave yesterday the-present to-Dina (3b) S NP+D+ACC-OBJ VP-PRD ADVP NP-SBJ PP-COM et-ha-matana natan etmol Dani le-dina the-present gave yesterday Dani to-Dina Figure 1: The State-Splits Approach for Ex. (3) (3a) S V P@S L,ΔL1, V P@S NP Dani VP Dani natan gave R,ΔR2, V P@S NP+D+ACC R,ΔR3, V P@S et-ha-matana the-Present PP le-dina to-Di</context>
<context position="21040" citStr="Klein and Manning, 2003" startWordPosition="3252" endWordPosition="3255">assume that morphological features are known up front. Morphological segmentation is also ambiguous, but for our purposes it is unavoidable. When comparing different models on an individual sentence they may propose segmentation to sequences of different lengths, for which accuracy results cannot be faithfully compared. See (Tsarfaty, 2006) for discussion. 10The flat canonical representation also allows for a fair comparison that is not biased by the differing branching factors of the different models. 11In HD models, a head-tag is already assumed in the conditioning context for sister nodes (Klein and Manning, 2003). 846 SP-PCFG Grand-Parent − − + + Head-Tag − + − + Prec/Rec 70.05/72.40 71.14/72.03 74.66/74.35 71.99/72.17 (#Params) (4995) (8366) (7385) (11633) HD-PCFG Grand-Parent − − + + Markov 0 1 0 1 Prec/Rec 66.87/71.64 70.40/74.35 73.04/71.94 73.52/74.84 (#Params) (6678) (10015) (19066) (21399) RR-PCFG Grand-Parent − − + + Head Tag − + − + Prec/Rec 69.90/73.96 72.96/75.73 74.19/75.03 76.32/76.51 (#Params) (3791) (7546) (7611) (13618) Table 4: The Performance of Different Models in Parsing Hebrew: Parsing Results Prec/Recall for Sentences of Length G 40. For all models, grandparent encoding is helpfu</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. Manning. 2003. Accurate Unlexicalized Parsing. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kubler</author>
</authors>
<title>The PaGe Shared task on Parsing German.</title>
<date>2008</date>
<booktitle>In ACL Workshop on Parsing German.</booktitle>
<contexts>
<context position="3729" citStr="Kubler, 2008" startWordPosition="564" endWordPosition="565"> we proposed the RelationalRealizational (RR) approach that rests upon different premises (Tsarfaty and Sima’an, 2008). The question of how the RR model fares against the HD models that have so far been predominantly used has never been tackled. Yet, it is precisely such a comparison that can shed new light on the question of adequacy we posed above. Empirically quantifying the effects of different modeling choices has been addressed for English by, e.g., (Johnson, 1998; Klein and Manning, 2003), and for German by, e.g., (Dubey, 2004; 1Consider, e.g., “The PaGe shared task on parsing German” (Kubler, 2008), reporting F,75, F,79, F,83 for the participating parsers. 842 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 842–851, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP Rafferty and Manning, 2008). This paper provides an empirical systematic comparison of conceptually different modeling strategies with respect to parsing Hebrew. This comparison is intended to provide a first answer to the question of parser adequacy in the face of word-order freedom. Our two empirical results are unequivocal. Firstly, RR models significantly outperform HD models (</context>
<context position="10402" citStr="Kubler, 2008" startWordPosition="1608" endWordPosition="1609">and functional features (such as subject, object, etc.) decorate phrase-level constituent labels (Sima’an et al., 2001). The S-level representation of our example sentences (3a)–(3b) then would be as we depict in figure 1, which can be read off as feature-rich 3Such clauses are defined formally as exocentric in formal theories of syntax, and are used to describe syntactic structures in, e.g., Tagalog, Hungarian and Warlpiri (Bresnan, 2001, page 110). This flat representation format is characteristic of treebanks for other languages with relatively-free word-order as well, such as German (cf. (Kubler, 2008)). PCFG productions. We refer to this approach as the State-Splits (SP) approach, which serves as the baseline for the rest of our investigation. 3.2 The Head-Driven Approach Following the linguistic wisdom that the internal organization of syntactic constituents revolves around their heads, Head-Driven (HD) models have been proposed by (Magerman, 1995; Charniak, 1997; Collins, 1999). In a generative HD model, the head daughter is generated first, conditioned on properties of the mother node. Then, sisters of the head daughter are generated conditioned on the head, typically by left and right </context>
</contexts>
<marker>Kubler, 2008</marker>
<rawString>S. Kubler. 2008. The PaGe Shared task on Parsing German. In ACL Workshop on Parsing German.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Maamouri</author>
<author>A Bies</author>
<author>T Buckwalter</author>
<author>W Mekki</author>
</authors>
<title>The Penn Arabic Treebank: Building a LargeScale Annotated Arabic Corpus.</title>
<date>2004</date>
<booktitle>In Proceedings of NEMLAR.</booktitle>
<contexts>
<context position="2302" citStr="Maamouri et al., 2004" startWordPosition="333" endWordPosition="336"> results are starting to level off at around F192.1(McClosky et al., 2008). As the interest of the NLP community grows to encompass more languages, we observe efforts towards adapting an English parser for parsing other languages (e.g., (Collins et al., 1999)), or towards designing a language-independent framework based on principles underlying the models for parsing English (Bikel, 2002). The performance curve for parsing other languages with these models looks rather different. A case in point is Modern Standard Arabic. Since the initial effort of (Bikel, 2002) to parse the Arabic treebank (Maamouri et al., 2004), which yielded F175 accuracy, four years and successive revisions have led to no more than F179 (Maamouri et al., 2008). This pattern from Arabic is not peculiar. The level of state-of-the-art results for other languages still lags behind those for English, even after putting considerable effort into the adaptation.1 Given that these languages are inherently different from English and from one another, it appears that we cannot avoid a question concerning the adequacy of the models used to parse them. That is, given the properties of a language, which modeling strategy would be appropriate fo</context>
</contexts>
<marker>Maamouri, Bies, Buckwalter, Mekki, 2004</marker>
<rawString>M. Maamouri, A. Bies, T. Buckwalter, and W. Mekki. 2004. The Penn Arabic Treebank: Building a LargeScale Annotated Arabic Corpus. In Proceedings of NEMLAR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Maamouri</author>
<author>A Bies</author>
<author>S Kulick</author>
</authors>
<title>Enhanced Annotation and Parsing of the Arabic treebank.</title>
<date>2008</date>
<booktitle>In Proceedings ofINFOS.</booktitle>
<contexts>
<context position="2422" citStr="Maamouri et al., 2008" startWordPosition="354" endWordPosition="357">o encompass more languages, we observe efforts towards adapting an English parser for parsing other languages (e.g., (Collins et al., 1999)), or towards designing a language-independent framework based on principles underlying the models for parsing English (Bikel, 2002). The performance curve for parsing other languages with these models looks rather different. A case in point is Modern Standard Arabic. Since the initial effort of (Bikel, 2002) to parse the Arabic treebank (Maamouri et al., 2004), which yielded F175 accuracy, four years and successive revisions have led to no more than F179 (Maamouri et al., 2008). This pattern from Arabic is not peculiar. The level of state-of-the-art results for other languages still lags behind those for English, even after putting considerable effort into the adaptation.1 Given that these languages are inherently different from English and from one another, it appears that we cannot avoid a question concerning the adequacy of the models used to parse them. That is, given the properties of a language, which modeling strategy would be appropriate for parsing it? Until recently, there has been practically no computationally affordable alternative to the Head-Driven (H</context>
<context position="24394" citStr="Maamouri et al., 2008" startWordPosition="3812" endWordPosition="3815">sion improvement of the RR relative to HD is larger than the improvement relative to SP, and the Recall improvement pattern is reversed. So it seems that the HD model generalizes better than the SP model, but also gets generalizations wrong more often than the SP model. The RR model combines the generalization advantage of breaking down context-free events while it maintains the coherence advantage of learning flat trees (cf. (Johnson, 1998)). The best RR model obtains the best performance among all models: F176.41. To put this result in context, for the setting in which the Arabic parser of (Maamouri et al., 2008) obtains F178.1, — i.e., with gold standard feature-rich tags — the best RR model obtains F183.3 accuracy which is the best parsing result reported for a Semitic language so far. RR models also have the advantage of resulting in more compact grammars, which makes learning and parsing with them much more computationally efficient. 5.2 Per-Category Break-Down Analysis To understand better the merits of the different models we conducted a break-down analysis of performance-per-category for the best performing models of each kind. The break-down results are shown in table 5. We divided the table i</context>
</contexts>
<marker>Maamouri, Bies, Kulick, 2008</marker>
<rawString>M. Maamouri, A. Bies, and S. Kulick. 2008. Enhanced Annotation and Parsing of the Arabic treebank. In Proceedings ofINFOS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Magerman</author>
</authors>
<title>Statistical Decision-Tree Models for Parsing.</title>
<date>1995</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="10756" citStr="Magerman, 1995" startWordPosition="1660" endWordPosition="1661">to describe syntactic structures in, e.g., Tagalog, Hungarian and Warlpiri (Bresnan, 2001, page 110). This flat representation format is characteristic of treebanks for other languages with relatively-free word-order as well, such as German (cf. (Kubler, 2008)). PCFG productions. We refer to this approach as the State-Splits (SP) approach, which serves as the baseline for the rest of our investigation. 3.2 The Head-Driven Approach Following the linguistic wisdom that the internal organization of syntactic constituents revolves around their heads, Head-Driven (HD) models have been proposed by (Magerman, 1995; Charniak, 1997; Collins, 1999). In a generative HD model, the head daughter is generated first, conditioned on properties of the mother node. Then, sisters of the head daughter are generated conditioned on the head, typically by left and right generation processes. Overall, HD processes have the modeling advantage that they capture structurallymarked positions that characterize the argument structure of the sentence. The simplest possible process uses unigram probabilities, but (Klein and Manning, 2003) show that using vertical and horizontal Markovization improves parsing accuracy.4 An unle</context>
</contexts>
<marker>Magerman, 1995</marker>
<rawString>D. M. Magerman. 1995. Statistical Decision-Tree Models for Parsing. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics.</title>
<date>1993</date>
<contexts>
<context position="1136" citStr="Marcus et al., 1993" startWordPosition="155" endWordPosition="158">w that the recently proposed RelationalRealizational (RR) model consistently outperforms state-of-the-art Head-Driven (HD) models on the Hebrew Treebank. Our analysis reveals a weakness of HD models: their intrinsic focus on configurational information. We conclude that the form-function separation ingrained in RR models makes them better suited for parsing nonconfigurational phenomena. 1 Introduction Parsing technology has come a long way since Charniak (1996) demonstrated that a simple treebank PCFG performs better than any other parser (with F175 accuracy) on parsing the WSJ Penn treebank (Marcus et al., 1993). Treebank Grammars (Scha, 1990; Charniak, 1996) trained on large corpora nowadays present the best available means to parse natural language text. The performance curve for parsing the WSJ was a steep one at first, as the incorporation of notions such as head, distance, subcategorization (Charniak, 1997; Collins, 1999) brought about a dramatic increase in parsing accuracy to the level of F188. Discriminative approaches, DataOriented Parsing (‘all-subtrees’) approaches, and self-training techniques brought further improvements, and recent results are starting to level off at around F192.1(McCl</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P H Matthews</author>
</authors>
<date>1993</date>
<publisher>Morphology. Cambridge.</publisher>
<contexts>
<context position="7579" citStr="Matthews, 1993" startWordPosition="1165" endWordPosition="1166">n its configurational position, the grammatical function Object in Hebrew is indicated by Differential Object Marking (DOM) (Aissen, 2003). NP objects in Hebrew are marked for accusativity (using the marker et) if they are also marked for definiteness (indicated by the prefix ha). So, in contrast with (2a)-(2b), the indefinite object renders (2c) ungrammatical, and the missing accusativity renders (2d) awkward. The fact that marking NP objects involves the joint contribution of multiple surface elements (et, ha) contributing features to the NP constituent is referred to as extended exponence (Matthews, 1993, p. 182). (2) a. dani natan matana ledina Dani gave present to-Dina “Dani gave a present to Dina” b. dani natan et hamatana ledina Dani gave ACC the-present to-Dina “Dani gave the present to Dina” c. *dani natan et matana ledina Dani gave ACC present to-Dina d. ??dani natan hamatana ledina Dani gave the-present to-Dina These data pose a challenge to generative parsing models, as they would be required to generate alternative word-order patterns while maintaining a coherent pattern of object marking, encom843 passing the contribution of multiple surface exponents. The question this paper addre</context>
</contexts>
<marker>Matthews, 1993</marker>
<rawString>P. H. Matthews. 1993. Morphology. Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McClosky</author>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>When is self-training effective for parsing?</title>
<date>2008</date>
<booktitle>In Proceedings of CoLing.</booktitle>
<contexts>
<context position="1754" citStr="McClosky et al., 2008" startWordPosition="246" endWordPosition="249">993). Treebank Grammars (Scha, 1990; Charniak, 1996) trained on large corpora nowadays present the best available means to parse natural language text. The performance curve for parsing the WSJ was a steep one at first, as the incorporation of notions such as head, distance, subcategorization (Charniak, 1997; Collins, 1999) brought about a dramatic increase in parsing accuracy to the level of F188. Discriminative approaches, DataOriented Parsing (‘all-subtrees’) approaches, and self-training techniques brought further improvements, and recent results are starting to level off at around F192.1(McClosky et al., 2008). As the interest of the NLP community grows to encompass more languages, we observe efforts towards adapting an English parser for parsing other languages (e.g., (Collins et al., 1999)), or towards designing a language-independent framework based on principles underlying the models for parsing English (Bikel, 2002). The performance curve for parsing other languages with these models looks rather different. A case in point is Modern Standard Arabic. Since the initial effort of (Bikel, 2002) to parse the Arabic treebank (Maamouri et al., 2004), which yielded F175 accuracy, four years and succes</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2008</marker>
<rawString>D. McClosky, E. Charniak, and M. Johnson. 2008. When is self-training effective for parsing? In Proceedings of CoLing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Melnik</author>
</authors>
<date>2002</date>
<booktitle>Verb-Initial Constructions in Modern Hebrew. Ph.D. thesis,</booktitle>
<location>Berkeley, California.</location>
<contexts>
<context position="5871" citStr="Melnik, 2002" startWordPosition="891" endWordPosition="892">ties affect the syntactic representations found in the Hebrew Treebank and the kind of syntactic phenomena a parser for Hebrew has to cope with. Modern Hebrew is a Semitic language with a canonical SVO word-order pattern,2 yet it allows considerable freedom in the placement of syntactic constituents in a clause. For example, linguistic elements of any kind may be fronted, triggering an inversion familiar from Germanic languages as in (1b) (Triggered Inversion (TI) in (Shlonsky, 1997)). Under some information structuring conditions, Verb Initial (VI) constructions are also allowed, as in (1c) (Melnik, 2002). All sentences in (1) thus mean “Dani gave the present to Dina”, despite their different word-ordering. (1) a. dani natan et hamatana ledina Dani gave ACC the-present to-Dina b. et hamatana natan dani ledina ACC the-present gave Dani to-Dina 2SVO is an abbreviation for the Subject-Verb-Object type in the basic word-order typology of (Greenberg, 1963). Word Order Frequency Relative Frequency SV 1612 41% VS 1144 29% No S 624 16% No V 550 14% Table 1: Modern Hebrew Predicative ClauseTypes in 3930 Predicative Matrix Clauses in the Training Set of the Modern Hebrew Treebank. c. natan dani et hamat</context>
</contexts>
<marker>Melnik, 2002</marker>
<rawString>N. Melnik. 2002. Verb-Initial Constructions in Modern Hebrew. Ph.D. thesis, Berkeley, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Sandra K¨ubler</author>
<author>Ryan McDonald</author>
<author>Jens Nilsson</author>
<author>Sebastian Riedel</author>
<author>Deniz Yuret</author>
</authors>
<title>The Shared Task on Dependency Parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task.</booktitle>
<marker>Nivre, Hall, K¨ubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The Shared Task on Dependency Parsing. In Proceedings of the CoNLL Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Perlmutter</author>
</authors>
<title>Syntactic Representation, Syntactic Levels, and the Notion of a Subject.</title>
<date>1982</date>
<booktitle>The Nature of Syntactic Representation.</booktitle>
<editor>In Pauline Jacobson and Geoffrey Pullum, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="12468" citStr="Perlmutter, 1982" startWordPosition="1922" endWordPosition="1923">nd the distance from the head (check, e.g., nodes ΔL1, ΔR2). 3.3 The Relational-Realizational Approach The Relational-Realizational (RR) parsing model of (Tsarfaty and Sima’an, 2008) similarly decomposes the generation of the context-free events in figure 1 into multiple independent parameters, but does so in a conceptually different way. Instead of decomposing a context-free event to head and sisters, the RR model is best viewed as a generative grammar that decomposes it to form and function. The RR grammar first generates a set of grammatical functions depicting the Relational Network (RN) (Perlmutter, 1982) of the clause. This 4The success of Head-Driven models (Charniak, 1997; Collins, 2003) was initially attributed to the fact that they were fully lexicalized, but (Klein and Manning, 2003) show that an unlexicalized model combining Head-Driven Markovian processes with linguistically motivated state-splits can approach the performance of fully lexicalized models. 844 (3a) S NP-SBJ VP-PRD ADVP NP+D+ACC-OBJ PP-COM Dani natan etmol et-hamatana le-dina gave yesterday the-present to-Dina (3b) S NP+D+ACC-OBJ VP-PRD ADVP NP-SBJ PP-COM et-ha-matana natan etmol Dani le-dina the-present gave yesterday Da</context>
<context position="35000" citStr="Perlmutter, 1982" startWordPosition="5449" endWordPosition="5450">transparently related to its formal position, which entails word-order rigidity. Such transparent relations between configurational positions and grammatical functions are assumed by other kinds of parsing frameworks such as the ‘allsubtrees’ approach of Data-Oriented Parsing, and the distinction between left and right application in CCG-based parsers. The RR modeling strategy stipulates a strict separation between form — parametrizing explicitly basic word-order (Greenberg, 1963) and morphological realization (Greenberg, 1954) — and function — parametrizing relational networks borrowed from (Perlmutter, 1982) — which makes it possible to statistically learn complex formfunction mapping reflected in the data. This is an adequate means to capture, e.g., morphosyntactic interactions, which characterize the lessconfigurational languages on the scale. 8 Conclusion In our comparison of the HD and RR modeling approaches, the RR approach is shown to be empirically superior and typologically more adequate for parsing a language exhibiting word-order variation interleaved with extended morphology. HD models are less accurate and more vulnerable to sparseness as they assume transparent mappings between form </context>
</contexts>
<marker>Perlmutter, 1982</marker>
<rawString>D. M. Perlmutter. 1982. Syntactic Representation, Syntactic Levels, and the Notion of a Subject. In Pauline Jacobson and Geoffrey Pullum, editors, The Nature of Syntactic Representation. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>L Barrett</author>
<author>R Thibaux</author>
<author>D Klein</author>
</authors>
<title>Learning Accurate, Compact, and Interpretable Tree Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="34096" citStr="Petrov et al., 2006" startWordPosition="5328" endWordPosition="5331">of realization of the grammatical functions within the phrase-structure representation of trees. Recent morphological theories employ Form-Function separation as a widelyaccepted practice for enhancing the adequacy of models describing variability in the realization of grammatical properties. Our results suggest that the adequacy of syntactic processing models is related to such typological insights as well, and is enhanced by adopting a similar form-function separation for expressing grammatical relations. cally rich languages. A promising strategy then would be to smooth or split-and-merge (Petrov et al., 2006)) RR-based models rather than to add an elaborate smoothing component to configurationally-based HD models. 849 configurational nonconfigurational Chinese&gt;English&gt;{German,Hebrew}&gt;Warlpiri Figure 5: The Configurationality Scale The HD assumptions take the function of a constituent to be transparently related to its formal position, which entails word-order rigidity. Such transparent relations between configurational positions and grammatical functions are assumed by other kinds of parsing frameworks such as the ‘allsubtrees’ approach of Data-Oriented Parsing, and the distinction between left an</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. Learning Accurate, Compact, and Interpretable Tree Annotation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rafferty</author>
<author>C D Manning</author>
</authors>
<title>Parsing Three German Treebanks: Lexicalized and Unlexicalized Baselines.</title>
<date>2008</date>
<booktitle>In ACL WorkShop on Parsing German.</booktitle>
<contexts>
<context position="3972" citStr="Rafferty and Manning, 2008" startWordPosition="597" endWordPosition="600">er been tackled. Yet, it is precisely such a comparison that can shed new light on the question of adequacy we posed above. Empirically quantifying the effects of different modeling choices has been addressed for English by, e.g., (Johnson, 1998; Klein and Manning, 2003), and for German by, e.g., (Dubey, 2004; 1Consider, e.g., “The PaGe shared task on parsing German” (Kubler, 2008), reporting F,75, F,79, F,83 for the participating parsers. 842 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 842–851, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP Rafferty and Manning, 2008). This paper provides an empirical systematic comparison of conceptually different modeling strategies with respect to parsing Hebrew. This comparison is intended to provide a first answer to the question of parser adequacy in the face of word-order freedom. Our two empirical results are unequivocal. Firstly, RR models significantly outperform HD models (about 2 points absolute improvement in Fi) in parsing the Modern Hebrew treebank. In particular, RR models show better performance in identifying the constituents for which syntactic positions are relatively free. Secondly, we show a novel var</context>
<context position="31785" citStr="Rafferty and Manning, 2008" startWordPosition="4987" endWordPosition="4990">els. The function-set variation performs slightly (but not significantly) better than the category-set. What seems to be still standing in the way of getting useful disambiguation cues for HD models is the fact that the left and right direction of realization is hardwired in their representation. This breaks down a coherent distribution over morphosyntactic representations realizing grammatical relations to arbitrary position-dependent fragments, which results in larger grammars and inferior performance.14 13The startegy of adding grammatical functions as statesplits is used in, e.g., German (Rafferty and Manning, 2008). 14Due to the difference in the size of the grammars, one could argue that smoothing will bridge the gap between the HD and RR modeling strategies. However, the better size/accuracy trade-off shown here for RR models suggests that they provide a good bias/variance balancing point, especially for feature-rich models characterizing morphologi7 A Typological Detour Hebrew, Arabic and other Semitic Languages are known to be substantially different from English in that English is strongly configurational. In configurational languages word-order is fixed, and information about the grammatical funct</context>
</contexts>
<marker>Rafferty, Manning, 2008</marker>
<rawString>A. Rafferty and C. D. Manning. 2008. Parsing Three German Treebanks: Lexicalized and Unlexicalized Baselines. In ACL WorkShop on Parsing German.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Scha</author>
</authors>
<title>Language Theory and Language Technology; Competence and Performance. In</title>
<date>1990</date>
<booktitle>Computertoepassingen in de Neerlandistiek.</booktitle>
<editor>Q. A. M. de Kort and G. L. J. Leerdam, editors,</editor>
<publisher>LVVN.</publisher>
<location>Almere:</location>
<contexts>
<context position="1167" citStr="Scha, 1990" startWordPosition="162" endWordPosition="163">alizational (RR) model consistently outperforms state-of-the-art Head-Driven (HD) models on the Hebrew Treebank. Our analysis reveals a weakness of HD models: their intrinsic focus on configurational information. We conclude that the form-function separation ingrained in RR models makes them better suited for parsing nonconfigurational phenomena. 1 Introduction Parsing technology has come a long way since Charniak (1996) demonstrated that a simple treebank PCFG performs better than any other parser (with F175 accuracy) on parsing the WSJ Penn treebank (Marcus et al., 1993). Treebank Grammars (Scha, 1990; Charniak, 1996) trained on large corpora nowadays present the best available means to parse natural language text. The performance curve for parsing the WSJ was a steep one at first, as the incorporation of notions such as head, distance, subcategorization (Charniak, 1997; Collins, 1999) brought about a dramatic increase in parsing accuracy to the level of F188. Discriminative approaches, DataOriented Parsing (‘all-subtrees’) approaches, and self-training techniques brought further improvements, and recent results are starting to level off at around F192.1(McClosky et al., 2008). As the inte</context>
</contexts>
<marker>Scha, 1990</marker>
<rawString>R. Scha. 1990. Language Theory and Language Technology; Competence and Performance. In Q. A. M. de Kort and G. L. J. Leerdam, editors, Computertoepassingen in de Neerlandistiek. Almere: LVVN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Efficient Parsing of Highly Ambiguous Context-Free Grammars with Bit vectors.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="18741" citStr="Schmid, 2004" startWordPosition="2886" endWordPosition="2887">e available at www. science.uva.nl/˜rtsarfat/resources.htm. 8Our training procedure is strictly equivalent to the transform-detransform methodology of (Johnson, 1998), but we implement a tree-traverse procedure as in (Bikel, 2002) collecting all parameters per event at once. sign the syntactic as well as morphological analysis to the surface segments.9 We use the standard development/training/test split as in (Tsarfaty and Sima’an, 2008). Since our goal is a detailed comparison and fine-grained analysis of the results we concentrate on the development set. We use a general-purpose CKY parser (Schmid, 2004) to exhaustively parse the sentences, and we strip off all model-specific information prior to evaluation. Evaluation We use standard Parseval measures calculated for the original, flat, canonical representation of the parse trees.10 We report Precision/Recall for the coarse-grained non-terminal categories. In addition to overall Parseval scores we report the accuracy results Per Syntactic Category. We further report model size in terms of the number of parameters. As is well known in Machine Learning, models with more parameters require more data to learn, and are more vulnerable to sparsenes</context>
</contexts>
<marker>Schmid, 2004</marker>
<rawString>H. Schmid. 2004. Efficient Parsing of Highly Ambiguous Context-Free Grammars with Bit vectors. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Shlonsky</author>
</authors>
<date>1997</date>
<booktitle>Clause Structure and Word Order in Hebrew and Arabic.</booktitle>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="5746" citStr="Shlonsky, 1997" startWordPosition="872" endWordPosition="873">describes some properties of Modern Hebrew (henceforth, Hebrew) that make it significantly different from English. These properties affect the syntactic representations found in the Hebrew Treebank and the kind of syntactic phenomena a parser for Hebrew has to cope with. Modern Hebrew is a Semitic language with a canonical SVO word-order pattern,2 yet it allows considerable freedom in the placement of syntactic constituents in a clause. For example, linguistic elements of any kind may be fronted, triggering an inversion familiar from Germanic languages as in (1b) (Triggered Inversion (TI) in (Shlonsky, 1997)). Under some information structuring conditions, Verb Initial (VI) constructions are also allowed, as in (1c) (Melnik, 2002). All sentences in (1) thus mean “Dani gave the present to Dina”, despite their different word-ordering. (1) a. dani natan et hamatana ledina Dani gave ACC the-present to-Dina b. et hamatana natan dani ledina ACC the-present gave Dani to-Dina 2SVO is an abbreviation for the Subject-Verb-Object type in the basic word-order typology of (Greenberg, 1963). Word Order Frequency Relative Frequency SV 1612 41% VS 1144 29% No S 624 16% No V 550 14% Table 1: Modern Hebrew Predica</context>
</contexts>
<marker>Shlonsky, 1997</marker>
<rawString>U. Shlonsky. 1997. Clause Structure and Word Order in Hebrew and Arabic. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Song</author>
</authors>
<title>Linguistic Typology: Morphology and Syntax.</title>
<date>2001</date>
<booktitle>In Proceedings of CoLing.</booktitle>
<publisher>Pearson Education</publisher>
<contexts>
<context position="33007" citStr="Song, 2001" startWordPosition="5162" endWordPosition="5163">stituents (e.g., subject or object) is often correlated with structurally-marked positions inside highly-nested constituency structures. Nonconfigurational languages (Hale, 1983), in contrast, allow for freedom in their word-ordering and information about grammatical relations between constituents is often marked by means of morphology. Configurationality is hardly a clear-cut notion. The difference in the configurationality level of different languages is often conceived as depicted in figure 7. In linguistic typology, the branch of linguistics that studies the differences between languages (Song, 2001), the division of labor between linear ordering and morphological marking in the realization of grammatical relations is often viewed as a continuum. Common wisdom has it that the lower a language is on the configurationality scale, the more morphological marking we expect to be used (Bresnan, 2001, page 6). For a statistical parser to cope with nonconfigurational phenomena as observed in, for instance, Hebrew or German, it should allow for flexibility in the form of realization of the grammatical functions within the phrase-structure representation of trees. Recent morphological theories empl</context>
</contexts>
<marker>Song, 2001</marker>
<rawString>J. J. Song. 2001. Linguistic Typology: Morphology and Syntax. Pearson Education Limited, Edinbrugh. R. Tsarfaty and K. Sima’an. 2008. RelationalRealizational Parsing. In Proceedings of CoLing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Tsarfaty</author>
</authors>
<title>Integrated Morphological and Syntactic Disambiguation for Modern Hebrew.</title>
<date>2006</date>
<booktitle>In Proceeding ofACL-SRW.</booktitle>
<contexts>
<context position="20185" citStr="Tsarfaty, 2006" startWordPosition="3117" endWordPosition="3118">d Analysis 5.1 Overall Results Table 4 shows the parsing results for the StateSplit (SP) PCFG, the Head-Driven (HD) PCFG and the Relational-Realizational (RR) PCFG models on parsing the Modern Hebrew Treebank, with definiteness and accusativity marked on PoStags as well as phrase-level categories. For all models, we experiment with grandparent encoding. For non-HD models, we also examine the utility of a head-category split.11 9This setup is more difficult than, e.g., the Arabic parsing setup of (Bikel, 2002), as they assume gold-standard pos-tags as input. Yet it is easier than the setup of (Tsarfaty, 2006; Goldberg and Tsarfaty, 2008) which uses unsegmented surface forms as input. The decision to use segmented and untagged forms was made to retain a realistic scenario. Morphological analysis is known to be ambiguous, and we do not assume that morphological features are known up front. Morphological segmentation is also ambiguous, but for our purposes it is unavoidable. When comparing different models on an individual sentence they may propose segmentation to sequences of different lengths, for which accuracy results cannot be faithfully compared. See (Tsarfaty, 2006) for discussion. 10The flat</context>
</contexts>
<marker>Tsarfaty, 2006</marker>
<rawString>R. Tsarfaty. 2006. Integrated Morphological and Syntactic Disambiguation for Modern Hebrew. In Proceeding ofACL-SRW.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>