<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.99954">
Convolution Kernels with Feature Selection
for Natural Language Processing Tasks
</title>
<author confidence="0.905274">
Jun Suzuki, Hideki Isozaki and Eisaku Maeda
</author>
<affiliation confidence="0.731996">
NTT Communication Science Laboratories, NTT Corp.
</affiliation>
<address confidence="0.802671">
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto,619-0237 Japan
</address>
<email confidence="0.994362">
{jun, isozaki, maeda}@cslab.kecl.ntt.co.jp
</email>
<sectionHeader confidence="0.993814" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999845333333333">
Convolution kernels, such as sequence and tree ker-
nels, are advantageous for both the concept and ac-
curacy of many natural language processing (NLP)
tasks. Experiments have, however, shown that the
over-fitting problem often arises when these ker-
nels are used in NLP tasks. This paper discusses
this issue of convolution kernels, and then proposes
a new approach based on statistical feature selec-
tion that avoids this issue. To enable the proposed
method to be executed efficiently, it is embedded
into an original kernel calculation process by using
sub-structure mining algorithms. Experiments are
undertaken on real NLP tasks to confirm the prob-
lem with a conventional method and to compare its
performance with that of the proposed method.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999944565217391">
Over the past few years, many machine learn-
ing methods have been successfully applied to
tasks in natural language processing (NLP). Espe-
cially, state-of-the-art performance can be achieved
with kernel methods, such as Support Vector
Machine (Cortes and Vapnik, 1995). Exam-
ples include text categorization (Joachims, 1998),
chunking (Kudo and Matsumoto, 2002) and pars-
ing (Collins and Duffy, 2001).
Another feature of this kernel methodology is that
it not only provides high accuracy but also allows us
to design a kernel function suited to modeling the
task at hand. Since natural language data take the
form of sequences of words, and are generally ana-
lyzed using discrete structures, such as trees (parsed
trees) and graphs (relational graphs), discrete ker-
nels, such as sequence kernels (Lodhi et al., 2002),
tree kernels (Collins and Duffy, 2001), and graph
kernels (Suzuki et al., 2003a), have been shown to
offer excellent results.
These discrete kernels are related to convolution
kernels (Haussler, 1999), which provides the con-
cept of kernels over discrete structures. Convolution
kernels allow us to treat structural features without
explicitly representing the feature vectors from the
input object. That is, convolution kernels are well
suited to NLP tasks in terms of both accuracy and
concept.
Unfortunately, experiments have shown that in
some cases there is a critical issue with convolution
kernels, especially in NLP tasks (Collins and Duffy,
2001; Cancedda et al., 2003; Suzuki et al., 2003b).
That is, the over-fitting problem arises if large “sub-
structures” are used in the kernel calculations. As a
result, the machine learning approach can never be
trained efficiently.
To solve this issue, we generally eliminate large
sub-structures from the set of features used. How-
ever, the main reason for using convolution kernels
is that we aim to use structural features easily and
efficiently. If use is limited to only very small struc-
tures, it negates the advantages of using convolution
kernels.
This paper discusses this issue of convolution
kernels, and proposes a new method based on statis-
tical feature selection. The proposed method deals
only with those features that are statistically signif-
icant for kernel calculation, large significant sub-
structures can be used without over-fitting. More-
over, the proposed method can be executed effi-
ciently by embedding it in an original kernel cal-
culation process by using sub-structure mining al-
gorithms.
In the next section, we provide a brief overview
of convolution kernels. Section 3 discusses one is-
sue of convolution kernels, the main topic of this
paper, and introduces some conventional methods
for solving this issue. In Section 4, we propose
a new approach based on statistical feature selec-
tion to offset the issue of convolution kernels us-
ing an example consisting of sequence kernels. In
Section 5, we briefly discuss the application of the
proposed method to other convolution kernels. In
Section 6, we compare the performance of conven-
tional methods with that of the proposed method by
using real NLP tasks: question classification and
sentence modality identification. The experimental
results described in Section 7 clarify the advantages
of the proposed method.
</bodyText>
<sectionHeader confidence="0.9602" genericHeader="introduction">
2 Convolution Kernels
</sectionHeader>
<bodyText confidence="0.9996423">
Convolution kernels have been proposed as a con-
cept of kernels for discrete structures, such as se-
quences, trees and graphs. This framework defines
the kernel function between input objects as the con-
volution of “sub-kernels”, i.e. the kernels for the
decompositions (parts) of the objects.
Let X and Y be discrete objects. Conceptually,
convolution kernels K(X, Y ) enumerate all sub-
structures occurring in X and Y and then calculate
their inner product, which is simply written as:
</bodyText>
<equation confidence="0.9816895">
K(X, Y ) = �(X), (Y )� = E i(X) - i(Y ). (1)
i
</equation>
<bodyText confidence="0.986420384615385">
O represents the feature mapping from the
discrete object to the feature space; that is,
O(X) = (O1(X), ... , Oi(X),...). With sequence
kernels (Lodhi et al., 2002), input objects X and Y
are sequences, and Oi(X) is a sub-sequence. With
tree kernels (Collins and Duffy, 2001), X and Y are
trees, and Oi(X) is a sub-tree.
When implemented, these kernels can be effi-
ciently calculated in quadratic time by using dy-
namic programming (DP).
Finally, since the size of the input objects is not
constant, the kernel value is normalized using the
following equation.
</bodyText>
<equation confidence="0.8729605">
K(X,Y )
ˆK(X, Y ) = �/K(X , X) - K(Y, Y ) (2)
</equation>
<bodyText confidence="0.980362">
The value of ˆK(X, Y ) is from 0 to 1, ˆK(X, Y ) = 1
if and only if X = Y .
</bodyText>
<subsectionHeader confidence="0.997906">
2.1 Sequence Kernels
</subsectionHeader>
<bodyText confidence="0.999809">
To simplify the discussion, we restrict ourselves
hereafter to sequence kernels. Other convolution
kernels are briefly addressed in Section 5.
Many kinds of sequence kernels have been pro-
posed for a variety of different tasks. This paper
basically follows the framework of word sequence
kernels (Cancedda et al., 2003), and so processes
gapped word sequences to yield the kernel value.
Let  be a set of finite symbols, and n be a set
of possible (symbol) sequences whose sizes are n
or less that are constructed by symbols in . The
meaning of “size” in this paper is the number of
symbols in the sub-structure. Namely, in the case of
sequence, size n means length n. S and T can rep-
resent any sequence. si and tj represent the ith and
jth symbols in S and T, respectively. Therefore, a
</bodyText>
<figure confidence="0.221570333333333">
sequences sub-sequences
S=abc (a, b, c, ab, ac, bc, abc)
T = abac (a, b, c, aa, ab, ac, ba, bc, aba, aac, abc, bac, abac)
</figure>
<figureCaption confidence="0.99911">
Figure 1: Example of sequence kernel output
</figureCaption>
<bodyText confidence="0.8449595">
sequence S can be written as S = s1 ... si ... s|S|,
where |S |represents the length of S. If sequence
</bodyText>
<equation confidence="0.6953475">
u is contained in sub-sequence S[i : j] def = si . . . sj
of S (allowing the existence of gaps), the position
of u in S is written as i = (i1 : i|u|). The length
of S[i] is l(i) = i|u |− i1 + 1. For example, if
u = ab and S = cacbd, then i = (2 : 4) and
l(i) = 4 − 2 + 1 = 3.
</equation>
<bodyText confidence="0.992221">
By using the above notations, sequence kernels
can be defined as:
</bodyText>
<equation confidence="0.994945">
KS-(S,T) = E E (i) E (j), (3)
uEEn i|u=S[i] j|u=T[j]
</equation>
<bodyText confidence="0.999898583333333">
where A is the decay factor that handles the gap
present in a common sub-sequence u, and -y(i) =
l(i)−|u|. In this paper,  |means “such that”. Figure 1
shows a simple example of the output of this kernel.
However, in general, the number of features |n|,
which is the dimension of the feature space, be-
comes very high, and it is computationally infeasi-
ble to calculate Equation (3) explicitly. The efficient
recursive calculation has been introduced in (Can-
cedda et al., 2003). To clarify the discussion, we
redefine the sequence kernels with our notation.
The sequence kernel can be written as follows:
</bodyText>
<equation confidence="0.981814">
E Jm(Si,Tj). (4)
1&lt;j&lt;|T|
</equation>
<bodyText confidence="0.99854225">
where Si and Tj represent the sub-sequences Si =
s1, s2, ... , si and Tj = t1, t2, ... , tj, respectively.
Let Jm(Si,Tj) be a function that returns the
value of common sub-sequences if si = tj.
</bodyText>
<equation confidence="0.7562745">
Jm(Si,Tj) = Jim−1(Si,Tj) - I(si,tj) (5)
I(si, tj) is a function that returns a matching
</equation>
<bodyText confidence="0.972171">
value between si and tj. This paper defines I(si, tj)
as an indicator function that returns 1 if si = tj, oth-
erwise 0.
</bodyText>
<figure confidence="0.933358">
kernel value 5+3A+A
3
1
1
u a, b, c, aa, ab, ac, ba, bc, aba, aac, abc, bac, abac
0 1
1 2
1 +�
2 1 1 0 1 �+� 0 � 0 0 � 0
3
s
T
prod.
2
1
1
1
A
A
0
1
A
1
0 0
1
A A
1
0
1
0
1
0
KS-(S,T) = n E
E 1&lt;i&lt;|S|
m=1
Then, J�m(Si, Tj) and Jm(Si, Tj) are introduced
to calculate the common gapped sub-sequences be-
tween Si and Tj.
</figure>
<tableCaption confidence="0.77396">
Table 1: Contingency table and notation for the chi-
squared value
</tableCaption>
<equation confidence="0.993700857142857">
I
(6)
Jm(Si, Tj) =
1 if m = 0,
0 if j = 0 and m &gt; 0,
AJm(Si, Tj−1) + Jm(Si, Tj−1)
otherwise
c c¯ E row
u Ouc = y Ou¯c Ou = x
u¯ O¯uc O¯u¯c O¯u
E column Oc = M O¯c N
0 if i = 0,
AJm(Si−1, Tj) + Jm(Si−1, Tj) (7)
otherwise
</equation>
<bodyText confidence="0.999628666666667">
If we calculate Equations (5) to (7) recursively,
Equation (4) provides exactly the same value as
Equation (3).
</bodyText>
<sectionHeader confidence="0.982774" genericHeader="method">
3 Problem of Applying Convolution
Kernels to NLP tasks
</sectionHeader>
<bodyText confidence="0.999990095238095">
This section discusses an issue that arises when ap-
plying convolution kernels to NLP tasks.
According to the original definition of convolu-
tion kernels, all the sub-structures are enumerated
and calculated for the kernels. The number of sub-
structures in the input object usually becomes ex-
ponential against input object size. As a result, all
kernel values ˆK(X, Y ) are nearly 0 except the ker-
nel value of the object itself, ˆK(X, X), which is 1.
In this situation, the machine learning process be-
comes almost the same as memory-based learning.
This means that we obtain a result that is very pre-
cise but with very low recall.
To avoid this, most conventional methods use an
approach that involves smoothing the kernel values
or eliminating features based on the sub-structure
size.
For sequence kernels, (Cancedda et al., 2003) use
a feature elimination method based on the size of
sub-sequence n. This means that the kernel calcula-
tion deals only with those sub-sequences whose size
is n or less. For tree kernels, (Collins and Duffy,
2001) proposed a method that restricts the features
based on sub-trees depth. These methods seem to
work well on the surface, however, good results are
achieved only when n is very small, i.e. n = 2.
The main reason for using convolution kernels
is that they allow us to employ structural features
simply and efficiently. When only small sized sub-
structures are used (i.e. n = 2), the full benefits of
convolution kernels are missed.
Moreover, these results do not mean that larger
sized sub-structures are not useful. In some cases
we already know that larger sub-structures are sig-
nificant features as regards solving the target prob-
lem. That is, these significant larger sub-structures,
which the conventional methods cannot deal with
efficiently, should have a possibility of improving
the performance furthermore.
The aim of the work described in this paper is
to be able to use any significant sub-structure effi-
ciently, regardless of its size, to solve NLP tasks.
</bodyText>
<sectionHeader confidence="0.97073" genericHeader="method">
4 Proposed Feature Selection Method
</sectionHeader>
<bodyText confidence="0.999988285714286">
Our approach is based on statistical feature selection
in contrast to the conventional methods, which use
sub-structure size.
For a better understanding, consider the two-
class (positive and negative) supervised classifica-
tion problem. In our approach we test the statisti-
cal deviation of all the sub-structures in the training
samples between the appearance of positive samples
and negative samples. This allows us to select only
the statistically significant sub-structures when cal-
culating the kernel value.
Our approach, which uses a statistical metric to
select features, is quite natural. We note, however,
that kernels are calculated using the DP algorithm.
Therefore, it is not clear how to calculate kernels ef-
ficiently with a statistical feature selection method.
First, we briefly explain a statistical metric, the chi-
squared (x2) value, and provide an idea of how
to select significant features. We then describe a
method for embedding statistical feature selection
into kernel calculation.
</bodyText>
<subsectionHeader confidence="0.99326">
4.1 Statistical Metric: Chi-squared Value
</subsectionHeader>
<bodyText confidence="0.999980083333333">
There are many kinds of statistical metrics, such as
chi-squared value, correlation coefficient and mu-
tual information. (Rogati and Yang, 2002) reported
that chi-squared feature selection is the most effec-
tive method for text classification. Following this
information, we use x2 values as statistical feature
selection criteria. Although we selected x2 values,
any other statistical metric can be used as long as it
is based on the contingency table shown in Table 1.
We briefly explain how to calculate the x2 value
by referring to Table 1. In the table, c and c¯ rep-
resent the names of classes, c for the positive class
</bodyText>
<equation confidence="0.670272">
Jm(Si,Tj) = I
</equation>
<figureCaption confidence="0.999238">
Figure 2: Example of statistical feature selection
</figureCaption>
<bodyText confidence="0.999957083333333">
and c¯ for the negative class. Ouc, Ou¯c, O¯uc and O¯u¯c
represent the number of u that appeared in the pos-
itive sample c, the number of u that appeared in the
negative sample ¯c, the number of u that did not ap-
pear in c, and the number of u that did not appear
in ¯c, respectively. Let y be the number of samples
of positive class c that contain sub-sequence u, and
x be the number of samples that contain u. Let N
be the total number of (training) samples, and M be
the number of positive samples.
Since N and M are constant for (fixed) data, 2
can be written as a function of x and y,
</bodyText>
<equation confidence="0.7826585">
2(x, y) = N(Ouc O¯u¯c _ O¯uc Ou¯c)2 .(8)
Ou O¯u Oc O¯c
</equation>
<bodyText confidence="0.997718666666667">
2 expresses the normalized deviation of the obser-
vation from the expectation.
We simply represent 2(x, y) as 2(u).
</bodyText>
<subsectionHeader confidence="0.994189">
4.2 Feature Selection Criterion
</subsectionHeader>
<bodyText confidence="0.999593428571429">
The basic idea of feature selection is quite natural.
First, we decide the threshold  of the 2 value. If
2(u) &lt;  holds, that is, u is not statistically signif-
icant, then u is eliminated from the features and the
value of u is presumed to be 0 for the kernel value.
The sequence kernel with feature selection
(FSSK) can be defined as follows:
</bodyText>
<equation confidence="0.874061">
EKFSSK(S, T) = E (i) E (j). (9)
~2(u)ju2En iju=S[i] jju=T[j]
</equation>
<bodyText confidence="0.9997225">
The difference between Equations (3) and (9) is
simply the condition of the first summation. FSSK
selects significant sub-sequence u by using the con-
dition of the statistical metric  &lt; 2(u).
Figure 2 shows a simple example of what FSSK
calculates for the kernel value.
</bodyText>
<subsectionHeader confidence="0.906314">
4.3 Efficient 2(u) Calculation Method
</subsectionHeader>
<bodyText confidence="0.999714388888889">
It is computationally infeasible to calculate 2(u)
for all possible u with a naive exhaustive method.
In our approach, we use a sub-structure mining al-
gorithm to calculate 2(u). The basic idea comes
from a sequential pattern mining technique, PrefixS-
pan (Pei et al., 2001), and a statistical metric prun-
ing (SMP) method, Apriori SMP (Morishita and
Sese, 2000). By using these techniques, all the sig-
nificant sub-sequences u that satisfy  &lt; 2(u) can
be found efficiently by depth-first search and prun-
ing. Below, we briefly explain the concept involved
in finding the significant features.
First, we denote uv, which is the concatenation of
sequences u and v. Then, u is a specific sequence
and uv is any sequence that is constructed by u with
any suffix v. The upper bound of the 2 value of
uv can be defined by the value of u (Morishita and
Sese, 2000).
</bodyText>
<equation confidence="0.7311635">
2(uv)&lt;max (2(yu, yu), 2(xu _ yu, 0))
=x2(u)
</equation>
<bodyText confidence="0.999749538461539">
where xu and yu represent the value of x and y
of u. This inequation indicates that if V(u) is less
than a certain threshold , all sub-sequences uv can
be eliminated from the features, because no sub-
sequence uv can be a feature.
The PrefixSpan algorithm enumerates all the sig-
nificant sub-sequences by using a depth-first search
and constructing a TRIE structure to store the sig-
nificant sequences of internal results efficiently.
Specifically, PrefixSpan algorithm evaluates uw,
where uw represents a concatenation of a sequence
u and a symbol w, using the following three condi-
tions.
</bodyText>
<equation confidence="0.844189666666667">
 &lt; 2(uw)
 &gt; 2(uw),  &gt; 5�2(uw)
 &gt; 2(uw),  &lt; 5�2(uw)
</equation>
<bodyText confidence="0.9980975">
With 1, sub-sequence uw is selected as a significant
feature. With 2, sub-sequence uw and arbitrary sub-
sequences uwv, are less than the threshold . Then
w is pruned from the TRIE, that is, all uwv where v
represents any suffix pruned from the search space.
With 3, uw is not selected as a significant feature
because the 2 value of uw is less than , however,
uwv can be a significant feature because the upper-
bound 2 value of uwv is greater than , thus the
search is continued to uwv.
Figure 3 shows a simple example of PrefixSpan
with SMP that searches for the significant features
</bodyText>
<figure confidence="0.986986634615385">
1
1
0
2 1 1 0 1 A+A 0 A 0 0 A 0
3
1.2
2.5
t&apos; u 0.1 0.5
( )
1.5 0.9 0.8
2+A
kernel value under the feature selection
u a, b, c, aa, ab, ac, ba, bc, aba, aac, abc, bac, abac
1
1 2
1 +�
2 1 1 0 1 A+A 0 A 0 0 � 0
3
s
T
prod.
2
1
1
1
A
0
A
0
1
A
1
0 0
1
A A
1
0
1
0
1
0
0 0 1 1 0 0 A
5+3A+A&apos;
kernel value
feature selection
r =1.0
threshold
S=abc
T = abac
( a, b, c, ab, ac, bc, abc)
( a, b, c, aa, ab, ac, ba, bc, aba, aac, abc, bac, abac)
sequences sub-sequences
</figure>
<bodyText confidence="0.999666166666667">
The following five equations are introduced to se-
lect a set of significant sub-sequences. Fm(Si, Tj)
and �Fm(Si,Tj) are sets of sub-sequences (features)
that satisfy condition 1 and 3, respectively, when
calculating the value between Si and Tj in Equa-
tions (11) and (12).
</bodyText>
<equation confidence="0.771173">
m(Si,Tj) = {u  |u  �m(Si,Tj),  2(u)} (15)
</equation>
<figure confidence="0.995554865384616">
5
4
4
2
2
2
2
0
w =
1
1
1
0
class training data
a b c c
d b c a
b a c
a c
d a b d
suffix
+ 1
-1
+ 1
-1
-1
u = A
search order
2
a b c d
5.0 5.0 5.0
0.0 0.8 0.8
pruned
X y
2 .2
2 .2
suffix
a b c c
d b c
b a c
a c
d a b d
+ 1
-1
+ 1
-1
-1
+ 1
-1
+ 1
-1
-1
w =
N =5
M=
2
X y
1 1
u=a
a b c c
d b c a
b a c
a c
d a b d
2
3
1
1
2
1
w =
+ 1
-1
+ 1
-1
+ 1
suffix
a b c c
d b c
b a c
a c
d a b d
b1 .9 5.0
0.1 c2.2
pruned
4
d 0.8
0.8
...
...
z =1.0
w
� ˆ
x&apos;
c 1 .9
1.9
u =ab
X y
d 0.8
0.8
3
5
TRIE representation
c 1.9
1 .9
</figure>
<figureCaption confidence="0.976192666666667">
Figure 3: Efficient search for statistically significant
sub-sequences using the PrefixSpan algorithm with
SMP
</figureCaption>
<bodyText confidence="0.999269222222222">
by using a depth-first search with a TRIE represen-
tation of the significant sequences. The values of
each symbol represent 2(u) and x2(u) that can be
calculated from the number of xu and yu. The TRIE
structure in the figure represents the statistically sig-
nificant sub-sequences that can be shown in a path
from L to the symbol.
We exploit this TRIE structure and PrefixSpan
pruning method in our kernel calculation.
</bodyText>
<subsectionHeader confidence="0.8867785">
4.4 Embedding Feature Selection in Kernel
Calculation
</subsectionHeader>
<bodyText confidence="0.999417333333333">
This section shows how to integrate statistical fea-
ture selection in the kernel calculation. Our pro-
posed method is defined in the following equations.
Let Km(Si, Tj) be a function that returns the sum
value of all statistically significant common sub-
sequences u if si = tj.
</bodyText>
<equation confidence="0.9924315">
Km(Si,Tj) = E Ju(Si,Tj), (11)
urm(Si,Tj)
</equation>
<bodyText confidence="0.995685">
where Fm(Si, Tj) represents a set of sub-sequences
whose size |u |is m and that satisfy the above condi-
tion 1. The Fm(Si,Tj) is defined in detail in Equa-
tion (15).
Then, let Ju(Si,Tj), Ju(Si,Tj) and Ju�� (Si, Tj)
be functions that calculate the value of the common
sub-sequences between Si and Tj recursively, as
well as equations (5) to (7) for sequence kernels. We
</bodyText>
<equation confidence="0.997139583333333">
n
KFSSK(S, T) = E
m=1
E
1i|S|
E Km(Si,Tj) (10)
1j|T|
Juw(Si,Tj) =
{ Ju(Si, Tj) ·I(w)
if uw  �|uw|(Si, Tj),
0 otherwise
1 if u = ,
0 if j = 0 and u =,
Ju(Si, Tj−1) + Ju (Si, Tj−1)
otherwise
0 if i = 0,
Ju(Si−1,Tj) + Ju(Si−1,Tj)
otherwise
�
��
��
Ju(Si, Tj) =
{
Ju(Si, Tj) =
</equation>
<bodyText confidence="0.990582333333333">
introduce a special symbol A to represent an “empty
sequence”, and define Aw = w and |Aw |= 1.
where Z(w) is a function that returns a matching
value of w. In this paper, we define Z(w) is 1.
Fm(Si, Tj) has realized conditions 2 and 3; the
details are defined in Equation (16).
</bodyText>
<equation confidence="0.799491">
(F, w) = {uw  |u  F,  V(uw)}, (17)
</equation>
<bodyText confidence="0.999205375">
where F represents a set of sub-sequences. No-
tice that Fm(Si, Tj) and fm(Si, Tj) have only sub-
sequences u that satisfy  &lt; 2(uw) or  &lt;
V(uw), respectively, if si = tj(= w); otherwise
they become empty sets.
The following two equations are introduced for
recursive set operations to calculate Fm(Si,Tj) and
�Fm(Si,Tj).
</bodyText>
<equation confidence="0.982269">
{  if i = 0 ,
�m(Si−1,Tj) 
otherwise
�m(Si, Tj) =
�m(Si−1, Tj) (19)
� m−1(Si,Tj),si)
if si = tj
 otherwise
�m(Si,Tj) =
{
(
(16)
{ {} if m = 0,
 if j = 0 and m &gt; 0,
�m(Si,Tj−1) 
Pm(Si,Tj) =
m(Si,Tj−1)
otherwise
(18)
</equation>
<bodyText confidence="0.999918">
In the implementation, Equations (11) to (14) can
be performed in the same way as those used to cal-
culate the original sequence kernels, if the feature
selection condition of Equations (15) to (19) has
been removed. Then, Equations (15) to (19), which
select significant features, are performed by the Pre-
fixSpan algorithm described above and the TRIE
representation of statistically significant features.
The recursive calculation of Equations (12) to
(14) and Equations (16) to (19) can be executed in
the same way and at the same time in parallel. As a
result, statistical feature selection can be embedded
in oroginal sequence kernel calculation based on a
dynamic programming technique.
</bodyText>
<subsectionHeader confidence="0.958145">
4.5 Properties
</subsectionHeader>
<bodyText confidence="0.997147">
The proposed method has several important advan-
tages over the conventional methods.
First, the feature selection criterion is based on
a statistical measure, so statistically significant fea-
tures are automatically selected.
Second, according to Equations (10) to (18), the
proposed method can be embedded in an original
kernel calculation process, which allows us to use
the same calculation procedure as the conventional
methods. The only difference between the original
sequence kernels and the proposed method is that
the latter calculates a statistical metric x2(u) by us-
ing a sub-structure mining algorithm in the kernel
calculation.
Third, although the kernel calculation, which uni-
fies our proposed method, requires a longer train-
ing time because of the feature selection, the se-
lected sub-sequences have a TRIE data structure.
This means a fast calculation technique proposed
in (Kudo and Matsumoto, 2003) can be simply ap-
plied to our method, which yields classification very
quickly. In the classification part, the features (sub-
sequences) selected in the learning part must be
known. Therefore, we store the TRIE of selected
sub-sequences and use them during classification.
</bodyText>
<sectionHeader confidence="0.983372" genericHeader="method">
5 Proposed Method Applied to Other
Convolution Kernels
</sectionHeader>
<bodyText confidence="0.938597291666667">
We have insufficient space to discuss this subject in
detail in relation to other convolution kernels. How-
ever, our proposals can be easily applied to tree ker-
nels (Collins and Duffy, 2001) by using string en-
coding for trees. We enumerate nodes (labels) of
tree in postorder traversal. After that, we can em-
ploy a sequential pattern mining technique to select
statistically significant sub-trees. This is because we
can convert to the original sub-tree form from the
string encoding representation.
Table 2: Parameter values of proposed kernels and
Support Vector Machines
parameter value
soft margin for SVM (C) 1000
decay factor of gap (A) 0.5
2.7055
threshold of x2 (T) 3.8415
As a result, we can calculate tree kernels with sta-
tistical feature selection by using the original tree
kernel calculation with the sequential pattern min-
ing technique introduced in this paper. Moreover,
we can expand our proposals to hierarchically struc-
tured graph kernels (Suzuki et al., 2003a) by using
a simple extension to cover hierarchical structures.
</bodyText>
<sectionHeader confidence="0.999868" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.9999635">
We evaluated the performance of the proposed
method in actual NLP tasks, namely English ques-
tion classification (EQC), Japanese question classi-
fication (JQC) and sentence modality identification
(MI) tasks.
We compared the proposed method (FSSK) with
a conventional method (SK), as discussed in Sec-
tion 3, and with bag-of-words (BOW) Kernel
(BOW-K)(Joachims, 1998) as baseline methods.
Support Vector Machine (SVM) was selected as
the kernel-based classifier for training and classifi-
cation. Table 2 shows some of the parameter values
that we used in the comparison. We set thresholds
of T = 2.7055 (FSSK1) and T = 3.8415 (FSSK2)
for the proposed methods; these values represent the
10% and 5% level of significance in the x2 distribu-
tion with one degree of freedom, which used the x2
significant test.
</bodyText>
<subsectionHeader confidence="0.995343">
6.1 Question Classification
</subsectionHeader>
<bodyText confidence="0.956613">
Question classification is defined as a task similar to
text categorization; it maps a given question into a
question type.
We evaluated the performance by using data
provided by (Li and Roth, 2002) for English
and (Suzuki et al., 2003b) for Japanese question
classification and followed the experimental setting
used in these papers; namely we use four typical
question types, LOCATION, NUMEX, ORGANI-
ZATION, and TIME TOP for JQA, and “coarse”
and “fine” classes for EQC. We used the one-vs-rest
classifier of SVM as the multi-class classification
method for EQC.
Figure 4 shows examples of the question classifi-
cation data used here.
</bodyText>
<figure confidence="0.958498375">
question types
input object: word sequences ([ ]: information of chunk and ( ): named entity)
ABBREVIATION
DESCRIPTION
what,[B-NP] be,[B-VP] the,[B-NP] abbreviation,[I-NP] for,[B-PP] Texas,[B-NP],(B-GPE) ?,[O]
what,[B-NP] be,[B-VP] Aborigines,[B-NP] ?,[O]
HUMAN
who,[B-NP] discover,[B-VP] America,[B-NP],(B-GPE) ?,[O]
</figure>
<figureCaption confidence="0.995951">
Figure 4: Examples of English question classification data
</figureCaption>
<tableCaption confidence="0.831367">
Table 3: Results of the Japanese question classification (F-measure)
</tableCaption>
<table confidence="0.994930666666667">
(a) TIME TOP (b) LOCATION (c) ORGANIZATION (d) NUMEX
n 1 2 3 4 oo 1 2 3 4 oo 1 2 3 4 oo 1 2 3 4 oo
FSSK1 - .961 .958 .957 .956 - .795 .793 .798 .792 - .709 .720 .720 .723 - .912 .915 .908 .908
FSSK2 - .961 .956 .957 .956 - .788 .799 .804 .800 - .703 .710 .716 .720 - .913 .916 .911 .913
SK - .946 .910 .866 .223 - .791 .775 .732 .169 - .705 .668 .594 .035 - .912 .885 .817 .036
BOW-K .902 .909 .886 .855 - .744 .768 .756 .747 - .641 690 .636 .572 - .842 .852 .807 .726 -
</table>
<subsectionHeader confidence="0.989411">
6.2 Sentence Modality Identification
</subsectionHeader>
<bodyText confidence="0.999962181818182">
For example, sentence modality identification tech-
niques are used in automatic text analysis systems
that identify the modality of a sentence, such as
“opinion” or “description”.
The data set was created from Mainichi news arti-
cles and one of three modality tags, “opinion”, “de-
cision” and “description” was applied to each sen-
tence. The data size was 1135 sentences consist-
ing of 123 sentences of “opinion”, 326 of “decision”
and 686 of “description”. We evaluated the results
by using 5-fold cross validation.
</bodyText>
<sectionHeader confidence="0.999555" genericHeader="evaluation">
7 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999976217391304">
Tables 3 and 4 show the results of Japanese and En-
glish question classification, respectively. Table 5
shows the results of sentence modality identifica-
tion. n in each table indicates the threshold of the
sub-sequence size. n = oc means all possible sub-
sequences are used.
First, SK was consistently superior to BOW-K.
This indicates that the structural features were quite
efficient in performing these tasks. In general we
can say that the use of structural features can im-
prove the performance of NLP tasks that require the
details of the contents to perform the task.
Most of the results showed that SK achieves its
maximum performance when n = 2. The per-
formance deteriorates considerably once n exceeds
4. This implies that SK with larger sub-structures
degrade classification performance. These results
show the same tendency as the previous studies dis-
cussed in Section 3. Table 6 shows the precision and
recall of SK when n = oc. As shown in Table 6, the
classifier offered high precision but low recall. This
is evidence of over-fitting in learning.
As shown by the above experiments, FSSK pro-
</bodyText>
<tableCaption confidence="0.995478">
Table 6: Precision and recall of SK: n = oc
</tableCaption>
<table confidence="0.999309333333333">
Precision Recall F
MI:Opinion .917 .209 .339
JQA:LOCATION .896 .093 .168
</table>
<bodyText confidence="0.999903047619048">
vided consistently better performance than the con-
ventional methods. Moreover, the experiments con-
firmed one important fact. That is, in some cases
maximum performance was achieved with n =
oc. This indicates that sub-sequences created us-
ing very large structures can be extremely effective.
Of course, a larger feature space also includes the
smaller feature spaces, En C En+1. If the perfor-
mance is improved by using a larger n, this means
that significant features do exist. Thus, we can im-
prove the performance of some classification prob-
lems by dealing with larger substructures. Even if
optimum performance was not achieved with n =
oc, difference between the performance of smaller
n are quite small compared to that of SK. This indi-
cates that our method is very robust as regards sub-
structure size; It therefore becomes unnecessary for
us to decide sub-structure size carefully. This in-
dicates our approach, using large sub-structures, is
better than the conventional approach of eliminating
sub-sequences based on size.
</bodyText>
<sectionHeader confidence="0.998707" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.998402285714286">
This paper proposed a statistical feature selection
method for convolution kernels. Our approach can
select significant features automatically based on a
statistical significance test. Our proposed method
can be embedded in the DP based kernel calcula-
tion process for convolution kernels by using sub-
structure mining algorithms.
</bodyText>
<tableCaption confidence="0.99499">
Table 4: Results of English question classification (Accuracy)
</tableCaption>
<figure confidence="0.573117">
(a) coarse (b) fine
</figure>
<table confidence="0.97714">
n 1 2 3 4 oo
FSSK1 - .908 .914 .916 .912
FSSK2 - .902 .896 .902 .906
SK - .912 .914 .912 .892
BOW-K .728 .836 .864 .858 -
1 2 3 4 oo
- .852 .854 .852 .850
- .858 .856 .854 .854
- .850 .840 .830 .796
.754 .792 .790 .778 -
</table>
<tableCaption confidence="0.99813">
Table 5: Results of sentence modality identification (F-measure)
</tableCaption>
<table confidence="0.764475833333333">
(a) opinion (b) decision (c) description
n 1 2 3 4 oo 1 2 3 4 oo 1 2 3 4 oo
FSSK1 - .734 .743 .746 .751 - .828 .858 .854 .857 - .896 .906 .910 .910
FSSK2 - .740 .748 .750 .750 - .824 .855 .859 .860 - .894 .903 .909 .909
SK - .706 .672 .577 .058 - .816 .834 .830 .339 - .902 .913 .910 .808
BOW-K .507 .531 .438 .368 - .652 .708 .686 .665 - .819 .839 .826 .793 -
</table>
<bodyText confidence="0.999085">
Experiments show that our method is superior to
conventional methods. Moreover, the results indi-
cate that complex features exist and can be effective.
Our method can employ them without over-fitting
problems, which yields benefits in terms of concept
and performance.
</bodyText>
<sectionHeader confidence="0.999113" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999315631578947">
N. Cancedda, E. Gaussier, C. Goutte, and J.-M.
Renders. 2003. Word-Sequence Kernels. Jour-
nal ofMachine Learning Research, 3:1059–1082.
M. Collins and N. Duffy. 2001. Convolution Ker-
nels for Natural Language. In Proc. ofNeural In-
formation Processing Systems (NIPS’2001).
C. Cortes and V. N. Vapnik. 1995. Support Vector
Networks. Machine Learning, 20:273–297.
D. Haussler. 1999. Convolution Kernels on Dis-
crete Structures. In Technical Report UCS-CRL-
99-10. UC Santa Cruz.
T. Joachims. 1998. Text Categorization with Sup-
port Vector Machines: Learning with Many Rel-
evant Features. In Proc. ofEuropean Conference
on Machine Learning (ECML ’98), pages 137–
142.
T. Kudo and Y. Matsumoto. 2002. Japanese Depen-
dency Analysis Using Cascaded Chunking. In
Proc. of the 6th Conference on Natural Language
Learning (CoNLL 2002), pages 63–69.
T. Kudo and Y. Matsumoto. 2003. Fast Methods for
Kernel-based Text Analysis. In Proc. of the 41st
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-2003), pages 24–31.
X. Li and D. Roth. 2002. Learning Question Clas-
sifiers. In Proc. of the 19th International Con-
ference on Computational Linguistics (COLING
2002), pages 556–562.
H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cris-
tianini, and C. Watkins. 2002. Text Classification
Using String Kernel. Journal of Machine Learn-
ing Research, 2:419–444.
S. Morishita and J. Sese. 2000. Traversing Item-
set Lattices with Statistical Metric Pruning. In
Proc. ofACM SIGACT-SIGMOD-SIGART Symp.
on Database Systems (PODS’00), pages 226–
236.
J. Pei, J. Han, B. Mortazavi-Asl, and H. Pinto.
2001. PrefixSpan: Mining Sequential Patterns
Efficiently by Prefix-Projected Pattern Growth.
In Proc. of the 17th International Conference on
Data Engineering (ICDE 2001), pages 215–224.
M. Rogati and Y. Yang. 2002. High-performing
Feature Selection for Text Classification. In
Proc. of the 2002 ACM CIKMInternational Con-
ference on Information and Knowledge Manage-
ment, pages 659–661.
J. Suzuki, T. Hirao, Y. Sasaki, and E. Maeda.
2003a. Hierarchical Directed Acyclic Graph Ker-
nel: Methods for Natural Language Data. In
Proc. of the 41st Annual Meeting of the Associ-
ation for Computational Linguistics (ACL-2003),
pages 32–39.
J. Suzuki, Y. Sasaki, and E. Maeda. 2003b. Kernels
for Structured Natural Language Data. In Proc.
of the 17th Annual Conference on Neural Infor-
mation Processing Systems (NIPS2003).
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.959033">
<title confidence="0.999404">Convolution Kernels with Feature Selection for Natural Language Processing Tasks</title>
<author confidence="0.998838">Jun Suzuki</author>
<author confidence="0.998838">Hideki Isozaki</author>
<author confidence="0.998838">Eisaku Maeda</author>
<affiliation confidence="0.99888">NTT Communication Science Laboratories, NTT Corp.</affiliation>
<address confidence="0.990248">2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto,619-0237 Japan</address>
<email confidence="0.987918">isozaki,</email>
<abstract confidence="0.998876">Convolution kernels, such as sequence and tree kernels, are advantageous for both the concept and acof many language processing tasks. Experiments have, however, shown that the over-fitting problem often arises when these kernels are used in NLP tasks. This paper discusses this issue of convolution kernels, and then proposes a new approach based on statistical feature selection that avoids this issue. To enable the proposed method to be executed efficiently, it is embedded into an original kernel calculation process by using mining Experiments are undertaken on real NLP tasks to confirm the problem with a conventional method and to compare its performance with that of the proposed method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Cancedda</author>
<author>E Gaussier</author>
<author>C Goutte</author>
<author>J-M Renders</author>
</authors>
<title>Word-Sequence Kernels.</title>
<date>2003</date>
<journal>Journal ofMachine Learning Research,</journal>
<pages>3--1059</pages>
<contexts>
<context position="2536" citStr="Cancedda et al., 2003" startWordPosition="380" endWordPosition="383">ls (Suzuki et al., 2003a), have been shown to offer excellent results. These discrete kernels are related to convolution kernels (Haussler, 1999), which provides the concept of kernels over discrete structures. Convolution kernels allow us to treat structural features without explicitly representing the feature vectors from the input object. That is, convolution kernels are well suited to NLP tasks in terms of both accuracy and concept. Unfortunately, experiments have shown that in some cases there is a critical issue with convolution kernels, especially in NLP tasks (Collins and Duffy, 2001; Cancedda et al., 2003; Suzuki et al., 2003b). That is, the over-fitting problem arises if large “substructures” are used in the kernel calculations. As a result, the machine learning approach can never be trained efficiently. To solve this issue, we generally eliminate large sub-structures from the set of features used. However, the main reason for using convolution kernels is that we aim to use structural features easily and efficiently. If use is limited to only very small structures, it negates the advantages of using convolution kernels. This paper discusses this issue of convolution kernels, and proposes a ne</context>
<context position="5871" citStr="Cancedda et al., 2003" startWordPosition="940" endWordPosition="943">me by using dynamic programming (DP). Finally, since the size of the input objects is not constant, the kernel value is normalized using the following equation. K(X,Y ) ˆK(X, Y ) = �/K(X , X) - K(Y, Y ) (2) The value of ˆK(X, Y ) is from 0 to 1, ˆK(X, Y ) = 1 if and only if X = Y . 2.1 Sequence Kernels To simplify the discussion, we restrict ourselves hereafter to sequence kernels. Other convolution kernels are briefly addressed in Section 5. Many kinds of sequence kernels have been proposed for a variety of different tasks. This paper basically follows the framework of word sequence kernels (Cancedda et al., 2003), and so processes gapped word sequences to yield the kernel value. Let  be a set of finite symbols, and n be a set of possible (symbol) sequences whose sizes are n or less that are constructed by symbols in . The meaning of “size” in this paper is the number of symbols in the sub-structure. Namely, in the case of sequence, size n means length n. S and T can represent any sequence. si and tj represent the ith and jth symbols in S and T, respectively. Therefore, a sequences sub-sequences S=abc (a, b, c, ab, ac, bc, abc) T = abac (a, b, c, aa, ab, ac, ba, bc, aba, aac, abc, bac, abac) Figure </context>
<context position="7485" citStr="Cancedda et al., 2003" startWordPosition="1263" endWordPosition="1267">d l(i) = 4 − 2 + 1 = 3. By using the above notations, sequence kernels can be defined as: KS-(S,T) = E E (i) E (j), (3) uEEn i|u=S[i] j|u=T[j] where A is the decay factor that handles the gap present in a common sub-sequence u, and -y(i) = l(i)−|u|. In this paper, |means “such that”. Figure 1 shows a simple example of the output of this kernel. However, in general, the number of features |n|, which is the dimension of the feature space, becomes very high, and it is computationally infeasible to calculate Equation (3) explicitly. The efficient recursive calculation has been introduced in (Cancedda et al., 2003). To clarify the discussion, we redefine the sequence kernels with our notation. The sequence kernel can be written as follows: E Jm(Si,Tj). (4) 1&lt;j&lt;|T| where Si and Tj represent the sub-sequences Si = s1, s2, ... , si and Tj = t1, t2, ... , tj, respectively. Let Jm(Si,Tj) be a function that returns the value of common sub-sequences if si = tj. Jm(Si,Tj) = Jim−1(Si,Tj) - I(si,tj) (5) I(si, tj) is a function that returns a matching value between si and tj. This paper defines I(si, tj) as an indicator function that returns 1 if si = tj, otherwise 0. kernel value 5+3A+A 3 1 1 u a, b, c, aa, ab, a</context>
<context position="9642" citStr="Cancedda et al., 2003" startWordPosition="1692" endWordPosition="1695">or the kernels. The number of substructures in the input object usually becomes exponential against input object size. As a result, all kernel values ˆK(X, Y ) are nearly 0 except the kernel value of the object itself, ˆK(X, X), which is 1. In this situation, the machine learning process becomes almost the same as memory-based learning. This means that we obtain a result that is very precise but with very low recall. To avoid this, most conventional methods use an approach that involves smoothing the kernel values or eliminating features based on the sub-structure size. For sequence kernels, (Cancedda et al., 2003) use a feature elimination method based on the size of sub-sequence n. This means that the kernel calculation deals only with those sub-sequences whose size is n or less. For tree kernels, (Collins and Duffy, 2001) proposed a method that restricts the features based on sub-trees depth. These methods seem to work well on the surface, however, good results are achieved only when n is very small, i.e. n = 2. The main reason for using convolution kernels is that they allow us to employ structural features simply and efficiently. When only small sized substructures are used (i.e. n = 2), the full b</context>
</contexts>
<marker>Cancedda, Gaussier, Goutte, Renders, 2003</marker>
<rawString>N. Cancedda, E. Gaussier, C. Goutte, and J.-M. Renders. 2003. Word-Sequence Kernels. Journal ofMachine Learning Research, 3:1059–1082.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>N Duffy</author>
</authors>
<title>Convolution Kernels for Natural Language. In</title>
<date>2001</date>
<booktitle>Proc. ofNeural Information Processing Systems (NIPS’2001).</booktitle>
<contexts>
<context position="1443" citStr="Collins and Duffy, 2001" startWordPosition="208" endWordPosition="211">n process by using sub-structure mining algorithms. Experiments are undertaken on real NLP tasks to confirm the problem with a conventional method and to compare its performance with that of the proposed method. 1 Introduction Over the past few years, many machine learning methods have been successfully applied to tasks in natural language processing (NLP). Especially, state-of-the-art performance can be achieved with kernel methods, such as Support Vector Machine (Cortes and Vapnik, 1995). Examples include text categorization (Joachims, 1998), chunking (Kudo and Matsumoto, 2002) and parsing (Collins and Duffy, 2001). Another feature of this kernel methodology is that it not only provides high accuracy but also allows us to design a kernel function suited to modeling the task at hand. Since natural language data take the form of sequences of words, and are generally analyzed using discrete structures, such as trees (parsed trees) and graphs (relational graphs), discrete kernels, such as sequence kernels (Lodhi et al., 2002), tree kernels (Collins and Duffy, 2001), and graph kernels (Suzuki et al., 2003a), have been shown to offer excellent results. These discrete kernels are related to convolution kernels</context>
<context position="5126" citStr="Collins and Duffy, 2001" startWordPosition="802" endWordPosition="805">cts as the convolution of “sub-kernels”, i.e. the kernels for the decompositions (parts) of the objects. Let X and Y be discrete objects. Conceptually, convolution kernels K(X, Y ) enumerate all substructures occurring in X and Y and then calculate their inner product, which is simply written as: K(X, Y ) = �(X), (Y )� = E i(X) - i(Y ). (1) i O represents the feature mapping from the discrete object to the feature space; that is, O(X) = (O1(X), ... , Oi(X),...). With sequence kernels (Lodhi et al., 2002), input objects X and Y are sequences, and Oi(X) is a sub-sequence. With tree kernels (Collins and Duffy, 2001), X and Y are trees, and Oi(X) is a sub-tree. When implemented, these kernels can be efficiently calculated in quadratic time by using dynamic programming (DP). Finally, since the size of the input objects is not constant, the kernel value is normalized using the following equation. K(X,Y ) ˆK(X, Y ) = �/K(X , X) - K(Y, Y ) (2) The value of ˆK(X, Y ) is from 0 to 1, ˆK(X, Y ) = 1 if and only if X = Y . 2.1 Sequence Kernels To simplify the discussion, we restrict ourselves hereafter to sequence kernels. Other convolution kernels are briefly addressed in Section 5. Many kinds of sequence kernels</context>
<context position="9856" citStr="Collins and Duffy, 2001" startWordPosition="1729" endWordPosition="1732">tself, ˆK(X, X), which is 1. In this situation, the machine learning process becomes almost the same as memory-based learning. This means that we obtain a result that is very precise but with very low recall. To avoid this, most conventional methods use an approach that involves smoothing the kernel values or eliminating features based on the sub-structure size. For sequence kernels, (Cancedda et al., 2003) use a feature elimination method based on the size of sub-sequence n. This means that the kernel calculation deals only with those sub-sequences whose size is n or less. For tree kernels, (Collins and Duffy, 2001) proposed a method that restricts the features based on sub-trees depth. These methods seem to work well on the surface, however, good results are achieved only when n is very small, i.e. n = 2. The main reason for using convolution kernels is that they allow us to employ structural features simply and efficiently. When only small sized substructures are used (i.e. n = 2), the full benefits of convolution kernels are missed. Moreover, these results do not mean that larger sized sub-structures are not useful. In some cases we already know that larger sub-structures are significant features as r</context>
<context position="22112" citStr="Collins and Duffy, 2001" startWordPosition="3980" endWordPosition="3983">s have a TRIE data structure. This means a fast calculation technique proposed in (Kudo and Matsumoto, 2003) can be simply applied to our method, which yields classification very quickly. In the classification part, the features (subsequences) selected in the learning part must be known. Therefore, we store the TRIE of selected sub-sequences and use them during classification. 5 Proposed Method Applied to Other Convolution Kernels We have insufficient space to discuss this subject in detail in relation to other convolution kernels. However, our proposals can be easily applied to tree kernels (Collins and Duffy, 2001) by using string encoding for trees. We enumerate nodes (labels) of tree in postorder traversal. After that, we can employ a sequential pattern mining technique to select statistically significant sub-trees. This is because we can convert to the original sub-tree form from the string encoding representation. Table 2: Parameter values of proposed kernels and Support Vector Machines parameter value soft margin for SVM (C) 1000 decay factor of gap (A) 0.5 2.7055 threshold of x2 (T) 3.8415 As a result, we can calculate tree kernels with statistical feature selection by using the original tree kern</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>M. Collins and N. Duffy. 2001. Convolution Kernels for Natural Language. In Proc. ofNeural Information Processing Systems (NIPS’2001).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cortes</author>
<author>V N Vapnik</author>
</authors>
<title>Support Vector Networks.</title>
<date>1995</date>
<booktitle>Machine Learning,</booktitle>
<pages>20--273</pages>
<contexts>
<context position="1313" citStr="Cortes and Vapnik, 1995" startWordPosition="189" endWordPosition="192">hat avoids this issue. To enable the proposed method to be executed efficiently, it is embedded into an original kernel calculation process by using sub-structure mining algorithms. Experiments are undertaken on real NLP tasks to confirm the problem with a conventional method and to compare its performance with that of the proposed method. 1 Introduction Over the past few years, many machine learning methods have been successfully applied to tasks in natural language processing (NLP). Especially, state-of-the-art performance can be achieved with kernel methods, such as Support Vector Machine (Cortes and Vapnik, 1995). Examples include text categorization (Joachims, 1998), chunking (Kudo and Matsumoto, 2002) and parsing (Collins and Duffy, 2001). Another feature of this kernel methodology is that it not only provides high accuracy but also allows us to design a kernel function suited to modeling the task at hand. Since natural language data take the form of sequences of words, and are generally analyzed using discrete structures, such as trees (parsed trees) and graphs (relational graphs), discrete kernels, such as sequence kernels (Lodhi et al., 2002), tree kernels (Collins and Duffy, 2001), and graph ker</context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>C. Cortes and V. N. Vapnik. 1995. Support Vector Networks. Machine Learning, 20:273–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Haussler</author>
</authors>
<title>Convolution Kernels on Discrete Structures. In</title>
<date>1999</date>
<tech>Technical Report UCS-CRL99-10.</tech>
<institution>UC Santa Cruz.</institution>
<contexts>
<context position="2060" citStr="Haussler, 1999" startWordPosition="309" endWordPosition="310">Another feature of this kernel methodology is that it not only provides high accuracy but also allows us to design a kernel function suited to modeling the task at hand. Since natural language data take the form of sequences of words, and are generally analyzed using discrete structures, such as trees (parsed trees) and graphs (relational graphs), discrete kernels, such as sequence kernels (Lodhi et al., 2002), tree kernels (Collins and Duffy, 2001), and graph kernels (Suzuki et al., 2003a), have been shown to offer excellent results. These discrete kernels are related to convolution kernels (Haussler, 1999), which provides the concept of kernels over discrete structures. Convolution kernels allow us to treat structural features without explicitly representing the feature vectors from the input object. That is, convolution kernels are well suited to NLP tasks in terms of both accuracy and concept. Unfortunately, experiments have shown that in some cases there is a critical issue with convolution kernels, especially in NLP tasks (Collins and Duffy, 2001; Cancedda et al., 2003; Suzuki et al., 2003b). That is, the over-fitting problem arises if large “substructures” are used in the kernel calculatio</context>
</contexts>
<marker>Haussler, 1999</marker>
<rawString>D. Haussler. 1999. Convolution Kernels on Discrete Structures. In Technical Report UCS-CRL99-10. UC Santa Cruz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Text Categorization with Support Vector Machines: Learning with Many Relevant Features.</title>
<date>1998</date>
<booktitle>In Proc. ofEuropean Conference on Machine Learning (ECML ’98),</booktitle>
<pages>137--142</pages>
<contexts>
<context position="1368" citStr="Joachims, 1998" startWordPosition="198" endWordPosition="199">ted efficiently, it is embedded into an original kernel calculation process by using sub-structure mining algorithms. Experiments are undertaken on real NLP tasks to confirm the problem with a conventional method and to compare its performance with that of the proposed method. 1 Introduction Over the past few years, many machine learning methods have been successfully applied to tasks in natural language processing (NLP). Especially, state-of-the-art performance can be achieved with kernel methods, such as Support Vector Machine (Cortes and Vapnik, 1995). Examples include text categorization (Joachims, 1998), chunking (Kudo and Matsumoto, 2002) and parsing (Collins and Duffy, 2001). Another feature of this kernel methodology is that it not only provides high accuracy but also allows us to design a kernel function suited to modeling the task at hand. Since natural language data take the form of sequences of words, and are generally analyzed using discrete structures, such as trees (parsed trees) and graphs (relational graphs), discrete kernels, such as sequence kernels (Lodhi et al., 2002), tree kernels (Collins and Duffy, 2001), and graph kernels (Suzuki et al., 2003a), have been shown to offer e</context>
<context position="23342" citStr="Joachims, 1998" startWordPosition="4172" endWordPosition="4173">with the sequential pattern mining technique introduced in this paper. Moreover, we can expand our proposals to hierarchically structured graph kernels (Suzuki et al., 2003a) by using a simple extension to cover hierarchical structures. 6 Experiments We evaluated the performance of the proposed method in actual NLP tasks, namely English question classification (EQC), Japanese question classification (JQC) and sentence modality identification (MI) tasks. We compared the proposed method (FSSK) with a conventional method (SK), as discussed in Section 3, and with bag-of-words (BOW) Kernel (BOW-K)(Joachims, 1998) as baseline methods. Support Vector Machine (SVM) was selected as the kernel-based classifier for training and classification. Table 2 shows some of the parameter values that we used in the comparison. We set thresholds of T = 2.7055 (FSSK1) and T = 3.8415 (FSSK2) for the proposed methods; these values represent the 10% and 5% level of significance in the x2 distribution with one degree of freedom, which used the x2 significant test. 6.1 Question Classification Question classification is defined as a task similar to text categorization; it maps a given question into a question type. We evalua</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>T. Joachims. 1998. Text Categorization with Support Vector Machines: Learning with Many Relevant Features. In Proc. ofEuropean Conference on Machine Learning (ECML ’98), pages 137– 142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>Japanese Dependency Analysis Using Cascaded Chunking.</title>
<date>2002</date>
<booktitle>In Proc. of the 6th Conference on Natural Language Learning (CoNLL</booktitle>
<pages>63--69</pages>
<contexts>
<context position="1405" citStr="Kudo and Matsumoto, 2002" startWordPosition="201" endWordPosition="204">dded into an original kernel calculation process by using sub-structure mining algorithms. Experiments are undertaken on real NLP tasks to confirm the problem with a conventional method and to compare its performance with that of the proposed method. 1 Introduction Over the past few years, many machine learning methods have been successfully applied to tasks in natural language processing (NLP). Especially, state-of-the-art performance can be achieved with kernel methods, such as Support Vector Machine (Cortes and Vapnik, 1995). Examples include text categorization (Joachims, 1998), chunking (Kudo and Matsumoto, 2002) and parsing (Collins and Duffy, 2001). Another feature of this kernel methodology is that it not only provides high accuracy but also allows us to design a kernel function suited to modeling the task at hand. Since natural language data take the form of sequences of words, and are generally analyzed using discrete structures, such as trees (parsed trees) and graphs (relational graphs), discrete kernels, such as sequence kernels (Lodhi et al., 2002), tree kernels (Collins and Duffy, 2001), and graph kernels (Suzuki et al., 2003a), have been shown to offer excellent results. These discrete kern</context>
</contexts>
<marker>Kudo, Matsumoto, 2002</marker>
<rawString>T. Kudo and Y. Matsumoto. 2002. Japanese Dependency Analysis Using Cascaded Chunking. In Proc. of the 6th Conference on Natural Language Learning (CoNLL 2002), pages 63–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>Fast Methods for Kernel-based Text Analysis.</title>
<date>2003</date>
<booktitle>In Proc. of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-2003),</booktitle>
<pages>24--31</pages>
<contexts>
<context position="21596" citStr="Kudo and Matsumoto, 2003" startWordPosition="3898" endWordPosition="3901"> can be embedded in an original kernel calculation process, which allows us to use the same calculation procedure as the conventional methods. The only difference between the original sequence kernels and the proposed method is that the latter calculates a statistical metric x2(u) by using a sub-structure mining algorithm in the kernel calculation. Third, although the kernel calculation, which unifies our proposed method, requires a longer training time because of the feature selection, the selected sub-sequences have a TRIE data structure. This means a fast calculation technique proposed in (Kudo and Matsumoto, 2003) can be simply applied to our method, which yields classification very quickly. In the classification part, the features (subsequences) selected in the learning part must be known. Therefore, we store the TRIE of selected sub-sequences and use them during classification. 5 Proposed Method Applied to Other Convolution Kernels We have insufficient space to discuss this subject in detail in relation to other convolution kernels. However, our proposals can be easily applied to tree kernels (Collins and Duffy, 2001) by using string encoding for trees. We enumerate nodes (labels) of tree in postorde</context>
</contexts>
<marker>Kudo, Matsumoto, 2003</marker>
<rawString>T. Kudo and Y. Matsumoto. 2003. Fast Methods for Kernel-based Text Analysis. In Proc. of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-2003), pages 24–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Li</author>
<author>D Roth</author>
</authors>
<title>Learning Question Classifiers.</title>
<date>2002</date>
<booktitle>In Proc. of the 19th International Conference on Computational Linguistics (COLING</booktitle>
<pages>556--562</pages>
<contexts>
<context position="24007" citStr="Li and Roth, 2002" startWordPosition="4281" endWordPosition="4284">VM) was selected as the kernel-based classifier for training and classification. Table 2 shows some of the parameter values that we used in the comparison. We set thresholds of T = 2.7055 (FSSK1) and T = 3.8415 (FSSK2) for the proposed methods; these values represent the 10% and 5% level of significance in the x2 distribution with one degree of freedom, which used the x2 significant test. 6.1 Question Classification Question classification is defined as a task similar to text categorization; it maps a given question into a question type. We evaluated the performance by using data provided by (Li and Roth, 2002) for English and (Suzuki et al., 2003b) for Japanese question classification and followed the experimental setting used in these papers; namely we use four typical question types, LOCATION, NUMEX, ORGANIZATION, and TIME TOP for JQA, and “coarse” and “fine” classes for EQC. We used the one-vs-rest classifier of SVM as the multi-class classification method for EQC. Figure 4 shows examples of the question classification data used here. question types input object: word sequences ([ ]: information of chunk and ( ): named entity) ABBREVIATION DESCRIPTION what,[B-NP] be,[B-VP] the,[B-NP] abbreviatio</context>
</contexts>
<marker>Li, Roth, 2002</marker>
<rawString>X. Li and D. Roth. 2002. Learning Question Classifiers. In Proc. of the 19th International Conference on Computational Linguistics (COLING 2002), pages 556–562.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Lodhi</author>
<author>C Saunders</author>
<author>J Shawe-Taylor</author>
<author>N Cristianini</author>
<author>C Watkins</author>
</authors>
<title>Text Classification Using String Kernel.</title>
<date>2002</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>2--419</pages>
<contexts>
<context position="1858" citStr="Lodhi et al., 2002" startWordPosition="277" endWordPosition="280">h kernel methods, such as Support Vector Machine (Cortes and Vapnik, 1995). Examples include text categorization (Joachims, 1998), chunking (Kudo and Matsumoto, 2002) and parsing (Collins and Duffy, 2001). Another feature of this kernel methodology is that it not only provides high accuracy but also allows us to design a kernel function suited to modeling the task at hand. Since natural language data take the form of sequences of words, and are generally analyzed using discrete structures, such as trees (parsed trees) and graphs (relational graphs), discrete kernels, such as sequence kernels (Lodhi et al., 2002), tree kernels (Collins and Duffy, 2001), and graph kernels (Suzuki et al., 2003a), have been shown to offer excellent results. These discrete kernels are related to convolution kernels (Haussler, 1999), which provides the concept of kernels over discrete structures. Convolution kernels allow us to treat structural features without explicitly representing the feature vectors from the input object. That is, convolution kernels are well suited to NLP tasks in terms of both accuracy and concept. Unfortunately, experiments have shown that in some cases there is a critical issue with convolution ke</context>
<context position="5015" citStr="Lodhi et al., 2002" startWordPosition="783" endWordPosition="786">ctures, such as sequences, trees and graphs. This framework defines the kernel function between input objects as the convolution of “sub-kernels”, i.e. the kernels for the decompositions (parts) of the objects. Let X and Y be discrete objects. Conceptually, convolution kernels K(X, Y ) enumerate all substructures occurring in X and Y and then calculate their inner product, which is simply written as: K(X, Y ) = �(X), (Y )� = E i(X) - i(Y ). (1) i O represents the feature mapping from the discrete object to the feature space; that is, O(X) = (O1(X), ... , Oi(X),...). With sequence kernels (Lodhi et al., 2002), input objects X and Y are sequences, and Oi(X) is a sub-sequence. With tree kernels (Collins and Duffy, 2001), X and Y are trees, and Oi(X) is a sub-tree. When implemented, these kernels can be efficiently calculated in quadratic time by using dynamic programming (DP). Finally, since the size of the input objects is not constant, the kernel value is normalized using the following equation. K(X,Y ) ˆK(X, Y ) = �/K(X , X) - K(Y, Y ) (2) The value of ˆK(X, Y ) is from 0 to 1, ˆK(X, Y ) = 1 if and only if X = Y . 2.1 Sequence Kernels To simplify the discussion, we restrict ourselves hereafter to</context>
</contexts>
<marker>Lodhi, Saunders, Shawe-Taylor, Cristianini, Watkins, 2002</marker>
<rawString>H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini, and C. Watkins. 2002. Text Classification Using String Kernel. Journal of Machine Learning Research, 2:419–444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Morishita</author>
<author>J Sese</author>
</authors>
<title>Traversing Itemset Lattices with Statistical Metric Pruning.</title>
<date>2000</date>
<booktitle>In Proc. ofACM SIGACT-SIGMOD-SIGART Symp. on Database Systems (PODS’00),</booktitle>
<pages>226--236</pages>
<contexts>
<context position="14471" citStr="Morishita and Sese, 2000" startWordPosition="2504" endWordPosition="2507">s simply the condition of the first summation. FSSK selects significant sub-sequence u by using the condition of the statistical metric  &lt; 2(u). Figure 2 shows a simple example of what FSSK calculates for the kernel value. 4.3 Efficient 2(u) Calculation Method It is computationally infeasible to calculate 2(u) for all possible u with a naive exhaustive method. In our approach, we use a sub-structure mining algorithm to calculate 2(u). The basic idea comes from a sequential pattern mining technique, PrefixSpan (Pei et al., 2001), and a statistical metric pruning (SMP) method, Apriori SMP (Morishita and Sese, 2000). By using these techniques, all the significant sub-sequences u that satisfy  &lt; 2(u) can be found efficiently by depth-first search and pruning. Below, we briefly explain the concept involved in finding the significant features. First, we denote uv, which is the concatenation of sequences u and v. Then, u is a specific sequence and uv is any sequence that is constructed by u with any suffix v. The upper bound of the 2 value of uv can be defined by the value of u (Morishita and Sese, 2000). 2(uv)&lt;max (2(yu, yu), 2(xu _ yu, 0)) =x2(u) where xu and yu represent the value of x and y of u. T</context>
</contexts>
<marker>Morishita, Sese, 2000</marker>
<rawString>S. Morishita and J. Sese. 2000. Traversing Itemset Lattices with Statistical Metric Pruning. In Proc. ofACM SIGACT-SIGMOD-SIGART Symp. on Database Systems (PODS’00), pages 226– 236.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pei</author>
<author>J Han</author>
<author>B Mortazavi-Asl</author>
<author>H Pinto</author>
</authors>
<title>PrefixSpan: Mining Sequential Patterns Efficiently by Prefix-Projected Pattern Growth.</title>
<date>2001</date>
<booktitle>In Proc. of the 17th International Conference on Data Engineering (ICDE</booktitle>
<pages>215--224</pages>
<contexts>
<context position="14384" citStr="Pei et al., 2001" startWordPosition="2490" endWordPosition="2493">) ~2(u)ju2En iju=S[i] jju=T[j] The difference between Equations (3) and (9) is simply the condition of the first summation. FSSK selects significant sub-sequence u by using the condition of the statistical metric  &lt; 2(u). Figure 2 shows a simple example of what FSSK calculates for the kernel value. 4.3 Efficient 2(u) Calculation Method It is computationally infeasible to calculate 2(u) for all possible u with a naive exhaustive method. In our approach, we use a sub-structure mining algorithm to calculate 2(u). The basic idea comes from a sequential pattern mining technique, PrefixSpan (Pei et al., 2001), and a statistical metric pruning (SMP) method, Apriori SMP (Morishita and Sese, 2000). By using these techniques, all the significant sub-sequences u that satisfy  &lt; 2(u) can be found efficiently by depth-first search and pruning. Below, we briefly explain the concept involved in finding the significant features. First, we denote uv, which is the concatenation of sequences u and v. Then, u is a specific sequence and uv is any sequence that is constructed by u with any suffix v. The upper bound of the 2 value of uv can be defined by the value of u (Morishita and Sese, 2000). 2(uv)&lt;max (2</context>
</contexts>
<marker>Pei, Han, Mortazavi-Asl, Pinto, 2001</marker>
<rawString>J. Pei, J. Han, B. Mortazavi-Asl, and H. Pinto. 2001. PrefixSpan: Mining Sequential Patterns Efficiently by Prefix-Projected Pattern Growth. In Proc. of the 17th International Conference on Data Engineering (ICDE 2001), pages 215–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rogati</author>
<author>Y Yang</author>
</authors>
<title>High-performing Feature Selection for Text Classification.</title>
<date>2002</date>
<booktitle>In Proc. of the 2002 ACM CIKMInternational Conference on Information and Knowledge Management,</booktitle>
<pages>659--661</pages>
<contexts>
<context position="12045" citStr="Rogati and Yang, 2002" startWordPosition="2069" endWordPosition="2072"> features, is quite natural. We note, however, that kernels are calculated using the DP algorithm. Therefore, it is not clear how to calculate kernels efficiently with a statistical feature selection method. First, we briefly explain a statistical metric, the chisquared (x2) value, and provide an idea of how to select significant features. We then describe a method for embedding statistical feature selection into kernel calculation. 4.1 Statistical Metric: Chi-squared Value There are many kinds of statistical metrics, such as chi-squared value, correlation coefficient and mutual information. (Rogati and Yang, 2002) reported that chi-squared feature selection is the most effective method for text classification. Following this information, we use x2 values as statistical feature selection criteria. Although we selected x2 values, any other statistical metric can be used as long as it is based on the contingency table shown in Table 1. We briefly explain how to calculate the x2 value by referring to Table 1. In the table, c and c¯ represent the names of classes, c for the positive class Jm(Si,Tj) = I Figure 2: Example of statistical feature selection and c¯ for the negative class. Ouc, Ou¯c, O¯uc and O¯</context>
</contexts>
<marker>Rogati, Yang, 2002</marker>
<rawString>M. Rogati and Y. Yang. 2002. High-performing Feature Selection for Text Classification. In Proc. of the 2002 ACM CIKMInternational Conference on Information and Knowledge Management, pages 659–661.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Suzuki</author>
<author>T Hirao</author>
<author>Y Sasaki</author>
<author>E Maeda</author>
</authors>
<title>Hierarchical Directed Acyclic Graph Kernel: Methods for Natural Language Data.</title>
<date>2003</date>
<booktitle>In Proc. of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-2003),</booktitle>
<pages>32--39</pages>
<contexts>
<context position="1938" citStr="Suzuki et al., 2003" startWordPosition="290" endWordPosition="293">ples include text categorization (Joachims, 1998), chunking (Kudo and Matsumoto, 2002) and parsing (Collins and Duffy, 2001). Another feature of this kernel methodology is that it not only provides high accuracy but also allows us to design a kernel function suited to modeling the task at hand. Since natural language data take the form of sequences of words, and are generally analyzed using discrete structures, such as trees (parsed trees) and graphs (relational graphs), discrete kernels, such as sequence kernels (Lodhi et al., 2002), tree kernels (Collins and Duffy, 2001), and graph kernels (Suzuki et al., 2003a), have been shown to offer excellent results. These discrete kernels are related to convolution kernels (Haussler, 1999), which provides the concept of kernels over discrete structures. Convolution kernels allow us to treat structural features without explicitly representing the feature vectors from the input object. That is, convolution kernels are well suited to NLP tasks in terms of both accuracy and concept. Unfortunately, experiments have shown that in some cases there is a critical issue with convolution kernels, especially in NLP tasks (Collins and Duffy, 2001; Cancedda et al., 2003; </context>
<context position="22899" citStr="Suzuki et al., 2003" startWordPosition="4106" endWordPosition="4109">ct statistically significant sub-trees. This is because we can convert to the original sub-tree form from the string encoding representation. Table 2: Parameter values of proposed kernels and Support Vector Machines parameter value soft margin for SVM (C) 1000 decay factor of gap (A) 0.5 2.7055 threshold of x2 (T) 3.8415 As a result, we can calculate tree kernels with statistical feature selection by using the original tree kernel calculation with the sequential pattern mining technique introduced in this paper. Moreover, we can expand our proposals to hierarchically structured graph kernels (Suzuki et al., 2003a) by using a simple extension to cover hierarchical structures. 6 Experiments We evaluated the performance of the proposed method in actual NLP tasks, namely English question classification (EQC), Japanese question classification (JQC) and sentence modality identification (MI) tasks. We compared the proposed method (FSSK) with a conventional method (SK), as discussed in Section 3, and with bag-of-words (BOW) Kernel (BOW-K)(Joachims, 1998) as baseline methods. Support Vector Machine (SVM) was selected as the kernel-based classifier for training and classification. Table 2 shows some of the par</context>
</contexts>
<marker>Suzuki, Hirao, Sasaki, Maeda, 2003</marker>
<rawString>J. Suzuki, T. Hirao, Y. Sasaki, and E. Maeda. 2003a. Hierarchical Directed Acyclic Graph Kernel: Methods for Natural Language Data. In Proc. of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-2003), pages 32–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Suzuki</author>
<author>Y Sasaki</author>
<author>E Maeda</author>
</authors>
<title>Kernels for Structured Natural Language Data.</title>
<date>2003</date>
<booktitle>In Proc. of the 17th Annual Conference on Neural Information Processing Systems (NIPS2003).</booktitle>
<contexts>
<context position="1938" citStr="Suzuki et al., 2003" startWordPosition="290" endWordPosition="293">ples include text categorization (Joachims, 1998), chunking (Kudo and Matsumoto, 2002) and parsing (Collins and Duffy, 2001). Another feature of this kernel methodology is that it not only provides high accuracy but also allows us to design a kernel function suited to modeling the task at hand. Since natural language data take the form of sequences of words, and are generally analyzed using discrete structures, such as trees (parsed trees) and graphs (relational graphs), discrete kernels, such as sequence kernels (Lodhi et al., 2002), tree kernels (Collins and Duffy, 2001), and graph kernels (Suzuki et al., 2003a), have been shown to offer excellent results. These discrete kernels are related to convolution kernels (Haussler, 1999), which provides the concept of kernels over discrete structures. Convolution kernels allow us to treat structural features without explicitly representing the feature vectors from the input object. That is, convolution kernels are well suited to NLP tasks in terms of both accuracy and concept. Unfortunately, experiments have shown that in some cases there is a critical issue with convolution kernels, especially in NLP tasks (Collins and Duffy, 2001; Cancedda et al., 2003; </context>
<context position="22899" citStr="Suzuki et al., 2003" startWordPosition="4106" endWordPosition="4109">ct statistically significant sub-trees. This is because we can convert to the original sub-tree form from the string encoding representation. Table 2: Parameter values of proposed kernels and Support Vector Machines parameter value soft margin for SVM (C) 1000 decay factor of gap (A) 0.5 2.7055 threshold of x2 (T) 3.8415 As a result, we can calculate tree kernels with statistical feature selection by using the original tree kernel calculation with the sequential pattern mining technique introduced in this paper. Moreover, we can expand our proposals to hierarchically structured graph kernels (Suzuki et al., 2003a) by using a simple extension to cover hierarchical structures. 6 Experiments We evaluated the performance of the proposed method in actual NLP tasks, namely English question classification (EQC), Japanese question classification (JQC) and sentence modality identification (MI) tasks. We compared the proposed method (FSSK) with a conventional method (SK), as discussed in Section 3, and with bag-of-words (BOW) Kernel (BOW-K)(Joachims, 1998) as baseline methods. Support Vector Machine (SVM) was selected as the kernel-based classifier for training and classification. Table 2 shows some of the par</context>
</contexts>
<marker>Suzuki, Sasaki, Maeda, 2003</marker>
<rawString>J. Suzuki, Y. Sasaki, and E. Maeda. 2003b. Kernels for Structured Natural Language Data. In Proc. of the 17th Annual Conference on Neural Information Processing Systems (NIPS2003).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>