<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.995848">
Re-Ranking Algorithms for Name Tagging
</title>
<author confidence="0.999061">
Heng Ji Cynthia Rudin Ralph Grishman
</author>
<affiliation confidence="0.999093">
Dept. of Computer Science Center for Neural Science and Courant Dept. of Computer Science
Institute of Mathematical Sciences
</affiliation>
<address confidence="0.783813">
New York University
New York, N.Y. 10003
</address>
<email confidence="0.999739">
hengji@cs.nyu.edu rudin@nyu.edu grishman@cs.nyu.edu
</email>
<sectionHeader confidence="0.998601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999366538461539">
Integrating information from different
stages of an NLP processing pipeline can
yield significant error reduction. We dem-
onstrate how re-ranking can improve name
tagging in a Chinese information extrac-
tion system by incorporating information
from relation extraction, event extraction,
and coreference. We evaluate three state-
of-the-art re-ranking algorithms (MaxEnt-
Rank, SVMRank, and p-Norm Push Rank-
ing), and show the benefit of multi-stage
re-ranking for cross-sentence and cross-
document inference.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996835478260869">
In recent years, re-ranking techniques have been
successfully applied to enhance the performance
of NLP analysis components based on generative
models. A baseline generative model produces N-
best candidates, which are then re-ranked using a
rich set of local and global features in order to
select the best analysis. Various supervised learn-
ing algorithms have been adapted to the task of re-
ranking for NLP systems, such as MaxEnt-Rank
(Charniak and Johnson, 2005; Ji and Grishman,
2005), SVMRank (Shen and Joshi, 2003), Voted
Perceptron (Collins, 2002; Collins and Duffy,
2002; Shen and Joshi, 2004), Kernel Based Meth-
ods (Henderson and Titov, 2005), and RankBoost
(Collins, 2002; Collins and Koo, 2003; Kudo et al.,
2005).
These algorithms have been used primarily
within the context of a single NLP analysis com-
ponent, with the most intensive study devoted to
improving parsing performance. The re-ranking
models for parsing, for example, normally rely on
structures generated within the baseline parser
itself. Achieving really high performance for some
analysis components, however, requires that we
take a broader view, one that looks outside a sin-
gle component in order to bring to bear knowl-
edge from the entire NL analysis process. In this
paper we will demonstrate the potential of this
approach in enhancing the performance of Chi-
nese name tagging within an information extrac-
tion application.
Combining information from other stages in the
analysis pipeline allows us to incorporate informa-
tion from a much wider context, spanning the en-
tire document and even going across documents.
This will give rise to new design issues; we will
examine and compare different re-ranking algo-
rithms when applied to this task.
We shall first describe the general setting and
the special characteristics of re-ranking for name
tagging. Then we present and evaluate three re-
ranking algorithms – MaxEnt-Rank, SVMRank
and a new algorithm, p-Norm Push Ranking – for
this problem, and show how an approach based on
multi-stage re-ranking can effectively handle fea-
tures across sentence and document boundaries.
</bodyText>
<sectionHeader confidence="0.998796" genericHeader="introduction">
2 Prior Work
</sectionHeader>
<subsectionHeader confidence="0.864029">
2.1 Ranking
</subsectionHeader>
<bodyText confidence="0.993731333333333">
We will describe the three state-of-the-art super-
vised ranking techniques considered in this work.
Later we shall apply and evaluate these algorithms
for re-ranking in the context of name tagging.
Maximum Entropy modeling (MaxEnt) has
been extremely successful for many NLP classifi-
</bodyText>
<page confidence="0.992989">
49
</page>
<note confidence="0.4390705">
Workshop on Computationally Hard Problemsand Joint Inference in Speech and Language Processing, pages 49–56,
New York City, New York, June 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999779789473684">
cation tasks, so it is natural to apply it to re-
ranking problems. (Charniak and Johnson, 2005)
applied MaxEnt to improve the performance of a
state-of-art parser; also in (Ji and Grishman, 2005)
we used it to improve a Chinese name tagger.
Using SVMRank, (Shen and Joshi, 2003)
achieved significant improvement on parse re-
ranking. They compared two different sample
creation methods, and presented an efficient train-
ing method by separating the training samples into
subsets.
The last approach we consider is a boosting-
style approach. We implement a new algorithm
called p-Norm Push Ranking (Rudin, 2006). This
algorithm is a generalization of RankBoost
(Freund et al. 1998) which concentrates specifi-
cally on the top portion of a ranked list. The pa-
rameter “p” determines how much the algorithm
concentrates at the top.
</bodyText>
<subsectionHeader confidence="0.998256">
2.2 Enhancing Named Entity Taggers
</subsectionHeader>
<bodyText confidence="0.999965363636364">
There have been a very large number of NE tagger
implementations since this task was introduced at
MUC-6 (Grishman and Sundheim, 1996). Most
implementations use local features and a unifying
learning algorithm based on, e.g., an HMM, Max-
Ent, or SVM. Collins (2002) augmented a baseline
NE tagger with a re-ranker that used only local,
NE-oriented features. Roth and Yih (2002) com-
bined NE and semantic relation tagging, but
within a quite different framework (using a linear
programming model for joint inference).
</bodyText>
<sectionHeader confidence="0.994298" genericHeader="method">
3 A Framework for Name Re-Ranking
</sectionHeader>
<subsectionHeader confidence="0.994043">
3.1 The Information Extraction Pipeline
</subsectionHeader>
<bodyText confidence="0.999141909090909">
The extraction task we are addressing is that of the
Automatic Content Extraction (ACE)1 evaluations.
The 2005 ACE evaluation had 7 types of entities,
of which the most common were PER (persons),
ORG (organizations), LOC (natural locations) and
GPE (‘geo-political entities’ – locations which are
also political units, such as countries, counties,
and cities). There were 6 types of semantic rela-
tions, with 18 subtypes. Examples of these rela-
tions are “the CEO of Microsoft” (an
organization-affiliation relation), “Fred’s wife” (a
</bodyText>
<footnote confidence="0.9942135">
1 The ACE task description can be found at
http://www.itl.nist.gov/iad/894.01/tests/ace/
</footnote>
<bodyText confidence="0.998880888888889">
personal-social relation), and “a military base in
Germany” (a located relation). And there were 8
types of events, with 33 subtypes, such as “Kurt
Schork died in Sierra Leone yesterday” (a Die
event), and “Schweitzer founded a hospital in
1913” (a Start-Org event).
To extract these elements we have developed a
Chinese information extraction pipeline that con-
sists of the following stages:
</bodyText>
<listItem confidence="0.999700846153846">
• Name tagging and name structure parsing
(which identifies the internal structure of some
names);
• Coreference resolution, which links &amp;quot;men-
tions&amp;quot; (referring phrases of selected semantic
types) into &amp;quot;entities&amp;quot;: this stage is a combina-
tion of high-precision heuristic rules and
maximum entropy models;
• Relation tagging, using a K-nearest-neighbor
algorithm to identify relation types and sub-
types;
• Event patterns, semi-automatically extracted
from ACE training corpora.
</listItem>
<subsectionHeader confidence="0.96016">
3.2 Hypothesis Representation and Genera-
tion
</subsectionHeader>
<bodyText confidence="0.99997072">
Again, the central idea is to apply the baseline
name tagger to generate N-Best multiple hypothe-
ses for each sentence; the results from subsequent
components are then exploited to re-rank these
hypotheses and the new top hypothesis is output
as the final result.
In our name re-ranking model, each hypothesis
is an NE tagging of the entire sentence. For ex-
ample, “&lt;PER&gt;John&lt;/PER&gt; was born in
&lt;GPE&gt;New York&lt;/GPE&gt;.” is one hypothesis
for the sentence “John was born in New York”.
We apply a HMM tagger to identify four named
entity types: Person, GPE, Organization and Loca-
tion. The HMM tagger generally follows the
Nymble model (Bikel et al, 1997), and uses best-
first search to generate N-Best hypotheses. It also
computes the “margin”, which is the difference
between the log probabilities of the top two hy-
potheses. This is used as a rough measure of con-
fidence in the top hypothesis. A large margin
indicates greater confidence that the first hypothe-
sis is correct. The margin also determines the
number of hypotheses (N) that we will store. Us-
ing cross-validation on the training data, we de-
termine the value of N required to include the best
</bodyText>
<page confidence="0.992395">
50
</page>
<bodyText confidence="0.999858176470588">
hypothesis, as a function of the margin. We then
divide the margin into ranges of values, and set a
value of N for each range, with a maximum of 30.
To obtain the training data for the re-ranking
algorithm, we separate the name tagging training
corpus into k folders, and train the HMM name
tagger on k-1 folders. We then use the HMM to
generate N-Best hypotheses H = {h1, h2,...,hN} for
each sentence in the remaining folder. Each hi in
H is then paired with its NE F-measure, measured
against the key in the annotated corpus.
We define a “crucial pair” as a pair of hypothe-
ses such that, according to F-Measure, the first
hypothesis in the pair should be more highly
ranked than the second. That is, if for a sentence,
the F-Measure of hypothesis hi is larger than that
of hj, then (hi, hj) is a crucial pair.
</bodyText>
<subsectionHeader confidence="0.997677">
3.3 Re-Ranking Functions
</subsectionHeader>
<bodyText confidence="0.9987485">
We investigated the following three different for-
mulations of the re-ranking problem:
</bodyText>
<listItem confidence="0.99745">
• Direct Re-Ranking by Score
</listItem>
<bodyText confidence="0.744287333333333">
For each hypothesis hi, we attempt to learn a scor-
ing function f : H 4 R, such that f(hi) &gt; f(hj) if the
F-Measure of hi is higher than the F-measure of hj.
</bodyText>
<listItem confidence="0.998257">
• Direct Re-Ranking by Classification
</listItem>
<bodyText confidence="0.6790206">
For each hypothesis hi, we attempt to learn f : H
4 {-1, 1}, such that f(hi) = 1 if hi has the top F-
Measure among H; otherwise f(hi) = -1. This can
be considered a special case of re-ranking by
score.
</bodyText>
<listItem confidence="0.982805">
• Indirect Re-Ranking Function
</listItem>
<bodyText confidence="0.999611428571429">
For each “crucial” pair of hypotheses (hi, hj), we
learn f : H × H 4 {-1, 1}, such that f(hi, hj) = 1 if
hi is better than hj; f (hi, hj) = -1 if hi is worse than
hj. We call this “indirect” ranking because we
need to apply an additional decoding step to pick
the best hypothesis from these pair-wise compari-
son results.
</bodyText>
<sectionHeader confidence="0.99994" genericHeader="method">
4 Features for Re-Ranking
</sectionHeader>
<subsectionHeader confidence="0.99334">
4.1 Inferences From Subsequent Stages
</subsectionHeader>
<bodyText confidence="0.9998037">
Information extraction is a potentially symbiotic
pipeline with strong dependencies between stages
(Roth and Yih, 2002&amp;2004; Ji and Grishman,
2005). Thus, we use features based on the output
of four subsequent stages – name structure parsing,
relation extraction, event patterns, and coreference
analysis – to seek the best hypothesis.
We included ten features based on name struc-
ture parsing to capture the local information
missed by the baseline name tagger such as details
of the structure of Chinese person names.
The relation and event re-ranking features are
based on matching patterns of words or constitu-
ents. They serve to correct name boundary errors
(because such errors would prevent some patterns
from matching). They also exert selectional pref-
erences on their arguments, and so serve to correct
name type errors. For each relation argument, we
included a feature whose value is the likelihood
that relation appears with an argument of that se-
mantic type (these probabilities are obtained from
the training corpus and binned). For each event
pattern, a feature records whether the types of the
arguments match those required by the pattern.
Coreference can link multiple mentions of
names provided they have the same spelling
(though if a name has several parts, some may be
dropped) and same semantic type. So if the
boundary or type of one mention can be deter-
mined with some confidence, coreference can be
used to disambiguate other mentions, by favoring
hypotheses which support more coreference. To
this end, we incorporate several features based on
coreference, such as the number of mentions re-
ferring to a name candidate.
Each of these features is defined for individual
name candidates; the value of the feature for a
hypothesis is the sum of its values over all names
in the hypothesis. The complete set of detailed
features is listed in (Ji and Grishman, 2006).
</bodyText>
<subsectionHeader confidence="0.9995055">
4.2 Handling Cross-Sentence Features by
Multi-Stage Re-Ranking
</subsectionHeader>
<bodyText confidence="0.9999575">
Coreference is potentially a powerful contributor
for enhancing NE recognition, because it provides
information from other sentences and even docu-
ments, and it applies to all sentences that include
names. For a name candidate, 62% of its corefer-
ence relations span sentence boundaries. How-
ever, this breadth poses a problem because it
means that the score of a hypothesis for a given
</bodyText>
<page confidence="0.991375">
51
</page>
<bodyText confidence="0.999774206896552">
sentence may depend on the tags assigned to the
same names in other sentences.2
Ideally, when we re-rank the hypotheses for one
sentence 5, the other sentences that include men-
tions of the same name should already have been
re-ranked, but this is not possible because of the
mutual dependence. Repeated re-ranking of a sen-
tence would be time-consuming, so we have
adopted an alternative approach. Instead of incor-
porating coreference evidence with all other in-
formation in one re-ranker, we apply two re-
rankers in succession.
In the first re-ranking step, we generate new
rankings for all sentences based on name structure,
relation and event features, which are all sentence-
internal evidence. Then in a second pass, we ap-
ply a re-ranker based on coreference between the
names in each hypothesis of sentence 5 and the
mentions in the top-ranking hypothesis (from the
first re-ranker) of all other sentences.3 In this way,
the coreference re-ranker can propagate globally
(across sentences and documents) high-confidence
decisions based on the other evidence. In our final
MaxEnt Ranker we obtained a small additional
gain by further splitting the first re-ranker into
three separate steps: a name structure based re-
ranker, a relation based re-ranker and an event
based re-ranker; these were incorporated in an
incremental structure.
</bodyText>
<subsectionHeader confidence="0.999653">
4.3 Adding Cross-Document Information
</subsectionHeader>
<bodyText confidence="0.943571555555555">
The idea in coreference is to link a name mention
whose tag is locally ambiguous to another men-
tion that is unambiguously tagged based on local
evidence. The wider a net we can cast, the greater
the chance of success. To cast the widest net pos-
sible, we have used cross-document coreference
for the test set. We cluster the documents using a
cross-entropy metric and then treat the entire clus-
ter as a single document.
We take all the name candidates in the top N
hypotheses for each sentence in each cluster T to
construct a “query set” Q. The metric used for the
clustering is the cross entropy H(T, d) between the
distribution of the name candidates in T and
2 For in-document coreference, this problem could be avoided if the tagging of
an entire document constituted a hypothesis, but that would be impractical ... a
very large N would be required to capture sufficient alternative taggings in an
N-best framework.
</bodyText>
<footnote confidence="0.7649975">
3 This second pass is skipped for sentences for which the confidence in the top
hypothesis produced by the first re-ranker is above a threshold.
</footnote>
<bodyText confidence="0.9738785">
document d. If H(T, d) is smaller than a threshold
then we add d to T. H(T, d) is defined by:
</bodyText>
<equation confidence="0.9987304">
H T d
( , ) = −E prob T x
( , ) log ( , )
x prob d x .
Q
</equation>
<bodyText confidence="0.999777">
We built these clusters two ways: first, just
clustering the test documents; second, by aug-
menting these clusters with related documents
retrieved from a large unlabeled corpus (with
document relevance measured using cross-
entropy).
</bodyText>
<sectionHeader confidence="0.998338" genericHeader="method">
5 Re-Ranking Algorithms
</sectionHeader>
<bodyText confidence="0.999734">
We have been focusing on selecting appropriate
ranking algorithms to fit our application. We
choose three state-of-the-art ranking algorithms
that have good generalization ability. We now
describe these algorithms.
</bodyText>
<subsectionHeader confidence="0.817109">
5.1 MaxEnt-Rank
5.1.1 Sampling and Pruning
</subsectionHeader>
<bodyText confidence="0.999970214285715">
Maximum Entropy models are useful for the task
of ranking because they compute a reliable rank-
ing probability for each hypothesis. We have tried
two different sampling methods – single sampling
and pairwise sampling.
The first approach is to use each single hy-
pothesis hi as a sample. Only the best hypothesis
of each sentence is regarded as a positive sample;
all the rest are regarded as negative samples. In
general, absolute values of features are not good
indicators of whether a hypothesis will be the best
hypothesis for a sentence; for example, a co-
referring mention count of 7 may be excellent for
one sentence and poor for another. Consequently,
in this single-hypothesis-sampling approach, we
convert each feature to a Boolean value, which is
true if the original feature takes on its maximum
value (among all hypotheses) for this hypothesis.
This does, however, lose some of the detail about
the differences between hypotheses.
In pairwise sampling we used each pair of hy-
potheses (hi, hj) as a sample. The value of a fea-
ture for a sample is the difference between its
values for the two hypotheses. However, consid-
ering all pairs causes the number of samples to
grow quadratically (O(N2)) with the number of
hypotheses, compared to the linear growth with
best/non-best sampling. To make the training and
</bodyText>
<equation confidence="0.69161">
x ∈
</equation>
<page confidence="0.961115">
52
</page>
<bodyText confidence="0.99994575">
test procedures more efficient, we prune the data
in several ways.
We perform pruning by beam setting, removing
candidate hypotheses that possess very low prob-
abilities from the HMM, and during training we
discard the hypotheses with very low F-measure
scores. Additionally, we incorporate the pruning
techniques used in (Chiang 2005), by which any
hypothesis with a probability lower thanαtimes
the highest probability for one sentence is dis-
carded. We also discard the pairs very close in
performance or probability.
</bodyText>
<subsectionHeader confidence="0.521005">
5.1.2 Decoding
</subsectionHeader>
<bodyText confidence="0.943005538461538">
If f is the ranking function, the MaxEnt model
produces a probability for each un-pruned “cru-
cial” pair: prob(f(hi, hj) = 1), i.e., the probability
that for the given sentence, hi is a better hypothe-
sis than hj. We need an additional decoding step to
select the best hypothesis. Inspired by the caching
idea and the multi-class solution proposed by
(Platt et al. 2000), we use a dynamic decoding
algorithm with complexity O(n) as follows.
We scale the probability values into three types:
CompareResult (hi, hj) = “better” if prob(f(hi, hj) =
1) &gt;81, “worse” if prob(f(hi, hj) = 1) &lt;82, and
“unsure” otherwise, where 81%82. 4
</bodyText>
<equation confidence="0.788292764705882">
Prune
for i = 1 to n
Num = 0;
for j = 1 to n and jai
If CompareResult(hi, hj) = “worse”
Num++;
if Num&gt;0then discard hi from H
Select
Initialize: i = 1, j = n
while (i&lt;j)
if CompareResult(hi, hj) = “better”
discard hj from H;
j--;
else if CompareResult(hi, hj) = “worse”
discard hi from H;
i++;
else break;
</equation>
<bodyText confidence="0.692539166666667">
4 In the final stage re-ranker we use81=82 so that we don’t generate the
output of “unsure”, and one hypothesis is finally selected.
Output
If the number of remaining hypotheses in H is 1,
then output it as the best hypothesis; else propa-
gate all hypothesis pairs into the next re-ranker.
</bodyText>
<subsectionHeader confidence="0.993377">
5.2 SVMRank
</subsectionHeader>
<bodyText confidence="0.999985944444445">
We implemented an SVM-based model, which
can theoretically achieve very low generalization
error. We use the SVMLight package (Joachims,
1998), with the pairwise sampling scheme as for
MaxEnt-Rank. In addition we made the following
adaptations: we calibrated the SVM outputs, and
separated the data into subsets.
To speed up training, we divided our training
samples into k subsets. Each subset contains N(N-
1)/k pairs of hypotheses of each sentence.
In order to combine the results from these dif-
ferent SVMs, we must calibrate the function val-
ues; the output of an SVM yields a distance to the
separating hyperplane, but not a probability. We
have applied the method described in (Shen and
Joshi, 2003), to map SVM’s results to probabili-
ties via a sigmoid. Thus from the kth SVM, we get
the probability for each pair of hypotheses:
</bodyText>
<equation confidence="0.718563">
prob(fk (hi, hj ) =1) ,
</equation>
<bodyText confidence="0.854539">
namely the probability of hi being better than hj.
Then combining all k SVMs’ results we get:
</bodyText>
<equation confidence="0.99721925">
Z hi h j
( , ) =∏ prob fk hi h j
( ( , ) =1) .
k
</equation>
<bodyText confidence="0.986508333333333">
So the hypothesis hi with maximal value is cho-
sen as the top hypothesis:
arg max(∏ Z(hi , h j )) .hi j
</bodyText>
<subsectionHeader confidence="0.996598">
5.3 P-Norm Push Ranking
</subsectionHeader>
<bodyText confidence="0.984931777777778">
The third algorithm we have tried is a general
boosting-style supervised ranking algorithm called
p-Norm Push Ranking (Rudin, 2006). We de-
scribe this algorithm in more detail since it is quite
new and we do not expect many readers to be fa-
miliar with it.
The parameter
determines how much em-
phasis (or
is placed closer to the top of the
ranked list, where
The p-Norm Push Ranking
algorithm generalizes RankBoost (take p=1 for
RankBoost). When p is set at a large value, the
rankings at the top of the list are given higher pri-
ority (a large
at the expense of possibly
making misranks towards the bott
</bodyText>
<equation confidence="0.55345825">
“p”
“push”)
p≥1.
“push”),
</equation>
<bodyText confidence="0.877648">
om of the list.
</bodyText>
<page confidence="0.997218">
53
</page>
<bodyText confidence="0.999981478260869">
Since for our application, we do not care about the
rankings at the bottom of the list (i.e., we do not
care about the exact rank ordering of the bad hy-
potheses), this algorithm is suitable for our prob-
lem. There is a tradeoff for the choice of p; larger
p yields more accurate results at the very top of
the list for the training data. If we want to consider
more than simply the very top of the list, we may
desire a smaller value of p. Note that larger values
of p also require more training data in order to
maintain generalization ability (as shown both by
theoretical generalization bounds and experi-
ments). If we want large p, we must aim to choose
the largest value of p that allows generalization,
given our amount of training data. When we are
working on the first stage of re-ranking, we con-
sider the whole top portion of the ranked list, be-
cause we use the rank in the list as a feature for
the next stage. Thus, we have chosen the value
p1=4 (a small “push”) for the first re-ranker. For
the second re-ranker we choose p2=16 (a large
“push”).
The objective of the p-Norm Push Ranking al-
gorithm is to create a scoring function f: H4R
such that for each crucial pair (hi, hj), we shall
have f(hi) &gt; f(hj). The form of the scoring function
is f(hi) = ∑αkgk(hi), where gk is called a weak
ranker: gk : H 4 [0,1]. The values of αk are de-
termined by the p-Norm Push algorithm in an it-
erative way.
The weak rankers gk are the features described
in Section 4. Note that we sometimes allow the
algorithm to use both gk and g’k(hi)=1-gk(hi) as
weak rankers, namely when gk has low accuracy
on the training set; this way the algorithm itself
can decide which to use.
As in the style of boosting algorithms, real-
valued weights are placed on each of the training
crucial pairs, and these weights are successively
updated by the algorithm. Higher weights are
given to those crucial pairs that were misranked at
the previous iteration, especially taking into ac-
count the pairs near the top of the list. At each
iteration, one weak ranker gk is chosen by the al-
gorithm, based on the weights. The coefficient αk
is then updated accordingly.
</bodyText>
<sectionHeader confidence="0.97116" genericHeader="method">
6 Experiment Results
</sectionHeader>
<subsectionHeader confidence="0.997069">
6.1 Data and Resources
</subsectionHeader>
<bodyText confidence="0.999933333333333">
We use 100 texts from the ACE 04 training corpus
for a blind test. The test set included 2813 names:
1126 persons, 712 GPEs, 785 organizations and
190 locations. The performance is measured via
Precision (P), Recall (R) and F-Measure (F).
The baseline name tagger is trained from 2978
texts from the People’s Daily news in 1998 and
also 1300 texts from ACE training data.
The 1,071,285 training samples (pairs of hy-
potheses) for the re-rankers are obtained from the
name tagger applied on the ACE training data, in
the manner described in Section 3.2.
We use OpenNLP5 for the MaxEnt-Rank ex-
periments. We use SVMlight (Joachims, 1998) for
SVMRank, with a linear kernel and the soft mar-
gin parameter set to the default value. For the p-
Norm Push Ranking, we apply 33 weak rankers,
i.e., features described in Section 4. The number
of iterations was fixed at 110, this number was
chosen by optimizing the performance on a devel-
opment set of 100 documents.
</bodyText>
<subsectionHeader confidence="0.999985">
6.2 Effect of Pairwise Sampling
</subsectionHeader>
<bodyText confidence="0.999840571428572">
We have tried both single-hypothesis and pairwise
sampling (described in section 5.1.1) in MaxEnt-
Rank and p-Norm Push Ranking. Table 1 shows
that pairwise sampling helps both algorithms.
MaxEnt-Rank benefited more from it, with preci-
sion and recall increased 2.2% and 0.4% respec-
tively.
</bodyText>
<table confidence="0.999845714285714">
Model P R F
MaxEnt- Single Sampling 89.6 90.2 89.9
Rank
Pairwise Sampling 91.8 90.6 91.2
p-Norm Single Sampling 91.4 89.6 90.5
Push
Pairwise Sampling 91.2 90.8 91.0
</table>
<tableCaption confidence="0.999904">
Table 1. Effect of Pairwise Sampling
</tableCaption>
<subsectionHeader confidence="0.99959">
6.3 Overall Performance
</subsectionHeader>
<bodyText confidence="0.99988325">
In Table 2 we report the overall performance for
these three algorithms. All of them achieved im-
provements on the baseline name tagger. MaxEnt
yields the highest precision, while p-Norm Push
Ranking with p2 = 16 yields the highest recall.
A larger value of “p” encourages the p-Norm
Push Ranking algorithm to perform better near the
top of the ranked list. As we discussed in section
</bodyText>
<footnote confidence="0.944076">
5 http://maxent.sourceforge.net/index.html
</footnote>
<page confidence="0.998635">
54
</page>
<bodyText confidence="0.988287">
5.3, we use p1 = 4 (a small “push”) for the first re-
ranker and p2 = 16 (a big “push”) for the second
re-ranker. From Table 2 we can see that p2 = 16
obviously performed better than p2 = 1. In general,
we have observed that for p2 ≤16, larger p2 corre-
lates with better results.
</bodyText>
<table confidence="0.999757285714286">
Model P R F
Baseline 87.4 87.6 87.5
MaxEnt-Rank 91.8 90.6 91.2
SVMRank 89.5 90.1 89.8
p-Norm Push Ranking (p2 =16) 91.2 90.8 91.0
p-Norm Push Ranking 89.3 89.7 89.5
(p2 =1, RankBoost)
</table>
<tableCaption confidence="0.99913">
Table 2. Overall Performance
</tableCaption>
<bodyText confidence="0.999330625">
The improved NE results brought better per-
formance for the subsequent stages of information
extraction too. We use the NE outputs from Max-
Ent-Ranker as inputs for coreference resolver and
relation tagger. The ACE value6 of entity detec-
tion (mention detection + coreference resolution)
is increased from 73.2 to 76.5; the ACE value of
relation detection is increased from 34.2 to 34.8.
</bodyText>
<subsectionHeader confidence="0.995204">
6.4 Effect of Cross-document Information
</subsectionHeader>
<bodyText confidence="0.9998896">
As described in Section 4.3, our algorithm incor-
porates cross-document coreference information.
The 100 texts in the test set were first clustered
into 28 topics (clusters). We then apply cross-
document coreference on each cluster. Compared
to single document coreference, cross-document
coreference obtained 0.5% higher F-Measure, us-
ing MaxEnt-Ranker, improving performance for
15 of these 28 clusters.
These clusters were then extended by selecting
84 additional related texts from a corpus of 15,000
unlabeled Chinese news articles (using a cross-
entropy metric to select texts). 24 clusters gave
further improvement, and an overall 0.2% further
improvement on F-Measure was obtained.
</bodyText>
<sectionHeader confidence="0.553204" genericHeader="method">
6.5 Efficiency
</sectionHeader>
<table confidence="0.97853175">
Model Training Test
MaxEnt-Rank 7 hours 55 minutes
SVMRank 48 hours 2 hours
p-Norm Push Ranking 3.2 hours 10 minutes
</table>
<tableCaption confidence="0.995651">
Table 3. Efficiency Comparison
</tableCaption>
<footnote confidence="0.7332475">
6 The ACE04 value scoring metric can be found at:
http://www.nist.gov/speech/tests/ace/ace04/doc/ace04-evalplan-v7.pdf
</footnote>
<bodyText confidence="0.8269245">
In Table 3 we summarize the running time of
these three algorithms in our application.
</bodyText>
<sectionHeader confidence="0.999442" genericHeader="discussions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999127588235294">
We have shown that the other components of an
IE pipeline can provide information which can
substantially improve the performance of an NE
tagger, and that these improvements can be real-
ized through a variety of re-ranking algorithms.
MaxEnt re-ranking using binary sampling and p-
Norm Push Ranking proved about equally effec-
tive.7 p-Norm Push Ranking was particularly ef-
ficient for decoding (about 10 documents /
minute), although no great effort was invested in
tuning these procedures for speed.
We presented methods to handle cross-sentence
inference using staged re-ranking and to incorpo-
rate additional evidence through document clus-
tering.
An N-best / re-ranking strategy has proven ef-
fective for this task because with relatively small
</bodyText>
<subsectionHeader confidence="0.505222">
values of N we are already able to include highly-
</subsectionHeader>
<bodyText confidence="0.9987102">
rated hypotheses for most sentences. Using the
values of N we have used throughout (dependent
on the margin of the baseline HMM, but never
above 30), the upper bound of N-best performance
(if we always picked the top-scoring hypothesis)
is 97.4% recall, 96.2% precision, F=96.8%.
Collins (2002) also applied re-ranking to im-
prove name tagging. Our work has addressed both
name identification and classification, while his
only evaluated name identification. Our re-ranker
used features from other pipeline stages, while his
were limited to local features involving lexical
information and &apos;word-shape&apos; in a 5-token window.
Since these feature sets are essentially disjoint, it
is quite possible that a combination of the two
could yield even further improvements. His boost-
ing algorithm is a modification of the method in
(Freund et al., 1998), an adaptation of AdaBoost,
whereas our p-Norm Push Ranking algorithm can
emphasize the hypotheses near the top, matching
our objective.
Roth and Yih (2004) combined information
from named entities and semantic relation tagging,
adopting a similar overall goal but using a quite
different approach based on linear programming.
</bodyText>
<footnote confidence="0.978555666666667">
7 The features were initially developed and tested using the MaxEnt re-ranker,
so it is encouraging that they worked equally well with the p-Norm Push
Ranker without further tuning.
</footnote>
<page confidence="0.998598">
55
</page>
<bodyText confidence="0.999982789473684">
They limited themselves to name classification,
assuming the identification given. This may be a
natural subtask for English, where capitalization is
a strong indicator of a name, but is much less use-
ful for Chinese, where there is no capitalization or
word segmentation, and boundary errors on name
identification are frequent. Expanding their ap-
proach to cover identification would have greatly
increased the number of hypotheses and made
their approach slower. In contrast, we adjust the
number of hypotheses based on the margin in or-
der to maintain efficiency while minimizing the
chance of losing a high-quality hypothesis.
In addition we were able to capture selectional
preferences (probabilities of semantic types as
arguments of particular semantic relations as
computed from the corpus), whereas Roth and Yih
limited themselves to hard (boolean) type con-
straints.
</bodyText>
<sectionHeader confidence="0.986552" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.999071666666666">
This material is based upon work supported by the
Defense Advanced Research Projects Agency un-
der Contract No. HR0011-06-C-0023, and the Na-
tional Science Foundation under Grant IIS-
00325657 and a postdoctoral research fellowship.
Any opinions, findings and conclusions expressed
in this material are those of the authors and do not
necessarily reflect the views of the U. S. Govern-
ment.
</bodyText>
<sectionHeader confidence="0.999549" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999933914285714">
Daniel M. Bikel, Scott Miller, Richard Schwartz, and
Ralph Weischedel. 1997. Nymble: a high-
performance Learning Name-finder. Proc.
ANLP1997. pp. 194-201. Washington, D.C.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
Fine N-Best Parsing and MaxEnt Discriminative
Reranking. Proc. ACL2005. pp. 173-180. Ann Arbor,
USA
David Chiang. 2005. A Hierarchical Phrase-Based
Model for Statistical Machine Translation. Proc.
ACL2005. pp. 263-270. Ann Arbor, USA
Michael Collins. 2002. Ranking Algorithms for
Named-Entity Extraction: Boosting and the Voted
Perceptron. Proc. ACL 2002. pp. 489-496
Michael Collins and Nigel Duffy. 2002. New Ranking
Algorithms for Parsing and Tagging: Kernels over
Discrete Structures, and the Voted Perceptron. Proc.
ACL2002. pp. 263-270. Philadelphia, USA
Michael Collins and Terry Koo. 2003. Discriminative
Reranking for Natural Language Parsing. Journal of
Association for Computational Linguistics. pp. 175-
182.
Yoav Freund, Raj Iyer, Robert E. Schapire and Yoram
Singer. 1998. An efficient boosting algorithm for
combining preferences. Machine Learning: Pro-
ceedings of the Fifteenth International Conference.
pp. 170-178
Ralph Grishman and Beth Sundheim. 1996. Message
understanding conference - 6: A brief history. Proc.
COLING1996,. pp. 466-471. Copenhagen.
James Henderson and Ivan Titov. 2005. Data-Defined
Kernels for Parse Reranking Derived from Probabil-
istic Models. Proc. ACL2005. pp. 181-188. Ann Ar-
bor, USA.
Heng Ji and Ralph Grishman. 2005. Improving Name
Tagging by Reference Resolution and Relation De-
tection. Proc. ACL2005. pp. 411-418. Ann Arbor,
USA.
Heng Ji and Ralph Grishman. 2006. Analysis and Re-
pair of Name Tagger Errors. Proc. ACL2006
(POSTER). Sydney, Australia.
Thorsten Joachims. 1998. Making large-scale support
vector machine learning practical. Advances in Ker-
nel Methods: Support Vector Machine. MIT Press.
Taku Kudo, Jun Suzuki and Hideki Isozaki. 2005.
Boosting-based Parse Reranking Derived from
Probabilistic Models. Proc. ACL2005. pp. 189-196.
Ann Arbor, USA.
John Platt, Nello Cristianini, and John Shawe-Taylor.
2000. Large margin dags for multiclass classifica-
tion. Advances in Neural Information Processing
Systems 12. pp. 547-553
Dan Roth and Wen-tau Yih. 2004. A Linear Program-
ming Formulation for Global Inference in Natural
Language Tasks. Proc. CONLL2004. pp. 1-8
Dan Roth and Wen-tau Yih. 2002. Probabilistic Rea-
soning for Entity &amp; Relation Recognition. Proc.
COLING2002. pp. 835-841
Cynthia Rudin. 2006. Ranking with a p-Norm Push.
Proc. Nineteenth Annual Conference on Computa-
tional Learning Theory (CoLT 2006), Pittsburgh,
Pennsylvania.
Libin Shen and Aravind K. Joshi. 2003. An SVM
Based Voting Algorithm with Application to Parse
ReRanking. Proc. HLT-NAACL 2003 workshop on
Analysis of Geographic References. pp. 9-16
Libin Shen and Aravind K. Joshi. 2004. Flexible Mar-
gin Selection for Reranking with Full Pairwise Sam-
ples. Proc.IJCNLP2004. pp. 446-455. Hainan Island,
China.
</reference>
<page confidence="0.998422">
56
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.866089">
<title confidence="0.999875">Re-Ranking Algorithms for Name Tagging</title>
<author confidence="0.999821">Heng Ji Cynthia Rudin Ralph Grishman</author>
<affiliation confidence="0.999929">Dept. of Computer Science Center for Neural Science and Courant Dept. of Computer Institute of Mathematical</affiliation>
<address confidence="0.992985">New York New York, N.Y. 10003</address>
<email confidence="0.999626">hengji@cs.nyu.edurudin@nyu.edugrishman@cs.nyu.edu</email>
<abstract confidence="0.990743642857143">Integrating information from different stages of an NLP processing pipeline can yield significant error reduction. We demonstrate how re-ranking can improve name tagging in a Chinese information extraction system by incorporating information from relation extraction, event extraction, and coreference. We evaluate three stateof-the-art re-ranking algorithms (MaxEnt- Rank, SVMRank, and p-Norm Push Ranking), and show the benefit of multi-stage re-ranking for cross-sentence and crossdocument inference.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
<author>Scott Miller</author>
<author>Richard Schwartz</author>
<author>Ralph Weischedel</author>
</authors>
<title>Nymble: a highperformance Learning Name-finder.</title>
<date>1997</date>
<booktitle>Proc. ANLP1997.</booktitle>
<pages>194--201</pages>
<location>Washington, D.C.</location>
<contexts>
<context position="7050" citStr="Bikel et al, 1997" startWordPosition="1077" endWordPosition="1080">o apply the baseline name tagger to generate N-Best multiple hypotheses for each sentence; the results from subsequent components are then exploited to re-rank these hypotheses and the new top hypothesis is output as the final result. In our name re-ranking model, each hypothesis is an NE tagging of the entire sentence. For example, “&lt;PER&gt;John&lt;/PER&gt; was born in &lt;GPE&gt;New York&lt;/GPE&gt;.” is one hypothesis for the sentence “John was born in New York”. We apply a HMM tagger to identify four named entity types: Person, GPE, Organization and Location. The HMM tagger generally follows the Nymble model (Bikel et al, 1997), and uses bestfirst search to generate N-Best hypotheses. It also computes the “margin”, which is the difference between the log probabilities of the top two hypotheses. This is used as a rough measure of confidence in the top hypothesis. A large margin indicates greater confidence that the first hypothesis is correct. The margin also determines the number of hypotheses (N) that we will store. Using cross-validation on the training data, we determine the value of N required to include the best 50 hypothesis, as a function of the margin. We then divide the margin into ranges of values, and set</context>
</contexts>
<marker>Bikel, Miller, Schwartz, Weischedel, 1997</marker>
<rawString>Daniel M. Bikel, Scott Miller, Richard Schwartz, and Ralph Weischedel. 1997. Nymble: a highperformance Learning Name-finder. Proc. ANLP1997. pp. 194-201. Washington, D.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<date>2005</date>
<booktitle>Coarse-toFine N-Best Parsing and MaxEnt Discriminative Reranking. Proc. ACL2005.</booktitle>
<pages>173--180</pages>
<location>Ann Arbor, USA</location>
<contexts>
<context position="1282" citStr="Charniak and Johnson, 2005" startWordPosition="180" endWordPosition="183">ithms (MaxEntRank, SVMRank, and p-Norm Push Ranking), and show the benefit of multi-stage re-ranking for cross-sentence and crossdocument inference. 1 Introduction In recent years, re-ranking techniques have been successfully applied to enhance the performance of NLP analysis components based on generative models. A baseline generative model produces Nbest candidates, which are then re-ranked using a rich set of local and global features in order to select the best analysis. Various supervised learning algorithms have been adapted to the task of reranking for NLP systems, such as MaxEnt-Rank (Charniak and Johnson, 2005; Ji and Grishman, 2005), SVMRank (Shen and Joshi, 2003), Voted Perceptron (Collins, 2002; Collins and Duffy, 2002; Shen and Joshi, 2004), Kernel Based Methods (Henderson and Titov, 2005), and RankBoost (Collins, 2002; Collins and Koo, 2003; Kudo et al., 2005). These algorithms have been used primarily within the context of a single NLP analysis component, with the most intensive study devoted to improving parsing performance. The re-ranking models for parsing, for example, normally rely on structures generated within the baseline parser itself. Achieving really high performance for some analy</context>
<context position="3520" citStr="Charniak and Johnson, 2005" startWordPosition="529" endWordPosition="532">nce and document boundaries. 2 Prior Work 2.1 Ranking We will describe the three state-of-the-art supervised ranking techniques considered in this work. Later we shall apply and evaluate these algorithms for re-ranking in the context of name tagging. Maximum Entropy modeling (MaxEnt) has been extremely successful for many NLP classifi49 Workshop on Computationally Hard Problemsand Joint Inference in Speech and Language Processing, pages 49–56, New York City, New York, June 2006. c�2006 Association for Computational Linguistics cation tasks, so it is natural to apply it to reranking problems. (Charniak and Johnson, 2005) applied MaxEnt to improve the performance of a state-of-art parser; also in (Ji and Grishman, 2005) we used it to improve a Chinese name tagger. Using SVMRank, (Shen and Joshi, 2003) achieved significant improvement on parse reranking. They compared two different sample creation methods, and presented an efficient training method by separating the training samples into subsets. The last approach we consider is a boostingstyle approach. We implement a new algorithm called p-Norm Push Ranking (Rudin, 2006). This algorithm is a generalization of RankBoost (Freund et al. 1998) which concentrates </context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-toFine N-Best Parsing and MaxEnt Discriminative Reranking. Proc. ACL2005. pp. 173-180. Ann Arbor, USA</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A Hierarchical Phrase-Based Model for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>Proc. ACL2005.</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, USA</location>
<contexts>
<context position="16414" citStr="Chiang 2005" startWordPosition="2670" endWordPosition="2671">mple is the difference between its values for the two hypotheses. However, considering all pairs causes the number of samples to grow quadratically (O(N2)) with the number of hypotheses, compared to the linear growth with best/non-best sampling. To make the training and x ∈ 52 test procedures more efficient, we prune the data in several ways. We perform pruning by beam setting, removing candidate hypotheses that possess very low probabilities from the HMM, and during training we discard the hypotheses with very low F-measure scores. Additionally, we incorporate the pruning techniques used in (Chiang 2005), by which any hypothesis with a probability lower thanαtimes the highest probability for one sentence is discarded. We also discard the pairs very close in performance or probability. 5.1.2 Decoding If f is the ranking function, the MaxEnt model produces a probability for each un-pruned “crucial” pair: prob(f(hi, hj) = 1), i.e., the probability that for the given sentence, hi is a better hypothesis than hj. We need an additional decoding step to select the best hypothesis. Inspired by the caching idea and the multi-class solution proposed by (Platt et al. 2000), we use a dynamic decoding algo</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A Hierarchical Phrase-Based Model for Statistical Machine Translation. Proc. ACL2005. pp. 263-270. Ann Arbor, USA</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Ranking Algorithms for Named-Entity Extraction: Boosting and the Voted Perceptron.</title>
<date>2002</date>
<booktitle>Proc. ACL</booktitle>
<pages>489--496</pages>
<contexts>
<context position="1371" citStr="Collins, 2002" startWordPosition="195" endWordPosition="196">or cross-sentence and crossdocument inference. 1 Introduction In recent years, re-ranking techniques have been successfully applied to enhance the performance of NLP analysis components based on generative models. A baseline generative model produces Nbest candidates, which are then re-ranked using a rich set of local and global features in order to select the best analysis. Various supervised learning algorithms have been adapted to the task of reranking for NLP systems, such as MaxEnt-Rank (Charniak and Johnson, 2005; Ji and Grishman, 2005), SVMRank (Shen and Joshi, 2003), Voted Perceptron (Collins, 2002; Collins and Duffy, 2002; Shen and Joshi, 2004), Kernel Based Methods (Henderson and Titov, 2005), and RankBoost (Collins, 2002; Collins and Koo, 2003; Kudo et al., 2005). These algorithms have been used primarily within the context of a single NLP analysis component, with the most intensive study devoted to improving parsing performance. The re-ranking models for parsing, for example, normally rely on structures generated within the baseline parser itself. Achieving really high performance for some analysis components, however, requires that we take a broader view, one that looks outside a s</context>
<context position="4546" citStr="Collins (2002)" startWordPosition="695" endWordPosition="696">is a boostingstyle approach. We implement a new algorithm called p-Norm Push Ranking (Rudin, 2006). This algorithm is a generalization of RankBoost (Freund et al. 1998) which concentrates specifically on the top portion of a ranked list. The parameter “p” determines how much the algorithm concentrates at the top. 2.2 Enhancing Named Entity Taggers There have been a very large number of NE tagger implementations since this task was introduced at MUC-6 (Grishman and Sundheim, 1996). Most implementations use local features and a unifying learning algorithm based on, e.g., an HMM, MaxEnt, or SVM. Collins (2002) augmented a baseline NE tagger with a re-ranker that used only local, NE-oriented features. Roth and Yih (2002) combined NE and semantic relation tagging, but within a quite different framework (using a linear programming model for joint inference). 3 A Framework for Name Re-Ranking 3.1 The Information Extraction Pipeline The extraction task we are addressing is that of the Automatic Content Extraction (ACE)1 evaluations. The 2005 ACE evaluation had 7 types of entities, of which the most common were PER (persons), ORG (organizations), LOC (natural locations) and GPE (‘geo-political entities’ </context>
<context position="26797" citStr="Collins (2002)" startWordPosition="4462" endWordPosition="4463">ese procedures for speed. We presented methods to handle cross-sentence inference using staged re-ranking and to incorporate additional evidence through document clustering. An N-best / re-ranking strategy has proven effective for this task because with relatively small values of N we are already able to include highlyrated hypotheses for most sentences. Using the values of N we have used throughout (dependent on the margin of the baseline HMM, but never above 30), the upper bound of N-best performance (if we always picked the top-scoring hypothesis) is 97.4% recall, 96.2% precision, F=96.8%. Collins (2002) also applied re-ranking to improve name tagging. Our work has addressed both name identification and classification, while his only evaluated name identification. Our re-ranker used features from other pipeline stages, while his were limited to local features involving lexical information and &apos;word-shape&apos; in a 5-token window. Since these feature sets are essentially disjoint, it is quite possible that a combination of the two could yield even further improvements. His boosting algorithm is a modification of the method in (Freund et al., 1998), an adaptation of AdaBoost, whereas our p-Norm Pus</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Ranking Algorithms for Named-Entity Extraction: Boosting and the Voted Perceptron. Proc. ACL 2002. pp. 489-496</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron.</title>
<date>2002</date>
<booktitle>Proc. ACL2002.</booktitle>
<pages>263--270</pages>
<location>Philadelphia, USA</location>
<contexts>
<context position="1396" citStr="Collins and Duffy, 2002" startWordPosition="197" endWordPosition="200">ce and crossdocument inference. 1 Introduction In recent years, re-ranking techniques have been successfully applied to enhance the performance of NLP analysis components based on generative models. A baseline generative model produces Nbest candidates, which are then re-ranked using a rich set of local and global features in order to select the best analysis. Various supervised learning algorithms have been adapted to the task of reranking for NLP systems, such as MaxEnt-Rank (Charniak and Johnson, 2005; Ji and Grishman, 2005), SVMRank (Shen and Joshi, 2003), Voted Perceptron (Collins, 2002; Collins and Duffy, 2002; Shen and Joshi, 2004), Kernel Based Methods (Henderson and Titov, 2005), and RankBoost (Collins, 2002; Collins and Koo, 2003; Kudo et al., 2005). These algorithms have been used primarily within the context of a single NLP analysis component, with the most intensive study devoted to improving parsing performance. The re-ranking models for parsing, for example, normally rely on structures generated within the baseline parser itself. Achieving really high performance for some analysis components, however, requires that we take a broader view, one that looks outside a single component in order </context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Michael Collins and Nigel Duffy. 2002. New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron. Proc. ACL2002. pp. 263-270. Philadelphia, USA</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>Discriminative Reranking for Natural Language Parsing.</title>
<date>2003</date>
<journal>Journal of Association for Computational Linguistics.</journal>
<pages>175--182</pages>
<contexts>
<context position="1522" citStr="Collins and Koo, 2003" startWordPosition="217" endWordPosition="220">the performance of NLP analysis components based on generative models. A baseline generative model produces Nbest candidates, which are then re-ranked using a rich set of local and global features in order to select the best analysis. Various supervised learning algorithms have been adapted to the task of reranking for NLP systems, such as MaxEnt-Rank (Charniak and Johnson, 2005; Ji and Grishman, 2005), SVMRank (Shen and Joshi, 2003), Voted Perceptron (Collins, 2002; Collins and Duffy, 2002; Shen and Joshi, 2004), Kernel Based Methods (Henderson and Titov, 2005), and RankBoost (Collins, 2002; Collins and Koo, 2003; Kudo et al., 2005). These algorithms have been used primarily within the context of a single NLP analysis component, with the most intensive study devoted to improving parsing performance. The re-ranking models for parsing, for example, normally rely on structures generated within the baseline parser itself. Achieving really high performance for some analysis components, however, requires that we take a broader view, one that looks outside a single component in order to bring to bear knowledge from the entire NL analysis process. In this paper we will demonstrate the potential of this approa</context>
</contexts>
<marker>Collins, Koo, 2003</marker>
<rawString>Michael Collins and Terry Koo. 2003. Discriminative Reranking for Natural Language Parsing. Journal of Association for Computational Linguistics. pp. 175-182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Raj Iyer</author>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>An efficient boosting algorithm for combining preferences.</title>
<date>1998</date>
<booktitle>Machine Learning: Proceedings of the Fifteenth International Conference.</booktitle>
<pages>170--178</pages>
<contexts>
<context position="4100" citStr="Freund et al. 1998" startWordPosition="620" endWordPosition="623"> problems. (Charniak and Johnson, 2005) applied MaxEnt to improve the performance of a state-of-art parser; also in (Ji and Grishman, 2005) we used it to improve a Chinese name tagger. Using SVMRank, (Shen and Joshi, 2003) achieved significant improvement on parse reranking. They compared two different sample creation methods, and presented an efficient training method by separating the training samples into subsets. The last approach we consider is a boostingstyle approach. We implement a new algorithm called p-Norm Push Ranking (Rudin, 2006). This algorithm is a generalization of RankBoost (Freund et al. 1998) which concentrates specifically on the top portion of a ranked list. The parameter “p” determines how much the algorithm concentrates at the top. 2.2 Enhancing Named Entity Taggers There have been a very large number of NE tagger implementations since this task was introduced at MUC-6 (Grishman and Sundheim, 1996). Most implementations use local features and a unifying learning algorithm based on, e.g., an HMM, MaxEnt, or SVM. Collins (2002) augmented a baseline NE tagger with a re-ranker that used only local, NE-oriented features. Roth and Yih (2002) combined NE and semantic relation tagging</context>
<context position="27346" citStr="Freund et al., 1998" startWordPosition="4544" endWordPosition="4547">g hypothesis) is 97.4% recall, 96.2% precision, F=96.8%. Collins (2002) also applied re-ranking to improve name tagging. Our work has addressed both name identification and classification, while his only evaluated name identification. Our re-ranker used features from other pipeline stages, while his were limited to local features involving lexical information and &apos;word-shape&apos; in a 5-token window. Since these feature sets are essentially disjoint, it is quite possible that a combination of the two could yield even further improvements. His boosting algorithm is a modification of the method in (Freund et al., 1998), an adaptation of AdaBoost, whereas our p-Norm Push Ranking algorithm can emphasize the hypotheses near the top, matching our objective. Roth and Yih (2004) combined information from named entities and semantic relation tagging, adopting a similar overall goal but using a quite different approach based on linear programming. 7 The features were initially developed and tested using the MaxEnt re-ranker, so it is encouraging that they worked equally well with the p-Norm Push Ranker without further tuning. 55 They limited themselves to name classification, assuming the identification given. This</context>
</contexts>
<marker>Freund, Iyer, Schapire, Singer, 1998</marker>
<rawString>Yoav Freund, Raj Iyer, Robert E. Schapire and Yoram Singer. 1998. An efficient boosting algorithm for combining preferences. Machine Learning: Proceedings of the Fifteenth International Conference. pp. 170-178</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>Beth Sundheim</author>
</authors>
<title>Message understanding conference - 6: A brief history.</title>
<date>1996</date>
<booktitle>Proc. COLING1996,.</booktitle>
<pages>466--471</pages>
<location>Copenhagen.</location>
<contexts>
<context position="4416" citStr="Grishman and Sundheim, 1996" startWordPosition="672" endWordPosition="675">ple creation methods, and presented an efficient training method by separating the training samples into subsets. The last approach we consider is a boostingstyle approach. We implement a new algorithm called p-Norm Push Ranking (Rudin, 2006). This algorithm is a generalization of RankBoost (Freund et al. 1998) which concentrates specifically on the top portion of a ranked list. The parameter “p” determines how much the algorithm concentrates at the top. 2.2 Enhancing Named Entity Taggers There have been a very large number of NE tagger implementations since this task was introduced at MUC-6 (Grishman and Sundheim, 1996). Most implementations use local features and a unifying learning algorithm based on, e.g., an HMM, MaxEnt, or SVM. Collins (2002) augmented a baseline NE tagger with a re-ranker that used only local, NE-oriented features. Roth and Yih (2002) combined NE and semantic relation tagging, but within a quite different framework (using a linear programming model for joint inference). 3 A Framework for Name Re-Ranking 3.1 The Information Extraction Pipeline The extraction task we are addressing is that of the Automatic Content Extraction (ACE)1 evaluations. The 2005 ACE evaluation had 7 types of enti</context>
</contexts>
<marker>Grishman, Sundheim, 1996</marker>
<rawString>Ralph Grishman and Beth Sundheim. 1996. Message understanding conference - 6: A brief history. Proc. COLING1996,. pp. 466-471. Copenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
<author>Ivan Titov</author>
</authors>
<title>Data-Defined Kernels for Parse Reranking Derived from Probabilistic Models.</title>
<date>2005</date>
<booktitle>Proc. ACL2005.</booktitle>
<pages>181--188</pages>
<location>Ann Arbor, USA.</location>
<contexts>
<context position="1469" citStr="Henderson and Titov, 2005" startWordPosition="209" endWordPosition="212">king techniques have been successfully applied to enhance the performance of NLP analysis components based on generative models. A baseline generative model produces Nbest candidates, which are then re-ranked using a rich set of local and global features in order to select the best analysis. Various supervised learning algorithms have been adapted to the task of reranking for NLP systems, such as MaxEnt-Rank (Charniak and Johnson, 2005; Ji and Grishman, 2005), SVMRank (Shen and Joshi, 2003), Voted Perceptron (Collins, 2002; Collins and Duffy, 2002; Shen and Joshi, 2004), Kernel Based Methods (Henderson and Titov, 2005), and RankBoost (Collins, 2002; Collins and Koo, 2003; Kudo et al., 2005). These algorithms have been used primarily within the context of a single NLP analysis component, with the most intensive study devoted to improving parsing performance. The re-ranking models for parsing, for example, normally rely on structures generated within the baseline parser itself. Achieving really high performance for some analysis components, however, requires that we take a broader view, one that looks outside a single component in order to bring to bear knowledge from the entire NL analysis process. In this p</context>
</contexts>
<marker>Henderson, Titov, 2005</marker>
<rawString>James Henderson and Ivan Titov. 2005. Data-Defined Kernels for Parse Reranking Derived from Probabilistic Models. Proc. ACL2005. pp. 181-188. Ann Arbor, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Ralph Grishman</author>
</authors>
<title>Improving Name Tagging by Reference Resolution and Relation Detection.</title>
<date>2005</date>
<booktitle>Proc. ACL2005.</booktitle>
<pages>411--418</pages>
<location>Ann Arbor, USA.</location>
<contexts>
<context position="1306" citStr="Ji and Grishman, 2005" startWordPosition="184" endWordPosition="187">and p-Norm Push Ranking), and show the benefit of multi-stage re-ranking for cross-sentence and crossdocument inference. 1 Introduction In recent years, re-ranking techniques have been successfully applied to enhance the performance of NLP analysis components based on generative models. A baseline generative model produces Nbest candidates, which are then re-ranked using a rich set of local and global features in order to select the best analysis. Various supervised learning algorithms have been adapted to the task of reranking for NLP systems, such as MaxEnt-Rank (Charniak and Johnson, 2005; Ji and Grishman, 2005), SVMRank (Shen and Joshi, 2003), Voted Perceptron (Collins, 2002; Collins and Duffy, 2002; Shen and Joshi, 2004), Kernel Based Methods (Henderson and Titov, 2005), and RankBoost (Collins, 2002; Collins and Koo, 2003; Kudo et al., 2005). These algorithms have been used primarily within the context of a single NLP analysis component, with the most intensive study devoted to improving parsing performance. The re-ranking models for parsing, for example, normally rely on structures generated within the baseline parser itself. Achieving really high performance for some analysis components, however,</context>
<context position="3620" citStr="Ji and Grishman, 2005" startWordPosition="545" endWordPosition="548">sed ranking techniques considered in this work. Later we shall apply and evaluate these algorithms for re-ranking in the context of name tagging. Maximum Entropy modeling (MaxEnt) has been extremely successful for many NLP classifi49 Workshop on Computationally Hard Problemsand Joint Inference in Speech and Language Processing, pages 49–56, New York City, New York, June 2006. c�2006 Association for Computational Linguistics cation tasks, so it is natural to apply it to reranking problems. (Charniak and Johnson, 2005) applied MaxEnt to improve the performance of a state-of-art parser; also in (Ji and Grishman, 2005) we used it to improve a Chinese name tagger. Using SVMRank, (Shen and Joshi, 2003) achieved significant improvement on parse reranking. They compared two different sample creation methods, and presented an efficient training method by separating the training samples into subsets. The last approach we consider is a boostingstyle approach. We implement a new algorithm called p-Norm Push Ranking (Rudin, 2006). This algorithm is a generalization of RankBoost (Freund et al. 1998) which concentrates specifically on the top portion of a ranked list. The parameter “p” determines how much the algorith</context>
<context position="9464" citStr="Ji and Grishman, 2005" startWordPosition="1517" endWordPosition="1520">s can be considered a special case of re-ranking by score. • Indirect Re-Ranking Function For each “crucial” pair of hypotheses (hi, hj), we learn f : H × H 4 {-1, 1}, such that f(hi, hj) = 1 if hi is better than hj; f (hi, hj) = -1 if hi is worse than hj. We call this “indirect” ranking because we need to apply an additional decoding step to pick the best hypothesis from these pair-wise comparison results. 4 Features for Re-Ranking 4.1 Inferences From Subsequent Stages Information extraction is a potentially symbiotic pipeline with strong dependencies between stages (Roth and Yih, 2002&amp;2004; Ji and Grishman, 2005). Thus, we use features based on the output of four subsequent stages – name structure parsing, relation extraction, event patterns, and coreference analysis – to seek the best hypothesis. We included ten features based on name structure parsing to capture the local information missed by the baseline name tagger such as details of the structure of Chinese person names. The relation and event re-ranking features are based on matching patterns of words or constituents. They serve to correct name boundary errors (because such errors would prevent some patterns from matching). They also exert sele</context>
</contexts>
<marker>Ji, Grishman, 2005</marker>
<rawString>Heng Ji and Ralph Grishman. 2005. Improving Name Tagging by Reference Resolution and Relation Detection. Proc. ACL2005. pp. 411-418. Ann Arbor, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Ralph Grishman</author>
</authors>
<title>Analysis and Repair of Name Tagger Errors.</title>
<date>2006</date>
<booktitle>Proc. ACL2006</booktitle>
<location>(POSTER). Sydney, Australia.</location>
<contexts>
<context position="11203" citStr="Ji and Grishman, 2006" startWordPosition="1803" endWordPosition="1806">everal parts, some may be dropped) and same semantic type. So if the boundary or type of one mention can be determined with some confidence, coreference can be used to disambiguate other mentions, by favoring hypotheses which support more coreference. To this end, we incorporate several features based on coreference, such as the number of mentions referring to a name candidate. Each of these features is defined for individual name candidates; the value of the feature for a hypothesis is the sum of its values over all names in the hypothesis. The complete set of detailed features is listed in (Ji and Grishman, 2006). 4.2 Handling Cross-Sentence Features by Multi-Stage Re-Ranking Coreference is potentially a powerful contributor for enhancing NE recognition, because it provides information from other sentences and even documents, and it applies to all sentences that include names. For a name candidate, 62% of its coreference relations span sentence boundaries. However, this breadth poses a problem because it means that the score of a hypothesis for a given 51 sentence may depend on the tags assigned to the same names in other sentences.2 Ideally, when we re-rank the hypotheses for one sentence 5, the othe</context>
</contexts>
<marker>Ji, Grishman, 2006</marker>
<rawString>Heng Ji and Ralph Grishman. 2006. Analysis and Repair of Name Tagger Errors. Proc. ACL2006 (POSTER). Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale support vector machine learning practical. Advances in Kernel Methods: Support Vector Machine.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="17989" citStr="Joachims, 1998" startWordPosition="2944" endWordPosition="2945">nitialize: i = 1, j = n while (i&lt;j) if CompareResult(hi, hj) = “better” discard hj from H; j--; else if CompareResult(hi, hj) = “worse” discard hi from H; i++; else break; 4 In the final stage re-ranker we use81=82 so that we don’t generate the output of “unsure”, and one hypothesis is finally selected. Output If the number of remaining hypotheses in H is 1, then output it as the best hypothesis; else propagate all hypothesis pairs into the next re-ranker. 5.2 SVMRank We implemented an SVM-based model, which can theoretically achieve very low generalization error. We use the SVMLight package (Joachims, 1998), with the pairwise sampling scheme as for MaxEnt-Rank. In addition we made the following adaptations: we calibrated the SVM outputs, and separated the data into subsets. To speed up training, we divided our training samples into k subsets. Each subset contains N(N1)/k pairs of hypotheses of each sentence. In order to combine the results from these different SVMs, we must calibrate the function values; the output of an SVM yields a distance to the separating hyperplane, but not a probability. We have applied the method described in (Shen and Joshi, 2003), to map SVM’s results to probabilities </context>
<context position="22433" citStr="Joachims, 1998" startWordPosition="3761" endWordPosition="3762">rom the ACE 04 training corpus for a blind test. The test set included 2813 names: 1126 persons, 712 GPEs, 785 organizations and 190 locations. The performance is measured via Precision (P), Recall (R) and F-Measure (F). The baseline name tagger is trained from 2978 texts from the People’s Daily news in 1998 and also 1300 texts from ACE training data. The 1,071,285 training samples (pairs of hypotheses) for the re-rankers are obtained from the name tagger applied on the ACE training data, in the manner described in Section 3.2. We use OpenNLP5 for the MaxEnt-Rank experiments. We use SVMlight (Joachims, 1998) for SVMRank, with a linear kernel and the soft margin parameter set to the default value. For the pNorm Push Ranking, we apply 33 weak rankers, i.e., features described in Section 4. The number of iterations was fixed at 110, this number was chosen by optimizing the performance on a development set of 100 documents. 6.2 Effect of Pairwise Sampling We have tried both single-hypothesis and pairwise sampling (described in section 5.1.1) in MaxEntRank and p-Norm Push Ranking. Table 1 shows that pairwise sampling helps both algorithms. MaxEnt-Rank benefited more from it, with precision and recall </context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>Thorsten Joachims. 1998. Making large-scale support vector machine learning practical. Advances in Kernel Methods: Support Vector Machine. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
</authors>
<title>Boosting-based Parse Reranking Derived from Probabilistic Models.</title>
<date>2005</date>
<booktitle>Proc. ACL2005.</booktitle>
<pages>189--196</pages>
<location>Ann Arbor, USA.</location>
<contexts>
<context position="1542" citStr="Kudo et al., 2005" startWordPosition="221" endWordPosition="224">analysis components based on generative models. A baseline generative model produces Nbest candidates, which are then re-ranked using a rich set of local and global features in order to select the best analysis. Various supervised learning algorithms have been adapted to the task of reranking for NLP systems, such as MaxEnt-Rank (Charniak and Johnson, 2005; Ji and Grishman, 2005), SVMRank (Shen and Joshi, 2003), Voted Perceptron (Collins, 2002; Collins and Duffy, 2002; Shen and Joshi, 2004), Kernel Based Methods (Henderson and Titov, 2005), and RankBoost (Collins, 2002; Collins and Koo, 2003; Kudo et al., 2005). These algorithms have been used primarily within the context of a single NLP analysis component, with the most intensive study devoted to improving parsing performance. The re-ranking models for parsing, for example, normally rely on structures generated within the baseline parser itself. Achieving really high performance for some analysis components, however, requires that we take a broader view, one that looks outside a single component in order to bring to bear knowledge from the entire NL analysis process. In this paper we will demonstrate the potential of this approach in enhancing the </context>
</contexts>
<marker>Kudo, Suzuki, Isozaki, 2005</marker>
<rawString>Taku Kudo, Jun Suzuki and Hideki Isozaki. 2005. Boosting-based Parse Reranking Derived from Probabilistic Models. Proc. ACL2005. pp. 189-196. Ann Arbor, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Platt</author>
<author>Nello Cristianini</author>
<author>John Shawe-Taylor</author>
</authors>
<title>Large margin dags for multiclass classification.</title>
<date>2000</date>
<booktitle>Advances in Neural Information Processing Systems 12.</booktitle>
<pages>547--553</pages>
<contexts>
<context position="16982" citStr="Platt et al. 2000" startWordPosition="2763" endWordPosition="2766">orate the pruning techniques used in (Chiang 2005), by which any hypothesis with a probability lower thanαtimes the highest probability for one sentence is discarded. We also discard the pairs very close in performance or probability. 5.1.2 Decoding If f is the ranking function, the MaxEnt model produces a probability for each un-pruned “crucial” pair: prob(f(hi, hj) = 1), i.e., the probability that for the given sentence, hi is a better hypothesis than hj. We need an additional decoding step to select the best hypothesis. Inspired by the caching idea and the multi-class solution proposed by (Platt et al. 2000), we use a dynamic decoding algorithm with complexity O(n) as follows. We scale the probability values into three types: CompareResult (hi, hj) = “better” if prob(f(hi, hj) = 1) &gt;81, “worse” if prob(f(hi, hj) = 1) &lt;82, and “unsure” otherwise, where 81%82. 4 Prune for i = 1 to n Num = 0; for j = 1 to n and jai If CompareResult(hi, hj) = “worse” Num++; if Num&gt;0then discard hi from H Select Initialize: i = 1, j = n while (i&lt;j) if CompareResult(hi, hj) = “better” discard hj from H; j--; else if CompareResult(hi, hj) = “worse” discard hi from H; i++; else break; 4 In the final stage re-ranker we us</context>
</contexts>
<marker>Platt, Cristianini, Shawe-Taylor, 2000</marker>
<rawString>John Platt, Nello Cristianini, and John Shawe-Taylor. 2000. Large margin dags for multiclass classification. Advances in Neural Information Processing Systems 12. pp. 547-553</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>A Linear Programming Formulation for Global Inference in Natural Language Tasks.</title>
<date>2004</date>
<booktitle>Proc. CONLL2004.</booktitle>
<pages>1--8</pages>
<contexts>
<context position="27503" citStr="Roth and Yih (2004)" startWordPosition="4568" endWordPosition="4571">tification and classification, while his only evaluated name identification. Our re-ranker used features from other pipeline stages, while his were limited to local features involving lexical information and &apos;word-shape&apos; in a 5-token window. Since these feature sets are essentially disjoint, it is quite possible that a combination of the two could yield even further improvements. His boosting algorithm is a modification of the method in (Freund et al., 1998), an adaptation of AdaBoost, whereas our p-Norm Push Ranking algorithm can emphasize the hypotheses near the top, matching our objective. Roth and Yih (2004) combined information from named entities and semantic relation tagging, adopting a similar overall goal but using a quite different approach based on linear programming. 7 The features were initially developed and tested using the MaxEnt re-ranker, so it is encouraging that they worked equally well with the p-Norm Push Ranker without further tuning. 55 They limited themselves to name classification, assuming the identification given. This may be a natural subtask for English, where capitalization is a strong indicator of a name, but is much less useful for Chinese, where there is no capitaliz</context>
</contexts>
<marker>Roth, Yih, 2004</marker>
<rawString>Dan Roth and Wen-tau Yih. 2004. A Linear Programming Formulation for Global Inference in Natural Language Tasks. Proc. CONLL2004. pp. 1-8</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>Probabilistic Reasoning for Entity &amp; Relation Recognition.</title>
<date>2002</date>
<booktitle>Proc. COLING2002.</booktitle>
<pages>835--841</pages>
<contexts>
<context position="4658" citStr="Roth and Yih (2002)" startWordPosition="711" endWordPosition="714">gorithm is a generalization of RankBoost (Freund et al. 1998) which concentrates specifically on the top portion of a ranked list. The parameter “p” determines how much the algorithm concentrates at the top. 2.2 Enhancing Named Entity Taggers There have been a very large number of NE tagger implementations since this task was introduced at MUC-6 (Grishman and Sundheim, 1996). Most implementations use local features and a unifying learning algorithm based on, e.g., an HMM, MaxEnt, or SVM. Collins (2002) augmented a baseline NE tagger with a re-ranker that used only local, NE-oriented features. Roth and Yih (2002) combined NE and semantic relation tagging, but within a quite different framework (using a linear programming model for joint inference). 3 A Framework for Name Re-Ranking 3.1 The Information Extraction Pipeline The extraction task we are addressing is that of the Automatic Content Extraction (ACE)1 evaluations. The 2005 ACE evaluation had 7 types of entities, of which the most common were PER (persons), ORG (organizations), LOC (natural locations) and GPE (‘geo-political entities’ – locations which are also political units, such as countries, counties, and cities). There were 6 types of sema</context>
<context position="9435" citStr="Roth and Yih, 2002" startWordPosition="1513" endWordPosition="1516">otherwise f(hi) = -1. This can be considered a special case of re-ranking by score. • Indirect Re-Ranking Function For each “crucial” pair of hypotheses (hi, hj), we learn f : H × H 4 {-1, 1}, such that f(hi, hj) = 1 if hi is better than hj; f (hi, hj) = -1 if hi is worse than hj. We call this “indirect” ranking because we need to apply an additional decoding step to pick the best hypothesis from these pair-wise comparison results. 4 Features for Re-Ranking 4.1 Inferences From Subsequent Stages Information extraction is a potentially symbiotic pipeline with strong dependencies between stages (Roth and Yih, 2002&amp;2004; Ji and Grishman, 2005). Thus, we use features based on the output of four subsequent stages – name structure parsing, relation extraction, event patterns, and coreference analysis – to seek the best hypothesis. We included ten features based on name structure parsing to capture the local information missed by the baseline name tagger such as details of the structure of Chinese person names. The relation and event re-ranking features are based on matching patterns of words or constituents. They serve to correct name boundary errors (because such errors would prevent some patterns from ma</context>
</contexts>
<marker>Roth, Yih, 2002</marker>
<rawString>Dan Roth and Wen-tau Yih. 2002. Probabilistic Reasoning for Entity &amp; Relation Recognition. Proc. COLING2002. pp. 835-841</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia Rudin</author>
</authors>
<title>Ranking with a p-Norm Push.</title>
<date>2006</date>
<booktitle>Proc. Nineteenth Annual Conference on Computational Learning Theory (CoLT</booktitle>
<location>Pittsburgh, Pennsylvania.</location>
<contexts>
<context position="4030" citStr="Rudin, 2006" startWordPosition="611" endWordPosition="612">uistics cation tasks, so it is natural to apply it to reranking problems. (Charniak and Johnson, 2005) applied MaxEnt to improve the performance of a state-of-art parser; also in (Ji and Grishman, 2005) we used it to improve a Chinese name tagger. Using SVMRank, (Shen and Joshi, 2003) achieved significant improvement on parse reranking. They compared two different sample creation methods, and presented an efficient training method by separating the training samples into subsets. The last approach we consider is a boostingstyle approach. We implement a new algorithm called p-Norm Push Ranking (Rudin, 2006). This algorithm is a generalization of RankBoost (Freund et al. 1998) which concentrates specifically on the top portion of a ranked list. The parameter “p” determines how much the algorithm concentrates at the top. 2.2 Enhancing Named Entity Taggers There have been a very large number of NE tagger implementations since this task was introduced at MUC-6 (Grishman and Sundheim, 1996). Most implementations use local features and a unifying learning algorithm based on, e.g., an HMM, MaxEnt, or SVM. Collins (2002) augmented a baseline NE tagger with a re-ranker that used only local, NE-oriented f</context>
<context position="19104" citStr="Rudin, 2006" startWordPosition="3151" endWordPosition="3152">ve applied the method described in (Shen and Joshi, 2003), to map SVM’s results to probabilities via a sigmoid. Thus from the kth SVM, we get the probability for each pair of hypotheses: prob(fk (hi, hj ) =1) , namely the probability of hi being better than hj. Then combining all k SVMs’ results we get: Z hi h j ( , ) =∏ prob fk hi h j ( ( , ) =1) . k So the hypothesis hi with maximal value is chosen as the top hypothesis: arg max(∏ Z(hi , h j )) .hi j 5.3 P-Norm Push Ranking The third algorithm we have tried is a general boosting-style supervised ranking algorithm called p-Norm Push Ranking (Rudin, 2006). We describe this algorithm in more detail since it is quite new and we do not expect many readers to be familiar with it. The parameter determines how much emphasis (or is placed closer to the top of the ranked list, where The p-Norm Push Ranking algorithm generalizes RankBoost (take p=1 for RankBoost). When p is set at a large value, the rankings at the top of the list are given higher priority (a large at the expense of possibly making misranks towards the bott “p” “push”) p≥1. “push”), om of the list. 53 Since for our application, we do not care about the rankings at the bottom of the lis</context>
</contexts>
<marker>Rudin, 2006</marker>
<rawString>Cynthia Rudin. 2006. Ranking with a p-Norm Push. Proc. Nineteenth Annual Conference on Computational Learning Theory (CoLT 2006), Pittsburgh, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Aravind K Joshi</author>
</authors>
<title>An SVM Based Voting Algorithm with Application to Parse ReRanking.</title>
<date>2003</date>
<booktitle>Proc. HLT-NAACL</booktitle>
<pages>9--16</pages>
<contexts>
<context position="1338" citStr="Shen and Joshi, 2003" startWordPosition="189" endWordPosition="192">w the benefit of multi-stage re-ranking for cross-sentence and crossdocument inference. 1 Introduction In recent years, re-ranking techniques have been successfully applied to enhance the performance of NLP analysis components based on generative models. A baseline generative model produces Nbest candidates, which are then re-ranked using a rich set of local and global features in order to select the best analysis. Various supervised learning algorithms have been adapted to the task of reranking for NLP systems, such as MaxEnt-Rank (Charniak and Johnson, 2005; Ji and Grishman, 2005), SVMRank (Shen and Joshi, 2003), Voted Perceptron (Collins, 2002; Collins and Duffy, 2002; Shen and Joshi, 2004), Kernel Based Methods (Henderson and Titov, 2005), and RankBoost (Collins, 2002; Collins and Koo, 2003; Kudo et al., 2005). These algorithms have been used primarily within the context of a single NLP analysis component, with the most intensive study devoted to improving parsing performance. The re-ranking models for parsing, for example, normally rely on structures generated within the baseline parser itself. Achieving really high performance for some analysis components, however, requires that we take a broader</context>
<context position="3703" citStr="Shen and Joshi, 2003" startWordPosition="560" endWordPosition="563">ese algorithms for re-ranking in the context of name tagging. Maximum Entropy modeling (MaxEnt) has been extremely successful for many NLP classifi49 Workshop on Computationally Hard Problemsand Joint Inference in Speech and Language Processing, pages 49–56, New York City, New York, June 2006. c�2006 Association for Computational Linguistics cation tasks, so it is natural to apply it to reranking problems. (Charniak and Johnson, 2005) applied MaxEnt to improve the performance of a state-of-art parser; also in (Ji and Grishman, 2005) we used it to improve a Chinese name tagger. Using SVMRank, (Shen and Joshi, 2003) achieved significant improvement on parse reranking. They compared two different sample creation methods, and presented an efficient training method by separating the training samples into subsets. The last approach we consider is a boostingstyle approach. We implement a new algorithm called p-Norm Push Ranking (Rudin, 2006). This algorithm is a generalization of RankBoost (Freund et al. 1998) which concentrates specifically on the top portion of a ranked list. The parameter “p” determines how much the algorithm concentrates at the top. 2.2 Enhancing Named Entity Taggers There have been a ver</context>
<context position="18549" citStr="Shen and Joshi, 2003" startWordPosition="3036" endWordPosition="3039">alization error. We use the SVMLight package (Joachims, 1998), with the pairwise sampling scheme as for MaxEnt-Rank. In addition we made the following adaptations: we calibrated the SVM outputs, and separated the data into subsets. To speed up training, we divided our training samples into k subsets. Each subset contains N(N1)/k pairs of hypotheses of each sentence. In order to combine the results from these different SVMs, we must calibrate the function values; the output of an SVM yields a distance to the separating hyperplane, but not a probability. We have applied the method described in (Shen and Joshi, 2003), to map SVM’s results to probabilities via a sigmoid. Thus from the kth SVM, we get the probability for each pair of hypotheses: prob(fk (hi, hj ) =1) , namely the probability of hi being better than hj. Then combining all k SVMs’ results we get: Z hi h j ( , ) =∏ prob fk hi h j ( ( , ) =1) . k So the hypothesis hi with maximal value is chosen as the top hypothesis: arg max(∏ Z(hi , h j )) .hi j 5.3 P-Norm Push Ranking The third algorithm we have tried is a general boosting-style supervised ranking algorithm called p-Norm Push Ranking (Rudin, 2006). We describe this algorithm in more detail s</context>
</contexts>
<marker>Shen, Joshi, 2003</marker>
<rawString>Libin Shen and Aravind K. Joshi. 2003. An SVM Based Voting Algorithm with Application to Parse ReRanking. Proc. HLT-NAACL 2003 workshop on Analysis of Geographic References. pp. 9-16</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Aravind K Joshi</author>
</authors>
<title>Flexible Margin Selection for Reranking with Full Pairwise Samples.</title>
<date>2004</date>
<booktitle>Proc.IJCNLP2004.</booktitle>
<pages>446--455</pages>
<location>Hainan Island, China.</location>
<contexts>
<context position="1419" citStr="Shen and Joshi, 2004" startWordPosition="201" endWordPosition="204">rence. 1 Introduction In recent years, re-ranking techniques have been successfully applied to enhance the performance of NLP analysis components based on generative models. A baseline generative model produces Nbest candidates, which are then re-ranked using a rich set of local and global features in order to select the best analysis. Various supervised learning algorithms have been adapted to the task of reranking for NLP systems, such as MaxEnt-Rank (Charniak and Johnson, 2005; Ji and Grishman, 2005), SVMRank (Shen and Joshi, 2003), Voted Perceptron (Collins, 2002; Collins and Duffy, 2002; Shen and Joshi, 2004), Kernel Based Methods (Henderson and Titov, 2005), and RankBoost (Collins, 2002; Collins and Koo, 2003; Kudo et al., 2005). These algorithms have been used primarily within the context of a single NLP analysis component, with the most intensive study devoted to improving parsing performance. The re-ranking models for parsing, for example, normally rely on structures generated within the baseline parser itself. Achieving really high performance for some analysis components, however, requires that we take a broader view, one that looks outside a single component in order to bring to bear knowle</context>
</contexts>
<marker>Shen, Joshi, 2004</marker>
<rawString>Libin Shen and Aravind K. Joshi. 2004. Flexible Margin Selection for Reranking with Full Pairwise Samples. Proc.IJCNLP2004. pp. 446-455. Hainan Island, China.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>