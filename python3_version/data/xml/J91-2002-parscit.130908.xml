<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.975372">
How to Encode Semantic Knowledge: A
Method for Meaning Representation and
Computer-Aided Acquisition
</title>
<author confidence="0.997816">
Paola Velardi. Maria Teresa Pazienzat
</author>
<affiliation confidence="0.999257">
Universita of Ancona University of Roma II
</affiliation>
<author confidence="0.428502">
Michela Fasolot
</author>
<sectionHeader confidence="0.32676" genericHeader="abstract">
Cervedomani
</sectionHeader>
<bodyText confidence="0.999387473684211">
Natural language processing will not be able to compete with traditional information retrieval
unless high-coverage techniques are developed. It is commonly agreed that a poor encoding of
the semantic lexicon is the bottleneck of many existing systems. A hand encoding of semantic
knowledge on an extensive basis is not realistic; hence, it is important to devise methods by
which such knowledge can be acquired in part or entirely by a computer. But what type of
semantic knowledge could be automatically learned, from which sources, and by what methods?
This paper explores the above issues and proposes an algorithm to learn syncategorematic
concepts from text exemplars. What is learned about a concept is not its defining features,
such as kinship, but rather its patterns of use.
The knowledge acquisition method is based on learning by observations; observations are
examples of word co-occurrences (collocations) in a large corpus, detected by a morphosyntactic
analyzer. A semantic bias is used to associate collocations with the appropriate meaning relation,
if one exists. Based upon single or multiple examples, the acquired knowledge is then generalized
to create semantic rules on concept uses.
Interactive human intervention is required in the training phase, when the bias is defined
and refined. The duration of this phase depends upon the semantic closure of the sublanguage
on which the experiment is carried out. After training, final approval by a linguist is still
needed for the acquired semantic rules. At the current stage of experimentation of this system,
it is unclear whether and when human supervision could be further reduced.
</bodyText>
<sectionHeader confidence="0.924216" genericHeader="method">
1. Four Important Questions in Lexical Semantics
</sectionHeader>
<bodyText confidence="0.9999605">
In the past few years there has been a growing interest in the field of lexical semantics.
It is now commonly agreed that the the acquisition of the semantic lexicon is a central
issue in computational linguistics. The major contributions on this topic are collected
in Computational Linguistics (1987), Evens (1988), and Zernik (in press).
Research in lexical semantics is, however, rather heterogeneous in scope, methods,
and results. Paraphrasing the famous &amp;quot;What, when and how?&amp;quot; questions introduced by
Kathleen McKeown (1985) in her studies on language generation, we pose four similar
questions that we believe important to understand the consequences of conceptual and
</bodyText>
<footnote confidence="0.893895">
* Instituto di Informatica, via Brecce Bianche, Ancona
t Dipartimento di Elettronica, Roma
corso Stati Uniti, Padova
</footnote>
<note confidence="0.7765665">
C) 1991 Association for Computational Linguistics
Computational Linguistics Volume 17, Number 2
</note>
<bodyText confidence="0.9200072">
linguistic decisions in lexical semantics:
1. VVhy? Why is lexical semantics useful in the first place? Linguists and
psychologists are interested in the study of word senses to shed light on
important aspects of human communication, such as concept formation
and language use. Lexicographers need computational aids to analyze in
a more compact and extensive way word definitions in dictionaries.
Computer scientists need semantics for the purpose of natural language
processing.
•The option and choices in lexical semantics are deeply related to the
ultimate objective of research.1
</bodyText>
<listItem confidence="0.98961975">
2. VVhat? What is encoded in a semantic lexicon? Each word is a world.
Despite the interest that semantics has received from the scholars of
different disciplines since the early history of humanity, a unifying
theory of meaning does not exist. In practice, the type and quality of the
</listItem>
<bodyText confidence="0.950076235294118">
expressed phenomena again depend upon the end user: a psychologist, a
lexicographer, or a computer. For example, much work on lexical
acquisition from dictionaries produces lexical entries in a format that is
clearly useful for lexicographers (see the many examples in Byrd (1987)
and Boguraev (1989)), but their utility for the purpose of automatic
language processing is less evident. In fact, the output of a lexical
analyzer is an entry in which several semantic and syntactic fields are
identified, but many of these fields are raw text.
In psychology and linguistics, semantic knowledge is modeled with very deep,
more or less formal, expressions (see Figure 1 in Section 2). Often semantic mod-
els focus on some very specific aspect of language communication, according to the
scientific interest of a scholar.
In natural language processing (NLP hereafter), lexical entries typically express
linguistic knowledge as commonsensically understood and used by humans. The en-
tries are entirely formatted in some knowledge representation language and can be
manipulated by a computer. Within the field of NLP, many options are still possible,
as summarized in Section 2.
</bodyText>
<listItem confidence="0.998836272727273">
3. Where? What are the sources of lexical semantic knowledge?
• Introspection is often a source of data, no matter what is the
background of the scientist. However, introspection poses
theoretical and implementation problems. Theoretical, because
&amp;quot;different researchers with different theories would observe
different things about their internal thoughts... &amp;quot; (Anderson
1989). Implementation, because consistency becomes a major
problem when the size of the lexicon exceeds a few hundred
entries.
• Psycholinguistic experiments, such as verbal protocols, are more
appropriate, because they produce observable and measurable
</listItem>
<footnote confidence="0.97576375">
1 This is not to say that there is a clear cut among the interests of linguists, psychologists, and computer
scientists. We believe that, just to take one example, a poor knowledge of the results and methods of
lexicography, linguistics, and cognitive science in part motivates the nonstriking success of NLP in
developing large-scale systems (Velardi 1990).
</footnote>
<page confidence="0.998665">
154
</page>
<bodyText confidence="0.4865875">
Velardi et al. Encoding Semantic Knowledge
example 1 from (Leech 1981):
boy = +animate —adult +male
example 2 from (Mekuk 1987):
</bodyText>
<figure confidence="0.833618875">
help =
Y carrying out Z, X uses his resources W in order for W to help
Y to carry out Z; the use of resources by X and the carrying out of Z
by Y are simultaneous
example 3 from (Schank 1972):
throw =
actor PROPELs and object from a source LOCation to a
destination LOCation
</figure>
<figureCaption confidence="0.977713">
Figure 1
</figureCaption>
<subsectionHeader confidence="0.481793">
Examples of conceptual meaning representation in the literature
</subsectionHeader>
<bodyText confidence="0.996426833333333">
data. One of the largest hand-encoded semantic lexicons
(Dahlgren 1989 and Dahlgren, communication to AAAI90
Stanford seminars) was built by asking subjects to freelist
features common to objects. Despite the scientific interest of
such experiments, they cannot be extensively repeated for the
purpose of acquiring several thousand word sense definitions.
</bodyText>
<listItem confidence="0.644482">
• On-line corpora and dictionaries are widely available today and
provide experimental evidence of word uses and word
</listItem>
<bodyText confidence="0.922927214285714">
definitions. The major advantage of on-line resources is that in
principle they provide the basis for very large experiments, even
though at present the methods of analysis are not fully
developed. This paper describes an experiment in lexical
acquisition from corpora.
4. How? The final issue is implementation. Implementation may be thought
of at various levels. It is the hard work of implementing a system in a
real domain, or the more conceptual task of defining a mathematical
framework to manipulate the objects defined within a linguistic model.
Quite obviously the &amp;quot;hows&amp;quot; in the literature are much more than the &amp;quot;wheres&amp;quot;
and &amp;quot;whats&amp;quot;. No classification of lexical semantic methods is attempted here for sake
of brevity. In Section 5 we examine the research that is more closely related to the
problem under examination in this paper, i.e. lexical acquisition for natural language
processing.
</bodyText>
<listItem confidence="0.610336">
2. &amp;quot;What?&amp;quot;: Meaning Representation in a Semantic Lexicon
</listItem>
<bodyText confidence="0.998333666666667">
This section further considers the issue: what knowledge can be encoded in a seman-
tic lexicon? We will not attempt an overall survey of the field of semantics, which
has provided material for many fascinating books; rather, we will focus on the very
practical problem of representing language expressions on a computer, in a way that
can be useful for natural language processing applications, e.g. machine translation,
information retrieval, user-friendly interfaces.
</bodyText>
<page confidence="0.995537">
155
</page>
<note confidence="0.301403">
Computational Linguistics Volume 17, Number 2
</note>
<bodyText confidence="0.999879625">
In the field of NLP, several approaches have been adopted to represent semantic
knowledge. We are not concerned here with semantic languages, which are relatively
well developed, but with meaning representation principles.
A thorough classification of meaning types was attempted by Leech in his book
on semantics (Leech 1981). In surveying the meaning representation styles adopted
in the computational linguistic literature, we found that many implemented natural
language processors adopt one of the following two meaning types for the lexicon:
conceptual (or deep) and collocative (or superficial)
</bodyText>
<listItem confidence="0.999877">
1. Conceptual meaning. Conceptual meaning is the cognitive content of
words; it can be expressed by features or by primitives: conceptual
meaning is &amp;quot;deep&amp;quot; in that it expresses phenomena that are deeply
embedded in language.
2. Collocative meaning. What is communicated through associations between
</listItem>
<bodyText confidence="0.976737321428572">
words or word classes. Collocative meaning is &amp;quot;superficial&amp;quot; in that it
does not seek &amp;quot;the real sense&amp;quot; of a word, but rather &amp;quot;describes&amp;quot; its uses
in everyday language, or in some subworld language (economy,
computers, etc.). It provides more than a simple analysis of
co-occurrences, because it attempts an explanation of word associations
in terms of meaning relations between a lexical item and other items or
classes.
The conceptual vs. collocative distinction in computational linguistics closely cor-
responds to the defining vs. syncategorematic features distinction in psychology.
Syncategorematie concepts (Keil 1989) are those &amp;quot;almost entirely defined by their
pattern of use, and (the) others by almost pure belief.&amp;quot; The use of syncategorematic
concepts is supported by evidence from psycholinguistic studies. Humans more nat-
urally describe word senses with their characteristics and relations with other words
than with kindship and other internal features. Not surprisingly, a similar &amp;quot;natural-
ness&amp;quot; is observed in the examples of collocative meaning descriptions. Examples show
an evident similarity and are easily readable, whereas concept definitions in the con-
ceptual framework are very different in different papers, and more obscure.3
Both conceptual (defining) and collocative (syncategorematic) features are formally
represented in the NLP literature using some subjective, human-produced set of prim-
itives (conceptual dependencies, semantic relations, conceptual categories) on which
there is no shared agreement at the current time. As far as conceptual meaning is
concerned, the quality and quantity of phenomena to be shown in a representation
is subjective as well: the linguist relies mostly on his/her introspection. Collocative
meaning can rely on the solid evidence represented by word associations; the inter-
pretation of an association is subjective, but valid associations are an observable, even
though vast, phenomenon.
In principle, the inferential power of collocative, or surface, meaning represen-
tation is lower than for conceptual meaning, because it does not account for many
</bodyText>
<footnote confidence="0.993395">
2 Since the terms surface semantics, collocative meaning, and syncategorematic features all refer to the
very fact that concepts (in humans and computers) are frequently described by lists of characteristics
and usage types, we will use all the above terms interchangeably.
3 Notice in the examples of collocative meaning descriptions that the &amp;quot;is-a&amp;quot; relation is conceptual, not
collocative, in nature. Kindship relations are very important in NLP because they provide the basis for
inferences in semantic interpretation. However, the classification of word senses in type hierarchies is a
conceptually very complex, almost unsolvable problem.
</footnote>
<page confidence="0.996493">
156
</page>
<note confidence="0.841315">
Velardi et al. Encoding Semantic Knowledge
</note>
<bodyText confidence="0.999900652173913">
important aspects of human communication, such as beliefs, preconditions, and knowl-
edge of cause-effect relations. These phenomena cannot be captured by an analysis of
meaning relations between uttered words in a sentence.
Despite this, collocative meaning has been shown to be a useful knowledge scheme
for a number of computer applications in language processing. In Niremburg (1987),
the validity of this approach is demonstrated for TRANSLATOR, a system used for
machine translation in the computer subworld. A semantic knowledge framework in
the style of collocative meaning is also adopted in Ace (Jacobs 1987), which has been
used by the TRUMP language analyzer in a variety of applications.
In our previous work on semantic knowledge representation (Pazienza 1988, Ve-
lardi 1988, Antonacci 1989) we showed that a semantic dictionary in the style of col-
locative meaning is a powerful basis for semantic interpretation.
The knowledge power provided by the semantic lexicon (about 1000 manually en-
tered definitions) was measured by the capability of the language processor DANTE to
answer a variety of questions concerning previously analyzed sentences (press agency
releases on economics). It was found that, even though the system was unable to per-
form complex inferences, it could successfully answer more than 90% of the questions
(Pazienza 1988).4
Representing word senses and sentences with surface semantics is hence useful
(though not entirely sufficient) for many NLP applications.
An additional and very important advantage of surface semantics is that it makes it
feasible to acquire large lexicons, as discussed in the following sections. &amp;quot;Acquirability&amp;quot;
in our view is extremely important to evaluate a knowledge representation framework.
</bodyText>
<sectionHeader confidence="0.647494" genericHeader="method">
3. &amp;quot;Where?&amp;quot;: Sources for Acquiring Lexical Semantic Knowledge
</sectionHeader>
<bodyText confidence="0.999441214285714">
Acquiring semantic knowledge on a systematic basis is quite a complex task. One
needs not to look at metaphors or idioms to find this; even the interpretation of
apparently simple sentences is riddled with such difficulties that it is difficult to isolate
a piece of the problem. A manual codification of the lexicon is a prohibitive task,
regardless of the framework adopted for semantic knowledge representation; even
when a large team of knowledge enterers is available, consistency and completeness
are a major problem. We believe that automatic or semi-automatic acquisition of the
lexicon is a critical factor in determining how widespread the use of natural language
processors will be in the next few years.
Recently a few methods were presented for computer-aided semantic knowledge
acquisition. The majority of these methods use standard on-line dictionaries as a source
of data.
The information presented in a dictionary has in our view some intrinsic limita-
tions:
</bodyText>
<listItem confidence="0.99828275">
• definitions are often circular; e.g., the definition of a term A may refer to
a term B that in turn points to A;
• definitions are not homogeneous as far as the quality and quantity of
information provided: they can be very sketchy, or give detailed
</listItem>
<footnote confidence="0.9918888">
4 The test was performed over a six-month period on about 50 occasional visitors and staff members of
the IBM Rome scientific center, unaware of the system capabilities and structure. The user would look
at 60 different releases, previously analyzed by the system (or re-analyzed during the demo), and freely
ask questions about the content of these texts. See the referenced papers for examples of sentences and
of (answered and not answered) query types.
</footnote>
<page confidence="0.980119">
157
</page>
<note confidence="0.264021">
Computational Linguistics Volume 17, Number 2
</note>
<bodyText confidence="0.748674">
structural information, or list examples of use-types, or describe more
internal features;
</bodyText>
<listItem confidence="0.899030666666667">
• a dictionary is the result of a conceptualization effort performed by some
human specialist(s); this effort may not be consistent with, or suitable for,
the objectives of an application for which a language processor is built.
</listItem>
<bodyText confidence="0.999975">
A second approach is using corpora rather than human-oriented dictionary entries.
Corpora provide experimental evidence of word uses, word associations, and such
language phenomena as metaphors, idioms, and metonyms. Corpora are a genuine,
&amp;quot;naive&amp;quot; example of language use, whereas dictionaries result from an effort of intro-
spection performed by language experts, i.e. lexicographers. Corpora are more inter-
esting than dictionaries as a source of linguistic knowledge, just as tribes are more
interesting than &amp;quot;civilized&amp;quot; communities in anthropology
The problem and at the same time the advantage of corpora is that they are raw
texts; dictionary entries use some formal notation that facilitates the task of linguistic
data processing.
No computer program may ever be able to derive formatted data from a com-
pletely unformatted source. Hence the ability to extract lexical semantic information
from a corpus depends upon a powerful set of mapping rules between phrasal patterns
and human-produced semantic primitives and relations. In machine learning, this is
referred to as the semantic bias.
There is no evidence of innate conceptual primitives, apart from some very gen-
eral ones (time, animacy, place, etc.), and even on these there is no shared agreement.
We must hence accept the intrinsic limitation of using a bias whose source is the intro-
spection of a single, or of a community of scientists. But even though the symbols we
choose are arbitrary, their role is the prediction of basic statements, i.e. the processing
of NL sentences in a way that is useful to some computational purpose, and should
be evaluated on this ground.
</bodyText>
<sectionHeader confidence="0.709692" genericHeader="method">
4. A Method for the Acquisition and Interpretation of a Semantic Lexicon
</sectionHeader>
<bodyText confidence="0.999953684210526">
Our research on lexical acquisition from corpora started in 1988, when a first version
of the system was built as utility for the DANTE natural language processor (Velardi
1989), a system that analyzes press agency releases on finance and economics. The
current version, described hereafter, is a self-contained tool on which we are running
a large experiment using a nationwide corpus of enterprise descriptions (overall, more
than one million descriptions). The objective is to derive a domain-dependent semantic
lexicon of 10,000 entries to be used for information retrieval in the sub-domain of
agricultural enterprises (Fasolo 1990). The project is a cooperative effort among the
Universities of Ancona and Roma II, and the CERVED, the company that owns and
manages the database of all commercial enterprises registered at the Chambers of
Commerce in Italy.
In the current version, the system is able to acquire syncategorematic concepts,
learning and interpreting patterns of use from text exemplars. Generated lexical entries
are of the type shown in Figure 2 of Section 2. We do not exclude the possibility of
acquiring defining features from dictionary entries and query collections at a later
stage of this project. At present, however, we are interested in fully exploring the
power of collocative semantics for NLP. We are also interested in the analysis of the
linguistic material that is being produced by the system.
In what follows the methodology is described in detail.
</bodyText>
<page confidence="0.981963">
158
</page>
<figure confidence="0.843851388888889">
Velardi et al. Encoding Semantic Knowledge
example 1 from (Velardi 1988)
agreement =
is _a decision_act
participant person, organization
theme transaction
cause communication_exchange
manner interesting important effective ...
example 2 from (Niremburg 1987):
person =
isa creature
agent_of take put find speech-action mental-action
consist_of hand foot...
source_of speech-action
destination_of speech-action
power human
speed slow
mass human
</figure>
<figureCaption confidence="0.957302">
Figure 2
</figureCaption>
<bodyText confidence="0.931603">
Examples of collocative meaning representation in the literature
</bodyText>
<subsectionHeader confidence="0.999713">
4.1 What Is Given
</subsectionHeader>
<bodyText confidence="0.985886913043478">
The input to the system is:
1. a list of syntactic collocates, e.g. subject-verb, verb-object,
noun-preposition-noun, noun-adjective, etc. extracted through
morphologic (Russo 1987) and syntactic analysis of the selected corpus.
The level at which syntactic analysis should be performed to derive
collocates is a matter of debate. In Smadja (1989) it is suggested that
parsing can be avoided by simply examining the neighborhood of a
word w at a distance of ±5. Our experience demonstrates that this
algorithm produces too many collocations, of which a minority are
actually semantically related.
At the other extreme is full syntactic parsing, as performed in Velardi
(1989). This is computationally too expensive for large corpora and fails
to produce useful collocations when sentences are not fully grammatical,
as for example in the agricultural businesses database. A typical text in
this corpus is:
vendita al minuto di legno da costruzione e manufatti in abete
produzione mobili di legno e di metallo
(*) retail sale of wood for construction and hand-manufactured in
fir-tree production furniture of wood and of metal
A better trade-off between speed and accuracy is to enable syntactic
parsing of sentence parts and use some context-dependent heuristics to
cut sentences into clauses (Fasolo 1990). However, the ±5 collocations are
also collected for a reason that will be clarified later on.
</bodyText>
<page confidence="0.98032">
159
</page>
<figure confidence="0.286412">
Computational Linguistics Volume 17, Number 2
</figure>
<listItem confidence="0.751339166666667">
2. A semantic bias. The semantic bias is the kernel of any learning algorithm,
as no system can learn much more than what it already knows (Micalski
1983). This consists of:
(a) a domain-dependent concept hierarchy. This is a many-to-many
mapping from words to word sense names and an ordered list of
conceptual categories.
</listItem>
<bodyText confidence="0.94704995">
The hypothesis of hand-entering a type hierarchy would not
require an unreasonable amount of time, because the task is
comparable to entering a morphologic lexicon (Russo 1987). The
problem is rather a conceptual one. The way humans categorize
concepts in classes is far from being understood. Mere property
inheritance seems to be inadequate at fully modeling
categorization in humans (Lakoff 1987; Rosch 1975), and this
very fact discouraged us from attempting some automatic
hierarchy acquisition in the framework of machine learning
(Gennari 1989). The limitations of current machine learning
approaches when applied to language learning are discussed in
Section 5.
We consider the problem of acquiring type hierachies as an
open issue, to which more thought and more research are being
devoted in our current work.
(b) a set of domain-dependent conceptual relations, and a
many-to-many mapping (synt-sem) between syntactic relations
and the corresponding conceptual relations (see Velardi 1988
and Antonacci 1989 for extensive examples of
syntax-to-semantics mapping);
</bodyText>
<listItem confidence="0.98180125">
(c) a set of coarse-grained selectional restrictions on the use of
conceptual relations, represented by concept-relation-concept
(CRC) triples. CRCs are expressed in Conceptual Graph notation
(Sowa 1984).
</listItem>
<subsectionHeader confidence="0.998069">
4.2 The Output
</subsectionHeader>
<bodyText confidence="0.998965">
The system produces two types of output:
</bodyText>
<listItem confidence="0.999352333333333">
1. a set of fine-grained CRCs, that are clustered around concepts or around
conceptual relations;
2. an average-grained semantic knowledge base, organized in CRC triples.
</listItem>
<bodyText confidence="0.992361">
The semantic knowledge base is acquired from a source sub-corpus and is used, before
a final approval, for the semantic interpretation of sentences in a test-bed sub-corpus.
The semantic interpreter is basically that described in Velardi (1988) and in other
papers, to which the interested reader may refer.
Such terms as &apos;fine,&apos; average,&apos; and &apos;coarse&apos; are obviously fuzzy. To the extent this
makes sense, we ranged the above terms as follows:
i) fine-grained CRC are those in which concepts directly map into content
words (e.g. [COW] &lt;— (PATIENT) &lt;— [BREED]). These CRCs are true
because they are observed in the domain subworld.
</bodyText>
<page confidence="0.986285">
160
</page>
<note confidence="0.887503">
Velardi et al. Encoding Semantic Knowledge
</note>
<listItem confidence="0.824845285714286">
ii) average-grained CRC are those in which concepts are fathers or
grand-fathers of content-word concepts. These CRC are &apos;typically&apos; true,
as they may have a limited number of exceptions observed in the
domain sublanguage (e.g. [ANIMAL] 4- (PATIENT) 4-- [BREED] is
typically true, even though breeding mosquitoes is quite odd).
iii) coarse-grained CRC are those in which concepts are at a higher level in
the taxonomy (e.g. [ACTION] —4 (BENEFICIARY) --4
</listItem>
<bodyText confidence="0.9982306">
[ANIMATE_ENTITY]). They state necessary, but not sufficient, conditions
on the use of conceptual relations.
The notion of &amp;quot;high-level&amp;quot; and &amp;quot;low-level&amp;quot; in a taxonomy is also relative to the ap-
plication domain. For example, in a computer world, the concept COMPUTER_SOFT-
WARE may be rather high-level.
</bodyText>
<subsectionHeader confidence="0.999838">
4.3 Learning Syncategorematic Concepts from Text Exemplars
</subsectionHeader>
<bodyText confidence="0.9754125">
To acquire syncategorematic knowledge on concepts, the algorithm proceeds as fol-
lows: For any syntactic collocate sc(wl,w2):
</bodyText>
<listItem confidence="0.99926625">
1. Restrict the set of conceptual relations that could correspond to the
syntactic collocate using the synt-sem table.
2. Use coarse-grained knowledge and taxonomic knowledge to further
restrict the hypotheses.
3. If no interpretation is found, reject the collocate. If one or more
interpretations is found, put the resulting CRC(s) on a temporary
knowledge base of fine-grained knowledge.
4. Generalize the result by replacing the concepts in the CRC with their
closest supertypes, using the structural overcommitment principle
(Webster 1989). Add the result to a temporary knowledge base of
average-grained knowledge.
5. Repeat steps 1-4 for all the collocates of the same syntactic type, or (user
choice) those including the same word W. Further generalize one step up
in the hierarchy, based on at least three examples.
6. Present the results to the linguist for a final approval, then add to the
permanent knowledge base.
</listItem>
<bodyText confidence="0.996994">
In a first, training phase, the linguist is requested to inspect the system output in
step 3, to verify and refine the semantic bias.
</bodyText>
<subsectionHeader confidence="0.999006">
4.4 A Complete Example
</subsectionHeader>
<bodyText confidence="0.998285285714286">
Figure 3 shows the output of step 3. On the left-hand side of window 1 (ok-sema)
and window 2 (no-sema) flow the syntactic collocates acquired during the syntactic
analysis of the corpus. Syntactic collocates are couples, like adj-noun, noun-verb, verb-
noun, or triples, like verb-prep-noun (G_V_P_N), noun-prep-noun (G_N_P_N) etc. In
Figure 3 the triples are shown with the preposition &amp;quot;in&amp;quot;.
For each collocate, the learner accesses the knowledge provided by the semantic
bias and generates a CRC triple, corresponding to a plausible semantic interpretation
</bodyText>
<page confidence="0.990513">
161
</page>
<table confidence="0.9682986">
Computational Linguistics Volume 17, Number 2
# File Edit Search Windows Fonts Eual Hermes KB DRL Learn li
Ok_sema
0_H_2_H - pesco in ocgutopcoat luogo ensue
o_m_y_il - intervent, in :ntti.i n int.,,vnen L.9._fi9,...nt. setter.
Girl P_N - coltivereftione in serre . coltivere luogo serre_edificio
GJI_PJ4 - coltivere+sione in .erre a coltivore luogo serre_edificie
0_14_2_H - fiore in :erre . flora 1uo90 serro_edificio
021_2_14 - conmerciere in itnlio . conwarciere lunge itelie
Gii_l_N - ellevorevmento in ecgue . eller., luogo coque
G_N_Y_H - fogielo in scatele . fogiolo inclusion. nettle
Gji_lt H - menufotto in abet.. menufette meterie ebete
G_H_P:N - lever, in ft.* = lovoro_eggette moterie ferro
G_pp_Y_N - surgelere in scotele . surgelere luogo scetole
02_2_N - pesters in ...Ivo = p.c.. luogo ecgmo
0_V_,J4 - utili eeeee in esiend. = utilixxere luogo oxienda_edificio
G_V_P_N - utili.... in osienda . utili.xere lung* esiende_edificio
G_V_P_M - surgelere in scetele = surgelere luogo scotele
G_V_P_N - surgelore in scotole . surgelere steto_finele ...tole
No_sema KB Filtro
G_N_P_N : frengitutro in forme . frangitutto_ettivite / ferns. ( 1 ) /* modo &amp;quot;gestire a
G_N_P_N . frengitutte in forme . frengitutto_nmcchino / forme ( 1 ) noleggio&amp;quot; */
G_H_P_H . commrsiere in forme . commerciare / farm.* ( 1 ) f modoCa&apos;,
cji_p_m . gr.,. in negosie = gyn.. / negosio_littivite ( 1 ) —
G_H_P_H . gr.. in negosio .= grono / negosio edifkie ( 1 ) c_svolgere_attivita::_ ,
0H)14 : lever, in fe,ro = levoro ettivite 7 Cerra ( 1 ) c_oggetto_economico::_:: )
0_V:0_5 : produrre in gene. = profiurre I 9.n.., ( 3 ) . _
G_V_P_N : utili.ore in oxiende = utilixxere / oxitnfla_ettivito . 1 /* modo &amp;quot;commercio al r—
G_V_P_H . utilissere in exienda = utilis.m. / osiende_orgenissexione ( 1 )
G V_1_N : utile...ix...* in usiende . utile /&apos; exiendo_ettivite ( 1
G:V_Y_H : utilefifixxere in ...end. = utile / exiendn_edificio ( 1 )
G_V_P_N . utilt+ixzere in exiende = utile / esiendo_orgenissexione ( 1
&apos;30— Sint_sema Filtro —EF-
,—
X modo „,,-
tab_s_s_oCG_V_P_NA;a&apos;,13, :&apos;g
modo,4,8).
X destinazione_beneficiorio
Figure 3
Production of CRC triples from syntactic collocates
</table>
<bodyText confidence="0.97697">
of the syntactic collocate (shown to the right of the &apos;=&apos; in the window ok-sema). If
an interpretation is not found, because the syntactic attachment is not semantically
validated, the collocate is shown in the window no-sema, i.e., it is rejected.&apos;
An example may clarify the steps:
</bodyText>
<listItem confidence="0.96115775">
1. acquired syntactic collocate: &amp;quot;coltivazione in serra” (farming in
greenhouse)
2. possible subsumed conceptual relations (in Italian and in the specific
domain) by the noun-&amp;quot;in&amp;quot;-noun collocation:
</listItem>
<bodyText confidence="0.805158888888889">
luogo (location) (ex.: farming in greenhouse)
stato_finale (final state) (ex.: trasformazione in vino = transformation in
wine)
materia (matter) (ex.: lavoro in ferro = craft in iron)
luogo_figurato (figurative location) (ex.: intervento in settore = intervention
in field)
etc.
5 Window &amp;quot;KB Filtro&amp;quot; shows an excerpt of coarse-grained knowledge of conceptual relations and the
window &amp;quot;sint_sema Filtro&amp;quot; is an excerpt of the synt-sem table.
</bodyText>
<page confidence="0.993524">
162
</page>
<note confidence="0.894737">
Velardi et al. Encoding Semantic Knowledge
</note>
<listItem confidence="0.990740666666667">
3. coarse-grained knowledge used to select the most plausible
interpretation(s), expressed in Conceptual Graph notation:
I. [ACTIVITY] —(LOCATION)--+[PLACE]
2. [CHANGE] —(FINAL_STATE)—(PRODUCT]
3. [ARTIFACT] —(MATTER)--- [MATTER]
4. [ACTIVITY] —(FIGURATIVE_LOCATION)--[AMBIT]
4. generated fine-grained CRC (applying rule 1):
[FARMING]-4(LOCATION) —[GREENHOUSE]
5. generated average-grained CRC:
</listItem>
<bodyText confidence="0.971489625">
[AGRICULTIJRAL_ACTIVITY]—(LOCATION)--[BUILDING_FOR_CULTIVA-
TION]
When the algorithm runs the first several times, the linguist user inspects the fine-
grained output, as shown in Figure 3, to verify a correct partition of the collocates
among the two windows, and a correct interpretation of the semantically plausible
syntactic collocates. Errors are used to refine the bias. The bias is now stable, at least
for what concerns the &apos;known&apos; words (about 5000 root-form words). In our domain,
this took two or three round steps through the algorithm (i.e. run the algorithm,
verify, and correct the bias), for each type of syntactic collocation. We believe that the
(relatively) low semantic ambiguity of the domain sub-language and the availability
of a well-defined set of conceptual relations contributed to the result.
After this first system training phase, the linguist only overviews the average
grained CRCs, which must be tested before final acquisition in the knowledge base.
Whether and how human intervention can still be reduced is unclear at the present
stage of the experiment. Figure 4 shows a generalization session. Given one or more ex-
amples, such as allevamento di pesce (fish breeding) that are interpreted by the PATIENT
(=animate direct object) relation, the system proposes to acquire the rule (shown with
a reverse video in Figure 4):
Rule 1
[BREED1—*(PTNT)-4ANIMAL_FOR_FEEDING]
The user can acquire the rule by clicking on the &amp;quot;Acquisisce&amp;quot; button, or test the rule
before approval. The test is performed by showing the possible implications of that
rule. These are obtained by listing all low-level CRCs with non-zero probability of
occurrence in the corpus,6 shown in the lower window of Figure 4.
In case of exceptions, it is the choice of the linguist to reject the rule (button &amp;quot;Ri-
fiuta&amp;quot;), or to explicitly account for the exception(s) in a negative-example knowledge
base, by marking with an &amp;quot;n&amp;quot; the triples that are not semantically correct. The rule
shown in Figure 4 above generates only correct associations. The subtypes of BREED,
e.g. DRAIN and DRILL, would in principle produce odd, if not totally unreasonable,
associations, such as &amp;quot;fish training.&amp;quot; Such associations, however, are not listed in the
lower window of Figure 4 because the words &amp;quot;fish&amp;quot; and &amp;quot;drill&amp;quot; never occur in the
corpus at a distance of ±5.
</bodyText>
<footnote confidence="0.7140404">
6 For very large corpora, as the one used here, producing the complete list of possible implications
would generate in some case hundreds of examples, especially when both concepts in a CRC triple are
not terminal nodes in the hierarchy. The linguist is therefore presented only with the list of collocates
that have a non-zero probability of occurrence. This list is the list of all the collocations found in the
(sub-)corpus using the +5 algorithm of Smadja (1989).
</footnote>
<page confidence="0.995623">
163
</page>
<table confidence="0.8940888125">
Computational Linguistics Volume 17, Number 2
...,,
_g.
File Edit Search Windows Fonts Emil Hermes KB DRL Learn
Acquisizione RestrizioniSenlantiche
Gruppo sintattico
G_N_P_N= alieuare+nnento dipesce
Relazione da acquisire
(alleuare)-&gt;Ipazientel-&gt;(c_animale_alimentazione)
Generalizzazioni
(alleuere)-&gt;Ipaziente1-“c_animale_alimentazione) 4
(alletrare)-4pazientel-&gt;(pesce) a
lallevare.1-&gt;Ipaziente1-&gt;lc_animale_alimentazionel
( Flitiela ) ( Requisis(e ) (Test) ( Resetta ) (Soapimde)
‘
Generalizza
</table>
<figureCaption confidence="0.778764083333333">
y - allevare paziente pesce. E.
y - allevare paziente suino.
y - allevare paziente caprino.
y - allevare paziente bovino.
y - allevare paziente ovino.
y - allevare paziente cavallo.
y - allevare paziente coniglio.
y - allevare paziente maiale.
y - allevare paziente gallina.
y - allevare paziente polio.
y - allevare paziente magrone.
Figure 4
</figureCaption>
<bodyText confidence="0.513179">
Example rule generalization session with the computer system
</bodyText>
<subsectionHeader confidence="0.980743">
4.5 Discussion
</subsectionHeader>
<bodyText confidence="0.989079111111111">
This section describes the major problems and results of the acquisition algorithm.
4.5.1 Definition of the Semantic Bias. The most difficult task, both in the domain
described in this paper and in the press agency releases domain described in Velardi
(1989), is to define at the appropriate level of generality the selectional restrictions on
conceptual relations. If restrictions are very coarse, they give rise to errors or (more
frequently) to multiple interpretations at the end of step 2 in Section 4.3. If they are
very selective, we get back to the case of a hand-encoded semantic lexicon.
In the first application (economy and finance) we used about 50 conceptual rela-
tions. The language domain was quite rich, and often it was not possible to express a
selectional restriction with a single CRC triple; on the average, 1.5 CRCs per relation
were necessary.
For example, the following are the selectional restrictions for the relation PARTIC-
IPANT:
[HUMAN_ENTITY1&lt;—(PARTICIPANT)—[FLY],[SAIL1,[MEETING],[AGREEMENT]
Examples of phrasal patterns interpreted by this relation are: John flies (to New York);
an agreement between Fiat and Nissan; a contract among the companies, the assembly
of the administrators, etc. (In the third example, notice that the word &amp;quot;contract&amp;quot; was
classified in the economy domain both as an AGREEMENT and as a DOCUMENT.)
</bodyText>
<page confidence="0.996913">
164
</page>
<note confidence="0.905775">
Velardi et al. Encoding Semantic Knowledge
</note>
<bodyText confidence="0.988637256410256">
As for the agricultural enterprises, about 20 conceptual relations could provide
enough expressive power to the semantic representation language. Most relations were
defined by a single, coarse-grained CRC. Only a few required more detailed selectional
restrictions, as for example the PART_OF relation:
[ANIMATE]-&gt;(PART_OF)--JBODY_PART]
[VEGETABLE]-4(PART_OF) [VEGETABLE _PART]
[CONCRETE _OBJ] -&gt;(PART_OF)-4[CONCRETE_OBJ]
The major difficulty here is that a conceptual category is not always available to express
a selectional restriction in a compact form. For example, the third restriction above is
intended to capture patterns such as: &amp;quot;the pages of the book; the engine of the car; the
top of the hill, etc.&amp;quot; To express the relations among word couples such as (top,hill),
(engine,car), (page,book) etc., a better CRC is:
[CONCRETE_OBJ]-4(PART_OF)--[OBJECT_PART]
However, the type hierarchy defined for the agricultural domain does not have a class
named OBJECT_PART, simply because this was not found useful, given the subworld
lexicon. Hence, the third restriction looks very coarse, but turned out to be selective
enough for the sub-language. Errors are of course possible, but remember that: first,
the output of the acquisition process is supervised by a linguist for final approval (step
6 of Section 4.3); second, coarse-grained knowledge is used only for acquisition, not
during the semantic interpretation of sentences. In other words, coarse knowledge is
only a bias from which a more refined semantic knowledge is acquired.
4.5.2 Semantic Ambiguity. A second issue is semantic ambiguity. The system inter-
prets word associations outside a context. This may give rise to several interpreta-
tions for a single syntactic collocate, even though the domain has very little lexical
ambiguity.&apos;
A good example of what could happen in a more general domain is the fol-
lowing: Consider the phrase &amp;quot;... run towards the bank... ,&amp;quot; that gives rise to the
V_towards_N triple &amp;quot;run,towards,bank.&amp;quot; Without a context, two CRCs are created:
[RUN]-(DIR)-4[RIVER_BANK] and [RUN]-(DIR)-+[BANK_BUILDING]. The other
interpretations, such as BANK_ORGANIZATION and BANK_ACTIVITY are rejected
because their association in a &amp;quot;V_towards_N&amp;quot; position gives no plausible interpreta-
tion, if the semantic bias is &amp;quot;smart&amp;quot; enough. Now, the very fact that in the sentence
from which the collocate was extracted &amp;quot;bank&amp;quot; was a river bank rather than a build-
ing simply does not matter. We are learning more than what the sentence was sug-
gesting. In fact, we acquire two use types of the concepts RUN, RIVERBANK and
BANK_ACTIVITY (namely, that it is possible to run in the DIRection where a &apos;bank&amp;quot;
is located), that are perfectly correct and can be used for semantic disambiguation.
When a syntactic collocate is interpreted in a context, as during the semantic analysis
of a sentence, the interpretation algorithm makes it possible to consider simultaneously
</bodyText>
<footnote confidence="0.724074666666667">
7 In fact, the word-to-concept table that maps words into concept names has fewer entries than the
morphologic lexicon. For example, words that end with &amp;quot;zione&amp;quot; (tion) have the same concept type as
the correspondent verb, e.g. production and produce. Other more complex examples of word to
concept mappings are not mentioned for brevity. Ambiguous words are mostly those designating both
an activity and a building where the activity takes place, as detailed in an example later on. But this is
taken care of with a single metonymic rule (Lakoff 1987), rather than replicating the entries.
</footnote>
<page confidence="0.991694">
165
</page>
<note confidence="0.573722">
Computational Linguistics Volume 17, Number 2
</note>
<bodyText confidence="0.978456074074074">
several restrictions (Velardi 1988); for example, the direct-indirect object relations, etc.
It is in sentence interpretation that a full disambiguation is necessary, though not
always possible.
4.5.3 Syntactic Disambiguation. The semantic knowledge base (SKB) acquired by our
system is a large set of selectional restrictions on word uses, expressed by CRCs, that
can be ordered either around concepts or around conceptual relations. The represen-
tation language is Conceptual Graphs, because we believe that this formalism has
several advantages (Velardi 1988). However, standard logic could also be adopted.
The majority of implemented NLP systems use selectional restrictions for syntactic
and semantic disambiguation in a more or less standard way. We claim hence that the
applicability of the algorithm presented in this paper does not depend upon the specific
algorithm used for semantic interpretation, and the method can be adapted with minor
changes to many NLP systems.
For the sake of completeness, we provide hereafter a brief summary of the semantic
algorithm used in the DANTE system and in a system for information retrieval of
agricultural businesses descriptions, focusing on the important problem of syntactic
ambiguity. Details are given in the referenced papers.
The semantic interpreter proceeds bottom-up, in parallel with syntactic analysis. At
the lower level nodes of the tree, it verifies whether it is possible to find the appropriate
concepts and relations that interpret a given syntactic relation between words. While
progressing up toward the root of the tree, it replaces syntactic relations between
phrases by conceptual relations between partial Conceptual Graphs. It does this by
following steps 1 and 2 of the acquisition algorithm (Section 4.3), but in step 2 rather
than using selectional restrictions on conceptual relations (coarse-grained knowledge)
it accesses the SKB. If at some point no interpretation is found, the system backtracks
and selects a different syntactic attachment. For example, consider the following three
sentences:
</bodyText>
<listItem confidence="0.999194">
1. produrre vino in bottiglia (*to produce wine in bottle)
2. vendere uva all&apos;ingrosso (*to sell grapes at wholesale)
3. produrre vino per i soci (*to supply wine for the shareholders).
</listItem>
<bodyText confidence="0.974770916666667">
All the above three phrases give rise to syntactic ambiguity, and precisely: 1:((V-N1)-
prep-N2) or 2:(V-(N1-prep-N2)). For sentence 1 and tree 1, the interpreter first generates
the graph: [PRODUCE]—(OBJ)-4WINE]. Then, a join is attempted between the head
of the graph, [PRODUCE], with the rest of the sentence (in bottle). The preposition
&amp;quot;in&amp;quot; corresponds to several conceptual relations (see the example in Section 4.4), but
the selectional restrictions on concept uses available in the SKB do not suggest any
valid interpretation. As no complete Conceptual Graph can be produced for tree 1,
tree 2 is explored. Tree 2 generates first the graph:
[WINE]--4(LOCATION)—&gt;[BOTTLE]
where the head concept is [WINE]. Then, a join is attempted between [PRODUCE]
and [WINE]. This produces the final graph:
[PRODUCE]—&gt;(OBJ)--[WINE1—(LOCATION)—+[BOTTLE].
</bodyText>
<page confidence="0.996556">
166
</page>
<note confidence="0.940729">
Velardi et al. Encoding Semantic Knowledge
</note>
<bodyText confidence="0.984308171428572">
Through a similar process, tree 1 is selected for sentence 2, that gives the graph:
[SALE] --4 (OBJ)--[GRAPE]
--+ (MANNER)— [WHOLESALE]
In sentence 3, both trees are indeed plausible: the shareholders are the DESTI-
NATION both of the wine and of the supply. In this case, for information retrieval
purposes, it really does not matter which solution is preferred.
4.5.4 A First Evaluation. After some training and changes due to refinements in bias,
the system was used to acquire the SKB for a prototype system for semantic codifi-
cation of a test-bed set of agricultural texts, different from the one used for defining
and testing the bias. The NLP system is described in Fasolo (1990) and is similar to
the one described in Velardi (1988) and Antonacci (1989) and in the other papers on
the DANTE system, except for the use of shallow methods in syntax and the ability
to produce partial interpretations if parts of a text are not understood.
Many parts of this NLP system are still under development, but for what con-
cerns the adequacy of the semantic knowledge base, the results are very encouraging.
Currently, the test runs on about 3000 collocates, but these numbers keep changing.
We plan to produce more accurate statistics after one full-year experimentation. The
following is a very partial discussion of the results, useful for pinpointing the major
problems.
1. About 10% of the syntactic collocates are rejected because one or both
the words were unknown (no morphologic entry). Some of these are
typos or abbreviations, and the others are really unknown. A major
effort is being devoted toward an extension of the morphologic lexicon
and the introduction of error correction algorithms. Recovery procedures
such as the use of semantic information attached to word endings (e.g.
&amp;quot;zione&amp;quot;=&amp;quot;tion&amp;quot; always indicate an action, &amp;quot;x-ficio&amp;quot; is always a building
for making some product x, etc.) can also be introduced.
2. Only about 1-2% correct syntactic collocates produce two interpretations
where one is incorrect in whatever context. This is to some extent
unavoidable, because the selectional restrictions on conceptual relations
have obviously many exceptions, even though the probability of actually
encountering such exceptions is low. It is unclear to what extent the
presence of errors in the semantic knowledge base can induce errors in
sentence interpretation, when multiple constraints are analyzed together.
Currently, errors are purged by the linguist before final acquisition.
</bodyText>
<listItem confidence="0.843290833333333">
3. No incorrect syntactic collocates are erroneously given an interpretation.
4. Some collocates were rejected because they included pronouns.
Anaphoric references are not handled in the current version of the NLP
system.
5. One correct collocate was erroneously rejected showing the need for a
new conceptual relation, FIGURATIVE_LOCATION (see Section 4.4).
</listItem>
<bodyText confidence="0.902028">
There is no doubt that in order to fully validate this experiment we need to analyze
thousands of texts, not hundreds.
</bodyText>
<page confidence="0.981788">
167
</page>
<note confidence="0.555598">
Computational Linguistics Volume 17, Number 2
</note>
<bodyText confidence="0.99863">
This requires: first, that the morphologic lexicon be extended; second, that the
NLP system that uses the acquired semantic knowledge be completed in several parts,
primarily the question-answering module; third, and more importantly, that several
linguistic issues be given more thorough solutions, in particular the definition and
acquisition of conceptual categories.
</bodyText>
<sectionHeader confidence="0.994789" genericHeader="method">
5. Related Research
</sectionHeader>
<bodyText confidence="0.999881833333333">
The method presented in this paper is at the frontier between machine learning and
computational linguistics. It is closely related to two research areas: concept formation
and lexical acquisition.
The similarities and differences of the problem discussed in this paper with that of
concept formation are better understood by comparing our objectives with the formal
tasks of concept formation (Fisher 1985; Gennari 1989):
</bodyText>
<listItem confidence="0.999678">
1. Given: a sequential presentation of instances and their associated
descriptions;
2. Find: a clustering of those instances in categories;
3. Find: an intensional definition for each category;
4. Find: a hierarchical organization for those categories.
</listItem>
<bodyText confidence="0.506573666666667">
Unlike the work on concept formation, instances (words in contexts) are not associated
with descriptions (CRC triples), hence (1) is only in part a given. Conversely, (2) and
(4) are a &amp;quot;given&amp;quot; rather than a &amp;quot;find.&amp;quot; In summary:
</bodyText>
<listItem confidence="0.990183285714286">
1. Given: a sequential presentation of word associations;
2. Given: a many to many mapping from words to concept types;
3. Given: a hierarchical ordering of concept types;
4. Find: for each observation (two associated words), the concept types and
the conceptual relation that interpret that observation (CRC);
5. Find: a definition for each concept that summarizes its instances (derived
CRCs).
</listItem>
<bodyText confidence="0.999944642857143">
The more substantial difference is that the typical application dealt with in concept
formation is clustering natural kinds (animals, biological species, etc.), whereas most of
the terms we deal with are nominal kinds (verbs, etc.) and artifacts. Natural kinds are
more easily described in terms of defining (internal) features, and their origin seem
more basic to their &amp;quot;kindhood&amp;quot; (Keil 1989) than does the origin of nominal kinds.
The latter are better described by their external relations with other objects. In other
words, finding a type hierarchy is less basic but much more difficult for nominal kinds
and artifacts than for natural kinds. This justifies the focus given to the derivation of
concept descriptions rather than to conceptual clustering, even though we are aware
that the hand-entering of the type hierarchy is the major limitation of our algorithm.
A second research area related to our work is lexical acquisition. An up-to-date
survey of the most recent papers in this area is found in Computational Linguistics
(1987) and Zernik (forthcoming). Many of the papers collected in the above two issues
are more relevant to the fields of lexicography and cognition than to NLP. One of the
</bodyText>
<page confidence="0.994959">
168
</page>
<note confidence="0.930879">
Velardi et al. Encoding Semantic Knowledge
</note>
<bodyText confidence="0.999929666666667">
few lexical knowledge acquisition systems for NLP is described in Jacobs (1988) and
Zernik (1989). When an unknown word is encountered, the system uses pre-existing
knowledge of the context in which the word occurred to derive its conceptual category
The context is provided by on-line texts in the economic domain. For example, the
unknown word merger in &amp;quot;another merger offer&amp;quot; is categorized as merger-transaction
using semantic knowledge of the word offer and of pre-analyzed sentences referring to
a previous offer event, as suggested by the word another. This method is interesting but
allows a conceptual typing of unknown words only when everything else is known. It
does not attack the problem of extensive lexical acquisition, but rather that of robust
language processing.
In Binot (1987) prepositional attachments found in dictionary definitions are in-
terpreted in terms of conceptual relations (e.g. WITH=INSTRUMENT) and used to
solve syntactic ambiguity in parsing. The problem is that the information necessary
to disambiguate is often not found, or requires several complex searches through the
dictionary because of circularity and cross-references.
In Smadja (1989) the system EXTRACT, which uses shallow methods to extensively
derive collocates from corpora, is described. The system produces a list of tuples
(w1,w2,0, where w1 and w2 are two co-occurring words and f is the frequency of
appearance in the corpus. No semantic interpretation is attempted, but it is claimed
that mere co-occurrence knowledge can help language generators to correctly handle
collocationally restricted sentences.
</bodyText>
<sectionHeader confidence="0.984435" genericHeader="method">
6. Concluding Remarks
</sectionHeader>
<bodyText confidence="0.9425858">
The algorithm presented in this paper automatically detects co-occurrences in contexts
and provides a semantic interpretation of the meaning relation between co-occurring
words. Interpreted co-occurrences are used to build a semantic lexicon based on col-
locative meaning descriptions. The acquired concepts are syncategorematic; e.g., are
completely defined by their pattern of use. It is assumed that such knowledge is suffi-
cient to produce a surface semantic interpretation of raw text, i.e. a Conceptual Graph
where content words and syntactic relations are replaced by the appropriate concept
types and conceptual relations. This assumption indeed proved reasonable in our pre-
vious work on semantic interpretation, where the same type of semantic knowledge
was hand-entered.
The observation of co-occurrences is language-dependent, context-dependent; the
interpretation algorithm is (to some extent) language-independent, but relies upon
human-derived primitives and relations that are in general context-dependent. No
language model can prove to be objective, or even plausible. In principle, language
rules and primitives do not exist. But even though symbols are arbitrary, their role is
not to mimic human comprehension, but rather to produce some formal description of
raw textual input, in a form that is ultimately useful for some relevant NLP application.
We feel that no human-invented semantic language will ever provide a full inter-
pretation of language phenomena. We also strongly believe that more shallow methods
such as the one discussed in this paper must be devised to give current NLP systems
more breadth, as this will ultimately determine how widespread the use of NLP tech-
nology will be in the near future.
Acknowledgments PRO-ART 1989 and 1990, and in part by the
This work has been in part supported by CERVED.
the European Community, under grant
</bodyText>
<page confidence="0.993354">
169
</page>
<reference confidence="0.98970087394958">
Computational Linguistics Volume 17, Number 2
References
Anderson, J. R. (1989). A Theory of the
Origins of Human Knowledge. Artificial
Intelligence. vol. 31-40, September.
Antonacci, Pazienza, and Russo, Velardi
(1989). A System for Text Analysis and
Lexical Knowledge Acquisition. Data and
Knowledge Engineering. n. 4.
Binot, J. L., and Jensen, K. (1987). A
Semantic Expert Using an On-line
Standard Dictionary. Proceedings of the
IICAI. Milano.
Boguraev, B.; Byrd, R.; Klavans, J.; and Neff,
M. (1989). From Machine Readable
Dictionaries to Lexical Databases. First
Lexical Acquisition Workshop, Zernik ed.,
Detroit.
Byrd, R.; Calzolari, N.; Chodorow, M.;
Klavans, J.; Neff, M.; Rizk, 0. (1987).
Tools and Methods for Computational
Lexicography. Computational Linguistics.
Computational Linguistics special issue on the
Lexicon, Walker, D., Zampolli, A., and
Calzolari, N., eds. July-December, 1987.
Dahlgren, K., and McDowell, J. (1989).
Knowledge Representation for
Commonsense Reasoning with Texts.
Computational Linguistics, vol. 15, n. 3,
September.
Evens, M. (ed.) (1988). Relational Models of
the Lexicon. Cambridge University Press.
Fasolo, M.; Garbuio, L.; and Guarino, N.
(1990). Comprensione di descrizioni di
attivita&apos; economico-produttive espresse in
linguaggio naturale. GULP Conference
on Logic Programming. Padova.
Fisher, D., and Langley, P. (1986).
Conceptual Clustering and Its Relation to
Numerical Taxonomy. Artificial Intelligence
and Statistics. Addison-Wesley.
Gennari, J.; Langley, P.; and Fisher, D.
(1989). Model of Incremental Concept
Formation. Artificial Intelligence,
vol. 31-40, September.
Jacobs, P. (1987). A Knowledge Framework
for Natural Language Analysis, In
Proceedings of IJCAI87. Milano, August.
Jacobs, P. (1988). Making Sense of Lexical
Acquisition, In Proceedings of AAAI88.
St. Paul, August.
Keil, F. (1989). Concepts, Kinds and
Cognitive Development, In Data and
Knowledge Engineering. The MIT Press.
Lakoff, G. (1987). Woman, Fire and Dangerous
Things. The University of Chicago Press.
Leech, Geoffrey. (1981). Semantics: The Study
of Meaning, Second edition, Penguin
Books.
McKeown, K. (1985). Text Generation.
Cambridge University Press.
Michalski, R. S.; Carbonell, J. C.; and
Mitchell, T. M. (1983). Machine Learning
Vol I. Tioga Publishing Company, Palo
Alto.
Mekuk, I., and Polguere, A. (1987). A
Formal Lexicon in Meaning-Text Theory
(or How To Do Lexica with Words).
Computational Linguistics 13(3-4): 261-275.
Niremburg, S., and Raskin, V. (1987). The
Subworld Concept Lexicon and the
Lexicon Management System.
Computational Linguistics.
Pazienza, M. T., and Velardi, P. (1988).
Using a Semantic Knowledge Base to
Support A Natural Language Interface to
a Text Database. 7th International
Conference on Entity-Relationship Approach,
Rome, November 16-18.
Rosch, E. (1975). Cognitive Representation
of Semantic Categories. Journal of
Experimental Psychology 104,192-233.
Russo, M. (1987). A Generative Grammar
Approach for the Morphologic and
Morphosyntactic Analysis of Italian, In
Third Conference of the European Chapter of
the ACL, Copenhagen, April 1-3.
Schank, R. C. (1972). Conceptual
Dependency: A Theory of Natural
Language Understanding, In Cognitive
Psychology, vol. 3.
Sowa, J. E (1984). Conceptual Structures in
Mind and Machine. Addison-Wesley.
Smadja, F. (1989). Lexical Co-occurrence:
The Missing Link, In Literary and
Linguistic Computing, Vol. 4, n. 3.
Velardi, P.; Pazienza, M. T.; and De
Giovanetti, M. (1988). Conceptual Graphs
for the Analysis and Generation of
Sentences, In IBM Journal of R&amp;D, special
issue on language processing, March.
Velardi, P., and Pazienza, M. T. (1989).
Computer-Aided Acquisition of Lexical
Cooccurrences, In Proceedings of the ACL
1989. Vancouver.
Velardi, P. (1990). Why Human Translators
Still Sleep in Peace? (Four Engineering
and Linguistic Gaps in NLP), In Proc. of
COLING 90. Helsinki, August.
Webster, M., and Marcus, M. (1989).
Automatic Acquisition of the Lexical
Semantics of Verbs from Sentence Frames.
In Proceedings of the ACL 1989. Vancouver.
Zernik, U. (1989). Lexicon Acquisition:
Learning from Corpus by Capitalizing on
Lexical Categories, In IJCAI 1989, Detroit.
Zernik, U., ed. (in press) Lexical Acquisition:
Using On-line Resources to Build a Lexicon,
Lawrence Erlbaum.
</reference>
<page confidence="0.997357">
170
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.023971">
<title confidence="0.998905">How to Encode Semantic Knowledge: A Method for Meaning Representation and Computer-Aided Acquisition</title>
<author confidence="0.999619">Maria Teresa Pazienzat</author>
<affiliation confidence="0.999471">Universita of Ancona University of Roma II</affiliation>
<author confidence="0.877577">Michela Fasolot</author>
<abstract confidence="0.990610183076925">Cervedomani Natural language processing will not be able to compete with traditional information retrieval unless high-coverage techniques are developed. It is commonly agreed that a poor encoding of the semantic lexicon is the bottleneck of many existing systems. A hand encoding of semantic knowledge on an extensive basis is not realistic; hence, it is important to devise methods by which such knowledge can be acquired in part or entirely by a computer. But what type of semantic knowledge could be automatically learned, from which sources, and by what methods? This paper explores the above issues and proposes an algorithm to learn syncategorematic concepts from text exemplars. What is learned about a concept is not its defining features, such as kinship, but rather its patterns of use. knowledge acquisition method is based on by observations; are examples of word co-occurrences (collocations) in a large corpus, detected by a morphosyntactic analyzer. A semantic bias is used to associate collocations with the appropriate meaning relation, if one exists. Based upon single or multiple examples, the acquired knowledge is then generalized to create semantic rules on concept uses. Interactive human intervention is required in the training phase, when the bias is defined and refined. The duration of this phase depends upon the semantic closure of the sublanguage on which the experiment is carried out. After training, final approval by a linguist is still needed for the acquired semantic rules. At the current stage of experimentation of this system, it is unclear whether and when human supervision could be further reduced. 1. Four Important Questions in Lexical Semantics In the past few years there has been a growing interest in the field of lexical semantics. It is now commonly agreed that the the acquisition of the semantic lexicon is a central issue in computational linguistics. The major contributions on this topic are collected Linguistics Evens (1988), and Zernik (in press). Research in lexical semantics is, however, rather heterogeneous in scope, methods, and results. Paraphrasing the famous &amp;quot;What, when and how?&amp;quot; questions introduced by Kathleen McKeown (1985) in her studies on language generation, we pose four similar questions that we believe important to understand the consequences of conceptual and * Instituto di Informatica, via Brecce Bianche, Ancona t Dipartimento di Elettronica, Roma corso Stati Uniti, Padova C) 1991 Association for Computational Linguistics Computational Linguistics Volume 17, Number 2 linguistic decisions in lexical semantics: 1. VVhy? Why is lexical semantics useful in the first place? Linguists and psychologists are interested in the study of word senses to shed light on important aspects of human communication, such as concept formation and language use. Lexicographers need computational aids to analyze in a more compact and extensive way word definitions in dictionaries. Computer scientists need semantics for the purpose of natural language processing. •The option and choices in lexical semantics are deeply related to the objective of VVhat? is encoded in a semantic lexicon? Each word is a world. Despite the interest that semantics has received from the scholars of different disciplines since the early history of humanity, a unifying theory of meaning does not exist. In practice, the type and quality of the expressed phenomena again depend upon the end user: a psychologist, a lexicographer, or a computer. For example, much work on lexical acquisition from dictionaries produces lexical entries in a format that is clearly useful for lexicographers (see the many examples in Byrd (1987) and Boguraev (1989)), but their utility for the purpose of automatic language processing is less evident. In fact, the output of a lexical analyzer is an entry in which several semantic and syntactic fields are identified, but many of these fields are raw text. In psychology and linguistics, semantic knowledge is modeled with very deep, more or less formal, expressions (see Figure 1 in Section 2). Often semantic models focus on some very specific aspect of language communication, according to the scientific interest of a scholar. In natural language processing (NLP hereafter), lexical entries typically express linguistic knowledge as commonsensically understood and used by humans. The entries are entirely formatted in some knowledge representation language and can be manipulated by a computer. Within the field of NLP, many options are still possible, as summarized in Section 2. Where? are the sources of lexical semantic knowledge? Introspection often a source of data, no matter what is the background of the scientist. However, introspection poses theoretical and implementation problems. Theoretical, because &amp;quot;different researchers with different theories would observe different things about their internal thoughts... &amp;quot; (Anderson 1989). Implementation, because consistency becomes a major problem when the size of the lexicon exceeds a few hundred entries. Psycholinguistic experiments, as verbal protocols, are more appropriate, because they produce observable and measurable 1 This is not to say that there is a clear cut among the interests of linguists, psychologists, and computer scientists. We believe that, just to take one example, a poor knowledge of the results and methods of lexicography, linguistics, and cognitive science in part motivates the nonstriking success of NLP in developing large-scale systems (Velardi 1990). 154 Velardi et al. Encoding Semantic Knowledge example 1 from (Leech 1981): = —adult +male example 2 from (Mekuk 1987): help = Y carrying out Z, X uses his resources W in order for W to help Y to carry out Z; the use of resources by X and the carrying out of Z by Y are simultaneous example 3 from (Schank 1972): throw = actor PROPELs and object from a source LOCation to a destination LOCation Examples of conceptual meaning representation in the literature data. One of the largest hand-encoded semantic lexicons (Dahlgren 1989 and Dahlgren, communication to AAAI90 Stanford seminars) was built by asking subjects to freelist features common to objects. Despite the scientific interest of such experiments, they cannot be extensively repeated for the purpose of acquiring several thousand word sense definitions. On-line corpora and dictionaries widely available today and provide experimental evidence of word uses and word definitions. The major advantage of on-line resources is that in principle they provide the basis for very large experiments, even though at present the methods of analysis are not fully developed. This paper describes an experiment in lexical acquisition from corpora. final issue is implementation. Implementation may be thought of at various levels. It is the hard work of implementing a system in a real domain, or the more conceptual task of defining a mathematical framework to manipulate the objects defined within a linguistic model. Quite obviously the &amp;quot;hows&amp;quot; in the literature are much more than the &amp;quot;wheres&amp;quot; and &amp;quot;whats&amp;quot;. No classification of lexical semantic methods is attempted here for sake of brevity. In Section 5 we examine the research that is more closely related to the problem under examination in this paper, i.e. lexical acquisition for natural language processing. 2. &amp;quot;What?&amp;quot;: Meaning Representation in a Semantic Lexicon This section further considers the issue: what knowledge can be encoded in a semantic lexicon? We will not attempt an overall survey of the field of semantics, which has provided material for many fascinating books; rather, we will focus on the very practical problem of representing language expressions on a computer, in a way that can be useful for natural language processing applications, e.g. machine translation, information retrieval, user-friendly interfaces. 155 Computational Linguistics Volume 17, Number 2 In the field of NLP, several approaches have been adopted to represent semantic knowledge. We are not concerned here with semantic languages, which are relatively well developed, but with meaning representation principles. A thorough classification of meaning types was attempted by Leech in his book on semantics (Leech 1981). In surveying the meaning representation styles adopted in the computational linguistic literature, we found that many implemented natural language processors adopt one of the following two meaning types for the lexicon: conceptual (or deep) and collocative (or superficial) Conceptual meaning. meaning is the cognitive content of it can be expressed by by meaning is &amp;quot;deep&amp;quot; in that it expresses phenomena that are deeply embedded in language. Collocative meaning. is communicated through associations between words or word classes. Collocative meaning is &amp;quot;superficial&amp;quot; in that it does not seek &amp;quot;the real sense&amp;quot; of a word, but rather &amp;quot;describes&amp;quot; its uses in everyday language, or in some subworld language (economy, computers, etc.). It provides more than a simple analysis of co-occurrences, because it attempts an explanation of word associations terms of relations a lexical item and other items or classes. The conceptual vs. collocative distinction in computational linguistics closely corresponds to the defining vs. syncategorematic features distinction in psychology. Syncategorematie concepts (Keil 1989) are those &amp;quot;almost entirely defined by their pattern of use, and (the) others by almost pure belief.&amp;quot; The use of syncategorematic concepts is supported by evidence from psycholinguistic studies. Humans more naturally describe word senses with their characteristics and relations with other words than with kindship and other internal features. Not surprisingly, a similar &amp;quot;naturalness&amp;quot; is observed in the examples of collocative meaning descriptions. Examples show an evident similarity and are easily readable, whereas concept definitions in the conframework are very different in different papers, and more Both conceptual (defining) and collocative (syncategorematic) features are formally represented in the NLP literature using some subjective, human-produced set of primitives (conceptual dependencies, semantic relations, conceptual categories) on which there is no shared agreement at the current time. As far as conceptual meaning is concerned, the quality and quantity of phenomena to be shown in a representation is subjective as well: the linguist relies mostly on his/her introspection. Collocative meaning can rely on the solid evidence represented by word associations; the interpretation of an association is subjective, but valid associations are an observable, even though vast, phenomenon. In principle, the inferential power of collocative, or surface, meaning representation is lower than for conceptual meaning, because it does not account for many 2 Since the terms surface semantics, collocative meaning, and syncategorematic features all refer to the very fact that concepts (in humans and computers) are frequently described by lists of characteristics and usage types, we will use all the above terms interchangeably. 3 Notice in the examples of collocative meaning descriptions that the &amp;quot;is-a&amp;quot; relation is conceptual, not collocative, in nature. Kindship relations are very important in NLP because they provide the basis for inferences in semantic interpretation. However, the classification of word senses in type hierarchies is a conceptually very complex, almost unsolvable problem. 156 Velardi et al. Encoding Semantic Knowledge of human communication, such as beliefs, preconditions, and knowledge of cause-effect relations. These phenomena cannot be captured by an analysis of meaning relations between uttered words in a sentence. Despite this, collocative meaning has been shown to be a useful knowledge scheme for a number of computer applications in language processing. In Niremburg (1987), the validity of this approach is demonstrated for TRANSLATOR, a system used for machine translation in the computer subworld. A semantic knowledge framework in the style of collocative meaning is also adopted in Ace (Jacobs 1987), which has been used by the TRUMP language analyzer in a variety of applications. In our previous work on semantic knowledge representation (Pazienza 1988, Velardi 1988, Antonacci 1989) we showed that a semantic dictionary in the style of collocative meaning is a powerful basis for semantic interpretation. The knowledge power provided by the semantic lexicon (about 1000 manually entered definitions) was measured by the capability of the language processor DANTE to answer a variety of questions concerning previously analyzed sentences (press agency releases on economics). It was found that, even though the system was unable to perform complex inferences, it could successfully answer more than 90% of the questions Representing word senses and sentences with surface semantics is hence useful (though not entirely sufficient) for many NLP applications. An additional and very important advantage of surface semantics is that it makes it feasible to acquire large lexicons, as discussed in the following sections. &amp;quot;Acquirability&amp;quot; in our view is extremely important to evaluate a knowledge representation framework. 3. &amp;quot;Where?&amp;quot;: Sources for Acquiring Lexical Semantic Knowledge Acquiring semantic knowledge on a systematic basis is quite a complex task. One needs not to look at metaphors or idioms to find this; even the interpretation of apparently simple sentences is riddled with such difficulties that it is difficult to isolate a piece of the problem. A manual codification of the lexicon is a prohibitive task, regardless of the framework adopted for semantic knowledge representation; even a large team of enterers available, consistency and completeness are a major problem. We believe that automatic or semi-automatic acquisition of the lexicon is a critical factor in determining how widespread the use of natural language processors will be in the next few years. Recently a few methods were presented for computer-aided semantic knowledge acquisition. The majority of these methods use standard on-line dictionaries as a source of data. The information presented in a dictionary has in our view some intrinsic limitations: definitions are often the definition of a term A may refer to a term B that in turn points to A; definitions are homogeneous far as the quality and quantity information provided: they can be very sketchy, or give detailed 4 The test was performed over a six-month period on about 50 occasional visitors and staff members of the IBM Rome scientific center, unaware of the system capabilities and structure. The user would look at 60 different releases, previously analyzed by the system (or re-analyzed during the demo), and freely ask questions about the content of these texts. See the referenced papers for examples of sentences and of (answered and not answered) query types. 157 Computational Linguistics Volume 17, Number 2 structural information, or list examples of use-types, or describe more internal features; • a dictionary is the result of a conceptualization effort performed by some human specialist(s); this effort may not be consistent with, or suitable for, the objectives of an application for which a language processor is built. second approach is using than human-oriented dictionary entries. Corpora provide experimental evidence of word uses, word associations, and such language phenomena as metaphors, idioms, and metonyms. Corpora are a genuine, &amp;quot;naive&amp;quot; example of language use, whereas dictionaries result from an effort of introspection performed by language experts, i.e. lexicographers. Corpora are more interesting than dictionaries as a source of linguistic knowledge, just as tribes are more interesting than &amp;quot;civilized&amp;quot; communities in anthropology The problem and at the same time the advantage of corpora is that they are raw texts; dictionary entries use some formal notation that facilitates the task of linguistic data processing. No computer program may ever be able to derive formatted data from a completely unformatted source. Hence the ability to extract lexical semantic information a corpus depends upon a powerful set of rules phrasal patterns and human-produced semantic primitives and relations. In machine learning, this is to as the bias. There is no evidence of innate conceptual primitives, apart from some very general ones (time, animacy, place, etc.), and even on these there is no shared agreement. We must hence accept the intrinsic limitation of using a bias whose source is the introspection of a single, or of a community of scientists. But even though the symbols we choose are arbitrary, their role is the prediction of basic statements, i.e. the processing of NL sentences in a way that is useful to some computational purpose, and should be evaluated on this ground. 4. A Method for the Acquisition and Interpretation of a Semantic Lexicon Our research on lexical acquisition from corpora started in 1988, when a first version of the system was built as utility for the DANTE natural language processor (Velardi 1989), a system that analyzes press agency releases on finance and economics. The current version, described hereafter, is a self-contained tool on which we are running a large experiment using a nationwide corpus of enterprise descriptions (overall, more than one million descriptions). The objective is to derive a domain-dependent semantic lexicon of 10,000 entries to be used for information retrieval in the sub-domain of agricultural enterprises (Fasolo 1990). The project is a cooperative effort among the Universities of Ancona and Roma II, and the CERVED, the company that owns and manages the database of all commercial enterprises registered at the Chambers of Commerce in Italy. In the current version, the system is able to acquire syncategorematic concepts, learning and interpreting patterns of use from text exemplars. Generated lexical entries are of the type shown in Figure 2 of Section 2. We do not exclude the possibility of acquiring defining features from dictionary entries and query collections at a later stage of this project. At present, however, we are interested in fully exploring the power of collocative semantics for NLP. We are also interested in the analysis of the linguistic material that is being produced by the system. In what follows the methodology is described in detail. 158 Velardi et al. Encoding Semantic Knowledge example 1 from (Velardi 1988) agreement = _a organization important effective ... example 2 from (Niremburg 1987): person = put find speech-action mental-action foot... Figure 2 Examples of collocative meaning representation in the literature 4.1 What Is Given The input to the system is: list of syntactic collocates, subject-verb, verb-object, noun-preposition-noun, noun-adjective, etc. extracted through morphologic (Russo 1987) and syntactic analysis of the selected corpus. The level at which syntactic analysis should be performed to derive collocates is a matter of debate. In Smadja (1989) it is suggested that parsing can be avoided by simply examining the neighborhood of a word w at a distance of ±5. Our experience demonstrates that this algorithm produces too many collocations, of which a minority are actually semantically related. At the other extreme is full syntactic parsing, as performed in Velardi (1989). This is computationally too expensive for large corpora and fails to produce useful collocations when sentences are not fully grammatical, as for example in the agricultural businesses database. A typical text in this corpus is: vendita al minuto di legno da costruzione e manufatti in abete produzione mobili di legno e di metallo (*) retail sale of wood for construction and hand-manufactured in fir-tree production furniture of wood and of metal A better trade-off between speed and accuracy is to enable syntactic parsing of sentence parts and use some context-dependent heuristics to cut sentences into clauses (Fasolo 1990). However, the ±5 collocations are also collected for a reason that will be clarified later on. 159 Computational Linguistics Volume 17, Number 2 A bias. semantic bias is the kernel of any learning algorithm, as no system can learn much more than what it already knows (Micalski 1983). This consists of: a domain-dependent hierarchy. is a many-to-many from words to word sense an ordered list of conceptual categories. The hypothesis of hand-entering a type hierarchy would not require an unreasonable amount of time, because the task is comparable to entering a morphologic lexicon (Russo 1987). The problem is rather a conceptual one. The way humans categorize concepts in classes is far from being understood. Mere property inheritance seems to be inadequate at fully modeling categorization in humans (Lakoff 1987; Rosch 1975), and this very fact discouraged us from attempting some automatic hierarchy acquisition in the framework of machine learning (Gennari 1989). The limitations of current machine learning approaches when applied to language learning are discussed in Section 5. We consider the problem of acquiring type hierachies as an open issue, to which more thought and more research are being devoted in our current work. a set of domain-dependent relations, a mapping syntactic relations and the corresponding conceptual relations (see Velardi 1988 and Antonacci 1989 for extensive examples of syntax-to-semantics mapping); a set of coarse-grained restrictions the use of conceptual relations, represented by concept-relation-concept (CRC) triples. CRCs are expressed in Conceptual Graph notation (Sowa 1984). 4.2 The Output The system produces two types of output: 1. a set of fine-grained CRCs, that are clustered around concepts or around conceptual relations; 2. an average-grained semantic knowledge base, organized in CRC triples. The semantic knowledge base is acquired from a source sub-corpus and is used, before a final approval, for the semantic interpretation of sentences in a test-bed sub-corpus. The semantic interpreter is basically that described in Velardi (1988) and in other papers, to which the interested reader may refer. Such terms as &apos;fine,&apos; average,&apos; and &apos;coarse&apos; are obviously fuzzy. To the extent this makes sense, we ranged the above terms as follows: i) fine-grained CRC are those in which concepts directly map into content (e.g. [COW] &lt;— (PATIENT) &lt;— [BREED]). These CRCs are because they are observed in the domain subworld. 160 Velardi et al. Encoding Semantic Knowledge ii) average-grained CRC are those in which concepts are fathers or grand-fathers of content-word concepts. These CRC are &apos;typically&apos; true, as they may have a limited number of exceptions observed in the sublanguage (e.g. [ANIMAL] (PATIENT) 4-- [BREED] is typically true, even though breeding mosquitoes is quite odd). iii) coarse-grained CRC are those in which concepts are at a higher level in the taxonomy (e.g. [ACTION] —4 (BENEFICIARY) --4 [ANIMATE_ENTITY]). They state necessary, but not sufficient, conditions on the use of conceptual relations. The notion of &amp;quot;high-level&amp;quot; and &amp;quot;low-level&amp;quot; in a taxonomy is also relative to the application domain. For example, in a computer world, the concept COMPUTER_SOFT- WARE may be rather high-level. 4.3 Learning Syncategorematic Concepts from Text Exemplars To acquire syncategorematic knowledge on concepts, the algorithm proceeds as follows: For any syntactic collocate sc(wl,w2): 1. Restrict the set of conceptual relations that could correspond to the syntactic collocate using the synt-sem table. 2. Use coarse-grained knowledge and taxonomic knowledge to further restrict the hypotheses. 3. If no interpretation is found, reject the collocate. If one or more interpretations is found, put the resulting CRC(s) on a temporary knowledge base of fine-grained knowledge. 4. Generalize the result by replacing the concepts in the CRC with their closest supertypes, using the structural overcommitment principle (Webster 1989). Add the result to a temporary knowledge base of average-grained knowledge. 5. Repeat steps 1-4 for all the collocates of the same syntactic type, or (user choice) those including the same word W. Further generalize one step up in the hierarchy, based on at least three examples. 6. Present the results to the linguist for a final approval, then add to the permanent knowledge base. In a first, training phase, the linguist is requested to inspect the system output in step 3, to verify and refine the semantic bias. 4.4 A Complete Example Figure 3 shows the output of step 3. On the left-hand side of window 1 (ok-sema) and window 2 (no-sema) flow the syntactic collocates acquired during the syntactic analysis of the corpus. Syntactic collocates are couples, like adj-noun, noun-verb, verbnoun, or triples, like verb-prep-noun (G_V_P_N), noun-prep-noun (G_N_P_N) etc. In Figure 3 the triples are shown with the preposition &amp;quot;in&amp;quot;. For each collocate, the learner accesses the knowledge provided by the semantic bias and generates a CRC triple, corresponding to a plausible semantic interpretation 161 Computational Linguistics Volume 17, Number 2 Ok_sema 0_H_2_H pesco in ocgutopcoat luogo ensue in :ntti.i n int.,,vnen L.9._fi9,...nt. P_N coltivereftione in serre . coltivere luogo serre_edificio GJI_PJ4 coltivere+sione in .erre a coltivore luogo serre_edificie 0_14_2_H fiore in :erre . flora 1uo90 serro_edificio 021_2_14 conmerciere in itnlio . conwarciere lunge itelie Gii_l_N ellevorevmento in ecgue . eller., luogo coque G_N_Y_H fogielo in scatele . fogiolo inclusion. nettle H menufotto menufette meterie ebete G_H_P:N lever, in ft.* = lovoro_eggette moterie ferro G_pp_Y_N surgelere in scotele . surgelere luogo scetole 02_2_N pesters in ...Ivo = p.c.. luogo ecgmo 0_V_,J4 utili eeeee in esiend. = utilixxere luogo oxienda_edificio G_V_P_N utili.... in osienda . utili.xere lung* esiende_edificio G_V_P_M surgelere in scetele = surgelere luogo scotele G_V_P_N surgelore in scotole . surgelere steto_finele ...tole No_sema KB Filtro G_N_P_N : frengitutro in forme . frangitutto_ettivite / ferns. ( 1 ) &amp;quot;gestire a noleggio&amp;quot; */ G_N_P_N . frengitutte in forme . frengitutto_nmcchino / forme ( 1 ) f modoCa&apos;, . commrsiere in forme . commerciare / ) . negosie = gyn.. / negosio_littivite ( 1 ) G_H_P_H . gr.. in negosio .= grono / negosio edifkie ( 1 ) — : lever, in = ettivite ( 1 c_svolgere_attivita::_ , produrre in gene. = profiurre ( ) c_oggetto_economico::_:: ) utili.ore in oxiende = utilixxere / oxitnfla_ettivito . 1 . _ utilissere in exienda = utilis.m. / osiende_orgenissexione ( 1 ) /* &amp;quot;commercio al G V_1_N : utile...ix...* in usiende . utile /&apos; exiendo_ettivite ( 1 : in ...end. = utile / exiendn_edificio ( 1 G_V_P_N . utilt+ixzere in exiende = utile / esiendo_orgenissexione ( 1 Filtro —EF- ,— X modo „,,tab_s_s_oCG_V_P_NA;a&apos;,13, :&apos;g modo,4,8). X destinazione_beneficiorio Production of CRC triples from syntactic collocates of the syntactic collocate (shown to the right of the &apos;=&apos; in the window ok-sema). If an interpretation is not found, because the syntactic attachment is not semantically validated, the collocate is shown in the window no-sema, i.e., it is rejected.&apos; An example may clarify the steps: 1. acquired syntactic collocate: &amp;quot;coltivazione in serra” (farming in greenhouse) 2. possible subsumed conceptual relations (in Italian and in the specific domain) by the noun-&amp;quot;in&amp;quot;-noun collocation: luogo (location) (ex.: farming in greenhouse) stato_finale (final state) (ex.: trasformazione in vino = transformation in wine) materia (matter) (ex.: lavoro in ferro = craft in iron) luogo_figurato (figurative location) (ex.: intervento in settore = intervention in field) etc. 5 Window &amp;quot;KB Filtro&amp;quot; shows an excerpt of coarse-grained knowledge of conceptual relations and the window &amp;quot;sint_sema Filtro&amp;quot; is an excerpt of the synt-sem table. 162 Velardi et al. Encoding Semantic Knowledge 3. coarse-grained knowledge used to select the most plausible interpretation(s), expressed in Conceptual Graph notation: I. [ACTIVITY] —(LOCATION)--+[PLACE] 2. [CHANGE] —(FINAL_STATE)—(PRODUCT] 3. [ARTIFACT] —(MATTER)--- [MATTER] 4. [ACTIVITY] —(FIGURATIVE_LOCATION)--[AMBIT] 4. generated fine-grained CRC (applying rule 1): [FARMING]-4(LOCATION) —[GREENHOUSE] 5. generated average-grained CRC: [AGRICULTIJRAL_ACTIVITY]—(LOCATION)--[BUILDING_FOR_CULTIVA- TION] When the algorithm runs the first several times, the linguist user inspects the finegrained output, as shown in Figure 3, to verify a correct partition of the collocates among the two windows, and a correct interpretation of the semantically plausible syntactic collocates. Errors are used to refine the bias. The bias is now stable, at least for what concerns the &apos;known&apos; words (about 5000 root-form words). In our domain, this took two or three round steps through the algorithm (i.e. run the algorithm, verify, and correct the bias), for each type of syntactic collocation. We believe that the (relatively) low semantic ambiguity of the domain sub-language and the availability of a well-defined set of conceptual relations contributed to the result. After this first system training phase, the linguist only overviews the average grained CRCs, which must be tested before final acquisition in the knowledge base. Whether and how human intervention can still be reduced is unclear at the present stage of the experiment. Figure 4 shows a generalization session. Given one or more exsuch as di pesce breeding) that are interpreted by the PATIENT (=animate direct object) relation, the system proposes to acquire the rule (shown with a reverse video in Figure 4): Rule 1 [BREED1—*(PTNT)-4ANIMAL_FOR_FEEDING] The user can acquire the rule by clicking on the &amp;quot;Acquisisce&amp;quot; button, or test the rule before approval. The test is performed by showing the possible implications of that rule. These are obtained by listing all low-level CRCs with non-zero probability of in the shown in the lower window of Figure 4. In case of exceptions, it is the choice of the linguist to reject the rule (button &amp;quot;Rifiuta&amp;quot;), or to explicitly account for the exception(s) in a negative-example knowledge base, by marking with an &amp;quot;n&amp;quot; the triples that are not semantically correct. The rule shown in Figure 4 above generates only correct associations. The subtypes of BREED, e.g. DRAIN and DRILL, would in principle produce odd, if not totally unreasonable, associations, such as &amp;quot;fish training.&amp;quot; Such associations, however, are not listed in the lower window of Figure 4 because the words &amp;quot;fish&amp;quot; and &amp;quot;drill&amp;quot; never occur in the corpus at a distance of ±5. 6 For very large corpora, as the one used here, producing the complete list of possible implications would generate in some case hundreds of examples, especially when both concepts in a CRC triple are not terminal nodes in the hierarchy. The linguist is therefore presented only with the list of collocates that have a non-zero probability of occurrence. This list is the list of all the collocations found in the (sub-)corpus using the +5 algorithm of Smadja (1989). 163 Computational Linguistics Volume 17, Number 2 ...,, _g. File Edit Search Windows Fonts Emil Hermes KB DRL Learn Acquisizione RestrizioniSenlantiche Gruppo sintattico G_N_P_N= alieuare+nnento dipesce Relazione da acquisire Generalizzazioni (alleuere)-&gt;Ipaziente1-“c_animale_alimentazione) (alletrare)-4pazientel-&gt;(pesce) 4 a lallevare.1-&gt;Ipaziente1-&gt;lc_animale_alimentazionel (Flitiela ( Requisis(e ) (Test)(Resetta (Soapimde) ‘ Generalizza y allevare paziente pesce. y allevare paziente suino. y allevare paziente caprino. y allevare paziente bovino. y allevare paziente ovino. y allevare paziente cavallo. y allevare paziente coniglio. y allevare paziente maiale. y allevare paziente gallina. y allevare paziente polio. y allevare paziente magrone. E. Figure 4 Example rule generalization session with the computer system 4.5 Discussion This section describes the major problems and results of the acquisition algorithm. Definition of the Semantic Bias. most difficult task, both in the domain described in this paper and in the press agency releases domain described in Velardi (1989), is to define at the appropriate level of generality the selectional restrictions on conceptual relations. If restrictions are very coarse, they give rise to errors or (more frequently) to multiple interpretations at the end of step 2 in Section 4.3. If they are very selective, we get back to the case of a hand-encoded semantic lexicon. In the first application (economy and finance) we used about 50 conceptual relations. The language domain was quite rich, and often it was not possible to express a selectional restriction with a single CRC triple; on the average, 1.5 CRCs per relation were necessary. For example, the following are the selectional restrictions for the relation PARTIC- IPANT: [HUMAN_ENTITY1&lt;—(PARTICIPANT)—[FLY],[SAIL1,[MEETING],[AGREEMENT] Examples of phrasal patterns interpreted by this relation are: John flies (to New York); an agreement between Fiat and Nissan; a contract among the companies, the assembly of the administrators, etc. (In the third example, notice that the word &amp;quot;contract&amp;quot; was classified in the economy domain both as an AGREEMENT and as a DOCUMENT.) 164 Velardi et al. Encoding Semantic Knowledge As for the agricultural enterprises, about 20 conceptual relations could provide enough expressive power to the semantic representation language. Most relations were defined by a single, coarse-grained CRC. Only a few required more detailed selectional restrictions, as for example the PART_OF relation: [ANIMATE]-&gt;(PART_OF)--JBODY_PART] [VEGETABLE]-4(PART_OF) [VEGETABLE _PART] [CONCRETE _OBJ] -&gt;(PART_OF)-4[CONCRETE_OBJ] The major difficulty here is that a conceptual category is not always available to express a selectional restriction in a compact form. For example, the third restriction above is intended to capture patterns such as: &amp;quot;the pages of the book; the engine of the car; the top of the hill, etc.&amp;quot; To express the relations among word couples such as (top,hill), (engine,car), (page,book) etc., a better CRC is: [CONCRETE_OBJ]-4(PART_OF)--[OBJECT_PART] However, the type hierarchy defined for the agricultural domain does not have a class named OBJECT_PART, simply because this was not found useful, given the subworld lexicon. Hence, the third restriction looks very coarse, but turned out to be selective enough for the sub-language. Errors are of course possible, but remember that: first, the output of the acquisition process is supervised by a linguist for final approval (step 6 of Section 4.3); second, coarse-grained knowledge is used only for acquisition, not during the semantic interpretation of sentences. In other words, coarse knowledge is only a bias from which a more refined semantic knowledge is acquired. Semantic Ambiguity. second issue is semantic ambiguity. The system interword associations a context. may give rise to several interpretations for a single syntactic collocate, even though the domain has very little lexical ambiguity.&apos; A good example of what could happen in a more general domain is the following: Consider the phrase &amp;quot;... run towards the bank... ,&amp;quot; that gives rise to the V_towards_N triple &amp;quot;run,towards,bank.&amp;quot; Without a context, two CRCs are created: [RUN]-(DIR)-4[RIVER_BANK] and [RUN]-(DIR)-+[BANK_BUILDING]. The other interpretations, such as BANK_ORGANIZATION and BANK_ACTIVITY are rejected because their association in a &amp;quot;V_towards_N&amp;quot; position gives no plausible interpretation, if the semantic bias is &amp;quot;smart&amp;quot; enough. Now, the very fact that in the sentence from which the collocate was extracted &amp;quot;bank&amp;quot; was a river bank rather than a building simply does not matter. We are learning more than what the sentence was suggesting. In fact, we acquire two use types of the concepts RUN, RIVERBANK and BANK_ACTIVITY (namely, that it is possible to run in the DIRection where a &apos;bank&amp;quot; is located), that are perfectly correct and can be used for semantic disambiguation. When a syntactic collocate is interpreted in a context, as during the semantic analysis of a sentence, the interpretation algorithm makes it possible to consider simultaneously 7 In fact, the word-to-concept table that maps words into concept names has fewer entries than the morphologic lexicon. For example, words that end with &amp;quot;zione&amp;quot; (tion) have the same concept type as the correspondent verb, e.g. production and produce. Other more complex examples of word to concept mappings are not mentioned for brevity. Ambiguous words are mostly those designating both an activity and a building where the activity takes place, as detailed in an example later on. But this is taken care of with a single metonymic rule (Lakoff 1987), rather than replicating the entries. 165 Computational Linguistics Volume 17, Number 2 several restrictions (Velardi 1988); for example, the direct-indirect object relations, etc. It is in sentence interpretation that a full disambiguation is necessary, though not always possible. Syntactic Disambiguation. semantic knowledge base (SKB) acquired by our system is a large set of selectional restrictions on word uses, expressed by CRCs, that can be ordered either around concepts or around conceptual relations. The representation language is Conceptual Graphs, because we believe that this formalism has several advantages (Velardi 1988). However, standard logic could also be adopted. The majority of implemented NLP systems use selectional restrictions for syntactic and semantic disambiguation in a more or less standard way. We claim hence that the of the algorithm presented in this paper not depend upon the specific used for semantic interpretation, the method can be adapted with minor changes to many NLP systems. For the sake of completeness, we provide hereafter a brief summary of the semantic algorithm used in the DANTE system and in a system for information retrieval of agricultural businesses descriptions, focusing on the important problem of syntactic ambiguity. Details are given in the referenced papers. The semantic interpreter proceeds bottom-up, in parallel with syntactic analysis. At the lower level nodes of the tree, it verifies whether it is possible to find the appropriate concepts and relations that interpret a given syntactic relation between words. While progressing up toward the root of the tree, it replaces syntactic relations between phrases by conceptual relations between partial Conceptual Graphs. It does this by following steps 1 and 2 of the acquisition algorithm (Section 4.3), but in step 2 rather than using selectional restrictions on conceptual relations (coarse-grained knowledge) it accesses the SKB. If at some point no interpretation is found, the system backtracks and selects a different syntactic attachment. For example, consider the following three sentences: 1. produrre vino in bottiglia (*to produce wine in bottle) 2. vendere uva all&apos;ingrosso (*to sell grapes at wholesale) 3. produrre vino per i soci (*to supply wine for the shareholders). All the above three phrases give rise to syntactic ambiguity, and precisely: 1:((V-N1)prep-N2) or 2:(V-(N1-prep-N2)). For sentence 1 and tree 1, the interpreter first generates the graph: [PRODUCE]—(OBJ)-4WINE]. Then, a join is attempted between the head of the graph, [PRODUCE], with the rest of the sentence (in bottle). The preposition &amp;quot;in&amp;quot; corresponds to several conceptual relations (see the example in Section 4.4), but the selectional restrictions on concept uses available in the SKB do not suggest any valid interpretation. As no complete Conceptual Graph can be produced for tree 1, tree 2 is explored. Tree 2 generates first the graph: [WINE]--4(LOCATION)—&gt;[BOTTLE] where the head concept is [WINE]. Then, a join is attempted between [PRODUCE] and [WINE]. This produces the final graph: [PRODUCE]—&gt;(OBJ)--[WINE1—(LOCATION)—+[BOTTLE]. 166 Velardi et al. Encoding Semantic Knowledge Through a similar process, tree 1 is selected for sentence 2, that gives the graph: [SALE] --4 (OBJ)--[GRAPE] --+ (MANNER)— [WHOLESALE] In sentence 3, both trees are indeed plausible: the shareholders are the DESTI- NATION both of the wine and of the supply. In this case, for information retrieval purposes, it really does not matter which solution is preferred. A First Evaluation. some training and changes due to refinements in bias, the system was used to acquire the SKB for a prototype system for semantic codification of a test-bed set of agricultural texts, different from the one used for defining and testing the bias. The NLP system is described in Fasolo (1990) and is similar to the one described in Velardi (1988) and Antonacci (1989) and in the other papers on the DANTE system, except for the use of shallow methods in syntax and the ability to produce partial interpretations if parts of a text are not understood. Many parts of this NLP system are still under development, but for what concerns the adequacy of the semantic knowledge base, the results are very encouraging. Currently, the test runs on about 3000 collocates, but these numbers keep changing. We plan to produce more accurate statistics after one full-year experimentation. The following is a very partial discussion of the results, useful for pinpointing the major problems. 1. About 10% of the syntactic collocates are rejected because one or both the words were unknown (no morphologic entry). Some of these are typos or abbreviations, and the others are really unknown. A major effort is being devoted toward an extension of the morphologic lexicon</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J R Anderson</author>
</authors>
<title>A Theory of the Origins of Human Knowledge.</title>
<date>1989</date>
<journal>Artificial Intelligence.</journal>
<volume>vol.</volume>
<pages>31--40</pages>
<contexts>
<context position="5224" citStr="Anderson 1989" startWordPosition="793" endWordPosition="794">ically understood and used by humans. The entries are entirely formatted in some knowledge representation language and can be manipulated by a computer. Within the field of NLP, many options are still possible, as summarized in Section 2. 3. Where? What are the sources of lexical semantic knowledge? • Introspection is often a source of data, no matter what is the background of the scientist. However, introspection poses theoretical and implementation problems. Theoretical, because &amp;quot;different researchers with different theories would observe different things about their internal thoughts... &amp;quot; (Anderson 1989). Implementation, because consistency becomes a major problem when the size of the lexicon exceeds a few hundred entries. • Psycholinguistic experiments, such as verbal protocols, are more appropriate, because they produce observable and measurable 1 This is not to say that there is a clear cut among the interests of linguists, psychologists, and computer scientists. We believe that, just to take one example, a poor knowledge of the results and methods of lexicography, linguistics, and cognitive science in part motivates the nonstriking success of NLP in developing large-scale systems (Velardi</context>
</contexts>
<marker>Anderson, 1989</marker>
<rawString>Anderson, J. R. (1989). A Theory of the Origins of Human Knowledge. Artificial Intelligence. vol. 31-40, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pazienza Antonacci</author>
<author>Velardi Russo</author>
</authors>
<title>A System for Text Analysis and Lexical Knowledge Acquisition.</title>
<date>1989</date>
<journal>Data and Knowledge Engineering. n.</journal>
<volume>4</volume>
<marker>Antonacci, Russo, 1989</marker>
<rawString>Antonacci, Pazienza, and Russo, Velardi (1989). A System for Text Analysis and Lexical Knowledge Acquisition. Data and Knowledge Engineering. n. 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Binot</author>
<author>K Jensen</author>
</authors>
<title>A Semantic Expert Using an On-line Standard Dictionary.</title>
<date>1987</date>
<booktitle>Proceedings of the IICAI.</booktitle>
<location>Milano.</location>
<marker>Binot, Jensen, 1987</marker>
<rawString>Binot, J. L., and Jensen, K. (1987). A Semantic Expert Using an On-line Standard Dictionary. Proceedings of the IICAI. Milano.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Boguraev</author>
<author>R Byrd</author>
<author>J Klavans</author>
<author>M Neff</author>
</authors>
<date>1989</date>
<booktitle>From Machine Readable Dictionaries to Lexical Databases. First Lexical Acquisition Workshop, Zernik ed.,</booktitle>
<location>Detroit.</location>
<marker>Boguraev, Byrd, Klavans, Neff, 1989</marker>
<rawString>Boguraev, B.; Byrd, R.; Klavans, J.; and Neff, M. (1989). From Machine Readable Dictionaries to Lexical Databases. First Lexical Acquisition Workshop, Zernik ed., Detroit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Byrd</author>
<author>N Calzolari</author>
<author>M Chodorow</author>
<author>J Klavans</author>
<author>M Neff</author>
<author>Rizk</author>
</authors>
<date>1987</date>
<booktitle>Tools and Methods for Computational Lexicography. Computational Linguistics.</booktitle>
<marker>Byrd, Calzolari, Chodorow, Klavans, Neff, Rizk, 1987</marker>
<rawString>Byrd, R.; Calzolari, N.; Chodorow, M.; Klavans, J.; Neff, M.; Rizk, 0. (1987). Tools and Methods for Computational Lexicography. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<date>1987</date>
<booktitle>Computational Linguistics special issue on the Lexicon,</booktitle>
<editor>Walker, D., Zampolli, A., and Calzolari, N., eds.</editor>
<publisher>July-December,</publisher>
<contexts>
<context position="2219" citStr="(1987)" startWordPosition="341" endWordPosition="341">he experiment is carried out. After training, final approval by a linguist is still needed for the acquired semantic rules. At the current stage of experimentation of this system, it is unclear whether and when human supervision could be further reduced. 1. Four Important Questions in Lexical Semantics In the past few years there has been a growing interest in the field of lexical semantics. It is now commonly agreed that the the acquisition of the semantic lexicon is a central issue in computational linguistics. The major contributions on this topic are collected in Computational Linguistics (1987), Evens (1988), and Zernik (in press). Research in lexical semantics is, however, rather heterogeneous in scope, methods, and results. Paraphrasing the famous &amp;quot;What, when and how?&amp;quot; questions introduced by Kathleen McKeown (1985) in her studies on language generation, we pose four similar questions that we believe important to understand the consequences of conceptual and * Instituto di Informatica, via Brecce Bianche, Ancona t Dipartimento di Elettronica, Roma corso Stati Uniti, Padova C) 1991 Association for Computational Linguistics Computational Linguistics Volume 17, Number 2 linguistic de</context>
<context position="3957" citStr="(1987)" startWordPosition="603" endWordPosition="603">to the ultimate objective of research.1 2. VVhat? What is encoded in a semantic lexicon? Each word is a world. Despite the interest that semantics has received from the scholars of different disciplines since the early history of humanity, a unifying theory of meaning does not exist. In practice, the type and quality of the expressed phenomena again depend upon the end user: a psychologist, a lexicographer, or a computer. For example, much work on lexical acquisition from dictionaries produces lexical entries in a format that is clearly useful for lexicographers (see the many examples in Byrd (1987) and Boguraev (1989)), but their utility for the purpose of automatic language processing is less evident. In fact, the output of a lexical analyzer is an entry in which several semantic and syntactic fields are identified, but many of these fields are raw text. In psychology and linguistics, semantic knowledge is modeled with very deep, more or less formal, expressions (see Figure 1 in Section 2). Often semantic models focus on some very specific aspect of language communication, according to the scientific interest of a scholar. In natural language processing (NLP hereafter), lexical entries</context>
<context position="12352" citStr="(1987)" startWordPosition="1867" endWordPosition="1867">he basis for inferences in semantic interpretation. However, the classification of word senses in type hierarchies is a conceptually very complex, almost unsolvable problem. 156 Velardi et al. Encoding Semantic Knowledge important aspects of human communication, such as beliefs, preconditions, and knowledge of cause-effect relations. These phenomena cannot be captured by an analysis of meaning relations between uttered words in a sentence. Despite this, collocative meaning has been shown to be a useful knowledge scheme for a number of computer applications in language processing. In Niremburg (1987), the validity of this approach is demonstrated for TRANSLATOR, a system used for machine translation in the computer subworld. A semantic knowledge framework in the style of collocative meaning is also adopted in Ace (Jacobs 1987), which has been used by the TRUMP language analyzer in a variety of applications. In our previous work on semantic knowledge representation (Pazienza 1988, Velardi 1988, Antonacci 1989) we showed that a semantic dictionary in the style of collocative meaning is a powerful basis for semantic interpretation. The knowledge power provided by the semantic lexicon (about </context>
<context position="47266" citStr="(1987)" startWordPosition="7219" endWordPosition="7219"> latter are better described by their external relations with other objects. In other words, finding a type hierarchy is less basic but much more difficult for nominal kinds and artifacts than for natural kinds. This justifies the focus given to the derivation of concept descriptions rather than to conceptual clustering, even though we are aware that the hand-entering of the type hierarchy is the major limitation of our algorithm. A second research area related to our work is lexical acquisition. An up-to-date survey of the most recent papers in this area is found in Computational Linguistics (1987) and Zernik (forthcoming). Many of the papers collected in the above two issues are more relevant to the fields of lexicography and cognition than to NLP. One of the 168 Velardi et al. Encoding Semantic Knowledge few lexical knowledge acquisition systems for NLP is described in Jacobs (1988) and Zernik (1989). When an unknown word is encountered, the system uses pre-existing knowledge of the context in which the word occurred to derive its conceptual category The context is provided by on-line texts in the economic domain. For example, the unknown word merger in &amp;quot;another merger offer&amp;quot; is categ</context>
</contexts>
<marker>1987</marker>
<rawString>Computational Linguistics special issue on the Lexicon, Walker, D., Zampolli, A., and Calzolari, N., eds. July-December, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Dahlgren</author>
<author>J McDowell</author>
</authors>
<title>Knowledge Representation for Commonsense Reasoning with Texts.</title>
<date>1989</date>
<journal>Computational Linguistics,</journal>
<volume>15</volume>
<marker>Dahlgren, McDowell, 1989</marker>
<rawString>Dahlgren, K., and McDowell, J. (1989). Knowledge Representation for Commonsense Reasoning with Texts. Computational Linguistics, vol. 15, n. 3, September.</rawString>
</citation>
<citation valid="true">
<title>Relational Models of the Lexicon.</title>
<date>1988</date>
<editor>Evens, M. (ed.)</editor>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="2233" citStr="(1988)" startWordPosition="343" endWordPosition="343">is carried out. After training, final approval by a linguist is still needed for the acquired semantic rules. At the current stage of experimentation of this system, it is unclear whether and when human supervision could be further reduced. 1. Four Important Questions in Lexical Semantics In the past few years there has been a growing interest in the field of lexical semantics. It is now commonly agreed that the the acquisition of the semantic lexicon is a central issue in computational linguistics. The major contributions on this topic are collected in Computational Linguistics (1987), Evens (1988), and Zernik (in press). Research in lexical semantics is, however, rather heterogeneous in scope, methods, and results. Paraphrasing the famous &amp;quot;What, when and how?&amp;quot; questions introduced by Kathleen McKeown (1985) in her studies on language generation, we pose four similar questions that we believe important to understand the consequences of conceptual and * Instituto di Informatica, via Brecce Bianche, Ancona t Dipartimento di Elettronica, Roma corso Stati Uniti, Padova C) 1991 Association for Computational Linguistics Computational Linguistics Volume 17, Number 2 linguistic decisions in lex</context>
<context position="23105" citStr="(1988)" startWordPosition="3523" endWordPosition="3523">ceptual relations, represented by concept-relation-concept (CRC) triples. CRCs are expressed in Conceptual Graph notation (Sowa 1984). 4.2 The Output The system produces two types of output: 1. a set of fine-grained CRCs, that are clustered around concepts or around conceptual relations; 2. an average-grained semantic knowledge base, organized in CRC triples. The semantic knowledge base is acquired from a source sub-corpus and is used, before a final approval, for the semantic interpretation of sentences in a test-bed sub-corpus. The semantic interpreter is basically that described in Velardi (1988) and in other papers, to which the interested reader may refer. Such terms as &apos;fine,&apos; average,&apos; and &apos;coarse&apos; are obviously fuzzy. To the extent this makes sense, we ranged the above terms as follows: i) fine-grained CRC are those in which concepts directly map into content words (e.g. [COW] &lt;— (PATIENT) &lt;— [BREED]). These CRCs are true because they are observed in the domain subworld. 160 Velardi et al. Encoding Semantic Knowledge ii) average-grained CRC are those in which concepts are fathers or grand-fathers of content-word concepts. These CRC are &apos;typically&apos; true, as they may have a limited</context>
<context position="42248" citStr="(1988)" startWordPosition="6451" endWordPosition="6451">LE] In sentence 3, both trees are indeed plausible: the shareholders are the DESTINATION both of the wine and of the supply. In this case, for information retrieval purposes, it really does not matter which solution is preferred. 4.5.4 A First Evaluation. After some training and changes due to refinements in bias, the system was used to acquire the SKB for a prototype system for semantic codification of a test-bed set of agricultural texts, different from the one used for defining and testing the bias. The NLP system is described in Fasolo (1990) and is similar to the one described in Velardi (1988) and Antonacci (1989) and in the other papers on the DANTE system, except for the use of shallow methods in syntax and the ability to produce partial interpretations if parts of a text are not understood. Many parts of this NLP system are still under development, but for what concerns the adequacy of the semantic knowledge base, the results are very encouraging. Currently, the test runs on about 3000 collocates, but these numbers keep changing. We plan to produce more accurate statistics after one full-year experimentation. The following is a very partial discussion of the results, useful for </context>
<context position="47558" citStr="(1988)" startWordPosition="7267" endWordPosition="7267">than to conceptual clustering, even though we are aware that the hand-entering of the type hierarchy is the major limitation of our algorithm. A second research area related to our work is lexical acquisition. An up-to-date survey of the most recent papers in this area is found in Computational Linguistics (1987) and Zernik (forthcoming). Many of the papers collected in the above two issues are more relevant to the fields of lexicography and cognition than to NLP. One of the 168 Velardi et al. Encoding Semantic Knowledge few lexical knowledge acquisition systems for NLP is described in Jacobs (1988) and Zernik (1989). When an unknown word is encountered, the system uses pre-existing knowledge of the context in which the word occurred to derive its conceptual category The context is provided by on-line texts in the economic domain. For example, the unknown word merger in &amp;quot;another merger offer&amp;quot; is categorized as merger-transaction using semantic knowledge of the word offer and of pre-analyzed sentences referring to a previous offer event, as suggested by the word another. This method is interesting but allows a conceptual typing of unknown words only when everything else is known. It does </context>
</contexts>
<marker>1988</marker>
<rawString>Evens, M. (ed.) (1988). Relational Models of the Lexicon. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Fasolo</author>
<author>L Garbuio</author>
<author>N Guarino</author>
</authors>
<title>Comprensione di descrizioni di attivita&apos; economico-produttive espresse in linguaggio naturale.</title>
<date>1990</date>
<booktitle>GULP Conference on Logic Programming.</booktitle>
<location>Padova.</location>
<marker>Fasolo, Garbuio, Guarino, 1990</marker>
<rawString>Fasolo, M.; Garbuio, L.; and Guarino, N. (1990). Comprensione di descrizioni di attivita&apos; economico-produttive espresse in linguaggio naturale. GULP Conference on Logic Programming. Padova.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Fisher</author>
<author>P Langley</author>
</authors>
<title>Conceptual Clustering and Its Relation to Numerical Taxonomy. Artificial Intelligence and Statistics.</title>
<date>1986</date>
<publisher>Addison-Wesley.</publisher>
<marker>Fisher, Langley, 1986</marker>
<rawString>Fisher, D., and Langley, P. (1986). Conceptual Clustering and Its Relation to Numerical Taxonomy. Artificial Intelligence and Statistics. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gennari</author>
<author>P Langley</author>
<author>D Fisher</author>
</authors>
<title>Model of Incremental Concept Formation.</title>
<date>1989</date>
<journal>Artificial Intelligence,</journal>
<volume>vol.</volume>
<pages>31--40</pages>
<marker>Gennari, Langley, Fisher, 1989</marker>
<rawString>Gennari, J.; Langley, P.; and Fisher, D. (1989). Model of Incremental Concept Formation. Artificial Intelligence, vol. 31-40, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Jacobs</author>
</authors>
<title>A Knowledge Framework for Natural Language Analysis,</title>
<date>1987</date>
<booktitle>In Proceedings of IJCAI87.</booktitle>
<location>Milano,</location>
<contexts>
<context position="12583" citStr="Jacobs 1987" startWordPosition="1902" endWordPosition="1903">rtant aspects of human communication, such as beliefs, preconditions, and knowledge of cause-effect relations. These phenomena cannot be captured by an analysis of meaning relations between uttered words in a sentence. Despite this, collocative meaning has been shown to be a useful knowledge scheme for a number of computer applications in language processing. In Niremburg (1987), the validity of this approach is demonstrated for TRANSLATOR, a system used for machine translation in the computer subworld. A semantic knowledge framework in the style of collocative meaning is also adopted in Ace (Jacobs 1987), which has been used by the TRUMP language analyzer in a variety of applications. In our previous work on semantic knowledge representation (Pazienza 1988, Velardi 1988, Antonacci 1989) we showed that a semantic dictionary in the style of collocative meaning is a powerful basis for semantic interpretation. The knowledge power provided by the semantic lexicon (about 1000 manually entered definitions) was measured by the capability of the language processor DANTE to answer a variety of questions concerning previously analyzed sentences (press agency releases on economics). It was found that, ev</context>
</contexts>
<marker>Jacobs, 1987</marker>
<rawString>Jacobs, P. (1987). A Knowledge Framework for Natural Language Analysis, In Proceedings of IJCAI87. Milano, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Jacobs</author>
</authors>
<title>Making Sense of Lexical Acquisition,</title>
<date>1988</date>
<booktitle>In Proceedings of AAAI88.</booktitle>
<publisher>St. Paul,</publisher>
<contexts>
<context position="47558" citStr="Jacobs (1988)" startWordPosition="7266" endWordPosition="7267">rather than to conceptual clustering, even though we are aware that the hand-entering of the type hierarchy is the major limitation of our algorithm. A second research area related to our work is lexical acquisition. An up-to-date survey of the most recent papers in this area is found in Computational Linguistics (1987) and Zernik (forthcoming). Many of the papers collected in the above two issues are more relevant to the fields of lexicography and cognition than to NLP. One of the 168 Velardi et al. Encoding Semantic Knowledge few lexical knowledge acquisition systems for NLP is described in Jacobs (1988) and Zernik (1989). When an unknown word is encountered, the system uses pre-existing knowledge of the context in which the word occurred to derive its conceptual category The context is provided by on-line texts in the economic domain. For example, the unknown word merger in &amp;quot;another merger offer&amp;quot; is categorized as merger-transaction using semantic knowledge of the word offer and of pre-analyzed sentences referring to a previous offer event, as suggested by the word another. This method is interesting but allows a conceptual typing of unknown words only when everything else is known. It does </context>
</contexts>
<marker>Jacobs, 1988</marker>
<rawString>Jacobs, P. (1988). Making Sense of Lexical Acquisition, In Proceedings of AAAI88. St. Paul, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Keil</author>
</authors>
<title>Concepts, Kinds and Cognitive Development,</title>
<date>1989</date>
<booktitle>In Data and Knowledge Engineering.</booktitle>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="9787" citStr="Keil 1989" startWordPosition="1492" endWordPosition="1493">d classes. Collocative meaning is &amp;quot;superficial&amp;quot; in that it does not seek &amp;quot;the real sense&amp;quot; of a word, but rather &amp;quot;describes&amp;quot; its uses in everyday language, or in some subworld language (economy, computers, etc.). It provides more than a simple analysis of co-occurrences, because it attempts an explanation of word associations in terms of meaning relations between a lexical item and other items or classes. The conceptual vs. collocative distinction in computational linguistics closely corresponds to the defining vs. syncategorematic features distinction in psychology. Syncategorematie concepts (Keil 1989) are those &amp;quot;almost entirely defined by their pattern of use, and (the) others by almost pure belief.&amp;quot; The use of syncategorematic concepts is supported by evidence from psycholinguistic studies. Humans more naturally describe word senses with their characteristics and relations with other words than with kindship and other internal features. Not surprisingly, a similar &amp;quot;naturalness&amp;quot; is observed in the examples of collocative meaning descriptions. Examples show an evident similarity and are easily readable, whereas concept definitions in the conceptual framework are very different in different </context>
<context position="46617" citStr="Keil 1989" startWordPosition="7113" endWordPosition="7114">: for each observation (two associated words), the concept types and the conceptual relation that interpret that observation (CRC); 5. Find: a definition for each concept that summarizes its instances (derived CRCs). The more substantial difference is that the typical application dealt with in concept formation is clustering natural kinds (animals, biological species, etc.), whereas most of the terms we deal with are nominal kinds (verbs, etc.) and artifacts. Natural kinds are more easily described in terms of defining (internal) features, and their origin seem more basic to their &amp;quot;kindhood&amp;quot; (Keil 1989) than does the origin of nominal kinds. The latter are better described by their external relations with other objects. In other words, finding a type hierarchy is less basic but much more difficult for nominal kinds and artifacts than for natural kinds. This justifies the focus given to the derivation of concept descriptions rather than to conceptual clustering, even though we are aware that the hand-entering of the type hierarchy is the major limitation of our algorithm. A second research area related to our work is lexical acquisition. An up-to-date survey of the most recent papers in this </context>
</contexts>
<marker>Keil, 1989</marker>
<rawString>Keil, F. (1989). Concepts, Kinds and Cognitive Development, In Data and Knowledge Engineering. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Lakoff</author>
</authors>
<title>Woman, Fire and Dangerous Things.</title>
<date>1987</date>
<publisher>The University of Chicago Press.</publisher>
<contexts>
<context position="21753" citStr="Lakoff 1987" startWordPosition="3326" endWordPosition="3327"> than what it already knows (Micalski 1983). This consists of: (a) a domain-dependent concept hierarchy. This is a many-to-many mapping from words to word sense names and an ordered list of conceptual categories. The hypothesis of hand-entering a type hierarchy would not require an unreasonable amount of time, because the task is comparable to entering a morphologic lexicon (Russo 1987). The problem is rather a conceptual one. The way humans categorize concepts in classes is far from being understood. Mere property inheritance seems to be inadequate at fully modeling categorization in humans (Lakoff 1987; Rosch 1975), and this very fact discouraged us from attempting some automatic hierarchy acquisition in the framework of machine learning (Gennari 1989). The limitations of current machine learning approaches when applied to language learning are discussed in Section 5. We consider the problem of acquiring type hierachies as an open issue, to which more thought and more research are being devoted in our current work. (b) a set of domain-dependent conceptual relations, and a many-to-many mapping (synt-sem) between syntactic relations and the corresponding conceptual relations (see Velardi 1988</context>
<context position="38263" citStr="Lakoff 1987" startWordPosition="5843" endWordPosition="5844">ation algorithm makes it possible to consider simultaneously 7 In fact, the word-to-concept table that maps words into concept names has fewer entries than the morphologic lexicon. For example, words that end with &amp;quot;zione&amp;quot; (tion) have the same concept type as the correspondent verb, e.g. production and produce. Other more complex examples of word to concept mappings are not mentioned for brevity. Ambiguous words are mostly those designating both an activity and a building where the activity takes place, as detailed in an example later on. But this is taken care of with a single metonymic rule (Lakoff 1987), rather than replicating the entries. 165 Computational Linguistics Volume 17, Number 2 several restrictions (Velardi 1988); for example, the direct-indirect object relations, etc. It is in sentence interpretation that a full disambiguation is necessary, though not always possible. 4.5.3 Syntactic Disambiguation. The semantic knowledge base (SKB) acquired by our system is a large set of selectional restrictions on word uses, expressed by CRCs, that can be ordered either around concepts or around conceptual relations. The representation language is Conceptual Graphs, because we believe that th</context>
</contexts>
<marker>Lakoff, 1987</marker>
<rawString>Lakoff, G. (1987). Woman, Fire and Dangerous Things. The University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Leech</author>
</authors>
<title>Semantics: The Study of Meaning, Second edition,</title>
<date>1981</date>
<publisher>Penguin Books.</publisher>
<contexts>
<context position="5906" citStr="Leech 1981" startWordPosition="896" endWordPosition="897">size of the lexicon exceeds a few hundred entries. • Psycholinguistic experiments, such as verbal protocols, are more appropriate, because they produce observable and measurable 1 This is not to say that there is a clear cut among the interests of linguists, psychologists, and computer scientists. We believe that, just to take one example, a poor knowledge of the results and methods of lexicography, linguistics, and cognitive science in part motivates the nonstriking success of NLP in developing large-scale systems (Velardi 1990). 154 Velardi et al. Encoding Semantic Knowledge example 1 from (Leech 1981): boy = +animate —adult +male example 2 from (Mekuk 1987): help = Y carrying out Z, X uses his resources W in order for W to help Y to carry out Z; the use of resources by X and the carrying out of Z by Y are simultaneous example 3 from (Schank 1972): throw = actor PROPELs and object from a source LOCation to a destination LOCation Figure 1 Examples of conceptual meaning representation in the literature data. One of the largest hand-encoded semantic lexicons (Dahlgren 1989 and Dahlgren, communication to AAAI90 Stanford seminars) was built by asking subjects to freelist features common to objec</context>
<context position="8590" citStr="Leech 1981" startWordPosition="1319" endWordPosition="1320">cus on the very practical problem of representing language expressions on a computer, in a way that can be useful for natural language processing applications, e.g. machine translation, information retrieval, user-friendly interfaces. 155 Computational Linguistics Volume 17, Number 2 In the field of NLP, several approaches have been adopted to represent semantic knowledge. We are not concerned here with semantic languages, which are relatively well developed, but with meaning representation principles. A thorough classification of meaning types was attempted by Leech in his book on semantics (Leech 1981). In surveying the meaning representation styles adopted in the computational linguistic literature, we found that many implemented natural language processors adopt one of the following two meaning types for the lexicon: conceptual (or deep) and collocative (or superficial) 1. Conceptual meaning. Conceptual meaning is the cognitive content of words; it can be expressed by features or by primitives: conceptual meaning is &amp;quot;deep&amp;quot; in that it expresses phenomena that are deeply embedded in language. 2. Collocative meaning. What is communicated through associations between words or word classes. Co</context>
</contexts>
<marker>Leech, 1981</marker>
<rawString>Leech, Geoffrey. (1981). Semantics: The Study of Meaning, Second edition, Penguin Books.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McKeown</author>
</authors>
<title>Text Generation.</title>
<date>1985</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="2447" citStr="McKeown (1985)" startWordPosition="372" endWordPosition="373">ervision could be further reduced. 1. Four Important Questions in Lexical Semantics In the past few years there has been a growing interest in the field of lexical semantics. It is now commonly agreed that the the acquisition of the semantic lexicon is a central issue in computational linguistics. The major contributions on this topic are collected in Computational Linguistics (1987), Evens (1988), and Zernik (in press). Research in lexical semantics is, however, rather heterogeneous in scope, methods, and results. Paraphrasing the famous &amp;quot;What, when and how?&amp;quot; questions introduced by Kathleen McKeown (1985) in her studies on language generation, we pose four similar questions that we believe important to understand the consequences of conceptual and * Instituto di Informatica, via Brecce Bianche, Ancona t Dipartimento di Elettronica, Roma corso Stati Uniti, Padova C) 1991 Association for Computational Linguistics Computational Linguistics Volume 17, Number 2 linguistic decisions in lexical semantics: 1. VVhy? Why is lexical semantics useful in the first place? Linguists and psychologists are interested in the study of word senses to shed light on important aspects of human communication, such as</context>
</contexts>
<marker>McKeown, 1985</marker>
<rawString>McKeown, K. (1985). Text Generation. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R S Michalski</author>
<author>J C Carbonell</author>
<author>T M Mitchell</author>
</authors>
<title>Machine Learning Vol I. Tioga Publishing Company,</title>
<date>1983</date>
<location>Palo Alto.</location>
<marker>Michalski, Carbonell, Mitchell, 1983</marker>
<rawString>Michalski, R. S.; Carbonell, J. C.; and Mitchell, T. M. (1983). Machine Learning Vol I. Tioga Publishing Company, Palo Alto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mekuk</author>
<author>A Polguere</author>
</authors>
<title>A Formal Lexicon</title>
<date>1987</date>
<booktitle>in Meaning-Text Theory (or How To Do Lexica with Words). Computational Linguistics 13(3-4):</booktitle>
<pages>261--275</pages>
<marker>Mekuk, Polguere, 1987</marker>
<rawString>Mekuk, I., and Polguere, A. (1987). A Formal Lexicon in Meaning-Text Theory (or How To Do Lexica with Words). Computational Linguistics 13(3-4): 261-275.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Niremburg</author>
<author>V Raskin</author>
</authors>
<title>The Subworld Concept Lexicon and the Lexicon Management System. Computational Linguistics.</title>
<date>1987</date>
<marker>Niremburg, Raskin, 1987</marker>
<rawString>Niremburg, S., and Raskin, V. (1987). The Subworld Concept Lexicon and the Lexicon Management System. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M T Pazienza</author>
<author>P Velardi</author>
</authors>
<title>Using a Semantic Knowledge Base to Support A Natural Language Interface to a Text Database.</title>
<date>1988</date>
<booktitle>7th International Conference on Entity-Relationship Approach,</booktitle>
<pages>16--18</pages>
<location>Rome,</location>
<marker>Pazienza, Velardi, 1988</marker>
<rawString>Pazienza, M. T., and Velardi, P. (1988). Using a Semantic Knowledge Base to Support A Natural Language Interface to a Text Database. 7th International Conference on Entity-Relationship Approach, Rome, November 16-18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Rosch</author>
</authors>
<title>Cognitive Representation of Semantic Categories.</title>
<date>1975</date>
<journal>Journal of Experimental Psychology</journal>
<pages>104--192</pages>
<contexts>
<context position="21766" citStr="Rosch 1975" startWordPosition="3328" endWordPosition="3329"> already knows (Micalski 1983). This consists of: (a) a domain-dependent concept hierarchy. This is a many-to-many mapping from words to word sense names and an ordered list of conceptual categories. The hypothesis of hand-entering a type hierarchy would not require an unreasonable amount of time, because the task is comparable to entering a morphologic lexicon (Russo 1987). The problem is rather a conceptual one. The way humans categorize concepts in classes is far from being understood. Mere property inheritance seems to be inadequate at fully modeling categorization in humans (Lakoff 1987; Rosch 1975), and this very fact discouraged us from attempting some automatic hierarchy acquisition in the framework of machine learning (Gennari 1989). The limitations of current machine learning approaches when applied to language learning are discussed in Section 5. We consider the problem of acquiring type hierachies as an open issue, to which more thought and more research are being devoted in our current work. (b) a set of domain-dependent conceptual relations, and a many-to-many mapping (synt-sem) between syntactic relations and the corresponding conceptual relations (see Velardi 1988 and Antonacc</context>
</contexts>
<marker>Rosch, 1975</marker>
<rawString>Rosch, E. (1975). Cognitive Representation of Semantic Categories. Journal of Experimental Psychology 104,192-233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Russo</author>
</authors>
<title>A Generative Grammar Approach for the Morphologic and Morphosyntactic Analysis of Italian,</title>
<date>1987</date>
<booktitle>In Third Conference of the European Chapter of the ACL,</booktitle>
<pages>1--3</pages>
<location>Copenhagen,</location>
<contexts>
<context position="19760" citStr="Russo 1987" startWordPosition="3010" endWordPosition="3011">rticipant person, organization theme transaction cause communication_exchange manner interesting important effective ... example 2 from (Niremburg 1987): person = isa creature agent_of take put find speech-action mental-action consist_of hand foot... source_of speech-action destination_of speech-action power human speed slow mass human Figure 2 Examples of collocative meaning representation in the literature 4.1 What Is Given The input to the system is: 1. a list of syntactic collocates, e.g. subject-verb, verb-object, noun-preposition-noun, noun-adjective, etc. extracted through morphologic (Russo 1987) and syntactic analysis of the selected corpus. The level at which syntactic analysis should be performed to derive collocates is a matter of debate. In Smadja (1989) it is suggested that parsing can be avoided by simply examining the neighborhood of a word w at a distance of ±5. Our experience demonstrates that this algorithm produces too many collocations, of which a minority are actually semantically related. At the other extreme is full syntactic parsing, as performed in Velardi (1989). This is computationally too expensive for large corpora and fails to produce useful collocations when se</context>
<context position="21531" citStr="Russo 1987" startWordPosition="3292" endWordPosition="3293">also collected for a reason that will be clarified later on. 159 Computational Linguistics Volume 17, Number 2 2. A semantic bias. The semantic bias is the kernel of any learning algorithm, as no system can learn much more than what it already knows (Micalski 1983). This consists of: (a) a domain-dependent concept hierarchy. This is a many-to-many mapping from words to word sense names and an ordered list of conceptual categories. The hypothesis of hand-entering a type hierarchy would not require an unreasonable amount of time, because the task is comparable to entering a morphologic lexicon (Russo 1987). The problem is rather a conceptual one. The way humans categorize concepts in classes is far from being understood. Mere property inheritance seems to be inadequate at fully modeling categorization in humans (Lakoff 1987; Rosch 1975), and this very fact discouraged us from attempting some automatic hierarchy acquisition in the framework of machine learning (Gennari 1989). The limitations of current machine learning approaches when applied to language learning are discussed in Section 5. We consider the problem of acquiring type hierachies as an open issue, to which more thought and more rese</context>
</contexts>
<marker>Russo, 1987</marker>
<rawString>Russo, M. (1987). A Generative Grammar Approach for the Morphologic and Morphosyntactic Analysis of Italian, In Third Conference of the European Chapter of the ACL, Copenhagen, April 1-3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Schank</author>
</authors>
<title>Conceptual Dependency: A Theory of Natural Language Understanding,</title>
<date>1972</date>
<journal>In Cognitive Psychology,</journal>
<volume>3</volume>
<contexts>
<context position="6156" citStr="Schank 1972" startWordPosition="949" endWordPosition="950">inguists, psychologists, and computer scientists. We believe that, just to take one example, a poor knowledge of the results and methods of lexicography, linguistics, and cognitive science in part motivates the nonstriking success of NLP in developing large-scale systems (Velardi 1990). 154 Velardi et al. Encoding Semantic Knowledge example 1 from (Leech 1981): boy = +animate —adult +male example 2 from (Mekuk 1987): help = Y carrying out Z, X uses his resources W in order for W to help Y to carry out Z; the use of resources by X and the carrying out of Z by Y are simultaneous example 3 from (Schank 1972): throw = actor PROPELs and object from a source LOCation to a destination LOCation Figure 1 Examples of conceptual meaning representation in the literature data. One of the largest hand-encoded semantic lexicons (Dahlgren 1989 and Dahlgren, communication to AAAI90 Stanford seminars) was built by asking subjects to freelist features common to objects. Despite the scientific interest of such experiments, they cannot be extensively repeated for the purpose of acquiring several thousand word sense definitions. • On-line corpora and dictionaries are widely available today and provide experimental </context>
</contexts>
<marker>Schank, 1972</marker>
<rawString>Schank, R. C. (1972). Conceptual Dependency: A Theory of Natural Language Understanding, In Cognitive Psychology, vol. 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J E Sowa</author>
</authors>
<date>1984</date>
<booktitle>Conceptual Structures in Mind and Machine.</booktitle>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="22632" citStr="Sowa 1984" startWordPosition="3450" endWordPosition="3451">5. We consider the problem of acquiring type hierachies as an open issue, to which more thought and more research are being devoted in our current work. (b) a set of domain-dependent conceptual relations, and a many-to-many mapping (synt-sem) between syntactic relations and the corresponding conceptual relations (see Velardi 1988 and Antonacci 1989 for extensive examples of syntax-to-semantics mapping); (c) a set of coarse-grained selectional restrictions on the use of conceptual relations, represented by concept-relation-concept (CRC) triples. CRCs are expressed in Conceptual Graph notation (Sowa 1984). 4.2 The Output The system produces two types of output: 1. a set of fine-grained CRCs, that are clustered around concepts or around conceptual relations; 2. an average-grained semantic knowledge base, organized in CRC triples. The semantic knowledge base is acquired from a source sub-corpus and is used, before a final approval, for the semantic interpretation of sentences in a test-bed sub-corpus. The semantic interpreter is basically that described in Velardi (1988) and in other papers, to which the interested reader may refer. Such terms as &apos;fine,&apos; average,&apos; and &apos;coarse&apos; are obviously fuzz</context>
</contexts>
<marker>Sowa, 1984</marker>
<rawString>Sowa, J. E (1984). Conceptual Structures in Mind and Machine. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Smadja</author>
</authors>
<title>Lexical Co-occurrence: The Missing Link,</title>
<date>1989</date>
<booktitle>In Literary and Linguistic Computing,</booktitle>
<volume>4</volume>
<contexts>
<context position="19926" citStr="Smadja (1989)" startWordPosition="3037" endWordPosition="3038">sa creature agent_of take put find speech-action mental-action consist_of hand foot... source_of speech-action destination_of speech-action power human speed slow mass human Figure 2 Examples of collocative meaning representation in the literature 4.1 What Is Given The input to the system is: 1. a list of syntactic collocates, e.g. subject-verb, verb-object, noun-preposition-noun, noun-adjective, etc. extracted through morphologic (Russo 1987) and syntactic analysis of the selected corpus. The level at which syntactic analysis should be performed to derive collocates is a matter of debate. In Smadja (1989) it is suggested that parsing can be avoided by simply examining the neighborhood of a word w at a distance of ±5. Our experience demonstrates that this algorithm produces too many collocations, of which a minority are actually semantically related. At the other extreme is full syntactic parsing, as performed in Velardi (1989). This is computationally too expensive for large corpora and fails to produce useful collocations when sentences are not fully grammatical, as for example in the agricultural businesses database. A typical text in this corpus is: vendita al minuto di legno da costruzione</context>
<context position="32454" citStr="Smadja (1989)" startWordPosition="4978" endWordPosition="4979">tions, however, are not listed in the lower window of Figure 4 because the words &amp;quot;fish&amp;quot; and &amp;quot;drill&amp;quot; never occur in the corpus at a distance of ±5. 6 For very large corpora, as the one used here, producing the complete list of possible implications would generate in some case hundreds of examples, especially when both concepts in a CRC triple are not terminal nodes in the hierarchy. The linguist is therefore presented only with the list of collocates that have a non-zero probability of occurrence. This list is the list of all the collocations found in the (sub-)corpus using the +5 algorithm of Smadja (1989). 163 Computational Linguistics Volume 17, Number 2 ...,, _g. File Edit Search Windows Fonts Emil Hermes KB DRL Learn Acquisizione RestrizioniSenlantiche Gruppo sintattico G_N_P_N= alieuare+nnento dipesce Relazione da acquisire (alleuare)-&gt;Ipazientel-&gt;(c_animale_alimentazione) Generalizzazioni (alleuere)-&gt;Ipaziente1-“c_animale_alimentazione) 4 (alletrare)-4pazientel-&gt;(pesce) a lallevare.1-&gt;Ipaziente1-&gt;lc_animale_alimentazionel ( Flitiela ) ( Requisis(e ) (Test) ( Resetta ) (Soapimde) ‘ Generalizza y - allevare paziente pesce. E. y - allevare paziente suino. y - allevare paziente caprino. y - a</context>
<context position="48661" citStr="Smadja (1989)" startWordPosition="7432" endWordPosition="7433">od is interesting but allows a conceptual typing of unknown words only when everything else is known. It does not attack the problem of extensive lexical acquisition, but rather that of robust language processing. In Binot (1987) prepositional attachments found in dictionary definitions are interpreted in terms of conceptual relations (e.g. WITH=INSTRUMENT) and used to solve syntactic ambiguity in parsing. The problem is that the information necessary to disambiguate is often not found, or requires several complex searches through the dictionary because of circularity and cross-references. In Smadja (1989) the system EXTRACT, which uses shallow methods to extensively derive collocates from corpora, is described. The system produces a list of tuples (w1,w2,0, where w1 and w2 are two co-occurring words and f is the frequency of appearance in the corpus. No semantic interpretation is attempted, but it is claimed that mere co-occurrence knowledge can help language generators to correctly handle collocationally restricted sentences. 6. Concluding Remarks The algorithm presented in this paper automatically detects co-occurrences in contexts and provides a semantic interpretation of the meaning relati</context>
</contexts>
<marker>Smadja, 1989</marker>
<rawString>Smadja, F. (1989). Lexical Co-occurrence: The Missing Link, In Literary and Linguistic Computing, Vol. 4, n. 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Velardi</author>
<author>M T Pazienza</author>
<author>M De Giovanetti</author>
</authors>
<title>Conceptual Graphs for the Analysis and Generation of Sentences, In</title>
<date>1988</date>
<journal>IBM Journal of R&amp;D, special issue</journal>
<marker>Velardi, Pazienza, De Giovanetti, 1988</marker>
<rawString>Velardi, P.; Pazienza, M. T.; and De Giovanetti, M. (1988). Conceptual Graphs for the Analysis and Generation of Sentences, In IBM Journal of R&amp;D, special issue on language processing, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Velardi</author>
<author>M T Pazienza</author>
</authors>
<title>Computer-Aided Acquisition of Lexical Cooccurrences,</title>
<date>1989</date>
<booktitle>In Proceedings of the ACL</booktitle>
<location>Vancouver.</location>
<marker>Velardi, Pazienza, 1989</marker>
<rawString>Velardi, P., and Pazienza, M. T. (1989). Computer-Aided Acquisition of Lexical Cooccurrences, In Proceedings of the ACL 1989. Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Velardi</author>
</authors>
<title>Why Human Translators Still Sleep in Peace? (Four Engineering and Linguistic Gaps in NLP),</title>
<date>1990</date>
<booktitle>In Proc. of COLING 90.</booktitle>
<location>Helsinki,</location>
<contexts>
<context position="5830" citStr="Velardi 1990" startWordPosition="884" endWordPosition="885">n 1989). Implementation, because consistency becomes a major problem when the size of the lexicon exceeds a few hundred entries. • Psycholinguistic experiments, such as verbal protocols, are more appropriate, because they produce observable and measurable 1 This is not to say that there is a clear cut among the interests of linguists, psychologists, and computer scientists. We believe that, just to take one example, a poor knowledge of the results and methods of lexicography, linguistics, and cognitive science in part motivates the nonstriking success of NLP in developing large-scale systems (Velardi 1990). 154 Velardi et al. Encoding Semantic Knowledge example 1 from (Leech 1981): boy = +animate —adult +male example 2 from (Mekuk 1987): help = Y carrying out Z, X uses his resources W in order for W to help Y to carry out Z; the use of resources by X and the carrying out of Z by Y are simultaneous example 3 from (Schank 1972): throw = actor PROPELs and object from a source LOCation to a destination LOCation Figure 1 Examples of conceptual meaning representation in the literature data. One of the largest hand-encoded semantic lexicons (Dahlgren 1989 and Dahlgren, communication to AAAI90 Stanford</context>
</contexts>
<marker>Velardi, 1990</marker>
<rawString>Velardi, P. (1990). Why Human Translators Still Sleep in Peace? (Four Engineering and Linguistic Gaps in NLP), In Proc. of COLING 90. Helsinki, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Webster</author>
<author>M Marcus</author>
</authors>
<title>Automatic Acquisition of the Lexical Semantics of Verbs from Sentence Frames.</title>
<date>1989</date>
<booktitle>In Proceedings of the ACL</booktitle>
<location>Vancouver.</location>
<marker>Webster, Marcus, 1989</marker>
<rawString>Webster, M., and Marcus, M. (1989). Automatic Acquisition of the Lexical Semantics of Verbs from Sentence Frames. In Proceedings of the ACL 1989. Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Zernik</author>
</authors>
<title>Lexicon Acquisition: Learning from Corpus by Capitalizing on Lexical Categories,</title>
<date>1989</date>
<booktitle>In IJCAI 1989,</booktitle>
<location>Detroit.</location>
<contexts>
<context position="47576" citStr="Zernik (1989)" startWordPosition="7269" endWordPosition="7270">ceptual clustering, even though we are aware that the hand-entering of the type hierarchy is the major limitation of our algorithm. A second research area related to our work is lexical acquisition. An up-to-date survey of the most recent papers in this area is found in Computational Linguistics (1987) and Zernik (forthcoming). Many of the papers collected in the above two issues are more relevant to the fields of lexicography and cognition than to NLP. One of the 168 Velardi et al. Encoding Semantic Knowledge few lexical knowledge acquisition systems for NLP is described in Jacobs (1988) and Zernik (1989). When an unknown word is encountered, the system uses pre-existing knowledge of the context in which the word occurred to derive its conceptual category The context is provided by on-line texts in the economic domain. For example, the unknown word merger in &amp;quot;another merger offer&amp;quot; is categorized as merger-transaction using semantic knowledge of the word offer and of pre-analyzed sentences referring to a previous offer event, as suggested by the word another. This method is interesting but allows a conceptual typing of unknown words only when everything else is known. It does not attack the pro</context>
</contexts>
<marker>Zernik, 1989</marker>
<rawString>Zernik, U. (1989). Lexicon Acquisition: Learning from Corpus by Capitalizing on Lexical Categories, In IJCAI 1989, Detroit.</rawString>
</citation>
<citation valid="false">
<authors>
<author>U Zernik</author>
<author>ed</author>
</authors>
<title>(in press) Lexical Acquisition: Using On-line Resources to Build a Lexicon, Lawrence Erlbaum.</title>
<marker>Zernik, ed, </marker>
<rawString>Zernik, U., ed. (in press) Lexical Acquisition: Using On-line Resources to Build a Lexicon, Lawrence Erlbaum.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>