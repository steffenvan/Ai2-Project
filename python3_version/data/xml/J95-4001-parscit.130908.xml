<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.996154">
The Repair of Speech Act
Misunderstandings by Abductive
Inference
</title>
<author confidence="0.999011">
Susan W. McRoy* Graeme Hirstt
</author>
<affiliation confidence="0.997638">
University of Wisconsin-Milwaukee University of Toronto
</affiliation>
<bodyText confidence="0.993445388888889">
During a conversation, agents can easily come to have different beliefs about the meaning or
discourse role of some utterance. Participants normally rely on their expectations to determine
whether the conversation is proceeding smoothly: if nothing unusual is detected, then under-
standing is presumed to occur. Conversely, when an agent says something that is inconsistent
with another&apos;s expectations, then the other agent may change her interpretation of an earlier turn
and direct her response to the reinterpretation, accomplishing what is known as a fourth-turn
repair.
Here we describe an abductive account of the interpretation of speech acts and the repair of
speech act misunderstandings. Our discussion considers the kinds of information that participants
use to interpret an utterance, even if it is inconsistent with their beliefs. It also considers the
information used to design repairs. We describe a mapping between the utterance-level forms
(semantics) and discourse-level acts (pragmatics), and a relation between the discourse acts and
the beliefs and intentions that they express. We specify for each discourse act, the acts that might
be expected, if the hearer has understood the speaker correctly. We also describe our account of
belief and intention, distinguishing the beliefs agents actually have from the ones they act as if
they have when they perform a discourse act. To support repair, we model how misunderstandings
can lead to unexpected actions and utterances and describe the processes of interpretation and
repair. To illustrate the approach, we show how it accounts for an example repair.
</bodyText>
<sectionHeader confidence="0.992118" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999753125">
Speech act misunderstandings occur when two participants differ in their understand-
ing of the discourse role of some utterance. For example, one speaker might take an
utterance as an assertion while another understands it to be a request. Although many
researchers have considered the problem of avoiding misunderstanding (e.g., by cor-
recting misconceptions), previously none has addressed the problem of identifying
and repairing misunderstandings once they occurred. Here, we will consider a gen-
eral model of dialogue that also accounts for the detection and repair of speech act
misunderstandings.
</bodyText>
<subsectionHeader confidence="0.999534">
1.1 The difference between misunderstanding and misconception
</subsectionHeader>
<bodyText confidence="0.999850333333333">
The notions of misunderstanding and misconception are easily confounded, so we
shall begin by explicating the distinction. Misconceptions are errors in the prior knowl-
edge of a participant; for example, believing that Canada is one of the United States.
</bodyText>
<affiliation confidence="0.7503715">
* Department of Electrical Engineering and Computer Science, Milwaukee, WI 53201, mcroy@cs.uwm.edu
t Department of Computer Science, Toronto, Canada M5S 1A4, gh@cs.toronto.edu
</affiliation>
<note confidence="0.877477">
© 1995 Association for Computational Linguistics
Computational Linguistics Volume 21, Number 4
</note>
<bodyText confidence="0.999397615384615">
McCoy (1989), Calistri-Yeh (1991), Pollack (1986b), Pollack (1990), and others have stud-
ied the problem of how one participant can determine the misconceptions of another
during a conversation (see Section 5.3 below). Typically such errors can be recognized
immediately when an expression is not interpretable with respect to the computer&apos;s
(presumedly perfect!) knowledge of the world.
By contrast, a participant is not aware, at least initially, when misunderstanding has
occurred. In misunderstanding, a participant obtains an interpretation that she believes
is complete and correct, but which is, however, not the one that the other participant
intended her to obtain. At the point of misunderstanding, the interpretations of the
two participants begin to diverge. It is possible that a misunderstanding will remain
unnoticed in a conversation and the participants continue to talk at cross-purposes.
Alternatively, the conversation might break down, leading one participant or the other
to decide that a misunderstanding has occurred and (possibly) attempt to resolve it.
</bodyText>
<subsectionHeader confidence="0.949916">
1.2 The use of repair in the negotiation of meaning
</subsectionHeader>
<bodyText confidence="0.999964392857143">
Although they might not always recognize a misunderstanding when it occurs, dis-
course participants are aware that misunderstandings can occur. So, participants, rather
than just passively hoping that they have understood and have been understood, ac-
tively listen for trouble and let each other know whether things seem okay. Each
participant will use the subsequent discourse itself in order to judge whether previous
discourse has been understood correctly. When one participant produces a response
that is consistent and coherent with what the other has just said, then the other will
take it as a display of understanding. Otherwise, it might be taken as evidence of
misunderstanding. In either case, the response is used as an indication of how the
second participant interpreted the first, as presumably his response must have some
rational explanation; the indicated interpretation is called the displayed interpretation.
When a participant notices a discrepancy between her own interpretation and the one
displayed by the other participant, she can choose to initiate a repair or to let it pass.
By their choice of repairing or accepting a displayed interpretation, speakers in effect
negotiate the meaning of utterances.&apos;
Repairs can take many forms, depending on how and when a misunderstand-
ing becomes apparent. Conversation analysts classify repairs according to how soon
after the problematic turn a participant initiates a repair (Schegloff 1992). The most
common type occurs within the turn itself or immediately after it, before the other
participant has had a chance to reply. These are called first-turn repairs. The next most
common type, second-turn repairs, occur as the reply to the problematic turn (e.g.,
as a request for clarification). We will not consider these two types of repairs further,
because they do not involve misunderstanding per se. Rather, they are used to correct
misconceptions, misspeakings, nonhearings, etc.
Third-turn and fourth-turn2 repairs address actual misunderstandings. If a display
of misunderstanding occurs in the turn immediately following the one that was mis-
understood, and the speaker notices the problem immediately and acts to resolve it,
then we say that they have made a third-turn repair (see Example 1).
</bodyText>
<footnote confidence="0.995129333333333">
1 Note that this choice allows for a speaker feigning the occurrence of a misunderstanding in order to
achieve some social goal.
2 Schegloff (1992) distinguishes nth-turn repair from nth-position repair. The former corresponds to
repairs that begin exactly n — 1 turns after the problematic utterance, while the latter allows an
arbitrary number of intervening pairs of turns. We shall use &amp;quot;nth-turn&amp;quot; to refer to both types, allowing
intervening exchanges.
</footnote>
<page confidence="0.994483">
436
</page>
<note confidence="0.914889">
McRoy and Hirst The Repair of Speech Act Misunderstandings
</note>
<equation confidence="0.7408942">
Example 1
Ti S: Where do you do this?
T2 H: To make the crops grow.
T3 S: I said where do you do it.
T4 H: In a tin hut in Greeba.
</equation>
<bodyText confidence="0.976519428571429">
If a display of misunderstanding occurs during a subsequent turn by the same speaker
who generated the misunderstood turn, and the hearer then reinterprets the earlier
turn and produces a new response to it, then we say that they have made a fourth-turn
repair. The fragment of conversation shown in Example 2 (Terasaki 1976) includes a
fourth-turn repair. Initially, Russ interprets T1 as expressing Mother&apos;s desire to tell, that
is, as a pretelling or preannouncement, but finds this interpretation inconsistent with her
next utterance. In T3, instead of telling him who&apos;s going (as one would expect after
a pretelling), Mother claims that she does not know (and therefore could not tell).
Russ recovers by reinterpreting Ti as an indirect request, which his T4 attempts to
satisfy. Fox (1987) points out that such repairs involve, in effect, a reconstruction of the
initial utterance. From an AT perspective, these reconstructions resemble the operation
of a truth-maintenance system upon an abductive assumption that has proved to be
incorrect.&apos;
Example 2
</bodyText>
<listItem confidence="0.9724254">
Ti Mother: Do you know who&apos;s going to that meeting?
T2 Russ: Who?
T3 Mother: I don&apos;t know.
T4 Russ: Oh. Probably Mrs. McOwen and probably Mrs. Cadry and some
of the teachers.
</listItem>
<bodyText confidence="0.9882279375">
1.3 The need for both intentional and social information
The problem of interpreting an utterance involves deciding what actions the speaker is
doing or trying to do. This process involves not only looking at the surface form of an
utterance—for example, was it stated as a declarative?—but also at the context in which
it was uttered. This context includes the tasks that the participants are involved in,
the prior beliefs that they had, and the discourse itself. Context is important because
it allows speakers to use the same set of words, for example, &amp;quot;Do you know what
time it is?&amp;quot;, to request the time, to express a complaint, or to ask a yes-no question.
Intentional information can rule out some of these readings; for example, a belief that
the speaker already knows the time might rule out the &apos;request&apos; interpretation.
The difficulty in considering misunderstandings in addition to intended interpre-
tations is that it greatly increases the number of alternatives that an interpreter needs
to consider, because one cannot simply ignore the interpretations that seem inconsis-
tent. However, predominant computational approaches to dialogue, which are based
solely on inference of intention, already have difficulty constraining the interpretation
process. Sociological accounts suggest a more constrained approach to interpretation
</bodyText>
<footnote confidence="0.995382333333333">
3 This is distinct from the kind of plan repair described by Spencer (1990), which he models using an
assumption-based truth-maintenance system. In his work, &amp;quot;repair&amp;quot; addresses the problem of
incompleteness in a taxonomy of plans, rather than errors in interpretation.
</footnote>
<page confidence="0.989813">
437
</page>
<note confidence="0.890945">
Computational Linguistics Volume 21, Number 4
</note>
<bodyText confidence="0.999866">
and the recognition of misunderstanding, but none are computational. Our model
extends the intentional and social accounts of discourse, combining the strengths of
both.
In the intentional accounts, speakers use their beliefs, goals, and expectations to
decide what to say; when they interpret an utterance, they identify goals that might
account for it. For example, a speaker who wants someone to know that she lacks a
pencil might say &amp;quot;I don&apos;t have a pencil.&amp;quot; A hearer might then interpret this utterance
as an attempt to convey the information. However, for any goal that would explain an
utterance, the reasons for having that goal would also be potential interpretations of
the utterance. Thus, for the above utterance, intentional accounts might also consider
interpretations corresponding to an attempt to express a need for a pencil, a request
to be given the pencil, an incomplete attempt to fill out a questionnaire, and so on.4
The inherent difficulty with this approach is thus knowing when to stop searching for
potential meanings.
According to the ethnomethodological account of human communication known
as Conversation Analysis (CA), agents design their behavior with the understanding
that they will be held accountable for it. Agents know that their utterances will be taken
to display their understanding of some (culturally determined) rules of conversation
and the situation prior to the utterance. Agents, aware of some rule or norm that is
relevant to their current situation, choose to follow (or not follow) the rule, depending
on how they view the consequences of their choice. One important convention is the
adjacency pair. Adjacency pairs are sequentially constrained pairs of utterances, (such
as question-answer), in which an utterance of the first type creates an expectation for
one of the second. A hearer is not bound to produce the expected reply, but if he
does not, he must be ready to justify his action and to accept responsibility for any
inferences that the speaker might make (Schegloff and Sacks 1973). Where the CA
approach is weakest is in its explanation of how the recipient of an utterance is able
to understand an utterance that is the first part of an adjacency pair. For this, an agent
needs linguistic knowledge linking the features of an utterance to a range of speech
acts that can form adjacency pairs. Agents also need to have some idea of the beliefs
and intentions that particular actions express, so they can make judgments about their
appropriateness in the context.
</bodyText>
<subsectionHeader confidence="0.408269">
1.4 Overview
</subsectionHeader>
<bodyText confidence="0.999407083333333">
The aim of our research is to construct a model of communicative interaction that will
be able to support the negotiation of meaning. In particular, we want to develop a
general model of conversation that is flexible enough to handle misunderstandings.
To support this degree of flexibility, the agents that we model form expectations on
the basis of what they hear, monitor for differences in understanding, and, when
appropriate, change their own interpretations in response to new information. The
model specifies the relationship between this reasoning and discourse participants&apos;
beliefs, intentions, and previously expressed attitudes, as well as their knowledge of
social conventions.
In the account, speakers select speech acts on the basis of both their goals and
their knowledge of which speech acts are expected to follow upon a given speech
act. They must select an utterance form that both parties would agree (in the current
</bodyText>
<footnote confidence="0.992107">
4 The amount of reasoning is a function of the size of one&apos;s plan hierarchy. So, if it is believed that
questionnaires are used to obtain a driver&apos;s license, which is needed to drive a car, which is needed to
get to California, then this same utterance could even be interpreted as an incomplete attempt to get to
California. Thus, the hearer must also assume that he and the speaker share the same plan hierarchy.
</footnote>
<page confidence="0.994099">
438
</page>
<note confidence="0.885282">
McRoy and Hirst The Repair of Speech Act Misunderstandings
</note>
<bodyText confidence="0.999235166666667">
discourse context) could accomplish the desired goal. Interpretation and repair attempt
to apply this process in reverse, working back from an observed utterance to the
underlying goal. Such reasoning is clearly nonmonotonic; here we suggest that it can be
characterized quite naturally as abduction. The model is expressed as a logical theory
in the Prioritized Theorist framework (Poole, Goebel, and Aleliunas 1987; van Arragon
1990).
</bodyText>
<sectionHeader confidence="0.785851" genericHeader="keywords">
2. The structured intentional approach
</sectionHeader>
<bodyText confidence="0.999438222222222">
We now introduce a model of dialogue that extends both intentional and social ac-
counts of discourse. The model unifies theories of speech act production, interpretation,
and the repair of misunderstandings. This unification is achieved by treating produc-
tion as default reasoning, while using abduction to model interpretation and repair. In
addition, the model avoids open-ended inference about goals by using expectations
derived from social norms to guide interpretation. As a result, the model provides a
constrained, yet principled, account of interpretation; it also links social accounts of
expectation with other mental states.
In this section, we will discuss how the model addresses the following concerns:
</bodyText>
<listItem confidence="0.981743285714286">
• The need to control the inference from observed actions to expected
replies. Extended inference about goals is usually unnecessary and a
waste of resources.
• The need to account for nonmonotonicity in both the interpretation and
production of utterances. This nonmonotonicity takes two forms. First,
utterances can make only a part of the speaker&apos;s goals explicit to the
hearer, so hearers must reason abductively to account for them. Second,
expectations are defeasible. At any given moment, speakers may differ in
their beliefs about the dialogue and hence can only assume that they
understand each other. Speakers manage the nonmonotonicity by
negotiating with each other to achieve understanding.
• The need to detect and correct misunderstandings. Speakers rely on their
expectations to decide whether they have understood each other. When
hearers identify an apparent inconsistency, they can reinterpret an earlier
utterance and respond to it anew. However, if they fail to identify a
misunderstanding, the communication might mislead them into
prematurely believing that their goals have been achieved.
• The need for an alternative to the notion of mutual belief. Typically,
models rely on mutual beliefs without accounting for how speakers
achieve them or for why speakers should believe that they have
achieved them.
</listItem>
<subsectionHeader confidence="0.99974">
2.1 Using social conventions to guide interpretation and repair
</subsectionHeader>
<bodyText confidence="0.999938714285714">
Our account of interpretation avoids the extended inference required by plan-based
models by reversing the standard dependency between an agent&apos;s expectations and
task-related goals. Plan-based approaches (Allen and Perrault 1979; Litman 1986; Car-
berry 1990; Lambert and Carberry 1991) start by applying context-independent infer-
ence rules to identify the agent&apos;s task-related plan, possibly favoring alternatives that
extend a previously recognized plan. By contrast, our approach begins with an expec-
tation, using it to premise both the analysis of utterance meaning and any inference
</bodyText>
<page confidence="0.9981">
439
</page>
<note confidence="0.891511">
Computational Linguistics Volume 21, Number 4
</note>
<bodyText confidence="0.915065071428571">
about an agent&apos;s goals. Moreover, our approach treats apparent conflicts with expec-
tations as meaningful; for example, if an utterance is inconsistent with expectations,
then the reasoner will try to explain the inconsistency.
The model focuses on two convention-based sources of expectation. The first
is conventions about what attitudes (belief, desire, intention, etc) each speech act
expresses,&apos; we call these the linguistic intentions of the speech act. The second is con-
ventions for each speech act about what act should follow; we call these linguistic
expectations. Speakers will expect each other to display their understanding of these
conventions and how they apply to their conversation. Thus, they can expect each
other to be consistent in the attitudes that they express and to respond to each act
with its conventional reply, unless they have (and can provide) a valid reason not to.
Linguistic intentions are based on Grice&apos;s (1957) notion of reflexive intention. For
example, an inform(S,H,P) expresses the linguistic intentions whose content is P and
intend(S,know(H,P)) (i.e., the speaker intends the hearer to believe (1) that P is true and
(2) that the speaker intends that the hearer know P). Linguistic expectations capture
the notion of adjacency pairs.&apos;
In defining linguistic intentions, which are shown in Figure 1, we have followed
existing speech act taxonomies, especially those given by Bach and Harnish (1979),
Allen (1983), and Hinkelman (1990).7 Thus, when a speaker produces an askref about
P she expresses (and thereby intends the hearer to recognize that she expresses) that
she does not know the referent of some description in P. intends to find out the referent
of that description, and intends the hearer to tell her that referent. If the speaker is
sincere, she actually believes the content of what she expresses; if the hearer is trusting,
he might come to believe that she believes it.
Following Schegloff&apos;s (1988) analysis of Example 2, we provide a speech act def-
inition for prete11.8 In order to capture the linguistic intentions of pretelling, we also
add a new attitude, knowsBetterRef(S, H, P) that is true if the knowledge of S is strictly
better than the knowledge of P—for example, because S is the expert or S has had
more recent experience with P.
We allow that individuals might not all share the same taxonomy of speech acts
and linguistic intentions and that certain social groups or activities might have their
own specialized sets of linguistic expectations.9 Our theory supports this flexibility by
having each speaker evaluate the coherence of all utterances within her own view of
the discourse. Thus, where we refer to the &amp;quot;displayed interpretation&amp;quot; of an utterance,
we mean displayed given the perspective of a particular speaker.&apos;
5 We assume that these attitudes are a function of discourse or illocutionary level of speech acts, rather
than the surface or locutionary level. This approach has worked well for us, but, as one reviewer
remarked, it is an interesting issue as to whether they are also a function of the locutionary level.
6 Note that although linguistic intentions often express that an action is intended (e.g., questions express
an intention that the hearer answer), the two conventions are independent. For example, while an
invitation to visit at 6pm might create an expectation that dinner will be served, it does not express an
intention to serve it.
</bodyText>
<footnote confidence="0.975825">
7 In the figure, we have used the symbol intend to name both the intention to achieve a situation in
which a property holds and the intention to do action.
8 Schegloff actually argues against representing such sequences as speech acts; however, as in the
computational work cited above, we have used the notion of &amp;quot;discourse-level speech act&amp;quot; to represent
the functional relationship between the surface form of an utterance, the context, and the attitudes
expressed by the speaker.
9 Reithinger and Maier (1995) have used n-gram dialogue act probabilities to induce the adjacency pairs
from a corpus of dialogues for appointment scheduling.
10 Communication can occur despite such differences because speakers with similar linguistic experiences
presumably will develop similar expectations about how discourse works. Differences in expectations
might very well be one thing that new acquaintances must resolve in order to avoid social conflict.
</footnote>
<page confidence="0.993953">
440
</page>
<note confidence="0.561861">
McRoy and Hirst The Repair of Speech Act Misunderstandings
</note>
<construct confidence="0.97765632">
Act type Speech act name Linguistic intentions
informative assert(S, H, P) know(S, P)
assertref(S, H, P) knowref(S, P)
assertif(S, H, P) knowif(S, P)
inform(S, H, P) know(P)
intend(S, know(H, P))
informref(S, H, P) knowref(S, P)
intend(S, knowref(H, P))
informif(S, H, P) knowif(S, P)
intend(S, knowif(H, P))
inquisitive askref(S, H, P) not knowref(S, P)
intend(S, knowref(S, P))
intend(S, do(H, informref(H, S, P)))
askif(S, H, P) not knowif(S, P)
intend(S, knowif(S, P))
intend(S, do(H, informif(H, S. P)))
requestive request(S, H, do(H, P)) intend(S, do(H, P))
pretell(S,H, P) knowref(S, P)
knowsBetterRef(S, H, P)
intend(S, do(S, informref(S, H, P)))
intend(S, knowref(H, P))
testref(S, H, P) knowref(S, P)
intend(S, do(H, assertref(H, S, P)))
testif(S, H, P) knowif(S, P)
intend(S, do(H, assert,&apos; f(H, S, P)))
</construct>
<figureCaption confidence="0.887584">
Figure 1
Linguistic intentions.
</figureCaption>
<bodyText confidence="0.999871142857143">
The figure shows a list of attitudes that each act expresses; the lists are assumed
to be exhaustive with respect to the theory (but not to the various connotations that
might be associated with each act). The set of acts itself is not necessarily exhaustive,
but sufficient to handle the examples that we consider. While our taxonomy might
seem small, most other acts appear to be specializations of those that we selected.
Similarly, the model incorporates only a small number of linguistic expectations; these
are shown in Figure 2.11
</bodyText>
<subsectionHeader confidence="0.999531">
2.2 Characterizing interpretation, production, and repair
</subsectionHeader>
<bodyText confidence="0.999929714285714">
Our model unifies the fundamental tasks of interpreting speech acts, producing speech
acts, and repairing speech act interpretations within a nonmonotonic framework. In
particular, speakers&apos; knowledge about language is represented as a set of default rules.
The rules describe conventional strategies for producing coherent utterances, thereby
displaying understanding, and strategies for identifying misunderstanding. As a re-
sult, speakers&apos; decisions about what utterances they might coherently generate next
correspond to default inference over this theory, while decisions about possible in-
</bodyText>
<footnote confidence="0.71086">
11 Quantitative results by Jose (1988) and Nagata and Morimoto (1993) provide evidence for these
adjacency pairs. In addition, we have used pairs discovered by Conversation Analysis from real
dialogues (Schegloff 1988).
</footnote>
<page confidence="0.997032">
441
</page>
<figure confidence="0.824830272727273">
Computational Linguistics Volume 21, Number 4
First turn Expected reply
askref informref
askif informif
request comply
pretell askref
test ref assert ref
testif assertif
Figure 2
Adjacency pairs (Linguistic expectations).
Example Metaplan type
</figure>
<listItem confidence="0.845158">
1 A: Do you have a quarter? Plan adoption
2 B: No. Acceptance
3 B: I never lend money. Challenge
4 A: No, I meant to offer you one. Repair
5 B: Oh. Thanks. Repair
6 A: Bye. Closing
</listItem>
<figureCaption confidence="0.66104">
Figure 3
</figureCaption>
<bodyText confidence="0.841619333333333">
Examples of different types of coherence strategies.
terpretations of utterances (including recognizing misunderstanding) correspond to
abductive inference over the theory.
</bodyText>
<subsectionHeader confidence="0.858493">
Definition 1
</subsectionHeader>
<bodyText confidence="0.999956095238095">
Given a theory T and a goal proposition g, we say that one can abduce a set of
assumptions ZS, from g if Tu z H g and 7 U is consistent.
Abduction has been applied to the solution of local pragmatics problems (Hobbs et al.
1988, 1993) and to story understanding (Charniak and Goldman 1988).
The model incorporates five strategies, or meta plans, for generating coherent utter-
ances: plan adoption, acceptance, challenge, repair, and closing (the model treats opening
as a kind of plan adoption). Figure 3 contains a conversation that includes an example
for each of the five types. In plan adoption, speakers simply choose an action that can be
expected to achieve a desired illocutionary goal, given social norms and the discourse
context. (The goal itself must originate within the speaker&apos;s non-linguistic planning
mechanism.) The first utterance in the figure is a plan adoption. The second utterance
in the figure, if it occurs immediately after an utterance such as the first one, would
be an acceptance. With acceptance of an utterance, agents perform actions that have
been elicited by a discourse partner. That is, the hearer displays his understanding and
acceptance of the appropriateness of a speaker&apos;s utterance (independent of whether he
actually agrees with it). Challenges display understanding of an utterance, while deny-
ing its appropriateness. For example, an agent might challenge the presuppositions of
a previous action. The third utterance, if it occurs immediately after an utterance such
as the first one, would be a challenge. Repairs display non-acceptance of a previously
displayed interpretation (see Section 1.2). The fourth utterance, occurring after an ex-
change such as (1, 3), would be a third-turn repair by A; the fifth utterance, occurring
</bodyText>
<page confidence="0.992114">
442
</page>
<note confidence="0.485936">
McRoy and Hirst The Repair of Speech Act Misunderstandings
</note>
<bodyText confidence="0.999877470588235">
after (1, 3, 4), would be a fourth-turn repair by B.12 Closings signal that the participants
are ready to terminate the conversation (and that they accept the conversation as a
whole). The last utterance in the figure is a closing.
Misunderstandings are classified according to which participant recognizes that
the misunderstanding has occurred and whom she thinks has misunderstood. Self-
misunderstandings are those in which a hearer finds that a speaker&apos;s current utterance
is inconsistent with something that that speaker said earlier and decides that his own
interpretation of the earlier utterance must be incorrect. Conversely, other-misunder-
standings are those in which the hearer attributes a misunderstanding to the speaker.
Fourth-turn repairs may occur after a self-misunderstanding is recognized; third-turn
repairs may occur after other-misunderstanding.
The model addresses both classes of misunderstanding (see Section 3.3.3), but is
limited to misunderstandings that appear as misrecognized speech acts.&amp;quot; Such misun-
derstandings are especially important to detect, because the discourse role attributed
to an utterance creates expectations that influence the interpretation of subsequent
ones. These misunderstandings are also difficult to prevent, because they can result
from many common sources, including intra-sentential ambiguity and mishearing.
</bodyText>
<subsectionHeader confidence="0.999947">
2.3 Building a model of the interpreted discourse
</subsectionHeader>
<bodyText confidence="0.999954666666667">
For a hearer to interpret an utterance as a particular metaplan or as a manifestation
of misunderstanding, he needs a model of his understanding of the prior discourse.
The typical way to model interpretations has been to represent the discourse as a
partially completed plan corresponding to the actual beliefs (perhaps even mutual
beliefs) of the participants (cf. Carberry 1990). This representation incorporates two
assumptions that must be relaxed in any model that accounts for the negotiation
of meaning: first, that hearers are always credulous about what the speaker says,
and second, that neither participant makes mistakes. To relax these assumptions, the
hearer&apos;s model distinguishes the beliefs that speakers claim or act as if they have
during the dialogue from those that the hearer actually believes they have.&amp;quot; The
model also represents the alternative interpretations that the hearer has considered as
a result of repair.&amp;quot; We will now consider an axiomatization of the model.
</bodyText>
<subsectionHeader confidence="0.593743">
3. The architecture of the model
</subsectionHeader>
<bodyText confidence="0.9998534">
Our model characterizes a participant in a dialogue, alternately acting as speaker and
hearer. In this section, we will give both the knowledge structures that enable the
participant&apos;s behavior and the reasoning algorithms that produce it. (Section 4 and
Appendix A present machine-to-machine dialogues involving two instantiations of
the implemented model.)
</bodyText>
<subsectionHeader confidence="0.99985">
3.1 The reasoning framework: Prioritized Theorist
</subsectionHeader>
<bodyText confidence="0.999952666666667">
The model has been formulated using the Prioritized Theorist framework (Poole,
Goebel, and Aleliunas 1987; Brewka 1989; van Arragon 1990), because it supports
both default and abductive reasoning. Theorist typifies what is known as a &amp;quot;proof-
</bodyText>
<footnote confidence="0.995764428571429">
12 Non-understanding, which entails non-acceptance (or deferred acceptance), is signaled by second-turn
repair. This type of repair will not be considered here.
13 Other misunderstandings are possible; for example there can be disagreement about what object a
speaker is trying to identify with a referring expression (cf. Heeman and Hirst 1995; Hirst et al. 1994).
14 This distinction is similar to the one made by Luperfoy (1992).
15 For present purposes, we also assume that the complete model is accessible to the hearer; one could
better simulate the limitations of working memory by limiting access to only the most recent utterances.
</footnote>
<page confidence="0.99318">
443
</page>
<note confidence="0.464418">
Computational Linguistics Volume 21, Number 4
</note>
<bodyText confidence="0.9998674">
based approach&amp;quot; to abduction because it relies on a theorem prover to collect the
assumptions that would be needed to prove a given set of observations and to verify
their consistency. Our reasoning algorithm is based on Poole&apos;s implementation of The-
orist, which we extended to incorporate preferences among defaults as suggested by
van Arragon (1990).16 A Prioritized Theorist reasoner can assume any default d that
the programmer has designated as a potential hypothesis, unless it can prove from
some overriding fact or hypothesis. This makes the reasoning nonmonotonic, because
the addition of a new fact or overriding default may make less preferable hypotheses
underivable.
The syntax of Theorist is an extension of the predicate calculus. It distinguishes
two types of formulae, facts and defaults. In Poole&apos;s implementation, facts are given
by &amp;quot;FACT w.&amp;quot;, where w is a wff. A default can be given either by &amp;quot;DEFAULT (p, d).&amp;quot; or
&amp;quot;DEFAULT (p, d) : w.&amp;quot;, where p is a priority value, d is an atomic formula with only
free variables as arguments, and w is a wff. For example, we can express the default
that birds normally fly, as:
</bodyText>
<equation confidence="0.948593333333333">
DEFAULT (2, birdsFly(b)) : bird(b) D fly(b).
If .F is the set of facts and is the set of defaults with priority p, then an expres-
sion DEFAULT(p, d) : w asserts that d E AP and (d w) E F. The language lacks
</equation>
<bodyText confidence="0.971622153846154">
explicit quantification; as in Prolog, variable names are understood to be universally
quantified.
Facts are taken as true in the domain, whereas defaults correspond to the hy-
potheses of the domain (i.e., formulae that can be assumed true when the facts alone
are insufficient to explain some observation). A priority value is an integer associated
with a given default (and all ground instances of it), where a default with priority i is
stronger than one with priority j, if i &lt;j. When two defaults conflict, the stronger one
(i.e., the one having the lower priority value) takes precedence. For sets of defaults A&apos;
and Afi such that i &lt;j, no d E 6.1 can be used in an explanation if E A and --4 is
consistent with defaults usable from any Ah, h &lt;i.
In the Theorist framework, explanation is a process akin to scientific theory forma-
tion—if a closed formula representing an observation is a logical consequence of the
facts and a consistent set of default assumptions, then it can be explained:
</bodyText>
<subsectionHeader confidence="0.920469">
Definition 2
</subsectionHeader>
<bodyText confidence="0.999253666666667">
An explanation from the set of facts .F and the sets of prioritized defaults Al, , A&amp;quot;
of a closed formula g is a set .F U D1 U • • • U D&amp;quot;, where each D&apos; is a set of ground
instances of elements of A&apos;, such that:
</bodyText>
<listItem confidence="0.9930045">
1. F U D1 U • • • U Dn is consistent
2. FUD1UUD=g
3. For all D&apos; such that 2 &lt; i &lt; n, there is no F U D&apos; U • • • U D&apos; -1 that
satisfies the priority constraints and is inconsistent with D.
</listItem>
<footnote confidence="0.9572425">
16 Poole&apos;s Theorist implements a full first-order clausal theorem prover in Prolog. Like Prolog, it applies a
resolution-based procedure, reducing goals to their subgoals using rules of the form
goal 4— subgoali A • • • A subgoal. However, unlike Prolog, it incorporates a model-elimination strategy
(Loveland 1978; Stickel 1989; Umrigar and Pitchumani 1985) to reason by cases.
</footnote>
<page confidence="0.996755">
444
</page>
<note confidence="0.49065">
McRoy and Hirst The Repair of Speech Act Misunderstandings
</note>
<bodyText confidence="0.996936">
Priority constraints require that no ground instance of d E Ai can be in Di if its negation
is explainable with defaults usable from any Al, j &lt; i.
Priorities enable one to specify that one default is stronger than another, perhaps
because it represents an exception. In our model, defaults will have one of three
priority values: strong, weak, or very weak. The strongest value is reserved for attitudes
about the prior context, whereas assumptions about expectations are given as weak
defaults and assumptions about unexpected actions or interpretations are given as
very weak defaults. This allows us to specify a preference for expected analyses when
there is an ambiguity.
</bodyText>
<subsectionHeader confidence="0.999931">
3.2 The language of our model
</subsectionHeader>
<bodyText confidence="0.963559">
The model is based on a sorted first-order language, where every term is either an
agent, a turn, a sequence of turns, an action, a description, or a supposition. The
language includes an infinite number of variables and function symbols of every sort
and arity. We also define several special ones to characterize suppositions, actions, and
sequences of turns.
</bodyText>
<listItem confidence="0.999566444444444">
3.2.1 Suppositions. Suppositions are terms that name propositions that agents believe
or express. Suppositions can be thought of as quoted propositions, but with a limited
syntax and semantics. We define the following functional expressions:
• do(s, a) expresses that agent s has performed the action a;
• mistake(s, al, a2) expresses that agent s has mistaken an act al for act az;
• and(pi, p2) expresses the conjunction of suppositions p1 and p2, where p1
must be simple (i.e., not formed from others using the function symbol
and);
• not p expresses the negation of a simple supposition p.&amp;quot;
</listItem>
<bodyText confidence="0.9955832">
We also define several suppositions for expressions of knowledge and intention.
Two suppositions are equivalent if and only if they are syntactically identical. To
capture the notion that speakers are normally consistent in the suppositions that they
choose to express, we need to know how different suppositions relate to each other.
More to the point, we need to know when the expressing of two simple suppositions
is or is not consistent. A complete account must take into consideration possible en-
tailments among expressed propositions; however, no such account yet exists. As a
placeholder for such a theory, there is a compatibility relation for expressed supposi-
tions. Our approach is to make compatibility a default and define axioms to exclude
clearly incompatible cases, such as these:
</bodyText>
<listItem confidence="0.993962">
• The suppositions Q and not Q.
• The supposition of an intention to make Q true when Q is already true
in the agent&apos;s interpretation of the discourse.
</listItem>
<footnote confidence="0.93363">
17 The function not is distinct from the boolean connective We use it to capture the supposition
expressed by an agent who says something negative, e.g., &amp;quot;I do not want to go,&amp;quot; which might be
represented as inform(s, h, not wantToGo).
</footnote>
<page confidence="0.986067">
445
</page>
<note confidence="0.308551">
Computational Linguistics Volume 21, Number 4
</note>
<listItem confidence="0.999684625">
• The supposition of the performance of some act that expresses, via a
linguistic intention, any supposition that would be incompatible with
(another supposition of) the agent&apos;s interpretation of the discourse.
• The supposition of an intention to perform some act expressing any
supposition that is incompatible with the agent&apos;s interpretation of the
discourse.
• The supposition of an intention to knowif Q if either Q or not Q is
already true in the agent&apos;s interpretation of the discourse.
</listItem>
<bodyText confidence="0.972967076923077">
When suppositions are not simple, we check their compatibility by verifying that each
of the conjuncts of each supposition is compatible. (In the system, this is implemented
as a special predicate, inconsistentLI).
There is a danger in treating compatibility as a default in that one might miss
some intuitively incompatible cases and hence some misunderstandings might not be
detectable. An alternative would be to base compatibility on the notion of consistency
in the underlying logic, if a complete logic has been defined.&apos;
3.2.2 Speech acts. For simplicity, we represent utterances as surface-level speech acts
in the manner first used by Perrault and Allen (1980).19 Following Cohen and Levesque
(1985), we limit the surface language to the acts surface-request, surface-inform,
surface-informref, and surface-informif. Example 3 shows the representation of the
literal form of Example 2, the fourth-turn repair example. (We abbreviate &amp;quot;m&amp;quot; for
&amp;quot;Mother&amp;quot;, &amp;quot;r&amp;quot; for &amp;quot;Russ&amp;quot;, and &amp;quot;whoIsGoing&amp;quot; for &amp;quot;who&apos;s going&amp;quot;.)
</bodyText>
<tableCaption confidence="0.9701168">
Example 3
Ti m: surface-request(m, r, informif(r, m, knowref(r, whoIsGoing)))
T2 r: surface-request(r, m, informref(m, r, whoIsGoing))
T3 m: surface-inform(m, r, not knowref(m, whoIsGoing))
T4 r: surface-informref(r, m, whoIsGoing)
</tableCaption>
<bodyText confidence="0.987732428571428">
We assume that such forms can be identified by the parser, for example treating all
declarative sentences as surface-informs.&apos;
18 Note that human behavior lies somewhere in between these two extremes; in particular, people do not
seem to express all the entailments of what they utter (Walker 1991).
19 Other representation languages, such as one based on case semantics, would also be compatible with
the approach and would permit greater flexibility. The cost of the increased flexibility would be
increased difficulty in mapping surface descriptions onto speech acts; however, because less effort
would be required in sentence processing, the total complexity of the problem need not increase. Using
a more finely-grained representation, one could reason about sentence type, particles, and prosody
explicitly, instead of requiring the sentence processor to interpret this information (cf. Hinkelman 1990;
Beun 1990).
20 We also presume that a parser can recognize surface-informref and surface-informif syntactically
when the input is a sentence fragment, but it would not hurt our analysis to input them all as
surface-inform.
</bodyText>
<page confidence="0.979496">
446
</page>
<bodyText confidence="0.968392625">
McRoy and Hirst The Repair of Speech Act Misunderstandings
The theory includes the discourse-level acts inform, informif, informref, assert,
assertif, assertref, askref, askif, request, pretell, testref, and warn, which we represent
using a similar notation.&amp;quot;&apos;&amp;quot;
3.2.3 Turn sequences. A turn sequence represents the interpretations of the discourse
that a participant has considered up to a particular time. It is structured as a tree, where
each level below the root corresponds to a single turn in the sequence, ordered as they
occurred in time. Each path from the root to a leaf represents a single interpretation
of the dialogue. Nodes that are siblings (i.e., that have the same parent) correspond to
different interpretations of the same turn. Nodes at the same level, but having different
parents, represent repairs. The currently active interpretation is defined by its most
recent turn, which we shall call the focus of the sequence.
The purpose of this tree structure is to capture the sequential structure of the
dialogue and, for each state of the dialogue, what attitudes the participants are ac-
countable for having expressed.&amp;quot; Branches in the sequential structure enable the par-
ticipants to retract attitudes via repair and to reason about the alternatives that they
have achieved.
We will call the turn sequence whose focus is the current turn the &amp;quot;discourse
context&amp;quot;. In order to consider previous states of the context, such as before a possible
misunderstanding occurred, we define a successor relation on turn sequences:
Definition 3
A turn sequence TS2 is a successor to turn sequence TS/ if TS2 is identical to TS1
except that TS2 has an additional turn t that is not a turn of TS1 and t is the successor
to the focused turn of TS1.
</bodyText>
<subsectionHeader confidence="0.999344">
3.3 The characterization of a discourse participant
</subsectionHeader>
<bodyText confidence="0.999898666666667">
We will now consider the knowledge structures that enable a participant&apos;s behavior
and the reasoning algorithms that produce it. We divide our specification of a partic-
ipant into three subtheories:
</bodyText>
<listItem confidence="0.999494">
• A set B of prior assumptions about the beliefs and goals expressed by
the speakers (including assumptions about misunderstanding).
• A set M of potential assumptions about misunderstandings and
metaplanning decisions.
• A theory T describing his or her linguistic knowledge, including
principles of interaction and facts relating linguistic acts.
</listItem>
<bodyText confidence="0.8309455">
Given these three subtheories, an interpretation of an utterance is a set of ground
instances of assumptions that explain the utterance. An utterance would be a coherent
21 In the utterance language, a yes—no question is taken to be a surface-request to informif and a
wh-question is taken to be a surface-request to informref. We then translate these request forms into
the discourse-level actions askif and askref. An alternative would be to identify them as surface-askif
or surface-askref during sentence processing, as Hinkelman (1990) does.
22 Speech act names that end with the suffix -ref take a description as an argument; speech act names that
end with -if take a supposition. The act inform(s,p) asserts that the proposition is true. The act
informif(s,p) asserts the truth value of the proposition named by p (i.e., informif is equivalent to
&amp;quot;inform v inform-not&amp;quot;).
23 Tree structures are often used to represent discourse, but usually the hierarchical structure of the
discourse, rather than its temporal structure (see Lambert and Carberry 1991, 1992).
</bodyText>
<page confidence="0.986687">
447
</page>
<note confidence="0.445984">
Computational Linguistics Volume 21, Number 4
</note>
<bodyText confidence="0.9408656">
reply to an immediately preceding utterance if it would logically follow, given the
selection of some metaplan:
Definition 4
An interpretation of an utterance u to hearer h by speaker s in discourse context ts is
a set M of instances of elements of M, such that
</bodyText>
<listItem confidence="0.9984805">
1. TU BUM is consistent
2. TU BUM utter(s,h,u,ts)
3. TUBUM satisfies the priority constraints; that is, TuBuM is not in
conflict with any stronger defaults that might apply.
</listItem>
<bodyText confidence="0.8702435">
Definition 5
It would be coherent for s to utter u in discourse context ts if the utterance can be de-
rived from an agent&apos;s linguistic knowledge, assuming some set Mtn&apos; of metaplanning
decisions, such that
</bodyText>
<listItem confidence="0.999363">
1. TUBU We&apos; is consistent
2. TUBUMmeta utter(s, h, u, ts)
3. TUBU Mtn&apos; satisfies the priority constraints.
</listItem>
<equation confidence="0.408394">
That is, u is a solution to the following default reasoning problem:
TUBU WI&apos; H (ii) utter(s, h, u, ts)
</equation>
<bodyText confidence="0.9999316">
In the language of the model, the predicate shouldTry is used for discourse ac-
tions that are coherent (We&apos;) and the predicate try is for actions that are explainable
(M). If shouldTry(S1,S2,A,TS) is true, it means that, given discourse context TS (which
corresponds to a particular agent&apos;s perspective), it would be appropriate for speaker
Si to address speaker S2 with discourse-level speech act A (i.e., according to social
conventions, here represented by the linguistic expectations and the meta-plans, Si
should do A next).
By contrast, try(S1,52,A,T2) would mean. that, given a discourse context TS, Si has
performed the discourse-level act A. Discourse-level acts are related to surface-level
acts by the following default:
</bodyText>
<equation confidence="0.95194825">
DEFAULT (3, pickForm(s1,s2, asurfaceForm, a, ts)) :24
decomp(asurfaceForm, a)
A try(si,s2, a, ts)
D utter(si,52,asurfaceForm, ts).
</equation>
<bodyText confidence="0.985427666666667">
This says that the fact that the surface form asurfaceForm can be used to perform discourse
act a in some context and the apparent occurrence of a would be a reason for agent si
to utter asurfaceForm•
24 The model does not discriminate between equally acceptable alternatives. The default pickForm allows
us to account for the fact that the same surface form can perform several discourse acts and the same
discourse act might be accomplished by one of several different surface forms. In our system, this
default is also used as an oracle, allowing us to see how different interpretations affect the participants&apos;
understanding of subsequent turns. Because the default has a very weak priority it can be overridden
by user input, without influencing other defaults.
</bodyText>
<page confidence="0.984041">
448
</page>
<figure confidence="0.995174090909091">
McRoy and Hirst The Repair of Speech Act Misunderstandings
intentional acts
(shouldTry)
try
adopt plan
acceptance
challenge
repair
closing
other-misunderstanding
self-misunderstanding
</figure>
<figureCaption confidence="0.998102">
Figure 4
</figureCaption>
<bodyText confidence="0.99309325">
The relationship between try and shouldTry and their possible explanations.
The predicates shouldTry and try are related because the appropriateness of a po-
tential interpretation is taken as (default) evidence that it is, in fact, the correct inter-
pretation:
</bodyText>
<construct confidence="0.919019333333333">
DEFAULT (1, intentionalAct(si, s2, a, ts)) :
shouldTry(si, S2, a, ts)
D try (si, s2, a, ts).
</construct>
<bodyText confidence="0.937814434782609">
The key difference is that try allows that the best interpretation might be contextually
inappropriate (see Figure 4).
Interpretation corresponds to the following problem in Theorist:
EXPLAIN utter(sl, s2, u, ts).
Generation corresponds to the following problem in Theorist:
EXPLAIN shouldTry(sl, s2, ad, ts) A decomp (as, ad)-
In addition, acts of interpretation and generation update the set of beliefs and goals
assumed to be expressed during the discourse.25
3.3.1 The discourse context. The first component of the model, B, represents the beliefs
and goals that the participants have expressed during their conversation. We assume
that an agent will maintain a record of these expressed attitudes, represented as a turn
sequence. To keep track of the current interpretation of the dialogue, we introduce the
notion of activation of a supposition with respect to a turn sequence. If during a turn
T, a supposition is expressed by an agent through the utterance of some speech act or
the display of misunderstanding, then we say it becomes active in the turn sequence
that has T as its focus (see Section 3.2.3). Moreover, once active, a supposition will
remain active in all succeeding turn sequences, unless it is explicitly refuted.
Individual turns are represented by a set of facts of the form expressed(P,T) and
expressedNot(P,T), where P is an unnegated supposition that has not been formed from
any simpler suppositions using the function and.&apos;
25 A related concern is how an agent&apos;s beliefs might change after an utterance has been understood as an
act of a particular type. Although we have nothing new to add here, Perrault (1990) shows how default
logic might be used to address this problem.
</bodyText>
<page confidence="0.8303435">
26 The intended meaning of expressedNot(P,T) is that during turn T speakers have acted as if the
449
</page>
<figure confidence="0.984619375">
Computational Linguistics Volume 21, Number 4
utterance-level
form
expressed
beliefs
and goals
&apos;expectation
expectations
</figure>
<figureCaption confidence="0.973326">
Figure 5
</figureCaption>
<bodyText confidence="0.930168">
How the knowledge relations fit together.
</bodyText>
<subsubsectionHeader confidence="0.587511">
3.3.2 Possible hypotheses. The second component of the model is M, the set of poten-
</subsubsectionHeader>
<bodyText confidence="0.984615322580645">
tial assumptions about misunderstandings and metaplanning decisions. This is given
by the following set of Theorist defaults:&apos;
intentionalAct, expectedReply, acceptance, ado ptPlan, challenge, makeFourthTurnRepair, make-
ThirdTurnRepair, reconstruction, otherMisunderstanding, selfMisunderstanding, and done.
The theorem prover may assume ground instances of any of these predicates if they are
consistent with all facts and with any defaults having higher priority. As mentioned
in Section 3.1, each of these defaults will have one of three priority values: strong,
weak, or very weak. The strongest level is reserved for attitudes about beliefs and sup-
positions. Assumptions about expectations (i.e., expectedReply, acceptance, makeThird-
TurnRepair, and makeFourthTurnRepair) are given as weak defaults. Assumptions about
unexpected actions or interpretations (i.e., ado ptPlan, challenge, done, selfMisunderstand-
ing, and otherMisunderstanding) are given as very weak defaults, so that axioms can be
written to express a preference for expected analyses when there is an ambiguity. We
will consider each of these predicates in greater detail in the next section, when we
discuss the third component of the model.
3.3.3 A speaker&apos;s theory of language. The third component of the model is T, a
speaker&apos;s theory of communicative interaction. This theory includes strategies for ex-
pressing beliefs and intentions, for displaying understanding, and for identifying when
understanding has broken down. The strategies for displaying understanding suggest
performing speech acts that have an identifiable, but defeasible, relationship to other
speech acts in the discourse (or to the situation). Misunderstandings are recognized
when an utterance is inconsistent or incoherent; strategies for repair suggest reanalyz-
ing previous utterances or making the problem itself public.
Relations on linguistic knowledge. There are three important linguistic knowledge rela-
tions: decomp, lintention, and lexpectation. They are shown as circles in Figure 5; the
boxes in the figure are the objects that they relate.
supposition P were false. Although expressed(not(P), T) and expressedNot(P,T) represent the same state
of affairs, the latter expression avoids infinite recursion by Theorist.
27 The theory also contains defaults to capture the persistence of activation (persists), and the willingness
of participants to assume that others have a particular belief or goal (credulousB and credulousl,
respectively).
</bodyText>
<page confidence="0.922678">
450
</page>
<bodyText confidence="0.951917">
McRoy and Hirst The Repair of Speech Act Misunderstandings
The decomp relation links surface-level forms to the discourse-level forms that they
might accomplish in different contexts. It corresponds to the body relation in STRIPS-
based approaches.&apos; Two speech acts are ambiguous whenever they can be performed
with the same surface-level form. Lintentions relate discourse acts to the linguistic
intentions that they conventionally express (see Section 2.1). The lexpectation relation
captures the notion of linguistic expectation discussed in Section 2.1, relating each act
to the acts that might be expected to follow. Where there is more than one expected
act, a condition is used to distinguish them. For example, the axioms representing the
linguistic expectations of askref are shown below.&apos;
</bodyText>
<equation confidence="0.902253333333333">
FACT lexpectation(do(si, askref (51, s2, d)),
knowref(s2, d),
do(s2, informref(s2, s1, d))).
&amp;quot;A speaker s1 can expect that making an askref of d to s2
will result in s2 telling si the referent of d, if s2 knows it.&amp;quot;
FACT lexpectation(do(si, askref(si, 52, d)),
</equation>
<bodyText confidence="0.9426066">
not knowref(s2, d),
do (s2, inform (s2, sl, not knowref(s2, d)))).
&amp;quot;A speaker s1 can expect that making an askref of d to s2
will result in s2 telling sl that s2 does not know the referent of
d, if 52 does not know it.&amp;quot;
Beliefs and goals. In the model, participants&apos; actual beliefs and goals are distinguished
from those that they express through their utterances. For the examples considered
here, any model of belief would suffice; for simplicity we chose to include beliefs and
goals explicitly in the initial background theory and allow agents to make assumptions
about each other&apos;s beliefs and goals by default.3°
Expectation. In addition to the notion of linguistic expectations, which exist in any
situation, the model incorporates a cognitive, &amp;quot;belief-about-the-future&amp;quot; notion of ex-
pectation. These expectations depend on a speaker&apos;s knowledge of social norms, her
understanding of the discourse so far, and her beliefs about the world at a particular
time. They are captured by the following Theorist rules:
</bodyText>
<construct confidence="0.655254142857143">
DEFAULT (2, expectedReply(Pdo, Pcondition, do (si, a reply), ts)) :
active(pd„ ts)
A lexpectation (p do, Pcondition, do(si, arepiy))
A believe(si, Pcondition)
D expected(si, areply, ts).
FACT -,lintentionsOk(a, ts)
D -expectedReply(pdo, Pcondition, do (s, a), ts).
</construct>
<bodyText confidence="0.570798714285714">
28 Pollack (1986a) calls this the &amp;quot;is-a-way-to&amp;quot; relation.
29 It is actually controversial whether an askref followed by an inform-not-knowref is a valid adjacency
pair. If such questions are taken to presuppose that the hearer knows the answer, a response to the
contrary could also be considered a challenge of this presupposition (Tsui 1991).
30 It would have been possible to characterize actual belief using an appropriate set of axioms, such as
those defining a weak S4 modal logic. However, current formalizations do not seem to account for the
context-sensitivity of speakers&apos; beliefs. See McRoy (1993b) for a discussion.
</bodyText>
<page confidence="0.990392">
451
</page>
<note confidence="0.450074">
Computational Linguistics Volume 21, Number 4
</note>
<bodyText confidence="0.999911066666667">
The second rule says that one would not expect the action a„piy if the linguistic in-
tentions associated with it are incompatible with the context ts.&apos; Normally, as the
discourse progresses, expectations for action that held in previous states of the con-
text eventually cease to hold in the current context, because after the action occurs, it
would be incompatible for an agent to say that he intends to achieve something that is
already true. The compatibility between each of the linguistic intentions of a proposed
action and each of the active suppositions in a context is captured by the predicate
lintentionsOk, which is true if and only if none of the incompatibilities described in
Section 3.2.1 hold.
For convenience, we also define a subjunctive form of expectation to reason about
expectations that would arise as a result of future actions (e.g., plan adoption) or
that must be considered when evaluating a potential repair. This type of expectation
differs from the type defined above in that it depends on the real beliefs of the agent
performing the first (rather than the second) part of an adjacency pair and it does not
depend on the activity of any suppositions or actions.
</bodyText>
<equation confidence="0.970504666666667">
FACT lexpectation(do(si, ai), p, do(s2, a2))
A believe(si,p)
wouldExpect(s1,a1,a2).
</equation>
<bodyText confidence="0.999900928571429">
Meta plans and misunderstandings. Metaplans encode strategies for selecting an appro-
priate act. The antecedents of these axioms refer to expectations. In addition, in order
to preserve discourse coherence, they require either that the linguistic intentions of
suggested actions be compatible with the context or that there be some overt acknowl-
edgement of the discrepancy. (The theory presented here addresses only the former
case; the latter one might be handled by adding an extra default with a stronger
priority level.) Tables 1-6 give each of these axioms in detail.
Along with these metaplans, a speaker&apos;s linguistic theory includes two diagnos-
tic axioms that characterize speech act misunderstandings: self-misunderstanding and
other-misunderstanding. The antecedents of these axioms refer to ambiguities and
inconsistencies with expressed linguistic intentions, as well as expectations. For exam-
ple, Table 5 describes how an observed inconsistency of s1 performing anew might be
a symptom of 52&apos;5 misinterpretation of an earlier act by si . Such mistakes are possible
when the surface form of the earlier act might be used to accomplish either aobserved or
</bodyText>
<equation confidence="0.422227">
aintended.32
</equation>
<bodyText confidence="0.934560875">
The defaults that characterize misunderstandings have a lower priority than the
metaplans, because speakers consider misunderstandings only when no coherent inter-
pretation is possible. The preference for coherent interpretations is especially important
when there is more than one discourse-level act for which the utterance is a possible
decomposition.
31 Although, like expectedReply, active is a default, active will take precedence over expectedReply, because it
has been given a higher priority on the assumption that memory for suppositions is stronger than
expectation.
</bodyText>
<footnote confidence="0.79446">
32 It is possible that the same surface form might accomplish several different discourse acts, in which
case it might be desirable to evaluate the likelihood of alternative choices. The work discussed by
Reithinger and Maier (1995), for example, found statistical regularities in the misinterpretations that
occurred in their corpus of appointment-scheduling dialogues.
</footnote>
<page confidence="0.980912">
452
</page>
<note confidence="0.486235">
McRoy and Hirst The Repair of Speech Act Misunderstandings
</note>
<tableCaption confidence="0.998332">
Table 1
</tableCaption>
<table confidence="0.960019444444444">
Name Plan adoption
Purpose Introducing a new goal
Axiom DEFAULT (3, adoptPlan(si, 52, a2, ts)) :
hasGoal(si, do (s2, a2), ts)
A wouldExpect(si, do(si, al), do(s2,a2))
D shouldTry(si, s2, al, ts).
FACT -dintentionsOk(ai, ts)
-adoptPlan(si, s2, a2, ts)
Summary Speaker Si should do action al in discourse ts when:
</table>
<listItem confidence="0.9212215">
1. Si wants speaker sz to do action az;
2. Si would expect az to follow an action ai; and
3. si may adopt the plan of performing al to trigger az (i.e., the
linguistic intentions of al are compatible with ts).
</listItem>
<tableCaption confidence="0.995425">
Table 2
</tableCaption>
<table confidence="0.83706675">
Name Acceptance
Purpose Producing an expected reply
Axiom DEFAULT (2, acceptance(si, areplyr ts)) :
expected (si, areply, ts)
D shouldTry, (si , S2, arep/y, tS).
FACT active(do(sl, a), ts)
D --acceptance(si, a, ts).
Summary Speaker Si should do action arepiy in discourse ts when:
</table>
<listItem confidence="0.811087">
1. Si expects areply to occur next; and
2. si may accept the interpretation corresponding to ts.
4. A detailed example
</listItem>
<bodyText confidence="0.999708714285714">
To show how our abductive account of repair works, we offer two examples that show
repair of self-misunderstanding and other-misunderstanding, respectively. Here we
will discuss Example 2 from Russ&apos;s perspective, considering in detail Russ&apos;s reasoning
about each turn and showing an output trace from our implemented system. From
Russ&apos;s perspective, this example demonstrates the detection of a self-misunderstanding
and the production of a fourth-turn repair. In Appendix A we show the system&apos;s output
for a third-turn repair, interleaving the perspectives of its two participants.
</bodyText>
<page confidence="0.996295">
453
</page>
<figure confidence="0.477253928571429">
Computational Linguistics Volume 21, Number 4
Table 3
Name Fourth-turn repair
Purpose Recovering from one&apos;s own misunderstanding
Axiom DEFAULT (2, makeFourthTurnRepair(si, S2, areply, ts, tSreconstructed)) :
active (mistake(si, aintended, aobserved), ts)
A reconstruction (ts, tSreconstructed)
A expected (Si, areply, tSreconstructed)
D shouldTry, (Si, S2, areply, t5).
FACT active(do(si, a), ts)
—,makeFourthTurnRepair(si , s2, a, ts, tSreconstructed)-
Summary Speaker si should do action areply in discourse ts when:
1. Si has mistaken an instance of act a intended as an instance of act
aobserved;
</figure>
<listItem confidence="0.949622333333333">
2. A reconstruction of the discourse is possible;
3. si would expect to do areply in this reconstruction; and
4. s may perform a fourth-turn repair.
</listItem>
<tableCaption confidence="0.971139">
Table 4
</tableCaption>
<table confidence="0.569049">
Name Third-turn repair
Purpose Recovering from another speaker&apos;s misunderstanding
Axiom DEFAULT (2, makeThirdTurnRepair(si , s2, areply r ts)) :
</table>
<bodyText confidence="0.800747333333333">
active (mistake(s2, a intended, aobserved), ts)
A a =- inform(si, s2, do(si, a Intended))
A wouldExpect (si , do(si , a intended), do (s2, areply))
</bodyText>
<equation confidence="0.89193">
D shouldTry(si, s2, a, ts).
FACT —dintentionsOk(areply), ts)
9 —makeThirdTurnRepair(si, s2, areply, ts)
</equation>
<bodyText confidence="0.978284666666667">
Summary Speaker si should initiate a repair in discourse ts (that speaker 52 will
later complete) by si telling s2 that she had performed the action a Intended
if:
</bodyText>
<listItem confidence="0.9945185">
1. s2 has apparently mistaken an instance of act amtendea for act aobsemed;
2. Si would expect areply to follow a intended; and
3. Si may perform a third-turn repair (i.e., it would be reasonable and
compatible for s2 to perform areply).
</listItem>
<subsectionHeader confidence="0.905489">
4.1 Overview
</subsectionHeader>
<bodyText confidence="0.952955">
We now repeat Example 2:
</bodyText>
<listItem confidence="0.812775">
Ti Mother: Do you know who&apos;s going to that meeting?
T2 Russ: Who?
</listItem>
<page confidence="0.995987">
454
</page>
<note confidence="0.848138">
McRoy and Hirst The Repair of Speech Act Misunderstandings
</note>
<tableCaption confidence="0.95933">
Table 5
</tableCaption>
<figure confidence="0.987282125">
Name Self-misunderstanding
Purpose Detecting one&apos;s own misunderstanding
Axiom DEFAULT (3, selfMisunderstanding(si, s2, pm stake, anew, ts)) :
active (do(si,aobserved), ts)
A lintention(a. PI)
A lintention(a observed, P12)
A inconsistentLI(pi, pi2)
A ambiguous(aobserved, aintended)
A Nustake = mistake(s2, aintended, aobserved)
D try (si 52, anew, ts).
FACT —,(selfMisunderstanding(si, s2, P mistake, a1, ts)
A shouldTry(si, s2, ai, ts)).
FACT —,(selfMisunderstanding(si, S2, Pnnstake, al ts)
A ambiguous(ai, a2)
A shouldTry(si 92, a2, ts)).
Summary Speaker Si might be attempting action ar,eu, in discourse ts if:
</figure>
<listItem confidence="0.91911225">
1. Si has performed action aobserved;
2. But, the linguistic intentions of anew are inconsistent with the
linguistic intentions of aobse„ed;
3. aobserved and action aintended can be performed using a similar
surface-level speech act; and
4. s2 may have mistaken aintended for aobserved •
T3 Mother: I don&apos;t know.
T4 Russ: Oh. Probably Mrs. McOwen and probably Mrs. Cadry and some
</listItem>
<bodyText confidence="0.6206945">
of the teachers.
In the input we represent this dialogue as the following sequence:
</bodyText>
<tableCaption confidence="0.993179">
Ti m: surface-request(m, r, informif(r, m, knowref(r, whoIsGoing)))
T2 r: surface-request(r, m, informref(m, r, whoIsGoing))
13 m: surface-inform(m, r, not knowref(m, whoIsGoing))
T4 r: surface-informref(r, m, whoIsGoing)
</tableCaption>
<bodyText confidence="0.6133245">
From Russ&apos;s perspective, these utterances had the following discourse-level interpre-
tations at the time each was produced:
</bodyText>
<tableCaption confidence="0.968549">
Ti m: pretell(m, r, whoIsGoing)
T2 r: aslcref(r, m, whoIsGoing)
T3 m: inform(m, r, not knowref(m, whoIsGoing))
T4 r: informref(r, m, whoIsGoing)
</tableCaption>
<page confidence="0.958956">
455
</page>
<figure confidence="0.896472785714286">
Computational Linguistics Volume 21, Number 4
Table 6
Name Other-misunderstanding
Purpose Detecting another&apos;s misunderstanding
Axiom DEFAULT (3, otherMisunderstanding(sl, s2, Pmistake, an., ts)) :
active(do(s2, aintended), ts)
A ambiguous (a ended,int asimilar)
A wouldExpect(si , do(s2, asimika), do(si , anew))
A Pmistake = mistake (si, a intended 1 aSiMitar)
D try (si , s2, ane „,, ts).
FACT otherMisunderstanding (si , s2, Pmistake, ai, ts)
A ambiguous(ai , a2)
D —.shouldTry(si , s2, a, ts).
Summary Speaker sl might be attempting action anew in discourse ts if:
</figure>
<listItem confidence="0.874082">
1. Earlier, speaker 52 performed act aintended;
2. Actions a intended and as,milar can be performed using a similar surface
form;
3. If s2 had performed asimaar, then an would be expected;
4. Si may have mistaken amtended for asimitar.
</listItem>
<bodyText confidence="0.9992932">
After Russ hears T3, he decides that his interpretation of Mother&apos;s first turn as a
pretelling is incorrect. This revision then leads him to reinterpret it as an askref and
to provide a new response.
We will now show how Russ&apos;s beliefs might progress this way. In particular, we
shall address the following questions:
</bodyText>
<listItem confidence="0.999781">
• How Russ decides, after first concluding that T1 was a pretelling, that
he will respond with an askref.
• How Russ decides, after hearing Mother&apos;s response 13, that his earlier
decision was incorrect.
• How Russ decides to produce an informref in T4.
</listItem>
<bodyText confidence="0.9821035">
Figures 6, 7, 9, and 10 will show the output of the system for each of the four turns
of this dialogue, from Russ&apos;s perspective.
</bodyText>
<subsectionHeader confidence="0.994228">
4.2 Initial assumptions
</subsectionHeader>
<bodyText confidence="0.999943">
For this example, we shall assume that Russ believes that he knows who is going to
the meeting (but also allows that Mother&apos;s knowledge about the meeting would be
more accurate than his own). For simplicity, we represent these beliefs as facts.&apos;
</bodyText>
<footnote confidence="0.5540815">
FACT believe(r, knowref(r, whoIsGoing)).
FACT believe(r, knowsBetterRef(m,r,whoIsGoing)).
</footnote>
<page confidence="0.733565">
33 We might have used priorities to express different degrees of belief.
456
</page>
<figure confidence="0.96627572">
McRoy and Hirst The Repair of Speech Act Misunderstandings
I ?- startDialogue2.
&gt;&gt;&gt;surface-request(m,r,informif(r,m,knowref(r,whoIsGoing)))
***Interpreting Utterance***
Explaining
utter(m,r,surface-request(m,r,informif(r,m,knowref(r,whoIsGoing))),ts(0))
Is formula
pickForm(m,r,surface-request(m,r,informif(r,m,knowref(r,whoIsGoing))),
pretell(m,r,whoIsGoing),ts(0)) ok (y/n)?y.
Explanation:
intentionalAct(m,r,pretell(m,r,whoIsGoing),ts(0))
adoptPlan(m,r,pretell(m,r,whoIsGoing),askref(r,m,whoIsGoing),ts(0))
credulousB(m,knowsBetterRef(m,r,whoIsGoing))
credulousI(m,ts(0))
pickForm(m,r,surface-request(m,r,informif(r,m,knowref(r,whoIsGoing))),
pretell(m,r,whoIsGoing),ts(0))
***Updating Discourse Model***
Interpretation: pretell(m, r, whoIsGoing) (turn number 1)
expressed(do(m, pretell(m, r, whoIsGoing)), 1)
Linguistic Intentions of pretell(m,r,whoisGoing):
knowref(m,whoIsGoing)
knowsBetterRef(m,r,whoIsGoing)
intend(m,do(m,informref(m,r,whoIsGoing)))
intend(m,knowref(r,whoIsGoing))
Suppositions Added:
</figure>
<figureCaption confidence="0.892886833333333">
expressed(knowref(m, whoIsGoing), 1)
expressed(knowsBetterRef(m, r, whoIsGoing), 1)
expressed(intend(m, do(m, informref(m, r, whoIsGoing))), 1)
expressedantend(m, knowref(r, whoIsGoing)), 1)
Agent m adopted plan to achieve: askref(r,m,whoIsGoing)
Figure 6
</figureCaption>
<bodyText confidence="0.843287571428571">
The output for turn 1 from Russ&apos;s perspective.
We also assume that Russ believes that he knows whether (or not) he knows.
FACT believe(r, knowif(r,knowref(r,whoIsGoing))).
Lastly, we assume that he has linguistic expectations regarding pretell, askref, and
askif as in Section 21&apos;
34 To keep this example of manageable size, we will not assume that he has any expectations regarding
testif or testref, although in life he would.
</bodyText>
<page confidence="0.982276">
457
</page>
<note confidence="0.616837">
Computational Linguistics Volume 21, Number 4
</note>
<subsectionHeader confidence="0.987209">
4.3 Turn 1: Russ decides that Mother is pretelling
</subsectionHeader>
<bodyText confidence="0.990486615384615">
According to the model, after Russ hears Mother&apos;s surface-request, &amp;quot;Do you know
who is going to that meeting?&amp;quot;, he interprets it by attempting to construct a plausible
explanation of it. This requires tentatively choosing a discourse-level act on the basis of
the decomposition relation and then attempting to abduce either that it is an intentional
display of understanding or that it is a symptom of misunderstanding. Theorist is
called to explain the utterance and returns with a list of assumptions that were made
to complete the explanation. (The portion of the output from the update describes
Russ&apos;s interpretation of this explanation; see Figure 6.)
In this simulation, Tl was explained as an intentional pretelling. The explanation
contains the metaplanning assumption that Mother was pretelling as part of a plan to
get Russ to ask a question. The reasoner also attributed to her the linguistic intentions
of pretelling. We will now consider the complete explanation in detail.
Inference begins with a call to Theorist to explain the input:
</bodyText>
<equation confidence="0.78333">
utter(m, r, surface-request(m,r,informif(r,m,knowref(r,whoIsGoing))),ts(0))
</equation>
<bodyText confidence="0.999305">
This utterance must be explained by finding a discourse-level speech act that it might
accomplish and a metaplan or misunderstanding that would explain this act. This
makes use of the following default:
</bodyText>
<equation confidence="0.982599333333333">
DEFAULT (3, pickForm(si, S2, a
surfaceForm, a, ts))
decomp (a :
surfaceForm, a)
A try(si, s2, a, ts)
D utter(si, S2, asurfaceForm, tS) •
</equation>
<bodyText confidence="0.986475333333333">
To satisfy the first premise, the reasoner would need to find a speech act that is related
to the surface form by the decomp relation, for example, either an askif, an askref, or
a pretelling:
</bodyText>
<construct confidence="0.772559833333333">
decomp(surface-request(m, r, informif(r, m, knowref(r, whoIsGoing))),
pretell(m, r, whoIsGoing))
decomp(surface-request(m, r, informif(r, m, knowref(r, whoIsGoing))),
askref(m, r, whoIsGoing))
decomp(surface-request(m, r, informif(r, m, knowref(r, whoIsGoing))),
askif(m, r, knowref(r, whoIsGoing)))
</construct>
<bodyText confidence="0.987012888888889">
In this case, the possibility that Mother is attempting a pretelling was considered. (The
system uses an oracle, represented by the default pickForm, to simulate this choice.&apos;)
It is important to note that this is just one of the possible explanations available to
Russ. Nothing in his beliefs rules out abducing explanations from either the askif or
the askref interpretation.
To satisfy the second premise of the rule, the reasoner must explain:
try(m, r, pretell(m, r, whoIsGoing), ts(0))
Two kinds of explanation are possible: a hearer might assume that the act fulfills
the speaker&apos;s intention to coherently extend the discourse as he has understood it or
</bodyText>
<page confidence="0.8418405">
35 This oracle thus allows the analyst to test different interpretations.
458
</page>
<note confidence="0.647681">
McRoy and Hirst The Repair of Speech Act Misunderstandings
</note>
<bodyText confidence="0.9994506">
he might assume that one of the two types of misunderstanding has occurred.&apos; If
a discourse has just begun, then any utterance that starts an adjacency pair will be
coherent. In this case, Russ finds that the former type of explanation is possible using
the metaplan (for plan adoption) to explain shouldTry(m, r, pretell(m, r, whoIsGoing),
ts(0)). The relevant defaults are repeated here:
</bodyText>
<construct confidence="0.678549333333333">
DEFAULT (1, intentionalAct(si, s2, a, ts)) :
shouldTry(si, s2, a, ts)
try(si, s2, a, ts).
</construct>
<equation confidence="0.909601">
DEFAULT (3, adoptPlan(si, s 2, al, a2, ts)) :
hasGoal(si, do(s2, a2), ts)
A wouldExpect(si, do(si, a ), do(s2, a2))
shouldTry(si, s2, ai , ts).
</equation>
<bodyText confidence="0.970760222222222">
The conditions of the metaplan are satisfiable because there is a plausible goal
act that a pretelling would help Mother to achieve and it is consistent for Russ to
assume that achieving this act was, in fact, her goal.&apos; Also, when we consider possible
evidence against Mother adopting this plan, namely whether the linguistic intentions
of pretelling were incompatible with those that have been expressed, it would be
consistent to assume that Mother is intending this plan.
Russ infers
wouldExpect(r, pretell(m, r, whoIsGoing), asIcref(r, m, whoIsGoing))
because he has a linguistic expectation to that effect:
</bodyText>
<construct confidence="0.725562666666667">
FACT lexpectation(do(m, pretell(m, r, whoIsGoing)),
knowsBetterRef(m, r, whoIsGoing),
do(r, askref(r, m, whoIsGoing))).
</construct>
<subsectionHeader confidence="0.997995">
4.4 Turn 2: Russ decides to respond with an askref
</subsectionHeader>
<bodyText confidence="0.989185538461538">
In turn 2, Russ produces a surface-request. This utterance is appropriate, independent
of whether or not Russ actually wants to know who is going to the meeting, because
it displays acceptance of Mother&apos;s pretelling. From Russ&apos;s perspective it displays ac-
ceptance, because a surface-request is one way to perform an askref, an act that is
expected according to Russ&apos;s model of the discourse after the first turn.&apos;
As shown in Figure 7, Theorist finds that if Russ accepts Mother&apos;s pretelling, he
should perform an askref. An askref would demonstrate acceptance because it is the
expected next act. The derivation of this act relies on the rule for intentional action
shown earlier in Section 4.3, along with the metaplan for acceptance repeated here:
36 The former possibility admits that an utterance that displays a misconception, such as a mistaken belief
about initial knowledge, might still be coherent, unless such knowledge has been introduced into the
discourse explicitly. Misconceptions are addressed by second-turn repairs, which are not considered
here.
</bodyText>
<footnote confidence="0.916079166666667">
37 Because Russ&apos;s previous utterance had not been the first part of an adjacency pair, he cannot explain
her utterance as acceptance or challenge.
38 If, for some reason, Russ did not want to know the information, he might decide not to produce an
askref. However, he would then be accountable for justifying his action as well as for displaying his
acceptance of Mother&apos;s displayed understanding (e.g., by including an explicit rejection of her offer);
otherwise she might think that one of them has misunderstood.
</footnote>
<page confidence="0.997995">
459
</page>
<figure confidence="0.998553875">
Computational Linguistics Volume 21, Number 4
Explaining shouldTry(r,m,A,ts(1)),decomp(A2,A)
Answer: shouldTry(r,m,askref(r,m,whoIsGoing),ts(1)),
decomp(surface-request(r,m,informref(m,r,whoIsGoing)),
askref(r,m,whoIsGoing))
Explanation:
intentionalAct(r,m,askref(r,m,whoIsGoing),ts(1))
acceptance(r,askref(r,m,whoIsGoing),ts(1))
expectedReply(do(m,pretell(m,r,whoIsGoing)),
knowsBetterRef(m,r,whoIsGoing),
do(r,askref(r,m,whoIsGoing)),ts(1))
***Updating Discourse Model***
Interpretation: askref(r,m,whoIsGoing) (turn number 2)
expressed(do(r,askref(r,m,whoIsGoing)),2)
Linguistic Intentions of askref(r,m,whoIsGoing):
not knowref(r,whoIsGoing)
and intend(r,knowref(r,whoIsGoing))
and intend(r,do(m,informref(m,r,whoIsGoing)))
Suppositions Added:
expressedNot(knowref(r,whoIsGoing),2)
expressed(intend(r,knowref(r,whoIsGoing)),2)
expressed(intend(r,do(m,informref(m,r,whoIsGoing))),2)
Agent r performed expected act: askref(r,m,whoIsGoing)
***Generating Utterance***
</figure>
<figureCaption confidence="0.8049915">
&lt;&lt;&lt;surface-request(r, m, informref(m, r, whoIsGoing))
Figure 7
</figureCaption>
<bodyText confidence="0.7439356">
The output for turn 2 from Russ&apos;s perspective.
DEFAULT (2, acceptance(si, arep/p tS)) :
expected (si, a reply, tS)
D shouldTry, (si s2, arepiy, ts).
The askref would be expected (see Section 3.3.3) because:39
</bodyText>
<listItem confidence="0.997537428571429">
• According to the discourse model, it is true that active(do(m pretell(m, r,
whoIsGoing)), ts(1)).
• There is a linguistic expectation that askref follow pretell.
• Russ believes the conditions of this relation: knowsBetterRef(m, r,
whoIsGoing).
• The linguistic intentions of askref are compatible with those already
expressed.
</listItem>
<page confidence="0.8388975">
39 See Figure 8 for how Mother might interpret this turn.
460
</page>
<bodyText confidence="0.959018416666667">
McRoy and Hirst The Repair of Speech Act Misunderstandings
If we assume that Mother produced the first turn as an askif, she might also hear T2 as an
intentional askref, but for a reason different than Russ would. Her explanation would include
the metaplanning assumption that he was doing so as part of an adopted plan to get her to
produce an informref. Although T2 might also be explained by abducing that Russ misunder-
stood T1 as an attempted pretelling, we see that she considers this explanation to be less likely
because otherwise she would have been more inclined to make T3 a third-turn repair (&amp;quot;No, I&apos;m
asking you&amp;quot;).4°
Plan adoption (see Table 1) provides Mother a plausible explanation for T2 because:
I. wouldExpect(r, askref(r, m, whoIsGoing), informref(m, r, whoIsGoing)) is explained
because Mother has a linguistic expectation that says that an askref normally creates an
expectation for the listener to tell the speaker the answer:
</bodyText>
<construct confidence="0.715648">
FACT lexpectation(do(r, askref(r, m, whoIsGoing)),
knowref(m, whoIsGoing),
do(m, informref(m, r, whoIsGoing))).
</construct>
<listItem confidence="0.77001425">
2. Mother&apos;s credulousness about Russ&apos;s goals explains her belief that he wants her to
perform the expected informref.
3. The linguistic intentions of askref are compatible with those that have been expressed,
so it is consistent to assume that Russ is intending to use it as part of a plan. (They are
consistent with the context because T1 expresses only that Mother does not know
whether Russ knows and not that she does not herself know.)
4. Thus, by 1-3 and the metaplan for plan adoption, shouldTry(r, m, askref(r, m,
whoIsGoing), ts(0)) is explainable.
</listItem>
<figure confidence="0.4518355">
Assuming this interpretation, Mother can then demonstrate acceptance using an inform-not-
knowref.
</figure>
<figureCaption confidence="0.937929">
Figure 8
</figureCaption>
<bodyText confidence="0.657596">
How Mother interprets T2.
</bodyText>
<subsectionHeader confidence="0.995555">
4.5 Turn 3: Russ decides that his interpretation of Turn 1 was wrong
</subsectionHeader>
<bodyText confidence="0.777701315789474">
Mother replies with a surface-inform. This is interpreted as a discourse-level inform-
not-knowref. This act signals a misunderstanding, because the linguistic intentions
associated with it are incompatible with those previously assumed, ruling out an
explanation that uses the default for intentional acts.&apos;
Figure 9 shows that Theorist abduces that T3 is attributable to a misunderstanding
on Russ&apos;s part, in particular, to his having incorrectly interpreted one of Mother&apos;s
utterances as a pretelling, rather than as an askref. This explanation succeeded because
each of the conditions of the default for self-misunderstanding were explainable. Below
we will repeat this rule and then sketch the proof, considering each of the premises
in the default.
40 In the model, it is always possible to begin an embedded sequence without addressing the question on
the floor; however, when the embedded sequence is complete, the top-level one is resumed. It is a
limitation of the model that we do not distinguish interruptions from clarifications.
41 For Russ to have heard T3 as demonstrating Mother&apos;s acceptance of his T2 (i.e., as a display of
understanding), the linguistic intentions of inform(m, r, not knowref(m, whoIsGoing)) would need to
have been compatible with this interpretation of the discourse. However, not knowref(m,
whoIsGoing) is among these intentions, while active(knowref(m, whoIsGoing),ts(2)). As a result, T3
cannot be attributed to any expected act, and must be attributed to a misunderstanding either by Russ
or by Mother.
</bodyText>
<page confidence="0.998989">
461
</page>
<figure confidence="0.86510640909091">
Computational Linguistics Volume 21, Number 4
&gt;&gt;&gt;surface-inform(m, r, not knowref(m, whoIsGoing))
***Interpreting Utterance***
Explaining utter(m,r,inform(m,r,not knowref(m,whoIsGoing)),ts(2))
Is formula
pickForm(m,r,surface-inform(m,r,not knowref(m,whoIsGoing)),
inform(m,r,not knowref(m,whoIsGoing)),ts(2)) ok (y/n)?y.
Explanation:
selfMisunderstanding(m,r,mistake(r,askref(m,r,whoIsGoing),
pretell(m,r,whoIsGoing)),
inform(m,r,not knowref(m,whoIsGoing)),ts(2))
persists(do(m,pretell(m,r,whoIsGoing)),2)
pickForm(m,r,surface-inform(m,r,not knowref(m,whoIsGoing)),
inform(m,r,not knowref(m,whoIsGoing)),ts(2))
***Updating Discourse Model***
Interpretation:
inform(m, r, not knowref(m, whoIsGoing)) (turn number 3)
expressed(do(m, inform(m, r, not knowref(m, whoIsGoing))), 3)
Linguistic Intentions of inform(m,r,not knowref(m,whoIsGoing)):
not knowref(m,whoIsGoing)
intend(m,knowif(r,not knowref(m,whoIsGoing)))
Suppositions Added:
</figure>
<figureCaption confidence="0.509187428571429">
expressed(mistake(r, askref(m, r, whoIsGoing),
pretell(m, r, whoIsGoing)),3)
expressedNot(knowref(m, whoIsGoing), 3)
expressed(intend(m, knowif(r, not knowref(m, whoIsGoing))), 3)
Agent r misunderstood act do(m, askref(m, r, whoIsGoing))
as do(m, pretell(m, r, whoIsGoing))
Figure 9
</figureCaption>
<figure confidence="0.838576857142857">
The output for turn 3 from Russ&apos;s perspective.
DEFAULT (3, selfMisunderstanding(sl, S2, Pmistake, anew, ts)) :
active(do(si, a observed)/ ts)
A lintention (anew, pi)
A lintention (aobserved,
A inconsistentLI(pi,R)
A ambiguous (aobse,ed, a mt„ded)
A pmistake .= mistake (s2, a intended aobserved)
D try(si, s2, anew, ts).
FACT (selfMisunderstanding(sl, S2, Pnustake, al, ts)
A shouldTry (si s2, ai , ts)).
FACT —,(selfMisunderstanding(si, sz, pmistake, al, ts)
A ambiguous (ai, a2)
A shouldTry(si,52, a2, ts)) •
</figure>
<page confidence="0.985641">
462
</page>
<table confidence="0.420283">
McRoy and Hirst The Repair of Speech Act Misunderstandings
Premise 1: A pretelling was active in ts(2), because of Russ&apos;s interpretation of
T1.42
Premises 2-4: A pretelling would be incompatible with an inform-not-knowref
happening now. The linguistic intentions of the pretelling are:
</table>
<tableCaption confidence="0.74156475">
and(knowref(m, whoIsGoing),
and(knowsBetterRef(m, r, whoIsGoing),
andlintend(m, do(m, informref(m, r, whoIsGoing))),
intend(m, knowref(r, whoIsGoing)))))
</tableCaption>
<bodyText confidence="0.896112304347826">
The linguistic intentions of inform-not-knowref are:
and(not knowref(m, whoIsGoing),
intend(m, knowif(r, not knowref(m, whoIsGoing)))).
But these intentions are inconsistent, because knowref(m, whoIsGoing)
and not knowref(m, whoIsGoing) are incompatible. As a result,
inconsistentll holds for these linguistic intentions.
Premise 5: This is a plausible mistake because the acts pretell and askref both
have the same surface form:
surface-request(m, r, informif(r, m, knowref(r, whoIsGoing)))
So, ambiguous(pretell(m, r, whoIsGoing), askref(m, r, whoIsGoing)).
The constraints: There is no other coherent interpretation, so it is consistent to
assume that a misunderstanding occurred:
selfMisunderstanding(m,r,
mistake(r,askref(m, r, whoIsGoing),
pretell(m, r, whoIsGoing)),
inform(m, r, not knowref(m, whoIsGoing)),
ts(2)).
Thus, try(m, r, inform(m, r, not knowref(m, whoIsGoing)), ts(2)) is explained. As
a result of this interpretation, not knowref(m, whoIsGoing) is added to the discourse
model as the fact expressedNot(knowref(m, whoIsGoing)). This addition terminates
the activation of knowref(m, whoIsGoing) from the first turn. (At the same time, if
Russ had revised any of his real beliefs on the basis of the first turn, he might now
reconsider those revisions; however, our theory does not account for this.)
</bodyText>
<subsectionHeader confidence="0.997977">
4.6 Turn 4: Russ performs a repair
</subsectionHeader>
<bodyText confidence="0.999326">
After revising his understanding of Turn 1, Russ performs a surface-informref that
displays his acceptance of the revised interpretation. When Theorist is called to find a
coherent discourse-level act (i.e., by using the default for intentional acts) it finds that
Russ can perform a fourth-turn repair. The metaplan for this repair, repeated below, is
similar to that for acceptance, but involves the reconstruction of the discourse model.
</bodyText>
<footnote confidence="0.4641885">
42 In the discourse model, this was expressed as expressed(do(m, pretell(m, r, whoIsGoing)), 0), from
which one can assume persists(do(m, pretell(m, r, whoIsGoing)), 2) by default.
</footnote>
<page confidence="0.999504">
463
</page>
<figure confidence="0.893863333333333">
Computational Linguistics Volume 21, Number 4
DEFAULT (2, makeFourthTurnRepair (si , s2, arepiy, ts)) :
active (mistake (si , a intended/ aobserved), ts)
A reconstruction (ts, tSreconstructed)
A expected (Si, a reply, tSreconstructed)
D shouldTry(si, s2, a reply ts).
</figure>
<bodyText confidence="0.9965924">
This metaplan applies because Russ had misunderstood a prior utterance by Mother,
a reconstruction of the discourse is possible, and, within the reconstructed discourse,
an informref is expected (as a reply to the misunderstood askref).&apos;
An informref by Russ is expected (see Section 3.3.3) in the reconstructed dialogue
because:
</bodyText>
<listItem confidence="0.9974426">
• There is a linguistic expectation corresponding to the adjacency pair
askref—informref.
• Russ believes its conditions.
• The linguistic intentions of informref are compatible with the
reconstruction.
</listItem>
<sectionHeader confidence="0.994079" genericHeader="introduction">
5. Related work
</sectionHeader>
<subsectionHeader confidence="0.994343">
5.1 Accounts based on plan recognition
</subsectionHeader>
<bodyText confidence="0.985425423076923">
Plan-based accounts interpret speech acts by chaining from subaction to action, from
actions to effects of other actions, and from preconditions to actions to identify a
plan (i.e., a set of actions) that includes the observed act. Heuristics are applied to
discriminate among alternatives.
5.1.1 Allen and Perrault. Allen and Perrault (1979), Perrault and Allen (1980) show
how plan recognition can be used to understand indirect speech acts (such as the
use of &amp;quot;Can you pass the salt?&amp;quot; as a polite request to pass the salt). To interpret an
utterance, the approach applies a set of context-independent inference rules to identify
all plausible plans. For example, one rule says that if a speaker wants to know the truth
value of some proposition, then she might want the proposition to be made true. The
final interpretation is then determined by a set of rating heuristics, such as &amp;quot;Decrease
the rating of a path if it contains an action whose effects are already true at the time the
action starts.&amp;quot; These rating heuristics are problematic because they conflate linguistic
and pragmatic knowledge with knowledge about the search mechanism itself. This
approach cannot handle more than a few relationships between utterances and plans
and cannot handle any utterances that do not relate to the domain plan in a direct
manner.
Although we have not yet considered the problem of indirect utterances in detail,
we anticipate that such explanations might include as a subtask the kind of plan-
based inference that has been proposed, but this inference would be limited by the
hearer&apos;s own goals and expectations. However, many common uses of indirectness
can be explained by the existence of a well-accepted social convention that makes
them expected.
43 From Mother&apos;s perspective, if indeed she did make an askif in Ti, T4 can be seen as a display of
acceptance of it, because a surface-informref is one way to do an informif. Thus, from her perspective,
she need never recognize that Russ has misunderstood.
</bodyText>
<page confidence="0.998829">
464
</page>
<note confidence="0.596539">
McRoy and Hirst The Repair of Speech Act Misunderstandings
</note>
<table confidence="0.909350875">
Explaining shouldTry(r,m,A,ts(3)), decomp(As,A)
***Reconstructing Turn Number 1***
Suppositions Added:
expressed(do(m, askref(m, r, whoIsGoing)), alt(1))
expressedNot(knowref(m, whoIsGoing), alt(1))
expressed(intend(m, knowref(m, whoIsGoing)), alt(1))
expressed(intend(m, do(r, informref(r, m, whoIsGoing))), alt(1))
Answer: shouldTry(r,m,informref(r,m,whoIsGoing),ts(3)),
</table>
<figure confidence="0.692021529411765">
decomp(surface-informref(r,m,whoIsGoing),
informref(r,m,whoIsGoing)
Explanation:
intentionalAct(r,m,informref(r,m,whoIsGoing),ts(3))
makeFourthTurnRepair(r,m,informref(r,m,whoIsGoing),ts(3),ts(1))
reconstruction(ts(3),ts(alt(1)))
***Updating Discourse Model***
Interpretation: informref(r,m,whoIsGoing) (turn number 4)
expressed(do(r,informref(r,m,whoIsGoing)),4)
Linguistic Intentions of informref(r,m,whoIsGoing):
knowref(r,whoIsGoing)and intend(r,knowref(m,whoIsGoing))
Suppositions Added:
expressed(knowref(r,whoIsGoing),4)
expressed(intend(r,knowref(m,whoIsGoing)),4)
r performed fourth turn repair
***Generating Utterance***
«&lt;surface-informref(r,m,whoIsGoing)
</figure>
<figureCaption confidence="0.821091">
Figure 10
</figureCaption>
<bodyText confidence="0.919684928571429">
The output for turn 4 from Russ&apos;s perspective.
5.1.2 Litman. Work by Litman (1986) attempts to overcome some of the limitations of
Allen and Perrault&apos;s approach by extending the plan hierarchy to include discourse-
level meta plans, in addition to domain-level plans. Metaplans include actions, such as
introduce, continue, or clarify and are recognized, in part, by identifying cue phrases.
Although the metaplans add flexibility by increasing the number of possible paths,
they also add to the problem of pruning and ordering the paths, requiring additional
heuristics. For example, there are specific rules for choosing among alternative meta-
plans on the basis of clue words, implicit expectations, or default preferences. Litman
also adds a new general heuristic: stop chaining if an ambiguity cannot be resolved.
5.1.3 Carberry and Lambert. Carberry (1985, 1987, 1990) uses a similar approach.
Her model introduces a new set of discourse-level goals such as seek-confirmation that
are recognized on the basis of the current properties of the dialogue model and the
mutual beliefs of the participants. Once a discourse-level goal is selected, a set of can-
</bodyText>
<page confidence="0.998408">
465
</page>
<note confidence="0.751534">
Computational Linguistics Volume 21, Number 4
</note>
<bodyText confidence="0.990693088888889">
didate plans is identified, and Allen-style heuristics are applied to choose one of them.
Subsequent work by Lambert and Carberry (1991, 1992) introduces an intermediate,
problem-solving level of plans that link the discourse-level acts to domain plans. The
processing rules, by their specificity, eliminate the need for many of the heuristics. The
sacrifice here is a loss of generality; the mechanisms for recognizing goals are specific
to Carberry&apos;s implementation.
5.1.4 Cawsey. Cawsey (1991) proposes a method of extending Perrault and Allen&apos;s
(1980) inference rule approach to produce repairs. She also suggests including some
of the information captured by various rating heuristics as premises in the rules, allow-
ing that these new premises may be assumed by default. For example, the following
rule is proposed for capturing pretellings:
if request(S1, S2, informif(S2, Si, knowref(S2, D)))
and know(S2, knowref(S1, D))
then know(S2, wants(S1, knowref(S2, D)))
To handle misunderstandings, she suggests that such assumptions be retracted
if they become inconsistent and then any subsequent utterance whose interpretation
depends on a retracted belief be reinterpreted from scratch. This approach is thus
much stronger than most accounts of negotiation, such as ours, which allow that a
participant might choose to forego a complete repair. Allowing defeasible beliefs is a
step in the right direction; however, the approach still misses the point that participants
are able to negotiate meanings. Preconditions such as know(S2, not knowref (S1, D) )
influence interpretations only to the extent that they provide support for, or evidence
against, a particular (abductive) explanation. In Example 2, even if Mother knew who
was going, she could still be asking Russ a question, albeit insincerely. Similarly, even if
Russ suspected that Mother did not know who was going, he might still have chosen
to treat her utterance as a pretelling, perhaps to confirm his suspicions or to delay
answering.
5.1.5 Traum and Hinkelman. Hinkelman&apos;s (1990) work incorporates some abductive
reasoning in her model of utterance interpretation. The model treats different features
in the input, such as the mood of a sentence or the presence of a particular lexical
item, as manifestations of different speech acts. During interpretation, procedures that
test for particular features of the input suggest candidates. The system then removes
any candidates whose implicatures are inconsistent with prior beliefs.
Traum and Hinkelman (1992) extend this work by generalizing the notion of
speech act to conversation act. Conversation acts include traditional speech act types as
well as what Traum and Hinkelman call grounding acts. Conversation acts, however, are
not assumed to be understood without some positive evidence by the receiver, such as
an acknowledgment. Grounding acts include initiating, clarifying, or acknowledging
an utterance, and taking and releasing a turns. These acts differ from our own meta-
plans in that they are organized into a finite state grammar, and do not account for
grounding acts that would violate a receiver&apos;s expectations. In conversation, ground-
ing acts that violate the grammar are not recognized. Traum and Hinkelman suggest
that such violations should be used to trigger a repair, but admit that, except when a
repair has been requested explicitly, the model itself says nothing about when a repair
should be uttered (p. 593).&amp;quot;
</bodyText>
<page confidence="0.8817445">
44 Interpretations that have the right pragmatic force but inconsistent implicatures are ruled out as in
466
</page>
<note confidence="0.753547">
McRoy and Hirst The Repair of Speech Act Misunderstandings
</note>
<bodyText confidence="0.991159">
Traum and Allen (1994) extend the work to include a notion of social obligation,
which serves much the same purpose as expectations in our model.
</bodyText>
<subsectionHeader confidence="0.99892">
5.2 Other expectation-driven accounts
</subsectionHeader>
<bodyText confidence="0.9996415">
Within the speech understanding community, the word &amp;quot;expectation&amp;quot; has been used
differently from our use here. Expectation in the speech context refers to what the next
word or utterance is likely to be about.&apos; For example, after the computer asks the user
to perform some action A, it might expect any of the following types of responses:
</bodyText>
<listItem confidence="0.9991234">
1. A statement about background knowledge that might be needed.
2. A statement about the underlying purpose of A.
3. A statement about related task steps (i.e., subgoals of A, tasks that
contain A as a step, or tasks that might follow A).
4. A statement about the accomplishment of A.
</listItem>
<bodyText confidence="0.999694545454545">
These expectations are independent of the belief state of an agent and are specified
down to the semantic (and sometimes even lexical) level. This information has long
been used to discriminate between ambiguous interpretations and correct mistakes
made by the speech recognizer (Fink and Biermann 1986; Smith 1992). Typically, an
utterance will be interpreted according to the expectation that matches it most closely.
By contrast, our approach and that of the plan-based accounts use &amp;quot;expectation&amp;quot; to
refer to agents&apos; beliefs about how future utterances might relate to prior ones. These
expectations are determined both by an agent&apos;s understanding of typical behavior and
by his or her mental state. These two notions of expectation are complementary, and
any dialogue model that uses speech as input must be able to represent and reason
with both.
</bodyText>
<subsectionHeader confidence="0.99979">
5.3 Approaches to misconception
</subsectionHeader>
<bodyText confidence="0.999865125">
Misconceptions are a deficit in an agent&apos;s knowledge of the world; they can become a
barrier to understanding if they cause an agent to unintentionally evoke a concept or
relation. To prevent misconceptions from triggering a misunderstanding, agents can
check for evidence of misconception and try to resolve apparent errors. The symptoms
of misconception include references to entities that do not map to previously known
objects or operations (Webber and Mays 1983) or requests for clarification (Moore
1989). Errors are corrected by replacing or deleting parts of the problematic utterance
so that it makes sense. Several correction strategies have been suggested:
</bodyText>
<listItem confidence="0.994014857142857">
• Generalize a description by selectively ignoring some constraints (see
Goodman 1985; McCoy 1985, 1986, 1988; Carberry 1988; Calistri-Yeh
1991; Eller and Carberry 1992),
• Make a description more specific by adding extra constraints (see Eller
and Carberry 1992), and
• Choose a conceptual &amp;quot;sibling&amp;quot;, by combining generalization and
constraint operations. For example, if there is more than one strategy for
</listItem>
<footnote confidence="0.985889333333333">
Hinkelman (1990); however, as with grounding acts, presumably the elimination of all possible
interpretations could cue some type of repair mechanism, if they chose to incorporate one.
45 This is the same sense of &amp;quot;expectation&amp;quot; as used by Riesbeck (1974).
</footnote>
<page confidence="0.993922">
467
</page>
<note confidence="0.750433">
Computational Linguistics Volume 21, Number 4
</note>
<bodyText confidence="0.9998717">
achieving a goal, then an entity that corresponds to a step from one
strategy might be replaced by one corresponding to a step from one of
the other strategies (see Carberry 1985, 1987; Eller and Carberry 1992;
Moore 1989).
Although these approaches do quite well at preventing certain classes of misun-
derstandings, they cannot prevent them all. Moreover, these approaches may actually
trigger misunderstandings because they always find some substitution, and yet they
lack any mechanisms for detecting when one of their own previous repairs was inap-
propriate. Thus, a conversational participant will still need to be able to address actual
misunderstandings.
</bodyText>
<subsectionHeader confidence="0.961384">
5.4 Collaboration in the resolution of nonunderstanding
</subsectionHeader>
<bodyText confidence="0.999984647058824">
In this paper, we have concentrated on the repair of mis-understanding. Our colleagues
Heeman and Edmonds have looked at the repair of non-understanding. The difference
between the two situations is that in the former, the agent derives exactly one inter-
pretation of an utterance and hence is initially unaware of any problem; in the latter,
the agent derives either more than one interpretation, with no way to choose between
them, or no interpretation at all, and so the problem is immediately apparent. Heeman
and Edmonds looked in particular at cases in which a referring expression uttered by
one conversant was not understood by the other (Heeman and Hirst 1995; Edmonds
1994; Hirst et al. 1994). Clark and his colleagues (Clark and Wilkes-Gibbs 1986; Clark
1993) have shown that in such situations, conversants will collaborate on repairing
the problem by, in effect, negotiating a reconstruction or elaboration of the referring
expression. Heeman and Edmonds model this with a plan recognition and generation
system that can recognize faulty plans and try to repair them. Thus (as in our own
model) two copies of the system can converse with each other, negotiating referents
of referring expressions that are not understood by trying to recognize the referring
plans of the other, repairing them where necessary, and presenting the new referring
plan to the other for approval.
</bodyText>
<sectionHeader confidence="0.994256" genericHeader="method">
6. Conclusions
</sectionHeader>
<bodyText confidence="0.9999704">
In human dialogues, both the producer and the recipient of an utterance have a say
in determining its interpretation. Moreover, they may both change their minds in
the face of new information. Dialogue participants are able to negotiate the meaning
of utterances because in responding to what the hearer decides are the speaker&apos;s
goals and expectations regarding an utterance, the hearer also provides evidence of
that decision and hence constraints on what the speaker may do next. If the speaker
disagrees with a displayed interpretation, she can challenge it directly or decide to
respond in such a way that the hearer must infer a misunderstanding.
The long-term goal of our work is to construct a model of communicative interac-
tion that will be able to support the negotiation of meaning. We have considered the
information sources and reasoning processes that agents need to determine their be-
liefs about the goals and expectations associated with each other&apos;s utterances. Whereas
previous models of dialogue tend to represent discourse meaning from some global
perspective, make use of either purely structural or purely intentional information,
and give minimal attention to repair, in our model:
</bodyText>
<listItem confidence="0.901933">
• Each agent has his or her own model of the discourse.
</listItem>
<page confidence="0.995821">
468
</page>
<note confidence="0.710244">
McRoy and Hirst The Repair of Speech Act Misunderstandings
</note>
<listItem confidence="0.995854875">
• Agents rely on both structural and intentional information in the
discourse.
• Agents distinguish between intended actions and misunderstandings.
• Agents interpret utterances on the basis of expectations derived from
previous utterances as well as expectations for future actions that are
predicted by the utterance under interpretation.
• Agents are able to detect and repair their own misunderstandings as
well as those of others.
</listItem>
<bodyText confidence="0.999729466666667">
We see this work as providing some of the first steps toward a unified account of
interpretation, generation, and repair.
The primary contributions of this work have been to treat misunderstanding and
repair as intrinsic to conversants&apos; core language abilities and to account for them with
the same processing mechanisms that account for normal speech. In particular, both
interpretation and repair are treated as explanation problems, modeled as abduction.
In order to account for the repair of misunderstandings, we have proposed a repre-
sentation of the discourse that captures the agent&apos;s interpretation of the conversation
both before and after a repair and that is independent of the actual beliefs of the
participants—a dynamic mental artifact that is the object of belief and repair. With
such a record of the discourse, agents can refer to alternative interpretations or to the
repair process itself, potentially enabling them to recover from rejected repairs. By
addressing the problem of repair, this work should facilitate efforts to build natural
language interfaces that can better recover from their own mistakes as well as those
of their users.
</bodyText>
<sectionHeader confidence="0.921964" genericHeader="method">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999507266666667">
This work was supported by the Natural
Sciences and Engineering Research Council
of Canada. We thank Ray Reiter for his
suggestion that we use abduction and James
Allen for his advice about temporal logics.
We thank Hector Levesque, Mike
Gruninger, Sheila McIlraith, Javier Pinto,
and Stephen Shapiro, for their comments on
many of the formal aspects of this work. We
also thank David Chapman, Susan Haller,
Diane Horton, C. Raymond Perrault, Mark
Steedman, Evan Steeg, and the reviewers
for their comments on drafts of this paper
or the thesis on which it is based (McRoy
1993a).
</bodyText>
<sectionHeader confidence="0.960271" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.996632153846154">
Allen, James (1983). &amp;quot;Recognizing intentions
from natural language utterances.&amp;quot; In
Michael Brady, Robert C. Berwick, and
James Allen, editors, Computational Models
of Discourse. The MIT Press, 107-166.
Allen, James, and Perrault, Raymond (1979).
&amp;quot;Plans, inference, and indirect speech
acts.&amp;quot; In 17th Annual Meeting of the
Association for Computational Linguistics,
Proceedings of the Conference, 85-87.
Bach, Kent, and Harnish, Robert M. (1979).
Linguistic Communication and Speech Acts.
The MIT Press.
Beun, Robbert-Jan (1990). &amp;quot;Speech acts and
mental states.&amp;quot; In Proceedings of the Fifth
Rocky Mountain Conference on Artificial
Intelligence, Pragmatics in Artificial
Intelligence, 75-80, Las Cruces, New
Mexico.
Brewka, Gerhard (1989). &amp;quot;Preferred
subtheories: An extended logical
framework for default reasoning.&amp;quot; In
Proceedings of the 11th International Joint
Conference on Artificial Intelligence,
1043-1048, Detroit, MI.
Calistri-Yeh, Randall 1(1991). &amp;quot;Utilizing
user models to handle ambiguity and
misconceptions in robust plan
recognition.&amp;quot; User Modeling and
User-Adapted Interaction, 1(4), 289-322.
Carberry, Sandra (1985). Pragmatics Modeling
in Information Systems Interfaces. Doctoral
dissertation, University of Delaware,
Newark, Delaware.
Carberry, Sandra (1987). &amp;quot;Pragmatic
modeling: Toward a robust natural
language interface.&amp;quot; Computational
Intelligence, 3(3), 117-136.
Carberry Sandra (1988). &amp;quot;Modeling the
</reference>
<page confidence="0.998571">
469
</page>
<note confidence="0.706307">
Computational Linguistics Volume 21, Number 4
</note>
<reference confidence="0.993572762295082">
user&apos;s plans and goals. Computational
Linguistics, 14(3), 23-37.
Carberry, Sandra (1990). Plan Recognition in
Natural Language Dialogue. The MIT Press,
Cambridge, MA.
Cawsey, Alison J. (1991). &amp;quot;A belief revision
model of repair sequences in dialogue.&amp;quot;
In Ernesto Costa, editor, New Directions in
Intelligent Tutoring Systems. Springer
Verlag.
Charniak, Eugene, and Goldman, Robert
(1988). &amp;quot;A logic for semantic
interpretation.&amp;quot; In 26th Annual Meeting of
the Association for Computational Linguistics,
Proceedings of the Conference, 87-94, Buffalo,
NY.
Clark, Herbert H. (1993). Arenas of Language
Use. The University of Chicago Press, and
Stanford: Center for the Study of
Language and Information.
Clark, Herbert H., and Wilkes-Gibbs,
Deanna (1986). &amp;quot;Referring as a
collaborative process.&amp;quot; Cognition, 22:1-39.
(Reprinted in Intentions in Communication,
edited by Philip R. Cohen, Jerry Morgan,
and Martha Pollack. The MIT Press,
pages 463-493.)
Cohen, Philip R., and Levesque, Hector
(1985). &amp;quot;Speech acts and rationality.&amp;quot; In
The 23rd Annual Meeting of the Association
for Computational Linguistics, Proceedings of
the Conference, 49-60, Chicago.
Edmonds, Philip G. (1994). &amp;quot;Collaboration
on reference to objects that are not
mutually known.&amp;quot; In Proceedings, 15th
International Conference on Computational
Linguistics (COLING-94), 1118-1122, Kyoto.
Eller, Rhonda, and Carberry, Sandra (1992).
&amp;quot;A meta-rule approach to flexible plan
recognition in dialogue.&amp;quot; User Modeling
and User-Adapted Interaction, 2(1-2), 27-53.
Fink, Pamela E., and Biermann, Alan W.
(1986). &amp;quot;The correction of ill-formed input
using history-based expectation with
applications to speech understanding.&amp;quot;
Computational Linguistics, 12(1), 13-36.
Fox, Barbara (1987). &amp;quot;Interactional
reconstruction in real-time language
processing.&amp;quot; Cognitive Science, 11,365-387.
Goodman, Bradley (1985). &amp;quot;Repairing
reference identification failures by
relaxation.&amp;quot; In 23th Annual Meeting of the
Association for Computational Linguistics,
Proceedings of the Conference, 204-217,
Chicago.
Grice, H. P. (1957). &amp;quot;Meaning.&amp;quot; The
Philosophical Review, 66,377-388.
Heeman, Peter, and Hirst, Graeme (1995).
Collaborating on referring expressions.
Computational Linguistics, 21(3).
Hinkelman, Elizabeth A. (1990). &amp;quot;Linguistic
and Pragmatic Constraints on Utterance
Interpretation.&amp;quot; Doctoral dissertation,
Department of Computer Science,
University of Rochester, Rochester, New
York. Published as University of
Rochester Computer Science Technical
Report 288, May 1990.
Hirst, Graeme; McRoy, Susan; Heeman,
Peter; Edmonds, Philip; and Horton,
Diane (1994). &amp;quot;Repairing conversational
misunderstandings and
non-understandings.&amp;quot; Speech
Communication, 15,213-229.
Hobbs, Jerry R.; Stickel, Mark; Martin, Paul;
and Edwards, Douglas (1988).
&amp;quot;Interpretation as abduction.&amp;quot; In 26th
Annual Meeting of the Association for
Computational Linguistics, Proceedings of the
Conference, 95-103.
Hobbs, Jerry R.; Stickel, Mark; Appelt,
Douglas E.; and Martin, Paul (1993).
&amp;quot;Interpretation as abduction.&amp;quot; Artificial
Intelligence, 63,69-142.
Jose, Paul E. (1988). &amp;quot;Sequentiality of speech
acts in conversational structure.&amp;quot; Journal of
Psycholinguistic Research, 17(1), 65-88.
Lambert, Lynn, and Carberry, Sandra (1991).
&amp;quot;A tri-partite plan-based model of
dialogue.&amp;quot; In 29th Annual Meeting of the
Association for Computational Linguistics,
Proceedings of the Conference, 47-54,
Berkeley, CA.
Lambert, Lynn, and Carberry, Sandra (1992).
&amp;quot;Modeling negotiation dialogues.&amp;quot; In 30th
Annual Meeting of the Association for
Computational Linguistics, Proceedings of the
Conference, 193-200, Newark, Delaware.
Litman, Diane J. (1986). &amp;quot;Linguistic
coherence: A plan-based alternative.&amp;quot; In
24th Annual Meeting of the Association for
Computational Linguistics, Proceedings of the
Conference, 215-223, New York.
Loveland, D. W. (1978). Automated Theorem
Proving: A Logical Basis. North-Holland,
Amsterdam, The Netherlands.
Luperfoy, Susann (1992). &amp;quot;The
representation of multimodal user
interface dialogues using discourse pegs.&amp;quot;
In 30th Annual Meeting of the Association for
Computational Linguistics, Proceedings of the
Conference, 22-31, Newark, Delaware.
McCoy, Kathleen F. (1985). &amp;quot;The role of
perspective in responding to property
misconceptions.&amp;quot; In Proceedings of the
Ninth International Joint Conference on
Artificial Intelligence, volume 2,791-793.
McCoy, Kathleen F. (1986). &amp;quot;The ROMPER
system: responding to object-related
misconceptions using perspective.&amp;quot; In
24th Annual Meeting of the Association for
Computational Linguistics, Proceedings of the
</reference>
<page confidence="0.996735">
470
</page>
<note confidence="0.893336">
McRoy and Hirst The Repair of Speech Act Misunderstandings
</note>
<reference confidence="0.976799866666667">
Conference, 97-105.
McCoy, Kathleen F. (1988). &amp;quot;Reasoning on a
highlighted user model to respond to
misconceptions.&amp;quot; Computational Linguistics,
14(3), 52-63.
McCoy, Kathleen E (1989). &amp;quot;Generating
context-sensitive responses to object
misconceptions.&amp;quot; Artificial Intelligence,
41(2), 157-195.
McLaughlin, Margaret L. (1984).
Conversation: How Talk is Organized. Sage
Publications, Beverly Hills.
McRoy, Susan W. (1993a). Abductive
Interpretation and Reinterpretation of Natural
Language Utterances. Doctoral dissertation,
Department of Computer Science,
University of Toronto, Toronto, Canada.
Published as CSRI Technical Report No.
288, University of Toronto, Department of
Computer Science.
McRoy, Susan W. (1993b). &amp;quot;Belief as an
effect of an act of introspection.&amp;quot; In /993
AAAI Spring Symposium on Reasoning about
Mental States: Formal Theories and
Applications, 86-89, Stanford University.
Moore, Johanna D. (1989). &amp;quot;Responding to
&amp;quot;Huh?&amp;quot;: Answering vaguely articulated
follow-up questions.&amp;quot; In Conference on
Human Factors in Computing Systems
(CHI&apos;89), 91-96, Austin, TX. ACM Press /
Addison-Wesley. Also published as a
special issue of SIGCHI Bulletin
(unnumbered).
Nagata, Masaaki, and Morimoto, Tsuyoshi
(1993). &amp;quot;An experimental statistical
dialogue model to predict the speech act
type of the next utterance.&amp;quot; In
International Symposium on Spoken Dialogue,
Tokyo, Japan.
Perrault, C. Raymond (1990). &amp;quot;An
application of default logic to speech act
theory.&amp;quot; In Intentions in Communication.
Edited by Philip R. Cohen, Jerry Morgan,
and Martha Pollack, The MIT Press,
161-186. An earlier version of this paper
was published as Technical Report
CSLI-87-90 by the Center for the Study of
Language and Information.
Perrault, C. Raymond, and Allen, James
(1980). &amp;quot;A plan-based analysis of indirect
speech acts.&amp;quot; Computational Linguistics, 6,
167-183.
Pollack, Martha E. (1986a). &amp;quot;Inferring
domain plans in question-answering.&amp;quot;
Technical Report TR 403, Artificial
Intelligence Center, SRI International,
Menlo Park, CA.
Pollack, Martha E. (1986b). &amp;quot;A model of
plan inference that distinguishes between
the beliefs of actors and observers.&amp;quot; In
Proceedings, 24th annual meeting of the
Association for Computational Linguistics,
207-214, New York.
Pollack, Martha E. (1990). &amp;quot;Plans as complex
mental attitudes.&amp;quot; In Intentions in
Communication. Edited by Philip Cohen,
Jerry Morgan, and Martha Pollack, MIT
Press, 77-103.
Poole, David; Goebel, Randy; and Aleliunas,
Romas (1987). &amp;quot;Theorist: A logical
reasoning system for defaults and
diagnosis.&amp;quot; In The Knowledge Frontier:
Essays in the Representation of Knowledge.
Edited by Nick Cercone and Gordon
McCalla, Springer-Verlag, New York,
331-352. Also published as Research
Report CS-86-06, Faculty of Mathematics,
University of Waterloo, February, 1986.
Reithinger, Norbert, and Maier, Elisabeth
(1995). &amp;quot;Utilizing statistical dialogue act
processing in verbmobil.&amp;quot; In Proceedings,
33rd annual meeting of the Association for
Computational Linguistics, 116-121,
Cambridge, MA, June.
Riesbeck, Christopher (1974).
&amp;quot;Computational understanding: Analysis
of sentences and context.&amp;quot; Technical
Report STAN-CS-74-437, Computer
Science Department, Stanford University.
Schegloff, Emanuel A. (1988). &amp;quot;Presequences
and indirection: Applying speech act
theory to ordinary conversation.&amp;quot; Journal
of Pragmatics, 12,55-62.
Schegloff, Emanuel A. (1992). &amp;quot;Repair after
next turn: The last structurally provided
defense of intersubjectivity in
conversation.&amp;quot; American Journal of
Sociology, 97(5), 1295-1345.
Schegloff, Emanuel A., and Sacks, Harvey
(1973). &amp;quot;Opening up closings.&amp;quot; Semiotica,
7,289-327.
Smith, Ronnie (1992). &amp;quot;A Computational
Model of Expectation-Driven
Mixed-Initiative Dialog Processing.&amp;quot;
Doctoral dissertation, Department of
Computer Science, Duke University,
Durham, NC.
Spencer, Bruce (1990). &amp;quot;Assimilation in plan
recognition via truth maintenance with
reduced redundancy.&amp;quot; Doctoral
dissertation, Department of Computer
Science, University of Waterloo.
Stickel, Mark E. (1989). &amp;quot;A Prolog
technology theorem prover.&amp;quot; Journal of
Automated Reasoning, 4,353-360.
Terasaki, A. (1976). &amp;quot;Pre-announcement
sequences in conversation.&amp;quot; Social Science
Working Paper 99, School of Social
Science, University of California, Irvine.
Traum, David, and Allen, James (1994).
</reference>
<page confidence="0.996789">
471
</page>
<table confidence="0.41022125">
Computational Linguistics Volume 21, Number 4
&amp;quot;Discourse obligations in dialogue
processing.&amp;quot; In 32nd Annual Meeting of the
Association for Computational Linguistics,
</table>
<reference confidence="0.99772234375">
Proceedings of the Conference, 1-8, Las
Cruces, NM.
Traum, David, and Hinkelman, Elizabeth
(1992). &amp;quot;Conversation acts in task-oriented
spoken dialogue.&amp;quot; Computational
Intelligence, 8(3). Special Issue:
Computational Approaches to Non-Literal
Language.
Tsui, Amy B. M. (1991). &amp;quot;Sequencing rules
and coherence in discourse.&amp;quot; Journal of
Pragmatics, 15, 111-129.
Umrigar, Zerksis D., and Pitchumani, Vijay
(1985). &amp;quot;An experiment in programming
with full first-order logic.&amp;quot; In Symposium of
Logic Programming, Boston, MA. IEEE
Computer Society Press.
van Arragon, Paul (1990). &amp;quot;Nested Default
Reasoning for User Modeling.&amp;quot; Doctoral
dissertation, Department of Computer
Science, University of Waterloo, Waterloo,
Ontario. Published by the department as
Research Report CS-90-25.
Walker, Marilyn A. (1991). &amp;quot;Redundancy in
collaborative dialogue.&amp;quot; In 1991 AAAI Fall
Symposium on Discourse Structure in Natural
Language Understanding and Generation,
124-129, Pacific Grove, Monterey, CA.
Webber, Bonnie L., and Mays, Eric (1983).
&amp;quot;Varieties of user misconceptions:
Detection and correction.&amp;quot; In Proceedings
of the Eighth International Joint Conference on
Artificial Intelligence, 650-652, Karlsruhe.
</reference>
<sectionHeader confidence="0.745462" genericHeader="method">
Appendix A: A third-turn repair
</sectionHeader>
<bodyText confidence="0.99984075">
This appendix gives an annotated version of the output for the following example of
third-turn repair (from McLaughlin (1984)). We will consider the results of a simulation
in which two copies of our system, each with its own beliefs and goals, converse with
each other.
</bodyText>
<equation confidence="0.8718122">
Example 4
Ti A: When is the dinner for Alfred?
T2 B: Is it at seven-thirty?
T3 A: No, I&apos;m asking you.
T4 B: Oh, I don&apos;t know.
</equation>
<bodyText confidence="0.747245375">
For this example, we assume that A (a) wants B (b) to tell her the time of the
dinner for Alfred, that she believes that she does not already know, that he knows
when it is, and that he believes that she does not know, of any given time (including
seven-thirty), whether it is the time of the dinner. Notice that the last belief of a, is
her belief about what b believes about what she knows.
fact hasGoal(A, do(b, informref(b, a, whenD)),ts(0)).
fact believe(a, not knowref(a, whenD)).
fact believe(a, knowref(b, whenD)).
fact believe(b, not knowif(a, dinnerAtSevenThirty)).
We also assume that speaker b, believes that he does not know the time of the dinner,
but suspects it is at seven-thirty and believes that a does know when it is.
fact believe(b, not knowref(b, whenD)).
fact believe(b, suspectRef(b, whenD, dinnerAtSevenThirty)).
fact believe(b, knowref(a, whenD)).
We will show the output for both sides of the conversation, showing the perspective
of speaker b as boxed text.
</bodyText>
<page confidence="0.990361">
472
</page>
<figure confidence="0.971949607142857">
McRoy and Hirst The Repair of Speech Act Misunderstandings
First, speaker a generates a request to try to satisfy her goal:
When is the dinner for Alfred?
I ?- startDialoguel.
Explaining shouldTry(a,b,A,ts(0)),decomp(U,A)
Answer:
shouldTry(a,b,askref(a,b,whenD),ts(0)),
decomp(surface-request(a,b,informref(b,a,whenD)),
askref(a,b,whenD))
Explanation:
intentionalAct(a,b,askref(a,b,whenD),ts(0))
adoptPlan(a,b,askref(a,b,whenD),
informref(b,a,whenD),ts(0))
***Updating Discourse Model***
Interpretation: askref(a,b,whenD) (turn number 1)
Suppositions Added:
expressed(do(a,askref(a,b,whenD)),1)
expressedNot(knowref(a,whenD),1)
expressed(intend(a,knowref(a,whenD)),1)
expressed(intend(a,do(b,informref(b,a,whenD))),1)
Agent a adopted plan to achieve: informref(b,a,whenD)
***Generating Utterance***
&lt;&lt;&lt;surface-request(a,b,informref(b,a,whenD))
Speaker b interprets this utterance as a test:
I ?- startDialogue2.
&gt;&gt;&gt;surface-request(a,b,informref(b,a,whenD))
***Interpreting Utterance***
Explaining utter(a,b,request(a,b,informref(b,a,whenD)),ts(0))
</figure>
<reference confidence="0.693572545454545">
Is formula pickForm(a,b,surface-request(a,b,informref(b,a,whenD)),
askref(a,b,whenD),ts(0)) ok (y/n)?n.
Is formula pickForm(a,b,request(a,b,surface-informref(b,a,whenD)),
testref(a,b,whenD),ts(0)) ok (y/n)?y.
Explanation:
intentionalAct(a,b,testref(a,b,whenD),ts(0))
adoptPlan(a,b,testref(a,b,whenD),assertref(b,a,whenD,ts(0))
credulousB(a,knowref(b,whenD))
credulousI(a,ts(0))
pickForm(a,b,surface-request(a,b,assertref(b,a,whenD)),
testref(a,b,whenD),ts(0))
</reference>
<page confidence="0.977111">
473
</page>
<figure confidence="0.853432111111111">
Computational Linguistics Volume 21, Number 4
***Updating Discourse Model***
Interpretation: testref(a,b,whenD) (turn number 1)
Suppositions Added:
expressed(do(a,testref(a,b,whenD)),1)
expressed(knowref(a,whenD),1)
expressed(intend(a,do(b, assertref(b,a,whenD))),1)
Agent a adopted plan to achieve: assertref(b,a,whenD)
Speaker b now replies: Is it at seven-thirty?
</figure>
<bodyText confidence="0.99375175">
He produces this reply because he has a linguistic expectation that says that &amp;quot;if someone is
testing you and you suspect that X is the answer to their test, the appropriate thing to do
is to (tentatively) propose X as the answer&amp;quot; 46 (In the explanation below, this expectation
is indicated by the default &amp;quot;expectedReply&amp;quot;).
</bodyText>
<figure confidence="0.961603761904762">
Explaining shouldTry(b,a,A,ts(1)), decomp(U,A)
Answer:
shouldTry(b,a,askif(b,a,dinnerAtSevenThirty),ts(1)),
decomp(surface-request(b,a,informif(a,b,dinnerAtSevenThirty)),
askif(b,a,dinnerAtSevenThirty))
Explanation:
intentionalAct(b,a,askif(b,a,dinnerAtSevenThirty),ts(1))
acceptance(b,askif(b,a,dinnerAtSevenThirty),ts(1))
expectedReply(do(a,testref(a,b,whenD)),
suspectRef(b,whenD,dinnerAtSevenThirty),
do(b,askif(b,a,dinnerAtSevenThirty)),ts(1))
***Updating Discourse Model***
Interpretation: askif(b,a,dinnerAtSevenThirty) (turn number 2)
Suppositions Added:
expressed(do(b,askif(b,a,dinnerAtSevenThirty)),2)
expressedNot(knowif(b,dinnerAtSevenThirty),2)
expressed(intend(b,knowif(b,dinnerAtSevenThirty)),2)
expressed(intend(b,do(a,informif(a,b,dinnerAtSevenThirty))),2)
Agent b performed expected act: askif(b,a,dinnerAtSevenThirty)
***Generating Utterance***
&lt;&lt;&lt;surface-request(b,a,informif(a,b,dinnerAtSevenThirty))
</figure>
<footnote confidence="0.83815025">
46 suspectRef, like knowsBetterRef, is a supposition of a less than certain belief that an agent knows the
answer. Except for allowing an agent the (extra) possibility of responding with askif, it expresses the
same information as not knowref. Under our initial assumptions, speaker b might also have answered
&amp;quot;I don&apos;t know.&amp;quot;
</footnote>
<page confidence="0.995729">
474
</page>
<figure confidence="0.911029708333333">
McRoy and Hirst The Repair of Speech Act Misunderstandings
Speaker a recognizes that Speaker b has produced a yes-no question, misunderstanding her
request as a test:
&gt;&gt;&gt;surface-request(b,a,informif(a,b,dinnerAtSevenThirty))
***Interpreting Utterance***
Explaining utter(b,a,surface-request(b,a,informif(a,b,dinnerAtSevenThirty)),ts(1))
Is formula pickForm(b,a,surface-request(b,a,informif(a,b,dinnerAtSevenThirty)),
askif(b,a,dinnerAtSevenThirty),ts(1)) ok (y/n)?y.
Explanation:
otherMisunderstanding(b,a,mistake(b,askref(a,b,whenD),
testref(a,b,whenD)),
askif(b,a,dinnerAtSevenThirty),ts(1))
credulousB(b,suspectRef(b,whenD,dinnerAtSevenThirty))
pickForm(b,a,surface-request(b,a,informif(a,b,dinnerAtSevenThirty)),
askif(b,a,dinnerAtSevenThirty),ts(1))
***Updating Discourse Model***
Interpretation: askif(b,a,dinnerAtSevenThirty) (turn number 2)
Suppositions Added:
expressed(do(b,askif(b,a,dinnerAtSevenThirty)),2)
expressedNot(knowif(b,dinnerAtSevenThirty),2)
expressed(intend(b,knowif(b,dinnerAtSevenThirty)),2)
expressed(intend(b,do(a,informif(a,b,dinnerAtSevenThirty))),2)
Agent b mistook askref(a,b,whenD) for testref(a,b,whenD):
expressed(mistake(b,askref(a,b,whenD),testref(a,b,whenD)),2)
</figure>
<reference confidence="0.898930923076923">
Speaker a then produces a third-turn repair:
No, I&apos;m asking you.
Explaining shouldTry(a,b,A,ts(2)), decomp(U,A)
Answer:
shouldTry(a,b,inform(a,b,do(a,askref(a,b,whenD))),ts(2)),
decomp(surface-inform(a,b,do(a,askref(a,b,whenD))),
inform(a,b,do(a,askref(a,b,whenD))))
Explanation:
intentionalAct(a,b,inform(a,b,do(a,askref(a,b,whenD))),ts(2))
makeThirdTurnRepair(a,b,informref(b,a,whenD),ts(2))
***Updating Discourse Model***
Interpretation:
inform(a,b,do(a,askref(a,b,whenD))) (turn number 3)
</reference>
<page confidence="0.962053">
475
</page>
<figure confidence="0.880880138888889">
Computational Linguistics Volume 21, Number 4
Suppositions Added:
expressed(do(a,inform(a,b,do(a,askref(a,b,whenD)))),3)
expressed(do(avaskref(a,b,whenD)),3)
expressed(intend(a,knowif(b,do(a,askref(a,b,whenD)))),3)
m performed third turn repair
***Generating Utterance***
&lt;&lt;&lt;surface-inform(a,b,do(a,askref(a,b,whenD)))
Speaker b recognizes his misunderstanding:
&gt;&gt;&gt;surface-inform(a,b,do(a,askref(a,b,whenD)))
***Interpreting Utterance***
Explaining utter(a,b,inform(a,b,do(a,askref(a,b,whenD))), ts(2))
Is formula
pickForm(a,b,
surface-inform(a,b,do(a,askref(a,b,whenD))),
inform(a,b,do(a,askref(a,b,whenD))), ts(2))
ok (y/n)?y.
Explanation:
persists(do(a,testref(a,b,whenD)),2)
selfMisunderstanding(a,b,mistake(b,askref(a,b,whenD),
testref(a,b,whenD)),
inform(a,b,
do(a,askref(a,b,whenD))), ts(2))
pickForm(a,b,
surface-inform(a,b,do(a,askref(a,b,whenD))),
inform(a,b,do(a,askref(a,b,whenD))),ts(2))
***Updating Discourse Model***
Interpretation:
inform(a,b,do(a,askref(a,b,whenD))) (turn number 3)
Suppositions Added:
expressed(do(a,inform(a,b,do(a,askref(a,b,whenD)))),3)
expressed(do(a,askref(a,b,whenD)),3)
expressed(intend(a, knowif(b,do(a,askref(a,b,whenD)))), 3)
Agent b mistook askref(a,b,whenD) for testref(a,b,whenD):
expressed(mistake(b,askref(a,b,whenD),testref(a,b,whenD)),3)
Speaker b produces a fourth-turn repair: Oh, I don&apos;t know.
</figure>
<reference confidence="0.7081945">
Explaining shouldTry(b,a,A,ts(3)), decomp(U,A)
***Reconstructing Turn Number 1***
</reference>
<page confidence="0.977302">
476
</page>
<note confidence="0.433191">
McRoy and Hirst The Repair of Speech Act Misunderstandings
</note>
<reference confidence="0.796565731707317">
expressed(do(a,askref(a,b,whenD)),alt(1))
expressedNot(knowref(a,whenD),alt(1))
expressed(intend(a,knowref(a,whenD)),alt(1))
expressed(intend(a,do(b,informref(b,a,whenD))),alt(1))
Answer:
shouldTry(b,a,inform(b,a,not knowref(b,whenD)),ts(3)),
decomp(surface-inform(b,a,not knowref(b,whenD)),
inform(b,a,not knowref(b,whenD)))
Explanation:
intentionalAct(b,a,inform(b,a,not knowref(b,whenD)),ts(3))
makeFourthTurnRepair(b,a,inform(b,a,not knowref(b,whenD)),
ts(3),ts(alt(1)))
expectedReply(do(a,askref(a,b,whenD)),
not knowref(b,whenD),
do(b,inform(b,a,not knowref(b,whenD))),
ts(alt(1)))
reconstruction(ts(3),ts(alt(1)))
***Updating Discourse Model***
Interpretation: inform(b,a,not knowref(b,whenD)) (turn number 4)
Suppositions Added:
expressed(do(b,inform(b,a,not knowref(b,whenD))),4)
expressedNot(knowref(b,whenD),4)
expressed(intend(b,knowif(a,not knowref(b,whenD))),4)
r performed fourth turn repair
***Generating Utterance***
&lt;&lt;&lt;surface-inform(b,a,not knowref(b,whenD))
Speaker a takes this utterance as an acceptance of her initial request.
&gt;&gt;&gt;surface-inform(b,a,not knowref(b,whenD))
***Interpreting Utterance***
Explaining utter(b,a,inform(b,a,not knowref(b,whenD)),ts(3))
Is formula pickForm(b,a,surface-inform(b,a,not knowref(b,whenD)),
inform(b,a,not knowref(b,whenD)),ts(3)) ok (y/n)?y.
Explanation:
persists(do(a,askref(a,b,whenD)),3)
persists(do(a,askref(a,b,whenD)),2)
intentionalAct(b,a,inform(b,a,not knowref(b,whenD)),ts(3))
acceptance(b,inform(b,a,not knowref(b,whenD)),ts(3))
expectedReply(do(a,askref(a,b,whenD)),
not knowref(b,whenD),
do(b,inform(b,a,not knowref(b,whenD))),ts(3))
credulousB(b,not knowref(b,whenD))
</reference>
<page confidence="0.991439">
477
</page>
<figure confidence="0.9656657">
Computational Linguistics Volume 21, Number 4
pickForm(b,a,surface-inform(b,a,not knowref(b,whenD)),
inform(b,a,not knowref(b,whenD)),ts(3))
***Updating Discourse Model***
Interpretation: inform(b,a,not knowref(b,whenD)) (turn number 4)
Suppositions Added:
expressed(do(b,inform(b,a,not knowref(b,whenD))),4)
expressedNot(knowref(b,whenD),4)
expressed(intend(b,knowif(a,not knowref(b,whenD))),4)
Agent b performed expected act: inform(b,a,not knowref(b,whenD))
</figure>
<page confidence="0.982871">
478
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.902320">
<title confidence="0.986548666666667">The Repair of Speech Act Misunderstandings by Abductive Inference</title>
<author confidence="0.99961">Susan W McRoy Graeme Hirstt</author>
<affiliation confidence="0.996964">University of Wisconsin-Milwaukee University of Toronto</affiliation>
<abstract confidence="0.996018833333333">During a conversation, agents can easily come to have different beliefs about the meaning or discourse role of some utterance. Participants normally rely on their expectations to determine whether the conversation is proceeding smoothly: if nothing unusual is detected, then understanding is presumed to occur. Conversely, when an agent says something that is inconsistent with another&apos;s expectations, then the other agent may change her interpretation of an earlier turn direct her response to the reinterpretation, accomplishing what is known as a repair. Here we describe an abductive account of the interpretation of speech acts and the repair of speech act misunderstandings. Our discussion considers the kinds of information that participants use to interpret an utterance, even if it is inconsistent with their beliefs. It also considers the information used to design repairs. We describe a mapping between the utterance-level forms (semantics) and discourse-level acts (pragmatics), and a relation between the discourse acts and the beliefs and intentions that they express. We specify for each discourse act, the acts that might be expected, if the hearer has understood the speaker correctly. We also describe our account of belief and intention, distinguishing the beliefs agents actually have from the ones they act as if they have when they perform a discourse act. To support repair, we model how misunderstandings can lead to unexpected actions and utterances and describe the processes of interpretation and repair. To illustrate the approach, we show how it accounts for an example repair.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allen</author>
</authors>
<title>Recognizing intentions from natural language utterances.&amp;quot;</title>
<date>1983</date>
<booktitle>Computational Models of Discourse. The</booktitle>
<pages>107--166</pages>
<editor>In Michael Brady, Robert C. Berwick, and James Allen, editors,</editor>
<publisher>MIT Press,</publisher>
<contexts>
<context position="18563" citStr="Allen (1983)" startWordPosition="2870" endWordPosition="2871">less they have (and can provide) a valid reason not to. Linguistic intentions are based on Grice&apos;s (1957) notion of reflexive intention. For example, an inform(S,H,P) expresses the linguistic intentions whose content is P and intend(S,know(H,P)) (i.e., the speaker intends the hearer to believe (1) that P is true and (2) that the speaker intends that the hearer know P). Linguistic expectations capture the notion of adjacency pairs.&apos; In defining linguistic intentions, which are shown in Figure 1, we have followed existing speech act taxonomies, especially those given by Bach and Harnish (1979), Allen (1983), and Hinkelman (1990).7 Thus, when a speaker produces an askref about P she expresses (and thereby intends the hearer to recognize that she expresses) that she does not know the referent of some description in P. intends to find out the referent of that description, and intends the hearer to tell her that referent. If the speaker is sincere, she actually believes the content of what she expresses; if the hearer is trusting, he might come to believe that she believes it. Following Schegloff&apos;s (1988) analysis of Example 2, we provide a speech act definition for prete11.8 In order to capture the</context>
</contexts>
<marker>Allen, 1983</marker>
<rawString>Allen, James (1983). &amp;quot;Recognizing intentions from natural language utterances.&amp;quot; In Michael Brady, Robert C. Berwick, and James Allen, editors, Computational Models of Discourse. The MIT Press, 107-166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Allen</author>
<author>Raymond Perrault</author>
</authors>
<title>Plans, inference, and indirect speech acts.&amp;quot;</title>
<date>1979</date>
<booktitle>In 17th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference,</booktitle>
<pages>85--87</pages>
<contexts>
<context position="16700" citStr="Allen and Perrault 1979" startWordPosition="2583" endWordPosition="2586">erstanding, the communication might mislead them into prematurely believing that their goals have been achieved. • The need for an alternative to the notion of mutual belief. Typically, models rely on mutual beliefs without accounting for how speakers achieve them or for why speakers should believe that they have achieved them. 2.1 Using social conventions to guide interpretation and repair Our account of interpretation avoids the extended inference required by plan-based models by reversing the standard dependency between an agent&apos;s expectations and task-related goals. Plan-based approaches (Allen and Perrault 1979; Litman 1986; Carberry 1990; Lambert and Carberry 1991) start by applying context-independent inference rules to identify the agent&apos;s task-related plan, possibly favoring alternatives that extend a previously recognized plan. By contrast, our approach begins with an expectation, using it to premise both the analysis of utterance meaning and any inference 439 Computational Linguistics Volume 21, Number 4 about an agent&apos;s goals. Moreover, our approach treats apparent conflicts with expectations as meaningful; for example, if an utterance is inconsistent with expectations, then the reasoner will</context>
<context position="81241" citStr="Allen and Perrault (1979)" startWordPosition="12299" endWordPosition="12302">ructed dialogue because: • There is a linguistic expectation corresponding to the adjacency pair askref—informref. • Russ believes its conditions. • The linguistic intentions of informref are compatible with the reconstruction. 5. Related work 5.1 Accounts based on plan recognition Plan-based accounts interpret speech acts by chaining from subaction to action, from actions to effects of other actions, and from preconditions to actions to identify a plan (i.e., a set of actions) that includes the observed act. Heuristics are applied to discriminate among alternatives. 5.1.1 Allen and Perrault. Allen and Perrault (1979), Perrault and Allen (1980) show how plan recognition can be used to understand indirect speech acts (such as the use of &amp;quot;Can you pass the salt?&amp;quot; as a polite request to pass the salt). To interpret an utterance, the approach applies a set of context-independent inference rules to identify all plausible plans. For example, one rule says that if a speaker wants to know the truth value of some proposition, then she might want the proposition to be made true. The final interpretation is then determined by a set of rating heuristics, such as &amp;quot;Decrease the rating of a path if it contains an action w</context>
</contexts>
<marker>Allen, Perrault, 1979</marker>
<rawString>Allen, James, and Perrault, Raymond (1979). &amp;quot;Plans, inference, and indirect speech acts.&amp;quot; In 17th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, 85-87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kent Bach</author>
<author>Robert M Harnish</author>
</authors>
<title>Linguistic Communication and Speech Acts.</title>
<date>1979</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="18549" citStr="Bach and Harnish (1979)" startWordPosition="2866" endWordPosition="2869">ts conventional reply, unless they have (and can provide) a valid reason not to. Linguistic intentions are based on Grice&apos;s (1957) notion of reflexive intention. For example, an inform(S,H,P) expresses the linguistic intentions whose content is P and intend(S,know(H,P)) (i.e., the speaker intends the hearer to believe (1) that P is true and (2) that the speaker intends that the hearer know P). Linguistic expectations capture the notion of adjacency pairs.&apos; In defining linguistic intentions, which are shown in Figure 1, we have followed existing speech act taxonomies, especially those given by Bach and Harnish (1979), Allen (1983), and Hinkelman (1990).7 Thus, when a speaker produces an askref about P she expresses (and thereby intends the hearer to recognize that she expresses) that she does not know the referent of some description in P. intends to find out the referent of that description, and intends the hearer to tell her that referent. If the speaker is sincere, she actually believes the content of what she expresses; if the hearer is trusting, he might come to believe that she believes it. Following Schegloff&apos;s (1988) analysis of Example 2, we provide a speech act definition for prete11.8 In order </context>
</contexts>
<marker>Bach, Harnish, 1979</marker>
<rawString>Bach, Kent, and Harnish, Robert M. (1979). Linguistic Communication and Speech Acts. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robbert-Jan Beun</author>
</authors>
<title>Speech acts and mental states.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings of the Fifth Rocky Mountain Conference on Artificial Intelligence, Pragmatics in Artificial Intelligence,</booktitle>
<pages>75--80</pages>
<location>Las Cruces, New Mexico.</location>
<contexts>
<context position="38808" citStr="Beun 1990" startWordPosition="6082" endWordPosition="6083">representation languages, such as one based on case semantics, would also be compatible with the approach and would permit greater flexibility. The cost of the increased flexibility would be increased difficulty in mapping surface descriptions onto speech acts; however, because less effort would be required in sentence processing, the total complexity of the problem need not increase. Using a more finely-grained representation, one could reason about sentence type, particles, and prosody explicitly, instead of requiring the sentence processor to interpret this information (cf. Hinkelman 1990; Beun 1990). 20 We also presume that a parser can recognize surface-informref and surface-informif syntactically when the input is a sentence fragment, but it would not hurt our analysis to input them all as surface-inform. 446 McRoy and Hirst The Repair of Speech Act Misunderstandings The theory includes the discourse-level acts inform, informif, informref, assert, assertif, assertref, askref, askif, request, pretell, testref, and warn, which we represent using a similar notation.&amp;quot;&apos;&amp;quot; 3.2.3 Turn sequences. A turn sequence represents the interpretations of the discourse that a participant has considered u</context>
</contexts>
<marker>Beun, 1990</marker>
<rawString>Beun, Robbert-Jan (1990). &amp;quot;Speech acts and mental states.&amp;quot; In Proceedings of the Fifth Rocky Mountain Conference on Artificial Intelligence, Pragmatics in Artificial Intelligence, 75-80, Las Cruces, New Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerhard Brewka</author>
</authors>
<title>Preferred subtheories: An extended logical framework for default reasoning.&amp;quot;</title>
<date>1989</date>
<booktitle>In Proceedings of the 11th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1043--1048</pages>
<location>Detroit, MI.</location>
<contexts>
<context position="29247" citStr="Brewka 1989" startWordPosition="4509" endWordPosition="4510"> will now consider an axiomatization of the model. 3. The architecture of the model Our model characterizes a participant in a dialogue, alternately acting as speaker and hearer. In this section, we will give both the knowledge structures that enable the participant&apos;s behavior and the reasoning algorithms that produce it. (Section 4 and Appendix A present machine-to-machine dialogues involving two instantiations of the implemented model.) 3.1 The reasoning framework: Prioritized Theorist The model has been formulated using the Prioritized Theorist framework (Poole, Goebel, and Aleliunas 1987; Brewka 1989; van Arragon 1990), because it supports both default and abductive reasoning. Theorist typifies what is known as a &amp;quot;proof12 Non-understanding, which entails non-acceptance (or deferred acceptance), is signaled by second-turn repair. This type of repair will not be considered here. 13 Other misunderstandings are possible; for example there can be disagreement about what object a speaker is trying to identify with a referring expression (cf. Heeman and Hirst 1995; Hirst et al. 1994). 14 This distinction is similar to the one made by Luperfoy (1992). 15 For present purposes, we also assume that </context>
</contexts>
<marker>Brewka, 1989</marker>
<rawString>Brewka, Gerhard (1989). &amp;quot;Preferred subtheories: An extended logical framework for default reasoning.&amp;quot; In Proceedings of the 11th International Joint Conference on Artificial Intelligence, 1043-1048, Detroit, MI.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Randall Calistri-Yeh</author>
</authors>
<title>1(1991). &amp;quot;Utilizing user models to handle ambiguity and misconceptions in robust plan recognition.&amp;quot; User Modeling and User-Adapted Interaction,</title>
<volume>1</volume>
<issue>4</issue>
<pages>289--322</pages>
<marker>Calistri-Yeh, </marker>
<rawString>Calistri-Yeh, Randall 1(1991). &amp;quot;Utilizing user models to handle ambiguity and misconceptions in robust plan recognition.&amp;quot; User Modeling and User-Adapted Interaction, 1(4), 289-322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra Carberry</author>
</authors>
<title>Pragmatics Modeling in Information Systems Interfaces. Doctoral dissertation,</title>
<date>1985</date>
<institution>University of Delaware,</institution>
<location>Newark, Delaware.</location>
<contexts>
<context position="84878" citStr="Carberry (1985" startWordPosition="12784" endWordPosition="12785">o domain-level plans. Metaplans include actions, such as introduce, continue, or clarify and are recognized, in part, by identifying cue phrases. Although the metaplans add flexibility by increasing the number of possible paths, they also add to the problem of pruning and ordering the paths, requiring additional heuristics. For example, there are specific rules for choosing among alternative metaplans on the basis of clue words, implicit expectations, or default preferences. Litman also adds a new general heuristic: stop chaining if an ambiguity cannot be resolved. 5.1.3 Carberry and Lambert. Carberry (1985, 1987, 1990) uses a similar approach. Her model introduces a new set of discourse-level goals such as seek-confirmation that are recognized on the basis of the current properties of the dialogue model and the mutual beliefs of the participants. Once a discourse-level goal is selected, a set of can465 Computational Linguistics Volume 21, Number 4 didate plans is identified, and Allen-style heuristics are applied to choose one of them. Subsequent work by Lambert and Carberry (1991, 1992) introduces an intermediate, problem-solving level of plans that link the discourse-level acts to domain plan</context>
<context position="92107" citStr="Carberry 1985" startWordPosition="13908" endWordPosition="13909">conceptual &amp;quot;sibling&amp;quot;, by combining generalization and constraint operations. For example, if there is more than one strategy for Hinkelman (1990); however, as with grounding acts, presumably the elimination of all possible interpretations could cue some type of repair mechanism, if they chose to incorporate one. 45 This is the same sense of &amp;quot;expectation&amp;quot; as used by Riesbeck (1974). 467 Computational Linguistics Volume 21, Number 4 achieving a goal, then an entity that corresponds to a step from one strategy might be replaced by one corresponding to a step from one of the other strategies (see Carberry 1985, 1987; Eller and Carberry 1992; Moore 1989). Although these approaches do quite well at preventing certain classes of misunderstandings, they cannot prevent them all. Moreover, these approaches may actually trigger misunderstandings because they always find some substitution, and yet they lack any mechanisms for detecting when one of their own previous repairs was inappropriate. Thus, a conversational participant will still need to be able to address actual misunderstandings. 5.4 Collaboration in the resolution of nonunderstanding In this paper, we have concentrated on the repair of mis-under</context>
</contexts>
<marker>Carberry, 1985</marker>
<rawString>Carberry, Sandra (1985). Pragmatics Modeling in Information Systems Interfaces. Doctoral dissertation, University of Delaware, Newark, Delaware.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra Carberry</author>
</authors>
<title>Pragmatic modeling: Toward a robust natural language interface.&amp;quot;</title>
<date>1987</date>
<journal>Computational Intelligence,</journal>
<volume>3</volume>
<issue>3</issue>
<pages>117--136</pages>
<marker>Carberry, 1987</marker>
<rawString>Carberry, Sandra (1987). &amp;quot;Pragmatic modeling: Toward a robust natural language interface.&amp;quot; Computational Intelligence, 3(3), 117-136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carberry Sandra</author>
</authors>
<title>Modeling the user&apos;s plans and goals.</title>
<date>1988</date>
<journal>Computational Linguistics,</journal>
<volume>14</volume>
<issue>3</issue>
<pages>23--37</pages>
<marker>Sandra, 1988</marker>
<rawString>Carberry Sandra (1988). &amp;quot;Modeling the user&apos;s plans and goals. Computational Linguistics, 14(3), 23-37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra Carberry</author>
</authors>
<title>Plan Recognition in Natural Language Dialogue.</title>
<date>1990</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="16728" citStr="Carberry 1990" startWordPosition="2589" endWordPosition="2591">slead them into prematurely believing that their goals have been achieved. • The need for an alternative to the notion of mutual belief. Typically, models rely on mutual beliefs without accounting for how speakers achieve them or for why speakers should believe that they have achieved them. 2.1 Using social conventions to guide interpretation and repair Our account of interpretation avoids the extended inference required by plan-based models by reversing the standard dependency between an agent&apos;s expectations and task-related goals. Plan-based approaches (Allen and Perrault 1979; Litman 1986; Carberry 1990; Lambert and Carberry 1991) start by applying context-independent inference rules to identify the agent&apos;s task-related plan, possibly favoring alternatives that extend a previously recognized plan. By contrast, our approach begins with an expectation, using it to premise both the analysis of utterance meaning and any inference 439 Computational Linguistics Volume 21, Number 4 about an agent&apos;s goals. Moreover, our approach treats apparent conflicts with expectations as meaningful; for example, if an utterance is inconsistent with expectations, then the reasoner will try to explain the inconsis</context>
<context position="28074" citStr="Carberry 1990" startWordPosition="4335" endWordPosition="4336">tion of subsequent ones. These misunderstandings are also difficult to prevent, because they can result from many common sources, including intra-sentential ambiguity and mishearing. 2.3 Building a model of the interpreted discourse For a hearer to interpret an utterance as a particular metaplan or as a manifestation of misunderstanding, he needs a model of his understanding of the prior discourse. The typical way to model interpretations has been to represent the discourse as a partially completed plan corresponding to the actual beliefs (perhaps even mutual beliefs) of the participants (cf. Carberry 1990). This representation incorporates two assumptions that must be relaxed in any model that accounts for the negotiation of meaning: first, that hearers are always credulous about what the speaker says, and second, that neither participant makes mistakes. To relax these assumptions, the hearer&apos;s model distinguishes the beliefs that speakers claim or act as if they have during the dialogue from those that the hearer actually believes they have.&amp;quot; The model also represents the alternative interpretations that the hearer has considered as a result of repair.&amp;quot; We will now consider an axiomatization o</context>
</contexts>
<marker>Carberry, 1990</marker>
<rawString>Carberry, Sandra (1990). Plan Recognition in Natural Language Dialogue. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alison J Cawsey</author>
</authors>
<title>A belief revision model of repair sequences in dialogue.&amp;quot;</title>
<date>1991</date>
<booktitle>New Directions in Intelligent Tutoring Systems.</booktitle>
<editor>In Ernesto Costa, editor,</editor>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="85723" citStr="Cawsey (1991)" startWordPosition="12913" endWordPosition="12914">participants. Once a discourse-level goal is selected, a set of can465 Computational Linguistics Volume 21, Number 4 didate plans is identified, and Allen-style heuristics are applied to choose one of them. Subsequent work by Lambert and Carberry (1991, 1992) introduces an intermediate, problem-solving level of plans that link the discourse-level acts to domain plans. The processing rules, by their specificity, eliminate the need for many of the heuristics. The sacrifice here is a loss of generality; the mechanisms for recognizing goals are specific to Carberry&apos;s implementation. 5.1.4 Cawsey. Cawsey (1991) proposes a method of extending Perrault and Allen&apos;s (1980) inference rule approach to produce repairs. She also suggests including some of the information captured by various rating heuristics as premises in the rules, allowing that these new premises may be assumed by default. For example, the following rule is proposed for capturing pretellings: if request(S1, S2, informif(S2, Si, knowref(S2, D))) and know(S2, knowref(S1, D)) then know(S2, wants(S1, knowref(S2, D))) To handle misunderstandings, she suggests that such assumptions be retracted if they become inconsistent and then any subseque</context>
</contexts>
<marker>Cawsey, 1991</marker>
<rawString>Cawsey, Alison J. (1991). &amp;quot;A belief revision model of repair sequences in dialogue.&amp;quot; In Ernesto Costa, editor, New Directions in Intelligent Tutoring Systems. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Robert Goldman</author>
</authors>
<title>A logic for semantic interpretation.&amp;quot;</title>
<date>1988</date>
<booktitle>In 26th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference,</booktitle>
<pages>87--94</pages>
<location>Buffalo, NY.</location>
<contexts>
<context position="24727" citStr="Charniak and Goldman 1988" startWordPosition="3835" endWordPosition="3838">on 2 B: No. Acceptance 3 B: I never lend money. Challenge 4 A: No, I meant to offer you one. Repair 5 B: Oh. Thanks. Repair 6 A: Bye. Closing Figure 3 Examples of different types of coherence strategies. terpretations of utterances (including recognizing misunderstanding) correspond to abductive inference over the theory. Definition 1 Given a theory T and a goal proposition g, we say that one can abduce a set of assumptions ZS, from g if Tu z H g and 7 U is consistent. Abduction has been applied to the solution of local pragmatics problems (Hobbs et al. 1988, 1993) and to story understanding (Charniak and Goldman 1988). The model incorporates five strategies, or meta plans, for generating coherent utterances: plan adoption, acceptance, challenge, repair, and closing (the model treats opening as a kind of plan adoption). Figure 3 contains a conversation that includes an example for each of the five types. In plan adoption, speakers simply choose an action that can be expected to achieve a desired illocutionary goal, given social norms and the discourse context. (The goal itself must originate within the speaker&apos;s non-linguistic planning mechanism.) The first utterance in the figure is a plan adoption. The se</context>
</contexts>
<marker>Charniak, Goldman, 1988</marker>
<rawString>Charniak, Eugene, and Goldman, Robert (1988). &amp;quot;A logic for semantic interpretation.&amp;quot; In 26th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, 87-94, Buffalo, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
</authors>
<title>Arenas of Language Use. The University of Chicago Press, and Stanford: Center for the Study of Language and Information.</title>
<date>1993</date>
<contexts>
<context position="93414" citStr="Clark 1993" startWordPosition="14110" endWordPosition="14111">difference between the two situations is that in the former, the agent derives exactly one interpretation of an utterance and hence is initially unaware of any problem; in the latter, the agent derives either more than one interpretation, with no way to choose between them, or no interpretation at all, and so the problem is immediately apparent. Heeman and Edmonds looked in particular at cases in which a referring expression uttered by one conversant was not understood by the other (Heeman and Hirst 1995; Edmonds 1994; Hirst et al. 1994). Clark and his colleagues (Clark and Wilkes-Gibbs 1986; Clark 1993) have shown that in such situations, conversants will collaborate on repairing the problem by, in effect, negotiating a reconstruction or elaboration of the referring expression. Heeman and Edmonds model this with a plan recognition and generation system that can recognize faulty plans and try to repair them. Thus (as in our own model) two copies of the system can converse with each other, negotiating referents of referring expressions that are not understood by trying to recognize the referring plans of the other, repairing them where necessary, and presenting the new referring plan to the ot</context>
</contexts>
<marker>Clark, 1993</marker>
<rawString>Clark, Herbert H. (1993). Arenas of Language Use. The University of Chicago Press, and Stanford: Center for the Study of Language and Information.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
<author>Deanna Wilkes-Gibbs</author>
</authors>
<title>Referring as a collaborative process.&amp;quot;</title>
<date>1986</date>
<journal>Cognition,</journal>
<pages>22--1</pages>
<publisher>The MIT Press,</publisher>
<note>Reprinted in Intentions in Communication, edited by</note>
<contexts>
<context position="93401" citStr="Clark and Wilkes-Gibbs 1986" startWordPosition="14106" endWordPosition="14109">ir of non-understanding. The difference between the two situations is that in the former, the agent derives exactly one interpretation of an utterance and hence is initially unaware of any problem; in the latter, the agent derives either more than one interpretation, with no way to choose between them, or no interpretation at all, and so the problem is immediately apparent. Heeman and Edmonds looked in particular at cases in which a referring expression uttered by one conversant was not understood by the other (Heeman and Hirst 1995; Edmonds 1994; Hirst et al. 1994). Clark and his colleagues (Clark and Wilkes-Gibbs 1986; Clark 1993) have shown that in such situations, conversants will collaborate on repairing the problem by, in effect, negotiating a reconstruction or elaboration of the referring expression. Heeman and Edmonds model this with a plan recognition and generation system that can recognize faulty plans and try to repair them. Thus (as in our own model) two copies of the system can converse with each other, negotiating referents of referring expressions that are not understood by trying to recognize the referring plans of the other, repairing them where necessary, and presenting the new referring p</context>
</contexts>
<marker>Clark, Wilkes-Gibbs, 1986</marker>
<rawString>Clark, Herbert H., and Wilkes-Gibbs, Deanna (1986). &amp;quot;Referring as a collaborative process.&amp;quot; Cognition, 22:1-39. (Reprinted in Intentions in Communication, edited by Philip R. Cohen, Jerry Morgan, and Martha Pollack. The MIT Press, pages 463-493.)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip R Cohen</author>
<author>Hector Levesque</author>
</authors>
<title>Speech acts and rationality.&amp;quot;</title>
<date>1985</date>
<booktitle>In The 23rd Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference,</booktitle>
<pages>49--60</pages>
<location>Chicago.</location>
<contexts>
<context position="37352" citStr="Cohen and Levesque (1985)" startWordPosition="5872" endWordPosition="5875">ch of the conjuncts of each supposition is compatible. (In the system, this is implemented as a special predicate, inconsistentLI). There is a danger in treating compatibility as a default in that one might miss some intuitively incompatible cases and hence some misunderstandings might not be detectable. An alternative would be to base compatibility on the notion of consistency in the underlying logic, if a complete logic has been defined.&apos; 3.2.2 Speech acts. For simplicity, we represent utterances as surface-level speech acts in the manner first used by Perrault and Allen (1980).19 Following Cohen and Levesque (1985), we limit the surface language to the acts surface-request, surface-inform, surface-informref, and surface-informif. Example 3 shows the representation of the literal form of Example 2, the fourth-turn repair example. (We abbreviate &amp;quot;m&amp;quot; for &amp;quot;Mother&amp;quot;, &amp;quot;r&amp;quot; for &amp;quot;Russ&amp;quot;, and &amp;quot;whoIsGoing&amp;quot; for &amp;quot;who&apos;s going&amp;quot;.) Example 3 Ti m: surface-request(m, r, informif(r, m, knowref(r, whoIsGoing))) T2 r: surface-request(r, m, informref(m, r, whoIsGoing)) T3 m: surface-inform(m, r, not knowref(m, whoIsGoing)) T4 r: surface-informref(r, m, whoIsGoing) We assume that such forms can be identified by the parser, for </context>
</contexts>
<marker>Cohen, Levesque, 1985</marker>
<rawString>Cohen, Philip R., and Levesque, Hector (1985). &amp;quot;Speech acts and rationality.&amp;quot; In The 23rd Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, 49-60, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip G Edmonds</author>
</authors>
<title>Collaboration on reference to objects that are not mutually known.&amp;quot;</title>
<date>1994</date>
<booktitle>In Proceedings, 15th International Conference on Computational Linguistics (COLING-94),</booktitle>
<pages>1118--1122</pages>
<location>Kyoto.</location>
<contexts>
<context position="93326" citStr="Edmonds 1994" startWordPosition="14096" endWordPosition="14097">g. Our colleagues Heeman and Edmonds have looked at the repair of non-understanding. The difference between the two situations is that in the former, the agent derives exactly one interpretation of an utterance and hence is initially unaware of any problem; in the latter, the agent derives either more than one interpretation, with no way to choose between them, or no interpretation at all, and so the problem is immediately apparent. Heeman and Edmonds looked in particular at cases in which a referring expression uttered by one conversant was not understood by the other (Heeman and Hirst 1995; Edmonds 1994; Hirst et al. 1994). Clark and his colleagues (Clark and Wilkes-Gibbs 1986; Clark 1993) have shown that in such situations, conversants will collaborate on repairing the problem by, in effect, negotiating a reconstruction or elaboration of the referring expression. Heeman and Edmonds model this with a plan recognition and generation system that can recognize faulty plans and try to repair them. Thus (as in our own model) two copies of the system can converse with each other, negotiating referents of referring expressions that are not understood by trying to recognize the referring plans of th</context>
</contexts>
<marker>Edmonds, 1994</marker>
<rawString>Edmonds, Philip G. (1994). &amp;quot;Collaboration on reference to objects that are not mutually known.&amp;quot; In Proceedings, 15th International Conference on Computational Linguistics (COLING-94), 1118-1122, Kyoto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rhonda Eller</author>
<author>Sandra Carberry</author>
</authors>
<title>A meta-rule approach to flexible plan recognition in dialogue.&amp;quot; User Modeling and User-Adapted Interaction,</title>
<date>1992</date>
<pages>2--1</pages>
<contexts>
<context position="91383" citStr="Eller and Carberry 1992" startWordPosition="13790" endWordPosition="13793">riggering a misunderstanding, agents can check for evidence of misconception and try to resolve apparent errors. The symptoms of misconception include references to entities that do not map to previously known objects or operations (Webber and Mays 1983) or requests for clarification (Moore 1989). Errors are corrected by replacing or deleting parts of the problematic utterance so that it makes sense. Several correction strategies have been suggested: • Generalize a description by selectively ignoring some constraints (see Goodman 1985; McCoy 1985, 1986, 1988; Carberry 1988; Calistri-Yeh 1991; Eller and Carberry 1992), • Make a description more specific by adding extra constraints (see Eller and Carberry 1992), and • Choose a conceptual &amp;quot;sibling&amp;quot;, by combining generalization and constraint operations. For example, if there is more than one strategy for Hinkelman (1990); however, as with grounding acts, presumably the elimination of all possible interpretations could cue some type of repair mechanism, if they chose to incorporate one. 45 This is the same sense of &amp;quot;expectation&amp;quot; as used by Riesbeck (1974). 467 Computational Linguistics Volume 21, Number 4 achieving a goal, then an entity that corresponds to a</context>
</contexts>
<marker>Eller, Carberry, 1992</marker>
<rawString>Eller, Rhonda, and Carberry, Sandra (1992). &amp;quot;A meta-rule approach to flexible plan recognition in dialogue.&amp;quot; User Modeling and User-Adapted Interaction, 2(1-2), 27-53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pamela E Fink</author>
<author>Alan W Biermann</author>
</authors>
<title>The correction of ill-formed input using history-based expectation with applications to speech understanding.&amp;quot;</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<volume>12</volume>
<issue>1</issue>
<pages>13--36</pages>
<contexts>
<context position="89964" citStr="Fink and Biermann 1986" startWordPosition="13574" endWordPosition="13577"> the following types of responses: 1. A statement about background knowledge that might be needed. 2. A statement about the underlying purpose of A. 3. A statement about related task steps (i.e., subgoals of A, tasks that contain A as a step, or tasks that might follow A). 4. A statement about the accomplishment of A. These expectations are independent of the belief state of an agent and are specified down to the semantic (and sometimes even lexical) level. This information has long been used to discriminate between ambiguous interpretations and correct mistakes made by the speech recognizer (Fink and Biermann 1986; Smith 1992). Typically, an utterance will be interpreted according to the expectation that matches it most closely. By contrast, our approach and that of the plan-based accounts use &amp;quot;expectation&amp;quot; to refer to agents&apos; beliefs about how future utterances might relate to prior ones. These expectations are determined both by an agent&apos;s understanding of typical behavior and by his or her mental state. These two notions of expectation are complementary, and any dialogue model that uses speech as input must be able to represent and reason with both. 5.3 Approaches to misconception Misconceptions are</context>
</contexts>
<marker>Fink, Biermann, 1986</marker>
<rawString>Fink, Pamela E., and Biermann, Alan W. (1986). &amp;quot;The correction of ill-formed input using history-based expectation with applications to speech understanding.&amp;quot; Computational Linguistics, 12(1), 13-36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Fox</author>
</authors>
<title>Interactional reconstruction in real-time language processing.&amp;quot;</title>
<date>1987</date>
<journal>Cognitive Science,</journal>
<pages>11--365</pages>
<contexts>
<context position="7825" citStr="Fox (1987)" startWordPosition="1196" endWordPosition="1197"> a new response to it, then we say that they have made a fourth-turn repair. The fragment of conversation shown in Example 2 (Terasaki 1976) includes a fourth-turn repair. Initially, Russ interprets T1 as expressing Mother&apos;s desire to tell, that is, as a pretelling or preannouncement, but finds this interpretation inconsistent with her next utterance. In T3, instead of telling him who&apos;s going (as one would expect after a pretelling), Mother claims that she does not know (and therefore could not tell). Russ recovers by reinterpreting Ti as an indirect request, which his T4 attempts to satisfy. Fox (1987) points out that such repairs involve, in effect, a reconstruction of the initial utterance. From an AT perspective, these reconstructions resemble the operation of a truth-maintenance system upon an abductive assumption that has proved to be incorrect.&apos; Example 2 Ti Mother: Do you know who&apos;s going to that meeting? T2 Russ: Who? T3 Mother: I don&apos;t know. T4 Russ: Oh. Probably Mrs. McOwen and probably Mrs. Cadry and some of the teachers. 1.3 The need for both intentional and social information The problem of interpreting an utterance involves deciding what actions the speaker is doing or trying </context>
</contexts>
<marker>Fox, 1987</marker>
<rawString>Fox, Barbara (1987). &amp;quot;Interactional reconstruction in real-time language processing.&amp;quot; Cognitive Science, 11,365-387.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley Goodman</author>
</authors>
<title>Repairing reference identification failures by relaxation.&amp;quot;</title>
<date>1985</date>
<booktitle>In 23th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference,</booktitle>
<pages>204--217</pages>
<location>Chicago.</location>
<contexts>
<context position="91299" citStr="Goodman 1985" startWordPosition="13780" endWordPosition="13781">entionally evoke a concept or relation. To prevent misconceptions from triggering a misunderstanding, agents can check for evidence of misconception and try to resolve apparent errors. The symptoms of misconception include references to entities that do not map to previously known objects or operations (Webber and Mays 1983) or requests for clarification (Moore 1989). Errors are corrected by replacing or deleting parts of the problematic utterance so that it makes sense. Several correction strategies have been suggested: • Generalize a description by selectively ignoring some constraints (see Goodman 1985; McCoy 1985, 1986, 1988; Carberry 1988; Calistri-Yeh 1991; Eller and Carberry 1992), • Make a description more specific by adding extra constraints (see Eller and Carberry 1992), and • Choose a conceptual &amp;quot;sibling&amp;quot;, by combining generalization and constraint operations. For example, if there is more than one strategy for Hinkelman (1990); however, as with grounding acts, presumably the elimination of all possible interpretations could cue some type of repair mechanism, if they chose to incorporate one. 45 This is the same sense of &amp;quot;expectation&amp;quot; as used by Riesbeck (1974). 467 Computational Li</context>
</contexts>
<marker>Goodman, 1985</marker>
<rawString>Goodman, Bradley (1985). &amp;quot;Repairing reference identification failures by relaxation.&amp;quot; In 23th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, 204-217, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Grice</author>
</authors>
<title>Meaning.&amp;quot; The Philosophical Review,</title>
<date>1957</date>
<pages>66--377</pages>
<marker>Grice, 1957</marker>
<rawString>Grice, H. P. (1957). &amp;quot;Meaning.&amp;quot; The Philosophical Review, 66,377-388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Heeman</author>
<author>Graeme Hirst</author>
</authors>
<title>Collaborating on referring expressions.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>3</issue>
<contexts>
<context position="29713" citStr="Heeman and Hirst 1995" startWordPosition="4577" endWordPosition="4580">ning framework: Prioritized Theorist The model has been formulated using the Prioritized Theorist framework (Poole, Goebel, and Aleliunas 1987; Brewka 1989; van Arragon 1990), because it supports both default and abductive reasoning. Theorist typifies what is known as a &amp;quot;proof12 Non-understanding, which entails non-acceptance (or deferred acceptance), is signaled by second-turn repair. This type of repair will not be considered here. 13 Other misunderstandings are possible; for example there can be disagreement about what object a speaker is trying to identify with a referring expression (cf. Heeman and Hirst 1995; Hirst et al. 1994). 14 This distinction is similar to the one made by Luperfoy (1992). 15 For present purposes, we also assume that the complete model is accessible to the hearer; one could better simulate the limitations of working memory by limiting access to only the most recent utterances. 443 Computational Linguistics Volume 21, Number 4 based approach&amp;quot; to abduction because it relies on a theorem prover to collect the assumptions that would be needed to prove a given set of observations and to verify their consistency. Our reasoning algorithm is based on Poole&apos;s implementation of Theori</context>
<context position="93312" citStr="Heeman and Hirst 1995" startWordPosition="14092" endWordPosition="14095">air of mis-understanding. Our colleagues Heeman and Edmonds have looked at the repair of non-understanding. The difference between the two situations is that in the former, the agent derives exactly one interpretation of an utterance and hence is initially unaware of any problem; in the latter, the agent derives either more than one interpretation, with no way to choose between them, or no interpretation at all, and so the problem is immediately apparent. Heeman and Edmonds looked in particular at cases in which a referring expression uttered by one conversant was not understood by the other (Heeman and Hirst 1995; Edmonds 1994; Hirst et al. 1994). Clark and his colleagues (Clark and Wilkes-Gibbs 1986; Clark 1993) have shown that in such situations, conversants will collaborate on repairing the problem by, in effect, negotiating a reconstruction or elaboration of the referring expression. Heeman and Edmonds model this with a plan recognition and generation system that can recognize faulty plans and try to repair them. Thus (as in our own model) two copies of the system can converse with each other, negotiating referents of referring expressions that are not understood by trying to recognize the referri</context>
</contexts>
<marker>Heeman, Hirst, 1995</marker>
<rawString>Heeman, Peter, and Hirst, Graeme (1995). Collaborating on referring expressions. Computational Linguistics, 21(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth A Hinkelman</author>
</authors>
<title>Linguistic and Pragmatic Constraints on Utterance Interpretation.&amp;quot; Doctoral dissertation,</title>
<date>1990</date>
<tech>Technical Report 288,</tech>
<institution>Department of Computer Science, University of Rochester,</institution>
<location>Rochester, New York.</location>
<note>Published as</note>
<contexts>
<context position="18585" citStr="Hinkelman (1990)" startWordPosition="2873" endWordPosition="2874">d can provide) a valid reason not to. Linguistic intentions are based on Grice&apos;s (1957) notion of reflexive intention. For example, an inform(S,H,P) expresses the linguistic intentions whose content is P and intend(S,know(H,P)) (i.e., the speaker intends the hearer to believe (1) that P is true and (2) that the speaker intends that the hearer know P). Linguistic expectations capture the notion of adjacency pairs.&apos; In defining linguistic intentions, which are shown in Figure 1, we have followed existing speech act taxonomies, especially those given by Bach and Harnish (1979), Allen (1983), and Hinkelman (1990).7 Thus, when a speaker produces an askref about P she expresses (and thereby intends the hearer to recognize that she expresses) that she does not know the referent of some description in P. intends to find out the referent of that description, and intends the hearer to tell her that referent. If the speaker is sincere, she actually believes the content of what she expresses; if the hearer is trusting, he might come to believe that she believes it. Following Schegloff&apos;s (1988) analysis of Example 2, we provide a speech act definition for prete11.8 In order to capture the linguistic intentions</context>
<context position="38796" citStr="Hinkelman 1990" startWordPosition="6080" endWordPosition="6081">1991). 19 Other representation languages, such as one based on case semantics, would also be compatible with the approach and would permit greater flexibility. The cost of the increased flexibility would be increased difficulty in mapping surface descriptions onto speech acts; however, because less effort would be required in sentence processing, the total complexity of the problem need not increase. Using a more finely-grained representation, one could reason about sentence type, particles, and prosody explicitly, instead of requiring the sentence processor to interpret this information (cf. Hinkelman 1990; Beun 1990). 20 We also presume that a parser can recognize surface-informref and surface-informif syntactically when the input is a sentence fragment, but it would not hurt our analysis to input them all as surface-inform. 446 McRoy and Hirst The Repair of Speech Act Misunderstandings The theory includes the discourse-level acts inform, informif, informref, assert, assertif, assertref, askref, askif, request, pretell, testref, and warn, which we represent using a similar notation.&amp;quot;&apos;&amp;quot; 3.2.3 Turn sequences. A turn sequence represents the interpretations of the discourse that a participant has </context>
<context position="41917" citStr="Hinkelman (1990)" startWordPosition="6575" endWordPosition="6576">ge, including principles of interaction and facts relating linguistic acts. Given these three subtheories, an interpretation of an utterance is a set of ground instances of assumptions that explain the utterance. An utterance would be a coherent 21 In the utterance language, a yes—no question is taken to be a surface-request to informif and a wh-question is taken to be a surface-request to informref. We then translate these request forms into the discourse-level actions askif and askref. An alternative would be to identify them as surface-askif or surface-askref during sentence processing, as Hinkelman (1990) does. 22 Speech act names that end with the suffix -ref take a description as an argument; speech act names that end with -if take a supposition. The act inform(s,p) asserts that the proposition is true. The act informif(s,p) asserts the truth value of the proposition named by p (i.e., informif is equivalent to &amp;quot;inform v inform-not&amp;quot;). 23 Tree structures are often used to represent discourse, but usually the hierarchical structure of the discourse, rather than its temporal structure (see Lambert and Carberry 1991, 1992). 447 Computational Linguistics Volume 21, Number 4 reply to an immediately</context>
<context position="91639" citStr="Hinkelman (1990)" startWordPosition="13831" endWordPosition="13832">ts for clarification (Moore 1989). Errors are corrected by replacing or deleting parts of the problematic utterance so that it makes sense. Several correction strategies have been suggested: • Generalize a description by selectively ignoring some constraints (see Goodman 1985; McCoy 1985, 1986, 1988; Carberry 1988; Calistri-Yeh 1991; Eller and Carberry 1992), • Make a description more specific by adding extra constraints (see Eller and Carberry 1992), and • Choose a conceptual &amp;quot;sibling&amp;quot;, by combining generalization and constraint operations. For example, if there is more than one strategy for Hinkelman (1990); however, as with grounding acts, presumably the elimination of all possible interpretations could cue some type of repair mechanism, if they chose to incorporate one. 45 This is the same sense of &amp;quot;expectation&amp;quot; as used by Riesbeck (1974). 467 Computational Linguistics Volume 21, Number 4 achieving a goal, then an entity that corresponds to a step from one strategy might be replaced by one corresponding to a step from one of the other strategies (see Carberry 1985, 1987; Eller and Carberry 1992; Moore 1989). Although these approaches do quite well at preventing certain classes of misunderstand</context>
</contexts>
<marker>Hinkelman, 1990</marker>
<rawString>Hinkelman, Elizabeth A. (1990). &amp;quot;Linguistic and Pragmatic Constraints on Utterance Interpretation.&amp;quot; Doctoral dissertation, Department of Computer Science, University of Rochester, Rochester, New York. Published as University of Rochester Computer Science Technical Report 288, May 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
<author>Susan McRoy</author>
<author>Peter Heeman</author>
<author>Philip Edmonds</author>
<author>Diane Horton</author>
</authors>
<title>Repairing conversational misunderstandings and non-understandings.&amp;quot;</title>
<date>1994</date>
<journal>Speech Communication,</journal>
<pages>15--213</pages>
<contexts>
<context position="29733" citStr="Hirst et al. 1994" startWordPosition="4581" endWordPosition="4584">ized Theorist The model has been formulated using the Prioritized Theorist framework (Poole, Goebel, and Aleliunas 1987; Brewka 1989; van Arragon 1990), because it supports both default and abductive reasoning. Theorist typifies what is known as a &amp;quot;proof12 Non-understanding, which entails non-acceptance (or deferred acceptance), is signaled by second-turn repair. This type of repair will not be considered here. 13 Other misunderstandings are possible; for example there can be disagreement about what object a speaker is trying to identify with a referring expression (cf. Heeman and Hirst 1995; Hirst et al. 1994). 14 This distinction is similar to the one made by Luperfoy (1992). 15 For present purposes, we also assume that the complete model is accessible to the hearer; one could better simulate the limitations of working memory by limiting access to only the most recent utterances. 443 Computational Linguistics Volume 21, Number 4 based approach&amp;quot; to abduction because it relies on a theorem prover to collect the assumptions that would be needed to prove a given set of observations and to verify their consistency. Our reasoning algorithm is based on Poole&apos;s implementation of Theorist, which we extende</context>
<context position="93346" citStr="Hirst et al. 1994" startWordPosition="14098" endWordPosition="14101">ues Heeman and Edmonds have looked at the repair of non-understanding. The difference between the two situations is that in the former, the agent derives exactly one interpretation of an utterance and hence is initially unaware of any problem; in the latter, the agent derives either more than one interpretation, with no way to choose between them, or no interpretation at all, and so the problem is immediately apparent. Heeman and Edmonds looked in particular at cases in which a referring expression uttered by one conversant was not understood by the other (Heeman and Hirst 1995; Edmonds 1994; Hirst et al. 1994). Clark and his colleagues (Clark and Wilkes-Gibbs 1986; Clark 1993) have shown that in such situations, conversants will collaborate on repairing the problem by, in effect, negotiating a reconstruction or elaboration of the referring expression. Heeman and Edmonds model this with a plan recognition and generation system that can recognize faulty plans and try to repair them. Thus (as in our own model) two copies of the system can converse with each other, negotiating referents of referring expressions that are not understood by trying to recognize the referring plans of the other, repairing t</context>
</contexts>
<marker>Hirst, McRoy, Heeman, Edmonds, Horton, 1994</marker>
<rawString>Hirst, Graeme; McRoy, Susan; Heeman, Peter; Edmonds, Philip; and Horton, Diane (1994). &amp;quot;Repairing conversational misunderstandings and non-understandings.&amp;quot; Speech Communication, 15,213-229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
<author>Mark Stickel</author>
<author>Paul Martin</author>
<author>Douglas Edwards</author>
</authors>
<title>Interpretation as abduction.&amp;quot;</title>
<date>1988</date>
<booktitle>In 26th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference,</booktitle>
<pages>95--103</pages>
<contexts>
<context position="24665" citStr="Hobbs et al. 1988" startWordPosition="3826" endWordPosition="3829">Metaplan type 1 A: Do you have a quarter? Plan adoption 2 B: No. Acceptance 3 B: I never lend money. Challenge 4 A: No, I meant to offer you one. Repair 5 B: Oh. Thanks. Repair 6 A: Bye. Closing Figure 3 Examples of different types of coherence strategies. terpretations of utterances (including recognizing misunderstanding) correspond to abductive inference over the theory. Definition 1 Given a theory T and a goal proposition g, we say that one can abduce a set of assumptions ZS, from g if Tu z H g and 7 U is consistent. Abduction has been applied to the solution of local pragmatics problems (Hobbs et al. 1988, 1993) and to story understanding (Charniak and Goldman 1988). The model incorporates five strategies, or meta plans, for generating coherent utterances: plan adoption, acceptance, challenge, repair, and closing (the model treats opening as a kind of plan adoption). Figure 3 contains a conversation that includes an example for each of the five types. In plan adoption, speakers simply choose an action that can be expected to achieve a desired illocutionary goal, given social norms and the discourse context. (The goal itself must originate within the speaker&apos;s non-linguistic planning mechanism.</context>
</contexts>
<marker>Hobbs, Stickel, Martin, Edwards, 1988</marker>
<rawString>Hobbs, Jerry R.; Stickel, Mark; Martin, Paul; and Edwards, Douglas (1988). &amp;quot;Interpretation as abduction.&amp;quot; In 26th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, 95-103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
<author>Mark Stickel</author>
<author>Douglas E Appelt</author>
<author>Paul Martin</author>
</authors>
<title>Interpretation as abduction.&amp;quot;</title>
<date>1993</date>
<journal>Artificial Intelligence,</journal>
<pages>63--69</pages>
<marker>Hobbs, Stickel, Appelt, Martin, 1993</marker>
<rawString>Hobbs, Jerry R.; Stickel, Mark; Appelt, Douglas E.; and Martin, Paul (1993). &amp;quot;Interpretation as abduction.&amp;quot; Artificial Intelligence, 63,69-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul E Jose</author>
</authors>
<title>Sequentiality of speech acts in conversational structure.&amp;quot;</title>
<date>1988</date>
<journal>Journal of Psycholinguistic Research,</journal>
<volume>17</volume>
<issue>1</issue>
<pages>65--88</pages>
<contexts>
<context position="23632" citStr="Jose (1988)" startWordPosition="3658" endWordPosition="3659">e fundamental tasks of interpreting speech acts, producing speech acts, and repairing speech act interpretations within a nonmonotonic framework. In particular, speakers&apos; knowledge about language is represented as a set of default rules. The rules describe conventional strategies for producing coherent utterances, thereby displaying understanding, and strategies for identifying misunderstanding. As a result, speakers&apos; decisions about what utterances they might coherently generate next correspond to default inference over this theory, while decisions about possible in11 Quantitative results by Jose (1988) and Nagata and Morimoto (1993) provide evidence for these adjacency pairs. In addition, we have used pairs discovered by Conversation Analysis from real dialogues (Schegloff 1988). 441 Computational Linguistics Volume 21, Number 4 First turn Expected reply askref informref askif informif request comply pretell askref test ref assert ref testif assertif Figure 2 Adjacency pairs (Linguistic expectations). Example Metaplan type 1 A: Do you have a quarter? Plan adoption 2 B: No. Acceptance 3 B: I never lend money. Challenge 4 A: No, I meant to offer you one. Repair 5 B: Oh. Thanks. Repair 6 A: By</context>
</contexts>
<marker>Jose, 1988</marker>
<rawString>Jose, Paul E. (1988). &amp;quot;Sequentiality of speech acts in conversational structure.&amp;quot; Journal of Psycholinguistic Research, 17(1), 65-88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynn Lambert</author>
<author>Sandra Carberry</author>
</authors>
<title>A tri-partite plan-based model of dialogue.&amp;quot;</title>
<date>1991</date>
<booktitle>In 29th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference,</booktitle>
<pages>47--54</pages>
<location>Berkeley, CA.</location>
<contexts>
<context position="16756" citStr="Lambert and Carberry 1991" startWordPosition="2592" endWordPosition="2595"> prematurely believing that their goals have been achieved. • The need for an alternative to the notion of mutual belief. Typically, models rely on mutual beliefs without accounting for how speakers achieve them or for why speakers should believe that they have achieved them. 2.1 Using social conventions to guide interpretation and repair Our account of interpretation avoids the extended inference required by plan-based models by reversing the standard dependency between an agent&apos;s expectations and task-related goals. Plan-based approaches (Allen and Perrault 1979; Litman 1986; Carberry 1990; Lambert and Carberry 1991) start by applying context-independent inference rules to identify the agent&apos;s task-related plan, possibly favoring alternatives that extend a previously recognized plan. By contrast, our approach begins with an expectation, using it to premise both the analysis of utterance meaning and any inference 439 Computational Linguistics Volume 21, Number 4 about an agent&apos;s goals. Moreover, our approach treats apparent conflicts with expectations as meaningful; for example, if an utterance is inconsistent with expectations, then the reasoner will try to explain the inconsistency. The model focuses on </context>
<context position="42435" citStr="Lambert and Carberry 1991" startWordPosition="6657" endWordPosition="6660">ould be to identify them as surface-askif or surface-askref during sentence processing, as Hinkelman (1990) does. 22 Speech act names that end with the suffix -ref take a description as an argument; speech act names that end with -if take a supposition. The act inform(s,p) asserts that the proposition is true. The act informif(s,p) asserts the truth value of the proposition named by p (i.e., informif is equivalent to &amp;quot;inform v inform-not&amp;quot;). 23 Tree structures are often used to represent discourse, but usually the hierarchical structure of the discourse, rather than its temporal structure (see Lambert and Carberry 1991, 1992). 447 Computational Linguistics Volume 21, Number 4 reply to an immediately preceding utterance if it would logically follow, given the selection of some metaplan: Definition 4 An interpretation of an utterance u to hearer h by speaker s in discourse context ts is a set M of instances of elements of M, such that 1. TU BUM is consistent 2. TU BUM utter(s,h,u,ts) 3. TUBUM satisfies the priority constraints; that is, TuBuM is not in conflict with any stronger defaults that might apply. Definition 5 It would be coherent for s to utter u in discourse context ts if the utterance can be derive</context>
<context position="85362" citStr="Lambert and Carberry (1991" startWordPosition="12859" endWordPosition="12862">rences. Litman also adds a new general heuristic: stop chaining if an ambiguity cannot be resolved. 5.1.3 Carberry and Lambert. Carberry (1985, 1987, 1990) uses a similar approach. Her model introduces a new set of discourse-level goals such as seek-confirmation that are recognized on the basis of the current properties of the dialogue model and the mutual beliefs of the participants. Once a discourse-level goal is selected, a set of can465 Computational Linguistics Volume 21, Number 4 didate plans is identified, and Allen-style heuristics are applied to choose one of them. Subsequent work by Lambert and Carberry (1991, 1992) introduces an intermediate, problem-solving level of plans that link the discourse-level acts to domain plans. The processing rules, by their specificity, eliminate the need for many of the heuristics. The sacrifice here is a loss of generality; the mechanisms for recognizing goals are specific to Carberry&apos;s implementation. 5.1.4 Cawsey. Cawsey (1991) proposes a method of extending Perrault and Allen&apos;s (1980) inference rule approach to produce repairs. She also suggests including some of the information captured by various rating heuristics as premises in the rules, allowing that these</context>
</contexts>
<marker>Lambert, Carberry, 1991</marker>
<rawString>Lambert, Lynn, and Carberry, Sandra (1991). &amp;quot;A tri-partite plan-based model of dialogue.&amp;quot; In 29th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, 47-54, Berkeley, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynn Lambert</author>
<author>Sandra Carberry</author>
</authors>
<title>Modeling negotiation dialogues.&amp;quot;</title>
<date>1992</date>
<booktitle>In 30th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference,</booktitle>
<pages>193--200</pages>
<location>Newark, Delaware.</location>
<marker>Lambert, Carberry, 1992</marker>
<rawString>Lambert, Lynn, and Carberry, Sandra (1992). &amp;quot;Modeling negotiation dialogues.&amp;quot; In 30th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, 193-200, Newark, Delaware.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane J Litman</author>
</authors>
<title>Linguistic coherence: A plan-based alternative.&amp;quot;</title>
<date>1986</date>
<booktitle>In 24th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference,</booktitle>
<pages>215--223</pages>
<location>New York.</location>
<contexts>
<context position="16713" citStr="Litman 1986" startWordPosition="2587" endWordPosition="2588">tion might mislead them into prematurely believing that their goals have been achieved. • The need for an alternative to the notion of mutual belief. Typically, models rely on mutual beliefs without accounting for how speakers achieve them or for why speakers should believe that they have achieved them. 2.1 Using social conventions to guide interpretation and repair Our account of interpretation avoids the extended inference required by plan-based models by reversing the standard dependency between an agent&apos;s expectations and task-related goals. Plan-based approaches (Allen and Perrault 1979; Litman 1986; Carberry 1990; Lambert and Carberry 1991) start by applying context-independent inference rules to identify the agent&apos;s task-related plan, possibly favoring alternatives that extend a previously recognized plan. By contrast, our approach begins with an expectation, using it to premise both the analysis of utterance meaning and any inference 439 Computational Linguistics Volume 21, Number 4 about an agent&apos;s goals. Moreover, our approach treats apparent conflicts with expectations as meaningful; for example, if an utterance is inconsistent with expectations, then the reasoner will try to expla</context>
<context position="84102" citStr="Litman (1986)" startWordPosition="12667" endWordPosition="12668">nRepair(r,m,informref(r,m,whoIsGoing),ts(3),ts(1)) reconstruction(ts(3),ts(alt(1))) ***Updating Discourse Model*** Interpretation: informref(r,m,whoIsGoing) (turn number 4) expressed(do(r,informref(r,m,whoIsGoing)),4) Linguistic Intentions of informref(r,m,whoIsGoing): knowref(r,whoIsGoing)and intend(r,knowref(m,whoIsGoing)) Suppositions Added: expressed(knowref(r,whoIsGoing),4) expressed(intend(r,knowref(m,whoIsGoing)),4) r performed fourth turn repair ***Generating Utterance*** «&lt;surface-informref(r,m,whoIsGoing) Figure 10 The output for turn 4 from Russ&apos;s perspective. 5.1.2 Litman. Work by Litman (1986) attempts to overcome some of the limitations of Allen and Perrault&apos;s approach by extending the plan hierarchy to include discourselevel meta plans, in addition to domain-level plans. Metaplans include actions, such as introduce, continue, or clarify and are recognized, in part, by identifying cue phrases. Although the metaplans add flexibility by increasing the number of possible paths, they also add to the problem of pruning and ordering the paths, requiring additional heuristics. For example, there are specific rules for choosing among alternative metaplans on the basis of clue words, impli</context>
</contexts>
<marker>Litman, 1986</marker>
<rawString>Litman, Diane J. (1986). &amp;quot;Linguistic coherence: A plan-based alternative.&amp;quot; In 24th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, 215-223, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D W Loveland</author>
</authors>
<title>Automated Theorem Proving: A Logical Basis.</title>
<date>1978</date>
<publisher>North-Holland,</publisher>
<location>Amsterdam, The Netherlands.</location>
<contexts>
<context position="33124" citStr="Loveland 1978" startWordPosition="5189" endWordPosition="5190">mula g is a set .F U D1 U • • • U D&amp;quot;, where each D&apos; is a set of ground instances of elements of A&apos;, such that: 1. F U D1 U • • • U Dn is consistent 2. FUD1UUD=g 3. For all D&apos; such that 2 &lt; i &lt; n, there is no F U D&apos; U • • • U D&apos; -1 that satisfies the priority constraints and is inconsistent with D. 16 Poole&apos;s Theorist implements a full first-order clausal theorem prover in Prolog. Like Prolog, it applies a resolution-based procedure, reducing goals to their subgoals using rules of the form goal 4— subgoali A • • • A subgoal. However, unlike Prolog, it incorporates a model-elimination strategy (Loveland 1978; Stickel 1989; Umrigar and Pitchumani 1985) to reason by cases. 444 McRoy and Hirst The Repair of Speech Act Misunderstandings Priority constraints require that no ground instance of d E Ai can be in Di if its negation is explainable with defaults usable from any Al, j &lt; i. Priorities enable one to specify that one default is stronger than another, perhaps because it represents an exception. In our model, defaults will have one of three priority values: strong, weak, or very weak. The strongest value is reserved for attitudes about the prior context, whereas assumptions about expectations are</context>
</contexts>
<marker>Loveland, 1978</marker>
<rawString>Loveland, D. W. (1978). Automated Theorem Proving: A Logical Basis. North-Holland, Amsterdam, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susann Luperfoy</author>
</authors>
<title>The representation of multimodal user interface dialogues using discourse pegs.&amp;quot;</title>
<date>1992</date>
<booktitle>In 30th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference,</booktitle>
<pages>22--31</pages>
<location>Newark, Delaware.</location>
<contexts>
<context position="29800" citStr="Luperfoy (1992)" startWordPosition="4595" endWordPosition="4596">rist framework (Poole, Goebel, and Aleliunas 1987; Brewka 1989; van Arragon 1990), because it supports both default and abductive reasoning. Theorist typifies what is known as a &amp;quot;proof12 Non-understanding, which entails non-acceptance (or deferred acceptance), is signaled by second-turn repair. This type of repair will not be considered here. 13 Other misunderstandings are possible; for example there can be disagreement about what object a speaker is trying to identify with a referring expression (cf. Heeman and Hirst 1995; Hirst et al. 1994). 14 This distinction is similar to the one made by Luperfoy (1992). 15 For present purposes, we also assume that the complete model is accessible to the hearer; one could better simulate the limitations of working memory by limiting access to only the most recent utterances. 443 Computational Linguistics Volume 21, Number 4 based approach&amp;quot; to abduction because it relies on a theorem prover to collect the assumptions that would be needed to prove a given set of observations and to verify their consistency. Our reasoning algorithm is based on Poole&apos;s implementation of Theorist, which we extended to incorporate preferences among defaults as suggested by van Arr</context>
</contexts>
<marker>Luperfoy, 1992</marker>
<rawString>Luperfoy, Susann (1992). &amp;quot;The representation of multimodal user interface dialogues using discourse pegs.&amp;quot; In 30th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, 22-31, Newark, Delaware.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen F McCoy</author>
</authors>
<title>The role of perspective in responding to property misconceptions.&amp;quot;</title>
<date>1985</date>
<booktitle>In Proceedings of the Ninth International Joint Conference on Artificial Intelligence,</booktitle>
<volume>volume</volume>
<pages>2--791</pages>
<contexts>
<context position="91311" citStr="McCoy 1985" startWordPosition="13782" endWordPosition="13783">ke a concept or relation. To prevent misconceptions from triggering a misunderstanding, agents can check for evidence of misconception and try to resolve apparent errors. The symptoms of misconception include references to entities that do not map to previously known objects or operations (Webber and Mays 1983) or requests for clarification (Moore 1989). Errors are corrected by replacing or deleting parts of the problematic utterance so that it makes sense. Several correction strategies have been suggested: • Generalize a description by selectively ignoring some constraints (see Goodman 1985; McCoy 1985, 1986, 1988; Carberry 1988; Calistri-Yeh 1991; Eller and Carberry 1992), • Make a description more specific by adding extra constraints (see Eller and Carberry 1992), and • Choose a conceptual &amp;quot;sibling&amp;quot;, by combining generalization and constraint operations. For example, if there is more than one strategy for Hinkelman (1990); however, as with grounding acts, presumably the elimination of all possible interpretations could cue some type of repair mechanism, if they chose to incorporate one. 45 This is the same sense of &amp;quot;expectation&amp;quot; as used by Riesbeck (1974). 467 Computational Linguistics Vo</context>
</contexts>
<marker>McCoy, 1985</marker>
<rawString>McCoy, Kathleen F. (1985). &amp;quot;The role of perspective in responding to property misconceptions.&amp;quot; In Proceedings of the Ninth International Joint Conference on Artificial Intelligence, volume 2,791-793.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen F McCoy</author>
</authors>
<title>The ROMPER system: responding to object-related misconceptions using perspective.&amp;quot;</title>
<date>1986</date>
<booktitle>In 24th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference,</booktitle>
<pages>97--105</pages>
<marker>McCoy, 1986</marker>
<rawString>McCoy, Kathleen F. (1986). &amp;quot;The ROMPER system: responding to object-related misconceptions using perspective.&amp;quot; In 24th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, 97-105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen F McCoy</author>
</authors>
<title>Reasoning on a highlighted user model to respond to misconceptions.&amp;quot;</title>
<date>1988</date>
<journal>Computational Linguistics,</journal>
<volume>14</volume>
<issue>3</issue>
<pages>52--63</pages>
<marker>McCoy, 1988</marker>
<rawString>McCoy, Kathleen F. (1988). &amp;quot;Reasoning on a highlighted user model to respond to misconceptions.&amp;quot; Computational Linguistics, 14(3), 52-63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen E McCoy</author>
</authors>
<title>Generating context-sensitive responses to object misconceptions.&amp;quot;</title>
<date>1989</date>
<journal>Artificial Intelligence,</journal>
<volume>41</volume>
<issue>2</issue>
<pages>157--195</pages>
<contexts>
<context position="2992" citStr="McCoy (1989)" startWordPosition="439" endWordPosition="440">erstandings. 1.1 The difference between misunderstanding and misconception The notions of misunderstanding and misconception are easily confounded, so we shall begin by explicating the distinction. Misconceptions are errors in the prior knowledge of a participant; for example, believing that Canada is one of the United States. * Department of Electrical Engineering and Computer Science, Milwaukee, WI 53201, mcroy@cs.uwm.edu t Department of Computer Science, Toronto, Canada M5S 1A4, gh@cs.toronto.edu © 1995 Association for Computational Linguistics Computational Linguistics Volume 21, Number 4 McCoy (1989), Calistri-Yeh (1991), Pollack (1986b), Pollack (1990), and others have studied the problem of how one participant can determine the misconceptions of another during a conversation (see Section 5.3 below). Typically such errors can be recognized immediately when an expression is not interpretable with respect to the computer&apos;s (presumedly perfect!) knowledge of the world. By contrast, a participant is not aware, at least initially, when misunderstanding has occurred. In misunderstanding, a participant obtains an interpretation that she believes is complete and correct, but which is, however, n</context>
</contexts>
<marker>McCoy, 1989</marker>
<rawString>McCoy, Kathleen E (1989). &amp;quot;Generating context-sensitive responses to object misconceptions.&amp;quot; Artificial Intelligence, 41(2), 157-195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret L McLaughlin</author>
</authors>
<title>Conversation: How Talk is Organized.</title>
<date>1984</date>
<publisher>Sage Publications,</publisher>
<location>Beverly Hills.</location>
<marker>McLaughlin, 1984</marker>
<rawString>McLaughlin, Margaret L. (1984). Conversation: How Talk is Organized. Sage Publications, Beverly Hills.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan W McRoy</author>
</authors>
<title>Abductive Interpretation and Reinterpretation of Natural Language Utterances. Doctoral dissertation,</title>
<date>1993</date>
<tech>Technical Report No. 288,</tech>
<institution>Department of Computer Science, University of Toronto,</institution>
<location>Toronto, Canada.</location>
<note>Published as CSRI</note>
<contexts>
<context position="53036" citStr="McRoy (1993" startWordPosition="8292" endWordPosition="8293">). 28 Pollack (1986a) calls this the &amp;quot;is-a-way-to&amp;quot; relation. 29 It is actually controversial whether an askref followed by an inform-not-knowref is a valid adjacency pair. If such questions are taken to presuppose that the hearer knows the answer, a response to the contrary could also be considered a challenge of this presupposition (Tsui 1991). 30 It would have been possible to characterize actual belief using an appropriate set of axioms, such as those defining a weak S4 modal logic. However, current formalizations do not seem to account for the context-sensitivity of speakers&apos; beliefs. See McRoy (1993b) for a discussion. 451 Computational Linguistics Volume 21, Number 4 The second rule says that one would not expect the action a„piy if the linguistic intentions associated with it are incompatible with the context ts.&apos; Normally, as the discourse progresses, expectations for action that held in previous states of the context eventually cease to hold in the current context, because after the action occurs, it would be incompatible for an agent to say that he intends to achieve something that is already true. The compatibility between each of the linguistic intentions of a proposed action and </context>
</contexts>
<marker>McRoy, 1993</marker>
<rawString>McRoy, Susan W. (1993a). Abductive Interpretation and Reinterpretation of Natural Language Utterances. Doctoral dissertation, Department of Computer Science, University of Toronto, Toronto, Canada. Published as CSRI Technical Report No. 288, University of Toronto, Department of Computer Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan W McRoy</author>
</authors>
<title>Belief as an effect of an act of introspection.&amp;quot;</title>
<date>1993</date>
<booktitle>In /993 AAAI Spring Symposium on Reasoning about Mental States: Formal Theories and Applications,</booktitle>
<pages>86--89</pages>
<institution>Stanford University.</institution>
<contexts>
<context position="53036" citStr="McRoy (1993" startWordPosition="8292" endWordPosition="8293">). 28 Pollack (1986a) calls this the &amp;quot;is-a-way-to&amp;quot; relation. 29 It is actually controversial whether an askref followed by an inform-not-knowref is a valid adjacency pair. If such questions are taken to presuppose that the hearer knows the answer, a response to the contrary could also be considered a challenge of this presupposition (Tsui 1991). 30 It would have been possible to characterize actual belief using an appropriate set of axioms, such as those defining a weak S4 modal logic. However, current formalizations do not seem to account for the context-sensitivity of speakers&apos; beliefs. See McRoy (1993b) for a discussion. 451 Computational Linguistics Volume 21, Number 4 The second rule says that one would not expect the action a„piy if the linguistic intentions associated with it are incompatible with the context ts.&apos; Normally, as the discourse progresses, expectations for action that held in previous states of the context eventually cease to hold in the current context, because after the action occurs, it would be incompatible for an agent to say that he intends to achieve something that is already true. The compatibility between each of the linguistic intentions of a proposed action and </context>
</contexts>
<marker>McRoy, 1993</marker>
<rawString>McRoy, Susan W. (1993b). &amp;quot;Belief as an effect of an act of introspection.&amp;quot; In /993 AAAI Spring Symposium on Reasoning about Mental States: Formal Theories and Applications, 86-89, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johanna D Moore</author>
</authors>
<title>Responding to &amp;quot;Huh?&amp;quot;: Answering vaguely articulated follow-up questions.&amp;quot;</title>
<date>1989</date>
<booktitle>In Conference on Human Factors in Computing Systems (CHI&apos;89), 91-96,</booktitle>
<publisher>ACM Press</publisher>
<location>Austin, TX.</location>
<note>Bulletin (unnumbered).</note>
<contexts>
<context position="91056" citStr="Moore 1989" startWordPosition="13745" endWordPosition="13746">uses speech as input must be able to represent and reason with both. 5.3 Approaches to misconception Misconceptions are a deficit in an agent&apos;s knowledge of the world; they can become a barrier to understanding if they cause an agent to unintentionally evoke a concept or relation. To prevent misconceptions from triggering a misunderstanding, agents can check for evidence of misconception and try to resolve apparent errors. The symptoms of misconception include references to entities that do not map to previously known objects or operations (Webber and Mays 1983) or requests for clarification (Moore 1989). Errors are corrected by replacing or deleting parts of the problematic utterance so that it makes sense. Several correction strategies have been suggested: • Generalize a description by selectively ignoring some constraints (see Goodman 1985; McCoy 1985, 1986, 1988; Carberry 1988; Calistri-Yeh 1991; Eller and Carberry 1992), • Make a description more specific by adding extra constraints (see Eller and Carberry 1992), and • Choose a conceptual &amp;quot;sibling&amp;quot;, by combining generalization and constraint operations. For example, if there is more than one strategy for Hinkelman (1990); however, as wit</context>
</contexts>
<marker>Moore, 1989</marker>
<rawString>Moore, Johanna D. (1989). &amp;quot;Responding to &amp;quot;Huh?&amp;quot;: Answering vaguely articulated follow-up questions.&amp;quot; In Conference on Human Factors in Computing Systems (CHI&apos;89), 91-96, Austin, TX. ACM Press / Addison-Wesley. Also published as a special issue of SIGCHI Bulletin (unnumbered).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaaki Nagata</author>
<author>Tsuyoshi Morimoto</author>
</authors>
<title>An experimental statistical dialogue model to predict the speech act type of the next utterance.&amp;quot;</title>
<date>1993</date>
<booktitle>In International Symposium on Spoken Dialogue,</booktitle>
<location>Tokyo, Japan.</location>
<contexts>
<context position="23663" citStr="Nagata and Morimoto (1993)" startWordPosition="3661" endWordPosition="3664">sks of interpreting speech acts, producing speech acts, and repairing speech act interpretations within a nonmonotonic framework. In particular, speakers&apos; knowledge about language is represented as a set of default rules. The rules describe conventional strategies for producing coherent utterances, thereby displaying understanding, and strategies for identifying misunderstanding. As a result, speakers&apos; decisions about what utterances they might coherently generate next correspond to default inference over this theory, while decisions about possible in11 Quantitative results by Jose (1988) and Nagata and Morimoto (1993) provide evidence for these adjacency pairs. In addition, we have used pairs discovered by Conversation Analysis from real dialogues (Schegloff 1988). 441 Computational Linguistics Volume 21, Number 4 First turn Expected reply askref informref askif informif request comply pretell askref test ref assert ref testif assertif Figure 2 Adjacency pairs (Linguistic expectations). Example Metaplan type 1 A: Do you have a quarter? Plan adoption 2 B: No. Acceptance 3 B: I never lend money. Challenge 4 A: No, I meant to offer you one. Repair 5 B: Oh. Thanks. Repair 6 A: Bye. Closing Figure 3 Examples of</context>
</contexts>
<marker>Nagata, Morimoto, 1993</marker>
<rawString>Nagata, Masaaki, and Morimoto, Tsuyoshi (1993). &amp;quot;An experimental statistical dialogue model to predict the speech act type of the next utterance.&amp;quot; In International Symposium on Spoken Dialogue, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Raymond Perrault</author>
</authors>
<title>An application of default logic to speech act theory.&amp;quot;</title>
<date>1990</date>
<booktitle>In Intentions in Communication. Edited by</booktitle>
<pages>161--186</pages>
<publisher>The MIT Press,</publisher>
<contexts>
<context position="47168" citStr="Perrault (1990)" startWordPosition="7416" endWordPosition="7417">ay it becomes active in the turn sequence that has T as its focus (see Section 3.2.3). Moreover, once active, a supposition will remain active in all succeeding turn sequences, unless it is explicitly refuted. Individual turns are represented by a set of facts of the form expressed(P,T) and expressedNot(P,T), where P is an unnegated supposition that has not been formed from any simpler suppositions using the function and.&apos; 25 A related concern is how an agent&apos;s beliefs might change after an utterance has been understood as an act of a particular type. Although we have nothing new to add here, Perrault (1990) shows how default logic might be used to address this problem. 26 The intended meaning of expressedNot(P,T) is that during turn T speakers have acted as if the 449 Computational Linguistics Volume 21, Number 4 utterance-level form expressed beliefs and goals &apos;expectation expectations Figure 5 How the knowledge relations fit together. 3.3.2 Possible hypotheses. The second component of the model is M, the set of potential assumptions about misunderstandings and metaplanning decisions. This is given by the following set of Theorist defaults:&apos; intentionalAct, expectedReply, acceptance, ado ptPlan</context>
</contexts>
<marker>Perrault, 1990</marker>
<rawString>Perrault, C. Raymond (1990). &amp;quot;An application of default logic to speech act theory.&amp;quot; In Intentions in Communication. Edited by Philip R. Cohen, Jerry Morgan, and Martha Pollack, The MIT Press, 161-186. An earlier version of this paper was published as Technical Report CSLI-87-90 by the Center for the Study of Language and Information.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Raymond Perrault</author>
<author>James Allen</author>
</authors>
<title>A plan-based analysis of indirect speech acts.&amp;quot;</title>
<date>1980</date>
<journal>Computational Linguistics,</journal>
<volume>6</volume>
<pages>167--183</pages>
<contexts>
<context position="37313" citStr="Perrault and Allen (1980)" startWordPosition="5867" endWordPosition="5870">heir compatibility by verifying that each of the conjuncts of each supposition is compatible. (In the system, this is implemented as a special predicate, inconsistentLI). There is a danger in treating compatibility as a default in that one might miss some intuitively incompatible cases and hence some misunderstandings might not be detectable. An alternative would be to base compatibility on the notion of consistency in the underlying logic, if a complete logic has been defined.&apos; 3.2.2 Speech acts. For simplicity, we represent utterances as surface-level speech acts in the manner first used by Perrault and Allen (1980).19 Following Cohen and Levesque (1985), we limit the surface language to the acts surface-request, surface-inform, surface-informref, and surface-informif. Example 3 shows the representation of the literal form of Example 2, the fourth-turn repair example. (We abbreviate &amp;quot;m&amp;quot; for &amp;quot;Mother&amp;quot;, &amp;quot;r&amp;quot; for &amp;quot;Russ&amp;quot;, and &amp;quot;whoIsGoing&amp;quot; for &amp;quot;who&apos;s going&amp;quot;.) Example 3 Ti m: surface-request(m, r, informif(r, m, knowref(r, whoIsGoing))) T2 r: surface-request(r, m, informref(m, r, whoIsGoing)) T3 m: surface-inform(m, r, not knowref(m, whoIsGoing)) T4 r: surface-informref(r, m, whoIsGoing) We assume that such form</context>
<context position="81268" citStr="Perrault and Allen (1980)" startWordPosition="12303" endWordPosition="12306">There is a linguistic expectation corresponding to the adjacency pair askref—informref. • Russ believes its conditions. • The linguistic intentions of informref are compatible with the reconstruction. 5. Related work 5.1 Accounts based on plan recognition Plan-based accounts interpret speech acts by chaining from subaction to action, from actions to effects of other actions, and from preconditions to actions to identify a plan (i.e., a set of actions) that includes the observed act. Heuristics are applied to discriminate among alternatives. 5.1.1 Allen and Perrault. Allen and Perrault (1979), Perrault and Allen (1980) show how plan recognition can be used to understand indirect speech acts (such as the use of &amp;quot;Can you pass the salt?&amp;quot; as a polite request to pass the salt). To interpret an utterance, the approach applies a set of context-independent inference rules to identify all plausible plans. For example, one rule says that if a speaker wants to know the truth value of some proposition, then she might want the proposition to be made true. The final interpretation is then determined by a set of rating heuristics, such as &amp;quot;Decrease the rating of a path if it contains an action whose effects are already tr</context>
</contexts>
<marker>Perrault, Allen, 1980</marker>
<rawString>Perrault, C. Raymond, and Allen, James (1980). &amp;quot;A plan-based analysis of indirect speech acts.&amp;quot; Computational Linguistics, 6, 167-183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha E Pollack</author>
</authors>
<title>Inferring domain plans in question-answering.&amp;quot;</title>
<date>1986</date>
<tech>Technical Report TR 403,</tech>
<institution>Artificial Intelligence Center, SRI International,</institution>
<location>Menlo Park, CA.</location>
<contexts>
<context position="3028" citStr="Pollack (1986" startWordPosition="443" endWordPosition="444">ween misunderstanding and misconception The notions of misunderstanding and misconception are easily confounded, so we shall begin by explicating the distinction. Misconceptions are errors in the prior knowledge of a participant; for example, believing that Canada is one of the United States. * Department of Electrical Engineering and Computer Science, Milwaukee, WI 53201, mcroy@cs.uwm.edu t Department of Computer Science, Toronto, Canada M5S 1A4, gh@cs.toronto.edu © 1995 Association for Computational Linguistics Computational Linguistics Volume 21, Number 4 McCoy (1989), Calistri-Yeh (1991), Pollack (1986b), Pollack (1990), and others have studied the problem of how one participant can determine the misconceptions of another during a conversation (see Section 5.3 below). Typically such errors can be recognized immediately when an expression is not interpretable with respect to the computer&apos;s (presumedly perfect!) knowledge of the world. By contrast, a participant is not aware, at least initially, when misunderstanding has occurred. In misunderstanding, a participant obtains an interpretation that she believes is complete and correct, but which is, however, not the one that the other participan</context>
<context position="52444" citStr="Pollack (1986" startWordPosition="8199" endWordPosition="8200"> which exist in any situation, the model incorporates a cognitive, &amp;quot;belief-about-the-future&amp;quot; notion of expectation. These expectations depend on a speaker&apos;s knowledge of social norms, her understanding of the discourse so far, and her beliefs about the world at a particular time. They are captured by the following Theorist rules: DEFAULT (2, expectedReply(Pdo, Pcondition, do (si, a reply), ts)) : active(pd„ ts) A lexpectation (p do, Pcondition, do(si, arepiy)) A believe(si, Pcondition) D expected(si, areply, ts). FACT -,lintentionsOk(a, ts) D -expectedReply(pdo, Pcondition, do (s, a), ts). 28 Pollack (1986a) calls this the &amp;quot;is-a-way-to&amp;quot; relation. 29 It is actually controversial whether an askref followed by an inform-not-knowref is a valid adjacency pair. If such questions are taken to presuppose that the hearer knows the answer, a response to the contrary could also be considered a challenge of this presupposition (Tsui 1991). 30 It would have been possible to characterize actual belief using an appropriate set of axioms, such as those defining a weak S4 modal logic. However, current formalizations do not seem to account for the context-sensitivity of speakers&apos; beliefs. See McRoy (1993b) for a</context>
</contexts>
<marker>Pollack, 1986</marker>
<rawString>Pollack, Martha E. (1986a). &amp;quot;Inferring domain plans in question-answering.&amp;quot; Technical Report TR 403, Artificial Intelligence Center, SRI International, Menlo Park, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha E Pollack</author>
</authors>
<title>A model of plan inference that distinguishes between the beliefs of actors and observers.&amp;quot;</title>
<date>1986</date>
<booktitle>In Proceedings, 24th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>207--214</pages>
<location>New York.</location>
<contexts>
<context position="3028" citStr="Pollack (1986" startWordPosition="443" endWordPosition="444">ween misunderstanding and misconception The notions of misunderstanding and misconception are easily confounded, so we shall begin by explicating the distinction. Misconceptions are errors in the prior knowledge of a participant; for example, believing that Canada is one of the United States. * Department of Electrical Engineering and Computer Science, Milwaukee, WI 53201, mcroy@cs.uwm.edu t Department of Computer Science, Toronto, Canada M5S 1A4, gh@cs.toronto.edu © 1995 Association for Computational Linguistics Computational Linguistics Volume 21, Number 4 McCoy (1989), Calistri-Yeh (1991), Pollack (1986b), Pollack (1990), and others have studied the problem of how one participant can determine the misconceptions of another during a conversation (see Section 5.3 below). Typically such errors can be recognized immediately when an expression is not interpretable with respect to the computer&apos;s (presumedly perfect!) knowledge of the world. By contrast, a participant is not aware, at least initially, when misunderstanding has occurred. In misunderstanding, a participant obtains an interpretation that she believes is complete and correct, but which is, however, not the one that the other participan</context>
<context position="52444" citStr="Pollack (1986" startWordPosition="8199" endWordPosition="8200"> which exist in any situation, the model incorporates a cognitive, &amp;quot;belief-about-the-future&amp;quot; notion of expectation. These expectations depend on a speaker&apos;s knowledge of social norms, her understanding of the discourse so far, and her beliefs about the world at a particular time. They are captured by the following Theorist rules: DEFAULT (2, expectedReply(Pdo, Pcondition, do (si, a reply), ts)) : active(pd„ ts) A lexpectation (p do, Pcondition, do(si, arepiy)) A believe(si, Pcondition) D expected(si, areply, ts). FACT -,lintentionsOk(a, ts) D -expectedReply(pdo, Pcondition, do (s, a), ts). 28 Pollack (1986a) calls this the &amp;quot;is-a-way-to&amp;quot; relation. 29 It is actually controversial whether an askref followed by an inform-not-knowref is a valid adjacency pair. If such questions are taken to presuppose that the hearer knows the answer, a response to the contrary could also be considered a challenge of this presupposition (Tsui 1991). 30 It would have been possible to characterize actual belief using an appropriate set of axioms, such as those defining a weak S4 modal logic. However, current formalizations do not seem to account for the context-sensitivity of speakers&apos; beliefs. See McRoy (1993b) for a</context>
</contexts>
<marker>Pollack, 1986</marker>
<rawString>Pollack, Martha E. (1986b). &amp;quot;A model of plan inference that distinguishes between the beliefs of actors and observers.&amp;quot; In Proceedings, 24th annual meeting of the Association for Computational Linguistics, 207-214, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha E Pollack</author>
</authors>
<title>Plans as complex mental attitudes.&amp;quot;</title>
<date>1990</date>
<booktitle>In Intentions in Communication. Edited by</booktitle>
<pages>77--103</pages>
<publisher>MIT Press,</publisher>
<contexts>
<context position="3046" citStr="Pollack (1990)" startWordPosition="445" endWordPosition="446">ding and misconception The notions of misunderstanding and misconception are easily confounded, so we shall begin by explicating the distinction. Misconceptions are errors in the prior knowledge of a participant; for example, believing that Canada is one of the United States. * Department of Electrical Engineering and Computer Science, Milwaukee, WI 53201, mcroy@cs.uwm.edu t Department of Computer Science, Toronto, Canada M5S 1A4, gh@cs.toronto.edu © 1995 Association for Computational Linguistics Computational Linguistics Volume 21, Number 4 McCoy (1989), Calistri-Yeh (1991), Pollack (1986b), Pollack (1990), and others have studied the problem of how one participant can determine the misconceptions of another during a conversation (see Section 5.3 below). Typically such errors can be recognized immediately when an expression is not interpretable with respect to the computer&apos;s (presumedly perfect!) knowledge of the world. By contrast, a participant is not aware, at least initially, when misunderstanding has occurred. In misunderstanding, a participant obtains an interpretation that she believes is complete and correct, but which is, however, not the one that the other participant intended her to </context>
</contexts>
<marker>Pollack, 1990</marker>
<rawString>Pollack, Martha E. (1990). &amp;quot;Plans as complex mental attitudes.&amp;quot; In Intentions in Communication. Edited by Philip Cohen, Jerry Morgan, and Martha Pollack, MIT Press, 77-103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Poole</author>
<author>Randy Goebel</author>
<author>Romas Aleliunas</author>
</authors>
<title>Theorist: A logical reasoning system for defaults and diagnosis.&amp;quot;</title>
<date>1987</date>
<booktitle>In The Knowledge Frontier: Essays in the Representation of Knowledge. Edited by Nick Cercone and</booktitle>
<pages>331--352</pages>
<publisher>Gordon McCalla, Springer-Verlag,</publisher>
<institution>Faculty of Mathematics, University of Waterloo,</institution>
<location>New York,</location>
<note>Also published as Research Report CS-86-06,</note>
<marker>Poole, Goebel, Aleliunas, 1987</marker>
<rawString>Poole, David; Goebel, Randy; and Aleliunas, Romas (1987). &amp;quot;Theorist: A logical reasoning system for defaults and diagnosis.&amp;quot; In The Knowledge Frontier: Essays in the Representation of Knowledge. Edited by Nick Cercone and Gordon McCalla, Springer-Verlag, New York, 331-352. Also published as Research Report CS-86-06, Faculty of Mathematics, University of Waterloo, February, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norbert Reithinger</author>
<author>Elisabeth Maier</author>
</authors>
<title>Utilizing statistical dialogue act processing in verbmobil.&amp;quot;</title>
<date>1995</date>
<booktitle>In Proceedings, 33rd annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>116--121</pages>
<location>Cambridge, MA,</location>
<contexts>
<context position="21060" citStr="Reithinger and Maier (1995)" startWordPosition="3282" endWordPosition="3285">visit at 6pm might create an expectation that dinner will be served, it does not express an intention to serve it. 7 In the figure, we have used the symbol intend to name both the intention to achieve a situation in which a property holds and the intention to do action. 8 Schegloff actually argues against representing such sequences as speech acts; however, as in the computational work cited above, we have used the notion of &amp;quot;discourse-level speech act&amp;quot; to represent the functional relationship between the surface form of an utterance, the context, and the attitudes expressed by the speaker. 9 Reithinger and Maier (1995) have used n-gram dialogue act probabilities to induce the adjacency pairs from a corpus of dialogues for appointment scheduling. 10 Communication can occur despite such differences because speakers with similar linguistic experiences presumably will develop similar expectations about how discourse works. Differences in expectations might very well be one thing that new acquaintances must resolve in order to avoid social conflict. 440 McRoy and Hirst The Repair of Speech Act Misunderstandings Act type Speech act name Linguistic intentions informative assert(S, H, P) know(S, P) assertref(S, H, </context>
<context position="56371" citStr="Reithinger and Maier (1995)" startWordPosition="8804" endWordPosition="8807">e. The preference for coherent interpretations is especially important when there is more than one discourse-level act for which the utterance is a possible decomposition. 31 Although, like expectedReply, active is a default, active will take precedence over expectedReply, because it has been given a higher priority on the assumption that memory for suppositions is stronger than expectation. 32 It is possible that the same surface form might accomplish several different discourse acts, in which case it might be desirable to evaluate the likelihood of alternative choices. The work discussed by Reithinger and Maier (1995), for example, found statistical regularities in the misinterpretations that occurred in their corpus of appointment-scheduling dialogues. 452 McRoy and Hirst The Repair of Speech Act Misunderstandings Table 1 Name Plan adoption Purpose Introducing a new goal Axiom DEFAULT (3, adoptPlan(si, 52, a2, ts)) : hasGoal(si, do (s2, a2), ts) A wouldExpect(si, do(si, al), do(s2,a2)) D shouldTry(si, s2, al, ts). FACT -dintentionsOk(ai, ts) -adoptPlan(si, s2, a2, ts) Summary Speaker Si should do action al in discourse ts when: 1. Si wants speaker sz to do action az; 2. Si would expect az to follow an act</context>
</contexts>
<marker>Reithinger, Maier, 1995</marker>
<rawString>Reithinger, Norbert, and Maier, Elisabeth (1995). &amp;quot;Utilizing statistical dialogue act processing in verbmobil.&amp;quot; In Proceedings, 33rd annual meeting of the Association for Computational Linguistics, 116-121, Cambridge, MA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Riesbeck</author>
</authors>
<title>Computational understanding: Analysis of sentences and context.&amp;quot;</title>
<date>1974</date>
<tech>Technical Report STAN-CS-74-437,</tech>
<institution>Computer Science Department, Stanford University.</institution>
<contexts>
<context position="91877" citStr="Riesbeck (1974)" startWordPosition="13869" endWordPosition="13870">ng some constraints (see Goodman 1985; McCoy 1985, 1986, 1988; Carberry 1988; Calistri-Yeh 1991; Eller and Carberry 1992), • Make a description more specific by adding extra constraints (see Eller and Carberry 1992), and • Choose a conceptual &amp;quot;sibling&amp;quot;, by combining generalization and constraint operations. For example, if there is more than one strategy for Hinkelman (1990); however, as with grounding acts, presumably the elimination of all possible interpretations could cue some type of repair mechanism, if they chose to incorporate one. 45 This is the same sense of &amp;quot;expectation&amp;quot; as used by Riesbeck (1974). 467 Computational Linguistics Volume 21, Number 4 achieving a goal, then an entity that corresponds to a step from one strategy might be replaced by one corresponding to a step from one of the other strategies (see Carberry 1985, 1987; Eller and Carberry 1992; Moore 1989). Although these approaches do quite well at preventing certain classes of misunderstandings, they cannot prevent them all. Moreover, these approaches may actually trigger misunderstandings because they always find some substitution, and yet they lack any mechanisms for detecting when one of their own previous repairs was in</context>
</contexts>
<marker>Riesbeck, 1974</marker>
<rawString>Riesbeck, Christopher (1974). &amp;quot;Computational understanding: Analysis of sentences and context.&amp;quot; Technical Report STAN-CS-74-437, Computer Science Department, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emanuel A Schegloff</author>
</authors>
<title>Presequences and indirection: Applying speech act theory to ordinary conversation.&amp;quot;</title>
<date>1988</date>
<journal>Journal of Pragmatics,</journal>
<pages>12--55</pages>
<contexts>
<context position="23812" citStr="Schegloff 1988" startWordPosition="3684" endWordPosition="3685">edge about language is represented as a set of default rules. The rules describe conventional strategies for producing coherent utterances, thereby displaying understanding, and strategies for identifying misunderstanding. As a result, speakers&apos; decisions about what utterances they might coherently generate next correspond to default inference over this theory, while decisions about possible in11 Quantitative results by Jose (1988) and Nagata and Morimoto (1993) provide evidence for these adjacency pairs. In addition, we have used pairs discovered by Conversation Analysis from real dialogues (Schegloff 1988). 441 Computational Linguistics Volume 21, Number 4 First turn Expected reply askref informref askif informif request comply pretell askref test ref assert ref testif assertif Figure 2 Adjacency pairs (Linguistic expectations). Example Metaplan type 1 A: Do you have a quarter? Plan adoption 2 B: No. Acceptance 3 B: I never lend money. Challenge 4 A: No, I meant to offer you one. Repair 5 B: Oh. Thanks. Repair 6 A: Bye. Closing Figure 3 Examples of different types of coherence strategies. terpretations of utterances (including recognizing misunderstanding) correspond to abductive inference over</context>
</contexts>
<marker>Schegloff, 1988</marker>
<rawString>Schegloff, Emanuel A. (1988). &amp;quot;Presequences and indirection: Applying speech act theory to ordinary conversation.&amp;quot; Journal of Pragmatics, 12,55-62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emanuel A Schegloff</author>
</authors>
<title>Repair after next turn: The last structurally provided defense of intersubjectivity in conversation.&amp;quot;</title>
<date>1992</date>
<journal>American Journal of Sociology,</journal>
<volume>97</volume>
<issue>5</issue>
<pages>1295--1345</pages>
<contexts>
<context position="5569" citStr="Schegloff 1992" startWordPosition="828" endWordPosition="829">l explanation; the indicated interpretation is called the displayed interpretation. When a participant notices a discrepancy between her own interpretation and the one displayed by the other participant, she can choose to initiate a repair or to let it pass. By their choice of repairing or accepting a displayed interpretation, speakers in effect negotiate the meaning of utterances.&apos; Repairs can take many forms, depending on how and when a misunderstanding becomes apparent. Conversation analysts classify repairs according to how soon after the problematic turn a participant initiates a repair (Schegloff 1992). The most common type occurs within the turn itself or immediately after it, before the other participant has had a chance to reply. These are called first-turn repairs. The next most common type, second-turn repairs, occur as the reply to the problematic turn (e.g., as a request for clarification). We will not consider these two types of repairs further, because they do not involve misunderstanding per se. Rather, they are used to correct misconceptions, misspeakings, nonhearings, etc. Third-turn and fourth-turn2 repairs address actual misunderstandings. If a display of misunderstanding occu</context>
</contexts>
<marker>Schegloff, 1992</marker>
<rawString>Schegloff, Emanuel A. (1992). &amp;quot;Repair after next turn: The last structurally provided defense of intersubjectivity in conversation.&amp;quot; American Journal of Sociology, 97(5), 1295-1345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emanuel A Schegloff</author>
<author>Harvey Sacks</author>
</authors>
<title>Opening up closings.&amp;quot;</title>
<date>1973</date>
<journal>Semiotica,</journal>
<pages>7--289</pages>
<contexts>
<context position="11971" citStr="Schegloff and Sacks 1973" startWordPosition="1847" endWordPosition="1850">Agents, aware of some rule or norm that is relevant to their current situation, choose to follow (or not follow) the rule, depending on how they view the consequences of their choice. One important convention is the adjacency pair. Adjacency pairs are sequentially constrained pairs of utterances, (such as question-answer), in which an utterance of the first type creates an expectation for one of the second. A hearer is not bound to produce the expected reply, but if he does not, he must be ready to justify his action and to accept responsibility for any inferences that the speaker might make (Schegloff and Sacks 1973). Where the CA approach is weakest is in its explanation of how the recipient of an utterance is able to understand an utterance that is the first part of an adjacency pair. For this, an agent needs linguistic knowledge linking the features of an utterance to a range of speech acts that can form adjacency pairs. Agents also need to have some idea of the beliefs and intentions that particular actions express, so they can make judgments about their appropriateness in the context. 1.4 Overview The aim of our research is to construct a model of communicative interaction that will be able to suppor</context>
</contexts>
<marker>Schegloff, Sacks, 1973</marker>
<rawString>Schegloff, Emanuel A., and Sacks, Harvey (1973). &amp;quot;Opening up closings.&amp;quot; Semiotica, 7,289-327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronnie Smith</author>
</authors>
<title>A Computational Model of Expectation-Driven Mixed-Initiative Dialog Processing.&amp;quot;</title>
<date>1992</date>
<tech>Doctoral dissertation,</tech>
<institution>Department of Computer Science, Duke University,</institution>
<location>Durham, NC.</location>
<contexts>
<context position="89977" citStr="Smith 1992" startWordPosition="13578" endWordPosition="13579">responses: 1. A statement about background knowledge that might be needed. 2. A statement about the underlying purpose of A. 3. A statement about related task steps (i.e., subgoals of A, tasks that contain A as a step, or tasks that might follow A). 4. A statement about the accomplishment of A. These expectations are independent of the belief state of an agent and are specified down to the semantic (and sometimes even lexical) level. This information has long been used to discriminate between ambiguous interpretations and correct mistakes made by the speech recognizer (Fink and Biermann 1986; Smith 1992). Typically, an utterance will be interpreted according to the expectation that matches it most closely. By contrast, our approach and that of the plan-based accounts use &amp;quot;expectation&amp;quot; to refer to agents&apos; beliefs about how future utterances might relate to prior ones. These expectations are determined both by an agent&apos;s understanding of typical behavior and by his or her mental state. These two notions of expectation are complementary, and any dialogue model that uses speech as input must be able to represent and reason with both. 5.3 Approaches to misconception Misconceptions are a deficit in</context>
</contexts>
<marker>Smith, 1992</marker>
<rawString>Smith, Ronnie (1992). &amp;quot;A Computational Model of Expectation-Driven Mixed-Initiative Dialog Processing.&amp;quot; Doctoral dissertation, Department of Computer Science, Duke University, Durham, NC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruce Spencer</author>
</authors>
<title>Assimilation in plan recognition via truth maintenance with reduced redundancy.&amp;quot;</title>
<date>1990</date>
<institution>Department of Computer Science, University of Waterloo.</institution>
<note>Doctoral dissertation,</note>
<contexts>
<context position="9681" citStr="Spencer (1990)" startWordPosition="1489" endWordPosition="1490">he &apos;request&apos; interpretation. The difficulty in considering misunderstandings in addition to intended interpretations is that it greatly increases the number of alternatives that an interpreter needs to consider, because one cannot simply ignore the interpretations that seem inconsistent. However, predominant computational approaches to dialogue, which are based solely on inference of intention, already have difficulty constraining the interpretation process. Sociological accounts suggest a more constrained approach to interpretation 3 This is distinct from the kind of plan repair described by Spencer (1990), which he models using an assumption-based truth-maintenance system. In his work, &amp;quot;repair&amp;quot; addresses the problem of incompleteness in a taxonomy of plans, rather than errors in interpretation. 437 Computational Linguistics Volume 21, Number 4 and the recognition of misunderstanding, but none are computational. Our model extends the intentional and social accounts of discourse, combining the strengths of both. In the intentional accounts, speakers use their beliefs, goals, and expectations to decide what to say; when they interpret an utterance, they identify goals that might account for it. F</context>
</contexts>
<marker>Spencer, 1990</marker>
<rawString>Spencer, Bruce (1990). &amp;quot;Assimilation in plan recognition via truth maintenance with reduced redundancy.&amp;quot; Doctoral dissertation, Department of Computer Science, University of Waterloo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark E Stickel</author>
</authors>
<title>A Prolog technology theorem prover.&amp;quot;</title>
<date>1989</date>
<journal>Journal of Automated Reasoning,</journal>
<pages>4--353</pages>
<contexts>
<context position="33138" citStr="Stickel 1989" startWordPosition="5191" endWordPosition="5192"> .F U D1 U • • • U D&amp;quot;, where each D&apos; is a set of ground instances of elements of A&apos;, such that: 1. F U D1 U • • • U Dn is consistent 2. FUD1UUD=g 3. For all D&apos; such that 2 &lt; i &lt; n, there is no F U D&apos; U • • • U D&apos; -1 that satisfies the priority constraints and is inconsistent with D. 16 Poole&apos;s Theorist implements a full first-order clausal theorem prover in Prolog. Like Prolog, it applies a resolution-based procedure, reducing goals to their subgoals using rules of the form goal 4— subgoali A • • • A subgoal. However, unlike Prolog, it incorporates a model-elimination strategy (Loveland 1978; Stickel 1989; Umrigar and Pitchumani 1985) to reason by cases. 444 McRoy and Hirst The Repair of Speech Act Misunderstandings Priority constraints require that no ground instance of d E Ai can be in Di if its negation is explainable with defaults usable from any Al, j &lt; i. Priorities enable one to specify that one default is stronger than another, perhaps because it represents an exception. In our model, defaults will have one of three priority values: strong, weak, or very weak. The strongest value is reserved for attitudes about the prior context, whereas assumptions about expectations are given as weak</context>
</contexts>
<marker>Stickel, 1989</marker>
<rawString>Stickel, Mark E. (1989). &amp;quot;A Prolog technology theorem prover.&amp;quot; Journal of Automated Reasoning, 4,353-360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Terasaki</author>
</authors>
<title>Pre-announcement sequences in conversation.&amp;quot; Social Science Working Paper 99,</title>
<date>1976</date>
<institution>School of Social Science, University of California, Irvine.</institution>
<contexts>
<context position="7355" citStr="Terasaki 1976" startWordPosition="1122" endWordPosition="1123">airs of turns. We shall use &amp;quot;nth-turn&amp;quot; to refer to both types, allowing intervening exchanges. 436 McRoy and Hirst The Repair of Speech Act Misunderstandings Example 1 Ti S: Where do you do this? T2 H: To make the crops grow. T3 S: I said where do you do it. T4 H: In a tin hut in Greeba. If a display of misunderstanding occurs during a subsequent turn by the same speaker who generated the misunderstood turn, and the hearer then reinterprets the earlier turn and produces a new response to it, then we say that they have made a fourth-turn repair. The fragment of conversation shown in Example 2 (Terasaki 1976) includes a fourth-turn repair. Initially, Russ interprets T1 as expressing Mother&apos;s desire to tell, that is, as a pretelling or preannouncement, but finds this interpretation inconsistent with her next utterance. In T3, instead of telling him who&apos;s going (as one would expect after a pretelling), Mother claims that she does not know (and therefore could not tell). Russ recovers by reinterpreting Ti as an indirect request, which his T4 attempts to satisfy. Fox (1987) points out that such repairs involve, in effect, a reconstruction of the initial utterance. From an AT perspective, these reconst</context>
</contexts>
<marker>Terasaki, 1976</marker>
<rawString>Terasaki, A. (1976). &amp;quot;Pre-announcement sequences in conversation.&amp;quot; Social Science Working Paper 99, School of Social Science, University of California, Irvine.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Traum</author>
<author>James Allen</author>
</authors>
<date>1994</date>
<booktitle>Proceedings of the Conference,</booktitle>
<pages>1--8</pages>
<location>Las Cruces, NM.</location>
<contexts>
<context position="88875" citStr="Traum and Allen (1994)" startWordPosition="13395" endWordPosition="13398">organized into a finite state grammar, and do not account for grounding acts that would violate a receiver&apos;s expectations. In conversation, grounding acts that violate the grammar are not recognized. Traum and Hinkelman suggest that such violations should be used to trigger a repair, but admit that, except when a repair has been requested explicitly, the model itself says nothing about when a repair should be uttered (p. 593).&amp;quot; 44 Interpretations that have the right pragmatic force but inconsistent implicatures are ruled out as in 466 McRoy and Hirst The Repair of Speech Act Misunderstandings Traum and Allen (1994) extend the work to include a notion of social obligation, which serves much the same purpose as expectations in our model. 5.2 Other expectation-driven accounts Within the speech understanding community, the word &amp;quot;expectation&amp;quot; has been used differently from our use here. Expectation in the speech context refers to what the next word or utterance is likely to be about.&apos; For example, after the computer asks the user to perform some action A, it might expect any of the following types of responses: 1. A statement about background knowledge that might be needed. 2. A statement about the underlyin</context>
</contexts>
<marker>Traum, Allen, 1994</marker>
<rawString>Traum, David, and Allen, James (1994). Proceedings of the Conference, 1-8, Las Cruces, NM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Traum</author>
<author>Elizabeth Hinkelman</author>
</authors>
<title>Conversation acts in task-oriented spoken dialogue.&amp;quot;</title>
<date>1992</date>
<journal>Computational Intelligence,</journal>
<volume>8</volume>
<issue>3</issue>
<contexts>
<context position="87755" citStr="Traum and Hinkelman (1992)" startWordPosition="13218" endWordPosition="13221">osen to treat her utterance as a pretelling, perhaps to confirm his suspicions or to delay answering. 5.1.5 Traum and Hinkelman. Hinkelman&apos;s (1990) work incorporates some abductive reasoning in her model of utterance interpretation. The model treats different features in the input, such as the mood of a sentence or the presence of a particular lexical item, as manifestations of different speech acts. During interpretation, procedures that test for particular features of the input suggest candidates. The system then removes any candidates whose implicatures are inconsistent with prior beliefs. Traum and Hinkelman (1992) extend this work by generalizing the notion of speech act to conversation act. Conversation acts include traditional speech act types as well as what Traum and Hinkelman call grounding acts. Conversation acts, however, are not assumed to be understood without some positive evidence by the receiver, such as an acknowledgment. Grounding acts include initiating, clarifying, or acknowledging an utterance, and taking and releasing a turns. These acts differ from our own metaplans in that they are organized into a finite state grammar, and do not account for grounding acts that would violate a rece</context>
</contexts>
<marker>Traum, Hinkelman, 1992</marker>
<rawString>Traum, David, and Hinkelman, Elizabeth (1992). &amp;quot;Conversation acts in task-oriented spoken dialogue.&amp;quot; Computational Intelligence, 8(3). Special Issue: Computational Approaches to Non-Literal Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amy B M Tsui</author>
</authors>
<title>Sequencing rules and coherence in discourse.&amp;quot;</title>
<date>1991</date>
<journal>Journal of Pragmatics,</journal>
<volume>15</volume>
<pages>111--129</pages>
<contexts>
<context position="52771" citStr="Tsui 1991" startWordPosition="8250" endWordPosition="8251">: DEFAULT (2, expectedReply(Pdo, Pcondition, do (si, a reply), ts)) : active(pd„ ts) A lexpectation (p do, Pcondition, do(si, arepiy)) A believe(si, Pcondition) D expected(si, areply, ts). FACT -,lintentionsOk(a, ts) D -expectedReply(pdo, Pcondition, do (s, a), ts). 28 Pollack (1986a) calls this the &amp;quot;is-a-way-to&amp;quot; relation. 29 It is actually controversial whether an askref followed by an inform-not-knowref is a valid adjacency pair. If such questions are taken to presuppose that the hearer knows the answer, a response to the contrary could also be considered a challenge of this presupposition (Tsui 1991). 30 It would have been possible to characterize actual belief using an appropriate set of axioms, such as those defining a weak S4 modal logic. However, current formalizations do not seem to account for the context-sensitivity of speakers&apos; beliefs. See McRoy (1993b) for a discussion. 451 Computational Linguistics Volume 21, Number 4 The second rule says that one would not expect the action a„piy if the linguistic intentions associated with it are incompatible with the context ts.&apos; Normally, as the discourse progresses, expectations for action that held in previous states of the context eventu</context>
</contexts>
<marker>Tsui, 1991</marker>
<rawString>Tsui, Amy B. M. (1991). &amp;quot;Sequencing rules and coherence in discourse.&amp;quot; Journal of Pragmatics, 15, 111-129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zerksis D Umrigar</author>
<author>Vijay Pitchumani</author>
</authors>
<title>An experiment in programming with full first-order logic.&amp;quot;</title>
<date>1985</date>
<booktitle>In Symposium of Logic Programming,</booktitle>
<publisher>IEEE Computer Society Press.</publisher>
<location>Boston, MA.</location>
<contexts>
<context position="33168" citStr="Umrigar and Pitchumani 1985" startWordPosition="5193" endWordPosition="5196"> • U D&amp;quot;, where each D&apos; is a set of ground instances of elements of A&apos;, such that: 1. F U D1 U • • • U Dn is consistent 2. FUD1UUD=g 3. For all D&apos; such that 2 &lt; i &lt; n, there is no F U D&apos; U • • • U D&apos; -1 that satisfies the priority constraints and is inconsistent with D. 16 Poole&apos;s Theorist implements a full first-order clausal theorem prover in Prolog. Like Prolog, it applies a resolution-based procedure, reducing goals to their subgoals using rules of the form goal 4— subgoali A • • • A subgoal. However, unlike Prolog, it incorporates a model-elimination strategy (Loveland 1978; Stickel 1989; Umrigar and Pitchumani 1985) to reason by cases. 444 McRoy and Hirst The Repair of Speech Act Misunderstandings Priority constraints require that no ground instance of d E Ai can be in Di if its negation is explainable with defaults usable from any Al, j &lt; i. Priorities enable one to specify that one default is stronger than another, perhaps because it represents an exception. In our model, defaults will have one of three priority values: strong, weak, or very weak. The strongest value is reserved for attitudes about the prior context, whereas assumptions about expectations are given as weak defaults and assumptions abou</context>
</contexts>
<marker>Umrigar, Pitchumani, 1985</marker>
<rawString>Umrigar, Zerksis D., and Pitchumani, Vijay (1985). &amp;quot;An experiment in programming with full first-order logic.&amp;quot; In Symposium of Logic Programming, Boston, MA. IEEE Computer Society Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul van Arragon</author>
</authors>
<title>Nested Default Reasoning for User Modeling.&amp;quot; Doctoral dissertation,</title>
<date>1990</date>
<institution>Department of Computer Science, University of Waterloo,</institution>
<location>Waterloo, Ontario.</location>
<note>Published by the department as Research Report CS-90-25.</note>
<marker>van Arragon, 1990</marker>
<rawString>van Arragon, Paul (1990). &amp;quot;Nested Default Reasoning for User Modeling.&amp;quot; Doctoral dissertation, Department of Computer Science, University of Waterloo, Waterloo, Ontario. Published by the department as Research Report CS-90-25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
</authors>
<title>Redundancy in collaborative dialogue.&amp;quot;</title>
<date>1991</date>
<booktitle>In 1991 AAAI Fall Symposium on Discourse Structure in Natural Language Understanding and Generation,</booktitle>
<pages>124--129</pages>
<location>Pacific Grove, Monterey, CA.</location>
<contexts>
<context position="38187" citStr="Walker 1991" startWordPosition="5993" endWordPosition="5994">abbreviate &amp;quot;m&amp;quot; for &amp;quot;Mother&amp;quot;, &amp;quot;r&amp;quot; for &amp;quot;Russ&amp;quot;, and &amp;quot;whoIsGoing&amp;quot; for &amp;quot;who&apos;s going&amp;quot;.) Example 3 Ti m: surface-request(m, r, informif(r, m, knowref(r, whoIsGoing))) T2 r: surface-request(r, m, informref(m, r, whoIsGoing)) T3 m: surface-inform(m, r, not knowref(m, whoIsGoing)) T4 r: surface-informref(r, m, whoIsGoing) We assume that such forms can be identified by the parser, for example treating all declarative sentences as surface-informs.&apos; 18 Note that human behavior lies somewhere in between these two extremes; in particular, people do not seem to express all the entailments of what they utter (Walker 1991). 19 Other representation languages, such as one based on case semantics, would also be compatible with the approach and would permit greater flexibility. The cost of the increased flexibility would be increased difficulty in mapping surface descriptions onto speech acts; however, because less effort would be required in sentence processing, the total complexity of the problem need not increase. Using a more finely-grained representation, one could reason about sentence type, particles, and prosody explicitly, instead of requiring the sentence processor to interpret this information (cf. Hinke</context>
</contexts>
<marker>Walker, 1991</marker>
<rawString>Walker, Marilyn A. (1991). &amp;quot;Redundancy in collaborative dialogue.&amp;quot; In 1991 AAAI Fall Symposium on Discourse Structure in Natural Language Understanding and Generation, 124-129, Pacific Grove, Monterey, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie L Webber</author>
<author>Mays</author>
</authors>
<title>Eric</title>
<date>1983</date>
<booktitle>In Proceedings of the Eighth International Joint Conference on Artificial Intelligence,</booktitle>
<pages>650--652</pages>
<location>Karlsruhe.</location>
<contexts>
<context position="91013" citStr="Webber and Mays 1983" startWordPosition="13737" endWordPosition="13740">ation are complementary, and any dialogue model that uses speech as input must be able to represent and reason with both. 5.3 Approaches to misconception Misconceptions are a deficit in an agent&apos;s knowledge of the world; they can become a barrier to understanding if they cause an agent to unintentionally evoke a concept or relation. To prevent misconceptions from triggering a misunderstanding, agents can check for evidence of misconception and try to resolve apparent errors. The symptoms of misconception include references to entities that do not map to previously known objects or operations (Webber and Mays 1983) or requests for clarification (Moore 1989). Errors are corrected by replacing or deleting parts of the problematic utterance so that it makes sense. Several correction strategies have been suggested: • Generalize a description by selectively ignoring some constraints (see Goodman 1985; McCoy 1985, 1986, 1988; Carberry 1988; Calistri-Yeh 1991; Eller and Carberry 1992), • Make a description more specific by adding extra constraints (see Eller and Carberry 1992), and • Choose a conceptual &amp;quot;sibling&amp;quot;, by combining generalization and constraint operations. For example, if there is more than one str</context>
</contexts>
<marker>Webber, Mays, 1983</marker>
<rawString>Webber, Bonnie L., and Mays, Eric (1983). &amp;quot;Varieties of user misconceptions: Detection and correction.&amp;quot; In Proceedings of the Eighth International Joint Conference on Artificial Intelligence, 650-652, Karlsruhe.</rawString>
</citation>
<citation valid="false">
<title>Is formula pickForm(a,b,surface-request(a,b,informref(b,a,whenD)), askref(a,b,whenD),ts(0)) ok (y/n)?n. Is formula pickForm(a,b,request(a,b,surface-informref(b,a,whenD)), testref(a,b,whenD),ts(0)) ok (y/n)?y. Explanation:</title>
<booktitle>intentionalAct(a,b,testref(a,b,whenD),ts(0)) adoptPlan(a,b,testref(a,b,whenD),assertref(b,a,whenD,ts(0)) credulousB(a,knowref(b,whenD)) credulousI(a,ts(0)) pickForm(a,b,surface-request(a,b,assertref(b,a,whenD)), testref(a,b,whenD),ts(0</booktitle>
<marker></marker>
<rawString>Is formula pickForm(a,b,surface-request(a,b,informref(b,a,whenD)), askref(a,b,whenD),ts(0)) ok (y/n)?n. Is formula pickForm(a,b,request(a,b,surface-informref(b,a,whenD)), testref(a,b,whenD),ts(0)) ok (y/n)?y. Explanation: intentionalAct(a,b,testref(a,b,whenD),ts(0)) adoptPlan(a,b,testref(a,b,whenD),assertref(b,a,whenD,ts(0)) credulousB(a,knowref(b,whenD)) credulousI(a,ts(0)) pickForm(a,b,surface-request(a,b,assertref(b,a,whenD)), testref(a,b,whenD),ts(0))</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>