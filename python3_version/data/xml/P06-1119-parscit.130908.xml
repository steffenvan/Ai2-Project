<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003492">
<title confidence="0.9968105">
Leveraging Reusability: Cost-effective Lexical Acquisition
for Large-scale Ontology Translation
</title>
<author confidence="0.606864333333333">
G. Craig Murray
Bonnie J. Dorr
Jimmy Lin
</author>
<affiliation confidence="0.985539">
Institute for Advanced Computer Studies
University of Maryland
</affiliation>
<email confidence="0.999087">
{gcraigm,bdorr,jimmylin}@umd.edu
</email>
<author confidence="0.7074225">
Jan Hajic
Pavel Pecina
</author>
<affiliation confidence="0.611613">
Institute for Formal and Applied Linguistics
Charles University
</affiliation>
<email confidence="0.993222">
{hajic,pecina}@ufal.mff.cuni.cz
</email>
<sectionHeader confidence="0.995484" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999968636363636">
Thesauri and ontologies provide impor-
tant value in facilitating access to digital
archives by representing underlying prin-
ciples of organization. Translation of
such resources into multiple languages is
an important component for providing
multilingual access. However, the speci-
ficity of vocabulary terms in most on-
tologies precludes fully-automated ma-
chine translation using general-domain
lexical resources. In this paper, we pre-
sent an efficient process for leveraging
human translations when constructing
domain-specific lexical resources. We
evaluate the effectiveness of this process
by producing a probabilistic phrase dic-
tionary and translating a thesaurus of
56,000 concepts used to catalogue a large
archive of oral histories. Our experi-
ments demonstrate a cost-effective tech-
nique for accurate machine translation of
large ontologies.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999855843137256">
Multilingual access to digital collections is an
important problem in today’s increasingly inter-
connected world. Although technologies such as
cross-language information retrieval and ma-
chine translation help humans access information
they could not otherwise find or understand, they
are often inadequate for highly specific domains.
Most digital collections of any significant size
use a system of organization that facilitates easy
access to collection contents. Generally, the or-
ganizing principles are captured in the form of a
controlled vocabulary of keyword phrases (de-
scriptors) representing specific concepts. These
descriptors are usually arranged in a hierarchic
thesaurus or ontology, and are assigned to collec-
tion items as a means of providing access (either
via searching for keyword phases, browsing the
hierarchy, or a combination both). MeSH (Medi-
cal Subject Headings) serves as a good example
of such an ontology; it is a hierarchically-
arranged collection of controlled vocabulary
terms manually assigned to medical abstracts in a
number of databases. It provides multilingual
access to the contents of these databases, but
maintaining translations of such a complex struc-
ture is challenging (Nelson, et al, 2004).
For the most part, research in multilingual in-
formation access focuses on the content of digital
repositories themselves, often neglecting signifi-
cant knowledge that is explicitly encoded in the
associated ontologies. However, information
systems cannot utilize such ontologies by simply
applying off-the-shelf machine translation. Gen-
eral-purpose translation resources provide insuf-
ficient coverage of the vocabulary contained
within these domain-specific ontologies.
This paper tackles the question of how one
might efficiently translate a large-scale ontology
to facilitate multilingual information access. If
we need humans to assist in the translation proc-
ess, how can we maximize access while mini-
mizing cost? Because human translation is asso-
ciated with a certain cost, it is preferable not to
incur costs of retranslation whenever compo-
nents of translated text are reused. Moreover,
when exhaustive human translation is not practi-
cal, the most “useful” components should be
translated first. Identifying reusable elements
and prioritizing their translation based on utility
is essential to maximizing effectiveness and re-
ducing cost.
</bodyText>
<page confidence="0.981473">
945
</page>
<note confidence="0.533374">
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 945–952,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999985681818182">
We present a process of prioritized translation
that balances the issues discussed above. Our
work is situated in the context of the MALACH
project, an NSF-funded effort to improve multi-
lingual information access to large archives of
spoken language (Gustman, et al., 2002). Our
process leverages a small set of manually-
acquired English-Czech translations to translate a
large ontology of keyword phrases, thereby pro-
viding Czech speakers access to 116,000 hours
of video testimonies in 32 languages. Starting
from an initial out-of-vocabulary (OOV) rate of
85%, we show that a small set of prioritized
translations can be elicited from human infor-
mants, aligned, decomposed and then recom-
bined to cover 90% of the access value in a com-
plex ontology. Moreover, we demonstrate that
prioritization based on hierarchical position and
frequency of use facilitates extremely efficient
reuse of human input. Evaluations show that our
technique is able to boost performance of a sim-
ple translation system by 65%.
</bodyText>
<sectionHeader confidence="0.975856" genericHeader="method">
2 The Problem
</sectionHeader>
<bodyText confidence="0.999756288888889">
The USC Shoah Foundation Institute for Vis-
ual History and Education manages what is pres-
ently the world&apos;s largest archive of videotaped
oral histories (USC, 2006). The archive contains
116,000 hours of video from the testimonies of
over 52,000 survivors, liberators, rescuers and
witnesses of the Holocaust. If viewed end to
end, the collection amounts to 13 years of con-
tinuous video. The Shoah Foundation uses a hi-
erarchically arranged thesaurus of 56,000 key-
word phrases representing domain-specific con-
cepts. These are assigned to time-points in the
video testimonies as a means of indexing the
video content. Although the testimonies in the
collection represent 32 different languages, the
thesaurus used to catalog them is currently avail-
able only in English. Our task was to translate
this resource to facilitate multilingual access,
with Czech as the first target language.
Our first pass at automating thesaurus transla-
tion revealed that only 15% of the words in the
vocabulary could be found in an available
aligned corpus (tmejrek, et al., 2004). The rest
of the vocabulary was not available from general
resources. Lexical information for translating
these terms had to be acquired from human in-
put. Reliable access to digital archives requires
accuracy. Highly accurate human translations
incur a cost that is generally proportional to the
number of words being translated. However, the
keyword phrases in the Shoah Foundation’s ar-
chive occur in a Zipfian distribution—a rela-
tively small number of terms provide access to a
large portion of the video content. Similarly, a
great number of highly specific terms describe
only a small fraction of content. Therefore, not
every keyword phrase in the thesaurus carries the
same value for access to the archive. The hierar-
chical arrangement of keyword phrases presents
another issue: some concepts, while not of great
value for access to segments of video, may be
important for organizing other concepts and for
browsing the hierarchy. These factors must be
balanced in developing a cost-effective process
that maximizes utility.
</bodyText>
<sectionHeader confidence="0.993925" genericHeader="method">
3 Our Solution
</sectionHeader>
<bodyText confidence="0.999884727272727">
This paper presents a prioritized human-in-the-
loop approach to translating large-scale ontolo-
gies that is fast, efficient, and cost effective. Us-
ing this approach, we collected 3,000 manual
translations of keyword phrases and reused the
translated terms to generate a lexicon for auto-
mated translation of the rest of the thesaurus.
The process begins by prioritizing keyword
phrases for manual translation in terms of their
value in accessing the collection and the reus-
ability of their component terms. Translations
collected from one human informant are then
checked and aligned to the original English terms
by a second informant. From these alignments
we induce a probabilistic English-Czech phrase
dictionary.
To test the effectiveness of this process we
implemented a simple translation system that
utilizes the newly generated lexical resources.
Section 4 reports on two evaluations of the trans-
lation output that quantify the effectiveness of
our human-in-the-loop approach.
</bodyText>
<subsectionHeader confidence="0.999984">
3.1 Maximizing Value and Reusability
</subsectionHeader>
<bodyText confidence="0.9998528">
To quantify their utility, we defined two values
for each keyword phrase in the thesaurus: a the-
saurus value, representing the importance of the
keyword phrase for providing access to the col-
lection, and a translation value, representing the
usefulness of having the keyword phrase trans-
lated. These values are not identical, but the
second is related to the first.
Thesaurus value: Keyword phrases in the
Shoah Foundation’s thesaurus are arranged into a
poly-hierarchy in which child nodes may have
multiple parents. Internal (non-leaf) nodes of the
hierarchy are used to organize concepts and sup-
port concept browsing. Some internal nodes are
also used to index video content. Leaf nodes are
</bodyText>
<page confidence="0.998056">
946
</page>
<bodyText confidence="0.999967594594594">
very specific and are only used to index video
content. Thus, the usefulness of any keyword
phrase for providing access to the digital collec-
tion is directly related to the concept’s position in
the thesaurus hierarchy.
A fragment of the hierarchy is shown in Fig-
ure 1. The keyword phrase “Auschwitz II-
Birkenau (Poland: Death Camp)”, which de-
scribes a Nazi death camp, is assigned to 17,555
video segments in the collection. It has broader
(parent) terms and narrower (child) terms. Some
of the broader and narrower terms are also as-
signed to segments, but not all. Notably, “Ger-
man death camps” is not assigned to any video
segments. However, “German death camps” has
very important narrower terms including
“Auschwitz II-Birkenau” and others.
From this example, we can see that an internal
node is valuable in providing access to its chil-
dren, even if the keyword phrase itself is not as-
signed to any segments. The value we assign to
any term must reflect this fact. If we were to
reduce cost by translating only the nodes as-
signed to video segments, we would neglect
nodes that are crucial for browsing. However, if
we value a node by the sum value of all its chil-
dren, grandchildren, etc., the resulting calcula-
tion would bias the top of the hierarchy. Any
prioritization based on this method would lead to
translation of the top of the hierarchy first.
Given limited resources, leaf nodes might never
be translated. Support for searching and brows-
ing calls for different approaches to prioritization.
To strike a balance between these factors, we
calculate a thesaurus value, which represents the
importance of each keyword phrase to the the-
saurus as a whole. This value is computed as:
</bodyText>
<figure confidence="0.892183117647059">
�icchildren(k) hi
hk = count(sk)+
Auschwitz II-Birkenau (Poland : Death Camp)
Assigned to 17555 video segments
Has as broader term phrases:
Cracow (Poland : Voivodship)
[ 534 narrower terms] [ 204 segments]
German death camps
[ 6 narrower terms] [ 0 segments]
Has seven narrower term phrases including:
Block 25 (Auschwitz II-Birkenau)
[leaf node] [ 35 segments]
Kanada (Auschwitz II-Birkenau)
[leaf node] [ 378 segments]
...
disinfection chamber (Auschwitz II-Birkenau)
[leaf node] [ 9 segments]
</figure>
<figureCaption confidence="0.999980333333333">
Figure 1. Sample keyword phrase
with broader and narrower terms
Figure 2. Bottom-up micro-averaging
</figureCaption>
<bodyText confidence="0.99983824">
For example, in Figure 2 each of the leaf
nodes n3, n4, and n5 have values based solely on
the number of segments to which they are as-
signed. Node n1 has value both as an access point
to the segments at s2 and as an access point to the
keyword phrases at nodes n3 and n4. Other inter-
nal nodes, such as n2 have value only in provid-
ing access to other nodes/keyword phrases.
Working from the bottom of the hierarchy up to
the primary node (n0) we can compute the the-
saurus value for each node in the hierarchy. In
our example, we start with nodes n3 through n5,
counting the number of the segments that have
been assigned each keyword phrase. Then we
move up to nodes n1 and n2. At n1 we count the
number of segments s2 to which n1 was assigned
and add that count to the average of the thesau-
rus values for n3, and n4. At n2 we simply aver-
age the thesaurus values for n4 and n5. The final
values quantify how valuable the translation of
any given keyword phrase would be in providing
access to video segments.
Translation value: After obtaining the the-
saurus value for each node, we can compute the
translation value for each word in the vocabulary
</bodyText>
<figure confidence="0.97662825">
keyword
phrases
s1
n3
segments
primary
keyword
s2
n1
n0
n4
s3
n2
n5
s4
children(k)
</figure>
<bodyText confidence="0.982896428571429">
For leaf nodes in our thesaurus, this value is sim-
ply the number of video segments to which the
concept has been assigned. For parent nodes, the
thesaurus value is the number of segments (if
any) to which the node has been assigned, plus
the average of the thesaurus value of any child
nodes.
This recursive calculation yields a micro-
averaged value that represents the reachability of
segments via downward edge traversals from a
given node in the hierarchy. That is, it gives a
kind of weighted value for the number of seg-
ments described by a given keyword phrase or its
narrower-term keyword phrases.
</bodyText>
<page confidence="0.990286">
947
</page>
<bodyText confidence="0.999994365384615">
as the sum of the thesaurus value for every key-
word phrase that contains that word:
For example, the word “Auschwitz” occurs in 35
concepts. As a candidate for translation, it car-
ries a large impact, both in terms of the number
of keyword phrases that contains this word, and
the potential value of those keyword phrases
(once they are translated) in providing access to
segments in the archive. The end result is a list
of vocabulary words and the impact that correct
translation of each word would have on the over-
all value of the translated thesaurus.
We elicited human translations of entire key-
word phrases rather than individual vocabulary
terms. Having humans translate individual
words without their surrounding context would
have been less efficient. Also, the value any
keyword phrase holds for translation is only indi-
rectly related to its own value as a point of access
to the collection (i.e., its thesaurus value). Some
keyword phrases contain words with high trans-
lation value, but the keyword phrase itself has
low thesaurus value. Thus, the value gained by
translating any given phrase is more accurately
estimated by the total value of any untranslated
words it contains. Therefore, we prioritized the
order of keyword phrase translations based on
the translation value of the untranslated words in
each keyword phrase.
Our next step was to iterate through the the-
saurus keyword phrases, prioritizing their trans-
lation based on the assumption that any words
contained in a keyword phrase of higher priority
would already have been translated. Starting
from the assumption that the entire thesaurus is
untranslated, we select the one keyword phrase
that contains the most valuable un-translated
words—we simply add up the translation value
of all the untranslated words in each keyword
phrase, and select the keyword phrase with the
highest value. We add this keyword phrase to a
prioritized list of items to be manually translated
and we remove it from the list of untranslated
phrases. We update our vocabulary list and, as-
suming translations of all the words in the prior
keyword phrase to now be translated (neglecting
issues such as morphology), we again select the
keyword phrase that contains the most valuable
untranslated words. We iterate the process until
all vocabulary terms have been included at least
one keyword phrases on the prioritized list. Ul-
timately we end up with an ordered list of the
</bodyText>
<figureCaption confidence="0.8327425">
Figure 3. Gain rate of access value based on
number of human translations
</figureCaption>
<bodyText confidence="0.999921846153846">
keyword phrases that should be translated to
cover the entire vocabulary, with the most impor-
tant words being covered first.
A few words about additional characteristics
of this approach: note that it is greedy and biased
toward longer keyword phrases. As a result,
some words may be translated more than once
because they appear in more than one keyword
phrase with high translation value. This side
effect is actually desirable. To build an accurate
translation dictionary, it is helpful to have more
than one translation of frequently occuring words,
especially for morphologically rich languages
such as Czech. Our technique makes the opera-
tional assumption that translations of a word
gathered in one context can be reused in another
context. Obviously this is not always true, but
contexts of use are relatively stable in controlled
vocabularies. Our evaluations address the ac-
ceptability of this operational assumption and
demonstrate that the technique yields acceptable
translations.
Following this process model, the most impor-
tant elements of the thesaurus will be translated
first, and the most important vocabulary terms
will quickly become available for automated
translation of keyword phrases with high thesau-
rus value that do not make it onto the prioritized
list for manual translation (i.e., low translation
value). The overall access value of the thesaurus
rises very quickly after initial translations. With
each subsequent human translation of keyword
phrases on the prioritized list, we gain tremen-
dous value in terms of providing non-English
access to the collection of video testimonies.
Figure 3 shows this rate of gain. It can be seen
that prioritization based on translation value
gives a much higher yield of total access than
prioritization based on thesaurus value.
</bodyText>
<figure confidence="0.992050285714286">
tw= �KÎ w
k
hk where Kw={x  |phrase x contains w}
percent of total access value
100%
80%
40%
20%
60%
0%
0 500 1000 1500 2000
number of translations
priority by thesaurus value priority by translation value
Gain rate of prioritized translation schemes
</figure>
<page confidence="0.919199">
948
</page>
<subsectionHeader confidence="0.998695">
3.2 Alignment and Decomposition
</subsectionHeader>
<bodyText confidence="0.999337384615385">
Following the prioritization scheme above, we
obtained professional translations for the top
3000 English keyword phrases. We tokenized
these translations and presented them to another
bilingual Czech speaker for verification and
alignment. This second informant marked each
Czech word in a translated keyword phrase with
a link to the equivalent English word(s). Multi-
ple links were used to convey the relationship
between a single word in one language and a
string of words in another. The output of the
alignment process was then used to build a prob-
abilistic dictionary of words and phrases.
</bodyText>
<figureCaption confidence="0.988543">
Figure 4. Sample alignment
</figureCaption>
<bodyText confidence="0.998922125">
Figure 4 shows an example of an aligned
tranlsation. The word “stills” is recorded as a
translation for “statické snímky” and “kláštery”
is recorded as a translation for “convents and
monasteries.” We count the number of occur-
rences of each alignment in all of the translations
and calculate probabilities for each Czech word
or phrase given an English word or phrase. For
example, in the top 3000 keyword phrases
“stills” appears 29 times. It was aligned with
“statické snímky” 28 times and only once with
“statické zábery”, giving us a translation prob-
ability of 28/29=0.9655 for “statické snímky”.
Human translation of the 3000 English key-
word phrases into Czech took approximately 70
hours, and the alignments took 55 hours. The
overall cost of human input (translation and
alignment) was less than 1000 €. The projected
cost of full translation for the entire thesaurus
would have been close to 20000 € and would not
have produced any reusable resources. Naturally,
costs for building resources in this manner will
vary, but in our case the cost savings is approxi-
mately twenty fold.
</bodyText>
<subsectionHeader confidence="0.995878">
3.3 Machine Translation
</subsectionHeader>
<bodyText confidence="0.999970375">
To demonstrate the effectiveness of our approach,
we show that a probabilistic dictionary, induced
through the process we just described, facilitates
high quality machine translation of the rest of the
thesaurus. We evaluated translation quality us-
ing a relatively simple translation system. How-
ever, more sophisticated systems can draw equal
benefit from the same lexical resources.
Our translation system implemented a greedy
coverage algorithm with a simple back-off strat-
egy. It first scans the English input to find the
longest matching substring in our dictionary, and
replaces it with the most likely Czech translation.
Building on the example above, the system looks
up “monasteries and convents stills” in the dic-
tionary, finds no translation, and backs off to
“monasteries and convents”, which is translated
to “kláštery”. Had this phrase translation not
been found, the system would have attempted to
find a match for the individual tokens. Failing a
match in our dictionary, the system then backs
off to the Prague Czech-English Dependency
Treebank dictionary, a much larger dictionary
with broader scope. If no match is found in ei-
ther dictionary for the full token, we stem the
token and look for matches based on the stem.
Finally, tokens whose translations can not be
found are simply passed through untranslated.
A minimal set of heuristic rules was applied to
reordering the Czech tokens but the output is
primarily phrase by phrase/word by word transla-
tion. Our evaluation scores below will partially
reflect the simplicity of our system. Our system
is simple by design. Any improvement or degra-
dation to the input of our system has direct influ-
ence on the output. Thus, measures of transla-
tion accuracy for our system can be directly in-
terpreted as quality measures for the lexical re-
sources used and the process by which they were
developed.
</bodyText>
<sectionHeader confidence="0.998872" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.99717865">
We performed two different types of evaluation
to validate our process. First, we compared our
system output to human reference translations
using Bleu (Papineni, et al., 2002), a widely-
accepted objective metric for evaluation of ma-
chine translations. Second, we showed corrected
and uncorrected machine translations to Czech
speakers and collected subjective judgments of
fluency and accuracy.
For evaluation purposes, we selected 418
keyword phrases to be used as target translations.
These phrases were selected using a stratified
sampling technique so that different levels of
thesaurus value would be represented. There
was no overlap between these keyword phrases
and the 3000 prioritized keyword phrases used to
build our lexicon. Prior to machine translation
we obtained at least two independent human-
generated reference translations for each of the
418 keyword phrases.
</bodyText>
<equation confidence="0.432287">
convents and monasteries ( stills )
kláštery ( statické snímky )
</equation>
<page confidence="0.993689">
949
</page>
<bodyText confidence="0.999975436363637">
After collecting the first 2500 prioritized
translations, we induced a probabilistic diction-
ary and generated machine translations of the
418 target keyword phrases. These were then
corrected by native Czech speakers, who ad-
justed word order, word choice, and morphology.
We use this set of human-corrected machine
translations as a second reference for evaluation.
Measuring the difference between our uncor-
rected machine translations (MT) and the human-
generated reference establishes how accurate our
translations are compared to an independently
established target. Measuring the difference be-
tween our MT and the human-corrected machine
translations (corrected MT) establishes how ac-
ceptable our translations are. We also measured
the difference between corrected MT and the
human-generated translations. We take this to be
an upper bound on realistic system performance.
The results from our objective evaluation are
shown in Figure 5. Each set of bars in the graph
shows performance after adding a different num-
ber of aligned translations into the lexicon (i.e.,
performance after adding 500, 1000, ..., 3000
aligned translations.) The zero condition is our
baseline: translations generated using only the
dictionary available in the Prague Czech-English
Dependency Treebank. Three different reference
sets are shown: human-generated, corrected MT,
and a combination of the two.
There is a notable jump in Bleu score after the
very first translations are added into our prob-
abilistic dictionary. Without any elicitation and
alignment we got a baseline score of 0.46
(against the human-generated reference transla-
tions). After the aligned terms from only 500
translations were added to our dictionary, our
Bleu score rose to 0.66. After aligned terms
from 3000 translations were added, we achieved
0.69. Using corrected MT as the reference our
Bleu scores improve from 0.48 to 0.79. If hu-
man-generated and human-corrected references
are both considered to be correct translations, the
improvement goes from .49 to .80. Regardless
of the reference set, there is a consistent per-
formance improvement as more and more trans-
lations are added. We found the same trend us-
ing the TER metric on a smaller data set
(Murray, et al., 2006). The fact that the Bleu
scores continue to rise indicates that our ap-
proach is successful in quickly expanding the
lexicon with accurate translations. It is important
to point out that Bleu scores are not meaningful
in an absolute sense; the scores here should be
interpreted with respect to each other. The trend
</bodyText>
<figureCaption confidence="0.999344">
Figure 5. Objective evaluation results
Figure 6. Subjective evaluation results
</figureCaption>
<bodyText confidence="0.999947807692308">
in scores strongly indicates that our prioritization
scheme is effective for generating a high-quality
translation lexicon at relatively low cost.
To determine an upper bound on machine per-
formance, we compared our corrected MT output
to the initial human-generated reference transla-
tions, which were collected prior to machine
translation. Corrected MT achieved a Bleu score
of 0.82 when compared to the human-generated
reference translations. This upper bound is the
“limit” indicated in Figure 5.
To determine the impact of external resources,
we removed the Prague Czech-English Depend-
ency Treebank dictionary as a back-off resource
and retranslated keyword phrases using only the
lexicons induced from our aligned translations.
The results of this experiment showed only mar-
ginal degradation of the output. Even when as
few as 500 aligned translations were used for our
dictionary, we still achieved a Bleu score of 0.65
against the human reference translations. This
means that even for languages where prior re-
sources are not available our prioritization
scheme successfully addresses the OOV problem.
In our subjective evaluation, we presented a
random sample of our system output to seven
</bodyText>
<figure confidence="0.992741625">
corrected human reference both limit
Bleu-4
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
Bleu Scores After Increasing Translations
0 500 1000 1500 2000 2500 3000
Number of Translations
100%
Percent of scores
80%
60%
40%
20%
0%
MT
Corrected MT
Judgment scores
Distribution of Subjective Judgment Scores
1 2
3 4
5 1
2 3
4 5
1 2
3 4
5 1
2 3
4 5
fluency
accuracy
fluency
accuracy
</figure>
<page confidence="0.991382">
950
</page>
<bodyText confidence="0.999913307692308">
native Czech speakers and collected judgments
of accuracy and fluency using a 5-point Likert
scale (1=good, 3=neutral, 5=bad). An overview
of the results is presented in Figure 6. Scores are
shown for corrected and uncorrected MT. In all
cases, the mode is 1 (i.e., good fluency and good
accuracy). 59% of the machine translated
phrases were rated 2 or better for fluency. 66%
were rated 2 or better for accuracy. Only a small
percentage of the translations had meanings that
were far from the intended meaning. Disfluen-
cies were primarily due to errors in morphology
and word order.
</bodyText>
<sectionHeader confidence="0.999964" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999810923076923">
Several studies have taken a knowledge-
acquisition approach to collecting multilingual
word pairs. For example, Sadat et al. (2003)
automatically extracted bilingual word pairs
from comparable corpora. This approach is
based on the simple assumption that if two words
are mutual translations, then their most frequent
collocates are likely to be mutual translations as
well. However, the approach requires large com-
parable corpora, the collection of which presents
non-trivial challenges. Others have made similar
mutual-translation assumptions for lexical acqui-
sition (Echizen-ya, et al., 2005; Kaji &amp; Aizono,
1996; Rapp, 1999; Tanaka &amp; Iwasaki, 1996).
Most make use of either parallel corpora or a
bilingual dictionary for the task of bilingual term
extraction. Echizen-ya, et al. (2005) avoided
using a bilingual dictionary, but required a paral-
lel corpus to achieve their goal; whereas Fung
(2000) and others have relied on pre-existing
bilingual dictionaries. In either case, large bilin-
gual resources of some kind are required. In ad-
dition, these approaches focused on the extrac-
tion of single-word pairs, not phrasal units.
Many recent approaches to dictionary and the-
saurus translation are geared toward providing
domain-specific thesauri to specialists in a par-
ticular field, e.g., medical terminology (Déjean,
et al., 2005) and agricultural terminology (Chun
&amp; Wenlin, 2002). Researchers on these projects
are faced with either finding human translators
who are specialized enough to manage the do-
main-particular translations—or applying auto-
matic techniques to large-scale parallel corpora
where data sparsity poses a problem for low-
frequency terms. Data sparsity is also an issue
for more general state-of-the-art bilingual align-
ment approaches (Brown, et al., 2000; Och &amp;
Ney, 2003; Wantanabe &amp; Sumita, 2003).
</bodyText>
<sectionHeader confidence="0.99861" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999929240740741">
The task of translating large ontologies can be
recast as a problem of implementing fast and ef-
ficient processes for acquiring task-specific lexi-
cal resources. We developed a method for pri-
oritizing keyword phrases from an English the-
saurus of concepts and elicited Czech transla-
tions for a subset of the keyword phrases. From
these, we decomposed phrase elements for reuse
in an English-Czech probabilistic dictionary. We
then applied the dictionary in machine translation
of the rest of the thesaurus.
Our results show an overall improvement in
machine translation quality after collecting only
a few hundred human translations. Translation
quality continued to rise as more and more hu-
man translations were added. The test data used
in our evaluations are small relative to the overall
task. However, we fully expect these results to
hold across larger samples and for more sophisti-
cated translation systems.
We leveraged the reusability of translated
words to translate a thesaurus of 56,000 keyword
phrases using information gathered from only
3000 manual translations. Our probabilistic dic-
tionary was acquired at a fraction of the cost of
manually translating the entire thesaurus. By
prioritizing human translations based on the
translation value of the words and the thesaurus
value of the keyword phrases in which they ap-
pear, we optimized the rate of return on invest-
ment. This allowed us to choose a trade-off point
between cost and utility. For this project we
chose to stop human translation at a point where
less than 0.01% of the value of the thesaurus
would be gained from each additional human
translation. This choice produced a high-quality
lexicon with significant positive impact on ma-
chine translation systems. For other applications,
a different trade-off point will be appropriate,
depending on the initial OOV rate and the impor-
tance of detailed coverage.
The value of our work lies in the process
model we developed for cost-effective elicitation
of lexical resources. The metrics we established
for assessing the impact of each translation item
are key to our approach. We use these to opti-
mize the value gained from each human transla-
tion. In our case the items were keyword phrases
arranged in a hierarchical thesaurus that de-
scribes an ontology of concepts. The operational
value of these keyword phrases was determined
by the access they provide to video segments in a
large archive of oral histories. However, our
technique is not limited to this application.
</bodyText>
<page confidence="0.993689">
951
</page>
<bodyText confidence="0.99998175">
We have shown that careful prioritization of
elicited human translations facilitates cost-
effective thesaurus translation with minimal hu-
man input. Our use of a prioritization scheme
addresses the most important deficiencies in the
vocabulary first. We induced a framework
where the utility of lexical resources gained from
each additional human translation becomes
smaller and smaller. Under such a framework,
choosing the number of human translation to
elicit becomes merely a function of the financial
resources available for the task.
</bodyText>
<sectionHeader confidence="0.998285" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.995806363636364">
Our thanks to Doug Oard for his contribution to
this work. Thanks also to our Czech informants:
Robert Fischmann, Eliska Kozakova, Alena
Prunerova and Martin Smok; and to Soumya
Bhat for her programming efforts.
This work was supported in part by NSF IIS
Award 0122466 and NSF CISE RI Award
EIA0130422. Additional support also came
from grants of the MSMT CR #1P05ME786 and
#MSM0021620838, and the Grant Agency of the
CR #GA405/06/0589.
</bodyText>
<sectionHeader confidence="0.998949" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998215375">
Brown, P. F., Della-Pietra, V. J., Della-Pietra, S. A.,
&amp; Mercer, R. L. (1993). The mathematics of statis-
tical machine translation: Parameter estimation.
Computational Linguistics, 19(2), 263-311.
Chun, C., &amp; Wenlin, L. (2002). The translation of
agricultural multilingual thesaurus. In Proceedings
of the Third Asian Conference for Information
Technology in Agriculture. Beijing, China: Chinese
Academy of Agricultural Sciences (CAAS) and
Asian Federation for Information Technology in
Agriculture (AFITA).
tmejrek, M., Curin, J., Havelka, J., Hajic, J., &amp; Ku-
bon, V. (2004). Prague Czech-English dependecy
treebank: Syntactically annotated resources for ma-
chine translation. In 4th International Conference
on Language Resources and Evaluation Lisbon,
Portugal.
Déjean, H., Gaussier, E., Renders, J.-M., &amp; Sadat, F.
(2005). Automatic processing of multilingual
medical terminology: Applications to thesaurus en-
richment and cross-language information retrieval.
Artificial Intelligence in Medicine, 33(2 ), 111-124.
Echizen-ya, H., Araki, K., &amp; Momouchi, Y. (2005).
Automatic acquisition of bilingual rules for extrac-
tion of bilingual word pairs from parallel corpora.
In Proceedings of the ACL-SIGLEX Workshop on
Deep Lexical Acquisition (pp. 87-96).
Fung, P. (2000). A statistical view of bilingual lexicon
extraction: From parallel corpora to non-parallel
corpora. In Jean Veronis (ed.), Parallel Text Proc-
essing. Dordrecht: Kluwer Academic Publishers.
Gustman, Soergel, Oard, Byrne, Picheny, Ramabhad-
ran, &amp; Greenberg. (2002). Supporting access to
large digital oral history archives. In Proceedings
of the Joint Conference on Digital Libraries. Port-
land, Oregon. (pp. 18-27).
Kaji, H., &amp; Aizono, T. (1996). Extracting word corre-
spondences from bilingual corpora based on word
co-occurrence information. In Proceedings of
COLING &apos;96 (pp. 23-28).
Murray, G. C., Dorr, B., Lin, J., Hajic, J., &amp; Pecina, P.
(2006). Leveraging recurrent phrase structure in
large-scale ontology translation. In Proceedings of
the 11th Annual Conference of the European Asso-
ciation for Machine Translation. Oslo, Norway.
Nelson, S. J., Schopen, M., Savage, A. G., Schulman,
J.-L., &amp; Arluk, N. (2004). The MeSH translation
maintenance system: Structure, interface design,
and implementation. In Proceedings of the 11th
World Congress on Medical Informatics. (pp. 67-
69). Amsterdam: IOS Press.
Och, F. J., &amp; Ney, H. (2003). A systematic compari-
son of various statistical alignment models. Com-
putational Linguistics, 29(1), 19-51.
Papineni, K., Roukos, S., Ward, T., &amp; Zhu, W.-J.
(2002). BLEU: A method for automatic evaluation
of machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computa-
tional Linguistics (pp. 331-318).
Rapp, R. (1999). Automatic identification of word
translations from unrelated English and German
corpora. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguis-
tics. (pp. 519-526).
Sadat, F., Yoshikawa, M., &amp; Uemura, S. (2003). En-
hancing cross-language information retrieval by an
automatic acquisition of bilingual terminology
from comparable corpora . In Proceedings of the
26th Annual International ACM SIGIR Conference
on Research and Development in Information Re-
trieval (pp. 397-398).
Tanaka, K., &amp; Iwasaki, H. (1996). Extraction of lexi-
cal translations from non-aligned corpora. In Pro-
ceedings of COLING &apos;96. (pp. 580-585).
USC. (2006) USC Shoah Foundation Institute for
Visual History and Education, [online]
http://www.usc.edu/schools/college/vhi
Wantanabe, T., &amp; Sumita, E. (2003). Example-based
decoding for statistical machine translation. In Pro-
ceedings of MT Summit IX (pp. 410-417).
</reference>
<page confidence="0.99757">
952
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.910993">
<title confidence="0.9999">Leveraging Reusability: Cost-effective Lexical Acquisition for Large-scale Ontology Translation</title>
<author confidence="0.995045">G Craig Murray Bonnie J Dorr Jimmy Lin</author>
<affiliation confidence="0.9993475">Institute for Advanced Computer Studies University of Maryland</affiliation>
<email confidence="0.999587">gcraigm@umd.edu</email>
<email confidence="0.999587">bdorr@umd.edu</email>
<email confidence="0.999587">jimmylin@umd.edu</email>
<author confidence="0.999572">Pavel Pecina</author>
<affiliation confidence="0.997369">Institute for Formal and Applied Linguistics Charles University</affiliation>
<email confidence="0.937418">hajic@ufal.mff.cuni.cz</email>
<email confidence="0.937418">pecina@ufal.mff.cuni.cz</email>
<abstract confidence="0.999725739130435">Thesauri and ontologies provide important value in facilitating access to digital archives by representing underlying principles of organization. Translation of such resources into multiple languages is an important component for providing multilingual access. However, the specificity of vocabulary terms in most ontologies precludes fully-automated machine translation using general-domain lexical resources. In this paper, we present an efficient process for leveraging human translations when constructing domain-specific lexical resources. We evaluate the effectiveness of this process by producing a probabilistic phrase dictionary and translating a thesaurus of 56,000 concepts used to catalogue a large archive of oral histories. Our experiments demonstrate a cost-effective technique for accurate machine translation of large ontologies.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>V J Della-Pietra</author>
<author>S A Della-Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>263--311</pages>
<marker>Brown, Della-Pietra, Della-Pietra, Mercer, 1993</marker>
<rawString>Brown, P. F., Della-Pietra, V. J., Della-Pietra, S. A., &amp; Mercer, R. L. (1993). The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2), 263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chun</author>
<author>L Wenlin</author>
</authors>
<title>The translation of agricultural multilingual thesaurus.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third Asian Conference for Information Technology in Agriculture. Beijing, China: Chinese Academy of Agricultural Sciences (CAAS) and Asian Federation for Information Technology in Agriculture (AFITA).</booktitle>
<contexts>
<context position="28092" citStr="Chun &amp; Wenlin, 2002" startWordPosition="4440" endWordPosition="4443">Echizen-ya, et al. (2005) avoided using a bilingual dictionary, but required a parallel corpus to achieve their goal; whereas Fung (2000) and others have relied on pre-existing bilingual dictionaries. In either case, large bilingual resources of some kind are required. In addition, these approaches focused on the extraction of single-word pairs, not phrasal units. Many recent approaches to dictionary and thesaurus translation are geared toward providing domain-specific thesauri to specialists in a particular field, e.g., medical terminology (Déjean, et al., 2005) and agricultural terminology (Chun &amp; Wenlin, 2002). Researchers on these projects are faced with either finding human translators who are specialized enough to manage the domain-particular translations—or applying automatic techniques to large-scale parallel corpora where data sparsity poses a problem for lowfrequency terms. Data sparsity is also an issue for more general state-of-the-art bilingual alignment approaches (Brown, et al., 2000; Och &amp; Ney, 2003; Wantanabe &amp; Sumita, 2003). 6 Conclusion The task of translating large ontologies can be recast as a problem of implementing fast and efficient processes for acquiring task-specific lexical</context>
</contexts>
<marker>Chun, Wenlin, 2002</marker>
<rawString>Chun, C., &amp; Wenlin, L. (2002). The translation of agricultural multilingual thesaurus. In Proceedings of the Third Asian Conference for Information Technology in Agriculture. Beijing, China: Chinese Academy of Agricultural Sciences (CAAS) and Asian Federation for Information Technology in Agriculture (AFITA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M tmejrek</author>
<author>J Curin</author>
<author>J Havelka</author>
<author>J Hajic</author>
<author>V Kubon</author>
</authors>
<title>Prague Czech-English dependecy treebank: Syntactically annotated resources for machine translation.</title>
<date>2004</date>
<booktitle>In 4th International Conference on Language Resources and Evaluation</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="5877" citStr="tmejrek, et al., 2004" startWordPosition="856" endWordPosition="859">ged thesaurus of 56,000 keyword phrases representing domain-specific concepts. These are assigned to time-points in the video testimonies as a means of indexing the video content. Although the testimonies in the collection represent 32 different languages, the thesaurus used to catalog them is currently available only in English. Our task was to translate this resource to facilitate multilingual access, with Czech as the first target language. Our first pass at automating thesaurus translation revealed that only 15% of the words in the vocabulary could be found in an available aligned corpus (tmejrek, et al., 2004). The rest of the vocabulary was not available from general resources. Lexical information for translating these terms had to be acquired from human input. Reliable access to digital archives requires accuracy. Highly accurate human translations incur a cost that is generally proportional to the number of words being translated. However, the keyword phrases in the Shoah Foundation’s archive occur in a Zipfian distribution—a relatively small number of terms provide access to a large portion of the video content. Similarly, a great number of highly specific terms describe only a small fraction o</context>
</contexts>
<marker>tmejrek, Curin, Havelka, Hajic, Kubon, 2004</marker>
<rawString>tmejrek, M., Curin, J., Havelka, J., Hajic, J., &amp; Kubon, V. (2004). Prague Czech-English dependecy treebank: Syntactically annotated resources for machine translation. In 4th International Conference on Language Resources and Evaluation Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Déjean</author>
<author>E Gaussier</author>
<author>J-M Renders</author>
<author>F Sadat</author>
</authors>
<title>Automatic processing of multilingual medical terminology: Applications to thesaurus enrichment and cross-language information retrieval.</title>
<date>2005</date>
<journal>Artificial Intelligence in Medicine,</journal>
<volume>33</volume>
<issue>2</issue>
<pages>111--124</pages>
<contexts>
<context position="28041" citStr="Déjean, et al., 2005" startWordPosition="4433" endWordPosition="4436">ctionary for the task of bilingual term extraction. Echizen-ya, et al. (2005) avoided using a bilingual dictionary, but required a parallel corpus to achieve their goal; whereas Fung (2000) and others have relied on pre-existing bilingual dictionaries. In either case, large bilingual resources of some kind are required. In addition, these approaches focused on the extraction of single-word pairs, not phrasal units. Many recent approaches to dictionary and thesaurus translation are geared toward providing domain-specific thesauri to specialists in a particular field, e.g., medical terminology (Déjean, et al., 2005) and agricultural terminology (Chun &amp; Wenlin, 2002). Researchers on these projects are faced with either finding human translators who are specialized enough to manage the domain-particular translations—or applying automatic techniques to large-scale parallel corpora where data sparsity poses a problem for lowfrequency terms. Data sparsity is also an issue for more general state-of-the-art bilingual alignment approaches (Brown, et al., 2000; Och &amp; Ney, 2003; Wantanabe &amp; Sumita, 2003). 6 Conclusion The task of translating large ontologies can be recast as a problem of implementing fast and effi</context>
</contexts>
<marker>Déjean, Gaussier, Renders, Sadat, 2005</marker>
<rawString>Déjean, H., Gaussier, E., Renders, J.-M., &amp; Sadat, F. (2005). Automatic processing of multilingual medical terminology: Applications to thesaurus enrichment and cross-language information retrieval. Artificial Intelligence in Medicine, 33(2 ), 111-124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Echizen-ya</author>
<author>K Araki</author>
<author>Y Momouchi</author>
</authors>
<title>Automatic acquisition of bilingual rules for extraction of bilingual word pairs from parallel corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL-SIGLEX Workshop on Deep Lexical Acquisition</booktitle>
<pages>87--96</pages>
<contexts>
<context position="27302" citStr="Echizen-ya, et al., 2005" startWordPosition="4318" endWordPosition="4321">and word order. 5 Related Work Several studies have taken a knowledgeacquisition approach to collecting multilingual word pairs. For example, Sadat et al. (2003) automatically extracted bilingual word pairs from comparable corpora. This approach is based on the simple assumption that if two words are mutual translations, then their most frequent collocates are likely to be mutual translations as well. However, the approach requires large comparable corpora, the collection of which presents non-trivial challenges. Others have made similar mutual-translation assumptions for lexical acquisition (Echizen-ya, et al., 2005; Kaji &amp; Aizono, 1996; Rapp, 1999; Tanaka &amp; Iwasaki, 1996). Most make use of either parallel corpora or a bilingual dictionary for the task of bilingual term extraction. Echizen-ya, et al. (2005) avoided using a bilingual dictionary, but required a parallel corpus to achieve their goal; whereas Fung (2000) and others have relied on pre-existing bilingual dictionaries. In either case, large bilingual resources of some kind are required. In addition, these approaches focused on the extraction of single-word pairs, not phrasal units. Many recent approaches to dictionary and thesaurus translation </context>
</contexts>
<marker>Echizen-ya, Araki, Momouchi, 2005</marker>
<rawString>Echizen-ya, H., Araki, K., &amp; Momouchi, Y. (2005). Automatic acquisition of bilingual rules for extraction of bilingual word pairs from parallel corpora. In Proceedings of the ACL-SIGLEX Workshop on Deep Lexical Acquisition (pp. 87-96).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Fung</author>
</authors>
<title>A statistical view of bilingual lexicon extraction: From parallel corpora to non-parallel corpora.</title>
<date>2000</date>
<booktitle>Parallel Text Processing.</booktitle>
<editor>In Jean Veronis (ed.),</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<location>Dordrecht:</location>
<contexts>
<context position="27609" citStr="Fung (2000)" startWordPosition="4370" endWordPosition="4371">s, then their most frequent collocates are likely to be mutual translations as well. However, the approach requires large comparable corpora, the collection of which presents non-trivial challenges. Others have made similar mutual-translation assumptions for lexical acquisition (Echizen-ya, et al., 2005; Kaji &amp; Aizono, 1996; Rapp, 1999; Tanaka &amp; Iwasaki, 1996). Most make use of either parallel corpora or a bilingual dictionary for the task of bilingual term extraction. Echizen-ya, et al. (2005) avoided using a bilingual dictionary, but required a parallel corpus to achieve their goal; whereas Fung (2000) and others have relied on pre-existing bilingual dictionaries. In either case, large bilingual resources of some kind are required. In addition, these approaches focused on the extraction of single-word pairs, not phrasal units. Many recent approaches to dictionary and thesaurus translation are geared toward providing domain-specific thesauri to specialists in a particular field, e.g., medical terminology (Déjean, et al., 2005) and agricultural terminology (Chun &amp; Wenlin, 2002). Researchers on these projects are faced with either finding human translators who are specialized enough to manage </context>
</contexts>
<marker>Fung, 2000</marker>
<rawString>Fung, P. (2000). A statistical view of bilingual lexicon extraction: From parallel corpora to non-parallel corpora. In Jean Veronis (ed.), Parallel Text Processing. Dordrecht: Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soergel Gustman</author>
<author>Byrne Oard</author>
<author>Ramabhadran Picheny</author>
<author>Greenberg</author>
</authors>
<title>Supporting access to large digital oral history archives.</title>
<date>2002</date>
<booktitle>In Proceedings of the Joint Conference on Digital Libraries.</booktitle>
<pages>18--27</pages>
<location>Portland, Oregon.</location>
<contexts>
<context position="4074" citStr="Gustman, et al., 2002" startWordPosition="570" endWordPosition="573">irst. Identifying reusable elements and prioritizing their translation based on utility is essential to maximizing effectiveness and reducing cost. 945 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 945–952, Sydney, July 2006. c�2006 Association for Computational Linguistics We present a process of prioritized translation that balances the issues discussed above. Our work is situated in the context of the MALACH project, an NSF-funded effort to improve multilingual information access to large archives of spoken language (Gustman, et al., 2002). Our process leverages a small set of manuallyacquired English-Czech translations to translate a large ontology of keyword phrases, thereby providing Czech speakers access to 116,000 hours of video testimonies in 32 languages. Starting from an initial out-of-vocabulary (OOV) rate of 85%, we show that a small set of prioritized translations can be elicited from human informants, aligned, decomposed and then recombined to cover 90% of the access value in a complex ontology. Moreover, we demonstrate that prioritization based on hierarchical position and frequency of use facilitates extremely eff</context>
</contexts>
<marker>Gustman, Oard, Picheny, Greenberg, 2002</marker>
<rawString>Gustman, Soergel, Oard, Byrne, Picheny, Ramabhadran, &amp; Greenberg. (2002). Supporting access to large digital oral history archives. In Proceedings of the Joint Conference on Digital Libraries. Portland, Oregon. (pp. 18-27).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kaji</author>
<author>T Aizono</author>
</authors>
<title>Extracting word correspondences from bilingual corpora based on word co-occurrence information.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING &apos;96</booktitle>
<pages>23--28</pages>
<contexts>
<context position="27323" citStr="Kaji &amp; Aizono, 1996" startWordPosition="4322" endWordPosition="4325">Work Several studies have taken a knowledgeacquisition approach to collecting multilingual word pairs. For example, Sadat et al. (2003) automatically extracted bilingual word pairs from comparable corpora. This approach is based on the simple assumption that if two words are mutual translations, then their most frequent collocates are likely to be mutual translations as well. However, the approach requires large comparable corpora, the collection of which presents non-trivial challenges. Others have made similar mutual-translation assumptions for lexical acquisition (Echizen-ya, et al., 2005; Kaji &amp; Aizono, 1996; Rapp, 1999; Tanaka &amp; Iwasaki, 1996). Most make use of either parallel corpora or a bilingual dictionary for the task of bilingual term extraction. Echizen-ya, et al. (2005) avoided using a bilingual dictionary, but required a parallel corpus to achieve their goal; whereas Fung (2000) and others have relied on pre-existing bilingual dictionaries. In either case, large bilingual resources of some kind are required. In addition, these approaches focused on the extraction of single-word pairs, not phrasal units. Many recent approaches to dictionary and thesaurus translation are geared toward pro</context>
</contexts>
<marker>Kaji, Aizono, 1996</marker>
<rawString>Kaji, H., &amp; Aizono, T. (1996). Extracting word correspondences from bilingual corpora based on word co-occurrence information. In Proceedings of COLING &apos;96 (pp. 23-28).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G C Murray</author>
<author>B Dorr</author>
<author>J Lin</author>
<author>J Hajic</author>
<author>P Pecina</author>
</authors>
<title>Leveraging recurrent phrase structure in large-scale ontology translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Annual Conference of the European Association for Machine Translation.</booktitle>
<location>Oslo, Norway.</location>
<contexts>
<context position="24145" citStr="Murray, et al., 2006" startWordPosition="3813" endWordPosition="3816">ranslations). After the aligned terms from only 500 translations were added to our dictionary, our Bleu score rose to 0.66. After aligned terms from 3000 translations were added, we achieved 0.69. Using corrected MT as the reference our Bleu scores improve from 0.48 to 0.79. If human-generated and human-corrected references are both considered to be correct translations, the improvement goes from .49 to .80. Regardless of the reference set, there is a consistent performance improvement as more and more translations are added. We found the same trend using the TER metric on a smaller data set (Murray, et al., 2006). The fact that the Bleu scores continue to rise indicates that our approach is successful in quickly expanding the lexicon with accurate translations. It is important to point out that Bleu scores are not meaningful in an absolute sense; the scores here should be interpreted with respect to each other. The trend Figure 5. Objective evaluation results Figure 6. Subjective evaluation results in scores strongly indicates that our prioritization scheme is effective for generating a high-quality translation lexicon at relatively low cost. To determine an upper bound on machine performance, we comp</context>
</contexts>
<marker>Murray, Dorr, Lin, Hajic, Pecina, 2006</marker>
<rawString>Murray, G. C., Dorr, B., Lin, J., Hajic, J., &amp; Pecina, P. (2006). Leveraging recurrent phrase structure in large-scale ontology translation. In Proceedings of the 11th Annual Conference of the European Association for Machine Translation. Oslo, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S J Nelson</author>
<author>M Schopen</author>
<author>A G Savage</author>
<author>J-L Schulman</author>
<author>N Arluk</author>
</authors>
<title>The MeSH translation maintenance system: Structure, interface design, and implementation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 11th World Congress on Medical Informatics.</booktitle>
<pages>67--69</pages>
<publisher>IOS Press.</publisher>
<location>Amsterdam:</location>
<contexts>
<context position="2456" citStr="Nelson, et al, 2004" startWordPosition="338" endWordPosition="341">cepts. These descriptors are usually arranged in a hierarchic thesaurus or ontology, and are assigned to collection items as a means of providing access (either via searching for keyword phases, browsing the hierarchy, or a combination both). MeSH (Medical Subject Headings) serves as a good example of such an ontology; it is a hierarchicallyarranged collection of controlled vocabulary terms manually assigned to medical abstracts in a number of databases. It provides multilingual access to the contents of these databases, but maintaining translations of such a complex structure is challenging (Nelson, et al, 2004). For the most part, research in multilingual information access focuses on the content of digital repositories themselves, often neglecting significant knowledge that is explicitly encoded in the associated ontologies. However, information systems cannot utilize such ontologies by simply applying off-the-shelf machine translation. General-purpose translation resources provide insufficient coverage of the vocabulary contained within these domain-specific ontologies. This paper tackles the question of how one might efficiently translate a large-scale ontology to facilitate multilingual informat</context>
</contexts>
<marker>Nelson, Schopen, Savage, Schulman, Arluk, 2004</marker>
<rawString>Nelson, S. J., Schopen, M., Savage, A. G., Schulman, J.-L., &amp; Arluk, N. (2004). The MeSH translation maintenance system: Structure, interface design, and implementation. In Proceedings of the 11th World Congress on Medical Informatics. (pp. 67-69). Amsterdam: IOS Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="28502" citStr="Och &amp; Ney, 2003" startWordPosition="4501" endWordPosition="4504">us translation are geared toward providing domain-specific thesauri to specialists in a particular field, e.g., medical terminology (Déjean, et al., 2005) and agricultural terminology (Chun &amp; Wenlin, 2002). Researchers on these projects are faced with either finding human translators who are specialized enough to manage the domain-particular translations—or applying automatic techniques to large-scale parallel corpora where data sparsity poses a problem for lowfrequency terms. Data sparsity is also an issue for more general state-of-the-art bilingual alignment approaches (Brown, et al., 2000; Och &amp; Ney, 2003; Wantanabe &amp; Sumita, 2003). 6 Conclusion The task of translating large ontologies can be recast as a problem of implementing fast and efficient processes for acquiring task-specific lexical resources. We developed a method for prioritizing keyword phrases from an English thesaurus of concepts and elicited Czech translations for a subset of the keyword phrases. From these, we decomposed phrase elements for reuse in an English-Czech probabilistic dictionary. We then applied the dictionary in machine translation of the rest of the thesaurus. Our results show an overall improvement in machine tra</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Och, F. J., &amp; Ney, H. (2003). A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1), 19-51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</booktitle>
<pages>331--318</pages>
<contexts>
<context position="21140" citStr="Papineni, et al., 2002" startWordPosition="3355" endWordPosition="3358">rily phrase by phrase/word by word translation. Our evaluation scores below will partially reflect the simplicity of our system. Our system is simple by design. Any improvement or degradation to the input of our system has direct influence on the output. Thus, measures of translation accuracy for our system can be directly interpreted as quality measures for the lexical resources used and the process by which they were developed. 4 Evaluation We performed two different types of evaluation to validate our process. First, we compared our system output to human reference translations using Bleu (Papineni, et al., 2002), a widelyaccepted objective metric for evaluation of machine translations. Second, we showed corrected and uncorrected machine translations to Czech speakers and collected subjective judgments of fluency and accuracy. For evaluation purposes, we selected 418 keyword phrases to be used as target translations. These phrases were selected using a stratified sampling technique so that different levels of thesaurus value would be represented. There was no overlap between these keyword phrases and the 3000 prioritized keyword phrases used to build our lexicon. Prior to machine translation we obtain</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Papineni, K., Roukos, S., Ward, T., &amp; Zhu, W.-J. (2002). BLEU: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (pp. 331-318).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rapp</author>
</authors>
<title>Automatic identification of word translations from unrelated English and German corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>519--526</pages>
<contexts>
<context position="27335" citStr="Rapp, 1999" startWordPosition="4326" endWordPosition="4327">have taken a knowledgeacquisition approach to collecting multilingual word pairs. For example, Sadat et al. (2003) automatically extracted bilingual word pairs from comparable corpora. This approach is based on the simple assumption that if two words are mutual translations, then their most frequent collocates are likely to be mutual translations as well. However, the approach requires large comparable corpora, the collection of which presents non-trivial challenges. Others have made similar mutual-translation assumptions for lexical acquisition (Echizen-ya, et al., 2005; Kaji &amp; Aizono, 1996; Rapp, 1999; Tanaka &amp; Iwasaki, 1996). Most make use of either parallel corpora or a bilingual dictionary for the task of bilingual term extraction. Echizen-ya, et al. (2005) avoided using a bilingual dictionary, but required a parallel corpus to achieve their goal; whereas Fung (2000) and others have relied on pre-existing bilingual dictionaries. In either case, large bilingual resources of some kind are required. In addition, these approaches focused on the extraction of single-word pairs, not phrasal units. Many recent approaches to dictionary and thesaurus translation are geared toward providing domai</context>
</contexts>
<marker>Rapp, 1999</marker>
<rawString>Rapp, R. (1999). Automatic identification of word translations from unrelated English and German corpora. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics. (pp. 519-526).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sadat</author>
<author>M Yoshikawa</author>
<author>S Uemura</author>
</authors>
<title>Enhancing cross-language information retrieval by an automatic acquisition of bilingual terminology from comparable corpora .</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</booktitle>
<pages>397--398</pages>
<contexts>
<context position="26839" citStr="Sadat et al. (2003)" startWordPosition="4253" endWordPosition="4256">neutral, 5=bad). An overview of the results is presented in Figure 6. Scores are shown for corrected and uncorrected MT. In all cases, the mode is 1 (i.e., good fluency and good accuracy). 59% of the machine translated phrases were rated 2 or better for fluency. 66% were rated 2 or better for accuracy. Only a small percentage of the translations had meanings that were far from the intended meaning. Disfluencies were primarily due to errors in morphology and word order. 5 Related Work Several studies have taken a knowledgeacquisition approach to collecting multilingual word pairs. For example, Sadat et al. (2003) automatically extracted bilingual word pairs from comparable corpora. This approach is based on the simple assumption that if two words are mutual translations, then their most frequent collocates are likely to be mutual translations as well. However, the approach requires large comparable corpora, the collection of which presents non-trivial challenges. Others have made similar mutual-translation assumptions for lexical acquisition (Echizen-ya, et al., 2005; Kaji &amp; Aizono, 1996; Rapp, 1999; Tanaka &amp; Iwasaki, 1996). Most make use of either parallel corpora or a bilingual dictionary for the ta</context>
</contexts>
<marker>Sadat, Yoshikawa, Uemura, 2003</marker>
<rawString>Sadat, F., Yoshikawa, M., &amp; Uemura, S. (2003). Enhancing cross-language information retrieval by an automatic acquisition of bilingual terminology from comparable corpora . In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (pp. 397-398).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Tanaka</author>
<author>H Iwasaki</author>
</authors>
<title>Extraction of lexical translations from non-aligned corpora.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING &apos;96.</booktitle>
<pages>580--585</pages>
<contexts>
<context position="27360" citStr="Tanaka &amp; Iwasaki, 1996" startWordPosition="4328" endWordPosition="4331"> knowledgeacquisition approach to collecting multilingual word pairs. For example, Sadat et al. (2003) automatically extracted bilingual word pairs from comparable corpora. This approach is based on the simple assumption that if two words are mutual translations, then their most frequent collocates are likely to be mutual translations as well. However, the approach requires large comparable corpora, the collection of which presents non-trivial challenges. Others have made similar mutual-translation assumptions for lexical acquisition (Echizen-ya, et al., 2005; Kaji &amp; Aizono, 1996; Rapp, 1999; Tanaka &amp; Iwasaki, 1996). Most make use of either parallel corpora or a bilingual dictionary for the task of bilingual term extraction. Echizen-ya, et al. (2005) avoided using a bilingual dictionary, but required a parallel corpus to achieve their goal; whereas Fung (2000) and others have relied on pre-existing bilingual dictionaries. In either case, large bilingual resources of some kind are required. In addition, these approaches focused on the extraction of single-word pairs, not phrasal units. Many recent approaches to dictionary and thesaurus translation are geared toward providing domain-specific thesauri to sp</context>
</contexts>
<marker>Tanaka, Iwasaki, 1996</marker>
<rawString>Tanaka, K., &amp; Iwasaki, H. (1996). Extraction of lexical translations from non-aligned corpora. In Proceedings of COLING &apos;96. (pp. 580-585).</rawString>
</citation>
<citation valid="true">
<authors>
<author>USC</author>
</authors>
<title>USC Shoah Foundation Institute for Visual History and Education,</title>
<date>2006</date>
<note>[online] http://www.usc.edu/schools/college/vhi</note>
<contexts>
<context position="4983" citStr="USC, 2006" startWordPosition="716" endWordPosition="717">hat a small set of prioritized translations can be elicited from human informants, aligned, decomposed and then recombined to cover 90% of the access value in a complex ontology. Moreover, we demonstrate that prioritization based on hierarchical position and frequency of use facilitates extremely efficient reuse of human input. Evaluations show that our technique is able to boost performance of a simple translation system by 65%. 2 The Problem The USC Shoah Foundation Institute for Visual History and Education manages what is presently the world&apos;s largest archive of videotaped oral histories (USC, 2006). The archive contains 116,000 hours of video from the testimonies of over 52,000 survivors, liberators, rescuers and witnesses of the Holocaust. If viewed end to end, the collection amounts to 13 years of continuous video. The Shoah Foundation uses a hierarchically arranged thesaurus of 56,000 keyword phrases representing domain-specific concepts. These are assigned to time-points in the video testimonies as a means of indexing the video content. Although the testimonies in the collection represent 32 different languages, the thesaurus used to catalog them is currently available only in Engli</context>
</contexts>
<marker>USC, 2006</marker>
<rawString>USC. (2006) USC Shoah Foundation Institute for Visual History and Education, [online] http://www.usc.edu/schools/college/vhi</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wantanabe</author>
<author>E Sumita</author>
</authors>
<title>Example-based decoding for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of MT Summit IX</booktitle>
<pages>410--417</pages>
<contexts>
<context position="28529" citStr="Wantanabe &amp; Sumita, 2003" startWordPosition="4505" endWordPosition="4508">e geared toward providing domain-specific thesauri to specialists in a particular field, e.g., medical terminology (Déjean, et al., 2005) and agricultural terminology (Chun &amp; Wenlin, 2002). Researchers on these projects are faced with either finding human translators who are specialized enough to manage the domain-particular translations—or applying automatic techniques to large-scale parallel corpora where data sparsity poses a problem for lowfrequency terms. Data sparsity is also an issue for more general state-of-the-art bilingual alignment approaches (Brown, et al., 2000; Och &amp; Ney, 2003; Wantanabe &amp; Sumita, 2003). 6 Conclusion The task of translating large ontologies can be recast as a problem of implementing fast and efficient processes for acquiring task-specific lexical resources. We developed a method for prioritizing keyword phrases from an English thesaurus of concepts and elicited Czech translations for a subset of the keyword phrases. From these, we decomposed phrase elements for reuse in an English-Czech probabilistic dictionary. We then applied the dictionary in machine translation of the rest of the thesaurus. Our results show an overall improvement in machine translation quality after coll</context>
</contexts>
<marker>Wantanabe, Sumita, 2003</marker>
<rawString>Wantanabe, T., &amp; Sumita, E. (2003). Example-based decoding for statistical machine translation. In Proceedings of MT Summit IX (pp. 410-417).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>