<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.020884">
<title confidence="0.815765">
Word Alignment with
Stochastic Bracketing Linear Inversion Transduction Grammar
</title>
<author confidence="0.910332">
Markus SAERS and Joakim NIVRE Dekai WU
</author>
<affiliation confidence="0.94436075">
Computational Linguistics Group Human Language Technology Center
Dept. of Linguistics and Philology Dept. of Computer Science and Engineering
Uppsala University HKUST
Sweden Hong Kong
</affiliation>
<email confidence="0.990206">
first.last@lingfil.uu.se dekai@cs.ust.hk
</email>
<sectionHeader confidence="0.996691" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.989990285714286">
The class of Linear Inversion Transduction
Grammars (LITGs) is introduced, and used to
induce a word alignment over a parallel cor-
pus. We show that alignment via Stochas-
tic Bracketing LITGs is considerably faster
than Stochastic Bracketing ITGs, while still
yielding alignments superior to the widely-
used heuristic of intersecting bidirectional
IBM alignments. Performance is measured as
the translation quality of a phrase-based ma-
chine translation system built upon the word
alignments, and an improvement of 2.85 BLEU
points over baseline is noted for French–
English.
</bodyText>
<sectionHeader confidence="0.998785" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998180625">
Machine translation relies heavily on word align-
ments, which are usually produced by training IBM-
models (Brown et al., 1993) in both directions and
combining the resulting alignments via some heuris-
tic. Automatically training an Inversion Transduc-
tion Grammar (ITG) has been suggested as a viable
way of producing superior alignments (Saers and
Wu, 2009). The main problem of using Bracket-
ing ITGs for alignment is that exhaustive biparsing
runs in O(n6) time. Several ways to lower the com-
plexity of ITGs has been suggested, but in this paper,
a different approach is taken. Instead of using full
ITGs, we explore the possibility of subjecting the
grammar to a linear constraint, making exhaustive
biparsing of a sentence pair in O(n4) time possible.
This can be further improved by applying pruning.
</bodyText>
<sectionHeader confidence="0.992093" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.99995775">
A transduction is the bilingual version of a language.
A language (Ll) can be formally viewed as a set of
sentences, sequences of tokens taken from a speci-
fied vocabulary (Vl). A transduction (Te�f) between
two languages (Le and Lf) is then a set of sentence
pairs, sequences of bitokens from the cross produc-
tion of the vocabularies of the two languages being
transduced (Ve�f = Ve × Vf). This adds an extra
layer of complexity to finding transductions from
raw bitexts, as an alignment has to be imposed.
Simple (STG) and Syntax Directed (SDTG) Trans-
duction Grammars (Aho and Ullman, 1972) can
be used to parse transductions between context-free
languages. Both work fine as long as a grammar is
given and parsing is done as transduction, that is: a
sentence in one language is rewritten into the other
language. In NLP, interest has shifted away from
hand-crafted grammars, towards stochastic gram-
mars induced from corpora. To induce a stochas-
tic grammar from a parallel corpus, expectations of
all possible parses over a sentence pair are typically
needed. STGs can biparse sentence pairs in polyno-
mial time, but are unable to account for the complex-
ities typically found in natural languages. SDTGs do
account for the complexities in natural languages,
but are intractable for biparsing.
Inversion transductions (Wu, 1995; Wu, 1997) are
a special case of transductions that are not mono-
tone, but where permutations are severely limited.
By limiting the possible permutations, biparsing be-
comes tractable. This in turn means that ITGs can be
induced from parallel corpora in polynomial time,
</bodyText>
<page confidence="0.976618">
341
</page>
<subsubsectionHeader confidence="0.578608">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 341–344,
</subsubsectionHeader>
<subsectionHeader confidence="0.276484">
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.996201153846154">
as well as account for most of the reorderings found
between natural languages.
An Inversion transduction is limited so that it
must be expressible as non-overlapping groups, in-
ternally permuted either by the identity permuta-
tion or the inversion permutation (hence the name).
This requirement also means that the grammar is bi-
narizable, yielding a two-normal form. A produc-
tion with the identity permutation is written inside
square brackets, while productions with the inver-
sion permutation is written inside angled brackets.
This gives us a two-normal form that looks like this
(where e/f is a biterminal):
</bodyText>
<equation confidence="0.854695333333333">
A —* [B C]
A —* (B C)
A —* e/f
</equation>
<bodyText confidence="0.995869548387097">
The time complexity for exhaustive ITG biparsing is
O(Gn6), which is typically too large to be applica-
ble to large grammars and long sentence. The gram-
mar constant G can be eliminated by limiting the
grammar to a bracketing ITG (BITG), which only has
one nonterminal symbol. Saers &amp; Wu (2009) show
that it is possible to apply exhaustive biparsing to a
large parallel corpus (— 100, 000 sentence pairs) of
short sentences (G10 tokens in both language). The
word alignments read off the Viterbi parse also in-
creased translation quality when used instead of the
alignments from bidirectional IBM alignments.
The O(n6) time complexity is somewhat pro-
hibitive for large corpora, so pruning in some form is
needed. Saers, Nivre &amp; Wu (2009) introduce a beam
pruning scheme, which reduces time complexity to
O(bn3). They also show that severe pruning is pos-
sible without significant deterioration in alignment
quality. Haghighi et. al (2009) use a simpler aligner
as guidance for pruning, which reduce the time com-
plexity by two orders of magnitude, and also intro-
duce block ITG, which gives many-to-one instead of
one-to-one alignments. Zhang et. al (2008) present
a method for evaluating spans in the sentence pair to
determine whether they should be excluded or not.
The algorithm has a best case time complexity of
O(n3).
In this paper we introduce Linear ITG (LITG), and
apply it to a word-alignment task which is evaluated
by the phrase-based statistical machine translation
(PBSMT) system that can be built from that.
</bodyText>
<sectionHeader confidence="0.62514" genericHeader="method">
3 Stochastic Bracketing Linear Inversion
Transduction Grammar
</sectionHeader>
<bodyText confidence="0.9997742">
A Bracketing Linear Inversion Transduction Gram-
mar (BLITG) is a BITG where rules may have at most
one nonterminal symbol in their production. This
gives us a normal form that is somewhat different
from the usual ITG:
</bodyText>
<equation confidence="0.9993448">
X —* [Xe/f]
X —* [e/fX]
X —* (Xe/f)
X —* (e/fX)
X —* ǫ/ǫ
</equation>
<bodyText confidence="0.99998725">
where one but not both of the tokens in the bitermi-
nal may be the empty string ǫ, if a nonterminal is
produced. By associating each rule with a probabil-
ity, we get a Stochastic BLITG (SBLITG).
</bodyText>
<subsectionHeader confidence="0.999424">
3.1 Biparsing Algorithm
</subsectionHeader>
<bodyText confidence="0.999971454545455">
The sentence pair to be biparsed consists of two vec-
tors of tokens (e and f). An item is represented
as a nonterminal (X), and one span in each of the
languages (es..t and fu..v). For notational conve-
nience, an item will be written as the nonterminal
with the spans as subscripts (Xs,t,u,v). The length of
an item is defined as the sum of the length of the two
spans: |Xs,t,u,v |= t — s + v — u. Items are gath-
ered in buckets, Bn, according to their length so that
Xs,t,u,v E B|Xs,t,u,�|. The algorithm is initialized
with the item spanning the entire sentence pair:
</bodyText>
<equation confidence="0.943143">
X0,|e|,0,|f |E B|X0,|e|,0,|f||
</equation>
<bodyText confidence="0.999741454545455">
Starting from this top bucket, buckets are processed
in larger to smaller order: Bn, Bn−1, ... , B1. While
processing a bucket, only smaller items are added,
meaning that B0 is fully constructed by the time B1
has been processed. Each item in B0 can have the
rule X —* ǫ/ǫ applied to it, eliminating the nonter-
minal and halting processing. If there are no items
in B0, parsing has failed.
To process a bucket, each item is extended by all
applicable rules, and the nonterminals in the produc-
tions are added to their respective buckets.
</bodyText>
<page confidence="0.982609">
342
</page>
<table confidence="0.999954">
System BLEU NIST Phrases
GIZA++ (intersect) 0.2629 6.7968 146,581,109
GIZA++ (grow-diag-final) 0.2632 6.7410 1,298,566
GIZA++ (grow-diag-final-and) 0.2742 6.9228 7,340,369
SBLITG (b = 25) 0.3027 7.3664 13,551,915
SBLITG (b = oo) 0.3008 7.3303 12,673,361
</table>
<tableCaption confidence="0.997281">
Table 1: Results for French–English.
</tableCaption>
<equation confidence="0.996396111111111">
Xs,t,u,v __+
[es,s+1/fu,u+1 Xs+1,t,u+1,v]
 |[Xs,t−1,u,v−1 et−1,t/fv−1,v]
 |(es,s+1/fv−1,v Xs+1,t,u,v−1)
 |(Xs,t−1,u+1,v et−1,t/fu,u+1)
 |[es,s+1/E Xs+1,t,u,v]  |(es,s+1/E Xs+1,t,u,v)
 |[E/fu,u+1 Xs,t,u+1,v]  |(Xs,t,u+1,v E/fu,u+1)
 |[Xs,t−1,u,v et−1,t /E] |(Xs,t−1,u,v et−1,t/E)
[Xs,t,u,v−1 E/fv−1,v]  |(E/fv−1,v Xs,t,u,v−1)
</equation>
<bodyText confidence="0.999928222222222">
Note that there are two productions on each of the
four last rows. These are distinct rules, but the
symbols in the productions are identical. This phe-
nomenon is due to the fact that the empty sym-
bols can be “read” off either end of the span. In
our experiments, such rules were merged into their
non-inverting form, effectively eliminating the last
four inverted rules (productions enclosed in angled
brackets) above.
</bodyText>
<subsectionHeader confidence="0.998923">
3.2 Analysis
</subsectionHeader>
<bodyText confidence="0.996277222222222">
Let n be the length of the longer sentence in the
pair. The number of buckets will be O(n), since
the longest item will be at most 2n long. Within a
bucket, there can be O(n2) starting points for items,
but once the length of one of the spans is fixed, the
length of the other follows, adding a factor O(n),
making the total number of items in a bucket O(n3).
Each item in a bucket can be analyzed in 8 possible
ways, requiring O(1) time. In summary, we have:
</bodyText>
<equation confidence="0.737603">
O(n) x O(n3) x O(1) = O(n4)
</equation>
<bodyText confidence="0.981501">
The pruning scheme works by limiting the num-
ber of items that are processed from each bucket, re-
ducing the cost of processing a bucket from O(n3)
to O(b), where b is the beam width. This gives time
complexity O(n) x O(b) x O(1) = O(bn).
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999982971428572">
We used the guidelines of the shared task of
WMT’081 to train our baseline system as well as
our experimental system. This includes induction of
word alignments with GIZA++ (Och and Ney, 2003),
induction of a Phrase-based SMT system (Koehn et
al., 2007), and tuning with minimum error rate train-
ing (Och, 2003), as well as applying some utility
scripts provided for the workshop. The translation
model is combined with a 5-gram language model
(Stolcke, 2002).
Our experimental system uses alignments from
the Viterbi parses, extracted during EM training of an
SBLITG on the training corpus, instead of GIZA++.
Since EM will converge fairly slowly, it was limited
to 10 iterations, after which it was halted.
We used the French–English part of the WMT’08
shared task, but limited the training set to sentence
pairs where both sentences were of length 20 or less.
This was necessary in order to carry out exhaustive
search in the SBLITG algorithm. In total, we had
381,780 sentence pairs for training, and 2,000 sen-
tence pairs each for tuning and testing. The language
model was trained with the entire training set.
To evaluate the systems we used BLEU (Papineni
et al., 2002) and NIST (Doddington, 2002)
Results are presented in Table 1. It is interesting
to note that there is no correlation between the num-
ber of phrases extracted and translation quality. The
only explanation for the results we are seeing is that
the SBLITGs find better phrases. Since the only dif-
ference is the word alignment strategy, this suggests
that the word alignments from SBLITGs are better
suited for phrase extraction than those from bidirec-
tional IBM-models. The fact that SBLITGs extract
more phrases than bidirectional IBM-models under
</bodyText>
<footnote confidence="0.992123">
1http://www.statmt.org/wmt08/
</footnote>
<page confidence="0.998783">
343
</page>
<bodyText confidence="0.99994625">
the grow-diag-x heuristics is significant, since
more phrases means that more translation possibil-
ities are extracted. The fact that SBLITGs extract
fewer phrases than bidirectional IBM-models under
the intersect heuristic is also significant, since
it implies that simply adding more phrases is a bad
strategy. Combined, the two observations leads us
to believe that there are some alignments missed by
the bidirectional IBM-models that are found by the
SBLITG-models. It is also interesting to see that the
pruned version outperforms the exhaustive version.
We believe this to be because the pruned version ap-
proaches the correct grammar faster than the exhaus-
tive. That would mean that the exhaustive SBLITG
would be better in the limit, but the experiment was
limited to 10 iterations.
</bodyText>
<sectionHeader confidence="0.999281" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999423727272727">
In this paper we have focused on the benefits of ap-
plying SBLITGs to the task of inducing word align-
ments, which leads to a 2.85 BLEU points improve-
ment compared to the standard model (heuristically
combined bidirectional IBM-models). In the future,
we hope that LITGs will be a spring board towards
full ITGs, with more interesting nonterminals than
the BITGs seen in the literature so far. With the pos-
sibility of inducing full ITG from parallel corpora it
becomes viable to use ITG decoders directly as ma-
chine translation systems.
</bodyText>
<sectionHeader confidence="0.997565" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998485545454546">
This work was funded by the Swedish National Gradu-
ate School of Language Technology (GSLT), the Defense
Advanced Research Projects Agency (DARPA) under
GALE Contract No. HR0011-06-C-0023, and the Hong
Kong Research Grants Council (RGC) under research
grants GRF621008, DAG03/04.EG09, RGC6256/00E,
and RGC6083/99E. Any opinions, findings and conclu-
sions or recommendations expressed in this material are
those of the authors and do not necessarily reflect the
views of DARPA.The computations were performed on
UPPMAX resources under project p2007020.
</bodyText>
<sectionHeader confidence="0.998408" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999848">
A. V. Aho and J. D. Ullman. 1972. The Theory of Pars-
ing, Translation, and Compiling. Prentice-Halll, En-
glewood Cliffs, New Jersey.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263–311.
G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statis-
tics. In Proceedings of Human Language Technology
conference (HLT-2002), San Diego, California.
A. Haghighi, J. Blitzer, J. DeNero, and D. Klein. 2009.
Better word alignments with supervised itg models.
In Proceedings ofACL/IJCNLP 2009, pages 923–931,
Singapore.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proceedings of the
ACL 2007 Demo and Poster Session, pages 177–180,
Prague, Czech Republic.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19–51.
F. J. Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proceedings ofACL 2003,
pages 160–167, Sapporo, Japan.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of ACL 2002, pages 311–
318, Philadelphia, Pennsylvania.
M. Saers and D. Wu. 2009. Improving phrase-based
translation via word alignments from Stochastic In-
version Transduction Grammars. In Proceedings of
SSST-3 at NAACL HLT 2009, pages 28–36, Boulder,
Colorado.
M. Saers, J. Nivre, and D. Wu. 2009. Learning stochas-
tic bracketing inversion transduction grammars with
a cubic time biparsing algorithm. In Proceedings of
IWPT’09, pages 29–32, Paris, France.
A. Stolcke. 2002. SRILM – an extensible language mod-
eling toolkit. In International Conference on Spoken
Language Processing, Denver, Colorado.
D. Wu. 1995. An algorithm for simultaneously bracket-
ing parallel texts by aligning words. In Proceedings of
WVLC-3, pages 69–82, Cambridge, Massachusetts.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putationalLinguistics, 23(3):377–403.
H. Zhang, C. Quirk, R. C. Moore, and D. Gildea. 2008.
Bayesian learning of non-compositional phrases with
synchronous parsing. In Proceedings of ACL/HLT
2008, pages 97–105, Columbus, Ohio.
</reference>
<page confidence="0.99899">
344
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.160919">
<title confidence="0.996186">Word Alignment with</title>
<author confidence="0.764226">Stochastic Bracketing Linear Inversion Transduction Grammar</author>
<affiliation confidence="0.952980333333333">Computational Linguistics Group Human Language Technology Dept. of Linguistics and Philology Dept. of Computer Science and Engineering Uppsala University HKUST</affiliation>
<author confidence="0.586463">Sweden Hong Kong</author>
<email confidence="0.995557">dekai@cs.ust.hk</email>
<abstract confidence="0.951032333333333">The class of Linear Inversion Transduction is introduced, and used to induce a word alignment over a parallel corpus. We show that alignment via Stochas- Bracketing is considerably faster Stochastic Bracketing while still yielding alignments superior to the widelyused heuristic of intersecting bidirectional Performance is measured as the translation quality of a phrase-based machine translation system built upon the word and an improvement of points over baseline is noted for French– English.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A V Aho</author>
<author>J D Ullman</author>
</authors>
<date>1972</date>
<booktitle>The Theory of Parsing, Translation, and Compiling. Prentice-Halll,</booktitle>
<location>Englewood Cliffs, New Jersey.</location>
<contexts>
<context position="2346" citStr="Aho and Ullman, 1972" startWordPosition="361" endWordPosition="364">pplying pruning. 2 Background A transduction is the bilingual version of a language. A language (Ll) can be formally viewed as a set of sentences, sequences of tokens taken from a specified vocabulary (Vl). A transduction (Te�f) between two languages (Le and Lf) is then a set of sentence pairs, sequences of bitokens from the cross production of the vocabularies of the two languages being transduced (Ve�f = Ve × Vf). This adds an extra layer of complexity to finding transductions from raw bitexts, as an alignment has to be imposed. Simple (STG) and Syntax Directed (SDTG) Transduction Grammars (Aho and Ullman, 1972) can be used to parse transductions between context-free languages. Both work fine as long as a grammar is given and parsing is done as transduction, that is: a sentence in one language is rewritten into the other language. In NLP, interest has shifted away from hand-crafted grammars, towards stochastic grammars induced from corpora. To induce a stochastic grammar from a parallel corpus, expectations of all possible parses over a sentence pair are typically needed. STGs can biparse sentence pairs in polynomial time, but are unable to account for the complexities typically found in natural lang</context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>A. V. Aho and J. D. Ullman. 1972. The Theory of Parsing, Translation, and Compiling. Prentice-Halll, Englewood Cliffs, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1064" citStr="Brown et al., 1993" startWordPosition="147" endWordPosition="150">e a word alignment over a parallel corpus. We show that alignment via Stochastic Bracketing LITGs is considerably faster than Stochastic Bracketing ITGs, while still yielding alignments superior to the widelyused heuristic of intersecting bidirectional IBM alignments. Performance is measured as the translation quality of a phrase-based machine translation system built upon the word alignments, and an improvement of 2.85 BLEU points over baseline is noted for French– English. 1 Introduction Machine translation relies heavily on word alignments, which are usually produced by training IBMmodels (Brown et al., 1993) in both directions and combining the resulting alignments via some heuristic. Automatically training an Inversion Transduction Grammar (ITG) has been suggested as a viable way of producing superior alignments (Saers and Wu, 2009). The main problem of using Bracketing ITGs for alignment is that exhaustive biparsing runs in O(n6) time. Several ways to lower the complexity of ITGs has been suggested, but in this paper, a different approach is taken. Instead of using full ITGs, we explore the possibility of subjecting the grammar to a linear constraint, making exhaustive biparsing of a sentence p</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram co-occurrence statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of Human Language Technology conference (HLT-2002),</booktitle>
<location>San Diego, California.</location>
<contexts>
<context position="10384" citStr="Doddington, 2002" startWordPosition="1699" endWordPosition="1700">pus, instead of GIZA++. Since EM will converge fairly slowly, it was limited to 10 iterations, after which it was halted. We used the French–English part of the WMT’08 shared task, but limited the training set to sentence pairs where both sentences were of length 20 or less. This was necessary in order to carry out exhaustive search in the SBLITG algorithm. In total, we had 381,780 sentence pairs for training, and 2,000 sentence pairs each for tuning and testing. The language model was trained with the entire training set. To evaluate the systems we used BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) Results are presented in Table 1. It is interesting to note that there is no correlation between the number of phrases extracted and translation quality. The only explanation for the results we are seeing is that the SBLITGs find better phrases. Since the only difference is the word alignment strategy, this suggests that the word alignments from SBLITGs are better suited for phrase extraction than those from bidirectional IBM-models. The fact that SBLITGs extract more phrases than bidirectional IBM-models under 1http://www.statmt.org/wmt08/ 343 the grow-diag-x heuristics is significant, since</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>G. Doddington. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. In Proceedings of Human Language Technology conference (HLT-2002), San Diego, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>J Blitzer</author>
<author>J DeNero</author>
<author>D Klein</author>
</authors>
<title>Better word alignments with supervised itg models.</title>
<date>2009</date>
<booktitle>In Proceedings ofACL/IJCNLP 2009,</booktitle>
<pages>923--931</pages>
<marker>Haghighi, Blitzer, DeNero, Klein, 2009</marker>
<rawString>A. Haghighi, J. Blitzer, J. DeNero, and D. Klein. 2009. Better word alignments with supervised itg models. In Proceedings ofACL/IJCNLP 2009, pages 923–931, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL 2007 Demo and Poster Session,</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="9434" citStr="Koehn et al., 2007" startWordPosition="1539" endWordPosition="1542">a bucket can be analyzed in 8 possible ways, requiring O(1) time. In summary, we have: O(n) x O(n3) x O(1) = O(n4) The pruning scheme works by limiting the number of items that are processed from each bucket, reducing the cost of processing a bucket from O(n3) to O(b), where b is the beam width. This gives time complexity O(n) x O(b) x O(1) = O(bn). 4 Experiments We used the guidelines of the shared task of WMT’081 to train our baseline system as well as our experimental system. This includes induction of word alignments with GIZA++ (Och and Ney, 2003), induction of a Phrase-based SMT system (Koehn et al., 2007), and tuning with minimum error rate training (Och, 2003), as well as applying some utility scripts provided for the workshop. The translation model is combined with a 5-gram language model (Stolcke, 2002). Our experimental system uses alignments from the Viterbi parses, extracted during EM training of an SBLITG on the training corpus, instead of GIZA++. Since EM will converge fairly slowly, it was limited to 10 iterations, after which it was halted. We used the French–English part of the WMT’08 shared task, but limited the training set to sentence pairs where both sentences were of length 20 </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the ACL 2007 Demo and Poster Session, pages 177–180, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="9373" citStr="Och and Ney, 2003" startWordPosition="1529" endWordPosition="1532">g the total number of items in a bucket O(n3). Each item in a bucket can be analyzed in 8 possible ways, requiring O(1) time. In summary, we have: O(n) x O(n3) x O(1) = O(n4) The pruning scheme works by limiting the number of items that are processed from each bucket, reducing the cost of processing a bucket from O(n3) to O(b), where b is the beam width. This gives time complexity O(n) x O(b) x O(1) = O(bn). 4 Experiments We used the guidelines of the shared task of WMT’081 to train our baseline system as well as our experimental system. This includes induction of word alignments with GIZA++ (Och and Ney, 2003), induction of a Phrase-based SMT system (Koehn et al., 2007), and tuning with minimum error rate training (Och, 2003), as well as applying some utility scripts provided for the workshop. The translation model is combined with a 5-gram language model (Stolcke, 2002). Our experimental system uses alignments from the Viterbi parses, extracted during EM training of an SBLITG on the training corpus, instead of GIZA++. Since EM will converge fairly slowly, it was limited to 10 iterations, after which it was halted. We used the French–English part of the WMT’08 shared task, but limited the training </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL 2003,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="9491" citStr="Och, 2003" startWordPosition="1551" endWordPosition="1552">In summary, we have: O(n) x O(n3) x O(1) = O(n4) The pruning scheme works by limiting the number of items that are processed from each bucket, reducing the cost of processing a bucket from O(n3) to O(b), where b is the beam width. This gives time complexity O(n) x O(b) x O(1) = O(bn). 4 Experiments We used the guidelines of the shared task of WMT’081 to train our baseline system as well as our experimental system. This includes induction of word alignments with GIZA++ (Och and Ney, 2003), induction of a Phrase-based SMT system (Koehn et al., 2007), and tuning with minimum error rate training (Och, 2003), as well as applying some utility scripts provided for the workshop. The translation model is combined with a 5-gram language model (Stolcke, 2002). Our experimental system uses alignments from the Viterbi parses, extracted during EM training of an SBLITG on the training corpus, instead of GIZA++. Since EM will converge fairly slowly, it was limited to 10 iterations, after which it was halted. We used the French–English part of the WMT’08 shared task, but limited the training set to sentence pairs where both sentences were of length 20 or less. This was necessary in order to carry out exhaust</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings ofACL 2003, pages 160–167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL 2002,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="10356" citStr="Papineni et al., 2002" startWordPosition="1693" endWordPosition="1696"> of an SBLITG on the training corpus, instead of GIZA++. Since EM will converge fairly slowly, it was limited to 10 iterations, after which it was halted. We used the French–English part of the WMT’08 shared task, but limited the training set to sentence pairs where both sentences were of length 20 or less. This was necessary in order to carry out exhaustive search in the SBLITG algorithm. In total, we had 381,780 sentence pairs for training, and 2,000 sentence pairs each for tuning and testing. The language model was trained with the entire training set. To evaluate the systems we used BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) Results are presented in Table 1. It is interesting to note that there is no correlation between the number of phrases extracted and translation quality. The only explanation for the results we are seeing is that the SBLITGs find better phrases. Since the only difference is the word alignment strategy, this suggests that the word alignments from SBLITGs are better suited for phrase extraction than those from bidirectional IBM-models. The fact that SBLITGs extract more phrases than bidirectional IBM-models under 1http://www.statmt.org/wmt08/ 343 the grow-diag-x heur</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of ACL 2002, pages 311– 318, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Saers</author>
<author>D Wu</author>
</authors>
<title>Improving phrase-based translation via word alignments from Stochastic Inversion Transduction Grammars.</title>
<date>2009</date>
<booktitle>In Proceedings of SSST-3 at NAACL HLT 2009,</booktitle>
<pages>28--36</pages>
<location>Boulder, Colorado.</location>
<contexts>
<context position="1294" citStr="Saers and Wu, 2009" startWordPosition="182" endWordPosition="185">rsecting bidirectional IBM alignments. Performance is measured as the translation quality of a phrase-based machine translation system built upon the word alignments, and an improvement of 2.85 BLEU points over baseline is noted for French– English. 1 Introduction Machine translation relies heavily on word alignments, which are usually produced by training IBMmodels (Brown et al., 1993) in both directions and combining the resulting alignments via some heuristic. Automatically training an Inversion Transduction Grammar (ITG) has been suggested as a viable way of producing superior alignments (Saers and Wu, 2009). The main problem of using Bracketing ITGs for alignment is that exhaustive biparsing runs in O(n6) time. Several ways to lower the complexity of ITGs has been suggested, but in this paper, a different approach is taken. Instead of using full ITGs, we explore the possibility of subjecting the grammar to a linear constraint, making exhaustive biparsing of a sentence pair in O(n4) time possible. This can be further improved by applying pruning. 2 Background A transduction is the bilingual version of a language. A language (Ll) can be formally viewed as a set of sentences, sequences of tokens ta</context>
<context position="4486" citStr="Saers &amp; Wu (2009)" startWordPosition="707" endWordPosition="710">ammar is binarizable, yielding a two-normal form. A production with the identity permutation is written inside square brackets, while productions with the inversion permutation is written inside angled brackets. This gives us a two-normal form that looks like this (where e/f is a biterminal): A —* [B C] A —* (B C) A —* e/f The time complexity for exhaustive ITG biparsing is O(Gn6), which is typically too large to be applicable to large grammars and long sentence. The grammar constant G can be eliminated by limiting the grammar to a bracketing ITG (BITG), which only has one nonterminal symbol. Saers &amp; Wu (2009) show that it is possible to apply exhaustive biparsing to a large parallel corpus (— 100, 000 sentence pairs) of short sentences (G10 tokens in both language). The word alignments read off the Viterbi parse also increased translation quality when used instead of the alignments from bidirectional IBM alignments. The O(n6) time complexity is somewhat prohibitive for large corpora, so pruning in some form is needed. Saers, Nivre &amp; Wu (2009) introduce a beam pruning scheme, which reduces time complexity to O(bn3). They also show that severe pruning is possible without significant deterioration in</context>
</contexts>
<marker>Saers, Wu, 2009</marker>
<rawString>M. Saers and D. Wu. 2009. Improving phrase-based translation via word alignments from Stochastic Inversion Transduction Grammars. In Proceedings of SSST-3 at NAACL HLT 2009, pages 28–36, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Saers</author>
<author>J Nivre</author>
<author>D Wu</author>
</authors>
<title>Learning stochastic bracketing inversion transduction grammars with a cubic time biparsing algorithm.</title>
<date>2009</date>
<booktitle>In Proceedings of IWPT’09,</booktitle>
<pages>29--32</pages>
<location>Paris, France.</location>
<marker>Saers, Nivre, Wu, 2009</marker>
<rawString>M. Saers, J. Nivre, and D. Wu. 2009. Learning stochastic bracketing inversion transduction grammars with a cubic time biparsing algorithm. In Proceedings of IWPT’09, pages 29–32, Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In International Conference on Spoken Language Processing,</booktitle>
<location>Denver, Colorado.</location>
<contexts>
<context position="9639" citStr="Stolcke, 2002" startWordPosition="1574" endWordPosition="1575">educing the cost of processing a bucket from O(n3) to O(b), where b is the beam width. This gives time complexity O(n) x O(b) x O(1) = O(bn). 4 Experiments We used the guidelines of the shared task of WMT’081 to train our baseline system as well as our experimental system. This includes induction of word alignments with GIZA++ (Och and Ney, 2003), induction of a Phrase-based SMT system (Koehn et al., 2007), and tuning with minimum error rate training (Och, 2003), as well as applying some utility scripts provided for the workshop. The translation model is combined with a 5-gram language model (Stolcke, 2002). Our experimental system uses alignments from the Viterbi parses, extracted during EM training of an SBLITG on the training corpus, instead of GIZA++. Since EM will converge fairly slowly, it was limited to 10 iterations, after which it was halted. We used the French–English part of the WMT’08 shared task, but limited the training set to sentence pairs where both sentences were of length 20 or less. This was necessary in order to carry out exhaustive search in the SBLITG algorithm. In total, we had 381,780 sentence pairs for training, and 2,000 sentence pairs each for tuning and testing. The </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM – an extensible language modeling toolkit. In International Conference on Spoken Language Processing, Denver, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>An algorithm for simultaneously bracketing parallel texts by aligning words.</title>
<date>1995</date>
<booktitle>In Proceedings of WVLC-3,</booktitle>
<pages>69--82</pages>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="3081" citStr="Wu, 1995" startWordPosition="480" endWordPosition="481"> done as transduction, that is: a sentence in one language is rewritten into the other language. In NLP, interest has shifted away from hand-crafted grammars, towards stochastic grammars induced from corpora. To induce a stochastic grammar from a parallel corpus, expectations of all possible parses over a sentence pair are typically needed. STGs can biparse sentence pairs in polynomial time, but are unable to account for the complexities typically found in natural languages. SDTGs do account for the complexities in natural languages, but are intractable for biparsing. Inversion transductions (Wu, 1995; Wu, 1997) are a special case of transductions that are not monotone, but where permutations are severely limited. By limiting the possible permutations, biparsing becomes tractable. This in turn means that ITGs can be induced from parallel corpora in polynomial time, 341 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 341–344, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics as well as account for most of the reorderings found between natural languages. An Inversion transduction is limited so that it </context>
</contexts>
<marker>Wu, 1995</marker>
<rawString>D. Wu. 1995. An algorithm for simultaneously bracketing parallel texts by aligning words. In Proceedings of WVLC-3, pages 69–82, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>ComputationalLinguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="3092" citStr="Wu, 1997" startWordPosition="482" endWordPosition="483">ransduction, that is: a sentence in one language is rewritten into the other language. In NLP, interest has shifted away from hand-crafted grammars, towards stochastic grammars induced from corpora. To induce a stochastic grammar from a parallel corpus, expectations of all possible parses over a sentence pair are typically needed. STGs can biparse sentence pairs in polynomial time, but are unable to account for the complexities typically found in natural languages. SDTGs do account for the complexities in natural languages, but are intractable for biparsing. Inversion transductions (Wu, 1995; Wu, 1997) are a special case of transductions that are not monotone, but where permutations are severely limited. By limiting the possible permutations, biparsing becomes tractable. This in turn means that ITGs can be induced from parallel corpora in polynomial time, 341 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 341–344, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics as well as account for most of the reorderings found between natural languages. An Inversion transduction is limited so that it must be exp</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>D. Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. ComputationalLinguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhang</author>
<author>C Quirk</author>
<author>R C Moore</author>
<author>D Gildea</author>
</authors>
<title>Bayesian learning of non-compositional phrases with synchronous parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL/HLT</booktitle>
<pages>97--105</pages>
<location>Columbus, Ohio.</location>
<marker>Zhang, Quirk, Moore, Gildea, 2008</marker>
<rawString>H. Zhang, C. Quirk, R. C. Moore, and D. Gildea. 2008. Bayesian learning of non-compositional phrases with synchronous parsing. In Proceedings of ACL/HLT 2008, pages 97–105, Columbus, Ohio.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>