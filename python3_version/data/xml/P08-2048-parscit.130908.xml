<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007188">
<title confidence="0.998401">
Mapping between Compositional Semantic Representations and Lexical
Semantic Resources: Towards Accurate Deep Semantic Parsing
</title>
<author confidence="0.999214">
Sergio Roa†$, Valia Kordoni† and Yi Zhang†
</author>
<affiliation confidence="0.997125">
Dept. of Computational Linguistics, Saarland University, Germany†
German Research Center for Artificial Intelligence (DFKI GmbH)†
Dept. of Computer Science, University of Freiburg, Germany$
</affiliation>
<email confidence="0.995665">
{sergior,kordoni,yzhang}@coli.uni-sb.de
</email>
<sectionHeader confidence="0.993842" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999834866666667">
This paper introduces a machine learning
method based on bayesian networks which
is applied to the mapping between deep se-
mantic representations and lexical semantic
resources. A probabilistic model comprising
Minimal Recursion Semantics (MRS) struc-
tures and lexicalist oriented semantic features
is acquired. Lexical semantic roles enrich-
ing the MRS structures are inferred, which are
useful to improve the accuracy of deep seman-
tic parsing. Verb classes inference was also
investigated, which, together with lexical se-
mantic information provided by VerbNet and
PropBank resources, can be substantially ben-
eficial to the parse disambiguation task.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999761818181818">
Recent studies of natural language parsing have
shown a clear and steady shift of focus from pure
syntactic analyses to more semantically informed
structures. As a result, we have seen an emerging
interest in parser evaluation based on more theory-
neutral and semantically informed representations,
such as dependency structures. Some approaches
have even tried to acquire semantic representations
without full syntactic analyses. The so-called shal-
low semantic parsers build basic predicate-argument
structures or label semantic roles that reveal the par-
tial meaning of sentences (Carreras and M`arquez,
2005). Manually annotated lexical semantic re-
sources like PropBank (Palmer et al., 2005), Verb-
Net (Kipper-Schuler, 2005), or FrameNet (Baker et
al., 1998) are usually used as gold standards for
training and evaluation of such systems. In the
meantime, various existing parsing systems are also
adapted to provide semantic information in their out-
puts. The obvious advantage in such an approach
is that one can derive more fine-grained represen-
tations which are not typically available from shal-
low semantic parsers (e.g., modality and negation,
quantifiers and scopes, etc.). To this effect, var-
ious semantic representations have been proposed
and used in different parsing systems. Generally
speaking, such semantic representations should be
capable of embedding shallow semantic information
(i.e., predicate-argument or semantic roles). How-
ever, it is non-trivial to map even the basic predicate-
arguments between different representations. This
becomes a barrier to both sides, making the cross-
fertilization of systems and resources using different
semantic representations very difficult.
In this paper, we present a machine learning ap-
proach towards mapping between deep and shallow
semantic representations. More specifically, we use
Bayesian networks to acquire a statistical model that
enriches the Minimal Recursion Semantics struc-
tures produced by the English Resource Grammar
(ERG) (Flickinger, 2002) with VerbNet-like seman-
tic roles. Evaluation results show that the mapping
from MRS to semantic roles is reliable and benefi-
cial to deep parsing.
</bodyText>
<sectionHeader confidence="0.959931" genericHeader="method">
2 Minimal Recursion Semantics
</sectionHeader>
<bodyText confidence="0.999927666666667">
The semantic representation we are interested in
in this paper is the Minimal Recursion Semantics
(MRS). Because of its underspecifiability, it has
been widely used in many deep and shallow pro-
cessing systems. The main assumption behind MRS
is that the interesting linguistic units for compu-
tational semantics are the elementary predications
(EPs), which are single relations with associated ar-
guments (Copestake et al., 2006). In this paper,
the MRS structures are created with the English Re-
source Grammar (ERG), a HPSG-based broad cov-
erage precision grammar for English. The seman-
</bodyText>
<page confidence="0.990107">
189
</page>
<reference confidence="0.2224735">
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 189–192,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</reference>
<bodyText confidence="0.9961894">
tic predicates and their linguistic behaviour (includ-
ing the set of semantic roles, indication of optional
arguments, and their possible value constraints are
specified by the grammar as its semantic interface
(SEM-I) (Flickinger et al., 2005).
</bodyText>
<sectionHeader confidence="0.966478" genericHeader="method">
3 Relating MRS structures to lexical
semantic resources
</sectionHeader>
<subsectionHeader confidence="0.999895">
3.1 Feature extraction from linguistic resources
</subsectionHeader>
<bodyText confidence="0.9992041">
The first set of features used to find corresponding
lexical semantic roles for the MRS predicate argu-
ments are taken from Robust MRS (RMRS) struc-
tures (Copestake, 2006). The general idea of the
process is to traverse the bag of elementary predi-
cations looking for the verbs in the parsed sentence.
When a verb is found, then its arguments are taken
from the rarg tags and alternatively from the in-g
conjunctions related to the verb. So, given the sen-
tence:
</bodyText>
<listItem confidence="0.6334086">
(1) Yields on money-market mutual funds contin-
ued to slide, amid signs that portfolio managers
expect further declines in interest rates.
the obtained features for expect are shown in Table
1.
</listItem>
<table confidence="0.872273666666667">
SEM-I roles Features Words
ARG1 manager n of managers
ARG2 propositional m rel further declines
</table>
<tableCaption confidence="0.999759">
Table 1: RMRS features for the verb expect
</tableCaption>
<bodyText confidence="0.969004857142857">
The SEM-I role labels are based mainly on syn-
tactic characteristics of the verb. We employed
the data provided by the PropBank and VerbNet
projects to extract lexical semantic information. For
PropBank, the argument labels are named ARG1,...,
ARGN and additionally ARGM for adjuncts. In the
case of VerbNet, 31 different thematic roles are pro-
vided, e.g. Actor, Agent, Patient, Proposition, Predi-
cate, Theme, Topic. A treebank of RMRS structures
and derivations was generated by using the Prop-
Bank corpus. The process of RMRS feature extrac-
tion was applied and a new verb dependency trees
dataset was created.
To obtain a correspondence between the SEM-I
role labels and the PropBank (or VerbNet) role la-
bels, a procedure which maps these labellings for
each utterance and verb found in the corpus was im-
plemented. Due to the possible semantic roles that
subjects and objects in a sentence could bear, the
mapping between SEM-I roles and VerbNet role la-
bels is not one-to-one. The general idea of this align-
ment process is to use the words in a given utterance
which are selected by a given role label, both a SEM-
I and a PropBank one. With these words, a naive as-
sumption was applied that allows a reasonable com-
parison and alignment of these two sources of infor-
mation. The naive assumption considers that if all
the words selected by some SEM-I label are found in
a given PropBank (VerbNet) role label, then we can
deduce that these labels can be aligned. An impor-
tant constraint is that all the SEM-I labels must be
exhausted. An additional constraint is that ARG1,
ARG2 or ARG3 SEM-I labels cannot be mapped to
ARGM PropBank labels. When an alignment be-
tween a SEM-I role and a corresponding lexical se-
mantic role is found, no more mappings for these
labels are allowed. For instance, given the example
in Table 1, with the following Propbank (VerbNet)
labelling:
(2) [Argo(Experiencer) Portfolio managers] expect
[Arg,(Theme) further declines in interest rates.]
the alignment shown in Table 2 is obtained.
</bodyText>
<table confidence="0.956595333333333">
SEM-I roles Mapped roles Features
ARG1 Experiencer manager n of
ARG2 Theme propositional m rel
</table>
<tableCaption confidence="0.999595">
Table 2: Alignment instance obtained for the verb expect
</tableCaption>
<bodyText confidence="0.999982866666667">
Since the use of fine-grained features can make
the learning process very complex, the WordNet
semantic network (Fellbaum, 1998) was also em-
ployed to obtain generalisations of nouns. The al-
gorithm described in (Pedersen et al., 2004) was
used to disambiguate the sense, given the heads
of the verb arguments and the verb itself (by us-
ing the mapping from VerbNet senses to WordNet
verb senses (Kipper-Schuler, 2005)). Alternatively,
a naive model has also been proposed, in which
these features are simply generalized as nouns. For
prepositions, the ontology provided by the SEM-I
was used. Other words like adjectives or verbs in
arguments were simply generalised as their corre-
sponding type (e.g., adjectival rel or verbal rel).
</bodyText>
<page confidence="0.985596">
190
</page>
<subsectionHeader confidence="0.9215005">
3.2 Inference of semantic roles with Bayesian
Networks
</subsectionHeader>
<bodyText confidence="0.9979164">
The inference of semantic roles is based on train-
ing of BNs by presenting instances of the features
extracted, during the learning process. Thus, a train-
ing example corresponding to the features shown in
Table 2 might be represented as Figure 1 shows, us-
ing a first-order approach. After training, the net-
work can infer a proper PropBank (VerbNet) seman-
tic role, given some RMRS role corresponding to
some verb. The use of some of these features can
be relaxed to test different alternatives.
</bodyText>
<figureCaption confidence="0.9995745">
Figure 1: A priori structure of the BN for lexical semantic
roles inference.
</figureCaption>
<bodyText confidence="0.96285625">
Two algorithms are used to train the BNs. The
Maximum Likelihood (ML) estimation procedure is
used when the structure of the model is known. In
our experiments, the a priori structure shown in Fig-
ure 1 was employed. In the case of the Structural Ex-
pectation Maximization (SEM) Algorithm, the ini-
tial structure assumed for the ML algorithm serves
as an initial state for the network and then the learn-
ing phase is executed in order to learn other con-
ditional dependencies and parameters as well. The
training procedure is described in Figure 2.
procedure Train (Model)
</bodyText>
<listItem confidence="0.973231384615385">
1: for all Verbs do
2: for all Sentences and Parsings which include the current verb
do
3: Initialize vertices of the network with SEM-I labels and fea-
tures.
4: Initialize optionally vertices with the corresponding VerbNet
class.
5: Initialize edges connecting corresponding features.
6: Append the current features as evidence for the network.
7: end for
8: Start Training Model for the current Verb, where Model is ML
or SEM.
9: end for
</listItem>
<figureCaption confidence="0.992083">
Figure 2: Algorithm for training Bayesian Networks for
inference of lexical semantic roles
</figureCaption>
<bodyText confidence="0.999984">
After the training phase, a testing procedure using
the Markov Chain Monte Carlo (MCMC) inference
engine can be used to infer role labels. Since it is
reasonable to think that in some cases the VerbNet
class is not known, the presentation of this feature as
evidence can be left as optional. Thus, after present-
ing as evidence the SEM-I related features, a role
label with highest probability is obtained after using
the MCMC with the current evidence.
</bodyText>
<sectionHeader confidence="0.997129" genericHeader="evaluation">
4 Experimental results
</sectionHeader>
<bodyText confidence="0.999897083333333">
The experiment uses 10370 sentences from the
PropBank corpus which have a mapping to Verb-
Net (Loper et al., 2007) and are successfully parsed
by the ERG (December 2006 version). Up to 10
best parses are recorded for each sentence. The to-
tal number of instances, considering that each sen-
tence contains zero or more verbs, is 13589. The
algorithm described in section 3.1 managed to find
at least one mapping for 10960 of these instances
(1020 different verb lexemes). If the number of pars-
ing results is increased to 25 the results are improved
(1460 different verb lexemes were found). In the
second experiment, the sentences without VerbNet
mappings were also included.
The results for the probabilistic models for in-
fering lexical semantic roles are shown in Table 3,
where the term naive means that no WordNet fea-
tures were included in the training of the models, but
only simple features like noun rel for nouns. On the
contrary, when mode is complete, WordNet hyper-
nyms up to the 5th level in the hierarchy were used.
In this set of experiments the VerbNet class was also
included (in the marked cases) during the learning
and inference phases.
</bodyText>
<table confidence="0.997548833333333">
Corpus Nr. iter. Mode Model Verb Accuracy %
MCMC classes
PropBank with 1000 ML naive 78.41
VerbNet labels 10000 ML naive 84.48
10000 ML naive x 87.92
1000 ML complete 84.74
10000 ML complete 86.79
10000 ML complete x 87.76
1000 SEM naive 84.25
1000 SEM complete 87.26
PropBank with 1000 ML naive 87.46
PropBank labels 1000 SEM naive 90.27
</table>
<tableCaption confidence="0.999919">
Table 3: Results of role mapping with probabilistic model
</tableCaption>
<bodyText confidence="0.993697">
In Table 3, the errors are due to the problems in-
troduced by the alternation behaviour of the verbs,
which are not encoded in the SEM-I labelling and
</bodyText>
<figure confidence="0.999660375">
VerbNet class
wish−62
living_
thing_n
ARG1 ARG2 ARG3
propositional_m_rel
null
RMRS Features
Experiencer
living_
thing_n
Theme
propositional_m_rel
ARGM
null
PropBank/VerbNet Features
</figure>
<page confidence="0.994145">
191
</page>
<bodyText confidence="0.981509793103448">
also some contradictory annotations in the mapping
between PropBank and VerbNet. Furthermore, the
use of the WordNet features may also generate a
more complex model or problems derived from the
disambiguation process and hence produce errors in
the inference phase. In addition, it is reasonable
to use the VerbNet class information in the learn-
ing and inference phases, which in fact improves
slightly the results. The outcomes also show that
the use of the SEM algorithm improves accuracy
slightly, meaning that the conditional dependency
assumptions were reasonable, but still not perfect.
The model can be slightly modified for verb class
inference, by adding conditional dependencies be-
tween the VerbNet class and SEM-I features, which
can potentially improve the parse disambiguation
task, in a similar way of thinking to (Fujita et al.,
2007). For instance, for the following sentence, we
derive an incorrect mapping for the verb stay to the
VerbNet class EXIST-47.1-1 with the (falsely) fa-
vored parse where the PP “in one place” is treated as
an adjunct/modifier. For the correct reading where
the PP is a complement to stay, the mapping to the
correct VerbNet class LODGE-46 is derived, and the
correct LOCATION role is identified for the PP.
(3) Regardless of whether [Theme you] hike from
lodge to lodge or stayLODGE-46 [Location in one
place] and take day trips, there are plenty of
choices.
</bodyText>
<sectionHeader confidence="0.997702" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999979166666667">
In this paper, we have presented a study of mapping
between the HPSG parser semantic outputs in form
of MRS structures and lexical semantic resources.
The experiment result shows that the Bayesian net-
work reliably maps MRS predicate-argument struc-
tures to semantic roles. The automatic mapping en-
ables us to enrich the deep parser output with seman-
tic role information. Preliminary experiments have
also shown that verb class inference can potentially
improve the parse disambiguation task. Although
we have been focusing on improving the deep pars-
ing system with the mapping to annotated semantic
resources, it is important to realise that the mapping
also enables us to enrich the shallow semantic an-
notations with more fine-grained analyses from the
deep grammars. Such analyses can eventually be
helpful for applications like question answering, for
instance, and will be investigated in the future.
</bodyText>
<sectionHeader confidence="0.990295" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999596078431373">
Collin Baker, Charles Fillmore, and John Lowe. 1998.
The Berkeley FrameNet project. In Proceedings of
the 36th Annual Meeting of the ACL and 17th In-
ternational Conference on Computational Linguistics,
pages 86–90, San Francisco, CA.
Xavier Carreras and Lluis M`arquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: Semantic role
labeling. In Proceedings of the Ninth Conference on
Computational Natural Language Learning (CoNLL-
2005), pages 152–164, Ann Arbor, Michigan.
Ann Copestake, Dan P. Flickinger, and Ivan A. Sag.
2006. Minimal recursion semantics: An introduction.
Research on Language and Computation, 3(4):281–
332.
Ann Copestake. 2006. Robust minimal recursion seman-
tics. Working Paper.
Christiane D. Fellbaum. 1998. WordNet – An Electronic
Lexical Database. MIT Press.
Dan Flickinger, Jan T. Lønning, Helge Dyvik, Stephan
Oepen, and Francis Bond. 2005. SEM-I rational MT.
Enriching deep grammars with a semantic interface for
scalable machine translation. In Proceedings of the
10th Machine Translation Summit, pages 165–172,
Phuket, Thailand.
Dan Flickinger. 2002. On building a more efficient
grammar by exploiting types. In Stephan Oepen, Dan
Flickinger, Jun’ichi Tsujii, and Hans Uszkoreit, edi-
tors, Collaborative Language Engineering, pages 1–
17. CSLI Publications.
Sanae Fujita, Francis Bond, Stephan Oepen, and Takaaki
Tanaka. 2007. Exploiting semantic information for
hpsg parse selection. In ACL 2007 Workshop on Deep
Linguistic Processing, pages 25–32, Prague, Czech
Republic.
Karin Kipper-Schuler. 2005. VerbNet: A broad-
coverage, comprehensive verb lexicon. Ph.D. thesis,
University of Pennsylvania.
Edward Loper, Szu ting Yi, and Martha Palmer. 2007.
Combining lexical resources: Mapping between Prop-
bank and Verbnet. In Proceedings of the 7th In-
ternational Workshop on Computational Linguistics,
Tilburg, the Netherlands.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics,
31(1):71–106.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity - Measuring the Re-
latedness of Concepts. In Proceedings of the Nine-
teenth National Conference on Artificial Intelligence
(AAAI-04).
</reference>
<page confidence="0.998143">
192
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.738557">
<title confidence="0.999205">Mapping between Compositional Semantic Representations and Lexical Semantic Resources: Towards Accurate Deep Semantic Parsing</title>
<author confidence="0.999777">Valia Yi</author>
<affiliation confidence="0.942756333333333">of Computational Linguistics, Saarland University, Research Center for Artificial Intelligence (DFKI of Computer Science, University of Freiburg,</affiliation>
<abstract confidence="0.9892184375">This paper introduces a machine learning method based on bayesian networks which is applied to the mapping between deep semantic representations and lexical semantic resources. A probabilistic model comprising Minimal Recursion Semantics (MRS) structures and lexicalist oriented semantic features is acquired. Lexical semantic roles enriching the MRS structures are inferred, which are useful to improve the accuracy of deep semantic parsing. Verb classes inference was also investigated, which, together with lexical semantic information provided by VerbNet and PropBank resources, can be substantially beneficial to the parse disambiguation task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>Proceedings of ACL-08: HLT, Short Papers (Companion Volume),</booktitle>
<pages>189--192</pages>
<marker></marker>
<rawString>Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 189–192,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Columbus</author>
</authors>
<date>2008</date>
<booktitle>c�2008 Association for Computational Linguistics</booktitle>
<location>Ohio, USA,</location>
<marker>Columbus, 2008</marker>
<rawString>Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin Baker</author>
<author>Charles Fillmore</author>
<author>John Lowe</author>
</authors>
<title>The Berkeley FrameNet project.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the ACL and 17th International Conference on Computational Linguistics,</booktitle>
<pages>86--90</pages>
<location>San Francisco, CA.</location>
<contexts>
<context position="1830" citStr="Baker et al., 1998" startWordPosition="248" endWordPosition="251">nformed structures. As a result, we have seen an emerging interest in parser evaluation based on more theoryneutral and semantically informed representations, such as dependency structures. Some approaches have even tried to acquire semantic representations without full syntactic analyses. The so-called shallow semantic parsers build basic predicate-argument structures or label semantic roles that reveal the partial meaning of sentences (Carreras and M`arquez, 2005). Manually annotated lexical semantic resources like PropBank (Palmer et al., 2005), VerbNet (Kipper-Schuler, 2005), or FrameNet (Baker et al., 1998) are usually used as gold standards for training and evaluation of such systems. In the meantime, various existing parsing systems are also adapted to provide semantic information in their outputs. The obvious advantage in such an approach is that one can derive more fine-grained representations which are not typically available from shallow semantic parsers (e.g., modality and negation, quantifiers and scopes, etc.). To this effect, various semantic representations have been proposed and used in different parsing systems. Generally speaking, such semantic representations should be capable of </context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin Baker, Charles Fillmore, and John Lowe. 1998. The Berkeley FrameNet project. In Proceedings of the 36th Annual Meeting of the ACL and 17th International Conference on Computational Linguistics, pages 86–90, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Lluis M`arquez</author>
</authors>
<title>Introduction to the CoNLL-2005 shared task: Semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL2005),</booktitle>
<pages>152--164</pages>
<location>Ann Arbor, Michigan.</location>
<marker>Carreras, M`arquez, 2005</marker>
<rawString>Xavier Carreras and Lluis M`arquez. 2005. Introduction to the CoNLL-2005 shared task: Semantic role labeling. In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL2005), pages 152–164, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
<author>Dan P Flickinger</author>
<author>Ivan A Sag</author>
</authors>
<title>Minimal recursion semantics: An introduction.</title>
<date>2006</date>
<journal>Research on Language and Computation,</journal>
<volume>3</volume>
<issue>4</issue>
<pages>332</pages>
<marker>Copestake, Flickinger, Sag, 2006</marker>
<rawString>Ann Copestake, Dan P. Flickinger, and Ivan A. Sag. 2006. Minimal recursion semantics: An introduction. Research on Language and Computation, 3(4):281– 332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
</authors>
<title>Robust minimal recursion semantics. Working Paper.</title>
<date>2006</date>
<marker>Copestake, 2006</marker>
<rawString>Ann Copestake. 2006. Robust minimal recursion semantics. Working Paper.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane D Fellbaum</author>
</authors>
<title>WordNet – An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane D. Fellbaum. 1998. WordNet – An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Flickinger</author>
<author>Jan T Lønning</author>
<author>Helge Dyvik</author>
<author>Stephan Oepen</author>
<author>Francis Bond</author>
</authors>
<title>SEM-I rational MT. Enriching deep grammars with a semantic interface for scalable machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th Machine Translation Summit,</booktitle>
<pages>165--172</pages>
<location>Phuket, Thailand.</location>
<marker>Flickinger, Lønning, Dyvik, Oepen, Bond, 2005</marker>
<rawString>Dan Flickinger, Jan T. Lønning, Helge Dyvik, Stephan Oepen, and Francis Bond. 2005. SEM-I rational MT. Enriching deep grammars with a semantic interface for scalable machine translation. In Proceedings of the 10th Machine Translation Summit, pages 165–172, Phuket, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Flickinger</author>
</authors>
<title>On building a more efficient grammar by exploiting types.</title>
<date>2002</date>
<booktitle>Collaborative Language Engineering,</booktitle>
<pages>1--17</pages>
<editor>In Stephan Oepen, Dan Flickinger, Jun’ichi Tsujii, and Hans Uszkoreit, editors,</editor>
<publisher>CSLI Publications.</publisher>
<contexts>
<context position="3086" citStr="Flickinger, 2002" startWordPosition="431" endWordPosition="432">(i.e., predicate-argument or semantic roles). However, it is non-trivial to map even the basic predicatearguments between different representations. This becomes a barrier to both sides, making the crossfertilization of systems and resources using different semantic representations very difficult. In this paper, we present a machine learning approach towards mapping between deep and shallow semantic representations. More specifically, we use Bayesian networks to acquire a statistical model that enriches the Minimal Recursion Semantics structures produced by the English Resource Grammar (ERG) (Flickinger, 2002) with VerbNet-like semantic roles. Evaluation results show that the mapping from MRS to semantic roles is reliable and beneficial to deep parsing. 2 Minimal Recursion Semantics The semantic representation we are interested in in this paper is the Minimal Recursion Semantics (MRS). Because of its underspecifiability, it has been widely used in many deep and shallow processing systems. The main assumption behind MRS is that the interesting linguistic units for computational semantics are the elementary predications (EPs), which are single relations with associated arguments (Copestake et al., 20</context>
</contexts>
<marker>Flickinger, 2002</marker>
<rawString>Dan Flickinger. 2002. On building a more efficient grammar by exploiting types. In Stephan Oepen, Dan Flickinger, Jun’ichi Tsujii, and Hans Uszkoreit, editors, Collaborative Language Engineering, pages 1– 17. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanae Fujita</author>
<author>Francis Bond</author>
<author>Stephan Oepen</author>
<author>Takaaki Tanaka</author>
</authors>
<title>Exploiting semantic information for hpsg parse selection.</title>
<date>2007</date>
<booktitle>In ACL 2007 Workshop on Deep Linguistic Processing,</booktitle>
<pages>25--32</pages>
<location>Prague, Czech Republic.</location>
<marker>Fujita, Bond, Oepen, Tanaka, 2007</marker>
<rawString>Sanae Fujita, Francis Bond, Stephan Oepen, and Takaaki Tanaka. 2007. Exploiting semantic information for hpsg parse selection. In ACL 2007 Workshop on Deep Linguistic Processing, pages 25–32, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper-Schuler</author>
</authors>
<title>VerbNet: A broadcoverage, comprehensive verb lexicon.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1796" citStr="Kipper-Schuler, 2005" startWordPosition="244" endWordPosition="245">ctic analyses to more semantically informed structures. As a result, we have seen an emerging interest in parser evaluation based on more theoryneutral and semantically informed representations, such as dependency structures. Some approaches have even tried to acquire semantic representations without full syntactic analyses. The so-called shallow semantic parsers build basic predicate-argument structures or label semantic roles that reveal the partial meaning of sentences (Carreras and M`arquez, 2005). Manually annotated lexical semantic resources like PropBank (Palmer et al., 2005), VerbNet (Kipper-Schuler, 2005), or FrameNet (Baker et al., 1998) are usually used as gold standards for training and evaluation of such systems. In the meantime, various existing parsing systems are also adapted to provide semantic information in their outputs. The obvious advantage in such an approach is that one can derive more fine-grained representations which are not typically available from shallow semantic parsers (e.g., modality and negation, quantifiers and scopes, etc.). To this effect, various semantic representations have been proposed and used in different parsing systems. Generally speaking, such semantic rep</context>
</contexts>
<marker>Kipper-Schuler, 2005</marker>
<rawString>Karin Kipper-Schuler. 2005. VerbNet: A broadcoverage, comprehensive verb lexicon. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Loper</author>
<author>Szu ting Yi</author>
<author>Martha Palmer</author>
</authors>
<title>Combining lexical resources: Mapping between Propbank and Verbnet.</title>
<date>2007</date>
<booktitle>In Proceedings of the 7th International Workshop on Computational Linguistics,</booktitle>
<location>Tilburg, the Netherlands.</location>
<marker>Loper, Yi, Palmer, 2007</marker>
<rawString>Edward Loper, Szu ting Yi, and Martha Palmer. 2007. Combining lexical resources: Mapping between Propbank and Verbnet. In Proceedings of the 7th International Workshop on Computational Linguistics, Tilburg, the Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: An Annotated Corpus of Semantic Roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="1764" citStr="Palmer et al., 2005" startWordPosition="238" endWordPosition="241"> shift of focus from pure syntactic analyses to more semantically informed structures. As a result, we have seen an emerging interest in parser evaluation based on more theoryneutral and semantically informed representations, such as dependency structures. Some approaches have even tried to acquire semantic representations without full syntactic analyses. The so-called shallow semantic parsers build basic predicate-argument structures or label semantic roles that reveal the partial meaning of sentences (Carreras and M`arquez, 2005). Manually annotated lexical semantic resources like PropBank (Palmer et al., 2005), VerbNet (Kipper-Schuler, 2005), or FrameNet (Baker et al., 1998) are usually used as gold standards for training and evaluation of such systems. In the meantime, various existing parsing systems are also adapted to provide semantic information in their outputs. The obvious advantage in such an approach is that one can derive more fine-grained representations which are not typically available from shallow semantic parsers (e.g., modality and negation, quantifiers and scopes, etc.). To this effect, various semantic representations have been proposed and used in different parsing systems. Gener</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An Annotated Corpus of Semantic Roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>WordNet::Similarity - Measuring the Relatedness of Concepts.</title>
<date>2004</date>
<booktitle>In Proceedings of the Nineteenth National Conference on Artificial Intelligence (AAAI-04).</booktitle>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. WordNet::Similarity - Measuring the Relatedness of Concepts. In Proceedings of the Nineteenth National Conference on Artificial Intelligence (AAAI-04).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>