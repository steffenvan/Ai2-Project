<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9956205">
PENS: A Machine-aided English Writing System
for Chinese Users
</title>
<author confidence="0.999545">
Ting Liu1 Ming Zhou Jianfeng Gao Endong Xun Changning Huang
</author>
<affiliation confidence="0.8860985">
Natural Language Computing Group, Microsoft Research China, Microsoft Corporation
5F, Beijing Sigma Center
</affiliation>
<address confidence="0.891913">
100080 Beijing, P.R.C.
</address>
<email confidence="0.955084">
{ i-liutin, mingzhou, jfgao, i-edxun, cnhuang@microsoft.com }
</email>
<sectionHeader confidence="0.992353" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999955">
Writing English is a big barrier for most
Chinese users. To build a computer-aided system
that helps Chinese users not only on spelling
checking and grammar checking but also on
writing in the way of native-English is a
challenging task. Although machine translation is
widely used for this purpose, how to find an
efficient way in which human collaborates with
computers remains an open issue. In this paper,
based on the comprehensive study of Chinese
users requirements, we propose an approach to
machine aided English writing system, which
consists of two components: 1) a statistical
approach to word spelling help, and 2) an
information retrieval based approach to
intelligent recommendation by providing
suggestive example sentences. Both components
work together in a unified way, and highly
improve the productivity of English writing. We
also developed a pilot system, namely PENS
(Perfect ENglish System). Preliminary
experiments show very promising results.
</bodyText>
<sectionHeader confidence="0.960867" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.99955175">
With the rapid development of the Internet,
writing English becomes daily work for
computer users all over the world. However, for
Chinese users who have significantly different
culture and writing style, English writing is a big
barrier. Therefore, building a machine-aided
English writing system, which helps Chinese
users not only on spelling checking and grammar
checking but also on writing in the way of
native-English, is a very promising task.
Statistics shows that almost all Chinese
users who need to write in English1 have enough
knowledge of English that they can easily tell the
difference between two sentences written in
Chinese-English and native-English, respectively.
Thus, the machine-aided English writing system
should act as a consultant that provide various
kinds of help whenever necessary, and let users
play the major role during writing. These helps
include:
</bodyText>
<listItem confidence="0.966726">
1) Spelling help: help users input hard-to-spell
words, and check the usage in a certain
context simultaneously;
2) Example sentence help: help users refine the
writing by providing perfect example
sentences.
</listItem>
<bodyText confidence="0.999746368421053">
Several machine-aided approaches have
been proposed recently. They basically fall into
two categories, 1) automatic translation, and 2)
translation memory. Both work at the sentence
level. While in the former, the translation is not
readable even after a lot of manually editing. The
latter works like a case-based system, in that,
given a sentence, the system retrieve similar
sentences from translation example database, the
user then translates his sentences by analogy. To
build a computer-aided English writing system
that helps Chinese users on writing in the way of
native-English is a challenging task. Machine
translation is widely used for this purpose, but
how to find an efficient way in which human
collaborates well with computers remains an
open issue. Although the quality of fully
automatic machine translation at the sentence
level is by no means satisfied, it is hopeful to
</bodyText>
<note confidence="0.4004455">
1 Now Ting Liu is an associate professor in Harbin
Institute of Technology, P.R.C.
</note>
<bodyText confidence="0.999943857142857">
provide relatively acceptable quality translations
at the word or short phrase level. Therefore, we
can expect that combining word/phrase level
automatic translation with translation memory
will achieve a better solution to machine-aided
English writing system [Zhou, 95].
In this paper, we propose an approach to
machine aided English writing system, which
consists of two components: 1) a statistical
approach to word spelling help, and 2) an
information retrieval based approach to
intelligent recommendation by providing
suggestive example sentences. Both components
work together in a unified way, and highly
improve the productivity of English writing. We
also develop a pilot system, namely PENS.
Preliminary experiments show very promising
results.
The rest of this paper is structured as follows.
In section 2 we give an overview of the system,
introduce the components of the system, and
describe the resources needed. In section 3, we
discuss the word spelling help, and focus the
discussion on Chinese pinyin to English word
translation. In addition, we describe various
kinds of word level help functions, such as
automatic translation of Chinese word in the form
of either pinyin or Chinese characters, and
synonym suggestion, etc. We also describe the
user interface briefly. In section 4, an effective
retrieval algorithm is proposed to implement the
so-called intelligent recommendation function. In
section 5, we present preliminary experimental
results. Finally, concluding remarks is given in
section 6.
</bodyText>
<sectionHeader confidence="0.960574" genericHeader="method">
1 System Overview
</sectionHeader>
<subsectionHeader confidence="0.99353">
1.1 System Architecture
</subsectionHeader>
<figureCaption confidence="0.915246">
Figure 1 System Architecture
</figureCaption>
<bodyText confidence="0.9989651">
There are two modules in PENS. The first is
called the spelling help. Given an English word,
the spelling help performs two functions, 1)
retrieving its synonym, antonym, and thesaurus;
or 2) automatically giving the corresponding
translation of Chinese words in the form of
Chinese characters or pinyin. Statistical machine
translation techniques are used for this translation,
and therefore a Chinese-English bilingual
dictionary (MRD), an English language model,
and an English-Chinese word- translation model
(TM) are needed. The English language model is
a word trigram model, which consists of
247,238,396 trigrams, and the vocabulary used
contains 58541 words. The MRD dictionary
contains 115,200 Chinese entries as well as their
corresponding English translations, and other
information, such as part-of-speech, semantic
classification, etc. The TM is trained from a
word-aligned bilingual corpus, which occupies
approximately 96,362 bilingual sentence pairs.
The second module is an intelligent
recommendation system. It employs an effective
sentence retrieval algorithm on a large bilingual
corpus. The input is a sequence of keywords or a
short phrase given by users, and the output is
limited pairs bilingual sentences expressing
relevant meaning with users’ query, or just a few
pairs of bilingual sentences with syntactical
relevance.
</bodyText>
<subsectionHeader confidence="0.987317">
1.2 Bilingual Corpus Construction
</subsectionHeader>
<bodyText confidence="0.999830333333333">
We have collected bilingual texts extracted
from World Wide Web bilingual sites,
dictionaries, books, bilingual news and
magazines, and product manuals. The size of the
corpus is 96,362 sentence pairs. The corpus is
used in the following three cases:
</bodyText>
<listItem confidence="0.998820142857143">
1) Act as translation memory to support the
Intelligent Recommendation Function;
2) To be used to acquire English-Chinese
translation model to support translation at
word and phrase level;
3) To be used to extract bilingual terms to enrich
the Chinese-English MRD;
</listItem>
<bodyText confidence="0.99898125">
To construct a sentence aligned bilingual
corpus, we first use an alignment algorithm doing
the automatic alignment and then the alignment
result are corrected.
There have been quite a number of recent
papers on parallel text alignment. Lexically based
techniques use extensive online bilingual
lexicons to match sentences [Chen 93]. In
contrast, statistical techniques require almost no
prior knowledge and are based solely on the
lengths of sentences, i.e. length-based alignment
method. We use a novel method to incorporate
both approaches [Liu, 95]. First, the rough result
is obtained by using the length-based method.
Then anchors are identified in the text to reduce
the complexity. An anchor is defined as a block
that consists of n successive sentences. Our
experiments show best performance when n=3.
Finally, a small, restricted set of lexical cues is
applied to obtain for further improvement.
</bodyText>
<subsectionHeader confidence="0.820971">
1.3 Translation Model Training
</subsectionHeader>
<bodyText confidence="0.9999878">
Chinese sentences must be segmented
before word translation training, because written
Chinese consists of a character stream without
space between words. Therefore, we use a
wordlist, which consists of 65502 words, in
conjunction with an optimization procedure
described in [Gao, 2000]. The bilingual training
process employs a variant of the model in [Brown,
1993] and as such is based on an iterative EM
(expectation-maximization) procedure for
maximizing the likelihood of generating the
English given the Chinese portion. The output of
the training process is a set of potential English
translations for each Chinese word, together with
the probability estimate for each translation.
</bodyText>
<subsectionHeader confidence="0.9951995">
1.4 Extraction of Bilingual
Domain-specific Terms
</subsectionHeader>
<bodyText confidence="0.9997815625">
A domain-specific term is defined as a string
that consists of more than one successive word
and has certain occurrences in a text collection
within a specific domain. Such a string has a
complete meaning and lexical boundaries in
semantics; it might be a compound word, phrase
or linguistic template. We use two steps to extract
bilingual terms from sentence aligned corpus.
First we extract Chinese monolingual terms from
Chinese part of the corpus by a similar method
described in [Chien, 1998], then we extract the
English corresponding part by using the word
alignment information. A candidate list of the
Chinese-English bilingual terms can be obtained
as the result. Then we will check the list and add
the terms into the dictionary.
</bodyText>
<sectionHeader confidence="0.718097" genericHeader="method">
2 Spelling Help
</sectionHeader>
<bodyText confidence="0.999964666666667">
The spelling help works on the word or
phrase level. Given an English word or phrase, it
performs two functions, 1) retrieving
corresponding synonyms, antonyms, and
thesaurus; and 2) automatically giving the
corresponding translation of Chinese words in
the form of Chinese characters or pinyin. We will
focus our discussion on the latter function in the
section.
To use the latter function, the user may input
Chinese characters or just input pinyin. It is not
very convenient for Chinese users to input
Chinese characters by an English keyboard.
Furthermore the user must switch between
English input model and Chinese input model
time and again. These operations will interrupt
his train of thought. To avoid this shortcoming,
our system allows the user to input pinyin instead
of Chinese characters. The pinyin can be
translated into English word directly.
Let us take a user scenario for an example to
show how the spelling help works. Suppose that a
user input a Chinese word “” in the form of
pinyin, say “wancheng”, as shown in figure1-1.
</bodyText>
<figureCaption confidence="0.766625">
Figure 1-1
</figureCaption>
<bodyText confidence="0.99994125">
PENS is able to detect whether a string is a
pinyin string or an English string automatically.
For a pinyin string, PENS tries to translate it into
the corresponding English word or phrase
directly. The mapping from pinyin to Chinese
word is one-to-many, so does the mapping from
Chinese word to English words. Therefore, for
each pinyin string, there are alternative
translations. PENS employs a statistical approach
to determine the correct translation. PENS also
displays the corresponding Chinese word or
phrase for confirmation, as shown in figure 1-2.
</bodyText>
<equation confidence="0.858353136363636">
Figure 1-2
, EW1
, EW2
P PY EW
(  |ij
=
P PY EW EW EW
(  |, , )
ij 1 2
We get
2.1 Word Translation Algorithm
based on Statistical LM and TM
)
P CW EW P PY CW
(  |) ( |
×
i ij i
)
, EW2
×P EW EW
( |
ij 1
</equation>
<bodyText confidence="0.9985096">
If the user is not satisfied with the English
word determined by PENS, he can browse other
candidates as well as their bilingual example
sentences, and select a better one, as shown in
figure 1-3.
</bodyText>
<equation confidence="0.952953">
Figure 1-3
P EW PY EW EW
(  |, ,
ij 1 2
P PY EW
(  |1
Since the denominator is independent of EWij, we
rewrite (2-1) as
∝ (2-2)
P PY EW EW EW P EW EW EW
(  |, , ) (  |, )
×
ij 1 2 ij 1
</equation>
<bodyText confidence="0.833755">
Since CWi is a bridge which connect the pinyin
and the English translation, we introduce Chinese
word CWi into
</bodyText>
<equation confidence="0.99631275">
P(PY |EWij,EW1, EW2)
P EW PY EW EW
(  |, , )
ij 1 2
</equation>
<bodyText confidence="0.9997948">
Suppose that a user input two English words,
say EW1 and EW2, and then a pinyin string, say
PY. For PY, all candidate Chinese words are
determined by looking up a Pinyin-Chinese
dictionary. Then, a list of candidate English
translations is obtained according to a MRD.
These English translations are English words of
their original form, while they should be of
different forms in different contexts. We exploit
morphology for this purpose, and expand each
word to all possible forms. For instance,
inflections of “go” may be “went”, and “gone”.
In what follows, we will describe how to
determine the proper translation among the
candidate list.
</bodyText>
<equation confidence="0.989171857142857">
P CW EW EW EW P PY CW EW EW EW
(  |, , ) (  |, , , )
×
i ij 1 2 i ij 1 2
PY EW EW EW
, , , )
ij 1 2
</equation>
<bodyText confidence="0.994554666666667">
For simplicity, we assume that a Chinese word
doesn’t depends on the translation context, so we
can get the following approximate equation:
</bodyText>
<equation confidence="0.78227025">
P CW EW EW EW P CW EW
(  |, 1 , 2) (  |)
≈
i ij i ij
</equation>
<bodyText confidence="0.759392">
We can also assume that the pinyin of a Chinese
word is not concerned in the corresponding
English translation, namely:
</bodyText>
<equation confidence="0.918219">
P PY CW EW EW EW P PY CW
(  |, , , ) ( |
ij ≈
i 1 2 i
</equation>
<bodyText confidence="0.993278666666667">
It is almost impossible that two Chinese words
correspond to the same pinyin and the same
English translation, so we can suppose that:
</bodyText>
<equation confidence="0.769844">
P CW i PY EW ij EW 1 EW 2 ≈
(  |, , , ) 1
</equation>
<bodyText confidence="0.993704">
Therefore, we get the approximation of (2-3) as
follows:
</bodyText>
<equation confidence="0.996334111111111">
) = (2-4)
(2-3)
P CW
( |
i
)
P PY EW EW EW
(  |, 1 , 2
ij
</equation>
<bodyText confidence="0.996324666666667">
As shown in Figure 2-1, we assume that the
most proper translation of PY is the English word
with the highest conditional probability among
all leaf nodes, that is
According to Bayes’ law, the conditional
probability is estimated by
</bodyText>
<equation confidence="0.9848942">
) = (2-5)
P CW EW P PY CW
(  |) (  |)
×
i ij i
</equation>
<bodyText confidence="0.9992195">
where P(CWi |EWij) is the translation model, and
can be got from bilingual corpus, and P(PY  |CWi)
</bodyText>
<figure confidence="0.712588333333333">
)
(2-1)
)
=
)
, EW2
×P(EWij  |EW1
)
,EW2
</figure>
<figureCaption confidence="0.878781">
Figure 2-1: Word-level Pinyin-English
Translation
</figureCaption>
<bodyText confidence="0.741983">
According to formula (2-2) and (2-4), we get:
</bodyText>
<equation confidence="0.98995925">
P EW PY EW
(  |,
ij 1
, EW2
</equation>
<bodyText confidence="0.963039857142857">
is the polyphone model, here we suppose
P(PY|CWi) = 1, and P(EWij  |EW1, EW2) is the
English trigram language model.
To sum up, as indicated in (2-6), the spelling help
find the most proper translation of PY by
retrieving the English word with the highest
conditional probability.
</bodyText>
<equation confidence="0.8470918">
arg max ( |
P EW PY
EWij
arg max P(CWi  |EWij ) ❑P(EWij  |EW1 ,EW2)
EWij
</equation>
<sectionHeader confidence="0.989265" genericHeader="method">
3 Intelligent Recommendation
</sectionHeader>
<bodyText confidence="0.999965416666666">
The intelligent recommendation works on
the sentence level. When a user input a sequence
of Chinese characters, the character string will be
firstly segmented into one or more words. The
segmented word string acts as the user query in
IR. After query expansion, the intelligent
recommendation employs an effective sentence
retrieval algorithm on a large bilingual corpus,
and retrieves a pair (or a set of pairs) of bilingual
sentences related to the query. All the retrieved
sentence pairs are ranked based on a scoring
strategy.
</bodyText>
<subsectionHeader confidence="0.998354">
3.1 Query Expansion
</subsectionHeader>
<bodyText confidence="0.99998075">
Suppose that a user query is of the form CW1,
CW2, ... , CWm. We then list all synonyms for
each word of the queries based on a Chinese
thesaurus, as shown below.
</bodyText>
<equation confidence="0.98536825">
CW11
12
FFF FFF FFF FFF
1n1 CW2n2 FFF CWmn m
</equation>
<bodyText confidence="0.999537">
We can obtain an expanded query by
substituting a word in the query with its synonym.
To avoid over-generation, we restrict that only
one word is substituted at each time.
Let us take the query “ ” for an example.
The synonyms list is as follows:
=
=
The query consists of two words. By substituting
the first word, we get expanded queries, such as
</bodyText>
<equation confidence="0.534617">
“ ”“ ”“ ”, etc, and by
</equation>
<bodyText confidence="0.969477428571429">
substituting the second word, we get other
expanded queries, such as “ ”“
”“ ”,etc.
Then we select the expanded query, which is
used for retrieving example sentence pairs, by
estimating the mutual information of words with
the query. It is indicated as follows
</bodyText>
<equation confidence="0.763193666666667">
m
arg max ∑ MI (CW k, CW ij )
i,
</equation>
<bodyText confidence="0.998672166666667">
where CWk is a the kth Chinese word in the query,
and CWij is the jth synonym of the i-th Chinese
word. In the above example, “ ” is
selected. The selection well meets the common
sense. Therefore, bilingual example sentences
containing “ ” will be retrieved as well.
</bodyText>
<subsectionHeader confidence="0.999908">
3.2 Ranking Algorithm
</subsectionHeader>
<bodyText confidence="0.994480333333333">
The input of the ranking algorithm is a
query Q, as described above, Q is a Chinese
word string, as shown below
</bodyText>
<equation confidence="0.4520974">
Q= T1,T2,T3,...Tk
The output is a set of relevant bilingual
example sentence pairs in the form of,
S={(Chinsent, Engsent)  |Relevance(Q,Chinsent)
&gt; Relevance(Q,Engsent) &gt;
</equation>
<bodyText confidence="0.9998982">
where Chinsent is a Chinese sentence, and
Engsent is an English sentence, and
For each sentence, the relevance score is
computed in two parts, 1) the bonus which
represents the similarity of input query and the
target sentence, and 2) the penalty, which
represents the dissimilarity of input query and the
target sentence.
The bonus is computed by the following formula:
Where
</bodyText>
<equation confidence="0.769744">
Otf ..) ❑log(n/df .)/Li
Y J
</equation>
<bodyText confidence="0.965567">
Wj is the weight of the jth word in query Q, which
will be described later, tfij is the number of the jth
word occurring in sentence i, n is the number of
the sentences in corpus, dfj is the number of
</bodyText>
<figure confidence="0.9094984375">
, EW1 , EW2 ) 11 (2-6)
CW
CW FFF CW
21 m1
CW FFF CW
22 m 2
CW
j k 11 1
k 11 i
Bonus . ❑ m
a ∑ log( W j
j ❑1
sentence which contains Wj, and Li is the number
of word in the ith sentence.
4 Experimental Results &amp;
Evaluation
</figure>
<bodyText confidence="0.999708">
The above formula contains only the
algebraic similarities. To take the geometry
similarity into consideration, we designed a
penalty formula. The idea is that we use the
editing distance to compute that geometry
similarity.
</bodyText>
<subsectionHeader confidence="0.341752">
Ri ❑ Bonusi ❑Penalty
</subsectionHeader>
<bodyText confidence="0.995353666666667">
Suppose the matched word list between query Q
and a sentence are represented as A and B
respectively
</bodyText>
<equation confidence="0.98894">
A1, A2, A3, ... , Al
B1, B2, B3, ... , Bm
</equation>
<bodyText confidence="0.999767166666667">
The editing distance is defined as the
number of editing operation to revise B to A. The
penalty will increase for each editing operation,
but the score is different for different word
category. For example, the penalty will be serious
when operating a verb than operating a noun
</bodyText>
<equation confidence="0.9758195">
h
Penaltyi L1 ∑ log( Wj&apos; L1Ej) L1log(n / df j ) / L
</equation>
<bodyText confidence="0.9921974">
where
Wj’ is the penalty of the jth word
Ej the editing distance
We define the score and penalty for each kind of
part-or-speech
</bodyText>
<table confidence="0.995053266666667">
POS Score Penalty
Noun 6 6
Verb 10 10
Adjective 8 8
Adverb 8 8
Preposition 8 8
Conjuction 4 4
Digit 4 4
Digit-classifer 4 4
Classifer 4 4
Exclamation 4 4
Pronoun 4 4
Auxilary 6 6
Post-preposition 6 6
Idioms 6 6
</table>
<bodyText confidence="0.996123">
We then select the first
In this section, we will report the primary
experimental results on 1) word-level
pinyin-English translation, and 2) example
sentences retrieval.
</bodyText>
<subsectionHeader confidence="0.814537">
4.1 Word-level Pinyin-English
Translation
</subsectionHeader>
<bodyText confidence="0.9986294">
Firstly, we built a testing set based on the
word aligned bilingual corpus automatically.
Suppose that there is a word-aligned bilingual
sentence pair, and every Chinese word is labelled
with Pinyin. See Figure 4-1.
</bodyText>
<figureCaption confidence="0.735313">
Figure 5-1: An example of aligned bilingual
</figureCaption>
<bodyText confidence="0.920146071428571">
sentence
If we substitute an English word with the piny
Figure 4-1: An example of aligned bilingual
sentence
If we substitute an English word with the
pinyin of the Chinese word which the English
word is aligned to, we can get a testing example
for word-level Pinyin-English translation. Since
the user only cares about how to write content
words, rather than function words, we should
skip function words in the English sentence. In
this example, suppose EW1 is a function word,
EW2 and EW3 are content words, thus the
extracted testing examples are:
</bodyText>
<equation confidence="0.9623145">
EW1 PY2 (CW2, EW2)
EW1 EW2 PY4 (CW4, EW3)
</equation>
<bodyText confidence="0.9990988">
The Chinese words and English words in
brackets are standard answers to the pinyin. We
can get the precision of translation by comparing
the standard answers with the answers obtained
by the Pinyin-English translation module.
</bodyText>
<figure confidence="0.832879333333333">
i
i
1
�
j
is
</figure>
<bodyText confidence="0.998644">
The standard testing set includes 1198 testing
sentences, and all the pinyins are polysyllabic.
The experimental result is shown in Figure 4-2.
</bodyText>
<table confidence="0.9865149">
Shoot Rate
Chinese Word 0.964942
English Top 1 0.794658
English Top 5 0.932387
English Top 1 0.606845
(Considering
morphology)
English Top 5 0.834725
(Considering
morphology)
</table>
<figureCaption confidence="0.7673205">
Figure 4-2: Testing of Pinyin-English Word-level
Translation
</figureCaption>
<subsectionHeader confidence="0.966212">
4.2 Example Sentence Retrieval
</subsectionHeader>
<bodyText confidence="0.998527266666667">
We built a standard example sentences set
which consists of 964 bilingual example sentence
pairs. We also created 50 Chinese-phrase queries
manually based on the set. Then we labelled
every sentence with the 50 queries. For instance,
let’s say that the example sentence is
(He drew
the conclusion by building on his own
investigation.)
After labelling, the corresponding queries are “
”, and “ ”, that is, when a user
input these queries, the above example sentence
should be picked out.
After we labelled all 964 sentences, we
performed the sentence retrieval module on the
sentence set, that is, PENS retrieved example
sentences for each of the 50 queries. Therefore,
for each query, we compared the sentence set
retrieved by PENS with the sentence labelled
manually, and evaluate the performance by
estimating the precision and the recall.
Let A denotes the number of sentences which is
selected by both human and the machine, B
denotes the number of sentences which is
selected only by the machine, and C denotes the
number of sentences which is selected only by
human.
The precision of the retrieval to query i, say
Pi, is estimated by Pi = A / B and the recall Ri, is
estimated by Ri = A/C. The average precision
</bodyText>
<equation confidence="0.905964666666667">
50
∑ Pi
P = i=1 , and the average recall is
50
50
∑ Ri
= i=1 .
R
50
</equation>
<bodyText confidence="0.999618">
The experimental results are P = 83.3%, and
R = 55.7%. The user only cares if he could obtain
a useful example sentence, and it is unnecessary
for the system to find out all the relevant
sentences in the bilingual sentence corpus.
Therefore, example sentence retrieval in PENS is
different from conventional text retrieval at this
point.
</bodyText>
<sectionHeader confidence="0.951285" genericHeader="conclusions">
Conclusion
</sectionHeader>
<bodyText confidence="0.9999829">
In this paper, based on the comprehensive
study of Chinese users requirements, we propose
a unified approach to machine aided English
writing system, which consists of two
components: 1) a statistical approach to word
spelling help, and 2) an information retrieval
based approach to intelligent recommendation by
providing suggestive example sentences. While
the former works at the word or phrase level, the
latter works at the sentence level. Both
components work together in a unified way, and
highly improve the productivity of English
writing.
We also develop a pilot system, namely
PENS, where we try to find an efficient way in
which human collaborate with computers.
Although many components of PENS are under
development, primary experiments on two
standard testing sets have already shown very
promising results.
</bodyText>
<sectionHeader confidence="0.999172" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998631355555556">
Ming Zhou, Sheng Li, Tiejun Zhao, Min Zhang,
Xiaohu Liu, Meng Cai 1995 . DEAR: A
translator’s workstation. In Proceedings of
NLPRS’95, Dec. 5-7, Seoul.
Xin Liu, Ming Zhou, Shenghuo Zhu, Changning
Huang (1998), Aligning sentences in parallel
corpora using self-extracted lexical information,
Chinese Journal of Computers (in Chinese), 1998,
Vol. 21 (Supplement):151-158.
Chen, Stanley F.(1993). Aligning sentences in
bilingual corpora using lexical infromation. In
Proceedings of the 31st Annual Conference of the
Association for Computational Linguistics, 9-16,
Columbus, OH.
Brown. P.F., Jennifer C. Lai, and R.L. Merce. (1991).
Aligning sentences in parallel corpora.In
Proceedings of the 29th Annual Conference of the
Association for Computational Linguistics,
169-176,Berkeley.
Dekai Wu, Xuanyin Xia (1995). Large-scale
automatic extraction of an English-Chinese
translation lexicon. Machine Translation, 9:3-4,
285-313 (1995)
Church, K.W.(1993), Char-align. A program for
aligning parallel texts at the character level. In
Proceedings of the 31st Annual Conference of the
Association for Computational Linguistics, 1-8,
Columbus, OH.
Dagan, I., K.W. Church, and W.A. Gale (1993)
Robust bilingual word alignment for machine aided
translation. In Proceedings of the workshop on Very
Large Corpora, 69-85, Kyoto, Auguest.
Jianfeng Gao, Han-Feng Wang, Mingjing Li, and
Kai-Fu Lee, 2000. A Unified Approach to Statistical
Language Modeling for Chinese. In IEEE,
ICASPP2000.
Brown, P. F., S. A. DellaPietra, V.J. Dellapietra, and
R.L.Mercer. 1993. The Mathematics of Statistical
Machine Translation: Parameter Estimation.
Computational Linguistics, 19(2): 263-311
Lee-Feng Chien, 1998. PAT-Tree-Based Adaptive
Keyphrase Extraction for Intelligent Chinese
Information Retrieval. Special issue on
“Information Retrieval with Asian Language”
Information Processing and Management, 1998.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.818181">
<title confidence="0.9995475">PENS: A Machine-aided English Writing System for Chinese Users</title>
<author confidence="0.973961">Ming Zhou Jianfeng Gao Endong Xun Changning Huang</author>
<affiliation confidence="0.998546">Natural Language Computing Group, Microsoft Research China, Microsoft Corporation</affiliation>
<address confidence="0.947175">5F, Beijing Sigma Center 100080 Beijing, P.R.C.</address>
<email confidence="0.964366">i-liutin,mingzhou,jfgao,i-edxun,cnhuang@microsoft.com}</email>
<abstract confidence="0.998293130434783">Writing English is a big barrier for most Chinese users. To build a computer-aided system that helps Chinese users not only on spelling checking and grammar checking but also on writing in the way of native-English is a challenging task. Although machine translation is widely used for this purpose, how to find an efficient way in which human collaborates with computers remains an open issue. In this paper, based on the comprehensive study of Chinese users requirements, we propose an approach to machine aided English writing system, which consists of two components: 1) a statistical approach to word spelling help, and 2) an information retrieval based approach to intelligent recommendation by providing suggestive example sentences. Both components work together in a unified way, and highly improve the productivity of English writing. We also developed a pilot system, namely PENS (Perfect ENglish System). Preliminary experiments show very promising results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Ming Zhou</author>
<author>Sheng Li</author>
</authors>
<location>Tiejun Zhao, Min Zhang,</location>
<marker>Zhou, Li, </marker>
<rawString>Ming Zhou, Sheng Li, Tiejun Zhao, Min Zhang,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaohu Liu</author>
</authors>
<title>Meng Cai</title>
<date>1995</date>
<booktitle>In Proceedings of NLPRS’95,</booktitle>
<marker>Liu, 1995</marker>
<rawString>Xiaohu Liu, Meng Cai 1995 . DEAR: A translator’s workstation. In Proceedings of NLPRS’95, Dec. 5-7, Seoul.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Liu</author>
</authors>
<title>Ming Zhou, Shenghuo Zhu, Changning Huang</title>
<date>1998</date>
<journal>Chinese Journal of Computers (in Chinese),</journal>
<volume>21</volume>
<pages>151--158</pages>
<marker>Liu, 1998</marker>
<rawString>Xin Liu, Ming Zhou, Shenghuo Zhu, Changning Huang (1998), Aligning sentences in parallel corpora using self-extracted lexical information, Chinese Journal of Computers (in Chinese), 1998, Vol. 21 (Supplement):151-158.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Stanley Chen</author>
</authors>
<title>F.(1993). Aligning sentences in bilingual corpora using lexical infromation.</title>
<booktitle>In Proceedings of the 31st Annual Conference of the Association for Computational Linguistics,</booktitle>
<pages>9--16</pages>
<location>Columbus, OH.</location>
<marker>Chen, </marker>
<rawString>Chen, Stanley F.(1993). Aligning sentences in bilingual corpora using lexical infromation. In Proceedings of the 31st Annual Conference of the Association for Computational Linguistics, 9-16, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer C Lai P F</author>
<author>R L Merce</author>
</authors>
<title>Aligning sentences in parallel corpora.In</title>
<date>1991</date>
<booktitle>Proceedings of the 29th Annual Conference of the Association for Computational Linguistics,</booktitle>
<pages>169--176</pages>
<marker>F, Merce, 1991</marker>
<rawString>Brown. P.F., Jennifer C. Lai, and R.L. Merce. (1991). Aligning sentences in parallel corpora.In Proceedings of the 29th Annual Conference of the Association for Computational Linguistics, 169-176,Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Xuanyin Xia</title>
<date>1995</date>
<journal>Machine Translation,</journal>
<volume>9</volume>
<pages>285--313</pages>
<marker>Wu, 1995</marker>
<rawString>Dekai Wu, Xuanyin Xia (1995). Large-scale automatic extraction of an English-Chinese translation lexicon. Machine Translation, 9:3-4, 285-313 (1995)</rawString>
</citation>
<citation valid="false">
<authors>
<author>K W Church</author>
<author>Char-align</author>
</authors>
<title>A program for aligning parallel texts at the character level.</title>
<booktitle>In Proceedings of the 31st Annual Conference of the Association for Computational Linguistics,</booktitle>
<pages>1--8</pages>
<location>Columbus, OH.</location>
<marker>Church, Char-align, </marker>
<rawString>Church, K.W.(1993), Char-align. A program for aligning parallel texts at the character level. In Proceedings of the 31st Annual Conference of the Association for Computational Linguistics, 1-8, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>K W Church</author>
<author>W A Gale</author>
</authors>
<title>Robust bilingual word alignment for machine aided translation.</title>
<date>1993</date>
<booktitle>In Proceedings of the workshop on Very Large Corpora,</booktitle>
<pages>69--85</pages>
<location>Kyoto, Auguest.</location>
<marker>Dagan, Church, Gale, 1993</marker>
<rawString>Dagan, I., K.W. Church, and W.A. Gale (1993) Robust bilingual word alignment for machine aided translation. In Proceedings of the workshop on Very Large Corpora, 69-85, Kyoto, Auguest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Han-Feng Wang</author>
<author>Mingjing Li</author>
<author>Kai-Fu Lee</author>
</authors>
<title>A Unified Approach to Statistical Language Modeling for Chinese. In</title>
<date>2000</date>
<booktitle>IEEE, ICASPP2000.</booktitle>
<marker>Gao, Wang, Li, Lee, 2000</marker>
<rawString>Jianfeng Gao, Han-Feng Wang, Mingjing Li, and Kai-Fu Lee, 2000. A Unified Approach to Statistical Language Modeling for Chinese. In IEEE, ICASPP2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A DellaPietra</author>
<author>V J Dellapietra</author>
<author>R L Mercer</author>
</authors>
<date>1993</date>
<journal>The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>263--311</pages>
<marker>Brown, DellaPietra, Dellapietra, Mercer, 1993</marker>
<rawString>Brown, P. F., S. A. DellaPietra, V.J. Dellapietra, and R.L.Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2): 263-311</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lee-Feng Chien</author>
</authors>
<title>PAT-Tree-Based Adaptive Keyphrase Extraction for Intelligent Chinese Information Retrieval.</title>
<date>1998</date>
<booktitle>Special issue on “Information Retrieval with Asian Language” Information Processing and Management,</booktitle>
<contexts>
<context position="9043" citStr="Chien, 1998" startWordPosition="1362" endWordPosition="1363">rd, together with the probability estimate for each translation. 1.4 Extraction of Bilingual Domain-specific Terms A domain-specific term is defined as a string that consists of more than one successive word and has certain occurrences in a text collection within a specific domain. Such a string has a complete meaning and lexical boundaries in semantics; it might be a compound word, phrase or linguistic template. We use two steps to extract bilingual terms from sentence aligned corpus. First we extract Chinese monolingual terms from Chinese part of the corpus by a similar method described in [Chien, 1998], then we extract the English corresponding part by using the word alignment information. A candidate list of the Chinese-English bilingual terms can be obtained as the result. Then we will check the list and add the terms into the dictionary. 2 Spelling Help The spelling help works on the word or phrase level. Given an English word or phrase, it performs two functions, 1) retrieving corresponding synonyms, antonyms, and thesaurus; and 2) automatically giving the corresponding translation of Chinese words in the form of Chinese characters or pinyin. We will focus our discussion on the latter </context>
</contexts>
<marker>Chien, 1998</marker>
<rawString>Lee-Feng Chien, 1998. PAT-Tree-Based Adaptive Keyphrase Extraction for Intelligent Chinese Information Retrieval. Special issue on “Information Retrieval with Asian Language” Information Processing and Management, 1998.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>