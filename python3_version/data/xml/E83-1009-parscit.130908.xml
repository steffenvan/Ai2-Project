<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<bodyText confidence="0.364630833333333">
A PROLOG IMPLEMENTATION OF LEXICAL FUNCTIONAL GRAMMAR
AS. A BASE FOR A NATURAL LANGUAGE PROCESSING SYSTEM
Werner Frey and Uwe Reyle
Department of Linguistics
University of Stuttgart
W-Germany
</bodyText>
<sectionHeader confidence="0.746722" genericHeader="method">
O. ABSTRACT
</sectionHeader>
<bodyText confidence="0.96632">
The atm of this paper is to present parts of our system [2],
which is to construct a database out of a narrative natural
language text. We think the parts are of interest in their awn.
The paper consists of three sections:
(I) We give a detailed description of the PROLOG -
implementation of the parser which is based on the theory of
lexical functional grammar (LR). The parser covers the
fragment described in [1,44]. I.e., it is able to analyse
constructions involving functional control and long distance
dependencies. We will to show that
- PROLOG provides an efficient tool for LFC,implementation: a
phrase structure rule annotated with functional schemata likeS4
WP veis to be interpreted as, first, identifying the special
tts414 t.
grammatical relation of subject position of any sentence
analyzed by this clause to be the NP appearing in it, and
second, as identifying all grammatical relations of the sentence
with those of the VP. This universal interpretation of the
LFCfaretavariables t and 4, corresponds to the universal
quantification of variables appearing in PROIOG-clauses. The
procedural semantics of PROLOG is such that the instantiation of
the variables in a clause is inherited from the instantiation
given by its subgoals, if they succeed. Thus there is no need
for a separate component which solves the set of equations
obtained by applying the LFG algorithm.
-there is a canonical way of translating LFG into a PROLOG
programme.
</bodyText>
<listItem confidence="0.993910625">
(II) For the semantic representation of texts we use the
Discourse Representation Theory developped by Bans Kamp. At
present the impleanitation includes the fragnent described in
[4]. In addition it analyses different types of negation and
certain equi- and raising-verbs. We postulate same requirerents
a semantic representation has OD fulfill in order to be able to
analyse Whole texts. We show haw Vamp&apos;s theory meets these
requirements by analyzing sample discourses involving anaphoric
</listItem>
<bodyText confidence="0.980852818181818">
MP&apos;s.
(III) Finally we sketch how the parser formalism can be
augmented to yield as output discourse representation
structures. To do this we introduce the new notion of &apos;logical
head&apos; in addition to the Lit notion of &apos;grammatical head&apos;. The
reason is the wellknown fact that the logical structure of a
sentence is induced by the determiners and not by the verb which
on the other hand determines the thematic structure of the
sentence. However the verb is able to restrict quantifier scope
ambiguities or to induce a preference ordering on the set of
possible quantifier scope relations. Therefore there must be an
interaction between the grammatical head and the logical head of
a phrase.
I. A,FROLOG IMPLDIENrATION OF Lit
Amain topic in Al research is the interaction between different
components of a system. But insights in this field are
primarily reached by experience in constructing a complex
system. Right from the beginning, however, one should choose
formalisms which are suitable for a simple and transparent
transportion of information. We think Lit meets this
requirement. The formalism exhibiting the analysis of a
sentence can be expanded in a simple way to contain entries
which are used during the parse of a whole text, for example
discourse features like topic or domain dependent knowledge
coming from a database associated with the lexicon. Since Lit
is a kind of unification grammar it allows for constructing
patterns Which enable the following sentences to refine or to
change the content of these discourse features. Knowledge
gathered by a preceding sentence can be used to lead the search
in the lexicon by demanding that certain feature values match.
In short we hope that the nearly uniform status of the different
description tools allows simple procedures for the expansion and
manipulation by other components of the system.
But this was a look ahead. Let us now come to the less
ambitious task of Implementing the grammar of [1,44].
Lexical functional grammar (Lit) is a theory that extends phrase
structure grammars without using transformations. It emphasizes
the role of the grammatical functions and of the lexicon.
Another powerful formalism for describing natural languages -
follows from a method of expressing grammars in logic eafled
definite clause grammars (ECG). A DOG constitutes a PROLOG
programme.
We want to show first, how LPG can be translated into DCG and
second, that PROLOG provides an efficient tool for
LFC-implementation in that it allows for the construction of
functional structures directly during the parsing process. I.e.
it is not necessary to have separate components which first
derive a set of functional equations from the parse tree and
secondly generate an f-structure by solving these equations.
Let us look at an example to see how the Lit machinery works.
We take as the sample sentence &apos;a woman expects an american to
win&apos;. The parsing of the sentence proceeds along the following
lines. The Phrase structure rules in (1) generate the phrase
structure tree in (2) (without considering the schemata smitten
beneath the rule elements).
</bodyText>
<equation confidence="0.965919222222222">
(1) S ---&gt; NP VP
(1•SUEJ)=4. 1•=4 (4■TEAirE)
VP ---&gt; V NP NP PP VP&apos;
(1•OBJ)=4,. (108j2)--,4(4&apos;(-HCASE&gt;=4, (tXCOMP)=i
VP&apos; ---&gt; (to) VP
1=4,/
NP ---&gt; DET N
4=4. t=4,
I I
</equation>
<bodyText confidence="0.9936896">
a woman expects all Amehcan to win
Then the c-structure will be annotated with the functional
schemata associated with the rules . The schemata found in the
lexical entries are attached to the leave nodes of the tree.
This is shown in (3).
</bodyText>
<equation confidence="0.962015862068965">
14,4 4vof lx is supported. 63 DT G it0 2.45/&apos;13
S
PET N V NP VP&apos;
.,
1 xi II / \ VP
/
52
S
(1-SUBJ)=
=
NP
N VP
DET VP
(SPEC)=A =
ONIVO=SG V
(NIM)=SG
(1PERS)=3
(4&apos;FRE0=&apos;AMERICAN&apos;
(fPREE)=&apos;EYPECIK(SUBJ)(XCOMP)&gt;(OBJ)&apos;
(4TENSE)=PRES
(4 SUM NUM)=SG (TPRED)=&apos; WINC( SUBJ) &gt; &apos;
(4-SIBJ PERS)=3
(IXO/IP SUBJ)=( OBJ)
(4) ( fl SUBJ) = f2 f3 = f6
fl = f3 (f6 FRED) = &apos;EXPECW(SUBJ)(XCOMP)&gt;(OBJ)&apos;
f2 = f4 (f6 TENSE)= PRES
f2 = f5 (f6 XCCMP SUBJ) = (f6 OBJ)
(f5.NUM) = SG (f5 FRED)
•
</equation>
<bodyText confidence="0.991967316831684">
Then the tree will be indexed. The indices instantiate the up,
and down,-arrows. An up-arrow refers to the node dominating the
node the schema is attached to. A down-arrow refers to the node
will&amp; carries the functional schema.
The result of the instantiation process is a set of functional
equations. 1.b have listed part of them in (4). The solving of
these equations yields the functional structure in (5).
It is caaposed of grammatical function names, semantic forms and
feature syMbols. The crucial elements of LEO (in contrast to
transformational grammar) are the grammatical functions like
SUBJ, OBJ, xcamp and so on. The functional structure is to be
read as containing pointers from the function-names appearing in
the semantic forms to the corresponding f-structures.
The grammatical functions assuned by LEO are classified in
subcategorizable (or governable) and nonsubcategorizable
functions. The subcategorizable ones are those to which lexical
items can make reference. The item &apos;expects&apos; for example
subcategorizes three functions, but only the material inside the
angled brackets list the predicate&apos;s semantic arguments. XCOMP
and XATU are the only open grammatical functions, i.e. ,they can
denote functionally controlled clauses. In our example this
phenomena is lexically induced by the verb &apos;expects&apos;. This is
expressed by its.last schema &amp;quot;(tXCOMP SUBJ)=(TOBJ)&amp;quot;. It has the
effect that the njof the sentence will become the SUBJ of the
X(flMP, that means in our example it becomes the argument of the
predicate &apos;win&apos;.
Note that the analysis of the sentence &apos;a woman promises an
American to win&apos; would differ in two respects. First the verb
&apos;promises&apos; lists all the three functions subcategorized by it in
mp
FEi
its semantic argument structure. And second &apos;promises&apos; differs
from &apos;expects&apos; just in its functional control schema, i.e., here
we find the equation &amp;quot;(1XCOMP SUBJ)=(SIBJ)&amp;quot; yielding an arrow
from the SUBJ of the XCOMP to the SUBJ of the sentence in the
final f-structure.
An f-structure must fulfill the following conditions in order to
be a solution
-uniqueness: every f-name which has a value has a unique value
-completeness :the f-structure must contain f-values for all the
f-names subcategorized by its predicate
-coherence: all the subcategorizable functions the f-structure
contains must be subcategorized by its predicate
The ability of lexical items to determine the features of other
items is captured by the trivial equations. They propagate the
feature set which is inserted by the lexical item up the tree.
For example the features of the verb become features of the VP
and the features of the VP become features of S. The uniqueness
principle guarantees that any subject that the clause contains
will have the features required by the verb. The trivial
equation makes it also possible that a lexical item, here the
verb, can induce a functional control relationship between
different f-structures of the sentence. An important constraint
for all references to functions and functional features is the
principle of functional locality: designators in lexical and
grammatical schemata can specify no more than two iterated
function applications.
Our claim is that using DOG as a PROLOG programme the parsing
process of a sentence according to the LEG-theory can be done
more efficiently by doing all the three steps described above
simultaneously.
Why is esperially PROLOG useful for doing this?
In the annotated c-structure of the LEG theory the content of
the functional equations is only &amp;quot;known&amp;quot; by the node the
equation is annotated to and by the Immediately dominating node.
The memory is so to speak locally restricted. Thus during the
parse all those bits of information have to be protocolled for
sane other nodes. This is done by means of the equations. In a
PROLOG programme however the nodes turn into predicates with
arguments. The arguments could be the same for different
predicates within a clause. Therefore the memory is
&amp;quot;horizontally&amp;quot; not restricted at all. Furthermore by sharing of
variables the predicates which are goals can give information to
their sUbgoals. In short, once a phrase structure grammar has
been translated into a PROLOG pragramme every node is
potentially able to grasp information fram any other node.
Nonetheless the parser we get by enbedding the restricted LEO
formalism into the highly flexible DOG formalism respects the
constraints of Lexical functional grammar.
Another important fact is that LEO tells the PROLOG programmer
in an exact manner what information the parser needs at which
node and just because this information is purely locally
represented in the LFG formalism it leads to the possibility of
translating LEO into a PROLOG programme in a canonical way.
We have said that in solving the equations LEG sticks together
informations coming from different nodes to build up the final
output. &apos;Do mirror this the following PROLOG feature is of
greatest importance. For the construction of the wanted output
during the parsing process structures can he built up piecemeal,
leaving unspecified parts as variables. The construction of the
output need not be strictly parallel to the application of the
corresponding rules. Variables play the role of placeholders
for structures which are found possibly later in the parsing
process. A closer look at the verb entries as formulated by LEO
reveals that the role of the function names appearing there is
to function as plac=holders too.
To summarize: By embedding the restricted LEG formalism into
the higly flexible definite clause grammar formalism we make
life easier. Nonetheless the parser we get respects the
constraints which are formulated by the LEG theory.
Let us now consider some of the details. The rules under (1)
</bodyText>
<figure confidence="0.998641714285714">
4=4, = ( 4au)=3 (4.xect4p)= 4,
/7\
(3)
IET
1 1
(1SPEC)=A (1NUM)=SG
(NIM)=SG (GN)=FM
(PES)=3
(TFRED)=&apos;1,17:MAN&apos;
ERNUM SG
GEND
PS 3
(5) SUBJ EC A
TENSE PRES
FRED &apos;ENPECTK(SUBJ)(XCOMP)&gt;(OBJ)&apos;
OBJ ISPEC A PERS 3
IERED &apos;AMERICAN&apos; NUM SG
(BED NUNK(SUBJ)&gt;I]
X031P
[Sp
FRED &apos;WMAir
</figure>
<page confidence="0.981578">
53
</page>
<bodyText confidence="0.967222">
are transformed into the PROLOG program in (6). (* indicates
the variables.)
</bodyText>
<equation confidence="0.73556">
(6) S (*c10 *cll *outps) &lt;---
NP (*c10 *c12 *featnp *outpnp)
VP (*c12 *cll (SUBJ (*outpnp *featnp)) TEN *outps)
VP (*c10 *cll *outpsubj *featv *outps) &lt;—
</equation>
<bodyText confidence="0.96692275">
✓ (*coat (tprgtouti.4.110) *featv *outps)
FACNP (*c10 *c12 OBJ wi *11)
!functional FACNP (*c12 *c13 03,32 *11 t12)
controll FACPP (*c13 *c14 OBL 412 4122
FACR&apos; (*c14 *cll *cont XCOMP *12 al) Ichecklistl
FACT? (*c10 *cll (*gf *cant) *gf Utrj_lvtpxcempl......*10) *10)
4-VP&apos; (*c10 *cll *cont *outpxcomP)
NP (*c10 *cll *outpnp) &lt;—
</bodyText>
<listItem confidence="0.8642">
• i (*c10 *cll *outpdet)
</listItem>
<bodyText confidence="0.9792751">
N (*outpdet *outpnp)
We use the content of the function assigning equations to build
up parts of the whole f-structure during the parsing process.
Crucial for this is the fact that every phrase has a unique
category, called its head, with the property that the functional
features of each phrase are identified with those of its head.
The head category of a phrase is characterized by the assignment
of the trivial functional-equation and by the property of being
a major category. The output of each procedure is constructed
by the subprocedure corresponding to the head. This means that
all information resulting from the other subprocedures is given
to that goal. This is done by the &apos;outp&apos; variables in the
programme. Thus the V procedure builds up the f-structure of
the sentence. Since VP is the head of the S rule the VP
procedure has an argument variable for the SUBJ f-structure.
Since V is the head of the VP rule this variable together with
the structures coming fan the sister nodes are given to V for
the construction of the final output. As a consequence our
output does not contain pointers in contrast to Bresnan&apos;s
output. Rather the argument positions of the predicates are
instantiated by the indicated 6-structures. For each category
there is a fixed set of features. The head category is able to
impose restrictions on a fixed subset of that feature set. This
subset is placed on a prominent position. The corresponding
feature values percolating up towards the head category will end
up in the same position demanding that their values agree. This
is done by the &apos;feat&apos; variables. The uniqueness condition is
trivially fulfilled since the passing around of parts of the
f-structure is done by variables, and PROLOG instantiates a
variable with at most one value. \
</bodyText>
<equation confidence="0.950397333333333">
(7) V ( (VOCMP (SUBJ (*outpobj *featobj))) [functional controll
((SUM (*outpsubj (SG 3))) (-7-- &apos;check Usti
(OBJ (*outpobj *featobj)) (XCOMP *outpxcomp))
</equation>
<bodyText confidence="0.980880892857143">
TEN ,47-Ioutputl
((TENSE FRE) (FRED &apos;EXPECT (*outpsubj *outpxcomp)&apos;)) )
The checking of the completeness and coherence condition is done
by the Verb procedure. (7) shows the PROLOG assertion
corresponding to the lexical entry for &apos;expects&apos;. In every
assertion for verbs there is a list containing the grammatical
functions subcategorized by the verb. This is the second
argument in (7), called &apos;check list&apos;. This list is passed
around during the parse. This is done by the list underlined
with waves in (6). Every subcategorizable function appearing in
the sentence must be able to shorten the list. This guarantees
coherence. In the end the list must have diminished to NIL.
This guarantees completeness.
As can be seen in (7) a by-product of this passing around the
check list is to bring the values of the grammatical functions
subcategorized by the verb down to the verb&apos;s predicate argument
structure.
To handle functional control the verb entry contains an argument
to encode the controller. This is the first argument in (7).
The procedure which delivers XCOMP (here the VP&apos; procedure)
receives this variable (the underlined variable *cont in (6))
since verbs can induce functional control only upon the open
grammatical function XCOMP. For tough-movement constructions
the s-prime procedure receives the controller variable too. But
inside this clause the controller must be put onto the long
distance controller list, since SCOMP is not an open grammatical
function.
That leads us to the long distance dependencies
</bodyText>
<listItem confidence="0.9006075">
(8) The girl wonders whose playmate&apos;s nurse the baby saw .
(9) S&apos; ---&gt; NP tuP
</listItem>
<equation confidence="0.945432181818182">
(t0= .81 r *--1,
(Focus)=t,
NP VP
V S&apos;
/&apos;NP
•• ,/ L74Nmj
Orie
liT1&apos;\N NP I
\
Z-44gr
se playmate&apos;s nurse the ioa!by slca
</equation>
<bodyText confidence="0.958689615384615">
In English questions and relatives an element at the front of
the clause is understood as filling a particular grammatical
role within the clause, determined by the position of a
c-structure gap. Consider sentence (8). This kind of
dependency is called constituent control, because in contrast to
functional control the constituent structure configurations are
the primary conditioning factors and not lexical items.
Bresnan/kaplan introduce a new formal mechanism for representing
long- distance dependencies. To handle the embedded question
sentence they use the rule in (9). The double arrow downwards
represents the controller of the constituent control
relationship. Tb this arrow corresponds another double arrow
which points upwards and represents the controlee. This one is .
attached for example to the empty string NP —&gt;4,g4. But as the
arrow indexed with [4wh] shows the controller may affect also a
designated set of lexical items which includes interrogative
pronouns , determiners and adverbs. &apos;whose&apos; for example has the
lexical entry: whose N, OREM = &apos;who&apos;, CASE = OEN,4 3it.
(This kind of control relationship is needed to analyse&amp;quot; the
complex NP &amp;quot;whose playmate&apos;s nurse&amp;quot; in sentence (8))
The control relationships are illustrated in (10).
Corresponding controllers and controlees must have compatible
subscripts. The subscripts indicate the category of the
controllee. The superscript S of the one controller indicates
that the corresponding controlee has to be found in a S-rooted
control domain whereas the [440 controlee for the other
controller has to be found beneath a NP node.
Finally the box around the S-node needs to be explained. It
indicates the fact that the node is a bounding node.
Kaplan/Bresnan state the following convention
A node 14 belongs to a control domain with root node R if and
only if R dominates M and there are no bounding nodes on the
path from M up to but not including R.
This convention prevents constructions like the one in (11).
(11) The girl wondered what the nurse asked who saw
Long distance control is handle by the programme using a long
distance controller list, enriched at some special nodes with
new controllers, passed down the tree and not allowed to go
further at the bounding nodes.
</bodyText>
<equation confidence="0.3792872">
(12) S&apos; (tc1P_*cll *outpsc) &lt;---
1109g NP (UNPJ:I.Wh])......*c10) *cll *featnp *outpnp)
distance
controller rest (*cll *c10)
list&apos; S ((coutEnp *featmp (S NP)) nil *outpsc)
</equation>
<bodyText confidence="0.9994168">
Every time a controlee is found its subscript has to match the
corresponding entry of the first member of the controller list.
If this happens the first element will be deleted from the list.
The fact that a controlee can only match the first element
reflects the crossed dependency constraint. *c10 is the input
</bodyText>
<figure confidence="0.65772">
bounding
nodes
</figure>
<page confidence="0.971422">
54
</page>
<bodyText confidence="0.999785153846154">
controller variable of the S&apos; procedure in (12). *cll is the
output variable. *c10 is expanded by the [4u/h] controller
within the NP subgoal. This controller must find its controllee
during the execution of the NP goal. Note that the output
variable of the NP subgoal is identirAl with the output variable
of the mein goal and that the subgoal S&apos; does have different
controller lists. This reflects the effect of the box around
the S-node, i.e. no controller coming downwards can find its
controlee inside the S-procedure. The only controller going
into the S goal is the one introduced below the NP node with
domain root S. Clearly the output variable of S has to be nil.
There are rules which allow for certain controllers to pass a
boxed node Bresnan/Kaplan state for example the rule in (13).
</bodyText>
<equation confidence="0.657381">
(13) S&apos; ---&gt; (that) S
it.os
</equation>
<bodyText confidence="0.994112260869565">
This rule has the effect that S-rooted contollers are allowed to
pass the box. Here we use a test procedure which puts only the
contollers indexed by S onto the controller list going to the S
goal. Thereby we obtain the right treatment of sentence (14).
(14) The girl wondered who John believed that Mary claimed that
the baby saw.
In a corresponding manner the complex NP &apos;whose playmate&apos;s
nurse&apos; of sentence (8) is analysed.
II. SEMANTIC REPRESENTATION
As semantic representation we use the r(iscourse)
R(epresentation) Theory) developped by Hans Kamp [4]. I.e. we
do not adopt the semantic theory for L(exical) F(unctional)
G(rammar) proposed by Per-Kristian Halverson [2]. Halverson
translates the functional structures of LEG into so-called
semantic structures being of the same structural nature, namely
acyclic graphs. The semantic structures are the result of a
translation procedure which is based on the association of
formulas of intensional logic to the semantic forms appearing in
the functional structure. The reason not to take this approach
will be explained by postulating some requirements a semantic
representation has to fulfill in order to account for a
processing of texts. Then we will show that these requirements
are really necessary by analysing some sample sentences and
discourses. It will turn out that DRT accounts for them in an
intuitively fully satisfactory way.
Because we cannot review ERT in detail here the reader should
consult one of the papers explaining the fundamentals of the
theory (e.g. £43 ), or he should first look at the last
paragraph in which an outline is given of how our parser is to
be extended in order to yield an IR-tyed output - instead of
the &apos;traditional&apos; (semantic) functional structures.
The basic building principle of a semantic representation is to
associate with every significant lexical entry (i.e., every
entry which does contribute to the truthcondidtional aspect of
the meaning of a sentence) a semantic structure. Compositional
principles, then, will construct the semantic representation of
a sentence by combining these semantic structures according to
their syntactic relations. The desired underlying principle is
that the semantic structures associated with the semantic forms
should not be changed during the composition process. To put it
differently: one wants the association of the semantic
structures to be independent of the syntactic context in which
the semantic form appears. This requirement leads to
difficulties in the tradition of translating sentences into
formulas of e.g. predicate or intentional logic.
Consider sentences
</bodyText>
<listItem confidence="0.929673625">
(1) If john admires a woman then he kisses her
and
(2) Every man Who admires a woman kisses her
the truth conditions of which are determined by the first order
formulae
(3) Vx ( woman(x) &amp; admire(John,x) --&gt; kiss(John,x) )
and
(4) Vz:Vy ( man(z) &amp; wanan(y) &amp; admire(x,y) --&gt; kiss(x,y) )
</listItem>
<bodyText confidence="0.999921869565218">
respectively. The problem is that the definite description &amp;quot;a
woman&amp;quot; reemerges as universally quantified in the logical
representation - and there is no way out, because the pronoun
&amp;quot;she&amp;quot; has to be bound to the woman in question. ERT provides a
general account of the meaning of indefinite descriptions,
conditionals, universally quantified noun phrases and anaphoric
pronouns, s.t. our first requirement is satisfied. The
semantic representations (called IRS&apos; a) which are assigned to
sentences in which such constructions jointly appear have the
truth conditions which our intuitions attribute to then.
The second reason why we decided to use ETR as semantic
formalism for LEG is that the construction principles for a
sentence S(i) of a text D= S(1),...,S(n) are formulated with
respect to the semantic representation of the preceeding text
S(1),...,S(i-1). Therefore the theory can account for
intersentential semantic relationships in the same way as for
intrasentential ones. This is the second requirement: a
semantic representation has to represent the discourse as a
whole and not as the mere union of the semantic representations
of its isolated sentences.
A third requirement a semantic representation has to fulfill is
the reflection of configurational restrictions on anaphoric
links: If one embeds sentence (2) into a conditional
</bodyText>
<listItem confidence="0.62657175">
(6) *If every man who admires a woman kisses her then she is
stressed
the anaphoric link in (2) is preserved. But (6) does - for
configurational reasons - not allow for an anaphoric relation
between the &amp;quot;she&amp;quot; and &amp;quot;a Mae. The same happens
intersententially as shown by
(7) If John admires a woman then he kisses her. *She is
enraged.
</listItem>
<bodyText confidence="0.9547271">
A last requirement we will stipulate here is the following: It
is neccessAry to draw inferences already during the construction
of the semantic representation of a sentence 5(i) of the
discourse. The inferences must operate on the semantic
representation of the already analyzed discourse 5(1),...,S(i-1)
as well as on a datahasP containing the knowledge the text talks
about. This requirement is of major importance for the analysis
of definite descriptions. Consider
(8) Pedro is a farmer. If a woman loves him then he is happy.
Mary loves Pedro. The happy farmer marries her
in which the definite description &amp;quot;the happy farmer&amp;quot; is used to
refer to refer to the individual denoted by &amp;quot;Pedro&amp;quot;. In order
to get this link one has to infer that Pedro is indeed a happy
farmer and that he is the only one. If this were not the CASP
the use of the definite description would not be appropriate.
Such a deduction mechanism is also needed to analyse sentence
(9) John bought a car. The engine has 160 horse powers
In this rAsP one has to take into account some knowledge of the
world, namely the fact that every car has exactly one engine.
Tb illustrate the way the semantic representation has to be
interpreted let us have a brief look at the text-MS for the
sample discourse (8)
Thus a IRS K consists of
(i) a set of discourse referents: discourse individuals,
discourse events, discourse propositions, etc.
(ii) a set of conditions of the following types
- atomic conditions, i.e. ir-ary relations over discourse
referents
- complex conditions, i.e. n-ary relations (e.g. --&gt; or :)
over sub-IRS&apos;s and discourse referents (e.g. g(1) --&gt; K(2) or
</bodyText>
<figure confidence="0.4897305">
uv
Pedro = u
fanner(u)
--&gt;
love(y,u)
marry(u,v)
love(v ,u)
Mary = v
happy(u)i
utman(y)
</figure>
<page confidence="0.990366">
55
</page>
<bodyText confidence="0.997029096153846">
p:K, where p is a discourse proposition)
Awhole DRS can be understood as partial model representing the
individuals introduced by the discourse as well as the facts and
rules those individuals are subject to.
The truth conditions state that a ERS K is true in a model M if
there is a proper imbedding from K into M. Proper embedding is
defined as a function f from the set of discourse referents of K
in to /1 s.t. (i) it is a hommorphism for the atomic conditions
of the ERS and (ii) - for the case of a complex condition K(1)
--&gt; K(2) every proper Embedding of 101) that extends f is
extendable to a proper embedding of 102).
- for the caey. of a complex condition pa&lt; the mcdeltheoretic
object correlated with p (i.e. a proposition if p is a
discourse proposition, an event if p is a discourse event, etc.)
must be such that it allows for a proper embedding of K in it.
Note that the definition of proper embedding has to be made more
precise in order to adapt it to the special semantics one uses
for propositional attitudes. We cannot go into details here.
Nonetheless the truth condition as it stands should make clear
the following: whether a discourse referent introduced implies
existence or not depends on its position in the hierarchy of the
IRS&apos;s. Given a ERS which is true in M then eactly those
referents introduced in the very toplevel DRS imply existence;
all others are to be interpreted as universally quantified, if
they occur in an antecedent ERS, or as existentially quantified
if they occur in a consequent DRS, or as having opaque status if
they occur in a ERS specified by e.g. a discourse proposition.
Thus the role of the hierarchical order of the IRS&apos;s is to build
a base for the definition of truth conditions. But furthermore
the hierarchy defines an accessibility relation, which restricts
the set of possible antecedents of anaphoric NP&apos;s. This
accessibiltity relation is (for the fragnent in [41) defined as
follows:
For a given sub-ERS KO all referents occurring in KO or in any
of the IRS&apos;s in which KO is embedded are accessible.
Furthermore if KO is a consequent-ERS then the referents
occurring in its corresponding antecedent DRS on the left are
accessible too.
This gives us a correct treatment for (6) and (7).
For the time being - we have no algorithm which restricts and
orders the set of possible anaphoric antecedents according to
contextual conditions as given by e.g.
(5) John is reading a book on syntax and Bill is reading a book
on sematics.
The former tis enjoying himself&apos;
is a paperback
Therefore our selection set is restricted only by the
accessibility relation and the descriptive content of the
anaphoric NP&apos;s. Of course for anaphoric pronouns this content
is reduced to a minimal, namely the grammatical features
associated to them by the lexical entries. This accounts e.g.
for the difference in acceptability of (10) and (11).
</bodyText>
<listItem confidence="0.9994485">
(10) Mary persuaded every man to shave himself
(11) *Mary promised every man to shave himself
</listItem>
<bodyText confidence="0.997783974358974">
The IRS&apos;s for (10) and (11) show that both discourse referents,
the one for &amp;quot;Man,&apos; and the one for a &amp;quot;nye, are accessible from
the position at which the reflexive pronoun has to be resolved.
But if the &amp;quot;himself&amp;quot; of (11) is replaced by x it cannot be
identified with y having the (not explicitely shown) feature
female.
Eefinite descriptions bear more information by virtue of the
semantic content of their common-noun-phrases and the existence
and uniqueness conditions presupposed by them. Therefore in
order to analyse definite descriptions we look for a discourse
referent introduced in the preceding ERS for which the
description holds and we have to Check whether this descrition
holds for one referent only. Our algorithm proceeds as follows:
First we build up a small DRS KO encoding the descriptive
content of the common-noun-phrase of the definite description
together with its uniquness and existency condition:
KO:
Second we have to show that we can prove KO out of the text-ERS
of the preceeding discourse , with the restriction that only
accessible referents are taken into account. The instantiation
of *x by this proof gives us the correct antecedent the definite
description refers to. Now we forget about KO and replace the
antecedent discourse referent for the definite noun phrase to
get the whole text-ERS (8&apos;).
Of course it is possible that the presuppositions are not
mentioned explicitely in the discourse but follow implicitely
from the text alone or from the text together with the knowledge
of the domain it talks about. So in cases like
(9) John bought a car. The engine has 260 horse powers
Pere the identified referent is functionally related to
referents that are more directly accessible, namely to John&apos;s
car. FUrthermore such a functional dependency confers to a
definite description the power of introducing a new discourse
referent, namely the engine which is functionally determined by
the car of which it is part. This shifts the task from the
search for a direct antecedent for &amp;quot;the engine&amp;quot; to the =arch
for the referent it is functionally related to. But the basic
mechanism for finding this referent is the same deductive -
mechanism just outlined for the &amp;quot;happy farmer&amp;quot; example.
</bodyText>
<sectionHeader confidence="0.6518715" genericHeader="method">
III. TOWARIS AN INTERACTICV BETWEEN &amp;quot;GRAMMATICAL PARSDWP AND
&amp;quot;LOGICAL PARS111;&amp;quot;
</sectionHeader>
<bodyText confidence="0.999964071428572">
In this section we will outline the principles underlying the
extension of our parser to produce ERS&apos;s as output. Because
none of the fragments of IRT contains Raising-- and Equi-verbs
taking infinitival or that-complements we are confronted with
the task of writing construction rules for such verbs. It will
turn out, however, that it is not difficult to see how to extend
ERT to canprise such constructions. This is due to the fact
that using LFG as syntactic base for IRT - and not the
categorial syntax of Kamp - the unraveling of the thematic
relations in a sentence is already accomplished in f-structure.
Therefore it is straightforward to formulate construction rules
which give the correct readings for (10) and (11) of the
previous section, establish the propositional equivalence of
pairs with or without Raising, Equi (see (1), (2)), etc.
</bodyText>
<listItem confidence="0.9988115">
(1) John persuaded Mary to come
(2) John persuaded Mary that she should come
</listItem>
<bodyText confidence="0.564046">
Let us first describe the DRS construction rules by the familiar
example
</bodyText>
<listItem confidence="0.974886">
(3) every man loves a woman
</listItem>
<bodyText confidence="0.999134769230769">
USing Kamp&apos;s categorial syntax, the construction rules operate
top down the tree. The specification of the order in which the
parts of the tree are to be treated is assumed to be given by
the syntactic rules. I.e. the specification of scope order is
directly determined by the syntactic construction of &apos;the
sentence. We will deal with the point of scope ambiguities
after having described the way a IRS is constructed. Our
description - operating bottom up instead top down - is
different from the one given in [4] in order to come closer to
the point we want to make. But note that this difference is not
a genuine one. Thus according to the first requirement of the
previous section we assume that to each semantic from a semantic
structure is associated. For the lexical entries of (3) we have
</bodyText>
<equation confidence="0.603514272727273">
:nary = y
r persuade( y ,x , p)/
Pranise(Yot ,p) 1
x ishave(x,x) /
man(x) -, jshave(y,himself)1
P:
farmer(x)
happy(x)
farL(y)]
L happy(y)
y = x
</equation>
<page confidence="0.802365">
56
</page>
<bodyText confidence="0.87691075">
the following:
man --&gt; man(*)
woman --&gt; woman(*)
loves --&gt; love(*,*) .
The semantic structures for the common nouns and the verbs are
n,place predicates. The structure for &amp;quot;a&amp;quot; is a DRS with
discourse individual v introduced and conditions not yet
specified. The entry for &amp;quot;every&amp;quot; is a ERS with no discourse
individuals introduced on the toplevel. It contains however a
complex condition KO --&gt; K1 s.t a discourse individual x is
introduced in KO and both KO and El contain any other
conditions.
The DRS construction rules specify how these senantic structures
are to be combined by propagating them up the tree. The easiest
way to illustrate that is to do it by the following picture (for
the case of narrow sco read of &amp;quot;a worm&amp;quot; :
</bodyText>
<equation confidence="0.742315666666667">
(4)
man(*) love(*,*)
every man_ loves a
</equation>
<bodyText confidence="0.998385071428571">
For the wide scope reading the NP-tree of &amp;quot;a woman&amp;quot;
at the very end to give
(5)
The picture should make clear the way we want to extend the
parsing mechanism described in section 1 in order to produce
ERS&apos;s as output and no more f-structures: instead of partially
instantiated f-structures determined by the lexical entries
partially instantiated IRS&apos;s are passed around the tree getting
accomplished by unification. The control mechanism of LFG will
automatically put the discourse referents into the correct
argument position of the verb. Thus no additional work has to
be done for the grammatical relations of a sentence.
But what About the logical relations?
Recall that each clause has a unique head and that the
functional features of each phrase are identified with those of
its head. Fbr (3) the head of S -=&gt; NP VP is the VP and the
head of VP --&gt; V NP is the V. Thus the outstanding role of the
verb to determine and restrict the grammatical relations of the
sentence is captured. (4) , however, shows that the logical
relations of the sentence are mainly determined by its
determiners, which are not heads of the NP-phrases and the
NPphrases thenselveS are not the heads of the VP- and S-phrase
respectively. To account for this dichotomy we will call the
syntactically defined notion of head &amp;quot;grammatical head&amp;quot; and we
will introduce a further notion of &amp;quot;logical head&amp;quot; of a phrase.
Of course, in order to make the definition work it has to be
elaborated in a way that garantees that the logical head of a
phrase is tniquely determied too. Consider
</bodyText>
<listItem confidence="0.719879">
(6) John persuaded an american to win
(7) John expected an american to win
</listItem>
<bodyText confidence="0.959511">
for which we propose the following ERS&apos;s
(6 )
</bodyText>
<equation confidence="0.949635">
(7&apos;)
J Y
John = j John = j
expect(j,p) american(y)
p:l y expect(j,p)
annrican(y) p: twin(y)1
win(y)
</equation>
<bodyText confidence="0.991365054054054">
The tact that (7) does not neccesserily Imply existence of an
american whereas (6) does is triggered by the difference between
Equi- and Raising-verbs.
Suppose we define the NP to be the logical head of the phrase VP
--&gt; V NP VP 1 Then the logical relations of the VP would be
those of the NP. This amounts to incorporating the logical
structures of the V and the VP&apos; into the logical structure of the
NT, which is for both (6) and (7)
anerican(y)
and thus would lead to the readings represented in (6&apos;) and
(7&amp;quot;). Consequently (7&apos;) would not be produced.
Defining the logical head to be the VP/ would exclude the
readings (6&apos;) and (7&amp;quot;).
Evidently the last possibility of defining the logical head to
be identical to the grammatical head, namely the V itself, seems
to be the only solution. But this would block the construction
already at the stage of unifying the NP- and VIALstructures with
persuade(*,*,*) or expect(*,*). At first thought one easy way
out of this dilemma is to associate with the lexical entry of
the verb not the mere n-place predicate but a ERS containing
this predicate as atomic condition. This makes the unification
possible but gives us the following result:
(*)
Of course one can say t t is open to produce the set of
IRS&apos;s representing (6) and (7). But this means that one has to
work an (*) after having reached the top of the tree - a
consequence that seems undesirable to us.
Thus the only way out is to consider the logical head as not
being uniquely identified by the mere phrase structure
configurations. As the above example for the phrase VP --&gt; V NP
VPI shows its head depends on the verb class too. But we will
still go further.
We claim that it is possible to make the logical head to
additionally depend on the order of the surface string, on the
use of active and passive voice and probably others. This will
give us a preference ordering of the scope ambiguities of
.sentences as the following:
</bodyText>
<listItem confidence="0.9192335">
- Every man loves a woman
- Awoman is loved by every man
- A ticket is bought by every man
- Every man bought a ticket
</listItem>
<bodyText confidence="0.997940666666667">
The properties of unification grammers listed above show that
the theoretical framework does not impose any restrictions on
that plan.
</bodyText>
<sectionHeader confidence="0.998692" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.998198181818182">
[1] Bresnan, J. (ed.), &amp;quot;the Mental Representation of Grammatical
Relations&amp;quot;. MIT Press, Cambridge, Mess., 1982
[2] Frey, Werner/ Reyle, UWe/ Rohrer, Christian, &amp;quot;Autaaatic
Construction of a Knowledge Base by Analysing Texts in &apos;
Natural Language&amp;quot;, in: Proceedings of the Eigth Intern.
Joint Conference on Artificial Intelligence II, 1983
[3] Halverson, P.-k., &amp;quot;Semantics for Lexical Functional
Grammar&amp;quot;. In: Linguistic Inquiry 14, 1982
[4] Kamp, Hans, &amp;quot;A Theory of Truth and Semantic Representa=
tion&amp;quot;. In: J.A. Groenendijk, T.U.V. (ed.), Formal
Semantics in the Study of Natural Language I, 1981
</reference>
<equation confidence="0.6261744">
a—&gt;
every --&gt;
woman(*)
is treated
woman( Y)
love(x‘y)
j y
John = j
american(y) P: win( Y)
persuade(j,y,p)
</equation>
<page confidence="0.994453">
57
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.9974715">A PROLOG IMPLEMENTATION OF LEXICAL FUNCTIONAL GRAMMAR A BASE FOR A NATURAL LANGUAGE PROCESSING SYSTEM</title>
<author confidence="0.999223">Werner Frey</author>
<author confidence="0.999223">Uwe Reyle</author>
<affiliation confidence="0.9982895">Department of Linguistics University of Stuttgart</affiliation>
<email confidence="0.539472">W-Germany</email>
<abstract confidence="0.913221195652175">O. ABSTRACT this paper is to present parts of our system [2], which is to construct a database out of a narrative natural language text. We think the parts are of interest in their awn. The paper consists of three sections: (I) We give a detailed description of the PROLOG implementation of the parser which is based on the theory of lexical functional grammar (LR). The parser covers the fragment described in [1,44]. I.e., it is able to analyse constructions involving functional control and long distance to show that - PROLOG provides an efficient tool for LFC,implementation: a phrase structure rule annotated with functional schemata likeS4 WP veis to be interpreted as, first, identifying the special tts414 t. grammatical relation of subject position of any sentence analyzed by this clause to be the NP appearing in it, and second, as identifying all grammatical relations of the sentence with those of the VP. This universal interpretation of the LFCfaretavariables t and 4, corresponds to the universal quantification of variables appearing in PROIOG-clauses. The procedural semantics of PROLOG is such that the instantiation of the variables in a clause is inherited from the instantiation given by its subgoals, if they succeed. Thus there is no need for a separate component which solves the set of equations obtained by applying the LFG algorithm. -there is a canonical way of translating LFG into a PROLOG programme. (II) For the semantic representation of texts we use the Discourse Representation Theory developped by Bans Kamp. At present the impleanitation includes the fragnent described in [4]. In addition it analyses different types of negation and certain equiand raising-verbs. We postulate same requirerents semantic representation has order to be able to analyse Whole texts. We show haw Vamp&apos;s theory meets these requirements by analyzing sample discourses involving anaphoric MP&apos;s. (III) Finally we sketch how the parser formalism can be augmented to yield as output discourse representation structures. To do this we introduce the new notion of &apos;logical head&apos; in addition to the Lit notion of &apos;grammatical head&apos;. The reason is the wellknown fact that the logical structure of a sentence is induced by the determiners and not by the verb which on the other hand determines the thematic structure of the sentence. However the verb is able to restrict quantifier scope ambiguities or to induce a preference ordering on the set of possible quantifier scope relations. Therefore there must be an interaction between the grammatical head and the logical head of a phrase. I. A,FROLOG IMPLDIENrATION OF Lit Amain topic in Al research is the interaction between different components of a system. But insights in this field are primarily reached by experience in constructing a complex system. Right from the beginning, however, one should choose formalisms which are suitable for a simple and transparent transportion of information. We think Lit meets this requirement. The formalism exhibiting the analysis of a sentence can be expanded in a simple way to contain entries which are used during the parse of a whole text, for example discourse features like topic or domain dependent knowledge coming from a database associated with the lexicon. Since Lit is a kind of unification grammar it allows for constructing patterns Which enable the following sentences to refine or to change the content of these discourse features. Knowledge gathered by a preceding sentence can be used to lead the search in the lexicon by demanding that certain feature values match. In short we hope that the nearly uniform status of the different description tools allows simple procedures for the expansion and manipulation by other components of the system. But this was a look ahead. Let us now come to the less ambitious task of Implementing the grammar of [1,44]. Lexical functional grammar (Lit) is a theory that extends phrase structure grammars without using transformations. It emphasizes the role of the grammatical functions and of the lexicon. Another powerful formalism for describing natural languages follows from a method of expressing grammars in logic eafled definite clause grammars (ECG). A DOG constitutes a PROLOG programme. We want to show first, how LPG can be translated into DCG and second, that PROLOG provides an efficient tool for LFC-implementation in that it allows for the construction of functional structures directly during the parsing process. I.e. it is not necessary to have separate components which first derive a set of functional equations from the parse tree and secondly generate an f-structure by solving these equations. Let us look at an example to see how the Lit machinery works. We take as the sample sentence &apos;a woman expects an american to win&apos;. The parsing of the sentence proceeds along the following lines. The Phrase structure rules in (1) generate the phrase structure tree in (2) (without considering the schemata smitten beneath the rule elements). (1) S ---&gt; NP VP 1•=4 VP ---&gt; V NP NP PP VP&apos; (tXCOMP)=i VP&apos; ---&gt; (to) VP 1=4,/ NP ---&gt; DET N I I woman expects to win Then the c-structure will be annotated with the functional schemata associated with the rules . The schemata found in the lexical entries are attached to the leave nodes of the tree. is shownin (3). lx is 63 G it0 2.45/&apos;13 S PET N V NP VP&apos; ., II/ \ VP / 52 S = NP DET VP ONIVO=SG V (NIM)=SG (1PERS)=3 (4&apos;FRE0=&apos;AMERICAN&apos; (fPREE)=&apos;EYPECIK(SUBJ)(XCOMP)&gt;(OBJ)&apos; (4TENSE)=PRES (4 SUM NUM)=SG (TPRED)=&apos; WINC( SUBJ) &gt; &apos; (4-SIBJ PERS)=3 (IXO/IP SUBJ)=( OBJ) (4) ( fl SUBJ) = f2 f3 = f6 fl = f3 (f6 FRED) = &apos;EXPECW(SUBJ)(XCOMP)&gt;(OBJ)&apos; = f4 (f6 TENSE)= f2 = f5 (f6 XCCMP SUBJ) = (f6 OBJ) (f5.NUM) = SG (f5 FRED) • the tree will be indexed. The indices instantiate the and down,-arrows. An up-arrow refers to the node dominating the node the schema is attached to. A down-arrow refers to the node will&amp; carries the functional schema. The result of the instantiation process is a set of functional equations. 1.b have listed part of them in (4). The solving of these equations yields the functional structure in (5). It is caaposed of grammatical function names, semantic forms and feature syMbols. The crucial elements of LEO (in contrast to transformational grammar) are the grammatical functions like OBJ, so on. The functional structure is to be read as containing pointers from the function-names appearing in the semantic forms to the corresponding f-structures. The grammatical functions assuned by LEO are classified in subcategorizable (or governable) and nonsubcategorizable functions. The subcategorizable ones are those to which lexical items can make reference. The item &apos;expects&apos; for example subcategorizes three functions, but only the material inside the angled brackets list the predicate&apos;s semantic arguments. XCOMP and XATU are the only open grammatical functions, i.e. ,they can denote functionally controlled clauses. In our example this phenomena is lexically induced by the verb &apos;expects&apos;. This is expressed by its.last schema &amp;quot;(tXCOMP SUBJ)=(TOBJ)&amp;quot;. It has the effect that the njof the sentence will become the SUBJ of the X(flMP, that means in our example it becomes the argument of the predicate &apos;win&apos;. Note that the analysis of the sentence &apos;a woman promises an American to win&apos; would differ in two respects. First the verb &apos;promises&apos; lists all the three functions subcategorized by it in mp FEi its semantic argument structure. And second &apos;promises&apos; differs from &apos;expects&apos; just in its functional control schema, i.e., here we find the equation &amp;quot;(1XCOMP SUBJ)=(SIBJ)&amp;quot; yielding an arrow from the SUBJ of the XCOMP to the SUBJ of the sentence in the final f-structure. An f-structure must fulfill the following conditions in order to be a solution -uniqueness: every f-name which has a value has a unique value -completeness :the f-structure must contain f-values for all the f-names subcategorized by its predicate -coherence: all the subcategorizable functions the f-structure contains must be subcategorized by its predicate The ability of lexical items to determine the features of other items is captured by the trivial equations. They propagate the feature set which is inserted by the lexical item up the tree. For example the features of the verb become features of the VP and the features of the VP become features of S. The uniqueness principle guarantees that any subject that the clause contains will have the features required by the verb. The trivial equation makes it also possible that a lexical item, here the verb, can induce a functional control relationship between different f-structures of the sentence. An important constraint for all references to functions and functional features is the principle of functional locality: designators in lexical and grammatical schemata can specify no more than two iterated function applications. Our claim is that using DOG as a PROLOG programme the parsing process of a sentence according to the LEG-theory can be done more efficiently by doing all the three steps described above simultaneously. Why is esperially PROLOG useful for doing this? In the annotated c-structure of the LEG theory the content of the functional equations is only &amp;quot;known&amp;quot; by the node the equation is annotated to and by the Immediately dominating node. The memory is so to speak locally restricted. Thus during the parse all those bits of information have to be protocolled for sane other nodes. This is done by means of the equations. In a PROLOG programme however the nodes turn into predicates with arguments. The arguments could be the same for different predicates within a clause. Therefore the memory is &amp;quot;horizontally&amp;quot; not restricted at all. Furthermore by sharing of variables the predicates which are goals can give information to their sUbgoals. In short, once a phrase structure grammar has been translated into a PROLOG pragramme every node is potentially able to grasp information fram any other node. Nonetheless the parser we get by enbedding the restricted LEO formalism into the highly flexible DOG formalism respects the constraints of Lexical functional grammar. Another important fact is that LEO tells the PROLOG programmer in an exact manner what information the parser needs at which node and just because this information is purely locally represented in the LFG formalism it leads to the possibility of translating LEO into a PROLOG programme in a canonical way. We have said that in solving the equations LEG sticks together informations coming from different nodes to build up the final output. &apos;Do mirror this the following PROLOG feature is of greatest importance. For the construction of the wanted output during the parsing process structures can he built up piecemeal, leaving unspecified parts as variables. The construction of the output need not be strictly parallel to the application of the corresponding rules. Variables play the role of placeholders for structures which are found possibly later in the parsing process. A closer look at the verb entries as formulated by LEO reveals that the role of the function names appearing there is to function as plac=holders too. To summarize: By embedding the restricted LEG formalism into the higly flexible definite clause grammar formalism we make life easier. Nonetheless the parser we get respects the constraints which are formulated by the LEG theory.</abstract>
<note confidence="0.9494321">Let us now consider some of the details. The rules under (1) 4au)=3 /7\ (3) IET 1 1 (1SPEC)=A (1NUM)=SG (NIM)=SG (GN)=FM (PES)=3 (TFRED)=&apos;1,17:MAN&apos;</note>
<title confidence="0.926022">SG GEND PS 3 EC A TENSE PRES</title>
<author confidence="0.76406">FRED &apos;ENPECTK&apos; OBJ ISPEC A PERS</author>
<affiliation confidence="0.640237">IERED &apos;AMERICAN&apos; NUM SG</affiliation>
<abstract confidence="0.99081851637765">NUNK(SUBJ)&gt;I] X031P [Sp FRED &apos;WMAir 53 are transformed into the PROLOG program in (6). (* indicates the variables.) S (*c10 *cll *outps) *c12 *featnp *outpnp) VP (*c12 *cll (SUBJ (*outpnp *featnp)) TEN *outps) (*c10 *cll *outpsubj *featv *outps) &lt;— (*coat *outps) (*c10 *c12 OBJ !functionalFACNP (*c12 *c13 03,32 *11 FACPP (*c13 *c14 OBL (*c14 *cll *cont XCOMP al) (*c10 *cll (*gf *cant) *gf (*c10 *cll *cont *outpxcomP) *cll *outpnp) &lt;— • i (*c10 *cll *outpdet) N (*outpdet *outpnp) We use the content of the function assigning equations to build up parts of the whole f-structure during the parsing process. Crucial for this is the fact that every phrase has a unique category, called its head, with the property that the functional features of each phrase are identified with those of its head. The head category of a phrase is characterized by the assignment of the trivial functional-equation and by the property of being a major category. The output of each procedure is constructed by the subprocedure corresponding to the head. This means that all information resulting from the other subprocedures is given to that goal. This is done by the &apos;outp&apos; variables in the programme. Thus the V procedure builds up the f-structure of the sentence. Since VP is the head of the S rule the VP procedure has an argument variable for the SUBJ f-structure. Since V is the head of the VP rule this variable together with the structures coming fan the sister nodes are given to V for construction of the final output. consequence our output does not contain pointers in contrast to Bresnan&apos;s output. Rather the argument positions of the predicates are instantiated by the indicated 6-structures. For each category there is a fixed set of features. The head category is able to impose restrictions on a fixed subset of that feature set. This subset is placed on a prominent position. The corresponding feature values percolating up towards the head category will end up in the same position demanding that their values agree. This is done by the &apos;feat&apos; variables. The uniqueness condition is trivially fulfilled since the passing around of parts of the f-structure is done by variables, and PROLOG instantiates a variable with at most one value. \ (7) V ( (VOCMP (SUBJ (*outpobj *featobj))) [functional controll ((SUM (*outpsubj (SG 3))) (-7-- &apos;check Usti (OBJ (*outpobj *featobj)) (XCOMP *outpxcomp)) ((TENSE FRE) (FRED &apos;EXPECT (*outpsubj *outpxcomp)&apos;)) ) The checking of the completeness and coherence condition is done by the Verb procedure. (7) shows the PROLOG assertion corresponding to the lexical entry for &apos;expects&apos;. In every assertion for verbs there is a list containing the grammatical functions subcategorized by the verb. This is the second argument in (7), called &apos;check list&apos;. This list is passed around during the parse. This is done by the list underlined with waves in (6). Every subcategorizable function appearing in the sentence must be able to shorten the list. This guarantees coherence. In the end the list must have diminished to NIL. This guarantees completeness. As can be seen in (7) a by-product of this passing around the check list is to bring the values of the grammatical functions subcategorized by the verb down to the verb&apos;s predicate argument structure. To handle functional control the verb entry contains an argument to encode the controller. This is the first argument in (7). The procedure which delivers XCOMP (here the VP&apos; procedure) receives this variable (the underlined variable *cont in (6)) since verbs can induce functional control only upon the open grammatical function XCOMP. For tough-movement constructions the s-prime procedure receives the controller variable too. But inside this clause the controller must be put onto the long distance controller list, since SCOMP is not an open grammatical function. That leads us to the long distance dependencies (8) The girl wonders whose playmate&apos;s nurse the baby saw . S&apos; ---&gt; NP .81 r (Focus)=t, NP VP V S&apos; L74Nmj NP I \ se playmate&apos;s nurse the ioa!by slca In English questions and relatives an element at the front of the clause is understood as filling a particular grammatical role within the clause, determined by the position of a c-structure gap. Consider sentence (8). This kind of dependency is called constituent control, because in contrast to functional control the constituent structure configurations are the primary conditioning factors and not lexical items. Bresnan/kaplan introduce a new formal mechanism for representing longdistance dependencies. To handle the embedded question sentence they use the rule in (9). The double arrow downwards represents the controller of the constituent control relationship. Tb this arrow corresponds another double arrow which points upwards and represents the controlee. This one is . for example to the empty string NP But as the arrow indexed with [4wh] shows the controller may affect also a designated set of lexical items which includes interrogative pronouns , determiners and adverbs. &apos;whose&apos; for example has the entry: whose N, OREM = &apos;who&apos;, CASE = OEN,4 (This kind of control relationship is needed to analyse&amp;quot; the playmate&apos;s nurse&amp;quot; in sentence (8)) The control relationships are illustrated in (10). Corresponding controllers and controlees must have compatible subscripts. The subscripts indicate the category of the controllee. The superscript S of the one controller indicates that the corresponding controlee has to be found in a S-rooted control domain whereas the [440 controlee for the other controller has to be found beneath a NP node. Finally the box around the S-node needs to be explained. It indicates the fact that the node is a bounding node. Kaplan/Bresnan state the following convention A node 14 belongs to a control domain with root node R if and only if R dominates M and there are no bounding nodes on the path from M up to but not including R. This convention prevents constructions like the one in (11). (11) The girl wondered what the nurse asked who saw Long distance control is handle by the programme using a long distance controller list, enriched at some special nodes with new controllers, passed down the tree and not allowed to go further at the bounding nodes. S&apos; NP *cll *featnp *outpnp) controller rest (*cll *c10) list&apos; S ((coutEnp *featmp (S NP)) nil *outpsc) Every time a controlee is found its subscript has to match the corresponding entry of the first member of the controller list. If this happens the first element will be deleted from the list. The fact that a controlee can only match the first element reflects the crossed dependency constraint. *c10 is the input bounding nodes 54 controller variable of the S&apos; procedure in (12). *cll is the output variable. *c10 is expanded by the [4u/h] controller within the NP subgoal. This controller must find its controllee during the execution of the NP goal. Note that the output variable of the NP subgoal is identirAl with the output variable of the mein goal and that the subgoal S&apos; does have different controller lists. This reflects the effect of the box around the S-node, i.e. no controller coming downwards can find its controlee inside the S-procedure. The only controller going into the S goal is the one introduced below the NP node with domain root S. Clearly the output variable of S has to be nil. There are rules which allow for certain controllers to pass a boxed node Bresnan/Kaplan state for example the rule in (13). (13) S&apos; ---&gt; (that) S it.os This rule has the effect that S-rooted contollers are allowed to pass the box. Here we use a test procedure which puts only the contollers indexed by S onto the controller list going to the S goal. Thereby we obtain the right treatment of sentence (14). (14) The girl wondered who John believed that Mary claimed that the baby saw. In a corresponding manner the complex NP &apos;whose playmate&apos;s nurse&apos; of sentence (8) is analysed. II. SEMANTIC REPRESENTATION representation we use the r(iscourse) R(epresentation) Theory) developped by Hans Kamp [4]. I.e. we do not adopt the semantic theory for L(exical) F(unctional) G(rammar) proposed by Per-Kristian Halverson [2]. Halverson translates the functional structures of LEG into so-called semantic structures being of the same structural nature, namely acyclic graphs. The semantic structures are the result of a translation procedure which is based on the association of formulas of intensional logic to the semantic forms appearing in the functional structure. The reason not to take this approach will be explained by postulating some requirements a semantic representation has to fulfill in order to account for a processing of texts. Then we will show that these requirements are really necessary by analysing some sample sentences and discourses. It will turn out that DRT accounts for them in an intuitively fully satisfactory way. Because we cannot review ERT in detail here the reader should consult one of the papers explaining the fundamentals of the theory (e.g. £43 ), or he should first look at the last paragraph in which an outline is given of how our parser is to be extended in order to yield an IR-tyed output instead of the &apos;traditional&apos; (semantic) functional structures. The basic building principle of a semantic representation is to associate with every significant lexical entry (i.e., every entry which does contribute to the truthcondidtional aspect of the meaning of a sentence) a semantic structure. Compositional principles, then, will construct the semantic representation of a sentence by combining these semantic structures according to their syntactic relations. The desired underlying principle is that the semantic structures associated with the semantic forms should not be changed during the composition process. To put it differently: one wants the association of the semantic independent of the syntactic context in which the semantic form appears. This requirement leads to difficulties in the tradition of translating sentences into formulas of e.g. predicate or intentional logic. Consider sentences (1) If john admires a woman then he kisses her and (2) Every man Who admires a woman kisses her the truth conditions of which are determined by the first order formulae (3) Vx ( woman(x) &amp; admire(John,x) --&gt; kiss(John,x) ) and (4) Vz:Vy ( man(z) &amp; wanan(y) &amp; admire(x,y) --&gt; kiss(x,y) ) respectively. The problem is that the definite description &amp;quot;a woman&amp;quot; reemerges as universally quantified in the logical representation and there is no way out, because the pronoun &amp;quot;she&amp;quot; has to be bound to the woman in question. ERT provides a general account of the meaning of indefinite descriptions, conditionals, universally quantified noun phrases and anaphoric pronouns, s.t. our first requirement is satisfied. The semantic representations (called IRS&apos; a) which are assigned to sentences in which such constructions jointly appear have the truth conditions which our intuitions attribute to then. The second reason why we decided to use ETR as semantic formalism for LEG is that the construction principles for a sentence S(i) of a text D= S(1),...,S(n) are formulated with respect to the semantic representation of the preceeding text S(1),...,S(i-1). Therefore the theory can account for intersentential semantic relationships in the same way as for intrasentential ones. This is the second requirement: a semantic representation has to represent the discourse as a whole and not as the mere union of the semantic representations of its isolated sentences. A third requirement a semantic representation has to fulfill is the reflection of configurational restrictions on anaphoric links: If one embeds sentence (2) into a conditional (6) *If every man who admires a woman kisses her then she is stressed the anaphoric link in (2) is preserved. But (6) does for configurational reasons not allow for an anaphoric relation the &amp;quot;she&amp;quot; and &amp;quot;a same happens intersententially as shown by (7) If John admires a woman then he kisses her. *She is enraged. A last requirement we will stipulate here is the following: It is neccessAry to draw inferences already during the construction of the semantic representation of a sentence 5(i) of the discourse. The inferences must operate on the semantic representation of the already analyzed discourse 5(1),...,S(i-1) as well as on a datahasP containing the knowledge the text talks about. This requirement is of major importance for the analysis of definite descriptions. Consider (8) Pedro is a farmer. If a woman loves him then he is happy. Mary loves Pedro. The happy farmer marries her in which the definite description &amp;quot;the happy farmer&amp;quot; is used to refer to refer to the individual denoted by &amp;quot;Pedro&amp;quot;. In order to get this link one has to infer that Pedro is indeed a happy and that he is the only one. If this were not the the use of the definite description would not be appropriate. Such a deduction mechanism is also needed to analyse sentence (9) John bought a car. The engine has 160 horse powers In this rAsP one has to take into account some knowledge of the world, namely the fact that every car has exactly one engine. Tb illustrate the way the semantic representation has to be interpreted let us have a brief look at the text-MS for the sample discourse (8) Thus a IRS K consists of (i) a set of discourse referents: discourse individuals, discourse events, discourse propositions, etc. (ii) a set of conditions of the following types atomic conditions, i.e. ir-ary relations over discourse referents complex conditions, i.e. n-ary relations (e.g. --&gt; or :) over sub-IRS&apos;s and discourse referents (e.g. g(1) --&gt; K(2) or uv Pedro = u fanner(u) --&gt; love(y,u) marry(u,v) = happy(u)i utman(y) 55 p:K, where p is a discourse proposition) Awhole DRS can be understood as partial model representing the individuals introduced by the discourse as well as the facts and rules those individuals are subject to. The truth conditions state that a ERS K is true in a model M if there is a proper imbedding from K into M. Proper embedding is defined as a function f from the set of discourse referents of K in to /1 s.t. (i) it is a hommorphism for the atomic conditions of the ERS and (ii) for the case of a complex condition K(1) --&gt; K(2) every proper Embedding of 101) that extends f is extendable to a proper embedding of 102). for the of a complex condition mcdeltheoretic object correlated with p (i.e. a proposition if p is a discourse proposition, an event if p is a discourse event, etc.) must be such that it allows for a proper embedding of K in it. Note that the definition of proper embedding has to be made more in order it to the special semantics one uses for propositional attitudes. We cannot go into details here. Nonetheless the truth condition as it stands should make clear the following: whether a discourse referent introduced implies existence or not depends on its position in the hierarchy of the IRS&apos;s. Given a ERS which is true in M then eactly those referents introduced in the very toplevel DRS imply existence; all others are to be interpreted as universally quantified, if they occur in an antecedent ERS, or as existentially quantified if they occur in a consequent DRS, or as having opaque status if they occur in a ERS specified by e.g. a discourse proposition. Thus the role of the hierarchical order of the IRS&apos;s is to build a base for the definition of truth conditions. But furthermore the hierarchy defines an accessibility relation, which restricts the set of possible antecedents of anaphoric NP&apos;s. This relation is (for the fragnent in as follows: For a given sub-ERS KO all referents occurring in KO or in any of the IRS&apos;s in which KO is embedded are accessible. Furthermore if KO is a consequent-ERS then the referents occurring in its corresponding antecedent DRS on the left are This gives us a correct treatment for (6) and (7). For the time being we have no algorithm which restricts and orders the set of possible anaphoric antecedents according to contextual conditions as given by e.g. (5) John is reading a book on syntax and Bill is reading a book on sematics. The former tis enjoying himself&apos; is a paperback Therefore our selection set is restricted only by the accessibility relation and the descriptive content of the anaphoric NP&apos;s. Of course for anaphoric pronouns this content reduced minimal, namely the grammatical features associated to them by the lexical entries. This accounts e.g. for the difference in acceptability of (10) and (11). Mary persuaded every man himself (11) *Mary promised every man to shave himself The IRS&apos;s for (10) and (11) show that both discourse referents, one for and the one for a accessible from position at which the has to be resolved. But if the &amp;quot;himself&amp;quot; of (11) is replaced by x it cannot be identified with y having the (not explicitely shown) feature female. Eefinite descriptions bear more information by virtue of the semantic content of their common-noun-phrases and the existence and uniqueness conditions presupposed by them. Therefore in definite descriptions we look for a discourse referent introduced in the preceding ERS for which the description holds and we have to Check whether this descrition holds for one referent only. Our algorithm proceeds as follows: First we build up a small DRS KO encoding the descriptive content of the common-noun-phrase of the definite description together with its uniquness and existency condition: KO: Second we have to show that we can prove KO out of the text-ERS of the preceeding discourse , with the restriction that only accessible referents are taken into account. The instantiation of *x by this proof gives us the correct antecedent the definite refers to. forget about KO and replace the antecedent discourse referent for the definite noun phrase to get the whole text-ERS (8&apos;). Of course it is possible that the presuppositions are not mentioned explicitely in the discourse but follow implicitely from the text alone or from the text together with the knowledge of the domain it talks about. So in cases like (9) John bought a car. The engine has 260 horse powers Pere the identified referent is functionally related to referents that are more directly accessible, namely to John&apos;s car. FUrthermore such a functional dependency confers to a definite description the power of introducing a new discourse referent, namely the engine which is functionally determined by the car of which it is part. This shifts the task from the for a direct antecedent for &amp;quot;the engine&amp;quot; =arch for the referent it is functionally related to. But the basic mechanism for finding this referent is the same deductive mechanism just outlined for the &amp;quot;happy farmer&amp;quot; example. III. TOWARIS AN INTERACTICV BETWEEN &amp;quot;GRAMMATICAL PARSDWP AND &amp;quot;LOGICAL PARS111;&amp;quot; In this section we will outline the principles underlying the extension of our parser to produce ERS&apos;s as output. Because none of the fragments of IRT contains Raising-and Equi-verbs taking infinitival or that-complements we are confronted with the task of writing construction rules for such verbs. It will turn out, however, that it is not difficult to see how to extend ERT to canprise such constructions. This is due to the fact that using LFG as syntactic base for IRT and not the categorial syntax of Kamp the unraveling of the thematic relations in a sentence is already accomplished in f-structure. Therefore it is straightforward to formulate construction rules which give the correct readings for (10) and (11) of the previous section, establish the propositional equivalence of pairs with or without Raising, Equi (see (1), (2)), etc. (1) John persuaded Mary to come (2) John persuaded Mary that she should come Let us first describe the DRS construction rules by the familiar example (3) every man loves a woman USing Kamp&apos;s categorial syntax, the construction rules operate top down the tree. The specification of the order in which the parts of the tree are to be treated is assumed to be given by the syntactic rules. I.e. the specification of scope order is directly determined by the syntactic construction of &apos;the deal with the point of scope ambiguities after having described the way a IRS is constructed. Our description operating bottom up instead top down is different from the one given in [4] in order to come closer to the point we want to make. But note that this difference is not one. Thus according to the first requirement of the previous section we assume that to each semantic from a semantic structure is associated. For the lexical entries of (3) we have :nary = y y ,x , p)/ x ishave(x,x) / man(x) -, jshave(y,himself)1 P: farmer(x) happy(x) farL(y)] y = x 56 the following: man --&gt; man(*) woman --&gt; woman(*) loves --&gt; love(*,*) . The semantic structures for the common nouns and the verbs are n,place predicates. The structure for &amp;quot;a&amp;quot; is a DRS with discourse individual v introduced and conditions not yet specified. The entry for &amp;quot;every&amp;quot; is a ERS with no discourse individuals introduced on the toplevel. It contains however a complex condition KO --&gt; K1 s.t a discourse individual x is introduced in KO and both KO and El contain any other conditions. The DRS construction rules specify how these senantic structures combined by propagating them up the tree. The easiest that is it by the following picture (for case of narrow sco read of &amp;quot;a : (4) man(*) love(*,*) every man_ loves a For the wide scope reading the NP-tree of &amp;quot;a woman&amp;quot; at the very end to give (5) The picture should make clear the way we want to extend the parsing mechanism described in section 1 in order to produce ERS&apos;s as output and no more f-structures: instead of partially instantiated f-structures determined by the lexical entries partially instantiated IRS&apos;s are passed around the tree getting accomplished by unification. The control mechanism of LFG will automatically put the discourse referents into the correct argument position of the verb. Thus no additional work has to be done for the grammatical relations of a sentence. But what About the logical relations? that each clausehas a unique head and that the features of each identified with those of its head. Fbr (3) the head of S -=&gt; NP VP is the VP and the head of VP --&gt; V NP is the V. Thus the outstanding role of the verb to determine and restrict the grammatical relations of the sentence is captured. (4) , however, shows that the logical relations of the sentence are mainly determined by its determiners, which are not heads of the NP-phrases and the NPphrases thenselveS are not the heads of the VPand S-phrase respectively. To account for this dichotomy we will call the syntactically defined notion of head &amp;quot;grammatical head&amp;quot; and we will introduce a further notion of &amp;quot;logical head&amp;quot; of a phrase. Of course, in order to make the definition work it has to be elaborated in a way that garantees that the logical head of a phrase is tniquely determied too. Consider (6) John persuaded an american to win (7) John expected an american to win for which we propose the following ERS&apos;s (6 ) (7&apos;) J Y John = j John = j expect(j,p) american(y) p:l y expect(j,p) p: twin(y)1 win(y) The tact that (7) does not neccesserily Imply existence of an american whereas (6) does is triggered by the difference between Equiand Raising-verbs. the NP to be the logical head of the phrase VP -&gt; V NP VP the logical relations of the VP would be those of the NP. This amounts to incorporating the logical structures of the V and the VP&apos; into the logical structure of the NT, which is for both (6) and (7) anerican(y) and thus would lead to the readings represented in (6&apos;) and (7&amp;quot;). Consequently (7&apos;) would not be produced. the logical head to be the would exclude the (7&amp;quot;). Evidently the last possibility of defining the logical head to be identical to the grammatical head, namely the V itself, seems to be the only solution. But this would block the construction already at the stage of unifying the NPand VIALstructures with persuade(*,*,*) or expect(*,*). At first thought one easy way out of this dilemma is to associate with the lexical entry of the verb not the mere n-place predicate but a ERS containing this predicate as atomic condition. This makes the unification possible but gives us the following result: (*) Of course one can say t t is open to produce the set of (6) and (7). But this one has to work an (*) after having reached the top of the tree a consequence that seems undesirable to us. Thus the only way out is to consider the logical head as not being uniquely identified by the mere phrase structure above example for the phrase VP --&gt; V NP shows its head depends on the verb class too. But we will still go further. claim that it is possible to make the to additionally depend on the order of the surface string, on the use of active and passive voice and probably others. This will give us a preference ordering of the scope ambiguities of .sentences as the following: - Every man loves a woman - Awoman is loved by every man - A ticket is bought by every man - Every man bought a ticket The properties of unification grammers listed above show that the theoretical framework does not impose any restrictions on that plan.</abstract>
<note confidence="0.928180818181818">REFERENCES [1] Bresnan, J. (ed.), &amp;quot;the Mental Representation of Grammatical Relations&amp;quot;. MIT Press, Cambridge, Mess., 1982 [2] Frey, Werner/ Reyle, UWe/ Rohrer, Christian, &amp;quot;Autaaatic Construction of a Knowledge Base by Analysing Texts in &apos; Natural Language&amp;quot;, in: Proceedings of the Eigth Intern. Joint Conference on Artificial Intelligence II, 1983 [3] Halverson, P.-k., &amp;quot;Semantics for Lexical Functional Grammar&amp;quot;. In: Linguistic Inquiry 14, 1982 [4] Kamp, Hans, &amp;quot;A Theory of Truth and Semantic Representa= tion&amp;quot;. In: J.A. Groenendijk, T.U.V. (ed.), Formal</note>
<abstract confidence="0.9084558">in the Study of Natural Language a—&gt; every --&gt; woman(*) is treated woman( Y) j y John = j american(y) P: win( Y) persuade(j,y,p)</abstract>
<intro confidence="0.605238">57</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>the Mental Representation of Grammatical Relations&amp;quot;.</title>
<date>1982</date>
<editor>Bresnan, J. (ed.),</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mess.,</location>
<contexts>
<context position="622" citStr="[1,44]" startWordPosition="104" endWordPosition="104">NTATION OF LEXICAL FUNCTIONAL GRAMMAR AS. A BASE FOR A NATURAL LANGUAGE PROCESSING SYSTEM Werner Frey and Uwe Reyle Department of Linguistics University of Stuttgart W-Germany O. ABSTRACT The atm of this paper is to present parts of our system [2], which is to construct a database out of a narrative natural language text. We think the parts are of interest in their awn. The paper consists of three sections: (I) We give a detailed description of the PROLOG - implementation of the parser which is based on the theory of lexical functional grammar (LR). The parser covers the fragment described in [1,44]. I.e., it is able to analyse constructions involving functional control and long distance dependencies. We will to show that - PROLOG provides an efficient tool for LFC,implementation: a phrase structure rule annotated with functional schemata likeS4 WP veis to be interpreted as, first, identifying the special tts414 t. grammatical relation of subject position of any sentence analyzed by this clause to be the NP appearing in it, and second, as identifying all grammatical relations of the sentence with those of the VP. This universal interpretation of the LFCfaretavariables t and 4, correspond</context>
<context position="4117" citStr="[1,44]" startWordPosition="661" endWordPosition="661">on. Since Lit is a kind of unification grammar it allows for constructing patterns Which enable the following sentences to refine or to change the content of these discourse features. Knowledge gathered by a preceding sentence can be used to lead the search in the lexicon by demanding that certain feature values match. In short we hope that the nearly uniform status of the different description tools allows simple procedures for the expansion and manipulation by other components of the system. But this was a look ahead. Let us now come to the less ambitious task of Implementing the grammar of [1,44]. Lexical functional grammar (Lit) is a theory that extends phrase structure grammars without using transformations. It emphasizes the role of the grammatical functions and of the lexicon. Another powerful formalism for describing natural languages - follows from a method of expressing grammars in logic eafled definite clause grammars (ECG). A DOG constitutes a PROLOG programme. We want to show first, how LPG can be translated into DCG and second, that PROLOG provides an efficient tool for LFC-implementation in that it allows for the construction of functional structures directly during the pa</context>
</contexts>
<marker>[1]</marker>
<rawString>Bresnan, J. (ed.), &amp;quot;the Mental Representation of Grammatical Relations&amp;quot;. MIT Press, Cambridge, Mess., 1982</rawString>
</citation>
<citation valid="true">
<authors>
<author>Werner Reyle Frey</author>
<author>UWe Rohrer</author>
<author>Christian</author>
</authors>
<title>Autaaatic Construction of a Knowledge Base by Analysing Texts in &apos; Natural Language&amp;quot;, in:</title>
<date>1983</date>
<booktitle>Proceedings of the Eigth Intern. Joint Conference on Artificial Intelligence II,</booktitle>
<contexts>
<context position="20685" citStr="[2]" startWordPosition="3365" endWordPosition="3365">e box. Here we use a test procedure which puts only the contollers indexed by S onto the controller list going to the S goal. Thereby we obtain the right treatment of sentence (14). (14) The girl wondered who John believed that Mary claimed that the baby saw. In a corresponding manner the complex NP &apos;whose playmate&apos;s nurse&apos; of sentence (8) is analysed. II. SEMANTIC REPRESENTATION As semantic representation we use the r(iscourse) R(epresentation) Theory) developped by Hans Kamp [4]. I.e. we do not adopt the semantic theory for L(exical) F(unctional) G(rammar) proposed by Per-Kristian Halverson [2]. Halverson translates the functional structures of LEG into so-called semantic structures being of the same structural nature, namely acyclic graphs. The semantic structures are the result of a translation procedure which is based on the association of formulas of intensional logic to the semantic forms appearing in the functional structure. The reason not to take this approach will be explained by postulating some requirements a semantic representation has to fulfill in order to account for a processing of texts. Then we will show that these requirements are really necessary by analysing som</context>
</contexts>
<marker>[2]</marker>
<rawString>Frey, Werner/ Reyle, UWe/ Rohrer, Christian, &amp;quot;Autaaatic Construction of a Knowledge Base by Analysing Texts in &apos; Natural Language&amp;quot;, in: Proceedings of the Eigth Intern. Joint Conference on Artificial Intelligence II, 1983</rawString>
</citation>
<citation valid="true">
<authors>
<author>P-k Halverson</author>
</authors>
<title>Semantics for Lexical Functional Grammar&amp;quot;. In: Linguistic Inquiry 14,</title>
<date>1982</date>
<marker>[3]</marker>
<rawString>Halverson, P.-k., &amp;quot;Semantics for Lexical Functional Grammar&amp;quot;. In: Linguistic Inquiry 14, 1982</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Kamp</author>
</authors>
<title>A Theory of Truth and Semantic Representa= tion&amp;quot;.</title>
<date>1981</date>
<booktitle>Formal Semantics in the Study of Natural Language I,</booktitle>
<editor>In: J.A. Groenendijk, T.U.V. (ed.),</editor>
<contexts>
<context position="1842" citStr="[4]" startWordPosition="293" endWordPosition="293">l quantification of variables appearing in PROIOG-clauses. The procedural semantics of PROLOG is such that the instantiation of the variables in a clause is inherited from the instantiation given by its subgoals, if they succeed. Thus there is no need for a separate component which solves the set of equations obtained by applying the LFG algorithm. -there is a canonical way of translating LFG into a PROLOG programme. (II) For the semantic representation of texts we use the Discourse Representation Theory developped by Bans Kamp. At present the impleanitation includes the fragnent described in [4]. In addition it analyses different types of negation and certain equi- and raising-verbs. We postulate same requirerents a semantic representation has OD fulfill in order to be able to analyse Whole texts. We show haw Vamp&apos;s theory meets these requirements by analyzing sample discourses involving anaphoric MP&apos;s. (III) Finally we sketch how the parser formalism can be augmented to yield as output discourse representation structures. To do this we introduce the new notion of &apos;logical head&apos; in addition to the Lit notion of &apos;grammatical head&apos;. The reason is the wellknown fact that the logical str</context>
<context position="20567" citStr="[4]" startWordPosition="3348" endWordPosition="3348">the rule in (13). (13) S&apos; ---&gt; (that) S it.os This rule has the effect that S-rooted contollers are allowed to pass the box. Here we use a test procedure which puts only the contollers indexed by S onto the controller list going to the S goal. Thereby we obtain the right treatment of sentence (14). (14) The girl wondered who John believed that Mary claimed that the baby saw. In a corresponding manner the complex NP &apos;whose playmate&apos;s nurse&apos; of sentence (8) is analysed. II. SEMANTIC REPRESENTATION As semantic representation we use the r(iscourse) R(epresentation) Theory) developped by Hans Kamp [4]. I.e. we do not adopt the semantic theory for L(exical) F(unctional) G(rammar) proposed by Per-Kristian Halverson [2]. Halverson translates the functional structures of LEG into so-called semantic structures being of the same structural nature, namely acyclic graphs. The semantic structures are the result of a translation procedure which is based on the association of formulas of intensional logic to the semantic forms appearing in the functional structure. The reason not to take this approach will be explained by postulating some requirements a semantic representation has to fulfill in order</context>
<context position="33148" citStr="[4]" startWordPosition="5434" endWordPosition="5434">describe the DRS construction rules by the familiar example (3) every man loves a woman USing Kamp&apos;s categorial syntax, the construction rules operate top down the tree. The specification of the order in which the parts of the tree are to be treated is assumed to be given by the syntactic rules. I.e. the specification of scope order is directly determined by the syntactic construction of &apos;the sentence. We will deal with the point of scope ambiguities after having described the way a IRS is constructed. Our description - operating bottom up instead top down - is different from the one given in [4] in order to come closer to the point we want to make. But note that this difference is not a genuine one. Thus according to the first requirement of the previous section we assume that to each semantic from a semantic structure is associated. For the lexical entries of (3) we have :nary = y r persuade( y ,x , p)/ Pranise(Yot ,p) 1 x ishave(x,x) / man(x) -, jshave(y,himself)1 P: farmer(x) happy(x) farL(y)] L happy(y) y = x 56 the following: man --&gt; man(*) woman --&gt; woman(*) loves --&gt; love(*,*) . The semantic structures for the common nouns and the verbs are n,place predicates. The structure fo</context>
</contexts>
<marker>[4]</marker>
<rawString>Kamp, Hans, &amp;quot;A Theory of Truth and Semantic Representa= tion&amp;quot;. In: J.A. Groenendijk, T.U.V. (ed.), Formal Semantics in the Study of Natural Language I, 1981</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>