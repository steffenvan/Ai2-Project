<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000017">
<title confidence="0.7144715">
Accuracy-Based Scoring for DOT: Towards Direct Error Minimization for
Data-Oriented Translation
</title>
<author confidence="0.688411">
Daniel Galron
</author>
<affiliation confidence="0.422351">
CIMS
</affiliation>
<address confidence="0.49878">
New York University
</address>
<email confidence="0.995856">
galron@cs.nyu.edu
</email>
<author confidence="0.816447">
Sergio Penkale, Andy Way
</author>
<affiliation confidence="0.701383">
CNGL
Dublin City University
</affiliation>
<email confidence="0.7535525">
{spenkale,away}
@computing.dcu.ie
</email>
<note confidence="0.566962333333333">
I. Dan Melamed
AT&amp;T Shannon Laboratory
{lastname}
</note>
<email confidence="0.986861">
@research.att.com
</email>
<sectionHeader confidence="0.9972" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999912416666667">
In this work we present a novel technique
to rescore fragments in the Data-Oriented
Translation model based on their contri-
bution to translation accuracy. We de-
scribe three new rescoring methods, and
present the initial results of a pilot experi-
ment on a small subset of the Europarl cor-
pus. This work is a proof-of-concept, and
is the first step in directly optimizing trans-
lation decisions solely on the hypothesized
accuracy of potential translations resulting
from those decisions.
</bodyText>
<sectionHeader confidence="0.99938" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999912303030303">
The Data-Oriented Translation (DOT) (Poutsma,
2000) model is a tree-structured translation model,
in which linked subtree fragments extracted from
a parsed bitext are composed to cover a source-
language sentence to be translated. Each linked
fragment pair consists of a source-language side
and a target-language side, similar to (Wu, 1997).
Translating a new sentence involves composing
the linked fragments into derivations so that a
new source-language sentence is covered by the
source tree fragments of the linked pairs, where
the yields of the target-side derivations are the can-
didate translations. Derivations are scored accord-
ing to their likelihood, and the translation is se-
lected from the derivation pair with the highest
score. However, we have no reason to believe that
maximizing likelihood is the best way to maxi-
mize translation accuracy – likelihood and accu-
racy do not necessarily correlate well.
We can frame the problem as a search problem,
where we are searching a space of derivations for
the one that yields the highest scoring translation.
By putting weights on the derivations in the search
space, we wish to point the decoder in the direc-
tion of the optimal translation. Since we want
the decoder to find the translation with the high-
est evaluation score, we would want to score the
derivations with weights that correlate well with
the particular evaluation measure in mind.
Much of the work in the MT literature has
focused on the scoring of translation decisions
made. (Yamada and Knight, 2001) follow (Brown
et al., 1993) in using the noisy channel model,
by decomposing the translation decisions mod-
eled by the translation model into different types,
and inducing probability distributions via max-
imum likelihood estimation over each decision
type. This model is then decoded as described
in (Yamada and Knight, 2002). This type of ap-
proach is also followed in (Galley et al., 2006).
There has been some previous work on
accuracy-driven training techniques for SMT, such
as MERT (Och, 2003) and the Simplex Armijo
Downhill method (Zhao and Chen, 2009), which
tune the parameters in a linear combination of var-
ious phrase scores according to a held-out tun-
ing set. While this does tune the relative weights
of the scores to maximize the accuracy of candi-
dates in the tuning set, the scores themselves in the
linear combination are not necessarily correlated
with the accuracy of the translation. Tillmann and
Zhang (2006) present a procedure to directly opti-
mize the global scoring function used by a phrase-
based decoder on the accuracy of the translations.
Similarly to MERT, Tillmann and Zhang estimate
the parameters of a weight vector on a linear com-
bination of (binary) features using a global objec-
tive function correlated with BLEU (Papineni et
al., 2002).
In this work, we prototype some methods for
moving directly towards incorporating a measure
of the translation quality of each fragment used,
bringing DOT more into the mainstream of cur-
rent SMT research. In Section 2 we describe
probability-based DOT fragment scoring. In Sec-
tion 3 we describe our rescoring setup and the
</bodyText>
<page confidence="0.979973">
371
</page>
<note confidence="0.9918575">
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 371–380,
Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<figureCaption confidence="0.999496">
Figure 1: Example DOT Fragments.
</figureCaption>
<bodyText confidence="0.998881">
three rescoring methods. In Section 4, we describe
our experiments. In Section 5 we compare the
results of rescoring the fragments with the three
methods. In Section 6 we discuss some of the
decisions that are affected by our rescoring meth-
ods. Finally, we discuss the next steps in training
the DOT system by optimizing over a translation
accuracy-based objective function in Section 7.
</bodyText>
<sectionHeader confidence="0.994106" genericHeader="method">
2 DOT Scoring
</sectionHeader>
<bodyText confidence="0.999737788461538">
As described in previous work (Poutsma, 2000;
Hearne and Way, 2003), DOT scores translations
according to the probabilities of the derivations,
which are in turn computed from the relative fre-
quencies of linked tree fragments in a parallel tree-
bank. Linked fragment pairs are conditionally in-
dependent, so the score of a derivation is the prod-
uct of the probabilities of all the linked fragments
used. To find the probability of a translation,
DOT marginalizes over the scores of all deriva-
tions yielding the translation.
From a parallel treebank aligned at the sub-
sentential level, we extract all possible linked frag-
ment pairs by first selecting all linked pairs of
nodes in the treebank to be the roots of a new sub-
tree pair, and then selecting a (possibly empty) set
of linked node pairs that are descendants of the
newly selected fragment roots and deleting all sub-
tree pairs dominated by these nodes. Leaves of
fragments can either be terminals, or non-terminal
frontier nodes where we can compose other frag-
ments (c.f. (Eisner, 2003)). We give example DOT
fragment pairs in Figure 1.
Given two subtree pairs (s1, t1) and (s2, t2),
we can compose them using the DOT composi-
tion operator o if the leftmost non-terminal fron-
tier node of s1 is equal to the root node of s2,
and the leftmost non-terminal frontier node of s1’s
linked counterpart in t1 is equal to the root node
of t2. The resulting tree pair consists of a copy
of s1 where s2 has been inserted at the leftmost
frontier node, and a copy of t1 where t2 has been
inserted at the node linked to s1’s leftmost frontier
node (Hearne and Way, 2003).
In Figure 1, fragment pair (a) is a fragment with
two open substitution sites. If we compose this
fragment pair with fragment pair (b), the source
side composition must take place on the leftmost
non-terminal frontier node (the leftmost NP). On
the target side we compose on the frontier linked
to the leftmost source side non-terminal frontier.
The result is fragment pair (c). If we now com-
pose the resulting fragment pair with fragment pair
(d), we obtain a fragment pair with no open sub-
stitution sites whose source-side yield is John likes
Mary and whose target-side yield is Mary plait a`
John. Note that there are two different derivations
using the fragment pairs in Figure 1 that result in
the same fragment pair, namely (a) o (b) o (d), and
(c) o (d).
For a given linked fragment pair (ds, dt), the
probability assigned to it is
</bodyText>
<equation confidence="0.987200666666667">
E
r(u3)=r(d3)∧r(ut)=r(dt) |(us, ut)|
(1)
</equation>
<bodyText confidence="0.999855625">
where |(ds, dt) |is the number of times the frag-
ment pair (ds, dt) is found in the bitext, and r(d)
is the root nonterminal of d. Essentially, the prob-
ability assigned to the fragment pair is the relative
frequency of the fragment pair to the pair of non-
terminals that root the fragments.
Then, with the assumption that DOT fragments
are conditionally independent, the probability of a
derivation is
In the original DOT formulation, DOT disam-
biguated translations according to their probabil-
ities. Since a translation can have many possible
derivations, to obtain the probability of a transla-
tion it is necessary to marginalize over the distinct
derivations yielding a translation. The probabil-
ity of a translation wt of a source sentence ws, is
</bodyText>
<figure confidence="0.994347414634146">
S
S
NP VP
V
likes
NP VP
V
plait
(a)
NP
P
PP
NP
(b)
NP
NP
S
S
NP VP
NP
VP
John
V
likes
NP
V
plait
PP
NP
John
NP
(d)
Mary
NP
Mary
P
a`
(c)
a`
John
John
</figure>
<equation confidence="0.91358376923077">
|(ds, dt)|
P ((ds, dt)) =
P(d) = P((ds, dt)1 o ... o (ds, dt)N)
11 = P((ds,dt)i) (2)
i
372
given by (3):
1:
P(ws,wt) = P(d(ws,wt)) (3)
d∈D
and the translation is chosen so as to maximize (4):
ˆwt = argmaxP(ws, wt) (4)
wt
</equation>
<bodyText confidence="0.9993122">
Hearne and Way (2006) examined alternative dis-
ambiguation strategies. They found that rather
than disambiguating on the translation probability,
the translation quality would improve by disam-
biguating on the derivation probability, as in (5):
</bodyText>
<equation confidence="0.8170465">
ˆwt = argmaxP(d) (5)
d
</equation>
<bodyText confidence="0.999926310344827">
Our analysis suggest that this is because many
derivations with very low probabilities generate
the same, poor translation. When applying Equa-
tion (3) to marginalize over those derivations, the
resulting score is higher for the poor translation
than a better translation with fewer derivations but
where the derivations had higher likelihood.
Using the DOT model directly is difficult –
the number of fragments extracted from a paral-
lel treebank is exponential in the size of the tree-
bank. Therefore we use the Goodman reduction
of DOT (Hearne, 2005) to create an isomorphic
PCFG representation of the DOT model that is lin-
ear in the size of the treebank. The idea behind the
Goodman reduction is that rather than storing frag-
ments in the grammar and translating via compo-
sition, we simultaneously build up the fragments
using the PCFG reduction and compose them to-
gether. To perform the reduction, we first relabel
the two linked nodes (X, Y) with the new label
X=Y. We then label each node in the parallel tree-
bank with a unique Goodman index. Each binary-
branching node and its two children can be inter-
nal or root/frontier. We add rules to the grammar
reflecting the role that each node can take, keeping
unaligned nodes as fragment-internal nodes. So in
the case where a node and both of its children are
aligned, we commit 8 rules into the grammar, as
follows:
</bodyText>
<table confidence="0.96409175">
LHS RHS1 RHS2 LHS+a RHS1 RHS2
LHS RHS1+b RHS2 LHS+a RHS1+b RHS2
LHS RHS1 RHS2+c LHS+a RHS1 RHS2+c
LHS RHS1+b RHS+c LHS+a RHS1+b RHS2+c
</table>
<footnote confidence="0.576481">
A category label which ends in a ‘+’ symbol fol-
lowed by a Goodman index is fragment-internal
and all other nodes are either fragment roots or
</footnote>
<table confidence="0.71999">
Source PCFG a` John
Target PCFG
S=S — N=N VP+2 0.5 S=S — N=N VP+2 0.5
S=S — N=N+3 VP+2 0.5 S=S — N=N+4 VP+2 0.5
S=S+1 — N=N VP+2 0.5 S=S+1 — N=N VP+2 0.5
S=S+1 — N=N+3 VP+2 0.5 S=S+1 — N=N+4 VP+2 0.5
N=N — John 0.5 N=N — Mary 0.5
N=N+3 — John 1 N=N+4 — Mary 1
VP+2 — V+4 N=N 0.5 VP+2 — V+5 PP+3 1
VP+2 — V+4 N=N+5 0.5 V+5 — plait 1
V+4 — likes 1 PP+3 — P+6 N=N 0.5
N=N — Mary 0.5 PP+3 — P+6 N=N+7 0.5
N=N+5 — Mary 1 P+6 — a` 1
N=N — John 0.5
N=N+7 — John 1
</table>
<figureCaption confidence="0.9980555">
Figure 2: A parallel tree and its corresponding Goodman re-
duction.
</figureCaption>
<bodyText confidence="0.998799419354839">
frontier nodes. A fragment pair, then, is a pair of
subtrees in which the root does not have an index,
all internal nodes have indices, and all the leaves
are either terminals or un-indexed nodes. We give
an example Goodman reduction in Figure 2.
While we store the source grammar and the tar-
get grammar separately, we also keep track of the
correspondence between source and target Good-
man indices and can easily identify the alignments
according to the Goodman indices. Probabilities
for the PCFG rules are computed monolingually
as in the standard Goodman reduction for DOP
(Goodman, 1996). In decoding with the Goodman
reduction, we first find the n-best parses on the
source side, and for each source fragment, we con-
struct the k-best fragments on the target side. We
finally compute the bilingual derivation probabil-
ities by multiplying the source and target deriva-
tion probabilities by the target fragment relative
frequencies conditioned on the source fragment.
There are a few problems with a likelihood-
based scoring scheme. First, it is not clear that
if a fragment is more likely to be seen in training
data then it is more likely to be used in a correct
translation of an unseen sentence. In our analysis
of the candidate translations of the DOT system,
we observed that frequently, the highest-likelihood
candidate translation output by the system was not
the highest-accuracy candidate inferred. An addi-
tional problem is that, as described in (Johnson,
2002), the relative frequency estimator for DOP
</bodyText>
<figure confidence="0.996784117647059">
S=S1
S=S1
N=N3
John
V4
likes
N=N5
Mary
N=N4
Mary
VP2
V5
plait
PP3
N=N7
VP2
P6
</figure>
<page confidence="0.995627">
373
</page>
<bodyText confidence="0.9824625">
(and by extension, DOT) is known to be biased
and inconsistent.
</bodyText>
<sectionHeader confidence="0.969583" genericHeader="method">
3 Accuracy-Based Fragment Scoring
</sectionHeader>
<bodyText confidence="0.999984166666667">
In our work, we wish to incorporate a measure
of fragment accuracy into the scoring. To do so,
we reformulate the scoring of DOT as log-linear
rather than probabilistic, in order to incorporate
non-likelihood features into the derivation scores.
For all tree fragment pairs (ds, dt), let
</bodyText>
<equation confidence="0.578913">
l((ds, dt)) = log(p((ds, dt))) (6)
</equation>
<bodyText confidence="0.9444965">
The general form of a rescored tree fragment will
be
</bodyText>
<equation confidence="0.9309455">
k
s((ds,dt)) = α0l((ds, dt)) + αifi((ds, dt))
i=1
(7)
</equation>
<bodyText confidence="0.9993242">
where each αi is the weight of that term in the fi-
nal score, and each fi(d) is a feature. In this work,
we only consider f1(d), an accuracy-based score,
although in future work we will consider a wide
variety of features in the scoring function, includ-
ing combinations of the different scoring schemes
described below, binary lexical features, binary
source-side syntactic features, and local target side
features. The score of a derivation is now given by
(8):
</bodyText>
<equation confidence="0.970805333333333">
s(d) = s((ds, dt)1 o ... o (ds, dt)N)
�= s((ds, dt)i) (8)
i
</equation>
<bodyText confidence="0.999941666666667">
In order to disambiguate between candidate
translations, we follow (Hearne and Way, 2006)
by using Equation (5).
</bodyText>
<subsectionHeader confidence="0.99543">
3.1 Structured Fragment Rescoring
</subsectionHeader>
<bodyText confidence="0.999990111111111">
In all our approaches, we rescore fragments ac-
cording to their contribution to the accuracy of
a translation. We would like to give fragments
that contribute to good translations relatively high
scores, and give fragments that contribute to bad
translations relatively low scores, so that during
decoding fragments that are known to contribute to
good translations would be chosen over those that
are known to contribute to bad translations. Fur-
thermore, we would like to score each fragment in
a derivation independently, since bad translations
may contain good fragments, and vice-versa.
In practice, it is infeasible to rescore only those
fragments seen during the rescoring process, due
to the Goodman reduction for DOT. If we were to
properly rescore each fragment, a new rule would
need to be added to the grammar for each rule ap-
pearing in the fragment. Since the number of frag-
ments is exponential, this would lead to a substan-
tial increase in grammar size. Instead, we rescore
the individual rules in the fragments, by evenly di-
viding the total amount of scoring mass among the
rules of the particular fragment, and then assigning
them the average of the rule scores over all frag-
ments in which they appear. That is for each rule
r in a fragment f consisting of cf(r) rules with
score δ(f), the score of the rule is given as:
</bodyText>
<equation confidence="0.9881345">
s(r) = Ef:r∈f δ (f )/cf(r) (11)
|f|
</equation>
<bodyText confidence="0.9984894">
This has the further advantage that we are al-
lowing fragments that were unseen during tuning
to be rescored according to previously seen frag-
ment substructures.
To implement this scheme, we select a set of or-
acle translations for each sentence in the tuning
data by evaluating all the candidate translations
against the gold standard translation using the F-
score (Turian et al., 2003), and selecting those
with the highest F1-measure, with exponent 1. We
use GTM, rather than BLEU, because BLEU is
not known to work well on a per-sentence level
(Lavie et al., 2004) as needed for oracle selection.
We then compare all the target-side fragments in-
ferred in the translation process for each candidate
translation against the fragments that yielded the
oracles. There are two relevant parts of the frag-
ments – the internal yields (i.e. the terminal leaves
of the fragment) and the substitution sites (i.e. the
frontiers where other fragments attach). We score
the fragments rooted at the substitution sites sepa-
rately from the parent fragment. We can uniquely
identify the set of fragments that can be rooted at
substitution sites by determining the span of the
linked source-side derivation.
To compare two fragments, we define an edit
distance between them. For a given fragment d,
let r(d) be the root of the fragment, let r(d) �
rhs1 be the left subtree of r(d), and let r(d) �
rhs2 be the right subtree. The difference between
a candidate fragment dc and an oracle fragment
dgs is given by the equations in Table 1.
These equations define a minimum edit dis-
tance between two fragment trees, allowing sub-
fragment order inversion, insertion, and deletion
</bodyText>
<page confidence="0.980269">
374
</page>
<table confidence="0.934329">
S(dc, dgs) = 1 0 if d� d93 Base case: dc and dgs are unary subtrees or substitution sites (9)
l � g
S(dc, dgs) = min I S(dc → rhs1, dgs → rhs1) + S(dc → rhs2, dgs → rhs2), (10)
S(dc → rhs2, dgs → rhs1) + S(dc → rhs1, dgs → rhs2) + 1,
S(dc, dgs → rhs1) + |y(dgs → rhs2)|,
S(dc, dgs → rhs2) + |y(dgs → rhs1)|,
S(dc → rhs1, dgs) + |y(dc → rhs2)|,
S(dc → rhs2, dgs) + |y(dc → rhs1)|
</table>
<tableCaption confidence="0.999915">
Table 1: The recursive relation defining the fragment difference between two fragments.
</tableCaption>
<figure confidence="0.981653434782609">
F
e
B
(c) D
A
tween it and the oracle fragment it is most similar
to, as in (12):
E
f((ds, dt)) _
max
(dos,dot)EDo:dos=ds
−δ(dt, dot) (12)
c
b
b
c
(a) A
B
(b) A
C
B
C
b f
</figure>
<figureCaption confidence="0.999752333333333">
Figure 3: Comparing trees (a) and (b) with our distance met-
ric yields a value of 1. The difference between trees (a) and
(c) is 2, and for trees (b) and (c) the distance is 3.
</figureCaption>
<bodyText confidence="0.999970794117647">
as edit operations. For example, the only dif-
ference between trees (a) and (b) in Figure 3 is
that their children have been inverted. To com-
pare these trees using our distance metric, we first
compute the first argument of the min function in
Equation (10), directly comparing the structure of
each immediate subtree. We then compute the sec-
ond argument, obtaining the cost of performing an
inversion, and finally compute the remaining argu-
ments, assessing the cost of allowing each tree to
be a direct subtree of the other. The result of this
computation is 1, representing the inversion oper-
ation required to transform tree (a) into tree (b).
If we compare trees (a) and (c) in Figure 3, we
obtain a value of 2, given that the minimum opera-
tions required to transform tree (a) into tree (c) are
inserting an additional subtree at the top level and
then substituting the subtree rooted by C for the
subtree rooted by F. If we compare tree (b) with
tree (c) then the distance is 3, since we are now
required to also replace the subtree rooted by C by
the one rooted by B.
Since it is not efficient to compute the differ-
ences directly, we utilize common substructures
and derive a dynamic programming implementa-
tion of the recursion. We compare each fragment
against the set of oracle fragments for the same
source span, and select the lowest cost as the score,
assigning the candidate the negative difference be-
In practice, given the Goodman reduction for
DOT, we divide the fragment score by the number
of rules in the fragment, and assign the average of
those scores for each rule instance across all frag-
ments rescored.
</bodyText>
<subsectionHeader confidence="0.904901">
3.2 Normalized Structured Fragment
Rescoring
</subsectionHeader>
<bodyText confidence="0.999661071428571">
In the structured fragment rescoring scheme, the
scores that the fragments are assigned are the un-
normalized edit distances between the two frag-
ments. It may be better to normalize the fragment
scores, rather than using the minimum number of
tree transformations to convert one fragment into
the other. We would expect that when compar-
ing larger fragments, on average there would be
more transformations needed to change one into
the other than when comparing small fragments.
However in the previous scheme, small fragments
would have higher scores than large fragments,
since fewer differences would be observed. The
normalized score is given in (13):
</bodyText>
<equation confidence="0.998938333333333">
log(1 − δ(dt, dot)/
max(|dt|,|dot|))
(13)
</equation>
<bodyText confidence="0.999674">
Essentially, we are normalizing the edit distance
by the maximum edit distance possible, namely
the size of the largest fragment of the two being
compared.
</bodyText>
<subsectionHeader confidence="0.995414">
3.3 Fragment Surface Rescoring
</subsectionHeader>
<bodyText confidence="0.9873165">
The disadvantage of the minimum tree fragment
edit approach is that it explicitly takes the internal
</bodyText>
<equation confidence="0.529132">
f((ds,dt)) _ max
(dos,dot)EDo:dos=ds
</equation>
<page confidence="0.995238">
375
</page>
<bodyText confidence="0.999482">
syntactic structure of the fragment into account.
In comparing two fragments, they may have the
same (or very similar) surface yields, but differ-
ent internal structures. The previous approach
would penalize the candidate fragment, even if its
yield is quite close to the oracle. In this rescor-
ing method, we extract the leaves of the candi-
date and oracle fragments, representing the substi-
tution sites by the source span which their frag-
ments cover. We then compare them using the
Damerau-Levenshtein distance Sdl(dc, dys) (Dam-
erau, 1964) between the two fragment yields, and
score them as in (14):
</bodyText>
<equation confidence="0.997142">
−Sdl(dt,dot) (14)
</equation>
<bodyText confidence="0.999891">
In Equation (14) we are selecting the maximal
score for (ds, dt) from its comparison to all the
possible corresponding oracle fragments. In this
way, we are choosing to score (ds, dt) against the
oracle fragment it is closest to.
</bodyText>
<sectionHeader confidence="0.999733" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999666482758621">
For our pilot experiments, we tested all the rescor-
ing methods in the previous section on Spanish-to-
English translation against the relative-frequency
baseline. We randomly selected 10,000 sentences
from the Europarl corpus (Koehn, 2005), and
parsed and aligned the bitext as described in (Tins-
ley et al., 2009). From the parallel treebank, we
extracted a Goodman reduction DOT grammar, as
described in (Hearne, 2005), although on an order
of magnitude greater amount of training data. Un-
like (Bod, 2007), we did not use the unsupervised
version of DOT, and did not attempt to scale up
our amount of training data to his levels, although
in ongoing work we are optimizing our system to
be able to handle that amount of training data. To
perform the rescoring, we randomly chose an ad-
ditional 30K sentence pairs from the Spanish-to-
English bitext. We rescored the grammar by trans-
lating the source side of the 10K training sentence
pairs and 10K of the additional sentences, and us-
ing the methods in Section 3 to score the frag-
ments derived in the translation process. We then
performed the same experiment translating the full
40K-sentence set. Rules in the grammar that were
not used during tuning were rescored using a de-
fault score defined to be the median of all scores
observed.
Our system performs translation by first obtain-
ing the n-best parses for the source sentences and
</bodyText>
<table confidence="0.9995955">
BLEU NIST F-SCORE
Baseline 8.78 3.582 38.21
2-8 4-6 5-5 6-4 8-2
BLEU SFR 10.30 10.31 10.32 10.27 10.08
NSFR 8.31 9.37 9.53 9.66 9.90
FSR 10.19 10.25 10.18 10.19 9.93
NIST SFR 3.792 3.805 3.808 3.800 3.781
NSFR 3.431 3.638 3.661 3.693 3.722
FSR 3.784 3.799 3.792 3.795 3.764
F-SCORE SFR 40.92 40.82 40.86 40.84 40.78
NSFR 37.53 39.50 39.93 40.38 40.78
FSR 40.83 40.85 40.87 40.91 40.67
</table>
<tableCaption confidence="0.992866833333333">
Table 2: Results on test set. Rescoring on 20K sentences.
SFR stands for Structured Fragment Rescoring, NSFR for
Normalized SFR and FSR for Fragment Surface Rescoring.
system-i-j represents the corresponding system with α0 = i
and α1 = j. Underlined results are statistically significantly
better than the baseline at p = 0.01.
</tableCaption>
<table confidence="0.999923583333333">
BLEU NIST F-SCORE
Baseline 8.78 3.582 38.21
2-8 4-6 5-5 6-4 8-2
BLEU SFR 10.59 10.58 10.41 10.38 10.08
NSFR 8.61 9.71 9.90 9.96 9.93
FSR 10.49 10.48 10.35 10.38 10.06
NIST SFR 3.841 3.835 3.810 3.807 3.785
NSFR 3.515 3.694 3.713 3.734 3.727
FSR 3.834 3.833 3.820 3.816 3.784
F-SCORE SFR 41.12 40.99 40.86 40.88 40.75
NSFR 38.16 40.39 40.69 40.90 40.75
FSR 41.03 41.02 41.01 40.98 40.72
</table>
<tableCaption confidence="0.895696666666667">
Table 3: Results on test set. Rescoring on 40K sentences. Un-
derlined are statistically significantly better than the baseline
at p = 0.01.
</tableCaption>
<bodyText confidence="0.999355818181818">
then computing the k-best bilingual derivations for
each source parse. In our experiments we used
beams of n = 10, 000 and k = 5. We also ex-
perimented with different values of α0 and α1 in
Equation (7). We set these parameters manually,
although in future work we will automatically tune
them, perhaps using a MERT-like algorithm.
We tested our rescored grammars on a set of
2,000 randomly chosen Europarl sentences, and
used a set of 200 randomly chosen sentences as
a development test set. 1
</bodyText>
<sectionHeader confidence="0.99986" genericHeader="method">
5 Results
</sectionHeader>
<bodyText confidence="0.998650714285714">
Translation quality results can be found in Tables
2 and 3. In these tables, columns labeled i-j in-
dicate that the corresponding system was trained
using parameters α0 = i and α1 = j in Equa-
tion 7. Statistical significance tests for NIST and
BLEU were performed using Bootstrap Resam-
pling (Koehn, 2004).
</bodyText>
<footnote confidence="0.9910085">
1All sentences, including the ones used for training, were
limited to a length of at most 20 words.
</footnote>
<table confidence="0.9183496">
f((ds, dt)) = Max
(dos,do t )∈Do:dos=ds
376
BLEU NIST F-SCORE
Baseline 10.82 3.493 42.31
2-8 4-6 5-5 6-4 8-2
BLEU SFR 11.34 12.12 11.94 11.97 11.78
NSFR 9.68 10.99 11.38 11.63 11.30
FSR 11.40 11.49 11.72 11.91 11.72
NIST SFR 3.653 3.727 3.723 3.708 3.694
NSFR 3.376 3.530 3.554 3.616 3.572
FSR 3.655 3.675 3.698 3.701 3.675
F-SCORE SFR 44.84 45.47 45.36 45.33 45.08
NSFR 41.44 43.38 44.18 44.79 44.26
FSR 44.68 44.91 45.15 45.19 44.82
</table>
<tableCaption confidence="0.9920175">
Table 4: Results on development test set. Rescoring on 40K
sentences.
</tableCaption>
<bodyText confidence="0.999992447761194">
As Table 2 indicates, all three rescoring meth-
ods significantly outperform the relative frequency
baseline. The unnormalized structured fragment
rescoring method performed the best, with the
largest improvement of 1.5 BLEU points, a 17.5%
relative improvement. We note that the BLEU
scores for both the baseline and the experiments
are low. This is to be expected, because the gram-
mar is extracted from a very small bitext espe-
cially when the heterogeneity of the Europarl cor-
pus is considered. In our analysis, only 32.5 per-
cent of the test sentences had a complete source-
side parse, meaning that a lot of structural infor-
mation is lost contributing to arbitrary target-side
ordering. In these experiments we did not use an
additional language model. DOT (and many other
syntax-based SMT systems) essentially have the
target language model encoded within the trans-
lation model, since the inferences derived dur-
ing translations link source structures to target
structures, so in principle, no additional language
model should be necessary. Furthermore, we only
evaluate against a single reference, which also
contributes to the lowering of absolute scores. To
provide a sanity check against a state-of-the-art
system, we trained the Moses phrase-based MT
system (Koehn et al., 2007) using our training
corpus, using no language model and using uni-
form feature weights, to provide a fair comparison
against our baseline. We used this system to de-
code our development test set, and as a result we
obtained a BLEU score of 10.72, which is compa-
rable to the score obtained by our baseline on the
same set.
When we scale up to tuning on 40,000 sen-
tences we see an improvement in BLEU scores as
well, as shown in Table 3. When tuning on 40K
sentences, we observe an increase of 1.81 BLEU
points on the best-performing system, which is a
20.6% improvement over the baseline. We note
that rescoring on 20K sentences rescores approxi-
mately 275,000 rules out of 655,000 in the gram-
mar, whereas rescoring on 40K sentences rescores
approximately 280,000.
To analyze the benefits of the rescored gram-
mar, we set aside a separate development set that
we decoded with the grammar trained on 40K sen-
tences. The results are presented in Table 4. The
analysis is presented in Section 6.
Interestingly, there is a large difference between
the normalized and unnormalized versions of the
SFR scoring scheme. Our analysis suggests that
the differences are mostly due to numerical issues,
namely the difference in magnitude between the
NSFR scores and the likelihood scores in the linear
combination, and the default value assigned when
the NSFR score was zero. In ongoing work, we
are working to address these issues.
For most configurations the difference between
SFR and FSR was not statistically significant at
p = 0.05. Our analysis indicated that surface dif-
ferences tended to co-occur with structural differ-
ences. We hypothesize that as we scale up to larger
and more ambiguous grammars, the system will
infer more derivations with the same yields, ren-
dering a larger difference between the quality of
the two scoring mechanisms.
</bodyText>
<sectionHeader confidence="0.999059" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999775714285714">
To analyze the advantages and disadvantages of
our approach over the baseline, we closely ex-
amined and compared the derivations made on
the devset translation by the SFR-scored gram-
mar and the likelihood-scored grammar. Although
the BLEU scores are rather low, there were sev-
eral sentences in which the SFR-scored grammar
showed a marked improvement over the baseline.
We observed two types of improvements.
The first is where the rescored grammar gave
us translations that, while still generally bad, were
closer to the gold standard than the baseline trans-
lation. For example, the Spanish sentence “Y en
tercer lugar , estiel problema de la aplicaci´on uni-
forme del Derecho comunitario .” translates into
the gold standard “Thirdly , we have the problem
of the uniform application of Community law .”
The baseline grammar translates the sentence as
“on third place , Transport and Tourism . I are
the problems of the implementation standardised
is the EU law .” with a GTM F-Score of 0.378,
</bodyText>
<page confidence="0.973325">
377
</page>
<figure confidence="0.986040454545455">
sn=NP+67600 −1.97/−5.66 sn=NP+36950 −5.89/−5.09
sp=PP+36951 −4.28/−3.81
NP+67608
sp=PP+67601
NP+36952
the rapporteur
the rapporteur
s=IN −0.48/−0.37 sn=SBAR+165198 −1.39/−1.90 s=IN −0.48/−0.37 sn=NP+36953
nc=TO+165203 dn=VP 0/−0.49 dn=DT 0/−0.58 nc=NNS −1.03/−0.81
to make both questions
in in
</figure>
<figureCaption confidence="0.998959666666667">
Figure 4: Target side of the highest-scoring translations for a sentence, according to the baseline system (left) and the SFR
system (right). Boxed nodes are substitution sites. Scores in superscripts denote the score of the sub-derivation according to
the baseline and to the SFR system.
</figureCaption>
<bodyText confidence="0.999983639344263">
and the rescored grammar outputs the translation
“to there in the third place , I are the problem of
the implementation standardised is the Commu-
nity law .”, with an F-Score of 0.5. While many of
the fragments in the derivations that yielded these
two translations differ, the ones we would like to
focus on are the fragments that yield the transla-
tion of “comunitario”. The grammar contains sev-
eral competing unary fragment pairs for “comuni-
taro”. In the baseline grammar, the pair (aq=NNP
—* comunitario, aq=NNP —* EU) has a score
of −0.693147, whereas the pair (aq=NNP —*
comunitario, aq=NNP —* Community) has a
score of −1.38629. In the rescored grammar how-
ever, (aq=NNP —* comunitario, aq=NNP —*
EU) has a score of -0.762973, whereas (aq=NNP
—* comunitario, aq=NNP —* Community)
has a score of -0.74399. In effect, the rescoring
scheme rescored the word alignment itself. This
suggests that in future work, it may be possible
to integrate a word aligner or fragment aligner di-
rectly into the MT training method.
The other improvement was where the baseline
and the SFR-scored grammar output translations
of roughly the same quality according to the eval-
uation measure, yet in terms of human evaluation,
the SFR translation was much better than the base-
line translation. For instance, our devset contained
the Spanish sentence “Estoy de acuerdo con el po-
nente en dos cuestiones .” The baseline transla-
tion given is “I agree with the rapporteur in to
make .”, and the SFR-scored translation given is
“I agree with the rapporteur in both questions .”.
While both translations have the same GTM score
against the gold standard “I agree with the rap-
porteur on two issues .”, clearly, the second one
is of far higher quality than the first. As we can
see in Figure 4, the derivation over the substring
“in both questions” gets a higher score than “in
to make” when translated with the rescored gram-
mar. In the baseline, “en dos cuestiones” is not
translated as a whole unit – rather, the derivation of
“el ponente en dos cuestiones” is decomposed into
four subderivations, yielding “el” “ponente” “en”
“dos cuestiones”, where each of those is translated
separately, into “0” “the rapporteur” “in” and “to
make”. The SFR-scored grammar, however, out-
puts a different bilingual derivation. The source
is decomposed into five sub-derivations, one for
each word, and each word is translated separately.
Then, the rescored target fragments set the proper
target-side word order and select the target-side
words that maximize the score of the subderiva-
tion covering the source span. We note that in this
example, the score of translating “dos” to “make”
was higher than the score of translating “dos” to
“both”. However, the higher level target frag-
ment that composed the translation of “dos” to-
gether with the translation of “cuestiones” yielded
a higher score when composing “both questions”
rather than “to make”.
</bodyText>
<sectionHeader confidence="0.998007" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999990222222222">
The results presented above indicate that aug-
menting the scoring mechanism with an accuracy-
based measure is a promising direction for transla-
tion quality improvement. It gives us a statistically
significant improvement over the baseline, and our
analysis has indicated that the system is indeed
making better decisions, moving us a step closer
towards the goal of making translation decisions
based on the hypothesis of the resulting transla-
</bodyText>
<page confidence="0.996009">
378
</page>
<bodyText confidence="0.999649305555556">
tion’s accuracy.
Now that we have demonstrated that translation
quality can be improved by incorporating a mea-
sure of fragment quality into the scoring scheme,
our immediate next step is to optimize our sys-
tem so that we can scale up to significantly larger
training and tuning sets, and determine whether
the improvements we have noted carry over when
the likelihood is computed from more data. Af-
terwards, we will implement a training scheme
to maximize an accuracy-based objective func-
tion, for instance, by minimizing the difference
between the scores of the highest-scoring deriva-
tion and the oracle derivations, in effect maximiz-
ing the score of the highest-scoring translation.
The rescoring method presented in this paper
need not be limited to DOT. Fragments can be
thought of as analogous to phrases in Phrase-
Based SMT systems – we could implement a sim-
ilar rescoring system for phrase-based systems,
where we generate several candidate translations
for source sentences in a tuning set, and score each
phrase used against the phrases used in a set of or-
acles. More broadly, we could potentially take any
statistical MT system, and compare the features
of all candidates generated against those of oracle
translations, and score those that are closer to the
oracle higher than those further away.
Finally, by explicitly framing the translation
problem as a search problem, where we are di-
vorcing the inferences in the search space (i.e.
the model) from the path we take to find the op-
timal inference according to some criterion (i.e.
the scoring scheme), we can remove some of the
variability when comparing two models or scoring
mechanisms (Lopez, 2009).
</bodyText>
<sectionHeader confidence="0.998023" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999361">
This work is supported by Science Foundation Ire-
land (Grant No. 07/CE/I1142). We would like to
thank the anonymous reviewers for their helpful
comments and suggestions.
</bodyText>
<sectionHeader confidence="0.998415" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999941396226415">
R. Bod. 2007. Unsupervised syntax-based ma-
chine translation: The contribution of discontiguous
phrases. In Proceedings of the 11th Machine Trans-
lation Summit, pages 51–57, Copenhagen, Den-
mark.
P. F. Brown, S. Della Pietra, V. Della Pietra, and
R. Mercer. 1993. The mathematics of statistical ma-
chine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263–311.
F. J. Damerau. 1964. A technique for computer de-
tection and correction of spelling errors. Commun.
ACM, 7(3):171–176.
J. Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings of the
41st Annual Meeting of the Association for Com-
putational Linguistics (ACL), Companion Volume,
pages 205–208, Sapporo.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. De-
Neefe, W. Wang, and I. Thayer. 2006. Scalable in-
ference and training of context-rich syntactic trans-
lation models. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 961–968, Sydney, Aus-
tralia.
J. Goodman. 1996. Efficient algorithms for parsing the
DOP model. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 143–152, Philadelphia, PA.
M. Hearne and A. Way. 2003. Seeing the wood for the
trees: Data-oriented translation. In Proceedings of
the Ninth Machine Translation Summit, pages 165–
172, New Orleans, LA.
M. Hearne and A. Way. 2006. Disambiguation strate-
gies for data-oriented translation. In Proceedings of
the 11th Conference of the European Association for
Machine Translation, pages 59–68, Oslo, Norway.
M. Hearne. 2005. Data-Oriented Models of Parsing
and Translation. Ph.D. thesis, Dublin City Univer-
sity, Dublin, Ireland.
M. Johnson. 2002. The DOP estimation method is
biased and inconsistent. Computational Linguistics,
28(1):71–76, March.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings
of the Annual Meeting of the Association for Com-
putational Linguistics, demonstation session, pages
177–180, Prague, Czech Republic.
P. Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
</reference>
<page confidence="0.988972">
379
</page>
<reference confidence="0.999806015873016">
the Conference on Empirical Methods in Natural
Language Processing, pages 388–395, Barcelona,
Spain.
P. Koehn. 2005. Europarl: A Parallel Corpus for Sta-
tistical Machine Translation. In Machine Transla-
tion Summit X, pages 79–86, Phuket, Thailand.
A. Lavie, K. Sagae, and S. Jayaraman. 2004. The sig-
nificance of recall in automatic metrics for MT eval-
uation. In Proceedings of the 6th Conference of the
Association for Machine Translation in the Ameri-
cas, pages 134–143, Washington, DC.
A. Lopez. 2009. Translation as weighted deduction. In
Proceedings of the 12th Conference of the European
Chapter of the ACL (EACL 2009), pages 532–540,
Athens, Greece.
F. J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proceedings of the 41st
Annual Meeting of the Association for Computa-
tional Linguistics, pages 160–167, Sapporo, Japan.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of 40th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 311–318, Philadelphia, PA.
A. Poutsma. 2000. Data-oriented translation. In The
18th International Conference on Computational
Linguistics, pages 635–641, Saarbr¨ucken, Germany.
C. Tillmann and T. Zhang. 2006. A discrimina-
tive global training algorithm for statistical MT. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 721–728, Sydney, Australia.
J. Tinsley, M. Hearne, and A. Way. 2009. Parallel tree-
banks in phrase-based statistical machine transla-
tion. In Proceedings of the Tenth International Con-
ference on Intelligent Text Processing and Computa-
tional Linguistics (CICLing), pages 318–331, Mex-
ico City, Mexico.
J. Turian, L. Shen, and I. D. Melamed. 2003. Eval-
uation of machine translation and its evaluation. In
Proceedings of the Ninth Machine Translation Sum-
mit, pages 386–393, New Orleans, LA.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377–404.
K. Yamada and K. Knight. 2001. A syntax-based
statistical translation model. In Proceedings of
39th Annual Meeting of the Association for Com-
putational Linguistics, pages 523–530, Toulouse,
France.
K. Yamada and K. Knight. 2002. A decoder for
syntax-based statistical MT. In Proceedings of 40th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 303–310, Philadelphia, PA.
B. Zhao and S. Chen. 2009. A simplex armijo
downhill algorithm for optimizing statistical ma-
chine translation decoding parameters. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics, Companion Volume: Short Papers, pages 21–
24, Boulder, Colorado.
</reference>
<page confidence="0.998279">
380
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.496796">
<title confidence="0.998913">Accuracy-Based Scoring for DOT: Towards Direct Error Minimization for Data-Oriented Translation</title>
<author confidence="0.81542">Daniel New York</author>
<email confidence="0.999722">galron@cs.nyu.edu</email>
<author confidence="0.9337875">Sergio Penkale</author>
<author confidence="0.9337875">Andy Dublin City</author>
<email confidence="0.887114">@computing.dcu.ie</email>
<author confidence="0.99899">I Dan</author>
<affiliation confidence="0.993788">AT&amp;T Shannon</affiliation>
<email confidence="0.999701">@research.att.com</email>
<abstract confidence="0.999770076923077">In this work we present a novel technique to rescore fragments in the Data-Oriented Translation model based on their contribution to translation accuracy. We describe three new rescoring methods, and present the initial results of a pilot experiment on a small subset of the Europarl corpus. This work is a proof-of-concept, and is the first step in directly optimizing translation decisions solely on the hypothesized accuracy of potential translations resulting from those decisions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Bod</author>
</authors>
<title>Unsupervised syntax-based machine translation: The contribution of discontiguous phrases.</title>
<date>2007</date>
<booktitle>In Proceedings of the 11th Machine Translation Summit,</booktitle>
<pages>51--57</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="21264" citStr="Bod, 2007" startWordPosition="3655" endWordPosition="3656">n this way, we are choosing to score (ds, dt) against the oracle fragment it is closest to. 4 Experiments For our pilot experiments, we tested all the rescoring methods in the previous section on Spanish-toEnglish translation against the relative-frequency baseline. We randomly selected 10,000 sentences from the Europarl corpus (Koehn, 2005), and parsed and aligned the bitext as described in (Tinsley et al., 2009). From the parallel treebank, we extracted a Goodman reduction DOT grammar, as described in (Hearne, 2005), although on an order of magnitude greater amount of training data. Unlike (Bod, 2007), we did not use the unsupervised version of DOT, and did not attempt to scale up our amount of training data to his levels, although in ongoing work we are optimizing our system to be able to handle that amount of training data. To perform the rescoring, we randomly chose an additional 30K sentence pairs from the Spanish-toEnglish bitext. We rescored the grammar by translating the source side of the 10K training sentence pairs and 10K of the additional sentences, and using the methods in Section 3 to score the fragments derived in the translation process. We then performed the same experiment</context>
</contexts>
<marker>Bod, 2007</marker>
<rawString>R. Bod. 2007. Unsupervised syntax-based machine translation: The contribution of discontiguous phrases. In Proceedings of the 11th Machine Translation Summit, pages 51–57, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
<author>R Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="2369" citStr="Brown et al., 1993" startWordPosition="364" endWordPosition="367">problem as a search problem, where we are searching a space of derivations for the one that yields the highest scoring translation. By putting weights on the derivations in the search space, we wish to point the decoder in the direction of the optimal translation. Since we want the decoder to find the translation with the highest evaluation score, we would want to score the derivations with weights that correlate well with the particular evaluation measure in mind. Much of the work in the MT literature has focused on the scoring of translation decisions made. (Yamada and Knight, 2001) follow (Brown et al., 1993) in using the noisy channel model, by decomposing the translation decisions modeled by the translation model into different types, and inducing probability distributions via maximum likelihood estimation over each decision type. This model is then decoded as described in (Yamada and Knight, 2002). This type of approach is also followed in (Galley et al., 2006). There has been some previous work on accuracy-driven training techniques for SMT, such as MERT (Och, 2003) and the Simplex Armijo Downhill method (Zhao and Chen, 2009), which tune the parameters in a linear combination of various phrase</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. Della Pietra, V. Della Pietra, and R. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Damerau</author>
</authors>
<title>A technique for computer detection and correction of spelling errors.</title>
<date>1964</date>
<journal>Commun. ACM,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="20437" citStr="Damerau, 1964" startWordPosition="3520" endWordPosition="3522">t it explicitly takes the internal f((ds,dt)) _ max (dos,dot)EDo:dos=ds 375 syntactic structure of the fragment into account. In comparing two fragments, they may have the same (or very similar) surface yields, but different internal structures. The previous approach would penalize the candidate fragment, even if its yield is quite close to the oracle. In this rescoring method, we extract the leaves of the candidate and oracle fragments, representing the substitution sites by the source span which their fragments cover. We then compare them using the Damerau-Levenshtein distance Sdl(dc, dys) (Damerau, 1964) between the two fragment yields, and score them as in (14): −Sdl(dt,dot) (14) In Equation (14) we are selecting the maximal score for (ds, dt) from its comparison to all the possible corresponding oracle fragments. In this way, we are choosing to score (ds, dt) against the oracle fragment it is closest to. 4 Experiments For our pilot experiments, we tested all the rescoring methods in the previous section on Spanish-toEnglish translation against the relative-frequency baseline. We randomly selected 10,000 sentences from the Europarl corpus (Koehn, 2005), and parsed and aligned the bitext as d</context>
</contexts>
<marker>Damerau, 1964</marker>
<rawString>F. J. Damerau. 1964. A technique for computer detection and correction of spelling errors. Commun. ACM, 7(3):171–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Learning non-isomorphic tree mappings for machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL), Companion Volume,</booktitle>
<pages>205--208</pages>
<location>Sapporo.</location>
<contexts>
<context position="5566" citStr="Eisner, 2003" startWordPosition="893" endWordPosition="894">f a translation, DOT marginalizes over the scores of all derivations yielding the translation. From a parallel treebank aligned at the subsentential level, we extract all possible linked fragment pairs by first selecting all linked pairs of nodes in the treebank to be the roots of a new subtree pair, and then selecting a (possibly empty) set of linked node pairs that are descendants of the newly selected fragment roots and deleting all subtree pairs dominated by these nodes. Leaves of fragments can either be terminals, or non-terminal frontier nodes where we can compose other fragments (c.f. (Eisner, 2003)). We give example DOT fragment pairs in Figure 1. Given two subtree pairs (s1, t1) and (s2, t2), we can compose them using the DOT composition operator o if the leftmost non-terminal frontier node of s1 is equal to the root node of s2, and the leftmost non-terminal frontier node of s1’s linked counterpart in t1 is equal to the root node of t2. The resulting tree pair consists of a copy of s1 where s2 has been inserted at the leftmost frontier node, and a copy of t1 where t2 has been inserted at the node linked to s1’s leftmost frontier node (Hearne and Way, 2003). In Figure 1, fragment pair (</context>
</contexts>
<marker>Eisner, 2003</marker>
<rawString>J. Eisner. 2003. Learning non-isomorphic tree mappings for machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL), Companion Volume, pages 205–208, Sapporo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>J Graehl</author>
<author>K Knight</author>
<author>D Marcu</author>
<author>S DeNeefe</author>
<author>W Wang</author>
<author>I Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>961--968</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="2731" citStr="Galley et al., 2006" startWordPosition="422" endWordPosition="425">nt to score the derivations with weights that correlate well with the particular evaluation measure in mind. Much of the work in the MT literature has focused on the scoring of translation decisions made. (Yamada and Knight, 2001) follow (Brown et al., 1993) in using the noisy channel model, by decomposing the translation decisions modeled by the translation model into different types, and inducing probability distributions via maximum likelihood estimation over each decision type. This model is then decoded as described in (Yamada and Knight, 2002). This type of approach is also followed in (Galley et al., 2006). There has been some previous work on accuracy-driven training techniques for SMT, such as MERT (Och, 2003) and the Simplex Armijo Downhill method (Zhao and Chen, 2009), which tune the parameters in a linear combination of various phrase scores according to a held-out tuning set. While this does tune the relative weights of the scores to maximize the accuracy of candidates in the tuning set, the scores themselves in the linear combination are not necessarily correlated with the accuracy of the translation. Tillmann and Zhang (2006) present a procedure to directly optimize the global scoring f</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe, W. Wang, and I. Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 961–968, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goodman</author>
</authors>
<title>Efficient algorithms for parsing the DOP model.</title>
<date>1996</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>143--152</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="11160" citStr="Goodman, 1996" startWordPosition="1918" endWordPosition="1919">ng Goodman reduction. frontier nodes. A fragment pair, then, is a pair of subtrees in which the root does not have an index, all internal nodes have indices, and all the leaves are either terminals or un-indexed nodes. We give an example Goodman reduction in Figure 2. While we store the source grammar and the target grammar separately, we also keep track of the correspondence between source and target Goodman indices and can easily identify the alignments according to the Goodman indices. Probabilities for the PCFG rules are computed monolingually as in the standard Goodman reduction for DOP (Goodman, 1996). In decoding with the Goodman reduction, we first find the n-best parses on the source side, and for each source fragment, we construct the k-best fragments on the target side. We finally compute the bilingual derivation probabilities by multiplying the source and target derivation probabilities by the target fragment relative frequencies conditioned on the source fragment. There are a few problems with a likelihoodbased scoring scheme. First, it is not clear that if a fragment is more likely to be seen in training data then it is more likely to be used in a correct translation of an unseen s</context>
</contexts>
<marker>Goodman, 1996</marker>
<rawString>J. Goodman. 1996. Efficient algorithms for parsing the DOP model. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 143–152, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearne</author>
<author>A Way</author>
</authors>
<title>Seeing the wood for the trees: Data-oriented translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Ninth Machine Translation Summit,</booktitle>
<pages>165--172</pages>
<location>New Orleans, LA.</location>
<contexts>
<context position="4593" citStr="Hearne and Way, 2003" startWordPosition="727" endWordPosition="730">e on Empirical Methods in Natural Language Processing, pages 371–380, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP Figure 1: Example DOT Fragments. three rescoring methods. In Section 4, we describe our experiments. In Section 5 we compare the results of rescoring the fragments with the three methods. In Section 6 we discuss some of the decisions that are affected by our rescoring methods. Finally, we discuss the next steps in training the DOT system by optimizing over a translation accuracy-based objective function in Section 7. 2 DOT Scoring As described in previous work (Poutsma, 2000; Hearne and Way, 2003), DOT scores translations according to the probabilities of the derivations, which are in turn computed from the relative frequencies of linked tree fragments in a parallel treebank. Linked fragment pairs are conditionally independent, so the score of a derivation is the product of the probabilities of all the linked fragments used. To find the probability of a translation, DOT marginalizes over the scores of all derivations yielding the translation. From a parallel treebank aligned at the subsentential level, we extract all possible linked fragment pairs by first selecting all linked pairs of</context>
<context position="6136" citStr="Hearne and Way, 2003" startWordPosition="1000" endWordPosition="1003">we can compose other fragments (c.f. (Eisner, 2003)). We give example DOT fragment pairs in Figure 1. Given two subtree pairs (s1, t1) and (s2, t2), we can compose them using the DOT composition operator o if the leftmost non-terminal frontier node of s1 is equal to the root node of s2, and the leftmost non-terminal frontier node of s1’s linked counterpart in t1 is equal to the root node of t2. The resulting tree pair consists of a copy of s1 where s2 has been inserted at the leftmost frontier node, and a copy of t1 where t2 has been inserted at the node linked to s1’s leftmost frontier node (Hearne and Way, 2003). In Figure 1, fragment pair (a) is a fragment with two open substitution sites. If we compose this fragment pair with fragment pair (b), the source side composition must take place on the leftmost non-terminal frontier node (the leftmost NP). On the target side we compose on the frontier linked to the leftmost source side non-terminal frontier. The result is fragment pair (c). If we now compose the resulting fragment pair with fragment pair (d), we obtain a fragment pair with no open substitution sites whose source-side yield is John likes Mary and whose target-side yield is Mary plait a` Joh</context>
</contexts>
<marker>Hearne, Way, 2003</marker>
<rawString>M. Hearne and A. Way. 2003. Seeing the wood for the trees: Data-oriented translation. In Proceedings of the Ninth Machine Translation Summit, pages 165– 172, New Orleans, LA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearne</author>
<author>A Way</author>
</authors>
<title>Disambiguation strategies for data-oriented translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Association for Machine Translation,</booktitle>
<pages>59--68</pages>
<location>Oslo,</location>
<contexts>
<context position="8156" citStr="Hearne and Way (2006)" startWordPosition="1372" endWordPosition="1375"> can have many possible derivations, to obtain the probability of a translation it is necessary to marginalize over the distinct derivations yielding a translation. The probability of a translation wt of a source sentence ws, is S S NP VP V likes NP VP V plait (a) NP P PP NP (b) NP NP S S NP VP NP VP John V likes NP V plait PP NP John NP (d) Mary NP Mary P a` (c) a` John John |(ds, dt)| P ((ds, dt)) = P(d) = P((ds, dt)1 o ... o (ds, dt)N) 11 = P((ds,dt)i) (2) i 372 given by (3): 1: P(ws,wt) = P(d(ws,wt)) (3) d∈D and the translation is chosen so as to maximize (4): ˆwt = argmaxP(ws, wt) (4) wt Hearne and Way (2006) examined alternative disambiguation strategies. They found that rather than disambiguating on the translation probability, the translation quality would improve by disambiguating on the derivation probability, as in (5): ˆwt = argmaxP(d) (5) d Our analysis suggest that this is because many derivations with very low probabilities generate the same, poor translation. When applying Equation (3) to marginalize over those derivations, the resulting score is higher for the poor translation than a better translation with fewer derivations but where the derivations had higher likelihood. Using the DO</context>
<context position="13304" citStr="Hearne and Way, 2006" startWordPosition="2277" endWordPosition="2280">) i=1 (7) where each αi is the weight of that term in the final score, and each fi(d) is a feature. In this work, we only consider f1(d), an accuracy-based score, although in future work we will consider a wide variety of features in the scoring function, including combinations of the different scoring schemes described below, binary lexical features, binary source-side syntactic features, and local target side features. The score of a derivation is now given by (8): s(d) = s((ds, dt)1 o ... o (ds, dt)N) �= s((ds, dt)i) (8) i In order to disambiguate between candidate translations, we follow (Hearne and Way, 2006) by using Equation (5). 3.1 Structured Fragment Rescoring In all our approaches, we rescore fragments according to their contribution to the accuracy of a translation. We would like to give fragments that contribute to good translations relatively high scores, and give fragments that contribute to bad translations relatively low scores, so that during decoding fragments that are known to contribute to good translations would be chosen over those that are known to contribute to bad translations. Furthermore, we would like to score each fragment in a derivation independently, since bad translati</context>
</contexts>
<marker>Hearne, Way, 2006</marker>
<rawString>M. Hearne and A. Way. 2006. Disambiguation strategies for data-oriented translation. In Proceedings of the 11th Conference of the European Association for Machine Translation, pages 59–68, Oslo, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearne</author>
</authors>
<title>Data-Oriented Models of Parsing and Translation.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Dublin City University,</institution>
<location>Dublin, Ireland.</location>
<contexts>
<context position="8951" citStr="Hearne, 2005" startWordPosition="1497" endWordPosition="1498"> the derivation probability, as in (5): ˆwt = argmaxP(d) (5) d Our analysis suggest that this is because many derivations with very low probabilities generate the same, poor translation. When applying Equation (3) to marginalize over those derivations, the resulting score is higher for the poor translation than a better translation with fewer derivations but where the derivations had higher likelihood. Using the DOT model directly is difficult – the number of fragments extracted from a parallel treebank is exponential in the size of the treebank. Therefore we use the Goodman reduction of DOT (Hearne, 2005) to create an isomorphic PCFG representation of the DOT model that is linear in the size of the treebank. The idea behind the Goodman reduction is that rather than storing fragments in the grammar and translating via composition, we simultaneously build up the fragments using the PCFG reduction and compose them together. To perform the reduction, we first relabel the two linked nodes (X, Y) with the new label X=Y. We then label each node in the parallel treebank with a unique Goodman index. Each binarybranching node and its two children can be internal or root/frontier. We add rules to the gra</context>
<context position="21177" citStr="Hearne, 2005" startWordPosition="3640" endWordPosition="3641">ore for (ds, dt) from its comparison to all the possible corresponding oracle fragments. In this way, we are choosing to score (ds, dt) against the oracle fragment it is closest to. 4 Experiments For our pilot experiments, we tested all the rescoring methods in the previous section on Spanish-toEnglish translation against the relative-frequency baseline. We randomly selected 10,000 sentences from the Europarl corpus (Koehn, 2005), and parsed and aligned the bitext as described in (Tinsley et al., 2009). From the parallel treebank, we extracted a Goodman reduction DOT grammar, as described in (Hearne, 2005), although on an order of magnitude greater amount of training data. Unlike (Bod, 2007), we did not use the unsupervised version of DOT, and did not attempt to scale up our amount of training data to his levels, although in ongoing work we are optimizing our system to be able to handle that amount of training data. To perform the rescoring, we randomly chose an additional 30K sentence pairs from the Spanish-toEnglish bitext. We rescored the grammar by translating the source side of the 10K training sentence pairs and 10K of the additional sentences, and using the methods in Section 3 to score </context>
</contexts>
<marker>Hearne, 2005</marker>
<rawString>M. Hearne. 2005. Data-Oriented Models of Parsing and Translation. Ph.D. thesis, Dublin City University, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>The DOP estimation method is biased and inconsistent.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="12040" citStr="Johnson, 2002" startWordPosition="2062" endWordPosition="2063">target derivation probabilities by the target fragment relative frequencies conditioned on the source fragment. There are a few problems with a likelihoodbased scoring scheme. First, it is not clear that if a fragment is more likely to be seen in training data then it is more likely to be used in a correct translation of an unseen sentence. In our analysis of the candidate translations of the DOT system, we observed that frequently, the highest-likelihood candidate translation output by the system was not the highest-accuracy candidate inferred. An additional problem is that, as described in (Johnson, 2002), the relative frequency estimator for DOP S=S1 S=S1 N=N3 John V4 likes N=N5 Mary N=N4 Mary VP2 V5 plait PP3 N=N7 VP2 P6 373 (and by extension, DOT) is known to be biased and inconsistent. 3 Accuracy-Based Fragment Scoring In our work, we wish to incorporate a measure of fragment accuracy into the scoring. To do so, we reformulate the scoring of DOT as log-linear rather than probabilistic, in order to incorporate non-likelihood features into the derivation scores. For all tree fragment pairs (ds, dt), let l((ds, dt)) = log(p((ds, dt))) (6) The general form of a rescored tree fragment will be k</context>
</contexts>
<marker>Johnson, 2002</marker>
<rawString>M. Johnson. 2002. The DOP estimation method is biased and inconsistent. Computational Linguistics, 28(1):71–76, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics, demonstation session,</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="26074" citStr="Koehn et al., 2007" startWordPosition="4469" endWordPosition="4472"> target-side ordering. In these experiments we did not use an additional language model. DOT (and many other syntax-based SMT systems) essentially have the target language model encoded within the translation model, since the inferences derived during translations link source structures to target structures, so in principle, no additional language model should be necessary. Furthermore, we only evaluate against a single reference, which also contributes to the lowering of absolute scores. To provide a sanity check against a state-of-the-art system, we trained the Moses phrase-based MT system (Koehn et al., 2007) using our training corpus, using no language model and using uniform feature weights, to provide a fair comparison against our baseline. We used this system to decode our development test set, and as a result we obtained a BLEU score of 10.72, which is comparable to the score obtained by our baseline on the same set. When we scale up to tuning on 40,000 sentences we see an improvement in BLEU scores as well, as shown in Table 3. When tuning on 40K sentences, we observe an increase of 1.81 BLEU points on the best-performing system, which is a 20.6% improvement over the baseline. We note that r</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, demonstation session, pages 177–180, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>388--395</pages>
<location>Barcelona,</location>
<contexts>
<context position="24185" citStr="Koehn, 2004" startWordPosition="4164" endWordPosition="4165">ion (7). We set these parameters manually, although in future work we will automatically tune them, perhaps using a MERT-like algorithm. We tested our rescored grammars on a set of 2,000 randomly chosen Europarl sentences, and used a set of 200 randomly chosen sentences as a development test set. 1 5 Results Translation quality results can be found in Tables 2 and 3. In these tables, columns labeled i-j indicate that the corresponding system was trained using parameters α0 = i and α1 = j in Equation 7. Statistical significance tests for NIST and BLEU were performed using Bootstrap Resampling (Koehn, 2004). 1All sentences, including the ones used for training, were limited to a length of at most 20 words. f((ds, dt)) = Max (dos,do t )∈Do:dos=ds 376 BLEU NIST F-SCORE Baseline 10.82 3.493 42.31 2-8 4-6 5-5 6-4 8-2 BLEU SFR 11.34 12.12 11.94 11.97 11.78 NSFR 9.68 10.99 11.38 11.63 11.30 FSR 11.40 11.49 11.72 11.91 11.72 NIST SFR 3.653 3.727 3.723 3.708 3.694 NSFR 3.376 3.530 3.554 3.616 3.572 FSR 3.655 3.675 3.698 3.701 3.675 F-SCORE SFR 44.84 45.47 45.36 45.33 45.08 NSFR 41.44 43.38 44.18 44.79 44.26 FSR 44.68 44.91 45.15 45.19 44.82 Table 4: Results on development test set. Rescoring on 40K sent</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>P. Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 388–395, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl: A Parallel Corpus for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Machine Translation Summit X,</booktitle>
<pages>79--86</pages>
<location>Phuket, Thailand.</location>
<contexts>
<context position="20997" citStr="Koehn, 2005" startWordPosition="3610" endWordPosition="3611">au-Levenshtein distance Sdl(dc, dys) (Damerau, 1964) between the two fragment yields, and score them as in (14): −Sdl(dt,dot) (14) In Equation (14) we are selecting the maximal score for (ds, dt) from its comparison to all the possible corresponding oracle fragments. In this way, we are choosing to score (ds, dt) against the oracle fragment it is closest to. 4 Experiments For our pilot experiments, we tested all the rescoring methods in the previous section on Spanish-toEnglish translation against the relative-frequency baseline. We randomly selected 10,000 sentences from the Europarl corpus (Koehn, 2005), and parsed and aligned the bitext as described in (Tinsley et al., 2009). From the parallel treebank, we extracted a Goodman reduction DOT grammar, as described in (Hearne, 2005), although on an order of magnitude greater amount of training data. Unlike (Bod, 2007), we did not use the unsupervised version of DOT, and did not attempt to scale up our amount of training data to his levels, although in ongoing work we are optimizing our system to be able to handle that amount of training data. To perform the rescoring, we randomly chose an additional 30K sentence pairs from the Spanish-toEnglish</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>P. Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In Machine Translation Summit X, pages 79–86, Phuket, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lavie</author>
<author>K Sagae</author>
<author>S Jayaraman</author>
</authors>
<title>The significance of recall in automatic metrics for MT evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 6th Conference of the Association for Machine Translation in the Americas,</booktitle>
<pages>134--143</pages>
<location>Washington, DC.</location>
<contexts>
<context position="15298" citStr="Lavie et al., 2004" startWordPosition="2617" endWordPosition="2620">rule is given as: s(r) = Ef:r∈f δ (f )/cf(r) (11) |f| This has the further advantage that we are allowing fragments that were unseen during tuning to be rescored according to previously seen fragment substructures. To implement this scheme, we select a set of oracle translations for each sentence in the tuning data by evaluating all the candidate translations against the gold standard translation using the Fscore (Turian et al., 2003), and selecting those with the highest F1-measure, with exponent 1. We use GTM, rather than BLEU, because BLEU is not known to work well on a per-sentence level (Lavie et al., 2004) as needed for oracle selection. We then compare all the target-side fragments inferred in the translation process for each candidate translation against the fragments that yielded the oracles. There are two relevant parts of the fragments – the internal yields (i.e. the terminal leaves of the fragment) and the substitution sites (i.e. the frontiers where other fragments attach). We score the fragments rooted at the substitution sites separately from the parent fragment. We can uniquely identify the set of fragments that can be rooted at substitution sites by determining the span of the linked</context>
</contexts>
<marker>Lavie, Sagae, Jayaraman, 2004</marker>
<rawString>A. Lavie, K. Sagae, and S. Jayaraman. 2004. The significance of recall in automatic metrics for MT evaluation. In Proceedings of the 6th Conference of the Association for Machine Translation in the Americas, pages 134–143, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lopez</author>
</authors>
<title>Translation as weighted deduction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</booktitle>
<pages>532--540</pages>
<location>Athens, Greece.</location>
<marker>Lopez, 2009</marker>
<rawString>A. Lopez. 2009. Translation as weighted deduction. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 532–540, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="2839" citStr="Och, 2003" startWordPosition="441" endWordPosition="442">he work in the MT literature has focused on the scoring of translation decisions made. (Yamada and Knight, 2001) follow (Brown et al., 1993) in using the noisy channel model, by decomposing the translation decisions modeled by the translation model into different types, and inducing probability distributions via maximum likelihood estimation over each decision type. This model is then decoded as described in (Yamada and Knight, 2002). This type of approach is also followed in (Galley et al., 2006). There has been some previous work on accuracy-driven training techniques for SMT, such as MERT (Och, 2003) and the Simplex Armijo Downhill method (Zhao and Chen, 2009), which tune the parameters in a linear combination of various phrase scores according to a held-out tuning set. While this does tune the relative weights of the scores to maximize the accuracy of candidates in the tuning set, the scores themselves in the linear combination are not necessarily correlated with the accuracy of the translation. Tillmann and Zhang (2006) present a procedure to directly optimize the global scoring function used by a phrasebased decoder on the accuracy of the translations. Similarly to MERT, Tillmann and Z</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="3610" citStr="Papineni et al., 2002" startWordPosition="568" endWordPosition="571">o a held-out tuning set. While this does tune the relative weights of the scores to maximize the accuracy of candidates in the tuning set, the scores themselves in the linear combination are not necessarily correlated with the accuracy of the translation. Tillmann and Zhang (2006) present a procedure to directly optimize the global scoring function used by a phrasebased decoder on the accuracy of the translations. Similarly to MERT, Tillmann and Zhang estimate the parameters of a weight vector on a linear combination of (binary) features using a global objective function correlated with BLEU (Papineni et al., 2002). In this work, we prototype some methods for moving directly towards incorporating a measure of the translation quality of each fragment used, bringing DOT more into the mainstream of current SMT research. In Section 2 we describe probability-based DOT fragment scoring. In Section 3 we describe our rescoring setup and the 371 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 371–380, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP Figure 1: Example DOT Fragments. three rescoring methods. In Section 4, we describe our experiments. In Section 5 we co</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Poutsma</author>
</authors>
<title>Data-oriented translation.</title>
<date>2000</date>
<booktitle>In The 18th International Conference on Computational Linguistics,</booktitle>
<pages>635--641</pages>
<location>Saarbr¨ucken, Germany.</location>
<contexts>
<context position="869" citStr="Poutsma, 2000" startWordPosition="121" endWordPosition="122">AT&amp;T Shannon Laboratory {lastname} @research.att.com Abstract In this work we present a novel technique to rescore fragments in the Data-Oriented Translation model based on their contribution to translation accuracy. We describe three new rescoring methods, and present the initial results of a pilot experiment on a small subset of the Europarl corpus. This work is a proof-of-concept, and is the first step in directly optimizing translation decisions solely on the hypothesized accuracy of potential translations resulting from those decisions. 1 Introduction The Data-Oriented Translation (DOT) (Poutsma, 2000) model is a tree-structured translation model, in which linked subtree fragments extracted from a parsed bitext are composed to cover a sourcelanguage sentence to be translated. Each linked fragment pair consists of a source-language side and a target-language side, similar to (Wu, 1997). Translating a new sentence involves composing the linked fragments into derivations so that a new source-language sentence is covered by the source tree fragments of the linked pairs, where the yields of the target-side derivations are the candidate translations. Derivations are scored according to their like</context>
<context position="4570" citStr="Poutsma, 2000" startWordPosition="725" endWordPosition="726"> 2009 Conference on Empirical Methods in Natural Language Processing, pages 371–380, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP Figure 1: Example DOT Fragments. three rescoring methods. In Section 4, we describe our experiments. In Section 5 we compare the results of rescoring the fragments with the three methods. In Section 6 we discuss some of the decisions that are affected by our rescoring methods. Finally, we discuss the next steps in training the DOT system by optimizing over a translation accuracy-based objective function in Section 7. 2 DOT Scoring As described in previous work (Poutsma, 2000; Hearne and Way, 2003), DOT scores translations according to the probabilities of the derivations, which are in turn computed from the relative frequencies of linked tree fragments in a parallel treebank. Linked fragment pairs are conditionally independent, so the score of a derivation is the product of the probabilities of all the linked fragments used. To find the probability of a translation, DOT marginalizes over the scores of all derivations yielding the translation. From a parallel treebank aligned at the subsentential level, we extract all possible linked fragment pairs by first select</context>
</contexts>
<marker>Poutsma, 2000</marker>
<rawString>A. Poutsma. 2000. Data-oriented translation. In The 18th International Conference on Computational Linguistics, pages 635–641, Saarbr¨ucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Tillmann</author>
<author>T Zhang</author>
</authors>
<title>A discriminative global training algorithm for statistical MT.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>721--728</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="3269" citStr="Tillmann and Zhang (2006)" startWordPosition="511" endWordPosition="514">mada and Knight, 2002). This type of approach is also followed in (Galley et al., 2006). There has been some previous work on accuracy-driven training techniques for SMT, such as MERT (Och, 2003) and the Simplex Armijo Downhill method (Zhao and Chen, 2009), which tune the parameters in a linear combination of various phrase scores according to a held-out tuning set. While this does tune the relative weights of the scores to maximize the accuracy of candidates in the tuning set, the scores themselves in the linear combination are not necessarily correlated with the accuracy of the translation. Tillmann and Zhang (2006) present a procedure to directly optimize the global scoring function used by a phrasebased decoder on the accuracy of the translations. Similarly to MERT, Tillmann and Zhang estimate the parameters of a weight vector on a linear combination of (binary) features using a global objective function correlated with BLEU (Papineni et al., 2002). In this work, we prototype some methods for moving directly towards incorporating a measure of the translation quality of each fragment used, bringing DOT more into the mainstream of current SMT research. In Section 2 we describe probability-based DOT fragm</context>
</contexts>
<marker>Tillmann, Zhang, 2006</marker>
<rawString>C. Tillmann and T. Zhang. 2006. A discriminative global training algorithm for statistical MT. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 721–728, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tinsley</author>
<author>M Hearne</author>
<author>A Way</author>
</authors>
<title>Parallel treebanks in phrase-based statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Tenth International Conference on Intelligent Text Processing and Computational Linguistics (CICLing),</booktitle>
<pages>318--331</pages>
<location>Mexico City, Mexico.</location>
<contexts>
<context position="21071" citStr="Tinsley et al., 2009" startWordPosition="3621" endWordPosition="3625">two fragment yields, and score them as in (14): −Sdl(dt,dot) (14) In Equation (14) we are selecting the maximal score for (ds, dt) from its comparison to all the possible corresponding oracle fragments. In this way, we are choosing to score (ds, dt) against the oracle fragment it is closest to. 4 Experiments For our pilot experiments, we tested all the rescoring methods in the previous section on Spanish-toEnglish translation against the relative-frequency baseline. We randomly selected 10,000 sentences from the Europarl corpus (Koehn, 2005), and parsed and aligned the bitext as described in (Tinsley et al., 2009). From the parallel treebank, we extracted a Goodman reduction DOT grammar, as described in (Hearne, 2005), although on an order of magnitude greater amount of training data. Unlike (Bod, 2007), we did not use the unsupervised version of DOT, and did not attempt to scale up our amount of training data to his levels, although in ongoing work we are optimizing our system to be able to handle that amount of training data. To perform the rescoring, we randomly chose an additional 30K sentence pairs from the Spanish-toEnglish bitext. We rescored the grammar by translating the source side of the 10K</context>
</contexts>
<marker>Tinsley, Hearne, Way, 2009</marker>
<rawString>J. Tinsley, M. Hearne, and A. Way. 2009. Parallel treebanks in phrase-based statistical machine translation. In Proceedings of the Tenth International Conference on Intelligent Text Processing and Computational Linguistics (CICLing), pages 318–331, Mexico City, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Turian</author>
<author>L Shen</author>
<author>I D Melamed</author>
</authors>
<title>Evaluation of machine translation and its evaluation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Ninth Machine Translation Summit,</booktitle>
<pages>386--393</pages>
<location>New Orleans, LA.</location>
<contexts>
<context position="15117" citStr="Turian et al., 2003" startWordPosition="2585" endWordPosition="2588">ning them the average of the rule scores over all fragments in which they appear. That is for each rule r in a fragment f consisting of cf(r) rules with score δ(f), the score of the rule is given as: s(r) = Ef:r∈f δ (f )/cf(r) (11) |f| This has the further advantage that we are allowing fragments that were unseen during tuning to be rescored according to previously seen fragment substructures. To implement this scheme, we select a set of oracle translations for each sentence in the tuning data by evaluating all the candidate translations against the gold standard translation using the Fscore (Turian et al., 2003), and selecting those with the highest F1-measure, with exponent 1. We use GTM, rather than BLEU, because BLEU is not known to work well on a per-sentence level (Lavie et al., 2004) as needed for oracle selection. We then compare all the target-side fragments inferred in the translation process for each candidate translation against the fragments that yielded the oracles. There are two relevant parts of the fragments – the internal yields (i.e. the terminal leaves of the fragment) and the substitution sites (i.e. the frontiers where other fragments attach). We score the fragments rooted at the</context>
</contexts>
<marker>Turian, Shen, Melamed, 2003</marker>
<rawString>J. Turian, L. Shen, and I. D. Melamed. 2003. Evaluation of machine translation and its evaluation. In Proceedings of the Ninth Machine Translation Summit, pages 386–393, New Orleans, LA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="1157" citStr="Wu, 1997" startWordPosition="165" endWordPosition="166">a pilot experiment on a small subset of the Europarl corpus. This work is a proof-of-concept, and is the first step in directly optimizing translation decisions solely on the hypothesized accuracy of potential translations resulting from those decisions. 1 Introduction The Data-Oriented Translation (DOT) (Poutsma, 2000) model is a tree-structured translation model, in which linked subtree fragments extracted from a parsed bitext are composed to cover a sourcelanguage sentence to be translated. Each linked fragment pair consists of a source-language side and a target-language side, similar to (Wu, 1997). Translating a new sentence involves composing the linked fragments into derivations so that a new source-language sentence is covered by the source tree fragments of the linked pairs, where the yields of the target-side derivations are the candidate translations. Derivations are scored according to their likelihood, and the translation is selected from the derivation pair with the highest score. However, we have no reason to believe that maximizing likelihood is the best way to maximize translation accuracy – likelihood and accuracy do not necessarily correlate well. We can frame the problem</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>D. Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yamada</author>
<author>K Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings of 39th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>523--530</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="2341" citStr="Yamada and Knight, 2001" startWordPosition="359" endWordPosition="362">correlate well. We can frame the problem as a search problem, where we are searching a space of derivations for the one that yields the highest scoring translation. By putting weights on the derivations in the search space, we wish to point the decoder in the direction of the optimal translation. Since we want the decoder to find the translation with the highest evaluation score, we would want to score the derivations with weights that correlate well with the particular evaluation measure in mind. Much of the work in the MT literature has focused on the scoring of translation decisions made. (Yamada and Knight, 2001) follow (Brown et al., 1993) in using the noisy channel model, by decomposing the translation decisions modeled by the translation model into different types, and inducing probability distributions via maximum likelihood estimation over each decision type. This model is then decoded as described in (Yamada and Knight, 2002). This type of approach is also followed in (Galley et al., 2006). There has been some previous work on accuracy-driven training techniques for SMT, such as MERT (Och, 2003) and the Simplex Armijo Downhill method (Zhao and Chen, 2009), which tune the parameters in a linear c</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>K. Yamada and K. Knight. 2001. A syntax-based statistical translation model. In Proceedings of 39th Annual Meeting of the Association for Computational Linguistics, pages 523–530, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yamada</author>
<author>K Knight</author>
</authors>
<title>A decoder for syntax-based statistical MT.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>303--310</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="2666" citStr="Yamada and Knight, 2002" startWordPosition="409" endWordPosition="412">o find the translation with the highest evaluation score, we would want to score the derivations with weights that correlate well with the particular evaluation measure in mind. Much of the work in the MT literature has focused on the scoring of translation decisions made. (Yamada and Knight, 2001) follow (Brown et al., 1993) in using the noisy channel model, by decomposing the translation decisions modeled by the translation model into different types, and inducing probability distributions via maximum likelihood estimation over each decision type. This model is then decoded as described in (Yamada and Knight, 2002). This type of approach is also followed in (Galley et al., 2006). There has been some previous work on accuracy-driven training techniques for SMT, such as MERT (Och, 2003) and the Simplex Armijo Downhill method (Zhao and Chen, 2009), which tune the parameters in a linear combination of various phrase scores according to a held-out tuning set. While this does tune the relative weights of the scores to maximize the accuracy of candidates in the tuning set, the scores themselves in the linear combination are not necessarily correlated with the accuracy of the translation. Tillmann and Zhang (20</context>
</contexts>
<marker>Yamada, Knight, 2002</marker>
<rawString>K. Yamada and K. Knight. 2002. A decoder for syntax-based statistical MT. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 303–310, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Zhao</author>
<author>S Chen</author>
</authors>
<title>A simplex armijo downhill algorithm for optimizing statistical machine translation decoding parameters.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers,</booktitle>
<pages>21--24</pages>
<location>Boulder, Colorado.</location>
<contexts>
<context position="2900" citStr="Zhao and Chen, 2009" startWordPosition="449" endWordPosition="452">ring of translation decisions made. (Yamada and Knight, 2001) follow (Brown et al., 1993) in using the noisy channel model, by decomposing the translation decisions modeled by the translation model into different types, and inducing probability distributions via maximum likelihood estimation over each decision type. This model is then decoded as described in (Yamada and Knight, 2002). This type of approach is also followed in (Galley et al., 2006). There has been some previous work on accuracy-driven training techniques for SMT, such as MERT (Och, 2003) and the Simplex Armijo Downhill method (Zhao and Chen, 2009), which tune the parameters in a linear combination of various phrase scores according to a held-out tuning set. While this does tune the relative weights of the scores to maximize the accuracy of candidates in the tuning set, the scores themselves in the linear combination are not necessarily correlated with the accuracy of the translation. Tillmann and Zhang (2006) present a procedure to directly optimize the global scoring function used by a phrasebased decoder on the accuracy of the translations. Similarly to MERT, Tillmann and Zhang estimate the parameters of a weight vector on a linear c</context>
</contexts>
<marker>Zhao, Chen, 2009</marker>
<rawString>B. Zhao and S. Chen. 2009. A simplex armijo downhill algorithm for optimizing statistical machine translation decoding parameters. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers, pages 21– 24, Boulder, Colorado.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>