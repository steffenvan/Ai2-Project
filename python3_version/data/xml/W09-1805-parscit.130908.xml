<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.478794">
<title confidence="0.994682">
A Constraint Programming Approach to Probabilistic Syntactic Processing
</title>
<author confidence="0.771842">
Irene Langkilde-Geary
</author>
<affiliation confidence="0.6086">
Independent Consultant
</affiliation>
<address confidence="0.867233">
South Jordan, UT USA
</address>
<email confidence="0.997253">
i.l.geary@gmail.com
</email>
<sectionHeader confidence="0.999859" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999942690476191">
Integer linear programming (ILP) is a framework
for solving combinatorial problems with linear con-
straints of the form y = c1x1 + c2x2 + ... + cnxn
where the variables (ie., y and xis) take on only in-
teger values. ILP is a special case of a larger fam-
ily of contraint-based solving techniques in which
variables may take on additional types of values (eg.
discrete, symbolic, real, set, and structured) or in-
volve additional kinds of constraints (eg. logical
and non-linear, such as x ∧ y ⇒ z and y = cxn).
Constraint-based problem solving approaches offer
a more natural way of modeling many kinds of real-
world problems. Furthermore, the declarative nature
of constraint-based approaches makes them versatile
since the order in which the variables are solved is
not predetermined. The same program can thus be
reused for solving different subsets of the problem’s
variables. Additionally, in some cases, constraint-
based approaches can solve problems more effi-
ciently or accurately than alternative approaches.
Constraint Programming (CP) is a field of re-
search that develops algorithms and tools for
constraint-based problem solving. This abstract de-
scribes work-in-progress on a project to develop
a CP-based general-purpose broad-coverage prob-
abilistic syntactic language processing system for
English. Because of its declarative nature, the sys-
tem can be used for both parsing and realization as
well as their subtasks (such as tagging, chunk pars-
ing, lexical choice, or word ordering) or hybridiza-
tions (like text-to-text generation). We expect this
tool to be useful for a wide range of applications
from information extraction to machine translation
to human-computer dialog. An ambitious project
such as this poses a number of questions and difficult
challenges, including: a) how to declaratively repre-
sent the syntactic structure of sentences, b) how to
integrate the processing of hard constraints with soft
(probabilistic) ones, c) how to overcome problems
of intractibility associated with large problems and
rich representations in learning, inference, as well
as search.
</bodyText>
<sectionHeader confidence="0.9998" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999936842105263">
Declarative and constraint-based representations
and computation mechanisms have been the subject
of much research in the fields of both Linguistics
and Computer Science over the last 30-40 years,
at times motivating each other but also sometimes
developing independently. Although there is quite
a large literature on constraint-based processing in
NLP, the notion of a constraint and the methods for
processing them vary significantly from that in CP.
See (Duchier et al., 1998; Piwek and van Deemter,
2006; Blache, 2000). The CP approach has been
designed for a broader ranger of applications and
rests on a stronger, more general theoretical foun-
dation. It coherently integrates a variety of solving
techniques whereas theoretical linguistic formalisms
have traditionally used only a single kind of con-
straint solver, namely unification. In comparison,
the 2009 ILPNLP workshop focuses on NLP pro-
cessing using solely integer linear constraints.
</bodyText>
<page confidence="0.968701">
36
</page>
<note confidence="0.975366">
Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing, pages 36–37,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.99902" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.99996268627451">
Three key elements of our approach are its syntactic
representation, confidence-based beam search, and a
novel on-demand learning and inference algorithm.
The last is used to calculate probability-based fea-
ture costs and the confidences used to heuristically
guide the search for the best solution. A description
of the flat featurized dependency-style syntactic rep-
resentation we use is available in (Langkilde-Geary
and Betteridge, 2006), which describes how the en-
tire Penn Treebank (Marcus et al., 1993) was con-
verted to this representation. The representation has
been designed to offer finer-grained declarativeness
than other existing representations.
Our confidence-based search heuristic evaluates
the conditional likelihood of undetermined output
variables (ie., word features) at each step of search
and heuristically selects the case of the mostly likely
variable/value pair as the next (or only one) to ex-
plore. The likelihood is contextualized by the in-
put variables and any output variables which have
already been explored and tentatively solved. Al-
though one theoretical advantage of CP (and ILP)
is the ability to calculate an overall optimal solu-
tion through search, we unexpectedly found that
our confidence-based heuristic led to the first inter-
mediate solution typically being the optimal. This
allowed us to simplify the search methodology to
a one-best or threshold-based beam search without
any significant loss in accuracy. The result is dra-
matically improved scalability.
We use the concurrent CP language Mozart/Oz
to implement our approach. We previously im-
plemented an exploratory prototype that used raw
frequencies instead of smoothed probabilities for
the feature costs and search heuristic confidences.
(Langkilde-Geary, 2005; Langkilde-Geary, 2007).
The lack of smoothing severely limited the applica-
bility of the prototype. We are currently finishing
development of the before-mentioned on-demand
learning algorithm which will overcome that chal-
lenge and allow us to evaluate our approach’s ac-
curacy and efficiency on a variety of NLP tasks on
common test sets. Informal preliminary results on
the much-studied subtask of part-of-speech tagging
indicate that our method outperforms a Naive Bayes-
based baseline in terms of accuracy and within 2%
of state-of-the-art single-classifier methods, while
running in linear time with respect to the number of
output variables or word tokens. We are not aware
of any other approach that achieves this level of ac-
curacy in comparable algorithmic time.
</bodyText>
<sectionHeader confidence="0.995585" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999953769230769">
The versatility and potential scalability of our ap-
proach are its most noteworthy aspects. We ex-
pect it to be able to handle not only a wider vari-
ety of NLP tasks than existing approaches but also
to tackle harder tasks that have been intractible be-
fore now. Although ILP has the same theoretical
power as CP for efficiently solving problems, our
approach takes advantage of several capabilities that
CP offers that ILP doesn’t, including modeling with
not only linear constraints but also logical, set-based
and other kinds of constraints; customized search
methodology with dynamically computed costs, and
conditionally applied constraints, among others.
</bodyText>
<sectionHeader confidence="0.998859" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999146821428571">
P. Blache. 2000. Constraints, linguistic theories and nat-
ural language processing. Natural Language Process-
ing, 1835.
D. Duchier, C. Gardent, and J. Niehren. 1998. Concur-
rent constraint programming in oz for natural language
processing. Technical report, Universitt des Saarlan-
des.
I. Langkilde-Geary and J. Betteridge. 2006. A factored
functional dependency transformation of the english
penn treebank for probabilistic surface generation. In
Proc. LREC.
I. Langkilde-Geary. 2005. An exploratory applica-
tion of constraint optimization in mozart to probabilis-
tic natural language processing. In H. Christiansen,
P. Skadhauge, and J. Villadsen, editors, Proceedings of
the International Workshop on Constraint Solving and
Language Processing (CSLP), volume 3438. Springer-
Verlag LNAI.
I. Langkilde-Geary. 2007. Declarative syntactic process-
ing of natural language using concurrent constraint
programming and probabilistic dependency modeling.
In Proc. UCNLG.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of english: the Penn
treebank. Computational Linguistics, 19(2).
P. Piwek and K. van Deemter. 2006. Constraint-based
natural language generation: A survey. Technical re-
port, The Open University.
</reference>
<page confidence="0.999612">
37
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.836689">
<title confidence="0.999824">A Constraint Programming Approach to Probabilistic Syntactic Processing</title>
<author confidence="0.921843">Irene Langkilde-Geary</author>
<affiliation confidence="0.954955">Independent Consultant</affiliation>
<address confidence="0.916864">South Jordan, UT USA</address>
<email confidence="0.991249">i.l.geary@gmail.com</email>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Blache</author>
</authors>
<title>Constraints, linguistic theories and natural language processing.</title>
<date>2000</date>
<booktitle>Natural Language Processing,</booktitle>
<pages>1835</pages>
<contexts>
<context position="2797" citStr="Blache, 2000" startWordPosition="424" endWordPosition="425">oblems and rich representations in learning, inference, as well as search. 2 Related Work Declarative and constraint-based representations and computation mechanisms have been the subject of much research in the fields of both Linguistics and Computer Science over the last 30-40 years, at times motivating each other but also sometimes developing independently. Although there is quite a large literature on constraint-based processing in NLP, the notion of a constraint and the methods for processing them vary significantly from that in CP. See (Duchier et al., 1998; Piwek and van Deemter, 2006; Blache, 2000). The CP approach has been designed for a broader ranger of applications and rests on a stronger, more general theoretical foundation. It coherently integrates a variety of solving techniques whereas theoretical linguistic formalisms have traditionally used only a single kind of constraint solver, namely unification. In comparison, the 2009 ILPNLP workshop focuses on NLP processing using solely integer linear constraints. 36 Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing, pages 36–37, Boulder, Colorado, June 2009. c�2009 Association for Comp</context>
</contexts>
<marker>Blache, 2000</marker>
<rawString>P. Blache. 2000. Constraints, linguistic theories and natural language processing. Natural Language Processing, 1835.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Duchier</author>
<author>C Gardent</author>
<author>J Niehren</author>
</authors>
<title>Concurrent constraint programming in oz for natural language processing.</title>
<date>1998</date>
<tech>Technical report, Universitt des Saarlandes.</tech>
<contexts>
<context position="2753" citStr="Duchier et al., 1998" startWordPosition="415" endWordPosition="418">problems of intractibility associated with large problems and rich representations in learning, inference, as well as search. 2 Related Work Declarative and constraint-based representations and computation mechanisms have been the subject of much research in the fields of both Linguistics and Computer Science over the last 30-40 years, at times motivating each other but also sometimes developing independently. Although there is quite a large literature on constraint-based processing in NLP, the notion of a constraint and the methods for processing them vary significantly from that in CP. See (Duchier et al., 1998; Piwek and van Deemter, 2006; Blache, 2000). The CP approach has been designed for a broader ranger of applications and rests on a stronger, more general theoretical foundation. It coherently integrates a variety of solving techniques whereas theoretical linguistic formalisms have traditionally used only a single kind of constraint solver, namely unification. In comparison, the 2009 ILPNLP workshop focuses on NLP processing using solely integer linear constraints. 36 Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing, pages 36–37, Boulder, Colo</context>
</contexts>
<marker>Duchier, Gardent, Niehren, 1998</marker>
<rawString>D. Duchier, C. Gardent, and J. Niehren. 1998. Concurrent constraint programming in oz for natural language processing. Technical report, Universitt des Saarlandes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde-Geary</author>
<author>J Betteridge</author>
</authors>
<title>A factored functional dependency transformation of the english penn treebank for probabilistic surface generation.</title>
<date>2006</date>
<booktitle>In Proc. LREC.</booktitle>
<contexts>
<context position="3872" citStr="Langkilde-Geary and Betteridge, 2006" startWordPosition="576" endWordPosition="579">gs of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing, pages 36–37, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics 3 Methodology Three key elements of our approach are its syntactic representation, confidence-based beam search, and a novel on-demand learning and inference algorithm. The last is used to calculate probability-based feature costs and the confidences used to heuristically guide the search for the best solution. A description of the flat featurized dependency-style syntactic representation we use is available in (Langkilde-Geary and Betteridge, 2006), which describes how the entire Penn Treebank (Marcus et al., 1993) was converted to this representation. The representation has been designed to offer finer-grained declarativeness than other existing representations. Our confidence-based search heuristic evaluates the conditional likelihood of undetermined output variables (ie., word features) at each step of search and heuristically selects the case of the mostly likely variable/value pair as the next (or only one) to explore. The likelihood is contextualized by the input variables and any output variables which have already been explored </context>
</contexts>
<marker>Langkilde-Geary, Betteridge, 2006</marker>
<rawString>I. Langkilde-Geary and J. Betteridge. 2006. A factored functional dependency transformation of the english penn treebank for probabilistic surface generation. In Proc. LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde-Geary</author>
</authors>
<title>An exploratory application of constraint optimization in mozart to probabilistic natural language processing.</title>
<date>2005</date>
<booktitle>Proceedings of the International Workshop on Constraint Solving and Language Processing (CSLP),</booktitle>
<volume>volume</volume>
<pages>3438</pages>
<editor>In H. Christiansen, P. Skadhauge, and J. Villadsen, editors,</editor>
<publisher>SpringerVerlag LNAI.</publisher>
<contexts>
<context position="5192" citStr="Langkilde-Geary, 2005" startWordPosition="773" endWordPosition="774">ate an overall optimal solution through search, we unexpectedly found that our confidence-based heuristic led to the first intermediate solution typically being the optimal. This allowed us to simplify the search methodology to a one-best or threshold-based beam search without any significant loss in accuracy. The result is dramatically improved scalability. We use the concurrent CP language Mozart/Oz to implement our approach. We previously implemented an exploratory prototype that used raw frequencies instead of smoothed probabilities for the feature costs and search heuristic confidences. (Langkilde-Geary, 2005; Langkilde-Geary, 2007). The lack of smoothing severely limited the applicability of the prototype. We are currently finishing development of the before-mentioned on-demand learning algorithm which will overcome that challenge and allow us to evaluate our approach’s accuracy and efficiency on a variety of NLP tasks on common test sets. Informal preliminary results on the much-studied subtask of part-of-speech tagging indicate that our method outperforms a Naive Bayesbased baseline in terms of accuracy and within 2% of state-of-the-art single-classifier methods, while running in linear time wi</context>
</contexts>
<marker>Langkilde-Geary, 2005</marker>
<rawString>I. Langkilde-Geary. 2005. An exploratory application of constraint optimization in mozart to probabilistic natural language processing. In H. Christiansen, P. Skadhauge, and J. Villadsen, editors, Proceedings of the International Workshop on Constraint Solving and Language Processing (CSLP), volume 3438. SpringerVerlag LNAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde-Geary</author>
</authors>
<title>Declarative syntactic processing of natural language using concurrent constraint programming and probabilistic dependency modeling.</title>
<date>2007</date>
<booktitle>In Proc. UCNLG.</booktitle>
<contexts>
<context position="5216" citStr="Langkilde-Geary, 2007" startWordPosition="775" endWordPosition="776">solution through search, we unexpectedly found that our confidence-based heuristic led to the first intermediate solution typically being the optimal. This allowed us to simplify the search methodology to a one-best or threshold-based beam search without any significant loss in accuracy. The result is dramatically improved scalability. We use the concurrent CP language Mozart/Oz to implement our approach. We previously implemented an exploratory prototype that used raw frequencies instead of smoothed probabilities for the feature costs and search heuristic confidences. (Langkilde-Geary, 2005; Langkilde-Geary, 2007). The lack of smoothing severely limited the applicability of the prototype. We are currently finishing development of the before-mentioned on-demand learning algorithm which will overcome that challenge and allow us to evaluate our approach’s accuracy and efficiency on a variety of NLP tasks on common test sets. Informal preliminary results on the much-studied subtask of part-of-speech tagging indicate that our method outperforms a Naive Bayesbased baseline in terms of accuracy and within 2% of state-of-the-art single-classifier methods, while running in linear time with respect to the number</context>
</contexts>
<marker>Langkilde-Geary, 2007</marker>
<rawString>I. Langkilde-Geary. 2007. Declarative syntactic processing of natural language using concurrent constraint programming and probabilistic dependency modeling. In Proc. UCNLG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: the Penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="3940" citStr="Marcus et al., 1993" startWordPosition="588" endWordPosition="591">ssing, pages 36–37, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics 3 Methodology Three key elements of our approach are its syntactic representation, confidence-based beam search, and a novel on-demand learning and inference algorithm. The last is used to calculate probability-based feature costs and the confidences used to heuristically guide the search for the best solution. A description of the flat featurized dependency-style syntactic representation we use is available in (Langkilde-Geary and Betteridge, 2006), which describes how the entire Penn Treebank (Marcus et al., 1993) was converted to this representation. The representation has been designed to offer finer-grained declarativeness than other existing representations. Our confidence-based search heuristic evaluates the conditional likelihood of undetermined output variables (ie., word features) at each step of search and heuristically selects the case of the mostly likely variable/value pair as the next (or only one) to explore. The likelihood is contextualized by the input variables and any output variables which have already been explored and tentatively solved. Although one theoretical advantage of CP (an</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a large annotated corpus of english: the Penn treebank. Computational Linguistics, 19(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Piwek</author>
<author>K van Deemter</author>
</authors>
<title>Constraint-based natural language generation: A survey.</title>
<date>2006</date>
<tech>Technical report,</tech>
<institution>The Open University.</institution>
<marker>Piwek, van Deemter, 2006</marker>
<rawString>P. Piwek and K. van Deemter. 2006. Constraint-based natural language generation: A survey. Technical report, The Open University.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>