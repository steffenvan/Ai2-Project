<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.997062">
A Structured Language Model based on Context-Sensitive Probabilistic
Left-Corner Parsing
</title>
<author confidence="0.854834">
Dong Hoon Van Uytsel† Filip Van AeltenI Dirk Van Compernolle†
</author>
<affiliation confidence="0.420648">
donghoon@esat.kuleuven.ac.be filip.van.aelten@lhs.be compi@esat.kuleuven.ac.be
†Katholieke Universiteit Leuven, ESAT, Belgium
ILernout &amp; Hauspie, Belgium
</affiliation>
<sectionHeader confidence="0.966628" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99965375">
Recent contributions to statistical language model-
ing for speech recognition have shown that prob-
abilistically parsing a partial word sequence aids
the prediction of the next word, leading to “struc-
tured” language models that have the potential to
outperform n-grams. Existing approaches to struc-
tured language modeling construct nodes in the par-
tial parse tree after all of the underlying words have
been predicted. This paper presents a different ap-
proach, based on probabilistic left-corner grammar
(PLCG) parsing, that extends a partial parse both
from the bottom up and from the top down, lead-
ing to a more focused and more accurate, though
somewhat less robust, search of the parse space. At
the core of our new structured language model is a
fast context-sensitive and lexicalized PLCG parsing
algorithm that uses dynamic programming. Prelim-
inary perplexity and word-accuracy results appear
to be competitive with previous ones, while speed is
increased.
</bodyText>
<sectionHeader confidence="0.923311" genericHeader="method">
1 Structured language modeling
</sectionHeader>
<bodyText confidence="0.98717375">
In its current incarnation, (unconstrained) speech
recognition relies on a left-to-right language model
L, which estimates the occurrence of a next word
wj given a sequence of preceding words cj = wj−1
</bodyText>
<equation confidence="0.934878">
0
(the context):1
L(wj|cj) = ˆp(wj|cj).
L is called a language model (LM).
</equation>
<bodyText confidence="0.99952">
Obviously the context space is huge and even
in very large training corpora most contexts never
occur, which prohibits a reliable probability esti-
mation. Therefore the context space needs to be
mapped to a much smaller space, such that only
the essential information is retained. In spite of its
</bodyText>
<footnote confidence="0.9105895">
1As a shorthand, wba denotes a sequence wawa+1 ... wb if
b &gt; a, else it is the empty sequence.
</footnote>
<bodyText confidence="0.9897053125">
simplicity the trigram LM, that reduces cj to wj−1
j−2,
is hard to improve on and still the main language
model component in state-of-the-art speech recog-
nition systems. It is therefore commonly used as a
baseline in the evaluation of other models, including
the one described in this paper.
Structured language models (SLM) introduce
parsing into language modeling by alternating be-
tween predicting the next word using features of
partial parses of the context and extending the par-
tial parses to cover the next word. Following this
approach, Chelba and Jelinek (2000) obtained a
SLM that slightly improves on a trigram model
both in perplexity and recognition performance.
The Chelba-Jelinek SLM is, to our knowledge, the
first left-to-right LM using parsing techniques that
is successfully applied to large vocabulary speech
recognition. It is built on top of a lexicalized prob-
abilistic shift-reduce parser that predicts the next
word from the headwords (“exposed” heads) and
categories of the last two predicted isolated con-
stituents of the context. Then the predicted word
becomes the last isolated constituent and the last
two constituents are repeatedly recombined until the
parser decides to stop.
A dynamic programming (DP) version of
Chelba’s parser, inspired on the CYK chart parser,
was proposed in (Jelinek and Chelba, 1999). Our
implementation is roughly quadratic in the length
of the sentence, but not significantly faster than
Chelba’s non-DP parser. It scored somewhat lower
in perplexity before reestimation (presumably by
avoiding search errors), but remained roughly at
the same level after full inside-outside reestimation
(Van Aelten and Hogenhout, 2000).
An obvious weakness of the Chelba-Jelinek SLM
is the bottom-up behavior of the parser: it creates
isolated constituents and only afterwards is it able to
check whether a constituent fits into a higher struc-
ture. Van Uytsel (2000) developed a top-down al-
ternative along similar lines but based on a lexical-
ized and context-sensitive DP version of an efficient
Earley parser (Stolcke, 1995; Jelinek and Lafferty,
1991). The Earley-based SLM performed worse
than the Chelba-Jelinek SLM, mostly due to the fact
that the rule production probabilities cannot be con-
ditioned on the underlying lexical information, thus
producing a lot of wrong parses.
The weaknesses of our Earley SLM have led
us to consider probabilistic left-corner grammar
(PLCG) parsing (Manning and Carpenter, 1997),
which follows a mixed bottom-up and top-down ap-
proach. Its potential to enhance parsing efficiency
has been recognized by Roark and Johnson (2000),
who simulated a left-corner parser with a top-down
best-first parser applying a left-corner-transformed
PCFG grammar. For the language model described
in this paper, however, we implemented a DP ver-
sion of a native left-corner parser using a left-corner
treebank grammar (containing projection rules in-
stead of production rules). The efficiency of our im-
plementation further allowed to enrich the history
annotation of the parser states and to apply a lexi-
calized grammar.
The following section contains a brief review of
Manning’s PLCG parser. Section 3 describes how it
was adapted to our SLM framework: we introduce
lexicalization and context-sensitivity, present a DP
algorithm using a chart of parser states and finally
we define a language model based on the adapted
PLCG parser. At the end of the same section we ex-
plain how the initial language model can be trained
on additional plain text through a variant of inside-
outside reestimation. In section 4 we evaluate a few
PLCG-based SLMs obtained from the Penn Tree-
bank and BLLIP WSJ Corpus. We present test set
perplexity measurements and word accuracy after n-
best list rescoring to assess their viability for speech
recognition.
</bodyText>
<sectionHeader confidence="0.977051" genericHeader="method">
2 Classic PLCG parsing
</sectionHeader>
<bodyText confidence="0.999801">
The parameters of a PLCG are called projection
probabilities. They are of the form
</bodyText>
<equation confidence="0.946329">
p(Z --* X a|X, G),
</equation>
<bodyText confidence="0.999383636363636">
to be read as “given a completed constituent X dom-
inated by a goal category G, the probability that
there is a Z that has X as its first daughter and a
as its next daughters”. A PLCG contains essentially
the same rules as a probabilistic context-free gram-
mar (PCFG), but the latter conditions the rule prob-
abilities on the mother category Z (production prob-
abilities). In both cases the joint probability of the
entire parse tree and the parsed sentence is the prod-
uct of the production resp. projection probabilities
of the local trees it consists of.
</bodyText>
<listItem confidence="0.8807078">
While PCFG parsing proceeds from the top down
or from the bottom up, PLCG naturally leads to a
parsing scheme that is a mixture of both. The ad-
vantages of this are made clear in the subsections
below. Formally, a PLCG parser has three elemen-
tary operations:
• SHIFT: given that an unexpanded constituent
G starts from position i, shift the next word wi
with probability ps(wi|G) (G is called the goal
category);
• PROJECT: given a complete constituent X,
dominated by a goal category G, starting in po-
sition i and ending in j, predict a mother con-
stituent Z starting in position i and completed
up till position j, and zero or more unexpanded
sister constituents a starting in j with probabil-
ity pp(Z --* X a|X, G);
• ATTACH: given a complete constituent X dom-
inated by a goal category G, identify the first
as the latter with probability pa(X, G).
</listItem>
<sectionHeader confidence="0.993328" genericHeader="method">
3 Extending the PLCG framework
</sectionHeader>
<subsectionHeader confidence="0.999765">
3.1 Synchronous chart parsing with PLCG
</subsectionHeader>
<bodyText confidence="0.9999788">
In this subsection we present the basic parsing al-
gorithm and its data structures and operations. In
the subsections that follow, we will introduce lexi-
calization and context-sensitivity by extending this
framework.
The PLCG parsing process is interpreted as a
search through a network of states, a compact re-
presentation of the search space. The network nodes
correspond to states and the arcs to operations (an-
notated with transition probabilities). A (partial)
parse corresponds to a (partial) path through the net-
work. The joint probability of a partial parse and the
covered part of the sentence is equal to the partial
path probability, i.e. the product of the probabilities
of the transitions in the path.
</bodyText>
<sectionHeader confidence="0.384465" genericHeader="method">
3.1.1 PLCG states
</sectionHeader>
<bodyText confidence="0.956622769230769">
We write a state q as
q = (G; Z --*i X * jp; g, v) (1)
where G is the goal category, Z is the category of a
constituent from position i complete up till position
j, X is the first daughter category,  denotes the re-
maining unresolved daughters of Z, and µ and  are
forward and inner probabilities defined below. The
wildcard  symbolizes zero or more resolved daugh-
ter categories: we make abstraction of the identities
of resolved daughters (except the first one), because
further parser moves do not depend on them. If  is
empty, q is called a complete state, otherwise q is a
goal state.
</bodyText>
<subsectionHeader confidence="0.681245">
3.1.2 Forward and inner probability
</subsectionHeader>
<bodyText confidence="0.99969775">
Given a state q as defined in (1). We define its for-
ward probability µ = µ(q) as the sum of the prob-
abilities of the paths ending in q, starting in the ini-
tial state and generating wj−1
</bodyText>
<equation confidence="0.996604">
0 . As a consequence,
µ(q) = p(w j−1
0 , q) (joint probability).
</equation>
<bodyText confidence="0.9988875">
The inner probability  = (q) is the sum of the
probabilities of the paths generating w j−1
i , ending
in q and starting with a SHIFT of wi. As a conse-
quence, (q) = p(wj−1
i , q).
Note that the forward and inner probabilities of
the final state should be identical and equal to p(S).
</bodyText>
<subsectionHeader confidence="0.868316">
3.1.3 Parser operations
</subsectionHeader>
<bodyText confidence="0.97879625">
In this paragraph we reformulate the classic PLCG
parser operations in terms of transitions between
states. We hereby specify update formulas for for-
ward and inner probabilities.
Shift The SHIFT operation starts from a goal state
q = (G; Z --*i X  jY ; µ, ) (2)
and shifts the next word w at position j of the input
by updating q or generating a new state q where2
</bodyText>
<equation confidence="0.854455333333333">
q = (Y; W --* j w  j+1;µ+= µp,  = p) (3)
with transition probability
p = ps(w|Y). (4)
</equation>
<bodyText confidence="0.891245333333333">
If q already lives in the chart, only its forward prob-
ability is updated. The given update formula is jus-
tified by the relation
</bodyText>
<equation confidence="0.8769125">
µ(q) = � µ(q)p(q =:� q)
qsq&apos;
</equation>
<bodyText confidence="0.99981825">
where the sum is over all SHIFT transitions from q to
q and p(q =:� q) denotes the transition probability
from q to q. Computing (q) is a trivial case of the
definition.
</bodyText>
<footnote confidence="0.914438333333333">
2The C-like shorthand notation g+= gp means that g is
set to gp if there was no q in the chart yet, otherwise g is
incremented with gp.
</footnote>
<bodyText confidence="0.923862666666667">
Projection From a complete state, two transitions
are possible: ATTACH to a goal state with a prob-
ability pa or PROJECT with a probability 1 − pa.
PROJECT starts from a complete state
q = (G; Z --*i X  j; µ, ) (5)
and generates or updates a state
</bodyText>
<equation confidence="0.975101666666667">
q = (G; T --*i Z  j; µ+= µp, += p) (6)
with transition probability
p = pp(T, |Z, G) · (1 − pa(Z, G)). (7)
</equation>
<bodyText confidence="0.9999728">
Again, the forward probability is computed recur-
sively as a sum of products. Now  needs to be
accumulated, too: the constituent Z in general may
be resolved with more than one different X, which
each time adds to .
Note that a mother constituent inherits G from
her first daughter (left-corner).
Attachment Given a complete state q as in (5)
where G = Z and some goal state q in the par-
tial path leading to q
</bodyText>
<equation confidence="0.9837662">
q = (G; T --*h U i Z ; µ, ) (8)
then the ATTACH operation is a transition from q to
q with
q = (G; T --*h U  j; µ+= µp/, += p)
(9)
</equation>
<bodyText confidence="0.542363">
and transition probability
</bodyText>
<equation confidence="0.927119">
p = pa(Z, G) · . (10)
</equation>
<bodyText confidence="0.989075">
Why can µ not be updated from µ, similarly to (3)
and (6)? The reason is that ATTACH makes use of
non-local constraints: the transition from q to q is
only possible if a matching goal state q occurred in
a path leading to q. Therefore computing µ as in (3)
and (6) would include all paths that generate q, also
those that do not contain q. Instead, the update of
µ in (9) combines all paths leading to q with the
paths starting from q and ending in q. The update
of  follows an analogous reasoning.
</bodyText>
<subsubsectionHeader confidence="0.481047">
3.1.4 Chart representation
</subsubsectionHeader>
<bodyText confidence="0.9999816">
The parser produces a set of states that can be conve-
niently organized in a staircase-shaped chart similar
to the one used by the CYK parser. In the chart cell
with coordinates (i, j) we store all the states starting
in i and completed up till position j.
</bodyText>
<subsectionHeader confidence="0.895701">
3.1.5 Synchronous parsing algorithm
</subsectionHeader>
<bodyText confidence="0.998624666666667">
Following (Chelba, 2000), we represent a sentence
by a sequence of word identities starting with a
sentence-begin token (s), that is used in the con-
text but not predicted, followed by a sentence-end
token (/s), that is predicted by the model. We are
collecting the sentence proper together with (/s) un-
der a node labeled TOP, and the TOP node together
with (s) under a TOP node. The parser starts from
the initial state
</bodyText>
<equation confidence="0.68006">
qI = (TOP; TOP/(s) --*−1 SB/(s) *0TOP; 1, 1).
</equation>
<bodyText confidence="0.772993">
After processing the sentence S = wN−1
</bodyText>
<sectionHeader confidence="0.366924" genericHeader="method">
0 and pro-
</sectionHeader>
<bodyText confidence="0.731825">
vided a full parse was found, the final state
</bodyText>
<equation confidence="0.7522575">
qF = (TOP; TOP/(s) --*−1 SB/(s) * N; p(S), p(S))
is found in cell (−1, N).
</equation>
<bodyText confidence="0.998811666666667">
Now we are ready to formulate the parsing algo-
rithm. Note that we treat an ATTACH operation as a
special PROJECT, as explained in Sec. 4.1.
</bodyText>
<figure confidence="0.971429952380952">
1 for j  0,1 to N
2 for i  j − 1, j − 2 to −1
3 foreach complete state q in cell (i, j)
4 foreach proj in projections(q)
5 if goal(q) = cat(q) and proj = ‘attach’
6 for h  i − 1, i − 2 to −1
7 foreach goal state m in cell (h, i)
matching q
8 q  ATTACH(q, m)
9 add q to cell (h, j)
10 else
11 q  PROJECT(q)
12 add q to cell (i, j)
13 if q is complete, recursively add further
projections/attachments
14 ifj=N
15 break
16 for i  −1,0 to j − 1
17 foreach goal state q in cell (i, j)
18 q  SHIFT(q, wj)
19 add q to cell (j, j + 1)
</figure>
<subsectionHeader confidence="0.996335">
3.2 Lexicalization and context-sensitivity
</subsectionHeader>
<bodyText confidence="0.999921055555556">
Probably the most important shortcoming of
PCFG’s is the assumption of context-free rule prob-
abilities, i.e. the probability distribution over pos-
sible righthand sides given a lefthand side is inde-
pendent from the function or position of the left-
hand side. This assumption is quite wrong. For
instance, in the Penn Treebank an NP in subject
position produces a personal pronoun in 13.7% of
the cases, while in object position it only does so in
2.1% of the cases (Manning and Carpenter, 1997).
Furthermore, findings from corpus-based linguis-
tic studies and developments in functional gram-
mar indicate that the lexical realization of a con-
text, besides its syntactic analysis, strongly influ-
ences patterns of syntactic preference. Today’s best
automatic parsers are made substantially more ef-
ficient and accurate by applying lexicalized gram-
mar (Manning and Sch¨utze, 1999).
</bodyText>
<subsectionHeader confidence="0.748635">
3.2.1 Context-sensitive and lexicalized states
</subsectionHeader>
<bodyText confidence="0.999894384615384">
In our work we did not attempt to find semantic gen-
eralizations (such as casting a verb form to its infini-
tive form or finding semantic attributes); our simple
(but probably suboptimal) approach, borrowed from
(Magerman, 1994; Collins, 1996; Chelba, 2000), is
to percolate words upward in the parse tree in the
form in which they appear in the sentence. In our
experiments, we opted to hardcode the head posi-
tions as part of the projection rules.3 The nodes of
the resulting partial parse trees thus are annotated
with a category label (the CAT feature) and a lexical
label (the WORD feature).
The notation (1) of a state is now replaced with
</bodyText>
<equation confidence="0.547131">
q = (G, L1, L2; Z/z --*i X/x * jp; g, v) (13)
</equation>
<bodyText confidence="0.999917666666667">
where z is the WORD of the mother (possibly
empty), x is the WORD of the first daughter (not
empty), and the extended context contains
</bodyText>
<listItem confidence="0.9930524">
• G = CAT of a goal state qg;
• L1 = (CAT, WORD) of the state q1 projecting
qg;
• L2 = (CAT, WORD) of the state q2 projecting a
goal state dominating q1.
</listItem>
<bodyText confidence="0.999682666666667">
If the grammar only contains unary and binary
rules, L1 and L2 correspond with Chelba’s concept
of exposed heads — which was in fact the idea be-
hind the definition above. The mixed bottom-up and
top-down parsing order of PLCG allows to condi-
tion q on a goal constituent G higher up in the par-
tial tree containing q; this turns out to significantly
improve efficiency with respect to Jelinek’s bottom-
up chart parser.
</bodyText>
<footnote confidence="0.783057">
3Inserting a probabilistic head percolation model, as in
(Chelba, 2000), may be an alternative.
</footnote>
<subsectionHeader confidence="0.526414">
3.2.2 Extended parser operations
</subsectionHeader>
<bodyText confidence="0.999816857142857">
In this section, we extend the parser operations of
Sec. 3.1.3 to handle context-sensitive and lexical-
ized states. The forward and inner probability up-
date formulas remain formally the same and are not
repeated here.
The SHIFT operation q s q is a transition from
q to q with probability p where
</bodyText>
<equation confidence="0.878744">
q = (G, L1, L2; Z/z i X/x  jY ; µ, ) (2)
q = (Y, X/x, L1; W/w  j W/w  j+1; µ, )
(3)
p = ps(wj|q). (4)
</equation>
<bodyText confidence="0.9170165">
The PROJECT operation q p q is a transition
from q to q with probability p where
</bodyText>
<equation confidence="0.9842082">
q = (G, L1, L2; Z/z i X/x  j; µ, ) (5)
q = (G, L1, L2; T/t i Z/z  j; µ, ) (6)
p = pp(T, |q) · (1 − pa(q)) (7)
If Z is in head position, t = z; otherwise t is left
unspecified.
</equation>
<bodyText confidence="0.807334">
The ATTACH operation q a q is a transition
from q to q given q with a probability p where
</bodyText>
<equation confidence="0.925942166666667">
q = (G, L1, L2; Z/z h X/x iY; µ, )
(8)
q = (Y, X/x, L1; Y/y i T/t  j; µ, )
q = (G, L1, L2; Z/z h X/x  j; µ, ) (9)
p = pa(q) ·  (10)
If Y is in head position, z = y; otherwise, z = z.
</equation>
<subsectionHeader confidence="0.987378">
3.3 PLCG-based language model
</subsectionHeader>
<bodyText confidence="0.97184475">
A language model (LM) is a word sequence pre-
dictor (or an estimator of word sequence probabili-
ties). Following common practice in language mod-
eling for speech recognition, we predict words in a
sentence from left to right4 with probabilities of the
form p(wj|wj−1
0 ). Suppose the parser has worked
its way through wj−1
</bodyText>
<equation confidence="0.9594965">
0 and is about to make wj-
SHIFT transitions. Then we can write
p(wj|q)p(q|w j−1
0 ). (14)
</equation>
<footnote confidence="0.8446885">
4Since this allows the language model to be applied in early
stages of the search.
</footnote>
<bodyText confidence="0.9594435">
where j is the set of goal states in position j. The
factor p(wj|q) is given by the transition probability
associated with the SHIFT operation.5
On the other hand, note that
</bodyText>
<equation confidence="0.9949005">
µ(q) = p(w j−1
0 ) (15)
</equation>
<bodyText confidence="0.99988725">
where j is the set of states in position j that
resulted from SHIFT operations. The first equa-
tion holds because there are only PROJECT and AT-
TACH transitions between the elements of j and
j, since the sum of outgoing transitions from each
state in that region equals 1 and therefore the total
probability mass is preserved. By inserting (15) into
(14) we obtain
</bodyText>
<subsectionHeader confidence="0.993666">
3.4 Model reestimation
</subsectionHeader>
<bodyText confidence="0.997081538461538">
The pp, ps and pa submodels can be rees-
timated with iterative expectation-maximization,
which needs the computation of frequency expec-
tations. For this purpose we define the outer prob-
ability of a state q, written as (q), as the sum of
probabilities of precisely that part of the paths that
is not included in the inner probability of q. The
outer probability of a complete state is analogous to
Baker’s (1979) definition of an outside probability.
The outer probabilities are computed in the re-
verse direction starting from qF, provided that a list
of backward references were stored with each state
((q)  , (q)  ):6
</bodyText>
<listItem confidence="0.998039714285714">
• (qF) = 1.
• Reverse ATTACH (cfr. (8, 9, 10)): += p
and += p/. These formulas are made
clear in Fig. 1.
• Reverse PROJECT (cfr. (5, 6, 7)): +=  p.
• A reverse SHIFT is not necessary, but could be
used as a computational check.
</listItem>
<footnote confidence="0.998538142857143">
5Consequently the computation of LM probabilities re-
quires almost no extra work. A model p(wj |q) used in (14)
different from ps(wj |q) used by the parser may be chosen how-
ever.
6Care has to be taken that an outer probability is complete
before it propagates to other items. A topological sort could
serve this purpose.
</footnote>
<equation confidence="0.945840958333333">
�
p(wj|w j−1
0 ) =
q j
� µ(q) = �
q j q j
-1 Eq j p(wj  |q)µ(q)
p(wj|w0 ) =
E
q j µ(q)
. (16)
Now the expected frequency of a transition o 
{s, p, a} from q to q in a full parse of S is
E[Freq(q o q|S)] =
E Pr(path|S)Freq(q o q|path). (17)
all paths
Since all full parses terminate in qF, the final state,
v(qF) = µ(qF) = Pr(S). Therefore (17) is com-
putable as
E[Freq(q o q|S)] =
� 1
(qF)v(q)�(q) if o = s, (18)
(qF)v(q)p(q oq)�(q) else.
1
</equation>
<bodyText confidence="0.999996">
The expected frequencies required for the reesti-
mation of the conditional distributions are then ob-
tained by summing (18) over the state attributes
from which the required distribution is independent.
</bodyText>
<sectionHeader confidence="0.975206" genericHeader="method">
4 Empirical evaluation
</sectionHeader>
<subsectionHeader confidence="0.997415">
4.1 Modeling
</subsectionHeader>
<bodyText confidence="0.999975255813954">
We have trained two sets of models. The first set
was trained on sections 0–20 of the Penn Treebank
(PTB) (Marcus et al., 1995) using sections 21–22
for development decisions and tested on sections
23–24. The second set was trained on the BLLIP
WSJ Corpus (BWC), which is a machine-parsed
(Charniak, 2000) version of (a selection of) the
ACL/DCI corpus, very similar to the selection made
for the WSJ0/1 CSR corpus. As the training set,
we used the BWC minus the WSJ0/1 “dfiles” and
“efiles” intended for CSR development and evalua-
tion testing.
The PTB devset was used for fixing submodel pa-
rameterizations and software debugging, while per-
plexities are measured on the PTB testset. The
BWC trainset was used in rescoring N-best lists
in order to assess the models’ potential in speech
recognition. Both the PTB and BWC underwent
the following preprocessing steps: (a) A vocabu-
lary was fixed as the 10k (PTB) resp. 30k (BWC)
most frequent words; out-of-vocabulary words were
replaced by unk. Numbers in Arabic digits were
replaced by one token ‘N’. (b) Punctuation was re-
moved. (c) All characters were converted to lower-
case. (d) All parse trees were binarized in much the
same way as detailed in (Chelba, 2000, pp. 12–17);
non-terminal unary productions were eliminated by
collapsing two nodes connected by a unary branch
to one node annotated with a combined label. This
step allowed a simple implementation and compar-
ison of results with related publications. We dis-
tinguished 1891 different projections, 143 different
non-terminal categories and 41 different parts-of-
speech. (e) All constituents were annotated with a
lexical head using deterministic rules by Magerman
(1994).
The training then proceded by decomposing all
parse trees into sequences of SHIFT, PROJECT and
ATTACH transitions. The submodels were finally
estimated from smoothed relative counts of transi-
tions using standard language modeling techniques:
Good-Turing back-off (Katz, 1987) and deleted in-
terpolation (Jelinek, 1997).
</bodyText>
<subsectionHeader confidence="0.935945">
Shift submodel
</subsectionHeader>
<bodyText confidence="0.9823845">
The SHIFT submodel implements (4). Finding a
good parameterization entails fixing the features
that should explicitly appear in the context and in
which order, so that all information-bearing ele-
ments are incorporated, with limited data fragmen-
tation. This is not a straightforward task. We went
through an iterative process of intuitively guessing
which feature should be added or removed from
the context or changing the order, building a corre-
sponding model and evaluating its conditional per-
plexity (CPPL) against the devset. The CPPL of
a SHIFT submodel is its perplexity measured on
a test set consisting of (context, word to be pre-
dicted) pairs (i.e. the SHIFT transitions according to
a certain parameterization) extracted from the cor-
rect parse trees of a parsed test corpus. In other
words, the CPPL is an underbound of the PPL in
that it would be the PPL from an ideal parser. We fi-
nally concluded that the parameterization (notation
being consistent with (2))
ps(w|Y, x, L1.WORD), (19)
where the conditioning sequence is ordered from
most to least significant, is optimal for our purposes
in the given experimental conditions. The CPPL of
</bodyText>
<figure confidence="0.994274727272727">
�
v �
q q qF
s a
qI
qo
q
p
s a
� v �
V v �
</figure>
<figureCaption confidence="0.992963333333333">
Figure 1: Relations between inner and outer probabili-
ties along a single path at attachment of q to q resulting
into q.
</figureCaption>
<tableCaption confidence="0.917761">
Table 1: Word trigram (baseline) and PTB model per-
plexities.
</tableCaption>
<figure confidence="0.92792675">
model GT DI
(a) word trigram 190 193
(b) PLCG-based LM 185 187
(c) linear interpolation: .6(a) + .4(b) 159 166
</figure>
<bodyText confidence="0.959648666666667">
this model on the PTB devset is 48, which displays
the great potential of a correct syntactic partial parse
to predict the next word.
</bodyText>
<subsectionHeader confidence="0.727119">
Project/attach submodel
</subsectionHeader>
<bodyText confidence="0.975787578947368">
The ATTACH submodel can be incorporated into
the PROJECT submodel by treating the attachment
as a special kind of projection. This approach
was systematically applied since it sped up pars-
ing. Having the possibility to choose different pa-
rameterizations in separate PROJECT and ATTACH
submodels did not lower perplexity and increased
execution time. Therefore, we always used com-
bined PROJECT/ATTACH submodels in further ex-
periments.
The PROJECT/ATTACH submodel implements (7&apos;)
and (10&apos;). The process of finding an appropriate
parameterization used to build the SHIFT submodel
was also applied here. Finally we concluded that
the parameterization (notation being consistent with
(5&apos;))
pp(T, ot|Z, G, z) (20)
is optimal for our purposes in the given experimen-
tal conditions.
</bodyText>
<subsectionHeader confidence="0.997647">
4.2 Evaluation of PTB models
</subsectionHeader>
<bodyText confidence="0.981442888888889">
Table 1 lists test set perplexities (excluding OOVs
and unparsed parts of sentences) of Good-Turing
smoothed back-off models (GT) and deleted-
interpolation smoothed (DI) models trained on the
PTB trainset and tested on the PTB testset. We ob-
served similar results with both smoothing meth-
ods. As a baseline, word trigram (a) was trained
and tested on the same material. The PPL obtained
with the PLCG-based LM (b), using parametriza-
tions (19) and (20), is not much lower than the base-
line PPL.7 Interpolation (c) with the baseline how-
ever yields a relative PPL reduction of 14 to 16%
with respect to the baseline.
7Using parametrizations pp(T, a1z, G, L1.CAT) for projec-
tion from W-items and pp(T, aIG, Z, X, z) for other projec-
tions, we recently obtained a PPL of 178 (and 155 when inter-
polated). This result is left out from the discussion in order to
keep it clear and complete.
</bodyText>
<tableCaption confidence="0.9519034">
Table 2: WER results (%) after 100-best list rescoring
on the DARPA WSJ Nov ’92 evaluation test set, non-
verbalized punctuation. The models are smoothed with
Good-Turing back-off (WER results in column GT) or
deleted interpolation (DI).
</tableCaption>
<table confidence="0.521939">
rescoring model GT DI
(a) DARPA word trigram 10.44
(b) BWC word trigram 11.31 11.08
(c) BWC Chelba-Jelinek SLM 10.86
</table>
<listItem confidence="0.770515833333333">
(d) (a) and (c) combined 9.82
(e) (b) and (c) combined 10.60
(f) BWC PLCG-based SLM 11.45 11.48
(g) (a) and (e) combined 9.85 9.87
(h) (b) and (e) combined 10.38 10.58
(i) Best possible 4.46 4.46
</listItem>
<bodyText confidence="0.999819538461538">
Parse accuracy is around 79% for both labeled
precision and recall on section 23 of PTB (exclud-
ing unparsed sentences, about 4% of all sentences).
In comparison, with our own implementation of
Chelba-Jelinek, we measured a labeled precision
and recall of 57% and 75% on the same input. These
results seem fairly low compared to other recent
work on large-scale parsing, but may be partly due
to the left-to-right restriction of our language mod-
els,8 which for instance prohibits word-lookahead.
Moreover, while we measured accuracy against a
binarized version of PTB, the original parses are
rather flat, which may allow higher accuracies.
</bodyText>
<subsectionHeader confidence="0.999805">
4.3 Evaluation of BWC-models
</subsectionHeader>
<bodyText confidence="0.999930210526316">
The main target application of our research into
LM is speech recognition. We performed N-best
list rescoring experiments on the DARPA WSJ Nov
’92 evaluation test set, non-verbalized punctuation.
The N-best lists were obtained from the L&amp;H Voice
Xpress v4 speech recognizer using the standard tri-
gram model included in the test suite (20k open vo-
cabulary, no punctuation).
In Table 2 we report word-recognition error rates
(WER) after rescoring using Chelba-Jelinek and
PLCG-based models. Both DI and GT smooth-
ing methods yielded very comparable results. Due
to technical limitations, all the models except the
baseline trigram were trimmed by ignoring highest-
order events that occurred only once.
The best PLCG-based SLM trained on the BWC
train set (f) performs worse than the official word
trigram (a). However, since the BWC does not com-
pletely cover the complete WSJ0 LM train material
</bodyText>
<footnote confidence="0.270968">
8Not to be confused with left-to-right parsing.
</footnote>
<bodyText confidence="0.999956428571428">
and slightly differs in tokenization, it is more fair
to compare with the performance of a word trigram
trained on the BWC train set (b). Results (g) and
(h) show that the PLCG-based SLM lowers WER
with 4% relative when used in combination with the
baseline models. A comparable result was obtained
with the Chelba-Jelinek SLM (results (d) and (e)).
</bodyText>
<sectionHeader confidence="0.973561" genericHeader="conclusions">
5 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999969923076923">
The PLCG-based SLM exposes a slight loss of ro-
bustness in the reduced recognition rate when it
is used as a stand-alone rescoring LM. Combined
with a word trigram LM however, perplexity and
WER reductions with respect to a word 3-gram
baseline seem similar to those obtained with the
Chelba-Jelinek SLM and those previously reported
by Chelba (2000). On the other hand, the PLCG-
based SLM is significantly faster and obtains a
higher parsing accuracy.
In the future we plan to evaluate full EM reesti-
mation of the models on the trainset using the for-
mulas given in this paper.
</bodyText>
<sectionHeader confidence="0.99604" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999636">
The authors wish to thank Paul Vozila for discussing
intermediate results and for providing the authors
with the 100-best lists used for sentence rescoring.
The authors are also indebted to Saskia Janssens and
Kristin Daneels for their help with some of the ex-
periments.
This research is supported by the Institute for
the promotion of Innovation by Science and Tech-
nology in Flanders (IWT-Flanders), contract no.
000286.
</bodyText>
<sectionHeader confidence="0.995978" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.985191109375">
James K. Baker. 1979. Trainable grammars for
speech recognition. In Jared J. Wolf and Den-
nis H. Klatt, editors, Speech Communication Pa-
pers for the 97th Meeting of the Acoustical Soci-
ety of America, pages 547–550. The MIT Press,
Cambridge, MA.
Eugene Charniak. 2000. A maximum-entropy in-
spired parser. In Proc. of the NAACL, pages 132–
139.
Ciprian Chelba. 2000. Exploiting Syntactic Struc-
ture for Natural Language Modeling. Ph.D. the-
sis, Johns Hopkins University.
Michael J. Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In Proc.
of the 34th Annual Meeting of the ACL, pages
184–191.
Frederick Jelinek and Ciprian Chelba. 1999.
Putting language into language modeling. In
Proc. of Eurospeech ’99, volume I, pages KN–
1–6.
Frederik Jelinek and John Lafferty. 1991. Compu-
tation of the probability of initial substring gener-
ation by stochastic context-free grammars. Com-
putational Linguistics, 17(3):315–323.
Frederick Jelinek. 1997. Statistical Methods for
Speech Recognition. The MIT Press, Cambridge,
MA.
Slava M. Katz. 1987. Estimation of probabili-
ties from sparse data for the language model
component of a speech recognizer. IEEE Trans.
on Acoustics, Speech and Signal Processing,
35:400–401.
David M. Magerman. 1994. Natural Language
Parsing as Statistical Pattern Recognition. Ph.D.
thesis, Stanford University.
Christopher D. Manning and Bob Carpenter. 1997.
Probabilistic parsing using left corner language
models. In Proc. of the Fifth International Work-
shop on Parsing Technologies, pages 147–158.
Christopher D. Manning and Hinrich Sch¨utze.
1999. Foundations of Statistical Natural Lan-
guage Processing. The MIT Press, Cambridge,
MA.
Mitchell P. Marcus, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1995. Building a
large annotated corpus of English: the Penn Tree-
bank. Computational Linguistics, 19(2):313–
330.
Brian Roark and Mark Johnson. 2000. Efficient
probabilistic top-down and left-corner parsing.
In Proc. of the 37th Annual Meeting of the ACL,
pages 421–428.
Andreas Stolcke. 1995. An efficient probabilis-
tic context-free parsing algorithm that computes
prefix probabilities. Computational Linguistics,
21(2):165–201.
Filip Van Aelten and Marc Hogenhout. 2000.
Inside-outside reestimation of Chelba-Jelinek
models. Internal Report L&amp;H–SR–00–027,
Lernout &amp; Hauspie, Wemmel, Belgium.
Dong Hoon Van Uytsel. 2000. Earley-inspired
parsing language model: Background and pre-
liminaries. Internal Report PSI-SPCH-00-1,
K.U.Leuven, ESAT, Heverlee, Belgium.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000058">
<title confidence="0.98821">A Structured Language Model based on Context-Sensitive Left-Corner Parsing</title>
<author confidence="0.999668">Hoon Van_Van_Van</author>
<email confidence="0.529843">donghoon@esat.kuleuven.ac.befilip.van.aelten@lhs.becompi@esat.kuleuven.ac.be</email>
<affiliation confidence="0.548589">Universiteit Leuven, ESAT,</affiliation>
<address confidence="0.564927">amp; Hauspie, Belgium</address>
<abstract confidence="0.998469104065043">Recent contributions to statistical language modeling for speech recognition have shown that probabilistically parsing a partial word sequence aids the prediction of the next word, leading to “structured” language models that have the potential to outperform n-grams. Existing approaches to structured language modeling construct nodes in the partial parse tree after all of the underlying words have been predicted. This paper presents a different approach, based on probabilistic left-corner grammar (PLCG) parsing, that extends a partial parse both from the bottom up and from the top down, leading to a more focused and more accurate, though somewhat less robust, search of the parse space. At the core of our new structured language model is a fast context-sensitive and lexicalized PLCG parsing algorithm that uses dynamic programming. Preliminary perplexity and word-accuracy results appear to be competitive with previous ones, while speed is increased. 1 Structured language modeling In its current incarnation, (unconstrained) speech recognition relies on a left-to-right language model which estimates the occurrence of a next word a sequence of preceding words 0 called a language model (LM). Obviously the context space is huge and even in very large training corpora most contexts never occur, which prohibits a reliable probability estimation. Therefore the context space needs to be mapped to a much smaller space, such that only the essential information is retained. In spite of its a shorthand, denotes a sequence else it is the empty sequence. the trigram LM, that reduces is hard to improve on and still the main language model component in state-of-the-art speech recognition systems. It is therefore commonly used as a baseline in the evaluation of other models, including the one described in this paper. Structured language models (SLM) introduce parsing into language modeling by alternating between predicting the next word using features of partial parses of the context and extending the partial parses to cover the next word. Following this approach, Chelba and Jelinek (2000) obtained a SLM that slightly improves on a trigram model both in perplexity and recognition performance. The Chelba-Jelinek SLM is, to our knowledge, the first left-to-right LM using parsing techniques that is successfully applied to large vocabulary speech recognition. It is built on top of a lexicalized probabilistic shift-reduce parser that predicts the next word from the headwords (“exposed” heads) and categories of the last two predicted isolated constituents of the context. Then the predicted word becomes the last isolated constituent and the last two constituents are repeatedly recombined until the parser decides to stop. A dynamic programming (DP) version of Chelba’s parser, inspired on the CYK chart parser, was proposed in (Jelinek and Chelba, 1999). Our implementation is roughly quadratic in the length of the sentence, but not significantly faster than Chelba’s non-DP parser. It scored somewhat lower in perplexity before reestimation (presumably by avoiding search errors), but remained roughly at the same level after full inside-outside reestimation (Van Aelten and Hogenhout, 2000). An obvious weakness of the Chelba-Jelinek SLM is the bottom-up behavior of the parser: it creates isolated constituents and only afterwards is it able to check whether a constituent fits into a higher struc- Van Uytsel (2000) developed a top-down alternative along similar lines but based on a lexicalized and context-sensitive DP version of an efficient Earley parser (Stolcke, 1995; Jelinek and Lafferty, 1991). The Earley-based SLM performed worse than the Chelba-Jelinek SLM, mostly due to the fact that the rule production probabilities cannot be conditioned on the underlying lexical information, thus producing a lot of wrong parses. The weaknesses of our Earley SLM have led us to consider probabilistic left-corner grammar (PLCG) parsing (Manning and Carpenter, 1997), which follows a mixed bottom-up and top-down approach. Its potential to enhance parsing efficiency has been recognized by Roark and Johnson (2000), who simulated a left-corner parser with a top-down best-first parser applying a left-corner-transformed PCFG grammar. For the language model described in this paper, however, we implemented a DP version of a native left-corner parser using a left-corner treebank grammar (containing projection rules instead of production rules). The efficiency of our implementation further allowed to enrich the history annotation of the parser states and to apply a lexicalized grammar. The following section contains a brief review of Manning’s PLCG parser. Section 3 describes how it was adapted to our SLM framework: we introduce lexicalization and context-sensitivity, present a DP algorithm using a chart of parser states and finally we define a language model based on the adapted PLCG parser. At the end of the same section we explain how the initial language model can be trained on additional plain text through a variant of insideoutside reestimation. In section 4 we evaluate a few PLCG-based SLMs obtained from the Penn Treebank and BLLIP WSJ Corpus. We present test set perplexity measurements and word accuracy after nbest list rescoring to assess their viability for speech recognition. 2 Classic PLCG parsing parameters of a PLCG are called probabilities. They are of the form be read as “given a completed constituent domby a goal category the probability that is a has its first daughter and as its next daughters”. A PLCG contains essentially the same rules as a probabilistic context-free gram- (PCFG), but the latter conditions the rule probon the mother category probabilities). In both cases the joint probability of the entire parse tree and the parsed sentence is the product of the production resp. projection probabilities of the local trees it consists of. While PCFG parsing proceeds from the top down or from the bottom up, PLCG naturally leads to a parsing scheme that is a mixture of both. The advantages of this are made clear in the subsections below. Formally, a PLCG parser has three elementary operations: • given that an unexpanded constituent from position shift the next word probability called the • given a complete constituent by a goal category starting in poending in predict a mother conin position completed till position and zero or more unexpanded constituents in probabil- • given a complete constituent domby a goal category identify the first the latter with probability 3 Extending the PLCG framework 3.1 Synchronous chart parsing with PLCG In this subsection we present the basic parsing algorithm and its data structures and operations. In the subsections that follow, we will introduce lexicalization and context-sensitivity by extending this framework. The PLCG parsing process is interpreted as a search through a network of states, a compact representation of the search space. The network nodes correspond to states and the arcs to operations (annotated with transition probabilities). A (partial) parse corresponds to a (partial) path through the network. The joint probability of a partial parse and the covered part of the sentence is equal to the partial path probability, i.e. the product of the probabilities of the transitions in the path. 3.1.1 PLCG states write a state v) the goal category, the category of a from position up till position the first daughter category, the reunresolved daughters of and forward and inner probabilities defined below. The zero or more resolved daughter categories: we make abstraction of the identities of resolved daughters (except the first one), because parser moves do not depend on them. If called a otherwise a 3.1.2 Forward and inner probability a state defined in (1). We define its forthe sum of the probof the paths ending in starting in the inistate and generating As a consequence, probability). the sum of the of the paths generating ending starting with a As a conse- Note that the forward and inner probabilities of final state should be identical and equal to 3.1.3 Parser operations In this paragraph we reformulate the classic PLCG parser operations in terms of transitions between states. We hereby specify update formulas for forward and inner probabilities. starts from a goal state ) shifts the next word position the input updating or generating a new state jw  with transition probability already lives in the chart, only its forward probability is updated. The given update formula is justified by the relation the sum is over all from the transition probability Computing a trivial case of the definition. C-like shorthand notation that is to there was no in the chart yet, otherwise is with a complete state, two transitions possible: a goal state with a probor a probability 1 from a complete state ) and generates or updates a state with transition probability Again, the forward probability is computed recuras a sum of products. Now to be too: the constituent general may resolved with more than one different which time adds to that a mother constituent inherits her first daughter (left-corner). a complete state in (5) some goal state in the parpath leading to the is a transition from (9) and transition probability can be updated from similarly to (3) (6)? The reason is that use of constraints: the transition from is possible if a matching goal state occurred in path leading to Therefore computing in (3) (6) would include all paths that generate also that do not contain Instead, the update of (9) combines all paths leading to with the starting from and ending in The update an analogous reasoning. 3.1.4 Chart representation The parser produces a set of states that can be conveniently organized in a staircase-shaped chart similar to the one used by the CYK parser. In the chart cell coordinates store all the states starting completed up till position 3.1.5 Synchronous parsing algorithm Following (Chelba, 2000), we represent a sentence by a sequence of word identities starting with a token that is used in the context but not predicted, followed by a sentence-end that is predicted by the model. We are the sentence proper together with una node labeled and the node together a The parser starts from the initial state (11) processing the sentence provided a full parse was found, the final state (12) found in cell Now we are ready to formulate the parsing algo- Note that we treat an as a as explained in Sec. 4.1. state cell in = and proj = ‘attach’ state cell  to cell  to cell is complete, recursively add projections/attachments state cell  to cell 3.2 Lexicalization and context-sensitivity Probably the most important shortcoming of PCFG’s is the assumption of context-free rule probabilities, i.e. the probability distribution over possible righthand sides given a lefthand side is independent from the function or position of the lefthand side. This assumption is quite wrong. For instance, in the Penn Treebank an NP in subject position produces a personal pronoun in 13.7% of the cases, while in object position it only does so in 2.1% of the cases (Manning and Carpenter, 1997). Furthermore, findings from corpus-based linguistic studies and developments in functional grammar indicate that the lexical realization of a context, besides its syntactic analysis, strongly influences patterns of syntactic preference. Today’s best automatic parsers are made substantially more efficient and accurate by applying lexicalized grammar (Manning and Sch¨utze, 1999). 3.2.1 Context-sensitive and lexicalized states In our work we did not attempt to find semantic generalizations (such as casting a verb form to its infinitive form or finding semantic attributes); our simple (but probably suboptimal) approach, borrowed from (Magerman, 1994; Collins, 1996; Chelba, 2000), is to percolate words upward in the parse tree in the form in which they appear in the sentence. In our experiments, we opted to hardcode the head posias part of the projection The nodes of the resulting partial parse trees thus are annotated a category label (the and a lexical (the The notation (1) of a state is now replaced with v) the the mother (possibly the the first daughter (not empty), and the extended context contains G a goal state • of the state • of the state a state dominating If the grammar only contains unary and binary with Chelba’s concept heads which was in fact the idea behind the definition above. The mixed bottom-up and top-down parsing order of PLCG allows to condia goal constituent up in the partree containing this turns out to significantly improve efficiency with respect to Jelinek’s bottomup chart parser. a probabilistic head percolation model, as in (Chelba, 2000), may be an alternative. 3.2.2 Extended parser operations In this section, we extend the parser operations of Sec. 3.1.3 to handle context-sensitive and lexicalized states. The forward and inner probability update formulas remain formally the same and are not repeated here. is a transition from with probability )  is a transition with probability ) in head position, otherwise left unspecified. is a transition given with a probability ) in head position, = otherwise, = 3.3 PLCG-based language model A language model (LM) is a word sequence predictor (or an estimator of word sequence probabilities). Following common practice in language modeling for speech recognition, we predict words in a from left to with probabilities of the Suppose the parser has worked way through is about to make Then we can write 0). this allows the language model to be applied in early stages of the search. jis the set of goal states in position The given by the transition probability with the On the other hand, note that the set of states in position from The first equaholds because there are only between the elements of jand since the sum of outgoing transitions from each state in that region equals 1 and therefore the total probability mass is preserved. By inserting (15) into (14) we obtain 3.4 Model reestimation and submodels can be reestimated with iterative expectation-maximization, which needs the computation of frequency expec- For this purpose we define the proba state written as as the sum of probabilities of precisely that part of the paths that not included in the inner probability of The outer probability of a complete state is analogous to Baker’s (1979) definition of an outside probability. The outer probabilities are computed in the redirection starting from provided that a list of backward references were stored with each state • Reverse These formulas are made clear in Fig. 1. Reverse A reverse not necessary, but could be used as a computational check. the computation of LM probabilities realmost no extra work. A model in (14) from by the parser may be chosen however. has to be taken that an outer probability is complete before it propagates to other items. A topological sort could serve this purpose. � 0) ) E the expected frequency of a transition in a full parse of E all paths all full parses terminate in the final state, Therefore (17) is computable as The expected frequencies required for the reestimation of the conditional distributions are then obtained by summing (18) over the state attributes from which the required distribution is independent. 4 Empirical evaluation 4.1 Modeling We have trained two sets of models. The first set was trained on sections 0–20 of the Penn Treebank (PTB) (Marcus et al., 1995) using sections 21–22 for development decisions and tested on sections 23–24. The second set was trained on the BLLIP WSJ Corpus (BWC), which is a machine-parsed (Charniak, 2000) version of (a selection of) the ACL/DCI corpus, very similar to the selection made for the WSJ0/1 CSR corpus. As the training set, we used the BWC minus the WSJ0/1 “dfiles” and “efiles” intended for CSR development and evaluation testing. The PTB devset was used for fixing submodel parameterizations and software debugging, while perplexities are measured on the PTB testset. The BWC trainset was used in rescoring N-best lists in order to assess the models’ potential in speech recognition. Both the PTB and BWC underwent the following preprocessing steps: (a) A vocabulary was fixed as the 10k (PTB) resp. 30k (BWC) most frequent words; out-of-vocabulary words were by Numbers in Arabic digits were replaced by one token ‘N’. (b) Punctuation was removed. (c) All characters were converted to lowercase. (d) All parse trees were binarized in much the same way as detailed in (Chelba, 2000, pp. 12–17); non-terminal unary productions were eliminated by collapsing two nodes connected by a unary branch to one node annotated with a combined label. This step allowed a simple implementation and comparison of results with related publications. We distinguished 1891 different projections, 143 different non-terminal categories and 41 different parts-ofspeech. (e) All constituents were annotated with a lexical head using deterministic rules by Magerman (1994). The training then proceded by decomposing all trees into sequences of The submodels were finally estimated from smoothed relative counts of transitions using standard language modeling techniques: Good-Turing back-off (Katz, 1987) and deleted interpolation (Jelinek, 1997). Shift submodel implements Finding a good parameterization entails fixing the features that should explicitly appear in the context and in which order, so that all information-bearing elements are incorporated, with limited data fragmentation. This is not a straightforward task. We went through an iterative process of intuitively guessing which feature should be added or removed from the context or changing the order, building a corremodel and evaluating its perplexity (CPPL) against the devset. The CPPL of is its perplexity measured on a test set consisting of (context, word to be prepairs (i.e. the according to a certain parameterization) extracted from the correct parse trees of a parsed test corpus. In other words, the CPPL is an underbound of the PPL in that it would be the PPL from an ideal parser. We finally concluded that the parameterization (notation consistent with where the conditioning sequence is ordered from most to least significant, is optimal for our purposes in the given experimental conditions. The CPPL of s a p s a � v � 1: between inner and outer probabilialong a single path at attachment of resulting 1: trigram (baseline) and PTB model perplexities. model GT DI (a) word trigram 190 193 (b) PLCG-based LM 185 187 linear interpolation: .6(a) .4(b) 159 166 this model on the PTB devset is 48, which displays the great potential of a correct syntactic partial parse to predict the next word. Project/attach submodel can be incorporated into by treating the attachment as a special kind of projection. This approach was systematically applied since it sped up parsing. Having the possibility to choose different pain separate submodels did not lower perplexity and increased execution time. Therefore, we always used comin further experiments. implements The process of finding an appropriate used to build the was also applied here. Finally we concluded that the parameterization (notation being consistent with is optimal for our purposes in the given experimental conditions. 4.2 Evaluation of PTB models Table 1 lists test set perplexities (excluding OOVs and unparsed parts of sentences) of Good-Turing smoothed back-off models (GT) and deletedinterpolation smoothed (DI) models trained on the PTB trainset and tested on the PTB testset. We observed similar results with both smoothing methods. As a baseline, word trigram (a) was trained and tested on the same material. The PPL obtained with the PLCG-based LM (b), using parametrizations (19) and (20), is not much lower than the base- Interpolation (c) with the baseline however yields a relative PPL reduction of 14 to 16% with respect to the baseline. parametrizations projecfrom and other projections, we recently obtained a PPL of 178 (and 155 when interpolated). This result is left out from the discussion in order to keep it clear and complete. 2: results (%) after 100-best list rescoring on the DARPA WSJ Nov ’92 evaluation test set, nonverbalized punctuation. The models are smoothed with Good-Turing back-off (WER results in column GT) or deleted interpolation (DI). rescoring model GT DI (a) DARPA word trigram 10.44 (b) BWC word trigram 11.31 11.08 (c) BWC Chelba-Jelinek SLM 10.86 (d) (a) and (c) combined 9.82 (e) (b) and (c) combined 10.60 (f) BWC PLCG-based SLM 11.45 11.48 (g) (a) and (e) combined 9.85 9.87 (h) (b) and (e) combined 10.38 10.58 (i) Best possible 4.46 4.46 Parse accuracy is around 79% for both labeled precision and recall on section 23 of PTB (excluding unparsed sentences, about 4% of all sentences). In comparison, with our own implementation of Chelba-Jelinek, we measured a labeled precision and recall of 57% and 75% on the same input. These results seem fairly low compared to other recent work on large-scale parsing, but may be partly due to the left-to-right restriction of our language modwhich for instance prohibits word-lookahead. Moreover, while we measured accuracy against a binarized version of PTB, the original parses are rather flat, which may allow higher accuracies. 4.3 Evaluation of BWC-models The main target application of our research into LM is speech recognition. We performed N-best list rescoring experiments on the DARPA WSJ Nov ’92 evaluation test set, non-verbalized punctuation. The N-best lists were obtained from the L&amp;H Voice Xpress v4 speech recognizer using the standard trigram model included in the test suite (20k open vocabulary, no punctuation). In Table 2 we report word-recognition error rates (WER) after rescoring using Chelba-Jelinek and PLCG-based models. Both DI and GT smoothing methods yielded very comparable results. Due to technical limitations, all the models except the baseline trigram were trimmed by ignoring highestorder events that occurred only once. The best PLCG-based SLM trained on the BWC train set (f) performs worse than the official word trigram (a). However, since the BWC does not completely cover the complete WSJ0 LM train material to be confused with left-to-right parsing. and slightly differs in tokenization, it is more fair to compare with the performance of a word trigram trained on the BWC train set (b). Results (g) and (h) show that the PLCG-based SLM lowers WER with 4% relative when used in combination with the baseline models. A comparable result was obtained with the Chelba-Jelinek SLM (results (d) and (e)). 5 Conclusion and future work The PLCG-based SLM exposes a slight loss of robustness in the reduced recognition rate when it is used as a stand-alone rescoring LM. Combined with a word trigram LM however, perplexity and WER reductions with respect to a word 3-gram baseline seem similar to those obtained with the Chelba-Jelinek SLM and those previously reported by Chelba (2000). On the other hand, the PLCGbased SLM is significantly faster and obtains a higher parsing accuracy. In the future we plan to evaluate full EM reestimation of the models on the trainset using the formulas given in this paper. Acknowledgements The authors wish to thank Paul Vozila for discussing intermediate results and for providing the authors with the 100-best lists used for sentence rescoring. The authors are also indebted to Saskia Janssens and Kristin Daneels for their help with some of the experiments.</abstract>
<note confidence="0.7855305">This research is supported by the Institute for the promotion of Innovation by Science and Technology in Flanders (IWT-Flanders), contract no. 000286.</note>
<title confidence="0.770576">References</title>
<author confidence="0.933049">Trainable grammars for</author>
<note confidence="0.6621545">speech recognition. In Jared J. Wolf and Den- H. Klatt, editors, Communication Papers for the 97th Meeting of the Acoustical Sociof pages 547–550. The MIT Press, Cambridge, MA. Eugene Charniak. 2000. A maximum-entropy inparser. In of the pages 132– 139. Chelba. 2000. Syntactic Strucfor Natural Language Ph.D. thesis, Johns Hopkins University. Michael J. Collins. 1996. A new statistical parser on bigram lexical dependencies. In the 34th Annual Meeting of the pages 184–191. Frederick Jelinek and Ciprian Chelba. 1999. Putting language into language modeling. In of Eurospeech volume I, pages KN– 1–6. Frederik Jelinek and John Lafferty. 1991. Computation of the probability of initial substring generby stochastic context-free grammars. Com- 17(3):315–323. Jelinek. 1997. Methods for The MIT Press, Cambridge, MA. Slava M. Katz. 1987. Estimation of probabilities from sparse data for the language model of a speech recognizer. Trans. Acoustics, Speech and Signal 35:400–401. M. Magerman. 1994. Language as Statistical Pattern Ph.D. thesis, Stanford University. Christopher D. Manning and Bob Carpenter. 1997. Probabilistic parsing using left corner language In of the Fifth International Workon Parsing pages 147–158.</note>
<author confidence="0.891023">Christopher D Manning</author>
<author confidence="0.891023">Hinrich Sch¨utze</author>
<affiliation confidence="0.902541">of Statistical Natural Lan- The MIT Press, Cambridge,</affiliation>
<address confidence="0.500698">MA.</address>
<author confidence="0.763191">a Building</author>
<abstract confidence="0.859141611111111">large annotated corpus of English: the Penn Tree- 19(2):313– 330. Brian Roark and Mark Johnson. 2000. Efficient probabilistic top-down and left-corner parsing. of the 37th Annual Meeting of the pages 421–428. Andreas Stolcke. 1995. An efficient probabilistic context-free parsing algorithm that computes probabilities. 21(2):165–201. Filip Van Aelten and Marc Hogenhout. 2000. Inside-outside reestimation of Chelba-Jelinek models. Internal Report L&amp;H–SR–00–027, Lernout &amp; Hauspie, Wemmel, Belgium. Dong Hoon Van Uytsel. 2000. Earley-inspired parsing language model: Background and preliminaries. Internal Report PSI-SPCH-00-1,</abstract>
<address confidence="0.536171">K.U.Leuven, ESAT, Heverlee, Belgium.</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James K Baker</author>
</authors>
<title>Trainable grammars for speech recognition. In</title>
<date>1979</date>
<booktitle>Speech Communication Papers for the 97th Meeting of the Acoustical Society of America,</booktitle>
<pages>547--550</pages>
<editor>Jared J. Wolf and Dennis H. Klatt, editors,</editor>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Baker, 1979</marker>
<rawString>James K. Baker. 1979. Trainable grammars for speech recognition. In Jared J. Wolf and Dennis H. Klatt, editors, Speech Communication Papers for the 97th Meeting of the Acoustical Society of America, pages 547–550. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy inspired parser.</title>
<date>2000</date>
<booktitle>In Proc. of the NAACL,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="20157" citStr="Charniak, 2000" startWordPosition="3593" endWordPosition="3594">S)] = � 1 (qF)v(q)�(q) if o = s, (18) (qF)v(q)p(q oq)�(q) else. 1 The expected frequencies required for the reestimation of the conditional distributions are then obtained by summing (18) over the state attributes from which the required distribution is independent. 4 Empirical evaluation 4.1 Modeling We have trained two sets of models. The first set was trained on sections 0–20 of the Penn Treebank (PTB) (Marcus et al., 1995) using sections 21–22 for development decisions and tested on sections 23–24. The second set was trained on the BLLIP WSJ Corpus (BWC), which is a machine-parsed (Charniak, 2000) version of (a selection of) the ACL/DCI corpus, very similar to the selection made for the WSJ0/1 CSR corpus. As the training set, we used the BWC minus the WSJ0/1 “dfiles” and “efiles” intended for CSR development and evaluation testing. The PTB devset was used for fixing submodel parameterizations and software debugging, while perplexities are measured on the PTB testset. The BWC trainset was used in rescoring N-best lists in order to assess the models’ potential in speech recognition. Both the PTB and BWC underwent the following preprocessing steps: (a) A vocabulary was fixed as the 10k (P</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy inspired parser. In Proc. of the NAACL, pages 132– 139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
</authors>
<title>Exploiting Syntactic Structure for Natural Language Modeling.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>Johns Hopkins University.</institution>
<contexts>
<context position="12072" citStr="Chelba, 2000" startWordPosition="2070" endWordPosition="2071">uting µ as in (3) and (6) would include all paths that generate q, also those that do not contain q. Instead, the update of µ in (9) combines all paths leading to q with the paths starting from q and ending in q. The update of  follows an analogous reasoning. 3.1.4 Chart representation The parser produces a set of states that can be conveniently organized in a staircase-shaped chart similar to the one used by the CYK parser. In the chart cell with coordinates (i, j) we store all the states starting in i and completed up till position j. 3.1.5 Synchronous parsing algorithm Following (Chelba, 2000), we represent a sentence by a sequence of word identities starting with a sentence-begin token (s), that is used in the context but not predicted, followed by a sentence-end token (/s), that is predicted by the model. We are collecting the sentence proper together with (/s) under a node labeled TOP, and the TOP node together with (s) under a TOP node. The parser starts from the initial state qI = (TOP; TOP/(s) --*−1 SB/(s) *0TOP; 1, 1). After processing the sentence S = wN−1 0 and provided a full parse was found, the final state qF = (TOP; TOP/(s) --*−1 SB/(s) * N; p(S), p(S)) is found in </context>
<context position="14586" citStr="Chelba, 2000" startWordPosition="2533" endWordPosition="2534">pments in functional grammar indicate that the lexical realization of a context, besides its syntactic analysis, strongly influences patterns of syntactic preference. Today’s best automatic parsers are made substantially more efficient and accurate by applying lexicalized grammar (Manning and Sch¨utze, 1999). 3.2.1 Context-sensitive and lexicalized states In our work we did not attempt to find semantic generalizations (such as casting a verb form to its infinitive form or finding semantic attributes); our simple (but probably suboptimal) approach, borrowed from (Magerman, 1994; Collins, 1996; Chelba, 2000), is to percolate words upward in the parse tree in the form in which they appear in the sentence. In our experiments, we opted to hardcode the head positions as part of the projection rules.3 The nodes of the resulting partial parse trees thus are annotated with a category label (the CAT feature) and a lexical label (the WORD feature). The notation (1) of a state is now replaced with q = (G, L1, L2; Z/z --*i X/x * jp; g, v) (13) where z is the WORD of the mother (possibly empty), x is the WORD of the first daughter (not empty), and the extended context contains • G = CAT of a goal state qg; •</context>
<context position="21064" citStr="Chelba, 2000" startWordPosition="3746" endWordPosition="3747">ations and software debugging, while perplexities are measured on the PTB testset. The BWC trainset was used in rescoring N-best lists in order to assess the models’ potential in speech recognition. Both the PTB and BWC underwent the following preprocessing steps: (a) A vocabulary was fixed as the 10k (PTB) resp. 30k (BWC) most frequent words; out-of-vocabulary words were replaced by unk. Numbers in Arabic digits were replaced by one token ‘N’. (b) Punctuation was removed. (c) All characters were converted to lowercase. (d) All parse trees were binarized in much the same way as detailed in (Chelba, 2000, pp. 12–17); non-terminal unary productions were eliminated by collapsing two nodes connected by a unary branch to one node annotated with a combined label. This step allowed a simple implementation and comparison of results with related publications. We distinguished 1891 different projections, 143 different non-terminal categories and 41 different parts-ofspeech. (e) All constituents were annotated with a lexical head using deterministic rules by Magerman (1994). The training then proceded by decomposing all parse trees into sequences of SHIFT, PROJECT and ATTACH transitions. The submodels </context>
<context position="28098" citStr="Chelba (2000)" startWordPosition="4892" endWordPosition="4893">the BWC train set (b). Results (g) and (h) show that the PLCG-based SLM lowers WER with 4% relative when used in combination with the baseline models. A comparable result was obtained with the Chelba-Jelinek SLM (results (d) and (e)). 5 Conclusion and future work The PLCG-based SLM exposes a slight loss of robustness in the reduced recognition rate when it is used as a stand-alone rescoring LM. Combined with a word trigram LM however, perplexity and WER reductions with respect to a word 3-gram baseline seem similar to those obtained with the Chelba-Jelinek SLM and those previously reported by Chelba (2000). On the other hand, the PLCGbased SLM is significantly faster and obtains a higher parsing accuracy. In the future we plan to evaluate full EM reestimation of the models on the trainset using the formulas given in this paper. Acknowledgements The authors wish to thank Paul Vozila for discussing intermediate results and for providing the authors with the 100-best lists used for sentence rescoring. The authors are also indebted to Saskia Janssens and Kristin Daneels for their help with some of the experiments. This research is supported by the Institute for the promotion of Innovation by Scienc</context>
</contexts>
<marker>Chelba, 2000</marker>
<rawString>Ciprian Chelba. 2000. Exploiting Syntactic Structure for Natural Language Modeling. Ph.D. thesis, Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Collins</author>
</authors>
<title>A new statistical parser based on bigram lexical dependencies.</title>
<date>1996</date>
<booktitle>In Proc. of the 34th Annual Meeting of the ACL,</booktitle>
<pages>184--191</pages>
<contexts>
<context position="14571" citStr="Collins, 1996" startWordPosition="2531" endWordPosition="2532">dies and developments in functional grammar indicate that the lexical realization of a context, besides its syntactic analysis, strongly influences patterns of syntactic preference. Today’s best automatic parsers are made substantially more efficient and accurate by applying lexicalized grammar (Manning and Sch¨utze, 1999). 3.2.1 Context-sensitive and lexicalized states In our work we did not attempt to find semantic generalizations (such as casting a verb form to its infinitive form or finding semantic attributes); our simple (but probably suboptimal) approach, borrowed from (Magerman, 1994; Collins, 1996; Chelba, 2000), is to percolate words upward in the parse tree in the form in which they appear in the sentence. In our experiments, we opted to hardcode the head positions as part of the projection rules.3 The nodes of the resulting partial parse trees thus are annotated with a category label (the CAT feature) and a lexical label (the WORD feature). The notation (1) of a state is now replaced with q = (G, L1, L2; Z/z --*i X/x * jp; g, v) (13) where z is the WORD of the mother (possibly empty), x is the WORD of the first daughter (not empty), and the extended context contains • G = CAT of a g</context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>Michael J. Collins. 1996. A new statistical parser based on bigram lexical dependencies. In Proc. of the 34th Annual Meeting of the ACL, pages 184–191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
<author>Ciprian Chelba</author>
</authors>
<title>Putting language into language modeling.</title>
<date>1999</date>
<booktitle>In Proc. of Eurospeech ’99, volume I,</booktitle>
<pages>1--6</pages>
<contexts>
<context position="3313" citStr="Jelinek and Chelba, 1999" startWordPosition="507" endWordPosition="510">ledge, the first left-to-right LM using parsing techniques that is successfully applied to large vocabulary speech recognition. It is built on top of a lexicalized probabilistic shift-reduce parser that predicts the next word from the headwords (“exposed” heads) and categories of the last two predicted isolated constituents of the context. Then the predicted word becomes the last isolated constituent and the last two constituents are repeatedly recombined until the parser decides to stop. A dynamic programming (DP) version of Chelba’s parser, inspired on the CYK chart parser, was proposed in (Jelinek and Chelba, 1999). Our implementation is roughly quadratic in the length of the sentence, but not significantly faster than Chelba’s non-DP parser. It scored somewhat lower in perplexity before reestimation (presumably by avoiding search errors), but remained roughly at the same level after full inside-outside reestimation (Van Aelten and Hogenhout, 2000). An obvious weakness of the Chelba-Jelinek SLM is the bottom-up behavior of the parser: it creates isolated constituents and only afterwards is it able to check whether a constituent fits into a higher structure. Van Uytsel (2000) developed a top-down alterna</context>
</contexts>
<marker>Jelinek, Chelba, 1999</marker>
<rawString>Frederick Jelinek and Ciprian Chelba. 1999. Putting language into language modeling. In Proc. of Eurospeech ’99, volume I, pages KN– 1–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederik Jelinek</author>
<author>John Lafferty</author>
</authors>
<title>Computation of the probability of initial substring generation by stochastic context-free grammars.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<issue>3</issue>
<contexts>
<context position="4071" citStr="Jelinek and Lafferty, 1991" startWordPosition="622" endWordPosition="625">It scored somewhat lower in perplexity before reestimation (presumably by avoiding search errors), but remained roughly at the same level after full inside-outside reestimation (Van Aelten and Hogenhout, 2000). An obvious weakness of the Chelba-Jelinek SLM is the bottom-up behavior of the parser: it creates isolated constituents and only afterwards is it able to check whether a constituent fits into a higher structure. Van Uytsel (2000) developed a top-down alternative along similar lines but based on a lexicalized and context-sensitive DP version of an efficient Earley parser (Stolcke, 1995; Jelinek and Lafferty, 1991). The Earley-based SLM performed worse than the Chelba-Jelinek SLM, mostly due to the fact that the rule production probabilities cannot be conditioned on the underlying lexical information, thus producing a lot of wrong parses. The weaknesses of our Earley SLM have led us to consider probabilistic left-corner grammar (PLCG) parsing (Manning and Carpenter, 1997), which follows a mixed bottom-up and top-down approach. Its potential to enhance parsing efficiency has been recognized by Roark and Johnson (2000), who simulated a left-corner parser with a top-down best-first parser applying a left-c</context>
</contexts>
<marker>Jelinek, Lafferty, 1991</marker>
<rawString>Frederik Jelinek and John Lafferty. 1991. Computation of the probability of initial substring generation by stochastic context-free grammars. Computational Linguistics, 17(3):315–323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
</authors>
<title>Statistical Methods for Speech Recognition.</title>
<date>1997</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="21852" citStr="Jelinek, 1997" startWordPosition="3858" endWordPosition="3859">d a simple implementation and comparison of results with related publications. We distinguished 1891 different projections, 143 different non-terminal categories and 41 different parts-ofspeech. (e) All constituents were annotated with a lexical head using deterministic rules by Magerman (1994). The training then proceded by decomposing all parse trees into sequences of SHIFT, PROJECT and ATTACH transitions. The submodels were finally estimated from smoothed relative counts of transitions using standard language modeling techniques: Good-Turing back-off (Katz, 1987) and deleted interpolation (Jelinek, 1997). Shift submodel The SHIFT submodel implements (4). Finding a good parameterization entails fixing the features that should explicitly appear in the context and in which order, so that all information-bearing elements are incorporated, with limited data fragmentation. This is not a straightforward task. We went through an iterative process of intuitively guessing which feature should be added or removed from the context or changing the order, building a corresponding model and evaluating its conditional perplexity (CPPL) against the devset. The CPPL of a SHIFT submodel is its perplexity measu</context>
</contexts>
<marker>Jelinek, 1997</marker>
<rawString>Frederick Jelinek. 1997. Statistical Methods for Speech Recognition. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slava M Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recognizer.</title>
<date>1987</date>
<journal>IEEE Trans. on Acoustics, Speech and Signal Processing,</journal>
<pages>35--400</pages>
<contexts>
<context position="21810" citStr="Katz, 1987" startWordPosition="3852" endWordPosition="3853">with a combined label. This step allowed a simple implementation and comparison of results with related publications. We distinguished 1891 different projections, 143 different non-terminal categories and 41 different parts-ofspeech. (e) All constituents were annotated with a lexical head using deterministic rules by Magerman (1994). The training then proceded by decomposing all parse trees into sequences of SHIFT, PROJECT and ATTACH transitions. The submodels were finally estimated from smoothed relative counts of transitions using standard language modeling techniques: Good-Turing back-off (Katz, 1987) and deleted interpolation (Jelinek, 1997). Shift submodel The SHIFT submodel implements (4). Finding a good parameterization entails fixing the features that should explicitly appear in the context and in which order, so that all information-bearing elements are incorporated, with limited data fragmentation. This is not a straightforward task. We went through an iterative process of intuitively guessing which feature should be added or removed from the context or changing the order, building a corresponding model and evaluating its conditional perplexity (CPPL) against the devset. The CPPL o</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Slava M. Katz. 1987. Estimation of probabilities from sparse data for the language model component of a speech recognizer. IEEE Trans. on Acoustics, Speech and Signal Processing, 35:400–401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Magerman</author>
</authors>
<title>Natural Language Parsing as Statistical Pattern Recognition.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="14556" citStr="Magerman, 1994" startWordPosition="2529" endWordPosition="2530">d linguistic studies and developments in functional grammar indicate that the lexical realization of a context, besides its syntactic analysis, strongly influences patterns of syntactic preference. Today’s best automatic parsers are made substantially more efficient and accurate by applying lexicalized grammar (Manning and Sch¨utze, 1999). 3.2.1 Context-sensitive and lexicalized states In our work we did not attempt to find semantic generalizations (such as casting a verb form to its infinitive form or finding semantic attributes); our simple (but probably suboptimal) approach, borrowed from (Magerman, 1994; Collins, 1996; Chelba, 2000), is to percolate words upward in the parse tree in the form in which they appear in the sentence. In our experiments, we opted to hardcode the head positions as part of the projection rules.3 The nodes of the resulting partial parse trees thus are annotated with a category label (the CAT feature) and a lexical label (the WORD feature). The notation (1) of a state is now replaced with q = (G, L1, L2; Z/z --*i X/x * jp; g, v) (13) where z is the WORD of the mother (possibly empty), x is the WORD of the first daughter (not empty), and the extended context contains •</context>
<context position="21533" citStr="Magerman (1994)" startWordPosition="3814" endWordPosition="3815"> was removed. (c) All characters were converted to lowercase. (d) All parse trees were binarized in much the same way as detailed in (Chelba, 2000, pp. 12–17); non-terminal unary productions were eliminated by collapsing two nodes connected by a unary branch to one node annotated with a combined label. This step allowed a simple implementation and comparison of results with related publications. We distinguished 1891 different projections, 143 different non-terminal categories and 41 different parts-ofspeech. (e) All constituents were annotated with a lexical head using deterministic rules by Magerman (1994). The training then proceded by decomposing all parse trees into sequences of SHIFT, PROJECT and ATTACH transitions. The submodels were finally estimated from smoothed relative counts of transitions using standard language modeling techniques: Good-Turing back-off (Katz, 1987) and deleted interpolation (Jelinek, 1997). Shift submodel The SHIFT submodel implements (4). Finding a good parameterization entails fixing the features that should explicitly appear in the context and in which order, so that all information-bearing elements are incorporated, with limited data fragmentation. This is not</context>
</contexts>
<marker>Magerman, 1994</marker>
<rawString>David M. Magerman. 1994. Natural Language Parsing as Statistical Pattern Recognition. Ph.D. thesis, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Bob Carpenter</author>
</authors>
<title>Probabilistic parsing using left corner language models.</title>
<date>1997</date>
<booktitle>In Proc. of the Fifth International Workshop on Parsing Technologies,</booktitle>
<pages>147--158</pages>
<contexts>
<context position="4435" citStr="Manning and Carpenter, 1997" startWordPosition="677" endWordPosition="680">o check whether a constituent fits into a higher structure. Van Uytsel (2000) developed a top-down alternative along similar lines but based on a lexicalized and context-sensitive DP version of an efficient Earley parser (Stolcke, 1995; Jelinek and Lafferty, 1991). The Earley-based SLM performed worse than the Chelba-Jelinek SLM, mostly due to the fact that the rule production probabilities cannot be conditioned on the underlying lexical information, thus producing a lot of wrong parses. The weaknesses of our Earley SLM have led us to consider probabilistic left-corner grammar (PLCG) parsing (Manning and Carpenter, 1997), which follows a mixed bottom-up and top-down approach. Its potential to enhance parsing efficiency has been recognized by Roark and Johnson (2000), who simulated a left-corner parser with a top-down best-first parser applying a left-corner-transformed PCFG grammar. For the language model described in this paper, however, we implemented a DP version of a native left-corner parser using a left-corner treebank grammar (containing projection rules instead of production rules). The efficiency of our implementation further allowed to enrich the history annotation of the parser states and to apply </context>
<context position="13902" citStr="Manning and Carpenter, 1997" startWordPosition="2431" endWordPosition="2434">− 1 17 foreach goal state q in cell (i, j) 18 q  SHIFT(q, wj) 19 add q to cell (j, j + 1) 3.2 Lexicalization and context-sensitivity Probably the most important shortcoming of PCFG’s is the assumption of context-free rule probabilities, i.e. the probability distribution over possible righthand sides given a lefthand side is independent from the function or position of the lefthand side. This assumption is quite wrong. For instance, in the Penn Treebank an NP in subject position produces a personal pronoun in 13.7% of the cases, while in object position it only does so in 2.1% of the cases (Manning and Carpenter, 1997). Furthermore, findings from corpus-based linguistic studies and developments in functional grammar indicate that the lexical realization of a context, besides its syntactic analysis, strongly influences patterns of syntactic preference. Today’s best automatic parsers are made substantially more efficient and accurate by applying lexicalized grammar (Manning and Sch¨utze, 1999). 3.2.1 Context-sensitive and lexicalized states In our work we did not attempt to find semantic generalizations (such as casting a verb form to its infinitive form or finding semantic attributes); our simple (but probab</context>
</contexts>
<marker>Manning, Carpenter, 1997</marker>
<rawString>Christopher D. Manning and Bob Carpenter. 1997. Probabilistic parsing using left corner language models. In Proc. of the Fifth International Workshop on Parsing Technologies, pages 147–158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Christopher D. Manning and Hinrich Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>330</pages>
<contexts>
<context position="19979" citStr="Marcus et al., 1995" startWordPosition="3563" endWordPosition="3566"> q|S)] = E Pr(path|S)Freq(q o q|path). (17) all paths Since all full parses terminate in qF, the final state, v(qF) = µ(qF) = Pr(S). Therefore (17) is computable as E[Freq(q o q|S)] = � 1 (qF)v(q)�(q) if o = s, (18) (qF)v(q)p(q oq)�(q) else. 1 The expected frequencies required for the reestimation of the conditional distributions are then obtained by summing (18) over the state attributes from which the required distribution is independent. 4 Empirical evaluation 4.1 Modeling We have trained two sets of models. The first set was trained on sections 0–20 of the Penn Treebank (PTB) (Marcus et al., 1995) using sections 21–22 for development decisions and tested on sections 23–24. The second set was trained on the BLLIP WSJ Corpus (BWC), which is a machine-parsed (Charniak, 2000) version of (a selection of) the ACL/DCI corpus, very similar to the selection made for the WSJ0/1 CSR corpus. As the training set, we used the BWC minus the WSJ0/1 “dfiles” and “efiles” intended for CSR development and evaluation testing. The PTB devset was used for fixing submodel parameterizations and software debugging, while perplexities are measured on the PTB testset. The BWC trainset was used in rescoring N-bes</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1995</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1995. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313– 330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Mark Johnson</author>
</authors>
<title>Efficient probabilistic top-down and left-corner parsing.</title>
<date>2000</date>
<booktitle>In Proc. of the 37th Annual Meeting of the ACL,</booktitle>
<pages>421--428</pages>
<contexts>
<context position="4583" citStr="Roark and Johnson (2000)" startWordPosition="700" endWordPosition="703">lized and context-sensitive DP version of an efficient Earley parser (Stolcke, 1995; Jelinek and Lafferty, 1991). The Earley-based SLM performed worse than the Chelba-Jelinek SLM, mostly due to the fact that the rule production probabilities cannot be conditioned on the underlying lexical information, thus producing a lot of wrong parses. The weaknesses of our Earley SLM have led us to consider probabilistic left-corner grammar (PLCG) parsing (Manning and Carpenter, 1997), which follows a mixed bottom-up and top-down approach. Its potential to enhance parsing efficiency has been recognized by Roark and Johnson (2000), who simulated a left-corner parser with a top-down best-first parser applying a left-corner-transformed PCFG grammar. For the language model described in this paper, however, we implemented a DP version of a native left-corner parser using a left-corner treebank grammar (containing projection rules instead of production rules). The efficiency of our implementation further allowed to enrich the history annotation of the parser states and to apply a lexicalized grammar. The following section contains a brief review of Manning’s PLCG parser. Section 3 describes how it was adapted to our SLM fra</context>
</contexts>
<marker>Roark, Johnson, 2000</marker>
<rawString>Brian Roark and Mark Johnson. 2000. Efficient probabilistic top-down and left-corner parsing. In Proc. of the 37th Annual Meeting of the ACL, pages 421–428.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>An efficient probabilistic context-free parsing algorithm that computes prefix probabilities.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="4042" citStr="Stolcke, 1995" startWordPosition="620" endWordPosition="621">non-DP parser. It scored somewhat lower in perplexity before reestimation (presumably by avoiding search errors), but remained roughly at the same level after full inside-outside reestimation (Van Aelten and Hogenhout, 2000). An obvious weakness of the Chelba-Jelinek SLM is the bottom-up behavior of the parser: it creates isolated constituents and only afterwards is it able to check whether a constituent fits into a higher structure. Van Uytsel (2000) developed a top-down alternative along similar lines but based on a lexicalized and context-sensitive DP version of an efficient Earley parser (Stolcke, 1995; Jelinek and Lafferty, 1991). The Earley-based SLM performed worse than the Chelba-Jelinek SLM, mostly due to the fact that the rule production probabilities cannot be conditioned on the underlying lexical information, thus producing a lot of wrong parses. The weaknesses of our Earley SLM have led us to consider probabilistic left-corner grammar (PLCG) parsing (Manning and Carpenter, 1997), which follows a mixed bottom-up and top-down approach. Its potential to enhance parsing efficiency has been recognized by Roark and Johnson (2000), who simulated a left-corner parser with a top-down best-f</context>
</contexts>
<marker>Stolcke, 1995</marker>
<rawString>Andreas Stolcke. 1995. An efficient probabilistic context-free parsing algorithm that computes prefix probabilities. Computational Linguistics, 21(2):165–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Filip Van Aelten</author>
<author>Marc Hogenhout</author>
</authors>
<title>Inside-outside reestimation of Chelba-Jelinek models.</title>
<date>2000</date>
<booktitle>Internal Report L&amp;H–SR–00–027, Lernout &amp; Hauspie,</booktitle>
<location>Wemmel, Belgium.</location>
<marker>Van Aelten, Hogenhout, 2000</marker>
<rawString>Filip Van Aelten and Marc Hogenhout. 2000. Inside-outside reestimation of Chelba-Jelinek models. Internal Report L&amp;H–SR–00–027, Lernout &amp; Hauspie, Wemmel, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong Hoon Van Uytsel</author>
</authors>
<title>Earley-inspired parsing language model: Background and preliminaries.</title>
<date>2000</date>
<tech>Internal Report PSI-SPCH-00-1,</tech>
<location>K.U.Leuven, ESAT, Heverlee, Belgium.</location>
<marker>Van Uytsel, 2000</marker>
<rawString>Dong Hoon Van Uytsel. 2000. Earley-inspired parsing language model: Background and preliminaries. Internal Report PSI-SPCH-00-1, K.U.Leuven, ESAT, Heverlee, Belgium.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>