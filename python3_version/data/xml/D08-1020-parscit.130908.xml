<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000820">
<title confidence="0.959474">
Revisiting Readability: A Unified Framework for Predicting Text Quality
</title>
<author confidence="0.993447">
Emily Pitler
</author>
<affiliation confidence="0.99843">
Computer and Information Science
University of Pennsylvania
</affiliation>
<address confidence="0.523054">
Philadelphia, PA 19104, USA
</address>
<email confidence="0.999124">
epitler@seas.upenn.edu
</email>
<author confidence="0.981224">
Ani Nenkova
</author>
<affiliation confidence="0.997772">
Computer and Information Science
University of Pennsylvania
</affiliation>
<address confidence="0.52308">
Philadelphia, PA 19104, USA
</address>
<email confidence="0.999092">
nenkova@seas.upenn.edu
</email>
<sectionHeader confidence="0.998602" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999913111111111">
We combine lexical, syntactic, and discourse
features to produce a highly predictive model
of human readers’ judgments of text readabil-
ity. This is the first study to take into ac-
count such a variety of linguistic factors and
the first to empirically demonstrate that dis-
course relations are strongly associated with
the perceived quality of text. We show that
various surface metrics generally expected to
be related to readability are not very good pre-
dictors of readability judgments in our Wall
Street Journal corpus. We also establish that
readability predictors behave differently de-
pending on the task: predicting text readabil-
ity or ranking the readability. Our experi-
ments indicate that discourse relations are the
one class of features that exhibits robustness
across these two tasks.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999836173913043">
The quest for a precise definition of text quality—
pinpointing the factors that make text flow and easy
to read—has a long history and tradition. Way back
in 1944 Robert Gunning Associates was set up, of-
fering newspapers, magazines and business firms
consultations on clear writing (Gunning, 1952).
In education, teaching good writing technique and
grading student writing has always been of key
importance (Spandel, 2004; Attali and Burstein,
2006). Linguists have also studied various aspects of
text flow, with cohesion-building devices in English
(Halliday and Hasan, 1976), rhetorical structure the-
ory (Mann and Thompson, 1988) and centering the-
ory (Grosz et al., 1995) among the most influential
contributions.
Still, we do not have unified computational mod-
els that capture the interplay between various as-
pects of readability. Most studies focus on a sin-
gle factor contributing to readability for a given in-
tended audience. The use of rare words or technical
terminology for example can make text difficult to
read for certain audience types (Collins-Thompson
and Callan, 2004; Schwarm and Ostendorf, 2005;
Elhadad and Sutaria, 2007). Syntactic complexity
is associated with delayed processing time in un-
derstanding (Gibson, 1998) and is another factor
that can decrease readability. Text organization (dis-
course structure), topic development (entity coher-
ence) and the form of referring expressions also de-
termine readability. But we know little about the rel-
ative importance of each factor and how they com-
bine in determining perceived text quality.
In our work we use texts from the Wall Street
Journal intended for an educated adult audience
to analyze readability factors including vocabulary,
syntax, cohesion, entity coherence and discourse.
We study the association between these features and
reader assigned readability ratings, showing that dis-
course and vocabulary are the factors most strongly
linked to text quality. In the easier task of text qual-
ity ranking, entity coherence and syntax features
also become significant and the combination of fea-
tures allows for ranking prediction accuracy of 88%.
Our study is novel in the use of gold-standard dis-
course features for predicting readability and the si-
multaneous analysis of various readability factors.
</bodyText>
<page confidence="0.979515">
186
</page>
<note confidence="0.974416">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 186–195,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.998843" genericHeader="introduction">
2 Related work
</sectionHeader>
<subsectionHeader confidence="0.9021755">
2.1 Readability with respect to intended
readers
</subsectionHeader>
<bodyText confidence="0.999972870967742">
The definition of what one might consider to be
a well-written and readable text heavily depends
on the intended audience (Schriver, 1989). Obvi-
ously, even a superbly written scientific paper will
not be perceived as very readable by a lay person
and a great novel might not be appreciated by a
third grader. As a result, the vast majority of prior
work on readability deals with labeling texts with
the appropriate school grade level. A key observa-
tion in even the oldest work in this area is that the
vocabulary used in a text largely determines its read-
ability. More common words are easier, so some
metrics measured text readability by the percent-
age of words that were not among the N most fre-
quent in the language. It was also observed that fre-
quently occurring words are often short, so word
length was used to approximate readability more
robustly than using a predefined word frequency
list. Standard indices were developed based on the
link between word frequency/length and readabil-
ity, such as Flesch-Kincaid (Kincaid, 1975), Auto-
mated Readability Index (Kincaid, 1975), Gunning
Fog (Gunning, 1952), SMOG (McLaughlin, 1969),
and Coleman-Liau (Coleman and Liau, 1975). They
use only a few simple factors that are designed to
be easy to calculate and are rough approximations
to the linguistic factors that determine readability.
For example, Flesch-Kincaid uses the average num-
ber of syllables per word to approximate vocabulary
difficulty and the average number of words per sen-
tence to approximate syntactic difficulty.
In recent work, the idea of linking word frequency
and text readability has been explored for making
medical information more accessible to the general
public. (Elhadad and Sutaria, 2007) classified words
in medical texts as familiar or unfamiliar to a gen-
eral audience based on their frequencies in corpora.
When a description of the unfamiliar terms was pro-
vided, the perceived readability of the texts almost
doubled.
A more general and principled approach to using
vocabulary information for readability decisions has
been the use of language models. For any given text,
it is easy to compute its likelihood under a given lan-
guage model, i.e. one for text meant for children,
or for text meant for adults, or for a given grade
level. (Si and Callan, 2001), (Collins-Thompson and
Callan, 2004), (Schwarm and Ostendorf, 2005), and
(Heilman et al., 2007) used language models to pre-
dict the suitability of texts for a given school grade
level. But even for this type of task other factors
besides vocabulary use are at play in determining
readability. Syntactic complexity is an obvious fac-
tor: indeed (Heilman et al., 2007) and (Schwarm and
Ostendorf, 2005) also used syntactic features, such
as parse tree height or the number of passive sen-
tences, to predict reading grade levels. For the task
of deciding whether a text is written for an adult or
child reader, (Barzilay and Lapata, 2008) found that
adding entity coherence to (Schwarm and Ostendorf,
2005)’s list of features improves classification accu-
racy by 10%.
</bodyText>
<subsectionHeader confidence="0.9948485">
2.2 Readability as coherence for competent
language users
</subsectionHeader>
<bodyText confidence="0.999980814814815">
In linguistics and natural language processing, the
text properties rather than those of the reader are em-
phasized. Text coherence is defined as the ease with
which a person (tacitly assumed to be a competent
language user) understands a text. Coherent text is
characterized by various types of cohesive links that
facilitate text comprehension (Halliday and Hasan,
1976).
In recent work, considerable attention has been
devoted to entity coherence in text quality, espe-
cially in relation to information ordering. In many
applications such as text generation and summariza-
tion, systems need to decide the order in which se-
lected sentences or generated clauses should be pre-
sented to the user. Most models attempting to cap-
ture local coherence between sentences were based
on or inspired by centering theory (Grosz et al.,
1995), which postulated strong links between the
center of attention in comprehension of adjacent
sentences and syntactic position and form of refer-
ence. In a detailed study of information ordering
in three very different corpora, (Karamanis et al., to
appear) assessed the performance of various formu-
lations of centering. Their results were somewhat
unexpected, showing that while centering transition
preferences were useful, the most successful strat-
egy for information ordering was based on avoid-
</bodyText>
<page confidence="0.99687">
187
</page>
<bodyText confidence="0.999570821428572">
ing rough shifts, that is, sequences of sentences that
share no entities in common. This supports previous
findings that such types of transitions are associated
with poorly written text and can be used to improve
the accuracy of automatic grading of essays based
on various non-discourse features (Miltsakaki and
Kukich, 2000). In a more powerful generalization
of centering, Barzilay and Lapata (2008) developed
a novel approach which doesn’t postulate a prefer-
ence for any type of transition but rather computes
a set of features that capture transitions of all kinds
in the text and their relative proportion. Their en-
tity coherence features prove to be very suitable for
various tasks, notably for information ordering and
reading difficulty level.
Form of reference is also important in well-
written text and appropriate choices lead to im-
proved readability. Use of pronouns for reference
to highly salient entities is perceived as more de-
sirable than the use of definite noun phrases (Gor-
don et al., 1993; Krahmer and Theune, 2002). The
syntactic forms of first mention—when an entity is
first introduced in a text—differ from those of subse-
quent mentions (Poesio and Vieira, 1998; Nenkova
and McKeown, 2003) and can be exploited for im-
proving and predicting text coherence (Siddharthan,
2003; Nenkova and McKeown, 2003; Elsner and
Charniak, 2008).
</bodyText>
<sectionHeader confidence="0.997708" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999886666666666">
The objective of our study is to analyze various
readability factors, including discourse relations, be-
cause few empirical studies exist that directly link
discourse structure with text quality. In the past,
subsections of the Penn Treebank (Marcus et al.,
1994) have been annotated for discourse relations
(Carlson et al., 2001; Wolf and Gibson, 2005). For
our study we chose to work with the newly released
Penn Discourse Treebank which is the largest anno-
tated resource which focuses exclusively on implicit
local relations between adjacent sentences and ex-
plicit discourse connectives.
</bodyText>
<subsectionHeader confidence="0.998815">
3.1 Discourse annotation
</subsectionHeader>
<bodyText confidence="0.999889692307692">
The Penn Discourse Treebank (Prasad et al., 2008)
is a new resource with annotations of discourse con-
nectives and their senses in the Wall Street Journal
portion of the Penn Treebank (Marcus et al., 1994).
All explicit relations (those marked with a discourse
connective) are annotated. In addition, each adjacent
pair of sentences within a paragraph is annotated. If
there is a discourse relation, then it is marked im-
plicit and annotated with one or more connectives. If
there is a relation between the sentences but adding a
connective would be inappropriate, it is marked Al-
tLex. If the consecutive sentences are only related
by entity-based coherence (Knott et al., 2001) they
are annotated with EntRel. Otherwise, they are an-
notated with NoRel.
Besides labeling the connective, the PDTB also
annotates the sense of each relation. The relations
are organized into a hierarchy. The top level rela-
tions are Expansion, Comparison, Contingency, and
Temporal. Briefly, an expansion relation means that
the second clause continues the theme of the first
clause, a comparison relation indicates that some-
thing in the two clauses is being compared, contin-
gency means that there is a causal relation between
the clauses, and temporal means they occur either at
the same time or sequentially.
</bodyText>
<subsectionHeader confidence="0.999613">
3.2 Readability ratings
</subsectionHeader>
<bodyText confidence="0.999878285714286">
We randomly selected thirty articles from the Wall
Street Journal corpus that was used in both the Penn
Treebank and the Penn Discourse Treebank.1 Each
article was read by at least three college students,
each of whom was given unlimited time to read the
texts and perform the ratings.2 Subjects were asked
the following questions:
</bodyText>
<listItem confidence="0.99994775">
• How well-written is this article?
• How well does the text fit together?
• How easy was it to understand?
• How interesting is this article?
</listItem>
<bodyText confidence="0.6044946">
For each question, they provided a rating between 1
and 5, with 5 being the best and 1 being the worst.
1One of the selected articles was missing from the Penn
Treebank. Thus, results that do not require syntactic informa-
tion (Tables 1, 2, 4, and 6) are over all thirty articles, while
Tables 3, 5, and 7 report results for the twenty-nine articles with
Treebank parse trees.
2(Lapata, 2006) found that human ratings are significantly
correlated with self-paced reading times, a more direct measure
of processing effort which we plan to explore in future work.
</bodyText>
<page confidence="0.994879">
188
</page>
<bodyText confidence="0.999979578947368">
After collecting the data, it turned out that most of
the time subjects gave the same rating to all ques-
tions. For competent language users, we view text
readability and text coherence as equivalent prop-
erties, measuring the extent to which a text is well
written. Thus for all subsequent analysis, we will
use only the first question (“On a scale of 1 to 5,
how well written is this text?”). The score of an arti-
cle was then the average of all the ratings it received.
The article scores ranged from 1.5 to 4.33, with a
mean of 3.2008 and a standard deviation of .7242.
The median score was 3.286.
We define our task as predicting this average rat-
ing for each article. Note that this task may be
more difficult than predicting reading level, as each
of these articles appeared in the Wall Street Journal
and thus is aimed at the same target audience. We
suspected that in classifying adult text, more subtle
features might be necessary.
</bodyText>
<sectionHeader confidence="0.972486" genericHeader="method">
4 Identifying correlates of text quality
</sectionHeader>
<subsectionHeader confidence="0.981981">
4.1 Baseline measures
</subsectionHeader>
<bodyText confidence="0.999834384615385">
We first computed the Pearson correlation coeffi-
cients between the simple metrics that most tradi-
tional readability formulas use and the average hu-
man ratings. These results are shown in Table 1. We
tested the average number of characters per word,
average number of words per sentence, maximum
number of words per sentence, and article length
(F7).3 Article length (F7) was the only significant
baseline factor, with correlation of -0.37. Longer ar-
ticles are perceived as less well-written and harder
to read than shorter ones. None of the other baseline
metrics were close to being significant predictors of
readability.
</bodyText>
<table confidence="0.848815">
Average Characters/Word r = -.0859, p = .6519
Average Words/Sentence r = .1637, p = .3874
Max Words/Sentence r = .0866, p = .6489
F7 text length r = -.3713, p = .0434
</table>
<tableCaption confidence="0.998308">
Table 1: Baseline readability features
</tableCaption>
<footnote confidence="0.753256">
3For ease of reference, we number each non-baseline feature
in the text and tables.
</footnote>
<subsectionHeader confidence="0.962897">
4.2 Vocabulary
</subsectionHeader>
<bodyText confidence="0.9998775">
We use a unigram language model, where the prob-
ability of an article is:
</bodyText>
<equation confidence="0.847354">
P(w|M)°M (1)
</equation>
<bodyText confidence="0.90379">
P(wIM) is the probability of word-type w accord-
ing to a background corpus M, and C(w) is the
number of times w appears in the article.
The log likelihood of an article is then:
</bodyText>
<equation confidence="0.992076">
C(w)log(P(wIM)) (2)
</equation>
<bodyText confidence="0.999553153846154">
Note that this model will be biased in favor of
shorter articles. Since each word has probability less
than 1, the log probability of each word is less than
0, and hence including additional words decreases
the log likelihood. We compensate for this by per-
forming linear regressions with the unigram log like-
lihood and with the number of words in the article as
an additional variable.
The question then arises as to what to use as a
background corpus. We chose to experiment with
two corpora: the entire Wall Street Journal corpus
and a collection of general AP news, which is gen-
erally more diverse than the financial news found in
the WSJ. We predicted that the NEWS vocabulary
would be more representative of the types of words
our readers would be familiar with. In both cases we
used Laplace smoothing over the word frequencies
and a stoplist.
The vocabulary features we used are article like-
lihood estimated from a language model from WSJ
(F5), and article likelihood according to a unigram
language model from NEWS (F6). We also combine
the two likelihood features with article length, in or-
der to get a better estimate of the language model’s
influence on readability independent of the length of
the article.
</bodyText>
<table confidence="0.998002">
F5 Log likelihood, WSJ r = .3723, p = .0428
Fs Log likelihood, NEWS r= .4497, p = .0127
LL with length, WSJ r = .3732, p = .0422
LL with length, NEWS r = .6359, p = .0002
</table>
<tableCaption confidence="0.998146">
Table 2: Vocabulary features
</tableCaption>
<bodyText confidence="0.960523">
Both vocabulary-based features (F5 and F6) are
significantly correlated with the readability judg-
ments, with p-values smaller than 0.05 (see Table 2).
</bodyText>
<page confidence="0.995787">
189
</page>
<bodyText confidence="0.999946444444444">
The correlations are positive: the more probable an
article was based on its vocabulary, the higher it was
generally rated. As expected, the NEWS model that
included more general news stories had a higher cor-
relation with people’s judgments. When combined
with the length of the article, the unigram language
model from the NEWS corpus becomes very predic-
tive of readability, with the correlation between the
two as high as 0.63.
</bodyText>
<subsectionHeader confidence="0.99732">
4.3 Syntactic features
</subsectionHeader>
<bodyText confidence="0.999825666666667">
Syntactic constructions affect processing difficulty
and so might also affect readability judgments.
We examined the four syntactic features used in
(Schwarm and Ostendorf, 2005): average parse tree
height (F1), average number of noun phrases per
sentence (F2), average number of verb phrases per
sentence (F3), and average number of subordinate
clauses per sentence(SBARs in the Penn Treebank
tagset) (F4). The sentence “We’re talking about
years ago [SBAR before anyone heard of asbestos
having any questionable properties].” contains an
example of an SBAR clause.
Having multiple noun phrases (entities) in each
sentence requires the reader to remember more
items, but may make the article more interesting.
(Barzilay and Lapata, 2008) found that articles writ-
ten for adults tended to contain many more entities
than articles written for children. While including
more verb phrases in each sentence increases the
sentence complexity, adults might prefer to have re-
lated clauses explicitly grouped together.
</bodyText>
<table confidence="0.99833625">
F1 Average Parse Tree Height r = -.0634, p = .7439
F2 Average Noun Phrases r = .2189, p = .2539
F3 Average Verb Phrases r = .4213, p = .0228
F4 Average SBARs r = .3405, p = .0707
</table>
<tableCaption confidence="0.999241">
Table 3: Syntax-related features
</tableCaption>
<bodyText confidence="0.9621303">
The correlations between readability and syntac-
tic features is shown in Table 3. The strongest corre-
lation is that between readability and number of verb
phrases (0.42). This finding is in line with prescrip-
tive clear writing advice (Gunning, 1952; Spandel,
2004), but is to our knowledge novel in the compu-
tational linguistics literature. As (Bailin and Graf-
stein, 2001) point out, the sentences in (1) are eas-
ier to comprehend than the sentences in (2), even
though they are longer.
</bodyText>
<listItem confidence="0.96711575">
(1) It was late at night, but it was clear. The stars
were out and the moon was bright.
(2) It was late at night. It was clear. The stars were
out. The moon was bright.
</listItem>
<bodyText confidence="0.994698230769231">
Multiple verb phrases in one sentence may be in-
dicative of explicit discourse relations, which we
will discuss further in section 4.6.
Surprisingly, the use of clauses introduced
by a (possibly empty) subordinating conjunction
(SBAR), are actually positively correlated (and al-
most approaching significance) with readability. So
while for children or less educated adults these con-
structions might pose difficulties, they were favored
by our assessors. On the other hand, the average
parse tree height negatively correlated with readabil-
ity as expected, but surprisingly the correlation is
very weak (-0.06).
</bodyText>
<subsectionHeader confidence="0.998853">
4.4 Elements of lexical cohesion
</subsectionHeader>
<bodyText confidence="0.999818833333334">
In their classic study of cohesion in English, (Hal-
liday and Hasan, 1976) discuss the various aspects
of well written discourse, including the use of cohe-
sive devices such as pronouns, definite descriptions
and topic continuity from sentence to sentence.4 To
measure the association between these features and
readability rankings, we compute the number ofpro-
nouns per sentence (F11) and the number of defi-
nite articles per sentence (F12). In order to qual-
ify topic continuity from sentence to sentence in
the articles, we compute average cosine similarity
(F8), word overlap (F9) and word overlap over just
nouns and pronouns (F10) between pairs of adjacent
sentences5. Each sentence is turned into a vector
of word-types, where each type’s value is its tf-idf
(where document frequency is computed over all the
articles in the WSJ corpus). The cosine similarity
metric is then:
</bodyText>
<equation confidence="0.995989">
s � t
cos (s, t) = (3)
|s ||t|
</equation>
<footnote confidence="0.99929">
4Other cohesion building devises discussed by Halliday
and Hansan include lexical reiteration and discourse relations,
which we address next.
5Similar features have been used for automatic essay grad-
ing as well (Higgins et al., 2004).
</footnote>
<page confidence="0.974751">
190
</page>
<note confidence="0.6118094">
F8 Avr. Cosine Overlap r = -.1012, p = .5947
F9 Avr. Word Overlap r = -.0531, p = .7806
F10 Avr. Noun+Pronoun Overlap r = .0905, p = .6345
F11 Avr. # Pronouns/Sent r = .2381, p = .2051
F12 Avr # Definite Articles r = .2309, p = .2196
</note>
<tableCaption confidence="0.987883">
Table 4: Superficial measures of topic continuity and pro-
noun and definite description use
</tableCaption>
<bodyText confidence="0.999966222222222">
None of these features correlate significantly with
readability as can be seen from the results in Ta-
ble 4. The overlap features are particularly bad
predictors of readability, with average word/cosine
overlap in fact being negatively correlated with read-
ability. The form of reference—use of pronouns
and definite descriptions—exhibit a higher correla-
tion with readability (0.23), but these values are not
significant for the size of our corpus.
</bodyText>
<figureCaption confidence="0.989935875">
F17 Prob. of S-S transition r = -.1287, p = .5059
F18 Prob. of S-O transition r = -.0427, p = .8261
F19 Prob. of S-X transition r = -.1450, p = .4529
F20 Prob. of S-N transition r = .3116, p = .0999
F21 Prob. of O-S transition r = .1131, p = .5591
F22 Prob. of O-O transition r = .0825, p = .6706
F23 Prob. of O-X transition r = .0744, p = .7014
F24 Prob. of O-N transition r = .2590, p = .1749
F25 Prob. of X-S transition r = .1732, p = .3688
F26 Prob. of X-O transition r = .0098, p = .9598
F27 Prob. of X-X transition r = -.0655, p = .7357
F28 Prob. of X-N transition r = .1319, p = .4953
F29 Prob. of N-S transition r = .1898, p = .3242
F30 Prob. of N-O transition r = .2577, p = .1772
F31 Prob. of N-X transition r = .1854, p = .3355
F32 Prob. of N-N transition r = -.2349, p = .2200
</figureCaption>
<tableCaption confidence="0.9884955">
Table 5: Linear correlation between human readability
ratings and entity coherence.
</tableCaption>
<subsectionHeader confidence="0.979124">
4.5 Entity coherence
</subsectionHeader>
<bodyText confidence="0.9999556">
We use the Brown Coherence Toolkit6 to compute
entity grids (Barzilay and Lapata, 2008) for each ar-
ticle. In each sentence, an entity is identified as the
subject (S), object (O), other (X) (for example, part
of a prepositional phrase), or not present (N). The
probability of each transition type is computed. For
example, an S-O transition occurs when an entity
is the subject in one sentence then an object in the
next; X-N transition occurs when an entity appears
in non-subject or object position in one sentence and
not present in the next, etc.7 The entity coherence
features are the probability of each of these pairs of
transitions, for a total of 16 features (F17_32; see
complete results in Table 5).
None of the entity grid features are significantly
correlated with the readability ratings. One very in-
teresting result is that the proportion of S-S transi-
tions in which the same entity was mentioned in sub-
ject position in two adjacent sentences, is negatively
correlated with readability. In centering theory, this
is considered the most coherent type of transition,
keeping the same center of attention. Moreover, the
feature most strongly correlated with readability is
the S-N transition (0.31) in which the subject of one
sentence does not appear at all in the following sen-
</bodyText>
<footnote confidence="0.980761">
6http://www.cs.brown.edu/ melsner/manual.html
7The Brown Coherence Toolkit identifies NPs as the same
entity if they have identical head nouns.
</footnote>
<bodyText confidence="0.926879">
tence. Of course, it is difficult to interpret the en-
tity grid features one by one, since they are inter-
dependent and probably it is the interaction of fea-
tures (relative proportions of transitions) that capture
overall readability patterns.
</bodyText>
<subsectionHeader confidence="0.993555">
4.6 Discourse relations
</subsectionHeader>
<bodyText confidence="0.998869833333333">
Discourse relations are believed to be a major factor
in text coherence. We computed another language
model which is over discourse relations instead of
words. We treat each text as a bag of relations rather
than a bag of words. Each relation is annotated
for both its sense and how it is realized (implicit
or explicit). For example, one text might contain
{Implicit Comparison, Explicit Temporal, NoRel}.
We computed the probability of each of our articles
according to a multinomial model, where the proba-
bility of a text with n relation tokens and k relation
types is:
</bodyText>
<equation confidence="0.990521">
x1! ... xk!p��
1 ... pk1, (4)
</equation>
<bodyText confidence="0.997439125">
P(n) is the probability of an article having length
n, xi is the number of times relation i appeared, and
pi is the probability of relation i based on the Penn
Discourse Treebank. P(n) is the maximum likeli-
hood estimation of an article having n discourse re-
lations based on the entire Penn Discourse Treebank
(the number of articles with exactly n discourse re-
lations, divided by the total number of articles).
</bodyText>
<equation confidence="0.975245">
P(n) n!
</equation>
<page confidence="0.853527">
191
</page>
<bodyText confidence="0.8390025">
The log likelihood of an article based on its dis-
course relations (F13) feature is defined as:
</bodyText>
<equation confidence="0.95682275">
k
log(P(n)) + log(n!) + (xi log(pi) − log(xi!))
i=1
(5)
</equation>
<bodyText confidence="0.999917933333333">
The multinomial distribution is particularly suit-
able, because it directly incorporates length, which
significantly affects readability as we discussed ear-
lier. It also captures patterns of relative frequency of
relations, unlike the simpler unigram model. Note
also that this equation has an advantage over the un-
igram model that was not present for vocabulary.
While every article contains at least one word, some
articles do not contain any discourse relations. Since
the PDTB annotated all explicit relations and re-
lations between adjacent sentences in a paragraph,
an article with no discourse connectives and only
single sentence paragraphs would not contain any
annotated discourse relations. Under the unigram
model, these articles’ probabilities cannot be com-
puted. Under the multinomial model, the probabil-
ity of an article with zero relations is estimated as
Pr(N = 0), which can be calculated from the cor-
pus.
As in the case of vocabulary features, the presence
of more relations will lead to overall lower probabil-
ities so we also consider the number of discourse
relations (F14) and the log likelihood combined with
the number of relations as features. In order to iso-
late the effect of the type of discourse relation (ex-
plicitly expressed by a discourse connective such as
“because” or “however” versus implicitly expressed
by adjacency), we also compute multinomial model
features for the explicit discourse relations (F15) and
over just the implicit discourse relations (F16).
</bodyText>
<listItem confidence="0.788606333333333">
F13 LogL of discourse rels r = .4835, p = .0068
F14 # of discourse relations r = -.2729, p = .1445
LogL of rels with # of rels r = .5409, p = .0020
# of relations with # of words r = .3819, p = .0373
F15 Explicit relations only r = .1528, p = .4203
F16 Implicit relations only r = .2403, p = .2009
</listItem>
<tableCaption confidence="0.939431">
Table 6: Discourse features
</tableCaption>
<bodyText confidence="0.999966357142857">
The likelihood of discourse relations in the text
under a multinomial model is very highly and sig-
nificantly correlated with readability ratings, espe-
cially after text length is taken into account. Cor-
relations are 0.48 and 0.54 respectively. The prob-
ability of the explicit relations alone is not a suffi-
ciently strong indicator of readability. This fact is
disappointing as the explicit relations can be iden-
tified much more easily in unannotated text (Pitler
et al., 2008). Note that the sequence of just the im-
plicit relations is also not sufficient. This observa-
tion implies that the proportion of explicit and im-
plicit relations may be meaningful but we leave the
exploration of this issue for later work.
</bodyText>
<subsectionHeader confidence="0.999491">
4.7 Summary of findings
</subsectionHeader>
<bodyText confidence="0.999261">
So far, we introduced six classes of factors that have
been discussed in the literature as readability cor-
relates. Through statistical tests of associations we
identified the individual factors significantly corre-
lated with readability ratings. These are, in decreas-
ing order of association strength:
</bodyText>
<table confidence="0.9320836">
LogL of Discourse Relations (r = .4835)
LogL, NEWS (r= .4497)
Average Verb Phrases (.4213)
LogL, WSJ (r = .3723)
Number of words (r = -.3713)
</table>
<bodyText confidence="0.999781352941177">
Vocabulary and discourse relations are the
strongest predictors of readability, followed by aver-
age number of verb phrases and length of the text.
This empirical confirmation of the significance of
discourse relations as a readability factor is novel for
the computational linguistics literature. Note though
that for our work we use oracle discourse annota-
tions directly from the PDTB and no robust systems
for automatic discourse annotation exist today.
The significance of the average number of verb
phrases as a readability predictor is somewhat sur-
prising but intriguing. It would lead to reexamina-
tion of the role of verbs/predicates in written text,
which we also plan to address in future work. None
of the other factors showed significant association
with readability ratings, even though some correla-
tions had relatively large positive values.
</bodyText>
<sectionHeader confidence="0.875859" genericHeader="method">
5 Combining readability factors
</sectionHeader>
<bodyText confidence="0.9999194">
In this section, we turn to the question of how the
combination of various factors improves the predic-
tion of readability. We use the leaps package in R
to find the best subset of features for linear regres-
sion, for subsets of size one to eight. We use the
</bodyText>
<page confidence="0.996129">
192
</page>
<bodyText confidence="0.931006166666667">
squared multiple correlation coefficient (R2) to as-
sess the effectiveness of predictions. R2 is the pro-
portion of variance in readability ratings explained
by the model. If the model predicts readability per-
fectly, R2 = 1, and if the model has no predictive
capability, R2 = 0.
</bodyText>
<equation confidence="0.99909925">
F13, R2 = 0.2662
F6 + F7, R2 = 0.4351
F6 + F7 + F13, R2 = 0.5029
F6 + F7 + F13 + F14, R2 = 0.6308
F1 + F6 + F7 + F10 + F13, R2 = 0.6939
F1 + F6 + F7 + F10 + F13 + F23, R2 = 0.7316
F1 + F6 + F7 + F10 + F13 + F22 + F23, R2 = 0.7557
F1+F6+F7+F10+F11+F13+F1g+F30, R2 = 0.776.
</equation>
<bodyText confidence="0.995539227272727">
The linear regression results confirm the expec-
tation that the combination of different factors is a
rather complex issue. As expected, discourse, vo-
cabulary and length which were the significant in-
dividual factors appear in the best model for each
feature set size. Their combination gives the best
result for regression with three predictors, and they
explain half of the variance in readability ratings,
R2 = 0.5029.
But the other individually significant feature, av-
erage number of verb phrases per sentence (F3)
never appears in the best models. Instead, F1—the
depth of the parse tree—appears in the best model
with more than four features.
Also unexpectedly, two of the superficial cohe-
sion features appear in the larger models: F10 is
the average word overlap over nouns and pronouns
and F11 is the average number of pronouns per sen-
tence. Entity grid features also make their way into
the best models when more features are used for pre-
diction: S-X, O-O, O-X, N-O transitions (F19, F22,
F23, F30).
</bodyText>
<sectionHeader confidence="0.957649" genericHeader="method">
6 Readability as ranking
</sectionHeader>
<bodyText confidence="0.998334428571429">
In this section we consider the problem of pairwise
ranking of text readability. That is, rather than try-
ing to predict the readability of a single document,
we consider pairs of documents and predict which
one is better. This task may in fact be the more natu-
ral one, since in most applications the main concern
is with the relative quality of articles rather than their
absolute scores. This setting is also beneficial in
terms of data use, because each pair of articles with
different average readability scores now becomes a
data point for the classification task.
We thus create a classification problem: given two
articles, is article 1 more readable than article 2?
For each pair of texts whose readability ratings on
the 1 to 5 scale differed by at least 0.5, we form
one data point for the ranking problem, resulting in
243 examples. The predictors are the differences be-
tween the two articles’ features. For classification,
we used WEKA’s linear support vector implemen-
tation (SMO) and performance was evaluated using
10-fold cross-validation.
</bodyText>
<table confidence="0.998958954545455">
Features Accuracy
None (Majority Class) 50.21%
ALL 88.88%
log l discourse rels 77.77%
number discourse rels 74.07%
N-O transition 70.78%
O-N transition 69.95%
Avg VPs sen 69.54%
log l NEWS 66.25%
number of words 65.84%
Grid only 79.42%
Discourse only 77.36%
Syntax only 74.07%
Vocab only 66.66%
Length only 65.84%
Cohesion only 64.60%
no cohesion 89.30%
no vocab 88.88%
no length 88.47%
no discourse 88.06%
no grid 84.36%
no syntax 82.71%
</table>
<tableCaption confidence="0.999356">
Table 7: SVM prediction accuracy, linear kernel
</tableCaption>
<bodyText confidence="0.999757166666667">
The classification results are shown in Table 7.
When all features are used for prediction, the ac-
curacy is high, 88.88%. The length of the article
can serve as a baseline feature—longer articles are
ranked lower by the assessors, so this feature can
be taken as baseline indicator of readability. Only
six features used by themselves lead to accuracies
higher than the length baseline. These results indi-
cate that the most important individual factors in the
readability ranking task, in decreasing order of im-
portance, are log likelihood of discourse relations,
number of discourse relations, N-O transitions, O-N
</bodyText>
<page confidence="0.997407">
193
</page>
<bodyText confidence="0.999983545454546">
transitions, average number of VPs per sentence and
text probability under a general language model.
In terms of classes of features, the 16 entity
grid features perform the best, leading to an accu-
racy of 79.41%, followed by the combination of
the four discourse features (77.36%), and syntax
features (74.07%). This is evidence for the fact
that there is a complex interplay between readabil-
ity factors: the entity grid factors which individ-
ually have very weak correlation with readability
combine well, while adding the three additional dis-
course features to the likelihood of discourses rela-
tions actually worsens performance slightly. Simi-
lar indication for interplay between features is pro-
vided by the class ablation classification results, in
which classes of features are removed. Surprisingly,
removing syntactic features causes the biggest dete-
rioration in performance, a drop in accuracy from
88.88% to 82.71%. The removal of vocabulary,
length, or discourse features has a minimal negative
impact on performance, while removing the cohe-
sion features actually boosts performance.
</bodyText>
<sectionHeader confidence="0.99901" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999725">
We have investigated which linguistic features cor-
relate best with readability judgments. While sur-
face measures such as the average number of words
per sentence or the average number of characters
per word are not good predictors, there exist syn-
tactic, semantic, and discourse features that do cor-
relate highly. The average number of verb phrases
in each sentence, the number of words in the article,
the likelihood of the vocabulary, and the likelihood
of the discourse relations all are highly correlated
with humans’ judgments of how well an article is
written.
While using any one out of syntactic, lexical, co-
herence, or discourse features is substantally better
than the baseline surface features on the discrim-
ination task, using a combination of entity coher-
ence and discourse relations produces the best per-
formance.
</bodyText>
<sectionHeader confidence="0.99913" genericHeader="acknowledgments">
8 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999898">
This work was partially supported by an Inte-
grative Graduate Education and Research Trainee-
ship grant from National Science Foundation (NS-
FIGERT 0504487) and by NSF Grant IIS-07-05671.
We thank Aravind Joshi, Bonnie Webber, and the
anonymous reviewers for their many helpful com-
ments.
</bodyText>
<sectionHeader confidence="0.995685" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998949361702128">
Y. Attali and J. Burstein. 2006. Automated essay scoring
with e-rater v.2. The Journal of Technology, Learning
and Assessment, 4(3).
A. Bailin and A. Grafstein. 2001. The linguistic assump-
tions underlying readability formulae a critique. Lan-
guage and Communication, 21(3):285–301.
R. Barzilay and M. Lapata. 2008. Modeling local coher-
ence: An entity-based approach. Computational Lin-
guistics, 34(1):1–34.
L. Carlson, D. Marcu, and M. E. Okurowski. 2001.
Building a discourse-tagged corpus in the framework
of rhetorical structure theory. In Proceedings of the
Second SIGdial Workshop, pages 1–10.
M. Coleman and TL Liau. 1975. A computer readabil-
ity formula designed for machine scoring. Journal of
Applied Psychology, 60(2):283–284.
K. Collins-Thompson and J. Callan. 2004. A language
modeling approach to predicting reading difficulty. In
Proceedings of HLT/NAACL’04.
Noemie Elhadad and Komal Sutaria. 2007. Mining a lex-
icon of technical terms and lay equivalents. In Biolog-
ical, translational, and clinical language processing,
pages 49–56, Prague, Czech Republic. Association for
Computational Linguistics.
M. Elsner and E. Charniak. 2008. Coreference-inspired
coherence modeling. In Proceedings of ACL-HLT’08,
(short paper).
E. Gibson. 1998. Linguistic complexity: locality of syn-
tactic dependencies. Cognition, 68:1–76.
P. Gordon, B. Grosz, and L. Gilliom. 1993. Pronouns,
names, and the centering of attention in discourse.
Cognitive Science, 17:311–347.
B. Grosz, A. Joshi, and S. Weinstein. 1995. Centering:
a framework for modelling the local coherence of dis-
course. Computational Linguistics, 21(2):203–226.
Robert Gunning. 1952. The technique of clear writing.
McGraw-Hill; Fouth Printing edition.
Michael A.K. Halliday and Ruqaiya Hasan. 1976. Cohe-
sion in English. Longman Group Ltd, London, U.K.
M. Heilman, K. Collins-Thompson, J. Callan, and M. Es-
kenazi. 2007. Combining Lexical and Grammatical
Features to Improve Readability Measures for First
and Second Language Texts. Proceedings of NAACL
HLT, pages 460–467.
D. Higgins, J. Burstein, D. Marcu, and C. Gentile. 2004.
Evaluating multiple aspects of coherence in student es-
says. In Proceedings of HLT/NAACL’04.
</reference>
<page confidence="0.986734">
194
</page>
<reference confidence="0.999867696969697">
N. Karamanis, M. Poesio, C. Mellish, and J. Oberlander.
(to appear). Evaluating centering for information or-
dering using corpora. Computational Linguistics.
JP Kincaid. 1975. Derivation of New Readability For-
mulas (Automated Readability Index, Fog Count and
Flesch Reading Ease Formula) for Navy Enlisted Per-
sonnel.
A. Knott, J. Oberlander, M. ODonnell, and C. Mellish.
2001. Beyond elaboration: The interaction of relations
and focus in coherent text. Text representation: lin-
guistic and psycholinguistic aspects, pages 181–196.
E. Krahmer and M. Theune. 2002. Efficient context-
sensitive generation of referring expressions. In K. van
Deemter and R. Kibble, editors, Information Sharing:
Reference and Presupposition in Language Genera-
tion and Interpretation, pages 223–264. CSLI Publi-
cations.
M. Lapata. 2006. Automatic evaluation of information
ordering: Kendalls tau. Computational Linguistics,
32(4):471–484.
W. Mann and S. Thompson. 1988. Rhetorical structure
theory: Towards a functional theory of text organiza-
tion. Text, 8.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1994. Building a Large Annotated Corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313–330.
G.H. McLaughlin. 1969. SMOG grading: A new read-
ability formula. Journal of Reading, 12(8):639–646.
E. Miltsakaki and K. Kukich. 2000. The role of centering
theory’s rough-shift in the teaching and evaluation of
writing skills. In Proceedings of ACL’00, pages 408–
415.
A. Nenkova and K. McKeown. 2003. References to
named entities: a corpus study. In Proceedings of
HLT/NAACL 2003 (short paper).
E. Pitler, M. Raghupathy, H. Mehta, A. Nenkova, A. Lee,
and A. Joshi. 2008. Easily identifiable discourse re-
lations. In Coling 2008: Companion volume: Posters
and Demonstrations, pages 85–88, Manchester, UK,
August.
M. Poesio and R. Vieira. 1998. A corpus-based investi-
gation of definite description use. Computational Lin-
guistics, 24(2):183–216.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo,
A. Joshi, and B. Webber. 2008. The penn discourse
treebank 2.0. In Proceedings of LREC’08.
KA Schriver. 1989. Evaluating text quality: the con-
tinuum from text-focused toreader-focused methods.
Professional Communication, IEEE Transactions on,
32(4):238–255.
S. Schwarm and M. Ostendorf. 2005. Reading level as-
sessment using support vector machines and statistical
language models. In Proceedings of ACL’05, pages
523–530.
L. Si and J. Callan. 2001. A statistical model for sci-
entific readability. Proceedings of the tenth interna-
tional conference on Information and knowledge man-
agement, pages 574–576.
A. Siddharthan. 2003. Syntactic simplification and Text
Cohesion. Ph.D. thesis, University of Cambridge, UK.
V. Spandel. 2004. Creating writers through 6-trait writ-
ing assessment and instruction. Allyn &amp; Bacon.
F. Wolf and E. Gibson. 2005. Representing discourse
coherence: A corpus-based study. Computational Lin-
guistics, 31(2):249–288.
</reference>
<page confidence="0.99894">
195
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.685908">
<title confidence="0.999432">Revisiting Readability: A Unified Framework for Predicting Text Quality</title>
<author confidence="0.996392">Emily</author>
<affiliation confidence="0.9936475">Computer and Information University of</affiliation>
<address confidence="0.940802">Philadelphia, PA 19104,</address>
<email confidence="0.999636">epitler@seas.upenn.edu</email>
<author confidence="0.803617">Ani</author>
<affiliation confidence="0.9919275">Computer and Information University of</affiliation>
<address confidence="0.929429">Philadelphia, PA 19104,</address>
<email confidence="0.999306">nenkova@seas.upenn.edu</email>
<abstract confidence="0.999866315789473">We combine lexical, syntactic, and discourse features to produce a highly predictive model of human readers’ judgments of text readability. This is the first study to take into account such a variety of linguistic factors and the first to empirically demonstrate that discourse relations are strongly associated with the perceived quality of text. We show that various surface metrics generally expected to be related to readability are not very good predictors of readability judgments in our Wall Street Journal corpus. We also establish that readability predictors behave differently depending on the task: predicting text readability or ranking the readability. Our experiments indicate that discourse relations are the one class of features that exhibits robustness across these two tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Attali</author>
<author>J Burstein</author>
</authors>
<title>Automated essay scoring with e-rater v.2.</title>
<date>2006</date>
<journal>The Journal of Technology, Learning and Assessment,</journal>
<volume>4</volume>
<issue>3</issue>
<contexts>
<context position="1588" citStr="Attali and Burstein, 2006" startWordPosition="231" endWordPosition="234">ity or ranking the readability. Our experiments indicate that discourse relations are the one class of features that exhibits robustness across these two tasks. 1 Introduction The quest for a precise definition of text quality— pinpointing the factors that make text flow and easy to read—has a long history and tradition. Way back in 1944 Robert Gunning Associates was set up, offering newspapers, magazines and business firms consultations on clear writing (Gunning, 1952). In education, teaching good writing technique and grading student writing has always been of key importance (Spandel, 2004; Attali and Burstein, 2006). Linguists have also studied various aspects of text flow, with cohesion-building devices in English (Halliday and Hasan, 1976), rhetorical structure theory (Mann and Thompson, 1988) and centering theory (Grosz et al., 1995) among the most influential contributions. Still, we do not have unified computational models that capture the interplay between various aspects of readability. Most studies focus on a single factor contributing to readability for a given intended audience. The use of rare words or technical terminology for example can make text difficult to read for certain audience types</context>
</contexts>
<marker>Attali, Burstein, 2006</marker>
<rawString>Y. Attali and J. Burstein. 2006. Automated essay scoring with e-rater v.2. The Journal of Technology, Learning and Assessment, 4(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bailin</author>
<author>A Grafstein</author>
</authors>
<title>The linguistic assumptions underlying readability formulae a critique.</title>
<date>2001</date>
<journal>Language and Communication,</journal>
<volume>21</volume>
<issue>3</issue>
<contexts>
<context position="18260" citStr="Bailin and Grafstein, 2001" startWordPosition="2945" endWordPosition="2949">elated clauses explicitly grouped together. F1 Average Parse Tree Height r = -.0634, p = .7439 F2 Average Noun Phrases r = .2189, p = .2539 F3 Average Verb Phrases r = .4213, p = .0228 F4 Average SBARs r = .3405, p = .0707 Table 3: Syntax-related features The correlations between readability and syntactic features is shown in Table 3. The strongest correlation is that between readability and number of verb phrases (0.42). This finding is in line with prescriptive clear writing advice (Gunning, 1952; Spandel, 2004), but is to our knowledge novel in the computational linguistics literature. As (Bailin and Grafstein, 2001) point out, the sentences in (1) are easier to comprehend than the sentences in (2), even though they are longer. (1) It was late at night, but it was clear. The stars were out and the moon was bright. (2) It was late at night. It was clear. The stars were out. The moon was bright. Multiple verb phrases in one sentence may be indicative of explicit discourse relations, which we will discuss further in section 4.6. Surprisingly, the use of clauses introduced by a (possibly empty) subordinating conjunction (SBAR), are actually positively correlated (and almost approaching significance) with read</context>
</contexts>
<marker>Bailin, Grafstein, 2001</marker>
<rawString>A. Bailin and A. Grafstein. 2001. The linguistic assumptions underlying readability formulae a critique. Language and Communication, 21(3):285–301.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>M Lapata</author>
</authors>
<title>Modeling local coherence: An entity-based approach.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="6585" citStr="Barzilay and Lapata, 2008" startWordPosition="1025" endWordPosition="1028">ns-Thompson and Callan, 2004), (Schwarm and Ostendorf, 2005), and (Heilman et al., 2007) used language models to predict the suitability of texts for a given school grade level. But even for this type of task other factors besides vocabulary use are at play in determining readability. Syntactic complexity is an obvious factor: indeed (Heilman et al., 2007) and (Schwarm and Ostendorf, 2005) also used syntactic features, such as parse tree height or the number of passive sentences, to predict reading grade levels. For the task of deciding whether a text is written for an adult or child reader, (Barzilay and Lapata, 2008) found that adding entity coherence to (Schwarm and Ostendorf, 2005)’s list of features improves classification accuracy by 10%. 2.2 Readability as coherence for competent language users In linguistics and natural language processing, the text properties rather than those of the reader are emphasized. Text coherence is defined as the ease with which a person (tacitly assumed to be a competent language user) understands a text. Coherent text is characterized by various types of cohesive links that facilitate text comprehension (Halliday and Hasan, 1976). In recent work, considerable attention h</context>
<context position="8502" citStr="Barzilay and Lapata (2008)" startWordPosition="1320" endWordPosition="1323">nce of various formulations of centering. Their results were somewhat unexpected, showing that while centering transition preferences were useful, the most successful strategy for information ordering was based on avoid187 ing rough shifts, that is, sequences of sentences that share no entities in common. This supports previous findings that such types of transitions are associated with poorly written text and can be used to improve the accuracy of automatic grading of essays based on various non-discourse features (Miltsakaki and Kukich, 2000). In a more powerful generalization of centering, Barzilay and Lapata (2008) developed a novel approach which doesn’t postulate a preference for any type of transition but rather computes a set of features that capture transitions of all kinds in the text and their relative proportion. Their entity coherence features prove to be very suitable for various tasks, notably for information ordering and reading difficulty level. Form of reference is also important in wellwritten text and appropriate choices lead to improved readability. Use of pronouns for reference to highly salient entities is perceived as more desirable than the use of definite noun phrases (Gordon et al</context>
<context position="17405" citStr="Barzilay and Lapata, 2008" startWordPosition="2801" endWordPosition="2804"> the four syntactic features used in (Schwarm and Ostendorf, 2005): average parse tree height (F1), average number of noun phrases per sentence (F2), average number of verb phrases per sentence (F3), and average number of subordinate clauses per sentence(SBARs in the Penn Treebank tagset) (F4). The sentence “We’re talking about years ago [SBAR before anyone heard of asbestos having any questionable properties].” contains an example of an SBAR clause. Having multiple noun phrases (entities) in each sentence requires the reader to remember more items, but may make the article more interesting. (Barzilay and Lapata, 2008) found that articles written for adults tended to contain many more entities than articles written for children. While including more verb phrases in each sentence increases the sentence complexity, adults might prefer to have related clauses explicitly grouped together. F1 Average Parse Tree Height r = -.0634, p = .7439 F2 Average Noun Phrases r = .2189, p = .2539 F3 Average Verb Phrases r = .4213, p = .0228 F4 Average SBARs r = .3405, p = .0707 Table 3: Syntax-related features The correlations between readability and syntactic features is shown in Table 3. The strongest correlation is that b</context>
<context position="22088" citStr="Barzilay and Lapata, 2008" startWordPosition="3622" endWordPosition="3625"> of O-N transition r = .2590, p = .1749 F25 Prob. of X-S transition r = .1732, p = .3688 F26 Prob. of X-O transition r = .0098, p = .9598 F27 Prob. of X-X transition r = -.0655, p = .7357 F28 Prob. of X-N transition r = .1319, p = .4953 F29 Prob. of N-S transition r = .1898, p = .3242 F30 Prob. of N-O transition r = .2577, p = .1772 F31 Prob. of N-X transition r = .1854, p = .3355 F32 Prob. of N-N transition r = -.2349, p = .2200 Table 5: Linear correlation between human readability ratings and entity coherence. 4.5 Entity coherence We use the Brown Coherence Toolkit6 to compute entity grids (Barzilay and Lapata, 2008) for each article. In each sentence, an entity is identified as the subject (S), object (O), other (X) (for example, part of a prepositional phrase), or not present (N). The probability of each transition type is computed. For example, an S-O transition occurs when an entity is the subject in one sentence then an object in the next; X-N transition occurs when an entity appears in non-subject or object position in one sentence and not present in the next, etc.7 The entity coherence features are the probability of each of these pairs of transitions, for a total of 16 features (F17_32; see comple</context>
</contexts>
<marker>Barzilay, Lapata, 2008</marker>
<rawString>R. Barzilay and M. Lapata. 2008. Modeling local coherence: An entity-based approach. Computational Linguistics, 34(1):1–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Carlson</author>
<author>D Marcu</author>
<author>M E Okurowski</author>
</authors>
<title>Building a discourse-tagged corpus in the framework of rhetorical structure theory.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second SIGdial Workshop,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="9789" citStr="Carlson et al., 2001" startWordPosition="1526" endWordPosition="1529">ntion—when an entity is first introduced in a text—differ from those of subsequent mentions (Poesio and Vieira, 1998; Nenkova and McKeown, 2003) and can be exploited for improving and predicting text coherence (Siddharthan, 2003; Nenkova and McKeown, 2003; Elsner and Charniak, 2008). 3 Data The objective of our study is to analyze various readability factors, including discourse relations, because few empirical studies exist that directly link discourse structure with text quality. In the past, subsections of the Penn Treebank (Marcus et al., 1994) have been annotated for discourse relations (Carlson et al., 2001; Wolf and Gibson, 2005). For our study we chose to work with the newly released Penn Discourse Treebank which is the largest annotated resource which focuses exclusively on implicit local relations between adjacent sentences and explicit discourse connectives. 3.1 Discourse annotation The Penn Discourse Treebank (Prasad et al., 2008) is a new resource with annotations of discourse connectives and their senses in the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1994). All explicit relations (those marked with a discourse connective) are annotated. In addition, each adjacent</context>
</contexts>
<marker>Carlson, Marcu, Okurowski, 2001</marker>
<rawString>L. Carlson, D. Marcu, and M. E. Okurowski. 2001. Building a discourse-tagged corpus in the framework of rhetorical structure theory. In Proceedings of the Second SIGdial Workshop, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Coleman</author>
<author>TL Liau</author>
</authors>
<title>A computer readability formula designed for machine scoring.</title>
<date>1975</date>
<journal>Journal of Applied Psychology,</journal>
<volume>60</volume>
<issue>2</issue>
<contexts>
<context position="4837" citStr="Coleman and Liau, 1975" startWordPosition="738" endWordPosition="741">dability. More common words are easier, so some metrics measured text readability by the percentage of words that were not among the N most frequent in the language. It was also observed that frequently occurring words are often short, so word length was used to approximate readability more robustly than using a predefined word frequency list. Standard indices were developed based on the link between word frequency/length and readability, such as Flesch-Kincaid (Kincaid, 1975), Automated Readability Index (Kincaid, 1975), Gunning Fog (Gunning, 1952), SMOG (McLaughlin, 1969), and Coleman-Liau (Coleman and Liau, 1975). They use only a few simple factors that are designed to be easy to calculate and are rough approximations to the linguistic factors that determine readability. For example, Flesch-Kincaid uses the average number of syllables per word to approximate vocabulary difficulty and the average number of words per sentence to approximate syntactic difficulty. In recent work, the idea of linking word frequency and text readability has been explored for making medical information more accessible to the general public. (Elhadad and Sutaria, 2007) classified words in medical texts as familiar or unfamili</context>
</contexts>
<marker>Coleman, Liau, 1975</marker>
<rawString>M. Coleman and TL Liau. 1975. A computer readability formula designed for machine scoring. Journal of Applied Psychology, 60(2):283–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Collins-Thompson</author>
<author>J Callan</author>
</authors>
<title>A language modeling approach to predicting reading difficulty.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT/NAACL’04.</booktitle>
<contexts>
<context position="2223" citStr="Collins-Thompson and Callan, 2004" startWordPosition="330" endWordPosition="333">Linguists have also studied various aspects of text flow, with cohesion-building devices in English (Halliday and Hasan, 1976), rhetorical structure theory (Mann and Thompson, 1988) and centering theory (Grosz et al., 1995) among the most influential contributions. Still, we do not have unified computational models that capture the interplay between various aspects of readability. Most studies focus on a single factor contributing to readability for a given intended audience. The use of rare words or technical terminology for example can make text difficult to read for certain audience types (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005; Elhadad and Sutaria, 2007). Syntactic complexity is associated with delayed processing time in understanding (Gibson, 1998) and is another factor that can decrease readability. Text organization (discourse structure), topic development (entity coherence) and the form of referring expressions also determine readability. But we know little about the relative importance of each factor and how they combine in determining perceived text quality. In our work we use texts from the Wall Street Journal intended for an educated adult audience to analyze readability factors</context>
<context position="5988" citStr="Collins-Thompson and Callan, 2004" startWordPosition="924" endWordPosition="927">lhadad and Sutaria, 2007) classified words in medical texts as familiar or unfamiliar to a general audience based on their frequencies in corpora. When a description of the unfamiliar terms was provided, the perceived readability of the texts almost doubled. A more general and principled approach to using vocabulary information for readability decisions has been the use of language models. For any given text, it is easy to compute its likelihood under a given language model, i.e. one for text meant for children, or for text meant for adults, or for a given grade level. (Si and Callan, 2001), (Collins-Thompson and Callan, 2004), (Schwarm and Ostendorf, 2005), and (Heilman et al., 2007) used language models to predict the suitability of texts for a given school grade level. But even for this type of task other factors besides vocabulary use are at play in determining readability. Syntactic complexity is an obvious factor: indeed (Heilman et al., 2007) and (Schwarm and Ostendorf, 2005) also used syntactic features, such as parse tree height or the number of passive sentences, to predict reading grade levels. For the task of deciding whether a text is written for an adult or child reader, (Barzilay and Lapata, 2008) fo</context>
</contexts>
<marker>Collins-Thompson, Callan, 2004</marker>
<rawString>K. Collins-Thompson and J. Callan. 2004. A language modeling approach to predicting reading difficulty. In Proceedings of HLT/NAACL’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noemie Elhadad</author>
<author>Komal Sutaria</author>
</authors>
<title>Mining a lexicon of technical terms and lay equivalents. In Biological, translational, and clinical language processing,</title>
<date>2007</date>
<pages>49--56</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2280" citStr="Elhadad and Sutaria, 2007" startWordPosition="338" endWordPosition="341">ohesion-building devices in English (Halliday and Hasan, 1976), rhetorical structure theory (Mann and Thompson, 1988) and centering theory (Grosz et al., 1995) among the most influential contributions. Still, we do not have unified computational models that capture the interplay between various aspects of readability. Most studies focus on a single factor contributing to readability for a given intended audience. The use of rare words or technical terminology for example can make text difficult to read for certain audience types (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005; Elhadad and Sutaria, 2007). Syntactic complexity is associated with delayed processing time in understanding (Gibson, 1998) and is another factor that can decrease readability. Text organization (discourse structure), topic development (entity coherence) and the form of referring expressions also determine readability. But we know little about the relative importance of each factor and how they combine in determining perceived text quality. In our work we use texts from the Wall Street Journal intended for an educated adult audience to analyze readability factors including vocabulary, syntax, cohesion, entity coherence</context>
<context position="5379" citStr="Elhadad and Sutaria, 2007" startWordPosition="822" endWordPosition="825"> (Gunning, 1952), SMOG (McLaughlin, 1969), and Coleman-Liau (Coleman and Liau, 1975). They use only a few simple factors that are designed to be easy to calculate and are rough approximations to the linguistic factors that determine readability. For example, Flesch-Kincaid uses the average number of syllables per word to approximate vocabulary difficulty and the average number of words per sentence to approximate syntactic difficulty. In recent work, the idea of linking word frequency and text readability has been explored for making medical information more accessible to the general public. (Elhadad and Sutaria, 2007) classified words in medical texts as familiar or unfamiliar to a general audience based on their frequencies in corpora. When a description of the unfamiliar terms was provided, the perceived readability of the texts almost doubled. A more general and principled approach to using vocabulary information for readability decisions has been the use of language models. For any given text, it is easy to compute its likelihood under a given language model, i.e. one for text meant for children, or for text meant for adults, or for a given grade level. (Si and Callan, 2001), (Collins-Thompson and Call</context>
</contexts>
<marker>Elhadad, Sutaria, 2007</marker>
<rawString>Noemie Elhadad and Komal Sutaria. 2007. Mining a lexicon of technical terms and lay equivalents. In Biological, translational, and clinical language processing, pages 49–56, Prague, Czech Republic. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Elsner</author>
<author>E Charniak</author>
</authors>
<title>Coreference-inspired coherence modeling.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-HLT’08,</booktitle>
<pages>(short paper).</pages>
<contexts>
<context position="9452" citStr="Elsner and Charniak, 2008" startWordPosition="1474" endWordPosition="1477"> reading difficulty level. Form of reference is also important in wellwritten text and appropriate choices lead to improved readability. Use of pronouns for reference to highly salient entities is perceived as more desirable than the use of definite noun phrases (Gordon et al., 1993; Krahmer and Theune, 2002). The syntactic forms of first mention—when an entity is first introduced in a text—differ from those of subsequent mentions (Poesio and Vieira, 1998; Nenkova and McKeown, 2003) and can be exploited for improving and predicting text coherence (Siddharthan, 2003; Nenkova and McKeown, 2003; Elsner and Charniak, 2008). 3 Data The objective of our study is to analyze various readability factors, including discourse relations, because few empirical studies exist that directly link discourse structure with text quality. In the past, subsections of the Penn Treebank (Marcus et al., 1994) have been annotated for discourse relations (Carlson et al., 2001; Wolf and Gibson, 2005). For our study we chose to work with the newly released Penn Discourse Treebank which is the largest annotated resource which focuses exclusively on implicit local relations between adjacent sentences and explicit discourse connectives. 3</context>
</contexts>
<marker>Elsner, Charniak, 2008</marker>
<rawString>M. Elsner and E. Charniak. 2008. Coreference-inspired coherence modeling. In Proceedings of ACL-HLT’08, (short paper).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gibson</author>
</authors>
<title>Linguistic complexity: locality of syntactic dependencies.</title>
<date>1998</date>
<journal>Cognition,</journal>
<pages>68--1</pages>
<contexts>
<context position="2377" citStr="Gibson, 1998" startWordPosition="353" endWordPosition="354">, 1988) and centering theory (Grosz et al., 1995) among the most influential contributions. Still, we do not have unified computational models that capture the interplay between various aspects of readability. Most studies focus on a single factor contributing to readability for a given intended audience. The use of rare words or technical terminology for example can make text difficult to read for certain audience types (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005; Elhadad and Sutaria, 2007). Syntactic complexity is associated with delayed processing time in understanding (Gibson, 1998) and is another factor that can decrease readability. Text organization (discourse structure), topic development (entity coherence) and the form of referring expressions also determine readability. But we know little about the relative importance of each factor and how they combine in determining perceived text quality. In our work we use texts from the Wall Street Journal intended for an educated adult audience to analyze readability factors including vocabulary, syntax, cohesion, entity coherence and discourse. We study the association between these features and reader assigned readability r</context>
</contexts>
<marker>Gibson, 1998</marker>
<rawString>E. Gibson. 1998. Linguistic complexity: locality of syntactic dependencies. Cognition, 68:1–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Gordon</author>
<author>B Grosz</author>
<author>L Gilliom</author>
</authors>
<title>Pronouns, names, and the centering of attention in discourse.</title>
<date>1993</date>
<journal>Cognitive Science,</journal>
<pages>17--311</pages>
<contexts>
<context position="9109" citStr="Gordon et al., 1993" startWordPosition="1420" endWordPosition="1424">apata (2008) developed a novel approach which doesn’t postulate a preference for any type of transition but rather computes a set of features that capture transitions of all kinds in the text and their relative proportion. Their entity coherence features prove to be very suitable for various tasks, notably for information ordering and reading difficulty level. Form of reference is also important in wellwritten text and appropriate choices lead to improved readability. Use of pronouns for reference to highly salient entities is perceived as more desirable than the use of definite noun phrases (Gordon et al., 1993; Krahmer and Theune, 2002). The syntactic forms of first mention—when an entity is first introduced in a text—differ from those of subsequent mentions (Poesio and Vieira, 1998; Nenkova and McKeown, 2003) and can be exploited for improving and predicting text coherence (Siddharthan, 2003; Nenkova and McKeown, 2003; Elsner and Charniak, 2008). 3 Data The objective of our study is to analyze various readability factors, including discourse relations, because few empirical studies exist that directly link discourse structure with text quality. In the past, subsections of the Penn Treebank (Marcus</context>
</contexts>
<marker>Gordon, Grosz, Gilliom, 1993</marker>
<rawString>P. Gordon, B. Grosz, and L. Gilliom. 1993. Pronouns, names, and the centering of attention in discourse. Cognitive Science, 17:311–347.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Grosz</author>
<author>A Joshi</author>
<author>S Weinstein</author>
</authors>
<title>Centering: a framework for modelling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="1813" citStr="Grosz et al., 1995" startWordPosition="265" endWordPosition="268">ointing the factors that make text flow and easy to read—has a long history and tradition. Way back in 1944 Robert Gunning Associates was set up, offering newspapers, magazines and business firms consultations on clear writing (Gunning, 1952). In education, teaching good writing technique and grading student writing has always been of key importance (Spandel, 2004; Attali and Burstein, 2006). Linguists have also studied various aspects of text flow, with cohesion-building devices in English (Halliday and Hasan, 1976), rhetorical structure theory (Mann and Thompson, 1988) and centering theory (Grosz et al., 1995) among the most influential contributions. Still, we do not have unified computational models that capture the interplay between various aspects of readability. Most studies focus on a single factor contributing to readability for a given intended audience. The use of rare words or technical terminology for example can make text difficult to read for certain audience types (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005; Elhadad and Sutaria, 2007). Syntactic complexity is associated with delayed processing time in understanding (Gibson, 1998) and is another factor that can decr</context>
<context position="7599" citStr="Grosz et al., 1995" startWordPosition="1184" endWordPosition="1187"> language user) understands a text. Coherent text is characterized by various types of cohesive links that facilitate text comprehension (Halliday and Hasan, 1976). In recent work, considerable attention has been devoted to entity coherence in text quality, especially in relation to information ordering. In many applications such as text generation and summarization, systems need to decide the order in which selected sentences or generated clauses should be presented to the user. Most models attempting to capture local coherence between sentences were based on or inspired by centering theory (Grosz et al., 1995), which postulated strong links between the center of attention in comprehension of adjacent sentences and syntactic position and form of reference. In a detailed study of information ordering in three very different corpora, (Karamanis et al., to appear) assessed the performance of various formulations of centering. Their results were somewhat unexpected, showing that while centering transition preferences were useful, the most successful strategy for information ordering was based on avoid187 ing rough shifts, that is, sequences of sentences that share no entities in common. This supports pr</context>
</contexts>
<marker>Grosz, Joshi, Weinstein, 1995</marker>
<rawString>B. Grosz, A. Joshi, and S. Weinstein. 1995. Centering: a framework for modelling the local coherence of discourse. Computational Linguistics, 21(2):203–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Gunning</author>
</authors>
<title>The technique of clear writing. McGraw-Hill; Fouth Printing edition.</title>
<date>1952</date>
<contexts>
<context position="1436" citStr="Gunning, 1952" startWordPosition="211" endWordPosition="212">Wall Street Journal corpus. We also establish that readability predictors behave differently depending on the task: predicting text readability or ranking the readability. Our experiments indicate that discourse relations are the one class of features that exhibits robustness across these two tasks. 1 Introduction The quest for a precise definition of text quality— pinpointing the factors that make text flow and easy to read—has a long history and tradition. Way back in 1944 Robert Gunning Associates was set up, offering newspapers, magazines and business firms consultations on clear writing (Gunning, 1952). In education, teaching good writing technique and grading student writing has always been of key importance (Spandel, 2004; Attali and Burstein, 2006). Linguists have also studied various aspects of text flow, with cohesion-building devices in English (Halliday and Hasan, 1976), rhetorical structure theory (Mann and Thompson, 1988) and centering theory (Grosz et al., 1995) among the most influential contributions. Still, we do not have unified computational models that capture the interplay between various aspects of readability. Most studies focus on a single factor contributing to readabil</context>
<context position="4769" citStr="Gunning, 1952" startWordPosition="731" endWordPosition="732">at the vocabulary used in a text largely determines its readability. More common words are easier, so some metrics measured text readability by the percentage of words that were not among the N most frequent in the language. It was also observed that frequently occurring words are often short, so word length was used to approximate readability more robustly than using a predefined word frequency list. Standard indices were developed based on the link between word frequency/length and readability, such as Flesch-Kincaid (Kincaid, 1975), Automated Readability Index (Kincaid, 1975), Gunning Fog (Gunning, 1952), SMOG (McLaughlin, 1969), and Coleman-Liau (Coleman and Liau, 1975). They use only a few simple factors that are designed to be easy to calculate and are rough approximations to the linguistic factors that determine readability. For example, Flesch-Kincaid uses the average number of syllables per word to approximate vocabulary difficulty and the average number of words per sentence to approximate syntactic difficulty. In recent work, the idea of linking word frequency and text readability has been explored for making medical information more accessible to the general public. (Elhadad and Suta</context>
<context position="18136" citStr="Gunning, 1952" startWordPosition="2928" endWordPosition="2929"> including more verb phrases in each sentence increases the sentence complexity, adults might prefer to have related clauses explicitly grouped together. F1 Average Parse Tree Height r = -.0634, p = .7439 F2 Average Noun Phrases r = .2189, p = .2539 F3 Average Verb Phrases r = .4213, p = .0228 F4 Average SBARs r = .3405, p = .0707 Table 3: Syntax-related features The correlations between readability and syntactic features is shown in Table 3. The strongest correlation is that between readability and number of verb phrases (0.42). This finding is in line with prescriptive clear writing advice (Gunning, 1952; Spandel, 2004), but is to our knowledge novel in the computational linguistics literature. As (Bailin and Grafstein, 2001) point out, the sentences in (1) are easier to comprehend than the sentences in (2), even though they are longer. (1) It was late at night, but it was clear. The stars were out and the moon was bright. (2) It was late at night. It was clear. The stars were out. The moon was bright. Multiple verb phrases in one sentence may be indicative of explicit discourse relations, which we will discuss further in section 4.6. Surprisingly, the use of clauses introduced by a (possibly</context>
</contexts>
<marker>Gunning, 1952</marker>
<rawString>Robert Gunning. 1952. The technique of clear writing. McGraw-Hill; Fouth Printing edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A K Halliday</author>
<author>Ruqaiya Hasan</author>
</authors>
<title>Cohesion in English.</title>
<date>1976</date>
<publisher>Longman Group Ltd,</publisher>
<location>London, U.K.</location>
<contexts>
<context position="1716" citStr="Halliday and Hasan, 1976" startWordPosition="249" endWordPosition="252">ustness across these two tasks. 1 Introduction The quest for a precise definition of text quality— pinpointing the factors that make text flow and easy to read—has a long history and tradition. Way back in 1944 Robert Gunning Associates was set up, offering newspapers, magazines and business firms consultations on clear writing (Gunning, 1952). In education, teaching good writing technique and grading student writing has always been of key importance (Spandel, 2004; Attali and Burstein, 2006). Linguists have also studied various aspects of text flow, with cohesion-building devices in English (Halliday and Hasan, 1976), rhetorical structure theory (Mann and Thompson, 1988) and centering theory (Grosz et al., 1995) among the most influential contributions. Still, we do not have unified computational models that capture the interplay between various aspects of readability. Most studies focus on a single factor contributing to readability for a given intended audience. The use of rare words or technical terminology for example can make text difficult to read for certain audience types (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005; Elhadad and Sutaria, 2007). Syntactic complexity is associated</context>
<context position="7143" citStr="Halliday and Hasan, 1976" startWordPosition="1110" endWordPosition="1113">s written for an adult or child reader, (Barzilay and Lapata, 2008) found that adding entity coherence to (Schwarm and Ostendorf, 2005)’s list of features improves classification accuracy by 10%. 2.2 Readability as coherence for competent language users In linguistics and natural language processing, the text properties rather than those of the reader are emphasized. Text coherence is defined as the ease with which a person (tacitly assumed to be a competent language user) understands a text. Coherent text is characterized by various types of cohesive links that facilitate text comprehension (Halliday and Hasan, 1976). In recent work, considerable attention has been devoted to entity coherence in text quality, especially in relation to information ordering. In many applications such as text generation and summarization, systems need to decide the order in which selected sentences or generated clauses should be presented to the user. Most models attempting to capture local coherence between sentences were based on or inspired by centering theory (Grosz et al., 1995), which postulated strong links between the center of attention in comprehension of adjacent sentences and syntactic position and form of refere</context>
<context position="19258" citStr="Halliday and Hasan, 1976" startWordPosition="3110" endWordPosition="3114">hich we will discuss further in section 4.6. Surprisingly, the use of clauses introduced by a (possibly empty) subordinating conjunction (SBAR), are actually positively correlated (and almost approaching significance) with readability. So while for children or less educated adults these constructions might pose difficulties, they were favored by our assessors. On the other hand, the average parse tree height negatively correlated with readability as expected, but surprisingly the correlation is very weak (-0.06). 4.4 Elements of lexical cohesion In their classic study of cohesion in English, (Halliday and Hasan, 1976) discuss the various aspects of well written discourse, including the use of cohesive devices such as pronouns, definite descriptions and topic continuity from sentence to sentence.4 To measure the association between these features and readability rankings, we compute the number ofpronouns per sentence (F11) and the number of definite articles per sentence (F12). In order to qualify topic continuity from sentence to sentence in the articles, we compute average cosine similarity (F8), word overlap (F9) and word overlap over just nouns and pronouns (F10) between pairs of adjacent sentences5. Ea</context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>Michael A.K. Halliday and Ruqaiya Hasan. 1976. Cohesion in English. Longman Group Ltd, London, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Heilman</author>
<author>K Collins-Thompson</author>
<author>J Callan</author>
<author>M Eskenazi</author>
</authors>
<title>Combining Lexical and Grammatical Features to Improve Readability Measures for First and Second Language Texts.</title>
<date>2007</date>
<booktitle>Proceedings of NAACL HLT,</booktitle>
<pages>460--467</pages>
<contexts>
<context position="6047" citStr="Heilman et al., 2007" startWordPosition="933" endWordPosition="936">or unfamiliar to a general audience based on their frequencies in corpora. When a description of the unfamiliar terms was provided, the perceived readability of the texts almost doubled. A more general and principled approach to using vocabulary information for readability decisions has been the use of language models. For any given text, it is easy to compute its likelihood under a given language model, i.e. one for text meant for children, or for text meant for adults, or for a given grade level. (Si and Callan, 2001), (Collins-Thompson and Callan, 2004), (Schwarm and Ostendorf, 2005), and (Heilman et al., 2007) used language models to predict the suitability of texts for a given school grade level. But even for this type of task other factors besides vocabulary use are at play in determining readability. Syntactic complexity is an obvious factor: indeed (Heilman et al., 2007) and (Schwarm and Ostendorf, 2005) also used syntactic features, such as parse tree height or the number of passive sentences, to predict reading grade levels. For the task of deciding whether a text is written for an adult or child reader, (Barzilay and Lapata, 2008) found that adding entity coherence to (Schwarm and Ostendorf,</context>
</contexts>
<marker>Heilman, Collins-Thompson, Callan, Eskenazi, 2007</marker>
<rawString>M. Heilman, K. Collins-Thompson, J. Callan, and M. Eskenazi. 2007. Combining Lexical and Grammatical Features to Improve Readability Measures for First and Second Language Texts. Proceedings of NAACL HLT, pages 460–467.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Higgins</author>
<author>J Burstein</author>
<author>D Marcu</author>
<author>C Gentile</author>
</authors>
<title>Evaluating multiple aspects of coherence in student essays.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT/NAACL’04.</booktitle>
<contexts>
<context position="20329" citStr="Higgins et al., 2004" startWordPosition="3284" endWordPosition="3287">compute average cosine similarity (F8), word overlap (F9) and word overlap over just nouns and pronouns (F10) between pairs of adjacent sentences5. Each sentence is turned into a vector of word-types, where each type’s value is its tf-idf (where document frequency is computed over all the articles in the WSJ corpus). The cosine similarity metric is then: s � t cos (s, t) = (3) |s ||t| 4Other cohesion building devises discussed by Halliday and Hansan include lexical reiteration and discourse relations, which we address next. 5Similar features have been used for automatic essay grading as well (Higgins et al., 2004). 190 F8 Avr. Cosine Overlap r = -.1012, p = .5947 F9 Avr. Word Overlap r = -.0531, p = .7806 F10 Avr. Noun+Pronoun Overlap r = .0905, p = .6345 F11 Avr. # Pronouns/Sent r = .2381, p = .2051 F12 Avr # Definite Articles r = .2309, p = .2196 Table 4: Superficial measures of topic continuity and pronoun and definite description use None of these features correlate significantly with readability as can be seen from the results in Table 4. The overlap features are particularly bad predictors of readability, with average word/cosine overlap in fact being negatively correlated with readability. The f</context>
</contexts>
<marker>Higgins, Burstein, Marcu, Gentile, 2004</marker>
<rawString>D. Higgins, J. Burstein, D. Marcu, and C. Gentile. 2004. Evaluating multiple aspects of coherence in student essays. In Proceedings of HLT/NAACL’04.</rawString>
</citation>
<citation valid="false">
<authors>
<author>N Karamanis</author>
<author>M Poesio</author>
<author>C Mellish</author>
<author>J Oberlander</author>
</authors>
<title>(to appear). Evaluating centering for information ordering using corpora. Computational Linguistics.</title>
<marker>Karamanis, Poesio, Mellish, Oberlander, </marker>
<rawString>N. Karamanis, M. Poesio, C. Mellish, and J. Oberlander. (to appear). Evaluating centering for information ordering using corpora. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>JP Kincaid</author>
</authors>
<title>Derivation of New Readability Formulas (Automated Readability Index, Fog Count and Flesch Reading Ease Formula) for Navy Enlisted Personnel.</title>
<date>1975</date>
<contexts>
<context position="4695" citStr="Kincaid, 1975" startWordPosition="721" endWordPosition="722"> grade level. A key observation in even the oldest work in this area is that the vocabulary used in a text largely determines its readability. More common words are easier, so some metrics measured text readability by the percentage of words that were not among the N most frequent in the language. It was also observed that frequently occurring words are often short, so word length was used to approximate readability more robustly than using a predefined word frequency list. Standard indices were developed based on the link between word frequency/length and readability, such as Flesch-Kincaid (Kincaid, 1975), Automated Readability Index (Kincaid, 1975), Gunning Fog (Gunning, 1952), SMOG (McLaughlin, 1969), and Coleman-Liau (Coleman and Liau, 1975). They use only a few simple factors that are designed to be easy to calculate and are rough approximations to the linguistic factors that determine readability. For example, Flesch-Kincaid uses the average number of syllables per word to approximate vocabulary difficulty and the average number of words per sentence to approximate syntactic difficulty. In recent work, the idea of linking word frequency and text readability has been explored for making me</context>
</contexts>
<marker>Kincaid, 1975</marker>
<rawString>JP Kincaid. 1975. Derivation of New Readability Formulas (Automated Readability Index, Fog Count and Flesch Reading Ease Formula) for Navy Enlisted Personnel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Knott</author>
<author>J Oberlander</author>
<author>M ODonnell</author>
<author>C Mellish</author>
</authors>
<title>Beyond elaboration: The interaction of relations and focus in coherent text. Text representation: linguistic and psycholinguistic aspects,</title>
<date>2001</date>
<pages>181--196</pages>
<contexts>
<context position="10752" citStr="Knott et al., 2001" startWordPosition="1680" endWordPosition="1683">w resource with annotations of discourse connectives and their senses in the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1994). All explicit relations (those marked with a discourse connective) are annotated. In addition, each adjacent pair of sentences within a paragraph is annotated. If there is a discourse relation, then it is marked implicit and annotated with one or more connectives. If there is a relation between the sentences but adding a connective would be inappropriate, it is marked AltLex. If the consecutive sentences are only related by entity-based coherence (Knott et al., 2001) they are annotated with EntRel. Otherwise, they are annotated with NoRel. Besides labeling the connective, the PDTB also annotates the sense of each relation. The relations are organized into a hierarchy. The top level relations are Expansion, Comparison, Contingency, and Temporal. Briefly, an expansion relation means that the second clause continues the theme of the first clause, a comparison relation indicates that something in the two clauses is being compared, contingency means that there is a causal relation between the clauses, and temporal means they occur either at the same time or se</context>
</contexts>
<marker>Knott, Oberlander, ODonnell, Mellish, 2001</marker>
<rawString>A. Knott, J. Oberlander, M. ODonnell, and C. Mellish. 2001. Beyond elaboration: The interaction of relations and focus in coherent text. Text representation: linguistic and psycholinguistic aspects, pages 181–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Krahmer</author>
<author>M Theune</author>
</authors>
<title>Efficient contextsensitive generation of referring expressions.</title>
<date>2002</date>
<booktitle>Information Sharing: Reference and Presupposition in Language Generation and Interpretation,</booktitle>
<pages>223--264</pages>
<editor>In K. van Deemter and R. Kibble, editors,</editor>
<publisher>CSLI Publications.</publisher>
<contexts>
<context position="9136" citStr="Krahmer and Theune, 2002" startWordPosition="1425" endWordPosition="1428">d a novel approach which doesn’t postulate a preference for any type of transition but rather computes a set of features that capture transitions of all kinds in the text and their relative proportion. Their entity coherence features prove to be very suitable for various tasks, notably for information ordering and reading difficulty level. Form of reference is also important in wellwritten text and appropriate choices lead to improved readability. Use of pronouns for reference to highly salient entities is perceived as more desirable than the use of definite noun phrases (Gordon et al., 1993; Krahmer and Theune, 2002). The syntactic forms of first mention—when an entity is first introduced in a text—differ from those of subsequent mentions (Poesio and Vieira, 1998; Nenkova and McKeown, 2003) and can be exploited for improving and predicting text coherence (Siddharthan, 2003; Nenkova and McKeown, 2003; Elsner and Charniak, 2008). 3 Data The objective of our study is to analyze various readability factors, including discourse relations, because few empirical studies exist that directly link discourse structure with text quality. In the past, subsections of the Penn Treebank (Marcus et al., 1994) have been an</context>
</contexts>
<marker>Krahmer, Theune, 2002</marker>
<rawString>E. Krahmer and M. Theune. 2002. Efficient contextsensitive generation of referring expressions. In K. van Deemter and R. Kibble, editors, Information Sharing: Reference and Presupposition in Language Generation and Interpretation, pages 223–264. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lapata</author>
</authors>
<title>Automatic evaluation of information ordering: Kendalls tau.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>4</issue>
<contexts>
<context position="12254" citStr="Lapata, 2006" startWordPosition="1934" endWordPosition="1935">e texts and perform the ratings.2 Subjects were asked the following questions: • How well-written is this article? • How well does the text fit together? • How easy was it to understand? • How interesting is this article? For each question, they provided a rating between 1 and 5, with 5 being the best and 1 being the worst. 1One of the selected articles was missing from the Penn Treebank. Thus, results that do not require syntactic information (Tables 1, 2, 4, and 6) are over all thirty articles, while Tables 3, 5, and 7 report results for the twenty-nine articles with Treebank parse trees. 2(Lapata, 2006) found that human ratings are significantly correlated with self-paced reading times, a more direct measure of processing effort which we plan to explore in future work. 188 After collecting the data, it turned out that most of the time subjects gave the same rating to all questions. For competent language users, we view text readability and text coherence as equivalent properties, measuring the extent to which a text is well written. Thus for all subsequent analysis, we will use only the first question (“On a scale of 1 to 5, how well written is this text?”). The score of an article was then </context>
</contexts>
<marker>Lapata, 2006</marker>
<rawString>M. Lapata. 2006. Automatic evaluation of information ordering: Kendalls tau. Computational Linguistics, 32(4):471–484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Mann</author>
<author>S Thompson</author>
</authors>
<title>Rhetorical structure theory: Towards a functional theory of text organization.</title>
<date>1988</date>
<tech>Text, 8.</tech>
<contexts>
<context position="1771" citStr="Mann and Thompson, 1988" startWordPosition="257" endWordPosition="260"> for a precise definition of text quality— pinpointing the factors that make text flow and easy to read—has a long history and tradition. Way back in 1944 Robert Gunning Associates was set up, offering newspapers, magazines and business firms consultations on clear writing (Gunning, 1952). In education, teaching good writing technique and grading student writing has always been of key importance (Spandel, 2004; Attali and Burstein, 2006). Linguists have also studied various aspects of text flow, with cohesion-building devices in English (Halliday and Hasan, 1976), rhetorical structure theory (Mann and Thompson, 1988) and centering theory (Grosz et al., 1995) among the most influential contributions. Still, we do not have unified computational models that capture the interplay between various aspects of readability. Most studies focus on a single factor contributing to readability for a given intended audience. The use of rare words or technical terminology for example can make text difficult to read for certain audience types (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005; Elhadad and Sutaria, 2007). Syntactic complexity is associated with delayed processing time in understanding (Gibson,</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>W. Mann and S. Thompson. 1988. Rhetorical structure theory: Towards a functional theory of text organization. Text, 8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1994</date>
<contexts>
<context position="9723" citStr="Marcus et al., 1994" startWordPosition="1516" endWordPosition="1519">, 1993; Krahmer and Theune, 2002). The syntactic forms of first mention—when an entity is first introduced in a text—differ from those of subsequent mentions (Poesio and Vieira, 1998; Nenkova and McKeown, 2003) and can be exploited for improving and predicting text coherence (Siddharthan, 2003; Nenkova and McKeown, 2003; Elsner and Charniak, 2008). 3 Data The objective of our study is to analyze various readability factors, including discourse relations, because few empirical studies exist that directly link discourse structure with text quality. In the past, subsections of the Penn Treebank (Marcus et al., 1994) have been annotated for discourse relations (Carlson et al., 2001; Wolf and Gibson, 2005). For our study we chose to work with the newly released Penn Discourse Treebank which is the largest annotated resource which focuses exclusively on implicit local relations between adjacent sentences and explicit discourse connectives. 3.1 Discourse annotation The Penn Discourse Treebank (Prasad et al., 2008) is a new resource with annotations of discourse connectives and their senses in the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1994). All explicit relations (those marked with</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1994</marker>
<rawString>M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz. 1994. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G H McLaughlin</author>
</authors>
<title>SMOG grading: A new readability formula.</title>
<date>1969</date>
<journal>Journal of Reading,</journal>
<volume>12</volume>
<issue>8</issue>
<contexts>
<context position="4794" citStr="McLaughlin, 1969" startWordPosition="734" endWordPosition="735"> in a text largely determines its readability. More common words are easier, so some metrics measured text readability by the percentage of words that were not among the N most frequent in the language. It was also observed that frequently occurring words are often short, so word length was used to approximate readability more robustly than using a predefined word frequency list. Standard indices were developed based on the link between word frequency/length and readability, such as Flesch-Kincaid (Kincaid, 1975), Automated Readability Index (Kincaid, 1975), Gunning Fog (Gunning, 1952), SMOG (McLaughlin, 1969), and Coleman-Liau (Coleman and Liau, 1975). They use only a few simple factors that are designed to be easy to calculate and are rough approximations to the linguistic factors that determine readability. For example, Flesch-Kincaid uses the average number of syllables per word to approximate vocabulary difficulty and the average number of words per sentence to approximate syntactic difficulty. In recent work, the idea of linking word frequency and text readability has been explored for making medical information more accessible to the general public. (Elhadad and Sutaria, 2007) classified wor</context>
</contexts>
<marker>McLaughlin, 1969</marker>
<rawString>G.H. McLaughlin. 1969. SMOG grading: A new readability formula. Journal of Reading, 12(8):639–646.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Miltsakaki</author>
<author>K Kukich</author>
</authors>
<title>The role of centering theory’s rough-shift in the teaching and evaluation of writing skills.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL’00,</booktitle>
<pages>408--415</pages>
<contexts>
<context position="8426" citStr="Miltsakaki and Kukich, 2000" startWordPosition="1309" endWordPosition="1312">ee very different corpora, (Karamanis et al., to appear) assessed the performance of various formulations of centering. Their results were somewhat unexpected, showing that while centering transition preferences were useful, the most successful strategy for information ordering was based on avoid187 ing rough shifts, that is, sequences of sentences that share no entities in common. This supports previous findings that such types of transitions are associated with poorly written text and can be used to improve the accuracy of automatic grading of essays based on various non-discourse features (Miltsakaki and Kukich, 2000). In a more powerful generalization of centering, Barzilay and Lapata (2008) developed a novel approach which doesn’t postulate a preference for any type of transition but rather computes a set of features that capture transitions of all kinds in the text and their relative proportion. Their entity coherence features prove to be very suitable for various tasks, notably for information ordering and reading difficulty level. Form of reference is also important in wellwritten text and appropriate choices lead to improved readability. Use of pronouns for reference to highly salient entities is per</context>
</contexts>
<marker>Miltsakaki, Kukich, 2000</marker>
<rawString>E. Miltsakaki and K. Kukich. 2000. The role of centering theory’s rough-shift in the teaching and evaluation of writing skills. In Proceedings of ACL’00, pages 408– 415.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
<author>K McKeown</author>
</authors>
<title>References to named entities: a corpus study.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT/NAACL</booktitle>
<contexts>
<context position="9313" citStr="Nenkova and McKeown, 2003" startWordPosition="1453" endWordPosition="1456">heir relative proportion. Their entity coherence features prove to be very suitable for various tasks, notably for information ordering and reading difficulty level. Form of reference is also important in wellwritten text and appropriate choices lead to improved readability. Use of pronouns for reference to highly salient entities is perceived as more desirable than the use of definite noun phrases (Gordon et al., 1993; Krahmer and Theune, 2002). The syntactic forms of first mention—when an entity is first introduced in a text—differ from those of subsequent mentions (Poesio and Vieira, 1998; Nenkova and McKeown, 2003) and can be exploited for improving and predicting text coherence (Siddharthan, 2003; Nenkova and McKeown, 2003; Elsner and Charniak, 2008). 3 Data The objective of our study is to analyze various readability factors, including discourse relations, because few empirical studies exist that directly link discourse structure with text quality. In the past, subsections of the Penn Treebank (Marcus et al., 1994) have been annotated for discourse relations (Carlson et al., 2001; Wolf and Gibson, 2005). For our study we chose to work with the newly released Penn Discourse Treebank which is the larges</context>
</contexts>
<marker>Nenkova, McKeown, 2003</marker>
<rawString>A. Nenkova and K. McKeown. 2003. References to named entities: a corpus study. In Proceedings of HLT/NAACL 2003 (short paper).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Pitler</author>
<author>M Raghupathy</author>
<author>H Mehta</author>
<author>A Nenkova</author>
<author>A Lee</author>
<author>A Joshi</author>
</authors>
<title>Easily identifiable discourse relations.</title>
<date>2008</date>
<booktitle>In Coling 2008: Companion volume: Posters and Demonstrations,</booktitle>
<pages>85--88</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="27174" citStr="Pitler et al., 2008" startWordPosition="4472" endWordPosition="4475">= .3819, p = .0373 F15 Explicit relations only r = .1528, p = .4203 F16 Implicit relations only r = .2403, p = .2009 Table 6: Discourse features The likelihood of discourse relations in the text under a multinomial model is very highly and significantly correlated with readability ratings, especially after text length is taken into account. Correlations are 0.48 and 0.54 respectively. The probability of the explicit relations alone is not a sufficiently strong indicator of readability. This fact is disappointing as the explicit relations can be identified much more easily in unannotated text (Pitler et al., 2008). Note that the sequence of just the implicit relations is also not sufficient. This observation implies that the proportion of explicit and implicit relations may be meaningful but we leave the exploration of this issue for later work. 4.7 Summary of findings So far, we introduced six classes of factors that have been discussed in the literature as readability correlates. Through statistical tests of associations we identified the individual factors significantly correlated with readability ratings. These are, in decreasing order of association strength: LogL of Discourse Relations (r = .4835</context>
</contexts>
<marker>Pitler, Raghupathy, Mehta, Nenkova, Lee, Joshi, 2008</marker>
<rawString>E. Pitler, M. Raghupathy, H. Mehta, A. Nenkova, A. Lee, and A. Joshi. 2008. Easily identifiable discourse relations. In Coling 2008: Companion volume: Posters and Demonstrations, pages 85–88, Manchester, UK, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Poesio</author>
<author>R Vieira</author>
</authors>
<title>A corpus-based investigation of definite description use.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="9285" citStr="Poesio and Vieira, 1998" startWordPosition="1449" endWordPosition="1452">l kinds in the text and their relative proportion. Their entity coherence features prove to be very suitable for various tasks, notably for information ordering and reading difficulty level. Form of reference is also important in wellwritten text and appropriate choices lead to improved readability. Use of pronouns for reference to highly salient entities is perceived as more desirable than the use of definite noun phrases (Gordon et al., 1993; Krahmer and Theune, 2002). The syntactic forms of first mention—when an entity is first introduced in a text—differ from those of subsequent mentions (Poesio and Vieira, 1998; Nenkova and McKeown, 2003) and can be exploited for improving and predicting text coherence (Siddharthan, 2003; Nenkova and McKeown, 2003; Elsner and Charniak, 2008). 3 Data The objective of our study is to analyze various readability factors, including discourse relations, because few empirical studies exist that directly link discourse structure with text quality. In the past, subsections of the Penn Treebank (Marcus et al., 1994) have been annotated for discourse relations (Carlson et al., 2001; Wolf and Gibson, 2005). For our study we chose to work with the newly released Penn Discourse </context>
</contexts>
<marker>Poesio, Vieira, 1998</marker>
<rawString>M. Poesio and R. Vieira. 1998. A corpus-based investigation of definite description use. Computational Linguistics, 24(2):183–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Prasad</author>
<author>N Dinesh</author>
<author>A Lee</author>
<author>E Miltsakaki</author>
<author>L Robaldo</author>
<author>A Joshi</author>
<author>B Webber</author>
</authors>
<title>The penn discourse treebank 2.0.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC’08.</booktitle>
<contexts>
<context position="10125" citStr="Prasad et al., 2008" startWordPosition="1577" endWordPosition="1580">ious readability factors, including discourse relations, because few empirical studies exist that directly link discourse structure with text quality. In the past, subsections of the Penn Treebank (Marcus et al., 1994) have been annotated for discourse relations (Carlson et al., 2001; Wolf and Gibson, 2005). For our study we chose to work with the newly released Penn Discourse Treebank which is the largest annotated resource which focuses exclusively on implicit local relations between adjacent sentences and explicit discourse connectives. 3.1 Discourse annotation The Penn Discourse Treebank (Prasad et al., 2008) is a new resource with annotations of discourse connectives and their senses in the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1994). All explicit relations (those marked with a discourse connective) are annotated. In addition, each adjacent pair of sentences within a paragraph is annotated. If there is a discourse relation, then it is marked implicit and annotated with one or more connectives. If there is a relation between the sentences but adding a connective would be inappropriate, it is marked AltLex. If the consecutive sentences are only related by entity-based coh</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo, A. Joshi, and B. Webber. 2008. The penn discourse treebank 2.0. In Proceedings of LREC’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>KA Schriver</author>
</authors>
<title>Evaluating text quality: the continuum from text-focused toreader-focused methods.</title>
<date>1989</date>
<journal>Professional Communication, IEEE Transactions on,</journal>
<volume>32</volume>
<issue>4</issue>
<contexts>
<context position="3797" citStr="Schriver, 1989" startWordPosition="567" endWordPosition="568">and the combination of features allows for ranking prediction accuracy of 88%. Our study is novel in the use of gold-standard discourse features for predicting readability and the simultaneous analysis of various readability factors. 186 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 186–195, Honolulu, October 2008.c�2008 Association for Computational Linguistics 2 Related work 2.1 Readability with respect to intended readers The definition of what one might consider to be a well-written and readable text heavily depends on the intended audience (Schriver, 1989). Obviously, even a superbly written scientific paper will not be perceived as very readable by a lay person and a great novel might not be appreciated by a third grader. As a result, the vast majority of prior work on readability deals with labeling texts with the appropriate school grade level. A key observation in even the oldest work in this area is that the vocabulary used in a text largely determines its readability. More common words are easier, so some metrics measured text readability by the percentage of words that were not among the N most frequent in the language. It was also obser</context>
</contexts>
<marker>Schriver, 1989</marker>
<rawString>KA Schriver. 1989. Evaluating text quality: the continuum from text-focused toreader-focused methods. Professional Communication, IEEE Transactions on, 32(4):238–255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Schwarm</author>
<author>M Ostendorf</author>
</authors>
<title>Reading level assessment using support vector machines and statistical language models.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL’05,</booktitle>
<pages>523--530</pages>
<contexts>
<context position="2252" citStr="Schwarm and Ostendorf, 2005" startWordPosition="334" endWordPosition="337"> aspects of text flow, with cohesion-building devices in English (Halliday and Hasan, 1976), rhetorical structure theory (Mann and Thompson, 1988) and centering theory (Grosz et al., 1995) among the most influential contributions. Still, we do not have unified computational models that capture the interplay between various aspects of readability. Most studies focus on a single factor contributing to readability for a given intended audience. The use of rare words or technical terminology for example can make text difficult to read for certain audience types (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005; Elhadad and Sutaria, 2007). Syntactic complexity is associated with delayed processing time in understanding (Gibson, 1998) and is another factor that can decrease readability. Text organization (discourse structure), topic development (entity coherence) and the form of referring expressions also determine readability. But we know little about the relative importance of each factor and how they combine in determining perceived text quality. In our work we use texts from the Wall Street Journal intended for an educated adult audience to analyze readability factors including vocabulary, syntax</context>
<context position="6019" citStr="Schwarm and Ostendorf, 2005" startWordPosition="928" endWordPosition="931">words in medical texts as familiar or unfamiliar to a general audience based on their frequencies in corpora. When a description of the unfamiliar terms was provided, the perceived readability of the texts almost doubled. A more general and principled approach to using vocabulary information for readability decisions has been the use of language models. For any given text, it is easy to compute its likelihood under a given language model, i.e. one for text meant for children, or for text meant for adults, or for a given grade level. (Si and Callan, 2001), (Collins-Thompson and Callan, 2004), (Schwarm and Ostendorf, 2005), and (Heilman et al., 2007) used language models to predict the suitability of texts for a given school grade level. But even for this type of task other factors besides vocabulary use are at play in determining readability. Syntactic complexity is an obvious factor: indeed (Heilman et al., 2007) and (Schwarm and Ostendorf, 2005) also used syntactic features, such as parse tree height or the number of passive sentences, to predict reading grade levels. For the task of deciding whether a text is written for an adult or child reader, (Barzilay and Lapata, 2008) found that adding entity coherenc</context>
<context position="16845" citStr="Schwarm and Ostendorf, 2005" startWordPosition="2716" endWordPosition="2719"> correlations are positive: the more probable an article was based on its vocabulary, the higher it was generally rated. As expected, the NEWS model that included more general news stories had a higher correlation with people’s judgments. When combined with the length of the article, the unigram language model from the NEWS corpus becomes very predictive of readability, with the correlation between the two as high as 0.63. 4.3 Syntactic features Syntactic constructions affect processing difficulty and so might also affect readability judgments. We examined the four syntactic features used in (Schwarm and Ostendorf, 2005): average parse tree height (F1), average number of noun phrases per sentence (F2), average number of verb phrases per sentence (F3), and average number of subordinate clauses per sentence(SBARs in the Penn Treebank tagset) (F4). The sentence “We’re talking about years ago [SBAR before anyone heard of asbestos having any questionable properties].” contains an example of an SBAR clause. Having multiple noun phrases (entities) in each sentence requires the reader to remember more items, but may make the article more interesting. (Barzilay and Lapata, 2008) found that articles written for adults </context>
</contexts>
<marker>Schwarm, Ostendorf, 2005</marker>
<rawString>S. Schwarm and M. Ostendorf. 2005. Reading level assessment using support vector machines and statistical language models. In Proceedings of ACL’05, pages 523–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Si</author>
<author>J Callan</author>
</authors>
<title>A statistical model for scientific readability.</title>
<date>2001</date>
<booktitle>Proceedings of the tenth international conference on Information and knowledge management,</booktitle>
<pages>574--576</pages>
<contexts>
<context position="5951" citStr="Si and Callan, 2001" startWordPosition="920" endWordPosition="923"> the general public. (Elhadad and Sutaria, 2007) classified words in medical texts as familiar or unfamiliar to a general audience based on their frequencies in corpora. When a description of the unfamiliar terms was provided, the perceived readability of the texts almost doubled. A more general and principled approach to using vocabulary information for readability decisions has been the use of language models. For any given text, it is easy to compute its likelihood under a given language model, i.e. one for text meant for children, or for text meant for adults, or for a given grade level. (Si and Callan, 2001), (Collins-Thompson and Callan, 2004), (Schwarm and Ostendorf, 2005), and (Heilman et al., 2007) used language models to predict the suitability of texts for a given school grade level. But even for this type of task other factors besides vocabulary use are at play in determining readability. Syntactic complexity is an obvious factor: indeed (Heilman et al., 2007) and (Schwarm and Ostendorf, 2005) also used syntactic features, such as parse tree height or the number of passive sentences, to predict reading grade levels. For the task of deciding whether a text is written for an adult or child r</context>
</contexts>
<marker>Si, Callan, 2001</marker>
<rawString>L. Si and J. Callan. 2001. A statistical model for scientific readability. Proceedings of the tenth international conference on Information and knowledge management, pages 574–576.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Siddharthan</author>
</authors>
<title>Syntactic simplification and Text Cohesion.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<publisher>Allyn &amp; Bacon.</publisher>
<institution>University of</institution>
<contexts>
<context position="9397" citStr="Siddharthan, 2003" startWordPosition="1468" endWordPosition="1469">us tasks, notably for information ordering and reading difficulty level. Form of reference is also important in wellwritten text and appropriate choices lead to improved readability. Use of pronouns for reference to highly salient entities is perceived as more desirable than the use of definite noun phrases (Gordon et al., 1993; Krahmer and Theune, 2002). The syntactic forms of first mention—when an entity is first introduced in a text—differ from those of subsequent mentions (Poesio and Vieira, 1998; Nenkova and McKeown, 2003) and can be exploited for improving and predicting text coherence (Siddharthan, 2003; Nenkova and McKeown, 2003; Elsner and Charniak, 2008). 3 Data The objective of our study is to analyze various readability factors, including discourse relations, because few empirical studies exist that directly link discourse structure with text quality. In the past, subsections of the Penn Treebank (Marcus et al., 1994) have been annotated for discourse relations (Carlson et al., 2001; Wolf and Gibson, 2005). For our study we chose to work with the newly released Penn Discourse Treebank which is the largest annotated resource which focuses exclusively on implicit local relations between a</context>
</contexts>
<marker>Siddharthan, 2003</marker>
<rawString>A. Siddharthan. 2003. Syntactic simplification and Text Cohesion. Ph.D. thesis, University of Cambridge, UK. V. Spandel. 2004. Creating writers through 6-trait writing assessment and instruction. Allyn &amp; Bacon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Wolf</author>
<author>E Gibson</author>
</authors>
<title>Representing discourse coherence: A corpus-based study.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>2</issue>
<contexts>
<context position="9813" citStr="Wolf and Gibson, 2005" startWordPosition="1530" endWordPosition="1533">s first introduced in a text—differ from those of subsequent mentions (Poesio and Vieira, 1998; Nenkova and McKeown, 2003) and can be exploited for improving and predicting text coherence (Siddharthan, 2003; Nenkova and McKeown, 2003; Elsner and Charniak, 2008). 3 Data The objective of our study is to analyze various readability factors, including discourse relations, because few empirical studies exist that directly link discourse structure with text quality. In the past, subsections of the Penn Treebank (Marcus et al., 1994) have been annotated for discourse relations (Carlson et al., 2001; Wolf and Gibson, 2005). For our study we chose to work with the newly released Penn Discourse Treebank which is the largest annotated resource which focuses exclusively on implicit local relations between adjacent sentences and explicit discourse connectives. 3.1 Discourse annotation The Penn Discourse Treebank (Prasad et al., 2008) is a new resource with annotations of discourse connectives and their senses in the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1994). All explicit relations (those marked with a discourse connective) are annotated. In addition, each adjacent pair of sentences withi</context>
</contexts>
<marker>Wolf, Gibson, 2005</marker>
<rawString>F. Wolf and E. Gibson. 2005. Representing discourse coherence: A corpus-based study. Computational Linguistics, 31(2):249–288.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>