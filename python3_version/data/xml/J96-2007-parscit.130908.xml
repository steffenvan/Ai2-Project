<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.058506">
<figure confidence="0.721627615384615">
Book Reviews
Machine Translation and the Lexicon
Petra Steffens (editor)
(IBM Deutschland Informationssysteme GmbH)
Heidelberg: Springer-Verlag (Lecture
Notes in Artificial Intelligence, edited
by Jaime G. Carbonell and J. Siekmann,
volume 898), 1995, x+251 pp;
paperbound, ISBN 3-540-59040-4,
DM 58.00
Reviewed by
Inderjeet Mani
The MITRE Corporation
</figure>
<bodyText confidence="0.999932142857143">
The practical success of machine translation (MT) depends on the ability to acquire,
share, and manage lexical data. Rather than reinventing lexicons for each new sys-
tem and application, it is preferable to leverage common lexical resources. Increas-
ingly, researchers are using pre-existing resources such as machine-readable dictio-
naries (MRDs) and corpora to acquire lexicons and term banks for MT, as well as
developing new resources in such a way as to facilitate their sharing and reuse. Ma-
chine Translation and the Lexicon offers practical perspectives on these activities, from
the standpoint of researchers and of commercial developers and users in Europe. The
book consists of revised versions of a subset of the papers presented at the Third In-
ternational Workshop of the European Association for Machine Translation (EAMT)
held in Heidelberg in April 1993. The book&apos;s 15 papers are spread over three sections:
Part I, Acquiring Lexical Data (5 papers); Part II, Managing Lexical Data (7 papers);
and Part III, Describing Lexical Data (3 papers). The editor, Petra Steffens of IBM
Deutschland, presents an excellent introduction, which includes a useful bibliography
of recent lexical work related to MT.
In recent years, attention has shifted from machine-readable dictionaries towards
corpora as a source for acquiring lexical information. Part I kicks off with a well-
written article by Ide and Veronis illustrating some of the well-known problems in-
volved in attempting to extract class hierarchies from dictionaries, such as inconsis-
tencies, circularities, and incompleteness in dictionary sense definitions, as well as the
knowledge-intensive nature of the extraction task (many patterns have to be coded,
and word-sense disambiguation may require substantial world knowledge). The au-
thors conclude that &amp;quot;all of this means that in order to create resources for use in NLP
from MRDs, it is necessary to have full NLP capabilities—including full knowledge
bases—already at hand&amp;quot; (p. 27). However, they do not make clear what sorts of NLP
requirements they have in mind; their statement (with its vague use of &amp;quot;full&amp;quot;) does not
do justice to the sophistication of bootstrapping techniques used in various dictionary
extraction projects. For example, the approach of Wilks et al. (1993) relies on expand-
ing out from seed word senses identified in definitions of &amp;quot;controlled vocabulary&amp;quot;
words used in dictionary definitions, and Vanderwende (1995) describes a multipass
approach that defers processing of ambiguous patterns to later passes.
For the future, Ide and Veronis recommend backing off from fully automatic ex-
traction and focusing on merging information from multiple lexical resources, such as
corpora and multiple dictionaries. These are in fact the directions taken by numerous
lexical acquisition projects. Among the successes of the dictionary extraction work, Ide
</bodyText>
<page confidence="0.96525">
271
</page>
<note confidence="0.398266">
Computational Linguistics Volume 22, Number 2
</note>
<bodyText confidence="0.999979078431372">
and Veronis cite the Text Encoding Initiative&apos;s development of a dictionary encoding
format (Sperberg-McQueen and Burnard 1994), which is a significant step towards
dictionary reuse, and the increased synergy between electronic publishing, NLP, and
lexicography. Evidence of this synergy is found in Procter&apos;s paper on the exploitation
of corpora by lexicographers in the Cambridge Language Survey (CLS), which is a
large-scale multilingual project involving publishers, industrial labs, and universities
in several European countries, aimed at building lexical databases to support both dic-
tionary publishing and NLP lexicons. Procter describes how the CLS plans to collect
various corpora, including non-native language corpora. Each word in a corpus will
be annotated with codes for part of speech, semantic features, and subjects, and will
be linked (in the case of English) to a record in the Cambridge International Dictionary
of English. Procter doesn&apos;t discuss what standards, if any, will apply to these different
CLS coding schemes.
The other papers in Part I include those by Storrer and Schwall on the acquisition
of multiword lexemes, Ahmad on the acquisition of technical terms from corpora, and
Daelemans on using machine learning for lexical acquisition. I was surprised to find
no articles discussing the use of parallel corpora. Storrer and Schwall discuss some
highly informal feasibility studies investigating acquisition from dictionaries and cor-
pora of verbal idioms (e.g., kick the bucket) and support-verb constructions (e.g., take
into consideration). Ahmad describes statistical techniques to identify technical terms,
assuming that various &amp;quot;specialist&amp;quot; corpora, rich in technical terms, are available. Daele-
mans argues against the notion that there could ever be anything like a common NLP
lexicon for a language, taking the position that the types of lexical information needed
are highly task-specific. Instead, he envisages reuse arising from the reapplication of
a single learning method to different problems. However, he provides little by way of
quantitative results.
Part II is mainly about standards for reuse and management of lexical data in
commercial systems. I will confine myself to a few relevant and better-written papers.
Calzolari describes standardization efforts by the Expert Advisory Group on Language
Engineering Standards. In focusing on architectures for reuse of lexical and terminolog-
ical resources, IBM Deutschland&apos;s TransLexis system for managing MT system lexicons
and term banks counts as a fairly advanced framework. As Blaser describes it, their
goal is to build a &amp;quot;theory-neutral representation of multilingual lexical and termino-
logical data&amp;quot; (p. 159). The system supports four different formats for the exchange of
lexicons and term banks, including two SGML formats. Terminology and lexical en-
tries can be imported and merged automatically with existing entries using a statistical
algorithm. Another collaborative effort is the CEC-funded ESPRIT project Translator&apos;s
Workbench (TWB), which brings together several European translation departments,
tool vendors, and research groups to develop tools for professional translators. Mayer&apos;s
paper describes the design of the TWB multilingual term bank, which contains termi-
nology on automotive engineering in English, German, and Spanish.
In Part III, Caroli provides a fairly extensive classification of German multiword
lexemes, including idioms, collocations, support-verb constructions, metaphors, etc.,
in terms of a scale of compositionality. Unfortunately, examples of idiomatic expres-
sions in German are accompanied by corresponding English idiomatic translations, but
without word-for-word English glosses, making it difficult for readers unfamiliar with
German. The papers by Ostler and Heid describe the overall approach of the DELIS
project, another large-scale collaborative effort whose goal, like that of the CLS, is to
develop a system to build lexical databases for both dictionary publishing and NLP
applications. In particular, they aim for a corpus-based examination of the syntactic
and semantic properties of perception and speech-act vocabulary in five languages:
</bodyText>
<page confidence="0.990782">
272
</page>
<subsectionHeader confidence="0.925254">
Book Reviews
</subsectionHeader>
<bodyText confidence="0.999778">
English, Danish, Dutch, French, and Italian. Their use of Fillmore&apos;s frame semantics
makes possible certain fine semantic distinctions, for example, the source of the per-
cept (e.g., hear a dog) is distinguished from the stimulus perceived (e.g., hear a bark).
It will be interesting to see what sorts of cross-linguistic generalizations will emerge
from such distinctions and the corpora in use (which, by the way, aren&apos;t identified, ex-
cept for a footnote reference to an 18-million-word corpus used by Oxford University
Press). Their project expects to eventually link the different monolingual lexicons in
this vocabulary domain. With regard to structuring these multilingual lexicons, Heid
discusses the potential relevance of classifications of lexical differences in terms of di-
vergences and mismatches (e.g., Dorr 1990, Barnett, Mani, and Rich 1994); the corpora
to which these lexicons are to be linked could provide useful data for testing these
and other classifications.
Overall, despite the fine introduction and several interesting papers, the book
offers an uneven mix. While a high-level project report or system overview may work
well in the ambiance of a workshop setting, it becomes less attractive in the pages of
a book. I think the book will be of interest primarily to readers seeking an overview
of some of the issues of lexicon data management and reuse that various groups in
Europe are addressing through collaborative efforts. Although it is quite helpful in
indicating what sorts of products we can expect from this collaboration, it will be less
useful to readers with more specialized needs, for example, MT researchers examining
techniques for lexical acquisition.
</bodyText>
<sectionHeader confidence="0.968961" genericHeader="abstract">
References
</sectionHeader>
<reference confidence="0.998608777777778">
Barnett, James, Inderjeet Mani, and Elaine
Rich. 1994. &amp;quot;Reversible machine
translation: What to do when the
languages don&apos;t match up.&amp;quot; In Tomek
Strzalkowski, editor, Reversible Grammar in
Natural Language Processing. Dordrecht:
Kluwer Academic Publishers.
Dorr, Bonnie. 1990. &amp;quot;Solving thematic
divergences in machine translation.&amp;quot; In
Proceedings of the 28th Annual Meeting of the
Association for Computational Linguistics,
Pittsburgh, pages 127-134.
Sperberg-McQueen, C. M. and Lou Bumard,
editors. 1994. Guidelines for Electronic Text
Encoding and Interchange. Chicago and
Oxford: Text Encoding Initiative.
http://www.uic.edu/orgs/teilinfo/guide.html.
Vanderwende, Lucy. 1995. &amp;quot;Ambiguity in
the acquisition of lexical information.&amp;quot; In
Working Notes of the AAAI Spring
Symposium on the Representation and
Acquisition of Lexical Knowledge: Polysemy,
Ambiguity, and Generativity, pages 174-179,
Stanford University, March.
Wilks, Yorick, Dan Fass, Cheng-Ming Guo,
James McDonald, Tony Plate, and Brian
Slator. 1993. &amp;quot;Providing machine tractable
dictionary tools.&amp;quot; In James Pustejovsky,
editor, Semantics and the Lexicon,
Dordrecht: Kluwer Academic Publishers.
Inderjeet Mani is a Principal Scientist at MITRE. His research has addressed problems in lexical
semantics, machine translation, text generation, and information extraction and retrieval. His
doctoral dissertation is on the semantics of nominalizations. Recently, he has explored appli-
cations of NLP and machine learning to on-line news browsing and summarization. Mani&apos;s
address is: Artificial Intelligence Technical Center, W640, The MITRE Corporation, 11493 Sunset
Hills Road, Reston, VA 22090; e-mail: imani@mitre.org
</reference>
<page confidence="0.998892">
273
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.031767">
<title confidence="0.990961">Book Reviews Machine Translation and the Lexicon</title>
<author confidence="0.992696">Petra Steffens</author>
<affiliation confidence="0.985566">(IBM Deutschland Informationssysteme GmbH)</affiliation>
<title confidence="0.5810535">Heidelberg: Springer-Verlag (Lecture Notes in Artificial Intelligence, edited</title>
<author confidence="0.977291">by Jaime G Carbonell</author>
<author confidence="0.977291">J Siekmann</author>
<note confidence="0.54567775">volume 898), 1995, x+251 pp; paperbound, ISBN 3-540-59040-4, DM 58.00 Reviewed by</note>
<author confidence="0.988722">Inderjeet Mani</author>
<affiliation confidence="0.981613">The MITRE Corporation</affiliation>
<abstract confidence="0.9363456">The practical success of machine translation (MT) depends on the ability to acquire, share, and manage lexical data. Rather than reinventing lexicons for each new system and application, it is preferable to leverage common lexical resources. Increasingly, researchers are using pre-existing resources such as machine-readable dictionaries (MRDs) and corpora to acquire lexicons and term banks for MT, as well as new resources in such a way as to facilitate their sharing and reuse. Ma- Translation and the Lexicon practical perspectives on these activities, from the standpoint of researchers and of commercial developers and users in Europe. The book consists of revised versions of a subset of the papers presented at the Third International Workshop of the European Association for Machine Translation (EAMT)</abstract>
<note confidence="0.756825333333333">held in Heidelberg in April 1993. The book&apos;s 15 papers are spread over three sections: Part I, Acquiring Lexical Data (5 papers); Part II, Managing Lexical Data (7 papers); and Part III, Describing Lexical Data (3 papers). The editor, Petra Steffens of IBM</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Barnett</author>
<author>Inderjeet Mani</author>
<author>Elaine Rich</author>
</authors>
<title>Reversible machine translation: What to do when the languages don&apos;t match up.&amp;quot;</title>
<date>1994</date>
<booktitle>Reversible Grammar in Natural Language Processing.</booktitle>
<editor>In Tomek Strzalkowski, editor,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<location>Dordrecht:</location>
<marker>Barnett, Mani, Rich, 1994</marker>
<rawString>Barnett, James, Inderjeet Mani, and Elaine Rich. 1994. &amp;quot;Reversible machine translation: What to do when the languages don&apos;t match up.&amp;quot; In Tomek Strzalkowski, editor, Reversible Grammar in Natural Language Processing. Dordrecht: Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Dorr</author>
</authors>
<title>Solving thematic divergences in machine translation.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>127--134</pages>
<location>Pittsburgh,</location>
<contexts>
<context position="8330" citStr="Dorr 1990" startWordPosition="1224" endWordPosition="1225"> the stimulus perceived (e.g., hear a bark). It will be interesting to see what sorts of cross-linguistic generalizations will emerge from such distinctions and the corpora in use (which, by the way, aren&apos;t identified, except for a footnote reference to an 18-million-word corpus used by Oxford University Press). Their project expects to eventually link the different monolingual lexicons in this vocabulary domain. With regard to structuring these multilingual lexicons, Heid discusses the potential relevance of classifications of lexical differences in terms of divergences and mismatches (e.g., Dorr 1990, Barnett, Mani, and Rich 1994); the corpora to which these lexicons are to be linked could provide useful data for testing these and other classifications. Overall, despite the fine introduction and several interesting papers, the book offers an uneven mix. While a high-level project report or system overview may work well in the ambiance of a workshop setting, it becomes less attractive in the pages of a book. I think the book will be of interest primarily to readers seeking an overview of some of the issues of lexicon data management and reuse that various groups in Europe are addressing th</context>
</contexts>
<marker>Dorr, 1990</marker>
<rawString>Dorr, Bonnie. 1990. &amp;quot;Solving thematic divergences in machine translation.&amp;quot; In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics, Pittsburgh, pages 127-134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C M Sperberg-McQueen</author>
<author>Lou Bumard</author>
<author>editors</author>
</authors>
<date>1994</date>
<booktitle>Guidelines for Electronic Text Encoding and Interchange. Chicago and Oxford: Text Encoding Initiative. http://www.uic.edu/orgs/teilinfo/guide.html.</booktitle>
<marker>Sperberg-McQueen, Bumard, editors, 1994</marker>
<rawString>Sperberg-McQueen, C. M. and Lou Bumard, editors. 1994. Guidelines for Electronic Text Encoding and Interchange. Chicago and Oxford: Text Encoding Initiative. http://www.uic.edu/orgs/teilinfo/guide.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucy Vanderwende</author>
</authors>
<title>Ambiguity in the acquisition of lexical information.&amp;quot;</title>
<date>1995</date>
<booktitle>In Working Notes of the AAAI Spring Symposium on the Representation and Acquisition of Lexical Knowledge: Polysemy, Ambiguity, and Generativity,</booktitle>
<pages>174--179</pages>
<institution>Stanford University,</institution>
<contexts>
<context position="2808" citStr="Vanderwende (1995)" startWordPosition="422" endWordPosition="423">that in order to create resources for use in NLP from MRDs, it is necessary to have full NLP capabilities—including full knowledge bases—already at hand&amp;quot; (p. 27). However, they do not make clear what sorts of NLP requirements they have in mind; their statement (with its vague use of &amp;quot;full&amp;quot;) does not do justice to the sophistication of bootstrapping techniques used in various dictionary extraction projects. For example, the approach of Wilks et al. (1993) relies on expanding out from seed word senses identified in definitions of &amp;quot;controlled vocabulary&amp;quot; words used in dictionary definitions, and Vanderwende (1995) describes a multipass approach that defers processing of ambiguous patterns to later passes. For the future, Ide and Veronis recommend backing off from fully automatic extraction and focusing on merging information from multiple lexical resources, such as corpora and multiple dictionaries. These are in fact the directions taken by numerous lexical acquisition projects. Among the successes of the dictionary extraction work, Ide 271 Computational Linguistics Volume 22, Number 2 and Veronis cite the Text Encoding Initiative&apos;s development of a dictionary encoding format (Sperberg-McQueen and Burn</context>
</contexts>
<marker>Vanderwende, 1995</marker>
<rawString>Vanderwende, Lucy. 1995. &amp;quot;Ambiguity in the acquisition of lexical information.&amp;quot; In Working Notes of the AAAI Spring Symposium on the Representation and Acquisition of Lexical Knowledge: Polysemy, Ambiguity, and Generativity, pages 174-179, Stanford University, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yorick Wilks</author>
<author>Dan Fass</author>
<author>Cheng-Ming Guo</author>
<author>James McDonald</author>
<author>Tony Plate</author>
<author>Brian Slator</author>
</authors>
<title>Providing machine tractable dictionary tools.&amp;quot;</title>
<date>1993</date>
<booktitle>Semantics and the Lexicon,</booktitle>
<editor>In James Pustejovsky, editor,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<location>Dordrecht:</location>
<contexts>
<context position="2648" citStr="Wilks et al. (1993)" startWordPosition="397" endWordPosition="400">ction task (many patterns have to be coded, and word-sense disambiguation may require substantial world knowledge). The authors conclude that &amp;quot;all of this means that in order to create resources for use in NLP from MRDs, it is necessary to have full NLP capabilities—including full knowledge bases—already at hand&amp;quot; (p. 27). However, they do not make clear what sorts of NLP requirements they have in mind; their statement (with its vague use of &amp;quot;full&amp;quot;) does not do justice to the sophistication of bootstrapping techniques used in various dictionary extraction projects. For example, the approach of Wilks et al. (1993) relies on expanding out from seed word senses identified in definitions of &amp;quot;controlled vocabulary&amp;quot; words used in dictionary definitions, and Vanderwende (1995) describes a multipass approach that defers processing of ambiguous patterns to later passes. For the future, Ide and Veronis recommend backing off from fully automatic extraction and focusing on merging information from multiple lexical resources, such as corpora and multiple dictionaries. These are in fact the directions taken by numerous lexical acquisition projects. Among the successes of the dictionary extraction work, Ide 271 Comp</context>
</contexts>
<marker>Wilks, Fass, Guo, McDonald, Plate, Slator, 1993</marker>
<rawString>Wilks, Yorick, Dan Fass, Cheng-Ming Guo, James McDonald, Tony Plate, and Brian Slator. 1993. &amp;quot;Providing machine tractable dictionary tools.&amp;quot; In James Pustejovsky, editor, Semantics and the Lexicon, Dordrecht: Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="false">
<title>Inderjeet Mani is a Principal Scientist at MITRE. His research has addressed problems in lexical semantics, machine translation, text generation, and information extraction and retrieval. His doctoral dissertation is on the semantics of nominalizations. Recently, he has explored applications of NLP and machine learning to on-line news browsing and summarization.</title>
<booktitle>Mani&apos;s address is: Artificial Intelligence Technical Center, W640, The MITRE Corporation, 11493 Sunset Hills Road,</booktitle>
<location>Reston, VA</location>
<note>22090; e-mail: imani@mitre.org</note>
<marker></marker>
<rawString>Inderjeet Mani is a Principal Scientist at MITRE. His research has addressed problems in lexical semantics, machine translation, text generation, and information extraction and retrieval. His doctoral dissertation is on the semantics of nominalizations. Recently, he has explored applications of NLP and machine learning to on-line news browsing and summarization. Mani&apos;s address is: Artificial Intelligence Technical Center, W640, The MITRE Corporation, 11493 Sunset Hills Road, Reston, VA 22090; e-mail: imani@mitre.org</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>