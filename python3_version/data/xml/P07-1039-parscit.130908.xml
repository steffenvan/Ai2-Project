<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005639">
<title confidence="0.978955">
Bootstrapping Word Alignment via Word Packing
</title>
<author confidence="0.999377">
Yanjun Ma, Nicolas Stroppa, Andy Way
</author>
<affiliation confidence="0.9888325">
School of Computing
Dublin City University
</affiliation>
<address confidence="0.779803">
Glasnevin, Dublin 9, Ireland
</address>
<email confidence="0.99912">
{yma,nstroppa,away}@computing.dcu.ie
</email>
<sectionHeader confidence="0.996664" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999680692307692">
We introduce a simple method to pack words
for statistical word alignment. Our goal is to
simplify the task of automatic word align-
ment by packing several consecutive words
together when we believe they correspond
to a single word in the opposite language.
This is done using the word aligner itself,
i.e. by bootstrapping on its output. We
evaluate the performance of our approach
on a Chinese-to-English machine translation
task, and report a 12.2% relative increase in
BLEU score over a state-of-the art phrase-
based SMT system.
</bodyText>
<sectionHeader confidence="0.998786" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999931730769231">
Automatic word alignment can be defined as the
problem of determining a translational correspon-
dence at word level given a parallel corpus of aligned
sentences. Most current statistical models (Brown
et al., 1993; Vogel et al., 1996; Deng and Byrne,
2005) treat the aligned sentences in the corpus as se-
quences of tokens that are meant to be words; the
goal of the alignment process is to find links be-
tween source and target words. Before applying
such aligners, we thus need to segment the sentences
into words – a task which can be quite hard for lan-
guages such as Chinese for which word boundaries
are not orthographically marked. More importantly,
however, this segmentation is often performed in a
monolingual context, which makes the word align-
ment task more difficult since different languages
may realize the same concept using varying num-
bers of words (see e.g. (Wu, 1997)). Moreover, a
segmentation considered to be “good” from a mono-
lingual point of view may be unadapted for training
alignment models.
Although some statistical alignment models al-
low for 1-to-n word alignments for those reasons,
they rarely question the monolingual tokenization
and the basic unit of the alignment process remains
the word. In this paper, we focus on 1-to-n align-
ments with the goal of simplifying the task of auto-
matic word aligners by packing several consecutive
words together when we believe they correspond to a
single word in the opposite language; by identifying
enough such cases, we reduce the number of 1-to-n
alignments, thus making the task of word alignment
both easier and more natural.
Our approach consists of using the output from
an existing statistical word aligner to obtain a set of
candidates for word packing. We evaluate the re-
liability of these candidates, using simple metrics
based on co-occurence frequencies, similar to those
used in associative approaches to word alignment
(Kitamura and Matsumoto, 1996; Melamed, 2000;
Tiedemann, 2003). We then modify the segmenta-
tion of the sentences in the parallel corpus accord-
ing to this packing of words; these modified sen-
tences are then given back to the word aligner, which
produces new alignments. We evaluate the validity
of our approach by measuring the influence of the
alignment process on a Chinese-to-English Machine
Translation (MT) task.
The remainder of this paper is organized as fol-
lows. In Section 2, we study the case of 1-to-
n word alignment. Section 3 introduces an auto-
matic method to pack together groups of consecutive
</bodyText>
<page confidence="0.986032">
304
</page>
<note confidence="0.9273755">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 304–311,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<table confidence="0.996085571428572">
1:0 1:1 1:2 1:3 1:n (n &gt; 3)
IWSLT Chinese–English 21.64 63.76 9.49 3.36 1.75
IWSLT English–Chinese 29.77 57.47 10.03 1.65 1.08
IWSLT Italian–English 13.71 72.87 9.77 3.23 0.42
IWSLT English–Italian 20.45 71.08 7.02 0.9 0.55
Europarl Dutch–English 24.71 67.04 5.35 1.4 1.5
Europarl English–Dutch 23.76 69.07 4.85 1.2 1.12
</table>
<tableCaption confidence="0.999498">
Table 1: Distribution of alignment types for different language pairs (%)
</tableCaption>
<bodyText confidence="0.999755857142857">
words based on the output from a word aligner. In
Section 4, the experimental setting is described. In
Section 5, we evaluate the influence of our method
on the alignment process on a Chinese to English
MT task, and experimental results are presented.
Section 6 concludes the paper and gives avenues for
future work.
</bodyText>
<sectionHeader confidence="0.966957" genericHeader="method">
2 The Case of 1-to-n Alignment
</sectionHeader>
<bodyText confidence="0.99994656">
The same concept can be expressed in different lan-
guages using varying numbers of words; for exam-
ple, a single Chinese word may surface as a com-
pound or a collocation in English. This is fre-
quent for languages as different as Chinese and En-
glish. To quickly (and approximately) evaluate this
phenomenon, we trained the statistical IBM word-
alignment model 4 (Brown et al., 1993),1 using the
GIZA++ software (Och and Ney, 2003) for the fol-
lowing language pairs: Chinese–English, Italian–
English, and Dutch–English, using the IWSLT-2006
corpus (Takezawa et al., 2002; Paul, 2006) for the
first two language pairs, and the Europarl corpus
(Koehn, 2005) for the last one. These asymmet-
ric models produce 1-to-n alignments, with n &gt; 0,
in both directions. Here, it is important to mention
that the segmentation of sentences is performed to-
tally independently of the bilingual alignment pro-
cess, i.e. it is done in a monolingual context. For Eu-
ropean languages, we apply the maximum-entropy
based tokenizer of OpenNLP2; the Chinese sen-
tences were human segmented (Paul, 2006).
In Table 1, we report the frequencies of the dif-
ferent types of alignments for the various languages
and directions. As expected, the number of 1: n
</bodyText>
<footnote confidence="0.99644475">
1More specifically, we performed 5 iterations of Model 1, 5
iterations of HMM, 5 iterations of Model 3, and 5 iterations of
Model 4.
2http://opennlp.sourceforge.net/.
</footnote>
<bodyText confidence="0.9973882">
alignments with n 7� 1 is high for Chinese–English
(^- 40%), and significantly higher than for the Eu-
ropean languages. The case of 1-to-n alignments is,
therefore, obviously an important issue when deal-
ing with Chinese–English word alignment.3
</bodyText>
<subsectionHeader confidence="0.984124">
2.1 The Treatment of 1-to-n Alignments
</subsectionHeader>
<bodyText confidence="0.999948772727273">
Fertility-based models such as IBM models 3, 4, and
5 allow for alignments between one word and sev-
eral words (1-to-n or 1: n alignments in what fol-
lows), in particular for the reasons specified above.
They can be seen as extensions of the simpler IBM
models 1 and 2 (Brown et al., 1993). Similarly,
Deng and Byrne (2005) propose an HMM frame-
work capable of dealing with 1-to-n alignment,
which is an extension of the original model of (Vogel
et al., 1996).
However, these models rarely question the mono-
lingual tokenization, i.e. the basic unit of the align-
ment process is the word.4 One alternative to ex-
tending the expressivity of one model (and usually
its complexity) is to focus on the input representa-
tion; in particular, we argue that the alignment pro-
cess can benefit from a simplification of the input,
which consists of trying to reduce the number of
1-to-n alignments to consider. Note that the need
to consider segmentation and alignment at the same
time is also mentioned in (Tiedemann, 2003), and
related issues are reported in (Wu, 1997).
</bodyText>
<subsectionHeader confidence="0.987025">
2.2 Notation
</subsectionHeader>
<bodyText confidence="0.996875">
While in this paper, we focus on Chinese–English,
the method proposed is applicable to any language
</bodyText>
<footnote confidence="0.9644534">
3Note that a 1: 0 alignment may denote a failure to capture
a 1: n alignment with n &gt; 1.
4Interestingly, this is actually even the case for approaches
that directly model alignments between phrases (Marcu and
Wong, 2002; Birch et al., 2006).
</footnote>
<page confidence="0.999214">
305
</page>
<bodyText confidence="0.999894785714286">
pair – even for closely related languages, we ex-
pect improvements to be seen. The notation how-
ever assume Chinese–English MT. Given a Chi-
nese sentence cJ1 consisting of J words fc1, ... , cJI
and an English sentence eI1 consisting of I words
fe1, ... , eII, AC→E (resp. AE→C) will denote a
Chinese-to-English (resp. an English-to-Chinese)
word alignment between cJ1 and eI1. Since we are
primarily interested in 1-to-n alignments, AC→E
can be represented as a set of pairs aj = (cj, Ej)
denoting a link between one single Chinese word
cj and a few English words Ej (and similarly for
AE→C). The set Ej is empty if the word cj is not
aligned to any word in eI1.
</bodyText>
<sectionHeader confidence="0.991922" genericHeader="method">
3 Automatic Word Repacking
</sectionHeader>
<bodyText confidence="0.99999395">
Our approach consists of packing consecutive words
together when we believe they correspond to a sin-
gle word in the other language. This bilingually
motivated packing of words changes the basic unit
of the alignment process, and simplifies the task of
automatic word alignment. We thus minimize the
number of 1-to-n alignments in order to obtain more
comparable segmentations in the two languages. In
this section, we present an automatic method that
builds upon the output from an existing automatic
word aligner. More specifically, we (i) use a word
aligner to obtain 1-to-n alignments, (ii) extract can-
didates for word packing, (iii) estimate the reliability
of these candidates, (iv) replace the groups of words
to pack by a single token in the parallel corpus, and
(v) re-iterate the alignment process using the up-
dated corpus. The first three steps are performed
in both directions, and produce two bilingual dic-
tionaries (source-target and target-source) of groups
of words to pack.
</bodyText>
<subsectionHeader confidence="0.999214">
3.1 Candidate Extraction
</subsectionHeader>
<bodyText confidence="0.999085647058824">
In the following, we assume the availability of an
automatic word aligner that can output alignments
AC→E and AE→C for any sentence pair (cJ1 , eI1)
in a parallel corpus. We also assume that AC→E
and AE→C contain 1: n alignments. Our method for
repacking words is very simple: whenever a single
word is aligned with several consecutive words, they
are considered candidates for repacking. Formally,
given an alignment AC→E between cJ1 and eI1, if
aj = (cj, Ej) E AC→E, with Ej = fej.,..., ej„I
and bk E V, m − 1b jk+1 − jk = 1, then the align-
ment aj between cj and the sequence of words Ej
is considered a candidate for word repacking. The
same goes for AE→C. Some examples of such 1-
to-n alignments between Chinese and English (in
both directions) we can derive automatically are dis-
played in Figure 1.
</bodyText>
<listItem confidence="0.619692166666667">
i1 j )25: white wine
�&apos;i�j,&apos; 7: department store
Mk: excuse me
4 : call the police
4: cup of
,1:$q: have to
</listItem>
<figureCaption confidence="0.9849755">
Figure 1: Example of 1-to-n word alignments be-
tween Chinese and English
</figureCaption>
<subsectionHeader confidence="0.999608">
3.2 Candidate Reliability Estimation
</subsectionHeader>
<bodyText confidence="0.998491375">
Of course, the process described above is error-
prone and if we want to change the input to give to
the word aligner, we need to make sure that we are
not making harmful modifications.5 We thus addi-
tionally evaluate the reliability of the candidates we
extract and filter them before inclusion in our bilin-
gual dictionary. To perform this filtering, we use
two simple statistical measures. In the following,
aj = (cj, Ej) denotes a candidate.
The first measure we consider is co-occurrence
frequency (COOC(cj, Ej)), i.e. the number of
times cj and Ej co-occur in the bilingual corpus.
This very simple measure is frequently used in as-
sociative approaches (Melamed, 1997; Tiedemann,
2003). The second measure is the alignment confi-
dence, defined as
</bodyText>
<equation confidence="0.969550333333333">
C(aj)
AC(aj) =
COOC(cj, Ej),
</equation>
<bodyText confidence="0.999258222222222">
where C(aj) denotes the number of alignments pro-
posed by the word aligner that are identical to aj.
In other words, AC(aj) measures how often the
5Consequently, if we compare our approach to the problem
of collocation identification, we may say that we are more in-
terested in precision than recall (Smadja et al., 1996). However,
note that our goal is not recognizing specific sequences of words
such as compounds or collocations; it is making (bilingually
motivated) changes that simplify the alignment process.
</bodyText>
<figure confidence="0.979263">
closest: � it
fifteen: -}- -E
fine: * 4-f
flight: �k &amp;YE
get: * i&apos;1
here: A it_W_
</figure>
<page confidence="0.990889">
306
</page>
<bodyText confidence="0.997661912280702">
aligner aligns cj and Ej when they co-occur. We ever, we have not seen in practice much benefit from
also impose that |Ej  |G k, where k is a fixed inte- running it more than twice (few new candidates are
ger that may depend on the language pair (between extracted after two iterations).
3 and 5 in practice). The rationale behind this is that It is also important to note that this process is
it is very rare to get reliable alignment between one bilingually motivated and strongly depends on the
word and k consecutive words when k is high. language pair. For example, white wine, excuse me,
The candidates are included in our bilingual dic- call the police, and cup of (cf. Figure 1) translate re-
tionary if and only if their measures are above some spectively as vin blanc, excusez-moi, appellez la po-
fixed thresholds t,oo, and ta,, which allow for the lice, and tasse de in French. Those groupings would
control of the size of the dictionary and the quality not be found for a language pair such as French-
of its contents. Some other measures (including the English, which is consistent with the fact that they
Dice coefficient) could be considered; however, it are less useful for French–English than for Chinese-
has to be noted that we are more interested here in English in a MT perspective.
the filtering than in the discovery of alignment, since 3.4 Using Manually Developed Dictionaries
our method builds upon an existing aligner. More- We wanted to compare this automatic approach to
over, we will see that even these simple measures manually developed resources. For this purpose,
can lead to an improvement of the alignment pro- we used a dictionary built by the MT group of
cess in a MT context (cf. Section 5). Harbin Institute of Technology, as a preprocessing
3.3 Bootstrapped Word Repacking step to Chinese–English word alignment, and moti-
Once the candidates are extracted, we repack the vated by several years of Chinese–English MT prac-
words in the bilingual dictionaries constructed using tice. Some examples extracted from this resource
the method described above; this provides us with are displayed in Figure 2.
an updated training corpus, in which some word se- 有: there is
quences have been replaced by a single token. This A要: want to
update is totally naive: if an entry aj _ (cj, Ej) is 不必: need not
present in the dictionary and matches one sentence 前面: in front of
pair (ci , ei) (i.e. cj and Ej are respectively con- 一: as soon as
tained in ci and ei), then we replace the sequence 看: look at
of words Ej with a single token which becomes a Figure 2: Examples of entries from the manually de-
new lexical unit.6 Note that this replacement occurs veloped dictionary
even if no alignment was found between cj and Ej 4 Experimental Setting
for the pair (ci , ei). This is motivated by the fact 4.1 Evaluation
that the filtering described above is quite conserva- The intrinsic quality of word alignment can be as-
tive; we trust the entry ai to be correct. This update sessed using the Alignment Error Rate (AER) met-
is performed in both directions. It is then possible to ric (Och and Ney, 2003), that compares a system’s
run the word aligner using the updated (simplified) alignment output to a set of gold-standard align-
parallel corpus, in order to get new alignments. By ment. While this method gives a direct evaluation of
performing a deterministic word packing, we avoid the quality of word alignment, it is faced with sev-
the computation of the fertility parameters associ- eral limitations. First, it is really difficult to build
ated with fertility-based models. a reliable and objective gold-standard set, especially
Word packing can be applied several times: once for languages as different as Chinese and English.
we have grouped some words together, they become Second, an increase in AER does not necessarily im-
the new basic unit to consider, and we can re-run ply an improvement in translation quality (Liang et
the same method to get additional groupings. How- al., 2006) and vice-versa (Vilar et al., 2006). The
6In case of overlap between several groups of words to re-
place, we select the one with highest confidence (according to
tac).
307
relationship between word alignments and their im-
pact on MT is also investigated in (Ayan and Dorr,
2006; Lopez and Resnik, 2006; Fraser and Marcu,
2006). Consequently, we chose to extrinsically eval-
uate the performance of our approach via the transla-
tion task, i.e. we measure the influence of the align-
ment process on the final translation output. The
quality of the translation output is evaluated using
BLEU (Papineni et al., 2002).
</bodyText>
<subsectionHeader confidence="0.950067">
4.2 Data
</subsectionHeader>
<bodyText confidence="0.999927958333333">
The experiments were carried out using the
Chinese–English datasets provided within the
IWSLT 2006 evaluation campaign (Paul, 2006), ex-
tracted from the Basic Travel Expression Corpus
(BTEC) (Takezawa et al., 2002). This multilingual
speech corpus contains sentences similar to those
that are usually found in phrase-books for tourists
going abroad. Training was performed using the de-
fault training set, to which we added the sets de-
vset1, devset2, and devset3.7 The English side of
the test set was not available at the time we con-
ducted our experiments, so we split the development
set (devset 4) into two parts: one was kept for testing
(200 aligned sentences) with the rest (289 aligned
sentences) used for development purposes.
As a pre-processing step, the English sentences
were tokenized using the maximum-entropy based
tokenizer of the OpenNLP toolkit, and case infor-
mation was removed. For Chinese, the data pro-
vided were tokenized according to the output format
of ASR systems, and human-corrected (Paul, 2006).
Since segmentations are human-corrected, we are
sure that they are good from a monolingual point of
view. Table 2 contains the various corpus statistics.
</bodyText>
<subsectionHeader confidence="0.990678">
4.3 Baseline
</subsectionHeader>
<bodyText confidence="0.999899333333333">
We use a standard log-linear phrase-based statistical
machine translation system as a baseline: GIZA++
implementation of IBM word alignment model 4
(Brown et al., 1993; Och and Ney, 2003),8 the re-
finement and phrase-extraction heuristics described
in (Koehn et al., 2003), minimum-error-rate training
</bodyText>
<footnote confidence="0.9634588">
7More specifically, we choose the first English reference
from the 7 references and the Chinese sentence to construct new
sentence pairs.
8Training is performed using the same number of iterations
as in Section 2.
</footnote>
<table confidence="0.9995613">
Chinese English
Train Sentences 41,465
Running words 361,780 375,938
Vocabulary size 11,427 9,851
Dev. Sentences 289 (7 refs.)
Running words 3,350 26,223
Vocabulary size 897 1,331
Eval. Sentences 200 (7 refs.)
Running words 1,864 14,437
Vocabulary size 569 1,081
</table>
<tableCaption confidence="0.999437">
Table 2: Chinese–English corpus statistics
</tableCaption>
<bodyText confidence="0.999166777777778">
(Och, 2003) using Phramer (Olteanu et al., 2006),
a 3-gram language model with Kneser-Ney smooth-
ing trained with SRILM (Stolcke, 2002) on the En-
glish side of the training data and Pharaoh (Koehn,
2004) with default settings to decode. The log-linear
model is also based on standard features: condi-
tional probabilities and lexical smoothing of phrases
in both directions, and phrase penalty (Zens and
Ney, 2004).
</bodyText>
<sectionHeader confidence="0.901305" genericHeader="method">
5 Experimental Results
5.1 Results
</sectionHeader>
<bodyText confidence="0.999996272727273">
The initial word alignments are obtained using the
baseline configuration described above. From these,
we build two bilingual 1-to-n dictionaries (one for
each direction), and the training corpus is updated
by repacking the words in the dictionaries, using the
method presented in Section 2. As previously men-
tioned, this process can be repeated several times; at
each step, we can also choose to exploit only one of
the two available dictionaries, if so desired. We then
extract aligned phrases using the same procedure as
for the baseline system; the only difference is the ba-
sic unit we are considering. Once the phrases are ex-
tracted, we perform the estimation of the features of
the log-linear model and unpack the grouped words
to recover the initial words. Finally, minimum-error-
rate training and decoding are performed.
The various parameters of the method (k, tcooc,
tac, cf. Section 2) have been optimized on the devel-
opment set. We found out that it was enough to per-
form two iterations of repacking: the optimal set of
values was found to be k = 3, tac = 0.5, tcooc = 20
for the first iteration, and tcooc = 10 for the second
</bodyText>
<page confidence="0.994508">
308
</page>
<table confidence="0.463723375">
BLEU[%]
Baseline 15.14
n=1. with C-E dict. 15.92
n=1. with E-C dict. 15.77
n=1. with both 16.59
n=2. with C-E dict. 16.99
n=2. with E-C dict. 16.59
n=2. with both 16.88
</table>
<tableCaption confidence="0.959122">
Table 3: Influence of word repacking on Chinese-to-
English MT
</tableCaption>
<bodyText confidence="0.998872884615385">
iteration, for both directions.9 In Table 3, we report
the results obtained on the test set, where n denotes
the iteration. We first considered the inclusion of
only the Chinese–English dictionary, then only the
English–Chinese dictionary, and then both.
After the first step, we can already see an im-
provement over the baseline when considering one
of the two dictionaries. When using both, we ob-
serve an increase of 1.45 BLEU points, which cor-
responds to a 9.6% relative increase. Moreover, we
can gain from performing another step. However,
the inclusion of the English–Chinese dictionary is
harmful in this case, probably because 1-to-n align-
ments are less frequent for this direction, and have
been captured during the first step. By including the
Chinese–English dictionary only, we can achieve an
increase of 1.85 absolute BLEU points (12.2% rela-
tive) over the initial baseline.10
Quality of the Dictionaries To assess the qual-
ity of the extraction procedure, we simply manu-
ally evaluated the ratio of incorrect entries in the
dictionaries. After one step of word packing, the
Chinese–English and the English–Chinese dictio-
naries respectively contain 7.4% and 13.5% incor-
rect entries. After two steps of packing, they only
contain 5.9% and 10.3% incorrect entries.
</bodyText>
<subsectionHeader confidence="0.999759">
5.2 Alignment Types
</subsectionHeader>
<bodyText confidence="0.920633857142857">
Intuitively, the word alignments obtained after word
packing are more likely to be 1-to-1 than before. In-
9The parameters k, ta., and t.oo. are optimized for each
step, and the alignment obtained using the best set of parameters
for a given step are used as input for the following step.
10Note that this setting (using both dictionaries for the first
step and only the Chinese dictionary for the second step) is also
the best setting on the development set.
deed, the word sequences in one language that usu-
ally align to one single word in the other language
have been grouped together to form one single to-
ken. Table 4 shows the detail of the distribution of
alignment types after one and two steps of automatic
repacking. In particular, we can observe that the 1:1
</bodyText>
<tableCaption confidence="0.953964">
Table 4: Distribution of alignment types (%)
</tableCaption>
<bodyText confidence="0.99933925">
alignments are more frequent after the application
of repacking: the ratio of this type of alignment has
increased by 7.81% for Chinese–English and 5.26%
for English–Chinese.
</bodyText>
<subsectionHeader confidence="0.998836">
5.3 Influence of Word Segmentation
</subsectionHeader>
<bodyText confidence="0.9999128">
To test the influence of the initial word segmenta-
tion on the process of word packing, we considered
an additional segmentation configuration, based on
an automatic segmenter combining rule-based and
statistical techniques (Zhao et al., 2001).
</bodyText>
<tableCaption confidence="0.945404">
Table 5: Influence of Chinese segmentation
</tableCaption>
<bodyText confidence="0.941282625">
The results obtained are displayed in Table 5. As
expected, the automatic segmenter leads to slightly
lower results than the human-corrected segmenta-
tion. However, the proposed method seems to be
beneficial irrespective of the choice of segmentation.
Indeed, we can also observe an improvement in the
new setting: 2.6 points absolute increase in BLEU
(17.4% relative).11
</bodyText>
<footnote confidence="0.922529">
11We could actually consider an extreme case, which would
consist of splitting the sentences into characters, i.e. each char-
acter would be blindly treated as one word. The segmentation
</footnote>
<table confidence="0.999694076923077">
1:0 1:1 1:2 1:3 1: n
(n &gt; 3)
C-E Base. 21.64 63.76 9.49 3.36 1.75
n=1 19.69 69.43 6.32 2.79 1.78
n=2 19.67 71.57 4.87 2.12 1.76
E-C Base. 29.77 57.47 10.03 1.65 1.08
n=1 26.59 61.95 8.82 1.55 1.09
n=2 25.10 62.73 9.38 1.68 1.12
BLEU[%]
Original segmentation 15.14
Original segmentation + Word packing 16.99
Automatic segmentation 14.91
Automatic segmentation + Word packing 17.51
</table>
<page confidence="0.994922">
309
</page>
<subsectionHeader confidence="0.984254">
5.4 Exploiting Manually Developed Resources
</subsectionHeader>
<bodyText confidence="0.9997096">
We also compared our technique for automatic pack-
ing of words with the exploitation of manually
developed resources. More specifically, we used
a 1-to-n Chinese–English bilingual dictionary, de-
scribed in Section 3.4, and used it in place of the
automatically acquired dictionary. Words are thus
grouped according to this dictionary, and we then
apply the same word aligner as for previous experi-
ments. In this case, since we are not bootstrapping
from the output of a word aligner, this can actually
be seen as a pre-processing step prior to alignment.
These resources follow more or less the same for-
mat as the output of the word segmenter mentioned
in Section 5.1.2 (Zhao et al., 2001), so the experi-
ments are carried out using this segmentation.
</bodyText>
<table confidence="0.9952835">
BLEU[%]
Baseline 14.91
Automatic word packing 17.51
Packing with “manual” dictionary 16.15
</table>
<tableCaption confidence="0.999617">
Table 6: Exploiting manually developed resources
</tableCaption>
<bodyText confidence="0.999764555555556">
The results obtained are displayed in Table 6.We
can observe that the use of the manually developed
dictionary provides us with an improvement in trans-
lation quality: 1.24 BLEU points absolute (8.3% rel-
ative). However, there does not seem to be a clear
gain when compared with the automatic method.
Even if those manual resources were extended, we
do not believe the improvement is sufficient enough
to justify this additional effort.
</bodyText>
<sectionHeader confidence="0.996457" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99789840625">
In this paper, we have introduced a simple yet effec-
tive method to pack words together in order to give
a different and simplified input to automatic word
aligners. We use a bootstrap approach in which we
first extract 1-to-n word alignments using an exist-
ing word aligner, and then estimate the confidence
of those alignments to decide whether or not the n
words have to be grouped; if so, this group is con-
would thus be completely driven by the bilingual alignment pro-
cess (see also (Wu, 1997; Tiedemann, 2003) for related consid-
erations). In this case, our approach would be similar to the
approach of (Xu et al., 2004), except for the estimation of can-
didates.
sidered a new basic unit to consider. We can finally
re-apply the word aligner to the updated sentences.
We have evaluated the performance of our ap-
proach by measuring the influence of this process
on a Chinese-to-English MT task, based on the
IWSLT 2006 evaluation campaign. We report a
12.2% relative increase in BLEU score over a stan-
dard phrase-based SMT system. We have verified
that this process actually reduces the number of 1: n
alignments with n 7� 1, and that it is rather indepen-
dent from the (Chinese) segmentation strategy.
As for future work, we first plan to consider dif-
ferent confidence measures for the filtering of the
alignment candidates. We also want to bootstrap on
different word aligners; in particular, one possibility
is to use the flexible HMM word-to-phrase model of
Deng and Byrne (2005) in place of IBM model 4.
Finally, we would like to apply this method to other
corpora and language pairs.
</bodyText>
<sectionHeader confidence="0.92857" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.999911333333333">
This work is supported by Science Foundation Ire-
land (grant number OS/IN/1732). Prof. Tiejun Zhao
and Dr. Muyun Yang from the MT group of Harbin
Institute of Technology, and Yajuan Lv from the In-
stitute of Computing Technology, Chinese Academy
of Sciences, are kindly acknowledged for provid-
ing us with the Chinese segmenter and the manually
developed bilingual dictionary used in our experi-
ments.
</bodyText>
<sectionHeader confidence="0.994573" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.93515975">
Necip Fazil Ayan and Bonnie J. Dorr. 2006. Going be-
yond aer: An extensive analysis of word alignments
and their impact on mt. In Proceedings of COLING-
ACL 2006, pages 9–16, Sydney, Australia.
Alexandra Birch, Chris Callison-Burch, and Miles Os-
borne. 2006. Constraining the phrase-based, joint
probability statistical translation model. In Proceed-
ings ofAMTA 2006, pages 10–18, Boston, MA.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263–311.
Yonggang Deng and William Byrne. 2005. HMM word
and phrase alignment for statistical machine transla-
tion. In Proceedings of HLT-EMNLP 2005, pages
169–176, Vancouver, Canada.
</reference>
<page confidence="0.991705">
310
</page>
<reference confidence="0.999493473684211">
Alexander Fraser and Daniel Marcu. 2006. Measuring
word alignment quality for statistical machine transla-
tion. Technical Report ISI-TR-616, ISI/University of
Southern California.
Mihoko Kitamura and Yuji Matsumoto. 1996. Auto-
matic extraction of word sequence correspondences in
parallel corpora. In Proceedings of the 4th Workshop
on Very Large Corpora, pages 79–87, Copenhagen,
Denmark.
Philip Koehn, Franz Och, and Daniel Marcu. 2003. Sta-
tistical phrase-based translation. In Proceedings of
HLT-NAACL 2003, pages 48–54, Edmonton, Canada.
Philip Koehn. 2004. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of AMTA 2004, pages 115–124,
Washington, District of Columbia.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Machine Transla-
tion Summit X, pages 79–86, Phuket, Thailand.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of HLT-NAACL
2006, pages 104–111, New York, NY.
Adam Lopez and Philip Resnik. 2006. Word-based
alignment, phrase-based translation: What’s the link?
In Proceedings of AMTA 2006, pages 90–99, Cam-
bridge, MA.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proceedings ofEMNLP 2002, pages 133–139,
Morristown, NJ.
I. Dan Melamed. 1997. Automatic discovery of non-
compositional compounds in parallel data. In Pro-
ceedings of EMNLP 1997, pages 97–108, Somerset,
New Jersey.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221–249.
Franz Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Com-
putational Linguistics, 29(1):19–51.
Franz Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proceedings ofACL 2003,
pages 160–167, Sapporo, Japan.
Marian Olteanu, Chris Davis, Ionut Volosen, and Dan
Moldovan. 2006. Phramer - an open source statis-
tical phrase-based translator. In Proceedings of the
NAACL 2006 Workshop on Statistical Machine Trans-
lation, pages 146–149, New York, NY.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings ofACL
2002, pages 311–318, Philadelphia, PA.
Michael Paul. 2006. Overview of the IWSLT 2006 Eval-
uation Campaign. In Proceedings of IWSLT 2006,
pages 1–15, Kyoto, Japan.
Frank Smadja, Kathleen R. McKeown, and Vasileios
Hatzivassiloglou. 1996. Translating collocations for
bilingual lexicons: A statistical approach. Computa-
tional Linguistics, 22(1):1–38.
Andrea Stolcke. 2002. SRILM – An extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Processing,
pages 901–904, Denver, Colorado.
T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, and
S. Yamamoto. 2002. Toward a broad-coverage bilin-
gual corpus for speech translation of travel conversa-
tions in the real world. In Proceedings ofLREC 2002,
pages 147–152, Las Palmas, Spain.
J¨org Tiedemann. 2003. Combining clues for word align-
ment. In Proceedings of EACL 2003, pages 339–346,
Budapest, Hungary.
David Vilar, Maja Popovic, and Hermann Ney. 2006.
AER: Do we need to ”improve” our alignments? In
Proceedings of IWSLT 2006, pages 205–212, Kyoto,
Japan.
Stefan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of COLING 1996, pages 836–
841, Copenhagen, Denmark.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–403.
Jia Xu, Richard Zens, and Hermann Ney. 2004. Do
we need chinese word segmentation for statistical
machine translation? In Proceedings of the Third
SIGHAN Workshop on Chinese Language Learning,
pages 122–128, Barcelona, Spain.
Richard Zens and Hermann Ney. 2004. Improvements
in phrase-based statistical machine translation. In
Proceedings of HLT-NAACL 2004, pages 257–264,
Boston, MA.
Tiejun Zhao, Yajuan L¨u, and Hao Yu. 2001. Increas-
ing accuracy of chinese segmentation with strategy of
multi-step processing. Journal of Chinese Information
Processing, 15(1):13–18.
</reference>
<page confidence="0.999012">
311
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.900143">
<title confidence="0.999825">Bootstrapping Word Alignment via Word Packing</title>
<author confidence="0.999782">Yanjun Ma</author>
<author confidence="0.999782">Nicolas Stroppa</author>
<author confidence="0.999782">Andy Way</author>
<affiliation confidence="0.9995575">School of Computing Dublin City University</affiliation>
<address confidence="0.999564">Glasnevin, Dublin 9, Ireland</address>
<abstract confidence="0.992802357142857">We introduce a simple method to pack words for statistical word alignment. Our goal is to simplify the task of automatic word alignment by packing several consecutive words together when we believe they correspond to a single word in the opposite language. This is done using the word aligner itself, i.e. by bootstrapping on its output. We evaluate the performance of our approach on a Chinese-to-English machine translation and report a increase in BLEU score over a state-of-the art phrasebased SMT system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Necip Fazil Ayan</author>
<author>Bonnie J Dorr</author>
</authors>
<title>Going beyond aer: An extensive analysis of word alignments and their impact on mt.</title>
<date>2006</date>
<booktitle>In Proceedings of COLINGACL</booktitle>
<pages>9--16</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="15657" citStr="Ayan and Dorr, 2006" startWordPosition="2631" endWordPosition="2634">be applied several times: once for languages as different as Chinese and English. we have grouped some words together, they become Second, an increase in AER does not necessarily imthe new basic unit to consider, and we can re-run ply an improvement in translation quality (Liang et the same method to get additional groupings. How- al., 2006) and vice-versa (Vilar et al., 2006). The 6In case of overlap between several groups of words to replace, we select the one with highest confidence (according to tac). 307 relationship between word alignments and their impact on MT is also investigated in (Ayan and Dorr, 2006; Lopez and Resnik, 2006; Fraser and Marcu, 2006). Consequently, we chose to extrinsically evaluate the performance of our approach via the translation task, i.e. we measure the influence of the alignment process on the final translation output. The quality of the translation output is evaluated using BLEU (Papineni et al., 2002). 4.2 Data The experiments were carried out using the Chinese–English datasets provided within the IWSLT 2006 evaluation campaign (Paul, 2006), extracted from the Basic Travel Expression Corpus (BTEC) (Takezawa et al., 2002). This multilingual speech corpus contains se</context>
</contexts>
<marker>Ayan, Dorr, 2006</marker>
<rawString>Necip Fazil Ayan and Bonnie J. Dorr. 2006. Going beyond aer: An extensive analysis of word alignments and their impact on mt. In Proceedings of COLINGACL 2006, pages 9–16, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
</authors>
<title>Constraining the phrase-based, joint probability statistical translation model.</title>
<date>2006</date>
<booktitle>In Proceedings ofAMTA 2006,</booktitle>
<pages>10--18</pages>
<location>Boston, MA.</location>
<contexts>
<context position="7268" citStr="Birch et al., 2006" startWordPosition="1184" endWordPosition="1187">ification of the input, which consists of trying to reduce the number of 1-to-n alignments to consider. Note that the need to consider segmentation and alignment at the same time is also mentioned in (Tiedemann, 2003), and related issues are reported in (Wu, 1997). 2.2 Notation While in this paper, we focus on Chinese–English, the method proposed is applicable to any language 3Note that a 1: 0 alignment may denote a failure to capture a 1: n alignment with n &gt; 1. 4Interestingly, this is actually even the case for approaches that directly model alignments between phrases (Marcu and Wong, 2002; Birch et al., 2006). 305 pair – even for closely related languages, we expect improvements to be seen. The notation however assume Chinese–English MT. Given a Chinese sentence cJ1 consisting of J words fc1, ... , cJI and an English sentence eI1 consisting of I words fe1, ... , eII, AC→E (resp. AE→C) will denote a Chinese-to-English (resp. an English-to-Chinese) word alignment between cJ1 and eI1. Since we are primarily interested in 1-to-n alignments, AC→E can be represented as a set of pairs aj = (cj, Ej) denoting a link between one single Chinese word cj and a few English words Ej (and similarly for AE→C). The</context>
</contexts>
<marker>Birch, Callison-Burch, Osborne, 2006</marker>
<rawString>Alexandra Birch, Chris Callison-Burch, and Miles Osborne. 2006. Constraining the phrase-based, joint probability statistical translation model. In Proceedings ofAMTA 2006, pages 10–18, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="959" citStr="Brown et al., 1993" startWordPosition="142" endWordPosition="145">acking several consecutive words together when we believe they correspond to a single word in the opposite language. This is done using the word aligner itself, i.e. by bootstrapping on its output. We evaluate the performance of our approach on a Chinese-to-English machine translation task, and report a 12.2% relative increase in BLEU score over a state-of-the art phrasebased SMT system. 1 Introduction Automatic word alignment can be defined as the problem of determining a translational correspondence at word level given a parallel corpus of aligned sentences. Most current statistical models (Brown et al., 1993; Vogel et al., 1996; Deng and Byrne, 2005) treat the aligned sentences in the corpus as sequences of tokens that are meant to be words; the goal of the alignment process is to find links between source and target words. Before applying such aligners, we thus need to segment the sentences into words – a task which can be quite hard for languages such as Chinese for which word boundaries are not orthographically marked. More importantly, however, this segmentation is often performed in a monolingual context, which makes the word alignment task more difficult since different languages may realiz</context>
<context position="4567" citStr="Brown et al., 1993" startWordPosition="734" endWordPosition="737">tion 5, we evaluate the influence of our method on the alignment process on a Chinese to English MT task, and experimental results are presented. Section 6 concludes the paper and gives avenues for future work. 2 The Case of 1-to-n Alignment The same concept can be expressed in different languages using varying numbers of words; for example, a single Chinese word may surface as a compound or a collocation in English. This is frequent for languages as different as Chinese and English. To quickly (and approximately) evaluate this phenomenon, we trained the statistical IBM wordalignment model 4 (Brown et al., 1993),1 using the GIZA++ software (Och and Ney, 2003) for the following language pairs: Chinese–English, Italian– English, and Dutch–English, using the IWSLT-2006 corpus (Takezawa et al., 2002; Paul, 2006) for the first two language pairs, and the Europarl corpus (Koehn, 2005) for the last one. These asymmetric models produce 1-to-n alignments, with n &gt; 0, in both directions. Here, it is important to mention that the segmentation of sentences is performed totally independently of the bilingual alignment process, i.e. it is done in a monolingual context. For European languages, we apply the maximum-</context>
<context position="6147" citStr="Brown et al., 1993" startWordPosition="994" endWordPosition="997">ns of Model 4. 2http://opennlp.sourceforge.net/. alignments with n 7� 1 is high for Chinese–English (^- 40%), and significantly higher than for the European languages. The case of 1-to-n alignments is, therefore, obviously an important issue when dealing with Chinese–English word alignment.3 2.1 The Treatment of 1-to-n Alignments Fertility-based models such as IBM models 3, 4, and 5 allow for alignments between one word and several words (1-to-n or 1: n alignments in what follows), in particular for the reasons specified above. They can be seen as extensions of the simpler IBM models 1 and 2 (Brown et al., 1993). Similarly, Deng and Byrne (2005) propose an HMM framework capable of dealing with 1-to-n alignment, which is an extension of the original model of (Vogel et al., 1996). However, these models rarely question the monolingual tokenization, i.e. the basic unit of the alignment process is the word.4 One alternative to extending the expressivity of one model (and usually its complexity) is to focus on the input representation; in particular, we argue that the alignment process can benefit from a simplification of the input, which consists of trying to reduce the number of 1-to-n alignments to cons</context>
<context position="17356" citStr="Brown et al., 1993" startWordPosition="2899" endWordPosition="2902">re-processing step, the English sentences were tokenized using the maximum-entropy based tokenizer of the OpenNLP toolkit, and case information was removed. For Chinese, the data provided were tokenized according to the output format of ASR systems, and human-corrected (Paul, 2006). Since segmentations are human-corrected, we are sure that they are good from a monolingual point of view. Table 2 contains the various corpus statistics. 4.3 Baseline We use a standard log-linear phrase-based statistical machine translation system as a baseline: GIZA++ implementation of IBM word alignment model 4 (Brown et al., 1993; Och and Ney, 2003),8 the refinement and phrase-extraction heuristics described in (Koehn et al., 2003), minimum-error-rate training 7More specifically, we choose the first English reference from the 7 references and the Chinese sentence to construct new sentence pairs. 8Training is performed using the same number of iterations as in Section 2. Chinese English Train Sentences 41,465 Running words 361,780 375,938 Vocabulary size 11,427 9,851 Dev. Sentences 289 (7 refs.) Running words 3,350 26,223 Vocabulary size 897 1,331 Eval. Sentences 200 (7 refs.) Running words 1,864 14,437 Vocabulary size</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yonggang Deng</author>
<author>William Byrne</author>
</authors>
<title>HMM word and phrase alignment for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP</booktitle>
<pages>169--176</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="1002" citStr="Deng and Byrne, 2005" startWordPosition="150" endWordPosition="153">r when we believe they correspond to a single word in the opposite language. This is done using the word aligner itself, i.e. by bootstrapping on its output. We evaluate the performance of our approach on a Chinese-to-English machine translation task, and report a 12.2% relative increase in BLEU score over a state-of-the art phrasebased SMT system. 1 Introduction Automatic word alignment can be defined as the problem of determining a translational correspondence at word level given a parallel corpus of aligned sentences. Most current statistical models (Brown et al., 1993; Vogel et al., 1996; Deng and Byrne, 2005) treat the aligned sentences in the corpus as sequences of tokens that are meant to be words; the goal of the alignment process is to find links between source and target words. Before applying such aligners, we thus need to segment the sentences into words – a task which can be quite hard for languages such as Chinese for which word boundaries are not orthographically marked. More importantly, however, this segmentation is often performed in a monolingual context, which makes the word alignment task more difficult since different languages may realize the same concept using varying numbers of</context>
<context position="6181" citStr="Deng and Byrne (2005)" startWordPosition="999" endWordPosition="1002">ourceforge.net/. alignments with n 7� 1 is high for Chinese–English (^- 40%), and significantly higher than for the European languages. The case of 1-to-n alignments is, therefore, obviously an important issue when dealing with Chinese–English word alignment.3 2.1 The Treatment of 1-to-n Alignments Fertility-based models such as IBM models 3, 4, and 5 allow for alignments between one word and several words (1-to-n or 1: n alignments in what follows), in particular for the reasons specified above. They can be seen as extensions of the simpler IBM models 1 and 2 (Brown et al., 1993). Similarly, Deng and Byrne (2005) propose an HMM framework capable of dealing with 1-to-n alignment, which is an extension of the original model of (Vogel et al., 1996). However, these models rarely question the monolingual tokenization, i.e. the basic unit of the alignment process is the word.4 One alternative to extending the expressivity of one model (and usually its complexity) is to focus on the input representation; in particular, we argue that the alignment process can benefit from a simplification of the input, which consists of trying to reduce the number of 1-to-n alignments to consider. Note that the need to consid</context>
</contexts>
<marker>Deng, Byrne, 2005</marker>
<rawString>Yonggang Deng and William Byrne. 2005. HMM word and phrase alignment for statistical machine translation. In Proceedings of HLT-EMNLP 2005, pages 169–176, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
<author>Daniel Marcu</author>
</authors>
<title>Measuring word alignment quality for statistical machine translation.</title>
<date>2006</date>
<tech>Technical Report ISI-TR-616,</tech>
<institution>ISI/University of Southern California.</institution>
<contexts>
<context position="15706" citStr="Fraser and Marcu, 2006" startWordPosition="2639" endWordPosition="2642">as different as Chinese and English. we have grouped some words together, they become Second, an increase in AER does not necessarily imthe new basic unit to consider, and we can re-run ply an improvement in translation quality (Liang et the same method to get additional groupings. How- al., 2006) and vice-versa (Vilar et al., 2006). The 6In case of overlap between several groups of words to replace, we select the one with highest confidence (according to tac). 307 relationship between word alignments and their impact on MT is also investigated in (Ayan and Dorr, 2006; Lopez and Resnik, 2006; Fraser and Marcu, 2006). Consequently, we chose to extrinsically evaluate the performance of our approach via the translation task, i.e. we measure the influence of the alignment process on the final translation output. The quality of the translation output is evaluated using BLEU (Papineni et al., 2002). 4.2 Data The experiments were carried out using the Chinese–English datasets provided within the IWSLT 2006 evaluation campaign (Paul, 2006), extracted from the Basic Travel Expression Corpus (BTEC) (Takezawa et al., 2002). This multilingual speech corpus contains sentences similar to those that are usually found i</context>
</contexts>
<marker>Fraser, Marcu, 2006</marker>
<rawString>Alexander Fraser and Daniel Marcu. 2006. Measuring word alignment quality for statistical machine translation. Technical Report ISI-TR-616, ISI/University of Southern California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihoko Kitamura</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Automatic extraction of word sequence correspondences in parallel corpora.</title>
<date>1996</date>
<booktitle>In Proceedings of the 4th Workshop on Very Large Corpora,</booktitle>
<pages>79--87</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="2678" citStr="Kitamura and Matsumoto, 1996" startWordPosition="425" endWordPosition="428">k of automatic word aligners by packing several consecutive words together when we believe they correspond to a single word in the opposite language; by identifying enough such cases, we reduce the number of 1-to-n alignments, thus making the task of word alignment both easier and more natural. Our approach consists of using the output from an existing statistical word aligner to obtain a set of candidates for word packing. We evaluate the reliability of these candidates, using simple metrics based on co-occurence frequencies, similar to those used in associative approaches to word alignment (Kitamura and Matsumoto, 1996; Melamed, 2000; Tiedemann, 2003). We then modify the segmentation of the sentences in the parallel corpus according to this packing of words; these modified sentences are then given back to the word aligner, which produces new alignments. We evaluate the validity of our approach by measuring the influence of the alignment process on a Chinese-to-English Machine Translation (MT) task. The remainder of this paper is organized as follows. In Section 2, we study the case of 1-ton word alignment. Section 3 introduces an automatic method to pack together groups of consecutive 304 Proceedings of the</context>
</contexts>
<marker>Kitamura, Matsumoto, 1996</marker>
<rawString>Mihoko Kitamura and Yuji Matsumoto. 1996. Automatic extraction of word sequence correspondences in parallel corpora. In Proceedings of the 4th Workshop on Very Large Corpora, pages 79–87, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Koehn</author>
<author>Franz Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL 2003,</booktitle>
<pages>48--54</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="17460" citStr="Koehn et al., 2003" startWordPosition="2915" endWordPosition="2918">he OpenNLP toolkit, and case information was removed. For Chinese, the data provided were tokenized according to the output format of ASR systems, and human-corrected (Paul, 2006). Since segmentations are human-corrected, we are sure that they are good from a monolingual point of view. Table 2 contains the various corpus statistics. 4.3 Baseline We use a standard log-linear phrase-based statistical machine translation system as a baseline: GIZA++ implementation of IBM word alignment model 4 (Brown et al., 1993; Och and Ney, 2003),8 the refinement and phrase-extraction heuristics described in (Koehn et al., 2003), minimum-error-rate training 7More specifically, we choose the first English reference from the 7 references and the Chinese sentence to construct new sentence pairs. 8Training is performed using the same number of iterations as in Section 2. Chinese English Train Sentences 41,465 Running words 361,780 375,938 Vocabulary size 11,427 9,851 Dev. Sentences 289 (7 refs.) Running words 3,350 26,223 Vocabulary size 897 1,331 Eval. Sentences 200 (7 refs.) Running words 1,864 14,437 Vocabulary size 569 1,081 Table 2: Chinese–English corpus statistics (Och, 2003) using Phramer (Olteanu et al., 2006), </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philip Koehn, Franz Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of HLT-NAACL 2003, pages 48–54, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Koehn</author>
</authors>
<title>Pharaoh: A beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In Proceedings of AMTA 2004,</booktitle>
<pages>115--124</pages>
<location>Washington, District of Columbia.</location>
<contexts>
<context position="18211" citStr="Koehn, 2004" startWordPosition="3031" endWordPosition="3032">onstruct new sentence pairs. 8Training is performed using the same number of iterations as in Section 2. Chinese English Train Sentences 41,465 Running words 361,780 375,938 Vocabulary size 11,427 9,851 Dev. Sentences 289 (7 refs.) Running words 3,350 26,223 Vocabulary size 897 1,331 Eval. Sentences 200 (7 refs.) Running words 1,864 14,437 Vocabulary size 569 1,081 Table 2: Chinese–English corpus statistics (Och, 2003) using Phramer (Olteanu et al., 2006), a 3-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002) on the English side of the training data and Pharaoh (Koehn, 2004) with default settings to decode. The log-linear model is also based on standard features: conditional probabilities and lexical smoothing of phrases in both directions, and phrase penalty (Zens and Ney, 2004). 5 Experimental Results 5.1 Results The initial word alignments are obtained using the baseline configuration described above. From these, we build two bilingual 1-to-n dictionaries (one for each direction), and the training corpus is updated by repacking the words in the dictionaries, using the method presented in Section 2. As previously mentioned, this process can be repeated several </context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philip Koehn. 2004. Pharaoh: A beam search decoder for phrase-based statistical machine translation models. In Proceedings of AMTA 2004, pages 115–124, Washington, District of Columbia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Machine Translation Summit X,</booktitle>
<pages>79--86</pages>
<location>Phuket, Thailand.</location>
<contexts>
<context position="4839" citStr="Koehn, 2005" startWordPosition="777" endWordPosition="778">different languages using varying numbers of words; for example, a single Chinese word may surface as a compound or a collocation in English. This is frequent for languages as different as Chinese and English. To quickly (and approximately) evaluate this phenomenon, we trained the statistical IBM wordalignment model 4 (Brown et al., 1993),1 using the GIZA++ software (Och and Ney, 2003) for the following language pairs: Chinese–English, Italian– English, and Dutch–English, using the IWSLT-2006 corpus (Takezawa et al., 2002; Paul, 2006) for the first two language pairs, and the Europarl corpus (Koehn, 2005) for the last one. These asymmetric models produce 1-to-n alignments, with n &gt; 0, in both directions. Here, it is important to mention that the segmentation of sentences is performed totally independently of the bilingual alignment process, i.e. it is done in a monolingual context. For European languages, we apply the maximum-entropy based tokenizer of OpenNLP2; the Chinese sentences were human segmented (Paul, 2006). In Table 1, we report the frequencies of the different types of alignments for the various languages and directions. As expected, the number of 1: n 1More specifically, we perfor</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Machine Translation Summit X, pages 79–86, Phuket, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL 2006,</booktitle>
<pages>104--111</pages>
<location>New York, NY.</location>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of HLT-NAACL 2006, pages 104–111, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
<author>Philip Resnik</author>
</authors>
<title>Word-based alignment, phrase-based translation: What’s the link?</title>
<date>2006</date>
<booktitle>In Proceedings of AMTA</booktitle>
<pages>90--99</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="15681" citStr="Lopez and Resnik, 2006" startWordPosition="2635" endWordPosition="2638">mes: once for languages as different as Chinese and English. we have grouped some words together, they become Second, an increase in AER does not necessarily imthe new basic unit to consider, and we can re-run ply an improvement in translation quality (Liang et the same method to get additional groupings. How- al., 2006) and vice-versa (Vilar et al., 2006). The 6In case of overlap between several groups of words to replace, we select the one with highest confidence (according to tac). 307 relationship between word alignments and their impact on MT is also investigated in (Ayan and Dorr, 2006; Lopez and Resnik, 2006; Fraser and Marcu, 2006). Consequently, we chose to extrinsically evaluate the performance of our approach via the translation task, i.e. we measure the influence of the alignment process on the final translation output. The quality of the translation output is evaluated using BLEU (Papineni et al., 2002). 4.2 Data The experiments were carried out using the Chinese–English datasets provided within the IWSLT 2006 evaluation campaign (Paul, 2006), extracted from the Basic Travel Expression Corpus (BTEC) (Takezawa et al., 2002). This multilingual speech corpus contains sentences similar to those</context>
</contexts>
<marker>Lopez, Resnik, 2006</marker>
<rawString>Adam Lopez and Philip Resnik. 2006. Word-based alignment, phrase-based translation: What’s the link? In Proceedings of AMTA 2006, pages 90–99, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>William Wong</author>
</authors>
<title>A phrase-based, joint probability model for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings ofEMNLP 2002,</booktitle>
<pages>133--139</pages>
<location>Morristown, NJ.</location>
<contexts>
<context position="7247" citStr="Marcu and Wong, 2002" startWordPosition="1180" endWordPosition="1183">n benefit from a simplification of the input, which consists of trying to reduce the number of 1-to-n alignments to consider. Note that the need to consider segmentation and alignment at the same time is also mentioned in (Tiedemann, 2003), and related issues are reported in (Wu, 1997). 2.2 Notation While in this paper, we focus on Chinese–English, the method proposed is applicable to any language 3Note that a 1: 0 alignment may denote a failure to capture a 1: n alignment with n &gt; 1. 4Interestingly, this is actually even the case for approaches that directly model alignments between phrases (Marcu and Wong, 2002; Birch et al., 2006). 305 pair – even for closely related languages, we expect improvements to be seen. The notation however assume Chinese–English MT. Given a Chinese sentence cJ1 consisting of J words fc1, ... , cJI and an English sentence eI1 consisting of I words fe1, ... , eII, AC→E (resp. AE→C) will denote a Chinese-to-English (resp. an English-to-Chinese) word alignment between cJ1 and eI1. Since we are primarily interested in 1-to-n alignments, AC→E can be represented as a set of pairs aj = (cj, Ej) denoting a link between one single Chinese word cj and a few English words Ej (and sim</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>Daniel Marcu and William Wong. 2002. A phrase-based, joint probability model for statistical machine translation. In Proceedings ofEMNLP 2002, pages 133–139, Morristown, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Automatic discovery of noncompositional compounds in parallel data.</title>
<date>1997</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>97--108</pages>
<location>Somerset, New Jersey.</location>
<contexts>
<context position="10667" citStr="Melamed, 1997" startWordPosition="1773" endWordPosition="1774">d if we want to change the input to give to the word aligner, we need to make sure that we are not making harmful modifications.5 We thus additionally evaluate the reliability of the candidates we extract and filter them before inclusion in our bilingual dictionary. To perform this filtering, we use two simple statistical measures. In the following, aj = (cj, Ej) denotes a candidate. The first measure we consider is co-occurrence frequency (COOC(cj, Ej)), i.e. the number of times cj and Ej co-occur in the bilingual corpus. This very simple measure is frequently used in associative approaches (Melamed, 1997; Tiedemann, 2003). The second measure is the alignment confidence, defined as C(aj) AC(aj) = COOC(cj, Ej), where C(aj) denotes the number of alignments proposed by the word aligner that are identical to aj. In other words, AC(aj) measures how often the 5Consequently, if we compare our approach to the problem of collocation identification, we may say that we are more interested in precision than recall (Smadja et al., 1996). However, note that our goal is not recognizing specific sequences of words such as compounds or collocations; it is making (bilingually motivated) changes that simplify th</context>
</contexts>
<marker>Melamed, 1997</marker>
<rawString>I. Dan Melamed. 1997. Automatic discovery of noncompositional compounds in parallel data. In Proceedings of EMNLP 1997, pages 97–108, Somerset, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Models of translational equivalence among words.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>2</issue>
<contexts>
<context position="2693" citStr="Melamed, 2000" startWordPosition="429" endWordPosition="430">y packing several consecutive words together when we believe they correspond to a single word in the opposite language; by identifying enough such cases, we reduce the number of 1-to-n alignments, thus making the task of word alignment both easier and more natural. Our approach consists of using the output from an existing statistical word aligner to obtain a set of candidates for word packing. We evaluate the reliability of these candidates, using simple metrics based on co-occurence frequencies, similar to those used in associative approaches to word alignment (Kitamura and Matsumoto, 1996; Melamed, 2000; Tiedemann, 2003). We then modify the segmentation of the sentences in the parallel corpus according to this packing of words; these modified sentences are then given back to the word aligner, which produces new alignments. We evaluate the validity of our approach by measuring the influence of the alignment process on a Chinese-to-English Machine Translation (MT) task. The remainder of this paper is organized as follows. In Section 2, we study the case of 1-ton word alignment. Section 3 introduces an automatic method to pack together groups of consecutive 304 Proceedings of the 45th Annual Me</context>
</contexts>
<marker>Melamed, 2000</marker>
<rawString>I. Dan Melamed. 2000. Models of translational equivalence among words. Computational Linguistics, 26(2):221–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="4615" citStr="Och and Ney, 2003" startWordPosition="742" endWordPosition="745">n the alignment process on a Chinese to English MT task, and experimental results are presented. Section 6 concludes the paper and gives avenues for future work. 2 The Case of 1-to-n Alignment The same concept can be expressed in different languages using varying numbers of words; for example, a single Chinese word may surface as a compound or a collocation in English. This is frequent for languages as different as Chinese and English. To quickly (and approximately) evaluate this phenomenon, we trained the statistical IBM wordalignment model 4 (Brown et al., 1993),1 using the GIZA++ software (Och and Ney, 2003) for the following language pairs: Chinese–English, Italian– English, and Dutch–English, using the IWSLT-2006 corpus (Takezawa et al., 2002; Paul, 2006) for the first two language pairs, and the Europarl corpus (Koehn, 2005) for the last one. These asymmetric models produce 1-to-n alignments, with n &gt; 0, in both directions. Here, it is important to mention that the segmentation of sentences is performed totally independently of the bilingual alignment process, i.e. it is done in a monolingual context. For European languages, we apply the maximum-entropy based tokenizer of OpenNLP2; the Chinese</context>
<context position="14490" citStr="Och and Ney, 2003" startWordPosition="2440" endWordPosition="2443">he sequence 看: look at of words Ej with a single token which becomes a Figure 2: Examples of entries from the manually denew lexical unit.6 Note that this replacement occurs veloped dictionary even if no alignment was found between cj and Ej 4 Experimental Setting for the pair (ci , ei). This is motivated by the fact 4.1 Evaluation that the filtering described above is quite conserva- The intrinsic quality of word alignment can be astive; we trust the entry ai to be correct. This update sessed using the Alignment Error Rate (AER) metis performed in both directions. It is then possible to ric (Och and Ney, 2003), that compares a system’s run the word aligner using the updated (simplified) alignment output to a set of gold-standard alignparallel corpus, in order to get new alignments. By ment. While this method gives a direct evaluation of performing a deterministic word packing, we avoid the quality of word alignment, it is faced with sevthe computation of the fertility parameters associ- eral limitations. First, it is really difficult to build ated with fertility-based models. a reliable and objective gold-standard set, especially Word packing can be applied several times: once for languages as diff</context>
<context position="17376" citStr="Och and Ney, 2003" startWordPosition="2903" endWordPosition="2906">the English sentences were tokenized using the maximum-entropy based tokenizer of the OpenNLP toolkit, and case information was removed. For Chinese, the data provided were tokenized according to the output format of ASR systems, and human-corrected (Paul, 2006). Since segmentations are human-corrected, we are sure that they are good from a monolingual point of view. Table 2 contains the various corpus statistics. 4.3 Baseline We use a standard log-linear phrase-based statistical machine translation system as a baseline: GIZA++ implementation of IBM word alignment model 4 (Brown et al., 1993; Och and Ney, 2003),8 the refinement and phrase-extraction heuristics described in (Koehn et al., 2003), minimum-error-rate training 7More specifically, we choose the first English reference from the 7 references and the Chinese sentence to construct new sentence pairs. 8Training is performed using the same number of iterations as in Section 2. Chinese English Train Sentences 41,465 Running words 361,780 375,938 Vocabulary size 11,427 9,851 Dev. Sentences 289 (7 refs.) Running words 3,350 26,223 Vocabulary size 897 1,331 Eval. Sentences 200 (7 refs.) Running words 1,864 14,437 Vocabulary size 569 1,081 Table 2: </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL 2003,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="18021" citStr="Och, 2003" startWordPosition="2999" endWordPosition="3000">tion heuristics described in (Koehn et al., 2003), minimum-error-rate training 7More specifically, we choose the first English reference from the 7 references and the Chinese sentence to construct new sentence pairs. 8Training is performed using the same number of iterations as in Section 2. Chinese English Train Sentences 41,465 Running words 361,780 375,938 Vocabulary size 11,427 9,851 Dev. Sentences 289 (7 refs.) Running words 3,350 26,223 Vocabulary size 897 1,331 Eval. Sentences 200 (7 refs.) Running words 1,864 14,437 Vocabulary size 569 1,081 Table 2: Chinese–English corpus statistics (Och, 2003) using Phramer (Olteanu et al., 2006), a 3-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002) on the English side of the training data and Pharaoh (Koehn, 2004) with default settings to decode. The log-linear model is also based on standard features: conditional probabilities and lexical smoothing of phrases in both directions, and phrase penalty (Zens and Ney, 2004). 5 Experimental Results 5.1 Results The initial word alignments are obtained using the baseline configuration described above. From these, we build two bilingual 1-to-n dictionaries (one for each dire</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings ofACL 2003, pages 160–167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marian Olteanu</author>
<author>Chris Davis</author>
<author>Ionut Volosen</author>
<author>Dan Moldovan</author>
</authors>
<title>Phramer - an open source statistical phrase-based translator.</title>
<date>2006</date>
<booktitle>In Proceedings of the NAACL 2006 Workshop on Statistical Machine Translation,</booktitle>
<pages>146--149</pages>
<location>New York, NY.</location>
<contexts>
<context position="18058" citStr="Olteanu et al., 2006" startWordPosition="3003" endWordPosition="3006">in (Koehn et al., 2003), minimum-error-rate training 7More specifically, we choose the first English reference from the 7 references and the Chinese sentence to construct new sentence pairs. 8Training is performed using the same number of iterations as in Section 2. Chinese English Train Sentences 41,465 Running words 361,780 375,938 Vocabulary size 11,427 9,851 Dev. Sentences 289 (7 refs.) Running words 3,350 26,223 Vocabulary size 897 1,331 Eval. Sentences 200 (7 refs.) Running words 1,864 14,437 Vocabulary size 569 1,081 Table 2: Chinese–English corpus statistics (Och, 2003) using Phramer (Olteanu et al., 2006), a 3-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002) on the English side of the training data and Pharaoh (Koehn, 2004) with default settings to decode. The log-linear model is also based on standard features: conditional probabilities and lexical smoothing of phrases in both directions, and phrase penalty (Zens and Ney, 2004). 5 Experimental Results 5.1 Results The initial word alignments are obtained using the baseline configuration described above. From these, we build two bilingual 1-to-n dictionaries (one for each direction), and the training corpus is up</context>
</contexts>
<marker>Olteanu, Davis, Volosen, Moldovan, 2006</marker>
<rawString>Marian Olteanu, Chris Davis, Ionut Volosen, and Dan Moldovan. 2006. Phramer - an open source statistical phrase-based translator. In Proceedings of the NAACL 2006 Workshop on Statistical Machine Translation, pages 146–149, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL 2002,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="15988" citStr="Papineni et al., 2002" startWordPosition="2685" endWordPosition="2688">How- al., 2006) and vice-versa (Vilar et al., 2006). The 6In case of overlap between several groups of words to replace, we select the one with highest confidence (according to tac). 307 relationship between word alignments and their impact on MT is also investigated in (Ayan and Dorr, 2006; Lopez and Resnik, 2006; Fraser and Marcu, 2006). Consequently, we chose to extrinsically evaluate the performance of our approach via the translation task, i.e. we measure the influence of the alignment process on the final translation output. The quality of the translation output is evaluated using BLEU (Papineni et al., 2002). 4.2 Data The experiments were carried out using the Chinese–English datasets provided within the IWSLT 2006 evaluation campaign (Paul, 2006), extracted from the Basic Travel Expression Corpus (BTEC) (Takezawa et al., 2002). This multilingual speech corpus contains sentences similar to those that are usually found in phrase-books for tourists going abroad. Training was performed using the default training set, to which we added the sets devset1, devset2, and devset3.7 The English side of the test set was not available at the time we conducted our experiments, so we split the development set (</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings ofACL 2002, pages 311–318, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Paul</author>
</authors>
<title>Overview of the IWSLT</title>
<date>2006</date>
<booktitle>In Proceedings of IWSLT 2006,</booktitle>
<pages>1--15</pages>
<location>Kyoto, Japan.</location>
<contexts>
<context position="4767" citStr="Paul, 2006" startWordPosition="765" endWordPosition="766">k. 2 The Case of 1-to-n Alignment The same concept can be expressed in different languages using varying numbers of words; for example, a single Chinese word may surface as a compound or a collocation in English. This is frequent for languages as different as Chinese and English. To quickly (and approximately) evaluate this phenomenon, we trained the statistical IBM wordalignment model 4 (Brown et al., 1993),1 using the GIZA++ software (Och and Ney, 2003) for the following language pairs: Chinese–English, Italian– English, and Dutch–English, using the IWSLT-2006 corpus (Takezawa et al., 2002; Paul, 2006) for the first two language pairs, and the Europarl corpus (Koehn, 2005) for the last one. These asymmetric models produce 1-to-n alignments, with n &gt; 0, in both directions. Here, it is important to mention that the segmentation of sentences is performed totally independently of the bilingual alignment process, i.e. it is done in a monolingual context. For European languages, we apply the maximum-entropy based tokenizer of OpenNLP2; the Chinese sentences were human segmented (Paul, 2006). In Table 1, we report the frequencies of the different types of alignments for the various languages and d</context>
<context position="16130" citStr="Paul, 2006" startWordPosition="2707" endWordPosition="2708">onfidence (according to tac). 307 relationship between word alignments and their impact on MT is also investigated in (Ayan and Dorr, 2006; Lopez and Resnik, 2006; Fraser and Marcu, 2006). Consequently, we chose to extrinsically evaluate the performance of our approach via the translation task, i.e. we measure the influence of the alignment process on the final translation output. The quality of the translation output is evaluated using BLEU (Papineni et al., 2002). 4.2 Data The experiments were carried out using the Chinese–English datasets provided within the IWSLT 2006 evaluation campaign (Paul, 2006), extracted from the Basic Travel Expression Corpus (BTEC) (Takezawa et al., 2002). This multilingual speech corpus contains sentences similar to those that are usually found in phrase-books for tourists going abroad. Training was performed using the default training set, to which we added the sets devset1, devset2, and devset3.7 The English side of the test set was not available at the time we conducted our experiments, so we split the development set (devset 4) into two parts: one was kept for testing (200 aligned sentences) with the rest (289 aligned sentences) used for development purposes</context>
</contexts>
<marker>Paul, 2006</marker>
<rawString>Michael Paul. 2006. Overview of the IWSLT 2006 Evaluation Campaign. In Proceedings of IWSLT 2006, pages 1–15, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Smadja</author>
<author>Kathleen R McKeown</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Translating collocations for bilingual lexicons: A statistical approach.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="11094" citStr="Smadja et al., 1996" startWordPosition="1843" endWordPosition="1846">co-occurrence frequency (COOC(cj, Ej)), i.e. the number of times cj and Ej co-occur in the bilingual corpus. This very simple measure is frequently used in associative approaches (Melamed, 1997; Tiedemann, 2003). The second measure is the alignment confidence, defined as C(aj) AC(aj) = COOC(cj, Ej), where C(aj) denotes the number of alignments proposed by the word aligner that are identical to aj. In other words, AC(aj) measures how often the 5Consequently, if we compare our approach to the problem of collocation identification, we may say that we are more interested in precision than recall (Smadja et al., 1996). However, note that our goal is not recognizing specific sequences of words such as compounds or collocations; it is making (bilingually motivated) changes that simplify the alignment process. closest: � it fifteen: -}- -E fine: * 4-f flight: �k &amp;YE get: * i&apos;1 here: A it_W_ 306 aligner aligns cj and Ej when they co-occur. We ever, we have not seen in practice much benefit from also impose that |Ej |G k, where k is a fixed inte- running it more than twice (few new candidates are ger that may depend on the language pair (between extracted after two iterations). 3 and 5 in practice). The rationa</context>
</contexts>
<marker>Smadja, McKeown, Hatzivassiloglou, 1996</marker>
<rawString>Frank Smadja, Kathleen R. McKeown, and Vasileios Hatzivassiloglou. 1996. Translating collocations for bilingual lexicons: A statistical approach. Computational Linguistics, 22(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Stolcke</author>
</authors>
<title>SRILM – An extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<location>Denver, Colorado.</location>
<contexts>
<context position="18144" citStr="Stolcke, 2002" startWordPosition="3018" endWordPosition="3019">English reference from the 7 references and the Chinese sentence to construct new sentence pairs. 8Training is performed using the same number of iterations as in Section 2. Chinese English Train Sentences 41,465 Running words 361,780 375,938 Vocabulary size 11,427 9,851 Dev. Sentences 289 (7 refs.) Running words 3,350 26,223 Vocabulary size 897 1,331 Eval. Sentences 200 (7 refs.) Running words 1,864 14,437 Vocabulary size 569 1,081 Table 2: Chinese–English corpus statistics (Och, 2003) using Phramer (Olteanu et al., 2006), a 3-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002) on the English side of the training data and Pharaoh (Koehn, 2004) with default settings to decode. The log-linear model is also based on standard features: conditional probabilities and lexical smoothing of phrases in both directions, and phrase penalty (Zens and Ney, 2004). 5 Experimental Results 5.1 Results The initial word alignments are obtained using the baseline configuration described above. From these, we build two bilingual 1-to-n dictionaries (one for each direction), and the training corpus is updated by repacking the words in the dictionaries, using the method presented in Sectio</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andrea Stolcke. 2002. SRILM – An extensible language modeling toolkit. In Proceedings of the International Conference on Spoken Language Processing, pages 901–904, Denver, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Takezawa</author>
<author>E Sumita</author>
<author>F Sugaya</author>
<author>H Yamamoto</author>
<author>S Yamamoto</author>
</authors>
<title>Toward a broad-coverage bilingual corpus for speech translation of travel conversations in the real world.</title>
<date>2002</date>
<booktitle>In Proceedings ofLREC 2002,</booktitle>
<pages>147--152</pages>
<location>Las Palmas,</location>
<contexts>
<context position="4754" citStr="Takezawa et al., 2002" startWordPosition="761" endWordPosition="764"> avenues for future work. 2 The Case of 1-to-n Alignment The same concept can be expressed in different languages using varying numbers of words; for example, a single Chinese word may surface as a compound or a collocation in English. This is frequent for languages as different as Chinese and English. To quickly (and approximately) evaluate this phenomenon, we trained the statistical IBM wordalignment model 4 (Brown et al., 1993),1 using the GIZA++ software (Och and Ney, 2003) for the following language pairs: Chinese–English, Italian– English, and Dutch–English, using the IWSLT-2006 corpus (Takezawa et al., 2002; Paul, 2006) for the first two language pairs, and the Europarl corpus (Koehn, 2005) for the last one. These asymmetric models produce 1-to-n alignments, with n &gt; 0, in both directions. Here, it is important to mention that the segmentation of sentences is performed totally independently of the bilingual alignment process, i.e. it is done in a monolingual context. For European languages, we apply the maximum-entropy based tokenizer of OpenNLP2; the Chinese sentences were human segmented (Paul, 2006). In Table 1, we report the frequencies of the different types of alignments for the various la</context>
<context position="16212" citStr="Takezawa et al., 2002" startWordPosition="2718" endWordPosition="2721">and their impact on MT is also investigated in (Ayan and Dorr, 2006; Lopez and Resnik, 2006; Fraser and Marcu, 2006). Consequently, we chose to extrinsically evaluate the performance of our approach via the translation task, i.e. we measure the influence of the alignment process on the final translation output. The quality of the translation output is evaluated using BLEU (Papineni et al., 2002). 4.2 Data The experiments were carried out using the Chinese–English datasets provided within the IWSLT 2006 evaluation campaign (Paul, 2006), extracted from the Basic Travel Expression Corpus (BTEC) (Takezawa et al., 2002). This multilingual speech corpus contains sentences similar to those that are usually found in phrase-books for tourists going abroad. Training was performed using the default training set, to which we added the sets devset1, devset2, and devset3.7 The English side of the test set was not available at the time we conducted our experiments, so we split the development set (devset 4) into two parts: one was kept for testing (200 aligned sentences) with the rest (289 aligned sentences) used for development purposes. As a pre-processing step, the English sentences were tokenized using the maximum</context>
</contexts>
<marker>Takezawa, Sumita, Sugaya, Yamamoto, Yamamoto, 2002</marker>
<rawString>T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, and S. Yamamoto. 2002. Toward a broad-coverage bilingual corpus for speech translation of travel conversations in the real world. In Proceedings ofLREC 2002, pages 147–152, Las Palmas, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
</authors>
<title>Combining clues for word alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of EACL 2003,</booktitle>
<pages>339--346</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="2711" citStr="Tiedemann, 2003" startWordPosition="431" endWordPosition="432">al consecutive words together when we believe they correspond to a single word in the opposite language; by identifying enough such cases, we reduce the number of 1-to-n alignments, thus making the task of word alignment both easier and more natural. Our approach consists of using the output from an existing statistical word aligner to obtain a set of candidates for word packing. We evaluate the reliability of these candidates, using simple metrics based on co-occurence frequencies, similar to those used in associative approaches to word alignment (Kitamura and Matsumoto, 1996; Melamed, 2000; Tiedemann, 2003). We then modify the segmentation of the sentences in the parallel corpus according to this packing of words; these modified sentences are then given back to the word aligner, which produces new alignments. We evaluate the validity of our approach by measuring the influence of the alignment process on a Chinese-to-English Machine Translation (MT) task. The remainder of this paper is organized as follows. In Section 2, we study the case of 1-ton word alignment. Section 3 introduces an automatic method to pack together groups of consecutive 304 Proceedings of the 45th Annual Meeting of the Assoc</context>
<context position="6866" citStr="Tiedemann, 2003" startWordPosition="1117" endWordPosition="1118">hich is an extension of the original model of (Vogel et al., 1996). However, these models rarely question the monolingual tokenization, i.e. the basic unit of the alignment process is the word.4 One alternative to extending the expressivity of one model (and usually its complexity) is to focus on the input representation; in particular, we argue that the alignment process can benefit from a simplification of the input, which consists of trying to reduce the number of 1-to-n alignments to consider. Note that the need to consider segmentation and alignment at the same time is also mentioned in (Tiedemann, 2003), and related issues are reported in (Wu, 1997). 2.2 Notation While in this paper, we focus on Chinese–English, the method proposed is applicable to any language 3Note that a 1: 0 alignment may denote a failure to capture a 1: n alignment with n &gt; 1. 4Interestingly, this is actually even the case for approaches that directly model alignments between phrases (Marcu and Wong, 2002; Birch et al., 2006). 305 pair – even for closely related languages, we expect improvements to be seen. The notation however assume Chinese–English MT. Given a Chinese sentence cJ1 consisting of J words fc1, ... , cJI </context>
<context position="10685" citStr="Tiedemann, 2003" startWordPosition="1775" endWordPosition="1776"> change the input to give to the word aligner, we need to make sure that we are not making harmful modifications.5 We thus additionally evaluate the reliability of the candidates we extract and filter them before inclusion in our bilingual dictionary. To perform this filtering, we use two simple statistical measures. In the following, aj = (cj, Ej) denotes a candidate. The first measure we consider is co-occurrence frequency (COOC(cj, Ej)), i.e. the number of times cj and Ej co-occur in the bilingual corpus. This very simple measure is frequently used in associative approaches (Melamed, 1997; Tiedemann, 2003). The second measure is the alignment confidence, defined as C(aj) AC(aj) = COOC(cj, Ej), where C(aj) denotes the number of alignments proposed by the word aligner that are identical to aj. In other words, AC(aj) measures how often the 5Consequently, if we compare our approach to the problem of collocation identification, we may say that we are more interested in precision than recall (Smadja et al., 1996). However, note that our goal is not recognizing specific sequences of words such as compounds or collocations; it is making (bilingually motivated) changes that simplify the alignment proces</context>
<context position="25279" citStr="Tiedemann, 2003" startWordPosition="4194" endWordPosition="4195">ve the improvement is sufficient enough to justify this additional effort. 6 Conclusion and Future Work In this paper, we have introduced a simple yet effective method to pack words together in order to give a different and simplified input to automatic word aligners. We use a bootstrap approach in which we first extract 1-to-n word alignments using an existing word aligner, and then estimate the confidence of those alignments to decide whether or not the n words have to be grouped; if so, this group is conwould thus be completely driven by the bilingual alignment process (see also (Wu, 1997; Tiedemann, 2003) for related considerations). In this case, our approach would be similar to the approach of (Xu et al., 2004), except for the estimation of candidates. sidered a new basic unit to consider. We can finally re-apply the word aligner to the updated sentences. We have evaluated the performance of our approach by measuring the influence of this process on a Chinese-to-English MT task, based on the IWSLT 2006 evaluation campaign. We report a 12.2% relative increase in BLEU score over a standard phrase-based SMT system. We have verified that this process actually reduces the number of 1: n alignment</context>
</contexts>
<marker>Tiedemann, 2003</marker>
<rawString>J¨org Tiedemann. 2003. Combining clues for word alignment. In Proceedings of EACL 2003, pages 339–346, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vilar</author>
<author>Maja Popovic</author>
<author>Hermann Ney</author>
</authors>
<title>AER: Do we need to ”improve” our alignments?</title>
<date>2006</date>
<booktitle>In Proceedings of IWSLT</booktitle>
<pages>205--212</pages>
<location>Kyoto, Japan.</location>
<contexts>
<context position="15417" citStr="Vilar et al., 2006" startWordPosition="2589" endWordPosition="2592">ent, it is faced with sevthe computation of the fertility parameters associ- eral limitations. First, it is really difficult to build ated with fertility-based models. a reliable and objective gold-standard set, especially Word packing can be applied several times: once for languages as different as Chinese and English. we have grouped some words together, they become Second, an increase in AER does not necessarily imthe new basic unit to consider, and we can re-run ply an improvement in translation quality (Liang et the same method to get additional groupings. How- al., 2006) and vice-versa (Vilar et al., 2006). The 6In case of overlap between several groups of words to replace, we select the one with highest confidence (according to tac). 307 relationship between word alignments and their impact on MT is also investigated in (Ayan and Dorr, 2006; Lopez and Resnik, 2006; Fraser and Marcu, 2006). Consequently, we chose to extrinsically evaluate the performance of our approach via the translation task, i.e. we measure the influence of the alignment process on the final translation output. The quality of the translation output is evaluated using BLEU (Papineni et al., 2002). 4.2 Data The experiments we</context>
</contexts>
<marker>Vilar, Popovic, Ney, 2006</marker>
<rawString>David Vilar, Maja Popovic, and Hermann Ney. 2006. AER: Do we need to ”improve” our alignments? In Proceedings of IWSLT 2006, pages 205–212, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING</booktitle>
<pages>836--841</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="979" citStr="Vogel et al., 1996" startWordPosition="146" endWordPosition="149">cutive words together when we believe they correspond to a single word in the opposite language. This is done using the word aligner itself, i.e. by bootstrapping on its output. We evaluate the performance of our approach on a Chinese-to-English machine translation task, and report a 12.2% relative increase in BLEU score over a state-of-the art phrasebased SMT system. 1 Introduction Automatic word alignment can be defined as the problem of determining a translational correspondence at word level given a parallel corpus of aligned sentences. Most current statistical models (Brown et al., 1993; Vogel et al., 1996; Deng and Byrne, 2005) treat the aligned sentences in the corpus as sequences of tokens that are meant to be words; the goal of the alignment process is to find links between source and target words. Before applying such aligners, we thus need to segment the sentences into words – a task which can be quite hard for languages such as Chinese for which word boundaries are not orthographically marked. More importantly, however, this segmentation is often performed in a monolingual context, which makes the word alignment task more difficult since different languages may realize the same concept u</context>
<context position="6316" citStr="Vogel et al., 1996" startWordPosition="1023" endWordPosition="1026">he case of 1-to-n alignments is, therefore, obviously an important issue when dealing with Chinese–English word alignment.3 2.1 The Treatment of 1-to-n Alignments Fertility-based models such as IBM models 3, 4, and 5 allow for alignments between one word and several words (1-to-n or 1: n alignments in what follows), in particular for the reasons specified above. They can be seen as extensions of the simpler IBM models 1 and 2 (Brown et al., 1993). Similarly, Deng and Byrne (2005) propose an HMM framework capable of dealing with 1-to-n alignment, which is an extension of the original model of (Vogel et al., 1996). However, these models rarely question the monolingual tokenization, i.e. the basic unit of the alignment process is the word.4 One alternative to extending the expressivity of one model (and usually its complexity) is to focus on the input representation; in particular, we argue that the alignment process can benefit from a simplification of the input, which consists of trying to reduce the number of 1-to-n alignments to consider. Note that the need to consider segmentation and alignment at the same time is also mentioned in (Tiedemann, 2003), and related issues are reported in (Wu, 1997). 2</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stefan Vogel, Hermann Ney, and Christoph Tillmann. 1996. HMM-based word alignment in statistical translation. In Proceedings of COLING 1996, pages 836– 841, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="1629" citStr="Wu, 1997" startWordPosition="260" endWordPosition="261"> sentences in the corpus as sequences of tokens that are meant to be words; the goal of the alignment process is to find links between source and target words. Before applying such aligners, we thus need to segment the sentences into words – a task which can be quite hard for languages such as Chinese for which word boundaries are not orthographically marked. More importantly, however, this segmentation is often performed in a monolingual context, which makes the word alignment task more difficult since different languages may realize the same concept using varying numbers of words (see e.g. (Wu, 1997)). Moreover, a segmentation considered to be “good” from a monolingual point of view may be unadapted for training alignment models. Although some statistical alignment models allow for 1-to-n word alignments for those reasons, they rarely question the monolingual tokenization and the basic unit of the alignment process remains the word. In this paper, we focus on 1-to-n alignments with the goal of simplifying the task of automatic word aligners by packing several consecutive words together when we believe they correspond to a single word in the opposite language; by identifying enough such ca</context>
<context position="6913" citStr="Wu, 1997" startWordPosition="1125" endWordPosition="1126">t al., 1996). However, these models rarely question the monolingual tokenization, i.e. the basic unit of the alignment process is the word.4 One alternative to extending the expressivity of one model (and usually its complexity) is to focus on the input representation; in particular, we argue that the alignment process can benefit from a simplification of the input, which consists of trying to reduce the number of 1-to-n alignments to consider. Note that the need to consider segmentation and alignment at the same time is also mentioned in (Tiedemann, 2003), and related issues are reported in (Wu, 1997). 2.2 Notation While in this paper, we focus on Chinese–English, the method proposed is applicable to any language 3Note that a 1: 0 alignment may denote a failure to capture a 1: n alignment with n &gt; 1. 4Interestingly, this is actually even the case for approaches that directly model alignments between phrases (Marcu and Wong, 2002; Birch et al., 2006). 305 pair – even for closely related languages, we expect improvements to be seen. The notation however assume Chinese–English MT. Given a Chinese sentence cJ1 consisting of J words fc1, ... , cJI and an English sentence eI1 consisting of I wor</context>
<context position="25261" citStr="Wu, 1997" startWordPosition="4192" endWordPosition="4193"> not believe the improvement is sufficient enough to justify this additional effort. 6 Conclusion and Future Work In this paper, we have introduced a simple yet effective method to pack words together in order to give a different and simplified input to automatic word aligners. We use a bootstrap approach in which we first extract 1-to-n word alignments using an existing word aligner, and then estimate the confidence of those alignments to decide whether or not the n words have to be grouped; if so, this group is conwould thus be completely driven by the bilingual alignment process (see also (Wu, 1997; Tiedemann, 2003) for related considerations). In this case, our approach would be similar to the approach of (Xu et al., 2004), except for the estimation of candidates. sidered a new basic unit to consider. We can finally re-apply the word aligner to the updated sentences. We have evaluated the performance of our approach by measuring the influence of this process on a Chinese-to-English MT task, based on the IWSLT 2006 evaluation campaign. We report a 12.2% relative increase in BLEU score over a standard phrase-based SMT system. We have verified that this process actually reduces the number</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Xu</author>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Do we need chinese word segmentation for statistical machine translation?</title>
<date>2004</date>
<booktitle>In Proceedings of the Third SIGHAN Workshop on Chinese Language Learning,</booktitle>
<pages>122--128</pages>
<location>Barcelona,</location>
<contexts>
<context position="25389" citStr="Xu et al., 2004" startWordPosition="4212" endWordPosition="4215">s paper, we have introduced a simple yet effective method to pack words together in order to give a different and simplified input to automatic word aligners. We use a bootstrap approach in which we first extract 1-to-n word alignments using an existing word aligner, and then estimate the confidence of those alignments to decide whether or not the n words have to be grouped; if so, this group is conwould thus be completely driven by the bilingual alignment process (see also (Wu, 1997; Tiedemann, 2003) for related considerations). In this case, our approach would be similar to the approach of (Xu et al., 2004), except for the estimation of candidates. sidered a new basic unit to consider. We can finally re-apply the word aligner to the updated sentences. We have evaluated the performance of our approach by measuring the influence of this process on a Chinese-to-English MT task, based on the IWSLT 2006 evaluation campaign. We report a 12.2% relative increase in BLEU score over a standard phrase-based SMT system. We have verified that this process actually reduces the number of 1: n alignments with n 7� 1, and that it is rather independent from the (Chinese) segmentation strategy. As for future work,</context>
</contexts>
<marker>Xu, Zens, Ney, 2004</marker>
<rawString>Jia Xu, Richard Zens, and Hermann Ney. 2004. Do we need chinese word segmentation for statistical machine translation? In Proceedings of the Third SIGHAN Workshop on Chinese Language Learning, pages 122–128, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Improvements in phrase-based statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL 2004,</booktitle>
<pages>257--264</pages>
<location>Boston, MA.</location>
<contexts>
<context position="18420" citStr="Zens and Ney, 2004" startWordPosition="3061" endWordPosition="3064"> Dev. Sentences 289 (7 refs.) Running words 3,350 26,223 Vocabulary size 897 1,331 Eval. Sentences 200 (7 refs.) Running words 1,864 14,437 Vocabulary size 569 1,081 Table 2: Chinese–English corpus statistics (Och, 2003) using Phramer (Olteanu et al., 2006), a 3-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002) on the English side of the training data and Pharaoh (Koehn, 2004) with default settings to decode. The log-linear model is also based on standard features: conditional probabilities and lexical smoothing of phrases in both directions, and phrase penalty (Zens and Ney, 2004). 5 Experimental Results 5.1 Results The initial word alignments are obtained using the baseline configuration described above. From these, we build two bilingual 1-to-n dictionaries (one for each direction), and the training corpus is updated by repacking the words in the dictionaries, using the method presented in Section 2. As previously mentioned, this process can be repeated several times; at each step, we can also choose to exploit only one of the two available dictionaries, if so desired. We then extract aligned phrases using the same procedure as for the baseline system; the only diffe</context>
</contexts>
<marker>Zens, Ney, 2004</marker>
<rawString>Richard Zens and Hermann Ney. 2004. Improvements in phrase-based statistical machine translation. In Proceedings of HLT-NAACL 2004, pages 257–264, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tiejun Zhao</author>
<author>Yajuan L¨u</author>
<author>Hao Yu</author>
</authors>
<title>Increasing accuracy of chinese segmentation with strategy of multi-step processing.</title>
<date>2001</date>
<journal>Journal of Chinese Information Processing,</journal>
<volume>15</volume>
<issue>1</issue>
<marker>Zhao, L¨u, Yu, 2001</marker>
<rawString>Tiejun Zhao, Yajuan L¨u, and Hao Yu. 2001. Increasing accuracy of chinese segmentation with strategy of multi-step processing. Journal of Chinese Information Processing, 15(1):13–18.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>