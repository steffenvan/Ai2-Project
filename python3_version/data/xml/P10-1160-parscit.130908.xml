<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000198">
<title confidence="0.993329">
Beyond NomBank:
A Study of Implicit Arguments for Nominal Predicates
</title>
<author confidence="0.99754">
Matthew Gerber and Joyce Y. Chai
</author>
<affiliation confidence="0.877761333333333">
Department of Computer Science
Michigan State University
East Lansing, Michigan, USA
</affiliation>
<email confidence="0.999418">
{gerberm2,jchai}@cse.msu.edu
</email>
<sectionHeader confidence="0.997396" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999882588235294">
Despite its substantial coverage, Nom-
Bank does not account for all within-
sentence arguments and ignores extra-
sentential arguments altogether. These ar-
guments, which we call implicit, are im-
portant to semantic processing, and their
recovery could potentially benefit many
NLP applications. We present a study of
implicit arguments for a select group of
frequent nominal predicates. We show that
implicit arguments are pervasive for these
predicates, adding 65% to the coverage of
NomBank. We demonstrate the feasibil-
ity of recovering implicit arguments with
a supervised classification model. Our re-
sults and analyses provide a baseline for
future work on this emerging task.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99776250877193">
Verbal and nominal semantic role labeling (SRL)
have been studied independently of each other
(Carreras and M`arquez, 2005; Gerber et al., 2009)
as well as jointly (Surdeanu et al., 2008; Hajiˇc et
al., 2009). These studies have demonstrated the
maturity of SRL within an evaluation setting that
restricts the argument search space to the sentence
containing the predicate of interest. However, as
shown by the following example from the Penn
TreeBank (Marcus et al., 1993), this restriction ex-
cludes extra-sentential arguments:
(1) [arg0 The two companies] [pred produce]
[arg1 market pulp, containerboard and white
paper]. The goods could be manufactured
closer to customers, saving [pred shipping]
costs.
The first sentence in Example 1 includes the Prop-
Bank (Kingsbury et al., 2002) analysis of the ver-
bal predicate produce, where arg0 is the agentive
producer and arg1 is the produced entity. The sec-
ond sentence contains an instance of the nominal
predicate shipping that is not associated with argu-
ments in NomBank (Meyers, 2007).
From the sentences in Example 1, the reader can
infer that The two companies refers to the agents
(arg0) of the shipping predicate. The reader can
also infer that market pulp, containerboard and
white paper refers to the shipped entities (arg1
of shipping).1 These extra-sentential arguments
have not been annotated for the shipping predi-
cate and cannot be identified by a system that re-
stricts the argument search space to the sentence
containing the predicate. NomBank also ignores
many within-sentence arguments. This is shown
in the second sentence of Example 1, where The
goods can be interpreted as the arg1 of shipping.
These examples demonstrate the presence of argu-
ments that are not included in NomBank and can-
not easily be identified by systems trained on the
resource. We refer to these arguments as implicit.
This paper presents our study of implicit ar-
guments for nominal predicates. We began our
study by annotating implicit arguments for a se-
lect group of predicates. For these predicates, we
found that implicit arguments add 65% to the ex-
isting role coverage of NomBank.2 This increase
has implications for tasks (e.g., question answer-
ing, information extraction, and summarization)
that benefit from semantic analysis. Using our an-
notations, we constructed a feature-based model
for automatic implicit argument identification that
unifies standard verbal and nominal SRL. Our re-
sults indicate a 59% relative (15-point absolute)
gain in F1 over an informed baseline. Our analy-
ses highlight strengths and weaknesses of the ap-
proach, providing insights for future work on this
emerging task.
</bodyText>
<footnote confidence="0.986279666666667">
1In PropBank and NomBank, the interpretation of each
role (e.g., argo) is specific to a predicate sense.
2Role coverage indicates the percentage of roles filled.
</footnote>
<page confidence="0.656817">
1583
</page>
<note confidence="0.9529155">
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1583–1592,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.99952675">
In the following section, we review related re-
search, which is historically sparse but recently
gaining traction. We present our annotation effort
in Section 3, and follow with our implicit argu-
ment identification model in Section 4. In Section
5, we describe the evaluation setting and present
our experimental results. We analyze these results
in Section 6 and conclude in Section 7.
</bodyText>
<sectionHeader confidence="0.999908" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999960425925926">
Palmer et al. (1986) made one of the earliest at-
tempts to automatically recover extra-sentential
arguments. Their approach used a fine-grained do-
main model to assess the compatibility of candi-
date arguments and the slots needing to be filled.
A phenomenon similar to the implicit argu-
ment has been studied in the context of Japanese
anaphora resolution, where a missing case-marked
constituent is viewed as a zero-anaphoric expres-
sion whose antecedent is treated as the implicit ar-
gument of the predicate of interest. This behavior
has been annotated manually by Iida et al. (2007),
and researchers have applied standard SRL tech-
niques to this corpus, resulting in systems that
are able to identify missing case-marked expres-
sions in the surrounding discourse (Imamura et
al., 2009). Sasano et al. (2004) conducted sim-
ilar work with Japanese indirect anaphora. The
authors used automatically derived nominal case
frames to identify antecedents. However, as noted
by Iida et al., grammatical cases do not stand in
a one-to-one relationship with semantic roles in
Japanese (the same is true for English).
Fillmore and Baker (2001) provided a detailed
case study of implicit arguments (termed null in-
stantiations in that work), but did not provide con-
crete methods to account for them automatically.
Previously, we demonstrated the importance of fil-
tering out nominal predicates that take no local ar-
guments (Gerber et al., 2009); however, this work
did not address the identification of implicit ar-
guments. Burchardt et al. (2005) suggested ap-
proaches to implicit argument identification based
on observed coreference patterns; however, the au-
thors did not implement and evaluate such meth-
ods. We draw insights from all three of these
studies. We show that the identification of im-
plicit arguments for nominal predicates leads to
fuller semantic interpretations when compared to
traditional SRL methods. Furthermore, motivated
by Burchardt et al., our model uses a quantitative
analysis of naturally occurring coreference pat-
terns to aid implicit argument identification.
Most recently, Ruppenhofer et al. (2009) con-
ducted SemEval Task 10, “Linking Events and
Their Participants in Discourse”, which evaluated
implicit argument identification systems over a
common test set. The task organizers annotated
implicit arguments across entire passages, result-
ing in data that cover many distinct predicates,
each associated with a small number of annotated
instances. In contrast, our study focused on a se-
lect group of nominal predicates, each associated
with a large number of annotated instances.
</bodyText>
<sectionHeader confidence="0.810759" genericHeader="method">
3 Data annotation and analysis
</sectionHeader>
<subsectionHeader confidence="0.999267">
3.1 Data annotation
</subsectionHeader>
<bodyText confidence="0.999717857142857">
Implicit arguments have not been annotated within
the Penn TreeBank, which is the textual and syn-
tactic basis for NomBank. Thus, to facilitate
our study, we annotated implicit arguments for
instances of nominal predicates within the stan-
dard training, development, and testing sections of
the TreeBank. We limited our attention to nom-
inal predicates with unambiguous role sets (i.e.,
senses) that are derived from verbal role sets. We
then ranked this set of predicates using two pieces
of information: (1) the average difference between
the number of roles expressed in nominal form (in
NomBank) versus verbal form (in PropBank) and
(2) the frequency of the nominal form in the cor-
pus. We assumed that the former gives an indica-
tion as to how many implicit roles an instance of
the nominal predicate might have. The product of
(1) and (2) thus indicates the potential prevalence
of implicit arguments for a predicate. To focus our
study, we ranked the predicates in NomBank ac-
cording to this product and selected the top ten,
shown in Table 1.
We annotated implicit arguments document-by-
document, selecting all singular and plural nouns
derived from the predicates in Table 1. For each
missing argument position of each predicate in-
stance, we inspected the local discourse for a suit-
able implicit argument. We limited our attention to
the current sentence as well as all preceding sen-
tences in the document, annotating all mentions of
an implicit argument within this window.
In the remainder of this paper, we will use iargn
to refer to an implicit argument position n. We
will use argn to refer to an argument provided by
PropBank or NomBank. We will use p to mark
</bodyText>
<page confidence="0.987663">
1584
</page>
<table confidence="0.999901642857143">
Pre-annotation Post-annotation
Role average
Predicate # Role coverage (%) Noun Verb Role coverage (%) Noun role average
price 217 42.4 1.7 1.7 55.3 2.2
sale 185 24.3 1.2 2.0 42.0 2.1
investor 160 35.0 1.1 2.0 54.6 1.6
fund 109 8.7 0.4 2.0 21.6 0.9
loss 104 33.2 1.3 2.0 46.9 1.9
plan 102 30.9 1.2 1.8 49.3 2.0
investment 102 15.7 0.5 2.0 33.3 1.0
cost 101 26.2 1.1 2.3 47.5 1.9
bid 88 26.9 0.8 2.2 72.0 2.2
loan 85 22.4 1.1 2.5 41.2 2.1
Overall 1,253 28.0 1.1 2.0 46.2 1.8
</table>
<tableCaption confidence="0.999328">
Table 1: Predicates targeted for annotation. The second column gives the number of predicate instances
</tableCaption>
<bodyText confidence="0.970086413793103">
annotated. Pre-annotation numbers only include NomBank annotations, whereas Post-annotation num-
bers include NomBank and implicit argument annotations. Role coverage indicates the percentage of
roles filled. Role average indicates how many roles, on average, are filled for an instance of a predicate’s
noun form or verb form within the TreeBank. Verbal role averages were computed using PropBank.
predicate instances. Below, we give an example
annotation for an instance of the investment predi-
cate:
(2) [iarg0 Participants] will be able to transfer
[iarg1 money] to [iarg2 other investment
funds]. The [p investment] choices are
limited to [iarg2 a stock fund and a
money-market fund].
NomBank does not associate this instance of in-
vestment with any arguments; however, we were
able to identify the investor (iarg0), the thing in-
vested (iarg1), and two mentions of the thing in-
vested in (iarg2).
Our data set was also independently annotated
by an undergraduate linguistics student. For each
missing argument position, the student was asked
to identify the closest acceptable implicit argu-
ment within the current and preceding sentences.
The argument position was left unfilled if no ac-
ceptable constituent could be found. For a miss-
ing argument position, the student’s annotation
agreed with our own if both identified the same
constituent or both left the position unfilled. Anal-
ysis indicated an agreement of 67% using Cohen’s
kappa coefficient (Cohen, 1960).
</bodyText>
<subsectionHeader confidence="0.999988">
3.2 Annotation analysis
</subsectionHeader>
<bodyText confidence="0.9999755">
Role coverage for a predicate instance is equal to
the number of filled roles divided by the number
of roles in the predicate’s lexicon entry. Role cov-
erage for the marked predicate in Example 2 is
0/3 for NomBank-only arguments and 3/3 when
the annotated implicit arguments are also consid-
ered. Returning to Table 1, the third column gives
role coverage percentages for NomBank-only ar-
guments. The sixth column gives role coverage
percentages when both NomBank arguments and
the annotated implicit arguments are considered.
Overall, the addition of implicit arguments created
a 65% relative (18-point absolute) gain in role cov-
erage across the 1,253 predicate instances that we
annotated.
The predicates in Table 1 are typically associ-
ated with fewer arguments on average than their
corresponding verbal predicates. When consid-
ering NomBank-only arguments, this difference
(compare columns four and five) varies from zero
(for price) to a factor of five (for fund). When im-
plicit arguments are included in the comparison,
these differences are reduced and many nominal
predicates express approximately the same num-
ber of arguments on average as their verbal coun-
terparts (compare the fifth and seventh columns).
In addition to role coverage and average count,
we examined the location of implicit arguments.
Figure 1 shows that approximately 56% of the im-
plicit arguments in our data can be resolved within
the sentence containing the predicate. The remain-
ing implicit arguments require up to forty-six sen-
</bodyText>
<page confidence="0.988868">
1585
</page>
<figureCaption confidence="0.998192">
Figure 1: Location of implicit arguments. For
</figureCaption>
<bodyText confidence="0.840707">
missing argument positions with an implicit filler,
the y-axis indicates the likelihood of the filler be-
ing found at least once in the previous x sentences.
tences for resolution; however, a vast majority of
these can be resolved within the previous few sen-
tences. Section 6 discusses implications of this
skewed distribution.
</bodyText>
<sectionHeader confidence="0.980233" genericHeader="method">
4 Implicit argument identification
</sectionHeader>
<subsectionHeader confidence="0.997228">
4.1 Model formulation
</subsectionHeader>
<bodyText confidence="0.999127411764706">
In our study, we assumed that each sentence in a
document had been analyzed for PropBank and
NomBank predicate-argument structure. Nom-
Bank includes a lexicon listing the possible ar-
gument positions for a predicate, allowing us to
identify missing argument positions with a simple
lookup. Given a nominal predicate instance p with
a missing argument position iargn, the task is to
search the surrounding discourse for a constituent
c that fills iargn. Our model conducts this search
over all constituents annotated by either PropBank
or NomBank with non-adjunct labels.
A candidate constituent c will often form a
coreference chain with other constituents in the
discourse. Consider the following abridged sen-
tences, which are adjacent in their Penn TreeBank
document:
</bodyText>
<listItem confidence="0.99740675">
(3) [Mexico] desperately needs investment.
(4) Conservative Japanese investors are put off
by [Mexico’s] investment regulations.
(5) Japan is the fourth largest investor in
</listItem>
<bodyText confidence="0.989269703703704">
[c Mexico], with 5% of the total
[p investments].
NomBank does not associate the labeled instance
of investment with any arguments, but it is clear
from the surrounding discourse that constituent c
(referring to Mexico) is the thing being invested in
(the iarg2). When determining whether c is the
iarg2 of investment, one can draw evidence from
other mentions in c’s coreference chain. Example
3 states that Mexico needs investment. Example
4 states that Mexico regulates investment. These
propositions, which can be derived via traditional
SRL analyses, should increase our confidence that
c is the iarg2 of investment in Example 5.
Thus, the unit of classification for a candi-
date constituent c is the three-tuple (p, iargn, c&apos;),
where c&apos; is a coreference chain comprising c and
its coreferent constituents.3 We defined a binary
classification function Pr(+ |(p, iargn, c&apos;)) that
predicts the probability that the entity referred to
by c fills the missing argument position iargn of
predicate instance p. In the remainder of this pa-
per, we will refer to c as the primary filler, dif-
ferentiating it from other mentions in the corefer-
ence chain c&apos;. In the following section, we present
the feature set used to represent each three-tuple
within the classification function.
</bodyText>
<subsectionHeader confidence="0.998841">
4.2 Model features
</subsectionHeader>
<bodyText confidence="0.999933333333333">
Starting with a wide range of features, we per-
formed floating forward feature selection (Pudil
et al., 1994) over held-out development data com-
prising implicit argument annotations from section
24 of the Penn TreeBank. As part of the feature
selection process, we conducted a grid search for
the best per-class cost within LibLinear’s logistic
regression solver (Fan et al., 2008). This was done
to reduce the negative effects of data imbalance,
which is severe even when selecting candidates
from the current and previous few sentences. Ta-
ble 2 shows the selected features, which are quite
different from those used in our previous work to
identify traditional semantic arguments (Gerber et
al., 2009).4 Below, we give further explanations
for some of the features.
Feature 1 models the semantic role relationship
between each mention in c&apos; and the missing argu-
ment position iargn. To reduce data sparsity, this
feature generalizes predicates and argument posi-
tions to their VerbNet (Kipper, 2005) classes and
</bodyText>
<footnote confidence="0.9720748">
3We used OpenNLP for coreference identification:
http://opennlp.sourceforge.net
4We have omitted many of the lowest-ranked features.
Descriptions of these features can be obtained by contacting
the authors.
</footnote>
<figure confidence="0.997601636363636">
Implicit arguments
resolved
0.9
0.8
0.7
0.6
0.5
0.4
1
0 2 4 6 8 10 12 18 28 46
Sentences prior
</figure>
<page confidence="0.913704">
1586
</page>
<table confidence="0.996594444444444">
# Feature value description
1* For every f, the VerbNet class/role of pf/argf concatenated with the class/role of p/iargn.
2* Average pointwise mutual information between (p, iargn) and any (pf, argf).
3 Percentage of all f that are definite noun phrases.
4 Minimum absolute sentence distance from any f to p.
5* Minimum pointwise mutual information between (p, iargn) and any (pf, argf).
6 Frequency of the nominal form of p within the document that contains it.
7 Nominal form of p concatenated with iargn.
8 Nominal form of p concatenated with the sorted integer argument indexes from all argn of p.
9 Number of mentions in c&apos;.
10* Head word of p’s right sibling node.
11 For every f, the synset (Fellbaum, 1998) for the head of f concatenated with p and iargn.
12 Part of speech of the head of p’s parent node.
13 Average absolute sentence distance from any f to p.
14* Discourse relation whose two discourse units cover c (the primary filler) and p.
15 Number of left siblings of p.
16 Whether p is the head of its parent node.
17 Number of right siblings of p.
</table>
<tableCaption confidence="0.616047">
Table 2: Features for determining whether c fills iargn of predicate p. For each mention f (denoting a
</tableCaption>
<bodyText confidence="0.987009607142857">
filler) in the coreference chain c&apos;, we define pf and argf to be the predicate and argument position of f.
Features are sorted in descending order of feature selection gain. Unless otherwise noted, all predicates
were normalized to their verbal form and all argument positions (e.g., argn and iargn) were interpreted
as labels instead of word content. Features marked with an asterisk are explained in Section 4.2.
semantic roles using SemLink.5 For explanation
purposes, consider again Example 1, where we are
trying to fill the iargo of shipping. Let c&apos; contain
a single mention, The two companies, which is the
argo of produce. As described in Table 2, fea-
ture 1 is instantiated with a value of create.agent-
send.agent, where create and send are the VerbNet
classes that contain produce and ship, respectively.
In the conversion to LibLinear’s instance repre-
sentation, this instantiation is converted into a sin-
gle binary feature create.agent-send.agent whose
value is one. Features 1 and 11 are instantiated
once for each mention in c&apos;, allowing the model
to consider information from multiple mentions of
the same entity.
Features 2 and 5 are inspired by the work
of Chambers and Jurafsky (2008), who inves-
tigated unsupervised learning of narrative event
sequences using pointwise mutual information
(PMI) between syntactic positions. We used a sim-
ilar PMI score, but defined it with respect to se-
mantic arguments instead of syntactic dependen-
cies. Thus, the values for features 2 and 5 are
computed as follows (the notation is explained in
</bodyText>
<footnote confidence="0.844023">
5http://verbs.colorado.edu/semlink
</footnote>
<bodyText confidence="0.550154">
the caption for Table 2):
</bodyText>
<equation confidence="0.97293625">
pmi((p, iargn) , (pf, argf)) =
P.ref((p, iargn) , (pf, argf))
P��ref((p, iargn) , *)P.ref((pf, argf) , *)
(6)
</equation>
<bodyText confidence="0.9775086">
To compute Equation 6, we first labeled a subset of
the Gigaword corpus (Graff, 2003) using the ver-
bal SRL system of Punyakanok et al. (2008) and
the nominal SRL system of Gerber et al. (2009).
We then identified coreferent pairs of arguments
using OpenNLP. Suppose the resulting data has
N coreferential pairs of argument positions. Also
suppose that M of these pairs comprise (p, argn)
and (pf, argf). The numerator in Equation 6 is
defined as N . Each term in the denominator is
obtained similarly, except that M is computed as
the total number of coreference pairs compris-
ing an argument position (e.g., (p, argn)) and any
other argument position. Like Chambers and Ju-
rafsky, we also used the discounting method sug-
gested by Pantel and Ravichandran (2004) for low-
frequency observations. The PMI score is some-
what noisy due to imperfect output, but it provides
information that is useful for classification.
log
</bodyText>
<page confidence="0.927366">
1587
</page>
<bodyText confidence="0.981500666666667">
Feature 10 does not depend on c&apos; and is specific
to each predicate. Consider the following exam-
ple:
</bodyText>
<listItem confidence="0.998525666666667">
(7) Statistics Canada reported that its [arg1
industrial-product] [p price] index dropped
2% in September.
</listItem>
<bodyText confidence="0.957813947368421">
The “[p price] index” collocation is rarely associ-
ated with an arg0 in NomBank or with an iarg0 in
our annotations (both argument positions denote
the seller). Feature 10 accounts for this type of be-
havior by encoding the syntactic head of p’s right
sibling. The value of feature 10 for Example 7 is
price:index. Contrast this with the following:
(8) [iarg0 The company] is trying to prevent
further [p price] drops.
The value of feature 10 for Example 8 is
price:drop. This feature captures an important dis-
tinction between the two uses of price: the for-
mer rarely takes an iarg0, whereas the latter often
does. Features 12 and 15-17 account for predicate-
specific behaviors in a similar manner.
Feature 14 identifies the discourse relation (if
any) that holds between the candidate constituent
c and the filled predicate p. Consider the following
example:
</bodyText>
<listItem confidence="0.882070333333333">
(9) [iarg0 SFE Technologies] reported a net loss
of $889,000 on sales of $23.4 million.
(10) That compared with an operating [p loss] of
</listItem>
<bodyText confidence="0.9187504">
[arg1 $1.9 million] on sales of $27.4 million
in the year-earlier period.
In this case, a comparison discourse relation (sig-
naled by the underlined text) holds between the
first and sentence sentence. The coherence pro-
vided by this relation encourages an inference that
identifies the marked iarg0 (the loser). Through-
out our study, we used gold-standard discourse re-
lations provided by the Penn Discourse TreeBank
(Prasad et al., 2008).
</bodyText>
<sectionHeader confidence="0.99924" genericHeader="method">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999976763157895">
We trained the feature-based logistic regression
model over 816 annotated predicate instances as-
sociated with 650 implicitly filled argument posi-
tions (not all predicate instances had implicit ar-
guments). During training, a candidate three-tuple
(p, iarg,,,, c&apos;) was given a positive label if the can-
didate implicit argument c (the primary filler) was
annotated as filling the missing argument position.
To factor out errors from standard SRL analyses,
the model used gold-standard argument labels pro-
vided by PropBank and NomBank. As shown in
Figure 1 (Section 3.2), implicit arguments tend to
be located in close proximity to the predicate. We
found that using all candidate constituents c within
the current and previous two sentences worked
best on our development data.
We compared our supervised model with the
simple baseline heuristic defined below:6
Fill iarg,,, for predicate instance p
with the nearest constituent in the two-
sentence candidate window that fills
arg,,, for a different instance of p, where
all nominal predicates are normalized to
their verbal forms.
The normalization allows an existing arg0 for the
verb invested to fill an iarg0 for the noun in-
vestment. We also evaluated an oracle model
that made gold-standard predictions for candidates
within the two-sentence prediction window.
We evaluated these models using the methodol-
ogy proposed by Ruppenhofer et al. (2009). For
each missing argument position of a predicate in-
stance, the models were required to either (1) iden-
tify a single constituent that fills the missing argu-
ment position or (2) make no prediction and leave
the missing argument position unfilled. We scored
predictions using the Dice coefficient, which is de-
fined as follows:
</bodyText>
<equation confidence="0.9987235">
2 * |PredictednTrue |(11)
|Predicted |+ |True|
</equation>
<bodyText confidence="0.999757357142857">
Predicted is the set of tokens subsumed by the
constituent predicted by the model as filling a
missing argument position. True is the set of
tokens from a single annotated constituent that
fills the missing argument position. The model’s
prediction receives a score equal to the maxi-
mum Dice overlap across any one of the annotated
fillers. Precision is equal to the summed predic-
tion scores divided by the number of argument po-
sitions filled by the model. Recall is equal to the
summed prediction scores divided by the number
of argument positions filled in our annotated data.
Predictions not covering the head of a true filler
were assigned a score of zero.
</bodyText>
<footnote confidence="0.7627675">
6This heuristic outperformed a more complicated heuris-
tic that relied on the PMI score described in section 4.2.
</footnote>
<page confidence="0.962034">
1588
</page>
<table confidence="0.999899615384615">
Baseline Discriminative Oracle
# Imp. # P R F1 P R F1 p R F1
sale 64 60 50.0 28.3 36.2 47.2 41.7 44.2 0.118 80.0 88.9
price 121 53 24.0 11.3 15.4 36.0 32.6 34.2 0.008 88.7 94.0
investor 78 35 33.3 5.7 9.8 36.8 40.0 38.4 &lt; 0.001 91.4 95.5
bid 19 26 100.0 19.2 32.3 23.8 19.2 21.3 0.280 57.7 73.2
plan 25 20 83.3 25.0 38.5 78.6 55.0 64.7 0.060 82.7 89.4
cost 25 17 66.7 23.5 34.8 61.1 64.7 62.9 0.024 94.1 97.0
loss 30 12 71.4 41.7 52.6 83.3 83.3 83.3 0.020 100.0 100.0
loan 11 9 50.0 11.1 18.2 42.9 33.3 37.5 0.277 88.9 94.1
investment 21 8 0.0 0.0 0.0 40.0 25.0 30.8 0.182 87.5 93.3
fund 43 6 0.0 0.0 0.0 14.3 16.7 15.4 0.576 50.0 66.7
Overall 437 246 48.4 18.3 26.5 44.5 40.4 42.3 &lt; 0.001 83.1 90.7
</table>
<tableCaption confidence="0.999348">
Table 3: Evaluation results. The second column gives the number of predicate instances evaluated.
</tableCaption>
<bodyText confidence="0.994204869565217">
The third column gives the number of ground-truth implicitly filled argument positions for the predicate
instances (not all instances had implicit arguments). P, R, and F1 indicate precision, recall, and F-
measure (Q = 1), respectively. p-values denote the bootstrapped significance of the difference in F1
between the baseline and discriminative models. Oracle precision (not shown) is 100% for all predicates.
Our evaluation data comprised 437 predicate in-
stances associated with 246 implicitly filled ar-
gument positions. Table 3 presents the results.
Predicates with the highest number of implicit ar-
guments - sale and price - showed F1 increases
of 8 points and 18.8 points, respectively. Over-
all, the discriminative model increased F1 perfor-
mance 15.8 points (59.6%) over the baseline.
We measured human performance on this task
by running our undergraduate assistant’s annota-
tions against the evaluation data. Our assistant
achieved an overall F1 score of 58.4% using the
same candidate window as the baseline and dis-
criminative models. The difference in F1 between
the discriminative and human results had an ex-
act p-value of less than 0.001. All significance
testing was performed using a two-tailed bootstrap
method similar to the one described by Efron and
Tibshirani (1993).
</bodyText>
<sectionHeader confidence="0.999809" genericHeader="method">
6 Discussion
</sectionHeader>
<subsectionHeader confidence="0.999909">
6.1 Feature ablation
</subsectionHeader>
<bodyText confidence="0.999788">
We conducted an ablation study to measure the
contribution of specific feature sets. Table 4
presents the ablation configurations and results.
For each configuration, we retrained and retested
the discriminative model using the features de-
scribed. As shown, we observed significant losses
when excluding features that relate the seman-
tic roles of mentions in c&apos; to the semantic role
</bodyText>
<table confidence="0.9989825">
Percent change (p-value)
Configuration P R F1
Remove 1,2,5 -35.3 -36.1 -35.7
(&lt; 0.01) (&lt; 0.01) (&lt; 0.01)
Use 1,2,5 only -26.3 -11.9 -19.2
(&lt; 0.01) (0.05) (&lt; 0.01)
Remove 14 0.2 1.0 0.7
(0.95) (0.66) (0.73)
</table>
<tableCaption confidence="0.988574">
Table 4: Feature ablation results. The first column
</tableCaption>
<bodyText confidence="0.967270266666667">
lists the feature configurations. All changes are
percentages relative to the full-featured discrimi-
native model. p-values for the changes are indi-
cated in parentheses.
of the missing argument position (first configura-
tion). The second configuration tested the effect of
using only the SRL-based features. This also re-
sulted in significant performance losses, suggest-
ing that the other features contribute useful infor-
mation. Lastly, we tested the effect of removing
discourse relations (feature 14), which are likely
to be difficult to extract reliably in a practical set-
ting. As shown, this feature did not have a statis-
tically significant effect on performance and could
be excluded in future applications of the model.
</bodyText>
<subsectionHeader confidence="0.999675">
6.2 Unclassified true implicit arguments
</subsectionHeader>
<bodyText confidence="0.99868">
Of all the errors made by the system, approxi-
mately 19% were caused by the system’s failure to
</bodyText>
<page confidence="0.989586">
1589
</page>
<bodyText confidence="0.999639423076923">
generate a candidate constituent c that was a cor-
rect implicit argument. Without such a candidate,
the system stood no chance of identifying a cor-
rect implicit argument. Two factors contributed to
this type of error, the first being our assumption
that implicit arguments are also core (i.e., argn)
arguments to traditional SRL structures. Approxi-
mately 8% of the overall error was due to a failure
of this assumption. In many cases, the true im-
plicit argument filled a non-core (i.e., adjunct) role
within PropBank or NomBank.
More frequently, however, true implicit argu-
ments were missed because the candidate window
was too narrow. This accounts for 12% of the
overall error. Oracle recall (second-to-last col-
umn in Table 3) indicates the nominals that suf-
fered most from windowing errors. For exam-
ple, the sale predicate was associated with the
highest number of true implicit arguments, but
only 80% of those could be resolved within the
two-sentence candidate window. Empirically, we
found that extending the candidate window uni-
formly for all predicates did not increase perfor-
mance on the development data. The oracle re-
sults suggest that predicate-specific window set-
tings might offer some advantage.
</bodyText>
<subsectionHeader confidence="0.991442">
6.3 The investment and fund predicates
</subsectionHeader>
<bodyText confidence="0.9980326">
In Section 4.2, we discussed the price predicate,
which frequently occurs in the “[p price] index”
collocation. We observed that this collocation
is rarely associated with either an overt arg0 or
an implicit iarg0. Similar observations can be
made for the investment and fund predicates. Al-
though these two predicates are frequent, they are
rarely associated with implicit arguments: invest-
ment takes only eight implicit arguments across its
21 instances, and fund takes only six implicit ar-
guments across its 43 instances. This behavior is
due in large part to collocations such as “[p in-
vestment] banker”, “stock [p fund]”, and “mutual
[p fund]”, which use predicate senses that are not
eventive. Such collocations also violate our as-
sumption that differences between the PropBank
and NomBank argument structure for a predicate
are indicative of implicit arguments (see Section
3.1 for this assumption).
Despite their lack of implicit arguments, it is
important to account for predicates such as in-
vestment and fund because incorrect prediction of
implicit arguments for them can lower precision.
This is precisely what happened for the fund pred-
icate, where the model incorrectly identified many
implicit arguments for “stock [p fund]” and “mu-
tual [p fund]”. The left context offund should help
the model avoid this type of error; however, our
feature selection process did not identify any over-
all gains from including this information.
</bodyText>
<subsectionHeader confidence="0.962155">
6.4 Improvements versus the baseline
</subsectionHeader>
<bodyText confidence="0.986358875">
The baseline heuristic covers the simple case
where identical predicates share arguments in the
same position. Thus, it is interesting to examine
cases where the baseline heuristic failed but the
discriminative model succeeded. Consider the fol-
lowing sentence:
(12) Mr. Rogers recommends that [p investors]
sell [iarg2 takeover-related stock].
Neither NomBank nor the baseline heuristic asso-
ciate the marked predicate in Example 12 with any
arguments; however, the feature-based model was
able to correctly identify the marked iarg2 as the
entity being invested in. This inference captured a
tendency of investors to sell the things they have
invested in.
We conclude our discussion with an example of
an extra-sentential implicit argument:
(13) [iarg0 Olivetti] has denied that it violated
the rules, asserting that the shipments were
properly licensed. However, the legality of
these [p sales] is still an open question.
As shown in Example 13, the system was able to
correctly identify Olivetti as the agent in the sell-
ing event of the second sentence. This inference
involved two key steps. First, the system identified
coreferent mentions of Olivetti that participated in
exporting and supplying events (not shown). Sec-
ond, the system identified a tendency for exporters
and suppliers to also be sellers. Using this knowl-
edge, the system extracted information that could
not be extracted by the baseline heuristic or a tra-
ditional SRL system.
</bodyText>
<sectionHeader confidence="0.996283" genericHeader="conclusions">
7 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999206166666667">
Current SRL approaches limit the search for ar-
guments to the sentence containing the predicate
of interest. Many systems take this assumption
a step further and restrict the search to the predi-
cate’s local syntactic environment; however, pred-
icates and the sentences that contain them rarely
</bodyText>
<page confidence="0.967103">
1590
</page>
<bodyText confidence="0.999976238095238">
exist in isolation. As shown throughout this paper,
they are usually embedded in a coherent and se-
mantically rich discourse that must be taken into
account. We have presented a preliminary study
of implicit arguments for nominal predicates that
focused specifically on this problem.
Our contribution is three-fold. First, we have
created gold-standard implicit argument annota-
tions for a small set of pervasive nominal predi-
cates.7 Our analysis shows that these annotations
add 65% to the role coverage of NomBank. Sec-
ond, we have demonstrated the feasibility of re-
covering implicit arguments for many of the pred-
icates, thus establishing a baseline for future work
on this emerging task. Third, our study suggests
a few ways in which this research can be moved
forward. As shown in Section 6, many errors were
caused by the absence of true implicit arguments
within the set of candidate constituents. More in-
telligent windowing strategies in addition to al-
ternate candidate sources might offer some im-
provement. Although we consistently observed
development gains from using automatic coref-
erence resolution, this process creates errors that
need to be studied more closely. It will also be
important to study implicit argument patterns of
non-verbal predicates such as the partitive percent.
These predicates are among the most frequent in
the TreeBank and are likely to require approaches
that differ from the ones we pursued.
Finally, any extension of this work is likely to
encounter a significant knowledge acquisition bot-
tleneck. Implicit argument annotation is difficult
because it requires both argument and coreference
identification (the data produced by Ruppenhofer
et al. (2009) is similar). Thus, it might be produc-
tive to focus future work on (1) the extraction of
relevant knowledge from existing resources (e.g.,
our use of coreference patterns from Gigaword) or
(2) semi-supervised learning of implicit argument
models from a combination of labeled and unla-
beled data.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9995734">
We would like to thank the anonymous review-
ers for their helpful questions and comments. We
would also like to thank Malcolm Doering for his
annotation effort. This work was supported in part
by NSF grants IIS-0347548 and IIS-0840538.
</bodyText>
<footnote confidence="0.978407">
7Our annotation data can be freely downloaded at
http://links.cse.msu.edu:8000/lair/projects/semanticrole.html
</footnote>
<sectionHeader confidence="0.949288" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998703711538461">
Aljoscha Burchardt, Anette Frank, and Manfred
Pinkal. 2005. Building text meaning representa-
tions from contextually related frames - a case study.
In Proceedings of the Sixth International Workshop
on Computational Semantics.
Xavier Carreras and Llu´ıs M`arquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: Semantic role
labeling.
Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In Pro-
ceedings of the Association for Computational Lin-
guistics, pages 789–797, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20(1):3746.
Bradley Efron and Robert J. Tibshirani. 1993. An In-
troduction to the Bootstrap. Chapman &amp; Hall, New
York.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A Library for Large Linear Classification. Journal
of Machine Learning Research, 9:1871–1874.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database (Language, Speech, and Commu-
nication). The MIT Press, May.
C.J. Fillmore and C.F. Baker. 2001. Frame semantics
for text understanding. In Proceedings of WordNet
and Other Lexical Resources Workshop, NAACL.
Matthew Gerber, Joyce Y. Chai, and Adam Meyers.
2009. The role of implicit argumentation in nominal
SRL. In Proceedings of the North American Chap-
ter of the Association for Computational Linguistics,
pages 146–154, Boulder, Colorado, USA, June.
David Graff. 2003. English Gigaword. Linguistic
Data Consortium, Philadelphia.
Jan Hajiˇc, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of the
Thirteenth Conference on Computational Natural
Language Learning (CoNLL 2009): Shared Task,
pages 1–18, Boulder, Colorado, June. Association
for Computational Linguistics.
Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji
Matsumoto. 2007. Annotating a Japanese text cor-
pus with predicate-argument and coreference rela-
tions. In Proceedings of the Linguistic Annotation
Workshop in ACL-2007, page 132139.
</reference>
<page confidence="0.813892">
1591
</page>
<reference confidence="0.999191367647059">
Kenji Imamura, Kuniko Saito, and Tomoko Izumi.
2009. Discriminative approach to predicate-
argument structure analysis with zero-anaphora res-
olution. In Proceedings of the ACL-IJCNLP 2009
Conference Short Papers, pages 85–88, Suntec, Sin-
gapore, August. Association for Computational Lin-
guistics.
P. Kingsbury, M. Palmer, and M. Marcus. 2002.
Adding semantic annotation to the Penn TreeBank.
In Proceedings of the Human Language Technology
Conference (HLT’02).
Karin Kipper. 2005. VerbNet: A broad-coverage, com-
prehensive verb lexicon. Ph.D. thesis, Department
of Computer and Information Science University of
Pennsylvania.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn TreeBank. Computa-
tional Linguistics, 19:313–330.
Adam Meyers. 2007. Annotation guidelines for
NomBank - noun argument structure for PropBank.
Technical report, New York University.
Martha S. Palmer, Deborah A. Dahl, Rebecca J. Schiff-
man, Lynette Hirschman, Marcia Linebarger, and
John Dowding. 1986. Recovering implicit infor-
mation. In Proceedings of the 24th annual meeting
on Association for Computational Linguistics, pages
10–19, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Patrick Pantel and Deepak Ravichandran. 2004.
Automatically labeling semantic classes. In
Daniel Marcu Susan Dumais and Salim Roukos, ed-
itors, HLT-NAACL 2004: Main Proceedings, pages
321–328, Boston, Massachusetts, USA, May 2 -
May 7. Association for Computational Linguistics.
Rashmi Prasad, Alan Lee, Nikhil Dinesh, Eleni Milt-
sakaki, Geraud Campion, Aravind Joshi, and Bonnie
Webber. 2008. Penn discourse treebank version 2.0.
Linguistic Data Consortium, February.
P. Pudil, J. Novovicova, and J. Kittler. 1994. Floating
search methods in feature selection. Pattern Recog-
nition Letters, 15:1119–1125.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and infer-
ence in semantic role labeling. Comput. Linguist.,
34(2):257–287.
Josef Ruppenhofer, Caroline Sporleder, Roser
Morante, Collin Baker, and Martha Palmer. 2009.
Semeval-2010 task 10: Linking events and their
participants in discourse. In Proceedings of
the Workshop on Semantic Evaluations: Recent
Achievements and Future Directions (SEW-2009),
pages 106–111, Boulder, Colorado, June. Associa-
tion for Computational Linguistics.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
hashi. 2004. Automatic construction of nominal
case frames and its application to indirect anaphora
resolution. In Proceedings of Coling 2004, pages
1201–1207, Geneva, Switzerland, Aug 23–Aug 27.
COLING.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu´ıs M`arquez, and Joakim Nivre. 2008. The
CoNLL 2008 shared task on joint parsing of syn-
tactic and semantic dependencies. In CoNLL 2008:
Proceedings of the Twelfth Conference on Computa-
tional Natural Language Learning, pages 159–177,
Manchester, England, August. Coling 2008 Orga-
nizing Committee.
</reference>
<page confidence="0.99425">
1592
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.954444">
<title confidence="0.9983695">Beyond NomBank: A Study of Implicit Arguments for Nominal Predicates</title>
<author confidence="0.999907">Gerber Y Chai</author>
<affiliation confidence="0.99992">Department of Computer Science Michigan State University</affiliation>
<address confidence="0.999198">East Lansing, Michigan, USA</address>
<abstract confidence="0.997602888888889">Despite its substantial coverage, Nom- Bank does not account for all withinsentence arguments and ignores extrasentential arguments altogether. These arwhich we call are important to semantic processing, and their recovery could potentially benefit many NLP applications. We present a study of implicit arguments for a select group of frequent nominal predicates. We show that implicit arguments are pervasive for these predicates, adding 65% to the coverage of NomBank. We demonstrate the feasibility of recovering implicit arguments with a supervised classification model. Our results and analyses provide a baseline for future work on this emerging task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Aljoscha Burchardt</author>
<author>Anette Frank</author>
<author>Manfred Pinkal</author>
</authors>
<title>Building text meaning representations from contextually related frames - a case study.</title>
<date>2005</date>
<booktitle>In Proceedings of the Sixth International Workshop on Computational Semantics.</booktitle>
<contexts>
<context position="5846" citStr="Burchardt et al. (2005)" startWordPosition="907" endWordPosition="910">al case frames to identify antecedents. However, as noted by Iida et al., grammatical cases do not stand in a one-to-one relationship with semantic roles in Japanese (the same is true for English). Fillmore and Baker (2001) provided a detailed case study of implicit arguments (termed null instantiations in that work), but did not provide concrete methods to account for them automatically. Previously, we demonstrated the importance of filtering out nominal predicates that take no local arguments (Gerber et al., 2009); however, this work did not address the identification of implicit arguments. Burchardt et al. (2005) suggested approaches to implicit argument identification based on observed coreference patterns; however, the authors did not implement and evaluate such methods. We draw insights from all three of these studies. We show that the identification of implicit arguments for nominal predicates leads to fuller semantic interpretations when compared to traditional SRL methods. Furthermore, motivated by Burchardt et al., our model uses a quantitative analysis of naturally occurring coreference patterns to aid implicit argument identification. Most recently, Ruppenhofer et al. (2009) conducted SemEval</context>
</contexts>
<marker>Burchardt, Frank, Pinkal, 2005</marker>
<rawString>Aljoscha Burchardt, Anette Frank, and Manfred Pinkal. 2005. Building text meaning representations from contextually related frames - a case study. In Proceedings of the Sixth International Workshop on Computational Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Introduction to the CoNLL-2005 shared task: Semantic role labeling.</title>
<date>2005</date>
<marker>Carreras, M`arquez, 2005</marker>
<rawString>Xavier Carreras and Llu´ıs M`arquez. 2005. Introduction to the CoNLL-2005 shared task: Semantic role labeling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Unsupervised learning of narrative event chains.</title>
<date>2008</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<pages>789--797</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="18562" citStr="Chambers and Jurafsky (2008)" startWordPosition="2964" endWordPosition="2967">e mention, The two companies, which is the argo of produce. As described in Table 2, feature 1 is instantiated with a value of create.agentsend.agent, where create and send are the VerbNet classes that contain produce and ship, respectively. In the conversion to LibLinear’s instance representation, this instantiation is converted into a single binary feature create.agent-send.agent whose value is one. Features 1 and 11 are instantiated once for each mention in c&apos;, allowing the model to consider information from multiple mentions of the same entity. Features 2 and 5 are inspired by the work of Chambers and Jurafsky (2008), who investigated unsupervised learning of narrative event sequences using pointwise mutual information (PMI) between syntactic positions. We used a similar PMI score, but defined it with respect to semantic arguments instead of syntactic dependencies. Thus, the values for features 2 and 5 are computed as follows (the notation is explained in 5http://verbs.colorado.edu/semlink the caption for Table 2): pmi((p, iargn) , (pf, argf)) = P.ref((p, iargn) , (pf, argf)) P��ref((p, iargn) , *)P.ref((pf, argf) , *) (6) To compute Equation 6, we first labeled a subset of the Gigaword corpus (Graff, 200</context>
</contexts>
<marker>Chambers, Jurafsky, 2008</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2008. Unsupervised learning of narrative event chains. In Proceedings of the Association for Computational Linguistics, pages 789–797, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measurement,</booktitle>
<pages>20--1</pages>
<contexts>
<context position="10673" citStr="Cohen, 1960" startWordPosition="1690" endWordPosition="1691">two mentions of the thing invested in (iarg2). Our data set was also independently annotated by an undergraduate linguistics student. For each missing argument position, the student was asked to identify the closest acceptable implicit argument within the current and preceding sentences. The argument position was left unfilled if no acceptable constituent could be found. For a missing argument position, the student’s annotation agreed with our own if both identified the same constituent or both left the position unfilled. Analysis indicated an agreement of 67% using Cohen’s kappa coefficient (Cohen, 1960). 3.2 Annotation analysis Role coverage for a predicate instance is equal to the number of filled roles divided by the number of roles in the predicate’s lexicon entry. Role coverage for the marked predicate in Example 2 is 0/3 for NomBank-only arguments and 3/3 when the annotated implicit arguments are also considered. Returning to Table 1, the third column gives role coverage percentages for NomBank-only arguments. The sixth column gives role coverage percentages when both NomBank arguments and the annotated implicit arguments are considered. Overall, the addition of implicit arguments creat</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1):3746.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley Efron</author>
<author>Robert J Tibshirani</author>
</authors>
<title>An Introduction to the Bootstrap.</title>
<date>1993</date>
<publisher>Chapman &amp; Hall,</publisher>
<location>New York.</location>
<contexts>
<context position="26282" citStr="Efron and Tibshirani (1993)" startWordPosition="4244" endWordPosition="4247">ints and 18.8 points, respectively. Overall, the discriminative model increased F1 performance 15.8 points (59.6%) over the baseline. We measured human performance on this task by running our undergraduate assistant’s annotations against the evaluation data. Our assistant achieved an overall F1 score of 58.4% using the same candidate window as the baseline and discriminative models. The difference in F1 between the discriminative and human results had an exact p-value of less than 0.001. All significance testing was performed using a two-tailed bootstrap method similar to the one described by Efron and Tibshirani (1993). 6 Discussion 6.1 Feature ablation We conducted an ablation study to measure the contribution of specific feature sets. Table 4 presents the ablation configurations and results. For each configuration, we retrained and retested the discriminative model using the features described. As shown, we observed significant losses when excluding features that relate the semantic roles of mentions in c&apos; to the semantic role Percent change (p-value) Configuration P R F1 Remove 1,2,5 -35.3 -36.1 -35.7 (&lt; 0.01) (&lt; 0.01) (&lt; 0.01) Use 1,2,5 only -26.3 -11.9 -19.2 (&lt; 0.01) (0.05) (&lt; 0.01) Remove 14 0.2 1.0 0</context>
</contexts>
<marker>Efron, Tibshirani, 1993</marker>
<rawString>Bradley Efron and Robert J. Tibshirani. 1993. An Introduction to the Bootstrap. Chapman &amp; Hall, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A Library for Large Linear Classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="15255" citStr="Fan et al., 2008" startWordPosition="2409" endWordPosition="2412"> c as the primary filler, differentiating it from other mentions in the coreference chain c&apos;. In the following section, we present the feature set used to represent each three-tuple within the classification function. 4.2 Model features Starting with a wide range of features, we performed floating forward feature selection (Pudil et al., 1994) over held-out development data comprising implicit argument annotations from section 24 of the Penn TreeBank. As part of the feature selection process, we conducted a grid search for the best per-class cost within LibLinear’s logistic regression solver (Fan et al., 2008). This was done to reduce the negative effects of data imbalance, which is severe even when selecting candidates from the current and previous few sentences. Table 2 shows the selected features, which are quite different from those used in our previous work to identify traditional semantic arguments (Gerber et al., 2009).4 Below, we give further explanations for some of the features. Feature 1 models the semantic role relationship between each mention in c&apos; and the missing argument position iargn. To reduce data sparsity, this feature generalizes predicates and argument positions to their Verb</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A Library for Large Linear Classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database (Language, Speech, and Communication).</title>
<date>1998</date>
<publisher>The MIT Press,</publisher>
<contexts>
<context position="16908" citStr="Fellbaum, 1998" startWordPosition="2684" endWordPosition="2685">class/role of p/iargn. 2* Average pointwise mutual information between (p, iargn) and any (pf, argf). 3 Percentage of all f that are definite noun phrases. 4 Minimum absolute sentence distance from any f to p. 5* Minimum pointwise mutual information between (p, iargn) and any (pf, argf). 6 Frequency of the nominal form of p within the document that contains it. 7 Nominal form of p concatenated with iargn. 8 Nominal form of p concatenated with the sorted integer argument indexes from all argn of p. 9 Number of mentions in c&apos;. 10* Head word of p’s right sibling node. 11 For every f, the synset (Fellbaum, 1998) for the head of f concatenated with p and iargn. 12 Part of speech of the head of p’s parent node. 13 Average absolute sentence distance from any f to p. 14* Discourse relation whose two discourse units cover c (the primary filler) and p. 15 Number of left siblings of p. 16 Whether p is the head of its parent node. 17 Number of right siblings of p. Table 2: Features for determining whether c fills iargn of predicate p. For each mention f (denoting a filler) in the coreference chain c&apos;, we define pf and argf to be the predicate and argument position of f. Features are sorted in descending orde</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database (Language, Speech, and Communication). The MIT Press, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Fillmore</author>
<author>C F Baker</author>
</authors>
<title>Frame semantics for text understanding.</title>
<date>2001</date>
<booktitle>In Proceedings of WordNet and Other Lexical Resources Workshop,</booktitle>
<location>NAACL.</location>
<contexts>
<context position="5446" citStr="Fillmore and Baker (2001)" startWordPosition="843" endWordPosition="846">e of interest. This behavior has been annotated manually by Iida et al. (2007), and researchers have applied standard SRL techniques to this corpus, resulting in systems that are able to identify missing case-marked expressions in the surrounding discourse (Imamura et al., 2009). Sasano et al. (2004) conducted similar work with Japanese indirect anaphora. The authors used automatically derived nominal case frames to identify antecedents. However, as noted by Iida et al., grammatical cases do not stand in a one-to-one relationship with semantic roles in Japanese (the same is true for English). Fillmore and Baker (2001) provided a detailed case study of implicit arguments (termed null instantiations in that work), but did not provide concrete methods to account for them automatically. Previously, we demonstrated the importance of filtering out nominal predicates that take no local arguments (Gerber et al., 2009); however, this work did not address the identification of implicit arguments. Burchardt et al. (2005) suggested approaches to implicit argument identification based on observed coreference patterns; however, the authors did not implement and evaluate such methods. We draw insights from all three of t</context>
</contexts>
<marker>Fillmore, Baker, 2001</marker>
<rawString>C.J. Fillmore and C.F. Baker. 2001. Frame semantics for text understanding. In Proceedings of WordNet and Other Lexical Resources Workshop, NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Gerber</author>
<author>Joyce Y Chai</author>
<author>Adam Meyers</author>
</authors>
<title>The role of implicit argumentation in nominal SRL.</title>
<date>2009</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>146--154</pages>
<location>Boulder, Colorado, USA,</location>
<contexts>
<context position="1059" citStr="Gerber et al., 2009" startWordPosition="152" endWordPosition="155">ssing, and their recovery could potentially benefit many NLP applications. We present a study of implicit arguments for a select group of frequent nominal predicates. We show that implicit arguments are pervasive for these predicates, adding 65% to the coverage of NomBank. We demonstrate the feasibility of recovering implicit arguments with a supervised classification model. Our results and analyses provide a baseline for future work on this emerging task. 1 Introduction Verbal and nominal semantic role labeling (SRL) have been studied independently of each other (Carreras and M`arquez, 2005; Gerber et al., 2009) as well as jointly (Surdeanu et al., 2008; Hajiˇc et al., 2009). These studies have demonstrated the maturity of SRL within an evaluation setting that restricts the argument search space to the sentence containing the predicate of interest. However, as shown by the following example from the Penn TreeBank (Marcus et al., 1993), this restriction excludes extra-sentential arguments: (1) [arg0 The two companies] [pred produce] [arg1 market pulp, containerboard and white paper]. The goods could be manufactured closer to customers, saving [pred shipping] costs. The first sentence in Example 1 incl</context>
<context position="5744" citStr="Gerber et al., 2009" startWordPosition="891" endWordPosition="894">onducted similar work with Japanese indirect anaphora. The authors used automatically derived nominal case frames to identify antecedents. However, as noted by Iida et al., grammatical cases do not stand in a one-to-one relationship with semantic roles in Japanese (the same is true for English). Fillmore and Baker (2001) provided a detailed case study of implicit arguments (termed null instantiations in that work), but did not provide concrete methods to account for them automatically. Previously, we demonstrated the importance of filtering out nominal predicates that take no local arguments (Gerber et al., 2009); however, this work did not address the identification of implicit arguments. Burchardt et al. (2005) suggested approaches to implicit argument identification based on observed coreference patterns; however, the authors did not implement and evaluate such methods. We draw insights from all three of these studies. We show that the identification of implicit arguments for nominal predicates leads to fuller semantic interpretations when compared to traditional SRL methods. Furthermore, motivated by Burchardt et al., our model uses a quantitative analysis of naturally occurring coreference patter</context>
<context position="15577" citStr="Gerber et al., 2009" startWordPosition="2461" endWordPosition="2464">ction (Pudil et al., 1994) over held-out development data comprising implicit argument annotations from section 24 of the Penn TreeBank. As part of the feature selection process, we conducted a grid search for the best per-class cost within LibLinear’s logistic regression solver (Fan et al., 2008). This was done to reduce the negative effects of data imbalance, which is severe even when selecting candidates from the current and previous few sentences. Table 2 shows the selected features, which are quite different from those used in our previous work to identify traditional semantic arguments (Gerber et al., 2009).4 Below, we give further explanations for some of the features. Feature 1 models the semantic role relationship between each mention in c&apos; and the missing argument position iargn. To reduce data sparsity, this feature generalizes predicates and argument positions to their VerbNet (Kipper, 2005) classes and 3We used OpenNLP for coreference identification: http://opennlp.sourceforge.net 4We have omitted many of the lowest-ranked features. Descriptions of these features can be obtained by contacting the authors. Implicit arguments resolved 0.9 0.8 0.7 0.6 0.5 0.4 1 0 2 4 6 8 10 12 18 28 46 Sente</context>
<context position="19271" citStr="Gerber et al. (2009)" startWordPosition="3080" endWordPosition="3083">ual information (PMI) between syntactic positions. We used a similar PMI score, but defined it with respect to semantic arguments instead of syntactic dependencies. Thus, the values for features 2 and 5 are computed as follows (the notation is explained in 5http://verbs.colorado.edu/semlink the caption for Table 2): pmi((p, iargn) , (pf, argf)) = P.ref((p, iargn) , (pf, argf)) P��ref((p, iargn) , *)P.ref((pf, argf) , *) (6) To compute Equation 6, we first labeled a subset of the Gigaword corpus (Graff, 2003) using the verbal SRL system of Punyakanok et al. (2008) and the nominal SRL system of Gerber et al. (2009). We then identified coreferent pairs of arguments using OpenNLP. Suppose the resulting data has N coreferential pairs of argument positions. Also suppose that M of these pairs comprise (p, argn) and (pf, argf). The numerator in Equation 6 is defined as N . Each term in the denominator is obtained similarly, except that M is computed as the total number of coreference pairs comprising an argument position (e.g., (p, argn)) and any other argument position. Like Chambers and Jurafsky, we also used the discounting method suggested by Pantel and Ravichandran (2004) for lowfrequency observations. T</context>
</contexts>
<marker>Gerber, Chai, Meyers, 2009</marker>
<rawString>Matthew Gerber, Joyce Y. Chai, and Adam Meyers. 2009. The role of implicit argumentation in nominal SRL. In Proceedings of the North American Chapter of the Association for Computational Linguistics, pages 146–154, Boulder, Colorado, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
</authors>
<title>English Gigaword. Linguistic Data Consortium,</title>
<date>2003</date>
<location>Philadelphia.</location>
<contexts>
<context position="19164" citStr="Graff, 2003" startWordPosition="3061" endWordPosition="3062">sky (2008), who investigated unsupervised learning of narrative event sequences using pointwise mutual information (PMI) between syntactic positions. We used a similar PMI score, but defined it with respect to semantic arguments instead of syntactic dependencies. Thus, the values for features 2 and 5 are computed as follows (the notation is explained in 5http://verbs.colorado.edu/semlink the caption for Table 2): pmi((p, iargn) , (pf, argf)) = P.ref((p, iargn) , (pf, argf)) P��ref((p, iargn) , *)P.ref((pf, argf) , *) (6) To compute Equation 6, we first labeled a subset of the Gigaword corpus (Graff, 2003) using the verbal SRL system of Punyakanok et al. (2008) and the nominal SRL system of Gerber et al. (2009). We then identified coreferent pairs of arguments using OpenNLP. Suppose the resulting data has N coreferential pairs of argument positions. Also suppose that M of these pairs comprise (p, argn) and (pf, argf). The numerator in Equation 6 is defined as N . Each term in the denominator is obtained similarly, except that M is computed as the total number of coreference pairs comprising an argument position (e.g., (p, argn)) and any other argument position. Like Chambers and Jurafsky, we al</context>
</contexts>
<marker>Graff, 2003</marker>
<rawString>David Graff. 2003. English Gigaword. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jan Hajiˇc</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
<author>Daisuke Kawahara</author>
<author>Maria Ant`onia Mart´ı</author>
<author>Llu´ıs M`arquez</author>
<author>Adam Meyers</author>
<author>Joakim Nivre</author>
<author>Sebastian Pad´o</author>
<author>Jan ˇStˇep´anek</author>
<author>Pavel Straˇn´ak</author>
<author>Mihai Surdeanu</author>
<author>Nianwen Xue</author>
<author>Yi Zhang</author>
</authors>
<title>The CoNLL2009 shared task: Syntactic and semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL 2009): Shared Task,</booktitle>
<pages>1--18</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<marker>Hajiˇc, Ciaramita, Johansson, Kawahara, Mart´ı, M`arquez, Meyers, Nivre, Pad´o, ˇStˇep´anek, Straˇn´ak, Surdeanu, Xue, Zhang, 2009</marker>
<rawString>Jan Hajiˇc, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs M`arquez, Adam Meyers, Joakim Nivre, Sebastian Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu, Nianwen Xue, and Yi Zhang. 2009. The CoNLL2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL 2009): Shared Task, pages 1–18, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryu Iida</author>
<author>Mamoru Komachi</author>
<author>Kentaro Inui</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Annotating a Japanese text corpus with predicate-argument and coreference relations.</title>
<date>2007</date>
<booktitle>In Proceedings of the Linguistic Annotation Workshop in ACL-2007,</booktitle>
<pages>132139</pages>
<contexts>
<context position="4899" citStr="Iida et al. (2007)" startWordPosition="758" endWordPosition="761">onclude in Section 7. 2 Related work Palmer et al. (1986) made one of the earliest attempts to automatically recover extra-sentential arguments. Their approach used a fine-grained domain model to assess the compatibility of candidate arguments and the slots needing to be filled. A phenomenon similar to the implicit argument has been studied in the context of Japanese anaphora resolution, where a missing case-marked constituent is viewed as a zero-anaphoric expression whose antecedent is treated as the implicit argument of the predicate of interest. This behavior has been annotated manually by Iida et al. (2007), and researchers have applied standard SRL techniques to this corpus, resulting in systems that are able to identify missing case-marked expressions in the surrounding discourse (Imamura et al., 2009). Sasano et al. (2004) conducted similar work with Japanese indirect anaphora. The authors used automatically derived nominal case frames to identify antecedents. However, as noted by Iida et al., grammatical cases do not stand in a one-to-one relationship with semantic roles in Japanese (the same is true for English). Fillmore and Baker (2001) provided a detailed case study of implicit arguments</context>
</contexts>
<marker>Iida, Komachi, Inui, Matsumoto, 2007</marker>
<rawString>Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji Matsumoto. 2007. Annotating a Japanese text corpus with predicate-argument and coreference relations. In Proceedings of the Linguistic Annotation Workshop in ACL-2007, page 132139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Imamura</author>
<author>Kuniko Saito</author>
<author>Tomoko Izumi</author>
</authors>
<title>Discriminative approach to predicateargument structure analysis with zero-anaphora resolution.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,</booktitle>
<pages>85--88</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="5100" citStr="Imamura et al., 2009" startWordPosition="789" endWordPosition="792">sess the compatibility of candidate arguments and the slots needing to be filled. A phenomenon similar to the implicit argument has been studied in the context of Japanese anaphora resolution, where a missing case-marked constituent is viewed as a zero-anaphoric expression whose antecedent is treated as the implicit argument of the predicate of interest. This behavior has been annotated manually by Iida et al. (2007), and researchers have applied standard SRL techniques to this corpus, resulting in systems that are able to identify missing case-marked expressions in the surrounding discourse (Imamura et al., 2009). Sasano et al. (2004) conducted similar work with Japanese indirect anaphora. The authors used automatically derived nominal case frames to identify antecedents. However, as noted by Iida et al., grammatical cases do not stand in a one-to-one relationship with semantic roles in Japanese (the same is true for English). Fillmore and Baker (2001) provided a detailed case study of implicit arguments (termed null instantiations in that work), but did not provide concrete methods to account for them automatically. Previously, we demonstrated the importance of filtering out nominal predicates that t</context>
</contexts>
<marker>Imamura, Saito, Izumi, 2009</marker>
<rawString>Kenji Imamura, Kuniko Saito, and Tomoko Izumi. 2009. Discriminative approach to predicateargument structure analysis with zero-anaphora resolution. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 85–88, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Kingsbury</author>
<author>M Palmer</author>
<author>M Marcus</author>
</authors>
<title>Adding semantic annotation to the Penn TreeBank.</title>
<date>2002</date>
<booktitle>In Proceedings of the Human Language Technology Conference (HLT’02).</booktitle>
<contexts>
<context position="1701" citStr="Kingsbury et al., 2002" startWordPosition="251" endWordPosition="254"> (Surdeanu et al., 2008; Hajiˇc et al., 2009). These studies have demonstrated the maturity of SRL within an evaluation setting that restricts the argument search space to the sentence containing the predicate of interest. However, as shown by the following example from the Penn TreeBank (Marcus et al., 1993), this restriction excludes extra-sentential arguments: (1) [arg0 The two companies] [pred produce] [arg1 market pulp, containerboard and white paper]. The goods could be manufactured closer to customers, saving [pred shipping] costs. The first sentence in Example 1 includes the PropBank (Kingsbury et al., 2002) analysis of the verbal predicate produce, where arg0 is the agentive producer and arg1 is the produced entity. The second sentence contains an instance of the nominal predicate shipping that is not associated with arguments in NomBank (Meyers, 2007). From the sentences in Example 1, the reader can infer that The two companies refers to the agents (arg0) of the shipping predicate. The reader can also infer that market pulp, containerboard and white paper refers to the shipped entities (arg1 of shipping).1 These extra-sentential arguments have not been annotated for the shipping predicate and c</context>
</contexts>
<marker>Kingsbury, Palmer, Marcus, 2002</marker>
<rawString>P. Kingsbury, M. Palmer, and M. Marcus. 2002. Adding semantic annotation to the Penn TreeBank. In Proceedings of the Human Language Technology Conference (HLT’02).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper</author>
</authors>
<title>VerbNet: A broad-coverage, comprehensive verb lexicon.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer and Information Science University of Pennsylvania.</institution>
<contexts>
<context position="15873" citStr="Kipper, 2005" startWordPosition="2509" endWordPosition="2510">s was done to reduce the negative effects of data imbalance, which is severe even when selecting candidates from the current and previous few sentences. Table 2 shows the selected features, which are quite different from those used in our previous work to identify traditional semantic arguments (Gerber et al., 2009).4 Below, we give further explanations for some of the features. Feature 1 models the semantic role relationship between each mention in c&apos; and the missing argument position iargn. To reduce data sparsity, this feature generalizes predicates and argument positions to their VerbNet (Kipper, 2005) classes and 3We used OpenNLP for coreference identification: http://opennlp.sourceforge.net 4We have omitted many of the lowest-ranked features. Descriptions of these features can be obtained by contacting the authors. Implicit arguments resolved 0.9 0.8 0.7 0.6 0.5 0.4 1 0 2 4 6 8 10 12 18 28 46 Sentences prior 1586 # Feature value description 1* For every f, the VerbNet class/role of pf/argf concatenated with the class/role of p/iargn. 2* Average pointwise mutual information between (p, iargn) and any (pf, argf). 3 Percentage of all f that are definite noun phrases. 4 Minimum absolute sente</context>
</contexts>
<marker>Kipper, 2005</marker>
<rawString>Karin Kipper. 2005. VerbNet: A broad-coverage, comprehensive verb lexicon. Ph.D. thesis, Department of Computer and Information Science University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn TreeBank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="1388" citStr="Marcus et al., 1993" startWordPosition="205" endWordPosition="208">rguments with a supervised classification model. Our results and analyses provide a baseline for future work on this emerging task. 1 Introduction Verbal and nominal semantic role labeling (SRL) have been studied independently of each other (Carreras and M`arquez, 2005; Gerber et al., 2009) as well as jointly (Surdeanu et al., 2008; Hajiˇc et al., 2009). These studies have demonstrated the maturity of SRL within an evaluation setting that restricts the argument search space to the sentence containing the predicate of interest. However, as shown by the following example from the Penn TreeBank (Marcus et al., 1993), this restriction excludes extra-sentential arguments: (1) [arg0 The two companies] [pred produce] [arg1 market pulp, containerboard and white paper]. The goods could be manufactured closer to customers, saving [pred shipping] costs. The first sentence in Example 1 includes the PropBank (Kingsbury et al., 2002) analysis of the verbal predicate produce, where arg0 is the agentive producer and arg1 is the produced entity. The second sentence contains an instance of the nominal predicate shipping that is not associated with arguments in NomBank (Meyers, 2007). From the sentences in Example 1, th</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn TreeBank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Meyers</author>
</authors>
<title>Annotation guidelines for NomBank - noun argument structure for PropBank.</title>
<date>2007</date>
<tech>Technical report,</tech>
<location>New York University.</location>
<contexts>
<context position="1951" citStr="Meyers, 2007" startWordPosition="295" endWordPosition="296">ample from the Penn TreeBank (Marcus et al., 1993), this restriction excludes extra-sentential arguments: (1) [arg0 The two companies] [pred produce] [arg1 market pulp, containerboard and white paper]. The goods could be manufactured closer to customers, saving [pred shipping] costs. The first sentence in Example 1 includes the PropBank (Kingsbury et al., 2002) analysis of the verbal predicate produce, where arg0 is the agentive producer and arg1 is the produced entity. The second sentence contains an instance of the nominal predicate shipping that is not associated with arguments in NomBank (Meyers, 2007). From the sentences in Example 1, the reader can infer that The two companies refers to the agents (arg0) of the shipping predicate. The reader can also infer that market pulp, containerboard and white paper refers to the shipped entities (arg1 of shipping).1 These extra-sentential arguments have not been annotated for the shipping predicate and cannot be identified by a system that restricts the argument search space to the sentence containing the predicate. NomBank also ignores many within-sentence arguments. This is shown in the second sentence of Example 1, where The goods can be interpre</context>
</contexts>
<marker>Meyers, 2007</marker>
<rawString>Adam Meyers. 2007. Annotation guidelines for NomBank - noun argument structure for PropBank. Technical report, New York University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha S Palmer</author>
<author>Deborah A Dahl</author>
<author>Rebecca J Schiffman</author>
<author>Lynette Hirschman</author>
<author>Marcia Linebarger</author>
<author>John Dowding</author>
</authors>
<title>Recovering implicit information.</title>
<date>1986</date>
<booktitle>In Proceedings of the 24th annual meeting on Association for Computational Linguistics,</booktitle>
<pages>10--19</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="4338" citStr="Palmer et al. (1986)" startWordPosition="668" endWordPosition="671">led. 1583 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1583–1592, Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics In the following section, we review related research, which is historically sparse but recently gaining traction. We present our annotation effort in Section 3, and follow with our implicit argument identification model in Section 4. In Section 5, we describe the evaluation setting and present our experimental results. We analyze these results in Section 6 and conclude in Section 7. 2 Related work Palmer et al. (1986) made one of the earliest attempts to automatically recover extra-sentential arguments. Their approach used a fine-grained domain model to assess the compatibility of candidate arguments and the slots needing to be filled. A phenomenon similar to the implicit argument has been studied in the context of Japanese anaphora resolution, where a missing case-marked constituent is viewed as a zero-anaphoric expression whose antecedent is treated as the implicit argument of the predicate of interest. This behavior has been annotated manually by Iida et al. (2007), and researchers have applied standard</context>
</contexts>
<marker>Palmer, Dahl, Schiffman, Hirschman, Linebarger, Dowding, 1986</marker>
<rawString>Martha S. Palmer, Deborah A. Dahl, Rebecca J. Schiffman, Lynette Hirschman, Marcia Linebarger, and John Dowding. 1986. Recovering implicit information. In Proceedings of the 24th annual meeting on Association for Computational Linguistics, pages 10–19, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Deepak Ravichandran</author>
</authors>
<title>Automatically labeling semantic classes.</title>
<date>2004</date>
<booktitle>HLT-NAACL 2004: Main Proceedings,</booktitle>
<volume>2</volume>
<pages>321--328</pages>
<editor>In Daniel Marcu Susan Dumais and Salim Roukos, editors,</editor>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="19838" citStr="Pantel and Ravichandran (2004)" startWordPosition="3174" endWordPosition="3177">t al. (2008) and the nominal SRL system of Gerber et al. (2009). We then identified coreferent pairs of arguments using OpenNLP. Suppose the resulting data has N coreferential pairs of argument positions. Also suppose that M of these pairs comprise (p, argn) and (pf, argf). The numerator in Equation 6 is defined as N . Each term in the denominator is obtained similarly, except that M is computed as the total number of coreference pairs comprising an argument position (e.g., (p, argn)) and any other argument position. Like Chambers and Jurafsky, we also used the discounting method suggested by Pantel and Ravichandran (2004) for lowfrequency observations. The PMI score is somewhat noisy due to imperfect output, but it provides information that is useful for classification. log 1587 Feature 10 does not depend on c&apos; and is specific to each predicate. Consider the following example: (7) Statistics Canada reported that its [arg1 industrial-product] [p price] index dropped 2% in September. The “[p price] index” collocation is rarely associated with an arg0 in NomBank or with an iarg0 in our annotations (both argument positions denote the seller). Feature 10 accounts for this type of behavior by encoding the syntactic </context>
</contexts>
<marker>Pantel, Ravichandran, 2004</marker>
<rawString>Patrick Pantel and Deepak Ravichandran. 2004. Automatically labeling semantic classes. In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings, pages 321–328, Boston, Massachusetts, USA, May 2 -May 7. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Alan Lee</author>
<author>Nikhil Dinesh</author>
<author>Eleni Miltsakaki</author>
<author>Geraud Campion</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<title>Penn discourse treebank version 2.0. Linguistic Data Consortium,</title>
<date>2008</date>
<contexts>
<context position="21636" citStr="Prasad et al., 2008" startWordPosition="3470" endWordPosition="3473">d predicate p. Consider the following example: (9) [iarg0 SFE Technologies] reported a net loss of $889,000 on sales of $23.4 million. (10) That compared with an operating [p loss] of [arg1 $1.9 million] on sales of $27.4 million in the year-earlier period. In this case, a comparison discourse relation (signaled by the underlined text) holds between the first and sentence sentence. The coherence provided by this relation encourages an inference that identifies the marked iarg0 (the loser). Throughout our study, we used gold-standard discourse relations provided by the Penn Discourse TreeBank (Prasad et al., 2008). 5 Evaluation We trained the feature-based logistic regression model over 816 annotated predicate instances associated with 650 implicitly filled argument positions (not all predicate instances had implicit arguments). During training, a candidate three-tuple (p, iarg,,,, c&apos;) was given a positive label if the candidate implicit argument c (the primary filler) was annotated as filling the missing argument position. To factor out errors from standard SRL analyses, the model used gold-standard argument labels provided by PropBank and NomBank. As shown in Figure 1 (Section 3.2), implicit argument</context>
</contexts>
<marker>Prasad, Lee, Dinesh, Miltsakaki, Campion, Joshi, Webber, 2008</marker>
<rawString>Rashmi Prasad, Alan Lee, Nikhil Dinesh, Eleni Miltsakaki, Geraud Campion, Aravind Joshi, and Bonnie Webber. 2008. Penn discourse treebank version 2.0. Linguistic Data Consortium, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pudil</author>
<author>J Novovicova</author>
<author>J Kittler</author>
</authors>
<title>Floating search methods in feature selection.</title>
<date>1994</date>
<journal>Pattern Recognition Letters,</journal>
<pages>15--1119</pages>
<contexts>
<context position="14983" citStr="Pudil et al., 1994" startWordPosition="2367" endWordPosition="2370">its coreferent constituents.3 We defined a binary classification function Pr(+ |(p, iargn, c&apos;)) that predicts the probability that the entity referred to by c fills the missing argument position iargn of predicate instance p. In the remainder of this paper, we will refer to c as the primary filler, differentiating it from other mentions in the coreference chain c&apos;. In the following section, we present the feature set used to represent each three-tuple within the classification function. 4.2 Model features Starting with a wide range of features, we performed floating forward feature selection (Pudil et al., 1994) over held-out development data comprising implicit argument annotations from section 24 of the Penn TreeBank. As part of the feature selection process, we conducted a grid search for the best per-class cost within LibLinear’s logistic regression solver (Fan et al., 2008). This was done to reduce the negative effects of data imbalance, which is severe even when selecting candidates from the current and previous few sentences. Table 2 shows the selected features, which are quite different from those used in our previous work to identify traditional semantic arguments (Gerber et al., 2009).4 Bel</context>
</contexts>
<marker>Pudil, Novovicova, Kittler, 1994</marker>
<rawString>P. Pudil, J. Novovicova, and J. Kittler. 1994. Floating search methods in feature selection. Pattern Recognition Letters, 15:1119–1125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>The importance of syntactic parsing and inference in semantic role labeling.</title>
<date>2008</date>
<journal>Comput. Linguist.,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="19220" citStr="Punyakanok et al. (2008)" startWordPosition="3070" endWordPosition="3073">arning of narrative event sequences using pointwise mutual information (PMI) between syntactic positions. We used a similar PMI score, but defined it with respect to semantic arguments instead of syntactic dependencies. Thus, the values for features 2 and 5 are computed as follows (the notation is explained in 5http://verbs.colorado.edu/semlink the caption for Table 2): pmi((p, iargn) , (pf, argf)) = P.ref((p, iargn) , (pf, argf)) P��ref((p, iargn) , *)P.ref((pf, argf) , *) (6) To compute Equation 6, we first labeled a subset of the Gigaword corpus (Graff, 2003) using the verbal SRL system of Punyakanok et al. (2008) and the nominal SRL system of Gerber et al. (2009). We then identified coreferent pairs of arguments using OpenNLP. Suppose the resulting data has N coreferential pairs of argument positions. Also suppose that M of these pairs comprise (p, argn) and (pf, argf). The numerator in Equation 6 is defined as N . Each term in the denominator is obtained similarly, except that M is computed as the total number of coreference pairs comprising an argument position (e.g., (p, argn)) and any other argument position. Like Chambers and Jurafsky, we also used the discounting method suggested by Pantel and R</context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2008</marker>
<rawString>Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008. The importance of syntactic parsing and inference in semantic role labeling. Comput. Linguist., 34(2):257–287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Ruppenhofer</author>
<author>Caroline Sporleder</author>
<author>Roser Morante</author>
<author>Collin Baker</author>
<author>Martha Palmer</author>
</authors>
<title>Semeval-2010 task 10: Linking events and their participants in discourse.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions (SEW-2009),</booktitle>
<pages>106--111</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="6428" citStr="Ruppenhofer et al. (2009)" startWordPosition="992" endWordPosition="995">implicit arguments. Burchardt et al. (2005) suggested approaches to implicit argument identification based on observed coreference patterns; however, the authors did not implement and evaluate such methods. We draw insights from all three of these studies. We show that the identification of implicit arguments for nominal predicates leads to fuller semantic interpretations when compared to traditional SRL methods. Furthermore, motivated by Burchardt et al., our model uses a quantitative analysis of naturally occurring coreference patterns to aid implicit argument identification. Most recently, Ruppenhofer et al. (2009) conducted SemEval Task 10, “Linking Events and Their Participants in Discourse”, which evaluated implicit argument identification systems over a common test set. The task organizers annotated implicit arguments across entire passages, resulting in data that cover many distinct predicates, each associated with a small number of annotated instances. In contrast, our study focused on a select group of nominal predicates, each associated with a large number of annotated instances. 3 Data annotation and analysis 3.1 Data annotation Implicit arguments have not been annotated within the Penn TreeBan</context>
<context position="23048" citStr="Ruppenhofer et al. (2009)" startWordPosition="3689" endWordPosition="3692">a. We compared our supervised model with the simple baseline heuristic defined below:6 Fill iarg,,, for predicate instance p with the nearest constituent in the twosentence candidate window that fills arg,,, for a different instance of p, where all nominal predicates are normalized to their verbal forms. The normalization allows an existing arg0 for the verb invested to fill an iarg0 for the noun investment. We also evaluated an oracle model that made gold-standard predictions for candidates within the two-sentence prediction window. We evaluated these models using the methodology proposed by Ruppenhofer et al. (2009). For each missing argument position of a predicate instance, the models were required to either (1) identify a single constituent that fills the missing argument position or (2) make no prediction and leave the missing argument position unfilled. We scored predictions using the Dice coefficient, which is defined as follows: 2 * |PredictednTrue |(11) |Predicted |+ |True| Predicted is the set of tokens subsumed by the constituent predicted by the model as filling a missing argument position. True is the set of tokens from a single annotated constituent that fills the missing argument position. </context>
<context position="34015" citStr="Ruppenhofer et al. (2009)" startWordPosition="5463" endWordPosition="5466">rom using automatic coreference resolution, this process creates errors that need to be studied more closely. It will also be important to study implicit argument patterns of non-verbal predicates such as the partitive percent. These predicates are among the most frequent in the TreeBank and are likely to require approaches that differ from the ones we pursued. Finally, any extension of this work is likely to encounter a significant knowledge acquisition bottleneck. Implicit argument annotation is difficult because it requires both argument and coreference identification (the data produced by Ruppenhofer et al. (2009) is similar). Thus, it might be productive to focus future work on (1) the extraction of relevant knowledge from existing resources (e.g., our use of coreference patterns from Gigaword) or (2) semi-supervised learning of implicit argument models from a combination of labeled and unlabeled data. Acknowledgments We would like to thank the anonymous reviewers for their helpful questions and comments. We would also like to thank Malcolm Doering for his annotation effort. This work was supported in part by NSF grants IIS-0347548 and IIS-0840538. 7Our annotation data can be freely downloaded at http</context>
</contexts>
<marker>Ruppenhofer, Sporleder, Morante, Baker, Palmer, 2009</marker>
<rawString>Josef Ruppenhofer, Caroline Sporleder, Roser Morante, Collin Baker, and Martha Palmer. 2009. Semeval-2010 task 10: Linking events and their participants in discourse. In Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions (SEW-2009), pages 106–111, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryohei Sasano</author>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Automatic construction of nominal case frames and its application to indirect anaphora resolution.</title>
<date>2004</date>
<booktitle>In Proceedings of Coling</booktitle>
<pages>1201--1207</pages>
<publisher>COLING.</publisher>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="5122" citStr="Sasano et al. (2004)" startWordPosition="793" endWordPosition="796">of candidate arguments and the slots needing to be filled. A phenomenon similar to the implicit argument has been studied in the context of Japanese anaphora resolution, where a missing case-marked constituent is viewed as a zero-anaphoric expression whose antecedent is treated as the implicit argument of the predicate of interest. This behavior has been annotated manually by Iida et al. (2007), and researchers have applied standard SRL techniques to this corpus, resulting in systems that are able to identify missing case-marked expressions in the surrounding discourse (Imamura et al., 2009). Sasano et al. (2004) conducted similar work with Japanese indirect anaphora. The authors used automatically derived nominal case frames to identify antecedents. However, as noted by Iida et al., grammatical cases do not stand in a one-to-one relationship with semantic roles in Japanese (the same is true for English). Fillmore and Baker (2001) provided a detailed case study of implicit arguments (termed null instantiations in that work), but did not provide concrete methods to account for them automatically. Previously, we demonstrated the importance of filtering out nominal predicates that take no local arguments</context>
</contexts>
<marker>Sasano, Kawahara, Kurohashi, 2004</marker>
<rawString>Ryohei Sasano, Daisuke Kawahara, and Sadao Kurohashi. 2004. Automatic construction of nominal case frames and its application to indirect anaphora resolution. In Proceedings of Coling 2004, pages 1201–1207, Geneva, Switzerland, Aug 23–Aug 27. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Llu´ıs M`arquez</author>
<author>Joakim Nivre</author>
</authors>
<title>The CoNLL</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In CoNLL 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning,</booktitle>
<pages>159--177</pages>
<location>Manchester, England,</location>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>Mihai Surdeanu, Richard Johansson, Adam Meyers, Llu´ıs M`arquez, and Joakim Nivre. 2008. The CoNLL 2008 shared task on joint parsing of syntactic and semantic dependencies. In CoNLL 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning, pages 159–177, Manchester, England, August. Coling 2008 Organizing Committee.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>