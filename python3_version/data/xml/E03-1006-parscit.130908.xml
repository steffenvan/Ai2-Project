<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000222">
<title confidence="0.992255">
The adaptation of a machine-learned sentence realization system
to French
</title>
<author confidence="0.967358">
Martine Smets, Michael Gamon, Simon Corston-Oliver and Eric Ringger
</author>
<affiliation confidence="0.849782">
Microsoft Research
</affiliation>
<address confidence="0.909813">
One Microsoft Way
Redmond, WA 98052, USA
</address>
<email confidence="0.927302">
{ martines, mgamon, simonco, ringger } @microsoft.com
</email>
<sectionHeader confidence="0.995189" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999748307692308">
We describe the adaptation to French of a
machine-learned sentence realization
system called Amalgam that was
originally developed to be as language
independent as possible and was first
implemented for German. We discuss the
development of the French implementation
with particular attention to the degree to
which the original system could be re-
used, and we present the results of a
human evaluation of the quality of
sentence realization using the new French
system.
</bodyText>
<sectionHeader confidence="0.975704" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.999892307692308">
Recently, statistical and machine-learned
approaches have been applied to the sentence
realization phase of natural language generation.
The Nitrogen system, for example, uses a word
bigram language model to score and rank a large
set of alternative sentence realizations
(Langkilde and Knight, 1998a, 1998b). Other
recent approaches use syntactic representations.
FERGUS (Bangalore and Rambow, 2000),
Halogen (Langkilde 2000, Langkilde-Geary
2002) and Amalgam (Corston-Oliver et al.,
2002) use syntactic trees as an intermediate
representation to determine the optimal string
output.
The Amalgam system discussed here is a
sentence realization system which maps a
semantic representation to a surface syntactic
tree via intermediate syntactic representations.
The mappings are performed with linguistic
operations, the context for which is primarily
machine-learned. The resulting syntactic tree
contains all the necessary information on its leaf
nodes from which a surface string can be read.
The promise of machine-learned approaches to
sentence realization is that they can easily be
adapted to new domains and ideally to new
languages merely by retraining The architecture
of Amalgam was intended to be language-
independent, although the system has previously
only been applied to German sentence
realization. Adapting this system to French
allows us to assess which aspects of the system
are truly language-independent and what must be
added in order to account for French.
The purpose of this paper is to focus on the
adaptation of Amalgam to French. Discussions
about the general architecture of the system can
be found in Corston-Oliver et al. (2002) and
Gamon et al. (2002b).
</bodyText>
<sectionHeader confidence="0.83306" genericHeader="method">
1 Overview of German Amalgam
</sectionHeader>
<bodyText confidence="0.99996">
Amalgam takes as its input a logical form graph,
i.e., a sentence-level dependency graph with
fixed lexical choices for content words. This
graph represents the predicate-argument structure
of a sentence and includes semantic information
concerning relations between nodes of the graph
(Heidorn, 2002). Examples of French logical
forms are given in section 3. Amalgam first
degraphs the logical form into a tree and then
augments it by the insertion of function words,
</bodyText>
<page confidence="0.998495">
323
</page>
<bodyText confidence="0.993562285714286">
assignment of case and verb position features,
syntactic labels, etc., to produce an unordered
syntax tree. Amalgam then establishes intra-
constituent order. After syntactic aggregation,
insertion of punctuation, morphological
inflection, and capitalization, an output string is
read off the leaf nodes. The contexts for most of
these linguistic operations are machine-learned
(Gamon et al., 2002a). Figure 1 lists the eight
stages in German Amalgam: the label ML
denotes that the operation is applied in machine-
learned contexts, and the label Proc indicates
that the operation is procedural or deterministic.
Stage 1 Pre-processing (Proc)
</bodyText>
<listItem confidence="0.966690666666666">
• degraphing of the semantic
representation
• retrieval of lexical information
Stage 2 Flesh-Out (ML):
• assignment of syntactic labels
• insertion of function words
• assignment of case and verb position
features
Stage 3 Conversion to syntax tree (Proc):
• introduction of syntactic representation
for coordination
• splitting of separable prefix verbs based
on both lexical information and
previously assigned verb position
features
Stage 4 Movement:
• raising, wh movement (Proc)
Stage 5 Ordering (ML):
• ordering of constituents and leaf nodes in
the tree
Stage 6 Extraposition (ML)
Stage7 Surface clean-up (ML):
• lexical choice of determiners and relative
pronouns
• syntactic aggregation
Stage 8 Punctuation (ML)
Stage 9 Inflectional generation (Proc)
</listItem>
<figureCaption confidence="0.996155">
Figure 1 The stages of German Amalgam
</figureCaption>
<bodyText confidence="0.999893181818182">
All machine-learned components employ
decision trees for classification and for
probability distribution estimation (Gamon et al.,
2002b). The decision trees are built with the
WinMine toolkit (Chickering, 2002). There are a
total of twenty-one decision trees in the German
system. The complexity of the decision trees
varies with the complexity of the modeled task:
the number of branching nodes in the decision
tree models in the German system ranges from
just 4 to.7,876 in the order model.
</bodyText>
<sectionHeader confidence="0.540629" genericHeader="method">
2 Data and feature extraction
</sectionHeader>
<bodyText confidence="0.999983108108108">
The data for all models are automatically
extracted from of a set of 100,000 sentences
drawn from software manuals. Between 30,000
and one million cases are extracted from these
sentences, depending on the task to be modeled.
The sentences are analyzed in the NLPWin
system (Heidorn, 2002), which provides a
syntactic and logical form analysis. Nodes in the
logical form representation are linked to the
corresponding syntax nodes, allowing us to learn
contexts for the mapping from the semantic
representation to the surface syntax
representation. The data is split 70/30 for training
versus model parameter tuning. For each set of
data we build decision trees at several levels of
granularity and select the model with the
maximal accuracy as determined on the
parameter tuning set.
We attempt to standardize as much as possible
the set of features to be extracted. We exploit the
full set of features and attributes available in the
analysis, instead of pre-determining a small set
of potentially relevant features for each model.
This allows us to share the majority of code
among the individual feature extraction tasks and
among languages. Typically, we extract the full
set of available linguistic features of the node
under investigation, its parent and its
grandparent, with the only restriction being that
these features need to be available at the stage
where the model is consulted at generation run-
time. This yields approximately six hundred
features that provide a sufficiently large
structural context for the operations. In addition,
for some of the models we add a small set of
specially computed linguistic features that we
believe to be important for the task at hand.
</bodyText>
<sectionHeader confidence="0.986814" genericHeader="method">
3 French Amalgam
</sectionHeader>
<bodyText confidence="0.9781672">
French Amalgam re-uses the architecture of the
German system. Indeed, sentence realization
from a semantic graph must undergo many of the
same transformations regardless of the language:
pre-processing of the logical form, fleshing-out,
</bodyText>
<page confidence="0.994459">
324
</page>
<bodyText confidence="0.998549341463414">
conversion to syntax tree, etc. We outline below
the stages of the French system, and compare
them to the German system.
Stage 1, the pre-processing of the data, involves
language-neutral transformations from a graph
representation to a tree representation, and can be
reused without alteration by the French system.
The fleshing out of the logical form in Stage 2
required changes for French. French does not
need a machine-learned model for case. On the
other hand French requires a model for clitic
insertion which does not exist in German.
Language-specific details of feature selection for
Stage 2 will be discussed in section 3.2.
Because French does not have separable prefix
verbs, the lexical operation that splits prefixes in
German is not needed in Stage 3. French uses a
head-switching operation for verb phrases
headed by modal verbs, because of the status of
French modals as presented in section 3.1.3.
Stage 4 (raising and Wh movement) is identical
for both languages.
In stage 5, both German and French use a left-to-
right model of constituent order. For each
language, the model is a decision tree
representing the probability distributions
involved in ordering (see Ringger et al. (in
preparation) for a detailed discussion of different
approaches to constituent ordering).
Extraposition, which is common in German
(Gamon et al. 2002c), is rare in the French
technical software manuals: there were too few
examples of extraposition in the French data to
train an extraposition model for Stage 6.
Stage 7 (clean-up) uses language-specific
information, especially in the realization of
lexical forms of function words.
Finally, stage 8, the realization of inflection, is
completely language specific.
Figure 2 provides a summary of the French
Amalgam system.
</bodyText>
<figure confidence="0.92147247826087">
Stage 1 Pre-processing (Proc):
Degraphing of the logical form.
A Retrieval of lexical information.
Stage 2 Flesh-Out (ML):
A Assignment of syntactic labels_
A Insertion of function words.
A Insertion of clitics.
A Assignment of case (Proc).
Stage 3 Conversion to syntax tree (Proc)
A Introduction of syntactic representation
for coordination.
A Head-switching (ML).
Stage 4 Movement:
A Raising, wh movement (Procedural).
Stage 5 Ordering (ML):
A Ordering of constituents and leaf nodes
in the tree.
Stage 6 Surface clean-up (ML):
A Lexical choice of determiners and
relative pronouns.
A Syntactic aggregation.
Stage 7 Punctuation (ML)
Stage 8 Inflectional generation (Proc)
</figure>
<figureCaption confidence="0.999529">
Figure 2 The stages of French Amalgam
</figureCaption>
<bodyText confidence="0.999955933333334">
There are eighteen decision trees in the French
system, and the complexity of the decision trees
varies with the complexity of the task modeled.
The number of branching nodes in the decision
tree models in the French system ranges from 6
to 838, except for the order model which has
4682 branching nodes.
There are a number of differences between the
systems, some concerning models that are
language-specific, others relating to features
relevant only for one language. Most of the
differences are in feature extraction and in the
linguistic operations relying on the information
provided by the models. We discuss these
differences in the following sections
</bodyText>
<figure confidence="0.685797333333333">
etrei ((Verb) -Was,: +Pres +Perf +Proposition +Itsubj .
Time tentativet ((Noun] flors_de) -i-Def +Fem +Per
--de executionl ((Noun) +Fem +Pers3 +S.
&amp;quot;-----de requetel ({Noun) -i-Def +
---Possr vousl ({Pron}
sub conflitl ((Noun) +Indef 4-Foc 1lMasc +Pers3
</figure>
<figureCaption confidence="0.792784">
Figure 3 French logical form illustrating the i/y a construction
</figureCaption>
<page confidence="0.991832">
325
</page>
<subsectionHeader confidence="0.990352">
3.1 Models
</subsectionHeader>
<bodyText confidence="0.996118">
In this section we discuss the solutions adopted
for the treatment of case, clitics and modals in
the French Amalgam system.
</bodyText>
<subsubsectionHeader confidence="0.888313">
3.1.1 Case
</subsubsectionHeader>
<bodyText confidence="0.999864133333333">
As mentioned above, French generation does not
need a model for case, since case does not exist
anymore in French, except for some traces in the
pronominal system. Determining case for
pronouns is a trivial task in French that does not
require a machine-learned solution.
For example, le is the form of the third person
singular object clitic (accusative), while /ui is the
third person singular indirect object (or dative).
A knowledge-engineered module determines the
case of each pronoun on the basis of the
predicate-argument structure in our linguistic
representation, whereas the German system uses
a decision tree model to assign case to all
nominal constituents.
</bodyText>
<subsubsectionHeader confidence="0.976861">
3.1.2 Clitics
</subsubsectionHeader>
<bodyText confidence="0.9991275">
French clitics that function as arguments of the
verb are represented directly in the logical form.
Clitics that are used expletively are not
represented in the logical form, and thus need to
be inserted during sentence realization. For
example the clitic y in il y a (&amp;quot;there is&amp;quot;), is
inserted during the flesh-out stage. An example
is given in (1), with the logical form in Figure 3.
</bodyText>
<equation confidence="0.896733">
(1) 11
</equation>
<bodyText confidence="0.993646692307692">
y a eu un conflit lors de la tentative
d&apos;execution de votre requete
&amp;quot;There was a conflict at the time of
execution of your request.&amp;quot;
In the logical form in Figure 3, the verb avoir
(&apos;have&apos;) is represented by etre (&amp;quot;be&amp;quot;) and there is
no clitic y (`there&apos;), nor subject il (&apos;ie) . The
clitic y thus needs to be inserted in that
representation (as well as the expletive subject).
The context for y-insertion is learned and
represented in the clitic decision tree. This
component is necessary for French and would be
needed for other Romance languages also.
</bodyText>
<subsubsectionHeader confidence="0.969179">
3.1.3 Modals
</subsubsectionHeader>
<bodyText confidence="0.99004348275862">
Most of the changes in the French system were
required by modal verbs. In German and English
syntactic analysis, modals do not head a clause
but behave like auxiliaries. In French, however,
modals behave syntactically just like main verbs
(taking a clausal complement), but have semantic
properties characteristic of modals: pouvoir (&amp;quot;to
be able&amp;quot;) and devoir (&amp;quot;to have to&amp;quot;), for example,
can be used epistemically or deontically (Palmer,
1986). In our system, they share the same
semantic representation as modals of other
languages, although they do not exhibit the same
syntactic behavior.
In example (2), the modal pouvez (a second
person plural form of the verb pouvoir) is the
head of the main clause, and carries the syntactic
information of tense, mood, person number and
negation. The logical form for this example,
illustrated in Figure 4, is headed by envoyer (&amp;quot;to
send&amp;quot;). The modal pouvoir (&amp;quot;to be able&amp;quot;)
functions in the logical form as a semantic
modifier of the verb, and features such as tense,
mood, and negation are copied onto the semantic
head, envoyer.
(2) Vous ne pouvez pas envoyer un
message a plusieurs personnes en meme
temps.
&amp;quot;You cannot send a message to several
people at the same time.
</bodyText>
<table confidence="0.527958">
envoyerl (NerbI. +Modal +Pres +Neg +Indicat +Proposition
):-.1 odals--pouvoirL ({Verb} &lt;2&gt; +Pres +Indicat +Polite)
Time ----en_meme_tempsi ({Adv} +Comp +PosComp +FO +Tme)
sub ----vous ;Pron) +Fem +Masc +Pers2 +Plur +Anim +Hi
obj ----messagei ([Noun]. +Indef +Masc +Pers3 +Sing +Ca
rind ----personnel ({Noun} {a} +Indef +Quant +Fem +Peri
&apos;N.LOps ----plusieursi (fAdj} +Indef +Quant +Fem
</table>
<figureCaption confidence="0.823641">
Figure 4 French logical form illustrating a modal construction
</figureCaption>
<page confidence="0.997613">
326
</page>
<bodyText confidence="0.999914111111111">
During the generation process, the modal verb
has to become a syntactic head with a clausal
complement to reflect the syntactic properties of
French. The contexts for the operation which
switches semantic and syntactic headedness are
learned automatically. When a switch is
predicted, a knowledge-engineered module is
invoked to perform the switch and make the
necessary structural adjustments.&apos;
</bodyText>
<subsectionHeader confidence="0.99693">
3.2 Differences in features
</subsectionHeader>
<bodyText confidence="0.999951833333333">
Each decision tree must be trained for the given
language. Consequently, the decision trees
produced for each language may differ in their
target feature values or may require language-
dependent feature extraction.
The decision tree classifiers for the French
system are trained on a corpus of 100,000
sentences, drawn from technical software
manuals in French. The models are tested on a
test corpus of the same domain (but distinct from
the training corpus). All the examples in this
paper come from that technical corpus.
</bodyText>
<subsubsectionHeader confidence="0.988419">
3.2.1 Target feature values
</subsubsectionHeader>
<bodyText confidence="0.99955608">
Target feature values in many instances refer to
specific lexical items and are therefore language
specific. For example, for the insertion of
constituents such as auxiliaries, prepositions, and
infinitive markers, the value of the target feature
is the citation form of the word being inserted.
Thus, for most of the models of stage 2 (flesh-
out), the definition of the target feature is
language specific.
There are some cases, however, where the value
of the target feature is language independent.
The most obvious case is the syntactic labeling
of constituents such as NP, PP, etc. Other
examples include models with yes/no target
feature values, such as the model which
determines the probability that certain NPs are
not syntactically realized (for examples, the
subjects of infinitive or imperatives). In these
cases, the code defining these target features can
be re-used for a number of languages without
change.
I The switch operation also applies to partitive
constructions. Support verbs, on the other hand, have
the same representation as other verbs in our logical
form.
</bodyText>
<subsubsectionHeader confidence="0.996436">
3.2.2 Feature extraction
</subsubsectionHeader>
<bodyText confidence="0.999414214285714">
As noted previously, German feature extraction
modules have been re-used for the French
system. Feature extraction was unchanged (albeit
performed on French data) for the models that
capture the contexts for the insertion of negation,
prepositions, and subordinating conjunctions.
However, in every one of these cases, the set of
values for the target features changed to reflect
the language.
Most models, however, require slightly different
sets of extracted features. For example, the
French model responsible for the realization of
the determiner needs to check for the presence of
an adjective in between the determiner and the
noun. The form of the plural indefinite
determiner is de before an adjective or des
immediately before the noun. Besides that, the
model refers only to the gender and number of
the head noun. In German, however, the form of
the determiner is determined by the gender,
number and case of the head noun.
The model which determines the realization of
the relative pronoun also looks at the gender of
the pronoun&apos;s antecedent in French, because
some pronouns agree in gender and number with
their antecedent. For example, in (3), the
feminine form laquelle (&amp;quot;which&amp;quot;) agrees with its
antecedent, base de donnees.
</bodyText>
<listItem confidence="0.530233">
(3) Developpez Bases de donnees, puis
developpez la base de donnees a laquelle
appartient l&apos;utilisateur.
</listItem>
<bodyText confidence="0.999619647058824">
&amp;quot;Expand Databases, then expand the
database to which the user belongs.&amp;quot;
However, case information is not useful to
determine the form of the pronoun, even though
subject and object relative pronouns are marked
for case. Qui is the subject form and que the
object form of the relative pronoun, but in many
cases, qui is used to refer to a human antecedent
with any syntactic function (avec qui &amp;quot;with
whom&amp;quot;, pour qui &amp;quot;for whom&amp;quot;, etc.), and not to
the subject of the clause. Hence, this distinction
of forms, a vestige of erstwhile case marking, is
not relevant to automatically distinguish uses of
relative pronouns and is not amongst the
extracted features. Grammatical function
information is used instead by the decision tree
learner.
</bodyText>
<page confidence="0.995865">
327
</page>
<bodyText confidence="0.999990886792453">
To determine the insertion of expletive subjects,
French specific information was necessary. The
most common context of insertion is with etre
(&amp;quot;to be&amp;quot;), and a feature specific to that
environment was added to the set of extracted
features.
For determining the syntactic label of a
constituent, more information again is needed in
French, because of the nature of French modals.
Verbs used with a modal must be marked as
such, otherwise they are assigned the label of the
head instead of the label of its complement.
(Note that the assignment of syntactic labels
takes place before the operation that switches
semantic and syntactic headedness).
The examples above involve some features
which are not relevant for German, or which are
specific to French. In all cases, however, French
uses the same strategy as German: exploit the
full set of features available in the analysis on the
node, its parent and grand-parent. The sets of
features therefore largely overlap and are
language-independent for the most part. About
700 features are extracted for most of the French
models, 1000 for the order model. These sets
include syntactic features (category, arguments,
syntactic function, subcategorization features,
etc.), morpho-syntactic features (agreement
features, tense, mode, aspect features) and
semantic features (semantic roles, semantic
relations). A subset of features are selected as
relevant during the learning of each decision tree
classifier: complex models have over 100
features (120 and 177 features for the label
model and the order model respectively), simpler
models use much fewer features (12 for the clitic
model, 13 for the relative pronoun realization
model and 3 only for the switch model). The
features selected in the relative pronoun model,
for example, are the syntactic category of the
node, of its parent, the syntactic function of the
node, the voice of the parent, the arguments of
the parent, and the agreement features of the
grand-parent. These features correspond to
linguistic intuition: the choice of a relative
pronoun depends on its syntactic category and on
the function it fulfills. Its agreement features
depend, in French, on the agreement features of
the constituent modified by the relative clause.
Details of some models are given in the
appendix, with relevant statistics. The next
section briefly discusses linguistic operations
which rely on machine-learned contexts.
</bodyText>
<subsectionHeader confidence="0.999804">
3.3 Linguistic operations
</subsectionHeader>
<bodyText confidence="0.9999668125">
Most of the linguistic operations which are
employed in mapping a semantic representation
to a syntactic tree have machine-learned
contexts. Once the operation is triggered in a
given context, the action part of the operation
contains language-specific elements, such as
specific lexical choices for function word
insertions, etc. While the structure of most of
these operations could be re-used for the French
implementation, some adaptations had to be
made.
Linguistic operations which insert constituents
are often very similar, and differ only, in some
cases, in the citation form of the lexical element
being inserted. For example, specific
prepositions or infinitival markers are inserted.
The definitions of these operations are thus very
close in German and in French. This is not the
case, however, for configurations where modals
can occur, and which necessitate the definition of
special cases for French modals. Also, although
the conversion of the logical form to a syntactic
tree is language independent for the most part,
the operation which switches syntactic and
semantic headedness involves many
specifications for the contexts of French modals.
The last stage of sentence realization, inflection,
is also completely language-dependent.
The operations of the ordering stage and of the
surface-cleanup stages, on the other hand, are
completely language-neutral, albeit based on
machine-learned models trained on French data.
</bodyText>
<sectionHeader confidence="0.999613" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999974333333333">
We performed a human evaluation of French
generation. This was the first formal evaluation
of the French generation system. For this
evaluation, 545 test sentences from a blind
software manual corpus were analyzed with our
NLPWin analysis system, producing a logical
form for each sentence. From each logical form,
our sentence realization system then generated a
hypothesis sentence. We did not control for noise
introduced into the data by the analysis phase
(about 15% of the sentences did not have a
spanning parse). Nevertheless, this experiment
</bodyText>
<page confidence="0.99608">
328
</page>
<bodyText confidence="0.999169923076923">
gives us a good indication of the performance of
French Amalgam.
Five evaluators were asked to evaluate the same
set of sentences independently. Each generated
sentence was evaluated in isolation; i.e.,
discourse context was not taken into account. For
each sentence, raters were presented with the
original French sentence as a reference and the
hypothesis sentence from French Amalgam. All
the raters assigned an integer score comparing
each sentence to the reference. The scores were 1
&amp;quot;Unacceptable&amp;quot;, 2 &amp;quot;Possibly acceptable&amp;quot;, 3
&amp;quot;Acceptable&amp;quot; and 4 &amp;quot;Ideal&amp;quot;.
The score of a sentence is the average of the
scores from the five raters. The system score is
the average of the scores of all sentences.
The average score was 2.92 with a standard
deviation of 0.19. The maximum score was 4,
and 99/545 sentences (18.2%) received that
score. For 45 of those sentences, the score was
assigned automatically, because the sentences
were completely identical. The other sentences
with score 4 (54 sentences in total) differed in
some way from the original but had been
assigned that score by all 5 evaluators, who had
judged them equivalent to the reference sentence.
</bodyText>
<sectionHeader confidence="0.998957" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999881">
We have discussed the adaptation to French of a
machine-learned sentence realization system,
originally developed for German generation. We
have shown that, thanks to the language-
independent architecture and the machine-
learning orientation of the system, we were able
to re-use most of the original code. Feature
extraction and model building are language-
neutral, with the exception of the addition of
French-specific features. All remaining
differences are in the specific linguistic
operations which map the semantic
representation to the generated string and are
limited to specific lexical choices or to reverting
semantic and syntactic headedness in modal
contexts. Of course, a few components are
relevant only for one of the languages (such as
the clitic model in French), but these are very
few.
The results of the evaluation are very
encouraging: they are comparable to the results
for German sentence realization, reported in
Corston-Oliver et al. (2002): 2.96, with a
standard deviation of 0.81, with a similar rating
system.
Finally, it should be noted that the total
development time for adapting the system from
German to French was ten person-weeks. This
time includes training all of the models, and
general improvements in the system.
</bodyText>
<sectionHeader confidence="0.999232" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.99984425">
Our thanks go to the five anonymous,
independent evaluators for assistance with
evaluation and to the Microsoft Research NLP
group.
</bodyText>
<sectionHeader confidence="0.999302" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999875771428572">
Bangalore S. and Rambow 0. (2000) &amp;quot;Exploiting a
probabilistic hierarchical model for generation&amp;quot;. In
Proceedings of COLING 2000, Saarbracken,
Germany, pp. 42-48.
Chickering D. M. (2002) The WinMine Toolkit.
Microsoft Technical Report MSR-TR-2002-103.
Corston-Oliver S., Gamon M., Ringger E. and Moore
R. (2002) &amp;quot;An overview of Amalgam: a machine-
learned generation module&amp;quot;. In Proceedings of
INLG 2002, New York, pp.33-40.
Gamon M., Ringger E., Corston-Oliver S., Moore R.
(2002a) &amp;quot;Machine-learned contexts for linguistic
operations in German sentence realization&amp;quot;. In
Proceedings of ACL 2002, pp. 25-32.
Gamon M., Ringger E. and Corston-Oliver S. (2002b)
Amalgam: A machine-learned generation module.
Microsoft Research Technical Report MSR-TR-
2002-57.
Gamon M., Ringger E., Zhang Z., Moore R. and S.
Corston-Oliver (2002c) &amp;quot;Extraposition: A case
study in German sentence realization&amp;quot;. In:
Proceedings of COLING 2002, pp. 301-307.
Heidorn G. E. (2002) &amp;quot;Intelligent Writing
Assistance&amp;quot;. In R. Dale, H. Moisl, and H. Somers
(eds.), A Handbook of Natural Language
Processing: Techniques and Applications for the
Processing of Language as Text, Marce Dekker,
New York.
Langkilde I. (2000) &amp;quot;Forest-Based Statistical
Sentence generation&amp;quot;. In Proceedings of NAACL
2000, pp. 170-177.
Langkilde-Geary I. (2002) &amp;quot;An Empirical Verification
of Coverage and Correctness for a General-Purpose
Sentence Generator&amp;quot;. In Proceedings of INLG
2002, New York, pp.17-24.
</reference>
<page confidence="0.988969">
329
</page>
<reference confidence="0.828173">
Langkilde I. and Knight K. (1998a) &amp;quot;The practical
value of n-grams in generation&amp;quot;. In Proceedings of
the 9th International Workshop on Natural
Language Generation, Niagara-on-the-Lake,
Canada pp. 248-255.
Langkilde I. and Knight K. (1998b) &amp;quot;Generation that
exploits corpus-based statistical knowledge&amp;quot;. In
Proceedings of the 36th ACL and 17th COLING.
Montreal, Canada pp. 704-710.
Palmer F. (1986) Mood and Modality, Cambridge
University Press, Cambridge.
Ringger E., Gamon M., Smets M., Corston-Oliver S.
and Moore R. (in preparation) &amp;quot;Linguistically
informed models of constituent structure for
ordering in sentence realization&amp;quot;.
Appendix: Details on a subset of the decision tree models in French Amalgam
Model Values predicted Accuracy Baseline
Syntactic label Determiner phr., complement cl., VP, quantifier phr., 0.9925 0.3087
adverbial NP, imperative main cl., adverb phr., label,
appositive NP, question main cl., nominal relative, adjective
phr., relative cl., NP, possessor, present participial cl.,
comment, infinitival cl., PP, finite subordinate cl.,
declarative main cl., past participial cl., present participial
cl., absolute clauses.
Placeholder for NULL, Wh, proximal demonstrative, definite, indefinite 0.9892 0.6167
</reference>
<table confidence="0.9470230625">
determiner
Auxiliary NULL, etre-avoir, avoir-mod, etre, avoir 0.9979 0.9132
Prepositions NULL, de, par 0.9965 0.9793
Insert infinitive NULL, de, pour, a 0.9315 0.4024
marker
Insert negator NULL, ne, ne_pas 0.9057 0.7188
Realization of NP Yes, No 0.8871 0.6625
Insert clitics NULL, y, en 0.9981 0.9975
switch head Yes, No 0.9971 0.6606
Determiner form le, les, l&apos;, la, un, une, des, de, d&apos;, du, cc, cet, cette, ces, cettes, 0.9894 0.2705
quel, quelle, quels, quelles
Relative pron. form qui, oh, dont, que, quoi, lequel, laquelle, lesquels, lesquelles 0.9326 0.5303
Conjunction Spell out: first or last instance 0.9557 0.6739
reduction
Order: move Yes, No 0.9798 0.6272
constituent
</table>
<page confidence="0.989647">
330
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.877999">
<title confidence="0.99868">The adaptation of a machine-learned sentence realization system</title>
<author confidence="0.962429">to French Martine Smets</author>
<author confidence="0.962429">Michael Gamon</author>
<author confidence="0.962429">Simon Corston-Oliver</author>
<author confidence="0.962429">Eric Ringger</author>
<affiliation confidence="0.999684">Microsoft Research</affiliation>
<address confidence="0.9944785">One Microsoft Way Redmond, WA 98052, USA</address>
<email confidence="0.994476">martines@microsoft.com</email>
<email confidence="0.994476">mgamon@microsoft.com</email>
<email confidence="0.994476">simonco@microsoft.com</email>
<email confidence="0.994476">ringger@microsoft.com</email>
<abstract confidence="0.997594428571429">We describe the adaptation to French of a machine-learned sentence realization system called Amalgam that was originally developed to be as language independent as possible and was first implemented for German. We discuss the development of the French implementation with particular attention to the degree to which the original system could be reused, and we present the results of a human evaluation of the quality of sentence realization using the new French system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>Rambow</author>
</authors>
<title>Exploiting a probabilistic hierarchical model for generation&amp;quot;.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING 2000,</booktitle>
<pages>42--48</pages>
<location>Saarbracken, Germany,</location>
<contexts>
<context position="1148" citStr="Bangalore and Rambow, 2000" startWordPosition="162" endWordPosition="165">ementation with particular attention to the degree to which the original system could be reused, and we present the results of a human evaluation of the quality of sentence realization using the new French system. Introduction Recently, statistical and machine-learned approaches have been applied to the sentence realization phase of natural language generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternative sentence realizations (Langkilde and Knight, 1998a, 1998b). Other recent approaches use syntactic representations. FERGUS (Bangalore and Rambow, 2000), Halogen (Langkilde 2000, Langkilde-Geary 2002) and Amalgam (Corston-Oliver et al., 2002) use syntactic trees as an intermediate representation to determine the optimal string output. The Amalgam system discussed here is a sentence realization system which maps a semantic representation to a surface syntactic tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the context for which is primarily machine-learned. The resulting syntactic tree contains all the necessary information on its leaf nodes from which a surface string can be read. The p</context>
</contexts>
<marker>Bangalore, Rambow, 2000</marker>
<rawString>Bangalore S. and Rambow 0. (2000) &amp;quot;Exploiting a probabilistic hierarchical model for generation&amp;quot;. In Proceedings of COLING 2000, Saarbracken, Germany, pp. 42-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Chickering</author>
</authors>
<title>The WinMine Toolkit.</title>
<date>2002</date>
<tech>Microsoft Technical Report MSR-TR-2002-103.</tech>
<contexts>
<context position="4582" citStr="Chickering, 2002" startWordPosition="679" endWordPosition="680"> previously assigned verb position features Stage 4 Movement: • raising, wh movement (Proc) Stage 5 Ordering (ML): • ordering of constituents and leaf nodes in the tree Stage 6 Extraposition (ML) Stage7 Surface clean-up (ML): • lexical choice of determiners and relative pronouns • syntactic aggregation Stage 8 Punctuation (ML) Stage 9 Inflectional generation (Proc) Figure 1 The stages of German Amalgam All machine-learned components employ decision trees for classification and for probability distribution estimation (Gamon et al., 2002b). The decision trees are built with the WinMine toolkit (Chickering, 2002). There are a total of twenty-one decision trees in the German system. The complexity of the decision trees varies with the complexity of the modeled task: the number of branching nodes in the decision tree models in the German system ranges from just 4 to.7,876 in the order model. 2 Data and feature extraction The data for all models are automatically extracted from of a set of 100,000 sentences drawn from software manuals. Between 30,000 and one million cases are extracted from these sentences, depending on the task to be modeled. The sentences are analyzed in the NLPWin system (Heidorn, 200</context>
</contexts>
<marker>Chickering, 2002</marker>
<rawString>Chickering D. M. (2002) The WinMine Toolkit. Microsoft Technical Report MSR-TR-2002-103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Corston-Oliver</author>
<author>M Gamon</author>
<author>E Ringger</author>
<author>R Moore</author>
</authors>
<title>An overview of Amalgam: a machinelearned generation module&amp;quot;.</title>
<date>2002</date>
<booktitle>In Proceedings of INLG 2002,</booktitle>
<pages>33--40</pages>
<location>New York,</location>
<contexts>
<context position="1238" citStr="Corston-Oliver et al., 2002" startWordPosition="173" endWordPosition="176">reused, and we present the results of a human evaluation of the quality of sentence realization using the new French system. Introduction Recently, statistical and machine-learned approaches have been applied to the sentence realization phase of natural language generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternative sentence realizations (Langkilde and Knight, 1998a, 1998b). Other recent approaches use syntactic representations. FERGUS (Bangalore and Rambow, 2000), Halogen (Langkilde 2000, Langkilde-Geary 2002) and Amalgam (Corston-Oliver et al., 2002) use syntactic trees as an intermediate representation to determine the optimal string output. The Amalgam system discussed here is a sentence realization system which maps a semantic representation to a surface syntactic tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the context for which is primarily machine-learned. The resulting syntactic tree contains all the necessary information on its leaf nodes from which a surface string can be read. The promise of machine-learned approaches to sentence realization is that they can easily be ad</context>
</contexts>
<marker>Corston-Oliver, Gamon, Ringger, Moore, 2002</marker>
<rawString>Corston-Oliver S., Gamon M., Ringger E. and Moore R. (2002) &amp;quot;An overview of Amalgam: a machinelearned generation module&amp;quot;. In Proceedings of INLG 2002, New York, pp.33-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gamon</author>
<author>E Ringger</author>
<author>S Corston-Oliver</author>
<author>R Moore</author>
</authors>
<title>Machine-learned contexts for linguistic operations in German sentence realization&amp;quot;.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>25--32</pages>
<contexts>
<context position="2429" citStr="Gamon et al. (2002" startWordPosition="357" endWordPosition="360"> that they can easily be adapted to new domains and ideally to new languages merely by retraining The architecture of Amalgam was intended to be languageindependent, although the system has previously only been applied to German sentence realization. Adapting this system to French allows us to assess which aspects of the system are truly language-independent and what must be added in order to account for French. The purpose of this paper is to focus on the adaptation of Amalgam to French. Discussions about the general architecture of the system can be found in Corston-Oliver et al. (2002) and Gamon et al. (2002b). 1 Overview of German Amalgam Amalgam takes as its input a logical form graph, i.e., a sentence-level dependency graph with fixed lexical choices for content words. This graph represents the predicate-argument structure of a sentence and includes semantic information concerning relations between nodes of the graph (Heidorn, 2002). Examples of French logical forms are given in section 3. Amalgam first degraphs the logical form into a tree and then augments it by the insertion of function words, 323 assignment of case and verb position features, syntactic labels, etc., to produce an unordered</context>
<context position="4506" citStr="Gamon et al., 2002" startWordPosition="666" endWordPosition="669">n • splitting of separable prefix verbs based on both lexical information and previously assigned verb position features Stage 4 Movement: • raising, wh movement (Proc) Stage 5 Ordering (ML): • ordering of constituents and leaf nodes in the tree Stage 6 Extraposition (ML) Stage7 Surface clean-up (ML): • lexical choice of determiners and relative pronouns • syntactic aggregation Stage 8 Punctuation (ML) Stage 9 Inflectional generation (Proc) Figure 1 The stages of German Amalgam All machine-learned components employ decision trees for classification and for probability distribution estimation (Gamon et al., 2002b). The decision trees are built with the WinMine toolkit (Chickering, 2002). There are a total of twenty-one decision trees in the German system. The complexity of the decision trees varies with the complexity of the modeled task: the number of branching nodes in the decision tree models in the German system ranges from just 4 to.7,876 in the order model. 2 Data and feature extraction The data for all models are automatically extracted from of a set of 100,000 sentences drawn from software manuals. Between 30,000 and one million cases are extracted from these sentences, depending on the task </context>
<context position="8192" citStr="Gamon et al. 2002" startWordPosition="1257" endWordPosition="1260">n German is not needed in Stage 3. French uses a head-switching operation for verb phrases headed by modal verbs, because of the status of French modals as presented in section 3.1.3. Stage 4 (raising and Wh movement) is identical for both languages. In stage 5, both German and French use a left-toright model of constituent order. For each language, the model is a decision tree representing the probability distributions involved in ordering (see Ringger et al. (in preparation) for a detailed discussion of different approaches to constituent ordering). Extraposition, which is common in German (Gamon et al. 2002c), is rare in the French technical software manuals: there were too few examples of extraposition in the French data to train an extraposition model for Stage 6. Stage 7 (clean-up) uses language-specific information, especially in the realization of lexical forms of function words. Finally, stage 8, the realization of inflection, is completely language specific. Figure 2 provides a summary of the French Amalgam system. Stage 1 Pre-processing (Proc): Degraphing of the logical form. A Retrieval of lexical information. Stage 2 Flesh-Out (ML): A Assignment of syntactic labels_ A Insertion of func</context>
</contexts>
<marker>Gamon, Ringger, Corston-Oliver, Moore, 2002</marker>
<rawString>Gamon M., Ringger E., Corston-Oliver S., Moore R. (2002a) &amp;quot;Machine-learned contexts for linguistic operations in German sentence realization&amp;quot;. In Proceedings of ACL 2002, pp. 25-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gamon</author>
<author>E Ringger</author>
<author>S Corston-Oliver</author>
</authors>
<title>Amalgam: A machine-learned generation module.</title>
<date>2002</date>
<tech>Microsoft Research Technical Report MSR-TR2002-57.</tech>
<contexts>
<context position="2429" citStr="Gamon et al. (2002" startWordPosition="357" endWordPosition="360"> that they can easily be adapted to new domains and ideally to new languages merely by retraining The architecture of Amalgam was intended to be languageindependent, although the system has previously only been applied to German sentence realization. Adapting this system to French allows us to assess which aspects of the system are truly language-independent and what must be added in order to account for French. The purpose of this paper is to focus on the adaptation of Amalgam to French. Discussions about the general architecture of the system can be found in Corston-Oliver et al. (2002) and Gamon et al. (2002b). 1 Overview of German Amalgam Amalgam takes as its input a logical form graph, i.e., a sentence-level dependency graph with fixed lexical choices for content words. This graph represents the predicate-argument structure of a sentence and includes semantic information concerning relations between nodes of the graph (Heidorn, 2002). Examples of French logical forms are given in section 3. Amalgam first degraphs the logical form into a tree and then augments it by the insertion of function words, 323 assignment of case and verb position features, syntactic labels, etc., to produce an unordered</context>
<context position="4506" citStr="Gamon et al., 2002" startWordPosition="666" endWordPosition="669">n • splitting of separable prefix verbs based on both lexical information and previously assigned verb position features Stage 4 Movement: • raising, wh movement (Proc) Stage 5 Ordering (ML): • ordering of constituents and leaf nodes in the tree Stage 6 Extraposition (ML) Stage7 Surface clean-up (ML): • lexical choice of determiners and relative pronouns • syntactic aggregation Stage 8 Punctuation (ML) Stage 9 Inflectional generation (Proc) Figure 1 The stages of German Amalgam All machine-learned components employ decision trees for classification and for probability distribution estimation (Gamon et al., 2002b). The decision trees are built with the WinMine toolkit (Chickering, 2002). There are a total of twenty-one decision trees in the German system. The complexity of the decision trees varies with the complexity of the modeled task: the number of branching nodes in the decision tree models in the German system ranges from just 4 to.7,876 in the order model. 2 Data and feature extraction The data for all models are automatically extracted from of a set of 100,000 sentences drawn from software manuals. Between 30,000 and one million cases are extracted from these sentences, depending on the task </context>
<context position="8192" citStr="Gamon et al. 2002" startWordPosition="1257" endWordPosition="1260">n German is not needed in Stage 3. French uses a head-switching operation for verb phrases headed by modal verbs, because of the status of French modals as presented in section 3.1.3. Stage 4 (raising and Wh movement) is identical for both languages. In stage 5, both German and French use a left-toright model of constituent order. For each language, the model is a decision tree representing the probability distributions involved in ordering (see Ringger et al. (in preparation) for a detailed discussion of different approaches to constituent ordering). Extraposition, which is common in German (Gamon et al. 2002c), is rare in the French technical software manuals: there were too few examples of extraposition in the French data to train an extraposition model for Stage 6. Stage 7 (clean-up) uses language-specific information, especially in the realization of lexical forms of function words. Finally, stage 8, the realization of inflection, is completely language specific. Figure 2 provides a summary of the French Amalgam system. Stage 1 Pre-processing (Proc): Degraphing of the logical form. A Retrieval of lexical information. Stage 2 Flesh-Out (ML): A Assignment of syntactic labels_ A Insertion of func</context>
</contexts>
<marker>Gamon, Ringger, Corston-Oliver, 2002</marker>
<rawString>Gamon M., Ringger E. and Corston-Oliver S. (2002b) Amalgam: A machine-learned generation module. Microsoft Research Technical Report MSR-TR2002-57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gamon</author>
<author>E Ringger</author>
<author>Z Zhang</author>
<author>R Moore</author>
<author>S</author>
</authors>
<title>Corston-Oliver (2002c) &amp;quot;Extraposition: A case study in German sentence realization&amp;quot;. In:</title>
<date>2002</date>
<booktitle>Proceedings of COLING</booktitle>
<pages>301--307</pages>
<contexts>
<context position="2429" citStr="Gamon et al. (2002" startWordPosition="357" endWordPosition="360"> that they can easily be adapted to new domains and ideally to new languages merely by retraining The architecture of Amalgam was intended to be languageindependent, although the system has previously only been applied to German sentence realization. Adapting this system to French allows us to assess which aspects of the system are truly language-independent and what must be added in order to account for French. The purpose of this paper is to focus on the adaptation of Amalgam to French. Discussions about the general architecture of the system can be found in Corston-Oliver et al. (2002) and Gamon et al. (2002b). 1 Overview of German Amalgam Amalgam takes as its input a logical form graph, i.e., a sentence-level dependency graph with fixed lexical choices for content words. This graph represents the predicate-argument structure of a sentence and includes semantic information concerning relations between nodes of the graph (Heidorn, 2002). Examples of French logical forms are given in section 3. Amalgam first degraphs the logical form into a tree and then augments it by the insertion of function words, 323 assignment of case and verb position features, syntactic labels, etc., to produce an unordered</context>
<context position="4506" citStr="Gamon et al., 2002" startWordPosition="666" endWordPosition="669">n • splitting of separable prefix verbs based on both lexical information and previously assigned verb position features Stage 4 Movement: • raising, wh movement (Proc) Stage 5 Ordering (ML): • ordering of constituents and leaf nodes in the tree Stage 6 Extraposition (ML) Stage7 Surface clean-up (ML): • lexical choice of determiners and relative pronouns • syntactic aggregation Stage 8 Punctuation (ML) Stage 9 Inflectional generation (Proc) Figure 1 The stages of German Amalgam All machine-learned components employ decision trees for classification and for probability distribution estimation (Gamon et al., 2002b). The decision trees are built with the WinMine toolkit (Chickering, 2002). There are a total of twenty-one decision trees in the German system. The complexity of the decision trees varies with the complexity of the modeled task: the number of branching nodes in the decision tree models in the German system ranges from just 4 to.7,876 in the order model. 2 Data and feature extraction The data for all models are automatically extracted from of a set of 100,000 sentences drawn from software manuals. Between 30,000 and one million cases are extracted from these sentences, depending on the task </context>
<context position="8192" citStr="Gamon et al. 2002" startWordPosition="1257" endWordPosition="1260">n German is not needed in Stage 3. French uses a head-switching operation for verb phrases headed by modal verbs, because of the status of French modals as presented in section 3.1.3. Stage 4 (raising and Wh movement) is identical for both languages. In stage 5, both German and French use a left-toright model of constituent order. For each language, the model is a decision tree representing the probability distributions involved in ordering (see Ringger et al. (in preparation) for a detailed discussion of different approaches to constituent ordering). Extraposition, which is common in German (Gamon et al. 2002c), is rare in the French technical software manuals: there were too few examples of extraposition in the French data to train an extraposition model for Stage 6. Stage 7 (clean-up) uses language-specific information, especially in the realization of lexical forms of function words. Finally, stage 8, the realization of inflection, is completely language specific. Figure 2 provides a summary of the French Amalgam system. Stage 1 Pre-processing (Proc): Degraphing of the logical form. A Retrieval of lexical information. Stage 2 Flesh-Out (ML): A Assignment of syntactic labels_ A Insertion of func</context>
</contexts>
<marker>Gamon, Ringger, Zhang, Moore, S, 2002</marker>
<rawString>Gamon M., Ringger E., Zhang Z., Moore R. and S. Corston-Oliver (2002c) &amp;quot;Extraposition: A case study in German sentence realization&amp;quot;. In: Proceedings of COLING 2002, pp. 301-307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Heidorn</author>
</authors>
<title>Intelligent Writing Assistance&amp;quot;. In</title>
<date>2002</date>
<booktitle>A Handbook of Natural Language Processing: Techniques and Applications for the Processing of Language as Text,</booktitle>
<editor>R. Dale, H. Moisl, and H. Somers (eds.),</editor>
<publisher>Marce Dekker,</publisher>
<location>New York.</location>
<contexts>
<context position="2763" citStr="Heidorn, 2002" startWordPosition="407" endWordPosition="408">y language-independent and what must be added in order to account for French. The purpose of this paper is to focus on the adaptation of Amalgam to French. Discussions about the general architecture of the system can be found in Corston-Oliver et al. (2002) and Gamon et al. (2002b). 1 Overview of German Amalgam Amalgam takes as its input a logical form graph, i.e., a sentence-level dependency graph with fixed lexical choices for content words. This graph represents the predicate-argument structure of a sentence and includes semantic information concerning relations between nodes of the graph (Heidorn, 2002). Examples of French logical forms are given in section 3. Amalgam first degraphs the logical form into a tree and then augments it by the insertion of function words, 323 assignment of case and verb position features, syntactic labels, etc., to produce an unordered syntax tree. Amalgam then establishes intraconstituent order. After syntactic aggregation, insertion of punctuation, morphological inflection, and capitalization, an output string is read off the leaf nodes. The contexts for most of these linguistic operations are machine-learned (Gamon et al., 2002a). Figure 1 lists the eight stag</context>
<context position="5184" citStr="Heidorn, 2002" startWordPosition="780" endWordPosition="781">ering, 2002). There are a total of twenty-one decision trees in the German system. The complexity of the decision trees varies with the complexity of the modeled task: the number of branching nodes in the decision tree models in the German system ranges from just 4 to.7,876 in the order model. 2 Data and feature extraction The data for all models are automatically extracted from of a set of 100,000 sentences drawn from software manuals. Between 30,000 and one million cases are extracted from these sentences, depending on the task to be modeled. The sentences are analyzed in the NLPWin system (Heidorn, 2002), which provides a syntactic and logical form analysis. Nodes in the logical form representation are linked to the corresponding syntax nodes, allowing us to learn contexts for the mapping from the semantic representation to the surface syntax representation. The data is split 70/30 for training versus model parameter tuning. For each set of data we build decision trees at several levels of granularity and select the model with the maximal accuracy as determined on the parameter tuning set. We attempt to standardize as much as possible the set of features to be extracted. We exploit the full s</context>
</contexts>
<marker>Heidorn, 2002</marker>
<rawString>Heidorn G. E. (2002) &amp;quot;Intelligent Writing Assistance&amp;quot;. In R. Dale, H. Moisl, and H. Somers (eds.), A Handbook of Natural Language Processing: Techniques and Applications for the Processing of Language as Text, Marce Dekker, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde</author>
</authors>
<title>Forest-Based Statistical Sentence generation&amp;quot;.</title>
<date>2000</date>
<booktitle>In Proceedings of NAACL</booktitle>
<pages>170--177</pages>
<contexts>
<context position="1173" citStr="Langkilde 2000" startWordPosition="167" endWordPosition="168">o the degree to which the original system could be reused, and we present the results of a human evaluation of the quality of sentence realization using the new French system. Introduction Recently, statistical and machine-learned approaches have been applied to the sentence realization phase of natural language generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternative sentence realizations (Langkilde and Knight, 1998a, 1998b). Other recent approaches use syntactic representations. FERGUS (Bangalore and Rambow, 2000), Halogen (Langkilde 2000, Langkilde-Geary 2002) and Amalgam (Corston-Oliver et al., 2002) use syntactic trees as an intermediate representation to determine the optimal string output. The Amalgam system discussed here is a sentence realization system which maps a semantic representation to a surface syntactic tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the context for which is primarily machine-learned. The resulting syntactic tree contains all the necessary information on its leaf nodes from which a surface string can be read. The promise of machine-learned</context>
</contexts>
<marker>Langkilde, 2000</marker>
<rawString>Langkilde I. (2000) &amp;quot;Forest-Based Statistical Sentence generation&amp;quot;. In Proceedings of NAACL 2000, pp. 170-177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde-Geary</author>
</authors>
<title>An Empirical Verification of Coverage and Correctness for a General-Purpose Sentence Generator&amp;quot;.</title>
<date>2002</date>
<booktitle>In Proceedings of INLG 2002,</booktitle>
<pages>17--24</pages>
<location>New York,</location>
<contexts>
<context position="1196" citStr="Langkilde-Geary 2002" startWordPosition="169" endWordPosition="170">which the original system could be reused, and we present the results of a human evaluation of the quality of sentence realization using the new French system. Introduction Recently, statistical and machine-learned approaches have been applied to the sentence realization phase of natural language generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternative sentence realizations (Langkilde and Knight, 1998a, 1998b). Other recent approaches use syntactic representations. FERGUS (Bangalore and Rambow, 2000), Halogen (Langkilde 2000, Langkilde-Geary 2002) and Amalgam (Corston-Oliver et al., 2002) use syntactic trees as an intermediate representation to determine the optimal string output. The Amalgam system discussed here is a sentence realization system which maps a semantic representation to a surface syntactic tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the context for which is primarily machine-learned. The resulting syntactic tree contains all the necessary information on its leaf nodes from which a surface string can be read. The promise of machine-learned approaches to sentence</context>
</contexts>
<marker>Langkilde-Geary, 2002</marker>
<rawString>Langkilde-Geary I. (2002) &amp;quot;An Empirical Verification of Coverage and Correctness for a General-Purpose Sentence Generator&amp;quot;. In Proceedings of INLG 2002, New York, pp.17-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde</author>
<author>K Knight</author>
</authors>
<title>The practical value of n-grams in generation&amp;quot;.</title>
<date>1998</date>
<booktitle>In Proceedings of the 9th International Workshop on Natural Language Generation,</booktitle>
<pages>248--255</pages>
<location>Niagara-on-the-Lake, Canada</location>
<contexts>
<context position="1047" citStr="Langkilde and Knight, 1998" startWordPosition="150" endWordPosition="153">dent as possible and was first implemented for German. We discuss the development of the French implementation with particular attention to the degree to which the original system could be reused, and we present the results of a human evaluation of the quality of sentence realization using the new French system. Introduction Recently, statistical and machine-learned approaches have been applied to the sentence realization phase of natural language generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternative sentence realizations (Langkilde and Knight, 1998a, 1998b). Other recent approaches use syntactic representations. FERGUS (Bangalore and Rambow, 2000), Halogen (Langkilde 2000, Langkilde-Geary 2002) and Amalgam (Corston-Oliver et al., 2002) use syntactic trees as an intermediate representation to determine the optimal string output. The Amalgam system discussed here is a sentence realization system which maps a semantic representation to a surface syntactic tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the context for which is primarily machine-learned. The resulting syntactic tree co</context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>Langkilde I. and Knight K. (1998a) &amp;quot;The practical value of n-grams in generation&amp;quot;. In Proceedings of the 9th International Workshop on Natural Language Generation, Niagara-on-the-Lake, Canada pp. 248-255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde</author>
<author>K Knight</author>
</authors>
<title>Generation that exploits corpus-based statistical knowledge&amp;quot;.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th ACL and 17th COLING.</booktitle>
<pages>704--710</pages>
<location>Montreal, Canada</location>
<contexts>
<context position="1047" citStr="Langkilde and Knight, 1998" startWordPosition="150" endWordPosition="153">dent as possible and was first implemented for German. We discuss the development of the French implementation with particular attention to the degree to which the original system could be reused, and we present the results of a human evaluation of the quality of sentence realization using the new French system. Introduction Recently, statistical and machine-learned approaches have been applied to the sentence realization phase of natural language generation. The Nitrogen system, for example, uses a word bigram language model to score and rank a large set of alternative sentence realizations (Langkilde and Knight, 1998a, 1998b). Other recent approaches use syntactic representations. FERGUS (Bangalore and Rambow, 2000), Halogen (Langkilde 2000, Langkilde-Geary 2002) and Amalgam (Corston-Oliver et al., 2002) use syntactic trees as an intermediate representation to determine the optimal string output. The Amalgam system discussed here is a sentence realization system which maps a semantic representation to a surface syntactic tree via intermediate syntactic representations. The mappings are performed with linguistic operations, the context for which is primarily machine-learned. The resulting syntactic tree co</context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>Langkilde I. and Knight K. (1998b) &amp;quot;Generation that exploits corpus-based statistical knowledge&amp;quot;. In Proceedings of the 36th ACL and 17th COLING. Montreal, Canada pp. 704-710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Palmer</author>
</authors>
<title>Mood and Modality,</title>
<date>1986</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="12544" citStr="Palmer, 1986" startWordPosition="1955" endWordPosition="1956">sertion is learned and represented in the clitic decision tree. This component is necessary for French and would be needed for other Romance languages also. 3.1.3 Modals Most of the changes in the French system were required by modal verbs. In German and English syntactic analysis, modals do not head a clause but behave like auxiliaries. In French, however, modals behave syntactically just like main verbs (taking a clausal complement), but have semantic properties characteristic of modals: pouvoir (&amp;quot;to be able&amp;quot;) and devoir (&amp;quot;to have to&amp;quot;), for example, can be used epistemically or deontically (Palmer, 1986). In our system, they share the same semantic representation as modals of other languages, although they do not exhibit the same syntactic behavior. In example (2), the modal pouvez (a second person plural form of the verb pouvoir) is the head of the main clause, and carries the syntactic information of tense, mood, person number and negation. The logical form for this example, illustrated in Figure 4, is headed by envoyer (&amp;quot;to send&amp;quot;). The modal pouvoir (&amp;quot;to be able&amp;quot;) functions in the logical form as a semantic modifier of the verb, and features such as tense, mood, and negation are copied ont</context>
</contexts>
<marker>Palmer, 1986</marker>
<rawString>Palmer F. (1986) Mood and Modality, Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="false">
<authors>
<author>E Ringger</author>
<author>M Gamon</author>
<author>M Smets</author>
<author>S Corston-Oliver</author>
<author>R Moore</author>
</authors>
<title>(in preparation) &amp;quot;Linguistically informed models of constituent structure for ordering in sentence realization&amp;quot;.</title>
<marker>Ringger, Gamon, Smets, Corston-Oliver, Moore, </marker>
<rawString>Ringger E., Gamon M., Smets M., Corston-Oliver S. and Moore R. (in preparation) &amp;quot;Linguistically informed models of constituent structure for ordering in sentence realization&amp;quot;.</rawString>
</citation>
<citation valid="false">
<title>Appendix: Details on a subset of the decision tree models in French Amalgam Model Values predicted Accuracy Baseline Syntactic label Determiner phr., complement cl., VP, quantifier phr., adverbial NP, imperative main cl., adverb phr., label, appositive NP, question main cl., nominal relative, adjective phr., relative cl., NP, possessor, present participial cl., comment, infinitival cl., PP, finite subordinate cl., declarative main cl., past participial cl., present participial cl., absolute clauses. 0.9925 0.3087 Placeholder for determiner</title>
<marker></marker>
<rawString>Appendix: Details on a subset of the decision tree models in French Amalgam Model Values predicted Accuracy Baseline Syntactic label Determiner phr., complement cl., VP, quantifier phr., adverbial NP, imperative main cl., adverb phr., label, appositive NP, question main cl., nominal relative, adjective phr., relative cl., NP, possessor, present participial cl., comment, infinitival cl., PP, finite subordinate cl., declarative main cl., past participial cl., present participial cl., absolute clauses. 0.9925 0.3087 Placeholder for determiner</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>