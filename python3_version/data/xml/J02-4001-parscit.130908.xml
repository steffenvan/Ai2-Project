<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000008">
<title confidence="0.9927655">
Introduction to the Special Issue on
Summarization
</title>
<author confidence="0.99457">
Dragomir R. Radev∗ Eduard Hovy†
</author>
<affiliation confidence="0.989841">
University of Michigan USC/ISI
</affiliation>
<author confidence="0.996231">
Kathleen McKeown‡
</author>
<affiliation confidence="0.995161">
Columbia University
</affiliation>
<sectionHeader confidence="0.987848" genericHeader="abstract">
1. Introduction and Definitions
</sectionHeader>
<bodyText confidence="0.999773214285714">
As the amount of on-line information increases, systems that can automatically sum-
marize one or more documents become increasingly desirable. Recent research has
investigated types of summaries, methods to create them, and methods to evaluate
them. Several evaluation competitions (in the style of the National Institute of Stan-
dards and Technology’s [NIST’s] Text Retrieval Conference [TREC]) have helped de-
termine baseline performance levels and provide a limited set of training material.
Frequent workshops and symposia reflect the ongoing interest of researchers around
the world. The volume of papers edited by Mani and Maybury (1999) and a book
(Mani 2001) provide good introductions to the state of the art in this rapidly evolving
subfield.
A summary can be loosely defined as a text that is produced from one or more
texts, that conveys important information in the original text(s), and that is no longer
than half of the original text(s) and usually significantly less than that. Text here is
used rather loosely and can refer to speech, multimedia documents, hypertext, etc.
The main goal of a summary is to present the main ideas in a document in less
space. If all sentences in a text document were of equal importance, producing a sum-
mary would not be very effective, as any reduction in the size of a document would
carry a proportional decrease in its informativeness. Luckily, information content in a
document appears in bursts, and one can therefore distinguish between more and less
informative segments. Identifying the informative segments at the expense of the rest
is the main challenge in summarization.
Of the many types of summary that have been identified (Borko and Bernier 1975;
Cremmins 1996; Sparck Jones 1999; Hovy and Lin 1999), indicative summaries provide
an idea of what the text is about without conveying specific content, and informative
ones provide some shortened version of the content. Topic-oriented summaries con-
centrate on the reader’s desired topic(s) of interest, whereas generic summaries reflect
the author’s point of view. Extracts are summaries created by reusing portions (words,
sentences, etc.) of the input text verbatim, while abstracts are created by regenerating
</bodyText>
<affiliation confidence="0.6101204">
∗ Assistant Professor, School of Information, Department of Electrical Engineering and Computer Science
and Department of Linguistics, University of Michigan, Ann Arbor. E-mail: radev@umich.edu.
† ISI Fellow and Senior Project Leader, Information Sciences Institute of the University of Southern
California, Marina del Rey, CA. E-mail: hovy@isi.edu.
‡ Professor, Department of Computer Science, New York University, New York, NY. E-mail:
</affiliation>
<email confidence="0.745706">
kathy@cs.columbia.edu.
</email>
<note confidence="0.94789">
© 2002 Association for Computational Linguistics
Computational Linguistics Volume 28, Number 4
</note>
<bodyText confidence="0.999445294117647">
the extracted content. Extraction is the process of identifying important material in
the text, abstraction the process of reformulating it in novel terms, fusion the process
of combining extracted portions, and compression the process of squeezing out unim-
portant material. The need to maintain some degree of grammaticality and coherence
plays a role in all four processes.
The obvious overlap of text summarization with information extraction, and con-
nections from summarization to both automated question answering and natural lan-
guage generation, suggest that summarization is actually a part of a larger picture.
In fact, whereas early approaches drew more from information retrieval, more re-
cent approaches draw from the natural language field. Natural language generation
techniques have been adapted to work with typed textual phrases, in place of se-
mantics, as input, and this allows researchers to experiment with approaches to ab-
straction. Techniques that have been developed for topic-oriented summaries are now
being pushed further so that they can be applied to the production of long answers
for the question-answering task. However, as the articles in this special issue show,
domain-independent summarization has several specific, difficult aspects that make it
a research topic in its own right.
</bodyText>
<sectionHeader confidence="0.990664" genericHeader="method">
2. Major Approaches
</sectionHeader>
<bodyText confidence="0.9997365">
We provide a sketch of the current state of the art of summarization by describing
the general areas of research, including single-document summarization through ex-
traction, the beginnings of abstractive approaches to single-document summarization,
and a variety of approaches to multidocument summarization.
</bodyText>
<subsectionHeader confidence="0.916363">
2.1 Single-Document Summarization through Extraction
</subsectionHeader>
<bodyText confidence="0.999342041666667">
Despite the beginnings of research on alternatives to extraction, most work today
still relies on extraction of sentences from the original document to form a summary.
The majority of early extraction research focused on the development of relatively
simple surface-level techniques that tend to signal important passages in the source
text. Although most systems use sentences as units, some work with larger passages,
typically paragraphs. Typically, a set of features is computed for each passage, and
ultimately these features are normalized and summed. The passages with the highest
resulting scores are sorted and returned as the extract.
Early techniques for sentence extraction computed a score for each sentence based
on features such as position in the text (Baxendale 1958; Edmundson 1969), word
and phrase frequency (Luhn 1958), key phrases (e.g., “it is important to note”) (Ed-
mundson 1969). Recent extraction approaches use more sophisticated techniques for
deciding which sentences to extract; these techniques often rely on machine learning
to identify important features, on natural language analysis to identify key passages,
or on relations between words rather than bags of words.
The application of machine learning to summarization was pioneered by Kupiec,
Pedersen, and Chen (1995), who developed a summarizer using a Bayesian classifier
to combine features from a corpus of scientific articles and their abstracts. Aone et
al. (1999) and Lin (1999) experimented with other forms of machine learning and its
effectiveness. Machine learning has also been applied to learning individual features;
for example, Lin and Hovy (1997) applied machine learning to the problem of de-
termining how sentence position affects the selection of sentences, and Witbrock and
Mittal (1999) used statistical approaches to choose important words and phrases and
their syntactic context.
</bodyText>
<page confidence="0.948643">
400
</page>
<note confidence="0.544165">
Radev, Hovy, and McKeown Summarization: Introduction
</note>
<bodyText confidence="0.999845454545455">
Approaches involving more sophisticated natural language analysis to identify key
passages rely on analysis either of word relatedness or of discourse structure. Some
research uses the degree of lexical connectedness between potential passages and the
remainder of the text; connectedness may be measured by the number of shared words,
synonyms, or anaphora (e.g., Salton et al. 1997; Mani and Bloedorn 1997; Barzilay
and Elhadad 1999). Other research rewards passages that include topic words, that is,
words that have been determined to correlate well with the topic of interest to the user
(for topic-oriented summaries) or with the general theme of the source text (Buckley
and Cardie 1997; Strzalkowski et al. 1999; Radev, Jing, and Budzikowska 2000).
Alternatively, a summarizer may reward passages that occupy important positions
in the discourse structure of the text (Ono, Sumita, and Miike 1994; Marcu 1997b). This
method requires a system to compute discourse structure reliably, which is not possible
in all genres. This technique is the focus of one of the articles in this special issue (Teufel
and Moens 2002), which shows how particular types of rhetorical relations in the genre
of scientific journal articles can be reliably identified through the use of classification.
An open-source summarization environment, MEAD, was recently developed at the
Johns Hopkins summer workshop (Radev et al. 2002). MEAD allows researchers to
experiment with different features and methods for combination.
Some recent work (Conroy and O’Leary 2001) has turned to the use of hidden
Markov models (HMMs) and pivoted QR decomposition to reflect the fact that the
probability of inclusion of a sentence in an extract depends on whether the previous
sentence has been included as well.
</bodyText>
<subsectionHeader confidence="0.999467">
2.2 Single-Document Summarization through Abstraction
</subsectionHeader>
<bodyText confidence="0.999293185185185">
At this early stage in research on summarization, we categorize any approach that
does not use extraction as an abstractive approach. Abstractive approaches have used
information extraction, ontological information, information fusion, and compression.
Information extraction approaches can be characterized as “top-down,” since they
look for a set of predefined information types to include in the summary (in con-
trast, extractive approaches are more data-driven). For each topic, the user predefines
frames of expected information types, together with recognition criteria. For example,
an earthquake frame may contain slots for location, earthquake magnitude, number of
casualties, etc. The summarization engine must then locate the desired pieces of infor-
mation, fill them in, and generate a summary with the results (DeJong 1978; Rau and
Jacobs 1991). This method can produce high-quality and accurate summaries, albeit in
restricted domains only.
Compressive summarization results from approaching the problem from the point
of view of language generation. Using the smallest units from the original document,
Witbrock and Mittal (1999) extract a set of words from the input document and then
order the words into sentences using a bigram language model. Jing and McKeown
(1999) point out that human summaries are often constructed from the source docu-
ment by a process of cutting and pasting document fragments that are then combined
and regenerated as summary sentences. Hence a summarizer can be developed to
extract sentences, reduce them by dropping unimportant fragments, and then use in-
formation fusion and generation to combine the remaining fragments. In this special
issue, Jing (2002) reports on automated techniques to build a corpus representing the
cut-and-paste process used by humans; such a corpus can then be used to train an
automated summarizer.
Other researchers focus on the reduction process. In an attempt to learn rules for
reduction, Knight and Marcu (2000) use expectation maximization to train a system
to compress the syntactic parse tree of a sentence in order to produce a shorter but
</bodyText>
<page confidence="0.995492">
401
</page>
<note confidence="0.444472">
Computational Linguistics Volume 28, Number 4
</note>
<bodyText confidence="0.99993175">
still maximally grammatical version. Ultimately, this approach can likely be used for
shortening two sentences into one, three into two (or one), and so on.
Of course, true abstraction involves taking the process one step further. Abstraction
involves recognizing that a set of extracted passages together constitute something
new, something that is not explicitly mentioned in the source, and then replacing them
in the summary with the (ideally more concise) new concept(s). The requirement that
the new material not be in the text explicitly means that the system must have access
to external information of some kind, such as an ontology or a knowledge base, and be
able to perform combinatory inference (Hahn and Reimer 1997). Since no large-scale
resources of this kind yet exist, abstractive summarization has not progressed beyond
the proof-of-concept stage (although top-down information extraction can be seen as
one variant).
</bodyText>
<subsectionHeader confidence="0.999132">
2.3 Multidocument Summarization
</subsectionHeader>
<bodyText confidence="0.999988540540541">
Multidocument summarization, the process of producing a single summary of a set
of related source documents, is relatively new. The three major problems introduced
by having to handle multiple input documents are (1) recognizing and coping with
redundancy, (2) identifying important differences among documents, and (3) ensuring
summary coherence, even when material stems from different source documents.
In an early approach to multidocument summarization, information extraction
was used to facilitate the identification of similarities and differences (McKeown and
Radev 1995). As for single-document summarization, this approach produces more of a
briefing than a summary, as it contains only preidentified information types. Identity of
slot values are used to determine when information is reliable enough to include in the
summary. Later work merged information extraction approaches with regeneration of
extracted text to improve summary generation (Radev and McKeown 1998). Important
differences (e.g., updates, trends, direct contradictions) are identified through a set of
discourse rules. Recent work also follows this approach, using enhanced information
extraction and additional forms of contrasts (White and Cardie 2002).
To identify redundancy in text documents, various similarity measures are used.
A common approach is to measure similarity between all pairs of sentences and then
use clustering to identify themes of common information (McKeown et al. 1999; Radev,
Jing, and Budzikowska 2000; Marcu and Gerber 2001). Alternatively, systems measure
the similarity of a candidate passage to that of already-selected passages and retain
it only if it contains enough new (dissimilar) information. A popular such measure is
maximal marginal relevance (MMR) (Carbonell, Geng, and Goldstein 1997; Carbonell
and Goldstein 1998).
Once similar passages in the input documents have been identified, the infor-
mation they contain must be included in the summary. Rather than simply listing
all similar sentences (a lengthy solution), some approaches will select a representa-
tive passage to convey information in each cluster (Radev, Jing, and Budzikowska
2000), whereas other approaches use information fusion techniques to identify repet-
itive phrases from the clusters and combine the phrases into the summary (Barzilay,
McKeown, and Elhadad 1999). Mani, Gates, and Bloedorn (1999) describe the use of
human-generated compression and reformulation rules.
Ensuring coherence is difficult, because this in principle requires some understand-
ing of the content of each passage and knowledge about the structure of discourse.
In practice, most systems simply follow time order and text order (passages from
the oldest text appear first, sorted in the order in which they appear in the input).
To avoid misleading the reader when juxtaposed passages from different dates all
say “yesterday,” some systems add explicit time stamps (Lin and Hovy 2002a). Other
</bodyText>
<page confidence="0.976685">
402
</page>
<note confidence="0.514672">
Radev, Hovy, and McKeown Summarization: Introduction
</note>
<bodyText confidence="0.999971642857143">
systems use a combination of temporal and coherence constraints to order sentences
(Barzilay, Elhadad, and McKeown 2001). Recently, Otterbacher, Radev, and Luo (2002)
have focused on discourse-based revisions of multidocument clusters as a means for
improving summary coherence.
Although multidocument summarization is new and the approaches described
here are only the beginning, current research also branches out in other directions. Re-
search is beginning on the generation of updates on new information (Allan, Gupta,
and Khandelwal 2001). Researchers are currently studying the production of longer
answers (i.e., multidocument summaries) from retrieved documents, focusing on such
types as biographies of people, descriptions of multiple events of the same type
(e.g., multiple hurricanes), opinion pieces (e.g., editorials and letters discussing a con-
tentious topic), and causes of events. Another challenging ongoing topic is the gener-
ation of titles for either a single document or set of documents. This challenge will be
explored in an evaluation planned by NIST in 2003.
</bodyText>
<subsectionHeader confidence="0.972865">
2.4 Evaluation
</subsectionHeader>
<bodyText confidence="0.999112285714286">
Evaluating the quality of a summary has proven to be a difficult problem, principally
because there is no obvious “ideal” summary. Even for relatively straightforward news
articles, human summarizers tend to agree only approximately 60% of the time, mea-
suring sentence content overlap. The use of multiple models for system evaluation
could help alleviate this problem, but researchers also need to look at other methods
that can yield more acceptable models, perhaps using a task as motivation.
Two broad classes of metrics have been developed: form metrics and content met-
rics. Form metrics focus on grammaticality, overall text coherence, and organization
and are usually measured on a point scale (Brandow, Mitze, and Rau 1995). Content is
more difficult to measure. Typically, system output is compared sentence by sentence
or fragment by fragment to one or more human-made ideal abstracts, and as in in-
formation retrieval, the percentage of extraneous information present in the system’s
summary (precision) and the percentage of important information omitted from the
summary (recall) are recorded. Other commonly used measures include kappa (Car-
letta 1996) and relative utility (Radev, Jing, and Budzikowska 2000), both of which take
into account the performance of a summarizer that randomly picks passages from the
original document to produce an extract. In the Document Understanding Conference
(DUC)-01 and DUC-02 summarization competitions (Harman and Marcu 2001; Hahn
and Harman 2002), NIST used the Summary Evaluation Environment (SEE) interface
(Lin 2001) to record values for precision and recall. These two competitions, run along
the lines of TREC, have served to establish overall baselines for single-document and
multidocument summarization and have provided several hundred human abstracts
as training material. (Another popular source of training material is the Ziff-Davis cor-
pus of computer product announcements.) Despite low interjudge agreement, DUC
has shown that humans are better summary producers than machines and that, for
the news article genre, certain algorithms do in fact do better than the simple baseline
of picking the lead material.
The largest task-oriented evaluation to date, the Summarization Evaluation Con-
ference (SUMMAC) (Mani et al. 1998; Firmin and Chrzanowski 1999) included three
tests: the categorization task (how well can humans categorize a summary compared
to its full text?), the ad hoc task (how well can humans determine whether a full text is
relevant to a query just from reading the summary?) and the question task (how well
can humans answer questions about the main thrust of the source text from reading
just the summary?). But the interpretation of the results is not simple; studies (Jing et
al. 1998; Donaway, Drummey, and Mather 2000; Radev, Jing, and Budzikowska 2000)
</bodyText>
<page confidence="0.993124">
403
</page>
<note confidence="0.432211">
Computational Linguistics Volume 28, Number 4
</note>
<bodyText confidence="0.999942055555556">
show how the same summaries receive different scores under different measures or
when compared to different (but presumably equivalent) ideal summaries created by
humans. With regard to interhuman agreement, Jing et al. find fairly high consistency
in the news genre only when the summary (extract) length is fixed relatively short.
Marcu (1997a) provides some evidence that other genres will deliver less consistency.
With regard to the lengths of the summaries produced by humans when not con-
strained by a particular compression rate, both Jing and Marcu find great variation.
Nonetheless, it is now generally accepted that for single news articles, systems produce
generic summaries indistinguishable from those of humans.
Automated summary evaluation is a gleam in everyone’s eye. Clearly, when an
ideal extract has been created by human(s), extractive summaries are easy to evalu-
ate. Marcu (1999) and Goldstein et al. (1999) independently developed an automated
method to create extracts corresponding to abstracts. But when the number of available
extracts is not sufficient, it is not clear how to overcome the problems of low inter-
human agreement. Simply using a variant of the Bilingual Evaluation Understudy
(BLEU) scoring method (based on a linear combination of matching n-grams between
the system output and the ideal summary) developed for machine translation (Pap-
ineni et al. 2001) is promising but not sufficient (Lin and Hovy 2002b).
</bodyText>
<sectionHeader confidence="0.902845" genericHeader="method">
3. The Articles in this Issue
</sectionHeader>
<bodyText confidence="0.9999811">
The articles in this issue move beyond the current state of the art in various ways.
Whereas most research to date has focused on the use of sentence extraction for sum-
marization, we are beginning to see techniques that allow a system to extract, merge,
and edit phrases, as opposed to full sentences, to generate a summary. Whereas many
summarization systems are designed for summarization of news, new algorithms are
summarizing much longer and more complex documents, such as scientific journal
articles, medical journal articles, or patents. Whereas most research to date has fo-
cused on text summarization, we are beginning to see a move toward summarization
of speech, a medium that places additional demands on the summarization process.
Finally, in addition to providing full summarization systems, the articles in this issue
also focus on tools that can aid in the process of developing summarization systems,
on computational efficiency of algorithms, and on techniques needed for preprocessing
speech.
The four articles that focus on summarization of text share a common theme:
Each views the summarization process as consisting of two phases. In the first, mate-
rial within the original document that is important is identified and extracted. In the
second, this extracted material may be modified, merged, and edited using genera-
tion techniques. Two of the articles focus on the extraction stage (Teufel and Moens
2002; Silber and McCoy 2002), whereas Jing (2002) examines tools for automatically
constructing resources that can be used for the second stage.
Teufel and Moens propose significantly different techniques for sentence extraction
than have been used in the past. Noting the difference in both length and structure
between scientific articles and news, they claim that both the context of sentences and
a more focused search for sentences is needed in order to produce a good summary
that is only 2.5% of the original document. Their approach is to provide a summary
that focuses on the new contribution of the paper and its relation to previous work.
They rely on rhetorical relations to provide information about context and to identify
sentences relating to, for example, the aim of the paper, its basis in previous work,
or contrasts with other work. Their approach features the use of corpora annotated
both with rhetorical relations and with relevance; it uses text categorization to extract
</bodyText>
<page confidence="0.992356">
404
</page>
<note confidence="0.765687">
Radev, Hovy, and McKeown Summarization: Introduction
</note>
<bodyText confidence="0.999884186046512">
sentences corresponding to any of seven rhetorical categories. The result is a set of
sentences that situate the article in respect to its original claims and in relation to other
research.
Silber and McCoy focus on computationally efficient algorithms for sentence ex-
traction. They present a linear time algorithm to extract lexical chains from a source
document (the lexical-chain approach was originally developed by Barzilay and El-
hadad [1997] but used an exponential time algorithm). This approach facilitates the
use of lexical chains as an intermediate representation for summarization. Barzilay and
Elhadad present an evaluation of the approach for summarization with both scientific
documents and university textbooks.
Jing advocates the use of a cut-and-paste approach to summarization in which
phrases, rather than sentences, are extracted from the original document. She shows
that such an approach is often used by human abstractors. She then presents an auto-
mated tool that is used to analyze a corpus of paired documents and abstracts written
by humans, in order to identify the phrases within the documents that are used in
the abstracts. She has developed an HMM solution to the matching problem. The
decomposition program is a tool that can produce training and testing corpora for
summarization, and its results have been used for her own summarization program.
Saggion and Lapalme (2002) describe a system, SumUM, that generates indicative-
informative summaries from technical documents. To build their system, Saggion and
Lapalme have studied a corpus of professionally written (short) abstracts. They have
manually aligned the abstracts and the original documents. Given the structured form
of technical papers, most of the information in the abstracts was also found in either the
author abstract (20%) or in the first section of the paper (40%) or the headlines or cap-
tions (23%). Based on their observations, the authors have developed an approach to
summarization, called selective analysis, which mimics the human abstractors’ routine.
The four components of selective analysis are indicative selection, informative selection,
indicative generation, and informative generation.
The final article in the issue (Zechner 2002) is distinct from the other articles in
that it addresses problems in summarization of speech. As in text summarization,
Zechner also uses sentence extraction to determine the content of the summary. Given
the informal nature of speech, however, a number of significant steps must be taken
in order to identify useful segments for extraction. Zechner develops techniques for
removing disfluencies from speech, for identifying units for extraction that are in
some sense equivalent to sentences, and for identifying relations such as question-
answer across turns in order to determine when units from two separate turns should
be extracted as a whole. This preprocessing yields a transcript on which standard
techniques for extraction in text (here the use of MMR [Carbonell and Goldstein 1998]
to identify relevant units) can operate successfully.
Though true abstractive summarization remains a researcher’s dream, the success
of extractive summarizers and the rapid development of compressive and similar
techniques testifies to the effectiveness with which the research community can address
new problems and find workable solutions to them.
</bodyText>
<sectionHeader confidence="0.984738" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.933532333333333">
Allan, James, Rahul Gupta, and Vikas
Khandelwal. 2001. Temporal summaries
of news topics. In Proceedings of the 24th
</reference>
<footnote confidence="0.67999475">
Annual International ACM SIGIR Conference
on Research and Development in Information
Retrieval, pages 10–18.
Aone, Chinatsu, Mary Ellen Okurowski,
James Gorlinsky, and Bjornar Larsen.
1999. A trainable summarizer with
knowledge acquired from robust NLP
techniques. In I. Mani and M. T. Maybury,
editors, Advances in Automatic Text
Summarization. MIT Press, Cambridge,
pages 71–80.
Barzilay, Regina and Michael Elhadad. 1997.
</footnote>
<page confidence="0.995726">
405
</page>
<table confidence="0.843988642857143">
Computational Linguistics Volume 28, Number 4
Using lexical chains for text
summarization. In Proceedings of the
ACL/EACL’97 Workshop on Intelligent
Scalable Text Summarization, pages 10–17,
Madrid, July.
Barzilay, Regina and Michael Elhadad. 1999.
Using lexical chains for text
summarization. In I. Mani and M. T.
Maybury, editors, Advances in Automatic
Text Summarization. MIT Press,
Cambridge, pages 111–121.
Barzilay, Regina, No ´emie Elhadad, and
Kathy McKeown. 2001. Sentence ordering
</table>
<figure confidence="0.261737666666667">
in multidocument summarization. In
Proceedings of the Human Language
Technology Conference.
</figure>
<figureCaption confidence="0.922050096774193">
Barzilay, Regina, Kathleen McKeown, and
Michael Elhadad. 1999. Information
fusion in the context of multi-document
summarization. In Proceedings of the 37th
Annual Meeting of the Association for
Computational Linguistics, College Park,
MD, 20–26 June, pages 550–557.
Baxendale, P. B. 1958. Man-made index for
technical literature—An experiment. IBM
Journal of Research and Development,
2(4):354–361.
Borko, H. and C. Bernier. 1975. Abstracting
Concepts and Methods. Academic Press,
New York.
Brandow, Ron, Karl Mitze, and Lisa F. Rau.
1995. Automatic condensation of
electronic publications by sentence
selection. Information Processing and
Management, 31(5):675–685.
Buckley, Chris and Claire Cardie. 1997.
Using empire and smart for
high-precision IR and summarization. In
Proceedings of the TIPSTER Text Phase III
12-Month Workshop, San Diego, CA,
October.
Carbonell, Jaime, Y. Geng, and Jade
Goldstein. 1997. Automated
query-relevant summarization and
diversity-based reranking. In Proceedings
of the IJCAI-97 Workshop on AI in Digital
Libraries, pages 12–19.
</figureCaption>
<bodyText confidence="0.781603">
Carbonell, Jaime G. and Jade Goldstein.
1998. The use of MMR, diversity-based
reranking for reordering documents and
producing summaries. In Alistair Moffat
and Justin Zobel, editors, Proceedings of the
</bodyText>
<reference confidence="0.933325428571428">
21st Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval, Melbourne,
Australia, pages 335–336.
Carletta, Jean. 1996. Assessing agreement on
classification tasks: The kappa statistic.
Computational Linguistics, 22(2):249–254.
Conroy, John and Dianne O’Leary. 2001.
Text summarization via hidden Markov
models. In Proceedings of the 24th Annual
International ACM SIGIR Conference on
Research and Development in Information
Retrieval, pages 406–407.
Cremmins, Edward T. 1996. The Art of
Abstracting. Information Resources Press,
Arlington, VA, second edition.
DeJong, Gerald Francis. 1978. Fast Skimming
of News Stories: The FRUMP System. Ph.D.
thesis, Yale University, New Haven, CT.
Donaway, R. L., K. W. Drummey, and
L. A. Mather. 2000. A comparison of
rankings produced by summarization
evaluation measures. In Proceedings of the
Workshop on Automatic Summarization,
ANLP-NAACL2000, Association for
Computational Linguistics, 30 April,
pages 69–78.
Edmundson, H. P. 1969. New methods in
automatic extracting. Journal of the
Association for Computing Machinery,
16(2):264–285.
Firmin, T. and M. J. Chrzanowski. 1999. An
evaluation of automatic text
summarization systems. In I. Mani and
M. T. Maybury, editors, Advances in
Automatic Text Summarization. MIT Press,
Cambridge, pages 325–336.
Goldstein, Jade, Mark Kantrowitz, Vibhu O.
Mittal, and Jaime G. Carbonell. 1999.
Summarizing text documents: Sentence
selection and evaluation metrics. In
Research and Development in Information
Retrieval, pages 121–128, Berkeley, CA.
Hahn, Udo and Donna Harman, editors.
2002. Proceedings of the Document
Understanding Conference (DUC-02).
Philadelphia, July.
Hahn, Udo and Ulrich Reimer. 1997.
Knowledge-based text summarization:
Salience and generalization operators for
knowledge base abstraction. In I. Mani
and M. Maybury, editors, Advances in
Automatic Text Summarization. MIT Press,
Cambridge, pages 215–232.
Harman, Donna and Daniel Marcu, editors.
2001. Proceedings of the Document
Understanding Conference (DUC-01). New
Orleans, September.
Hovy, E. and C.-Y. Lin. 1999. Automated
text summarization in SUMMARIST. In
I. Mani and M. T. Maybury, editors,
Advances in Automatic Text Summarization.
MIT Press, Cambridge, pages 81–94.
Jing, Hongyan. 2002. Using hidden Markov
modeling to decompose human-written
summaries. Computational Linguistics,
28(4), 527–543.
Jing, Hongyan and Kathleen McKeown.
1999. The decomposition of
human-written summary sentences. In
</reference>
<page confidence="0.996678">
406
</page>
<note confidence="0.82703975">
Radev, Hovy, and McKeown Summarization: Introduction
M. Hearst, F. Gey, and R. Tong, editors,
Proceedings of SIGIR’99: 22nd International
Conference on Research and Development in
</note>
<reference confidence="0.994135176470588">
Information Retrieval, University of
California, Berkeley, August,
pages 129–136.
Jing, Hongyan, Kathleen McKeown, Regina
Barzilay, and Michael Elhadad. 1998.
Summarization evaluation methods:
Experiments and analysis. In Intelligent
Text Summarization: Papers from the 1998
AAAI Spring Symposium, Stanford, CA,
23–25 March. Technical Report SS-98-06.
AAAI Press, pages 60–68.
Knight, Kevin and Daniel Marcu. 2000.
Statistics-based summarization—Step one:
Sentence compression. In Proceedings of the
17th National Conference of the American
Association for Artificial Intelligence
(AAAI-2000), pages 703–710.
Kupiec, Julian, Jan O. Pedersen, and
Francine Chen. 1995. A trainable
document summarizer. In Research and
Development in Information Retrieval,
pages 68–73.
Lin, C. and E. Hovy. 1997. Identifying topics
by position. In Fifth Conference on Applied
Natural Language Processing, Association
for Computational Linguistics, 31
March–3 April, pages 283–290.
Lin, Chin-Yew. 1999. Training a selection
function for extraction. In Proceedings of
the Eighteenth Annual International ACM
Conference on Information and Knowledge
Management (CIKM), Kansas City, 6
November. ACM, pages 55–62.
Lin, Chin-Yew. 2001. Summary evaluation
environment.
http://www.isi.edu/cyl/SEE.
Lin, Chin-Yew and Eduard Hovy. 2002a.
From single to multi-document
summarization: A prototype system and
its evaluation. In Proceedings of the 40th
Conference of the Association of
Computational Linguistics, Philadelphia,
July, pages 457–464.
Lin, Chin-Yew and Eduard Hovy. 2002b.
Manual and automatic evaluation of
summaries. In Proceedings of the Document
Understanding Conference (DUC-02)
Workshop on Multi-Document Summarization
Evaluation at the ACL Conference,
Philadelphia, July, pages 45–51.
Luhn, H. P. 1958. The automatic creation of
literature abstracts. IBM Journal of Research
Development, 2(2):159–165.
Mani, Inderjeet. 2001. Automatic
Summarization. John Benjamins,
Amsterdam/Philadelphia.
Mani, Inderjeet and Eric Bloedorn. 1997.
Multi-document summarization by graph
search and matching. In Proceedings of the
Fourteenth National Conference on Artificial
Intelligence (AAAI-97), Providence, RI.
American Association for Artificial
Intelligence, pages 622–628.
Mani, Inderjeet, Barbara Gates, and Eric
Bloedorn. 1999. Improving summaries by
revising them. In Proceedings of the 37th
Annual Meeting of the Association for
Computational Linguistics (ACL 99), College
Park, MD, June, pages 558–565.
Mani, Inderjeet, David House, G. Klein,
Lynette Hirshman, Leo Obrst, Th ´er `ese
Firmin, Michael Chrzanowski, and Beth
Sundheim. 1998. The TIPSTER SUMMAC
text summarization evaluation. Technical
Report MTR 98W0000138, The Mitre
Corporation, McLean, VA.
Mani, Inderjeet and Mark Maybury, editors.
1999. Advances in Automatic Text
Summarization. MIT Press, Cambridge.
Marcu, Daniel. 1997a. From discourse
structures to text summaries. In
Proceedings of the ACL’97/EACL’97 Workshop
on Intelligent Scalable Text Summarization,
Madrid, July 11, pages 82–88.
Marcu, Daniel. 1997b. The Rhetorical Parsing,
Summarization, and Generation of Natural
Language Texts. Ph.D. thesis, University of
Toronto, Toronto.
Marcu, Daniel. 1999. The automatic
construction of large-scale corpora for
summarization research. In M. Hearst,
F. Gey, and R. Tong, editors, Proceedings of
SIGIR’99: 22nd International Conference on
Research and Development in Information
Retrieval, University of California,
Berkeley, August, pages 137–144.
Marcu, Daniel and Laurie Gerber. 2001. An
inquiry into the nature of multidocument
abstracts, extracts, and their evaluation. In
Proceedings of the NAACL-2001 Workshop on
Automatic Summarization, Pittsburgh, June.
NAACL, pages 1–8.
McKeown, Kathleen, Judith Klavans,
Vasileios Hatzivassiloglou, Regina
Barzilay, and Eleazar Eskin. 1999.
Towards multidocument summarization
by reformulation: Progress and prospects.
In Proceedings of the 16th National
Conference of the American Association for
Artificial Intelligence (AAAI-1999), 18–22
July, pages 453–460.
McKeown, Kathleen R. and Dragomir R.
Radev. 1995. Generating summaries of
multiple news articles. In Proceedings of the
18th Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval, Seattle, July,
pages 74–82.
Ono, K., K. Sumita, and S. Miike. 1994.
</reference>
<page confidence="0.993121">
407
</page>
<note confidence="0.681392">
Computational Linguistics Volume 28, Number 4
</note>
<reference confidence="0.996435211111111">
Abstract generation based on rhetorical
structure extraction. In Proceedings of the
International Conference on Computational
Linguistics, Kyoto, Japan, pages 344–348.
Otterbacher, Jahna, Dragomir R. Radev, and
Airong Luo. 2002. Revisions that improve
cohesion in multi-document summaries:
A preliminary study. In ACL Workshop on
Text Summarization, Philadelphia.
Papineni, K., S. Roukos, T. Ward, and W-J.
Zhu. 2001. BLEU: A method for automatic
evaluation of machine translation.
Research Report RC22176, IBM.
Radev, Dragomir, Simone Teufel, Horacio
Saggion, Wai Lam, John Blitzer, Arda
¸Celebi, Hong Qi, Elliott Drabek, and
Danyu Liu. 2002. Evaluation of text
summarization in a cross-lingual
information retrieval framework.
Technical Report, Center for Language
and Speech Processing, Johns Hopkins
University, Baltimore, June.
Radev, Dragomir R., Hongyan Jing, and
Malgorzata Budzikowska. 2000.
Centroid-based summarization of
multiple documents: Sentence extraction,
utility-based evaluation, and user studies.
In ANLP/NAACL Workshop on
Summarization, Seattle, April.
Radev, Dragomir R. and Kathleen R.
McKeown. 1998. Generating natural
language summaries from multiple
on-line sources. Computational Linguistics,
24(3):469–500.
Rau, Lisa and Paul Jacobs. 1991. Creating
segmented databases from free text for
text retrieval. In Proceedings of the 14th
Annual International ACM-SIGIR Conference
on Research and Development in Information
Retrieval, New York, pages 337–346.
Saggion, Horacio and Guy Lapalme. 2002.
Generating indicative-informative
summaries with SumUM. Computational
Linguistics, 28(4), 497–526.
Salton, G., A. Singhal, M. Mitra, and
C. Buckley. 1997. Automatic text
structuring and summarization.
Information Processing &amp; Management,
33(2):193–207.
Silber, H. Gregory and Kathleen McCoy.
2002. Efficiently computed lexical chains
as an intermediate representation for
automatic text summarization.
Computational Linguistics, 28(4), 487–496.
Sparck Jones, Karen. 1999. Automatic
summarizing: Factors and directions. In
I. Mani and M. T. Maybury, editors,
Advances in Automatic Text Summarization.
MIT Press, Cambridge, pages 1–13.
Strzalkowski, Tomek, Gees Stein, J. Wang,
and Bowden Wise. 1999. A robust
practical text summarizer. In I. Mani and
M. T. Maybury, editors, Advances in
Automatic Text Summarization. MIT Press,
Cambridge, pages 137–154.
Teufel, Simone and Marc Moens. 2002.
Summarizing scientific articles:
Experiments with relevance and rhetorical
status. Computational Linguistics, 28(4),
409–445.
White, Michael and Claire Cardie. 2002.
Selecting sentences for multidocument
summaries using randomized local
search. In Proceedings of the Workshop on
Automatic Summarization (including DUC
2002), Philadelphia, July. Association for
Computational Linguistics, New
Brunswick, NJ, pages 9–18.
Witbrock, Michael and Vibhu Mittal. 1999.
Ultra-summarization: A statistical
approach to generating highly condensed
non-extractive summaries. In Proceedings
of the 22nd Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval, Berkeley,
pages 315–316.
Zechner, Klaus. 2002. Automatic
summarization of open-domain
multiparty dialogues in diverse genres.
Computational Linguistics, 28(4), 447–485.
</reference>
<page confidence="0.997893">
408
</page>
</variant>
</algorithm>

<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allan</author>
<author>Rahul Gupta</author>
<author>Vikas Khandelwal</author>
</authors>
<title>Temporal summaries of news topics.</title>
<date>2001</date>
<booktitle>In Proceedings of the 24th 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>335--336</pages>
<location>Melbourne, Australia,</location>
<marker>Allan, Gupta, Khandelwal, 2001</marker>
<rawString>Allan, James, Rahul Gupta, and Vikas Khandelwal. 2001. Temporal summaries of news topics. In Proceedings of the 24th 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Melbourne, Australia, pages 335–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: The kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="16870" citStr="Carletta 1996" startWordPosition="2514" endWordPosition="2516"> metrics and content metrics. Form metrics focus on grammaticality, overall text coherence, and organization and are usually measured on a point scale (Brandow, Mitze, and Rau 1995). Content is more difficult to measure. Typically, system output is compared sentence by sentence or fragment by fragment to one or more human-made ideal abstracts, and as in information retrieval, the percentage of extraneous information present in the system’s summary (precision) and the percentage of important information omitted from the summary (recall) are recorded. Other commonly used measures include kappa (Carletta 1996) and relative utility (Radev, Jing, and Budzikowska 2000), both of which take into account the performance of a summarizer that randomly picks passages from the original document to produce an extract. In the Document Understanding Conference (DUC)-01 and DUC-02 summarization competitions (Harman and Marcu 2001; Hahn and Harman 2002), NIST used the Summary Evaluation Environment (SEE) interface (Lin 2001) to record values for precision and recall. These two competitions, run along the lines of TREC, have served to establish overall baselines for single-document and multidocument summarization </context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Carletta, Jean. 1996. Assessing agreement on classification tasks: The kappa statistic. Computational Linguistics, 22(2):249–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Conroy</author>
<author>Dianne O’Leary</author>
</authors>
<title>Text summarization via hidden Markov models.</title>
<date>2001</date>
<booktitle>In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>406--407</pages>
<marker>Conroy, O’Leary, 2001</marker>
<rawString>Conroy, John and Dianne O’Leary. 2001. Text summarization via hidden Markov models. In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 406–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward T Cremmins</author>
</authors>
<title>The Art of Abstracting.</title>
<date>1996</date>
<publisher>Information Resources Press,</publisher>
<location>Arlington, VA,</location>
<note>second edition.</note>
<contexts>
<context position="1908" citStr="Cremmins 1996" startWordPosition="299" endWordPosition="300">ary is to present the main ideas in a document in less space. If all sentences in a text document were of equal importance, producing a summary would not be very effective, as any reduction in the size of a document would carry a proportional decrease in its informativeness. Luckily, information content in a document appears in bursts, and one can therefore distinguish between more and less informative segments. Identifying the informative segments at the expense of the rest is the main challenge in summarization. Of the many types of summary that have been identified (Borko and Bernier 1975; Cremmins 1996; Sparck Jones 1999; Hovy and Lin 1999), indicative summaries provide an idea of what the text is about without conveying specific content, and informative ones provide some shortened version of the content. Topic-oriented summaries concentrate on the reader’s desired topic(s) of interest, whereas generic summaries reflect the author’s point of view. Extracts are summaries created by reusing portions (words, sentences, etc.) of the input text verbatim, while abstracts are created by regenerating ∗ Assistant Professor, School of Information, Department of Electrical Engineering and Computer Sci</context>
</contexts>
<marker>Cremmins, 1996</marker>
<rawString>Cremmins, Edward T. 1996. The Art of Abstracting. Information Resources Press, Arlington, VA, second edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Francis DeJong</author>
</authors>
<title>Fast Skimming of News Stories: The FRUMP System.</title>
<date>1978</date>
<publisher>Ph.D.</publisher>
<contexts>
<context position="9280" citStr="DeJong 1978" startWordPosition="1382" endWordPosition="1383">ation fusion, and compression. Information extraction approaches can be characterized as “top-down,” since they look for a set of predefined information types to include in the summary (in contrast, extractive approaches are more data-driven). For each topic, the user predefines frames of expected information types, together with recognition criteria. For example, an earthquake frame may contain slots for location, earthquake magnitude, number of casualties, etc. The summarization engine must then locate the desired pieces of information, fill them in, and generate a summary with the results (DeJong 1978; Rau and Jacobs 1991). This method can produce high-quality and accurate summaries, albeit in restricted domains only. Compressive summarization results from approaching the problem from the point of view of language generation. Using the smallest units from the original document, Witbrock and Mittal (1999) extract a set of words from the input document and then order the words into sentences using a bigram language model. Jing and McKeown (1999) point out that human summaries are often constructed from the source document by a process of cutting and pasting document fragments that are then c</context>
</contexts>
<marker>DeJong, 1978</marker>
<rawString>DeJong, Gerald Francis. 1978. Fast Skimming of News Stories: The FRUMP System. Ph.D.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R L Donaway</author>
<author>K W Drummey</author>
<author>L A Mather</author>
</authors>
<title>A comparison of rankings produced by summarization evaluation measures.</title>
<date>2000</date>
<booktitle>In Proceedings of the Workshop on Automatic Summarization, ANLP-NAACL2000, Association for Computational Linguistics,</booktitle>
<volume>30</volume>
<pages>69--78</pages>
<marker>Donaway, Drummey, Mather, 2000</marker>
<rawString>thesis, Yale University, New Haven, CT. Donaway, R. L., K. W. Drummey, and L. A. Mather. 2000. A comparison of rankings produced by summarization evaluation measures. In Proceedings of the Workshop on Automatic Summarization, ANLP-NAACL2000, Association for Computational Linguistics, 30 April, pages 69–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Edmundson</author>
</authors>
<title>New methods in automatic extracting.</title>
<date>1969</date>
<journal>Journal of the Association for Computing Machinery,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="5462" citStr="Edmundson 1969" startWordPosition="813" endWordPosition="814">arly extraction research focused on the development of relatively simple surface-level techniques that tend to signal important passages in the source text. Although most systems use sentences as units, some work with larger passages, typically paragraphs. Typically, a set of features is computed for each passage, and ultimately these features are normalized and summed. The passages with the highest resulting scores are sorted and returned as the extract. Early techniques for sentence extraction computed a score for each sentence based on features such as position in the text (Baxendale 1958; Edmundson 1969), word and phrase frequency (Luhn 1958), key phrases (e.g., “it is important to note”) (Edmundson 1969). Recent extraction approaches use more sophisticated techniques for deciding which sentences to extract; these techniques often rely on machine learning to identify important features, on natural language analysis to identify key passages, or on relations between words rather than bags of words. The application of machine learning to summarization was pioneered by Kupiec, Pedersen, and Chen (1995), who developed a summarizer using a Bayesian classifier to combine features from a corpus of sc</context>
</contexts>
<marker>Edmundson, 1969</marker>
<rawString>Edmundson, H. P. 1969. New methods in automatic extracting. Journal of the Association for Computing Machinery, 16(2):264–285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Firmin</author>
<author>M J Chrzanowski</author>
</authors>
<title>An evaluation of automatic text summarization systems.</title>
<date>1999</date>
<booktitle>Advances in Automatic Text Summarization.</booktitle>
<pages>325--336</pages>
<editor>In I. Mani and M. T. Maybury, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge,</location>
<contexts>
<context position="18026" citStr="Firmin and Chrzanowski 1999" startWordPosition="2682" endWordPosition="2685">h overall baselines for single-document and multidocument summarization and have provided several hundred human abstracts as training material. (Another popular source of training material is the Ziff-Davis corpus of computer product announcements.) Despite low interjudge agreement, DUC has shown that humans are better summary producers than machines and that, for the news article genre, certain algorithms do in fact do better than the simple baseline of picking the lead material. The largest task-oriented evaluation to date, the Summarization Evaluation Conference (SUMMAC) (Mani et al. 1998; Firmin and Chrzanowski 1999) included three tests: the categorization task (how well can humans categorize a summary compared to its full text?), the ad hoc task (how well can humans determine whether a full text is relevant to a query just from reading the summary?) and the question task (how well can humans answer questions about the main thrust of the source text from reading just the summary?). But the interpretation of the results is not simple; studies (Jing et al. 1998; Donaway, Drummey, and Mather 2000; Radev, Jing, and Budzikowska 2000) 403 Computational Linguistics Volume 28, Number 4 show how the same summarie</context>
</contexts>
<marker>Firmin, Chrzanowski, 1999</marker>
<rawString>Firmin, T. and M. J. Chrzanowski. 1999. An evaluation of automatic text summarization systems. In I. Mani and M. T. Maybury, editors, Advances in Automatic Text Summarization. MIT Press, Cambridge, pages 325–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jade Goldstein</author>
<author>Mark Kantrowitz</author>
<author>Vibhu O Mittal</author>
<author>Jaime G Carbonell</author>
</authors>
<title>Summarizing text documents: Sentence selection and evaluation metrics.</title>
<date>1999</date>
<booktitle>In Research and Development in Information Retrieval,</booktitle>
<pages>121--128</pages>
<location>Berkeley, CA.</location>
<contexts>
<context position="19529" citStr="Goldstein et al. (1999)" startWordPosition="2919" endWordPosition="2922">is fixed relatively short. Marcu (1997a) provides some evidence that other genres will deliver less consistency. With regard to the lengths of the summaries produced by humans when not constrained by a particular compression rate, both Jing and Marcu find great variation. Nonetheless, it is now generally accepted that for single news articles, systems produce generic summaries indistinguishable from those of humans. Automated summary evaluation is a gleam in everyone’s eye. Clearly, when an ideal extract has been created by human(s), extractive summaries are easy to evaluate. Marcu (1999) and Goldstein et al. (1999) independently developed an automated method to create extracts corresponding to abstracts. But when the number of available extracts is not sufficient, it is not clear how to overcome the problems of low interhuman agreement. Simply using a variant of the Bilingual Evaluation Understudy (BLEU) scoring method (based on a linear combination of matching n-grams between the system output and the ideal summary) developed for machine translation (Papineni et al. 2001) is promising but not sufficient (Lin and Hovy 2002b). 3. The Articles in this Issue The articles in this issue move beyond the curre</context>
</contexts>
<marker>Goldstein, Kantrowitz, Mittal, Carbonell, 1999</marker>
<rawString>Goldstein, Jade, Mark Kantrowitz, Vibhu O. Mittal, and Jaime G. Carbonell. 1999. Summarizing text documents: Sentence selection and evaluation metrics. In Research and Development in Information Retrieval, pages 121–128, Berkeley, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Udo Hahn</author>
<author>Donna Harman</author>
<author>editors</author>
</authors>
<date>2002</date>
<booktitle>Proceedings of the Document Understanding Conference (DUC-02).</booktitle>
<location>Philadelphia,</location>
<marker>Hahn, Harman, editors, 2002</marker>
<rawString>Hahn, Udo and Donna Harman, editors. 2002. Proceedings of the Document Understanding Conference (DUC-02). Philadelphia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Udo Hahn</author>
<author>Ulrich Reimer</author>
</authors>
<title>Knowledge-based text summarization: Salience and generalization operators for knowledge base abstraction.</title>
<date>1997</date>
<booktitle>Advances in Automatic Text Summarization.</booktitle>
<pages>215--232</pages>
<editor>In I. Mani and M. Maybury, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge,</location>
<contexts>
<context position="11349" citStr="Hahn and Reimer 1997" startWordPosition="1706" endWordPosition="1709">s into one, three into two (or one), and so on. Of course, true abstraction involves taking the process one step further. Abstraction involves recognizing that a set of extracted passages together constitute something new, something that is not explicitly mentioned in the source, and then replacing them in the summary with the (ideally more concise) new concept(s). The requirement that the new material not be in the text explicitly means that the system must have access to external information of some kind, such as an ontology or a knowledge base, and be able to perform combinatory inference (Hahn and Reimer 1997). Since no large-scale resources of this kind yet exist, abstractive summarization has not progressed beyond the proof-of-concept stage (although top-down information extraction can be seen as one variant). 2.3 Multidocument Summarization Multidocument summarization, the process of producing a single summary of a set of related source documents, is relatively new. The three major problems introduced by having to handle multiple input documents are (1) recognizing and coping with redundancy, (2) identifying important differences among documents, and (3) ensuring summary coherence, even when mat</context>
</contexts>
<marker>Hahn, Reimer, 1997</marker>
<rawString>Hahn, Udo and Ulrich Reimer. 1997. Knowledge-based text summarization: Salience and generalization operators for knowledge base abstraction. In I. Mani and M. Maybury, editors, Advances in Automatic Text Summarization. MIT Press, Cambridge, pages 215–232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donna Harman</author>
<author>Daniel Marcu</author>
<author>editors</author>
</authors>
<date>2001</date>
<booktitle>Proceedings of the Document Understanding Conference (DUC-01).</booktitle>
<location>New Orleans,</location>
<marker>Harman, Marcu, editors, 2001</marker>
<rawString>Harman, Donna and Daniel Marcu, editors. 2001. Proceedings of the Document Understanding Conference (DUC-01). New Orleans, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>C-Y Lin</author>
</authors>
<title>Automated text summarization</title>
<date>1999</date>
<booktitle>Advances in Automatic Text Summarization.</booktitle>
<pages>81--94</pages>
<editor>in SUMMARIST. In I. Mani and M. T. Maybury, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge,</location>
<contexts>
<context position="1947" citStr="Hovy and Lin 1999" startWordPosition="304" endWordPosition="307">n a document in less space. If all sentences in a text document were of equal importance, producing a summary would not be very effective, as any reduction in the size of a document would carry a proportional decrease in its informativeness. Luckily, information content in a document appears in bursts, and one can therefore distinguish between more and less informative segments. Identifying the informative segments at the expense of the rest is the main challenge in summarization. Of the many types of summary that have been identified (Borko and Bernier 1975; Cremmins 1996; Sparck Jones 1999; Hovy and Lin 1999), indicative summaries provide an idea of what the text is about without conveying specific content, and informative ones provide some shortened version of the content. Topic-oriented summaries concentrate on the reader’s desired topic(s) of interest, whereas generic summaries reflect the author’s point of view. Extracts are summaries created by reusing portions (words, sentences, etc.) of the input text verbatim, while abstracts are created by regenerating ∗ Assistant Professor, School of Information, Department of Electrical Engineering and Computer Science and Department of Linguistics, Uni</context>
</contexts>
<marker>Hovy, Lin, 1999</marker>
<rawString>Hovy, E. and C.-Y. Lin. 1999. Automated text summarization in SUMMARIST. In I. Mani and M. T. Maybury, editors, Advances in Automatic Text Summarization. MIT Press, Cambridge, pages 81–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
</authors>
<title>Using hidden Markov modeling to decompose human-written summaries.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>4</issue>
<pages>527--543</pages>
<contexts>
<context position="10148" citStr="Jing (2002)" startWordPosition="1516" endWordPosition="1517">om the original document, Witbrock and Mittal (1999) extract a set of words from the input document and then order the words into sentences using a bigram language model. Jing and McKeown (1999) point out that human summaries are often constructed from the source document by a process of cutting and pasting document fragments that are then combined and regenerated as summary sentences. Hence a summarizer can be developed to extract sentences, reduce them by dropping unimportant fragments, and then use information fusion and generation to combine the remaining fragments. In this special issue, Jing (2002) reports on automated techniques to build a corpus representing the cut-and-paste process used by humans; such a corpus can then be used to train an automated summarizer. Other researchers focus on the reduction process. In an attempt to learn rules for reduction, Knight and Marcu (2000) use expectation maximization to train a system to compress the syntactic parse tree of a sentence in order to produce a shorter but 401 Computational Linguistics Volume 28, Number 4 still maximally grammatical version. Ultimately, this approach can likely be used for shortening two sentences into one, three in</context>
<context position="21555" citStr="Jing (2002)" startWordPosition="3241" endWordPosition="3242">id in the process of developing summarization systems, on computational efficiency of algorithms, and on techniques needed for preprocessing speech. The four articles that focus on summarization of text share a common theme: Each views the summarization process as consisting of two phases. In the first, material within the original document that is important is identified and extracted. In the second, this extracted material may be modified, merged, and edited using generation techniques. Two of the articles focus on the extraction stage (Teufel and Moens 2002; Silber and McCoy 2002), whereas Jing (2002) examines tools for automatically constructing resources that can be used for the second stage. Teufel and Moens propose significantly different techniques for sentence extraction than have been used in the past. Noting the difference in both length and structure between scientific articles and news, they claim that both the context of sentences and a more focused search for sentences is needed in order to produce a good summary that is only 2.5% of the original document. Their approach is to provide a summary that focuses on the new contribution of the paper and its relation to previous work.</context>
</contexts>
<marker>Jing, 2002</marker>
<rawString>Jing, Hongyan. 2002. Using hidden Markov modeling to decompose human-written summaries. Computational Linguistics, 28(4), 527–543.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
<author>Kathleen McKeown</author>
</authors>
<title>The decomposition of human-written summary sentences. In Information Retrieval,</title>
<date>1999</date>
<pages>129--136</pages>
<institution>University of California, Berkeley,</institution>
<contexts>
<context position="9731" citStr="Jing and McKeown (1999)" startWordPosition="1449" endWordPosition="1452">, number of casualties, etc. The summarization engine must then locate the desired pieces of information, fill them in, and generate a summary with the results (DeJong 1978; Rau and Jacobs 1991). This method can produce high-quality and accurate summaries, albeit in restricted domains only. Compressive summarization results from approaching the problem from the point of view of language generation. Using the smallest units from the original document, Witbrock and Mittal (1999) extract a set of words from the input document and then order the words into sentences using a bigram language model. Jing and McKeown (1999) point out that human summaries are often constructed from the source document by a process of cutting and pasting document fragments that are then combined and regenerated as summary sentences. Hence a summarizer can be developed to extract sentences, reduce them by dropping unimportant fragments, and then use information fusion and generation to combine the remaining fragments. In this special issue, Jing (2002) reports on automated techniques to build a corpus representing the cut-and-paste process used by humans; such a corpus can then be used to train an automated summarizer. Other resear</context>
</contexts>
<marker>Jing, McKeown, 1999</marker>
<rawString>Jing, Hongyan and Kathleen McKeown. 1999. The decomposition of human-written summary sentences. In Information Retrieval, University of California, Berkeley, August, pages 129–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
<author>Kathleen McKeown</author>
<author>Regina Barzilay</author>
<author>Michael Elhadad</author>
</authors>
<title>Summarization evaluation methods: Experiments and analysis.</title>
<date>1998</date>
<booktitle>In Intelligent Text Summarization: Papers from the</booktitle>
<tech>Technical Report SS-98-06.</tech>
<pages>23--25</pages>
<publisher>AAAI Spring Symposium,</publisher>
<location>Stanford, CA,</location>
<contexts>
<context position="18478" citStr="Jing et al. 1998" startWordPosition="2760" endWordPosition="2763">ng the lead material. The largest task-oriented evaluation to date, the Summarization Evaluation Conference (SUMMAC) (Mani et al. 1998; Firmin and Chrzanowski 1999) included three tests: the categorization task (how well can humans categorize a summary compared to its full text?), the ad hoc task (how well can humans determine whether a full text is relevant to a query just from reading the summary?) and the question task (how well can humans answer questions about the main thrust of the source text from reading just the summary?). But the interpretation of the results is not simple; studies (Jing et al. 1998; Donaway, Drummey, and Mather 2000; Radev, Jing, and Budzikowska 2000) 403 Computational Linguistics Volume 28, Number 4 show how the same summaries receive different scores under different measures or when compared to different (but presumably equivalent) ideal summaries created by humans. With regard to interhuman agreement, Jing et al. find fairly high consistency in the news genre only when the summary (extract) length is fixed relatively short. Marcu (1997a) provides some evidence that other genres will deliver less consistency. With regard to the lengths of the summaries produced by hum</context>
</contexts>
<marker>Jing, McKeown, Barzilay, Elhadad, 1998</marker>
<rawString>Jing, Hongyan, Kathleen McKeown, Regina Barzilay, and Michael Elhadad. 1998. Summarization evaluation methods: Experiments and analysis. In Intelligent Text Summarization: Papers from the 1998 AAAI Spring Symposium, Stanford, CA, 23–25 March. Technical Report SS-98-06. AAAI Press, pages 60–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistics-based summarization—Step one: Sentence compression.</title>
<date>2000</date>
<booktitle>In Proceedings of the 17th National Conference of the American Association for Artificial Intelligence (AAAI-2000),</booktitle>
<pages>703--710</pages>
<contexts>
<context position="10436" citStr="Knight and Marcu (2000)" startWordPosition="1560" endWordPosition="1563">a process of cutting and pasting document fragments that are then combined and regenerated as summary sentences. Hence a summarizer can be developed to extract sentences, reduce them by dropping unimportant fragments, and then use information fusion and generation to combine the remaining fragments. In this special issue, Jing (2002) reports on automated techniques to build a corpus representing the cut-and-paste process used by humans; such a corpus can then be used to train an automated summarizer. Other researchers focus on the reduction process. In an attempt to learn rules for reduction, Knight and Marcu (2000) use expectation maximization to train a system to compress the syntactic parse tree of a sentence in order to produce a shorter but 401 Computational Linguistics Volume 28, Number 4 still maximally grammatical version. Ultimately, this approach can likely be used for shortening two sentences into one, three into two (or one), and so on. Of course, true abstraction involves taking the process one step further. Abstraction involves recognizing that a set of extracted passages together constitute something new, something that is not explicitly mentioned in the source, and then replacing them in </context>
</contexts>
<marker>Knight, Marcu, 2000</marker>
<rawString>Knight, Kevin and Daniel Marcu. 2000. Statistics-based summarization—Step one: Sentence compression. In Proceedings of the 17th National Conference of the American Association for Artificial Intelligence (AAAI-2000), pages 703–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
<author>Jan O Pedersen</author>
<author>Francine Chen</author>
</authors>
<title>A trainable document summarizer.</title>
<date>1995</date>
<booktitle>In Research and Development in Information Retrieval,</booktitle>
<pages>68--73</pages>
<marker>Kupiec, Pedersen, Chen, 1995</marker>
<rawString>Kupiec, Julian, Jan O. Pedersen, and Francine Chen. 1995. A trainable document summarizer. In Research and Development in Information Retrieval, pages 68–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lin</author>
<author>E Hovy</author>
</authors>
<title>Identifying topics by position.</title>
<date>1997</date>
<booktitle>In Fifth Conference on Applied Natural Language Processing, Association for Computational Linguistics, 31 March–3</booktitle>
<pages>283--290</pages>
<contexts>
<context position="6312" citStr="Lin and Hovy (1997)" startWordPosition="938" endWordPosition="941"> rely on machine learning to identify important features, on natural language analysis to identify key passages, or on relations between words rather than bags of words. The application of machine learning to summarization was pioneered by Kupiec, Pedersen, and Chen (1995), who developed a summarizer using a Bayesian classifier to combine features from a corpus of scientific articles and their abstracts. Aone et al. (1999) and Lin (1999) experimented with other forms of machine learning and its effectiveness. Machine learning has also been applied to learning individual features; for example, Lin and Hovy (1997) applied machine learning to the problem of determining how sentence position affects the selection of sentences, and Witbrock and Mittal (1999) used statistical approaches to choose important words and phrases and their syntactic context. 400 Radev, Hovy, and McKeown Summarization: Introduction Approaches involving more sophisticated natural language analysis to identify key passages rely on analysis either of word relatedness or of discourse structure. Some research uses the degree of lexical connectedness between potential passages and the remainder of the text; connectedness may be measure</context>
</contexts>
<marker>Lin, Hovy, 1997</marker>
<rawString>Lin, C. and E. Hovy. 1997. Identifying topics by position. In Fifth Conference on Applied Natural Language Processing, Association for Computational Linguistics, 31 March–3 April, pages 283–290.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Training a selection function for extraction.</title>
<date>1999</date>
<booktitle>In Proceedings of the Eighteenth Annual International ACM Conference on Information and Knowledge Management (CIKM), Kansas City,</booktitle>
<volume>6</volume>
<pages>55--62</pages>
<publisher>ACM,</publisher>
<contexts>
<context position="1947" citStr="Lin 1999" startWordPosition="306" endWordPosition="307">ent in less space. If all sentences in a text document were of equal importance, producing a summary would not be very effective, as any reduction in the size of a document would carry a proportional decrease in its informativeness. Luckily, information content in a document appears in bursts, and one can therefore distinguish between more and less informative segments. Identifying the informative segments at the expense of the rest is the main challenge in summarization. Of the many types of summary that have been identified (Borko and Bernier 1975; Cremmins 1996; Sparck Jones 1999; Hovy and Lin 1999), indicative summaries provide an idea of what the text is about without conveying specific content, and informative ones provide some shortened version of the content. Topic-oriented summaries concentrate on the reader’s desired topic(s) of interest, whereas generic summaries reflect the author’s point of view. Extracts are summaries created by reusing portions (words, sentences, etc.) of the input text verbatim, while abstracts are created by regenerating ∗ Assistant Professor, School of Information, Department of Electrical Engineering and Computer Science and Department of Linguistics, Uni</context>
<context position="6134" citStr="Lin (1999)" startWordPosition="914" endWordPosition="915">t is important to note”) (Edmundson 1969). Recent extraction approaches use more sophisticated techniques for deciding which sentences to extract; these techniques often rely on machine learning to identify important features, on natural language analysis to identify key passages, or on relations between words rather than bags of words. The application of machine learning to summarization was pioneered by Kupiec, Pedersen, and Chen (1995), who developed a summarizer using a Bayesian classifier to combine features from a corpus of scientific articles and their abstracts. Aone et al. (1999) and Lin (1999) experimented with other forms of machine learning and its effectiveness. Machine learning has also been applied to learning individual features; for example, Lin and Hovy (1997) applied machine learning to the problem of determining how sentence position affects the selection of sentences, and Witbrock and Mittal (1999) used statistical approaches to choose important words and phrases and their syntactic context. 400 Radev, Hovy, and McKeown Summarization: Introduction Approaches involving more sophisticated natural language analysis to identify key passages rely on analysis either of word re</context>
</contexts>
<marker>Lin, 1999</marker>
<rawString>Lin, Chin-Yew. 1999. Training a selection function for extraction. In Proceedings of the Eighteenth Annual International ACM Conference on Information and Knowledge Management (CIKM), Kansas City, 6 November. ACM, pages 55–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<date>2001</date>
<note>Summary evaluation environment. http://www.isi.edu/cyl/SEE.</note>
<contexts>
<context position="17278" citStr="Lin 2001" startWordPosition="2574" endWordPosition="2575">mation present in the system’s summary (precision) and the percentage of important information omitted from the summary (recall) are recorded. Other commonly used measures include kappa (Carletta 1996) and relative utility (Radev, Jing, and Budzikowska 2000), both of which take into account the performance of a summarizer that randomly picks passages from the original document to produce an extract. In the Document Understanding Conference (DUC)-01 and DUC-02 summarization competitions (Harman and Marcu 2001; Hahn and Harman 2002), NIST used the Summary Evaluation Environment (SEE) interface (Lin 2001) to record values for precision and recall. These two competitions, run along the lines of TREC, have served to establish overall baselines for single-document and multidocument summarization and have provided several hundred human abstracts as training material. (Another popular source of training material is the Ziff-Davis corpus of computer product announcements.) Despite low interjudge agreement, DUC has shown that humans are better summary producers than machines and that, for the news article genre, certain algorithms do in fact do better than the simple baseline of picking the lead mate</context>
</contexts>
<marker>Lin, 2001</marker>
<rawString>Lin, Chin-Yew. 2001. Summary evaluation environment. http://www.isi.edu/cyl/SEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>From single to multi-document summarization: A prototype system and its evaluation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Conference of the Association of Computational Linguistics,</booktitle>
<pages>457--464</pages>
<location>Philadelphia,</location>
<contexts>
<context position="14541" citStr="Lin and Hovy 2002" startWordPosition="2167" endWordPosition="2170">own, and Elhadad 1999). Mani, Gates, and Bloedorn (1999) describe the use of human-generated compression and reformulation rules. Ensuring coherence is difficult, because this in principle requires some understanding of the content of each passage and knowledge about the structure of discourse. In practice, most systems simply follow time order and text order (passages from the oldest text appear first, sorted in the order in which they appear in the input). To avoid misleading the reader when juxtaposed passages from different dates all say “yesterday,” some systems add explicit time stamps (Lin and Hovy 2002a). Other 402 Radev, Hovy, and McKeown Summarization: Introduction systems use a combination of temporal and coherence constraints to order sentences (Barzilay, Elhadad, and McKeown 2001). Recently, Otterbacher, Radev, and Luo (2002) have focused on discourse-based revisions of multidocument clusters as a means for improving summary coherence. Although multidocument summarization is new and the approaches described here are only the beginning, current research also branches out in other directions. Research is beginning on the generation of updates on new information (Allan, Gupta, and Khandel</context>
<context position="20047" citStr="Lin and Hovy 2002" startWordPosition="3000" endWordPosition="3003">ed by human(s), extractive summaries are easy to evaluate. Marcu (1999) and Goldstein et al. (1999) independently developed an automated method to create extracts corresponding to abstracts. But when the number of available extracts is not sufficient, it is not clear how to overcome the problems of low interhuman agreement. Simply using a variant of the Bilingual Evaluation Understudy (BLEU) scoring method (based on a linear combination of matching n-grams between the system output and the ideal summary) developed for machine translation (Papineni et al. 2001) is promising but not sufficient (Lin and Hovy 2002b). 3. The Articles in this Issue The articles in this issue move beyond the current state of the art in various ways. Whereas most research to date has focused on the use of sentence extraction for summarization, we are beginning to see techniques that allow a system to extract, merge, and edit phrases, as opposed to full sentences, to generate a summary. Whereas many summarization systems are designed for summarization of news, new algorithms are summarizing much longer and more complex documents, such as scientific journal articles, medical journal articles, or patents. Whereas most researc</context>
</contexts>
<marker>Lin, Hovy, 2002</marker>
<rawString>Lin, Chin-Yew and Eduard Hovy. 2002a. From single to multi-document summarization: A prototype system and its evaluation. In Proceedings of the 40th Conference of the Association of Computational Linguistics, Philadelphia, July, pages 457–464.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>Manual and automatic evaluation of summaries.</title>
<date>2002</date>
<booktitle>In Proceedings of the Document Understanding Conference (DUC-02) Workshop on Multi-Document Summarization Evaluation at the ACL Conference,</booktitle>
<pages>45--51</pages>
<location>Philadelphia,</location>
<contexts>
<context position="14541" citStr="Lin and Hovy 2002" startWordPosition="2167" endWordPosition="2170">own, and Elhadad 1999). Mani, Gates, and Bloedorn (1999) describe the use of human-generated compression and reformulation rules. Ensuring coherence is difficult, because this in principle requires some understanding of the content of each passage and knowledge about the structure of discourse. In practice, most systems simply follow time order and text order (passages from the oldest text appear first, sorted in the order in which they appear in the input). To avoid misleading the reader when juxtaposed passages from different dates all say “yesterday,” some systems add explicit time stamps (Lin and Hovy 2002a). Other 402 Radev, Hovy, and McKeown Summarization: Introduction systems use a combination of temporal and coherence constraints to order sentences (Barzilay, Elhadad, and McKeown 2001). Recently, Otterbacher, Radev, and Luo (2002) have focused on discourse-based revisions of multidocument clusters as a means for improving summary coherence. Although multidocument summarization is new and the approaches described here are only the beginning, current research also branches out in other directions. Research is beginning on the generation of updates on new information (Allan, Gupta, and Khandel</context>
<context position="20047" citStr="Lin and Hovy 2002" startWordPosition="3000" endWordPosition="3003">ed by human(s), extractive summaries are easy to evaluate. Marcu (1999) and Goldstein et al. (1999) independently developed an automated method to create extracts corresponding to abstracts. But when the number of available extracts is not sufficient, it is not clear how to overcome the problems of low interhuman agreement. Simply using a variant of the Bilingual Evaluation Understudy (BLEU) scoring method (based on a linear combination of matching n-grams between the system output and the ideal summary) developed for machine translation (Papineni et al. 2001) is promising but not sufficient (Lin and Hovy 2002b). 3. The Articles in this Issue The articles in this issue move beyond the current state of the art in various ways. Whereas most research to date has focused on the use of sentence extraction for summarization, we are beginning to see techniques that allow a system to extract, merge, and edit phrases, as opposed to full sentences, to generate a summary. Whereas many summarization systems are designed for summarization of news, new algorithms are summarizing much longer and more complex documents, such as scientific journal articles, medical journal articles, or patents. Whereas most researc</context>
</contexts>
<marker>Lin, Hovy, 2002</marker>
<rawString>Lin, Chin-Yew and Eduard Hovy. 2002b. Manual and automatic evaluation of summaries. In Proceedings of the Document Understanding Conference (DUC-02) Workshop on Multi-Document Summarization Evaluation at the ACL Conference, Philadelphia, July, pages 45–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Luhn</author>
</authors>
<title>The automatic creation of literature abstracts.</title>
<date>1958</date>
<journal>IBM Journal of Research Development,</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="5501" citStr="Luhn 1958" startWordPosition="819" endWordPosition="820">lopment of relatively simple surface-level techniques that tend to signal important passages in the source text. Although most systems use sentences as units, some work with larger passages, typically paragraphs. Typically, a set of features is computed for each passage, and ultimately these features are normalized and summed. The passages with the highest resulting scores are sorted and returned as the extract. Early techniques for sentence extraction computed a score for each sentence based on features such as position in the text (Baxendale 1958; Edmundson 1969), word and phrase frequency (Luhn 1958), key phrases (e.g., “it is important to note”) (Edmundson 1969). Recent extraction approaches use more sophisticated techniques for deciding which sentences to extract; these techniques often rely on machine learning to identify important features, on natural language analysis to identify key passages, or on relations between words rather than bags of words. The application of machine learning to summarization was pioneered by Kupiec, Pedersen, and Chen (1995), who developed a summarizer using a Bayesian classifier to combine features from a corpus of scientific articles and their abstracts. </context>
</contexts>
<marker>Luhn, 1958</marker>
<rawString>Luhn, H. P. 1958. The automatic creation of literature abstracts. IBM Journal of Research Development, 2(2):159–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
</authors>
<title>Automatic Summarization.</title>
<date>2001</date>
<location>John Benjamins, Amsterdam/Philadelphia.</location>
<contexts>
<context position="847" citStr="Mani 2001" startWordPosition="121" endWordPosition="122">ms that can automatically summarize one or more documents become increasingly desirable. Recent research has investigated types of summaries, methods to create them, and methods to evaluate them. Several evaluation competitions (in the style of the National Institute of Standards and Technology’s [NIST’s] Text Retrieval Conference [TREC]) have helped determine baseline performance levels and provide a limited set of training material. Frequent workshops and symposia reflect the ongoing interest of researchers around the world. The volume of papers edited by Mani and Maybury (1999) and a book (Mani 2001) provide good introductions to the state of the art in this rapidly evolving subfield. A summary can be loosely defined as a text that is produced from one or more texts, that conveys important information in the original text(s), and that is no longer than half of the original text(s) and usually significantly less than that. Text here is used rather loosely and can refer to speech, multimedia documents, hypertext, etc. The main goal of a summary is to present the main ideas in a document in less space. If all sentences in a text document were of equal importance, producing a summary would no</context>
</contexts>
<marker>Mani, 2001</marker>
<rawString>Mani, Inderjeet. 2001. Automatic Summarization. John Benjamins, Amsterdam/Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>Eric Bloedorn</author>
</authors>
<title>Multi-document summarization by graph search and matching.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fourteenth National Conference on Artificial Intelligence (AAAI-97),</booktitle>
<pages>622--628</pages>
<publisher>American</publisher>
<location>Providence, RI.</location>
<contexts>
<context position="7016" citStr="Mani and Bloedorn 1997" startWordPosition="1041" endWordPosition="1044">cts the selection of sentences, and Witbrock and Mittal (1999) used statistical approaches to choose important words and phrases and their syntactic context. 400 Radev, Hovy, and McKeown Summarization: Introduction Approaches involving more sophisticated natural language analysis to identify key passages rely on analysis either of word relatedness or of discourse structure. Some research uses the degree of lexical connectedness between potential passages and the remainder of the text; connectedness may be measured by the number of shared words, synonyms, or anaphora (e.g., Salton et al. 1997; Mani and Bloedorn 1997; Barzilay and Elhadad 1999). Other research rewards passages that include topic words, that is, words that have been determined to correlate well with the topic of interest to the user (for topic-oriented summaries) or with the general theme of the source text (Buckley and Cardie 1997; Strzalkowski et al. 1999; Radev, Jing, and Budzikowska 2000). Alternatively, a summarizer may reward passages that occupy important positions in the discourse structure of the text (Ono, Sumita, and Miike 1994; Marcu 1997b). This method requires a system to compute discourse structure reliably, which is not pos</context>
</contexts>
<marker>Mani, Bloedorn, 1997</marker>
<rawString>Mani, Inderjeet and Eric Bloedorn. 1997. Multi-document summarization by graph search and matching. In Proceedings of the Fourteenth National Conference on Artificial Intelligence (AAAI-97), Providence, RI. American Association for Artificial Intelligence, pages 622–628.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>Barbara Gates</author>
<author>Eric Bloedorn</author>
</authors>
<title>Improving summaries by revising them.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL 99),</booktitle>
<pages>558--565</pages>
<location>College Park, MD,</location>
<marker>Mani, Gates, Bloedorn, 1999</marker>
<rawString>Mani, Inderjeet, Barbara Gates, and Eric Bloedorn. 1999. Improving summaries by revising them. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL 99), College Park, MD, June, pages 558–565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>David House</author>
<author>G Klein</author>
</authors>
<title>Lynette Hirshman, Leo Obrst, Th ´er `ese</title>
<date>1998</date>
<tech>Technical Report MTR 98W0000138,</tech>
<institution>The Mitre Corporation,</institution>
<location>McLean, VA.</location>
<contexts>
<context position="17996" citStr="Mani et al. 1998" startWordPosition="2678" endWordPosition="2681">served to establish overall baselines for single-document and multidocument summarization and have provided several hundred human abstracts as training material. (Another popular source of training material is the Ziff-Davis corpus of computer product announcements.) Despite low interjudge agreement, DUC has shown that humans are better summary producers than machines and that, for the news article genre, certain algorithms do in fact do better than the simple baseline of picking the lead material. The largest task-oriented evaluation to date, the Summarization Evaluation Conference (SUMMAC) (Mani et al. 1998; Firmin and Chrzanowski 1999) included three tests: the categorization task (how well can humans categorize a summary compared to its full text?), the ad hoc task (how well can humans determine whether a full text is relevant to a query just from reading the summary?) and the question task (how well can humans answer questions about the main thrust of the source text from reading just the summary?). But the interpretation of the results is not simple; studies (Jing et al. 1998; Donaway, Drummey, and Mather 2000; Radev, Jing, and Budzikowska 2000) 403 Computational Linguistics Volume 28, Numbe</context>
</contexts>
<marker>Mani, House, Klein, 1998</marker>
<rawString>Mani, Inderjeet, David House, G. Klein, Lynette Hirshman, Leo Obrst, Th ´er `ese Firmin, Michael Chrzanowski, and Beth Sundheim. 1998. The TIPSTER SUMMAC text summarization evaluation. Technical Report MTR 98W0000138, The Mitre Corporation, McLean, VA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>Mark Maybury</author>
<author>editors</author>
</authors>
<date>1999</date>
<booktitle>Advances in Automatic Text Summarization.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge.</location>
<marker>Mani, Maybury, editors, 1999</marker>
<rawString>Mani, Inderjeet and Mark Maybury, editors. 1999. Advances in Automatic Text Summarization. MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>From discourse structures to text summaries.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL’97/EACL’97 Workshop on Intelligent Scalable Text Summarization,</booktitle>
<volume>11</volume>
<pages>82--88</pages>
<location>Madrid,</location>
<contexts>
<context position="7525" citStr="Marcu 1997" startWordPosition="1122" endWordPosition="1123">y the number of shared words, synonyms, or anaphora (e.g., Salton et al. 1997; Mani and Bloedorn 1997; Barzilay and Elhadad 1999). Other research rewards passages that include topic words, that is, words that have been determined to correlate well with the topic of interest to the user (for topic-oriented summaries) or with the general theme of the source text (Buckley and Cardie 1997; Strzalkowski et al. 1999; Radev, Jing, and Budzikowska 2000). Alternatively, a summarizer may reward passages that occupy important positions in the discourse structure of the text (Ono, Sumita, and Miike 1994; Marcu 1997b). This method requires a system to compute discourse structure reliably, which is not possible in all genres. This technique is the focus of one of the articles in this special issue (Teufel and Moens 2002), which shows how particular types of rhetorical relations in the genre of scientific journal articles can be reliably identified through the use of classification. An open-source summarization environment, MEAD, was recently developed at the Johns Hopkins summer workshop (Radev et al. 2002). MEAD allows researchers to experiment with different features and methods for combination. Some re</context>
<context position="18944" citStr="Marcu (1997" startWordPosition="2831" endWordPosition="2832">he main thrust of the source text from reading just the summary?). But the interpretation of the results is not simple; studies (Jing et al. 1998; Donaway, Drummey, and Mather 2000; Radev, Jing, and Budzikowska 2000) 403 Computational Linguistics Volume 28, Number 4 show how the same summaries receive different scores under different measures or when compared to different (but presumably equivalent) ideal summaries created by humans. With regard to interhuman agreement, Jing et al. find fairly high consistency in the news genre only when the summary (extract) length is fixed relatively short. Marcu (1997a) provides some evidence that other genres will deliver less consistency. With regard to the lengths of the summaries produced by humans when not constrained by a particular compression rate, both Jing and Marcu find great variation. Nonetheless, it is now generally accepted that for single news articles, systems produce generic summaries indistinguishable from those of humans. Automated summary evaluation is a gleam in everyone’s eye. Clearly, when an ideal extract has been created by human(s), extractive summaries are easy to evaluate. Marcu (1999) and Goldstein et al. (1999) independently </context>
</contexts>
<marker>Marcu, 1997</marker>
<rawString>Marcu, Daniel. 1997a. From discourse structures to text summaries. In Proceedings of the ACL’97/EACL’97 Workshop on Intelligent Scalable Text Summarization, Madrid, July 11, pages 82–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The Rhetorical Parsing, Summarization, and Generation of Natural Language Texts.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Toronto,</institution>
<location>Toronto.</location>
<contexts>
<context position="7525" citStr="Marcu 1997" startWordPosition="1122" endWordPosition="1123">y the number of shared words, synonyms, or anaphora (e.g., Salton et al. 1997; Mani and Bloedorn 1997; Barzilay and Elhadad 1999). Other research rewards passages that include topic words, that is, words that have been determined to correlate well with the topic of interest to the user (for topic-oriented summaries) or with the general theme of the source text (Buckley and Cardie 1997; Strzalkowski et al. 1999; Radev, Jing, and Budzikowska 2000). Alternatively, a summarizer may reward passages that occupy important positions in the discourse structure of the text (Ono, Sumita, and Miike 1994; Marcu 1997b). This method requires a system to compute discourse structure reliably, which is not possible in all genres. This technique is the focus of one of the articles in this special issue (Teufel and Moens 2002), which shows how particular types of rhetorical relations in the genre of scientific journal articles can be reliably identified through the use of classification. An open-source summarization environment, MEAD, was recently developed at the Johns Hopkins summer workshop (Radev et al. 2002). MEAD allows researchers to experiment with different features and methods for combination. Some re</context>
<context position="18944" citStr="Marcu (1997" startWordPosition="2831" endWordPosition="2832">he main thrust of the source text from reading just the summary?). But the interpretation of the results is not simple; studies (Jing et al. 1998; Donaway, Drummey, and Mather 2000; Radev, Jing, and Budzikowska 2000) 403 Computational Linguistics Volume 28, Number 4 show how the same summaries receive different scores under different measures or when compared to different (but presumably equivalent) ideal summaries created by humans. With regard to interhuman agreement, Jing et al. find fairly high consistency in the news genre only when the summary (extract) length is fixed relatively short. Marcu (1997a) provides some evidence that other genres will deliver less consistency. With regard to the lengths of the summaries produced by humans when not constrained by a particular compression rate, both Jing and Marcu find great variation. Nonetheless, it is now generally accepted that for single news articles, systems produce generic summaries indistinguishable from those of humans. Automated summary evaluation is a gleam in everyone’s eye. Clearly, when an ideal extract has been created by human(s), extractive summaries are easy to evaluate. Marcu (1999) and Goldstein et al. (1999) independently </context>
</contexts>
<marker>Marcu, 1997</marker>
<rawString>Marcu, Daniel. 1997b. The Rhetorical Parsing, Summarization, and Generation of Natural Language Texts. Ph.D. thesis, University of Toronto, Toronto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The automatic construction of large-scale corpora for summarization research.</title>
<date>1999</date>
<booktitle>Proceedings of SIGIR’99: 22nd International Conference on Research and Development in Information Retrieval,</booktitle>
<pages>137--144</pages>
<editor>In M. Hearst, F. Gey, and R. Tong, editors,</editor>
<institution>University of California, Berkeley,</institution>
<contexts>
<context position="19501" citStr="Marcu (1999)" startWordPosition="2916" endWordPosition="2917">(extract) length is fixed relatively short. Marcu (1997a) provides some evidence that other genres will deliver less consistency. With regard to the lengths of the summaries produced by humans when not constrained by a particular compression rate, both Jing and Marcu find great variation. Nonetheless, it is now generally accepted that for single news articles, systems produce generic summaries indistinguishable from those of humans. Automated summary evaluation is a gleam in everyone’s eye. Clearly, when an ideal extract has been created by human(s), extractive summaries are easy to evaluate. Marcu (1999) and Goldstein et al. (1999) independently developed an automated method to create extracts corresponding to abstracts. But when the number of available extracts is not sufficient, it is not clear how to overcome the problems of low interhuman agreement. Simply using a variant of the Bilingual Evaluation Understudy (BLEU) scoring method (based on a linear combination of matching n-grams between the system output and the ideal summary) developed for machine translation (Papineni et al. 2001) is promising but not sufficient (Lin and Hovy 2002b). 3. The Articles in this Issue The articles in this</context>
</contexts>
<marker>Marcu, 1999</marker>
<rawString>Marcu, Daniel. 1999. The automatic construction of large-scale corpora for summarization research. In M. Hearst, F. Gey, and R. Tong, editors, Proceedings of SIGIR’99: 22nd International Conference on Research and Development in Information Retrieval, University of California, Berkeley, August, pages 137–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Laurie Gerber</author>
</authors>
<title>An inquiry into the nature of multidocument abstracts, extracts, and their evaluation.</title>
<date>2001</date>
<booktitle>In Proceedings of the NAACL-2001 Workshop on Automatic Summarization,</booktitle>
<pages>1--8</pages>
<publisher>NAACL,</publisher>
<location>Pittsburgh,</location>
<contexts>
<context position="13125" citStr="Marcu and Gerber 2001" startWordPosition="1955" endWordPosition="1958">tracted text to improve summary generation (Radev and McKeown 1998). Important differences (e.g., updates, trends, direct contradictions) are identified through a set of discourse rules. Recent work also follows this approach, using enhanced information extraction and additional forms of contrasts (White and Cardie 2002). To identify redundancy in text documents, various similarity measures are used. A common approach is to measure similarity between all pairs of sentences and then use clustering to identify themes of common information (McKeown et al. 1999; Radev, Jing, and Budzikowska 2000; Marcu and Gerber 2001). Alternatively, systems measure the similarity of a candidate passage to that of already-selected passages and retain it only if it contains enough new (dissimilar) information. A popular such measure is maximal marginal relevance (MMR) (Carbonell, Geng, and Goldstein 1997; Carbonell and Goldstein 1998). Once similar passages in the input documents have been identified, the information they contain must be included in the summary. Rather than simply listing all similar sentences (a lengthy solution), some approaches will select a representative passage to convey information in each cluster (R</context>
</contexts>
<marker>Marcu, Gerber, 2001</marker>
<rawString>Marcu, Daniel and Laurie Gerber. 2001. An inquiry into the nature of multidocument abstracts, extracts, and their evaluation. In Proceedings of the NAACL-2001 Workshop on Automatic Summarization, Pittsburgh, June. NAACL, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen McKeown</author>
<author>Judith Klavans</author>
<author>Vasileios Hatzivassiloglou</author>
<author>Regina Barzilay</author>
<author>Eleazar Eskin</author>
</authors>
<title>Towards multidocument summarization by reformulation: Progress and prospects.</title>
<date>1999</date>
<booktitle>In Proceedings of the 16th National Conference of the American Association for Artificial Intelligence (AAAI-1999),</booktitle>
<pages>18--22</pages>
<contexts>
<context position="13066" citStr="McKeown et al. 1999" startWordPosition="1946" endWordPosition="1949">nformation extraction approaches with regeneration of extracted text to improve summary generation (Radev and McKeown 1998). Important differences (e.g., updates, trends, direct contradictions) are identified through a set of discourse rules. Recent work also follows this approach, using enhanced information extraction and additional forms of contrasts (White and Cardie 2002). To identify redundancy in text documents, various similarity measures are used. A common approach is to measure similarity between all pairs of sentences and then use clustering to identify themes of common information (McKeown et al. 1999; Radev, Jing, and Budzikowska 2000; Marcu and Gerber 2001). Alternatively, systems measure the similarity of a candidate passage to that of already-selected passages and retain it only if it contains enough new (dissimilar) information. A popular such measure is maximal marginal relevance (MMR) (Carbonell, Geng, and Goldstein 1997; Carbonell and Goldstein 1998). Once similar passages in the input documents have been identified, the information they contain must be included in the summary. Rather than simply listing all similar sentences (a lengthy solution), some approaches will select a repr</context>
</contexts>
<marker>McKeown, Klavans, Hatzivassiloglou, Barzilay, Eskin, 1999</marker>
<rawString>McKeown, Kathleen, Judith Klavans, Vasileios Hatzivassiloglou, Regina Barzilay, and Eleazar Eskin. 1999. Towards multidocument summarization by reformulation: Progress and prospects. In Proceedings of the 16th National Conference of the American Association for Artificial Intelligence (AAAI-1999), 18–22 July, pages 453–460.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R McKeown</author>
<author>Dragomir R Radev</author>
</authors>
<title>Generating summaries of multiple news articles.</title>
<date>1995</date>
<booktitle>In Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>74--82</pages>
<location>Seattle,</location>
<contexts>
<context position="12168" citStr="McKeown and Radev 1995" startWordPosition="1817" endWordPosition="1820"> variant). 2.3 Multidocument Summarization Multidocument summarization, the process of producing a single summary of a set of related source documents, is relatively new. The three major problems introduced by having to handle multiple input documents are (1) recognizing and coping with redundancy, (2) identifying important differences among documents, and (3) ensuring summary coherence, even when material stems from different source documents. In an early approach to multidocument summarization, information extraction was used to facilitate the identification of similarities and differences (McKeown and Radev 1995). As for single-document summarization, this approach produces more of a briefing than a summary, as it contains only preidentified information types. Identity of slot values are used to determine when information is reliable enough to include in the summary. Later work merged information extraction approaches with regeneration of extracted text to improve summary generation (Radev and McKeown 1998). Important differences (e.g., updates, trends, direct contradictions) are identified through a set of discourse rules. Recent work also follows this approach, using enhanced information extraction </context>
</contexts>
<marker>McKeown, Radev, 1995</marker>
<rawString>McKeown, Kathleen R. and Dragomir R. Radev. 1995. Generating summaries of multiple news articles. In Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Seattle, July, pages 74–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ono</author>
<author>K Sumita</author>
<author>S Miike</author>
</authors>
<title>Abstract generation based on rhetorical structure extraction.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics, Kyoto, Japan,</booktitle>
<pages>344--348</pages>
<marker>Ono, Sumita, Miike, 1994</marker>
<rawString>Ono, K., K. Sumita, and S. Miike. 1994. Abstract generation based on rhetorical structure extraction. In Proceedings of the International Conference on Computational Linguistics, Kyoto, Japan, pages 344–348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jahna Otterbacher</author>
<author>Dragomir R Radev</author>
<author>Airong Luo</author>
</authors>
<title>Revisions that improve cohesion in multi-document summaries: A preliminary study.</title>
<date>2002</date>
<booktitle>In ACL Workshop on Text Summarization,</booktitle>
<location>Philadelphia.</location>
<marker>Otterbacher, Radev, Luo, 2002</marker>
<rawString>Otterbacher, Jahna, Dragomir R. Radev, and Airong Luo. 2002. Revisions that improve cohesion in multi-document summaries: A preliminary study. In ACL Workshop on Text Summarization, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2001</date>
<tech>Research Report RC22176, IBM.</tech>
<contexts>
<context position="19996" citStr="Papineni et al. 2001" startWordPosition="2990" endWordPosition="2994">ne’s eye. Clearly, when an ideal extract has been created by human(s), extractive summaries are easy to evaluate. Marcu (1999) and Goldstein et al. (1999) independently developed an automated method to create extracts corresponding to abstracts. But when the number of available extracts is not sufficient, it is not clear how to overcome the problems of low interhuman agreement. Simply using a variant of the Bilingual Evaluation Understudy (BLEU) scoring method (based on a linear combination of matching n-grams between the system output and the ideal summary) developed for machine translation (Papineni et al. 2001) is promising but not sufficient (Lin and Hovy 2002b). 3. The Articles in this Issue The articles in this issue move beyond the current state of the art in various ways. Whereas most research to date has focused on the use of sentence extraction for summarization, we are beginning to see techniques that allow a system to extract, merge, and edit phrases, as opposed to full sentences, to generate a summary. Whereas many summarization systems are designed for summarization of news, new algorithms are summarizing much longer and more complex documents, such as scientific journal articles, medical</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Papineni, K., S. Roukos, T. Ward, and W-J. Zhu. 2001. BLEU: A method for automatic evaluation of machine translation. Research Report RC22176, IBM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir Radev</author>
<author>Simone Teufel</author>
<author>Horacio Saggion</author>
<author>Wai Lam</author>
<author>John Blitzer</author>
<author>Arda ¸Celebi</author>
<author>Hong Qi</author>
<author>Elliott Drabek</author>
<author>Danyu Liu</author>
</authors>
<title>Evaluation of text summarization in a cross-lingual information retrieval framework.</title>
<date>2002</date>
<tech>Technical Report,</tech>
<institution>Center for Language and Speech Processing, Johns Hopkins University,</institution>
<location>Baltimore,</location>
<marker>Radev, Teufel, Saggion, Lam, Blitzer, ¸Celebi, Qi, Drabek, Liu, 2002</marker>
<rawString>Radev, Dragomir, Simone Teufel, Horacio Saggion, Wai Lam, John Blitzer, Arda ¸Celebi, Hong Qi, Elliott Drabek, and Danyu Liu. 2002. Evaluation of text summarization in a cross-lingual information retrieval framework. Technical Report, Center for Language and Speech Processing, Johns Hopkins University, Baltimore, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Hongyan Jing</author>
<author>Malgorzata Budzikowska</author>
</authors>
<title>Centroid-based summarization of multiple documents: Sentence extraction, utility-based evaluation, and user studies.</title>
<date>2000</date>
<booktitle>In ANLP/NAACL Workshop on Summarization,</booktitle>
<location>Seattle,</location>
<marker>Radev, Jing, Budzikowska, 2000</marker>
<rawString>Radev, Dragomir R., Hongyan Jing, and Malgorzata Budzikowska. 2000. Centroid-based summarization of multiple documents: Sentence extraction, utility-based evaluation, and user studies. In ANLP/NAACL Workshop on Summarization, Seattle, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Generating natural language summaries from multiple on-line sources.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>3</issue>
<contexts>
<context position="12570" citStr="Radev and McKeown 1998" startWordPosition="1875" endWordPosition="1878">aterial stems from different source documents. In an early approach to multidocument summarization, information extraction was used to facilitate the identification of similarities and differences (McKeown and Radev 1995). As for single-document summarization, this approach produces more of a briefing than a summary, as it contains only preidentified information types. Identity of slot values are used to determine when information is reliable enough to include in the summary. Later work merged information extraction approaches with regeneration of extracted text to improve summary generation (Radev and McKeown 1998). Important differences (e.g., updates, trends, direct contradictions) are identified through a set of discourse rules. Recent work also follows this approach, using enhanced information extraction and additional forms of contrasts (White and Cardie 2002). To identify redundancy in text documents, various similarity measures are used. A common approach is to measure similarity between all pairs of sentences and then use clustering to identify themes of common information (McKeown et al. 1999; Radev, Jing, and Budzikowska 2000; Marcu and Gerber 2001). Alternatively, systems measure the similari</context>
</contexts>
<marker>Radev, McKeown, 1998</marker>
<rawString>Radev, Dragomir R. and Kathleen R. McKeown. 1998. Generating natural language summaries from multiple on-line sources. Computational Linguistics, 24(3):469–500.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lisa Rau</author>
<author>Paul Jacobs</author>
</authors>
<title>Creating segmented databases from free text for text retrieval.</title>
<date>1991</date>
<booktitle>In Proceedings of the 14th Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>337--346</pages>
<location>New York,</location>
<contexts>
<context position="9302" citStr="Rau and Jacobs 1991" startWordPosition="1384" endWordPosition="1387"> and compression. Information extraction approaches can be characterized as “top-down,” since they look for a set of predefined information types to include in the summary (in contrast, extractive approaches are more data-driven). For each topic, the user predefines frames of expected information types, together with recognition criteria. For example, an earthquake frame may contain slots for location, earthquake magnitude, number of casualties, etc. The summarization engine must then locate the desired pieces of information, fill them in, and generate a summary with the results (DeJong 1978; Rau and Jacobs 1991). This method can produce high-quality and accurate summaries, albeit in restricted domains only. Compressive summarization results from approaching the problem from the point of view of language generation. Using the smallest units from the original document, Witbrock and Mittal (1999) extract a set of words from the input document and then order the words into sentences using a bigram language model. Jing and McKeown (1999) point out that human summaries are often constructed from the source document by a process of cutting and pasting document fragments that are then combined and regenerate</context>
</contexts>
<marker>Rau, Jacobs, 1991</marker>
<rawString>Rau, Lisa and Paul Jacobs. 1991. Creating segmented databases from free text for text retrieval. In Proceedings of the 14th Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, New York, pages 337–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Horacio Saggion</author>
<author>Guy Lapalme</author>
</authors>
<title>Generating indicative-informative summaries with SumUM.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>4</issue>
<pages>497--526</pages>
<contexts>
<context position="23966" citStr="Saggion and Lapalme (2002)" startWordPosition="3616" endWordPosition="3619"> summarization in which phrases, rather than sentences, are extracted from the original document. She shows that such an approach is often used by human abstractors. She then presents an automated tool that is used to analyze a corpus of paired documents and abstracts written by humans, in order to identify the phrases within the documents that are used in the abstracts. She has developed an HMM solution to the matching problem. The decomposition program is a tool that can produce training and testing corpora for summarization, and its results have been used for her own summarization program. Saggion and Lapalme (2002) describe a system, SumUM, that generates indicativeinformative summaries from technical documents. To build their system, Saggion and Lapalme have studied a corpus of professionally written (short) abstracts. They have manually aligned the abstracts and the original documents. Given the structured form of technical papers, most of the information in the abstracts was also found in either the author abstract (20%) or in the first section of the paper (40%) or the headlines or captions (23%). Based on their observations, the authors have developed an approach to summarization, called selective </context>
</contexts>
<marker>Saggion, Lapalme, 2002</marker>
<rawString>Saggion, Horacio and Guy Lapalme. 2002. Generating indicative-informative summaries with SumUM. Computational Linguistics, 28(4), 497–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>A Singhal</author>
<author>M Mitra</author>
<author>C Buckley</author>
</authors>
<title>Automatic text structuring and summarization.</title>
<date>1997</date>
<journal>Information Processing &amp; Management,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="6992" citStr="Salton et al. 1997" startWordPosition="1037" endWordPosition="1040">ntence position affects the selection of sentences, and Witbrock and Mittal (1999) used statistical approaches to choose important words and phrases and their syntactic context. 400 Radev, Hovy, and McKeown Summarization: Introduction Approaches involving more sophisticated natural language analysis to identify key passages rely on analysis either of word relatedness or of discourse structure. Some research uses the degree of lexical connectedness between potential passages and the remainder of the text; connectedness may be measured by the number of shared words, synonyms, or anaphora (e.g., Salton et al. 1997; Mani and Bloedorn 1997; Barzilay and Elhadad 1999). Other research rewards passages that include topic words, that is, words that have been determined to correlate well with the topic of interest to the user (for topic-oriented summaries) or with the general theme of the source text (Buckley and Cardie 1997; Strzalkowski et al. 1999; Radev, Jing, and Budzikowska 2000). Alternatively, a summarizer may reward passages that occupy important positions in the discourse structure of the text (Ono, Sumita, and Miike 1994; Marcu 1997b). This method requires a system to compute discourse structure re</context>
</contexts>
<marker>Salton, Singhal, Mitra, Buckley, 1997</marker>
<rawString>Salton, G., A. Singhal, M. Mitra, and C. Buckley. 1997. Automatic text structuring and summarization. Information Processing &amp; Management, 33(2):193–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Gregory Silber</author>
<author>Kathleen McCoy</author>
</authors>
<title>Efficiently computed lexical chains as an intermediate representation for automatic text summarization.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>4</issue>
<pages>487--496</pages>
<contexts>
<context position="21534" citStr="Silber and McCoy 2002" startWordPosition="3236" endWordPosition="3239">e also focus on tools that can aid in the process of developing summarization systems, on computational efficiency of algorithms, and on techniques needed for preprocessing speech. The four articles that focus on summarization of text share a common theme: Each views the summarization process as consisting of two phases. In the first, material within the original document that is important is identified and extracted. In the second, this extracted material may be modified, merged, and edited using generation techniques. Two of the articles focus on the extraction stage (Teufel and Moens 2002; Silber and McCoy 2002), whereas Jing (2002) examines tools for automatically constructing resources that can be used for the second stage. Teufel and Moens propose significantly different techniques for sentence extraction than have been used in the past. Noting the difference in both length and structure between scientific articles and news, they claim that both the context of sentences and a more focused search for sentences is needed in order to produce a good summary that is only 2.5% of the original document. Their approach is to provide a summary that focuses on the new contribution of the paper and its relat</context>
</contexts>
<marker>Silber, McCoy, 2002</marker>
<rawString>Silber, H. Gregory and Kathleen McCoy. 2002. Efficiently computed lexical chains as an intermediate representation for automatic text summarization. Computational Linguistics, 28(4), 487–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sparck Jones</author>
<author>Karen</author>
</authors>
<title>Automatic summarizing: Factors and directions.</title>
<date>1999</date>
<booktitle>Advances in Automatic Text Summarization.</booktitle>
<pages>1--13</pages>
<editor>In I. Mani and M. T. Maybury, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge,</location>
<marker>Jones, Karen, 1999</marker>
<rawString>Sparck Jones, Karen. 1999. Automatic summarizing: Factors and directions. In I. Mani and M. T. Maybury, editors, Advances in Automatic Text Summarization. MIT Press, Cambridge, pages 1–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomek Strzalkowski</author>
<author>Gees Stein</author>
<author>J Wang</author>
<author>Bowden Wise</author>
</authors>
<title>A robust practical text summarizer.</title>
<date>1999</date>
<booktitle>Advances in Automatic Text Summarization.</booktitle>
<pages>137--154</pages>
<editor>In I. Mani and M. T. Maybury, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge,</location>
<contexts>
<context position="7328" citStr="Strzalkowski et al. 1999" startWordPosition="1091" endWordPosition="1094"> on analysis either of word relatedness or of discourse structure. Some research uses the degree of lexical connectedness between potential passages and the remainder of the text; connectedness may be measured by the number of shared words, synonyms, or anaphora (e.g., Salton et al. 1997; Mani and Bloedorn 1997; Barzilay and Elhadad 1999). Other research rewards passages that include topic words, that is, words that have been determined to correlate well with the topic of interest to the user (for topic-oriented summaries) or with the general theme of the source text (Buckley and Cardie 1997; Strzalkowski et al. 1999; Radev, Jing, and Budzikowska 2000). Alternatively, a summarizer may reward passages that occupy important positions in the discourse structure of the text (Ono, Sumita, and Miike 1994; Marcu 1997b). This method requires a system to compute discourse structure reliably, which is not possible in all genres. This technique is the focus of one of the articles in this special issue (Teufel and Moens 2002), which shows how particular types of rhetorical relations in the genre of scientific journal articles can be reliably identified through the use of classification. An open-source summarization e</context>
</contexts>
<marker>Strzalkowski, Stein, Wang, Wise, 1999</marker>
<rawString>Strzalkowski, Tomek, Gees Stein, J. Wang, and Bowden Wise. 1999. A robust practical text summarizer. In I. Mani and M. T. Maybury, editors, Advances in Automatic Text Summarization. MIT Press, Cambridge, pages 137–154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Marc Moens</author>
</authors>
<title>Summarizing scientific articles: Experiments with relevance and rhetorical status.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>4</issue>
<pages>409--445</pages>
<contexts>
<context position="7733" citStr="Teufel and Moens 2002" startWordPosition="1155" endWordPosition="1158">, words that have been determined to correlate well with the topic of interest to the user (for topic-oriented summaries) or with the general theme of the source text (Buckley and Cardie 1997; Strzalkowski et al. 1999; Radev, Jing, and Budzikowska 2000). Alternatively, a summarizer may reward passages that occupy important positions in the discourse structure of the text (Ono, Sumita, and Miike 1994; Marcu 1997b). This method requires a system to compute discourse structure reliably, which is not possible in all genres. This technique is the focus of one of the articles in this special issue (Teufel and Moens 2002), which shows how particular types of rhetorical relations in the genre of scientific journal articles can be reliably identified through the use of classification. An open-source summarization environment, MEAD, was recently developed at the Johns Hopkins summer workshop (Radev et al. 2002). MEAD allows researchers to experiment with different features and methods for combination. Some recent work (Conroy and O’Leary 2001) has turned to the use of hidden Markov models (HMMs) and pivoted QR decomposition to reflect the fact that the probability of inclusion of a sentence in an extract depends </context>
<context position="21510" citStr="Teufel and Moens 2002" startWordPosition="3232" endWordPosition="3235">e articles in this issue also focus on tools that can aid in the process of developing summarization systems, on computational efficiency of algorithms, and on techniques needed for preprocessing speech. The four articles that focus on summarization of text share a common theme: Each views the summarization process as consisting of two phases. In the first, material within the original document that is important is identified and extracted. In the second, this extracted material may be modified, merged, and edited using generation techniques. Two of the articles focus on the extraction stage (Teufel and Moens 2002; Silber and McCoy 2002), whereas Jing (2002) examines tools for automatically constructing resources that can be used for the second stage. Teufel and Moens propose significantly different techniques for sentence extraction than have been used in the past. Noting the difference in both length and structure between scientific articles and news, they claim that both the context of sentences and a more focused search for sentences is needed in order to produce a good summary that is only 2.5% of the original document. Their approach is to provide a summary that focuses on the new contribution of</context>
</contexts>
<marker>Teufel, Moens, 2002</marker>
<rawString>Teufel, Simone and Marc Moens. 2002. Summarizing scientific articles: Experiments with relevance and rhetorical status. Computational Linguistics, 28(4), 409–445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
<author>Claire Cardie</author>
</authors>
<title>Selecting sentences for multidocument summaries using randomized local search.</title>
<date>2002</date>
<booktitle>In Proceedings of the Workshop on Automatic Summarization (including DUC 2002),</booktitle>
<pages>9--18</pages>
<location>Philadelphia,</location>
<contexts>
<context position="12825" citStr="White and Cardie 2002" startWordPosition="1909" endWordPosition="1912">ion, this approach produces more of a briefing than a summary, as it contains only preidentified information types. Identity of slot values are used to determine when information is reliable enough to include in the summary. Later work merged information extraction approaches with regeneration of extracted text to improve summary generation (Radev and McKeown 1998). Important differences (e.g., updates, trends, direct contradictions) are identified through a set of discourse rules. Recent work also follows this approach, using enhanced information extraction and additional forms of contrasts (White and Cardie 2002). To identify redundancy in text documents, various similarity measures are used. A common approach is to measure similarity between all pairs of sentences and then use clustering to identify themes of common information (McKeown et al. 1999; Radev, Jing, and Budzikowska 2000; Marcu and Gerber 2001). Alternatively, systems measure the similarity of a candidate passage to that of already-selected passages and retain it only if it contains enough new (dissimilar) information. A popular such measure is maximal marginal relevance (MMR) (Carbonell, Geng, and Goldstein 1997; Carbonell and Goldstein </context>
</contexts>
<marker>White, Cardie, 2002</marker>
<rawString>White, Michael and Claire Cardie. 2002. Selecting sentences for multidocument summaries using randomized local search. In Proceedings of the Workshop on Automatic Summarization (including DUC 2002), Philadelphia, July. Association for Computational Linguistics, New Brunswick, NJ, pages 9–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Witbrock</author>
<author>Vibhu Mittal</author>
</authors>
<title>Ultra-summarization: A statistical approach to generating highly condensed non-extractive summaries.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>315--316</pages>
<location>Berkeley,</location>
<contexts>
<context position="6456" citStr="Witbrock and Mittal (1999)" startWordPosition="960" endWordPosition="963">words rather than bags of words. The application of machine learning to summarization was pioneered by Kupiec, Pedersen, and Chen (1995), who developed a summarizer using a Bayesian classifier to combine features from a corpus of scientific articles and their abstracts. Aone et al. (1999) and Lin (1999) experimented with other forms of machine learning and its effectiveness. Machine learning has also been applied to learning individual features; for example, Lin and Hovy (1997) applied machine learning to the problem of determining how sentence position affects the selection of sentences, and Witbrock and Mittal (1999) used statistical approaches to choose important words and phrases and their syntactic context. 400 Radev, Hovy, and McKeown Summarization: Introduction Approaches involving more sophisticated natural language analysis to identify key passages rely on analysis either of word relatedness or of discourse structure. Some research uses the degree of lexical connectedness between potential passages and the remainder of the text; connectedness may be measured by the number of shared words, synonyms, or anaphora (e.g., Salton et al. 1997; Mani and Bloedorn 1997; Barzilay and Elhadad 1999). Other rese</context>
<context position="9589" citStr="Witbrock and Mittal (1999)" startWordPosition="1424" endWordPosition="1427">cted information types, together with recognition criteria. For example, an earthquake frame may contain slots for location, earthquake magnitude, number of casualties, etc. The summarization engine must then locate the desired pieces of information, fill them in, and generate a summary with the results (DeJong 1978; Rau and Jacobs 1991). This method can produce high-quality and accurate summaries, albeit in restricted domains only. Compressive summarization results from approaching the problem from the point of view of language generation. Using the smallest units from the original document, Witbrock and Mittal (1999) extract a set of words from the input document and then order the words into sentences using a bigram language model. Jing and McKeown (1999) point out that human summaries are often constructed from the source document by a process of cutting and pasting document fragments that are then combined and regenerated as summary sentences. Hence a summarizer can be developed to extract sentences, reduce them by dropping unimportant fragments, and then use information fusion and generation to combine the remaining fragments. In this special issue, Jing (2002) reports on automated techniques to build</context>
</contexts>
<marker>Witbrock, Mittal, 1999</marker>
<rawString>Witbrock, Michael and Vibhu Mittal. 1999. Ultra-summarization: A statistical approach to generating highly condensed non-extractive summaries. In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Berkeley, pages 315–316.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Zechner</author>
</authors>
<title>Automatic summarization of open-domain multiparty dialogues in diverse genres.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>4</issue>
<pages>447--485</pages>
<contexts>
<context position="24808" citStr="Zechner 2002" startWordPosition="3740" endWordPosition="3741">aligned the abstracts and the original documents. Given the structured form of technical papers, most of the information in the abstracts was also found in either the author abstract (20%) or in the first section of the paper (40%) or the headlines or captions (23%). Based on their observations, the authors have developed an approach to summarization, called selective analysis, which mimics the human abstractors’ routine. The four components of selective analysis are indicative selection, informative selection, indicative generation, and informative generation. The final article in the issue (Zechner 2002) is distinct from the other articles in that it addresses problems in summarization of speech. As in text summarization, Zechner also uses sentence extraction to determine the content of the summary. Given the informal nature of speech, however, a number of significant steps must be taken in order to identify useful segments for extraction. Zechner develops techniques for removing disfluencies from speech, for identifying units for extraction that are in some sense equivalent to sentences, and for identifying relations such as questionanswer across turns in order to determine when units from t</context>
</contexts>
<marker>Zechner, 2002</marker>
<rawString>Zechner, Klaus. 2002. Automatic summarization of open-domain multiparty dialogues in diverse genres. Computational Linguistics, 28(4), 447–485.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>