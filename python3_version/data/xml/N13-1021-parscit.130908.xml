<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000457">
<title confidence="0.999365">
Discriminative Joint Modeling of Lexical Variation and Acoustic Confusion
for Automated Narrative Retelling Assessment
</title>
<author confidence="0.994541">
Maider Lehr†, Izhak Shafran†, Emily Prud’hommeaux&apos; and Brian Roark†
</author>
<affiliation confidence="0.8881875">
†Center for Spoken Language Understanding, Oregon Health &amp; Science University
&apos;Center for Language Sciences, University of Rochester
</affiliation>
<email confidence="0.992888">
imaiderlehr,zakshafran,emilpx,roarkbrl@gmail.com
</email>
<sectionHeader confidence="0.995524" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999853416666666">
Automatically assessing the fidelity of a
retelling to the original narrative – a task of
growing clinical importance – is challenging,
given extensive paraphrasing during retelling
along with cascading automatic speech recog-
nition (ASR) errors. We present a word tag-
ging approach using conditional random fields
(CRFs) that allows a diversity of features
to be considered during inference, including
some capturing acoustic confusions encoded
in word confusion networks. We evaluate the
approach under several scenarios, including
both supervised and unsupervised training, the
latter achieved by training on the output of
a baseline automatic word-alignment model.
We also adapt the ASR models to the domain,
and evaluate the impact of error rate on per-
formance. We find strong robustness to ASR
errors, even using just the 1-best system out-
put. A hybrid approach making use of both au-
tomatic alignment and CRFs trained tagging
models achieves the best performance, yield-
ing strong improvements over using either ap-
proach alone.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999906209302325">
Narrative production tasks are an essential compo-
nent of many standard neuropsychological test bat-
teries. For example, narration of a wordless pic-
ture book is part of the Autism Diagnostic Obser-
vation Schedule (ADOS) (Lord et al., 2002) and
retelling of previously narrated stories is part of both
the Developmental Neuropsychological Assessment
(NEPSY) (Korkman et al., 1998) and the Wech-
sler Logical Memory (WLM) test (Wechsler, 1997).
Such tests also arise in reading comprehension, sec-
ond language learning and other computer-based tu-
toring systems (Xie et al., 2012; Zhang et al., 2008).
The accuracy of automated scoring of a narrative
retelling depends on correctly identifying which of
the source narrative’s propositions or events (what
we will call ‘story elements’) have been included
in the retelling. Speakers may choose to relate
these elements using diverse words or phrases, and
an automated method of identifying these elements
needs to model the permissible variants and para-
phrasings. In previous work (Lehr et al., 2012;
Prud’hommeaux and Roark, 2012; Prud’hommeaux
and Roark, 2011), we developed models based on
automatic word-alignment methods, as described
briefly in Section 3. Such alignments are learned
in an unsupervised manner from a parallel corpus of
manual or ASR transcripts of retellings and the orig-
inal source narrative, much as in machine translation
training.
Relying on manual transcripts to train the align-
ment models limits the ability of these methods to
handle ASR errors. By instead training on ASR
transcripts, these methods can automatically capture
some regularities of lexical variants and their com-
mon realizations by the recognizer. Additionally, ev-
idence of acoustic confusability is available in word
lattice output from the recognizer, which can be ex-
ploited to yield more robust automatic scoring, par-
ticularly in high error-rate scenarios.
In this paper, we present and evaluate the use of
word tagging models for this task, in contrast to
just using automatic (unsupervised) word-alignment
methods. The approach is general enough to al-
</bodyText>
<page confidence="0.982993">
211
</page>
<note confidence="0.471863">
Proceedings of NAACL-HLT 2013, pages 211–220,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999510214285714">
low tagging of word confusion networks derived
from lattices, thus allowing us to explore the utility
of such representations to achieve robustness. We
present results under a range of experimental condi-
tions, including: variously adapting the ASR mod-
els to the domain; using maximum entropy models
rather than CRFs; differing tagsets (BIO versus IO);
and with varying degrees of supervision. Finally,
we demonstrate improved utility in terms of using
the automatic scores to classify elderly individuals
as having Mild Cognitive Impairment. Ultimately
we find that hybrid approaches, making use of both
word-alignment and tagging models, yield strong
improvements over either used independently.
</bodyText>
<sectionHeader confidence="0.774257" genericHeader="method">
2 Wechsler Logical Memory (WLM) task
</sectionHeader>
<bodyText confidence="0.997035945945946">
The Wechsler Logical Memory (WLM) task (Wech-
sler, 1997), a widely used subtest of a battery of neu-
ropsychological tests used to assess memory func-
tion in adults, has been shown to be a good indicator
of Mild Cognitive Impairment (MCI) (Storandt and
Hill, 1989; Petersen et al., 1999; Wang and Zhou,
2002; Nordlund et al., 2005), the stage of cogni-
tive decline that is often a precursor to dementia of
the Alzheimer’s type. In the WLM, the subject lis-
tens to the examiner read a brief narrative and then
retells the narrative twice: immediately upon hear-
ing it and after about 20 minutes. The examiner
grades the subject’s response by counting how many
of the story elements the subject recalled.
An excerpt of the text read by the clinician while
administering the WLM task is shown in Figure 1.
The story elements in the text are delineated using
slashes, 25 elements in all. An example retelling
is shown in Figure 2 to illustrate how the retellings
are scored. The clinical evaluation guidelines spec-
ify what lexical substitutions, if any, are allowed
for each element. Some elements, such as cafeteria
and Thompson, must be recalled verbatim. In other
cases, subjects are given credit for variants, such as
Annie for Anna, or paraphrasing of concepts such as
sympathetic for touched by the woman’s story. The
example retelling received a score of 12, with one
point for each of the recalled story elements: Anna,
Boston, employed, as a cook, and robbed of, she had
four, small children, reported, station, touched by
the woman’s story, took up a collection and for her.
Anna / Thompson / of South / Boston / em-
ployed / as a cook / in a school / cafeteria /
reported / at the police / station / that she had
been held up / on State Street / the night be-
fore / and robbed / ... / police / touched by the
woman’s story / took up a collection / for her.
</bodyText>
<figureCaption confidence="0.98224825">
Figure 1: Reference text and the set of story elements.
Ann Taylor worked in Boston as a cook. And
she was robbed of sixty-seven dollars. Is
that right? And she had four children and
reported at the some kind of station. The fel-
low sympathetic and made a collection for her
so that she can feed the children.
Figure 2: An example retelling with 12 recalled story elements.
</figureCaption>
<sectionHeader confidence="0.731254" genericHeader="method">
3 Unsupervised generative automated
scoring with word alignment
</sectionHeader>
<bodyText confidence="0.999978655172414">
In previous work (Lehr et al., 2012; Prud’hommeaux
and Roark, 2012; Prud’hommeaux and Roark,
2011), we developed a pipeline for automatically
scoring narrative retellings for the WLM task. The
utterances corresponding to a retelling were rec-
ognized using an ASR system. The story ele-
ments were identified from the 1-best ASR transcript
using word alignments produced by the Berkeley
aligner (Liang et al., 2006), an EM-based word
alignment package developed to align parallel texts
for machine translation. The word alignment model
was estimated in an unsupervised manner from a
parallel corpus consisting of source narrative and
manual transcripts of retellings from a small set of
training subjects, and from a pairwise parallel cor-
pus of manual retelling transcripts.
During inference or test, the ASR transcripts of
the retellings were aligned using the estimated align-
ment model to the source narrative text. If a word
in the retelling was mapped by the alignment model
to a content word in the source narrative, the ele-
ment associated with that content word was counted
as correctly recalled in that retelling. Recall that
the models were trained on unsupervised data so the
aligned words may not always be permissible vari-
ants of the target elements. To alleviate such extra-
neous as well as unaligned words, the alignments
below a threshold of posterior probability are dis-
carded while decoding.
</bodyText>
<page confidence="0.998228">
212
</page>
<sectionHeader confidence="0.664638" genericHeader="method">
4 Supervised discriminative automated
</sectionHeader>
<subsectionHeader confidence="0.421529">
scoring with log-linear models
</subsectionHeader>
<bodyText confidence="0.999950068965518">
In this work, we frame the task of detecting story
elements as a tagging task. Thus, our problem re-
duces to assigning a tag to each word position in the
retelling, the tag indicating the story element that the
word is associated with. In its simplest form, we
have 26 tags: one for each of the 25 story elements
indicating the word is ‘in’ that element (e.g., I15);
and one for ‘outside’ of any story element (‘O’). By
tagging word positions, we are framing the problem
in a general enough way to allow tagging of word
confusion networks (Mangu et al., 2000), which en-
code word confusions that may provide additional
robustness, particularly in high word-error rate sce-
narios. We make use of log-linear models, which
have been used for tagging confusion networks (Ku-
rata et al., 2012), and which allow very flexible fea-
ture vector definition and discriminative optimiza-
tion.
The model allows us to experiment with three
types of inputs as illustrated in the Figure 3 – the
manual transcript, the 1-best ASR transcript, and the
word confusion network. To create supervised train-
ing data, we force-align ASR transcripts to manual
transcripts and transfer manually annotated story el-
ement tags from the reference transcripts to word po-
sitions in the confusion network or 1-best ASR out-
put using the word-level time marks. Our unsuper-
vised training scenario instead derives story element
tags from a baseline word-alignment based model.
</bodyText>
<figureCaption confidence="0.9948955">
Figure 3: Feature vectors at each word position includes lexi-
cal variants and acoustic confusions.
</figureCaption>
<table confidence="0.602135833333333">
Markov order 0 Markov order 1
(MaxEnt) (CRF)
Context yi yi−1yi
independent (CI) yixi yi−1yixi
Context yixi−1 yi−1yixi−1
dependent (CD) yixi+1 yi−1yixi+1
</table>
<tableCaption confidence="0.983529333333333">
Table 1: Feature templates either using or not using neighbor-
ing tag yi−1 (MaxEnt vs. CRF); and for using or not using
neighboring words xi−1, xi+1 (CI vs. CD).
</tableCaption>
<subsectionHeader confidence="0.948315">
4.1 Features
</subsectionHeader>
<bodyText confidence="0.999987891891892">
Given a sequence of word positions x = x1 ... xn,
the tagger assigns a sequence of labels y = y1 ... yn
from a tag lexicon. For each word xi in the se-
quence, we can define features in the log-linear
model based on word and tag identities. Table 1
presents several sets of features, defined over words
and tags at various positions relative to the current
word xi and tag yi and compound features are de-
noted as concatenated symbols.
Features that rely only on the current tag yi are
used in a Markov order 0 model, i.e., one for which
each tag is labeled independently. A maximum en-
tropy classifier (see Section 4.2) is used with these
feature sets. Features that include prior tags en-
code dependencies between adjacent tags, and are
used within conditional random fields models (see
Section 4.3). To examine the utility of surrounding
words xi−1 and xi+1, we distinguish between mod-
els trained with context independent features (just
xi) and context dependent features. Note that mod-
els including context dependent feature sets also in-
clude the context independent features, and Markov
order 1 models also include Markov order 0 features.
Two other details about our use of the feature tem-
plates are worth noting. First, when tagging confu-
sion networks, each word in the network at position
i results in a feature instance. Thus, if there are five
confusable words at position i, then there will be
five different xi values being used to instantiate the
features in Table 1. Second, following Kurata et al.
(2012), we multiply the feature counts for the con-
text dependent features by a weight to control their
influence on the model. In this paper, the scaling
weight of the context-dependent features was 0.3.
We investigate two different tagsets for this task,
as presented in Table 2. The simpler tagset (IO) sim-
ply identifies words that are in a story element; the
</bodyText>
<page confidence="0.994946">
213
</page>
<bodyText confidence="0.857242">
Tagging anna rent was due
</bodyText>
<equation confidence="0.6246305">
IO-tags I1 I19 I19 I19
BIO-tags B1 B19 I19 I19
</equation>
<tableCaption confidence="0.951759">
Table 2: Two possible tagsets for labeling.
</tableCaption>
<bodyText confidence="0.988592">
larger tagset (BIO) differentiates among positions in
a story element chunk. The latter tagset is only of
utility for models with Markov order greater than
zero, and hence are only used with CRF models.
</bodyText>
<subsectionHeader confidence="0.998054">
4.2 MaxEnt-based multiclass classifier
</subsectionHeader>
<bodyText confidence="0.999927428571429">
Our baseline model is a Maximum Entropy (Max-
Ent) classifier where each position i from the
retelling x gets assigned one of the IO output tags
yi corresponding to the set of 25 story elements and
a null (‘O’) symbol. The output tag is modeled as
the conditional probability p(yi  |xi) given the word
xi at position i in the retelling.
</bodyText>
<equation confidence="0.987032">
IAkOk(xi, yi)
Z(xi)
</equation>
<bodyText confidence="0.999876714285714">
where Z(xi) is a normalization factor. The feature
functions O(xi, yi) are the Markov order 0 features
as defined as in the previous section. The parame-
ters A E Rd are estimated by optimizing the above
conditional probability, with L2 regularization. We
use the MALLET Toolkit (McCallum, 2002) with
default regularization parameters.
</bodyText>
<subsectionHeader confidence="0.987593">
4.3 CRF-based sequence labeling model
</subsectionHeader>
<bodyText confidence="0.999828538461539">
The MaxEnt models assign a tag to each position
from the input retelling independently. However,
there are a few reasons why reframing the task as
a sequence modeling problem may improve tagging
performance. First, some of the story elements are
multiword sequences, such as she had been held up
or on State Street. Second, even if a retelling orders
recalled elements differently than the original narra-
tive, there is a tendency for story elements to occur
in certain orders.
The parameters of the CRF model, A E Rd are
estimated by optimizing the following conditional
probability:
</bodyText>
<equation confidence="0.987832">
IAkOk(x, y)
Z(x)
</equation>
<bodyText confidence="0.999992714285714">
where 4b(x, y) aggregates features across the entire
sequence, and Z(x) is a global normalization con-
stant over the sequence, rather than local for a partic-
ular position as with MaxEnt. Features for the CRF
model are Markov order 1 features, and as with the
MaxEnt training, we use default (L2) regularization
parameters within the MALLET toolkit.
</bodyText>
<subsectionHeader confidence="0.61669">
5 Combining tagging and alignment
</subsectionHeader>
<bodyText confidence="0.9999952">
This paper contrasts a discriminatively trained tag-
ging approach with an unsupervised alignment-
based approach, but there are several ways in which
the two approaches can be combined. First, the
alignment model is unsupervised and can provide
its output as training data to the tagging approach,
resulting in an unsupervised discriminative model.
Second, the alignment model can provide features to
the log-linear tagging model in the supervised condi-
tion. We explore both methods of combination here.
</bodyText>
<subsectionHeader confidence="0.969045">
5.1 Unsupervised discriminative tagger
</subsectionHeader>
<bodyText confidence="0.999990227272727">
The tagging task based on log-linear models pro-
vides an appropriate framework to easily incorpo-
rate diverse features and discriminatively estimate
the parameters of the model. However, this ap-
proach requires supervised tagged training data, in
this case manual labels indicating the correspon-
dence of phrases in the retellings with story elements
in the original narrative. These manual annotations
are used to derive sequences of story element tags
labeling the words of the retelling. Manually la-
beling the retellings is costly, and the scoring (thus
labeling) scheme is very specific to the test being
analyzed. To avoid manual labeling and provide a
general framework that can easily be adopted in any
retelling based assessment task, we experiment here
with an unsupervised discriminative approach.
In this unsupervised approach, the labeled train-
ing data required by the log-linear model is provided
by the automatic word alignments trained without
supervision. The resulting tag sequences replace the
manual tag sequences used in the standard super-
vised approach.
</bodyText>
<subsectionHeader confidence="0.99916">
5.2 Word-alignment derived features
</subsectionHeader>
<bodyText confidence="0.999960333333333">
When training discriminative models it is a common
practice to incorporate into the feature space the out-
put from a generative model, since it is a good esti-
</bodyText>
<equation confidence="0.8882108">
exp
p(yi  |xi) _
d
E
k��
exp
P(y  |x) _
d
E
k��
</equation>
<page confidence="0.994904">
214
</page>
<bodyText confidence="0.998804545454545">
mator. Here we augment the feature space of the
log-linear models with the tags generated by the au-
tomatic word alignments. In addition to the features
defined in Section 4.1, we include new features that
match predicted labels zi from the word-alignment
model with possible labels in the tagger yi. Our fea-
tures include the current tagger label with (1) the
current predicted word-alignment label; (2) the pre-
vious predicted label; and (3) the next predicted la-
bel. Thus, the new features were yizi, yizi−1 and
yizi+1.
</bodyText>
<sectionHeader confidence="0.997848" genericHeader="method">
6 Experimental evaluations
</sectionHeader>
<bodyText confidence="0.999902956521739">
Corpus: Our models were trained on immediate and
delayed retellings from 144 subjects with a mean
age of 85.4, of whom 36 were clinically diagnosed
with MCI (training set). We evaluated our models
on a set of retellings from 70 non-overlapping sub-
jects with a mean age of 88.5, half of whom had
received a diagnosis of MCI (test set). In contrast
to the unsupervised word-alignment based method,
the method outlined here required manual story el-
ement labels of the retellings. The training and
test sets from this paper are therefore different from
the sets used in previous work (Lehr et al., 2012;
Prud’hommeaux and Roark, 2012; Prud’hommeaux
and Roark, 2011), and the results are not directly
comparable.
The recordings were sometimes made in an infor-
mal setting, such as the subject’s home or a senior
center. For this reason, there are often extraneous
noises in the recordings such as music, footsteps,
and clocks striking the hour. Although this presents
a challenge for ASR, part of the goal of our work
is to demonstrate the robustness of our methods to
noisy audio.
</bodyText>
<subsectionHeader confidence="0.996922">
6.1 Automatic transcription
</subsectionHeader>
<bodyText confidence="0.999865666666667">
The baseline ASR system used in the current work
is a Broadcast News system which is modeled af-
ter Kingsbury et al. (2011). Briefly, the acoustics
of speech are modeled by 4000 clustered allophone
states defined over a pentaphone context, where
states are represented by Gaussian mixture models
with a total of 150K mixture components. The ob-
servation vectors consist of PLP features, stacked
from 10 neighboring frames and projected to a 50-
</bodyText>
<table confidence="0.999828">
System 1-best oracle oracle
(%) WCN(%) lat(%)
Baseline 47.2 39.7 27.7
AM adaptation 38.2 35.5 21.2
LM adaptation 28.3 30.7 19.9
AM+LM adaptation 25.6 26.5 16.5
</table>
<tableCaption confidence="0.993479">
Table 3: Improvement in ASR word error-rate by adapting the
Broadcast News models to the domain of narrative retelling.
</tableCaption>
<bodyText confidence="0.999295076923077">
dimension space using linear discriminant analysis
(LDA). The acoustic models were trained on 430
hours of transcribed speech from Broadcast News
corpus (LDC97S44, LDC98S71). The language
model is defined over an 84K vocabulary and con-
sists of about 1.8M, 1M and and 331K bigrams, tri-
grams and 4-grams, estimated from standard Broad-
cast news corpus. The decoding is performed in sev-
eral stages using successively refined acoustic mod-
els – a context-dependent model, a vocal-tract nor-
malized model, a speaker-adapted maximum likeli-
hood linear regression (MLLR) model, and finally
a discriminatively trained model with the boosted
MMI criteria (Povey et al., 2008). The system gives
a word error rate of 13.1% on the 2004 Rich Tran-
scription benchmark by NIST (Fiscus et al., 2007),
which is comparable to state-of-the-art for equiva-
lent amounts of acoustic training data. On the WLM
corpus, the recognition word error rate was signifi-
cantly higher at 47.2% due to a mismatch in domain
and the skewed demographics (age) of the speakers.
We improved the performance of the above
Broadcast News models by adapting to the domain
of the WLM retellings. The acoustic models were
adapted using standard MLLR, where linear trans-
forms were estimated in an unsupervised manner
to maximize the likelihood over the transcripts of
the retellings. The transcripts were generated from
the baseline system after the final stage of decod-
ing with the discriminative model. The language
models were adapted by interpolating the in-domain
model (weight=0.7) with the out-of-domain model.
The gains from these adaptations are reported in
the Table 3. As expected, we find substantial gains
from both acoustic model (AM) and language model
(LM) adaptation. Furthermore, we find benefit in
employing them simultaneously. We also include
the oracle word error rate (WER) of the WCNs and
lattices for each ASR configuration.
</bodyText>
<page confidence="0.995005">
215
</page>
<bodyText confidence="0.9999465">
One thing to note is that the oracle WER of the
WCNs is worse than the 1-best WER when adapting
the language models. We speculate that this is due
to bias introduced by the language model adapted
to the story retellings, resulting in word candidates
in the bins that are not truly acoustically confusable
candidates. This is one potential reason for the lack
of utility of WCNs in low WER conditions.
</bodyText>
<subsectionHeader confidence="0.99984">
6.2 Evaluating retelling scoring
</subsectionHeader>
<bodyText confidence="0.999992375">
We analyzed the performance of the retelling scor-
ing methods under five different input conditions for
producing transcripts: (1) the out-of-domain Broad-
cast News recognizer with no adaptation; (2) do-
main adapted acoustic model; (3) domain adapted
language model; (4) domain adapted acoustic and
language models; and (5) manual (reference) tran-
scripts. Each story element is automatically labeled
by the systems as either having been recalled or not,
and this is compared with manual scores to derive an
F-score accuracy, by calculating precision and recall
of recalled story elements. Derived word alignments
or tag sequences are converted to binary story ele-
ment indicators by simply setting the element to 1
if any open-class word is tagged for (or aligned to)
that story element.
</bodyText>
<subsectionHeader confidence="0.889257">
6.2.1 Word alignment based scoring
</subsectionHeader>
<bodyText confidence="0.999912090909091">
We evaluate the word alignment approach only on
1-best ASR transcripts and manual transcripts, not
WCNs. The first row of Table 4 reports the story ele-
ment F-scores for a range of ASR adaptation scenar-
ios. The performance of the model improves signifi-
cantly as the WER reduces with adaptation. With the
fully adapted ASR the F-score improves more than
13%, and it is only 3.4% worse than with the man-
ual transcripts. The alignments produced in each of
these scenarios are used as training data in the unsu-
pervised condition evaluated below.
</bodyText>
<subsectionHeader confidence="0.899099">
6.2.2 Log-linear based automated scoring
</subsectionHeader>
<bodyText confidence="0.999970774193549">
Context-independent features Table 4 summa-
rizes the performance of the log-linear models us-
ing context independent features (CI) in supervised
(section 4), unsupervised (section 5.1) and hybrid
(section 5.2) training scenarios for different inputs
(reference transcript, ASR 1-best, and word confu-
sion network ASR output) and four different ASR
configurations.
The results show a few clear trends. Both in
the supervised and unsupervised training scenarios
the CRF model provides substantial improvements
over the MaxEnt classifier. The F-scores obtained
in the unsupervised training scenario are slightly
worse than with supervision, though they are compa-
rable to supervised results and an improvement over
just using the word alignment approach, particularly
in high WER scenarios. The hybrid training sce-
nario – supervised learning with word alignment de-
rived features – leads to reduced differences between
MaxEnt and CRF training compared to the other two
training scenarios. In fact, in high WER scenarios,
the MaxEnt slightly outperforms the CRF.
As expected the best performance is obtained with
manual transcripts and the worst with 1-best tran-
scripts generated by the out-of-domain ASR with
relatively high word error rate. For this ASR con-
figuration, using WCNs provide some gain, though
the gain is insignificant for the hybrid approach. In
the hybrid approach, the output labels of the word
alignment are already good indicators of the output
tag and incorporating the confusable words from the
</bodyText>
<tableCaption confidence="0.994032">
Table 4: Story element F-score achieved by baseline word-alignment model and log-linear models (MaxEnt and CRF) using
context independent features (CI) under 3 different scenarios, with 3 different inputs (1-best ASR, word confusion network, and
manual transcripts) and different ASR models (baseline out-of-domain, AM adapted, LM adapted and AM+LM adapted).
</tableCaption>
<table confidence="0.982641260869565">
Training Transcripts: 1-best WCN manual
Scenario
ASR: baseline AM LM AM+LM baseline AM LM AM+LM N/A
Baseline word-alignment: 71.9 77.3 84.3 85.4 N/A 88.8
Supervised MaxEnt-CI 76.0 81.7 84.6 85.6 78.9 83.4 84.0 84.7 86.4
CRF-CI 80.3 87.3 89.7 91.4 83.7 88.8 88.2 90.8 94.4
Unsupervised MaxEnt-CI 72.1 79.3 82.7 84.2 77.5 81.2 83.4 83.2 84.8
CRF-CI 79.4 85.4 86.8 88.0 81.2 85.8 86.2 87.2 90.5
Hybrid MaxEnt-CI 88.1 89.4 89.2 89.6 87.6 89.2 88.8 89.5 91.8
CRF-CI 87.0 90.9 91.5 92.1 87.4 91.5 90.1 92.4 94.6
216
Training Transcripts: 1-best WCN manual
Scenario
ASR: baseline AM LM AM+LM baseline AM LM AM+LM N/A
Supervised MaxEnt-CD 80.1 87.3 90.0 91.1 83.5 88.6 88.2 90.3 93.3
CRF-CD-IO 80.6 88.0 89.9 91.2 84.2 89.6 88.8 90.5 94.7
CRF-CD-BIO 81.1 87.9 90.6 91.7 84.5 89.5 88.8 90.8 94.7
Un- MaxEnt-CD 77.1 83.1 86.5 89.0 80.2 85.0 86.2 87.6 90.7
supervised CRF-CD-IO 79.1 85.3 87.1 88.3 81.0 85.9 86.4 87.5 90.3
CRF-CD-BIO 79.1 85.6 87.2 88.4 81.3 85.9 86.2 87.3 90.6
Hybrid MaxEnt-CD 88.4 90.2 90.7 91.6 88.6 90.5 90.4 91.4 93.5
CRF-CD-IO 87.9 91.3 91.6 92.5 88.3 91.7 90.7 92.1 94.8
CRF-BIO 87.8 91.9 91.8 93.0 88.7 92.0 90.7 92.3 94.7
</table>
<tableCaption confidence="0.669121666666667">
Table 5: Story element F-score achieved by log-linear models (MaxEnt and CRF) when adding context dependent features (CD)
and BIO tags for the CRF models, under 3 different scenarios, with 3 different inputs (1-best ASR, word confusion network, and
manual transcripts) and different ASR models (baseline out-of-domain, AM adapted, LM adapted and AM+LM adapted).
</tableCaption>
<bodyText confidence="0.999900727272727">
WCN into the feature vector apparently mainly adds
noise.
When the transcripts are generated with the
adapted models, the word confidence score of the 1-
best is higher and the WCN bins have fewer acous-
tically confusable words. Still, the WCN input is
helpful in the AM-adapted ASR system. When
the transcripts are generated with LM adapted mod-
els, the performance is better with 1-best than with
WCNs. As mentioned earlier, adapting the lan-
guage models may introduce a bias due to the rel-
atively low LM perplexity for this domain. In the
lowest WER scenarios, the best performing systems
achieve over 90% F-score, within two percent of the
performance achieved with manual transcripts.
Context-dependent features Exercising the flex-
ibility of log-linear models, we investigated the im-
pact of using context-dependent (CD) features in-
stead of the CI features used in the previous exper-
iments. Our CD features take into account the two
immediately neighboring word positions. As men-
tioned earlier, following Kurata et al. (2012), the
counts from the neighboring word positions were
weighted (α = 0.3) to avoid data sparsity. This re-
duces the sensitivity of the model to time alignment
errors between the tag and feature vector sequences
without increasing the dimensions. In Table 5, we
report the F-scores for the different ASR configu-
rations, inputs, and log-linear models with context
dependent features, using the standard IO tagset as
in Table 4.
Although there are some exceptions, adding con-
text information from the input features improves
the performance of the models. In particular, the
MaxEnt models benefit from incorporating this ex-
tra information. The MaxEnt models improve their
performance substantially for all three training sce-
narios, while the gains for the CRF models are more
modest, especially for the unsupervised approach
where the performance degrades or does not change
much, since some context information is already
captured by the Markov order 1 features.
BIO tagset As detailed in Section 4.1, story el-
ements sometimes span multiple words, so for the
CRF models we investigated two different schemes
for tagging, following typical practice in named en-
tity extraction (Ratinov and Roth, 2009) and syn-
tactic chunking (Sha and Pereira, 2003). The BIO
tagging scheme makes the distinction between the
tokens from the story elements that are in the be-
ginning from the ones that are not. The O tag is
assigned to the tokens that do not belong to any of
the story elements. The IO tagging uses a single tag
for the tokens that fall in the same story element,
which is the approach we have followed so far. In
addition to presenting results using context depen-
dent features, Table 5 presents results with the BIO
tagset.
For the supervised and hybrid approaches, the
BIO tagging provides insignificant but consistent
gains for most of the scenarios. The unsupervised
approach provides mixed results. This may be due to
the way in which the word alignment model scores
the retellings. It tags only those words from the
retelling that are aligned with a content word in the
source narrative, which may result in the loss of the
</bodyText>
<page confidence="0.993613">
217
</page>
<table confidence="0.9999107">
Training Transcripts: 1-best WCN manual
Scenario
ASR: baseline AM LM AM+LM baseline AM LM AM+LM N/A
Baseline word-alignment: 0.65 0.67 0.74 0.76 N/A 0.79
Supervised MaxEnt-CD 0.65 0.73 0.76 0.77 0.70 0.73 0.77 0.77 0.81
CRF-CD-BIO 0.69 0.76 0.77 0.76 0.73 0.76 0.77 0.78 0.82
Un- MaxEnt-CD 0.65 0.72 0.75 0.76 0.70 0.75 0.75 0.76 0.80
supervised CRF-CD-BIO 0.74 0.75 0.78 0.78 0.71 0.74 0.77 0.76 0.81
Hybrid MaxEnt-CD 0.72 0.76 0.77 0.78 0.74 0.76 0.77 0.77 0.82
CRF-CD-BIO 0.72 0.76 0.78 0.78 0.76 0.77 0.78 0.79 0.81
</table>
<tableCaption confidence="0.993899">
Table 6: Classification performance (AUC) for the baseline word-alignment model and the best performing log-linear models of
both types (MaxEnt and CRF) under 3 different scenarios with 3 types of input and 4 types of ASR models.
</tableCaption>
<bodyText confidence="0.997421">
structure of some multiwords story elements that we
are trying to capture with the BIO scheme.
</bodyText>
<subsectionHeader confidence="0.998632">
6.3 Evaluating MCI classification
</subsectionHeader>
<bodyText confidence="0.999858942857143">
Each of the individuals producing retellings in our
corpus underwent a battery of neuropsychological
tests, and were assigned a Clinical Dementia Rating
(CDR) (Morris, 1993), which is a composite score
derived from measures of cognitive function in six
domains, including memory. Importantly, it is as-
signed independently of the Wechsler Logical Mem-
ory test we are analyzing in this paper, which allows
us to evaluate the utility of our WLM analyses in
an unbiased manner. MCI is defined as a CDR of
0.5 (Ritchie and Touchon, 2000), and subjects in this
study have either a CDR of 0 (no impairment) or 0.5
(MCI).
In previous work, we found that the features
extracted from the retellings are useful in dis-
tinguishing subjects with MCI from neurotyp-
ical age-matched controls (Lehr et al., 2012;
Prud’hommeaux and Roark, 2012; Prud’hommeaux
and Roark, 2011). From each retellings, we extract
Boolean features for each story element, for a total
of 50 features for classification. Each feature indi-
cates whether the retelling contained that story ele-
ment.
In this paper, we carry out similar classification
experiments to investigate the impact of using log-
linear models on the extraction of features for classi-
fication. We build a support vector machine (SVM)
using the LibSVM (Chang and Lin, 2011) exten-
sion to the WEKA data mining Java API (Hall et al.,
2009). This allows recollection of different elements
to be weighted differently. This is unlike the manual
scoring of WLM based on clinical guidelines where
all elements are weighted equally irrespective of the
difficulty. The SVM was trained on manually ex-
tracted story element feature vectors. We compared
the performance of the MCI classification for three
types of input and four ASR configurations under
the supervised, unsupervised, and hybrid scenarios.
For each scenario we chose the best scoring system
from among the automated systems reported in Ta-
bles 4 and 5. Classification results, evaluated as area
under the curve (AUC), are reported in Table 6, both
for the log-linear trained tagging models and for the
baseline word-alignment based method. For refer-
ence (not shown in the table), the SVM classifier
performed at 0.83 when features values are manu-
ally populated.
The results show that the AUC improves steadily
as the quality of the transcription is improved, go-
ing from the baseline system to the adapted mod-
els. This is consistent with the improvements seen in
the F-score for detecting story elements. The differ-
ent approaches for detecting the story elements from
the transcriptions did not ultimately show significant
differences in MCI classification results. Overall,
the best classification values are given by the hy-
brid approach, which performs slightly better than
the other two approaches. The best AUC in the
hybrid scenario (0.79, very close to the AUC=0.81
achieved with manual transcripts) is obtained with
a CRF trained with WCNs from the fully adapted
ASR model and with context dependent features and
BIO tags.
Comparing WCN versus 1-best as inputs, using
WCN as input improves classification performance
when the 1-best transcripts are poor, as in the case
of out-of-domain ASR. The adapted recognizer im-
proves the performance of the 1-best significantly
making it unnecessary to resort to WCN as inputs.
Comparing the MaxEnt model with CRF model
</bodyText>
<page confidence="0.994879">
218
</page>
<bodyText confidence="0.999940625">
for extracting story elements, we see that the average
F-scores for the MaxEnt models trained on CD fea-
tures are nearly as good as and sometimes slightly
better than those produced using the CRF models.
The CRF extracted story elements, however, tend to
yield classifiers that perform slightly better, espe-
cially in the unsupervised approach with 1-best in-
puts.
</bodyText>
<sectionHeader confidence="0.980887" genericHeader="conclusions">
7 Summary and discussion
</sectionHeader>
<bodyText confidence="0.999995942028986">
This paper examines the task of automatically scor-
ing narrative retellings in terms of their fidelity to
the original narrative content, using discriminatively
trained log-linear tagging models. Fully automatic
scoring must account for both lexical variation and
acoustic confusion from ASR errors. Lexical vari-
ation – due to extensive paraphrasing on the part
of the individuals retelling the narrative – can be
modeled effectively using word-alignment models
such as those employed in machine translation sys-
tems (Lehr et al., 2012; Prud’hommeaux and Roark,
2011). This paper focuses on an alternative ap-
proach, where both lexical variation and ASR con-
fusions are modeled using log-linear models. In ad-
dition to very flexible feature definitions, the log-
linear models bring the advantage of a discrimina-
tive model to the task. We see improvements in
story element F-score using these models over unsu-
pervised word-alignment models. Further, the fea-
ture definition flexibility allows us to incorporate the
unsupervised word-alignment labels into these mod-
els, resulting either in fully unsupervised approaches
that perform competitively with the supervised mod-
els or in hybrid (supervised) approaches that provide
the best performing systems in this study.
Our tagging models are able to process word con-
fusion networks as inputs and thus improve perfor-
mance over using 1-best ASR transcripts in scenar-
ios where the speech recognition error rate is high.
These improvements carry through to the MCI clas-
sification task, making use of features computed
from the automatic scoring of narrative retelling.
One advantage of the word-alignment model is
that such approaches do not require manual anno-
tation of the story elements, which is more labor in-
tensive than typical manual transcription of speech.
Thus, the word-alignment model can exploit large
numbers of retellings in an unsupervised manner
when trained on ASR transcripts of the retellings.
Controlled experiments here with relatively limited
training sets demonstrate that semi-supervised ap-
proaches on larger untranscribed sets are likely to
be successful.
Finally, experiments with different amounts of
ASR adaptation show that both acoustic and lan-
guage model adaptations in this domain are effec-
tive, yielding scenarios that are competitive with
manual transcription both for detecting story ele-
ments as well as for subsequent classification. With
full model adaptation to the domain, the 1-best
transcripts improved significantly, and their perfor-
mance was found to be at par with WCNs.
In future work, we would like to investigate two
questions left open by these results. First, word-
alignment models can be extended to process ASR
lattices or word confusion networks as part of the
unsupervised alignment learning algorithm, and in-
corporated into our approach. Second, the con-
textual features can be refined (e.g., concatenated
features instead of smoothed features) when large
amounts of training data is available.
It is noteworthy to mention that the lexical vari-
ants and paraphrasing learned from the data using
automated method may be useful in refining the clin-
ical guidelines for scoring (e.g., allowing additional
lexical variants and paraphrasings, or assigning un-
equal credits for different story elements to reflect
the difficulty of recollecting them) or to create the
guidelines for new languages or stories.
</bodyText>
<sectionHeader confidence="0.997488" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999869818181818">
This research was supported in part by NIH awards
5K25AG033723-02 and P30 AG024978-05 and
NSF awards 1027834, 0958585, 0905095, 0964102
and 0826654. Any opinions, findings, conclusions
or recommendations expressed in this publication
are those of the authors and do not reflect the views
of the NIH or NSF. We thank Brian Kingsbury and
IBM for the use of their ASR software tools; Jeffrey
Kaye and Diane Howeison for their valuable input;
and the clinicians at the Oregon Center for Aging
and Technology for their care in collecting the data.
</bodyText>
<page confidence="0.998487">
219
</page>
<sectionHeader confidence="0.995632" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999833877551021">
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2(27):1–
27.
Jonathan Fiscus, John Garofolo, Audrey Le, Alvin Mar-
tin, greg Sanders, Mark Przybocki, and David Pallett.
2007. 2004 spring nist rich transcription (rt-04s)
evaluation data. http://www.ldc.upenn.edu/
Catalog/catalogEntry.jsp?catalogId=
LDC2007S12.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
SIGKDD Explorations, 11(1).
Brian Kingsbury, Hagen Soltau, George Saon,
Stephen M. Chu, Hong-Kwang Kuo, Lidia Mangu,
Suman V. Ravuri, Nelson Morgan, and Adam Janin.
2011. The IBM 2009 GALE Arabic speech tran-
scription system. In Proceedings of ICASSP, pages
4672–4675.
Marit Korkman, Ursula Kirk, and Sally Kemp. 1998.
NEPSY: A developmental neuropsychological assess-
ment. The Psychological Corporation, San Antonio.
Gakuto Kurata, Nobuyasu Itoh, Masafumi Nishimura,
Abhinav Sethy, and Bhuvana Ramabhadran. 2012.
Leveraging word confusion networks for named entity
modeling and detection from conversational telephone
speech. Speech Communication, 54(3):491–502.
Maider Lehr, Emily Prud’hommeaux, Izhak Shafran, and
Brian Roark. 2012. Fully automated neuropsycho-
logical assessment for detecting mild cognitive impair-
ment. In Interspeech.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of HLT-NAACL.
Catherine Lord, Michael Rutter, Pamela DiLavore, and
Susan Risi. 2002. Autism Diagnostic Observation
Schedule (ADOS). Western Psychological Services,
Los Angeles.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: Word error
minimization and other applications of confusion net-
works. Computer Speech and Language, 14(4):373–
400.
Andrew Kachites McCallum. 2002. Mallet: A machine
learning for language toolkit. http://mallet.
cs.umass.edu.
John Morris. 1993. The clinical dementia rating
(CDR): Current version and scoring rules. Neurology,
43:2412–2414.
A. Nordlund, S. Rolstad, P. Hellstrom, M. Sjogren,
S. Hansen, and A. Wallin. 2005. The Goteborg MCI
study: Mild cognitive impairment is a heterogeneous
condition. Journal of Neurology, Neurosurgery and
Psychiatry, 76(11):1485–1490.
Ronald Petersen, Glenn Smith, Stephen Waring, Robert
Ivnik, Eric Tangalos, and Emre Kokmen. 1999. Mild
cognitive impairment: Clinical characterizations and
outcomes. Archives of Neurology, 56:303–308.
Daniel Povey, Dimitri Kanevsky, Brian Kingsbury,
Bhuvana Ramabhadran, George Saon, and Karthik
Visweswariah. 2008. Boosted mmi for model and fea-
ture space discriminative training. In Proceedings of
ICASSP.
Emily Prud’hommeaux and Brian Roark. 2011. Align-
ment of spoken narratives for automated neuropsycho-
logical assessment. In Proceedings of ASRU.
Emily Prud’hommeaux and Brian Roark. 2012. Graph-
based alignment of narratives for automated neuropsy-
chological assessment. In Proceedings of the NAACL
2012 Workshop on Biomedical Natural Language Pro-
cessing (BioNLP).
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
EMNLP.
Karen Ritchie and Jacques Touchon. 2000. Mild cogni-
tive impairment: Conceptual basis and current noso-
logical status. Lancet, 355:225–228.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. In Proceedings of
HLT-NAACL.
Martha Storandt and Robert Hill. 1989. Very mild senile
dementia of the Alzheimer’s type: II Psychometric test
performance. Archives of Neurology, 46:383–386.
Qing-Song Wang and Jiang-Ning Zhou. 2002. Retrieval
and encoding of episodic memory in normal aging and
patients with mild cognitive impairment. Brain Re-
search, 924:113–115.
David Wechsler. 1997. Wechsler Memory Scale - Third
Edition. The Psychological Corporation, San Antonio.
Shasha Xie, Keelan Evanini, and Klaus Zechner. 2012.
Exploring content features for automated speech scor-
ing. In Proceedings of HLT-NAACL.
Xiaonan Zhang, Xiaonan Zhang, Jack Mostow, Jack
Mostow, Nell Duke, Christina Trotochaud, Joseph Va-
leri, and Al Corbett. 2008. Mining free-form spoken
responses to tutor prompts. In Proceedings of the First
International Conference on Educational Data Min-
ing, pages 234–241.
</reference>
<page confidence="0.997591">
220
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.656490">
<title confidence="0.998293">Discriminative Joint Modeling of Lexical Variation and Acoustic for Automated Narrative Retelling Assessment</title>
<author confidence="0.989609">Izhak Emily Brian</author>
<affiliation confidence="0.820942">for Spoken Language Understanding, Oregon Health &amp; Science for Language Sciences, University of Rochester</affiliation>
<email confidence="0.999603">imaiderlehr,zakshafran,emilpx,roarkbrl@gmail.com</email>
<abstract confidence="0.99669632">Automatically assessing the fidelity of a retelling to the original narrative – a task of growing clinical importance – is challenging, given extensive paraphrasing during retelling along with cascading automatic speech recognition (ASR) errors. We present a word tagging approach using conditional random fields (CRFs) that allows a diversity of features to be considered during inference, including some capturing acoustic confusions encoded in word confusion networks. We evaluate the approach under several scenarios, including both supervised and unsupervised training, the latter achieved by training on the output of a baseline automatic word-alignment model. We also adapt the ASR models to the domain, and evaluate the impact of error rate on performance. We find strong robustness to ASR errors, even using just the 1-best system output. A hybrid approach making use of both automatic alignment and CRFs trained tagging models achieves the best performance, yielding strong improvements over using either approach alone.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<volume>2</volume>
<issue>27</issue>
<pages>27</pages>
<contexts>
<context position="30609" citStr="Chang and Lin, 2011" startWordPosition="4954" endWordPosition="4957"> retellings are useful in distinguishing subjects with MCI from neurotypical age-matched controls (Lehr et al., 2012; Prud’hommeaux and Roark, 2012; Prud’hommeaux and Roark, 2011). From each retellings, we extract Boolean features for each story element, for a total of 50 features for classification. Each feature indicates whether the retelling contained that story element. In this paper, we carry out similar classification experiments to investigate the impact of using loglinear models on the extraction of features for classification. We build a support vector machine (SVM) using the LibSVM (Chang and Lin, 2011) extension to the WEKA data mining Java API (Hall et al., 2009). This allows recollection of different elements to be weighted differently. This is unlike the manual scoring of WLM based on clinical guidelines where all elements are weighted equally irrespective of the difficulty. The SVM was trained on manually extracted story element feature vectors. We compared the performance of the MCI classification for three types of input and four ASR configurations under the supervised, unsupervised, and hybrid scenarios. For each scenario we chose the best scoring system from among the automated syst</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2(27):1– 27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Fiscus</author>
<author>John Garofolo</author>
<author>Audrey Le</author>
<author>Alvin Martin</author>
<author>greg Sanders</author>
<author>Mark Przybocki</author>
<author>David Pallett</author>
</authors>
<title>spring nist rich transcription (rt-04s) evaluation data. http://www.ldc.upenn.edu/ Catalog/catalogEntry.jsp?catalogId= LDC2007S12.</title>
<date>2007</date>
<contexts>
<context position="18956" citStr="Fiscus et al., 2007" startWordPosition="3071" endWordPosition="3074">7S44, LDC98S71). The language model is defined over an 84K vocabulary and consists of about 1.8M, 1M and and 331K bigrams, trigrams and 4-grams, estimated from standard Broadcast news corpus. The decoding is performed in several stages using successively refined acoustic models – a context-dependent model, a vocal-tract normalized model, a speaker-adapted maximum likelihood linear regression (MLLR) model, and finally a discriminatively trained model with the boosted MMI criteria (Povey et al., 2008). The system gives a word error rate of 13.1% on the 2004 Rich Transcription benchmark by NIST (Fiscus et al., 2007), which is comparable to state-of-the-art for equivalent amounts of acoustic training data. On the WLM corpus, the recognition word error rate was significantly higher at 47.2% due to a mismatch in domain and the skewed demographics (age) of the speakers. We improved the performance of the above Broadcast News models by adapting to the domain of the WLM retellings. The acoustic models were adapted using standard MLLR, where linear transforms were estimated in an unsupervised manner to maximize the likelihood over the transcripts of the retellings. The transcripts were generated from the baseli</context>
</contexts>
<marker>Fiscus, Garofolo, Le, Martin, Sanders, Przybocki, Pallett, 2007</marker>
<rawString>Jonathan Fiscus, John Garofolo, Audrey Le, Alvin Martin, greg Sanders, Mark Przybocki, and David Pallett. 2007. 2004 spring nist rich transcription (rt-04s) evaluation data. http://www.ldc.upenn.edu/ Catalog/catalogEntry.jsp?catalogId= LDC2007S12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA data mining software: An update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="30672" citStr="Hall et al., 2009" startWordPosition="4967" endWordPosition="4970">eurotypical age-matched controls (Lehr et al., 2012; Prud’hommeaux and Roark, 2012; Prud’hommeaux and Roark, 2011). From each retellings, we extract Boolean features for each story element, for a total of 50 features for classification. Each feature indicates whether the retelling contained that story element. In this paper, we carry out similar classification experiments to investigate the impact of using loglinear models on the extraction of features for classification. We build a support vector machine (SVM) using the LibSVM (Chang and Lin, 2011) extension to the WEKA data mining Java API (Hall et al., 2009). This allows recollection of different elements to be weighted differently. This is unlike the manual scoring of WLM based on clinical guidelines where all elements are weighted equally irrespective of the difficulty. The SVM was trained on manually extracted story element feature vectors. We compared the performance of the MCI classification for three types of input and four ASR configurations under the supervised, unsupervised, and hybrid scenarios. For each scenario we chose the best scoring system from among the automated systems reported in Tables 4 and 5. Classification results, evaluat</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA data mining software: An update. SIGKDD Explorations, 11(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Kingsbury</author>
<author>Hagen Soltau</author>
<author>George Saon</author>
<author>Stephen M Chu</author>
<author>Hong-Kwang Kuo</author>
<author>Lidia Mangu</author>
<author>Suman V Ravuri</author>
<author>Nelson Morgan</author>
<author>Adam Janin</author>
</authors>
<title>GALE Arabic speech transcription system.</title>
<date>2011</date>
<booktitle>The IBM</booktitle>
<pages>4672--4675</pages>
<contexts>
<context position="17577" citStr="Kingsbury et al. (2011)" startWordPosition="2852" endWordPosition="2855">d’hommeaux and Roark, 2012; Prud’hommeaux and Roark, 2011), and the results are not directly comparable. The recordings were sometimes made in an informal setting, such as the subject’s home or a senior center. For this reason, there are often extraneous noises in the recordings such as music, footsteps, and clocks striking the hour. Although this presents a challenge for ASR, part of the goal of our work is to demonstrate the robustness of our methods to noisy audio. 6.1 Automatic transcription The baseline ASR system used in the current work is a Broadcast News system which is modeled after Kingsbury et al. (2011). Briefly, the acoustics of speech are modeled by 4000 clustered allophone states defined over a pentaphone context, where states are represented by Gaussian mixture models with a total of 150K mixture components. The observation vectors consist of PLP features, stacked from 10 neighboring frames and projected to a 50- System 1-best oracle oracle (%) WCN(%) lat(%) Baseline 47.2 39.7 27.7 AM adaptation 38.2 35.5 21.2 LM adaptation 28.3 30.7 19.9 AM+LM adaptation 25.6 26.5 16.5 Table 3: Improvement in ASR word error-rate by adapting the Broadcast News models to the domain of narrative retelling.</context>
</contexts>
<marker>Kingsbury, Soltau, Saon, Chu, Kuo, Mangu, Ravuri, Morgan, Janin, 2011</marker>
<rawString>Brian Kingsbury, Hagen Soltau, George Saon, Stephen M. Chu, Hong-Kwang Kuo, Lidia Mangu, Suman V. Ravuri, Nelson Morgan, and Adam Janin. 2011. The IBM 2009 GALE Arabic speech transcription system. In Proceedings of ICASSP, pages 4672–4675.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marit Korkman</author>
<author>Ursula Kirk</author>
<author>Sally Kemp</author>
</authors>
<title>NEPSY: A developmental neuropsychological assessment. The Psychological Corporation,</title>
<date>1998</date>
<location>San Antonio.</location>
<contexts>
<context position="1800" citStr="Korkman et al., 1998" startWordPosition="255" endWordPosition="258">ors, even using just the 1-best system output. A hybrid approach making use of both automatic alignment and CRFs trained tagging models achieves the best performance, yielding strong improvements over using either approach alone. 1 Introduction Narrative production tasks are an essential component of many standard neuropsychological test batteries. For example, narration of a wordless picture book is part of the Autism Diagnostic Observation Schedule (ADOS) (Lord et al., 2002) and retelling of previously narrated stories is part of both the Developmental Neuropsychological Assessment (NEPSY) (Korkman et al., 1998) and the Wechsler Logical Memory (WLM) test (Wechsler, 1997). Such tests also arise in reading comprehension, second language learning and other computer-based tutoring systems (Xie et al., 2012; Zhang et al., 2008). The accuracy of automated scoring of a narrative retelling depends on correctly identifying which of the source narrative’s propositions or events (what we will call ‘story elements’) have been included in the retelling. Speakers may choose to relate these elements using diverse words or phrases, and an automated method of identifying these elements needs to model the permissible </context>
</contexts>
<marker>Korkman, Kirk, Kemp, 1998</marker>
<rawString>Marit Korkman, Ursula Kirk, and Sally Kemp. 1998. NEPSY: A developmental neuropsychological assessment. The Psychological Corporation, San Antonio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gakuto Kurata</author>
<author>Nobuyasu Itoh</author>
<author>Masafumi Nishimura</author>
<author>Abhinav Sethy</author>
<author>Bhuvana Ramabhadran</author>
</authors>
<title>Leveraging word confusion networks for named entity modeling and detection from conversational telephone speech.</title>
<date>2012</date>
<journal>Speech Communication,</journal>
<volume>54</volume>
<issue>3</issue>
<contexts>
<context position="8933" citStr="Kurata et al., 2012" startWordPosition="1426" endWordPosition="1430">the tag indicating the story element that the word is associated with. In its simplest form, we have 26 tags: one for each of the 25 story elements indicating the word is ‘in’ that element (e.g., I15); and one for ‘outside’ of any story element (‘O’). By tagging word positions, we are framing the problem in a general enough way to allow tagging of word confusion networks (Mangu et al., 2000), which encode word confusions that may provide additional robustness, particularly in high word-error rate scenarios. We make use of log-linear models, which have been used for tagging confusion networks (Kurata et al., 2012), and which allow very flexible feature vector definition and discriminative optimization. The model allows us to experiment with three types of inputs as illustrated in the Figure 3 – the manual transcript, the 1-best ASR transcript, and the word confusion network. To create supervised training data, we force-align ASR transcripts to manual transcripts and transfer manually annotated story element tags from the reference transcripts to word positions in the confusion network or 1-best ASR output using the word-level time marks. Our unsupervised training scenario instead derives story element </context>
<context position="11522" citStr="Kurata et al. (2012)" startWordPosition="1860" endWordPosition="1863"> with context independent features (just xi) and context dependent features. Note that models including context dependent feature sets also include the context independent features, and Markov order 1 models also include Markov order 0 features. Two other details about our use of the feature templates are worth noting. First, when tagging confusion networks, each word in the network at position i results in a feature instance. Thus, if there are five confusable words at position i, then there will be five different xi values being used to instantiate the features in Table 1. Second, following Kurata et al. (2012), we multiply the feature counts for the context dependent features by a weight to control their influence on the model. In this paper, the scaling weight of the context-dependent features was 0.3. We investigate two different tagsets for this task, as presented in Table 2. The simpler tagset (IO) simply identifies words that are in a story element; the 213 Tagging anna rent was due IO-tags I1 I19 I19 I19 BIO-tags B1 B19 I19 I19 Table 2: Two possible tagsets for labeling. larger tagset (BIO) differentiates among positions in a story element chunk. The latter tagset is only of utility for model</context>
<context position="26308" citStr="Kurata et al. (2012)" startWordPosition="4245" endWordPosition="4248">mentioned earlier, adapting the language models may introduce a bias due to the relatively low LM perplexity for this domain. In the lowest WER scenarios, the best performing systems achieve over 90% F-score, within two percent of the performance achieved with manual transcripts. Context-dependent features Exercising the flexibility of log-linear models, we investigated the impact of using context-dependent (CD) features instead of the CI features used in the previous experiments. Our CD features take into account the two immediately neighboring word positions. As mentioned earlier, following Kurata et al. (2012), the counts from the neighboring word positions were weighted (α = 0.3) to avoid data sparsity. This reduces the sensitivity of the model to time alignment errors between the tag and feature vector sequences without increasing the dimensions. In Table 5, we report the F-scores for the different ASR configurations, inputs, and log-linear models with context dependent features, using the standard IO tagset as in Table 4. Although there are some exceptions, adding context information from the input features improves the performance of the models. In particular, the MaxEnt models benefit from inc</context>
</contexts>
<marker>Kurata, Itoh, Nishimura, Sethy, Ramabhadran, 2012</marker>
<rawString>Gakuto Kurata, Nobuyasu Itoh, Masafumi Nishimura, Abhinav Sethy, and Bhuvana Ramabhadran. 2012. Leveraging word confusion networks for named entity modeling and detection from conversational telephone speech. Speech Communication, 54(3):491–502.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maider Lehr</author>
<author>Emily Prud’hommeaux</author>
<author>Izhak Shafran</author>
<author>Brian Roark</author>
</authors>
<title>Fully automated neuropsychological assessment for detecting mild cognitive impairment.</title>
<date>2012</date>
<booktitle>In Interspeech.</booktitle>
<marker>Lehr, Prud’hommeaux, Shafran, Roark, 2012</marker>
<rawString>Maider Lehr, Emily Prud’hommeaux, Izhak Shafran, and Brian Roark. 2012. Fully automated neuropsychological assessment for detecting mild cognitive impairment. In Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="7084" citStr="Liang et al., 2006" startWordPosition="1120" endWordPosition="1123">ow sympathetic and made a collection for her so that she can feed the children. Figure 2: An example retelling with 12 recalled story elements. 3 Unsupervised generative automated scoring with word alignment In previous work (Lehr et al., 2012; Prud’hommeaux and Roark, 2012; Prud’hommeaux and Roark, 2011), we developed a pipeline for automatically scoring narrative retellings for the WLM task. The utterances corresponding to a retelling were recognized using an ASR system. The story elements were identified from the 1-best ASR transcript using word alignments produced by the Berkeley aligner (Liang et al., 2006), an EM-based word alignment package developed to align parallel texts for machine translation. The word alignment model was estimated in an unsupervised manner from a parallel corpus consisting of source narrative and manual transcripts of retellings from a small set of training subjects, and from a pairwise parallel corpus of manual retelling transcripts. During inference or test, the ASR transcripts of the retellings were aligned using the estimated alignment model to the source narrative text. If a word in the retelling was mapped by the alignment model to a content word in the source narr</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catherine Lord</author>
<author>Michael Rutter</author>
<author>Pamela DiLavore</author>
<author>Susan Risi</author>
</authors>
<title>Autism Diagnostic Observation Schedule (ADOS). Western Psychological Services,</title>
<date>2002</date>
<location>Los Angeles.</location>
<contexts>
<context position="1660" citStr="Lord et al., 2002" startWordPosition="236" endWordPosition="239">l. We also adapt the ASR models to the domain, and evaluate the impact of error rate on performance. We find strong robustness to ASR errors, even using just the 1-best system output. A hybrid approach making use of both automatic alignment and CRFs trained tagging models achieves the best performance, yielding strong improvements over using either approach alone. 1 Introduction Narrative production tasks are an essential component of many standard neuropsychological test batteries. For example, narration of a wordless picture book is part of the Autism Diagnostic Observation Schedule (ADOS) (Lord et al., 2002) and retelling of previously narrated stories is part of both the Developmental Neuropsychological Assessment (NEPSY) (Korkman et al., 1998) and the Wechsler Logical Memory (WLM) test (Wechsler, 1997). Such tests also arise in reading comprehension, second language learning and other computer-based tutoring systems (Xie et al., 2012; Zhang et al., 2008). The accuracy of automated scoring of a narrative retelling depends on correctly identifying which of the source narrative’s propositions or events (what we will call ‘story elements’) have been included in the retelling. Speakers may choose to</context>
</contexts>
<marker>Lord, Rutter, DiLavore, Risi, 2002</marker>
<rawString>Catherine Lord, Michael Rutter, Pamela DiLavore, and Susan Risi. 2002. Autism Diagnostic Observation Schedule (ADOS). Western Psychological Services, Los Angeles.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lidia Mangu</author>
<author>Eric Brill</author>
<author>Andreas Stolcke</author>
</authors>
<title>Finding consensus in speech recognition: Word error minimization and other applications of confusion networks.</title>
<date>2000</date>
<journal>Computer Speech and Language,</journal>
<volume>14</volume>
<issue>4</issue>
<pages>400</pages>
<contexts>
<context position="8707" citStr="Mangu et al., 2000" startWordPosition="1391" endWordPosition="1394">ised discriminative automated scoring with log-linear models In this work, we frame the task of detecting story elements as a tagging task. Thus, our problem reduces to assigning a tag to each word position in the retelling, the tag indicating the story element that the word is associated with. In its simplest form, we have 26 tags: one for each of the 25 story elements indicating the word is ‘in’ that element (e.g., I15); and one for ‘outside’ of any story element (‘O’). By tagging word positions, we are framing the problem in a general enough way to allow tagging of word confusion networks (Mangu et al., 2000), which encode word confusions that may provide additional robustness, particularly in high word-error rate scenarios. We make use of log-linear models, which have been used for tagging confusion networks (Kurata et al., 2012), and which allow very flexible feature vector definition and discriminative optimization. The model allows us to experiment with three types of inputs as illustrated in the Figure 3 – the manual transcript, the 1-best ASR transcript, and the word confusion network. To create supervised training data, we force-align ASR transcripts to manual transcripts and transfer manua</context>
</contexts>
<marker>Mangu, Brill, Stolcke, 2000</marker>
<rawString>Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000. Finding consensus in speech recognition: Word error minimization and other applications of confusion networks. Computer Speech and Language, 14(4):373– 400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet. cs.umass.edu.</note>
<contexts>
<context position="12888" citStr="McCallum, 2002" startWordPosition="2096" endWordPosition="2097">Entropy (MaxEnt) classifier where each position i from the retelling x gets assigned one of the IO output tags yi corresponding to the set of 25 story elements and a null (‘O’) symbol. The output tag is modeled as the conditional probability p(yi |xi) given the word xi at position i in the retelling. IAkOk(xi, yi) Z(xi) where Z(xi) is a normalization factor. The feature functions O(xi, yi) are the Markov order 0 features as defined as in the previous section. The parameters A E Rd are estimated by optimizing the above conditional probability, with L2 regularization. We use the MALLET Toolkit (McCallum, 2002) with default regularization parameters. 4.3 CRF-based sequence labeling model The MaxEnt models assign a tag to each position from the input retelling independently. However, there are a few reasons why reframing the task as a sequence modeling problem may improve tagging performance. First, some of the story elements are multiword sequences, such as she had been held up or on State Street. Second, even if a retelling orders recalled elements differently than the original narrative, there is a tendency for story elements to occur in certain orders. The parameters of the CRF model, A E Rd are </context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet. cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Morris</author>
</authors>
<title>The clinical dementia rating (CDR): Current version and scoring rules.</title>
<date>1993</date>
<journal>Neurology,</journal>
<pages>43--2412</pages>
<contexts>
<context position="29486" citStr="Morris, 1993" startWordPosition="4770" endWordPosition="4771">0.74 0.76 0.77 0.77 0.82 CRF-CD-BIO 0.72 0.76 0.78 0.78 0.76 0.77 0.78 0.79 0.81 Table 6: Classification performance (AUC) for the baseline word-alignment model and the best performing log-linear models of both types (MaxEnt and CRF) under 3 different scenarios with 3 types of input and 4 types of ASR models. structure of some multiwords story elements that we are trying to capture with the BIO scheme. 6.3 Evaluating MCI classification Each of the individuals producing retellings in our corpus underwent a battery of neuropsychological tests, and were assigned a Clinical Dementia Rating (CDR) (Morris, 1993), which is a composite score derived from measures of cognitive function in six domains, including memory. Importantly, it is assigned independently of the Wechsler Logical Memory test we are analyzing in this paper, which allows us to evaluate the utility of our WLM analyses in an unbiased manner. MCI is defined as a CDR of 0.5 (Ritchie and Touchon, 2000), and subjects in this study have either a CDR of 0 (no impairment) or 0.5 (MCI). In previous work, we found that the features extracted from the retellings are useful in distinguishing subjects with MCI from neurotypical age-matched controls</context>
</contexts>
<marker>Morris, 1993</marker>
<rawString>John Morris. 1993. The clinical dementia rating (CDR): Current version and scoring rules. Neurology, 43:2412–2414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nordlund</author>
<author>S Rolstad</author>
<author>P Hellstrom</author>
<author>M Sjogren</author>
<author>S Hansen</author>
<author>A Wallin</author>
</authors>
<title>The Goteborg MCI study: Mild cognitive impairment is a heterogeneous condition.</title>
<date>2005</date>
<journal>Journal of Neurology, Neurosurgery and Psychiatry,</journal>
<volume>76</volume>
<issue>11</issue>
<contexts>
<context position="4709" citStr="Nordlund et al., 2005" startWordPosition="702" endWordPosition="705">rms of using the automatic scores to classify elderly individuals as having Mild Cognitive Impairment. Ultimately we find that hybrid approaches, making use of both word-alignment and tagging models, yield strong improvements over either used independently. 2 Wechsler Logical Memory (WLM) task The Wechsler Logical Memory (WLM) task (Wechsler, 1997), a widely used subtest of a battery of neuropsychological tests used to assess memory function in adults, has been shown to be a good indicator of Mild Cognitive Impairment (MCI) (Storandt and Hill, 1989; Petersen et al., 1999; Wang and Zhou, 2002; Nordlund et al., 2005), the stage of cognitive decline that is often a precursor to dementia of the Alzheimer’s type. In the WLM, the subject listens to the examiner read a brief narrative and then retells the narrative twice: immediately upon hearing it and after about 20 minutes. The examiner grades the subject’s response by counting how many of the story elements the subject recalled. An excerpt of the text read by the clinician while administering the WLM task is shown in Figure 1. The story elements in the text are delineated using slashes, 25 elements in all. An example retelling is shown in Figure 2 to illus</context>
</contexts>
<marker>Nordlund, Rolstad, Hellstrom, Sjogren, Hansen, Wallin, 2005</marker>
<rawString>A. Nordlund, S. Rolstad, P. Hellstrom, M. Sjogren, S. Hansen, and A. Wallin. 2005. The Goteborg MCI study: Mild cognitive impairment is a heterogeneous condition. Journal of Neurology, Neurosurgery and Psychiatry, 76(11):1485–1490.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Petersen</author>
<author>Glenn Smith</author>
<author>Stephen Waring</author>
<author>Robert Ivnik</author>
</authors>
<title>Eric Tangalos, and Emre Kokmen.</title>
<date>1999</date>
<journal>Archives of Neurology,</journal>
<pages>56--303</pages>
<contexts>
<context position="4664" citStr="Petersen et al., 1999" startWordPosition="694" endWordPosition="697">nally, we demonstrate improved utility in terms of using the automatic scores to classify elderly individuals as having Mild Cognitive Impairment. Ultimately we find that hybrid approaches, making use of both word-alignment and tagging models, yield strong improvements over either used independently. 2 Wechsler Logical Memory (WLM) task The Wechsler Logical Memory (WLM) task (Wechsler, 1997), a widely used subtest of a battery of neuropsychological tests used to assess memory function in adults, has been shown to be a good indicator of Mild Cognitive Impairment (MCI) (Storandt and Hill, 1989; Petersen et al., 1999; Wang and Zhou, 2002; Nordlund et al., 2005), the stage of cognitive decline that is often a precursor to dementia of the Alzheimer’s type. In the WLM, the subject listens to the examiner read a brief narrative and then retells the narrative twice: immediately upon hearing it and after about 20 minutes. The examiner grades the subject’s response by counting how many of the story elements the subject recalled. An excerpt of the text read by the clinician while administering the WLM task is shown in Figure 1. The story elements in the text are delineated using slashes, 25 elements in all. An ex</context>
</contexts>
<marker>Petersen, Smith, Waring, Ivnik, 1999</marker>
<rawString>Ronald Petersen, Glenn Smith, Stephen Waring, Robert Ivnik, Eric Tangalos, and Emre Kokmen. 1999. Mild cognitive impairment: Clinical characterizations and outcomes. Archives of Neurology, 56:303–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Povey</author>
<author>Dimitri Kanevsky</author>
<author>Brian Kingsbury</author>
<author>Bhuvana Ramabhadran</author>
<author>George Saon</author>
<author>Karthik Visweswariah</author>
</authors>
<title>Boosted mmi for model and feature space discriminative training.</title>
<date>2008</date>
<booktitle>In Proceedings of ICASSP.</booktitle>
<contexts>
<context position="18840" citStr="Povey et al., 2008" startWordPosition="3049" endWordPosition="3052">nalysis (LDA). The acoustic models were trained on 430 hours of transcribed speech from Broadcast News corpus (LDC97S44, LDC98S71). The language model is defined over an 84K vocabulary and consists of about 1.8M, 1M and and 331K bigrams, trigrams and 4-grams, estimated from standard Broadcast news corpus. The decoding is performed in several stages using successively refined acoustic models – a context-dependent model, a vocal-tract normalized model, a speaker-adapted maximum likelihood linear regression (MLLR) model, and finally a discriminatively trained model with the boosted MMI criteria (Povey et al., 2008). The system gives a word error rate of 13.1% on the 2004 Rich Transcription benchmark by NIST (Fiscus et al., 2007), which is comparable to state-of-the-art for equivalent amounts of acoustic training data. On the WLM corpus, the recognition word error rate was significantly higher at 47.2% due to a mismatch in domain and the skewed demographics (age) of the speakers. We improved the performance of the above Broadcast News models by adapting to the domain of the WLM retellings. The acoustic models were adapted using standard MLLR, where linear transforms were estimated in an unsupervised mann</context>
</contexts>
<marker>Povey, Kanevsky, Kingsbury, Ramabhadran, Saon, Visweswariah, 2008</marker>
<rawString>Daniel Povey, Dimitri Kanevsky, Brian Kingsbury, Bhuvana Ramabhadran, George Saon, and Karthik Visweswariah. 2008. Boosted mmi for model and feature space discriminative training. In Proceedings of ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Prud’hommeaux</author>
<author>Brian Roark</author>
</authors>
<title>Alignment of spoken narratives for automated neuropsychological assessment.</title>
<date>2011</date>
<booktitle>In Proceedings of ASRU.</booktitle>
<marker>Prud’hommeaux, Roark, 2011</marker>
<rawString>Emily Prud’hommeaux and Brian Roark. 2011. Alignment of spoken narratives for automated neuropsychological assessment. In Proceedings of ASRU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Prud’hommeaux</author>
<author>Brian Roark</author>
</authors>
<title>Graphbased alignment of narratives for automated neuropsychological assessment.</title>
<date>2012</date>
<booktitle>In Proceedings of the NAACL 2012 Workshop on Biomedical Natural Language Processing (BioNLP).</booktitle>
<marker>Prud’hommeaux, Roark, 2012</marker>
<rawString>Emily Prud’hommeaux and Brian Roark. 2012. Graphbased alignment of narratives for automated neuropsychological assessment. In Proceedings of the NAACL 2012 Workshop on Biomedical Natural Language Processing (BioNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Ratinov</author>
<author>Dan Roth</author>
</authors>
<title>Design challenges and misconceptions in named entity recognition.</title>
<date>2009</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="27503" citStr="Ratinov and Roth, 2009" startWordPosition="4433" endWordPosition="4436">xEnt models benefit from incorporating this extra information. The MaxEnt models improve their performance substantially for all three training scenarios, while the gains for the CRF models are more modest, especially for the unsupervised approach where the performance degrades or does not change much, since some context information is already captured by the Markov order 1 features. BIO tagset As detailed in Section 4.1, story elements sometimes span multiple words, so for the CRF models we investigated two different schemes for tagging, following typical practice in named entity extraction (Ratinov and Roth, 2009) and syntactic chunking (Sha and Pereira, 2003). The BIO tagging scheme makes the distinction between the tokens from the story elements that are in the beginning from the ones that are not. The O tag is assigned to the tokens that do not belong to any of the story elements. The IO tagging uses a single tag for the tokens that fall in the same story element, which is the approach we have followed so far. In addition to presenting results using context dependent features, Table 5 presents results with the BIO tagset. For the supervised and hybrid approaches, the BIO tagging provides insignifica</context>
</contexts>
<marker>Ratinov, Roth, 2009</marker>
<rawString>Lev Ratinov and Dan Roth. 2009. Design challenges and misconceptions in named entity recognition. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Ritchie</author>
<author>Jacques Touchon</author>
</authors>
<title>Mild cognitive impairment: Conceptual basis and current nosological status.</title>
<date>2000</date>
<tech>Lancet,</tech>
<pages>355--225</pages>
<contexts>
<context position="29844" citStr="Ritchie and Touchon, 2000" startWordPosition="4830" endWordPosition="4833"> elements that we are trying to capture with the BIO scheme. 6.3 Evaluating MCI classification Each of the individuals producing retellings in our corpus underwent a battery of neuropsychological tests, and were assigned a Clinical Dementia Rating (CDR) (Morris, 1993), which is a composite score derived from measures of cognitive function in six domains, including memory. Importantly, it is assigned independently of the Wechsler Logical Memory test we are analyzing in this paper, which allows us to evaluate the utility of our WLM analyses in an unbiased manner. MCI is defined as a CDR of 0.5 (Ritchie and Touchon, 2000), and subjects in this study have either a CDR of 0 (no impairment) or 0.5 (MCI). In previous work, we found that the features extracted from the retellings are useful in distinguishing subjects with MCI from neurotypical age-matched controls (Lehr et al., 2012; Prud’hommeaux and Roark, 2012; Prud’hommeaux and Roark, 2011). From each retellings, we extract Boolean features for each story element, for a total of 50 features for classification. Each feature indicates whether the retelling contained that story element. In this paper, we carry out similar classification experiments to investigate </context>
</contexts>
<marker>Ritchie, Touchon, 2000</marker>
<rawString>Karen Ritchie and Jacques Touchon. 2000. Mild cognitive impairment: Conceptual basis and current nosological status. Lancet, 355:225–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Sha</author>
<author>Fernando Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="27550" citStr="Sha and Pereira, 2003" startWordPosition="4441" endWordPosition="4444">a information. The MaxEnt models improve their performance substantially for all three training scenarios, while the gains for the CRF models are more modest, especially for the unsupervised approach where the performance degrades or does not change much, since some context information is already captured by the Markov order 1 features. BIO tagset As detailed in Section 4.1, story elements sometimes span multiple words, so for the CRF models we investigated two different schemes for tagging, following typical practice in named entity extraction (Ratinov and Roth, 2009) and syntactic chunking (Sha and Pereira, 2003). The BIO tagging scheme makes the distinction between the tokens from the story elements that are in the beginning from the ones that are not. The O tag is assigned to the tokens that do not belong to any of the story elements. The IO tagging uses a single tag for the tokens that fall in the same story element, which is the approach we have followed so far. In addition to presenting results using context dependent features, Table 5 presents results with the BIO tagset. For the supervised and hybrid approaches, the BIO tagging provides insignificant but consistent gains for most of the scenari</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Fei Sha and Fernando Pereira. 2003. Shallow parsing with conditional random fields. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Storandt</author>
<author>Robert Hill</author>
</authors>
<title>Very mild senile dementia of the Alzheimer’s type: II Psychometric test performance.</title>
<date>1989</date>
<journal>Archives of Neurology,</journal>
<pages>46--383</pages>
<contexts>
<context position="4641" citStr="Storandt and Hill, 1989" startWordPosition="690" endWordPosition="693">egrees of supervision. Finally, we demonstrate improved utility in terms of using the automatic scores to classify elderly individuals as having Mild Cognitive Impairment. Ultimately we find that hybrid approaches, making use of both word-alignment and tagging models, yield strong improvements over either used independently. 2 Wechsler Logical Memory (WLM) task The Wechsler Logical Memory (WLM) task (Wechsler, 1997), a widely used subtest of a battery of neuropsychological tests used to assess memory function in adults, has been shown to be a good indicator of Mild Cognitive Impairment (MCI) (Storandt and Hill, 1989; Petersen et al., 1999; Wang and Zhou, 2002; Nordlund et al., 2005), the stage of cognitive decline that is often a precursor to dementia of the Alzheimer’s type. In the WLM, the subject listens to the examiner read a brief narrative and then retells the narrative twice: immediately upon hearing it and after about 20 minutes. The examiner grades the subject’s response by counting how many of the story elements the subject recalled. An excerpt of the text read by the clinician while administering the WLM task is shown in Figure 1. The story elements in the text are delineated using slashes, 25</context>
</contexts>
<marker>Storandt, Hill, 1989</marker>
<rawString>Martha Storandt and Robert Hill. 1989. Very mild senile dementia of the Alzheimer’s type: II Psychometric test performance. Archives of Neurology, 46:383–386.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qing-Song Wang</author>
<author>Jiang-Ning Zhou</author>
</authors>
<title>Retrieval and encoding of episodic memory in normal aging and patients with mild cognitive impairment.</title>
<date>2002</date>
<journal>Brain Research,</journal>
<pages>924--113</pages>
<contexts>
<context position="4685" citStr="Wang and Zhou, 2002" startWordPosition="698" endWordPosition="701">mproved utility in terms of using the automatic scores to classify elderly individuals as having Mild Cognitive Impairment. Ultimately we find that hybrid approaches, making use of both word-alignment and tagging models, yield strong improvements over either used independently. 2 Wechsler Logical Memory (WLM) task The Wechsler Logical Memory (WLM) task (Wechsler, 1997), a widely used subtest of a battery of neuropsychological tests used to assess memory function in adults, has been shown to be a good indicator of Mild Cognitive Impairment (MCI) (Storandt and Hill, 1989; Petersen et al., 1999; Wang and Zhou, 2002; Nordlund et al., 2005), the stage of cognitive decline that is often a precursor to dementia of the Alzheimer’s type. In the WLM, the subject listens to the examiner read a brief narrative and then retells the narrative twice: immediately upon hearing it and after about 20 minutes. The examiner grades the subject’s response by counting how many of the story elements the subject recalled. An excerpt of the text read by the clinician while administering the WLM task is shown in Figure 1. The story elements in the text are delineated using slashes, 25 elements in all. An example retelling is sh</context>
</contexts>
<marker>Wang, Zhou, 2002</marker>
<rawString>Qing-Song Wang and Jiang-Ning Zhou. 2002. Retrieval and encoding of episodic memory in normal aging and patients with mild cognitive impairment. Brain Research, 924:113–115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Wechsler</author>
</authors>
<title>Wechsler Memory Scale - Third Edition. The Psychological Corporation,</title>
<date>1997</date>
<location>San Antonio.</location>
<contexts>
<context position="1860" citStr="Wechsler, 1997" startWordPosition="267" endWordPosition="268">aking use of both automatic alignment and CRFs trained tagging models achieves the best performance, yielding strong improvements over using either approach alone. 1 Introduction Narrative production tasks are an essential component of many standard neuropsychological test batteries. For example, narration of a wordless picture book is part of the Autism Diagnostic Observation Schedule (ADOS) (Lord et al., 2002) and retelling of previously narrated stories is part of both the Developmental Neuropsychological Assessment (NEPSY) (Korkman et al., 1998) and the Wechsler Logical Memory (WLM) test (Wechsler, 1997). Such tests also arise in reading comprehension, second language learning and other computer-based tutoring systems (Xie et al., 2012; Zhang et al., 2008). The accuracy of automated scoring of a narrative retelling depends on correctly identifying which of the source narrative’s propositions or events (what we will call ‘story elements’) have been included in the retelling. Speakers may choose to relate these elements using diverse words or phrases, and an automated method of identifying these elements needs to model the permissible variants and paraphrasings. In previous work (Lehr et al., 2</context>
<context position="4437" citStr="Wechsler, 1997" startWordPosition="655" endWordPosition="657">a range of experimental conditions, including: variously adapting the ASR models to the domain; using maximum entropy models rather than CRFs; differing tagsets (BIO versus IO); and with varying degrees of supervision. Finally, we demonstrate improved utility in terms of using the automatic scores to classify elderly individuals as having Mild Cognitive Impairment. Ultimately we find that hybrid approaches, making use of both word-alignment and tagging models, yield strong improvements over either used independently. 2 Wechsler Logical Memory (WLM) task The Wechsler Logical Memory (WLM) task (Wechsler, 1997), a widely used subtest of a battery of neuropsychological tests used to assess memory function in adults, has been shown to be a good indicator of Mild Cognitive Impairment (MCI) (Storandt and Hill, 1989; Petersen et al., 1999; Wang and Zhou, 2002; Nordlund et al., 2005), the stage of cognitive decline that is often a precursor to dementia of the Alzheimer’s type. In the WLM, the subject listens to the examiner read a brief narrative and then retells the narrative twice: immediately upon hearing it and after about 20 minutes. The examiner grades the subject’s response by counting how many of </context>
</contexts>
<marker>Wechsler, 1997</marker>
<rawString>David Wechsler. 1997. Wechsler Memory Scale - Third Edition. The Psychological Corporation, San Antonio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shasha Xie</author>
<author>Keelan Evanini</author>
<author>Klaus Zechner</author>
</authors>
<title>Exploring content features for automated speech scoring.</title>
<date>2012</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="1994" citStr="Xie et al., 2012" startWordPosition="286" endWordPosition="289">r using either approach alone. 1 Introduction Narrative production tasks are an essential component of many standard neuropsychological test batteries. For example, narration of a wordless picture book is part of the Autism Diagnostic Observation Schedule (ADOS) (Lord et al., 2002) and retelling of previously narrated stories is part of both the Developmental Neuropsychological Assessment (NEPSY) (Korkman et al., 1998) and the Wechsler Logical Memory (WLM) test (Wechsler, 1997). Such tests also arise in reading comprehension, second language learning and other computer-based tutoring systems (Xie et al., 2012; Zhang et al., 2008). The accuracy of automated scoring of a narrative retelling depends on correctly identifying which of the source narrative’s propositions or events (what we will call ‘story elements’) have been included in the retelling. Speakers may choose to relate these elements using diverse words or phrases, and an automated method of identifying these elements needs to model the permissible variants and paraphrasings. In previous work (Lehr et al., 2012; Prud’hommeaux and Roark, 2012; Prud’hommeaux and Roark, 2011), we developed models based on automatic word-alignment methods, as </context>
</contexts>
<marker>Xie, Evanini, Zechner, 2012</marker>
<rawString>Shasha Xie, Keelan Evanini, and Klaus Zechner. 2012. Exploring content features for automated speech scoring. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaonan Zhang</author>
<author>Xiaonan Zhang</author>
<author>Jack Mostow</author>
<author>Jack Mostow</author>
<author>Nell Duke</author>
<author>Christina Trotochaud</author>
<author>Joseph Valeri</author>
<author>Al Corbett</author>
</authors>
<title>Mining free-form spoken responses to tutor prompts.</title>
<date>2008</date>
<booktitle>In Proceedings of the First International Conference on Educational Data Mining,</booktitle>
<pages>234--241</pages>
<contexts>
<context position="2015" citStr="Zhang et al., 2008" startWordPosition="290" endWordPosition="293">roach alone. 1 Introduction Narrative production tasks are an essential component of many standard neuropsychological test batteries. For example, narration of a wordless picture book is part of the Autism Diagnostic Observation Schedule (ADOS) (Lord et al., 2002) and retelling of previously narrated stories is part of both the Developmental Neuropsychological Assessment (NEPSY) (Korkman et al., 1998) and the Wechsler Logical Memory (WLM) test (Wechsler, 1997). Such tests also arise in reading comprehension, second language learning and other computer-based tutoring systems (Xie et al., 2012; Zhang et al., 2008). The accuracy of automated scoring of a narrative retelling depends on correctly identifying which of the source narrative’s propositions or events (what we will call ‘story elements’) have been included in the retelling. Speakers may choose to relate these elements using diverse words or phrases, and an automated method of identifying these elements needs to model the permissible variants and paraphrasings. In previous work (Lehr et al., 2012; Prud’hommeaux and Roark, 2012; Prud’hommeaux and Roark, 2011), we developed models based on automatic word-alignment methods, as described briefly in </context>
</contexts>
<marker>Zhang, Zhang, Mostow, Mostow, Duke, Trotochaud, Valeri, Corbett, 2008</marker>
<rawString>Xiaonan Zhang, Xiaonan Zhang, Jack Mostow, Jack Mostow, Nell Duke, Christina Trotochaud, Joseph Valeri, and Al Corbett. 2008. Mining free-form spoken responses to tutor prompts. In Proceedings of the First International Conference on Educational Data Mining, pages 234–241.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>