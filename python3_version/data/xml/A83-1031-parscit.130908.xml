<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<sectionHeader confidence="0.603266" genericHeader="abstract">
INTERACTIVE NATURAL LANGUAGE PROBLEM SOLVING:
A PRAGMATIC APPROACH
</sectionHeader>
<note confidence="0.526574">
** **
</note>
<author confidence="0.613484">
A. Biermann , R. Rodman , B. Ballard , T. Betancourt ,
</author>
<listItem confidence="0.763539">
•* a •
G. Bilbro , H. Dees , L. Fineman ,
• •
</listItem>
<author confidence="0.926887">
P. Fink , K. Gilbert , D. Gregory , F. Heidlage
</author>
<affiliation confidence="0.8053916">
* Department of Computer Science
Duke University
Durham, North Carolina
** Department of Computer Science
North Carolina State University
</affiliation>
<sectionHeader confidence="0.831753333333333" genericHeader="keywords">
Raleigh, North Carolina
INTRODUCTION
ABSTRACT
</sectionHeader>
<bodyText confidence="0.993302338235294">
A class of natural language proces-
sors is described which allow a user to
display objects of interest on a computer
terminal and manipulate them via typed or
spoken English sentences.
This paper concerns itself with the
implementation of the voice input facility
using an automatic speech recognizer, and
the touch input facility using a touch
sensitive screen. To overcome the high
error rates of the speech recognizer under
conditions of actual problem solving in
natural language, error correction
software has been designed and is
described here. Also described are prob-
lems involving the resolution of voice
input with touch input, and the identifi-
cation of the intended referents of touch
input.
To measure system performance we have
considered two classes of factors: the
various conditions of testing, and the
level and quality of training of the sys-
tem user. In the paper a sequence of five
different testing situations is observed,
each one resulting in a lowering of system
performance by several percentage points
below the previous one. A training pro-
cedure for potential users is described,
and an experiment is discussed which util-
izes the training procedure to enable
users to solve actual non-trivial problems
using natural language voice communica-
tion.
A class of natural language proces-
sors is under development which allow a
user to display objects of interest on a
computer terminal and manipulate them via
typed or spoken English imperative sen-
tences. Such a processor is designed to
respond within one to four seconds by exe-
cuting the input command and updating the
displayed world for user verification. If
an undesired action is observed, a
&amp;quot;backup&amp;quot; command makes it possible to undo
any action and return the system to a pre-
vious state. The domains of interest
include matrix computation, where one can
display tables of data and manipulate
them; office automation, where one can
work with texts, files, calendars, or mes-
sages; and machine control, where one
might wish to command a robot or other
equipment via natural language input.
The first such system (Biermann and
Ballard [61), called NLC, provides a
matrix computation facility and allows
users to display matrices, enter data, and
manipulate the entries, rows, and columns.
It became operative in 1979 and includes a
variety of special purpose features
1 This work was supported by National
Science Foundation Grants MCS 7904120 and
MCS 8113491, by the IBM Corporation under
GSD agreement no. 260880, and by the
Universite de Paris-Sud, Laboratoire de
Recherche en /nformatique during the sum-
mer of 1.482.
</bodyText>
<page confidence="0.993946">
160
</page>
<bodyText confidence="0.999953277777778">
including arbitrarily deep nesting of noun 181 events as they happen.
groups, extensive conjunction processing, A continuous program of human factors
user defined imperative verbs, and looping testing has been maintained by the project
and branching features. More recently, a in order to build a realistic view of
domain independent abstraction of the NLC potential users and to measure progress in
system has been constructed and now is achieving usability. For example, in a
being specialized to handle a text pro- test of the matrix computation system with
cessing task. In this system, text can be typed input, twenty-three subjects solved
displayed and modified or formatted with problems similar to those that might be
natural language commands. assigned in a first course in programming
Current work emphasizes the addition (Biermann, Ballard, and Sigmon [7]). In
of voice input, voice output, and a touch this test, the NLC system correctly pro-
sensitive display screen. Speech recogni- cessed 81 percent of the sentences and
tion is being done on an experimental users were quite satisfied with its gen-
basis with the Nippon Electric DP-200 Con- eral performance. Other tests of the sys-
nected Speech Recognizer in both discrete tem are described in Fink [14] and Geist
and connected speech modes, and with the et al. [15]. In another test (Fineman
Votan Corporation V-5000 Development Sys- [13]), a simulator for a voice driven
tem. The touch sensitive screen being office automation system was used to
used is a Carroll touch panel mounted on a obtain data on user behaviors when problem
19-inch color monitor. Voice response is solving is with discrete and slow con-
also provided by the Votan V-5000 which nected speech. It was found that users
assembles and vocalizes digitally recorded quickly adapted their speech to the
human voice messages. The work has pro- required discipline of slow, methodical,
gressed to the point where our natural and simple sentences which can be recog-
language matrix computer NLC is operative nized by machine. Since the data obtained
under voice control using the DP-200 and in any system test is heavily dependent on
the text processing system is beginning to the amount and kind of training given to
function using the V-5000 speech recog- subjects, it is necessary to have a stand-
nizer. The touch panel interface and ardized training procedure. In the
voice response systems are still in the current work, a voice tutorial has been
design phase. developed for training users to use a
The goal of the project is to make voice interactive system (Deas [11]).
possible voice and touch interactions of This paper reports on the current
the following kind: status of these projects with emphasis on
Retrieve file Budget83. system design, speech input facilities and
Find the largest number in this their performance, the touch input system
column and zero it. (with touch and human factors considerations.
input) SYSTEM OVERVIEW
Add this column putting the result The basic system design includes
here. (with two touch inputs) modules to do the following tasks:
Send this file to Jones and file it (1) token acquisition
as Budget83. (touch input) (2) parsing
That is, imperative sentences are to be (3) noun group resolution
processed that operate on domain objects (4) imperative verb execution
to produce modifications to the existing (5) flow-of-control semantics
objects or their relationship to each (6) system output
other. The objects are, for example, The token acquisition phase receives
rows, columns, numbers, entries, labels, typed inputs, word guesses from the voice
etc. in the matrix domain or sections, recognizer, and screen coordinates from
paragraphs, sentences, margins, pages, the touch panel. These inputs are prepro-
etc. in the text processing domain. The cessed and passed to the parser which uses
execution of each command is accompanied an augmented transition network to dis-
by an update of the displayed data with cover the structure of the command and the
highlighting to indicate changes. Prompts roles of the individual tokens. Voun
and error messages will be given by voice group resolution attempts to discover what
response. System design is aimed at domain objects are being referred to, and
allowing fast interactive control of the the verb execution module transforms thos
objects on the screen while the user main- objects as requested by the imperative
tains uninterrupted eye contact with the
verb. The flow-of-control semantics
module manages the execution of meta-
imperative verbs such as repeat, and han-
dles user-defined imperatives. Finally,
system output displays the state of the
world on the screen. Any module may issue
prompts and error messages via text or
spoken output. Backup from any given
module to an earlier stage may occur in
unusual situations. More details appear
in Ballard CO, Biermann [5], Biermann and
Ballard [6], and Ballard and Biermann [3].
</bodyText>
<sectionHeader confidence="0.922293" genericHeader="method">
SPEECH INPUT
</sectionHeader>
<bodyText confidence="0.999597409090909">
An automatic speech recognizer such
as the DP-200 or V-5000 recognizes speech
by means of pattern matching algorithms.
A subject is introduced to the device for
a training session, and asked to repeat
the various words of the vocabulary into a
microphone. The device extracts and
stores bit patterns corresponding to each
vocabulary word uttered by that particular
speaker. After training, when a speaker
wishes to use the device, the appropriate
bit patterns are loaded. Each utterance
of the speaker is compared with the pro-
ctored bit patterns and the best match
above a threshold limit is presented as
the recognized word. Depending on the
device being used, the speaker may be
required to talk with discrete or con-
nected speech. The results described
below were obtained primarily in the
discrete mode with a pause of at least 200
milliseconds after each word.
</bodyText>
<subsectionHeader confidence="0.933381">
Error Handling
</subsectionHeader>
<bodyText confidence="0.976844842105263">
The major difficulty facing users of
automatic speech recognition equipment is
the high error rate. Even the best dev-
ices in the best of circumstances are not
entirely free of error, and when cir-
cumstances are less than optimal, and more
like the real world, the error rate rises.
Thus, a good part of the project effort
has gone into coping with errors in recog-
nition. In our view the speech recogni-
tion device is a component of the larger
natural language computing system, and our
goal is to reduce the system error rate as
much as possible. We have therefore
designed error correction software that
corrects for certain kinds of errors, and
error messages that elicit repetition from
the human subject in less tractable cases.
Error correction essentially func-
tions by starting with a sequence of word
guesses from the input system and filter-
ing out the meaningless alternatives at
the appropriate stages of processing.
Beginning in the token acquisition phase,
certain unacceptable word sequences can be
disallowed. For example, a noun such as
&amp;quot;matrix&amp;quot; or &amp;quot;row&amp;quot; would be disallowed as
the first word in the sentence since this
is illegal in the system grammar. In the
parsing phase, a grammatical sequence of
words is selected from the incoming sets
of word guesses. Thus all ungrammatical
word sequences are eliminated. The parser
also disallows phrases containing certain
semantically unacceptable relationships
such as
the second row in 6.
or phrases containing disallowed opera-
tions such as
Add the matrix to 6.
In the noun group processor and later
stages, various other semantic errors can
be eliminated such as references to nonex-
istent objects or impossible operations.
For discrete mode operations, errors
are classified into four types:
a. Substitutions.
The device reports word B when
word A was actually spoken.
b. Rejections.
The device sends a rejection
code when a vocabulary word was
spoken.
c. Insertions.
The device reports a vocabulary
word when a non-vocabulary word,
or noise, was uttered:
</bodyText>
<listItem confidence="0.605819666666667">
d. Fusions. Two (or more) words are
spoken but only one word is
reported.
</listItem>
<subsectionHeader confidence="0.617618">
Substitution Errors
</subsectionHeader>
<bodyText confidence="0.999733095238095">
Substitution errors are the easiest
to correct since the substituted word
often resembles the actual word phoneti-
cally. Some of the substitutions are
fairly predictable, e.g. &amp;quot;by&amp;quot; for
&amp;quot;five&amp;quot;, &amp;quot;and&amp;quot; for &amp;quot;add&amp;quot;, or &amp;quot;up&amp;quot; for
&amp;quot;of&amp;quot;. We have coined the term synophone
to describe such sets. Many synophone
pairs are symmetrically interchangable:
however, some are not. For example, with
some speakers, the word &amp;quot;a&amp;quot; is fre-
quently reported as &amp;quot;eight&amp;quot; although the
converse seldom occurs.
Synophones of a particular word
utterance come from two sources: alter-
nate guesses offered by the recognition
device based on its pattern matching com-
putation, and a set of words stored in the
system that are known to be confused with
the selected word. Whenever a token is
collected by the scanner, its synophone
</bodyText>
<page confidence="0.99721">
182
</page>
<bodyText confidence="0.999493545454546">
list is compiled. Passing the complete
set of synophones for each word to the
parser would result in excessive parse
time so it is desirable to eliminate
beforehand any synophones whose occurrence
can be determined to be impossible based
on grammatical or contextual considera-
tions. For example the syntax of English
(and of NLC) prevents certain words from
occurring next to each other, or beginning
or ending sentences. This information is
recorded in a table of adjacencies. If
there is a synophone in a word slot that
cannot be preceded by any of the sync-
phones in the previous word slot that
synophone is deleted. This process is
repeated until no more deletions are pos-
sible. On average, roughly one-half of
the candidate synophones are deleted.
Since parsing time may increase exponen-
tially with the number of candidate syno-
phones, and this table driven elimination
process is very quick, considerable sav-
ings result.
For reasons of individual speech
variation some vocabulary words will have
synophones peculiar to an individual
speaker. The set of synophones of each
vocabulary word is therefore augmented to
accommodate this situation so that each
speaker has personalized synophone sets.
Early training includes a tutorial intro-
duction, part of which requires the sub-
ject to repeat sentences word for word.
In this mode, the software has a priori
knowledge of the correct token for each
word slot. If a given word slot does not
contain the correct token, the substituted
word can be added to the appropriate syno-
phone set for that subject. Thereafter,
if the same substitution error recurs dur-
ing a session with that subject, the
correct word will be included in the syno-
phone list for that word slot.
</bodyText>
<subsectionHeader confidence="0.525792">
Rejection Errors
</subsectionHeader>
<bodyText confidence="0.99952848">
The occurrence of one or more rejec-
tions in a sentence almost always results
in a request for repetition. However, we
are designing a number of facilities to
handle rejections. In some cases, the
rejected word can be determined from con-
text, and processing can continue uninter-
rupted. Otherwise, the current plan is to
handle a single rejection by returning an
audio response that repeats all of the
sentence with the word &amp;quot;what&amp;quot; in place of
the rejected element. The speaker will
then be able to choose to repeat the
rejected word or, in case other errors are
apparent, to repeat the entire utterance.
In cases of multiple rejection
errors, the speaker is requested to repeat
the entire utterance . In all cases previ-
ous utterances will not he discarded. The
scanner will merge them, complete with
synophones, in an attempt to eliminate
rejections and provide the broadest amount
of information from which to extract what
the speaker actually said. For example,
if the actual utterance were
</bodyText>
<sectionHeader confidence="0.693235" genericHeader="method">
ABCDEFG
</sectionHeader>
<bodyText confidence="0.980633">
and the recognizes returned
</bodyText>
<sectionHeader confidence="0.902451" genericHeader="method">
AB*ZE* G
</sectionHeader>
<bodyText confidence="0.939943142857143">
where * stands for rejection, the speaker
will be asked to repeat. If
A 8C*EFH
is then recognized, it will be combined
with the first utterance so that the
scanner considers the seven word slots to
contain:
</bodyText>
<equation confidence="0.925993">
s(A) s(B) s(c) s(z) s(E) s(F) s(G)
s(H)
</equation>
<bodyText confidence="0.99885475">
where s(X) is the union of X with its
synophones. (Hopefully D is in s(Z).) If
subsequent utterances are so different
from previous ones that they are unlikely
to be word-for-word repetitions (for exam-
ple, by containing a different number of
words), previous utterances will be dis-
carded and processing will be started
over.
It may also be possible to predict a
rejected word with some degree of cer-
tainty based on semantic or pragmatic
information. (We consider pragmatics to
involve discourse dependent contextual
factors.) For example suppose the scanner
receives from the recognizer:
</bodyText>
<subsectionHeader confidence="0.736878">
Oouble 4&apos; nine and add column four to it.
</subsectionHeader>
<bodyText confidence="0.99946155">
The most likely possibilities for the
rejection are entry, row and column.
Entry can be eliminatea-- on semantic
grounds since it is meaningless to add a
column to an entry. Row is semantically
possible, but pragmiTically less likely
than column since adding columns to
columns ismuch more common than adding
columns to rows. Thus column may be
chosen. Furthermore if the matrixin
focus is six by seven, then the nine is a
substitution error, and the sentence will
be rejected on pragmatic grounds ini-
tially. However, since five is a syno-
phone of nine the sentence ma- be tried
with fivi--in the place of nine. Ulti-
mateli-Mi user will see displq7073 on the
screen the result From:
Double column five and add column
four to it.
</bodyText>
<page confidence="0.995547">
183
</page>
<bodyText confidence="0.994302596774194">
The activity described above is tran-
sparent to the user. If the results are
unsatisfactory to the user, the command
&amp;quot;backup&amp;quot; will undo them.
An additional source of pragmatic
error correction comes from utterances in
historically similar dialogs. We are
developing a method for utilizing this
type of information. Considering the last
example, if the user had been adding
columns to rows quite frequPr&amp;quot;- in the
current and/or recent sessions, but rarely
if ever adding columns to columns, the
system would choose row as the rejected
word.
Insertion Errors and Fusion Errors
Most speech recognizers allow the
threshold value to be adjusted that deter-
mines whether the best match is &amp;quot;recog-
nized&amp;quot; or is rejected. Since rejections
are harder to correct for than substitu-
tions there is reason to lower this value.
Too low a value, however, aggravates the
insertion problem. When the speaker
utters a non-vocabulary word, or emits a
grunt or uncouth sound, the correct
response is a rejection. A non-rejection
in this situation may be difficult to deal
with.
In our experience users have little
trouble in confining themselves to the
trained vocabulary. Most insertion errors
occur between sentences, rather than
between words within a sentence. This
results in extraneous &amp;quot;words&amp;quot; in the first
one or two word slots. These can often be
eliminated because neither they nor their
synophones can begin a sentence in the NLC
grammar. Timing considerations, too,
could be used to eliminate, or at least
cast suspicion on, inter-sentence inser-
tions, though we have not found the need
for such measures.
We have observed fusion errors in
discrete mode. They arise when the
speaker neglects to pause long enough
between words. In our experience they
occur so infrequently we have not tried to
compensate for them. This type of error
is more crucial when operating in con-
nected mode. It may be the case that two
(or possibly more) words are reported as a
single word different from either of the
two originally uttered words. It may also
happen that two words, A and B. are
reported as either A or B. In this case
the fusion error takes on the appearance
of an omission. Our connected speech
parser, currently under construction, will
have the ability to guess an omission and
insert a correction if sufficient contex-
tual information is dvAilable.
</bodyText>
<sectionHeader confidence="0.440521" genericHeader="method">
Raw Error Rate
</sectionHeader>
<bodyText confidence="0.999863904761905">
Although a good deal of our inLerest
is in correcting or compensating for the
various kinds of errors in recognition, we
are also working on ways to reduce the
actual number of errors made by the recog-
nition devices (the raw error rate).
Careful vocabulary choice and proper tun-
ing of the hardware such as threshold
level selections are crucial factors.
It is important to choose vocabulary
words as widely separated phonetic.Ally as
circumstances allow. Additionally, we
have found that words containing non-
strident fricatives (e.g. the th in
fifth), affricates (e.g. the ch in
EHIRC-h), liquids (r and 1) and nasals (m,n
iga712) are more difficult to recognize
than words containing other sounds.
Monosyllabic words, in general, are not
recognized as readily as polysyllabic
ones, though words that are long and dif-
ficult to pronounce (e.g. anaesthetist)
are also to be avoided. Often the domain
leaves little latitude for vocabulary
choice. If ordinal numbers are needed it
is necessary to have fifth and sixth,
which are difficult to distinguish. But
instead of a word like rate which is
easily confused with eight, taxrate or
rate-of-21y (pronounced as a siT(ire word)
gigEt be a better choice.
Correct training procedures are
instrumental in reducing the raw error
rate as are such factors as whether the
user receives immediate feedback from the
recognizer, the form and frequency of
error messages requesting repetition, and
the degree of comfort felt by the user
insofar as attitude toward computers is
concerned. Some of these are discussed
below in the section Measuring System Per-
formance.
</bodyText>
<subsectionHeader confidence="0.923375">
Some Miscellaneous Questions
</subsectionHeader>
<bodyText confidence="0.99997675">
Apart from error correction, a number
of other questions have arisen during our
implementation of the voice driven system.
Among these are:
</bodyText>
<listItem confidence="0.795804">
a) How is the beginning of a sen-
tence detected?
b) How is the end of a sentence
detected?
c) How can a user make a correction
in mid-sentence?
</listItem>
<bodyText confidence="0.953376166666667">
Currently a sentence begins with any
input after the end of the previous sen-
tence. The instances of inter- or pre-
sentence insertions were discussed above.
Sentences are terminated by the
meta-word over. This word has few syno-
</bodyText>
<page confidence="0.995359">
184
</page>
<bodyText confidence="0.999436467741936">
phones in the current word set and has the
advantage of being widely understood to
mean &amp;quot;end of transmission.&amp;quot; However, we
plan to experiment with other kinds of
termination such as use of touch input or
timing information.
A user may misspeak in instructing
the computer to perform a task and may
wish to repeat all or part of the command.
Also, if the words from the voice recog-
nixer are displayed as they are spoken,
the user may desire to correct a misrecog-
nition. The metaword correction is
currently used to implement this facility.
There are several levels of correction.
Some may be accomplished by the scanner,
while others require more information than
is available to the scanner and must
therefore be handled by the parser. The
simplest type of correction consists of
changing one word at the end of the sen-
tence:
Add row one to row four
correction three.
Here the scanner merely deletes the word
slot before the metaword. If several
words follow &amp;quot;correction&amp;quot; as in
Add row one to row two correction
row one to column three.
the scanner detects this fact and scans
backward in the sentence, attempting to
match the largest possible number of word
slots before and immediately after the
metaword. In this example the tokens for
row, one and to match, so the scanner
copies-Tge last part of the sentence into
the earlier part of the buffer to arrive
at
Add row one to column three.
In the case of an utterance such as
Add row one to row two
correction column three.
it is impossible to match the tokens
before and after the metaword. The
scanner therefore deletes the token
innadiately before the metaword, flags the
word slot preceding that token and passes
the result to the parser. In the example,
Add row one to row column three.
is passed, with the word slot containing
row flagged. The parser attempts to make
sense of the set of tokens passed. If it
cannot, the flagged word slot is deleted,
the word previous to it is flagged and
another parse is attempted. The process
is repeated until a successful parse is
found. If none is found, an error message
is issued. Thus in the example, after
failing to parse the tokens as passed, the
parser tries
Add row one to column three.
which is parsed successfully.
</bodyText>
<sectionHeader confidence="0.943704" genericHeader="method">
TOUCH INPUT
</sectionHeader>
<bodyText confidence="0.993978341463415">
An important aspect of natural
language communication is pointing, which
Is often used in connection with words
such as this, that, here and there.
Pointing maT7Unctr3R-as iiiii5Easis, as in
Put the dog out.
where either the dog, the outside, or pos-
sibly both are pointed to. Pointing also
functions to put objects into focus,
allowing subsequent references to use a
definite pronoun: for example,
Move that there and cover it.
with a point to the object to be moved and
covered.
A pointing ability would fit in very
nicely with voice driven NLC and our pro-
ject includes a touch sensitive screen so
that the user can say &amp;quot;double this&amp;quot;, point
to a row, and cause the processor to dou-
ble every element in that row. More com-
plex sentences such as
Add this row to that row putting
the results here. (with three
touches)
also become possible.
Apart from being &amp;quot;natural&amp;quot; in the
sense that ordinary language users point
often, pointing may increase the effi-
ciency of communication.
There has been a good deal of
interest among human factors scientists as
to the efficiency of various modes of com-
munication. Past experiments, for exam-
ple, have compared the efficiency of typed
versus voice messages (voice messages are
more efficient). We carried out an exper-
iment to verify the hypothesis that voice
input together with touch input is more
efficient than voice input alone, and we
attempted to quantify the results. we
solved eight different types of matrix
</bodyText>
<page confidence="0.997932">
185
</page>
<bodyText confidence="0.999949789473684">
problems including Gaussian elimination,
divided differences and matrix inversion,
using MLC without touch. We then went
back and rewrote the solutions using the
touch facility, but without any other
changes. On the average 29% fewer words
were needed to solve the problem, and
individual sentences were shortened by
23%.
A number of interesting problems
arise when a touch facility is imple-
mented. One is how to pair up tactile and
verbal input in the way intended by the
user. Another problem is identifying the
actual object the user intends to refer to
once the tactile and verbal input have
been resolved.
An example of the latter problem
would be the command
</bodyText>
<subsectionHeader confidence="0.778162">
Double this
</subsectionHeader>
<bodyText confidence="0.98700875">
accompanied by a touch of element &lt;3,2&gt; of
a displayed matrix. Does the user want to
double element &lt;3,2&gt;, double row 3, double
column 2, or even double the entire
matrix? The same touch paired with
Double this entry.
Double this matrix.
Double this column.
</bodyText>
<subsectionHeader confidence="0.535411">
Or
</subsectionHeader>
<bodyText confidence="0.990019974358975">
0ouble this matrix.
would be unambiguous. If the demonstra-
tive is not accompanied by a nominal some
strategy is needed to process the sen-
tence. We opt for the smallest possible
noun group encompassed by the touch (the
&lt;3,2&gt; entry in the above case), and rely
on our &amp;quot;backup&amp;quot; facility in case the
user&apos;s intentions are not fulfilled. If
the utterance &amp;quot;double this&amp;quot; is accompanied
by a touch of the displayed name of a row,
column or matrix, then the named object
will be referenced.
Pairing up touches with spoken
phrases is straightforward when a single
noun group is used with-a single touch, as
in &amp;quot;double this entry.&amp;quot; In a more compli-
cated case we might have
Add this entry to that row
and put the result here.
accompanied by three touches. The stra-
tegy here is to -.air touches and utter-
ances in the order given by the user.
tioned to establish focus or resole noun
group reference. If the emphasis function
of touch is mixed in, a more difficult
situation arises. If three touches accom-
pany
Add this entry to the first row
and put the result here.
then the second touch was presumably to
emphasize the first row or even to estab-
lish a rhythm of touching. In any case
the facility to match touches with non-
deictic expressions is needed. If only
two touches accompany this last sentence
then the focusing function should take
precedence, and the touches should be
matched with &amp;quot;this entry&amp;quot; and &amp;quot;here.&amp;quot;
The situation is made even more com-
plex by the ability to establish focus
verbally. In NLC the user can say
Consider row four.
Double that row.
and the expression &amp;quot;that row&amp;quot; will refer
to row four. If the same utterance is
accompaqied by a touch to a row other than
four a potential conflict results. Our
strategy is to give precedence to touch,
since it is the more immediate focussing
mechanism. Thus the sequence
Consider row four.
Double that row. (touching row three)
will result in the doubling of row three.
When both verbal and touch focus are
present, nearly unresolvable ambiguities
may result. The sequence
Consider row four.
Add this row to that row.
accompanied by one touch, gives rise to
the problem as to which demonstrative noun
group to associate with row four, and
which to associate with the touch. One
strategy is to associate with a demonstra-
tive noun group the touch that occurred
closest to the time of utterance. Another
possible strategy is to assume that the
expression with that refers to the more
distant element in focus (the one esta-
blished verbally in this case). This
takes advantage of the fact that this and
that can be distinguished in English gram-
by the feature +NEAR. Unfortunately
by a simple change i stress pattern a
speaker can undo this fairly weak regular-
ity. Thus the sequence
Consider row four.
In the last example all touches func- Add this row to th&apos;at row.
</bodyText>
<page confidence="0.997721">
186
</page>
<bodyText confidence="0.986593674418605">
plus a single touch, where this bears pri-
mary stress and that bears secondary
stress, should find ti—touch referring to
&amp;quot;this row.&amp;quot; If the stress pattern were
Add this row to that row.
with primary stress on Add, the touch
would more likely be assoTated with that
row. It is unfortunate that to date we
Zia, of no voice equipment sensitive
enough to distinguish between two such
stress patterns.
Somewhat more complicated cases are
possible:
Consider row three.
Add this row to that row and
put the result in the first row.
accompanied by two touches. Since we
allow a touch to occur with expressions
such as &amp;quot;the first row,&amp;quot; and since it is
possible to disregard the element in ver-
bal focus altogether, such a case produces
multiple ambiguities. Although we foresee
being able to resolve these ambiguities
effectively, and can always fall back on
our &amp;quot;backup&amp;quot; facility in case of mistakes,
we also believe that such complex cases
will be extremely rare. No sentence of
such complexity was produced in our solu-
tions to the eight problems mentioned
above. With a voice and touch facility,
sentences tend to be shorter and simpler.
NLC has implemented plurals, but we
have not considered their use in touch
input. Such sentences as
Multiply these elements by
this element.
Or
Add these elements up.
with multiple touches, would be useful.
In the trial run of eight problems, the
introduction of plurality resulted in up
to fifty percent reduction in number of
words needed and sentence length.
</bodyText>
<sectionHeader confidence="0.981534" genericHeader="method">
MEASURING SYSTEM PERFORMANCE
</sectionHeader>
<bodyText confidence="0.998220578947368">
Progress in any endeavor is greatly
aided if the level of accomplishment can
be measured in some meaningful way. It is
desirable to give a figure of merit for a
system both so that a project can indicate
to the world the degree of the achievement
and also so that the project can inter-
nally judge its own improvements over
time. In voice language processing, one
can attempt to measure performance by the
word and sentence error rates. However,
experience shows that these measures are
highly dependent on two factors and that
almost any level of performance can be
reached if those factors are appropriately
adjusted. Those factors are
(a) the environment and type of test
within which the measurement is
made, and
</bodyText>
<listItem confidence="0.5015535">
(b) the level of training of the
system user.
</listItem>
<subsectionHeader confidence="0.658889">
Type of Testing Environment
</subsectionHeader>
<bodyText confidence="0.9998924">
Considering (a), we tend to classify
the type of test for a recognizer into one
of the following five categories and we
expect significant differences in device
response in each case.
</bodyText>
<listItem confidence="0.995223785714286">
(1) Lists of words are read in tests
performed by the manufacturer.
(2) Lists of words are read in our
laboratory.
(3) Sentences are read in our labora-
tory. (discrete or connected)
(4) Sentences are uttered in a prob-
lem solving situation in our
laboratory. (discrete or con-
nected)
(5) Sentences are uttered in a prob-
lem solving situation in the user
environment. (discrete or con-
nected)
</listItem>
<bodyText confidence="0.984972523809524">
In the first situation, a manufac-
turer is interested in advertising the
best performance achievable. Tests are
performed in controlled conditions with
microphone placement and all system param-
eters set for optimum performance, and an
expert speaker is used. In our labora-
tory, we are not interested in the best
possible system performance but rather
what we can realistically expect. The
parameters are set at medium levels, there
is some ambient noise, the microphone may
move during the test, and the user will be
anyone we happen to bring in regardless of
their speech characteristics.
As soon as the sequential words
become organized as sentences, situation
(3), the speaker begins to impose inflec-
tions on the utterance that will affect
recognition. Certain words may be
stressed, and intonation may rise ani fall
</bodyText>
<page confidence="0.995579">
187
</page>
<bodyText confidence="0.995173338028169">
as the sequential parts of each sentence
are voiced. Training samples based on
reading lists of vocabulary items tend to
be inaccurate templates for words spoken
in context. When sentences are spoken in
a problem solving environment, situation
(4), these effects increase and other
aspects of word pronunciation change.
When voice control stops being the central
concern of the speaker, larger variations
in speech are bound to occur with accom-
panying larger error rates.
The most difficult situation of all
occurs in situation (5) where the user
might not even be a person who could be
brought into a voice laboratory. In this
case, the user has only one concern,
achieving the desired machine performance.
Encouragement to speak carefully could be
met with impatience, and a few system
errors could result in even worse speech
quality and further degraded performance.
Our experience has been that word
error rates increase from about three to
seven percent as one moves to each more
difficult situation type depending on the
vocabulary, the equipment, and other fac-
tors. Constquently, we tend to distrust
any figures gathered in the easier classes
of environments and attempt to do our own
testing in the more difficult and more
interesting situations. Most of our
recent data is of type (4) and we hope to
gain some type (5) experience in the com-
ing year.
Training the System User
The second major factor affecting
voice recognition performance is the level
of training of the system user. Humans
are extremely adaptive and capable of
learning behaviors to a high degree of
perfection. Thus the designer of a voice
system might, over the years, learn to
chat with it like an old friend whereas
others might not be able to use the system
at all. Again, almost any level of system
performance can be observed depending on
the quality of training of the user.
Our approach to controlling this fac-
tor has been to develop a standardized
training procedure and to only report
statistics on uninitiated users whose
experience with the system is limited to
this procedure. Ideally this procedure
would be administered by machine to obtain
maximum uniformity in training but this
has not yet been possible.
The training procedure has two parts.
The first part is an informal session in
which the user is told how to speak indi-
vidual words to the system and examples of
the complete vocabulary are collected by
the recognition system. The second part
is administered very mechanically by read-
ing a tutorial document to the user and
requesting the utterance of trial sen-
tences. This portion of the training
introduces the user to the interactive
system&apos;s capabilities and is specifically
designed to be administered by the
machine.
</bodyText>
<subsectionHeader confidence="0.980975">
Some Performance Data
</subsectionHeader>
<bodyText confidence="0.999908814814815">
An experiment was run during the sum-
mer of 1982 to obtain DP-200 performance
data in an environment of type (4) as
described above. Because no voice
interactive system was yet available, a
system simulation was used. After the
first part of the training session in
which the voice samples were collected,
the subject was placed in a room behind a
display terminal with a head mounted
microphone. The voice tutorial was read
to the subject through a loudspeaker at
the terminal introducing the capabilities
of the simulated system and the types of
voice commands that could be executed.
The subject&apos;s commands were recognized by
the DP-200 and executed by the simulation.
Thus each user command resulted in either
appropriate action visible on the screen
or a voice error message. In the final
portion of the experiment, the subject was
asked to solve an invoice problem that
involved computing costs for a series of
individual items and finding the tax and
total. The experiment gave a reasonably
accurate simulation of the expected NLC
system behavior when it becomes completely
voice interactive. The experiment
attempted to simulate a syntactic level of
voice error correction but nothing deeper.
It was found that the DP-200 word
error rate rose to about 20 percent in
this test with about 14 of the 20 percent
being automatically correctable. The
vocabulary size was BO, with three samples
of most words, and six samples of a few of
the difficult words, stored in the DP-200.
This means that roughly every two to four
sentences will have a single word error
not correctable at shallow levels. This
data comes from the first two hours of
usage for these subjects and we expect
significant improvement as usage experi-
ence increases over time.
More recently, the NLC system has
become operative in a voice driven mode
and subject testing has begun using the
same training procedure. It is too early
to report results but it appears that the
performance predicted in the simulation
will be approximately achieved. This
experiment will include longer usage by
the subjects and thus indicate how much
error rates decrease over time.
</bodyText>
<page confidence="0.995634">
188
</page>
<bodyText confidence="0.999913571428572">
In conclusion, we have at this time
only fragmentary information regarding
what levels of performance can be
achieved. However, we have developed some
tools for making measurements and will
report the results as they become avail-
able.
</bodyText>
<sectionHeader confidence="0.991183" genericHeader="conclusions">
OTHER WORK
</sectionHeader>
<bodyText confidence="0.934952516666667">
Much of the applied work in natural
language processing has concerned database
query (Bronnenberg et al. [8], Codd[9],
Harris[17,18], Hendrix[22], Mylo-
poulos[27], Plath[29], Thompson and Thomp.-
son[32], Waltz[35], and Woods et al.
C36]). At least one such system is being
marketed (namely INTELLECT (181), while
several others have been successfully used
in pilot studies. (Damerau[10], Egly and
Wescourt[12], Hershman et al. [24].
Krause[25], Tennant[311).
As described in this paper, our ini-
tial work with NLC involved programming as
an application area, while our more recent
interest has shifted toward office
domains. However, as Petrick[28]
observes, many of the same technical prob-
lems arise regardless of application area.
For the most part, the imperative sentence
structures we are dealing with are simpler
than the question forms recognized by the
database systems cited above, while our
noun phrases tend to exhibit more ela-
borate structures. Furthermore, whereas
typical database symtems process each
input separately, or perhaps seek to han-
dle ellipsis by consulting the immediately
preceding input, we build up a richer
semantic context as a session proceeds to
be used in handling matters such as focus
and pronoun resolution.
The most distinctive features of our
present work are (a) the inclusion of
voice input and output facilities, and (b)
an attempt to deal with relatively &amp;quot;deep&amp;quot;
relationships among domain objects. A
more detailed discussion of the domain-
independent mechanisms appears in Bier-
mann(5), and as described in Ballard [21
the related LDC project being conducted in
our laboratory is built around many of
these techniques. Similar research pro-
jects which are moving away from a fixed
database setting include work by Haas and
Hendrix[16], Heidorn[201. Hendrix and
Lewis[231, and Thompson and Thompson (33).
During the 1970&apos;s a number of speech
understanding systems were developed under
ARPA support (Lea C26), Reddy [301, Walker
[34], Woods [37]) and currently some sys-
tems are being built in other countries,
for example [191. However, none of these
systems has been refined to the point that
it could actually support user interac-
tions in real time as we are attempting to
do. Our project uses well developed
speaker dependent voice recognition equip-
ment with a small enough vocabulary to
achieve usable accuracy rates.
</bodyText>
<sectionHeader confidence="0.999616" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.9996346875">
C11 B.W. Ballard, &amp;quot;Semantic and Pro-
cedural Processing for a Natural
Language Programming System,&amp;quot; Ph.D.
Dissertation, Report CS-1979-5, Dept.
of Computer Science, Duke University,
Durham, NC, 1979.
B.W. Ballard. &amp;quot;A Domain-Class
Approach to Transportable Natural
Language Processing,&amp;quot; Cognition and
Brain Theory. 5. pp. 269-287, 1982.
B.W. Ballard and A.W. Biermann, &amp;quot;Pro-
gramming in Natural Language: MLC dS
Prototype,&amp;quot; Proceedings of the 1979
ACM National Conference, October,
T479.
A.W. Biermann. &amp;quot;A Natural Language
Processor for Office Automation,&amp;quot;
Proceedings of the 1982 Office Auto-
mation Conigrence, San Francisco,
California, April, 1982.
A.W. Biermann, &amp;quot;Natural Language Pro-
gramming,&amp;quot; to appear in Com?uter Pro-
gam Synthesis Methodologies (eai:
Biermann and Guiho), Reidel, f983.
A-W. Biermann and B.W. Ballard,
&amp;quot;Towards Natural Language Computa-
tion,&amp;quot; American Journal of Computa-
tional Linguistics, Vol. 6, No. 2,
557-7T-86, 1580.
[71 A.W. Biermann, R.W. Ballard, and A.H.
Sigmon, &amp;quot;An Experimental Study of
Natural Language Programming,&amp;quot; to
appear in International Journal of
Man-Machine Studies, 1983.
rel W. Bronnenberg, S. Landsbergen, R.
Scha, and W. Schoenmaker, &amp;quot;PHL/0A-1,
A Question-Answering System for
Data-Base Consultation in Natural
English,&amp;quot; Philips Tech. Rev., 38, pp.
229-239 and 2 - 847-T97K-1979.
E.F. Codd, &amp;quot;Seven Steps to RENDEVOUS
with the Casua&apos;. User,&amp;quot; IBM Report
J1333, 1974.
[10] F.J. Damerau, &amp;quot;Operating Statistics
for the Transformational Question
Answering System,&amp;quot; American Journal
of Computational LinFiTiETC7, 1713T7-77
No. 1, pp. 30-42, 1981.
</reference>
<page confidence="0.723429">
[9]
189
</page>
<reference confidence="0.999192156521739">
Ell] H. Deas, M.Sc. Thesis, Dept. of Com-
puter Science, Duke University. Dur-
ham, N.C., November 1982.
[12] D. Egly and K. Wescourt, &amp;quot;Cognitive
Style, Categorizations, and Voca-
tional Effects on Performance of REL
Database Users,&amp;quot; Joint Conference on
Easier and More Productive Use Fa
Computing Systems, Ann Arbor,-Michi=
gan, May 1981.
[13] L. Fineman, &amp;quot;Preliminary Results on
the Voice Driven Information System
Simulation Experiment,&amp;quot; Report to IBM
Corporation, Dept. of Computer Sci-
ence, Duke University, Durham, N.C.,
1981.
[14] P.K. Fink, &amp;quot;Conditionals in a Natural
Language System&amp;quot; (Master&apos;s Thesis),
Report CS-1981-8, Duke University,
Durham, N.C., 1981.
[15] R. Geist, D. Kraines, and P. Fink,
&amp;quot;Natural Language Computing in a
Linear Algebra Course.&amp;quot; Proceedings
of the National Educational Computing
c3nTe7ence, June, 1982.
[16] N. Haas and G. Hendrix, &amp;quot;An Approach
to Acquiring and Applying Knowledge,&amp;quot;
First National Conference on Artifi-
CITT-ITIFFITit7ince, 1990.
[17] L.R. Harris, &amp;quot;User Oriented Data Base
Query with the ROBOT Natural Language
Query System,&amp;quot; International Journal
of Man-Machine Studies, pp. 697-713,
septiiffier 1977.
(18] L. Harris, &amp;quot;The ROBOT System:
Natural Language Processing Applied
to Database. Query,&amp;quot; Proceedings of
the 1978 ACM National Conference, PP.
TAT-1777 T178.
[19] J.P. Haton and J.M. Pierrel, &amp;quot;Data
Structures and Organization of the
MYRTILLE II System,&amp;quot; Fourth
T./.C.P.R., Kyoto, Japan, 1978.
[20] G. Heidorn, &amp;quot;Natural Language Dialo-
gue for Managing an On-Line Calen-
dar,&amp;quot; IBM Research Report RC7447,
1978.
[211 G.G. Hendrix, E.D. Sacerdoti, D.
Sagalowicz, and J. Slocum, &amp;quot;Develop-
ing a Natural Language interface to
Complex Data,&amp;quot; ACM Transactions on
Database Systems, Vol. 3, No. 2, PP.
105-147, 191.
C22] G.G. Hendrix, &amp;quot;Human Engineering for
Applied Natural Language Processing,&amp;quot;
Fifth International Conference on
Artificial Intelligence, pp. 183-191,
1977.
[23] G. Hendrix and W. Lewis, &amp;quot;Transport-
able Natural Language Interfaces to
Databases,&amp;quot; Annual Meeting of the
Assoc. for Computational Linguistics,
1981.
[24] R. Hershman, R. Kelly, and H. Miller,
&amp;quot;User Performance with a Natural
Language Query System for Command
Control,&amp;quot; NPRDC TR 7R-7, Navy Person-
nel Research and Development Center,
San Diego, California, January 1979.
[25) J. Krause, &amp;quot;Results of a User Study
with the &apos;User Specialty Language,&apos;
System and Consequences for the
Architecture of Natural Language
Interfaces,&amp;quot; Technical Report
79.04.003, IBM Heidelberg Scientific
Center, May 1979.
(26] W.A. Lea (Ed.), Trends in Speech
Recognition, Prenti-67i=f710:1, 1982.
[27] J. Mylopoulos, A. Borgida, P. Crthen,
N. Roussopoulos, J. Tsotsos, and H.
Wong, &amp;quot;TORUS - A Natural Language
Understanding System for Data manage-
ment,&amp;quot; Proceedings of the Fourth
International Conference on AFETTT=
cial Intelligence, 1975.
r281 S.R. Petrick, &amp;quot;On Natural Language
Based Computer Systems,&amp;quot; IBM Journal
of Research and Development, Vol. 20,
113.-47-5177-3N=335, 1976.
(29] W.J. Plath, &amp;quot;REQUEST: A Natural
Language Question-Answering System,&amp;quot;
IBM Journal of Research and Develop-
Vol. 20, No. 4, pp. 326-335,
WM.
C301 D.R. Reddy, &amp;quot;Speech Recognition by
Machine: A Review,&amp;quot; Proceedings of
the IEEE, Vol. 64, No. 4, pp. S01-
T. &apos;T76.
C311 H. Tennant, &amp;quot;Experience with the
Evaluation of Natural Language Ques-
tion Answerers,&amp;quot; Working Paper 18,
Advanced Automation Group, Coordi-
nated Science Lab., U-iv. of Illi-
nois, January 1979.
(32] F.B. Thompson and B.H. Thompson,
&amp;quot;Practical Natural Language Process-
ing: The REL System as Prototype,&amp;quot;
in Advances in Computers, Vol. 13
(Eds7-q7-06ini-irf and M.C. Yovits),
Academic Press, New York, 1975.
1331 F. Thompson and B. Thompson, &amp;quot;Shift-
ing to a Higher Gear in a Natural
Language System,&amp;quot; AFIRs Proc. of the
National Computer Conf., Vol. 50, pp.
657-662, 1981.
</reference>
<page confidence="0.972872">
190
</page>
<reference confidence="0.998228904761905">
C341 D.E. Walker (ed.), Understanding Spo-
ken Language, Elsevier North-Holland,
New York, 1978.
[35] D.L. Waltz, &amp;quot;An English Language
Question Answering System for a Large
Relational Database,&amp;quot; Communications
of the ACM, Vol. 21, No. 7, pp. 526-
39,
(36) W.A. Woods, R.N. Kaplan, and B.
Nash-Webber, &amp;quot;The Lunar Sciences
Natural Language Information System:
Final Report,&amp;quot; Report 2378, Bolt,
Beranek, and Newman, Cambridge, MA.,
1972.
C373 W.A. Woods, &amp;quot;Motivation and Overview
of SPEECHLIS: An Experimental Proto-
type for Speech Understanding
Research,&amp;quot; IEEE Transactions on
Acoustics, SpaTEK, and Signal Pro-
Etwirxi7— vol. ASSP-777 No. 1, pp7-77
10, 1976.
</reference>
<page confidence="0.998481">
191
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.102422">
<title confidence="0.905447666666667">INTERACTIVE NATURAL LANGUAGE PROBLEM SOLVING: A PRAGMATIC APPROACH ** **</title>
<author confidence="0.823372">B Ballard</author>
<title confidence="0.833378">a •</title>
<author confidence="0.779933333333333">P Fink</author>
<affiliation confidence="0.9985285">Department of Computer Science Duke University</affiliation>
<address confidence="0.648479">Durham, North Carolina</address>
<affiliation confidence="0.9920045">Department of Computer Science North Carolina State University</affiliation>
<address confidence="0.976486">Raleigh, North Carolina</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C11 B W Ballard</author>
</authors>
<title>Semantic and Procedural Processing for a Natural Language Programming System,&amp;quot;</title>
<date>1979</date>
<tech>Ph.D. Dissertation, Report CS-1979-5,</tech>
<institution>Dept. of Computer Science, Duke University,</institution>
<location>Durham, NC,</location>
<marker>Ballard, 1979</marker>
<rawString>C11 B.W. Ballard, &amp;quot;Semantic and Procedural Processing for a Natural Language Programming System,&amp;quot; Ph.D. Dissertation, Report CS-1979-5, Dept. of Computer Science, Duke University, Durham, NC, 1979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B W Ballard</author>
</authors>
<title>A Domain-Class Approach to Transportable Natural Language Processing,&amp;quot;</title>
<date>1982</date>
<journal>Cognition and Brain Theory.</journal>
<volume>5</volume>
<pages>269--287</pages>
<marker>Ballard, 1982</marker>
<rawString>B.W. Ballard. &amp;quot;A Domain-Class Approach to Transportable Natural Language Processing,&amp;quot; Cognition and Brain Theory. 5. pp. 269-287, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B W Ballard</author>
<author>A W Biermann</author>
</authors>
<title>Programming in Natural Language: MLC dS Prototype,&amp;quot;</title>
<date></date>
<booktitle>Proceedings of the 1979 ACM National Conference,</booktitle>
<marker>Ballard, Biermann, </marker>
<rawString>B.W. Ballard and A.W. Biermann, &amp;quot;Programming in Natural Language: MLC dS Prototype,&amp;quot; Proceedings of the 1979 ACM National Conference, October, T479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A W Biermann</author>
</authors>
<title>A Natural Language Processor for Office Automation,&amp;quot;</title>
<date>1982</date>
<booktitle>Proceedings of the 1982 Office Automation Conigrence,</booktitle>
<location>San Francisco, California,</location>
<marker>Biermann, 1982</marker>
<rawString>A.W. Biermann. &amp;quot;A Natural Language Processor for Office Automation,&amp;quot; Proceedings of the 1982 Office Automation Conigrence, San Francisco, California, April, 1982.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A W Biermann</author>
</authors>
<title>Natural Language Programming,&amp;quot; to appear in</title>
<booktitle>Com?uter Progam Synthesis Methodologies (eai: Biermann and Guiho),</booktitle>
<location>Reidel, f983.</location>
<marker>Biermann, </marker>
<rawString>A.W. Biermann, &amp;quot;Natural Language Programming,&amp;quot; to appear in Com?uter Progam Synthesis Methodologies (eai: Biermann and Guiho), Reidel, f983.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A-W Biermann</author>
<author>B W Ballard</author>
</authors>
<title>Towards Natural Language Computation,&amp;quot;</title>
<journal>American Journal of Computational Linguistics,</journal>
<volume>6</volume>
<pages>557--7</pages>
<marker>Biermann, Ballard, </marker>
<rawString>A-W. Biermann and B.W. Ballard, &amp;quot;Towards Natural Language Computation,&amp;quot; American Journal of Computational Linguistics, Vol. 6, No. 2, 557-7T-86, 1580.</rawString>
</citation>
<citation valid="true">
<title>An Experimental Study of Natural Language Programming,&amp;quot; to appear in</title>
<date>1983</date>
<journal>International Journal of Man-Machine Studies,</journal>
<marker>1983</marker>
<rawString>[71 A.W. Biermann, R.W. Ballard, and A.H. Sigmon, &amp;quot;An Experimental Study of Natural Language Programming,&amp;quot; to appear in International Journal of Man-Machine Studies, 1983.</rawString>
</citation>
<citation valid="false">
<authors>
<author>rel W Bronnenberg</author>
<author>S Landsbergen</author>
<author>R Scha</author>
<author>W Schoenmaker</author>
</authors>
<title>PHL/0A-1, A Question-Answering System for Data-Base Consultation in Natural English,&amp;quot;</title>
<journal>Philips Tech. Rev.,</journal>
<volume>38</volume>
<pages>229--239</pages>
<marker>Bronnenberg, Landsbergen, Scha, Schoenmaker, </marker>
<rawString>rel W. Bronnenberg, S. Landsbergen, R. Scha, and W. Schoenmaker, &amp;quot;PHL/0A-1, A Question-Answering System for Data-Base Consultation in Natural English,&amp;quot; Philips Tech. Rev., 38, pp. 229-239 and 2 - 847-T97K-1979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Codd</author>
</authors>
<title>Seven Steps to RENDEVOUS with the Casua&apos;. User,&amp;quot;</title>
<date>1974</date>
<tech>IBM Report J1333,</tech>
<marker>Codd, 1974</marker>
<rawString>E.F. Codd, &amp;quot;Seven Steps to RENDEVOUS with the Casua&apos;. User,&amp;quot; IBM Report J1333, 1974.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Damerau</author>
</authors>
<title>Operating Statistics for the Transformational Question Answering System,&amp;quot;</title>
<date>1981</date>
<journal>American Journal of Computational LinFiTiETC7, 1713T7-77</journal>
<volume>1</volume>
<pages>30--42</pages>
<marker>Damerau, 1981</marker>
<rawString>[10] F.J. Damerau, &amp;quot;Operating Statistics for the Transformational Question Answering System,&amp;quot; American Journal of Computational LinFiTiETC7, 1713T7-77 No. 1, pp. 30-42, 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Deas</author>
</authors>
<date>1982</date>
<tech>M.Sc. Thesis,</tech>
<institution>Dept. of Computer Science, Duke University.</institution>
<location>Durham, N.C.,</location>
<marker>Deas, 1982</marker>
<rawString>Ell] H. Deas, M.Sc. Thesis, Dept. of Computer Science, Duke University. Durham, N.C., November 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Egly</author>
<author>K Wescourt</author>
</authors>
<title>Cognitive Style, Categorizations, and Vocational Effects on Performance of REL Database Users,&amp;quot;</title>
<date>1981</date>
<booktitle>Joint Conference on Easier and More Productive Use Fa Computing Systems,</booktitle>
<location>Ann Arbor,-Michi= gan,</location>
<marker>Egly, Wescourt, 1981</marker>
<rawString>[12] D. Egly and K. Wescourt, &amp;quot;Cognitive Style, Categorizations, and Vocational Effects on Performance of REL Database Users,&amp;quot; Joint Conference on Easier and More Productive Use Fa Computing Systems, Ann Arbor,-Michi= gan, May 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Fineman</author>
</authors>
<title>Preliminary Results on the Voice Driven Information System Simulation Experiment,&amp;quot; Report to</title>
<date>1981</date>
<institution>IBM Corporation, Dept. of Computer Science, Duke University,</institution>
<location>Durham, N.C.,</location>
<marker>Fineman, 1981</marker>
<rawString>[13] L. Fineman, &amp;quot;Preliminary Results on the Voice Driven Information System Simulation Experiment,&amp;quot; Report to IBM Corporation, Dept. of Computer Science, Duke University, Durham, N.C., 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P K Fink</author>
</authors>
<title>Conditionals in a Natural Language System&amp;quot;</title>
<date>1981</date>
<tech>(Master&apos;s Thesis), Report CS-1981-8,</tech>
<institution>Duke University,</institution>
<location>Durham, N.C.,</location>
<marker>Fink, 1981</marker>
<rawString>[14] P.K. Fink, &amp;quot;Conditionals in a Natural Language System&amp;quot; (Master&apos;s Thesis), Report CS-1981-8, Duke University, Durham, N.C., 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Geist</author>
<author>D Kraines</author>
<author>P Fink</author>
</authors>
<title>Natural Language Computing in a Linear Algebra Course.&amp;quot;</title>
<date>1982</date>
<booktitle>Proceedings of the National Educational Computing c3nTe7ence,</booktitle>
<marker>Geist, Kraines, Fink, 1982</marker>
<rawString>[15] R. Geist, D. Kraines, and P. Fink, &amp;quot;Natural Language Computing in a Linear Algebra Course.&amp;quot; Proceedings of the National Educational Computing c3nTe7ence, June, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Haas</author>
<author>G Hendrix</author>
</authors>
<title>An Approach to Acquiring and Applying Knowledge,&amp;quot;</title>
<date>1990</date>
<booktitle>First National Conference on ArtifiCITT-ITIFFITit7ince,</booktitle>
<marker>Haas, Hendrix, 1990</marker>
<rawString>[16] N. Haas and G. Hendrix, &amp;quot;An Approach to Acquiring and Applying Knowledge,&amp;quot; First National Conference on ArtifiCITT-ITIFFITit7ince, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Harris</author>
</authors>
<title>User Oriented Data Base Query with the ROBOT Natural Language Query System,&amp;quot;</title>
<date>1977</date>
<journal>International Journal of Man-Machine Studies,</journal>
<pages>697--713</pages>
<marker>Harris, 1977</marker>
<rawString>[17] L.R. Harris, &amp;quot;User Oriented Data Base Query with the ROBOT Natural Language Query System,&amp;quot; International Journal of Man-Machine Studies, pp. 697-713, septiiffier 1977.</rawString>
</citation>
<citation valid="false">
<title>The ROBOT System: Natural Language Processing Applied to Database. Query,&amp;quot;</title>
<booktitle>Proceedings of the 1978 ACM National Conference, PP. TAT-1777</booktitle>
<pages>178</pages>
<marker></marker>
<rawString>(18] L. Harris, &amp;quot;The ROBOT System: Natural Language Processing Applied to Database. Query,&amp;quot; Proceedings of the 1978 ACM National Conference, PP. TAT-1777 T178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J P Haton</author>
<author>J M Pierrel</author>
</authors>
<title>Data Structures and Organization of the MYRTILLE II System,&amp;quot; Fourth T./.C.P.R.,</title>
<date>1978</date>
<location>Kyoto, Japan,</location>
<marker>Haton, Pierrel, 1978</marker>
<rawString>[19] J.P. Haton and J.M. Pierrel, &amp;quot;Data Structures and Organization of the MYRTILLE II System,&amp;quot; Fourth T./.C.P.R., Kyoto, Japan, 1978.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Heidorn</author>
</authors>
<title>Natural Language Dialogue for Managing an On-Line Calendar,&amp;quot;</title>
<date>1978</date>
<journal>IBM Research Report</journal>
<volume>7447</volume>
<marker>Heidorn, 1978</marker>
<rawString>[20] G. Heidorn, &amp;quot;Natural Language Dialogue for Managing an On-Line Calendar,&amp;quot; IBM Research Report RC7447, 1978.</rawString>
</citation>
<citation valid="false">
<title>Developing a Natural Language interface to Complex Data,&amp;quot;</title>
<journal>ACM Transactions on Database Systems,</journal>
<volume>3</volume>
<pages>105--147</pages>
<marker></marker>
<rawString>[211 G.G. Hendrix, E.D. Sacerdoti, D. Sagalowicz, and J. Slocum, &amp;quot;Developing a Natural Language interface to Complex Data,&amp;quot; ACM Transactions on Database Systems, Vol. 3, No. 2, PP. 105-147, 191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G G Hendrix</author>
</authors>
<title>Human Engineering for Applied Natural Language Processing,&amp;quot;</title>
<date>1977</date>
<booktitle>Fifth International Conference on Artificial Intelligence,</booktitle>
<pages>183--191</pages>
<marker>Hendrix, 1977</marker>
<rawString>C22] G.G. Hendrix, &amp;quot;Human Engineering for Applied Natural Language Processing,&amp;quot; Fifth International Conference on Artificial Intelligence, pp. 183-191, 1977.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hendrix</author>
<author>W Lewis</author>
</authors>
<title>Transportable Natural Language Interfaces to Databases,&amp;quot; Annual Meeting of the Assoc. for Computational Linguistics,</title>
<date>1981</date>
<marker>Hendrix, Lewis, 1981</marker>
<rawString>[23] G. Hendrix and W. Lewis, &amp;quot;Transportable Natural Language Interfaces to Databases,&amp;quot; Annual Meeting of the Assoc. for Computational Linguistics, 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hershman</author>
<author>R Kelly</author>
<author>H Miller</author>
</authors>
<title>User Performance with a Natural Language Query System for Command Control,&amp;quot;</title>
<date>1979</date>
<booktitle>NPRDC TR 7R-7, Navy Personnel Research and Development Center,</booktitle>
<location>San Diego, California,</location>
<marker>Hershman, Kelly, Miller, 1979</marker>
<rawString>[24] R. Hershman, R. Kelly, and H. Miller, &amp;quot;User Performance with a Natural Language Query System for Command Control,&amp;quot; NPRDC TR 7R-7, Navy Personnel Research and Development Center, San Diego, California, January 1979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Krause</author>
</authors>
<title>Results of a User Study with the &apos;User Specialty Language,&apos; System and Consequences for the Architecture of Natural Language Interfaces,&amp;quot;</title>
<date>1979</date>
<tech>Technical Report 79.04.003,</tech>
<institution>IBM Heidelberg Scientific Center,</institution>
<marker>Krause, 1979</marker>
<rawString>[25) J. Krause, &amp;quot;Results of a User Study with the &apos;User Specialty Language,&apos; System and Consequences for the Architecture of Natural Language Interfaces,&amp;quot; Technical Report 79.04.003, IBM Heidelberg Scientific Center, May 1979.</rawString>
</citation>
<citation valid="true">
<title>Trends in Speech Recognition,</title>
<date>1982</date>
<location>Prenti-67i=f710:1,</location>
<marker>1982</marker>
<rawString>(26] W.A. Lea (Ed.), Trends in Speech Recognition, Prenti-67i=f710:1, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mylopoulos</author>
<author>A Borgida</author>
<author>P Crthen</author>
<author>N Roussopoulos</author>
<author>J Tsotsos</author>
<author>H Wong</author>
</authors>
<title>TORUS - A Natural Language Understanding System for Data management,&amp;quot;</title>
<date>1975</date>
<booktitle>Proceedings of the Fourth International Conference on AFETTT= cial Intelligence,</booktitle>
<marker>Mylopoulos, Borgida, Crthen, Roussopoulos, Tsotsos, Wong, 1975</marker>
<rawString>[27] J. Mylopoulos, A. Borgida, P. Crthen, N. Roussopoulos, J. Tsotsos, and H. Wong, &amp;quot;TORUS - A Natural Language Understanding System for Data management,&amp;quot; Proceedings of the Fourth International Conference on AFETTT= cial Intelligence, 1975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>r281 S R Petrick</author>
</authors>
<title>On Natural Language Based Computer Systems,&amp;quot;</title>
<date>1976</date>
<journal>IBM Journal of Research and Development,</journal>
<volume>20</volume>
<pages>113--47</pages>
<marker>Petrick, 1976</marker>
<rawString>r281 S.R. Petrick, &amp;quot;On Natural Language Based Computer Systems,&amp;quot; IBM Journal of Research and Development, Vol. 20, 113.-47-5177-3N=335, 1976.</rawString>
</citation>
<citation valid="false">
<title>REQUEST: A Natural Language Question-Answering System,&amp;quot;</title>
<journal>IBM Journal of Research and</journal>
<volume>20</volume>
<pages>326--335</pages>
<marker></marker>
<rawString>(29] W.J. Plath, &amp;quot;REQUEST: A Natural Language Question-Answering System,&amp;quot; IBM Journal of Research and DevelopVol. 20, No. 4, pp. 326-335, WM.</rawString>
</citation>
<citation valid="false">
<authors>
<author>C301 D R Reddy</author>
</authors>
<title>Speech Recognition by Machine: A Review,&amp;quot;</title>
<booktitle>Proceedings of the IEEE,</booktitle>
<volume>64</volume>
<pages>01--76</pages>
<marker>Reddy, </marker>
<rawString>C301 D.R. Reddy, &amp;quot;Speech Recognition by Machine: A Review,&amp;quot; Proceedings of the IEEE, Vol. 64, No. 4, pp. S01-T. &apos;T76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C311 H Tennant</author>
</authors>
<title>Experience with the Evaluation of Natural Language Question Answerers,&amp;quot; Working Paper 18, Advanced Automation Group, Coordinated Science Lab., U-iv. of Illinois,</title>
<date>1979</date>
<marker>Tennant, 1979</marker>
<rawString>C311 H. Tennant, &amp;quot;Experience with the Evaluation of Natural Language Question Answerers,&amp;quot; Working Paper 18, Advanced Automation Group, Coordinated Science Lab., U-iv. of Illinois, January 1979.</rawString>
</citation>
<citation valid="true">
<title>Practical Natural Language Processing: The REL System as Prototype,&amp;quot;</title>
<date>1975</date>
<booktitle>in Advances in Computers,</booktitle>
<volume>13</volume>
<pages>7--7</pages>
<publisher>Academic Press,</publisher>
<location>New York,</location>
<marker>1975</marker>
<rawString>(32] F.B. Thompson and B.H. Thompson, &amp;quot;Practical Natural Language Processing: The REL System as Prototype,&amp;quot; in Advances in Computers, Vol. 13 (Eds7-q7-06ini-irf and M.C. Yovits), Academic Press, New York, 1975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Thompson</author>
<author>B Thompson</author>
</authors>
<title>Shifting to a Higher Gear in a Natural Language System,&amp;quot; AFIRs</title>
<date>1981</date>
<booktitle>Proc. of the National Computer Conf.,</booktitle>
<volume>50</volume>
<pages>657--662</pages>
<marker>Thompson, Thompson, 1981</marker>
<rawString>1331 F. Thompson and B. Thompson, &amp;quot;Shifting to a Higher Gear in a Natural Language System,&amp;quot; AFIRs Proc. of the National Computer Conf., Vol. 50, pp. 657-662, 1981.</rawString>
</citation>
<citation valid="true">
<title>Understanding Spoken Language,</title>
<date>1978</date>
<editor>C341 D.E. Walker (ed.),</editor>
<publisher>Elsevier North-Holland,</publisher>
<location>New York,</location>
<marker>1978</marker>
<rawString>C341 D.E. Walker (ed.), Understanding Spoken Language, Elsevier North-Holland, New York, 1978.</rawString>
</citation>
<citation valid="false">
<authors>
<author>D L Waltz</author>
</authors>
<title>An English Language Question Answering System for a Large Relational Database,&amp;quot;</title>
<journal>Communications of the ACM,</journal>
<volume>21</volume>
<pages>526--39</pages>
<marker>Waltz, </marker>
<rawString>[35] D.L. Waltz, &amp;quot;An English Language Question Answering System for a Large Relational Database,&amp;quot; Communications of the ACM, Vol. 21, No. 7, pp. 526-39,</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Woods</author>
<author>R N Kaplan</author>
<author>B Nash-Webber</author>
</authors>
<title>The Lunar Sciences Natural Language Information System: Final Report,&amp;quot;</title>
<date>1972</date>
<tech>Report 2378,</tech>
<location>Bolt, Beranek, and Newman, Cambridge, MA.,</location>
<marker>Woods, Kaplan, Nash-Webber, 1972</marker>
<rawString>(36) W.A. Woods, R.N. Kaplan, and B. Nash-Webber, &amp;quot;The Lunar Sciences Natural Language Information System: Final Report,&amp;quot; Report 2378, Bolt, Beranek, and Newman, Cambridge, MA., 1972.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C373 W A Woods</author>
</authors>
<title>Motivation and Overview of SPEECHLIS: An Experimental Prototype for Speech Understanding Research,&amp;quot;</title>
<date>1976</date>
<journal>IEEE Transactions on Acoustics, SpaTEK, and Signal ProEtwirxi7—</journal>
<volume>777</volume>
<pages>7--77</pages>
<marker>Woods, 1976</marker>
<rawString>C373 W.A. Woods, &amp;quot;Motivation and Overview of SPEECHLIS: An Experimental Prototype for Speech Understanding Research,&amp;quot; IEEE Transactions on Acoustics, SpaTEK, and Signal ProEtwirxi7— vol. ASSP-777 No. 1, pp7-77 10, 1976.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>