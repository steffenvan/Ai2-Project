<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.891913">
Turbo Parsers: Dependency Parsing by Approximate Variational Inference
</title>
<author confidence="0.985427">
Andr´e F. T. Martins*† Noah A. Smith* Eric P. Xing*
</author>
<affiliation confidence="0.887174666666667">
*School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.994848">
{afm,nasmith,epxing}@cs.cmu.edu
</email>
<author confidence="0.99732">
Pedro M. Q. Aguiar$
</author>
<affiliation confidence="0.9454825">
$Instituto de Sistemas e Rob´otica
Instituto Superior T´ecnico
</affiliation>
<address confidence="0.865874">
Lisboa, Portugal
</address>
<email confidence="0.996527">
aguiar@isr.ist.utl.pt
</email>
<sectionHeader confidence="0.996624" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999726294117647">
We present a unified view of two state-of-the-
art non-projective dependency parsers, both
approximate: the loopy belief propagation
parser of Smith and Eisner (2008) and the re-
laxed linear program of Martins et al. (2009).
By representing the model assumptions with
a factor graph, we shed light on the optimiza-
tion problems tackled in each method. We also
propose a new aggressive online algorithm to
learn the model parameters, which makes use
of the underlying variational representation.
The algorithm does not require a learning rate
parameter and provides a single framework for
a wide family of convex loss functions, includ-
ing CRFs and structured SVMs. Experiments
show state-of-the-art performance for 14 lan-
guages.
</bodyText>
<sectionHeader confidence="0.998781" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999878733333333">
Feature-rich discriminative models that break local-
ity/independence assumptions can boost a parser’s
performance (McDonald et al., 2006; Huang, 2008;
Finkel et al., 2008; Smith and Eisner, 2008; Martins
et al., 2009; Koo and Collins, 2010). Often, infer-
ence with such models becomes computationally in-
tractable, causing a demand for understanding and
improving approximate parsing algorithms.
In this paper, we show a formal connection be-
tween two recently-proposed approximate inference
techniques for non-projective dependency parsing:
loopy belief propagation (Smith and Eisner, 2008)
and linear programming relaxation (Martins et al.,
2009). While those two parsers are differently moti-
vated, we show that both correspond to inference in
</bodyText>
<page confidence="0.987515">
34
</page>
<address confidence="0.557345333333333">
M´ario A. T. Figueiredo††Instituto de Telecomunicac¸˜oes
Instituto Superior T´ecnico
Lisboa, Portugal
</address>
<email confidence="0.984024">
mtf@lx.it.pt
</email>
<bodyText confidence="0.999879433333333">
a factor graph, and both optimize objective functions
over local approximations of the marginal polytope.
The connection is made clear by writing the explicit
declarative optimization problem underlying Smith
and Eisner (2008) and by showing the factor graph
underlying Martins et al. (2009). The success of
both approaches parallels similar approximations in
other fields, such as statistical image processing and
error-correcting coding. Throughtout, we call these
turbo parsers.1
Our contributions are not limited to dependency
parsing: we present a general method for inference
in factor graphs with hard constraints (§2), which
extends some combinatorial factors considered by
Smith and Eisner (2008). After presenting a geo-
metric view of the variational approximations un-
derlying message-passing algorithms (§3), and clos-
ing the gap between the two aforementioned parsers
(§4), we consider the problem of learning the model
parameters (§5). To this end, we propose an ag-
gressive online algorithm that generalizes MIRA
(Crammer et al., 2006) to arbitrary loss functions.
We adopt a family of losses subsuming CRFs (Laf-
ferty et al., 2001) and structured SVMs (Taskar et
al., 2003; Tsochantaridis et al., 2004). Finally, we
present a technique for including features not at-
tested in the training data, allowing for richer mod-
els without substantial runtime costs. Our experi-
ments (§6) show state-of-the-art performance on de-
pendency parsing benchmarks.
</bodyText>
<footnote confidence="0.955061">
1The name stems from “turbo codes,” a class of high-
performance error-correcting codes introduced by Berrou et al.
(1993) for which decoding algorithms are equivalent to running
belief propagation in a graph with loops (McEliece et al., 1998).
</footnote>
<note confidence="0.839512333333333">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 34–44,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
2 Structured Inference and Factor Graphs
</note>
<bodyText confidence="0.996244888888889">
Denote by X a set of input objects from which we
want to infer some hidden structure conveyed in an
output set Y. Each input x ∈ X (e.g., a sentence)
is associated with a set of candidate outputs Y(x) ⊆
Y (e.g., parse trees); we are interested in the case
where Y(x) is a large structured set.
Choices about the representation of elements of
Y(x) play a major role in algorithm design. In
many problems, the elements of Y(x) can be rep-
resented as discrete-valued vectors of the form y =
hy1, ... , yIi, each yi taking values in a label set Yi.
For example, in unlabeled dependency parsing, I is
the number of candidate dependency arcs (quadratic
in the sentence length), and each Yi = {0, 1}. Of
course, the yi are highly interdependent.
Factor Graphs. Probabilistic models like CRFs
(Lafferty et al., 2001) assume a factorization of the
conditional distribution of Y ,
</bodyText>
<equation confidence="0.990591">
Pr(Y = y  |X = x) ∝ HCEC &apos;FC(x,yC), (1)
</equation>
<bodyText confidence="0.98725875">
where each C ⊆ {1, ... , I} is a factor, C is the set
of factors, each yC °_ hyiiiEC denotes a partial out-
put assignment, and each &apos;FC is a nonnegative po-
tential function that depends on the output only via
its restriction to C. A factor graph (Kschischang
et al., 2001) is a convenient representation for the
factorization in Eq. 1: it is a bipartite graph Gx com-
prised of variable nodes {1, ... , I} and factor nodes
C ∈ C, with an edge connecting the ith variable
node and a factor node C iff i ∈ C. Hence, the fac-
tor graph Gx makes explicit the direct dependencies
among the variables {y1, ... , yI}.
Factor graphs have been used for several NLP
tasks, such as dependency parsing, segmentation,
and co-reference resolution (Sutton et al., 2007;
Smith and Eisner, 2008; McCallum et al., 2009).
Hard and Soft Constraint Factors. It may be
the case that valid outputs are a proper subset of
Y1 × · · · × YI—for example, in dependency pars-
ing, the entries of the output vector y must jointly
define a spanning tree. This requires hard constraint
factors that rule out forbidden partial assignments
by mapping them to zero potential values. See Ta-
ble 1 for an inventory of hard constraint factors used
in this paper. Factors that are not of this special kind
are called soft factors, and have strictly positive po-
tentials. We thus have a partition C = Chard ∪ Csoft.
We let the soft factor potentials take the form
`pC(x,yC) °_ exp(θTφC(x,yC)), where θ ∈ Rd
is a vector of parameters (shared across factors) and
φC(x, yC) is a local feature vector. The conditional
distribution of Y (Eq. 1) thus becomes log-linear:
</bodyText>
<equation confidence="0.999563">
Pre(y|x) = Zx(θ)−1 exp(θTφ(x, y)), (2)
</equation>
<bodyText confidence="0.7155885">
where Zx(θ) °_ Ey,E%x) exp(θTφ(x, y&apos;)) is the
partition function, and the features decompose as:
</bodyText>
<equation confidence="0.983621">
φ(x, y) o ECEC&apos;goec φC(x,yC). (3)
</equation>
<bodyText confidence="0.99797924137931">
Dependency Parsing. Smith and Eisner (2008)
proposed a factor graph representation for depen-
dency parsing (Fig. 1). The graph has O(n2) vari-
able nodes (n is the sentence length), one per candi-
date arc a °_ hh, mi linking a head h and modifier
m. Outputs are binary, with ya = 1 iff arc a belongs
to the dependency tree. There is a hard factor TREE
connected to all variables, that constrains the overall
arc configurations to form a spanning tree. There is a
unary soft factor per arc, whose log-potential reflects
the score of that arc. There are also O(n3) pair-
wise factors; their log-potentials reflect the scores
of sibling and grandparent arcs. These factors cre-
ate loops, thus calling for approximate inference.
Without them, the model is arc-factored, and ex-
act inference in it is well studied: finding the most
probable parse tree takes O(n3) time with the Chu-
Liu-Edmonds algorithm (McDonald et al., 2005),2
and computing posterior marginals for all arcs takes
O(n3) time via the matrix-tree theorem (Smith and
Smith, 2007; Koo et al., 2007).
Message-passing algorithms. In general
factor graphs, both inference problems—
obtaining the most probable output (the MAP)
argmaxyE%x) Pre(y|x), and computing the
marginals Pre(Yi = yi|x)—can be addressed
with the belief propagation (BP) algorithm (Pearl,
1988), which iteratively passes messages between
variables and factors reflecting their local “beliefs.”
</bodyText>
<footnote confidence="0.996775">
2There is a faster but more involved O(n2) algorithm due to
Tarjan (1977).
</footnote>
<page confidence="0.99757">
35
</page>
<listItem confidence="0.921792054054054">
�1 v1, . . . , vn ∈ SC
A general binary factor: ΨC(v1, ... , vn) = where SC ⊆ {0, 1}n.
0 otherwise,
• Message-induced distribution: ω , hmj→Cij=1,...,n • Partition function: ZC(ω) , Ehv1, vni∈8CrIi=1 mvi i→C
• Marginals: MARGi(ω) , Pr,,,{Vi = 1|hV1, ... , Vni ∈ SC} • Max-marginals: MAX-MARGi,b(ω) , maxV∈8C Pr,,,(v|vi = b)
• Sum-prod.: mC→i = m−1
i→C · MARGi(ω)/(1 − MARGi(ω)) • Max-prod.: mC→i = mi-C · MAX-MARGi, 1 (ω)/MAX-MARGi,0 (ω)
• Local agreem. constr.: z ∈ conv SC, where z = hri (1)ii 1 • Entropy: HC = log ZC (ω) − Ei=1 MARGi (ω) log mi→C
1 1 y ∈Ytree (i.e., {a ∈ A  |ya = 1} is a directed spanning tree) where A is the set of candidate arcs.
TREE ΨTREE(hyaia∈A) = 1 0 otherwise,
• Partition function Zt (ω) hand marginals hMARGa(ω)ia∈A computed via the matrix-tree theorem, with ω , hma→TREEia∈A
m
• Sum-prod.: mTREE→a = m−1
a→TREE · MARGa(ω)/(1 − MARGa(ω))
• Max-prod.: mTREE→a = m−1
a→TREE · MAX-MARGa,1(ω)/MAX-MARGa,0(ω), where MAX-MARGa,b(ω) , maxY∈IJtree Pr„(y|ya = b)
• Local agreem. constr.: z ∈ Ztree, where Ztree , conv Ytree is the arborescence polytope
• Entropy: Htree = log Ztree(ω) − Ea∈A MARGa(ω) log ma→TREE
1 =1
XOR (“one-hot”)ΨXOR(v1, . . . , vn) = 1 Ei lvi
1l 0 otherwise.
1
• Sum-prod.: mXOR→i = (Ej6=i mj→XOR) 1 • Max-prod.: mXOR→i = (maxj6=i mj→ XOR) −
• Local agreem. constr.: Ei zi = 1, zi ∈ [0, 1], ∀i • HXOR = − Ei(mi→XOR/ Ej mj→XOR) log(mi→XOR/ Ej mj→XOR)
1 i 1
�1 vi ≥
ORΨOR(v1, ... , vn) = E
0 otherwise.
1
• Sum-prod.: mOR→i = (1 − l 1-7 ljoi(l + mj→OR)−1) • Max-prod.: mOR→i = max{1, minj6=i m−1
j→OR}
• Local agreem. constr.: Ei zi ≥ 1, zi ∈ [0, 1], ∀i
OR-WITH-OUTPUT Ψ = l 1 vn — Vi 11 vi
OR-OUT (711, • • , vn) l _
0 otherwise.
_ 1
• Sum-prod.: — (1 — (1 − mn 1OR-OUT) rIj6=i,n(1 + mj→ OR-OUT)−1) i &lt; n
</listItem>
<table confidence="0.9788466">
MOR-OUT—.i = 7r&amp;quot;&amp;quot;7�
l ljOn(1 + mj→OR-OUT) − 1 i = n.
11 •Max-prod.: mOR-OUT→i = min (mn→OR-OUTrIj6=&apos;n” i maX{1, mj→OR-OUT}, max{1, minj6=i,n mj−→1OR-OUT }} i &lt; n
1 l
II\\ rI j6=n max{1, mj→OR-OUT} min{1, maxj6=n mj→OR-OUT} i = n.
</table>
<tableCaption confidence="0.903814">
Table 1: Hard constraint factors, their potentials, messages, and entropies. The top row shows expressions for a
general binary factor: each outgoing message is computed from incoming marginals (in the sum-product case), or
</tableCaption>
<figureCaption confidence="0.531600111111111">
max-marginals (in the max-product case); the entropy of the factor (see §3) is computed from these marginals and the
partition function; the local agreement constraints (§4) involve the convex hull of the set SC of allowed configurations
(see footnote 5). The TREE, XOR, OR and OR-WITH-OUTPUT factors allow tractable computation of all these quantities
(rows 2–5). Two of these factors (TREE and XOR) had been proposed by Smith and Eisner (2008); we provide further
information (max-product messages, entropies, and local agreement constraints). Factors OR and OR-WITH-OUTPUT
are novel to the best of our knowledge. This inventory covers many cases, since the above formulae can be extended
to the case where some inputs are negated: just replace the corresponding messages by their reciprocal, vi by 1 − vi,
etc. This allows building factors NAND (an OR factor with negated inputs), IMPLY (a 2-input OR with the first input
negated), and XOR-WITH-OUTPUT (an XOR factor with the last input negated).
</figureCaption>
<bodyText confidence="0.84853">
In sum-product BP, the messages take the form:3
</bodyText>
<equation confidence="0.9994055">
Mi→C(yi) a HD6=C MD→i(yi) (4)
MC→i(yi) a EyC∼yi`FC(YC) Hj6=i Mj→C(yj). (5)
</equation>
<bodyText confidence="0.999521666666667">
In max-product BP, the summation in Eq. 5 is re-
placed by a maximization. Upon convergence, vari-
able and factor beliefs are computed as:
</bodyText>
<equation confidence="0.9743615">
Ti(yi) a HC MC→i(yi) (6)
TC(YC) a `&amp;C(YC) Hi Mi→C(yi). (7)
</equation>
<footnote confidence="0.9223086">
BP is exact when the factor graph is a tree: in the
sum-product case, the beliefs in Eqs. 6–7 correspond
3We employ the standard — notation, where a summa-
tion/maximization indexed by yC — yi means that it is over
all yC with the i-th component held fixed and set to yi.
</footnote>
<bodyText confidence="0.999965923076923">
to the true marginals, and in the max-product case,
maximizing each Ti(yi) yields the MAP output. In
graphs with loops, BP is an approximate method, not
guaranteed to converge, nicknamed loopy BP. We
highlight a variational perspective of loopy BP in §3;
for now we consider algorithmic issues. Note that
computing the factor-to-variable messages for each
factor C (Eq. 5) requires a summation/maximization
over exponentially many configurations. Fortu-
nately, for all the hard constraint factors in rows 3–5
of Table 1, this computation can be done in linear
time (and polynomial for the TREE factor)—this ex-
tends results presented in Smith and Eisner (2008).4
</bodyText>
<footnote confidence="0.987289">
4The insight behind these speed-ups is that messages on
binary-valued potentials can be expressed as MC→i(yi) oc
</footnote>
<page confidence="0.99757">
36
</page>
<figureCaption confidence="0.9852258">
Figure 1: Factor graph corresponding to the dependency
parsing model of Smith and Eisner (2008) with sibling
and grandparent features. Circles denote variable nodes,
and squares denote factor nodes. Note the loops created
by the inclusion of pairwise factors (GRAND and SIB).
</figureCaption>
<bodyText confidence="0.99997175">
In Table 1 we present closed-form expressions
for the factor-to-variable message ratios mC→i ,
MC→i(1)/MC→i(0) in terms of their variable-to-
factor counterparts mi→C , Mi→C(1)/Mi→C(0);
these ratios are all that is necessary when the vari-
ables are binary. Detailed derivations are presented
in an extended version of this paper (Martins et al.,
2010b).
</bodyText>
<sectionHeader confidence="0.980079" genericHeader="method">
3 Variational Representations
</sectionHeader>
<bodyText confidence="0.999761529411765">
Let Tx , {Prθ(.|x)  |θ ∈ Rd} be the family of all
distributions of the form in Eq. 2. We next present
an alternative parametrization for the distributions in
Tx in terms of factor marginals. We will see that
each distribution can be seen as a point in the so-
called marginal polytope (Wainwright and Jordan,
2008); this will pave the way for the variational rep-
resentations to be derived next.
Parts and Output Indicators. A part is a pair
hC, yCi, where C is a soft factor and yC a partial
output assignment. We let 9Z = {hC, yCi  |C ∈
Osofk, yC ∈ Hi∈C �i} be the set of all parts. Given
an output y0 ∈ �(x), a part hC, yCi is said to be ac-
tive if it locally matches the output, i.e., if yC = y0C.
Any output y0 ∈ �(x) can be mapped to a |9Z|-
dimensional binary vector χ(y0) indicating which
parts are active, i.e., [χ(y0)]hC,yCi = 1 if yC = y0C
</bodyText>
<equation confidence="0.5997265">
Pr{TC(YC) = 1IYi = yi} and MC—i(yi) a
maxq,c(yc)_1 Pr{YC = yClYi = yi}, respectively for the
</equation>
<bodyText confidence="0.844486857142857">
sum-product and max-product cases; these probabilities are in-
duced by the messages in Eq. 4: for an event A C REC
Pr{YC E A} o Eyc H(yC E A) FLEC Mi—C(yi).
and 0 otherwise; χ(y0) is called the output indicator
vector. This mapping allows decoupling the feature
vector in Eq. 3 as the product of an input matrix and
an output vector:
</bodyText>
<equation confidence="0.9230845">
φ(x, y) = � φC(x,yC) = F(x)χ(y), (8)
C∈Q.oec
</equation>
<bodyText confidence="0.996499875">
where F(x) is a d-by-|9Z |matrix whose columns
contain the part-local feature vectors φC(x, yC).
Observe, however, that not every vector in {0,1}|T-|
corresponds necessarily to a valid output in �(x).
Marginal Polytope. Moving to vector representa-
tions of outputs leads naturally to a geometric view
of the problem. The marginal polytope is the convex
hull5 of all the “valid” output indicator vectors:
</bodyText>
<equation confidence="0.891676">
M(Sx) , conv{χ(y)  |y ∈ �(x)}.
</equation>
<bodyText confidence="0.999019736842105">
Note that M(Sx) only depends on the factor graph
Sx and the hard constraints (i.e., it is independent of
the parameters θ). The importance of the marginal
polytope stems from two facts: (i) each vertex of
M(Sx) corresponds to an output in �(x); (ii) each
point in M(Sx) corresponds to a vector of marginal
probabilities that is realizable by some distribution
(not necessarily in Tx) that factors according to Sx.
Variational Representations. We now describe
formally how the points in M(Sx) are linked to the
distributions in Tx. We extend the “canonical over-
complete parametrization” case, studied by Wain-
wright and Jordan (2008), to our scenario (common
in NLP), where arbitrary features are allowed and
the parameters are tied (shared by all factors). Let
H(Prθ(.|x)) ,− Ey∈%x) Prθ(y|x) log Prθ(y|x)
denote the entropy of Prθ(.|x), and Eθ[.] the ex-
pectation under Prθ(.|x). The component of µ ∈
M(Sx) indexed by part hC, yCi is denoted µC(yC).
</bodyText>
<construct confidence="0.896298571428571">
Proposition 1. There is a map coupling each distri-
bution Prθ(.|x) ∈ Tx to a unique µ ∈ M(Sx) such
that Eθ[χ(Y )] = µ. Define H(µ) , H(Prθ(.|x))
if some Prθ(.|x) is coupled to µ, and H(µ) = −∞
if no such Prθ(.|x) exists. Then:
1. The following variational representation for the
log-partition function (mentioned in Eq. 2) holds:
</construct>
<equation confidence="0.816375">
θ&gt;F(x)µ + H(µ). (9)
</equation>
<footnote confidence="0.9639635">
5The convex hull of {z1, ... , zk} is the set of points that can
be written as Ek i�1 Aizi, where Ek i�1 Ai = 1 and each Ai &gt; 0.
</footnote>
<figure confidence="0.978523636363636">
GRAND
(g,h,m )1
SIB
(h,m1,m2)
SIB
(h,m 2 ,m 3 )
SIB
(h,m1,m3)
ARC
(g,h)
ARC
(h,m1)
ARC ARC
(h,m2) (h,m )
3
TREE
log Zx(θ) = max
µ∈M(jx)
37
Parameter space Factor log-potentials
space
Marginal polytope
</figure>
<figureCaption confidence="0.951598571428572">
Figure 2: Dual parametrization of the distributions in
Px. Our parameter space (left) is first linearly mapped to
the space of factor log-potentials (middle). The latter is
mapped to the marginal polytope M(Gx) (right). In gen-
eral only a subset of M(Gx) is reachable from our param-
eter space. Any distribution in Px can be parametrized by
a vector θ E Rd or by a point µ E M(Gx).
</figureCaption>
<listItem confidence="0.997837">
2. The problem in Eq. 9 is convex and its solution
is attained at the factor marginals, i.e., there is a
maximizer µ� s.t. �µC(yC) = Pro(YC = yC|x)
for each C E C. The gradient of the log-partition
function is V log Zx(θ) = F(x)µ.
3. The MAP y�, argmaxy∈%x) Pro(y|x) can be
obtained by solving the linear program
</listItem>
<equation confidence="0.991598">
µ� ,χ(Y) = argmax θ&gt;F(x)µ. (10)
µ∈M(oj.)
</equation>
<bodyText confidence="0.999926">
A proof of this proposition can be found in Mar-
tins et al. (2010a). Fig. 2 provides an illustration of
the dual parametrization implied by Prop. 1.
</bodyText>
<sectionHeader confidence="0.977935" genericHeader="method">
4 Approximate Inference &amp; Turbo Parsing
</sectionHeader>
<bodyText confidence="0.9999306">
We now show how the variational machinery just
described relates to message-passing algorithms and
provides a common framework for analyzing two re-
cent dependency parsers. Later (§5), Prop. 1 is used
constructively for learning the model parameters.
</bodyText>
<subsectionHeader confidence="0.994098">
4.1 Loopy BP as a Variational Approximation
</subsectionHeader>
<bodyText confidence="0.966340071428571">
For general factor graphs with loops, the marginal
polytope M(Gx) cannot be compactly specified and
the entropy term H(µ) lacks a closed form, render-
ing exact optimizations in Eqs. 9–10 intractable. A
popular approximate algorithm for marginal infer-
ence is sum-product loopy BP, which passes mes-
sages as described in §2 and, upon convergence,
computes beliefs via Eqs. 6–7. Were loopy BP exact,
these beliefs would be the true marginals and hence
a point in the marginal polytope M(Gx). However,
this need not be the case, as elucidated by Yedidia et
al. (2001) and others, who first analyzed loopy BP
from a variational perspective. The following two
approximations underlie loopy BP:
</bodyText>
<listItem confidence="0.67969425">
• The marginal polytope M(Gx) is approximated by
the local polytope L(Gx). This is an outer bound;
its name derives from the fact that it only imposes
local agreement constraints Vi, yi E Yi, C E C:
</listItem>
<equation confidence="0.88103">
Eyz Ti(yi) = 1, EyC∼y,, TC(yC) = Ti(yi). (11)
</equation>
<bodyText confidence="0.9756516">
Namely, it is characterized by L(Gx) , {τ E
R|� |� |Eq. 11 holds Vi, yi E Yi, C E C}. The
elements of L(Gx) are called pseudo-marginals.
Clearly, the true marginals satisfy Eq. 11, and
therefore M(Gx) C L(Gx).
</bodyText>
<listItem confidence="0.544631">
• The entropy H is replaced by its Bethe approx-
</listItem>
<equation confidence="0.707137833333333">
imation HBethe(τ) , EIi�1(1 − di)H(τi) +
E
C∈e H(τC), where di = |{C  |i E C} |is the
number of factors connected to the ith variable,
H(τi) , − Ey, Ti(yi) log Ti(yi) and H(τC) ,
− EyC TC(yC) log TC(yC).
</equation>
<bodyText confidence="0.999902636363636">
Any stationary point of sum-product BP is a lo-
cal optimum of the variational problem in Eq. 9
with M(Gx) replaced by L(Gx) and H replaced by
HBethe (Yedidia et al., 2001). Note however that
multiple optima may exist, since HBethe is not nec-
essarily concave, and that BP may not converge.
Table 1 shows closed form expressions for the
local agreement constraints and entropies of some
hard-constraint factors, obtained by invoking Eq. 7
and observing that TC(yC) must be zero if configu-
ration yC is forbidden. See Martins et al. (2010b).
</bodyText>
<subsectionHeader confidence="0.994914">
4.2 Two Dependency Turbo Parsers
</subsectionHeader>
<bodyText confidence="0.999976769230769">
We next present our main contribution: a formal
connection between two recent approximate depen-
dency parsers, which at first sight appear unrelated.
Recall that (i) Smith and Eisner (2008) proposed a
factor graph (Fig. 1) in which they run loopy BP,
and that (ii) Martins et al. (2009) approximate pars-
ing as the solution of a linear program. Here, we
fill the blanks in the two approaches: we derive ex-
plicitly the variational problem addressed in (i) and
we provide the underlying factor graph in (ii). This
puts the two approaches side-by-side as approximate
methods for marginal and MAP inference. Since
both rely on “local” approximations (in the sense
</bodyText>
<page confidence="0.996026">
38
</page>
<bodyText confidence="0.955126066666667">
of Eq. 11) that ignore the loops in their graphical
models, we dub them turbo parsers by analogy with
error-correcting turbo decoders (see footnote 1).
Turbo Parser #1: Sum-Product Loopy BP. The
factor graph depicted in Fig. 1—call it Gx—includes
pairwise soft factors connecting sibling and grand-
parent arcs.6 We next characterize the local polytope
L(Gx) and the Bethe approximation HBethe inherent
in Smith and Eisner’s loopy BP algorithm.
Let A be the set of candidate arcs, and P C_
A2 the set of pairs of arcs that have factors. Let
τ = (τA,τP) with τA = (Ta)aEA and τP =
(Tab)(a,b)EP. Since all variables are binary, we may
write, for each a E A, Ta(1) = za and Ta(0) =
1 − za, where za is a variable constrained to [0, 1].
</bodyText>
<equation confidence="0.708825">
0
</equation>
<bodyText confidence="0.9989135">
Let zA = (za)aEA; the local agreement constraints
at the TREE factor (see Table 1) are written as zA E
Ztree(x), where Ztree(x) is the arborescence poly-
tope, i.e., the convex hull of all incidence vectors
of dependency trees (Martins et al., 2009). It is
straightforward to write a contingency table and ob-
tain the following local agreement constraints at the
pairwise factors:
</bodyText>
<equation confidence="0.999894">
Tab(1,1) = zab, Tab(0,0) = 1 − za − zb + zab
Tab(1,0) = za − zab, Tab(0,1) = zb − zab.
</equation>
<bodyText confidence="0.999518">
Noting that all these pseudo-marginals are con-
strained to the unit interval, one can get rid of all
variables Tab and write everything as
</bodyText>
<equation confidence="0.887764">
za E [0, 1], zb E [0, 1], zab E [0, 1],
zab :5 za, zab :5 zb, zab &gt;_ za + zb − 1,
(12)
</equation>
<bodyText confidence="0.997869222222222">
inequalities which, along with zA E Ztree(x), de-
fine the local polytope L(Gx). As for the factor en-
tropies, start by noting that the TREE-factor entropy
Htree can be obtained in closed form by computing
the marginals ZA and the partition function Zx(θ)
(via the matrix-tree theorem) and recalling the vari-
ational representation in Eq. 9, yielding Htree =
log Zx(θ) − θTF(x)zA. Some algebra allows writ-
ing the overall Bethe entropy approximation as:
</bodyText>
<equation confidence="0.992681">
�HBethe(τ) = Htree(zA) − Ia;b(za, zb, zab), (13)
(a,b)EP
</equation>
<bodyText confidence="0.999586">
where we introduced the mutual information asso-
ciated with each pairwise factor, Ia;b(za, zb, zab) =
</bodyText>
<footnote confidence="0.9671435">
6Smith and Eisner (2008) also proposed other variants with
more factors, which we omit for brevity.
</footnote>
<figureCaption confidence="0.998268666666667">
Figure 3: Details of the factor graph underlying the parser
of Martins et al. (2009). Dashed circles represent auxil-
iary variables. See text and Table 1.
</figureCaption>
<equation confidence="0.996487125">
E
ya,yb Tab(ya, yb) log τa(ya)τb(yb). The approximate
�maxZ θTF(x)z + Htree(zA) − Ia;b(za, zb, zab)
(a,b)EP
s.t. zab :5 za, zab :5 zb,
zab &gt;_ za + zb − 1, b(a,b) E P,
zA E Ztree,
(14)
</equation>
<bodyText confidence="0.995552333333333">
whose maximizer corresponds to the beliefs re-
turned by the Smith and Eisner’s loopy BP algorithm
(if it converges).
Turbo Parser #2: LP-Relaxed MAP. We now
turn to the concise integer LP formulation of Mar-
tins et al. (2009). The formulation is exact but NP-
hard, and so an LP relaxation is made there by drop-
ping the integer constraints. We next construct a fac-
tor graph G&apos; and show that the LP relaxation corre-
sponds to an optimization of the form in Eq. 10, with
the marginal polytope M(G&apos;) replaced by L(G&apos;).
G&apos; includes the following auxiliary variable
nodes: path variables (pij)i=0,...,n,j=1,...,n, which
indicate whether word j descends from i in the de-
pendency tree, and flow variables (fka )aEA,k=1,...,n,
which evaluate to 1 iff arc a “carries flow” to k,
i.e., iff there is a path from the root to k that passes
through a. We need to seed these variables imposing
</bodyText>
<equation confidence="0.949239">
p0k = pkk = 1, bk, fh (h,m) = 0, bh, m, (15)
</equation>
<bodyText confidence="0.9965715">
i.e., any word descends from the root and from it-
self, and arcs leaving a word carry no flow to that
</bodyText>
<figure confidence="0.999318826086957">
ARC
(h,m)
FLOW
(h,m,k)
ARC
(n,m)
ARC
(0,m)
FLOW-IMPLIES-ARC
(h,m,k)
XOR
OR
SINGLE-PARENT
(m)
FLOW FLOW PATH FLOW FLOW PATH
(0,m,k) (n,m,k) (m,k) (h,1,k) (h,n,k) (h,k)
XOR
FLOW-DELTA
(h,k)
PATH-BUILDER
(m,k)
τab(ya,yb)
variational expression becomes log Zx(θ) �
</figure>
<page confidence="0.997844">
39
</page>
<bodyText confidence="0.999643333333333">
word. This can be done with unary hard constraint
factors. We then replace the TREE factor in Fig. 1 by
the factors shown in Fig. 3:
</bodyText>
<listItem confidence="0.8647030625">
• O(n) XOR factors, each connecting all arc vari-
ables of the form {hh, mi}h=0,...,n. These ensure
that each word has exactly one parent. Each factor
yields a local agreement constraint (see Table 1):
Enh=0 zhh,mi = 1, m ∈ {1, ... , n} (16)
• O(n3) IMPLY factors, each expressing that if an
arc carries flow, then that arc must be active. Such
factors are OR factors with the first input negated,
hence, the local agreement constraints are:
fka ≤ za, a ∈ A, k ∈ {1, ... , n}. (17)
• O(n2) XOR-WITH-OUTPUT factors, which im-
pose the constraint that each path variable pmk is
active if and only if exactly one incoming arc in
{hh, mi}h=0,...,n carries flow to k. Such factors
are XOR factors with the last input negated, and
hence their local constraints are:
</listItem>
<equation confidence="0.9808895">
n k
pmk = Eh=0 fhh,mi, m, k ∈ {1, ... , n} (18)
</equation>
<listItem confidence="0.9881756">
• O(n2) XOR-WITH-OUTPUT factors to impose the
constraint that words don’t consume other words’
commodities; i.e., if h =6 k and k =6 0, then there
is a path from h to k iff exactly one outgoing arc
in {hh, mi}m=1,...,n carries flow to k:
</listItem>
<equation confidence="0.9676365">
phk = Enm=1 fkhh,mi, h, k ∈ {0, ... , n}, k ∈� {0, h}.
(19)
</equation>
<bodyText confidence="0.9298625">
L(G0x) is thus defined by the constraints in Eq. 12
and 15–19. The approximate MAP problem, that
replaces M(G0 x) by L(G0x) in Eq. 10, thus becomes:
maxz,f,p θ&gt;F(x)z (20)
s.t. Eqs. 12 and 15–19 are satisfied.
This is exactly the LP relaxation considered by Mar-
tins et al. (2009) in their multi-commodity flow
model, for the configuration with siblings and grand-
parent features.7 They also considered a config-
uration with non-projectivity features—which fire
if an arc is non-projective.8 That configuration
can also be obtained here if variables {nhh,mi} are
</bodyText>
<footnote confidence="0.926072">
7To be precise, the constraints of Martins et al. (2009) are
recovered after eliminating the path variables, via Eqs. 18–19.
8An arc (h, m) is non-projective if there is some word in its
span not descending from h (Kahane et al., 1998).
</footnote>
<bodyText confidence="0.9995389">
added to indicate non-projective arcs and OR-WITH-
OUTPUT hard constraint factors are inserted to en-
force nhh,mi = zhh,mi∧V min(h,m)&lt;j&lt;min(h,m) ¬phj.
Details are omitted for space.
In sum, although the approaches of Smith and Eis-
ner (2008) and Martins et al. (2009) look very dif-
ferent, in reality both are variational approximations
emanating from Prop. 1, respectively for marginal
and MAP inference. However, they operate on dis-
tinct factor graphs, respectively Figs. 1 and 3.9
</bodyText>
<sectionHeader confidence="0.997281" genericHeader="method">
5 Online Learning
</sectionHeader>
<bodyText confidence="0.999277666666667">
Our learning algorithm is presented in Alg. 1. It is a
generalized online learner that tackles E2-regularized
empirical risk minimization of the form
</bodyText>
<equation confidence="0.988012">
mine∈Rd 2kθk2 + m Em1 L(θ; xi, yi), (21)
</equation>
<bodyText confidence="0.99904825">
where each hxi, yii is a training example, A ≥ 0 is
the regularization constant, and L(θ; x, y) is a non-
negative convex loss. Examples include the logistic
loss used in CRFs (− log Pro(y|x)) and the hinge
loss of structured SVMs (maxy,∈%x) θ&gt;(φ(x, y0)−
φ(x, y)) + E(y0, y) for some cost function E). These
are both special cases of the family defined in Fig. 4,
which also includes the structured perceptron’s loss
(Q → ∞, -y = 0) and the softmax-margin loss of
Gimpel and Smith (2010; Q = -y = 1).
Alg. 1 is closely related to stochastic or online
gradient descent methods, but with the key advan-
tage of not needing a learning rate hyperparameter.
We sketch the derivation of Alg. 1; full details can
be found in Martins et al. (2010a). On the tth round,
one example hxt, yti is considered. We seek to solve
</bodyText>
<equation confidence="0.997875">
ming,ξ λm2 kθ − θtk2 + � (23)
s.t. L(θ; xt, yt) ≤ ≥ 0,
</equation>
<bodyText confidence="0.997274923076923">
9Given what was just exposed, it seems appealing to try
max-product loopy BP on the factor graph of Fig. 1, or sum-
product loopy BP on the one in Fig. 3. Both attempts present se-
rious challenges: the former requires computing messages sent
by the tree factor, which requires O(n2) calls to the Chu-Liu-
Edmonds algorithm and hence O(n5) time. No obvious strat-
egy seems to exist for simultaneous computation of all mes-
sages, unlike in the sum-product case. The latter is even more
challenging, as standard sum-product loopy BP has serious is-
sues in the factor graph of Fig. 3; we construct in Martins et al.
(2010b) a simple example with a very poor Bethe approxima-
tion. This might be fixed by using other variants of sum-product
BP, e.g., ones in which the entropy approximation is concave.
</bodyText>
<page confidence="0.894771">
40
</page>
<equation confidence="0.979985">
[ � �]
Lβ,γ(θ; x, y) g β 1 log �y&apos;∈�(x) exp β θ&gt;�φ(x, y0) − φ(x, y)� +γ`(y0, y) (22)
</equation>
<figureCaption confidence="0.95118025">
Figure 4: A family of loss functions including as particular cases the ones used in CRFs, structured SVMs, and the
structured perceptron. The hyperparameter β is the analogue of the inverse temperature in a Gibbs distribution, while
γ scales the cost. For any choice of β &gt; 0 and γ ≥ 0, the resulting loss function is convex in θ, since, up to a scale
factor, it is the composition of the (convex) log-sum-exp function with an affine map.
</figureCaption>
<construct confidence="0.334491">
Algorithm 1 Aggressive Online Learning
</construct>
<listItem confidence="0.804609363636364">
1: Input: {hxi, yii}mi=1, λ, number of epochs K
2: Initialize θ1 ← 0; set T = mK
3: fort = 1 to T do
4: Receive instance hxt, yti and set µt = χ(yt)
5: Solve Eq. 24 to obtain µt and Lβ,γ(θt, xt, yt)
6: Compute ∇Lβ,γ(θt, xt, yt)=F(xt)(fit−µt)
� 7 Compute 1 La,y(et;xt,yt) J
: �1t =min λm, k∇La,y(et;xt,yt)kI
8: Return θt+1 = θt − ηt∇Lβ,γ(θt; xt, yt)
9: end for
10: Return the averaged model
</listItem>
<bodyText confidence="0.985435">
which trades off conservativeness (stay close to the
most recent solution θt) and correctness (keep the
loss small). Alg. 1’s lines 7–8 are the result of tak-
ing the first-order Taylor approximation of L around
θt, which yields the lower bound L(θ; xt, yt) ≥
L(θt; xt, yt) + (θ − θt)&gt;∇L(θt; xt, yt), and plug-
ging that linear approximation into the constraint of
Eq. 23, which gives a simple Euclidean projection
problem (with slack) with a closed-form solution.
The online updating requires evaluating the loss
and computing its gradient. Both quantities can
be computed using the variational expression in
Prop. 1, for any loss Lβ,γ(θ; x, y) in Fig. 4.10 Our
only assumption is that the cost function `(y0, y)
can be written as a sum over factor-local costs; let-
ting µ = χ(y) and µ0 = χ(y0), this implies
`(y0, y) = p&gt;µ0 + q for some p and q which are
constant with respect to µ0.11 Under this assump-
tion, Lβ,γ(θ; x, y) becomes expressible in terms of
the log-partition function of a distribution whose
log-potentials are set to β(F(x)&gt;θ + γp). From
Eq. 9 and after some algebra, we finally obtain
Lβ,γ(θ;x,y) =
</bodyText>
<footnote confidence="0.99732375">
10Our description also applies to the (non-differentiable)
hinge loss case, when ,3 --+ oo, if we replace all instances of
“the gradient” in the text by “a subgradient.”
11For the Hamming cost, this holds with p = 1 − 2µ and
</footnote>
<equation confidence="0.739358333333333">
q = 1Tµ. See Taskar et al. (2006) for other examples.
θ&gt;F(x)(µ0−µ)+1βH(µ0)+γ(p&gt;µ0+q).
(24)
</equation>
<bodyText confidence="0.99976825">
Let µ� be a maximizer in Eq. 24; from the second
statement of Prop. 1 we obtain ∇Lβ,γ(θ; x, y) =
F(x)(µ−µ). When the inference problem in Eq. 24
is intractable, approximate message-passing algo-
rithms like loopy BP still allow us to obtain approx-
imations of the loss Lβ,γ and its gradient.
For the hinge loss, we arrive precisely at the max-
loss variant of 1-best MIRA (Crammer et al., 2006).
For the logistic loss, we arrive at a new online learn-
ing algorithm for CRFs that resembles stochastic
gradient descent but with an automatic step size that
follows from our variational representation.
Unsupported Features. As datasets grow, so do
the sets of features, creating further computational
challenges. Often only “supported” features—those
observed in the training data—are included, and
even those are commonly eliminated when their fre-
quencies fall below a threshold. Important infor-
mation may be lost as a result of these expedi-
ent choices. Formally, the supported feature set
is Fsupp g Umi=1 supp φ(xi, yi), where supp u °_
{j  |uj =6 0} denotes the support of vector u. Fsupp
is a subset of the complete feature set, comprised of
those features that occur in some candidate output,
</bodyText>
<equation confidence="0.9016455">
Fcomp Um U i). Features
i=1 y&apos; �∈�(x�) supp φ(xi, y0
</equation>
<bodyText confidence="0.990221428571429">
in Fcomp\Fsupp are called unsupported.
Sha and Pereira (2003) have shown that training a
CRF-based shallow parser with the complete feature
set may improve performance (over the supported
one), at the cost of 4.6 times more features. De-
pendency parsing has a much higher ratio (around
20 for bilexical word-word features, as estimated in
the Penn Treebank), due to the quadratic or faster
growth of the number of parts, of which only a few
are active in a legal output. We propose a simple
strategy for handling Fcomp efficiently, which can
be applied for those losses in Fig. 4 where β = ∞.
(e.g., the structured SVM and perceptron). Our pro-
cedure is the following: keep an active set F contain-
</bodyText>
<equation confidence="0.89465325">
1 T
θ�←T �t=1 θt
max
µ&apos;∈3V[(S.)
</equation>
<page confidence="0.997786">
41
</page>
<table confidence="0.999502">
CRF (TURBO PARS. #1) SVM (TURBO PARS. #2) |� |SVM (TURBO #2)
ARC-FACT. SEC. ORD. ARC-FACT. SEC. ORD. +NONPROJ., COMPL.
3�upp
ARABIC 78.28 79.12 79.04 79.42 6,643,191 2.8 80.02 (-0.14)
BULGARIAN 91.02 91.78 90.84 92.30 13,018,431 2.1 92.88 (+0.34) (†)
CHINESE 90.58 90.87 91.09 91.77 28,271,086 2.1 91.89 (+0.26)
CZECH 86.18 87.72 86.78 88.52 83,264,645 2.3 88.78 (+0.44) (†)
DANISH 89.58 90.08 89.78 90.78 7,900,061 2.3 91.50 (+0.68)
DUTCH 82.91 84.31 82.73 84.17 15,652,800 2.1 84.91 (-0.08)
GERMAN 89.34 90.58 89.04 91.19 49,934,403 2.5 91.49 (+0.32) (†)
JAPANESE 92.90 93.22 93.18 93.38 4,256,857 2.2 93.42 (+0.32)
PORTUGUESE 90.64 91.00 90.56 91.50 16,067,150 2.1 91.87 (-0.04)
SLOVENE 83.03 83.17 83.49 84.35 4,603,295 2.7 85.53 (+0.80)
SPANISH 83.83 85.07 84.19 85.95 11,629,964 2.6 87.04 (+0.50) (†)
SWEDISH 87.81 89.01 88.55 88.99 18,374,160 2.8 89.80 (+0.42)
TURKISH 76.86 76.28 74.79 76.10 6,688,373 2.2 76.62 (+0.62)
ENGLISH NON-PROJ. 90.15 91.08 90.66 91.79 57,615,709 2.5 92.13 (+0.12)
ENGLISH PROJ. 91.23 91.94 91.65 92.91 55,247,093 2.4 93.26 (+0.41) (†)
</table>
<tableCaption confidence="0.9031345">
Table 2: Unlabeled attachment scores, ignoring punctuation. The leftmost columns show the performance of arc-
factored and second-order models for the CRF and SVM losses, after 10 epochs with 1/(am) = 0.001 (tuned on the
</tableCaption>
<figureCaption confidence="0.842887285714286">
English Non-Proj. dev.-set). The rightmost columns refer to a model to which non-projectivity features were added,
trained under the SVM loss, that handles the complete feature set. Shown is the total number of features instantiated,
the multiplicative factor w.r.t. the number of supported features, and the accuracies (in parenthesis, we display the
difference w.r.t. a model trained with the supported features only). Entries marked with † are the highest reported in
the literature, to the best of our knowledge, beating (sometimes slightly) McDonald et al. (2006), Martins et al. (2008),
Martins et al. (2009), and, in the case of English Proj., also the third-order parser of Koo and Collins (2010), which
achieves 93.04% on that dataset (their experiments in Czech are not comparable, since the datasets are different).
</figureCaption>
<bodyText confidence="0.999171636363636">
ing all features that have been instantiated in Alg. 1.
At each round, run lines 4–5 as usual, using only
features in T. Since the other features have not been
used before, they have a zero weight, hence can be
ignored. When Q = oo, the variational problem in
Eq. 24 consists of a MAP computation and the solu-
tion corresponds to one output Yt E �(xt). Only the
parts that are active in Yt but not in yt, or vice-versa,
will have features that might receive a nonzero up-
date. Those parts are reexamined for new features
and the active set T is updated accordingly.
</bodyText>
<sectionHeader confidence="0.999748" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999818027027027">
We trained non-projective dependency parsers for
14 languages, using datasets from the CoNLL-X
shared task (Buchholz and Marsi, 2006) and two
datasets for English: one from the CoNLL-2008
shared task (Surdeanu et al., 2008), which contains
non-projective arcs, and another derived from the
Penn Treebank applying the standard head rules of
Yamada and Matsumoto (2003), in which all parse
trees are projective.12 We implemented Alg. 1,
12We used the provided train/test splits for all datasets. For
English, we used the standard test partitions (section 23 of the
Wall Street Journal). We did not exploit the fact that some
datasets only contain projective trees and have unique roots.
which handles any loss function Lo�,y.13 When 0 &lt;
oo, Turbo Parser #1 and the loopy BP algorithm of
Smith and Eisner (2008) is used; otherwise, Turbo
Parser #2 is used and the LP relaxation is solved with
CPLEX. In both cases, we employed the same prun-
ing strategy as Martins et al. (2009).
Two different feature configurations were first
tried: an arc-factored model and a model with
second-order features (siblings and grandparents).
We used the same arc-factored features as McDon-
ald et al. (2005) and second-order features that con-
join words and lemmas (at most two), parts-of-
speech tags, and (if available) morphological infor-
mation; this was the same set of features as in Mar-
tins et al. (2009). Table 2 shows the results obtained
in both configurations, for CRF and SVM loss func-
tions. While in the arc-factored case performance is
similar, in second-order models there seems to be a
consistent gain when the SVM loss is used. There
are two possible reasons: first, SVMs take the cost
function into consideration; second, Turbo Parser #2
is less approximate than Turbo Parser #1, since only
the marginal polytope is approximated (the entropy
function is not involved).
</bodyText>
<footnote confidence="0.947881">
13The code is available at http://www.ark.cs.cmu.edu/
TurboParser.
</footnote>
<page confidence="0.990477">
42
</page>
<table confidence="0.94154875">
0 1 1 1 1 3 5 oo
7 0 (CRF) 1 3 5 1 1 1 (SVM)
ARC-F. 90.15 90.41 90.38 90.53 90.80 90.83 90.66
2 ORD. 91.08 91.85 91.89 91.51 92.04 91.98 91.79
</table>
<tableCaption confidence="0.826531">
Table 3: Varying β and γ: neither the CRF nor the
SVM is optimal. Results are UAS on the English Non-
Projective dataset, with λ tuned with dev.-set validation.
</tableCaption>
<bodyText confidence="0.999925666666667">
The loopy BP algorithm managed to converge for
nearly all sentences (with message damping). The
last three columns show the beneficial effect of un-
supported features for the SVM case (with a more
powerful model with non-projectivity features). For
most languages, unsupported features convey help-
ful information, which can be used with little extra
cost (on average, 2.5 times more features are instan-
tiated). A combination of the techniques discussed
here yields parsers that are in line with very strong
competitors—for example, the parser of Koo and
Collins (2010), which is exact, third-order, and con-
strains the outputs to be projective, does not outper-
form ours on the projective English dataset.14
Finally, Table 3 shows results obtained for differ-
ent settings of β and γ. Interestingly, we observe
that higher scores are obtained for loss functions that
are “between” SVMs and CRFs.
</bodyText>
<sectionHeader confidence="0.999935" genericHeader="method">
7 Related Work
</sectionHeader>
<bodyText confidence="0.99984425">
There has been recent work studying efficient com-
putation of messages in combinatorial factors: bi-
partite matchings (Duchi et al., 2007), projective
and non-projective arborescences (Smith and Eis-
ner, 2008), as well as high order factors with count-
based potentials (Tarlow et al., 2010), among others.
Some of our combinatorial factors (OR, OR-WITH-
OUTPUT) and the analogous entropy computations
were never considered, to the best of our knowledge.
Prop. 1 appears in Wainwright and Jordan (2008)
for canonical overcomplete models; we adapt it here
for models with shared features. We rely on the vari-
ational interpretation of loopy BP, due to Yedidia et
al. (2001), to derive the objective being optimized
by Smith and Eisner’s loopy BP parser.
Independently of our work, Koo et al. (2010)
</bodyText>
<footnote confidence="0.5747942">
14This might be due to the fact that Koo and Collins (2010)
trained with the perceptron algorithm and did not use unsup-
ported features. Experiments plugging the perceptron loss
(CI --+ oo, 7 --+ 0) into Alg. 1 yielded worse performance than
with the hinge loss.
</footnote>
<bodyText confidence="0.9998973">
recently proposed an efficient dual decomposition
method to solve an LP problem similar (but not
equal) to the one in Eq. 20,15 with excellent pars-
ing performance. Their parser is also an instance
of a turbo parser since it relies on a local approxi-
mation of a marginal polytope. While one can also
use dual decomposition to address our MAP prob-
lem, the fact that our model does not decompose as
nicely as the one in Koo et al. (2010) would likely
result in slower convergence.
</bodyText>
<sectionHeader confidence="0.997329" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999987176470588">
We presented a unified view of two recent approxi-
mate dependency parsers, by stating their underlying
factor graphs and by deriving the variational prob-
lems that they address. We introduced new hard con-
straint factors, along with formulae for their mes-
sages, local belief constraints, and entropies. We
provided an aggressive online algorithm for training
the models with a broad family of losses.
There are several possible directions for future
work. Recent progress in message-passing algo-
rithms yield “convexified” Bethe approximations
that can be used for marginal inference (Wainwright
et al., 2005), and provably convergent max-product
variants that solve the relaxed LP (Globerson and
Jaakkola, 2008). Other parsing formalisms can be
handled with the inventory of factors shown here—
among them, phrase-structure parsing.
</bodyText>
<sectionHeader confidence="0.998931" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.627684818181818">
The authors would like to thank the reviewers for their
comments, and Kevin Gimpel, David Smith, David Son-
tag, and Terry Koo for helpful discussions. A. M. was
supported by a grant from FCT/ICTI through the CMU-
Portugal Program, and also by Priberam Inform´atica.
N. S. was supported in part by Qatar NRF NPRP-08-485-
1-083. E. X. was supported by AFOSR FA9550010247,
ONR N000140910758, NSF CAREER DBI-0546594,
NSF IIS-0713379, and an Alfred P. Sloan Fellowship.
M. F. and P. A. were supported by the FET programme
(EU FP7), under the SIMBAD project (contract 213250).
</bodyText>
<footnote confidence="0.76392425">
15The difference is that the model of Koo et al. (2010)
includes features that depend on consecutive siblings—
making it decompose into subproblems amenable to dynamic
programming—while we have factors for all pairs of siblings.
</footnote>
<page confidence="0.999811">
43
</page>
<sectionHeader confidence="0.993914" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999919830188679">
C. Berrou, A. Glavieux, and P. Thitimajshima. 1993.
Near Shannon limit error-correcting coding and decod-
ing. In Proc. of ICC, volume 93, pages 1064–1070.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared task
on multilingual dependency parsing. In CoNLL.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. JMLR, 7:551–585.
J. Duchi, D. Tarlow, G. Elidan, and D. Koller. 2007.
Using combinatorial optimization within max-product
belief propagation. NIPS, 19.
J. R. Finkel, A. Kleeman, and C. D. Manning. 2008. Effi-
cient, feature-based, conditional random field parsing.
Proc. of ACL.
K. Gimpel and N. A. Smith. 2010. Softmax-margin
crfs: Training log-linear models with loss functions.
In Proc. of NAACL.
A. Globerson and T. Jaakkola. 2008. Fixing max-
product: Convergent message passing algorithms for
MAP LP-relaxations. NIPS, 20.
L. Huang. 2008. Forest reranking: Discriminative pars-
ing with non-local features. In Proc. of ACL.
S. Kahane, A. Nasr, and O. Rambow. 1998. Pseudo-
projectivity: a polynomially parsable non-projective
dependency grammar. In Proc. of COLING.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proc. of ACL.
T. Koo, A. Globerson, X. Carreras, and M. Collins. 2007.
Structured prediction models via the matrix-tree theo-
rem. In Proc. of EMNLP.
T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual decomposition for parsing with non-
projective head automata. In Proc. of EMNLP.
F. R. Kschischang, B. J. Frey, and H. A. Loeliger. 2001.
Factor graphs and the sum-product algorithm. IEEE
Trans. Inf. Theory, 47(2):498–519.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. ofICML.
A. F. T. Martins, D. Das, N. A. Smith, and E. P. Xing.
2008. Stacking dependency parsers. In EMNLP.
A. F. T. Martins, N. A. Smith, and E. P. Xing. 2009.
Concise integer linear programming formulations for
dependency parsing. In Proc. of ACL-IJCNLP.
A. F. T. Martins, K. Gimpel, N. A. Smith, E. P. Xing,
P. M. Q. Aguiar, and M. A. T. Figueiredo. 2010a.
Learning structured classifiers with dual coordinate
descent. Technical Report CMU-ML-10-109.
A. F. T. Martins, N. A. Smith, E. P. Xing, P. M. Q. Aguiar,
and M. A. T. Figueiredo. 2010b. Turbo parsers:
Dependency parsing by approximate variational infer-
ence (extended version).
A. McCallum, K. Schultz, and S. Singh. 2009. Fac-
torie: Probabilistic programming via imperatively de-
fined factor graphs. In NIPS.
R. T. McDonald, F. Pereira, K. Ribarov, and J. Hajic.
2005. Non-projective dependency parsing using span-
ning tree algorithms. In Proc. of HLT-EMNLP.
R. McDonald, K. Lerman, and F. Pereira. 2006. Multi-
lingual dependency analysis with a two-stage discrim-
inative parser. In Proc. of CoNLL.
R. J. McEliece, D. J. C. MacKay, and J. F. Cheng. 1998.
Turbo decoding as an instance of Pearl’s “belief prop-
agation” algorithm. IEEE Journal on Selected Areas
in Communications, 16(2).
J. Pearl. 1988. Probabilistic Reasoning in Intelligent
Systems: Networks of Plausible Inference. Morgan
Kaufmann.
F. Sha and F. Pereira. 2003. Shallow parsing with condi-
tional random fields. In Proc. of HLT-NAACL.
D. A. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In Proc. of EMNLP.
D. A. Smith and N. A. Smith. 2007. Probabilistic models
of nonprojective dependency trees. In EMNLP.
M. Surdeanu, R. Johansson, A. Meyers, L. M`arquez, and
J. Nivre. 2008. The CoNLL-2008 shared task on
joint parsing of syntactic and semantic dependencies.
CoNLL.
C. Sutton, A. McCallum, and K. Rohanimanesh. 2007.
Dynamic conditional random fields: Factorized prob-
abilistic models for labeling and segmenting sequence
data. JMLR, 8:693–723.
R. E. Tarjan. 1977. Finding optimum branchings. Net-
works, 7(1):25–36.
D. Tarlow, I. E. Givoni, and R. S. Zemel. 2010. HOP-
MAP: Efficient message passing with high order po-
tentials. In Proc. of AISTATS.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In NIPS.
B. Taskar, S. Lacoste-Julien, and M. I. Jordan. 2006.
Structured prediction, dual extragradient and Bregman
projections. JMLR, 7:1627–1653.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2004. Support vector machine learning for interdepen-
dent and structured output spaces. In Proc. of ICML.
M. J. Wainwright and M. I. Jordan. 2008. Graphical
Models, Exponential Families, and Variational Infer-
ence. Now Publishers.
M. J. Wainwright, T.S. Jaakkola, and A.S. Willsky. 2005.
A new class of upper bounds on the log partition func-
tion. IEEE Trans. Inf. Theory, 51(7):2313–2335.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
of IWPT.
J. S. Yedidia, W. T. Freeman, and Y. Weiss. 2001. Gen-
eralized belief propagation. In NIPS.
</reference>
<page confidence="0.999294">
44
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.687668">
<title confidence="0.977826">Turbo Parsers: Dependency Parsing by Approximate Variational Inference</title>
<author confidence="0.995318">F T A Eric P</author>
<affiliation confidence="0.9968605">of Computer Carnegie Mellon</affiliation>
<address confidence="0.996656">Pittsburgh, PA 15213,</address>
<author confidence="0.985091">M Q</author>
<affiliation confidence="0.9712075">de Sistemas e Instituto Superior</affiliation>
<address confidence="0.889144">Lisboa,</address>
<email confidence="0.995732">aguiar@isr.ist.utl.pt</email>
<abstract confidence="0.992434833333334">We present a unified view of two state-of-theart non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al. (2009). By representing the model assumptions with a factor graph, we shed light on the optimization problems tackled in each method. We also propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation. The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, including CRFs and structured SVMs. Experiments show state-of-the-art performance for 14 languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Berrou</author>
<author>A Glavieux</author>
<author>P Thitimajshima</author>
</authors>
<title>Near Shannon limit error-correcting coding and decoding.</title>
<date>1993</date>
<booktitle>In Proc. of ICC,</booktitle>
<volume>93</volume>
<pages>1064--1070</pages>
<contexts>
<context position="3542" citStr="Berrou et al. (1993)" startWordPosition="512" endWordPosition="515">nd, we propose an aggressive online algorithm that generalizes MIRA (Crammer et al., 2006) to arbitrary loss functions. We adopt a family of losses subsuming CRFs (Lafferty et al., 2001) and structured SVMs (Taskar et al., 2003; Tsochantaridis et al., 2004). Finally, we present a technique for including features not attested in the training data, allowing for richer models without substantial runtime costs. Our experiments (§6) show state-of-the-art performance on dependency parsing benchmarks. 1The name stems from “turbo codes,” a class of highperformance error-correcting codes introduced by Berrou et al. (1993) for which decoding algorithms are equivalent to running belief propagation in a graph with loops (McEliece et al., 1998). Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 34–44, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics 2 Structured Inference and Factor Graphs Denote by X a set of input objects from which we want to infer some hidden structure conveyed in an output set Y. Each input x ∈ X (e.g., a sentence) is associated with a set of candidate outputs Y(x) ⊆ Y (e.g., parse trees); we are intereste</context>
</contexts>
<marker>Berrou, Glavieux, Thitimajshima, 1993</marker>
<rawString>C. Berrou, A. Glavieux, and P. Thitimajshima. 1993. Near Shannon limit error-correcting coding and decoding. In Proc. of ICC, volume 93, pages 1064–1070.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>E Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="35830" citStr="Buchholz and Marsi, 2006" startWordPosition="6170" endWordPosition="6173">s usual, using only features in T. Since the other features have not been used before, they have a zero weight, hence can be ignored. When Q = oo, the variational problem in Eq. 24 consists of a MAP computation and the solution corresponds to one output Yt E �(xt). Only the parts that are active in Yt but not in yt, or vice-versa, will have features that might receive a nonzero update. Those parts are reexamined for new features and the active set T is updated accordingly. 6 Experiments We trained non-projective dependency parsers for 14 languages, using datasets from the CoNLL-X shared task (Buchholz and Marsi, 2006) and two datasets for English: one from the CoNLL-2008 shared task (Surdeanu et al., 2008), which contains non-projective arcs, and another derived from the Penn Treebank applying the standard head rules of Yamada and Matsumoto (2003), in which all parse trees are projective.12 We implemented Alg. 1, 12We used the provided train/test splits for all datasets. For English, we used the standard test partitions (section 23 of the Wall Street Journal). We did not exploit the fact that some datasets only contain projective trees and have unique roots. which handles any loss function Lo�,y.13 When 0 </context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>S. Buchholz and E. Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>O Dekel</author>
<author>J Keshet</author>
<author>S Shalev-Shwartz</author>
<author>Y Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>JMLR,</journal>
<pages>7--551</pages>
<contexts>
<context position="3012" citStr="Crammer et al., 2006" startWordPosition="429" endWordPosition="432">ecting coding. Throughtout, we call these turbo parsers.1 Our contributions are not limited to dependency parsing: we present a general method for inference in factor graphs with hard constraints (§2), which extends some combinatorial factors considered by Smith and Eisner (2008). After presenting a geometric view of the variational approximations underlying message-passing algorithms (§3), and closing the gap between the two aforementioned parsers (§4), we consider the problem of learning the model parameters (§5). To this end, we propose an aggressive online algorithm that generalizes MIRA (Crammer et al., 2006) to arbitrary loss functions. We adopt a family of losses subsuming CRFs (Lafferty et al., 2001) and structured SVMs (Taskar et al., 2003; Tsochantaridis et al., 2004). Finally, we present a technique for including features not attested in the training data, allowing for richer models without substantial runtime costs. Our experiments (§6) show state-of-the-art performance on dependency parsing benchmarks. 1The name stems from “turbo codes,” a class of highperformance error-correcting codes introduced by Berrou et al. (1993) for which decoding algorithms are equivalent to running belief propag</context>
<context position="31420" citStr="Crammer et al., 2006" startWordPosition="5447" endWordPosition="5450">--+ oo, if we replace all instances of “the gradient” in the text by “a subgradient.” 11For the Hamming cost, this holds with p = 1 − 2µ and q = 1Tµ. See Taskar et al. (2006) for other examples. θ&gt;F(x)(µ0−µ)+1βH(µ0)+γ(p&gt;µ0+q). (24) Let µ� be a maximizer in Eq. 24; from the second statement of Prop. 1 we obtain ∇Lβ,γ(θ; x, y) = F(x)(µ−µ). When the inference problem in Eq. 24 is intractable, approximate message-passing algorithms like loopy BP still allow us to obtain approximations of the loss Lβ,γ and its gradient. For the hinge loss, we arrive precisely at the maxloss variant of 1-best MIRA (Crammer et al., 2006). For the logistic loss, we arrive at a new online learning algorithm for CRFs that resembles stochastic gradient descent but with an automatic step size that follows from our variational representation. Unsupported Features. As datasets grow, so do the sets of features, creating further computational challenges. Often only “supported” features—those observed in the training data—are included, and even those are commonly eliminated when their frequencies fall below a threshold. Important information may be lost as a result of these expedient choices. Formally, the supported feature set is Fsup</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz, and Y. Singer. 2006. Online passive-aggressive algorithms. JMLR, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Duchi</author>
<author>D Tarlow</author>
<author>G Elidan</author>
<author>D Koller</author>
</authors>
<title>Using combinatorial optimization within max-product belief propagation.</title>
<date>2007</date>
<journal>NIPS,</journal>
<volume>19</volume>
<contexts>
<context position="38972" citStr="Duchi et al., 2007" startWordPosition="6693" endWordPosition="6696"> of the techniques discussed here yields parsers that are in line with very strong competitors—for example, the parser of Koo and Collins (2010), which is exact, third-order, and constrains the outputs to be projective, does not outperform ours on the projective English dataset.14 Finally, Table 3 shows results obtained for different settings of β and γ. Interestingly, we observe that higher scores are obtained for loss functions that are “between” SVMs and CRFs. 7 Related Work There has been recent work studying efficient computation of messages in combinatorial factors: bipartite matchings (Duchi et al., 2007), projective and non-projective arborescences (Smith and Eisner, 2008), as well as high order factors with countbased potentials (Tarlow et al., 2010), among others. Some of our combinatorial factors (OR, OR-WITHOUTPUT) and the analogous entropy computations were never considered, to the best of our knowledge. Prop. 1 appears in Wainwright and Jordan (2008) for canonical overcomplete models; we adapt it here for models with shared features. We rely on the variational interpretation of loopy BP, due to Yedidia et al. (2001), to derive the objective being optimized by Smith and Eisner’s loopy BP</context>
</contexts>
<marker>Duchi, Tarlow, Elidan, Koller, 2007</marker>
<rawString>J. Duchi, D. Tarlow, G. Elidan, and D. Koller. 2007. Using combinatorial optimization within max-product belief propagation. NIPS, 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>A Kleeman</author>
<author>C D Manning</author>
</authors>
<title>Efficient, feature-based, conditional random field parsing.</title>
<date>2008</date>
<booktitle>Proc. of ACL.</booktitle>
<contexts>
<context position="1275" citStr="Finkel et al., 2008" startWordPosition="180" endWordPosition="183">light on the optimization problems tackled in each method. We also propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation. The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, including CRFs and structured SVMs. Experiments show state-of-the-art performance for 14 languages. 1 Introduction Feature-rich discriminative models that break locality/independence assumptions can boost a parser’s performance (McDonald et al., 2006; Huang, 2008; Finkel et al., 2008; Smith and Eisner, 2008; Martins et al., 2009; Koo and Collins, 2010). Often, inference with such models becomes computationally intractable, causing a demand for understanding and improving approximate parsing algorithms. In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing: loopy belief propagation (Smith and Eisner, 2008) and linear programming relaxation (Martins et al., 2009). While those two parsers are differently motivated, we show that both correspond to inference in 34 M´ario A. T. Figueiredo†</context>
</contexts>
<marker>Finkel, Kleeman, Manning, 2008</marker>
<rawString>J. R. Finkel, A. Kleeman, and C. D. Manning. 2008. Efficient, feature-based, conditional random field parsing. Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Gimpel</author>
<author>N A Smith</author>
</authors>
<title>Softmax-margin crfs: Training log-linear models with loss functions.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="27479" citStr="Gimpel and Smith (2010" startWordPosition="4726" endWordPosition="4729">a generalized online learner that tackles E2-regularized empirical risk minimization of the form mine∈Rd 2kθk2 + m Em1 L(θ; xi, yi), (21) where each hxi, yii is a training example, A ≥ 0 is the regularization constant, and L(θ; x, y) is a nonnegative convex loss. Examples include the logistic loss used in CRFs (− log Pro(y|x)) and the hinge loss of structured SVMs (maxy,∈%x) θ&gt;(φ(x, y0)− φ(x, y)) + E(y0, y) for some cost function E). These are both special cases of the family defined in Fig. 4, which also includes the structured perceptron’s loss (Q → ∞, -y = 0) and the softmax-margin loss of Gimpel and Smith (2010; Q = -y = 1). Alg. 1 is closely related to stochastic or online gradient descent methods, but with the key advantage of not needing a learning rate hyperparameter. We sketch the derivation of Alg. 1; full details can be found in Martins et al. (2010a). On the tth round, one example hxt, yti is considered. We seek to solve ming,ξ λm2 kθ − θtk2 + � (23) s.t. L(θ; xt, yt) ≤ ≥ 0, 9Given what was just exposed, it seems appealing to try max-product loopy BP on the factor graph of Fig. 1, or sumproduct loopy BP on the one in Fig. 3. Both attempts present serious challenges: the former requires compu</context>
</contexts>
<marker>Gimpel, Smith, 2010</marker>
<rawString>K. Gimpel and N. A. Smith. 2010. Softmax-margin crfs: Training log-linear models with loss functions. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Globerson</author>
<author>T Jaakkola</author>
</authors>
<title>Fixing maxproduct: Convergent message passing algorithms for MAP LP-relaxations.</title>
<date>2008</date>
<journal>NIPS,</journal>
<volume>20</volume>
<contexts>
<context position="41087" citStr="Globerson and Jaakkola, 2008" startWordPosition="7038" endWordPosition="7041">ating their underlying factor graphs and by deriving the variational problems that they address. We introduced new hard constraint factors, along with formulae for their messages, local belief constraints, and entropies. We provided an aggressive online algorithm for training the models with a broad family of losses. There are several possible directions for future work. Recent progress in message-passing algorithms yield “convexified” Bethe approximations that can be used for marginal inference (Wainwright et al., 2005), and provably convergent max-product variants that solve the relaxed LP (Globerson and Jaakkola, 2008). Other parsing formalisms can be handled with the inventory of factors shown here— among them, phrase-structure parsing. Acknowledgments The authors would like to thank the reviewers for their comments, and Kevin Gimpel, David Smith, David Sontag, and Terry Koo for helpful discussions. A. M. was supported by a grant from FCT/ICTI through the CMUPortugal Program, and also by Priberam Inform´atica. N. S. was supported in part by Qatar NRF NPRP-08-485- 1-083. E. X. was supported by AFOSR FA9550010247, ONR N000140910758, NSF CAREER DBI-0546594, NSF IIS-0713379, and an Alfred P. Sloan Fellowship. </context>
</contexts>
<marker>Globerson, Jaakkola, 2008</marker>
<rawString>A. Globerson and T. Jaakkola. 2008. Fixing maxproduct: Convergent message passing algorithms for MAP LP-relaxations. NIPS, 20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1254" citStr="Huang, 2008" startWordPosition="178" endWordPosition="179">aph, we shed light on the optimization problems tackled in each method. We also propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation. The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, including CRFs and structured SVMs. Experiments show state-of-the-art performance for 14 languages. 1 Introduction Feature-rich discriminative models that break locality/independence assumptions can boost a parser’s performance (McDonald et al., 2006; Huang, 2008; Finkel et al., 2008; Smith and Eisner, 2008; Martins et al., 2009; Koo and Collins, 2010). Often, inference with such models becomes computationally intractable, causing a demand for understanding and improving approximate parsing algorithms. In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing: loopy belief propagation (Smith and Eisner, 2008) and linear programming relaxation (Martins et al., 2009). While those two parsers are differently motivated, we show that both correspond to inference in 34 M´a</context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>L. Huang. 2008. Forest reranking: Discriminative parsing with non-local features. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kahane</author>
<author>A Nasr</author>
<author>O Rambow</author>
</authors>
<title>Pseudoprojectivity: a polynomially parsable non-projective dependency grammar.</title>
<date>1998</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="26305" citStr="Kahane et al., 1998" startWordPosition="4526" endWordPosition="4529">. Eqs. 12 and 15–19 are satisfied. This is exactly the LP relaxation considered by Martins et al. (2009) in their multi-commodity flow model, for the configuration with siblings and grandparent features.7 They also considered a configuration with non-projectivity features—which fire if an arc is non-projective.8 That configuration can also be obtained here if variables {nhh,mi} are 7To be precise, the constraints of Martins et al. (2009) are recovered after eliminating the path variables, via Eqs. 18–19. 8An arc (h, m) is non-projective if there is some word in its span not descending from h (Kahane et al., 1998). added to indicate non-projective arcs and OR-WITHOUTPUT hard constraint factors are inserted to enforce nhh,mi = zhh,mi∧V min(h,m)&lt;j&lt;min(h,m) ¬phj. Details are omitted for space. In sum, although the approaches of Smith and Eisner (2008) and Martins et al. (2009) look very different, in reality both are variational approximations emanating from Prop. 1, respectively for marginal and MAP inference. However, they operate on distinct factor graphs, respectively Figs. 1 and 3.9 5 Online Learning Our learning algorithm is presented in Alg. 1. It is a generalized online learner that tackles E2-reg</context>
</contexts>
<marker>Kahane, Nasr, Rambow, 1998</marker>
<rawString>S. Kahane, A. Nasr, and O. Rambow. 1998. Pseudoprojectivity: a polynomially parsable non-projective dependency grammar. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>M Collins</author>
</authors>
<title>Efficient third-order dependency parsers.</title>
<date>2010</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1345" citStr="Koo and Collins, 2010" startWordPosition="192" endWordPosition="195">propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation. The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, including CRFs and structured SVMs. Experiments show state-of-the-art performance for 14 languages. 1 Introduction Feature-rich discriminative models that break locality/independence assumptions can boost a parser’s performance (McDonald et al., 2006; Huang, 2008; Finkel et al., 2008; Smith and Eisner, 2008; Martins et al., 2009; Koo and Collins, 2010). Often, inference with such models becomes computationally intractable, causing a demand for understanding and improving approximate parsing algorithms. In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing: loopy belief propagation (Smith and Eisner, 2008) and linear programming relaxation (Martins et al., 2009). While those two parsers are differently motivated, we show that both correspond to inference in 34 M´ario A. T. Figueiredo††Instituto de Telecomunicac¸˜oes Instituto Superior T´ecnico Lisboa, P</context>
<context position="34996" citStr="Koo and Collins (2010)" startWordPosition="6025" endWordPosition="6028">rojectivity features were added, trained under the SVM loss, that handles the complete feature set. Shown is the total number of features instantiated, the multiplicative factor w.r.t. the number of supported features, and the accuracies (in parenthesis, we display the difference w.r.t. a model trained with the supported features only). Entries marked with † are the highest reported in the literature, to the best of our knowledge, beating (sometimes slightly) McDonald et al. (2006), Martins et al. (2008), Martins et al. (2009), and, in the case of English Proj., also the third-order parser of Koo and Collins (2010), which achieves 93.04% on that dataset (their experiments in Czech are not comparable, since the datasets are different). ing all features that have been instantiated in Alg. 1. At each round, run lines 4–5 as usual, using only features in T. Since the other features have not been used before, they have a zero weight, hence can be ignored. When Q = oo, the variational problem in Eq. 24 consists of a MAP computation and the solution corresponds to one output Yt E �(xt). Only the parts that are active in Yt but not in yt, or vice-versa, will have features that might receive a nonzero update. Th</context>
<context position="38497" citStr="Koo and Collins (2010)" startWordPosition="6616" endWordPosition="6619">ective dataset, with λ tuned with dev.-set validation. The loopy BP algorithm managed to converge for nearly all sentences (with message damping). The last three columns show the beneficial effect of unsupported features for the SVM case (with a more powerful model with non-projectivity features). For most languages, unsupported features convey helpful information, which can be used with little extra cost (on average, 2.5 times more features are instantiated). A combination of the techniques discussed here yields parsers that are in line with very strong competitors—for example, the parser of Koo and Collins (2010), which is exact, third-order, and constrains the outputs to be projective, does not outperform ours on the projective English dataset.14 Finally, Table 3 shows results obtained for different settings of β and γ. Interestingly, we observe that higher scores are obtained for loss functions that are “between” SVMs and CRFs. 7 Related Work There has been recent work studying efficient computation of messages in combinatorial factors: bipartite matchings (Duchi et al., 2007), projective and non-projective arborescences (Smith and Eisner, 2008), as well as high order factors with countbased potenti</context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>T. Koo and M. Collins. 2010. Efficient third-order dependency parsers. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>A Globerson</author>
<author>X Carreras</author>
<author>M Collins</author>
</authors>
<title>Structured prediction models via the matrix-tree theorem.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="7643" citStr="Koo et al., 2007" startWordPosition="1230" endWordPosition="1233"> spanning tree. There is a unary soft factor per arc, whose log-potential reflects the score of that arc. There are also O(n3) pairwise factors; their log-potentials reflect the scores of sibling and grandparent arcs. These factors create loops, thus calling for approximate inference. Without them, the model is arc-factored, and exact inference in it is well studied: finding the most probable parse tree takes O(n3) time with the ChuLiu-Edmonds algorithm (McDonald et al., 2005),2 and computing posterior marginals for all arcs takes O(n3) time via the matrix-tree theorem (Smith and Smith, 2007; Koo et al., 2007). Message-passing algorithms. In general factor graphs, both inference problems— obtaining the most probable output (the MAP) argmaxyE%x) Pre(y|x), and computing the marginals Pre(Yi = yi|x)—can be addressed with the belief propagation (BP) algorithm (Pearl, 1988), which iteratively passes messages between variables and factors reflecting their local “beliefs.” 2There is a faster but more involved O(n2) algorithm due to Tarjan (1977). 35 �1 v1, . . . , vn ∈ SC A general binary factor: ΨC(v1, ... , vn) = where SC ⊆ {0, 1}n. 0 otherwise, • Message-induced distribution: ω , hmj→Cij=1,...,n • Part</context>
</contexts>
<marker>Koo, Globerson, Carreras, Collins, 2007</marker>
<rawString>T. Koo, A. Globerson, X. Carreras, and M. Collins. 2007. Structured prediction models via the matrix-tree theorem. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>A M Rush</author>
<author>M Collins</author>
<author>T Jaakkola</author>
<author>D Sontag</author>
</authors>
<title>Dual decomposition for parsing with nonprojective head automata.</title>
<date>2010</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="39625" citStr="Koo et al. (2010)" startWordPosition="6798" endWordPosition="6801">rescences (Smith and Eisner, 2008), as well as high order factors with countbased potentials (Tarlow et al., 2010), among others. Some of our combinatorial factors (OR, OR-WITHOUTPUT) and the analogous entropy computations were never considered, to the best of our knowledge. Prop. 1 appears in Wainwright and Jordan (2008) for canonical overcomplete models; we adapt it here for models with shared features. We rely on the variational interpretation of loopy BP, due to Yedidia et al. (2001), to derive the objective being optimized by Smith and Eisner’s loopy BP parser. Independently of our work, Koo et al. (2010) 14This might be due to the fact that Koo and Collins (2010) trained with the perceptron algorithm and did not use unsupported features. Experiments plugging the perceptron loss (CI --+ oo, 7 --+ 0) into Alg. 1 yielded worse performance than with the hinge loss. recently proposed an efficient dual decomposition method to solve an LP problem similar (but not equal) to the one in Eq. 20,15 with excellent parsing performance. Their parser is also an instance of a turbo parser since it relies on a local approximation of a marginal polytope. While one can also use dual decomposition to address our </context>
</contexts>
<marker>Koo, Rush, Collins, Jaakkola, Sontag, 2010</marker>
<rawString>T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Sontag. 2010. Dual decomposition for parsing with nonprojective head automata. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F R Kschischang</author>
<author>B J Frey</author>
<author>H A Loeliger</author>
</authors>
<title>Factor graphs and the sum-product algorithm.</title>
<date>2001</date>
<journal>IEEE Trans. Inf. Theory,</journal>
<volume>47</volume>
<issue>2</issue>
<contexts>
<context position="5079" citStr="Kschischang et al., 2001" startWordPosition="783" endWordPosition="786">e, in unlabeled dependency parsing, I is the number of candidate dependency arcs (quadratic in the sentence length), and each Yi = {0, 1}. Of course, the yi are highly interdependent. Factor Graphs. Probabilistic models like CRFs (Lafferty et al., 2001) assume a factorization of the conditional distribution of Y , Pr(Y = y |X = x) ∝ HCEC &apos;FC(x,yC), (1) where each C ⊆ {1, ... , I} is a factor, C is the set of factors, each yC °_ hyiiiEC denotes a partial output assignment, and each &apos;FC is a nonnegative potential function that depends on the output only via its restriction to C. A factor graph (Kschischang et al., 2001) is a convenient representation for the factorization in Eq. 1: it is a bipartite graph Gx comprised of variable nodes {1, ... , I} and factor nodes C ∈ C, with an edge connecting the ith variable node and a factor node C iff i ∈ C. Hence, the factor graph Gx makes explicit the direct dependencies among the variables {y1, ... , yI}. Factor graphs have been used for several NLP tasks, such as dependency parsing, segmentation, and co-reference resolution (Sutton et al., 2007; Smith and Eisner, 2008; McCallum et al., 2009). Hard and Soft Constraint Factors. It may be the case that valid outputs a</context>
</contexts>
<marker>Kschischang, Frey, Loeliger, 2001</marker>
<rawString>F. R. Kschischang, B. J. Frey, and H. A. Loeliger. 2001. Factor graphs and the sum-product algorithm. IEEE Trans. Inf. Theory, 47(2):498–519.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>Proc. ofICML.</booktitle>
<contexts>
<context position="3108" citStr="Lafferty et al., 2001" startWordPosition="445" endWordPosition="449">dependency parsing: we present a general method for inference in factor graphs with hard constraints (§2), which extends some combinatorial factors considered by Smith and Eisner (2008). After presenting a geometric view of the variational approximations underlying message-passing algorithms (§3), and closing the gap between the two aforementioned parsers (§4), we consider the problem of learning the model parameters (§5). To this end, we propose an aggressive online algorithm that generalizes MIRA (Crammer et al., 2006) to arbitrary loss functions. We adopt a family of losses subsuming CRFs (Lafferty et al., 2001) and structured SVMs (Taskar et al., 2003; Tsochantaridis et al., 2004). Finally, we present a technique for including features not attested in the training data, allowing for richer models without substantial runtime costs. Our experiments (§6) show state-of-the-art performance on dependency parsing benchmarks. 1The name stems from “turbo codes,” a class of highperformance error-correcting codes introduced by Berrou et al. (1993) for which decoding algorithms are equivalent to running belief propagation in a graph with loops (McEliece et al., 1998). Proceedings of the 2010 Conference on Empir</context>
<context position="4707" citStr="Lafferty et al., 2001" startWordPosition="709" endWordPosition="712">te outputs Y(x) ⊆ Y (e.g., parse trees); we are interested in the case where Y(x) is a large structured set. Choices about the representation of elements of Y(x) play a major role in algorithm design. In many problems, the elements of Y(x) can be represented as discrete-valued vectors of the form y = hy1, ... , yIi, each yi taking values in a label set Yi. For example, in unlabeled dependency parsing, I is the number of candidate dependency arcs (quadratic in the sentence length), and each Yi = {0, 1}. Of course, the yi are highly interdependent. Factor Graphs. Probabilistic models like CRFs (Lafferty et al., 2001) assume a factorization of the conditional distribution of Y , Pr(Y = y |X = x) ∝ HCEC &apos;FC(x,yC), (1) where each C ⊆ {1, ... , I} is a factor, C is the set of factors, each yC °_ hyiiiEC denotes a partial output assignment, and each &apos;FC is a nonnegative potential function that depends on the output only via its restriction to C. A factor graph (Kschischang et al., 2001) is a convenient representation for the factorization in Eq. 1: it is a bipartite graph Gx comprised of variable nodes {1, ... , I} and factor nodes C ∈ C, with an edge connecting the ith variable node and a factor node C iff i </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. ofICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F T Martins</author>
<author>D Das</author>
<author>N A Smith</author>
<author>E P Xing</author>
</authors>
<title>Stacking dependency parsers.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="34883" citStr="Martins et al. (2008)" startWordPosition="6005" endWordPosition="6008"> 1/(am) = 0.001 (tuned on the English Non-Proj. dev.-set). The rightmost columns refer to a model to which non-projectivity features were added, trained under the SVM loss, that handles the complete feature set. Shown is the total number of features instantiated, the multiplicative factor w.r.t. the number of supported features, and the accuracies (in parenthesis, we display the difference w.r.t. a model trained with the supported features only). Entries marked with † are the highest reported in the literature, to the best of our knowledge, beating (sometimes slightly) McDonald et al. (2006), Martins et al. (2008), Martins et al. (2009), and, in the case of English Proj., also the third-order parser of Koo and Collins (2010), which achieves 93.04% on that dataset (their experiments in Czech are not comparable, since the datasets are different). ing all features that have been instantiated in Alg. 1. At each round, run lines 4–5 as usual, using only features in T. Since the other features have not been used before, they have a zero weight, hence can be ignored. When Q = oo, the variational problem in Eq. 24 consists of a MAP computation and the solution corresponds to one output Yt E �(xt). Only the par</context>
</contexts>
<marker>Martins, Das, Smith, Xing, 2008</marker>
<rawString>A. F. T. Martins, D. Das, N. A. Smith, and E. P. Xing. 2008. Stacking dependency parsers. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F T Martins</author>
<author>N A Smith</author>
<author>E P Xing</author>
</authors>
<title>Concise integer linear programming formulations for dependency parsing.</title>
<date>2009</date>
<booktitle>In Proc. of ACL-IJCNLP.</booktitle>
<contexts>
<context position="1321" citStr="Martins et al., 2009" startWordPosition="188" endWordPosition="191"> each method. We also propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation. The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, including CRFs and structured SVMs. Experiments show state-of-the-art performance for 14 languages. 1 Introduction Feature-rich discriminative models that break locality/independence assumptions can boost a parser’s performance (McDonald et al., 2006; Huang, 2008; Finkel et al., 2008; Smith and Eisner, 2008; Martins et al., 2009; Koo and Collins, 2010). Often, inference with such models becomes computationally intractable, causing a demand for understanding and improving approximate parsing algorithms. In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing: loopy belief propagation (Smith and Eisner, 2008) and linear programming relaxation (Martins et al., 2009). While those two parsers are differently motivated, we show that both correspond to inference in 34 M´ario A. T. Figueiredo††Instituto de Telecomunicac¸˜oes Instituto Sup</context>
<context position="20148" citStr="Martins et al. (2009)" startWordPosition="3417" endWordPosition="3420">ot necessarily concave, and that BP may not converge. Table 1 shows closed form expressions for the local agreement constraints and entropies of some hard-constraint factors, obtained by invoking Eq. 7 and observing that TC(yC) must be zero if configuration yC is forbidden. See Martins et al. (2010b). 4.2 Two Dependency Turbo Parsers We next present our main contribution: a formal connection between two recent approximate dependency parsers, which at first sight appear unrelated. Recall that (i) Smith and Eisner (2008) proposed a factor graph (Fig. 1) in which they run loopy BP, and that (ii) Martins et al. (2009) approximate parsing as the solution of a linear program. Here, we fill the blanks in the two approaches: we derive explicitly the variational problem addressed in (i) and we provide the underlying factor graph in (ii). This puts the two approaches side-by-side as approximate methods for marginal and MAP inference. Since both rely on “local” approximations (in the sense 38 of Eq. 11) that ignore the loops in their graphical models, we dub them turbo parsers by analogy with error-correcting turbo decoders (see footnote 1). Turbo Parser #1: Sum-Product Loopy BP. The factor graph depicted in Fig.</context>
<context position="21504" citStr="Martins et al., 2009" startWordPosition="3656" endWordPosition="3659">d the Bethe approximation HBethe inherent in Smith and Eisner’s loopy BP algorithm. Let A be the set of candidate arcs, and P C_ A2 the set of pairs of arcs that have factors. Let τ = (τA,τP) with τA = (Ta)aEA and τP = (Tab)(a,b)EP. Since all variables are binary, we may write, for each a E A, Ta(1) = za and Ta(0) = 1 − za, where za is a variable constrained to [0, 1]. 0 Let zA = (za)aEA; the local agreement constraints at the TREE factor (see Table 1) are written as zA E Ztree(x), where Ztree(x) is the arborescence polytope, i.e., the convex hull of all incidence vectors of dependency trees (Martins et al., 2009). It is straightforward to write a contingency table and obtain the following local agreement constraints at the pairwise factors: Tab(1,1) = zab, Tab(0,0) = 1 − za − zb + zab Tab(1,0) = za − zab, Tab(0,1) = zb − zab. Noting that all these pseudo-marginals are constrained to the unit interval, one can get rid of all variables Tab and write everything as za E [0, 1], zb E [0, 1], zab E [0, 1], zab :5 za, zab :5 zb, zab &gt;_ za + zb − 1, (12) inequalities which, along with zA E Ztree(x), define the local polytope L(Gx). As for the factor entropies, start by noting that the TREE-factor entropy Htre</context>
<context position="22738" citStr="Martins et al. (2009)" startWordPosition="3878" endWordPosition="3881">ined in closed form by computing the marginals ZA and the partition function Zx(θ) (via the matrix-tree theorem) and recalling the variational representation in Eq. 9, yielding Htree = log Zx(θ) − θTF(x)zA. Some algebra allows writing the overall Bethe entropy approximation as: �HBethe(τ) = Htree(zA) − Ia;b(za, zb, zab), (13) (a,b)EP where we introduced the mutual information associated with each pairwise factor, Ia;b(za, zb, zab) = 6Smith and Eisner (2008) also proposed other variants with more factors, which we omit for brevity. Figure 3: Details of the factor graph underlying the parser of Martins et al. (2009). Dashed circles represent auxiliary variables. See text and Table 1. E ya,yb Tab(ya, yb) log τa(ya)τb(yb). The approximate �maxZ θTF(x)z + Htree(zA) − Ia;b(za, zb, zab) (a,b)EP s.t. zab :5 za, zab :5 zb, zab &gt;_ za + zb − 1, b(a,b) E P, zA E Ztree, (14) whose maximizer corresponds to the beliefs returned by the Smith and Eisner’s loopy BP algorithm (if it converges). Turbo Parser #2: LP-Relaxed MAP. We now turn to the concise integer LP formulation of Martins et al. (2009). The formulation is exact but NPhard, and so an LP relaxation is made there by dropping the integer constraints. We next c</context>
<context position="25789" citStr="Martins et al. (2009)" startWordPosition="4443" endWordPosition="4447"> pmk = Eh=0 fhh,mi, m, k ∈ {1, ... , n} (18) • O(n2) XOR-WITH-OUTPUT factors to impose the constraint that words don’t consume other words’ commodities; i.e., if h =6 k and k =6 0, then there is a path from h to k iff exactly one outgoing arc in {hh, mi}m=1,...,n carries flow to k: phk = Enm=1 fkhh,mi, h, k ∈ {0, ... , n}, k ∈� {0, h}. (19) L(G0x) is thus defined by the constraints in Eq. 12 and 15–19. The approximate MAP problem, that replaces M(G0 x) by L(G0x) in Eq. 10, thus becomes: maxz,f,p θ&gt;F(x)z (20) s.t. Eqs. 12 and 15–19 are satisfied. This is exactly the LP relaxation considered by Martins et al. (2009) in their multi-commodity flow model, for the configuration with siblings and grandparent features.7 They also considered a configuration with non-projectivity features—which fire if an arc is non-projective.8 That configuration can also be obtained here if variables {nhh,mi} are 7To be precise, the constraints of Martins et al. (2009) are recovered after eliminating the path variables, via Eqs. 18–19. 8An arc (h, m) is non-projective if there is some word in its span not descending from h (Kahane et al., 1998). added to indicate non-projective arcs and OR-WITHOUTPUT hard constraint factors ar</context>
<context position="34906" citStr="Martins et al. (2009)" startWordPosition="6009" endWordPosition="6012">on the English Non-Proj. dev.-set). The rightmost columns refer to a model to which non-projectivity features were added, trained under the SVM loss, that handles the complete feature set. Shown is the total number of features instantiated, the multiplicative factor w.r.t. the number of supported features, and the accuracies (in parenthesis, we display the difference w.r.t. a model trained with the supported features only). Entries marked with † are the highest reported in the literature, to the best of our knowledge, beating (sometimes slightly) McDonald et al. (2006), Martins et al. (2008), Martins et al. (2009), and, in the case of English Proj., also the third-order parser of Koo and Collins (2010), which achieves 93.04% on that dataset (their experiments in Czech are not comparable, since the datasets are different). ing all features that have been instantiated in Alg. 1. At each round, run lines 4–5 as usual, using only features in T. Since the other features have not been used before, they have a zero weight, hence can be ignored. When Q = oo, the variational problem in Eq. 24 consists of a MAP computation and the solution corresponds to one output Yt E �(xt). Only the parts that are active in Y</context>
<context position="36671" citStr="Martins et al. (2009)" startWordPosition="6312" endWordPosition="6315">tsumoto (2003), in which all parse trees are projective.12 We implemented Alg. 1, 12We used the provided train/test splits for all datasets. For English, we used the standard test partitions (section 23 of the Wall Street Journal). We did not exploit the fact that some datasets only contain projective trees and have unique roots. which handles any loss function Lo�,y.13 When 0 &lt; oo, Turbo Parser #1 and the loopy BP algorithm of Smith and Eisner (2008) is used; otherwise, Turbo Parser #2 is used and the LP relaxation is solved with CPLEX. In both cases, we employed the same pruning strategy as Martins et al. (2009). Two different feature configurations were first tried: an arc-factored model and a model with second-order features (siblings and grandparents). We used the same arc-factored features as McDonald et al. (2005) and second-order features that conjoin words and lemmas (at most two), parts-ofspeech tags, and (if available) morphological information; this was the same set of features as in Martins et al. (2009). Table 2 shows the results obtained in both configurations, for CRF and SVM loss functions. While in the arc-factored case performance is similar, in second-order models there seems to be </context>
</contexts>
<marker>Martins, Smith, Xing, 2009</marker>
<rawString>A. F. T. Martins, N. A. Smith, and E. P. Xing. 2009. Concise integer linear programming formulations for dependency parsing. In Proc. of ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F T Martins</author>
<author>K Gimpel</author>
<author>N A Smith</author>
<author>E P Xing</author>
<author>P M Q Aguiar</author>
<author>M A T Figueiredo</author>
</authors>
<title>Learning structured classifiers with dual coordinate descent.</title>
<date>2010</date>
<tech>Technical Report CMU-ML-10-109.</tech>
<contexts>
<context position="13276" citStr="Martins et al., 2010" startWordPosition="2193" endWordPosition="2196">ure 1: Factor graph corresponding to the dependency parsing model of Smith and Eisner (2008) with sibling and grandparent features. Circles denote variable nodes, and squares denote factor nodes. Note the loops created by the inclusion of pairwise factors (GRAND and SIB). In Table 1 we present closed-form expressions for the factor-to-variable message ratios mC→i , MC→i(1)/MC→i(0) in terms of their variable-tofactor counterparts mi→C , Mi→C(1)/Mi→C(0); these ratios are all that is necessary when the variables are binary. Detailed derivations are presented in an extended version of this paper (Martins et al., 2010b). 3 Variational Representations Let Tx , {Prθ(.|x) |θ ∈ Rd} be the family of all distributions of the form in Eq. 2. We next present an alternative parametrization for the distributions in Tx in terms of factor marginals. We will see that each distribution can be seen as a point in the socalled marginal polytope (Wainwright and Jordan, 2008); this will pave the way for the variational representations to be derived next. Parts and Output Indicators. A part is a pair hC, yCi, where C is a soft factor and yC a partial output assignment. We let 9Z = {hC, yCi |C ∈ Osofk, yC ∈ Hi∈C �i} be the set </context>
<context position="17484" citStr="Martins et al. (2010" startWordPosition="2959" endWordPosition="2963">d to the marginal polytope M(Gx) (right). In general only a subset of M(Gx) is reachable from our parameter space. Any distribution in Px can be parametrized by a vector θ E Rd or by a point µ E M(Gx). 2. The problem in Eq. 9 is convex and its solution is attained at the factor marginals, i.e., there is a maximizer µ� s.t. �µC(yC) = Pro(YC = yC|x) for each C E C. The gradient of the log-partition function is V log Zx(θ) = F(x)µ. 3. The MAP y�, argmaxy∈%x) Pro(y|x) can be obtained by solving the linear program µ� ,χ(Y) = argmax θ&gt;F(x)µ. (10) µ∈M(oj.) A proof of this proposition can be found in Martins et al. (2010a). Fig. 2 provides an illustration of the dual parametrization implied by Prop. 1. 4 Approximate Inference &amp; Turbo Parsing We now show how the variational machinery just described relates to message-passing algorithms and provides a common framework for analyzing two recent dependency parsers. Later (§5), Prop. 1 is used constructively for learning the model parameters. 4.1 Loopy BP as a Variational Approximation For general factor graphs with loops, the marginal polytope M(Gx) cannot be compactly specified and the entropy term H(µ) lacks a closed form, rendering exact optimizations in Eqs. 9</context>
<context position="19826" citStr="Martins et al. (2010" startWordPosition="3364" endWordPosition="3367">the ith variable, H(τi) , − Ey, Ti(yi) log Ti(yi) and H(τC) , − EyC TC(yC) log TC(yC). Any stationary point of sum-product BP is a local optimum of the variational problem in Eq. 9 with M(Gx) replaced by L(Gx) and H replaced by HBethe (Yedidia et al., 2001). Note however that multiple optima may exist, since HBethe is not necessarily concave, and that BP may not converge. Table 1 shows closed form expressions for the local agreement constraints and entropies of some hard-constraint factors, obtained by invoking Eq. 7 and observing that TC(yC) must be zero if configuration yC is forbidden. See Martins et al. (2010b). 4.2 Two Dependency Turbo Parsers We next present our main contribution: a formal connection between two recent approximate dependency parsers, which at first sight appear unrelated. Recall that (i) Smith and Eisner (2008) proposed a factor graph (Fig. 1) in which they run loopy BP, and that (ii) Martins et al. (2009) approximate parsing as the solution of a linear program. Here, we fill the blanks in the two approaches: we derive explicitly the variational problem addressed in (i) and we provide the underlying factor graph in (ii). This puts the two approaches side-by-side as approximate m</context>
<context position="27729" citStr="Martins et al. (2010" startWordPosition="4773" endWordPosition="4776">onvex loss. Examples include the logistic loss used in CRFs (− log Pro(y|x)) and the hinge loss of structured SVMs (maxy,∈%x) θ&gt;(φ(x, y0)− φ(x, y)) + E(y0, y) for some cost function E). These are both special cases of the family defined in Fig. 4, which also includes the structured perceptron’s loss (Q → ∞, -y = 0) and the softmax-margin loss of Gimpel and Smith (2010; Q = -y = 1). Alg. 1 is closely related to stochastic or online gradient descent methods, but with the key advantage of not needing a learning rate hyperparameter. We sketch the derivation of Alg. 1; full details can be found in Martins et al. (2010a). On the tth round, one example hxt, yti is considered. We seek to solve ming,ξ λm2 kθ − θtk2 + � (23) s.t. L(θ; xt, yt) ≤ ≥ 0, 9Given what was just exposed, it seems appealing to try max-product loopy BP on the factor graph of Fig. 1, or sumproduct loopy BP on the one in Fig. 3. Both attempts present serious challenges: the former requires computing messages sent by the tree factor, which requires O(n2) calls to the Chu-LiuEdmonds algorithm and hence O(n5) time. No obvious strategy seems to exist for simultaneous computation of all messages, unlike in the sum-product case. The latter is eve</context>
</contexts>
<marker>Martins, Gimpel, Smith, Xing, Aguiar, Figueiredo, 2010</marker>
<rawString>A. F. T. Martins, K. Gimpel, N. A. Smith, E. P. Xing, P. M. Q. Aguiar, and M. A. T. Figueiredo. 2010a. Learning structured classifiers with dual coordinate descent. Technical Report CMU-ML-10-109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F T Martins</author>
<author>N A Smith</author>
<author>E P Xing</author>
<author>P M Q Aguiar</author>
<author>M A T Figueiredo</author>
</authors>
<title>Turbo parsers: Dependency parsing by approximate variational inference (extended version).</title>
<date>2010</date>
<contexts>
<context position="13276" citStr="Martins et al., 2010" startWordPosition="2193" endWordPosition="2196">ure 1: Factor graph corresponding to the dependency parsing model of Smith and Eisner (2008) with sibling and grandparent features. Circles denote variable nodes, and squares denote factor nodes. Note the loops created by the inclusion of pairwise factors (GRAND and SIB). In Table 1 we present closed-form expressions for the factor-to-variable message ratios mC→i , MC→i(1)/MC→i(0) in terms of their variable-tofactor counterparts mi→C , Mi→C(1)/Mi→C(0); these ratios are all that is necessary when the variables are binary. Detailed derivations are presented in an extended version of this paper (Martins et al., 2010b). 3 Variational Representations Let Tx , {Prθ(.|x) |θ ∈ Rd} be the family of all distributions of the form in Eq. 2. We next present an alternative parametrization for the distributions in Tx in terms of factor marginals. We will see that each distribution can be seen as a point in the socalled marginal polytope (Wainwright and Jordan, 2008); this will pave the way for the variational representations to be derived next. Parts and Output Indicators. A part is a pair hC, yCi, where C is a soft factor and yC a partial output assignment. We let 9Z = {hC, yCi |C ∈ Osofk, yC ∈ Hi∈C �i} be the set </context>
<context position="17484" citStr="Martins et al. (2010" startWordPosition="2959" endWordPosition="2963">d to the marginal polytope M(Gx) (right). In general only a subset of M(Gx) is reachable from our parameter space. Any distribution in Px can be parametrized by a vector θ E Rd or by a point µ E M(Gx). 2. The problem in Eq. 9 is convex and its solution is attained at the factor marginals, i.e., there is a maximizer µ� s.t. �µC(yC) = Pro(YC = yC|x) for each C E C. The gradient of the log-partition function is V log Zx(θ) = F(x)µ. 3. The MAP y�, argmaxy∈%x) Pro(y|x) can be obtained by solving the linear program µ� ,χ(Y) = argmax θ&gt;F(x)µ. (10) µ∈M(oj.) A proof of this proposition can be found in Martins et al. (2010a). Fig. 2 provides an illustration of the dual parametrization implied by Prop. 1. 4 Approximate Inference &amp; Turbo Parsing We now show how the variational machinery just described relates to message-passing algorithms and provides a common framework for analyzing two recent dependency parsers. Later (§5), Prop. 1 is used constructively for learning the model parameters. 4.1 Loopy BP as a Variational Approximation For general factor graphs with loops, the marginal polytope M(Gx) cannot be compactly specified and the entropy term H(µ) lacks a closed form, rendering exact optimizations in Eqs. 9</context>
<context position="19826" citStr="Martins et al. (2010" startWordPosition="3364" endWordPosition="3367">the ith variable, H(τi) , − Ey, Ti(yi) log Ti(yi) and H(τC) , − EyC TC(yC) log TC(yC). Any stationary point of sum-product BP is a local optimum of the variational problem in Eq. 9 with M(Gx) replaced by L(Gx) and H replaced by HBethe (Yedidia et al., 2001). Note however that multiple optima may exist, since HBethe is not necessarily concave, and that BP may not converge. Table 1 shows closed form expressions for the local agreement constraints and entropies of some hard-constraint factors, obtained by invoking Eq. 7 and observing that TC(yC) must be zero if configuration yC is forbidden. See Martins et al. (2010b). 4.2 Two Dependency Turbo Parsers We next present our main contribution: a formal connection between two recent approximate dependency parsers, which at first sight appear unrelated. Recall that (i) Smith and Eisner (2008) proposed a factor graph (Fig. 1) in which they run loopy BP, and that (ii) Martins et al. (2009) approximate parsing as the solution of a linear program. Here, we fill the blanks in the two approaches: we derive explicitly the variational problem addressed in (i) and we provide the underlying factor graph in (ii). This puts the two approaches side-by-side as approximate m</context>
<context position="27729" citStr="Martins et al. (2010" startWordPosition="4773" endWordPosition="4776">onvex loss. Examples include the logistic loss used in CRFs (− log Pro(y|x)) and the hinge loss of structured SVMs (maxy,∈%x) θ&gt;(φ(x, y0)− φ(x, y)) + E(y0, y) for some cost function E). These are both special cases of the family defined in Fig. 4, which also includes the structured perceptron’s loss (Q → ∞, -y = 0) and the softmax-margin loss of Gimpel and Smith (2010; Q = -y = 1). Alg. 1 is closely related to stochastic or online gradient descent methods, but with the key advantage of not needing a learning rate hyperparameter. We sketch the derivation of Alg. 1; full details can be found in Martins et al. (2010a). On the tth round, one example hxt, yti is considered. We seek to solve ming,ξ λm2 kθ − θtk2 + � (23) s.t. L(θ; xt, yt) ≤ ≥ 0, 9Given what was just exposed, it seems appealing to try max-product loopy BP on the factor graph of Fig. 1, or sumproduct loopy BP on the one in Fig. 3. Both attempts present serious challenges: the former requires computing messages sent by the tree factor, which requires O(n2) calls to the Chu-LiuEdmonds algorithm and hence O(n5) time. No obvious strategy seems to exist for simultaneous computation of all messages, unlike in the sum-product case. The latter is eve</context>
</contexts>
<marker>Martins, Smith, Xing, Aguiar, Figueiredo, 2010</marker>
<rawString>A. F. T. Martins, N. A. Smith, E. P. Xing, P. M. Q. Aguiar, and M. A. T. Figueiredo. 2010b. Turbo parsers: Dependency parsing by approximate variational inference (extended version).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>K Schultz</author>
<author>S Singh</author>
</authors>
<title>Factorie: Probabilistic programming via imperatively defined factor graphs.</title>
<date>2009</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="5604" citStr="McCallum et al., 2009" startWordPosition="878" endWordPosition="881">at depends on the output only via its restriction to C. A factor graph (Kschischang et al., 2001) is a convenient representation for the factorization in Eq. 1: it is a bipartite graph Gx comprised of variable nodes {1, ... , I} and factor nodes C ∈ C, with an edge connecting the ith variable node and a factor node C iff i ∈ C. Hence, the factor graph Gx makes explicit the direct dependencies among the variables {y1, ... , yI}. Factor graphs have been used for several NLP tasks, such as dependency parsing, segmentation, and co-reference resolution (Sutton et al., 2007; Smith and Eisner, 2008; McCallum et al., 2009). Hard and Soft Constraint Factors. It may be the case that valid outputs are a proper subset of Y1 × · · · × YI—for example, in dependency parsing, the entries of the output vector y must jointly define a spanning tree. This requires hard constraint factors that rule out forbidden partial assignments by mapping them to zero potential values. See Table 1 for an inventory of hard constraint factors used in this paper. Factors that are not of this special kind are called soft factors, and have strictly positive potentials. We thus have a partition C = Chard ∪ Csoft. We let the soft factor potent</context>
</contexts>
<marker>McCallum, Schultz, Singh, 2009</marker>
<rawString>A. McCallum, K. Schultz, and S. Singh. 2009. Factorie: Probabilistic programming via imperatively defined factor graphs. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R T McDonald</author>
<author>F Pereira</author>
<author>K Ribarov</author>
<author>J Hajic</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proc. of HLT-EMNLP.</booktitle>
<contexts>
<context position="7507" citStr="McDonald et al., 2005" startWordPosition="1208" endWordPosition="1211">ongs to the dependency tree. There is a hard factor TREE connected to all variables, that constrains the overall arc configurations to form a spanning tree. There is a unary soft factor per arc, whose log-potential reflects the score of that arc. There are also O(n3) pairwise factors; their log-potentials reflect the scores of sibling and grandparent arcs. These factors create loops, thus calling for approximate inference. Without them, the model is arc-factored, and exact inference in it is well studied: finding the most probable parse tree takes O(n3) time with the ChuLiu-Edmonds algorithm (McDonald et al., 2005),2 and computing posterior marginals for all arcs takes O(n3) time via the matrix-tree theorem (Smith and Smith, 2007; Koo et al., 2007). Message-passing algorithms. In general factor graphs, both inference problems— obtaining the most probable output (the MAP) argmaxyE%x) Pre(y|x), and computing the marginals Pre(Yi = yi|x)—can be addressed with the belief propagation (BP) algorithm (Pearl, 1988), which iteratively passes messages between variables and factors reflecting their local “beliefs.” 2There is a faster but more involved O(n2) algorithm due to Tarjan (1977). 35 �1 v1, . . . , vn ∈ SC</context>
<context position="36882" citStr="McDonald et al. (2005)" startWordPosition="6342" endWordPosition="6346">Wall Street Journal). We did not exploit the fact that some datasets only contain projective trees and have unique roots. which handles any loss function Lo�,y.13 When 0 &lt; oo, Turbo Parser #1 and the loopy BP algorithm of Smith and Eisner (2008) is used; otherwise, Turbo Parser #2 is used and the LP relaxation is solved with CPLEX. In both cases, we employed the same pruning strategy as Martins et al. (2009). Two different feature configurations were first tried: an arc-factored model and a model with second-order features (siblings and grandparents). We used the same arc-factored features as McDonald et al. (2005) and second-order features that conjoin words and lemmas (at most two), parts-ofspeech tags, and (if available) morphological information; this was the same set of features as in Martins et al. (2009). Table 2 shows the results obtained in both configurations, for CRF and SVM loss functions. While in the arc-factored case performance is similar, in second-order models there seems to be a consistent gain when the SVM loss is used. There are two possible reasons: first, SVMs take the cost function into consideration; second, Turbo Parser #2 is less approximate than Turbo Parser #1, since only th</context>
</contexts>
<marker>McDonald, Pereira, Ribarov, Hajic, 2005</marker>
<rawString>R. T. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proc. of HLT-EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Lerman</author>
<author>F Pereira</author>
</authors>
<title>Multilingual dependency analysis with a two-stage discriminative parser.</title>
<date>2006</date>
<booktitle>In Proc. of CoNLL.</booktitle>
<contexts>
<context position="1241" citStr="McDonald et al., 2006" startWordPosition="174" endWordPosition="177">ptions with a factor graph, we shed light on the optimization problems tackled in each method. We also propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation. The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, including CRFs and structured SVMs. Experiments show state-of-the-art performance for 14 languages. 1 Introduction Feature-rich discriminative models that break locality/independence assumptions can boost a parser’s performance (McDonald et al., 2006; Huang, 2008; Finkel et al., 2008; Smith and Eisner, 2008; Martins et al., 2009; Koo and Collins, 2010). Often, inference with such models becomes computationally intractable, causing a demand for understanding and improving approximate parsing algorithms. In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing: loopy belief propagation (Smith and Eisner, 2008) and linear programming relaxation (Martins et al., 2009). While those two parsers are differently motivated, we show that both correspond to infere</context>
<context position="34860" citStr="McDonald et al. (2006)" startWordPosition="6001" endWordPosition="6004">es, after 10 epochs with 1/(am) = 0.001 (tuned on the English Non-Proj. dev.-set). The rightmost columns refer to a model to which non-projectivity features were added, trained under the SVM loss, that handles the complete feature set. Shown is the total number of features instantiated, the multiplicative factor w.r.t. the number of supported features, and the accuracies (in parenthesis, we display the difference w.r.t. a model trained with the supported features only). Entries marked with † are the highest reported in the literature, to the best of our knowledge, beating (sometimes slightly) McDonald et al. (2006), Martins et al. (2008), Martins et al. (2009), and, in the case of English Proj., also the third-order parser of Koo and Collins (2010), which achieves 93.04% on that dataset (their experiments in Czech are not comparable, since the datasets are different). ing all features that have been instantiated in Alg. 1. At each round, run lines 4–5 as usual, using only features in T. Since the other features have not been used before, they have a zero weight, hence can be ignored. When Q = oo, the variational problem in Eq. 24 consists of a MAP computation and the solution corresponds to one output Y</context>
</contexts>
<marker>McDonald, Lerman, Pereira, 2006</marker>
<rawString>R. McDonald, K. Lerman, and F. Pereira. 2006. Multilingual dependency analysis with a two-stage discriminative parser. In Proc. of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J McEliece</author>
<author>D J C MacKay</author>
<author>J F Cheng</author>
</authors>
<title>Turbo decoding as an instance of Pearl’s “belief propagation” algorithm.</title>
<date>1998</date>
<journal>IEEE Journal on Selected Areas in Communications,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="3663" citStr="McEliece et al., 1998" startWordPosition="531" endWordPosition="534"> We adopt a family of losses subsuming CRFs (Lafferty et al., 2001) and structured SVMs (Taskar et al., 2003; Tsochantaridis et al., 2004). Finally, we present a technique for including features not attested in the training data, allowing for richer models without substantial runtime costs. Our experiments (§6) show state-of-the-art performance on dependency parsing benchmarks. 1The name stems from “turbo codes,” a class of highperformance error-correcting codes introduced by Berrou et al. (1993) for which decoding algorithms are equivalent to running belief propagation in a graph with loops (McEliece et al., 1998). Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 34–44, MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics 2 Structured Inference and Factor Graphs Denote by X a set of input objects from which we want to infer some hidden structure conveyed in an output set Y. Each input x ∈ X (e.g., a sentence) is associated with a set of candidate outputs Y(x) ⊆ Y (e.g., parse trees); we are interested in the case where Y(x) is a large structured set. Choices about the representation of elements of Y(x) play a major rol</context>
</contexts>
<marker>McEliece, MacKay, Cheng, 1998</marker>
<rawString>R. J. McEliece, D. J. C. MacKay, and J. F. Cheng. 1998. Turbo decoding as an instance of Pearl’s “belief propagation” algorithm. IEEE Journal on Selected Areas in Communications, 16(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pearl</author>
</authors>
<title>Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.</title>
<date>1988</date>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="7907" citStr="Pearl, 1988" startWordPosition="1267" endWordPosition="1268">imate inference. Without them, the model is arc-factored, and exact inference in it is well studied: finding the most probable parse tree takes O(n3) time with the ChuLiu-Edmonds algorithm (McDonald et al., 2005),2 and computing posterior marginals for all arcs takes O(n3) time via the matrix-tree theorem (Smith and Smith, 2007; Koo et al., 2007). Message-passing algorithms. In general factor graphs, both inference problems— obtaining the most probable output (the MAP) argmaxyE%x) Pre(y|x), and computing the marginals Pre(Yi = yi|x)—can be addressed with the belief propagation (BP) algorithm (Pearl, 1988), which iteratively passes messages between variables and factors reflecting their local “beliefs.” 2There is a faster but more involved O(n2) algorithm due to Tarjan (1977). 35 �1 v1, . . . , vn ∈ SC A general binary factor: ΨC(v1, ... , vn) = where SC ⊆ {0, 1}n. 0 otherwise, • Message-induced distribution: ω , hmj→Cij=1,...,n • Partition function: ZC(ω) , Ehv1, vni∈8CrIi=1 mvi i→C • Marginals: MARGi(ω) , Pr,,,{Vi = 1|hV1, ... , Vni ∈ SC} • Max-marginals: MAX-MARGi,b(ω) , maxV∈8C Pr,,,(v|vi = b) • Sum-prod.: mC→i = m−1 i→C · MARGi(ω)/(1 − MARGi(ω)) • Max-prod.: mC→i = mi-C · MAX-MARGi, 1 (ω)/</context>
</contexts>
<marker>Pearl, 1988</marker>
<rawString>J. Pearl. 1988. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sha</author>
<author>F Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proc. of</booktitle>
<contexts>
<context position="32334" citStr="Sha and Pereira (2003)" startWordPosition="5597" endWordPosition="5600">putational challenges. Often only “supported” features—those observed in the training data—are included, and even those are commonly eliminated when their frequencies fall below a threshold. Important information may be lost as a result of these expedient choices. Formally, the supported feature set is Fsupp g Umi=1 supp φ(xi, yi), where supp u °_ {j |uj =6 0} denotes the support of vector u. Fsupp is a subset of the complete feature set, comprised of those features that occur in some candidate output, Fcomp Um U i). Features i=1 y&apos; �∈�(x�) supp φ(xi, y0 in Fcomp\Fsupp are called unsupported. Sha and Pereira (2003) have shown that training a CRF-based shallow parser with the complete feature set may improve performance (over the supported one), at the cost of 4.6 times more features. Dependency parsing has a much higher ratio (around 20 for bilexical word-word features, as estimated in the Penn Treebank), due to the quadratic or faster growth of the number of parts, of which only a few are active in a legal output. We propose a simple strategy for handling Fcomp efficiently, which can be applied for those losses in Fig. 4 where β = ∞. (e.g., the structured SVM and perceptron). Our procedure is the follo</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>F. Sha and F. Pereira. 2003. Shallow parsing with conditional random fields. In Proc. of HLT-NAACL. D. A. Smith and J. Eisner. 2008. Dependency parsing by belief propagation. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Smith</author>
<author>N A Smith</author>
</authors>
<title>Probabilistic models of nonprojective dependency trees.</title>
<date>2007</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="7624" citStr="Smith and Smith, 2007" startWordPosition="1226" endWordPosition="1229">onfigurations to form a spanning tree. There is a unary soft factor per arc, whose log-potential reflects the score of that arc. There are also O(n3) pairwise factors; their log-potentials reflect the scores of sibling and grandparent arcs. These factors create loops, thus calling for approximate inference. Without them, the model is arc-factored, and exact inference in it is well studied: finding the most probable parse tree takes O(n3) time with the ChuLiu-Edmonds algorithm (McDonald et al., 2005),2 and computing posterior marginals for all arcs takes O(n3) time via the matrix-tree theorem (Smith and Smith, 2007; Koo et al., 2007). Message-passing algorithms. In general factor graphs, both inference problems— obtaining the most probable output (the MAP) argmaxyE%x) Pre(y|x), and computing the marginals Pre(Yi = yi|x)—can be addressed with the belief propagation (BP) algorithm (Pearl, 1988), which iteratively passes messages between variables and factors reflecting their local “beliefs.” 2There is a faster but more involved O(n2) algorithm due to Tarjan (1977). 35 �1 v1, . . . , vn ∈ SC A general binary factor: ΨC(v1, ... , vn) = where SC ⊆ {0, 1}n. 0 otherwise, • Message-induced distribution: ω , hmj</context>
</contexts>
<marker>Smith, Smith, 2007</marker>
<rawString>D. A. Smith and N. A. Smith. 2007. Probabilistic models of nonprojective dependency trees. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Surdeanu</author>
<author>R Johansson</author>
<author>A Meyers</author>
<author>L M`arquez</author>
<author>J Nivre</author>
</authors>
<title>The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<journal>CoNLL.</journal>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>M. Surdeanu, R. Johansson, A. Meyers, L. M`arquez, and J. Nivre. 2008. The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies. CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sutton</author>
<author>A McCallum</author>
<author>K Rohanimanesh</author>
</authors>
<title>Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data.</title>
<date>2007</date>
<journal>JMLR,</journal>
<pages>8--693</pages>
<contexts>
<context position="5556" citStr="Sutton et al., 2007" startWordPosition="870" endWordPosition="873">ch &apos;FC is a nonnegative potential function that depends on the output only via its restriction to C. A factor graph (Kschischang et al., 2001) is a convenient representation for the factorization in Eq. 1: it is a bipartite graph Gx comprised of variable nodes {1, ... , I} and factor nodes C ∈ C, with an edge connecting the ith variable node and a factor node C iff i ∈ C. Hence, the factor graph Gx makes explicit the direct dependencies among the variables {y1, ... , yI}. Factor graphs have been used for several NLP tasks, such as dependency parsing, segmentation, and co-reference resolution (Sutton et al., 2007; Smith and Eisner, 2008; McCallum et al., 2009). Hard and Soft Constraint Factors. It may be the case that valid outputs are a proper subset of Y1 × · · · × YI—for example, in dependency parsing, the entries of the output vector y must jointly define a spanning tree. This requires hard constraint factors that rule out forbidden partial assignments by mapping them to zero potential values. See Table 1 for an inventory of hard constraint factors used in this paper. Factors that are not of this special kind are called soft factors, and have strictly positive potentials. We thus have a partition </context>
</contexts>
<marker>Sutton, McCallum, Rohanimanesh, 2007</marker>
<rawString>C. Sutton, A. McCallum, and K. Rohanimanesh. 2007. Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data. JMLR, 8:693–723.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Tarjan</author>
</authors>
<title>Finding optimum branchings.</title>
<date>1977</date>
<journal>Networks,</journal>
<volume>7</volume>
<issue>1</issue>
<contexts>
<context position="8080" citStr="Tarjan (1977)" startWordPosition="1292" endWordPosition="1293">dmonds algorithm (McDonald et al., 2005),2 and computing posterior marginals for all arcs takes O(n3) time via the matrix-tree theorem (Smith and Smith, 2007; Koo et al., 2007). Message-passing algorithms. In general factor graphs, both inference problems— obtaining the most probable output (the MAP) argmaxyE%x) Pre(y|x), and computing the marginals Pre(Yi = yi|x)—can be addressed with the belief propagation (BP) algorithm (Pearl, 1988), which iteratively passes messages between variables and factors reflecting their local “beliefs.” 2There is a faster but more involved O(n2) algorithm due to Tarjan (1977). 35 �1 v1, . . . , vn ∈ SC A general binary factor: ΨC(v1, ... , vn) = where SC ⊆ {0, 1}n. 0 otherwise, • Message-induced distribution: ω , hmj→Cij=1,...,n • Partition function: ZC(ω) , Ehv1, vni∈8CrIi=1 mvi i→C • Marginals: MARGi(ω) , Pr,,,{Vi = 1|hV1, ... , Vni ∈ SC} • Max-marginals: MAX-MARGi,b(ω) , maxV∈8C Pr,,,(v|vi = b) • Sum-prod.: mC→i = m−1 i→C · MARGi(ω)/(1 − MARGi(ω)) • Max-prod.: mC→i = mi-C · MAX-MARGi, 1 (ω)/MAX-MARGi,0 (ω) • Local agreem. constr.: z ∈ conv SC, where z = hri (1)ii 1 • Entropy: HC = log ZC (ω) − Ei=1 MARGi (ω) log mi→C 1 1 y ∈Ytree (i.e., {a ∈ A |ya = 1} is a dir</context>
</contexts>
<marker>Tarjan, 1977</marker>
<rawString>R. E. Tarjan. 1977. Finding optimum branchings. Networks, 7(1):25–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Tarlow</author>
<author>I E Givoni</author>
<author>R S Zemel</author>
</authors>
<title>HOPMAP: Efficient message passing with high order potentials. In</title>
<date>2010</date>
<booktitle>Proc. of AISTATS.</booktitle>
<contexts>
<context position="39122" citStr="Tarlow et al., 2010" startWordPosition="6716" endWordPosition="6719">ch is exact, third-order, and constrains the outputs to be projective, does not outperform ours on the projective English dataset.14 Finally, Table 3 shows results obtained for different settings of β and γ. Interestingly, we observe that higher scores are obtained for loss functions that are “between” SVMs and CRFs. 7 Related Work There has been recent work studying efficient computation of messages in combinatorial factors: bipartite matchings (Duchi et al., 2007), projective and non-projective arborescences (Smith and Eisner, 2008), as well as high order factors with countbased potentials (Tarlow et al., 2010), among others. Some of our combinatorial factors (OR, OR-WITHOUTPUT) and the analogous entropy computations were never considered, to the best of our knowledge. Prop. 1 appears in Wainwright and Jordan (2008) for canonical overcomplete models; we adapt it here for models with shared features. We rely on the variational interpretation of loopy BP, due to Yedidia et al. (2001), to derive the objective being optimized by Smith and Eisner’s loopy BP parser. Independently of our work, Koo et al. (2010) 14This might be due to the fact that Koo and Collins (2010) trained with the perceptron algorith</context>
</contexts>
<marker>Tarlow, Givoni, Zemel, 2010</marker>
<rawString>D. Tarlow, I. E. Givoni, and R. S. Zemel. 2010. HOPMAP: Efficient message passing with high order potentials. In Proc. of AISTATS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>C Guestrin</author>
<author>D Koller</author>
</authors>
<title>Max-margin Markov networks.</title>
<date>2003</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="3149" citStr="Taskar et al., 2003" startWordPosition="453" endWordPosition="456">hod for inference in factor graphs with hard constraints (§2), which extends some combinatorial factors considered by Smith and Eisner (2008). After presenting a geometric view of the variational approximations underlying message-passing algorithms (§3), and closing the gap between the two aforementioned parsers (§4), we consider the problem of learning the model parameters (§5). To this end, we propose an aggressive online algorithm that generalizes MIRA (Crammer et al., 2006) to arbitrary loss functions. We adopt a family of losses subsuming CRFs (Lafferty et al., 2001) and structured SVMs (Taskar et al., 2003; Tsochantaridis et al., 2004). Finally, we present a technique for including features not attested in the training data, allowing for richer models without substantial runtime costs. Our experiments (§6) show state-of-the-art performance on dependency parsing benchmarks. 1The name stems from “turbo codes,” a class of highperformance error-correcting codes introduced by Berrou et al. (1993) for which decoding algorithms are equivalent to running belief propagation in a graph with loops (McEliece et al., 1998). Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processi</context>
</contexts>
<marker>Taskar, Guestrin, Koller, 2003</marker>
<rawString>B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin Markov networks. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>S Lacoste-Julien</author>
<author>M I Jordan</author>
</authors>
<title>Structured prediction, dual extragradient and Bregman projections.</title>
<date>2006</date>
<journal>JMLR,</journal>
<pages>7--1627</pages>
<contexts>
<context position="30973" citStr="Taskar et al. (2006)" startWordPosition="5371" endWordPosition="5374">l costs; letting µ = χ(y) and µ0 = χ(y0), this implies `(y0, y) = p&gt;µ0 + q for some p and q which are constant with respect to µ0.11 Under this assumption, Lβ,γ(θ; x, y) becomes expressible in terms of the log-partition function of a distribution whose log-potentials are set to β(F(x)&gt;θ + γp). From Eq. 9 and after some algebra, we finally obtain Lβ,γ(θ;x,y) = 10Our description also applies to the (non-differentiable) hinge loss case, when ,3 --+ oo, if we replace all instances of “the gradient” in the text by “a subgradient.” 11For the Hamming cost, this holds with p = 1 − 2µ and q = 1Tµ. See Taskar et al. (2006) for other examples. θ&gt;F(x)(µ0−µ)+1βH(µ0)+γ(p&gt;µ0+q). (24) Let µ� be a maximizer in Eq. 24; from the second statement of Prop. 1 we obtain ∇Lβ,γ(θ; x, y) = F(x)(µ−µ). When the inference problem in Eq. 24 is intractable, approximate message-passing algorithms like loopy BP still allow us to obtain approximations of the loss Lβ,γ and its gradient. For the hinge loss, we arrive precisely at the maxloss variant of 1-best MIRA (Crammer et al., 2006). For the logistic loss, we arrive at a new online learning algorithm for CRFs that resembles stochastic gradient descent but with an automatic step size</context>
</contexts>
<marker>Taskar, Lacoste-Julien, Jordan, 2006</marker>
<rawString>B. Taskar, S. Lacoste-Julien, and M. I. Jordan. 2006. Structured prediction, dual extragradient and Bregman projections. JMLR, 7:1627–1653.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Tsochantaridis</author>
<author>T Hofmann</author>
<author>T Joachims</author>
<author>Y Altun</author>
</authors>
<title>Support vector machine learning for interdependent and structured output spaces.</title>
<date>2004</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="3179" citStr="Tsochantaridis et al., 2004" startWordPosition="457" endWordPosition="460">factor graphs with hard constraints (§2), which extends some combinatorial factors considered by Smith and Eisner (2008). After presenting a geometric view of the variational approximations underlying message-passing algorithms (§3), and closing the gap between the two aforementioned parsers (§4), we consider the problem of learning the model parameters (§5). To this end, we propose an aggressive online algorithm that generalizes MIRA (Crammer et al., 2006) to arbitrary loss functions. We adopt a family of losses subsuming CRFs (Lafferty et al., 2001) and structured SVMs (Taskar et al., 2003; Tsochantaridis et al., 2004). Finally, we present a technique for including features not attested in the training data, allowing for richer models without substantial runtime costs. Our experiments (§6) show state-of-the-art performance on dependency parsing benchmarks. 1The name stems from “turbo codes,” a class of highperformance error-correcting codes introduced by Berrou et al. (1993) for which decoding algorithms are equivalent to running belief propagation in a graph with loops (McEliece et al., 1998). Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 34–44, MIT, Massachu</context>
</contexts>
<marker>Tsochantaridis, Hofmann, Joachims, Altun, 2004</marker>
<rawString>I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. 2004. Support vector machine learning for interdependent and structured output spaces. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Wainwright</author>
<author>M I Jordan</author>
</authors>
<title>Graphical Models, Exponential Families, and Variational Inference.</title>
<date>2008</date>
<publisher>Now Publishers.</publisher>
<contexts>
<context position="13621" citStr="Wainwright and Jordan, 2008" startWordPosition="2253" endWordPosition="2256">variable message ratios mC→i , MC→i(1)/MC→i(0) in terms of their variable-tofactor counterparts mi→C , Mi→C(1)/Mi→C(0); these ratios are all that is necessary when the variables are binary. Detailed derivations are presented in an extended version of this paper (Martins et al., 2010b). 3 Variational Representations Let Tx , {Prθ(.|x) |θ ∈ Rd} be the family of all distributions of the form in Eq. 2. We next present an alternative parametrization for the distributions in Tx in terms of factor marginals. We will see that each distribution can be seen as a point in the socalled marginal polytope (Wainwright and Jordan, 2008); this will pave the way for the variational representations to be derived next. Parts and Output Indicators. A part is a pair hC, yCi, where C is a soft factor and yC a partial output assignment. We let 9Z = {hC, yCi |C ∈ Osofk, yC ∈ Hi∈C �i} be the set of all parts. Given an output y0 ∈ �(x), a part hC, yCi is said to be active if it locally matches the output, i.e., if yC = y0C. Any output y0 ∈ �(x) can be mapped to a |9Z|- dimensional binary vector χ(y0) indicating which parts are active, i.e., [χ(y0)]hC,yCi = 1 if yC = y0C Pr{TC(YC) = 1IYi = yi} and MC—i(yi) a maxq,c(yc)_1 Pr{YC = yClYi =</context>
<context position="15690" citStr="Wainwright and Jordan (2008)" startWordPosition="2623" endWordPosition="2627">e that M(Sx) only depends on the factor graph Sx and the hard constraints (i.e., it is independent of the parameters θ). The importance of the marginal polytope stems from two facts: (i) each vertex of M(Sx) corresponds to an output in �(x); (ii) each point in M(Sx) corresponds to a vector of marginal probabilities that is realizable by some distribution (not necessarily in Tx) that factors according to Sx. Variational Representations. We now describe formally how the points in M(Sx) are linked to the distributions in Tx. We extend the “canonical overcomplete parametrization” case, studied by Wainwright and Jordan (2008), to our scenario (common in NLP), where arbitrary features are allowed and the parameters are tied (shared by all factors). Let H(Prθ(.|x)) ,− Ey∈%x) Prθ(y|x) log Prθ(y|x) denote the entropy of Prθ(.|x), and Eθ[.] the expectation under Prθ(.|x). The component of µ ∈ M(Sx) indexed by part hC, yCi is denoted µC(yC). Proposition 1. There is a map coupling each distribution Prθ(.|x) ∈ Tx to a unique µ ∈ M(Sx) such that Eθ[χ(Y )] = µ. Define H(µ) , H(Prθ(.|x)) if some Prθ(.|x) is coupled to µ, and H(µ) = −∞ if no such Prθ(.|x) exists. Then: 1. The following variational representation for the log-p</context>
<context position="39331" citStr="Wainwright and Jordan (2008)" startWordPosition="6748" endWordPosition="6751">and γ. Interestingly, we observe that higher scores are obtained for loss functions that are “between” SVMs and CRFs. 7 Related Work There has been recent work studying efficient computation of messages in combinatorial factors: bipartite matchings (Duchi et al., 2007), projective and non-projective arborescences (Smith and Eisner, 2008), as well as high order factors with countbased potentials (Tarlow et al., 2010), among others. Some of our combinatorial factors (OR, OR-WITHOUTPUT) and the analogous entropy computations were never considered, to the best of our knowledge. Prop. 1 appears in Wainwright and Jordan (2008) for canonical overcomplete models; we adapt it here for models with shared features. We rely on the variational interpretation of loopy BP, due to Yedidia et al. (2001), to derive the objective being optimized by Smith and Eisner’s loopy BP parser. Independently of our work, Koo et al. (2010) 14This might be due to the fact that Koo and Collins (2010) trained with the perceptron algorithm and did not use unsupported features. Experiments plugging the perceptron loss (CI --+ oo, 7 --+ 0) into Alg. 1 yielded worse performance than with the hinge loss. recently proposed an efficient dual decompo</context>
</contexts>
<marker>Wainwright, Jordan, 2008</marker>
<rawString>M. J. Wainwright and M. I. Jordan. 2008. Graphical Models, Exponential Families, and Variational Inference. Now Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Wainwright</author>
<author>T S Jaakkola</author>
<author>A S Willsky</author>
</authors>
<title>A new class of upper bounds on the log partition function.</title>
<date>2005</date>
<journal>IEEE Trans. Inf. Theory,</journal>
<volume>51</volume>
<issue>7</issue>
<contexts>
<context position="40984" citStr="Wainwright et al., 2005" startWordPosition="7024" endWordPosition="7027">ence. 8 Conclusion We presented a unified view of two recent approximate dependency parsers, by stating their underlying factor graphs and by deriving the variational problems that they address. We introduced new hard constraint factors, along with formulae for their messages, local belief constraints, and entropies. We provided an aggressive online algorithm for training the models with a broad family of losses. There are several possible directions for future work. Recent progress in message-passing algorithms yield “convexified” Bethe approximations that can be used for marginal inference (Wainwright et al., 2005), and provably convergent max-product variants that solve the relaxed LP (Globerson and Jaakkola, 2008). Other parsing formalisms can be handled with the inventory of factors shown here— among them, phrase-structure parsing. Acknowledgments The authors would like to thank the reviewers for their comments, and Kevin Gimpel, David Smith, David Sontag, and Terry Koo for helpful discussions. A. M. was supported by a grant from FCT/ICTI through the CMUPortugal Program, and also by Priberam Inform´atica. N. S. was supported in part by Qatar NRF NPRP-08-485- 1-083. E. X. was supported by AFOSR FA9550</context>
</contexts>
<marker>Wainwright, Jaakkola, Willsky, 2005</marker>
<rawString>M. J. Wainwright, T.S. Jaakkola, and A.S. Willsky. 2005. A new class of upper bounds on the log partition function. IEEE Trans. Inf. Theory, 51(7):2313–2335.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
<author>Y Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proc. of IWPT.</booktitle>
<contexts>
<context position="36064" citStr="Yamada and Matsumoto (2003)" startWordPosition="6206" endWordPosition="6209">esponds to one output Yt E �(xt). Only the parts that are active in Yt but not in yt, or vice-versa, will have features that might receive a nonzero update. Those parts are reexamined for new features and the active set T is updated accordingly. 6 Experiments We trained non-projective dependency parsers for 14 languages, using datasets from the CoNLL-X shared task (Buchholz and Marsi, 2006) and two datasets for English: one from the CoNLL-2008 shared task (Surdeanu et al., 2008), which contains non-projective arcs, and another derived from the Penn Treebank applying the standard head rules of Yamada and Matsumoto (2003), in which all parse trees are projective.12 We implemented Alg. 1, 12We used the provided train/test splits for all datasets. For English, we used the standard test partitions (section 23 of the Wall Street Journal). We did not exploit the fact that some datasets only contain projective trees and have unique roots. which handles any loss function Lo�,y.13 When 0 &lt; oo, Turbo Parser #1 and the loopy BP algorithm of Smith and Eisner (2008) is used; otherwise, Turbo Parser #2 is used and the LP relaxation is solved with CPLEX. In both cases, we employed the same pruning strategy as Martins et al.</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>H. Yamada and Y. Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proc. of IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Yedidia</author>
<author>W T Freeman</author>
<author>Y Weiss</author>
</authors>
<title>Generalized belief propagation.</title>
<date>2001</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="18463" citStr="Yedidia et al. (2001)" startWordPosition="3116" endWordPosition="3119">del parameters. 4.1 Loopy BP as a Variational Approximation For general factor graphs with loops, the marginal polytope M(Gx) cannot be compactly specified and the entropy term H(µ) lacks a closed form, rendering exact optimizations in Eqs. 9–10 intractable. A popular approximate algorithm for marginal inference is sum-product loopy BP, which passes messages as described in §2 and, upon convergence, computes beliefs via Eqs. 6–7. Were loopy BP exact, these beliefs would be the true marginals and hence a point in the marginal polytope M(Gx). However, this need not be the case, as elucidated by Yedidia et al. (2001) and others, who first analyzed loopy BP from a variational perspective. The following two approximations underlie loopy BP: • The marginal polytope M(Gx) is approximated by the local polytope L(Gx). This is an outer bound; its name derives from the fact that it only imposes local agreement constraints Vi, yi E Yi, C E C: Eyz Ti(yi) = 1, EyC∼y,, TC(yC) = Ti(yi). (11) Namely, it is characterized by L(Gx) , {τ E R|� |� |Eq. 11 holds Vi, yi E Yi, C E C}. The elements of L(Gx) are called pseudo-marginals. Clearly, the true marginals satisfy Eq. 11, and therefore M(Gx) C L(Gx). • The entropy H is r</context>
<context position="39500" citStr="Yedidia et al. (2001)" startWordPosition="6777" endWordPosition="6780">nt computation of messages in combinatorial factors: bipartite matchings (Duchi et al., 2007), projective and non-projective arborescences (Smith and Eisner, 2008), as well as high order factors with countbased potentials (Tarlow et al., 2010), among others. Some of our combinatorial factors (OR, OR-WITHOUTPUT) and the analogous entropy computations were never considered, to the best of our knowledge. Prop. 1 appears in Wainwright and Jordan (2008) for canonical overcomplete models; we adapt it here for models with shared features. We rely on the variational interpretation of loopy BP, due to Yedidia et al. (2001), to derive the objective being optimized by Smith and Eisner’s loopy BP parser. Independently of our work, Koo et al. (2010) 14This might be due to the fact that Koo and Collins (2010) trained with the perceptron algorithm and did not use unsupported features. Experiments plugging the perceptron loss (CI --+ oo, 7 --+ 0) into Alg. 1 yielded worse performance than with the hinge loss. recently proposed an efficient dual decomposition method to solve an LP problem similar (but not equal) to the one in Eq. 20,15 with excellent parsing performance. Their parser is also an instance of a turbo pars</context>
</contexts>
<marker>Yedidia, Freeman, Weiss, 2001</marker>
<rawString>J. S. Yedidia, W. T. Freeman, and Y. Weiss. 2001. Generalized belief propagation. In NIPS.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>