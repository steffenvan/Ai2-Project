<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999262">
Word Alignment for Languages with Scarce Resources
Using Bilingual Corpora of Other Language Pairs
</title>
<author confidence="0.997703">
Haifeng Wang Hua Wu Zhanyi Liu
</author>
<affiliation confidence="0.989256">
Toshiba (China) Research and Development Center
</affiliation>
<address confidence="0.9786745">
5/F., Tower W2, Oriental Plaza, No.1, East Chang An Ave., Dong Cheng District
Beijing, 100738, China
</address>
<email confidence="0.998552">
{wanghaifeng, wuhua, liuzhanyi}@rdc.toshiba.com.cn
</email>
<sectionHeader confidence="0.995652" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999984423076923">
This paper proposes an approach to im-
prove word alignment for languages with
scarce resources using bilingual corpora
of other language pairs. To perform word
alignment between languages L1 and L2,
we introduce a third language L3. Al-
though only small amounts of bilingual
data are available for the desired lan-
guage pair L1-L2, large-scale bilingual
corpora in L1-L3 and L2-L3 are available.
Based on these two additional corpora
and with L3 as the pivot language, we
build a word alignment model for L1 and
L2. This approach can build a word
alignment model for two languages even
if no bilingual corpus is available in this
language pair. In addition, we build an-
other word alignment model for L1 and
L2 using the small L1-L2 bilingual cor-
pus. Then we interpolate the above two
models to further improve word align-
ment between L1 and L2. Experimental
results indicate a relative error rate reduc-
tion of 21.30% as compared with the
method only using the small bilingual
corpus in L1 and L2.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999976979166667">
Word alignment was first proposed as an inter-
mediate result of statistical machine translation
(Brown et al., 1993). Many researchers build
alignment links with bilingual corpora (Wu,
1997; Och and Ney, 2003; Cherry and Lin, 2003;
Zhang and Gildea, 2005). In order to achieve
satisfactory results, all of these methods require a
large-scale bilingual corpus for training. When
the large-scale bilingual corpus is unavailable,
some researchers acquired class-based alignment
rules with existing dictionaries to improve word
alignment (Ker and Chang, 1997). Wu et al.
(2005) used a large-scale bilingual corpus in
general domain to improve domain-specific word
alignment when only a small-scale domain-
specific bilingual corpus is available.
This paper proposes an approach to improve
word alignment for languages with scarce re-
sources using bilingual corpora of other language
pairs. To perform word alignment between lan-
guages L1 and L2, we introduce a third language
L3 as the pivot language. Although only small
amounts of bilingual data are available for the
desired language pair L1-L2, large-scale bilin-
gual corpora in L1-L3 and L2-L3 are available.
Using these two additional bilingual corpora, we
train two word alignment models for language
pairs L1-L3 and L2-L3, respectively. And then,
with L3 as a pivot language, we can build a word
alignment model for L1 and L2 based on the
above two models. Here, we call this model an
induced model. With this induced model, we per-
form word alignment between languages L1 and
L2 even if no parallel corpus is available for this
language pair. In addition, using the small bilin-
gual corpus in L1 and L2, we train another word
alignment model for this language pair. Here, we
call this model an original model. An interpo-
lated model can be built by interpolating the in-
duced model and the original model.
As a case study, this paper uses English as the
pivot language to improve word alignment be-
tween Chinese and Japanese. Experimental re-
sults show that the induced model performs bet-
ter than the original model trained on the small
Chinese-Japanese corpus. And the interpolated
model further improves the word alignment re-
sults, achieving a relative error rate reduction of
</bodyText>
<page confidence="0.97762">
874
</page>
<note confidence="0.723578">
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 874–881,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999771727272727">
21.30% as compared with results produced by
the original model.
The remainder of this paper is organized as
follows. Section 2 discusses the related work.
Section 3 introduces the statistical word align-
ment models. Section 4 describes the parameter
estimation method using bilingual corpora of
other language pairs. Section 5 presents the in-
terpolation model. Section 6 reports the experi-
mental results. Finally, we conclude and present
the future work in section 7.
</bodyText>
<sectionHeader confidence="0.999781" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999974787234043">
A shared task on word alignment was organized
as part of the ACL 2005 Workshop on Building
and Using Parallel Texts (Martin et al., 2005).
The focus of the task was on languages with
scarce resources. Two different subtasks were
defined: Limited resources and Unlimited re-
sources. The former subtask only allows partici-
pating systems to use the resources provided.
The latter subtask allows participating systems to
use any resources in addition to those provided.
For the subtask of unlimited resources, As-
wani and Gaizauskas (2005) used a multi-feature
approach for many-to-many word alignment on
English-Hindi parallel corpora. This approach
performed local word grouping on Hindi sen-
tences and used other methods such as dictionary
lookup, transliteration similarity, expected Eng-
lish words, and nearest aligned neighbors. Martin
et al. (2005) reported that this method resulted in
absolute improvements of up to 20% as com-
pared with the case of only using limited re-
sources. Tufis et al. (2005) combined two word
aligners: one is based on the limited resources
and the other is based on the unlimited resources.
The unlimited resource consists of a translation
dictionary extracted from the alignment of Ro-
manian and English WordNet. Lopez and Resnik
(2005) extended the HMM model by integrating
a tree distortion model based on a dependency
parser built on the English side of the parallel
corpus. The latter two methods produced compa-
rable results with those methods using limited
resources. All the above three methods use some
language dependent resources such as dictionary,
thesaurus, and dependency parser. And some
methods, such as transliteration similarity, can
only be used for very similar language pairs.
In this paper, besides the limited resources for
the given language pair, we make use of large
amounts of resources available for other lan-
guage pairs to address the alignment problem for
languages with scarce resources. Our method
does not need language-dependent resources or
deep linguistic processing. Thus, it is easy to
adapt to any language pair where a pivot lan-
guage and corresponding large-scale bilingual
corpora are available.
</bodyText>
<sectionHeader confidence="0.989203" genericHeader="method">
3 Statistical Word Alignment
</sectionHeader>
<bodyText confidence="0.993730333333333">
According to the IBM models (Brown et al.,
1993), the statistical word alignment model can
be generally represented as in equation (1).
</bodyText>
<figure confidence="0.875783714285714">
Pr( )
a, f  |c
Pr( )
a, f  |c =
∑Pr( )
a&apos;, f  |c (1)
a&apos;
</figure>
<bodyText confidence="0.566824">
Where, and represent the source sentence
</bodyText>
<subsectionHeader confidence="0.411707">
c f
</subsectionHeader>
<bodyText confidence="0.9146118">
and the target sentence, respectively1.
In this paper, we use a simplified IBM model
4 (Al-Onaizan et al., 1999), which is shown in
equation (2). This version does not take into ac-
count word classes in Brown et al. (1993).
</bodyText>
<equation confidence="0.993871">
a, f  |c
Pr( )
⎛ ⎜ m−
φ0 ⎞ m⎠p0 −2φ0 φ0 ⋅
= p1
⎝ φ0
l m
∏ n(φi  |ci) • ∏ t(f j  |ca j
= a j ≠
1, 0
([j ≠h (a j)] d&gt;1
)� (j − p(j))))
1, 0
= aj ≠
</equation>
<bodyText confidence="0.929248428571429">
l, m are the lengths of the source sentence and
the target sentence respectively.
j is the position index of the target word.
aj is the position of the source word aligned to
the jth target word.
φi is the fertility of ci .
p0 , are the fertility probabilities for ,
</bodyText>
<equation confidence="0.990261">
p 1 c0
and p0+ p1 =1 .
t(fj  |caj ) is the word translation probability.
n(φi  |ci) is the fertility probability.
</equation>
<bodyText confidence="0.64551">
d1 (j− ⊙i−1) is the distortion probability for the
head word of the cept.
d&gt;1 (j − p(j)) is the distortion probability for
the non-head words of the cept.
</bodyText>
<footnote confidence="0.995117666666667">
1 This paper uses c and f to represent a Chinese sentence
and a Japanese sentence, respectively. And e represents an
English sentence.
</footnote>
<equation confidence="0.979484947368421">
i =1 j=1 (2)
m
( ([j (j − ⊙i−1))
∏
+
⋅
)
j
m
∏
j
875
h i =
( ) min{ :
k i = a is the head of cept i.
k }
k
p j = k &lt; j k a = a
( ) max { : j k
</equation>
<bodyText confidence="0.998493857142857">
⊙i is the center of cept i.
During the training process, IBM model 3 is
first trained, and then the parameters in model 3
are employed to train model 4. For convenience,
we describe model 3 in equation (3). The main
difference between model 3 and model 4 lies in
the calculation of distortion probability.
</bodyText>
<equation confidence="0.996791615384615">
⎛−
m φ0 ⎞ m −2 φ φ
0 0
Pr( )
a, f  |c = ⎜ ⎟ p p
0 1
⎝ φ0 ⎠
l l
∏ n(φi  |ci ) ⋅ ∏ φi
=1 i=
m
∏t(fj  |caj)⋅ ∏d(j |a
=1 j=1,aj≠0
</equation>
<sectionHeader confidence="0.97967" genericHeader="method">
4 Parameter Estimation Using Bilingual
</sectionHeader>
<subsectionHeader confidence="0.886556">
Corpora of Other Language Pairs
</subsectionHeader>
<bodyText confidence="0.9999878">
As shown in section 3, the word alignment
model mainly has three kinds of parameters that
must be specified, including the translation prob-
ability, the fertility probability, and the distortion
probability. The parameters are usually estimated
by using bilingual sentence pairs in the desired
languages, namely Chinese and Japanese here. In
this section, we describe how to estimate the pa-
rameters without using the Chinese-Japanese
bilingual corpus. We introduce English as the
pivot language, and use the Chinese-English and
English-Japanese bilingual corpora to estimate
the parameters of the Chinese-Japanese word
alignment model. With these two corpora, we
first build Chinese-English and English-Japanese
word alignment models as described in section 3.
Then, based on these two models, we estimate
the parameters of Chinese-Japanese word align-
ment model. The estimated model is named in-
duced model.
The following subsections describe the
method to estimate the parameters of Chinese-
Japanese alignment model. For reversed Japa-
nese-Chinese word alignment, the parameters
can be estimated with the same method.
</bodyText>
<subsectionHeader confidence="0.971767">
4.1 Translation Probability
Basic Translation Probability
</subsectionHeader>
<bodyText confidence="0.999129166666667">
We use the translation probabilities trained
with Chinese-English and English-Japanese cor-
pora to estimate the Chinese-Japanese probabil-
ity as shown in equation (4). In (4), we assume
that the translation probability tEJ (f j  |ek , ci) is
independent of the Chinese word .
</bodyText>
<equation confidence="0.9917745">
ci
(4)
(  |) (  |)
f e t e c
⋅
j k CE k
</equation>
<bodyText confidence="0.823453833333333">
Where tCJ (f j  |ci) is the translation probability
for Chinese-Japanese word alignment.
tEJ (f j  |ek) is the translation probability trained
using the English-Japanese corpus. tCE (ek  |ci) is
the translation probability trained using the Chi-
nese-English corpus.
</bodyText>
<subsectionHeader confidence="0.937816">
Cross-Language Word Similarity
</subsectionHeader>
<bodyText confidence="0.999658142857143">
In any language, there are ambiguous words
with more than one sense. Thus, some noise may
be introduced by the ambiguous English word
when we estimate the Chinese-Japanese transla-
tion probability using English as the pivot lan-
guage. For example, the English word &amp;quot;bank&amp;quot; has
at least two senses, namely:
bank1 - a financial organization
bank2 - the border of a river
Let us consider the Chinese word:
河岸 - bank2 (the border of a river)
And the Japanese word:
銀行 - bank1 (a financial organization)
In the Chinese-English corpus, there is high
probability that the Chinese word &amp;quot;河岸(bank2)&amp;quot;
would be translated into the English word &amp;quot;bank&amp;quot;.
And in the English-Japanese corpus, there is also
high probability that the English word &amp;quot;bank&amp;quot;
would be translated into the Japanese word &amp;quot;銀
行(bank1)&amp;quot;.
As a result, when we estimate the translation
probability using equation (4), the translation
probability of &amp;quot; 銀 行 (bank1)&amp;quot; given &amp;quot; 河 岸
(bank2)&amp;quot; is high. Such a result is not what we
expect.
In order to alleviate this problem, we intro-
duce cross-language word similarity to improve
translation probability estimation in equation (4).
The cross-language word similarity describes
how likely a Chinese word is to be translated into
a Japanese word with an English word as the
pivot. We make use of both the Chinese-English
corpus and the English-Japanese corpus to calcu-
late the cross language word similarity between a
Chinese word c and a Japanese word f given an
</bodyText>
<equation confidence="0.952986533333333">
.
}
i
1
m
! ⋅ (3)
⋅
)
l m
, ,
j
j
i
)
tCJ (fj  |c
=
i
)
k
∑
tEJ (fj  |e
ek
c t e c
i ) ( |
⋅ CE k
,
=
tEJ
∑
ek
</equation>
<page confidence="0.938482">
876
</page>
<bodyText confidence="0.490116333333333">
Input: An English word e , a Chinese word c, and a Japanese word ;
f
The Chinese-English corpus; The English-Japanese corpus.
</bodyText>
<listItem confidence="0.9997128">
(1) Construct Set 1: identify those Chinese-English sentence pairs that include the given Chinese
word c and English word e, and put the English sentences in the pairs into Set 1.
(2) Construct Set 2: identify those English-Japanese sentence pairs that include the given English
word e and Japanese word f , and put the English sentences in the pairs into Set 2.
(3) Construct the feature vectors and of the given English word using all other words as
</listItem>
<equation confidence="0.915890833333333">
VCE VEJ
context in Set 1 and Set 2, respectively.
VCE =&lt; (e1 ,ct11),(e2,ct12), ... , (en ,ct1n) &gt;
VEJ =&lt; (e1 , ct21), (e2 , ct22 ), ... , (en , ct2n ) &gt;
Where ctij is the frequency of the context word .
ej ctij = 0 if e j does not occur in Set i .
</equation>
<listItem confidence="0.872343333333333">
(4) Given the English word e , calculate the cross-language word similarity between the Chinese
word c and the Japanese word as in equation (5)
f
</listItem>
<bodyText confidence="0.9806195">
Output: The cross language word similarity sim(c, f; e) of the Chinese word c and the Japanese
word given the English word
</bodyText>
<figure confidence="0.543852">
f e
</figure>
<figureCaption confidence="0.957237">
Figure 1. Similarity Calculation
</figureCaption>
<equation confidence="0.997676">
∑ ct1 j ⋅ ct2 j
sim (c, f ; e) = cos( VCE , VEJ) = j 2 2 (5)
∑
j
∑
j
(ct1 j )
(ct2j)
</equation>
<bodyText confidence="0.9959518">
English word e. For the ambiguous English word
e, both the Chinese word c and the Japanese
word f can be translated into e. The sense of an
instance of the ambiguous English word e can be
determined by the context in which the instance
appears. Thus, the cross-language word similar-
ity between the Chinese word c and the Japanese
word f can be calculated according to the con-
texts of their English translation e. We use the
feature vector constructed using the context
words in the English sentence to represent the
context. So we can calculate the cross-language
word similarity using the feature vectors. The
detailed algorithm is shown in figure 1. This idea
is similar to translation lexicon extraction via a
bridge language (Schafer and Yarowsky, 2002).
For example, the Chinese word &amp;quot;河岸&amp;quot; and its
English translation &amp;quot;bank&amp;quot; (the border of a river)
appears in the following Chinese-English sen-
tence pair:
</bodyText>
<figure confidence="0.534347">
(a) 他们沿着河岸走回家。
(b) They walked home along the river bank.
</figure>
<bodyText confidence="0.9973835">
The Japanese word &amp;quot;銀行&amp;quot; and its English
translation &amp;quot;bank&amp;quot; (a financial organization) ap-
pears in the following English-Japanese sentence
pair:
</bodyText>
<listItem confidence="0.994338">
(c) He has plenty of money in the bank.
(d) 彼は銀行預金が相当ある。
</listItem>
<bodyText confidence="0.951765333333334">
The context words of the English word &amp;quot;bank&amp;quot; in
sentences (b) and (c) are quite different. The dif-
ference indicates the cross language word simi-
larity of the Chinese word &amp;quot;河岸&amp;quot; and the Japa-
nese word &amp;quot;銀行&amp;quot; is low. So they tend to have
different senses.
Translation Probability Embedded with Cross
Language Word Similarity
Based on the cross language word similarity
calculation in equation (5), we re-estimate the
translation probability as shown in (6). Then we
normalize it in equation (7).
The word similarity of the Chinese word &amp;quot;河
岸 (bank2)&amp;quot; and the Japanese word &amp;quot; 銀 行
(bank1)&amp;quot; given the word English word &amp;quot;bank&amp;quot; is
low. Thus, using the updated estimation method,
the translation probability of &amp;quot; 銀 行 (bank1)&amp;quot;
given &amp;quot;河岸(bank2)&amp;quot; becomes low.
</bodyText>
<equation confidence="0.9279835">
t&apos;CJ (fj  |ci )
6
= ∑ (tEJ (fj  |ek )⋅ tCE (ek  |ci )⋅ sim(ci , fj ; ek)) ()
ek
</equation>
<subsectionHeader confidence="0.946488">
4.2 Fertility Probability
</subsectionHeader>
<bodyText confidence="0.999927">
The induced fertility probability is calculated as
shown in (8). Here, we assume that the probabil-
</bodyText>
<figure confidence="0.666263545454545">
tCJ (fj  |ci) = &apos; &apos;
t CJ (f  |cO
f
∑
&apos;
(7)
&apos;
t
f c
CJ (  |)
j i
</figure>
<page confidence="0.974101">
877
</page>
<bodyText confidence="0.993859">
Where, dCJ (j  |i, l.m) is the estimated distortion
probability. is the introduced position of an
k
English word. n is the introduced length of an
English sentence.
In the above equation, we assume that the po-
sition probability Pr(j  |k, n, i,l, m) is independent
of the position of the Chinese word and the
length of the Chinese sentence. And we assume
that the position probability Pr(k  |n, i, l, m) is in-
dependent of the length of Japanese sentence.
Thus, we rewrite these two probabilities as fol-
lows.
</bodyText>
<equation confidence="0.949256333333333">
Pr(j  |k, n, i,l, m) ≈ Pr(j  |k, n, m) = d EJ (j  |k, n, m)
Pr(  |, , , ) Pr(  |, , ) (  |, , )
k i l m n ≈ k i l n = d CE k i l n
</equation>
<bodyText confidence="0.998746">
For the length probability, the English sen-
tence length n is independent of the word posi-
tions i . And we assume that it is uniformly dis-
tributed. Thus, we take it as a constant, and re-
write it as follows.
</bodyText>
<equation confidence="0.723107666666667">
Pr(n  |i,l, m) = Pr(n |l, m) = constant
ity nEJ (φi  |ek, ci ) is independent of the Chinese
word ci .
nCJ (φi  |ci )
= ∑ nEJ (φi  |e k ,ci)⋅ tCE(ek  |ci) (8)
ek
</equation>
<bodyText confidence="0.71073425">
Where, nCJ (φi  |ci ) is the fertility probability for
the Chinese-Japanese alignment. nEJ (φi  |ek) is
the trained fertility probability for the English-
Japanese alignment.
</bodyText>
<subsectionHeader confidence="0.902153">
4.3 Distortion Probability in Model 3
</subsectionHeader>
<bodyText confidence="0.998797142857143">
With the English language as a pivot language,
we calculate the distortion probability of model 3.
For this probability, we introduce two additional
parameters: one is the position of English word
and the other is the length of English sentence.
The distortion probability is estimated as shown
in (9).
</bodyText>
<figure confidence="0.966297431818181">
(φi  |ek) ⋅ tCE (ek  |ci )
n
=
EJ
∑
ek
|
)
i ,
(j
dCJ
l m
,
Pr( j
l m
,
)
=
∑
k,n
, ,  |,
k n i
⋅
=
)
(Pr(  |, , ,
j k n i
l m
,
∑
k,n
k n i l m
 |, , , ) Pr(  |,
⋅ n i
Pr(
))
l m
,
Pr(  |, , , , ) Pr( ,  |, , )
j k n i l m k n i l m
⋅
=
∑
k,n
</figure>
<subsectionHeader confidence="0.566941">
4.4 Distortion Probability in Model 4
</subsectionHeader>
<bodyText confidence="0.903982384615385">
In model 4, there are two parameters for the dis-
tortion probability: one for head words and the
other for non-head words.
Distortion Probability for Head Words
The distortion probability d1 (j − ⊙i−1) for head
words represents the relative position of the head
word of the ith cept and the center of the (i-1)th
cept. Let Δj = j − ⊙i−1 , then is independent of
Δj
the absolute position. Thus, we estimate the dis-
tortion probability by introducing another rela-
tive position Δj&apos; of English words, which is
shown in (11).
</bodyText>
<equation confidence="0.962149342857143">
d 1, CJ (Δj = j − ⊙i−1)
&apos;
)
PrEJ
(Δj  |Δj
1
)
&apos; ,
&apos; −
=
|j
∑
⊙i
Pr ( ,
j ⊙
EJ i −1
( j − ⊙  |&apos;
j − ⊙
i − 1 i &apos; 1
−
j j
, :
⊙ − =Δ
⊙ j
i − 1 i − 1
j &apos;, ⊙ : &apos;
j − ⊙ =Δ
i &apos; 1
− i &apos; 1
−
)
=
PrEJ
&apos;
j
</equation>
<bodyText confidence="0.960675413793104">
The English word in position is aligned to
j&apos;
the Japanese word in position j, and the English
word in position is aligned to the Japanese
⊙i &apos;− 1
word in position .
⊙i−1
We assume that and are independent,
j ⊙i −1
j only depends on , and only depends
j&apos; ⊙i −1
on . Then
⊙i &apos;− 1 PrEJ (j,⊙i−1  |j&apos; ,⊙i&apos;−1) can be esti-
mated as shown in (13).
Where, d1 ,CJ (Δj = j − ⊙i−1) is the estimated dis-
tortion probability for head words in Chinese-
Japanese alignment. d1,CE(Δj&apos; ) is the distortion
probability for head word in Chinese-English
alignment. PrEJ (Δj  |Δj&apos; ) is the translation prob-
ability of relative Japanese position given rela-
tive English position.
In order to simplify PrEJ (Δj  |Δj&apos; ) , we introduce
j&apos; and and let
⊙i &apos;−1 Δj&apos; = j&apos;− ⊙i&apos;−1 , where and
j&apos;
are positions of English words. We rewrite
PrEJ(Δj  |Δj&apos; ) in (12).
According to the above three assumptions, we
ignore the length probability Pr(n |l, m) . Equa-
</bodyText>
<figure confidence="0.76018225">
tion (9) is rewritten in (10).
dCJ
|
(j
i, l . m)
= ∑ d EJ (j  |k, n, m) ⋅ dCE (  |, , ) (10)
k i l n
k,n
&apos;
&apos;
=
d1,
CE
)
⋅ PrEJ
)
(Δj
&apos;
∑
Δj
(Δj  |Δj
1
⊙i &apos;−
(9)
</figure>
<page confidence="0.764343">
878
</page>
<equation confidence="0.829594">
= PrEJ U  |j&apos; ) ⋅ PrEJ (Oi−1  |O−1) (13)
</equation>
<bodyText confidence="0.936642222222222">
Both of the two parameters in (13) represent
the position translation probabilities. Thus, we
can estimate them from the distortion probability
in model 3. PrEJ(j  |j&apos; ) is estimated as shown in
(14). And PrEJ(⊙i−1 |⊙i&apos;−1) can be estimated in
the same way. In (14), we also assume that the
sentence length distribution Pr(l,m  |j&apos; ) is inde-
pendent of the word position and that it is uni-
formly distributed.
</bodyText>
<figure confidence="0.756746444444444">
=∑ j
&apos; ) Pr (
EJ
l m
,
(  |&apos; , l m ) Pr( ,  |&apos; ) (14)
j j &apos; , , ⋅ l m j
(  |l m )
j j ,
</figure>
<sectionHeader confidence="0.384377" genericHeader="method">
Distortion Probability for Non-Head Words
</sectionHeader>
<bodyText confidence="0.950684">
The distortion probability d&gt;1(j − p(j)) de-
scribes the distribution of the relative position of
non-head words. In the same way, we introduce
relative position of English words, and model
</bodyText>
<equation confidence="0.8927998">
Δj&apos;
the probability in (15).
(Δ = −
j j p j
( ))
</equation>
<bodyText confidence="0.9296408">
d&gt;1 ,CJ (Δj = j − p(j)) is the estimated distortion
probability for the non-head words in Chinese-
Japanese alignment. d&gt;1,CE(Δj&apos; ) is the distortion
probability for non-head words in Chinese-
English alignment. PrEJ (Δj  |Δj&apos; ) is the translation
probability of the relative Japanese position
given the relative English position.
In fact, PrEJ (Δj  |Δj&apos; ) has the same interpreta-
tion as in (12). Thus, we introduce two parame-
ters and and let
</bodyText>
<equation confidence="0.773825333333333">
j&apos; p (j&apos; ) Δj&apos;= j&apos;−p(j&apos; ) , where
j&apos; and are positions of English words. The
p ( j&apos; )
</equation>
<bodyText confidence="0.9875965">
final distortion probability for non-head words
can be estimated as shown in (16).
</bodyText>
<equation confidence="0.929888875">
(Δ = −
j j p j
( )) = ∑ (d&gt;1,CE (Δj
Δj
&apos;
∑ Pr (  |&apos; ) Pr ( ( )  |(
j j ⋅ p j p j
EJ EJ
</equation>
<sectionHeader confidence="0.998307" genericHeader="method">
5 Interpolation Model
</sectionHeader>
<bodyText confidence="0.999632142857143">
With the Chinese-English and English-Japanese
corpora, we can build the induced model for Chi-
nese-Japanese word alignment as described in
section 4. If we have small amounts of Chinese-
Japanese corpora, we can build another word
alignment model using the method described in
section 3, which is called the original model here.
In order to further improve the performance of
Chinese-Japanese word alignment, we build an
interpolated model by interpolating the induced
model and the original model.
Generally, we can interpolate the induced
model and the original model as shown in equa-
tion (17).
</bodyText>
<equation confidence="0.670032666666667">
Pr( )
=λ⋅ PrO(a,f  |c)+(1−λ)⋅ PrI(a,f  |c) (17)
a, f  |c
</equation>
<bodyText confidence="0.980269869565218">
Where PrO (a, f  |c) is the original model trained
from the Chinese-Japanese corpus, and
PrI(a,f  |c) is the induced model trained from the
Chinese-English and English-Japanese corpora.
λ is an interpolation weight. It can be a constant
or a function of f and c .
In both model 3 and model 4, there are mainly
three kinds of parameters: translation probability,
fertility probability and distortion probability.
These three kinds of parameters have their own
interpretation in these two models. In order to
obtain fine-grained interpolation models, we in-
terpolate the three kinds of parameters using dif-
ferent weights, which are obtained in the same
way as described in Wu et al. (2005). λt repre-
sents the weights for translation probability. λn
represents the weights for fertility probability.
λd3 and λd4 represent the weights for distortion
probability in model 3 and in model 4, respec-
tively. λd4 is set as the interpolation weight for
both the head words and the non-head words.
The above four weights are obtained using a
manually annotated held-out set.
</bodyText>
<sectionHeader confidence="0.998807" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999827571428572">
In this section, we compare different word
alignment methods for Chinese-Japanese align-
ment. The &amp;quot;Original&amp;quot; method uses the original
model trained with the small Chinese-Japanese
corpus. The &amp;quot;Basic Induced&amp;quot; method uses the
induced model that employs the basic translation
probability without introducing cross-language
word similarity. The &amp;quot;Advanced Induced&amp;quot;
method uses the induced model that introduces
the cross-language word similarity into the calcu-
lation of the translation probability. The &amp;quot;Inter-
polated&amp;quot; method uses the interpolation of the
word alignment models in the &amp;quot;Advanced In-
duced&amp;quot; and &amp;quot;Original&amp;quot; methods.
</bodyText>
<figure confidence="0.997784189655172">
&apos;
&apos;
)
(15)
(Δj
=
∑
) ⋅ PrEJ (Δj  |Δj
d&gt;1, CE
&apos;
Δj
=
j p j j p
, ( ): −
( )
j
Δj
&apos;
=
Δj
j p j j p
&apos;, ( &apos;) : &apos; −
( &apos;)
j
EJ − 1 &apos; 1
−
(j
,Oi
Pr ,
|j
Oi
)
&apos;
PrEJ
=
( |
j j
dEJ
∑
l m
,
=
dEJ
∑
l m
,
l m  |j &apos; )
, ,
d&gt;
1, CJ
d&gt;
1 ,CJ
&apos;
&apos;
⋅
)
)))
(16)
</figure>
<page confidence="0.978496">
879
</page>
<subsectionHeader confidence="0.926839">
6.1 Data
</subsectionHeader>
<bodyText confidence="0.9959092">
There are three training corpora used in this pa-
per: Chinese-Japanese (CJ) corpus, Chinese-
English (CE) Corpus, and English-Japanese (EJ)
Corpus. All of these tree corpora are from gen-
eral domain. The Chinese sentences and Japa-
nese sentences in the data are automatically seg-
mented into words. The statistics of these three
corpora are shown in table 1. &amp;quot;# Source Words&amp;quot;
and &amp;quot;# Target Words&amp;quot; mean the word number of
the source and target sentences, respectively.
</bodyText>
<table confidence="0.9998624">
Language #Sentence # Source # Target
Pairs Pairs Words Words
CJ 21,977 197,072 237,834
CE 329,350 4,682,103 4,480,034
EJ 160,535 1,460,043 1,685,204
</table>
<tableCaption confidence="0.999186">
Table 1. Statistics for Training Data
</tableCaption>
<bodyText confidence="0.9979769">
Besides the training data, we also have held-
out data and testing data. The held-out data in-
cludes 500 Chinese-Japanese sentence pairs,
which is used to set the interpolated weights de-
scribed in section 5. We use another 1,000 Chi-
nese-Japanese sentence pairs as testing data,
which is not included in the training data and the
held-out data. The alignment links in the held-out
data and the testing data are manually annotated.
Testing data includes 4,926 alignment links2.
</bodyText>
<subsectionHeader confidence="0.990875">
6.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.9999424">
We use the same metrics as described in Wu et al.
(2005), which is similar to those in (Och and Ney,
2000). The difference lies in that Wu et al. (2005)
took all alignment links as sure links.
If we use to represent the set of alignment
</bodyText>
<sectionHeader confidence="0.452752" genericHeader="evaluation">
SG
</sectionHeader>
<bodyText confidence="0.903632583333333">
links identified by the proposed methods and SC
to denote the reference alignment set, the meth-
ods to calculate the precision, recall, f-measure,
and alignment error rate (AER) are shown in
equations (18), (19), (20), and (21), respectively.
It can be seen that the higher the f-measure is,
the lower the alignment error rate is. Thus, we
will only show precision, recall and AER scores
in the evaluation results.
2 For a non one-to-one link, if m source words are aligned to
n target words, we take it as one alignment link instead of
m∗n alignment links.
</bodyText>
<equation confidence="0.9897072">
2  |SS |
G ∩ C
AER = −
1 = −
1
</equation>
<subsectionHeader confidence="0.986652">
6.3 Experimental Results
</subsectionHeader>
<bodyText confidence="0.999185133333333">
We use the held-out data described in section 6.1
to set the interpolation weights in section 5. λt is
set to 0.3, λn is set to 0.1, λd3 for model 3 is set
to 0.5, and λd4 for model 4 is set to 0.1. With
these parameters, we get the lowest alignment
error rate on the held-out data.
For each method described above, we perform
bi-directional (source to target and target to
source) word alignment and obtain two align-
ment results. Based on the two results, we get a
result using &amp;quot;refined&amp;quot; combination as described
in (Och and Ney, 2000). Thus, all of the results
reported here describe the results of the &amp;quot;refined&amp;quot;
combination. For model training, we use the
GIZA++ toolkit3.
</bodyText>
<table confidence="0.999577285714286">
Method Precision Recall AER
Interpolated 0.6955 0.5802 0.3673
Advanced 0.7382 0.4803 0.4181
Induced
Basic 0.6787 0.4602 0.4515
Induced
Original 0.6026 0.4783 0.4667
</table>
<tableCaption confidence="0.999621">
Table 2. Word Alignment Results
</tableCaption>
<bodyText confidence="0.99993505">
The evaluation results on the testing data are
shown in table 2. From the results, it can be seen
that both of the two induced models perform bet-
ter than the &amp;quot;Original&amp;quot; method that only uses the
limited Chinese-Japanese sentence pairs. The
&amp;quot;Advanced Induced&amp;quot; method achieves a relative
error rate reduction of 10.41% as compared with
the &amp;quot;Original&amp;quot; method. Thus, with the Chinese-
English corpus and the English-Japanese corpus,
we can achieve a good word alignment results
even if no Chinese-Japanese parallel corpus is
available. After introducing the cross-language
word similarity into the translation probability,
the &amp;quot;Advanced Induced&amp;quot; method achieves a rela-
tive error rate reduction of 7.40% as compared
with the &amp;quot;Basic Induced&amp;quot; method. It indicates
that cross-language word similarity is effective in
the calculation of the translation probability.
Moreover, the &amp;quot;interpolated&amp;quot; method further im-
proves the result, which achieves relative error
</bodyText>
<footnote confidence="0.542118">
3 It is located at http://www.fjoch.com/ GIZA++.html.
</footnote>
<figure confidence="0.99793432">
 |SG ∩ SC |
=
precision
 ||
SG
|
=
recall
|
S
|
C
S S
G ∩ C
|
2 |S S |
G ∩ C
=
fmeasure
(20)
|
SG|+  |SC |
 |S G  |+ ||
SC
fmeasure (21)
</figure>
<page confidence="0.987128">
880
</page>
<bodyText confidence="0.996212333333333">
rate reductions of 12.51% and 21.30% as com-
pared with the &amp;quot;Advanced Induced&amp;quot; method and
the &amp;quot;Original&amp;quot; method.
</bodyText>
<sectionHeader confidence="0.974174" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999989289473684">
This paper presented a word alignment approach
for languages with scarce resources using bilin-
gual corpora of other language pairs. To perform
word alignment between languages L1 and L2,
we introduce a pivot language L3 and bilingual
corpora in L1-L3 and L2-L3. Based on these two
corpora and with the L3 as a pivot language, we
proposed an approach to estimate the parameters
of the statistical word alignment model. This ap-
proach can build a word alignment model for the
desired language pair even if no bilingual corpus
is available in this language pair. Experimental
results indicate a relative error reduction of
10.41% as compared with the method using the
small bilingual corpus.
In addition, we interpolated the above model
with the model trained on the small L1-L2 bilin-
gual corpus to further improve word alignment
between L1 and L2. This interpolated model fur-
ther improved the word alignment results by
achieving a relative error rate reduction of
12.51% as compared with the method using the
two corpora in L1-L3 and L3-L2, and a relative
error rate reduction of 21.30% as compared with
the method using the small bilingual corpus in
L1 and L2.
In future work, we will perform more evalua-
tions. First, we will further investigate the effect
of the size of corpora on the alignment results.
Second, we will investigate different parameter
combination of the induced model and the origi-
nal model. Third, we will also investigate how
simpler IBM models 1 and 2 perform, in com-
parison with IBM models 3 and 4. Last, we will
evaluate the word alignment results in a real ma-
chine translation system, to examine whether
lower word alignment error rate will result in
higher translation accuracy.
</bodyText>
<sectionHeader confidence="0.999475" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999888888888889">
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John Lafferty, Dan Melamed, Franz-Josef
Och, David Purdy, Noah A. Smith, and David
Yarowsky. 1999. Statistical Machine Translation
Final Report. Johns Hopkins University Workshop.
Niraj Aswani and Robert Gaizauskas. 2005. Aligning
Words in English-Hindi Parallel Corpora. In Proc.
of the ACL 2005 Workshop on Building and Using
Parallel Texts: Data-driven Machine Translation
and Beyond, pages 115-118.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation:
Parameter Estimation. Computational Linguistics,
19(2): 263-311.
Colin Cherry and Dekang Lin. 2003. A Probability
Model to Improve Word Alignment. In Proc. of the
41st Annual Meeting of the Association for Compu-
tational Linguistics (ACL-2003), pages 88-95.
Sue J. Ker and Jason S. Chang. 1997. A Class-based
Approach to Word Alignment. Computational Lin-
guistics, 23(2): 313-343.
Adam Lopez and Philip Resnik. 2005. Improved
HMM Alignment Models for Languages with
Scarce Resources. In Proc. of the ACL-2005 Work-
shop on Building and Using Parallel Texts: Data-
driven Machine Translation and Beyond, pages 83-
86.
Joel Martin, Rada Mihalcea, and Ted Pedersen. 2005.
Word Alignment for Languages with Scarce Re-
sources. In Proc. of the ACL-2005 Workshop on
Building and Using Parallel Texts: Data-driven
Machine Translation and Beyond, pages 65-74.
Charles Schafer and David Yarowsky. 2002. Inducing
Translation Lexicons via Diverse Similarity Meas-
ures and Bridge Languages. In Proc. of the 6th
Conference on Natural Language Learning 2002
(CoNLL-2002), pages 1-7.
Dan Tufis, Radu Ion, Alexandru Ceausu, and Dan
Stefanescu. 2005. Combined Word Alignments. In
Proc. of the ACL-2005 Workshop on Building and
Using Parallel Texts: Data-driven Machine Trans-
lation and Beyond, pages 107-110.
Franz Josef Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. In Proc. of the 38th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-2000), pages 440-447.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19-51.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23(3):377-403.
Hua Wu, Haifeng Wang, and Zhanyi Liu. 2005.
Alignment Model Adaptation for Domain-Specific
Word Alignment. In Proc. of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL-2005), pages 467-474.
Hao Zhang and Daniel Gildea. 2005. Stochastic Lexi-
calized Inversion Transduction Grammar for
Alignment. In Proc. of the 43rd Annual Meeting of
the Association for Computational Linguistics
(ACL-2005), pages 475-482.
</reference>
<page confidence="0.998436">
881
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.605262">
<title confidence="0.997682">Word Alignment for Languages with Scarce Resources Using Bilingual Corpora of Other Language Pairs</title>
<author confidence="0.986684">Haifeng Wang Hua Wu Zhanyi Liu</author>
<affiliation confidence="0.989726">Toshiba (China) Research and Development Center</affiliation>
<address confidence="0.9937775">5/F., Tower W2, Oriental Plaza, No.1, East Chang An Ave., Dong Cheng District Beijing, 100738, China</address>
<email confidence="0.994089">wanghaifeng@rdc.toshiba.com.cn</email>
<email confidence="0.994089">wuhua@rdc.toshiba.com.cn</email>
<email confidence="0.994089">liuzhanyi@rdc.toshiba.com.cn</email>
<abstract confidence="0.985995592592593">This paper proposes an approach to improve word alignment for languages with scarce resources using bilingual corpora of other language pairs. To perform word alignment between languages L1 and L2, we introduce a third language L3. Although only small amounts of bilingual data are available for the desired language pair L1-L2, large-scale bilingual corpora in L1-L3 and L2-L3 are available. Based on these two additional corpora and with L3 as the pivot language, we build a word alignment model for L1 and L2. This approach can build a word alignment model for two languages even if no bilingual corpus is available in this language pair. In addition, we build another word alignment model for L1 and L2 using the small L1-L2 bilingual corpus. Then we interpolate the above two models to further improve word alignment between L1 and L2. Experimental results indicate a relative error rate reduction of 21.30% as compared with the method only using the small bilingual corpus in L1 and L2.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Jan Curin</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>John Lafferty</author>
<author>Dan Melamed</author>
<author>Franz-Josef Och</author>
<author>David Purdy</author>
<author>Noah A Smith</author>
<author>David Yarowsky</author>
</authors>
<title>Statistical Machine Translation Final Report.</title>
<date>1999</date>
<institution>Johns Hopkins University Workshop.</institution>
<contexts>
<context position="6743" citStr="Al-Onaizan et al., 1999" startWordPosition="1081" endWordPosition="1084">nguages with scarce resources. Our method does not need language-dependent resources or deep linguistic processing. Thus, it is easy to adapt to any language pair where a pivot language and corresponding large-scale bilingual corpora are available. 3 Statistical Word Alignment According to the IBM models (Brown et al., 1993), the statistical word alignment model can be generally represented as in equation (1). Pr( ) a, f |c Pr( ) a, f |c = ∑Pr( ) a&apos;, f |c (1) a&apos; Where, and represent the source sentence c f and the target sentence, respectively1. In this paper, we use a simplified IBM model 4 (Al-Onaizan et al., 1999), which is shown in equation (2). This version does not take into account word classes in Brown et al. (1993). a, f |c Pr( ) ⎛ ⎜ m− φ0 ⎞ m⎠p0 −2φ0 φ0 ⋅ = p1 ⎝ φ0 l m ∏ n(φi |ci) • ∏ t(f j |ca j = a j ≠ 1, 0 ([j ≠h (a j)] d&gt;1 )� (j − p(j)))) 1, 0 = aj ≠ l, m are the lengths of the source sentence and the target sentence respectively. j is the position index of the target word. aj is the position of the source word aligned to the jth target word. φi is the fertility of ci . p0 , are the fertility probabilities for , p 1 c0 and p0+ p1 =1 . t(fj |caj ) is the word translation probability. n(φi |ci</context>
</contexts>
<marker>Al-Onaizan, Curin, Jahr, Knight, Lafferty, Melamed, Och, Purdy, Smith, Yarowsky, 1999</marker>
<rawString>Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin Knight, John Lafferty, Dan Melamed, Franz-Josef Och, David Purdy, Noah A. Smith, and David Yarowsky. 1999. Statistical Machine Translation Final Report. Johns Hopkins University Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niraj Aswani</author>
<author>Robert Gaizauskas</author>
</authors>
<title>Aligning Words in English-Hindi Parallel Corpora.</title>
<date>2005</date>
<booktitle>In Proc. of the ACL 2005 Workshop on Building and Using Parallel Texts: Data-driven Machine Translation and Beyond,</booktitle>
<pages>115--118</pages>
<contexts>
<context position="4739" citStr="Aswani and Gaizauskas (2005)" startWordPosition="755" endWordPosition="759">imental results. Finally, we conclude and present the future work in section 7. 2 Related Work A shared task on word alignment was organized as part of the ACL 2005 Workshop on Building and Using Parallel Texts (Martin et al., 2005). The focus of the task was on languages with scarce resources. Two different subtasks were defined: Limited resources and Unlimited resources. The former subtask only allows participating systems to use the resources provided. The latter subtask allows participating systems to use any resources in addition to those provided. For the subtask of unlimited resources, Aswani and Gaizauskas (2005) used a multi-feature approach for many-to-many word alignment on English-Hindi parallel corpora. This approach performed local word grouping on Hindi sentences and used other methods such as dictionary lookup, transliteration similarity, expected English words, and nearest aligned neighbors. Martin et al. (2005) reported that this method resulted in absolute improvements of up to 20% as compared with the case of only using limited resources. Tufis et al. (2005) combined two word aligners: one is based on the limited resources and the other is based on the unlimited resources. The unlimited re</context>
</contexts>
<marker>Aswani, Gaizauskas, 2005</marker>
<rawString>Niraj Aswani and Robert Gaizauskas. 2005. Aligning Words in English-Hindi Parallel Corpora. In Proc. of the ACL 2005 Workshop on Building and Using Parallel Texts: Data-driven Machine Translation and Beyond, pages 115-118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter Estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>263--311</pages>
<contexts>
<context position="1462" citStr="Brown et al., 1993" startWordPosition="233" endWordPosition="236"> and L2. This approach can build a word alignment model for two languages even if no bilingual corpus is available in this language pair. In addition, we build another word alignment model for L1 and L2 using the small L1-L2 bilingual corpus. Then we interpolate the above two models to further improve word alignment between L1 and L2. Experimental results indicate a relative error rate reduction of 21.30% as compared with the method only using the small bilingual corpus in L1 and L2. 1 Introduction Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993). Many researchers build alignment links with bilingual corpora (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003; Zhang and Gildea, 2005). In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training. When the large-scale bilingual corpus is unavailable, some researchers acquired class-based alignment rules with existing dictionaries to improve word alignment (Ker and Chang, 1997). Wu et al. (2005) used a large-scale bilingual corpus in general domain to improve domain-specific word alignment when only a small-scale domainspecific bilingu</context>
<context position="6445" citStr="Brown et al., 1993" startWordPosition="1023" endWordPosition="1026">nd some methods, such as transliteration similarity, can only be used for very similar language pairs. In this paper, besides the limited resources for the given language pair, we make use of large amounts of resources available for other language pairs to address the alignment problem for languages with scarce resources. Our method does not need language-dependent resources or deep linguistic processing. Thus, it is easy to adapt to any language pair where a pivot language and corresponding large-scale bilingual corpora are available. 3 Statistical Word Alignment According to the IBM models (Brown et al., 1993), the statistical word alignment model can be generally represented as in equation (1). Pr( ) a, f |c Pr( ) a, f |c = ∑Pr( ) a&apos;, f |c (1) a&apos; Where, and represent the source sentence c f and the target sentence, respectively1. In this paper, we use a simplified IBM model 4 (Al-Onaizan et al., 1999), which is shown in equation (2). This version does not take into account word classes in Brown et al. (1993). a, f |c Pr( ) ⎛ ⎜ m− φ0 ⎞ m⎠p0 −2φ0 φ0 ⋅ = p1 ⎝ φ0 l m ∏ n(φi |ci) • ∏ t(f j |ca j = a j ≠ 1, 0 ([j ≠h (a j)] d&gt;1 )� (j − p(j)))) 1, 0 = aj ≠ l, m are the lengths of the source sentence and t</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2): 263-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Dekang Lin</author>
</authors>
<title>A Probability Model to Improve Word Alignment.</title>
<date>2003</date>
<booktitle>In Proc. of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-2003),</booktitle>
<pages>88--95</pages>
<contexts>
<context position="1576" citStr="Cherry and Lin, 2003" startWordPosition="251" endWordPosition="254">e in this language pair. In addition, we build another word alignment model for L1 and L2 using the small L1-L2 bilingual corpus. Then we interpolate the above two models to further improve word alignment between L1 and L2. Experimental results indicate a relative error rate reduction of 21.30% as compared with the method only using the small bilingual corpus in L1 and L2. 1 Introduction Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993). Many researchers build alignment links with bilingual corpora (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003; Zhang and Gildea, 2005). In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training. When the large-scale bilingual corpus is unavailable, some researchers acquired class-based alignment rules with existing dictionaries to improve word alignment (Ker and Chang, 1997). Wu et al. (2005) used a large-scale bilingual corpus in general domain to improve domain-specific word alignment when only a small-scale domainspecific bilingual corpus is available. This paper proposes an approach to improve word alignment for languages with scarce resour</context>
</contexts>
<marker>Cherry, Lin, 2003</marker>
<rawString>Colin Cherry and Dekang Lin. 2003. A Probability Model to Improve Word Alignment. In Proc. of the 41st Annual Meeting of the Association for Computational Linguistics (ACL-2003), pages 88-95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sue J Ker</author>
<author>Jason S Chang</author>
</authors>
<title>A Class-based Approach to Word Alignment.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>2</issue>
<pages>313--343</pages>
<contexts>
<context position="1901" citStr="Ker and Chang, 1997" startWordPosition="296" endWordPosition="299">nly using the small bilingual corpus in L1 and L2. 1 Introduction Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993). Many researchers build alignment links with bilingual corpora (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003; Zhang and Gildea, 2005). In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training. When the large-scale bilingual corpus is unavailable, some researchers acquired class-based alignment rules with existing dictionaries to improve word alignment (Ker and Chang, 1997). Wu et al. (2005) used a large-scale bilingual corpus in general domain to improve domain-specific word alignment when only a small-scale domainspecific bilingual corpus is available. This paper proposes an approach to improve word alignment for languages with scarce resources using bilingual corpora of other language pairs. To perform word alignment between languages L1 and L2, we introduce a third language L3 as the pivot language. Although only small amounts of bilingual data are available for the desired language pair L1-L2, large-scale bilingual corpora in L1-L3 and L2-L3 are available. </context>
</contexts>
<marker>Ker, Chang, 1997</marker>
<rawString>Sue J. Ker and Jason S. Chang. 1997. A Class-based Approach to Word Alignment. Computational Linguistics, 23(2): 313-343.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Lopez</author>
<author>Philip Resnik</author>
</authors>
<title>Improved HMM Alignment Models for Languages with Scarce Resources.</title>
<date>2005</date>
<booktitle>In Proc. of the ACL-2005 Workshop on Building and Using Parallel Texts: Datadriven Machine Translation and Beyond,</booktitle>
<pages>83--86</pages>
<contexts>
<context position="5468" citStr="Lopez and Resnik (2005)" startWordPosition="870" endWordPosition="873">roach performed local word grouping on Hindi sentences and used other methods such as dictionary lookup, transliteration similarity, expected English words, and nearest aligned neighbors. Martin et al. (2005) reported that this method resulted in absolute improvements of up to 20% as compared with the case of only using limited resources. Tufis et al. (2005) combined two word aligners: one is based on the limited resources and the other is based on the unlimited resources. The unlimited resource consists of a translation dictionary extracted from the alignment of Romanian and English WordNet. Lopez and Resnik (2005) extended the HMM model by integrating a tree distortion model based on a dependency parser built on the English side of the parallel corpus. The latter two methods produced comparable results with those methods using limited resources. All the above three methods use some language dependent resources such as dictionary, thesaurus, and dependency parser. And some methods, such as transliteration similarity, can only be used for very similar language pairs. In this paper, besides the limited resources for the given language pair, we make use of large amounts of resources available for other lan</context>
</contexts>
<marker>Lopez, Resnik, 2005</marker>
<rawString>Adam Lopez and Philip Resnik. 2005. Improved HMM Alignment Models for Languages with Scarce Resources. In Proc. of the ACL-2005 Workshop on Building and Using Parallel Texts: Datadriven Machine Translation and Beyond, pages 83-86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Martin</author>
<author>Rada Mihalcea</author>
<author>Ted Pedersen</author>
</authors>
<title>Word Alignment for Languages with Scarce Resources.</title>
<date>2005</date>
<booktitle>In Proc. of the ACL-2005 Workshop on Building and Using Parallel Texts: Data-driven Machine Translation and Beyond,</booktitle>
<pages>65--74</pages>
<contexts>
<context position="4343" citStr="Martin et al., 2005" startWordPosition="695" endWordPosition="698"> 21.30% as compared with results produced by the original model. The remainder of this paper is organized as follows. Section 2 discusses the related work. Section 3 introduces the statistical word alignment models. Section 4 describes the parameter estimation method using bilingual corpora of other language pairs. Section 5 presents the interpolation model. Section 6 reports the experimental results. Finally, we conclude and present the future work in section 7. 2 Related Work A shared task on word alignment was organized as part of the ACL 2005 Workshop on Building and Using Parallel Texts (Martin et al., 2005). The focus of the task was on languages with scarce resources. Two different subtasks were defined: Limited resources and Unlimited resources. The former subtask only allows participating systems to use the resources provided. The latter subtask allows participating systems to use any resources in addition to those provided. For the subtask of unlimited resources, Aswani and Gaizauskas (2005) used a multi-feature approach for many-to-many word alignment on English-Hindi parallel corpora. This approach performed local word grouping on Hindi sentences and used other methods such as dictionary l</context>
</contexts>
<marker>Martin, Mihalcea, Pedersen, 2005</marker>
<rawString>Joel Martin, Rada Mihalcea, and Ted Pedersen. 2005. Word Alignment for Languages with Scarce Resources. In Proc. of the ACL-2005 Workshop on Building and Using Parallel Texts: Data-driven Machine Translation and Beyond, pages 65-74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Schafer</author>
<author>David Yarowsky</author>
</authors>
<title>Inducing Translation Lexicons via Diverse Similarity Measures and Bridge Languages.</title>
<date>2002</date>
<booktitle>In Proc. of the 6th Conference on Natural Language Learning</booktitle>
<pages>1--7</pages>
<contexts>
<context position="13626" citStr="Schafer and Yarowsky, 2002" startWordPosition="2375" endWordPosition="2378">nse of an instance of the ambiguous English word e can be determined by the context in which the instance appears. Thus, the cross-language word similarity between the Chinese word c and the Japanese word f can be calculated according to the contexts of their English translation e. We use the feature vector constructed using the context words in the English sentence to represent the context. So we can calculate the cross-language word similarity using the feature vectors. The detailed algorithm is shown in figure 1. This idea is similar to translation lexicon extraction via a bridge language (Schafer and Yarowsky, 2002). For example, the Chinese word &amp;quot;河岸&amp;quot; and its English translation &amp;quot;bank&amp;quot; (the border of a river) appears in the following Chinese-English sentence pair: (a) 他们沿着河岸走回家。 (b) They walked home along the river bank. The Japanese word &amp;quot;銀行&amp;quot; and its English translation &amp;quot;bank&amp;quot; (a financial organization) appears in the following English-Japanese sentence pair: (c) He has plenty of money in the bank. (d) 彼は銀行預金が相当ある。 The context words of the English word &amp;quot;bank&amp;quot; in sentences (b) and (c) are quite different. The difference indicates the cross language word similarity of the Chinese word &amp;quot;河岸&amp;quot; and the Japanes</context>
</contexts>
<marker>Schafer, Yarowsky, 2002</marker>
<rawString>Charles Schafer and David Yarowsky. 2002. Inducing Translation Lexicons via Diverse Similarity Measures and Bridge Languages. In Proc. of the 6th Conference on Natural Language Learning 2002 (CoNLL-2002), pages 1-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Tufis</author>
<author>Radu Ion</author>
<author>Alexandru Ceausu</author>
<author>Dan Stefanescu</author>
</authors>
<title>Combined Word Alignments.</title>
<date>2005</date>
<booktitle>In Proc. of the ACL-2005 Workshop on Building and Using Parallel Texts: Data-driven Machine Translation and Beyond,</booktitle>
<pages>107--110</pages>
<contexts>
<context position="5205" citStr="Tufis et al. (2005)" startWordPosition="828" endWordPosition="831">ask allows participating systems to use any resources in addition to those provided. For the subtask of unlimited resources, Aswani and Gaizauskas (2005) used a multi-feature approach for many-to-many word alignment on English-Hindi parallel corpora. This approach performed local word grouping on Hindi sentences and used other methods such as dictionary lookup, transliteration similarity, expected English words, and nearest aligned neighbors. Martin et al. (2005) reported that this method resulted in absolute improvements of up to 20% as compared with the case of only using limited resources. Tufis et al. (2005) combined two word aligners: one is based on the limited resources and the other is based on the unlimited resources. The unlimited resource consists of a translation dictionary extracted from the alignment of Romanian and English WordNet. Lopez and Resnik (2005) extended the HMM model by integrating a tree distortion model based on a dependency parser built on the English side of the parallel corpus. The latter two methods produced comparable results with those methods using limited resources. All the above three methods use some language dependent resources such as dictionary, thesaurus, and</context>
</contexts>
<marker>Tufis, Ion, Ceausu, Stefanescu, 2005</marker>
<rawString>Dan Tufis, Radu Ion, Alexandru Ceausu, and Dan Stefanescu. 2005. Combined Word Alignments. In Proc. of the ACL-2005 Workshop on Building and Using Parallel Texts: Data-driven Machine Translation and Beyond, pages 107-110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved Statistical Alignment Models.</title>
<date>2000</date>
<booktitle>In Proc. of the 38th Annual Meeting of the Association for Computational Linguistics (ACL-2000),</booktitle>
<pages>440--447</pages>
<contexts>
<context position="23984" citStr="Och and Ney, 2000" startWordPosition="4421" endWordPosition="4424">r Training Data Besides the training data, we also have heldout data and testing data. The held-out data includes 500 Chinese-Japanese sentence pairs, which is used to set the interpolated weights described in section 5. We use another 1,000 Chinese-Japanese sentence pairs as testing data, which is not included in the training data and the held-out data. The alignment links in the held-out data and the testing data are manually annotated. Testing data includes 4,926 alignment links2. 6.2 Evaluation Metrics We use the same metrics as described in Wu et al. (2005), which is similar to those in (Och and Ney, 2000). The difference lies in that Wu et al. (2005) took all alignment links as sure links. If we use to represent the set of alignment SG links identified by the proposed methods and SC to denote the reference alignment set, the methods to calculate the precision, recall, f-measure, and alignment error rate (AER) are shown in equations (18), (19), (20), and (21), respectively. It can be seen that the higher the f-measure is, the lower the alignment error rate is. Thus, we will only show precision, recall and AER scores in the evaluation results. 2 For a non one-to-one link, if m source words are a</context>
<context position="25266" citStr="Och and Ney, 2000" startWordPosition="4660" endWordPosition="4663">ad of m∗n alignment links. 2 |SS | G ∩ C AER = − 1 = − 1 6.3 Experimental Results We use the held-out data described in section 6.1 to set the interpolation weights in section 5. λt is set to 0.3, λn is set to 0.1, λd3 for model 3 is set to 0.5, and λd4 for model 4 is set to 0.1. With these parameters, we get the lowest alignment error rate on the held-out data. For each method described above, we perform bi-directional (source to target and target to source) word alignment and obtain two alignment results. Based on the two results, we get a result using &amp;quot;refined&amp;quot; combination as described in (Och and Ney, 2000). Thus, all of the results reported here describe the results of the &amp;quot;refined&amp;quot; combination. For model training, we use the GIZA++ toolkit3. Method Precision Recall AER Interpolated 0.6955 0.5802 0.3673 Advanced 0.7382 0.4803 0.4181 Induced Basic 0.6787 0.4602 0.4515 Induced Original 0.6026 0.4783 0.4667 Table 2. Word Alignment Results The evaluation results on the testing data are shown in table 2. From the results, it can be seen that both of the two induced models perform better than the &amp;quot;Original&amp;quot; method that only uses the limited Chinese-Japanese sentence pairs. The &amp;quot;Advanced Induced&amp;quot; meth</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved Statistical Alignment Models. In Proc. of the 38th Annual Meeting of the Association for Computational Linguistics (ACL-2000), pages 440-447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<pages>29--1</pages>
<contexts>
<context position="1554" citStr="Och and Ney, 2003" startWordPosition="247" endWordPosition="250"> corpus is available in this language pair. In addition, we build another word alignment model for L1 and L2 using the small L1-L2 bilingual corpus. Then we interpolate the above two models to further improve word alignment between L1 and L2. Experimental results indicate a relative error rate reduction of 21.30% as compared with the method only using the small bilingual corpus in L1 and L2. 1 Introduction Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993). Many researchers build alignment links with bilingual corpora (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003; Zhang and Gildea, 2005). In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training. When the large-scale bilingual corpus is unavailable, some researchers acquired class-based alignment rules with existing dictionaries to improve word alignment (Ker and Chang, 1997). Wu et al. (2005) used a large-scale bilingual corpus in general domain to improve domain-specific word alignment when only a small-scale domainspecific bilingual corpus is available. This paper proposes an approach to improve word alignment for langua</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19-51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--3</pages>
<contexts>
<context position="1535" citStr="Wu, 1997" startWordPosition="245" endWordPosition="246"> bilingual corpus is available in this language pair. In addition, we build another word alignment model for L1 and L2 using the small L1-L2 bilingual corpus. Then we interpolate the above two models to further improve word alignment between L1 and L2. Experimental results indicate a relative error rate reduction of 21.30% as compared with the method only using the small bilingual corpus in L1 and L2. 1 Introduction Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993). Many researchers build alignment links with bilingual corpora (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003; Zhang and Gildea, 2005). In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training. When the large-scale bilingual corpus is unavailable, some researchers acquired class-based alignment rules with existing dictionaries to improve word alignment (Ker and Chang, 1997). Wu et al. (2005) used a large-scale bilingual corpus in general domain to improve domain-specific word alignment when only a small-scale domainspecific bilingual corpus is available. This paper proposes an approach to improve word a</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora. Computational Linguistics, 23(3):377-403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Wu</author>
<author>Haifeng Wang</author>
<author>Zhanyi Liu</author>
</authors>
<title>Alignment Model Adaptation for Domain-Specific Word Alignment.</title>
<date>2005</date>
<booktitle>In Proc. of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL-2005),</booktitle>
<pages>467--474</pages>
<contexts>
<context position="1919" citStr="Wu et al. (2005)" startWordPosition="300" endWordPosition="303">lingual corpus in L1 and L2. 1 Introduction Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993). Many researchers build alignment links with bilingual corpora (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003; Zhang and Gildea, 2005). In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training. When the large-scale bilingual corpus is unavailable, some researchers acquired class-based alignment rules with existing dictionaries to improve word alignment (Ker and Chang, 1997). Wu et al. (2005) used a large-scale bilingual corpus in general domain to improve domain-specific word alignment when only a small-scale domainspecific bilingual corpus is available. This paper proposes an approach to improve word alignment for languages with scarce resources using bilingual corpora of other language pairs. To perform word alignment between languages L1 and L2, we introduce a third language L3 as the pivot language. Although only small amounts of bilingual data are available for the desired language pair L1-L2, large-scale bilingual corpora in L1-L3 and L2-L3 are available. Using these two ad</context>
<context position="21466" citStr="Wu et al. (2005)" startWordPosition="3964" endWordPosition="3967">apanese corpus, and PrI(a,f |c) is the induced model trained from the Chinese-English and English-Japanese corpora. λ is an interpolation weight. It can be a constant or a function of f and c . In both model 3 and model 4, there are mainly three kinds of parameters: translation probability, fertility probability and distortion probability. These three kinds of parameters have their own interpretation in these two models. In order to obtain fine-grained interpolation models, we interpolate the three kinds of parameters using different weights, which are obtained in the same way as described in Wu et al. (2005). λt represents the weights for translation probability. λn represents the weights for fertility probability. λd3 and λd4 represent the weights for distortion probability in model 3 and in model 4, respectively. λd4 is set as the interpolation weight for both the head words and the non-head words. The above four weights are obtained using a manually annotated held-out set. 6 Experiments In this section, we compare different word alignment methods for Chinese-Japanese alignment. The &amp;quot;Original&amp;quot; method uses the original model trained with the small Chinese-Japanese corpus. The &amp;quot;Basic Induced&amp;quot; met</context>
<context position="23934" citStr="Wu et al. (2005)" startWordPosition="4411" endWordPosition="4414">0,535 1,460,043 1,685,204 Table 1. Statistics for Training Data Besides the training data, we also have heldout data and testing data. The held-out data includes 500 Chinese-Japanese sentence pairs, which is used to set the interpolated weights described in section 5. We use another 1,000 Chinese-Japanese sentence pairs as testing data, which is not included in the training data and the held-out data. The alignment links in the held-out data and the testing data are manually annotated. Testing data includes 4,926 alignment links2. 6.2 Evaluation Metrics We use the same metrics as described in Wu et al. (2005), which is similar to those in (Och and Ney, 2000). The difference lies in that Wu et al. (2005) took all alignment links as sure links. If we use to represent the set of alignment SG links identified by the proposed methods and SC to denote the reference alignment set, the methods to calculate the precision, recall, f-measure, and alignment error rate (AER) are shown in equations (18), (19), (20), and (21), respectively. It can be seen that the higher the f-measure is, the lower the alignment error rate is. Thus, we will only show precision, recall and AER scores in the evaluation results. 2 </context>
</contexts>
<marker>Wu, Wang, Liu, 2005</marker>
<rawString>Hua Wu, Haifeng Wang, and Zhanyi Liu. 2005. Alignment Model Adaptation for Domain-Specific Word Alignment. In Proc. of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL-2005), pages 467-474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
</authors>
<title>Stochastic Lexicalized Inversion Transduction Grammar for Alignment.</title>
<date>2005</date>
<booktitle>In Proc. of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL-2005),</booktitle>
<pages>475--482</pages>
<contexts>
<context position="1601" citStr="Zhang and Gildea, 2005" startWordPosition="255" endWordPosition="258">r. In addition, we build another word alignment model for L1 and L2 using the small L1-L2 bilingual corpus. Then we interpolate the above two models to further improve word alignment between L1 and L2. Experimental results indicate a relative error rate reduction of 21.30% as compared with the method only using the small bilingual corpus in L1 and L2. 1 Introduction Word alignment was first proposed as an intermediate result of statistical machine translation (Brown et al., 1993). Many researchers build alignment links with bilingual corpora (Wu, 1997; Och and Ney, 2003; Cherry and Lin, 2003; Zhang and Gildea, 2005). In order to achieve satisfactory results, all of these methods require a large-scale bilingual corpus for training. When the large-scale bilingual corpus is unavailable, some researchers acquired class-based alignment rules with existing dictionaries to improve word alignment (Ker and Chang, 1997). Wu et al. (2005) used a large-scale bilingual corpus in general domain to improve domain-specific word alignment when only a small-scale domainspecific bilingual corpus is available. This paper proposes an approach to improve word alignment for languages with scarce resources using bilingual corpo</context>
</contexts>
<marker>Zhang, Gildea, 2005</marker>
<rawString>Hao Zhang and Daniel Gildea. 2005. Stochastic Lexicalized Inversion Transduction Grammar for Alignment. In Proc. of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL-2005), pages 475-482.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>