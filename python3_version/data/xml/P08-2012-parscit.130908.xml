<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.066405">
<title confidence="0.99667">
Enforcing Transitivity in Coreference Resolution
</title>
<author confidence="0.983881">
Jenny Rose Finkel and Christopher D. Manning
</author>
<affiliation confidence="0.9862465">
Department of Computer Science
Stanford University
</affiliation>
<address confidence="0.918764">
Stanford, CA 94305
</address>
<email confidence="0.997369">
{jrfinkel|manning}@cs.stanford.edu
</email>
<sectionHeader confidence="0.993833" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999935954545454">
A desirable quality of a coreference resolution
system is the ability to handle transitivity con-
straints, such that even if it places high like-
lihood on a particular mention being corefer-
ent with each of two other mentions, it will
also consider the likelihood of those two men-
tions being coreferent when making a final as-
signment. This is exactly the kind of con-
straint that integer linear programming (ILP)
is ideal for, but, surprisingly, previous work
applying ILP to coreference resolution has not
encoded this type of constraint. We train a
coreference classifier over pairs of mentions,
and show how to encode this type of constraint
on top of the probabilities output from our
pairwise classifier to extract the most probable
legal entity assignments. We present results
on two commonly used datasets which show
that enforcement of transitive closure consis-
tently improves performance, including im-
provements of up to 3.6% using the V scorer,
and up to 16.5% using cluster f-measure.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9993874">
Much recent work on coreference resolution, which
is the task of deciding which noun phrases, or men-
tions, in a document refer to the same real world
entity, builds on Soon et al. (2001). They built a
decision tree classifier to label pairs of mentions as
coreferent or not. Using their classifier, they would
build up coreference chains, where each mention
was linked up with the most recent previous men-
tion that the classifier labeled as coreferent, if such
a mention existed. Transitive closure in this model
was done implicitly. If John Smith was labeled
coreferent with Smith, and Smith with Jane Smith,
then John Smith and Jane Smith were also corefer-
ent regardless of the classifier’s evaluation of that
pair. Much work that followed improved upon this
</bodyText>
<page confidence="0.988279">
45
</page>
<bodyText confidence="0.999302342105263">
strategy, by improving the features (Ng and Cardie,
2002b), the type of classifier (Denis and Baldridge,
2007), and changing mention links to be to the most
likely antecedent rather than the most recent posi-
tively labeled antecedent (Ng and Cardie, 2002b).
This line of work has largely ignored the implicit
transitivity of the decisions made, and can result in
unintuitive chains such as the Smith chain just de-
scribed, where each pairwise decision is sensible,
but the final result is not.
Ng and Cardie (2002a) and Ng (2004) highlight
the problem of determining whether or not common
noun phrases are anaphoric. They use two clas-
sifiers, an anaphoricity classifier, which decides if
a mention should have an antecedent and a pair-
wise classifier similar those just discussed, which
are combined in a cascaded manner. More recently,
Denis and Baldridge (2007) utilized an integer lin-
ear programming (ILP) solver to better combine the
decisions made by these two complementary clas-
sifiers, by finding the globally optimal solution ac-
cording to both classifiers. However, when encoding
constraints into their ILP solver, they did not enforce
transitivity.
The goal of the present work is simply to show
that transitivity constraints are a useful source of
information, which can and should be incorporated
into an ILP-based coreference system. For this goal,
we put aside the anaphoricity classifier and focus
on the pairwise classifier and transitivity constraints.
We build a pairwise logistic classifier, trained on all
pairs of mentions, and then at test time we use an
ILP solver equipped with transitivity constraints to
find the most likely legal assignment to the variables
which represent the pairwise decisions.1 Our re-
sults show a significant improvement compared to
the naive use of the pairwise classifier.
Other work on global models of coreference (as
</bodyText>
<footnote confidence="0.860567">
1A legal assignment is one which respects transitive closure.
</footnote>
<note confidence="0.501598">
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 45–48,
</note>
<page confidence="0.505914">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.999606625">
opposed to pairwise models) has included: Luo et al.
(2004) who used a Bell tree whose leaves represent
possible partitionings of the mentions into entities
and then trained a model for searching the tree; Mc-
Callum and Wellner (2004) who defined several con-
ditional random field-based models; Ng (2005) who
took a reranking approach; and Culotta et al. (2006)
who use a probabilistic first-order logic model.
</bodyText>
<sectionHeader confidence="0.926013" genericHeader="method">
2 Coreference Resolution
</sectionHeader>
<bodyText confidence="0.9998926">
For this task we are given a document which is an-
notated with a set of mentions, and the goal is to
cluster the mentions which refer to the same entity.
When describing our model, we build upon the no-
tation used by Denis and Baldridge (2007).
</bodyText>
<subsectionHeader confidence="0.970044">
2.1 Pairwise Classification
</subsectionHeader>
<bodyText confidence="0.999972333333333">
Our baseline systems are based on a logistic classi-
fier over pairs of mentions. The probability of a pair
of mentions takes the standard logistic form:
</bodyText>
<equation confidence="0.997611">
(P(x(i,j)|mi, mj; 0) = 1 + e−f(mi,mj)·θ )−, (1)
</equation>
<bodyText confidence="0.999944142857143">
where mi and mj correspond to mentions i and 3
respectively; f(mi, mj) is a feature function over a
pair of mentions; 0 are the feature weights we wish
to learn; and x(i j) is a boolean variable which takes
value 1 if mi and mj are coreferent, and 0 if they are
not. The log likelihood of a document is the sum of
the log likelihoods of all pairs of mentions:
</bodyText>
<equation confidence="0.9706125">
L(x|m; 0) = 11 log P(x(i,j)|mi, mj; 0)
mi,mjEm2
</equation>
<bodyText confidence="0.9888655">
(2)
where m is the set of mentions in the document, and
x is the set of variables representing each pairwise
coreference decision x(i,j). Note that this model is
degenerate, because it assigns probability mass to
nonsensical clusterings. Specifically, it will allow
</bodyText>
<equation confidence="0.735869">
x(i,j) = x(j,k) = 1 while x(i,k) = 0.
</equation>
<bodyText confidence="0.999980310344828">
Prior work (Soon et al., 2001; Denis and
Baldridge, 2007) has generated training data for
pairwise classifiers in the following manner. For
each mention, work backwards through the preced-
ing mentions in the document until you come to a
true coreferent mention. Create negative examples
for all intermediate mentions, and a positive exam-
ple for the mention and its correct antecedent. This
approach made sense for Soon et al. (2001) because
testing proceeded in a similar manner: for each men-
tion, work backwards until you find a previous men-
tion which the classifier thinks is coreferent, add
a link, and terminate the search. The COREF-ILP
model of Denis and Baldridge (2007) took a dif-
ferent approach at test time: for each mention they
would work backwards and add a link for all pre-
vious mentions which the classifier deemed coref-
erent. This is equivalent to finding the most likely
assignment to each x(i,j) in Equation 2. As noted,
these assignments may not be a legal clustering be-
cause there is no guarantee of transitivity. The tran-
sitive closure happens in an ad-hoc manner after
this assignment is found: any two mentions linked
through other mentions are determined to be coref-
erent. Our SOON-STYLE baseline used the same
training and testing regimen as Soon et al. (2001).
Our D&amp;B-STYLE baseline used the same test time
method as Denis and Baldridge (2007), however at
training time we created data for all mention pairs.
</bodyText>
<subsectionHeader confidence="0.7098365">
2.2 Integer Linear Programming to Enforce
Transitivity
</subsectionHeader>
<bodyText confidence="0.999927142857143">
Because of the ad-hoc manner in which transitiv-
ity is enforced in our baseline systems, we do not
necessarily find the most probable legal clustering.
This is exactly the kind of task at which integer
linear programming excels. We need to first for-
mulate the objective function which we wish the
ILP solver to maximize at test time.2 Let p(i j) =
log P(x(i ,j)  |mi, mj; 0), which is the log probabil-
ity that mi and mj are coreferent according to the
pairwise logistic classifier discussed in the previous
section, and let p(i,j) = log(1 − p(i,j)), be the log
probability that they are not coreferent. Our objec-
tive function is then the log probability of a particu-
lar (possibly illegal) variable assignment:
</bodyText>
<equation confidence="0.9543325">
11 max p(i,j) &apos;x(i,j) − �p(i,j) &apos;(1− x(i,j)) (3)
mi,mjEm2
</equation>
<bodyText confidence="0.993692333333333">
We add binary constraints on each of the variables:
x(i,j) E 10, 11. We also add constraints, over each
triple of mentions, to enforce transitivity:
</bodyText>
<equation confidence="0.872994">
(1 − x(i,j)) + (1 − x(j,k)) &gt; (1 − x(i,k)) (4)
</equation>
<footnote confidence="0.964269">
2Note that there are no changes from the D&amp;B-STYLE base-
line system at training time.
</footnote>
<page confidence="0.999571">
46
</page>
<bodyText confidence="0.9917505">
This constraint ensures that whenever x(zj) =
x(j�k) = 1 it must also be the case that x(z�k) = 1.
</bodyText>
<sectionHeader confidence="0.999179" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999985833333333">
We used lp solve3 to solve our ILP optimization
problems. We ran experiments on two datasets. We
used the MUC-6 formal training and test data, as
well as the NWIRE and BNEWS portions of the ACE
(Phase 2) corpus. This corpus had a third portion,
NPAPER, but we found that several documents where
too long for lp solve to find a solution.4
We added named entity (NE) tags to the data us-
ing the tagger of Finkel et al. (2005). The ACE data
is already annotated with NE tags, so when they con-
flicted they overrode the tags output by the tagger.
We also added part of speech (POS) tags to the data
using the tagger of Toutanova et al. (2003), and used
the tags to decide if mentions were plural or sin-
gular. The ACE data is labeled with mention type
(pronominal, nominal, and name), but the MUC-
6 data is not, so the POS and NE tags were used
to infer this information. Our feature set was sim-
ple, and included many features from (Soon et al.,
2001), including the pronoun, string match, definite
and demonstrative NP, number and gender agree-
ment, proper name and appositive features. We had
additional features for NE tags, head matching and
head substring matching.
</bodyText>
<subsectionHeader confidence="0.992901">
3.1 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.9959486">
The MUC scorer (Vilain et al., 1995) is a popular
coreference evaluation metric, but we found it to be
fatally flawed. As observed by Luo et al. (2004),
if all mentions in each document are placed into a
single entity, the results on the MUC-6 formal test
set are 100% recall, 78.9% precision, and 88.2%
F1 score – significantly higher than any published
system. The V scorer (Amit and Baldwin, 1998)
was proposed to overcome several shortcomings of
the MUC scorer. However, coreference resolution
is a clustering task, and many cluster scorers al-
ready exist. In addition to the MUC and V scorers,
we also evaluate using cluster f-measure (Ghosh,
2003), which is the standard f-measure computed
over true/false coreference decisions for pairs of
</bodyText>
<footnote confidence="0.9998425">
3From http://lpsolve.sourceforge.net/
4Integer linear programming is, after all, NP-hard.
</footnote>
<bodyText confidence="0.998728">
mentions; the Rand index (Rand, 1971), which is
pairwise accuracy of the clustering; and variation
of information (Meila, 2003), which utilizes the en-
tropy of the clusterings and their mutual information
(and for which lower values are better).
</bodyText>
<subsectionHeader confidence="0.959989">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.999940083333333">
Our results are summarized in Table 1. We show
performance for both baseline classifiers, as well as
our ILP-based classifier, which finds the most prob-
able legal assignment to the variables representing
coreference decisions over pairs of mentions. For
comparison, we also give the results of the COREF-
ILP system of Denis and Baldridge (2007), which
was also based on a naive pairwise classifier. They
used an ILP solver to find an assignment for the vari-
ables, but as they note at the end of Section 5.1, it is
equivalent to taking all links for which the classifier
returns a probability &gt; 0.5, and so the ILP solver is
not really necessary. We also include their JOINT-
ILP numbers, however that system makes use of an
additional anaphoricity classifier.
For all three corpora, the ILP model beat both
baselines for the cluster f-score, Rand index, and
variation of information metrics. Using the V met-
ric, the ILP system and the D&amp;B-STYLE baseline
performed about the same on the MUC-6 corpus,
though for both ACE corpora, the ILP system was
the clear winner. When using the MUC scorer, the
ILP system always did worse than the D&amp;B-STYLE
baseline. However, this is precisely because the
transitivity constraints tend to yield smaller clusters
(which increase precision while decreasing recall).
Remember that going in the opposite direction and
simply putting all mentions in one cluster produces
a MUC score which is higher than any in the table,
even though this clustering is clearly not useful in
applications. Hence, we are skeptical of this mea-
sure’s utility and provide it primarily for compari-
son with previous work. The improvements from
the ILP system are most clearly shown on the ACE
NWIRE corpus, where the V f-score improved 3.6%,
and the cluster f-score improved 16.5%.
</bodyText>
<sectionHeader confidence="0.999534" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999467">
We showed how to use integer linear program-
ming to encode transitivity constraints in a corefer-
</bodyText>
<page confidence="0.998373">
47
</page>
<table confidence="0.998066611111111">
MUC SCORER V SCORER CLUSTER
MODEL P R F1 P R F1 P R F1 RAND VOI
MUC-6
D&amp;B-STYLE BASELINE 84.8 59.4 69.9 79.7 54.4 64.6 43.8 44.4 44.1 89.9 1.78
SOON-STYLE BASELINE 91.5 51.5 65.9 94.4 46.7 62.5 88.2 31.9 46.9 93.5 1.65
ILP 89.7 55.1 68.3 90.9 49.7 64.3 74.1 37.1 49.5 93.2 1.65
ACE – NWIRE
D&amp;B COREF-ILP 74.8 60.1 66.8 – – – –
D&amp;B JOINT-ILP 75.8 60.8 67.5 – – – –
D&amp;B-STYLE BASELINE 73.3 67.6 70.4 70.1 71.4 70.8 31.1 54.0 39.4 91.7 1.42
SOON-STYLE BASELINE 85.3 37.8 52.4 94.1 56.9 70.9 67.7 19.8 30.6 95.5 1.38
ILP 78.7 58.5 67.1 86.8 65.2 74.5 76.1 44.2 55.9 96.5 1.09
ACE – BNEWS
D&amp;B COREF-ILP 75.5 62.2 68.2 – – – –
D&amp;B JOINT-ILP 78.0 62.1 69.2 – – – –
D&amp;B-STYLE BASELINE 77.9 51.1 61.7 80.3 64.2 71.4 35.5 33.8 34.6 0.89 1.32
SOON-STYLE BASELINE 90.0 43.2 58.3 95.6 58.4 72.5 83.3 21.5 34.1 0.93 1.09
ILP 87.8 46.8 61.1 93.5 59.9 73.1 77.5 26.1 39.1 0.93 1.06
</table>
<tableCaption confidence="0.999969">
Table 1: Results on all three datasets with all five scoring metrics. For VOI a lower number is better.
</tableCaption>
<bodyText confidence="0.99530175">
ence classifier which models pairwise decisions over
mentions. We also demonstrated that enforcing such
constraints at test time can significantly improve per-
formance, using a variety of evaluation metrics.
</bodyText>
<sectionHeader confidence="0.99747" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999904714285714">
Thanks to the following members of the Stanford
NLP reading group for helpful discussion: Sharon
Goldwater, Michel Galley, Anna Rafferty.
This paper is based on work funded by the Dis-
ruptive Technology Office (DTO) Phase III Program
for Advanced Question Answering for Intelligence
(AQUAINT).
</bodyText>
<sectionHeader confidence="0.998611" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999874739130435">
B. Amit and B. Baldwin. 1998. Algorithms for scoring
coreference chains. In MUC7.
A. Culotta, M. Wick, and A. McCallum. 2006. First-
order probabilistic models for coreference resolution.
In NAACL.
P. Denis and J. Baldridge. 2007. Joint determination of
anaphoricity and coreference resolution using integer
programming. In HLT-NAACL, Rochester, New York.
J. Finkel, T. Grenager, and C. Manning. 2005. Incorpo-
rating non-local information into information extrac-
tion systems by Gibbs sampling. In ACL.
J. Ghosh. 2003. Scalable clustering methods for data
mining. In N. Ye, editor, Handbook of Data Mining,
chapter 10, pages 247–277. Lawrence Erlbaum Assoc.
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and
S. Roukos. 2004. A mention-synchronous corefer-
ence resolution algorithm based on the Bell tree. In
ACL.
A. McCallum and B. Wellner. 2004. Conditional models
of identity uncertainty with application to noun coref-
erence. In NIPS.
M. Meila. 2003. Comparing clusterings by the variation
of information. In COLT.
V. Ng and C. Cardie. 2002a. Identifying anaphoric and
non-anaphoric noun phrases to improve coreference
resolution. In COLING.
V. Ng and C. Cardie. 2002b. Improving machine learn-
ing approaches to coreference resolution. In ACL.
V. Ng. 2004. Learning noun phrase anaphoricity to im-
prove coreference resolution: issues in representation
and optimization. In ACL.
V. Ng. 2005. Machine learning for coreference resolu-
tion: From local classification to global ranking. In
ACL.
W. M. Rand. 1971. Objective criteria for the evaluation
of clustering methods. In Journal of the American Sta-
tistical Association, 66, pages 846–850.
W. Soon, H. Ng, and D. Lim. 2001. A machine learning
approach to coreference resolution of noun phrases. In
Computational Linguistics, 27(4).
K. Toutanova, D. Klein, and C. Manning. 2003. Feature-
rich part-of-speech tagging with a cyclic dependency
network. In HLT-NAACL 2003.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference
scoring scheme. In MUC6.
</reference>
<page confidence="0.999354">
48
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.950191">
<title confidence="0.999762">Enforcing Transitivity in Coreference Resolution</title>
<author confidence="0.999972">Jenny Rose Finkel</author>
<author confidence="0.999972">Christopher D Manning</author>
<affiliation confidence="0.999919">Department of Computer Science Stanford University</affiliation>
<address confidence="0.999809">Stanford, CA 94305</address>
<abstract confidence="0.997653391304348">A desirable quality of a coreference resolution system is the ability to handle transitivity constraints, such that even if it places high likelihood on a particular mention being coreferent with each of two other mentions, it will also consider the likelihood of those two mentions being coreferent when making a final assignment. This is exactly the kind of constraint that integer linear programming (ILP) is ideal for, but, surprisingly, previous work applying ILP to coreference resolution has not encoded this type of constraint. We train a coreference classifier over pairs of mentions, and show how to encode this type of constraint on top of the probabilities output from our pairwise classifier to extract the most probable legal entity assignments. We present results on two commonly used datasets which show that enforcement of transitive closure consistently improves performance, including imof up to 3.6% using the and up to 16.5% using cluster f-measure.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Amit</author>
<author>B Baldwin</author>
</authors>
<title>Algorithms for scoring coreference chains.</title>
<date>1998</date>
<booktitle>In MUC7.</booktitle>
<contexts>
<context position="9917" citStr="Amit and Baldwin, 1998" startWordPosition="1667" endWordPosition="1670">noun, string match, definite and demonstrative NP, number and gender agreement, proper name and appositive features. We had additional features for NE tags, head matching and head substring matching. 3.1 Evaluation Metrics The MUC scorer (Vilain et al., 1995) is a popular coreference evaluation metric, but we found it to be fatally flawed. As observed by Luo et al. (2004), if all mentions in each document are placed into a single entity, the results on the MUC-6 formal test set are 100% recall, 78.9% precision, and 88.2% F1 score – significantly higher than any published system. The V scorer (Amit and Baldwin, 1998) was proposed to overcome several shortcomings of the MUC scorer. However, coreference resolution is a clustering task, and many cluster scorers already exist. In addition to the MUC and V scorers, we also evaluate using cluster f-measure (Ghosh, 2003), which is the standard f-measure computed over true/false coreference decisions for pairs of 3From http://lpsolve.sourceforge.net/ 4Integer linear programming is, after all, NP-hard. mentions; the Rand index (Rand, 1971), which is pairwise accuracy of the clustering; and variation of information (Meila, 2003), which utilizes the entropy of the c</context>
</contexts>
<marker>Amit, Baldwin, 1998</marker>
<rawString>B. Amit and B. Baldwin. 1998. Algorithms for scoring coreference chains. In MUC7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Culotta</author>
<author>M Wick</author>
<author>A McCallum</author>
</authors>
<title>Firstorder probabilistic models for coreference resolution.</title>
<date>2006</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="4421" citStr="Culotta et al. (2006)" startWordPosition="697" endWordPosition="700">her work on global models of coreference (as 1A legal assignment is one which respects transitive closure. Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 45–48, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics opposed to pairwise models) has included: Luo et al. (2004) who used a Bell tree whose leaves represent possible partitionings of the mentions into entities and then trained a model for searching the tree; McCallum and Wellner (2004) who defined several conditional random field-based models; Ng (2005) who took a reranking approach; and Culotta et al. (2006) who use a probabilistic first-order logic model. 2 Coreference Resolution For this task we are given a document which is annotated with a set of mentions, and the goal is to cluster the mentions which refer to the same entity. When describing our model, we build upon the notation used by Denis and Baldridge (2007). 2.1 Pairwise Classification Our baseline systems are based on a logistic classifier over pairs of mentions. The probability of a pair of mentions takes the standard logistic form: (P(x(i,j)|mi, mj; 0) = 1 + e−f(mi,mj)·θ )−, (1) where mi and mj correspond to mentions i and 3 respect</context>
</contexts>
<marker>Culotta, Wick, McCallum, 2006</marker>
<rawString>A. Culotta, M. Wick, and A. McCallum. 2006. Firstorder probabilistic models for coreference resolution. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Denis</author>
<author>J Baldridge</author>
</authors>
<title>Joint determination of anaphoricity and coreference resolution using integer programming.</title>
<date>2007</date>
<booktitle>In HLT-NAACL,</booktitle>
<location>Rochester, New York.</location>
<contexts>
<context position="2089" citStr="Denis and Baldridge, 2007" startWordPosition="329" endWordPosition="332">ions as coreferent or not. Using their classifier, they would build up coreference chains, where each mention was linked up with the most recent previous mention that the classifier labeled as coreferent, if such a mention existed. Transitive closure in this model was done implicitly. If John Smith was labeled coreferent with Smith, and Smith with Jane Smith, then John Smith and Jane Smith were also coreferent regardless of the classifier’s evaluation of that pair. Much work that followed improved upon this 45 strategy, by improving the features (Ng and Cardie, 2002b), the type of classifier (Denis and Baldridge, 2007), and changing mention links to be to the most likely antecedent rather than the most recent positively labeled antecedent (Ng and Cardie, 2002b). This line of work has largely ignored the implicit transitivity of the decisions made, and can result in unintuitive chains such as the Smith chain just described, where each pairwise decision is sensible, but the final result is not. Ng and Cardie (2002a) and Ng (2004) highlight the problem of determining whether or not common noun phrases are anaphoric. They use two classifiers, an anaphoricity classifier, which decides if a mention should have an</context>
<context position="4737" citStr="Denis and Baldridge (2007)" startWordPosition="754" endWordPosition="757">al. (2004) who used a Bell tree whose leaves represent possible partitionings of the mentions into entities and then trained a model for searching the tree; McCallum and Wellner (2004) who defined several conditional random field-based models; Ng (2005) who took a reranking approach; and Culotta et al. (2006) who use a probabilistic first-order logic model. 2 Coreference Resolution For this task we are given a document which is annotated with a set of mentions, and the goal is to cluster the mentions which refer to the same entity. When describing our model, we build upon the notation used by Denis and Baldridge (2007). 2.1 Pairwise Classification Our baseline systems are based on a logistic classifier over pairs of mentions. The probability of a pair of mentions takes the standard logistic form: (P(x(i,j)|mi, mj; 0) = 1 + e−f(mi,mj)·θ )−, (1) where mi and mj correspond to mentions i and 3 respectively; f(mi, mj) is a feature function over a pair of mentions; 0 are the feature weights we wish to learn; and x(i j) is a boolean variable which takes value 1 if mi and mj are coreferent, and 0 if they are not. The log likelihood of a document is the sum of the log likelihoods of all pairs of mentions: L(x|m; 0) </context>
<context position="6355" citStr="Denis and Baldridge (2007)" startWordPosition="1035" endWordPosition="1038">, 2007) has generated training data for pairwise classifiers in the following manner. For each mention, work backwards through the preceding mentions in the document until you come to a true coreferent mention. Create negative examples for all intermediate mentions, and a positive example for the mention and its correct antecedent. This approach made sense for Soon et al. (2001) because testing proceeded in a similar manner: for each mention, work backwards until you find a previous mention which the classifier thinks is coreferent, add a link, and terminate the search. The COREF-ILP model of Denis and Baldridge (2007) took a different approach at test time: for each mention they would work backwards and add a link for all previous mentions which the classifier deemed coreferent. This is equivalent to finding the most likely assignment to each x(i,j) in Equation 2. As noted, these assignments may not be a legal clustering because there is no guarantee of transitivity. The transitive closure happens in an ad-hoc manner after this assignment is found: any two mentions linked through other mentions are determined to be coreferent. Our SOON-STYLE baseline used the same training and testing regimen as Soon et al</context>
<context position="10953" citStr="Denis and Baldridge (2007)" startWordPosition="1824" endWordPosition="1827">g is, after all, NP-hard. mentions; the Rand index (Rand, 1971), which is pairwise accuracy of the clustering; and variation of information (Meila, 2003), which utilizes the entropy of the clusterings and their mutual information (and for which lower values are better). 3.2 Results Our results are summarized in Table 1. We show performance for both baseline classifiers, as well as our ILP-based classifier, which finds the most probable legal assignment to the variables representing coreference decisions over pairs of mentions. For comparison, we also give the results of the COREFILP system of Denis and Baldridge (2007), which was also based on a naive pairwise classifier. They used an ILP solver to find an assignment for the variables, but as they note at the end of Section 5.1, it is equivalent to taking all links for which the classifier returns a probability &gt; 0.5, and so the ILP solver is not really necessary. We also include their JOINTILP numbers, however that system makes use of an additional anaphoricity classifier. For all three corpora, the ILP model beat both baselines for the cluster f-score, Rand index, and variation of information metrics. Using the V metric, the ILP system and the D&amp;B-STYLE b</context>
</contexts>
<marker>Denis, Baldridge, 2007</marker>
<rawString>P. Denis and J. Baldridge. 2007. Joint determination of anaphoricity and coreference resolution using integer programming. In HLT-NAACL, Rochester, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Finkel</author>
<author>T Grenager</author>
<author>C Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by Gibbs sampling.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="8754" citStr="Finkel et al. (2005)" startWordPosition="1461" endWordPosition="1464">te that there are no changes from the D&amp;B-STYLE baseline system at training time. 46 This constraint ensures that whenever x(zj) = x(j�k) = 1 it must also be the case that x(z�k) = 1. 3 Experiments We used lp solve3 to solve our ILP optimization problems. We ran experiments on two datasets. We used the MUC-6 formal training and test data, as well as the NWIRE and BNEWS portions of the ACE (Phase 2) corpus. This corpus had a third portion, NPAPER, but we found that several documents where too long for lp solve to find a solution.4 We added named entity (NE) tags to the data using the tagger of Finkel et al. (2005). The ACE data is already annotated with NE tags, so when they conflicted they overrode the tags output by the tagger. We also added part of speech (POS) tags to the data using the tagger of Toutanova et al. (2003), and used the tags to decide if mentions were plural or singular. The ACE data is labeled with mention type (pronominal, nominal, and name), but the MUC6 data is not, so the POS and NE tags were used to infer this information. Our feature set was simple, and included many features from (Soon et al., 2001), including the pronoun, string match, definite and demonstrative NP, number an</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>J. Finkel, T. Grenager, and C. Manning. 2005. Incorporating non-local information into information extraction systems by Gibbs sampling. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ghosh</author>
</authors>
<title>Scalable clustering methods for data mining.</title>
<date>2003</date>
<booktitle>Handbook of Data Mining, chapter 10,</booktitle>
<pages>247--277</pages>
<editor>In N. Ye, editor,</editor>
<contexts>
<context position="10169" citStr="Ghosh, 2003" startWordPosition="1709" endWordPosition="1710">a popular coreference evaluation metric, but we found it to be fatally flawed. As observed by Luo et al. (2004), if all mentions in each document are placed into a single entity, the results on the MUC-6 formal test set are 100% recall, 78.9% precision, and 88.2% F1 score – significantly higher than any published system. The V scorer (Amit and Baldwin, 1998) was proposed to overcome several shortcomings of the MUC scorer. However, coreference resolution is a clustering task, and many cluster scorers already exist. In addition to the MUC and V scorers, we also evaluate using cluster f-measure (Ghosh, 2003), which is the standard f-measure computed over true/false coreference decisions for pairs of 3From http://lpsolve.sourceforge.net/ 4Integer linear programming is, after all, NP-hard. mentions; the Rand index (Rand, 1971), which is pairwise accuracy of the clustering; and variation of information (Meila, 2003), which utilizes the entropy of the clusterings and their mutual information (and for which lower values are better). 3.2 Results Our results are summarized in Table 1. We show performance for both baseline classifiers, as well as our ILP-based classifier, which finds the most probable le</context>
</contexts>
<marker>Ghosh, 2003</marker>
<rawString>J. Ghosh. 2003. Scalable clustering methods for data mining. In N. Ye, editor, Handbook of Data Mining, chapter 10, pages 247–277. Lawrence Erlbaum Assoc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Luo</author>
<author>A Ittycheriah</author>
<author>H Jing</author>
<author>N Kambhatla</author>
<author>S Roukos</author>
</authors>
<title>A mention-synchronous coreference resolution algorithm based on the Bell tree.</title>
<date>2004</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="4121" citStr="Luo et al. (2004)" startWordPosition="648" endWordPosition="651">rs of mentions, and then at test time we use an ILP solver equipped with transitivity constraints to find the most likely legal assignment to the variables which represent the pairwise decisions.1 Our results show a significant improvement compared to the naive use of the pairwise classifier. Other work on global models of coreference (as 1A legal assignment is one which respects transitive closure. Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 45–48, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics opposed to pairwise models) has included: Luo et al. (2004) who used a Bell tree whose leaves represent possible partitionings of the mentions into entities and then trained a model for searching the tree; McCallum and Wellner (2004) who defined several conditional random field-based models; Ng (2005) who took a reranking approach; and Culotta et al. (2006) who use a probabilistic first-order logic model. 2 Coreference Resolution For this task we are given a document which is annotated with a set of mentions, and the goal is to cluster the mentions which refer to the same entity. When describing our model, we build upon the notation used by Denis and </context>
<context position="9668" citStr="Luo et al. (2004)" startWordPosition="1624" endWordPosition="1627">labeled with mention type (pronominal, nominal, and name), but the MUC6 data is not, so the POS and NE tags were used to infer this information. Our feature set was simple, and included many features from (Soon et al., 2001), including the pronoun, string match, definite and demonstrative NP, number and gender agreement, proper name and appositive features. We had additional features for NE tags, head matching and head substring matching. 3.1 Evaluation Metrics The MUC scorer (Vilain et al., 1995) is a popular coreference evaluation metric, but we found it to be fatally flawed. As observed by Luo et al. (2004), if all mentions in each document are placed into a single entity, the results on the MUC-6 formal test set are 100% recall, 78.9% precision, and 88.2% F1 score – significantly higher than any published system. The V scorer (Amit and Baldwin, 1998) was proposed to overcome several shortcomings of the MUC scorer. However, coreference resolution is a clustering task, and many cluster scorers already exist. In addition to the MUC and V scorers, we also evaluate using cluster f-measure (Ghosh, 2003), which is the standard f-measure computed over true/false coreference decisions for pairs of 3From</context>
</contexts>
<marker>Luo, Ittycheriah, Jing, Kambhatla, Roukos, 2004</marker>
<rawString>X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and S. Roukos. 2004. A mention-synchronous coreference resolution algorithm based on the Bell tree. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>B Wellner</author>
</authors>
<title>Conditional models of identity uncertainty with application to noun coreference.</title>
<date>2004</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="4295" citStr="McCallum and Wellner (2004)" startWordPosition="676" endWordPosition="680">present the pairwise decisions.1 Our results show a significant improvement compared to the naive use of the pairwise classifier. Other work on global models of coreference (as 1A legal assignment is one which respects transitive closure. Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 45–48, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics opposed to pairwise models) has included: Luo et al. (2004) who used a Bell tree whose leaves represent possible partitionings of the mentions into entities and then trained a model for searching the tree; McCallum and Wellner (2004) who defined several conditional random field-based models; Ng (2005) who took a reranking approach; and Culotta et al. (2006) who use a probabilistic first-order logic model. 2 Coreference Resolution For this task we are given a document which is annotated with a set of mentions, and the goal is to cluster the mentions which refer to the same entity. When describing our model, we build upon the notation used by Denis and Baldridge (2007). 2.1 Pairwise Classification Our baseline systems are based on a logistic classifier over pairs of mentions. The probability of a pair of mentions takes the </context>
</contexts>
<marker>McCallum, Wellner, 2004</marker>
<rawString>A. McCallum and B. Wellner. 2004. Conditional models of identity uncertainty with application to noun coreference. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Meila</author>
</authors>
<title>Comparing clusterings by the variation of information.</title>
<date>2003</date>
<booktitle>In COLT.</booktitle>
<contexts>
<context position="10480" citStr="Meila, 2003" startWordPosition="1750" endWordPosition="1751">hed system. The V scorer (Amit and Baldwin, 1998) was proposed to overcome several shortcomings of the MUC scorer. However, coreference resolution is a clustering task, and many cluster scorers already exist. In addition to the MUC and V scorers, we also evaluate using cluster f-measure (Ghosh, 2003), which is the standard f-measure computed over true/false coreference decisions for pairs of 3From http://lpsolve.sourceforge.net/ 4Integer linear programming is, after all, NP-hard. mentions; the Rand index (Rand, 1971), which is pairwise accuracy of the clustering; and variation of information (Meila, 2003), which utilizes the entropy of the clusterings and their mutual information (and for which lower values are better). 3.2 Results Our results are summarized in Table 1. We show performance for both baseline classifiers, as well as our ILP-based classifier, which finds the most probable legal assignment to the variables representing coreference decisions over pairs of mentions. For comparison, we also give the results of the COREFILP system of Denis and Baldridge (2007), which was also based on a naive pairwise classifier. They used an ILP solver to find an assignment for the variables, but as </context>
</contexts>
<marker>Meila, 2003</marker>
<rawString>M. Meila. 2003. Comparing clusterings by the variation of information. In COLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
<author>C Cardie</author>
</authors>
<title>Identifying anaphoric and non-anaphoric noun phrases to improve coreference resolution.</title>
<date>2002</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="2035" citStr="Ng and Cardie, 2002" startWordPosition="321" endWordPosition="324">decision tree classifier to label pairs of mentions as coreferent or not. Using their classifier, they would build up coreference chains, where each mention was linked up with the most recent previous mention that the classifier labeled as coreferent, if such a mention existed. Transitive closure in this model was done implicitly. If John Smith was labeled coreferent with Smith, and Smith with Jane Smith, then John Smith and Jane Smith were also coreferent regardless of the classifier’s evaluation of that pair. Much work that followed improved upon this 45 strategy, by improving the features (Ng and Cardie, 2002b), the type of classifier (Denis and Baldridge, 2007), and changing mention links to be to the most likely antecedent rather than the most recent positively labeled antecedent (Ng and Cardie, 2002b). This line of work has largely ignored the implicit transitivity of the decisions made, and can result in unintuitive chains such as the Smith chain just described, where each pairwise decision is sensible, but the final result is not. Ng and Cardie (2002a) and Ng (2004) highlight the problem of determining whether or not common noun phrases are anaphoric. They use two classifiers, an anaphoricity</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>V. Ng and C. Cardie. 2002a. Identifying anaphoric and non-anaphoric noun phrases to improve coreference resolution. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
<author>C Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2035" citStr="Ng and Cardie, 2002" startWordPosition="321" endWordPosition="324">decision tree classifier to label pairs of mentions as coreferent or not. Using their classifier, they would build up coreference chains, where each mention was linked up with the most recent previous mention that the classifier labeled as coreferent, if such a mention existed. Transitive closure in this model was done implicitly. If John Smith was labeled coreferent with Smith, and Smith with Jane Smith, then John Smith and Jane Smith were also coreferent regardless of the classifier’s evaluation of that pair. Much work that followed improved upon this 45 strategy, by improving the features (Ng and Cardie, 2002b), the type of classifier (Denis and Baldridge, 2007), and changing mention links to be to the most likely antecedent rather than the most recent positively labeled antecedent (Ng and Cardie, 2002b). This line of work has largely ignored the implicit transitivity of the decisions made, and can result in unintuitive chains such as the Smith chain just described, where each pairwise decision is sensible, but the final result is not. Ng and Cardie (2002a) and Ng (2004) highlight the problem of determining whether or not common noun phrases are anaphoric. They use two classifiers, an anaphoricity</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>V. Ng and C. Cardie. 2002b. Improving machine learning approaches to coreference resolution. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
</authors>
<title>Learning noun phrase anaphoricity to improve coreference resolution: issues in representation and optimization.</title>
<date>2004</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2506" citStr="Ng (2004)" startWordPosition="402" endWordPosition="403">assifier’s evaluation of that pair. Much work that followed improved upon this 45 strategy, by improving the features (Ng and Cardie, 2002b), the type of classifier (Denis and Baldridge, 2007), and changing mention links to be to the most likely antecedent rather than the most recent positively labeled antecedent (Ng and Cardie, 2002b). This line of work has largely ignored the implicit transitivity of the decisions made, and can result in unintuitive chains such as the Smith chain just described, where each pairwise decision is sensible, but the final result is not. Ng and Cardie (2002a) and Ng (2004) highlight the problem of determining whether or not common noun phrases are anaphoric. They use two classifiers, an anaphoricity classifier, which decides if a mention should have an antecedent and a pairwise classifier similar those just discussed, which are combined in a cascaded manner. More recently, Denis and Baldridge (2007) utilized an integer linear programming (ILP) solver to better combine the decisions made by these two complementary classifiers, by finding the globally optimal solution according to both classifiers. However, when encoding constraints into their ILP solver, they di</context>
</contexts>
<marker>Ng, 2004</marker>
<rawString>V. Ng. 2004. Learning noun phrase anaphoricity to improve coreference resolution: issues in representation and optimization. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
</authors>
<title>Machine learning for coreference resolution: From local classification to global ranking.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="4364" citStr="Ng (2005)" startWordPosition="689" endWordPosition="690"> the naive use of the pairwise classifier. Other work on global models of coreference (as 1A legal assignment is one which respects transitive closure. Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 45–48, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics opposed to pairwise models) has included: Luo et al. (2004) who used a Bell tree whose leaves represent possible partitionings of the mentions into entities and then trained a model for searching the tree; McCallum and Wellner (2004) who defined several conditional random field-based models; Ng (2005) who took a reranking approach; and Culotta et al. (2006) who use a probabilistic first-order logic model. 2 Coreference Resolution For this task we are given a document which is annotated with a set of mentions, and the goal is to cluster the mentions which refer to the same entity. When describing our model, we build upon the notation used by Denis and Baldridge (2007). 2.1 Pairwise Classification Our baseline systems are based on a logistic classifier over pairs of mentions. The probability of a pair of mentions takes the standard logistic form: (P(x(i,j)|mi, mj; 0) = 1 + e−f(mi,mj)·θ )−, (</context>
</contexts>
<marker>Ng, 2005</marker>
<rawString>V. Ng. 2005. Machine learning for coreference resolution: From local classification to global ranking. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W M Rand</author>
</authors>
<title>Objective criteria for the evaluation of clustering methods.</title>
<date>1971</date>
<journal>In Journal of the American Statistical Association,</journal>
<volume>66</volume>
<pages>846--850</pages>
<contexts>
<context position="10390" citStr="Rand, 1971" startWordPosition="1737" endWordPosition="1738">e 100% recall, 78.9% precision, and 88.2% F1 score – significantly higher than any published system. The V scorer (Amit and Baldwin, 1998) was proposed to overcome several shortcomings of the MUC scorer. However, coreference resolution is a clustering task, and many cluster scorers already exist. In addition to the MUC and V scorers, we also evaluate using cluster f-measure (Ghosh, 2003), which is the standard f-measure computed over true/false coreference decisions for pairs of 3From http://lpsolve.sourceforge.net/ 4Integer linear programming is, after all, NP-hard. mentions; the Rand index (Rand, 1971), which is pairwise accuracy of the clustering; and variation of information (Meila, 2003), which utilizes the entropy of the clusterings and their mutual information (and for which lower values are better). 3.2 Results Our results are summarized in Table 1. We show performance for both baseline classifiers, as well as our ILP-based classifier, which finds the most probable legal assignment to the variables representing coreference decisions over pairs of mentions. For comparison, we also give the results of the COREFILP system of Denis and Baldridge (2007), which was also based on a naive pai</context>
</contexts>
<marker>Rand, 1971</marker>
<rawString>W. M. Rand. 1971. Objective criteria for the evaluation of clustering methods. In Journal of the American Statistical Association, 66, pages 846–850.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Soon</author>
<author>H Ng</author>
<author>D Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>In Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="1401" citStr="Soon et al. (2001)" startWordPosition="217" endWordPosition="220">pairs of mentions, and show how to encode this type of constraint on top of the probabilities output from our pairwise classifier to extract the most probable legal entity assignments. We present results on two commonly used datasets which show that enforcement of transitive closure consistently improves performance, including improvements of up to 3.6% using the V scorer, and up to 16.5% using cluster f-measure. 1 Introduction Much recent work on coreference resolution, which is the task of deciding which noun phrases, or mentions, in a document refer to the same real world entity, builds on Soon et al. (2001). They built a decision tree classifier to label pairs of mentions as coreferent or not. Using their classifier, they would build up coreference chains, where each mention was linked up with the most recent previous mention that the classifier labeled as coreferent, if such a mention existed. Transitive closure in this model was done implicitly. If John Smith was labeled coreferent with Smith, and Smith with Jane Smith, then John Smith and Jane Smith were also coreferent regardless of the classifier’s evaluation of that pair. Much work that followed improved upon this 45 strategy, by improving</context>
<context position="5708" citStr="Soon et al., 2001" startWordPosition="930" endWordPosition="933">the feature weights we wish to learn; and x(i j) is a boolean variable which takes value 1 if mi and mj are coreferent, and 0 if they are not. The log likelihood of a document is the sum of the log likelihoods of all pairs of mentions: L(x|m; 0) = 11 log P(x(i,j)|mi, mj; 0) mi,mjEm2 (2) where m is the set of mentions in the document, and x is the set of variables representing each pairwise coreference decision x(i,j). Note that this model is degenerate, because it assigns probability mass to nonsensical clusterings. Specifically, it will allow x(i,j) = x(j,k) = 1 while x(i,k) = 0. Prior work (Soon et al., 2001; Denis and Baldridge, 2007) has generated training data for pairwise classifiers in the following manner. For each mention, work backwards through the preceding mentions in the document until you come to a true coreferent mention. Create negative examples for all intermediate mentions, and a positive example for the mention and its correct antecedent. This approach made sense for Soon et al. (2001) because testing proceeded in a similar manner: for each mention, work backwards until you find a previous mention which the classifier thinks is coreferent, add a link, and terminate the search. Th</context>
<context position="6963" citStr="Soon et al. (2001)" startWordPosition="1140" endWordPosition="1143">dge (2007) took a different approach at test time: for each mention they would work backwards and add a link for all previous mentions which the classifier deemed coreferent. This is equivalent to finding the most likely assignment to each x(i,j) in Equation 2. As noted, these assignments may not be a legal clustering because there is no guarantee of transitivity. The transitive closure happens in an ad-hoc manner after this assignment is found: any two mentions linked through other mentions are determined to be coreferent. Our SOON-STYLE baseline used the same training and testing regimen as Soon et al. (2001). Our D&amp;B-STYLE baseline used the same test time method as Denis and Baldridge (2007), however at training time we created data for all mention pairs. 2.2 Integer Linear Programming to Enforce Transitivity Because of the ad-hoc manner in which transitivity is enforced in our baseline systems, we do not necessarily find the most probable legal clustering. This is exactly the kind of task at which integer linear programming excels. We need to first formulate the objective function which we wish the ILP solver to maximize at test time.2 Let p(i j) = log P(x(i ,j) |mi, mj; 0), which is the log pro</context>
<context position="9275" citStr="Soon et al., 2001" startWordPosition="1561" endWordPosition="1564">a solution.4 We added named entity (NE) tags to the data using the tagger of Finkel et al. (2005). The ACE data is already annotated with NE tags, so when they conflicted they overrode the tags output by the tagger. We also added part of speech (POS) tags to the data using the tagger of Toutanova et al. (2003), and used the tags to decide if mentions were plural or singular. The ACE data is labeled with mention type (pronominal, nominal, and name), but the MUC6 data is not, so the POS and NE tags were used to infer this information. Our feature set was simple, and included many features from (Soon et al., 2001), including the pronoun, string match, definite and demonstrative NP, number and gender agreement, proper name and appositive features. We had additional features for NE tags, head matching and head substring matching. 3.1 Evaluation Metrics The MUC scorer (Vilain et al., 1995) is a popular coreference evaluation metric, but we found it to be fatally flawed. As observed by Luo et al. (2004), if all mentions in each document are placed into a single entity, the results on the MUC-6 formal test set are 100% recall, 78.9% precision, and 88.2% F1 score – significantly higher than any published sys</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>W. Soon, H. Ng, and D. Lim. 2001. A machine learning approach to coreference resolution of noun phrases. In Computational Linguistics, 27(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>Featurerich part-of-speech tagging with a cyclic dependency network. In</title>
<date>2003</date>
<booktitle>HLT-NAACL</booktitle>
<contexts>
<context position="8968" citStr="Toutanova et al. (2003)" startWordPosition="1502" endWordPosition="1505">lve3 to solve our ILP optimization problems. We ran experiments on two datasets. We used the MUC-6 formal training and test data, as well as the NWIRE and BNEWS portions of the ACE (Phase 2) corpus. This corpus had a third portion, NPAPER, but we found that several documents where too long for lp solve to find a solution.4 We added named entity (NE) tags to the data using the tagger of Finkel et al. (2005). The ACE data is already annotated with NE tags, so when they conflicted they overrode the tags output by the tagger. We also added part of speech (POS) tags to the data using the tagger of Toutanova et al. (2003), and used the tags to decide if mentions were plural or singular. The ACE data is labeled with mention type (pronominal, nominal, and name), but the MUC6 data is not, so the POS and NE tags were used to infer this information. Our feature set was simple, and included many features from (Soon et al., 2001), including the pronoun, string match, definite and demonstrative NP, number and gender agreement, proper name and appositive features. We had additional features for NE tags, head matching and head substring matching. 3.1 Evaluation Metrics The MUC scorer (Vilain et al., 1995) is a popular c</context>
</contexts>
<marker>Toutanova, Klein, Manning, 2003</marker>
<rawString>K. Toutanova, D. Klein, and C. Manning. 2003. Featurerich part-of-speech tagging with a cyclic dependency network. In HLT-NAACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Vilain</author>
<author>J Burger</author>
<author>J Aberdeen</author>
<author>D Connolly</author>
<author>L Hirschman</author>
</authors>
<title>A model-theoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In MUC6.</booktitle>
<contexts>
<context position="9553" citStr="Vilain et al., 1995" startWordPosition="1603" endWordPosition="1606">e tagger of Toutanova et al. (2003), and used the tags to decide if mentions were plural or singular. The ACE data is labeled with mention type (pronominal, nominal, and name), but the MUC6 data is not, so the POS and NE tags were used to infer this information. Our feature set was simple, and included many features from (Soon et al., 2001), including the pronoun, string match, definite and demonstrative NP, number and gender agreement, proper name and appositive features. We had additional features for NE tags, head matching and head substring matching. 3.1 Evaluation Metrics The MUC scorer (Vilain et al., 1995) is a popular coreference evaluation metric, but we found it to be fatally flawed. As observed by Luo et al. (2004), if all mentions in each document are placed into a single entity, the results on the MUC-6 formal test set are 100% recall, 78.9% precision, and 88.2% F1 score – significantly higher than any published system. The V scorer (Amit and Baldwin, 1998) was proposed to overcome several shortcomings of the MUC scorer. However, coreference resolution is a clustering task, and many cluster scorers already exist. In addition to the MUC and V scorers, we also evaluate using cluster f-measu</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and L. Hirschman. 1995. A model-theoretic coreference scoring scheme. In MUC6.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>