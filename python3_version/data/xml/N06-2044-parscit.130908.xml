<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.047695">
<title confidence="0.986294">
Evolving optimal inspectable strategies for spoken dialogue systems
</title>
<author confidence="0.99837">
Dave Toney
</author>
<affiliation confidence="0.998847">
School of Informatics
Edinburgh University
</affiliation>
<address confidence="0.817062">
2 Buccleuch Place
Edinburgh EH8 9LW
</address>
<email confidence="0.99729">
dave@cstr.ed.ac.uk
</email>
<author confidence="0.991042">
Johanna Moore
</author>
<affiliation confidence="0.998678">
School of Informatics
Edinburgh University
</affiliation>
<address confidence="0.8171445">
2 Buccleuch Place
Edinburgh EH8 9LW
</address>
<email confidence="0.997578">
jmoore@inf.ed.ac.uk
</email>
<author confidence="0.995676">
Oliver Lemon
</author>
<affiliation confidence="0.998793">
School of Informatics
Edinburgh University
</affiliation>
<address confidence="0.8172235">
2 Buccleuch Place
Edinburgh EH8 9LW
</address>
<email confidence="0.997373">
olemon@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.995616" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99989375">
We report on a novel approach to gener-
ating strategies for spoken dialogue sys-
tems. We present a series of experiments
that illustrate how an evolutionary rein-
forcement learning algorithm can produce
strategies that are both optimal and easily
inspectable by human developers. Our ex-
perimental strategies achieve a mean per-
formance of 98.9% with respect to a pre-
defined evaluation metric. Our approach
also produces a dramatic reduction in
strategy size when compared with conven-
tional reinforcement learning techniques
(87% in one experiment). We conclude
that this algorithm can be used to evolve
optimal inspectable dialogue strategies.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999928893617021">
Developing a dialogue management strategy for a
spoken dialogue system is often a complex and time-
consuming task. This is because the number of
unique conversations that can occur between a user
and the system is almost unlimited. Consequently,
a system developer may spend a lot of time antic-
ipating how potential users might interact with the
system before deciding on the appropriate system re-
sponse.
Recent research has focused on generating dia-
logue strategies automatically. This work is based
on modelling dialogue as a markov decision process,
formalised by a finite state space S, a finite action
set A, a set of transition probabilities T and a re-
ward function R. Using this model an optimal dia-
logue strategy 7r* is represented by a mapping be-
tween the state space and the action set. That is, for
each state s E 5 this mapping defines its optimal ac-
tion a*�. How is this mapping constructed? Previous
approaches have employed reinforcement learning
(RL) algorithms to estimate an optimal value func-
tion Q* (Levin et al., 2000; Frampton and Lemon,
2005). For each state this function predicts the fu-
ture reward associated with each action available in
that state. This function makes it easy to extract the
optimal strategy (policy in the RL literature).
Progress has been made with this approach but
some important challenges remain. For instance,
very little success has been achieved with the large
state spaces that are typical of real-life systems.
Similarly, work on summarising learned strategies
for interpretation by human developers has so far
only been applied to tasks where each state-action
pair is explicitly represented (Lecœuche, 2001).
This tabular representation severely limits the size
of the state space.
We propose an alternative approach to finding op-
timal dialogue policies. We make use of XCS, an
evolutionary reinforcement learning algorithm that
seeks to represent a policy as a compact set of state-
action rules (Wilson, 1995). We suggest that this al-
gorithm could overcome both the challenge of large
state spaces and the desire for strategy inspectability.
In this paper, we focus on the issue of inspectabil-
ity. We present a series of experiments that illustrate
how XCS can be used to evolve dialogue strategies
that are both optimal and easily inspectable.
</bodyText>
<page confidence="0.983233">
173
</page>
<note confidence="0.944016">
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 173–176,
New York, June 2006. c�2006 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.599729" genericHeader="method">
2 Learning Classifier Systems and XCS
</sectionHeader>
<bodyText confidence="0.999675324324324">
Learning Classifier Systems were introduced by
John Holland in the 1970s as a framework for learn-
ing rule-based knowledge representations (Holland,
1976). In this model, a rule base consists of a popu-
lation of N state-action rules known as classifiers.
The state part of a classifier is represented by a
ternary string from the set {0,1,#} while the action
part is composed from {0,1}. The # symbol acts as
a wildcard allowing a classifier to aggregate states;
for example, the state string 1#1 matches the states
111 and 101. Classifier systems have been applied
to a number of learning tasks, including data mining,
optimisation and control (Bull, 2004).
Classifier systems combine two machine learning
techniques to find the optimal rule set. A genetic
algorithm is used to evaluate and modify the popu-
lation of rules while reinforcement learning is used
to assign rewards to existing rules. The search for
better rules is guided by the strength parameter as-
sociated with each classifier. This parameter serves
as a fitness score for the genetic algorithm and as a
predictor of future reward (payoff) for the RL algo-
rithm. This evolutionary learning process searches
the space of possible rule sets to find an optimal pol-
icy as defined by the reward function.
XCS (X Classifier System) incorporates a num-
ber of modifications to Holland’s original frame-
work (Wilson, 1995). In this system, a classifier’s
fitness is based on the accuracy of its payoff predic-
tion instead of the prediction itself. Furthermore, the
genetic algorithm operates on actions instead of the
population as a whole. These aspects of XCS result
in a more complete map of the state-action space
than would be the case with strength-based classi-
fier systems. Consequently, XCS often outperforms
strength-based systems in sequential decision prob-
lems (Kovacs, 2000).
</bodyText>
<sectionHeader confidence="0.998077" genericHeader="method">
3 Experimental Methodology
</sectionHeader>
<bodyText confidence="0.999808254545455">
In this section we present a simple slot-filling sys-
tem based on the hotel booking domain. The goal of
the system is to acquire the values for three slots: the
check-in date, the number of nights the user wishes
to stay and the type of room required (single, twin
etc.). In slot-filling dialogues, an optimal strategy is
one that interacts with the user in a satisfactory way
while trying to minimise the length of the dialogue.
A fundamental component of user satisfaction is the
system’s prevention and repair of any miscommuni-
cation between it and the user. Consequently, our
hotel booking system focuses on evolving essential
slot confirmation strategies.
We devised an experimental framework for mod-
elling the hotel system as a sequential decision task
and used XCS to evolve three behaviours. Firstly,
the system should execute its dialogue acts in a log-
ical sequence. In other words, the system should
greet the user, ask for the slot information, present
the query results and then finish the dialogue, in that
order (Experiment 1). Secondly, the system should
try to acquire the slot values as quickly as possible
while taking account of the possibility of misrecog-
nition (Experiments 2a and 2b). Thirdly, to increase
the likelihood of acquiring the slot values correctly,
each one should be confirmed at least once (Experi-
ments 3 and 4).
The reward function for Experiments 1, 2a and
2b was the same. During a dialogue, each non-
terminal system action received a reward value of
zero. At the end of each dialogue, the final reward
comprised three parts: (i) -1000 for each system
turn; (ii) 100,000 if all slots were filled; (iii) 100,000
if the first system act was a greeting. In Experiments
3 and 4, an additional reward of 100,000 was as-
signed if all slots were confirmed.
The transition probabilities were modelled using
two versions of a handcoded simulated user. A very
large number of test dialogues are usually required
for learning optimal dialogue strategies; simulated
users are a practical alternative to employing human
test users (Scheffler and Young, 2000; Lopez-Cozar
et al., 2002). Simulated user A represented a fully
cooperative user, always giving the slot information
that was asked. User B was less cooperative, giving
no response 20% of the time. This allowed us to
perform a two-fold cross validation of the evolved
strategies.
For each experiment we allowed the system’s
strategy to evolve over 100,000 dialogues with each
simulated user. Dialogues were limited to a maxi-
mum of 30 system turns. We then tested each strat-
egy with a further 10,000 dialogues. We logged the
total reward (payoff) for each test dialogue. Each
experiment was repeated ten times.
</bodyText>
<page confidence="0.996519">
174
</page>
<bodyText confidence="0.9993294375">
In each experiment, the presentation of the query
results and closure of the dialogue were combined
into a single dialogue act. Therefore, the dialogue
acts available to the system for the first experi-
ment were: Greeting, Query+Goodbye, Ask(Date),
Ask(Duration) and Ask(RoomType). Four boolean
variables were used to represent the state of the di-
alogue: GreetingFirst, DateFilled, DurationFilled,
RoomFilled.
Experiment 2 added a new dialogue act: Ask(All).
The goal here was to ask for all three slot values
if the probability of getting the slot values was rea-
sonably high. If the probability was low, the sys-
tem should ask for the slots one at a time as be-
fore. This information was modelled in the sim-
ulated users by 2 variables: Prob1SlotCorrect and
Prob3SlotsCorrect. The values for these variables
in Experiments 2a and 2b respectively were: 0.9 and
0.729 (=0.93); 0.5 and 0.125 (=0.53).
Experiment 3 added three new dialogue acts: Ex-
plicit Confirm(Date), Explicit Confirm(Duration),
Explicit Confirm(RoomType) and three new state
variables: DateConfirmed, DurationConfirmed,
RoomConfirmed. The goal here was for the sys-
tem to learn to confirm each of the slot val-
ues after the user has first given them. Experi-
ment 4 sought to reduce the dialogue length fur-
ther by allowing the system to confirm one slot
value while asking for another. Two new di-
alogue acts were available in this last experi-
ment: Implicit Confirm(Date)+Ask(Duration) and
Implicit Confirm(Duration)+Ask(RoomType).
</bodyText>
<sectionHeader confidence="0.997213" genericHeader="evaluation">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.997721307692308">
Table 1 lists the total reward (payoff) averaged over
the 10 cross-validated test trials for each experiment,
expressed as a percentage of the maximum payoff.
In these experiments, the maximum payoff repre-
sents the shortest possible successful dialogue. For
example, the maximum payoff for Experiment 1 is
195,000: 100,000 for filling the slots plus 100,000
for greeting the user at the start of the dialogue mi-
nus 5000 for the minimum number of turns (five)
taken to complete the dialogue successfully. The av-
erage payoff for the 10 trials trained on simulated
user A and tested on user B was 193,877 – approxi-
mately 99.4% of the maximum possible. In light of
</bodyText>
<table confidence="0.999750181818182">
Exp. Training/Test Users Payoff (%)
1 A, B 99.4
B, A 99.8
2a A, B 99.1
B, A 99.4
2b A, B 96.8
B, A 97.2
3 A, B 98.8
B, A 99.3
4 A, B 99.3
B, A 99.7
</table>
<tableCaption confidence="0.999867">
Table 1: Payoff results for the evolved strategies.
</tableCaption>
<bodyText confidence="0.999678642857143">
these results and the stochastic user responses, we
suggest that these evolved strategies would compare
favourably with any handcoded strategies.
It is instructive to compare the rate of convergence
for different strategies. Figure 1 shows the average
payoff for the 100,000 dialogues trained with sim-
ulated user A in Experiments 3 and 4. It shows
that Experiment 3 approached the optimal policy
after approximately 20,000 dialogues whereas Ex-
periment 4 converged after approximately 5000 dia-
logues. This is encouraging because it suggests that
XCS remains focused on finding the shortest suc-
cessful dialogue even when the number of available
actions increases.
</bodyText>
<figure confidence="0.402241">
Dialogues
</figure>
<figureCaption confidence="0.943703">
Figure 1: Convergence towards optimality during
training in Experiments 3 and 4 (simulated user A).
</figureCaption>
<bodyText confidence="0.99073325">
Finally, we look at how to represent an optimal
strategy. From the logs of the test dialogues we ex-
tracted the state-action rules (classifiers) that were
executed. For example, in Experiment 4, the op-
</bodyText>
<figure confidence="0.989443090909091">
x 105
3
2.5
2
Exp. 3
Exp. 4
1
0.5
00 25,000 50,000 75,000 100,000
Average Payoff
1.5
</figure>
<page confidence="0.95763">
175
</page>
<table confidence="0.988046571428571">
State Action
0 0 # # # # # Greeting
1 0 0 0 # # # Ask(Date)
1 1 # # 0 # # Implicit Confirm(Date) + Ask(Duration)
1 1 1 # 1 0 0 Implicit Confirm(Duration) + Ask(RoomType)
1 1 1 1 1 1 0 Explicit Confirm(RoomType)
1 1 1 1 1 1 1 Query + Goodbye
</table>
<tableCaption confidence="0.999211">
Table 2: A summary of the optimal strategy for Experiment 4.
</tableCaption>
<bodyText confidence="0.999930416666667">
timal strategy is represented by 17 classifiers. By
comparison, a purely RL-based strategy would de-
fine an optimal action for every theoretically pos-
sible state (i.e. 128). In this example, the evolu-
tionary approach has reduced the number of rules
from 128 to 17 (a reduction of 87%) and is therefore
much more easily inspectable. In fact, the size of the
optimal strategy can be reduced further by select-
ing the most general classifier for each action (Table
2). These rules are sufficient since they cover the 60
states that could actually occur while following the
optimal strategy.
</bodyText>
<sectionHeader confidence="0.993848" genericHeader="conclusions">
5 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.9999845">
We have presented a novel approach to generating
spoken dialogue strategies that are both optimal and
easily inspectable. The generalizing ability of the
evolutionary reinforcement learning (RL) algorithm,
XCS, can dramatically reduce the size of the opti-
mal strategy when compared with conventional RL
techniques. In future work, we intend to exploit this
generalization feature further by developing systems
that require much larger state representations. We
also plan to investigate other approaches to strategy
summarisation. Finally, we will evaluate our ap-
proach against purely RL-based methods.
</bodyText>
<sectionHeader confidence="0.99948" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999921714285714">
Larry Bull, editor. 2004. Applications ofLearning Clas-
sifier Systems. Springer.
Matthew Frampton and Oliver Lemon. 2005. Reinforce-
ment learning of dialogue strategies using the user’s
last dialogue act. In IJCAI Workshop on Knowledge
and Reasoning in Practical Dialogue Systems, Edin-
burgh, UK, July.
John Holland. 1976. Adaptation. In Rosen R.
and F. Snell, editors, Progress in theoretical biology.
Plenum, New York.
Tim Kovacs. 2000. Strength or accuracy? Fitness cal-
culation in learning classifier systems. In Pier Luca
Lanzi, Wolfgang Stolzmann, and Stewart Wilson, edi-
tors, Learning Classifier Systems. From Foundations to
Applications, Lecture Notes in Artificial Intelligence
1813, pages 143–160. Springer-Verlag.
Renaud Lecœuche. 2001. Learning optimal dialogue
management rules by using reinforcement learning
and inductive logic programming. In 2nd Meeting
of the North American Chapter of the Association of
Computational Linguistics, Pittsburgh, USA, June.
Esther Levin, Roberto Pieraccini, and Wieland Eckert.
2000. A stochastic model of human-machine inter-
action for learning dialogue strategies. IEEE Transac-
tions on Speech and Audio Processing, 8(1):11–23.
R. Lopez-Cozar, A. De la Torre, J. Segura, A. Rubio, and
V. S´anchez. 2002. Testing dialogue systems by means
of automatic generation of conversations. Interacting
with Computers, 14(5):521–546.
Konrad Scheffler and Steve Young. 2000. Probabilis-
tic simulation of human-machine dialogues. In Inter-
national Conference on Acoustics, Speech and Signal
Processing, pages 1217–1220, Istanbul, Turkey, June.
Stewart Wilson. 1995. Classifier fitness based on accu-
racy. Evolutionary Computation, 3(2):149–175.
</reference>
<page confidence="0.998747">
176
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.342278">
<title confidence="0.999866">Evolving optimal inspectable strategies for spoken dialogue systems</title>
<author confidence="0.99481">Dave</author>
<affiliation confidence="0.914886666666667">School of Edinburgh 2 Buccleuch</affiliation>
<address confidence="0.984003">Edinburgh EH8</address>
<email confidence="0.997113">dave@cstr.ed.ac.uk</email>
<author confidence="0.873778">Johanna</author>
<affiliation confidence="0.911021666666667">School of Edinburgh 2 Buccleuch</affiliation>
<address confidence="0.97152">Edinburgh EH8</address>
<email confidence="0.997614">jmoore@inf.ed.ac.uk</email>
<author confidence="0.823434">Oliver</author>
<affiliation confidence="0.909083666666667">School of Edinburgh 2 Buccleuch</affiliation>
<address confidence="0.962047">Edinburgh EH8</address>
<email confidence="0.998329">olemon@inf.ed.ac.uk</email>
<abstract confidence="0.999721823529412">We report on a novel approach to generating strategies for spoken dialogue systems. We present a series of experiments illustrate how an reinforcement learning algorithm can produce strategies that are both optimal and easily inspectable by human developers. Our experimental strategies achieve a mean performance of 98.9% with respect to a predefined evaluation metric. Our approach also produces a dramatic reduction in strategy size when compared with conventional reinforcement learning techniques (87% in one experiment). We conclude that this algorithm can be used to evolve optimal inspectable dialogue strategies.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>Applications ofLearning Classifier Systems.</title>
<date>2004</date>
<editor>Larry Bull, editor.</editor>
<publisher>Springer.</publisher>
<marker>2004</marker>
<rawString>Larry Bull, editor. 2004. Applications ofLearning Classifier Systems. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Frampton</author>
<author>Oliver Lemon</author>
</authors>
<title>Reinforcement learning of dialogue strategies using the user’s last dialogue act.</title>
<date>2005</date>
<booktitle>In IJCAI Workshop on Knowledge and Reasoning in Practical Dialogue Systems,</booktitle>
<location>Edinburgh, UK,</location>
<contexts>
<context position="2130" citStr="Frampton and Lemon, 2005" startWordPosition="327" endWordPosition="330">rating dialogue strategies automatically. This work is based on modelling dialogue as a markov decision process, formalised by a finite state space S, a finite action set A, a set of transition probabilities T and a reward function R. Using this model an optimal dialogue strategy 7r* is represented by a mapping between the state space and the action set. That is, for each state s E 5 this mapping defines its optimal action a*�. How is this mapping constructed? Previous approaches have employed reinforcement learning (RL) algorithms to estimate an optimal value function Q* (Levin et al., 2000; Frampton and Lemon, 2005). For each state this function predicts the future reward associated with each action available in that state. This function makes it easy to extract the optimal strategy (policy in the RL literature). Progress has been made with this approach but some important challenges remain. For instance, very little success has been achieved with the large state spaces that are typical of real-life systems. Similarly, work on summarising learned strategies for interpretation by human developers has so far only been applied to tasks where each state-action pair is explicitly represented (Lecœuche, 2001).</context>
</contexts>
<marker>Frampton, Lemon, 2005</marker>
<rawString>Matthew Frampton and Oliver Lemon. 2005. Reinforcement learning of dialogue strategies using the user’s last dialogue act. In IJCAI Workshop on Knowledge and Reasoning in Practical Dialogue Systems, Edinburgh, UK, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Holland</author>
</authors>
<date>1976</date>
<booktitle>Progress in theoretical biology.</booktitle>
<editor>Adaptation. In Rosen R. and F. Snell, editors,</editor>
<publisher>Plenum,</publisher>
<location>New York.</location>
<contexts>
<context position="3745" citStr="Holland, 1976" startWordPosition="580" endWordPosition="581">ire for strategy inspectability. In this paper, we focus on the issue of inspectability. We present a series of experiments that illustrate how XCS can be used to evolve dialogue strategies that are both optimal and easily inspectable. 173 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 173–176, New York, June 2006. c�2006 Association for Computational Linguistics 2 Learning Classifier Systems and XCS Learning Classifier Systems were introduced by John Holland in the 1970s as a framework for learning rule-based knowledge representations (Holland, 1976). In this model, a rule base consists of a population of N state-action rules known as classifiers. The state part of a classifier is represented by a ternary string from the set {0,1,#} while the action part is composed from {0,1}. The # symbol acts as a wildcard allowing a classifier to aggregate states; for example, the state string 1#1 matches the states 111 and 101. Classifier systems have been applied to a number of learning tasks, including data mining, optimisation and control (Bull, 2004). Classifier systems combine two machine learning techniques to find the optimal rule set. A genet</context>
</contexts>
<marker>Holland, 1976</marker>
<rawString>John Holland. 1976. Adaptation. In Rosen R. and F. Snell, editors, Progress in theoretical biology. Plenum, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Kovacs</author>
</authors>
<title>Strength or accuracy? Fitness calculation in learning classifier systems.</title>
<date>2000</date>
<booktitle>Learning Classifier Systems. From Foundations to Applications, Lecture Notes in Artificial Intelligence 1813,</booktitle>
<pages>143--160</pages>
<editor>In Pier Luca Lanzi, Wolfgang Stolzmann, and Stewart Wilson, editors,</editor>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="5429" citStr="Kovacs, 2000" startWordPosition="856" endWordPosition="857">icy as defined by the reward function. XCS (X Classifier System) incorporates a number of modifications to Holland’s original framework (Wilson, 1995). In this system, a classifier’s fitness is based on the accuracy of its payoff prediction instead of the prediction itself. Furthermore, the genetic algorithm operates on actions instead of the population as a whole. These aspects of XCS result in a more complete map of the state-action space than would be the case with strength-based classifier systems. Consequently, XCS often outperforms strength-based systems in sequential decision problems (Kovacs, 2000). 3 Experimental Methodology In this section we present a simple slot-filling system based on the hotel booking domain. The goal of the system is to acquire the values for three slots: the check-in date, the number of nights the user wishes to stay and the type of room required (single, twin etc.). In slot-filling dialogues, an optimal strategy is one that interacts with the user in a satisfactory way while trying to minimise the length of the dialogue. A fundamental component of user satisfaction is the system’s prevention and repair of any miscommunication between it and the user. Consequent</context>
</contexts>
<marker>Kovacs, 2000</marker>
<rawString>Tim Kovacs. 2000. Strength or accuracy? Fitness calculation in learning classifier systems. In Pier Luca Lanzi, Wolfgang Stolzmann, and Stewart Wilson, editors, Learning Classifier Systems. From Foundations to Applications, Lecture Notes in Artificial Intelligence 1813, pages 143–160. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Renaud Lecœuche</author>
</authors>
<title>Learning optimal dialogue management rules by using reinforcement learning and inductive logic programming.</title>
<date>2001</date>
<booktitle>In 2nd Meeting of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<location>Pittsburgh, USA,</location>
<contexts>
<context position="2729" citStr="Lecœuche, 2001" startWordPosition="421" endWordPosition="422">and Lemon, 2005). For each state this function predicts the future reward associated with each action available in that state. This function makes it easy to extract the optimal strategy (policy in the RL literature). Progress has been made with this approach but some important challenges remain. For instance, very little success has been achieved with the large state spaces that are typical of real-life systems. Similarly, work on summarising learned strategies for interpretation by human developers has so far only been applied to tasks where each state-action pair is explicitly represented (Lecœuche, 2001). This tabular representation severely limits the size of the state space. We propose an alternative approach to finding optimal dialogue policies. We make use of XCS, an evolutionary reinforcement learning algorithm that seeks to represent a policy as a compact set of stateaction rules (Wilson, 1995). We suggest that this algorithm could overcome both the challenge of large state spaces and the desire for strategy inspectability. In this paper, we focus on the issue of inspectability. We present a series of experiments that illustrate how XCS can be used to evolve dialogue strategies that are</context>
</contexts>
<marker>Lecœuche, 2001</marker>
<rawString>Renaud Lecœuche. 2001. Learning optimal dialogue management rules by using reinforcement learning and inductive logic programming. In 2nd Meeting of the North American Chapter of the Association of Computational Linguistics, Pittsburgh, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Esther Levin</author>
<author>Roberto Pieraccini</author>
<author>Wieland Eckert</author>
</authors>
<title>A stochastic model of human-machine interaction for learning dialogue strategies.</title>
<date>2000</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="2103" citStr="Levin et al., 2000" startWordPosition="323" endWordPosition="326"> has focused on generating dialogue strategies automatically. This work is based on modelling dialogue as a markov decision process, formalised by a finite state space S, a finite action set A, a set of transition probabilities T and a reward function R. Using this model an optimal dialogue strategy 7r* is represented by a mapping between the state space and the action set. That is, for each state s E 5 this mapping defines its optimal action a*�. How is this mapping constructed? Previous approaches have employed reinforcement learning (RL) algorithms to estimate an optimal value function Q* (Levin et al., 2000; Frampton and Lemon, 2005). For each state this function predicts the future reward associated with each action available in that state. This function makes it easy to extract the optimal strategy (policy in the RL literature). Progress has been made with this approach but some important challenges remain. For instance, very little success has been achieved with the large state spaces that are typical of real-life systems. Similarly, work on summarising learned strategies for interpretation by human developers has so far only been applied to tasks where each state-action pair is explicitly re</context>
</contexts>
<marker>Levin, Pieraccini, Eckert, 2000</marker>
<rawString>Esther Levin, Roberto Pieraccini, and Wieland Eckert. 2000. A stochastic model of human-machine interaction for learning dialogue strategies. IEEE Transactions on Speech and Audio Processing, 8(1):11–23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Lopez-Cozar</author>
<author>A De la Torre</author>
<author>J Segura</author>
<author>A Rubio</author>
<author>V S´anchez</author>
</authors>
<title>Testing dialogue systems by means of automatic generation of conversations.</title>
<date>2002</date>
<journal>Interacting with Computers,</journal>
<volume>14</volume>
<issue>5</issue>
<marker>Lopez-Cozar, Torre, Segura, Rubio, S´anchez, 2002</marker>
<rawString>R. Lopez-Cozar, A. De la Torre, J. Segura, A. Rubio, and V. S´anchez. 2002. Testing dialogue systems by means of automatic generation of conversations. Interacting with Computers, 14(5):521–546.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Konrad Scheffler</author>
<author>Steve Young</author>
</authors>
<title>Probabilistic simulation of human-machine dialogues.</title>
<date>2000</date>
<booktitle>In International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<pages>1217--1220</pages>
<location>Istanbul, Turkey,</location>
<contexts>
<context position="7538" citStr="Scheffler and Young, 2000" startWordPosition="1203" endWordPosition="1206">ction received a reward value of zero. At the end of each dialogue, the final reward comprised three parts: (i) -1000 for each system turn; (ii) 100,000 if all slots were filled; (iii) 100,000 if the first system act was a greeting. In Experiments 3 and 4, an additional reward of 100,000 was assigned if all slots were confirmed. The transition probabilities were modelled using two versions of a handcoded simulated user. A very large number of test dialogues are usually required for learning optimal dialogue strategies; simulated users are a practical alternative to employing human test users (Scheffler and Young, 2000; Lopez-Cozar et al., 2002). Simulated user A represented a fully cooperative user, always giving the slot information that was asked. User B was less cooperative, giving no response 20% of the time. This allowed us to perform a two-fold cross validation of the evolved strategies. For each experiment we allowed the system’s strategy to evolve over 100,000 dialogues with each simulated user. Dialogues were limited to a maximum of 30 system turns. We then tested each strategy with a further 10,000 dialogues. We logged the total reward (payoff) for each test dialogue. Each experiment was repeated</context>
</contexts>
<marker>Scheffler, Young, 2000</marker>
<rawString>Konrad Scheffler and Steve Young. 2000. Probabilistic simulation of human-machine dialogues. In International Conference on Acoustics, Speech and Signal Processing, pages 1217–1220, Istanbul, Turkey, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stewart Wilson</author>
</authors>
<title>Classifier fitness based on accuracy.</title>
<date>1995</date>
<journal>Evolutionary Computation,</journal>
<volume>3</volume>
<issue>2</issue>
<contexts>
<context position="3031" citStr="Wilson, 1995" startWordPosition="469" endWordPosition="470">stance, very little success has been achieved with the large state spaces that are typical of real-life systems. Similarly, work on summarising learned strategies for interpretation by human developers has so far only been applied to tasks where each state-action pair is explicitly represented (Lecœuche, 2001). This tabular representation severely limits the size of the state space. We propose an alternative approach to finding optimal dialogue policies. We make use of XCS, an evolutionary reinforcement learning algorithm that seeks to represent a policy as a compact set of stateaction rules (Wilson, 1995). We suggest that this algorithm could overcome both the challenge of large state spaces and the desire for strategy inspectability. In this paper, we focus on the issue of inspectability. We present a series of experiments that illustrate how XCS can be used to evolve dialogue strategies that are both optimal and easily inspectable. 173 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 173–176, New York, June 2006. c�2006 Association for Computational Linguistics 2 Learning Classifier Systems and XCS Learning Classifier Systems were introd</context>
<context position="4966" citStr="Wilson, 1995" startWordPosition="784" endWordPosition="785">ithm is used to evaluate and modify the population of rules while reinforcement learning is used to assign rewards to existing rules. The search for better rules is guided by the strength parameter associated with each classifier. This parameter serves as a fitness score for the genetic algorithm and as a predictor of future reward (payoff) for the RL algorithm. This evolutionary learning process searches the space of possible rule sets to find an optimal policy as defined by the reward function. XCS (X Classifier System) incorporates a number of modifications to Holland’s original framework (Wilson, 1995). In this system, a classifier’s fitness is based on the accuracy of its payoff prediction instead of the prediction itself. Furthermore, the genetic algorithm operates on actions instead of the population as a whole. These aspects of XCS result in a more complete map of the state-action space than would be the case with strength-based classifier systems. Consequently, XCS often outperforms strength-based systems in sequential decision problems (Kovacs, 2000). 3 Experimental Methodology In this section we present a simple slot-filling system based on the hotel booking domain. The goal of the s</context>
</contexts>
<marker>Wilson, 1995</marker>
<rawString>Stewart Wilson. 1995. Classifier fitness based on accuracy. Evolutionary Computation, 3(2):149–175.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>