<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001874">
<title confidence="0.914252">
Does more data always yield better translations?
</title>
<author confidence="0.8380725">
Guillem Gasc´o, Martha-Alicia Rocha, Germ´an Sanchis-Trilles,
Jes´us Andr´es-Ferrer and Francisco Casacuberta
</author>
<affiliation confidence="0.814549">
Departament de Sistemes Inform`atics i Computaci´o
</affiliation>
<address confidence="0.719032">
Universitat Polit`ecnica de Val`encia
Camide Vera s/n, 46022 Val`encia, Spain
</address>
<email confidence="0.998997">
{ggasco,mrocha,gsanchis,jandres,fcn}@dsic.upv.es
</email>
<sectionHeader confidence="0.994602" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996386272727273">
Nowadays, there are large amounts of data
available to train statistical machine trans-
lation systems. However, it is not clear
whether all the training data actually help
or not. A system trained on a subset of such
huge bilingual corpora might outperform
the use of all the bilingual data. This paper
studies such issues by analysing two train-
ing data selection techniques: one based
on approximating the probability of an in-
domain corpus; and another based on in-
frequent n-gram occurrence. Experimental
results not only report significant improve-
ments over random sentence selection but
also an improvement over a system trained
with the whole available data. Surprisingly,
the improvements are obtained with just a
small fraction of the data that accounts for
less than 0.5% of the sentences. After-
wards, we show that a much larger room for
improvement exists, although this is done
under non-realistic conditions.
</bodyText>
<sectionHeader confidence="0.998791" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99994765">
Globalisation and the popularisation of the Inter-
net have lead to a rapid increase in the amount of
bilingual corpora available. Entities such as the
European Union, the United Nations and other
multinational organisations need to translate all
the documentation they generate. Such transla-
tions happen every day and provide very large
multilingual corpora, which are oftentimes diffi-
cult to process and significantly increase the com-
putational requirements needed to train statistical
machine translation (SMT) systems. For instance,
the corpora made available for recent machine
translation evaluations are in the order of 1 billion
running words (Callison-Burch et al., 2010).
However, two main problems arise when at-
tempting to use this huge pool of sentences for
training SMT systems: firstly, a large portion of
this data is obtained from domains that differ from
that in which the SMT system is to be used or as-
sessed; secondly, the use of all this data for train-
ing the system increases the computational train-
ing requirements. Despite the previous remarks,
the de facto standard consists in training SMT sys-
tems with all the available data. This is due to
the widespread misconception that the more data
a system is trained with, the better its performance
should be. Although the previous statement is the-
oretically true if all the data belongs to the same
domain, this is not the case in the problems tack-
led by most of the SMT systems. For instance,
enterprises often need to build on-demand sys-
tems (Yuste et al., 2010). In this case, since we
are interested in translating some specific text, it
is not clear whether training a system with all data
yields better performance than training it with a
wisely selected subset of bilingual sentences.
The bilingual sentence selection (BSS) task is
stated as the problem of selecting the best sub-
set of bilingual sentences from an available pool
of sentences, with which to train a SMT system.
This paper is concerned to BSS, and mainly two
ideas are developed.
On the one hand, two BSS strategies that at-
tempt to build better translation systems are anal-
ysed. Such strategies are able to improve state-of-
the-art translation quality without the very high
computational resources that are required when
using the complete pool of sentences. Both tech-
niques span through two orthogonal criteria when
selecting bilingual sentences from the available
pool: avoiding to introduce a bias in the original
data distribution, and increasing the informative-
ness of the corpus.
On the other hand, we prove that among all pos-
sible subsets from the sentence pool, there is at
least a small one that yields large improvements
(up to 10 BLEU points) with respect to a system
trained with all the data. In order to retrieve such
subset, we had to use an oracle that employs infor-
mation extracted from the reference translations
</bodyText>
<page confidence="0.968317">
152
</page>
<note confidence="0.9767125">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 152–161,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.99998005">
only for the purpose of selecting bilingual sen-
tences. However, references are not used at any
stage within the translation system for obtaining
the hypotheses. Note that although we are not
able to achieve such an improvement without an
oracle, this result restates the BSS problem as an
interesting approach not only for reducing com-
putational effort but also for significantly boost-
ing performance. To our knowledge, no previous
work has quantified the room of improvement in
which BSS techniques could incur.
In order to assess the performance of the dif-
ferent BSS techniques, translation results are ob-
tained by using a standard state-of-the-art SMT
system (Koehn et al., 2007). The most recent lit-
erature defines the SMT problem (Papineni et al.,
1998; Och and Ney, 2002) as follows: given an
input sentence f from a certain source language,
the purpose is to find an output sentence e� in a
certain target language such that
</bodyText>
<equation confidence="0.998236666666667">
K
e� = arg max Akhk(f,e) (1)
e k=1
</equation>
<bodyText confidence="0.999686">
where hk(f, e) is a score function representing an
important feature for the translation of f into e,
as for example the language model of the target
language, a reordering model or several transla-
tion models. Ak are the log-linear combination
weights.
The main contributions of this paper are:
</bodyText>
<listItem confidence="0.9860756">
• A BSS technique is analysed, which im-
proves the results obtained with a random
bilingual sentence selection strategy when
the specific domain to be translated signifi-
cantly differs from that of the pool of sen-
tences.
• Another BSS technique is analysed that, us-
ing less than 0.5% of the sentences avail-
able, significantly improves over random se-
lection, beating a system trained with all the
pool of sentences.
• We prove, by means of an oracle, that a wise
BSS technique can yield large improvements
when compared with systems trained with all
data available.
</listItem>
<bodyText confidence="0.999772875">
The remaining of the paper is structured as fol-
lows. Section 2 summarises the related work.
Sections 3 and 4 present two BSS techniques,
namely, probabilistic sampling and recovery of
infrequent n-grams. In Section 5 experimental re-
sults are reported. Finally, the main results of the
work and several future work directions are dis-
cussed in Section 6.
</bodyText>
<sectionHeader confidence="0.999443" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999965466666667">
Training data selection has been receiving an in-
creasing amount of attention within the SMT
community. For instance, in (Li et al., 2010;
Gasc´o et al., 2010) several BSS techniques, sim-
ilar to those analysed in this paper, have been
applied for training MT systems when there are
large training corpora available. However, nei-
ther such techniques have been formalised, nor its
performance thoroughly analysed. A similar ap-
proach that gives weights to different subcorpora
was proposed in (Matsoukas et al., 2009).
In (Lu et al., 2007), information retrieval meth-
ods are used in order to produce different sub-
models which are then weighted according to the
sentence to be translated. In such work, authors
define the baseline as the result obtained train-
ing only with the corpus that share the same do-
main of the test. Afterwards they claim that they
are able to improve baseline translation quality by
adding new sentences retrieved with their method.
However, they neither compare their technique
with random sentence selection, nor with a model
trained with all the corpora.
Although the techniques that are applied for
BSS are often very similar to those applied for ac-
tive learning (AL), both problems are essentially
different. Since the AL strategies assume that
the pool of sentences are not translated, they are
usually interested in finding the best monolingual
subset of sentences to be translated by a human
annotator. In contrast, in BSS, it is assumed that a
fairly large amount of bilingual corpora is readily
available, and the main goal consists in selecting
only those sentences which will maximise system
performance.
Some works have applied sentence selection in
small scale AL frameworks. These works extend
the training corpora at most with 5000 sentences.
In (Ananthakrishnan et al., 2010), sentences are
selected by means of discriminative techniques.
In (Haffari et al., 2009) a technique is proposed
for increasing the counts of phrases that are con-
sidered infrequent. Both works significantly dif-
fer from the current work not only on the frame-
work, but also on the scale of the experiments, the
</bodyText>
<page confidence="0.99864">
153
</page>
<bodyText confidence="0.999136">
proposed techniques and the obtained improve-
ments. Similar ideas applied to adaptation prob-
lems have been proposed in (Moore and Lewis,
2010; Axelrod et al., 2011).
</bodyText>
<sectionHeader confidence="0.921013" genericHeader="method">
3 Probabilistic Sampling
</sectionHeader>
<bodyText confidence="0.999963652173913">
As discussed in Section 2, BSS has inherently
attached many meaningful links with AL tech-
niques. Selecting samples for learning our mod-
els, incurs in a well-known difficulty in AL, the
so-called sample bias problem (Dasgupta, 2009).
This problem, which is spread to the BSS case,
is summarised as the distortion introduced by the
active strategy into the probability distribution un-
derlying the training corpus. This bias forces the
training algorithm to learn a distorted probability
model which can significantly differ from the ac-
tual one.
In order to further analyse the sampling bias
problem, consider the maximum likelihood esti-
mation (MLE) of a probability model, pθ(e, f)
for a given corpus of N data points,{(en, fn)},
sampled from the actual probability distribution,
Pr(e, f). Recall that e denotes a target sen-
tence whereas f stands for its source counter-
part. MLE techniques aims at minimising the
Kullback-Leibler divergence between the actual
unknown probability distribution and the proba-
bility model (Bishop, 2006), defined as
</bodyText>
<equation confidence="0.97278">
Pr(e, f) log (pθ(e
Pr(e, f)
, f)
(2)
When minimising, Eq. (2) is simplified to
�θ� = arg max Pr(e, f) log(pθ(e, f)) (3)
θ
e,f
</equation>
<bodyText confidence="0.99777525">
which is approximated by a sufficiently large
dataset under the commonly hold assumption that
it is independently and identically distributed ac-
cording to Pr(e, f) as
</bodyText>
<equation confidence="0.948411">
�θ� = arg max
θ n
</equation>
<bodyText confidence="0.972609107142857">
Therefore, by perturbing the sample {(en, fn)}
with an active strategy, we are, in fact, modifying
the approximation to Eq.(3) and learning a differ-
ent underlying probability distribution.
In this section a statistical framework is pro-
posed to build systems with BSS while avoiding
the sample bias. The proposed approach relies in
conserving the probability distribution of the task
domain by wisely selecting the bilingual pairs to
be used from the whole pool of sentences. Hence,
it is mandatory to exclude sentences from the pool
that distort the actual probability. In order to ap-
proximate the probability distribution, we assume
that a small but representative corpus is avail-
able from the task domain. This corpus, referred
henceforth as the in-domain corpus, provides a
way to build an initial model which approximates
the actual probability of the system. The pool of
sentences will be oppositely denoted as the out-
of-domain corpus.
The actual probability of the task domain, the
so called in-domain probability, is approximated
with the following model
p(e, f, |e|, |f|) = p(e, f  ||e|, |f|) - p(|e|, |f|) (5)
where p(|e|, |f|) denotes the in-domain length
probability, and p(e, f  ||e|, |f|) the in-domain
bilingual probability.
The length probability is estimated by MLE
</bodyText>
<equation confidence="0.990321">
N(|e |+ |f|)
p(|e|, |f|) = (6)
N
</equation>
<bodyText confidence="0.99126">
where N(|e|+|f|) is the number of bilingual pairs
in the in-domain corpus such that their lengths
sum up to |e|+|f |and N denotes the total num-
ber of sentences. Note that no distinction is made
between source and target lengths since the model
is intended for sampling.
The complexity of the in-domain bilingual
probability distribution, p(e, f  ||e|, |f|), requires
a more sophisticated approximation
p(e, f/|e|, |f |) = exp Ek Lkfk(e, f)) (7)
being i a normalisation constant; and where
fk(...) and γk are the features of the model and
their respective parametric weights. Specifically,
four logarithmic features were considered for this
sampling technique: a direct and an inverse IBM
model 4 (Brown et al., 1994); and both, source
and target, 5-gram language models. All fea-
ture models are estimated in the in-domain cor-
pus with standard techniques (Brown et al., 1994;
Stolcke, 2002). As a first approach, the parame-
ters of the log-linear model in Eq. (7), γk, were
uniformly fixed to 1.
</bodyText>
<equation confidence="0.996786">
KL(Pr  |pθ) = �
e,f
log(pθ(en, fn)) (4)
</equation>
<page confidence="0.997339">
154
</page>
<bodyText confidence="0.999823375">
Once we have an appropriate model for the
in-domain probability distribution, the proposed
method randomly samples a given number of
bilingual pairs from the out-of-domain corpora
(the pool of sentences). The process of extend-
ing the in-domain corpus with additional bilin-
gual pairs from the out-of-domain corpus is sum-
marised as follows:
</bodyText>
<listItem confidence="0.9639244">
• Decide according to the in-domain length
probability in Eq. (6), how many samples
should be drawn for each length, i.e. divide
the number of sentences to add into length
dependent buckets.
• Randomly draw the number of samples
specified in each bucket according to the
in-domain bilingual probability in Eq. (7)
among all the bilingual sentences that share
the current bucket length.
</listItem>
<bodyText confidence="0.99820775">
Although the pool of sentences is typically
large, it is not large enough to gather a signifi-
cant amount of probability mass. Consequently,
a small set of sentences accumulate most of the
probability mass and tend to be selected multi-
ple times. To avoid this awkward and undesired
behaviour, the sampling is performed without re-
placement.
</bodyText>
<sectionHeader confidence="0.989993" genericHeader="method">
4 Infrequent n-gram Recovery
</sectionHeader>
<bodyText confidence="0.999913022222222">
Another criterion when confronting the BSS task
is to increase the informativeness of the training
set. Thus, it seems important to choose sentences
that provide information not seen in the training
corpus. Note that this criterion is sometimes op-
posed to the one presented in Section 3.
The performance of phrase-based machine
translation systems strongly relies in the quality
of the phrases extracted from the training sam-
ples. In most of the cases, the inference of such
phrases or rules is based on word alignments,
which cannot be computed accurately when ap-
pearing rarely in the training corpus. The extreme
case are the out-of-vocabulary words: words that
do not appear in the training set, cannot be trans-
lated. Moreover, this problem can be extended to
sequences of words (n-grams). Consider a 2-gram
fzfj appearing few or no times in the training set.
Although fz and fj may appear separately in the
training set, the system might not be able to in-
fer the translation of the 2-gram fzfj, which may
be different from the concatenation of the transla-
tions of both words separately.
When selecting sentences from the pool it is
important to choose sentences that contain n-
grams that have never been seen (or have been
seen just a few times) in the training set. Such
n-grams will be henceforth referred to as infre-
quent n-grams . An n-gram is considered infre-
quent when it appears less times than an infre-
quent threshold t. If the source language sen-
tences to be translated are known beforehand, the
set of infrequent n-grams can be reduced to those
present in such sentences. Then, the technique
consists in selecting from the pool those sentences
which contain infrequent n-grams present in the
source sentences to be translated.
Sentences in the pool are sorted by their infre-
quency score in order to select first the most in-
formative. Let X the set of n-grams that appear
in the sentences to be translated and w one of
them; Qw) the counts of w in the source lan-
guage training set; and N(w) the counts of w
in the source sentence f to be scored. The infre-
quency score of f is:
</bodyText>
<equation confidence="0.989484">
i(f) � J: min(1, N(w)) max(0, t−Qw)) (8)
wEX
</equation>
<bodyText confidence="0.999983590909091">
In order to avoid giving a high score to noisy
sentences with a lot of occurrences of the same in-
frequent n-gram, only one occurrence of each n-
gram is taken into account to compute the score.
In addition, the score gives more importance to
the n-grams with lowest counts in the training
set. Although it could be possible to select the
highest scored sentences, we updated the scores
each time a sentence is selected. This decision
was taken to avoid the selection of too many sen-
tences with the same infrequent n-gram. First,
sentences in the pool are scored using Equation
(8). Then, in each iteration, the sentence f* with
the highest score is selected, added to the training
set and removed from the pool. In addition, the
counts of the n-grams present in f* are updated
and, hence, the scores of the rest of the sentences
in the pool. Since rescoring the whole pool would
incur in a very high computational cost, a subop-
timal search strategy was followed, in which the
search was constrained to a given set of highest
scoring sentences. Here it was set to one million.
</bodyText>
<page confidence="0.989112">
155
</page>
<table confidence="0.924924291666667">
t = 1 t = 10 t = 25
tr all tr all tr all
1-gr 11.6 1.3 40.5 3.5 59.9 5.1
2-gr 38 9.8 73.2 21.3 84.9 27.9
3-gr 66.8 33.5 91.1 55.7 96.4 64.9
4-gr 87.1 65.8 98.2 85.5 99.4 90.7
Language
English
French
English
French
English
French
Subset
train
dev
test
|S ||W ||V |
47.5K 747K 24.6K
793K 31.7K
571 9.2K 1.9K
10.3K 2.2K
641 12.6K 2.4K
12.8K 2.7K
</table>
<tableCaption confidence="0.94005225">
Table 1: Percentage of infrequent n-grams in the TED
test set when considering only the TED training set
(tr), and when adding the out-of-domain pool (all),
for different infrequency thresholds t.
</tableCaption>
<bodyText confidence="0.9971571">
Table 1 shows the percentage of source lan-
guage infrequent n-grams for the test of a rela-
tively small corpus such as the TED corpus (for
details see Section 5) when considering just the
in-domain training set (≈ 40K sentences) and the
same percentage when adding the larger out of do-
main corpora. The percentages in the table have
been computed separately for different values of
the threshold t and for n-grams of order from 1 to
4. Note that the reduction in the number of infre-
quent n-grams is very high for the 1-grams but de-
creases progressively when considering n-grams
of higher order. This indicates that the infrequent
n-grams recovery technique should be very effec-
tive for lower order n-grams, but might have less
effect for higher order n-grams. Therefore, and
in order to lower the computational cost involved,
the experiments carried out for this paper were
performed considering only infrequent 1-grams,
2-grams and 3-grams.
</bodyText>
<sectionHeader confidence="0.999541" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99997">
In the present Section, we first describe the exper-
imental framework employed to assess the perfor-
mance of the BSS techniques described. Then, re-
sults for the probabilistic sentence selection strat-
egy are shown, followed by results obtained with
the infrequent n-grams technique. Some exam-
ple translations are shown and, finally, we also
report experiments using the infrequent n-grams
technique in Oracle mode, in order to establish
the potential improvement for such technique and
for BSS in general.
</bodyText>
<subsectionHeader confidence="0.923981">
5.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9997836">
All experiments were carried out using the
open-source SMT toolkit Moses (Koehn et al.,
2007), in its standard non-monotonic configura-
tion. The phrase tables were generated by means
of symmetrised word alignments obtained with
</bodyText>
<tableCaption confidence="0.9594345">
Table 2: TED corpus main figures. K denotes thou-
sands of elements. |S |stands for number of sentences,
|W  |for number of running words, and |V  |for vocab-
ulary size.
</tableCaption>
<equation confidence="0.975637666666667">
|S ||W ||V |
77.2K 1.71M 29.9K
1.99M 48K
2.1K 49.8K 8.7K
55.4K 7.7K
2.5K 65.6K 8.9K
72.5K 10.6K
2.5K 62K 8.9K
70.5K 10.3K
</equation>
<tableCaption confidence="0.98704">
Table 3: News Commentary corpus main figures.
</tableCaption>
<bodyText confidence="0.999668833333333">
GIZA++ (Och and Ney, 2003). The language
model used was a 5-gram with modified Kneser-
Ney smoothing (Kneser and Ney, 1995), built
with SRILM toolkit (Stolcke, 2002). The log-
linear combination weights in Eq. (1) were opti-
mised using Minimum Error Rate Training (Och
and Ney, 2002) on the corresponding develop-
ment sets.
Experiments were carried out on two corpora:
TED (Paul et al., 2010) and News Commentary
(NC) (Callison-Burch et al., 2010). TED is an
English-French corpus composed of subtitles for
a collection of public speeches on a variety of top-
ics. The same partitions as in the IWSLT2010
evaluation task (Paul et al., 2010) have been used.
Subtitles have been concatenated into complete
sentences. NC is a slightly larger English-French
corpus in the news domain. Main figures of both
corpora are shown in Tables 2 and 3. As for the
pool of sentences, three large corpora have been
used: Europarl (Euro), United Nations (UN) and
Gigaword (Giga), in the partition established for
the 2010 workshop on SMT of the ACL (Callison-
Burch et al., 2010). Sentences of length greater
than 50 have been pruned. Table 4 shows the main
figures of the tokenised and lowercased corpora.
When translating between some language
pairs, there are words that remain invariable, like
for example numbers or punctuation marks in the
case of European languages. In fact, an easy and
</bodyText>
<figure confidence="0.998787">
Subset
train
dev 08
test 09
test 10
Language
English
French
English
French
English
French
English
French
</figure>
<page confidence="0.968995">
156
</page>
<table confidence="0.996712285714286">
Corpus Language |� ||W  ||V |
English 25.6M 81K
Euro French 1.25M 28.2M 101K
UN English 5M 94.4M 302K
French 107M 283K
English 303M 1.6M
Giga French 15.5M 361M 1.6M
</table>
<tableCaption confidence="0.9654215">
Table 4: Figures of the corpora used as sentence pool.
M stands for millions of elements.
</tableCaption>
<figure confidence="0.9959932">
Relative frequency
0.03
0.02
0.01
0
Europarl
Gigaword
UN
TED
NC
</figure>
<bodyText confidence="0.999739926829268">
effective technique that is commonly used is to re-
produce out-of-vocabulary words from the source
sentence in the target hypothesis. However, in-
variable n-grams are usually infrequent as well,
which implies that the infrequent n-grams tech-
nique would select sentences containing such n-
grams, even though they do not provide further
information. As a first approach, we exclude n-
grams without any letter.
Baseline experiments have been carried out for
TED and NC corpora using the corresponding
training set. For comparison purposes, we also
included results for a purely random sentence se-
lection without replacement. In the plots, each
point corresponding to random selection represent
the average of 10 repetitions. Experiments using
all data are also reported, although a 64GB ma-
chine was necessary, even with binarized phrase
and distortion tables.
Experiments were conducted by selecting a
fixed amount of sentences according to each one
of the techniques described above. Then, these
sentences were included into the training data and
subsequent SMT systems were built for translat-
ing the test set.
Results are shown in terms of BLEU (Papineni
et al., 2001), which is an accuracy metric that
measures n-gram precision, with a penalty for
sentences that are too short. Although it could
be argued that improvements obtained might be
due to a side effect of the brevity penalty, this
was not found to be true: the BSS techniques (in-
cluding random) and considering all data yielded
very similar brevity penalties (±0.005), within
each corpus. In addition, TER scores (Snover et
al., 2006) were also computed, but are omitted
for clarity purposes and since they were found to
be coherent with BLEU. TER is an error metric
that computes the minimum number of edits re-
quired to modify the system hypotheses so that
they match the references translations.
</bodyText>
<figure confidence="0.945867">
0 10 20 30 40 50 60 70 80 90 100
Combined sentence length
</figure>
<figureCaption confidence="0.999204">
Figure 2: Combined length relative frequency.
</figureCaption>
<subsectionHeader confidence="0.556443">
5.2 Results for Probabilistic Sampling
</subsectionHeader>
<bodyText confidence="0.999959333333333">
In addition to the probabilistic sampling tech-
nique proposed in Section 3, we also analysed the
effect of sampling only according to the combined
source-reference length, with the purpose of es-
tablishing whether potential improvements were
only due to the length component, or rather to the
complete sampling model. Results for the 2009
test set are shown in Figure 1. Several things
should be noted:
</bodyText>
<listItem confidence="0.99504975">
• Performing sentence selection only according
to sentence lengths does not achieve better
performance than random selection.
• Selecting sentences according to probabilis-
</listItem>
<bodyText confidence="0.979907478260869">
tic sampling is able to improve random se-
lection in the case of the TED corpus, but
is not able to do so in the case of the NC
corpus. Significance tests for the 500K case
reported that the differences were significant
in the case of the TED corpus, but not in the
case of the NC corpus.
• In the case of the TED corpus, the perfor-
mance achieved with the system built by
sampling 500K sentences is only 0.5 BLEU
points below the performance achieved by
the system built with all the data available.
The explanation to the fact that probabilistic
sampling is able to improve over random sam-
pling only in the case of the TED corpus, but not
in the case of NC, relies in the nature of the cor-
pora. Although both of them belong to a very
generic domain, their characteristics are very dif-
ferent. In fact, the NC data is very similar to the
sentences in the pool, but, in contrast, the sen-
tences present in the TED corpus have a much
more different structure. This difference is illus-
trated in Figure 2, where the relative frequency of
</bodyText>
<page confidence="0.973299">
157
</page>
<figure confidence="0.999009192307692">
TED corpus
NC corpus
BLEU
24
23
22
21
in domain
all
random
length
sampling
BLEU
22
21
20
19
in domain
all
random
length
sampling
0 100K 200K 300K 400K 500K
Number of sentences added
0 100K 200K 300K 400K 500K
Number of sentences added
</figure>
<figureCaption confidence="0.995464666666667">
Figure 1: Effect of adding sentences over the BLEU score using the probabilistic sampling, length sampling and
random selection techniques for the two corpora, TED and News Commentary. Horizontal lines represent the
scores when using just the in domain training set and all the data available.
</figureCaption>
<figure confidence="0.989227">
0 50k 100k 200k 0 50k 100k 200k
Number of sentences added Number of sentences added
</figure>
<figureCaption confidence="0.986713">
Figure 3: Effect of adding sentences over the BLEU score using the infrequent n-grams (with different thresh-
olds) and random selection techniques for the two corpora, TED and News Commentary. Horizontal lines repre-
sent the scores when using just the in domain training set and all the data available.
</figureCaption>
<figure confidence="0.991146916666667">
TED corpus NC corpus
BLEU
23
22
21
20
19
all
in domain
random
t=10
t=25
BLEU
26
25
24
23
22
21
t=10
all
in domain
t=25
random
</figure>
<bodyText confidence="0.999521625">
each combined sentence length is shown. In this
plot, it stands out clearly that the TED corpus has
a very different length distribution than the other
four corpora considered, whereas the NC corpus
presents a very similar distribution. This implies
that, when considering TED, an intelligent data
selection strategy will have better chances to im-
prove random selection than in the case of NC.
</bodyText>
<sectionHeader confidence="0.536427" genericHeader="method">
5.3 Results for Infrequent n-grams Recovery
</sectionHeader>
<bodyText confidence="0.977459428571429">
Figure 3 shows the effect of adding sentences us-
ing the infrequent n-grams and the random se-
lection techniques on the 2009 test set. Once
all the infrequent n-grams have been covered
t times, the infrequency score for all the sen-
tences remaining in the pool is 0, and none of
them can be selected. Hence, the number of
sentences that can be selected for each t is lim-
ited. Although for clarity we only show results
for t = 110, 251, experiments have also been car-
ried out for t = 11, 5, 10, 251. Such results pre-
sented similar curves, although less sentences can
be selected and hence improvements obtained are
slightly lower. Several conclusions can be drawn:
</bodyText>
<listItem confidence="0.975470727272727">
• The translation quality provided by the in-
frequent n-grams technique is significantly
better than the results achieved with random
selection, comparing similar amount of sen-
tences. Specifically, the improvements ob-
tained are in the range of 3 BLEU points.
• Results for the TED corpus are more irreg-
ular. The best performance is achieved for
t = 25 and 50K sentences added. In NC, the
best result is for t = 10 and 112K.
• Selecting sentences with the infrequent n-
</listItem>
<bodyText confidence="0.9513045">
grams technique provides better results than
including all the available data. While using
less than 0.5% of the data, improvements be-
tween 0.5 and 1 BLEU points are achieved.
When looking at Figure 3, one might suspect
that t needs to be set specifically for a given test
</bodyText>
<page confidence="0.996434">
158
</page>
<bodyText confidence="0.998809666666667">
set, and that results from one set are not to be ex-
trapolated to other test sets. For this reason, we
selected the best configuration in Figure 3 and
used it to build a new system for translating the
unseen NC 2010 test set. Such experiment, with
t = 10 and including all sentences with score
greater than 0 (Pz� 110K), is shown in Table 5 and
evidences that improvements are actually coher-
ent among different test sets.
</bodyText>
<table confidence="0.932779">
technique BLEU TER #phrases
in-domain 19.0 65.2 5.1M
all data 22.7 60.8 1236M
infreq. t = 10 23.6 59.2 16.5M
</table>
<tableCaption confidence="0.982193333333333">
Table 5: Effect of the infrequent n-gram recovery tech-
nique for an unseen test set, when setting t = 10 and
number of phrases (parameters) of the models.
</tableCaption>
<subsectionHeader confidence="0.994235">
5.4 Oracle Results
</subsectionHeader>
<bodyText confidence="0.999991592592593">
In order to analyse the potential of BSS tech-
niques, the infrequent n-grams recovery tech-
nique in Section 4 was implemented in oracle
mode. In this way, sentences from the pool
were selected according to the infrequent n-grams
present in the reference translations of the test set.
Note that test references were not included into
the training data as such, but were rather used
to establish which bilingual sentences within the
pool were best suitable for training the SMT sys-
tem. In this way, we were able to establish the po-
tential for improvement of a BSS technique. In-
terestingly, the SMT system trained in this way
achieved 31 BLEU points on the News Commen-
tary 2009 test set, i.e. an 8 BLEU points improve-
ment over the system trained with all the data
available. This result would have beaten all the
systems that took part in the 2009 Workshop on
Machine translation (Callison-Burch et al., 2009).
This result is really important: although we are
aware that the sentences were selected in a non-
realistic manner, it proves that an appropriate BSS
technique would be able to boost SMT perfor-
mance in a very significant manner. Similar re-
sults were obtained with the TED and NC 2010
test sets, with 10 and 7 points improvement, re-
spectively.
</bodyText>
<subsectionHeader confidence="0.979924">
5.5 Example Translations
</subsectionHeader>
<bodyText confidence="0.95813175">
Example translations are shown in Figure 4. In
the first example, the baseline system is not able
the budget has also been criticised by klaus .
le budget a ´egalement ´et´e criticised par m. klaus .
Rdm le budget a ´egalement ´et´e critiqu´ees par m. klaus .
le budget a ´egalement ´et´e critiqu´ee par klaus .
le budget a ´egalement ´et´e critiqu´e par klaus .
le budget a ´egalement ´et´e critiqu´e par klaus .
klaus critique ´egalement le budget.
and one has come from music.
et un a de la musique .
Rdm et on vient de musique .
et on a viennent de musique .
et de la musique .
et un est venu de la musique .
et un vient du monde de la musique .
</bodyText>
<figureCaption confidence="0.9990482">
Figure 4: Examples of two translations for each of the
SMT systems built: Src (source sentence), Bsl (base-
line), Rdm (random selection), PS (probabilistic sam-
pling), All (all the data available), Infr (Infrequent n-
grams) and Ref (reference).
</figureCaption>
<bodyText confidence="0.999912608695652">
to translate criticised, which is considered out-of-
vocabulary. Even though random selection is able
to solve this problem (luckily), it does not achieve
to translate it correctly, introducing a concordance
error. A similar thing happens when using prob-
abilistic sampling, where a grammatical error is
also present, and only Tnfr and All are able
to present a correct translation. This is not only
casual, since, by ensuring that a given n-gram ap-
pears at least a certain number of times t, the odds
of including all possible translations of criticised
are incremented significantly. Note that, even if
the Tnfr translation is different from the refer-
ence, it is equally correct. In the second example,
the baseline translation is pretty much correct, but
has a different meaning (something like “and one
has music”). Similarly, when including all data
the translation obtained by the system means “and
some music”. In this case, both random and prob-
abilistic selection present grammatically incorrect
sentences, and only Tnfr is able to provide a cor-
rect translation, although pretty literal and differ-
ent from the reference.
</bodyText>
<sectionHeader confidence="0.999789" genericHeader="discussions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999792428571429">
Bilingual sentence selection (BSS) might be un-
derstood to be closely related to adaptation, even
though both paradigms tackle problems which
are, in essence, different. The goal of an adap-
tation technique is to adapt model parameters,
which have been estimated on a large out-of-
domain (or generic) data set, so that they are
</bodyText>
<figure confidence="0.995075916666667">
Src
Bsl
PS
All
Infr
Ref
Src
Bsl
PS
All
Infr
Ref
</figure>
<page confidence="0.991514">
159
</page>
<bodyText confidence="0.99999025">
best suitable for dealing with a domain-specific
test set. This adaptation process is ought to be
achieved by means of a (potentially small) adapta-
tion set, which belongs to the same domain as the
test data. In contrast, BSS tackles with the prob-
lem of how to select samples from a large pool
of training data, regardless of whether such pool
of data is in-domain or out-of-domain. Hence, in
one case we can assume to have a fairly well es-
timated translation model, which is to be adapted,
whereas in BSS we still have full control over the
estimation of such model and need not to aim at a
specific domain, although it might often be so.
BSS is related with instance weighting (Jiang
and Zhai, 2007; Foster et al., 2010). Adapta-
tion and BSS can be considered to be orthogo-
nal (yet complementary) problems under the in-
stance weighting paradigm. In such case, instance
weighting can be considered to span a complete
paradigmatic space between both. At one end,
there is sample selection (BSS for SMT), while at
the other end there is adaptation. For instance, it
is quite common to confront the adaptation prob-
lem by extracting different phrase-tables from dif-
ferent corpora, and then interpolate such tables.
This technique could be also applied to promote
the performance of the system built by means of
BSS. However, this is left out as future work.
We thoroughly analysed two BSS approaches
that obtain competitive results, while using a
small fraction of the training data, although there
is still much to be gained. For instance, oracle re-
sults have also been reported in this work, yield-
ing improvements of up to 10 BLEU points. Even
though the use of an oracle typically implies that
the results obtained are not realistic, recall that
the proposed oracle is special, in the sense that it
only uses the reference sentences for the specific
purpose of selecting training samples, but the ref-
erences are not included into the training data as
such. This is useful for assessing the potential be-
hind BSS: ideally, if we were able to design a BSS
strategy that, without using the references, would
select exactly those training samples, we would be
boosting system performance by 10 BLEU points.
This re-states BSS as a compelling technique that
has not yet received the attention it deserves.
BSS is not aimed at optimising computational
requirements, but does so as a byproduct. This
may seem despicable but it would allow to run
more experiments with the same resources, use
larger corpora or even more complex techniques,
such as synchronous grammars or hierarchical
models. For instance, the infrequent n-grams
technique has beaten all the other systems using
just a small fraction of the corpus, only 0.5%, and
is yet able to outperform a system trained with all
the data by 0.9 BLEU points and the random base-
line by 3 points. This baseline has been proved to
be difficult to beat by other works.
Preliminary experiments were performed in or-
der to analyse the perplexity of the references, the
number of out of vocabulary words (OoVs) and
the ratio of target-source phrases. These exper-
iments revealed that the improvements obtained
are largely correlated with a decrease in perplex-
ity and in the number of OoVs. On the one hand,
reducing the amount of OoVs was mirrored by
an important improvement in BLEU when the
amount of additional data was small, and also
entailed a decrease in perplexity. However, a
reduction in perplexity by itself did not always
imply significant improvements. Moreover, no
real conclusion could be drawn from the analy-
sis of target-source phrase ratio. Hence, we un-
derstand that the improvements obtained are pro-
vided mainly by a more specialised estimation of
the model parameters. However, further experi-
ments should still be conducted in order to verify
this conclusion.
</bodyText>
<sectionHeader confidence="0.998436" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999788909090909">
The research leading to these results has re-
ceived funding from the European Union Seventh
Framework Programme (FP7/2007-2013) under
grant agreement nr. 287755. This work was
also supported by the Spanish MEC/MICINN un-
der the MIPRCV ”Consolider Ingenio 2010” pro-
gram (CSD2007-00018), and iTrans2 (TIN2009-
14511) project. Also supported by the Span-
ish MITyC under the erudito.com (TSI-020110-
2009-439) project and Instituto Tecnol´ogico de
Le´on, DGEST-PROMEP y CONACYT, M´exico.
</bodyText>
<page confidence="0.995046">
160
</page>
<sectionHeader confidence="0.993858" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999959445454546">
Sankaranarayanan Ananthakrishnan, Rohit Prasad,
David Stallard, and Prem Natarajan. 2010. Dis-
criminative sample selection for statistical machine
translation. In Proc. of the EMNLP, pages 626–635,
Cambridge, MA, October.
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain
data selection. In Proc of the EMNLP, pages 355–
362.
Christopher M. Bishop. 2006. Pattern Recognition
and Machine Learning. Springer.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1994. The mathe-
matics of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263–
311.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proc of the WSMT, pages 1–28, Athens, Greece,
March.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint Workshop on Sta-
tistical Machine Translation and Metrics for Ma-
chine Translation. In Proc. of the MATR(ACL),
pages 17–53, Uppsala, Sweden, July.
Sanjoy Dasgupta. 2009. The two faces of active learn-
ing. In Proc. of The twentieth Conference on Algo-
rithmic Learning Theory, page 1, Porto (Portugal),
October.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In Proc. of
the EMNLP, pages 451–459, Cambridge, MA, Oc-
tober.
Guillem Gasc´o, Vicent Alabau, Jes´us Andr´es-Ferrer,
Jes´us Gonz´alez-Rubio, Martha-Alicia Rocha,
Germ´an Sanchis-Trilles, Francisco Casacuberta,
Jorge Gonz´alez, and Joan-Andreu S´anchez. 2010.
ITI-UPV system description for IWSLT 2010. In
Proc. of the IWSLT 2010, Paris, France, December.
Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.
2009. Active learning for statistical phrase-based
machine translation. In Proc. of HLT/NAACL’09,
pages 415–423, Morristown, NJ, USA.
Jing Jiang and ChengXiang Zhai. 2007. Instance
weighting for domain adaptation in NLP. In Proc.
of ACL’07, pages 264–271.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. Proc.
of ICASSP, II:181–184, May.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christie
Moran, Richard Zens, Chris Dyer, Ontraj Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proc. of ACL, pages 177–180.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren Thornton, Ziyuan Wang, Jonathan
Weese, and Omar Zaidan. 2010. Joshua 2.0: A
toolkit for parsing-based machine translation with
syntax, semirings, discriminative training and other
goodies. In Proc. of the MATR(ACL), pages 139–
143, Uppsala, Sweden, July.
Yajuan Lu, Jin Huang, and Qun Liu. 2007. Improv-
ing statistical machine translation performance by
training data selection and optimization. In Proc. of
the EMNLP-CoNLL, pages 343–350, Prague, Czech
Republic, June.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight es-
timation for machine translation. In Proc. of the
EMNLP, pages 708–717, Singapore, August.
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In
ACL (Short Papers), pages 220–224.
Franz J. Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proc. of ACL, pages
295–302.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
In Computational Linguistics, volume 29, pages
19–51.
Kishore Papineni, Salim Roukos, and Todd Ward.
1998. Maximum likelihood and discriminative
training of direct translation models. In Proc. of
ICASSP’98, pages 189–192.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2001. Bleu: A method for automatic
evaluation of machine translation. In Technical Re-
port RC22176 (W0109-022).
Michael Paul, Marcello Federico, and Sebastian Stker.
2010. Overview of the IWSLT 2010 evaluation
campaign. In Proc. of the IWSLT 2010, Paris,
France, December.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In Proc. of AMTA’06.
Andreas Stolcke. 2002. SRILM – an extensible lan-
guage modeling toolkit. In Proc. of ICSLP.
Elia Yuste, Manuel Herranz, Antonio Lagarda, Li-
onel Taraz´on, Isaias S´anchez-Cortina, and Fran-
cisco Casacuberta. 2010. Pangeamt - putting
open standards to work... well. In Proc. of the
AMTA2010. Denver, CO, USA, November.
</reference>
<page confidence="0.998233">
161
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.456931">
<title confidence="0.984875">Does more data always yield better translations?</title>
<author confidence="0.954175">Guillem Gasc´o</author>
<author confidence="0.954175">Martha-Alicia Rocha</author>
<author confidence="0.954175">Germ´an Jes´us Andr´es-Ferrer</author>
<author confidence="0.954175">Francisco</author>
<affiliation confidence="0.9937925">Departament de Sistemes Inform`atics i Universitat Polit`ecnica de</affiliation>
<address confidence="0.515999">Camide Vera s/n, 46022 Val`encia,</address>
<abstract confidence="0.999389304347826">Nowadays, there are large amounts of data available to train statistical machine translation systems. However, it is not clear whether all the training data actually help or not. A system trained on a subset of such huge bilingual corpora might outperform the use of all the bilingual data. This paper studies such issues by analysing two training data selection techniques: one based on approximating the probability of an indomain corpus; and another based on inoccurrence. Experimental results not only report significant improvements over random sentence selection but also an improvement over a system trained with the whole available data. Surprisingly, the improvements are obtained with just a small fraction of the data that accounts for less than 0.5% of the sentences. Afterwards, we show that a much larger room for improvement exists, although this is done under non-realistic conditions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sankaranarayanan Ananthakrishnan</author>
<author>Rohit Prasad</author>
<author>David Stallard</author>
<author>Prem Natarajan</author>
</authors>
<title>Discriminative sample selection for statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proc. of the EMNLP,</booktitle>
<pages>626--635</pages>
<location>Cambridge, MA,</location>
<contexts>
<context position="8368" citStr="Ananthakrishnan et al., 2010" startWordPosition="1349" endWordPosition="1352">ing (AL), both problems are essentially different. Since the AL strategies assume that the pool of sentences are not translated, they are usually interested in finding the best monolingual subset of sentences to be translated by a human annotator. In contrast, in BSS, it is assumed that a fairly large amount of bilingual corpora is readily available, and the main goal consists in selecting only those sentences which will maximise system performance. Some works have applied sentence selection in small scale AL frameworks. These works extend the training corpora at most with 5000 sentences. In (Ananthakrishnan et al., 2010), sentences are selected by means of discriminative techniques. In (Haffari et al., 2009) a technique is proposed for increasing the counts of phrases that are considered infrequent. Both works significantly differ from the current work not only on the framework, but also on the scale of the experiments, the 153 proposed techniques and the obtained improvements. Similar ideas applied to adaptation problems have been proposed in (Moore and Lewis, 2010; Axelrod et al., 2011). 3 Probabilistic Sampling As discussed in Section 2, BSS has inherently attached many meaningful links with AL techniques.</context>
</contexts>
<marker>Ananthakrishnan, Prasad, Stallard, Natarajan, 2010</marker>
<rawString>Sankaranarayanan Ananthakrishnan, Rohit Prasad, David Stallard, and Prem Natarajan. 2010. Discriminative sample selection for statistical machine translation. In Proc. of the EMNLP, pages 626–635, Cambridge, MA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amittai Axelrod</author>
<author>Xiaodong He</author>
<author>Jianfeng Gao</author>
</authors>
<title>Domain adaptation via pseudo in-domain data selection.</title>
<date>2011</date>
<booktitle>In Proc of the EMNLP,</booktitle>
<pages>355--362</pages>
<contexts>
<context position="8845" citStr="Axelrod et al., 2011" startWordPosition="1428" endWordPosition="1431">e selection in small scale AL frameworks. These works extend the training corpora at most with 5000 sentences. In (Ananthakrishnan et al., 2010), sentences are selected by means of discriminative techniques. In (Haffari et al., 2009) a technique is proposed for increasing the counts of phrases that are considered infrequent. Both works significantly differ from the current work not only on the framework, but also on the scale of the experiments, the 153 proposed techniques and the obtained improvements. Similar ideas applied to adaptation problems have been proposed in (Moore and Lewis, 2010; Axelrod et al., 2011). 3 Probabilistic Sampling As discussed in Section 2, BSS has inherently attached many meaningful links with AL techniques. Selecting samples for learning our models, incurs in a well-known difficulty in AL, the so-called sample bias problem (Dasgupta, 2009). This problem, which is spread to the BSS case, is summarised as the distortion introduced by the active strategy into the probability distribution underlying the training corpus. This bias forces the training algorithm to learn a distorted probability model which can significantly differ from the actual one. In order to further analyse th</context>
</contexts>
<marker>Axelrod, He, Gao, 2011</marker>
<rawString>Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011. Domain adaptation via pseudo in-domain data selection. In Proc of the EMNLP, pages 355– 362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher M Bishop</author>
</authors>
<date>2006</date>
<booktitle>Pattern Recognition and Machine Learning.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="9903" citStr="Bishop, 2006" startWordPosition="1595" endWordPosition="1596">ias forces the training algorithm to learn a distorted probability model which can significantly differ from the actual one. In order to further analyse the sampling bias problem, consider the maximum likelihood estimation (MLE) of a probability model, pθ(e, f) for a given corpus of N data points,{(en, fn)}, sampled from the actual probability distribution, Pr(e, f). Recall that e denotes a target sentence whereas f stands for its source counterpart. MLE techniques aims at minimising the Kullback-Leibler divergence between the actual unknown probability distribution and the probability model (Bishop, 2006), defined as Pr(e, f) log (pθ(e Pr(e, f) , f) (2) When minimising, Eq. (2) is simplified to �θ� = arg max Pr(e, f) log(pθ(e, f)) (3) θ e,f which is approximated by a sufficiently large dataset under the commonly hold assumption that it is independently and identically distributed according to Pr(e, f) as �θ� = arg max θ n Therefore, by perturbing the sample {(en, fn)} with an active strategy, we are, in fact, modifying the approximation to Eq.(3) and learning a different underlying probability distribution. In this section a statistical framework is proposed to build systems with BSS while avo</context>
</contexts>
<marker>Bishop, 2006</marker>
<rawString>Christopher M. Bishop. 2006. Pattern Recognition and Machine Learning. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>311</pages>
<contexts>
<context position="12255" citStr="Brown et al., 1994" startWordPosition="1984" endWordPosition="1987">s sum up to |e|+|f |and N denotes the total number of sentences. Note that no distinction is made between source and target lengths since the model is intended for sampling. The complexity of the in-domain bilingual probability distribution, p(e, f ||e|, |f|), requires a more sophisticated approximation p(e, f/|e|, |f |) = exp Ek Lkfk(e, f)) (7) being i a normalisation constant; and where fk(...) and γk are the features of the model and their respective parametric weights. Specifically, four logarithmic features were considered for this sampling technique: a direct and an inverse IBM model 4 (Brown et al., 1994); and both, source and target, 5-gram language models. All feature models are estimated in the in-domain corpus with standard techniques (Brown et al., 1994; Stolcke, 2002). As a first approach, the parameters of the log-linear model in Eq. (7), γk, were uniformly fixed to 1. KL(Pr |pθ) = � e,f log(pθ(en, fn)) (4) 154 Once we have an appropriate model for the in-domain probability distribution, the proposed method randomly samples a given number of bilingual pairs from the out-of-domain corpora (the pool of sentences). The process of extending the in-domain corpus with additional bilingual pai</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1994</marker>
<rawString>Peter F. Brown, Stephen Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1994. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263– 311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<date>2009</date>
<booktitle>Findings of the 2009 Workshop on Statistical Machine Translation. In Proc of the WSMT,</booktitle>
<pages>1--28</pages>
<location>Athens, Greece,</location>
<contexts>
<context position="29349" citStr="Callison-Burch et al., 2009" startWordPosition="4926" endWordPosition="4929"> set. Note that test references were not included into the training data as such, but were rather used to establish which bilingual sentences within the pool were best suitable for training the SMT system. In this way, we were able to establish the potential for improvement of a BSS technique. Interestingly, the SMT system trained in this way achieved 31 BLEU points on the News Commentary 2009 test set, i.e. an 8 BLEU points improvement over the system trained with all the data available. This result would have beaten all the systems that took part in the 2009 Workshop on Machine translation (Callison-Burch et al., 2009). This result is really important: although we are aware that the sentences were selected in a nonrealistic manner, it proves that an appropriate BSS technique would be able to boost SMT performance in a very significant manner. Similar results were obtained with the TED and NC 2010 test sets, with 10 and 7 points improvement, respectively. 5.5 Example Translations Example translations are shown in Figure 4. In the first example, the baseline system is not able the budget has also been criticised by klaus . le budget a ´egalement ´et´e criticised par m. klaus . Rdm le budget a ´egalement ´et´e</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Schroeder, 2009</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Josh Schroeder. 2009. Findings of the 2009 Workshop on Statistical Machine Translation. In Proc of the WSMT, pages 1–28, Athens, Greece, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Kay Peterson</author>
<author>Mark Przybocki</author>
<author>Omar Zaidan</author>
</authors>
<date>2010</date>
<booktitle>Findings of the 2010 joint Workshop on Statistical Machine Translation and Metrics for Machine Translation. In Proc. of the MATR(ACL),</booktitle>
<pages>17--53</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="1957" citStr="Callison-Burch et al., 2010" startWordPosition="281" endWordPosition="284">et have lead to a rapid increase in the amount of bilingual corpora available. Entities such as the European Union, the United Nations and other multinational organisations need to translate all the documentation they generate. Such translations happen every day and provide very large multilingual corpora, which are oftentimes difficult to process and significantly increase the computational requirements needed to train statistical machine translation (SMT) systems. For instance, the corpora made available for recent machine translation evaluations are in the order of 1 billion running words (Callison-Burch et al., 2010). However, two main problems arise when attempting to use this huge pool of sentences for training SMT systems: firstly, a large portion of this data is obtained from domains that differ from that in which the SMT system is to be used or assessed; secondly, the use of all this data for training the system increases the computational training requirements. Despite the previous remarks, the de facto standard consists in training SMT systems with all the available data. This is due to the widespread misconception that the more data a system is trained with, the better its performance should be. A</context>
<context position="19901" citStr="Callison-Burch et al., 2010" startWordPosition="3296" endWordPosition="3299">r vocabulary size. |S ||W ||V | 77.2K 1.71M 29.9K 1.99M 48K 2.1K 49.8K 8.7K 55.4K 7.7K 2.5K 65.6K 8.9K 72.5K 10.6K 2.5K 62K 8.9K 70.5K 10.3K Table 3: News Commentary corpus main figures. GIZA++ (Och and Ney, 2003). The language model used was a 5-gram with modified KneserNey smoothing (Kneser and Ney, 1995), built with SRILM toolkit (Stolcke, 2002). The loglinear combination weights in Eq. (1) were optimised using Minimum Error Rate Training (Och and Ney, 2002) on the corresponding development sets. Experiments were carried out on two corpora: TED (Paul et al., 2010) and News Commentary (NC) (Callison-Burch et al., 2010). TED is an English-French corpus composed of subtitles for a collection of public speeches on a variety of topics. The same partitions as in the IWSLT2010 evaluation task (Paul et al., 2010) have been used. Subtitles have been concatenated into complete sentences. NC is a slightly larger English-French corpus in the news domain. Main figures of both corpora are shown in Tables 2 and 3. As for the pool of sentences, three large corpora have been used: Europarl (Euro), United Nations (UN) and Gigaword (Giga), in the partition established for the 2010 workshop on SMT of the ACL (CallisonBurch et</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Peterson, Przybocki, Zaidan, 2010</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Kay Peterson, Mark Przybocki, and Omar Zaidan. 2010. Findings of the 2010 joint Workshop on Statistical Machine Translation and Metrics for Machine Translation. In Proc. of the MATR(ACL), pages 17–53, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjoy Dasgupta</author>
</authors>
<title>The two faces of active learning.</title>
<date>2009</date>
<booktitle>In Proc. of The twentieth Conference on Algorithmic Learning Theory,</booktitle>
<pages>page</pages>
<location>Porto</location>
<contexts>
<context position="9103" citStr="Dasgupta, 2009" startWordPosition="1470" endWordPosition="1471">creasing the counts of phrases that are considered infrequent. Both works significantly differ from the current work not only on the framework, but also on the scale of the experiments, the 153 proposed techniques and the obtained improvements. Similar ideas applied to adaptation problems have been proposed in (Moore and Lewis, 2010; Axelrod et al., 2011). 3 Probabilistic Sampling As discussed in Section 2, BSS has inherently attached many meaningful links with AL techniques. Selecting samples for learning our models, incurs in a well-known difficulty in AL, the so-called sample bias problem (Dasgupta, 2009). This problem, which is spread to the BSS case, is summarised as the distortion introduced by the active strategy into the probability distribution underlying the training corpus. This bias forces the training algorithm to learn a distorted probability model which can significantly differ from the actual one. In order to further analyse the sampling bias problem, consider the maximum likelihood estimation (MLE) of a probability model, pθ(e, f) for a given corpus of N data points,{(en, fn)}, sampled from the actual probability distribution, Pr(e, f). Recall that e denotes a target sentence whe</context>
</contexts>
<marker>Dasgupta, 2009</marker>
<rawString>Sanjoy Dasgupta. 2009. The two faces of active learning. In Proc. of The twentieth Conference on Algorithmic Learning Theory, page 1, Porto (Portugal), October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Cyril Goutte</author>
<author>Roland Kuhn</author>
</authors>
<title>Discriminative instance weighting for domain adaptation in statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proc. of the EMNLP,</booktitle>
<pages>451--459</pages>
<location>Cambridge, MA,</location>
<contexts>
<context position="32846" citStr="Foster et al., 2010" startWordPosition="5535" endWordPosition="5538">ieved by means of a (potentially small) adaptation set, which belongs to the same domain as the test data. In contrast, BSS tackles with the problem of how to select samples from a large pool of training data, regardless of whether such pool of data is in-domain or out-of-domain. Hence, in one case we can assume to have a fairly well estimated translation model, which is to be adapted, whereas in BSS we still have full control over the estimation of such model and need not to aim at a specific domain, although it might often be so. BSS is related with instance weighting (Jiang and Zhai, 2007; Foster et al., 2010). Adaptation and BSS can be considered to be orthogonal (yet complementary) problems under the instance weighting paradigm. In such case, instance weighting can be considered to span a complete paradigmatic space between both. At one end, there is sample selection (BSS for SMT), while at the other end there is adaptation. For instance, it is quite common to confront the adaptation problem by extracting different phrase-tables from different corpora, and then interpolate such tables. This technique could be also applied to promote the performance of the system built by means of BSS. However, th</context>
</contexts>
<marker>Foster, Goutte, Kuhn, 2010</marker>
<rawString>George Foster, Cyril Goutte, and Roland Kuhn. 2010. Discriminative instance weighting for domain adaptation in statistical machine translation. In Proc. of the EMNLP, pages 451–459, Cambridge, MA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guillem Gasc´o</author>
<author>Vicent Alabau</author>
<author>Jes´us Andr´es-Ferrer</author>
<author>Jes´us Gonz´alez-Rubio</author>
<author>Martha-Alicia Rocha</author>
<author>Germ´an Sanchis-Trilles</author>
<author>Francisco Casacuberta</author>
<author>Jorge Gonz´alez</author>
<author>Joan-Andreu S´anchez</author>
</authors>
<title>ITI-UPV system description for IWSLT</title>
<date>2010</date>
<booktitle>In Proc. of the IWSLT 2010,</booktitle>
<location>Paris, France,</location>
<marker>Gasc´o, Alabau, Andr´es-Ferrer, Gonz´alez-Rubio, Rocha, Sanchis-Trilles, Casacuberta, Gonz´alez, S´anchez, 2010</marker>
<rawString>Guillem Gasc´o, Vicent Alabau, Jes´us Andr´es-Ferrer, Jes´us Gonz´alez-Rubio, Martha-Alicia Rocha, Germ´an Sanchis-Trilles, Francisco Casacuberta, Jorge Gonz´alez, and Joan-Andreu S´anchez. 2010. ITI-UPV system description for IWSLT 2010. In Proc. of the IWSLT 2010, Paris, France, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gholamreza Haffari</author>
<author>Maxim Roy</author>
<author>Anoop Sarkar</author>
</authors>
<title>Active learning for statistical phrase-based machine translation.</title>
<date>2009</date>
<booktitle>In Proc. of HLT/NAACL’09,</booktitle>
<pages>415--423</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="8457" citStr="Haffari et al., 2009" startWordPosition="1362" endWordPosition="1365">f sentences are not translated, they are usually interested in finding the best monolingual subset of sentences to be translated by a human annotator. In contrast, in BSS, it is assumed that a fairly large amount of bilingual corpora is readily available, and the main goal consists in selecting only those sentences which will maximise system performance. Some works have applied sentence selection in small scale AL frameworks. These works extend the training corpora at most with 5000 sentences. In (Ananthakrishnan et al., 2010), sentences are selected by means of discriminative techniques. In (Haffari et al., 2009) a technique is proposed for increasing the counts of phrases that are considered infrequent. Both works significantly differ from the current work not only on the framework, but also on the scale of the experiments, the 153 proposed techniques and the obtained improvements. Similar ideas applied to adaptation problems have been proposed in (Moore and Lewis, 2010; Axelrod et al., 2011). 3 Probabilistic Sampling As discussed in Section 2, BSS has inherently attached many meaningful links with AL techniques. Selecting samples for learning our models, incurs in a well-known difficulty in AL, the </context>
</contexts>
<marker>Haffari, Roy, Sarkar, 2009</marker>
<rawString>Gholamreza Haffari, Maxim Roy, and Anoop Sarkar. 2009. Active learning for statistical phrase-based machine translation. In Proc. of HLT/NAACL’09, pages 415–423, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Instance weighting for domain adaptation in NLP.</title>
<date>2007</date>
<booktitle>In Proc. of ACL’07,</booktitle>
<pages>264--271</pages>
<contexts>
<context position="32824" citStr="Jiang and Zhai, 2007" startWordPosition="5531" endWordPosition="5534">ess is ought to be achieved by means of a (potentially small) adaptation set, which belongs to the same domain as the test data. In contrast, BSS tackles with the problem of how to select samples from a large pool of training data, regardless of whether such pool of data is in-domain or out-of-domain. Hence, in one case we can assume to have a fairly well estimated translation model, which is to be adapted, whereas in BSS we still have full control over the estimation of such model and need not to aim at a specific domain, although it might often be so. BSS is related with instance weighting (Jiang and Zhai, 2007; Foster et al., 2010). Adaptation and BSS can be considered to be orthogonal (yet complementary) problems under the instance weighting paradigm. In such case, instance weighting can be considered to span a complete paradigmatic space between both. At one end, there is sample selection (BSS for SMT), while at the other end there is adaptation. For instance, it is quite common to confront the adaptation problem by extracting different phrase-tables from different corpora, and then interpolate such tables. This technique could be also applied to promote the performance of the system built by mea</context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in NLP. In Proc. of ACL’07, pages 264–271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>Proc. of ICASSP, II:181–184,</booktitle>
<contexts>
<context position="19581" citStr="Kneser and Ney, 1995" startWordPosition="3244" endWordPosition="3247">oolkit Moses (Koehn et al., 2007), in its standard non-monotonic configuration. The phrase tables were generated by means of symmetrised word alignments obtained with Table 2: TED corpus main figures. K denotes thousands of elements. |S |stands for number of sentences, |W |for number of running words, and |V |for vocabulary size. |S ||W ||V | 77.2K 1.71M 29.9K 1.99M 48K 2.1K 49.8K 8.7K 55.4K 7.7K 2.5K 65.6K 8.9K 72.5K 10.6K 2.5K 62K 8.9K 70.5K 10.3K Table 3: News Commentary corpus main figures. GIZA++ (Och and Ney, 2003). The language model used was a 5-gram with modified KneserNey smoothing (Kneser and Ney, 1995), built with SRILM toolkit (Stolcke, 2002). The loglinear combination weights in Eq. (1) were optimised using Minimum Error Rate Training (Och and Ney, 2002) on the corresponding development sets. Experiments were carried out on two corpora: TED (Paul et al., 2010) and News Commentary (NC) (Callison-Burch et al., 2010). TED is an English-French corpus composed of subtitles for a collection of public speeches on a variety of topics. The same partitions as in the IWSLT2010 evaluation task (Paul et al., 2010) have been used. Subtitles have been concatenated into complete sentences. NC is a slight</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. Proc. of ICASSP, II:181–184, May.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Hieu Hoang,</title>
<location>Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christie</location>
<marker>Koehn, </marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christie</rawString>
</citation>
<citation valid="false">
<authors>
<author>Richard Zens Moran</author>
</authors>
<institution>Chris Dyer, Ontraj Bojar,</institution>
<marker>Moran, </marker>
<rawString>Moran, Richard Zens, Chris Dyer, Ontraj Bojar,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>177--180</pages>
<marker>Constantin, Herbst, 2007</marker>
<rawString>Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. of ACL, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Chris Callison-Burch</author>
<author>Chris Dyer</author>
<author>Juri Ganitkevitch</author>
<author>Ann Irvine</author>
<author>Sanjeev Khudanpur</author>
<author>Lane Schwartz</author>
<author>Wren Thornton</author>
<author>Ziyuan Wang</author>
<author>Jonathan Weese</author>
<author>Omar Zaidan</author>
</authors>
<title>Joshua 2.0: A toolkit for parsing-based machine translation with syntax, semirings, discriminative training and other goodies.</title>
<date>2010</date>
<booktitle>In Proc. of the MATR(ACL),</booktitle>
<pages>139--143</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="6692" citStr="Li et al., 2010" startWordPosition="1079" endWordPosition="1082"> a wise BSS technique can yield large improvements when compared with systems trained with all data available. The remaining of the paper is structured as follows. Section 2 summarises the related work. Sections 3 and 4 present two BSS techniques, namely, probabilistic sampling and recovery of infrequent n-grams. In Section 5 experimental results are reported. Finally, the main results of the work and several future work directions are discussed in Section 6. 2 Related Work Training data selection has been receiving an increasing amount of attention within the SMT community. For instance, in (Li et al., 2010; Gasc´o et al., 2010) several BSS techniques, similar to those analysed in this paper, have been applied for training MT systems when there are large training corpora available. However, neither such techniques have been formalised, nor its performance thoroughly analysed. A similar approach that gives weights to different subcorpora was proposed in (Matsoukas et al., 2009). In (Lu et al., 2007), information retrieval methods are used in order to produce different submodels which are then weighted according to the sentence to be translated. In such work, authors define the baseline as the res</context>
</contexts>
<marker>Li, Callison-Burch, Dyer, Ganitkevitch, Irvine, Khudanpur, Schwartz, Thornton, Wang, Weese, Zaidan, 2010</marker>
<rawString>Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Ganitkevitch, Ann Irvine, Sanjeev Khudanpur, Lane Schwartz, Wren Thornton, Ziyuan Wang, Jonathan Weese, and Omar Zaidan. 2010. Joshua 2.0: A toolkit for parsing-based machine translation with syntax, semirings, discriminative training and other goodies. In Proc. of the MATR(ACL), pages 139– 143, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yajuan Lu</author>
<author>Jin Huang</author>
<author>Qun Liu</author>
</authors>
<title>Improving statistical machine translation performance by training data selection and optimization.</title>
<date>2007</date>
<booktitle>In Proc. of the EMNLP-CoNLL,</booktitle>
<pages>343--350</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="7091" citStr="Lu et al., 2007" startWordPosition="1143" endWordPosition="1146">rk and several future work directions are discussed in Section 6. 2 Related Work Training data selection has been receiving an increasing amount of attention within the SMT community. For instance, in (Li et al., 2010; Gasc´o et al., 2010) several BSS techniques, similar to those analysed in this paper, have been applied for training MT systems when there are large training corpora available. However, neither such techniques have been formalised, nor its performance thoroughly analysed. A similar approach that gives weights to different subcorpora was proposed in (Matsoukas et al., 2009). In (Lu et al., 2007), information retrieval methods are used in order to produce different submodels which are then weighted according to the sentence to be translated. In such work, authors define the baseline as the result obtained training only with the corpus that share the same domain of the test. Afterwards they claim that they are able to improve baseline translation quality by adding new sentences retrieved with their method. However, they neither compare their technique with random sentence selection, nor with a model trained with all the corpora. Although the techniques that are applied for BSS are ofte</context>
</contexts>
<marker>Lu, Huang, Liu, 2007</marker>
<rawString>Yajuan Lu, Jin Huang, and Qun Liu. 2007. Improving statistical machine translation performance by training data selection and optimization. In Proc. of the EMNLP-CoNLL, pages 343–350, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spyros Matsoukas</author>
<author>Antti-Veikko I Rosti</author>
<author>Bing Zhang</author>
</authors>
<title>Discriminative corpus weight estimation for machine translation.</title>
<date>2009</date>
<booktitle>In Proc. of the EMNLP,</booktitle>
<pages>708--717</pages>
<location>Singapore,</location>
<contexts>
<context position="7069" citStr="Matsoukas et al., 2009" startWordPosition="1138" endWordPosition="1141">y, the main results of the work and several future work directions are discussed in Section 6. 2 Related Work Training data selection has been receiving an increasing amount of attention within the SMT community. For instance, in (Li et al., 2010; Gasc´o et al., 2010) several BSS techniques, similar to those analysed in this paper, have been applied for training MT systems when there are large training corpora available. However, neither such techniques have been formalised, nor its performance thoroughly analysed. A similar approach that gives weights to different subcorpora was proposed in (Matsoukas et al., 2009). In (Lu et al., 2007), information retrieval methods are used in order to produce different submodels which are then weighted according to the sentence to be translated. In such work, authors define the baseline as the result obtained training only with the corpus that share the same domain of the test. Afterwards they claim that they are able to improve baseline translation quality by adding new sentences retrieved with their method. However, they neither compare their technique with random sentence selection, nor with a model trained with all the corpora. Although the techniques that are ap</context>
</contexts>
<marker>Matsoukas, Rosti, Zhang, 2009</marker>
<rawString>Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing Zhang. 2009. Discriminative corpus weight estimation for machine translation. In Proc. of the EMNLP, pages 708–717, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>William Lewis</author>
</authors>
<title>Intelligent selection of language model training data.</title>
<date>2010</date>
<booktitle>In ACL (Short Papers),</booktitle>
<pages>220--224</pages>
<contexts>
<context position="8822" citStr="Moore and Lewis, 2010" startWordPosition="1424" endWordPosition="1427">ks have applied sentence selection in small scale AL frameworks. These works extend the training corpora at most with 5000 sentences. In (Ananthakrishnan et al., 2010), sentences are selected by means of discriminative techniques. In (Haffari et al., 2009) a technique is proposed for increasing the counts of phrases that are considered infrequent. Both works significantly differ from the current work not only on the framework, but also on the scale of the experiments, the 153 proposed techniques and the obtained improvements. Similar ideas applied to adaptation problems have been proposed in (Moore and Lewis, 2010; Axelrod et al., 2011). 3 Probabilistic Sampling As discussed in Section 2, BSS has inherently attached many meaningful links with AL techniques. Selecting samples for learning our models, incurs in a well-known difficulty in AL, the so-called sample bias problem (Dasgupta, 2009). This problem, which is spread to the BSS case, is summarised as the distortion introduced by the active strategy into the probability distribution underlying the training corpus. This bias forces the training algorithm to learn a distorted probability model which can significantly differ from the actual one. In orde</context>
</contexts>
<marker>Moore, Lewis, 2010</marker>
<rawString>Robert C. Moore and William Lewis. 2010. Intelligent selection of language model training data. In ACL (Short Papers), pages 220–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>295--302</pages>
<contexts>
<context position="5140" citStr="Och and Ney, 2002" startWordPosition="813" endWordPosition="816">. Note that although we are not able to achieve such an improvement without an oracle, this result restates the BSS problem as an interesting approach not only for reducing computational effort but also for significantly boosting performance. To our knowledge, no previous work has quantified the room of improvement in which BSS techniques could incur. In order to assess the performance of the different BSS techniques, translation results are obtained by using a standard state-of-the-art SMT system (Koehn et al., 2007). The most recent literature defines the SMT problem (Papineni et al., 1998; Och and Ney, 2002) as follows: given an input sentence f from a certain source language, the purpose is to find an output sentence e� in a certain target language such that K e� = arg max Akhk(f,e) (1) e k=1 where hk(f, e) is a score function representing an important feature for the translation of f into e, as for example the language model of the target language, a reordering model or several translation models. Ak are the log-linear combination weights. The main contributions of this paper are: • A BSS technique is analysed, which improves the results obtained with a random bilingual sentence selection strat</context>
<context position="19738" citStr="Och and Ney, 2002" startWordPosition="3270" endWordPosition="3273">d with Table 2: TED corpus main figures. K denotes thousands of elements. |S |stands for number of sentences, |W |for number of running words, and |V |for vocabulary size. |S ||W ||V | 77.2K 1.71M 29.9K 1.99M 48K 2.1K 49.8K 8.7K 55.4K 7.7K 2.5K 65.6K 8.9K 72.5K 10.6K 2.5K 62K 8.9K 70.5K 10.3K Table 3: News Commentary corpus main figures. GIZA++ (Och and Ney, 2003). The language model used was a 5-gram with modified KneserNey smoothing (Kneser and Ney, 1995), built with SRILM toolkit (Stolcke, 2002). The loglinear combination weights in Eq. (1) were optimised using Minimum Error Rate Training (Och and Ney, 2002) on the corresponding development sets. Experiments were carried out on two corpora: TED (Paul et al., 2010) and News Commentary (NC) (Callison-Burch et al., 2010). TED is an English-French corpus composed of subtitles for a collection of public speeches on a variety of topics. The same partitions as in the IWSLT2010 evaluation task (Paul et al., 2010) have been used. Subtitles have been concatenated into complete sentences. NC is a slightly larger English-French corpus in the news domain. Main figures of both corpora are shown in Tables 2 and 3. As for the pool of sentences, three large corpo</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz J. Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proc. of ACL, pages 295–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<booktitle>In Computational Linguistics,</booktitle>
<volume>29</volume>
<pages>pages</pages>
<contexts>
<context position="19486" citStr="Och and Ney, 2003" startWordPosition="3228" endWordPosition="3231">general. 5.1 Experimental Setup All experiments were carried out using the open-source SMT toolkit Moses (Koehn et al., 2007), in its standard non-monotonic configuration. The phrase tables were generated by means of symmetrised word alignments obtained with Table 2: TED corpus main figures. K denotes thousands of elements. |S |stands for number of sentences, |W |for number of running words, and |V |for vocabulary size. |S ||W ||V | 77.2K 1.71M 29.9K 1.99M 48K 2.1K 49.8K 8.7K 55.4K 7.7K 2.5K 65.6K 8.9K 72.5K 10.6K 2.5K 62K 8.9K 70.5K 10.3K Table 3: News Commentary corpus main figures. GIZA++ (Och and Ney, 2003). The language model used was a 5-gram with modified KneserNey smoothing (Kneser and Ney, 1995), built with SRILM toolkit (Stolcke, 2002). The loglinear combination weights in Eq. (1) were optimised using Minimum Error Rate Training (Och and Ney, 2002) on the corresponding development sets. Experiments were carried out on two corpora: TED (Paul et al., 2010) and News Commentary (NC) (Callison-Burch et al., 2010). TED is an English-French corpus composed of subtitles for a collection of public speeches on a variety of topics. The same partitions as in the IWSLT2010 evaluation task (Paul et al.,</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz J. Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. In Computational Linguistics, volume 29, pages 19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
</authors>
<title>Maximum likelihood and discriminative training of direct translation models.</title>
<date>1998</date>
<booktitle>In Proc. of ICASSP’98,</booktitle>
<pages>189--192</pages>
<contexts>
<context position="5120" citStr="Papineni et al., 1998" startWordPosition="809" endWordPosition="812">btaining the hypotheses. Note that although we are not able to achieve such an improvement without an oracle, this result restates the BSS problem as an interesting approach not only for reducing computational effort but also for significantly boosting performance. To our knowledge, no previous work has quantified the room of improvement in which BSS techniques could incur. In order to assess the performance of the different BSS techniques, translation results are obtained by using a standard state-of-the-art SMT system (Koehn et al., 2007). The most recent literature defines the SMT problem (Papineni et al., 1998; Och and Ney, 2002) as follows: given an input sentence f from a certain source language, the purpose is to find an output sentence e� in a certain target language such that K e� = arg max Akhk(f,e) (1) e k=1 where hk(f, e) is a score function representing an important feature for the translation of f into e, as for example the language model of the target language, a reordering model or several translation models. Ak are the log-linear combination weights. The main contributions of this paper are: • A BSS technique is analysed, which improves the results obtained with a random bilingual sent</context>
</contexts>
<marker>Papineni, Roukos, Ward, 1998</marker>
<rawString>Kishore Papineni, Salim Roukos, and Todd Ward. 1998. Maximum likelihood and discriminative training of direct translation models. In Proc. of ICASSP’98, pages 189–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>Bleu: A method for automatic evaluation of machine translation. In</title>
<date>2001</date>
<tech>Technical Report RC22176 (W0109-022).</tech>
<contexts>
<context position="22418" citStr="Papineni et al., 2001" startWordPosition="3708" endWordPosition="3711">included results for a purely random sentence selection without replacement. In the plots, each point corresponding to random selection represent the average of 10 repetitions. Experiments using all data are also reported, although a 64GB machine was necessary, even with binarized phrase and distortion tables. Experiments were conducted by selecting a fixed amount of sentences according to each one of the techniques described above. Then, these sentences were included into the training data and subsequent SMT systems were built for translating the test set. Results are shown in terms of BLEU (Papineni et al., 2001), which is an accuracy metric that measures n-gram precision, with a penalty for sentences that are too short. Although it could be argued that improvements obtained might be due to a side effect of the brevity penalty, this was not found to be true: the BSS techniques (including random) and considering all data yielded very similar brevity penalties (±0.005), within each corpus. In addition, TER scores (Snover et al., 2006) were also computed, but are omitted for clarity purposes and since they were found to be coherent with BLEU. TER is an error metric that computes the minimum number of edi</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2001. Bleu: A method for automatic evaluation of machine translation. In Technical Report RC22176 (W0109-022).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Paul</author>
<author>Marcello Federico</author>
<author>Sebastian Stker</author>
</authors>
<title>evaluation campaign.</title>
<date>2010</date>
<journal>Overview of the IWSLT</journal>
<booktitle>In Proc. of the IWSLT 2010,</booktitle>
<location>Paris, France,</location>
<contexts>
<context position="19846" citStr="Paul et al., 2010" startWordPosition="3288" endWordPosition="3291">, |W |for number of running words, and |V |for vocabulary size. |S ||W ||V | 77.2K 1.71M 29.9K 1.99M 48K 2.1K 49.8K 8.7K 55.4K 7.7K 2.5K 65.6K 8.9K 72.5K 10.6K 2.5K 62K 8.9K 70.5K 10.3K Table 3: News Commentary corpus main figures. GIZA++ (Och and Ney, 2003). The language model used was a 5-gram with modified KneserNey smoothing (Kneser and Ney, 1995), built with SRILM toolkit (Stolcke, 2002). The loglinear combination weights in Eq. (1) were optimised using Minimum Error Rate Training (Och and Ney, 2002) on the corresponding development sets. Experiments were carried out on two corpora: TED (Paul et al., 2010) and News Commentary (NC) (Callison-Burch et al., 2010). TED is an English-French corpus composed of subtitles for a collection of public speeches on a variety of topics. The same partitions as in the IWSLT2010 evaluation task (Paul et al., 2010) have been used. Subtitles have been concatenated into complete sentences. NC is a slightly larger English-French corpus in the news domain. Main figures of both corpora are shown in Tables 2 and 3. As for the pool of sentences, three large corpora have been used: Europarl (Euro), United Nations (UN) and Gigaword (Giga), in the partition established fo</context>
</contexts>
<marker>Paul, Federico, Stker, 2010</marker>
<rawString>Michael Paul, Marcello Federico, and Sebastian Stker. 2010. Overview of the IWSLT 2010 evaluation campaign. In Proc. of the IWSLT 2010, Paris, France, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proc. of AMTA’06.</booktitle>
<contexts>
<context position="22846" citStr="Snover et al., 2006" startWordPosition="3779" endWordPosition="3782">above. Then, these sentences were included into the training data and subsequent SMT systems were built for translating the test set. Results are shown in terms of BLEU (Papineni et al., 2001), which is an accuracy metric that measures n-gram precision, with a penalty for sentences that are too short. Although it could be argued that improvements obtained might be due to a side effect of the brevity penalty, this was not found to be true: the BSS techniques (including random) and considering all data yielded very similar brevity penalties (±0.005), within each corpus. In addition, TER scores (Snover et al., 2006) were also computed, but are omitted for clarity purposes and since they were found to be coherent with BLEU. TER is an error metric that computes the minimum number of edits required to modify the system hypotheses so that they match the references translations. 0 10 20 30 40 50 60 70 80 90 100 Combined sentence length Figure 2: Combined length relative frequency. 5.2 Results for Probabilistic Sampling In addition to the probabilistic sampling technique proposed in Section 3, we also analysed the effect of sampling only according to the combined source-reference length, with the purpose of es</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proc. of AMTA’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. of ICSLP.</booktitle>
<contexts>
<context position="12427" citStr="Stolcke, 2002" startWordPosition="2014" endWordPosition="2015"> complexity of the in-domain bilingual probability distribution, p(e, f ||e|, |f|), requires a more sophisticated approximation p(e, f/|e|, |f |) = exp Ek Lkfk(e, f)) (7) being i a normalisation constant; and where fk(...) and γk are the features of the model and their respective parametric weights. Specifically, four logarithmic features were considered for this sampling technique: a direct and an inverse IBM model 4 (Brown et al., 1994); and both, source and target, 5-gram language models. All feature models are estimated in the in-domain corpus with standard techniques (Brown et al., 1994; Stolcke, 2002). As a first approach, the parameters of the log-linear model in Eq. (7), γk, were uniformly fixed to 1. KL(Pr |pθ) = � e,f log(pθ(en, fn)) (4) 154 Once we have an appropriate model for the in-domain probability distribution, the proposed method randomly samples a given number of bilingual pairs from the out-of-domain corpora (the pool of sentences). The process of extending the in-domain corpus with additional bilingual pairs from the out-of-domain corpus is summarised as follows: • Decide according to the in-domain length probability in Eq. (6), how many samples should be drawn for each leng</context>
<context position="19623" citStr="Stolcke, 2002" startWordPosition="3252" endWordPosition="3253">d non-monotonic configuration. The phrase tables were generated by means of symmetrised word alignments obtained with Table 2: TED corpus main figures. K denotes thousands of elements. |S |stands for number of sentences, |W |for number of running words, and |V |for vocabulary size. |S ||W ||V | 77.2K 1.71M 29.9K 1.99M 48K 2.1K 49.8K 8.7K 55.4K 7.7K 2.5K 65.6K 8.9K 72.5K 10.6K 2.5K 62K 8.9K 70.5K 10.3K Table 3: News Commentary corpus main figures. GIZA++ (Och and Ney, 2003). The language model used was a 5-gram with modified KneserNey smoothing (Kneser and Ney, 1995), built with SRILM toolkit (Stolcke, 2002). The loglinear combination weights in Eq. (1) were optimised using Minimum Error Rate Training (Och and Ney, 2002) on the corresponding development sets. Experiments were carried out on two corpora: TED (Paul et al., 2010) and News Commentary (NC) (Callison-Burch et al., 2010). TED is an English-French corpus composed of subtitles for a collection of public speeches on a variety of topics. The same partitions as in the IWSLT2010 evaluation task (Paul et al., 2010) have been used. Subtitles have been concatenated into complete sentences. NC is a slightly larger English-French corpus in the new</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – an extensible language modeling toolkit. In Proc. of ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Yuste</author>
<author>Manuel Herranz</author>
<author>Antonio Lagarda</author>
<author>Lionel Taraz´on</author>
<author>Isaias S´anchez-Cortina</author>
<author>Francisco Casacuberta</author>
</authors>
<title>Pangeamt - putting open standards to work... well.</title>
<date>2010</date>
<booktitle>In Proc. of the AMTA2010.</booktitle>
<location>Denver, CO, USA,</location>
<marker>Yuste, Herranz, Lagarda, Taraz´on, S´anchez-Cortina, Casacuberta, 2010</marker>
<rawString>Elia Yuste, Manuel Herranz, Antonio Lagarda, Lionel Taraz´on, Isaias S´anchez-Cortina, and Francisco Casacuberta. 2010. Pangeamt - putting open standards to work... well. In Proc. of the AMTA2010. Denver, CO, USA, November.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>