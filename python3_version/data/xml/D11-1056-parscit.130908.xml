<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000393">
<title confidence="0.993512">
Non-parametric Bayesian Segmentation of Japanese Noun Phrases
</title>
<author confidence="0.985939">
Yugo Murawaki and Sadao Kurohashi
</author>
<affiliation confidence="0.9904725">
Graduate School of Informatics
Kyoto University
</affiliation>
<email confidence="0.994597">
{murawaki, kuro}@i.kyoto-u.ac.jp
</email>
<sectionHeader confidence="0.998546" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999567318181818">
A key factor of high quality word segmenta-
tion for Japanese is a high-coverage dictio-
nary, but it is costly to manually build such
a lexical resource. Although external lexical
resources for human readers are potentially
good knowledge sources, they have not been
utilized due to differences in segmentation cri-
teria. To supplement a morphological dictio-
nary with these resources, we propose a new
task of Japanese noun phrase segmentation.
We apply non-parametric Bayesian language
models to segment each noun phrase in these
resources according to the statistical behavior
of its supposed constituents in text. For in-
ference, we propose a novel block sampling
procedure named hybrid type-based sampling,
which has the ability to directly escape a lo-
cal optimum that is not too distant from the
global optimum. Experiments show that the
proposed method efficiently corrects the initial
segmentation given by a morphological ana-
lyzer.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999158909090909">
Word segmentation is the first step of natural lan-
guage processing for Japanese, Chinese and Thai
because they do not delimit words by white-space.
Segmentation for Japanese is a successful field of re-
search, achieving the F-score of nearly 99% (Kudo
et al., 2004). This success rests on a high-coverage
dictionary. Unknown words, or words not covered
by the dictionary, are often misidentified.
Historically, researchers have devoted exten-
sive human resources to build and maintain high-
coverage dictionaries (Yokoi, 1995). Since the or-
thography of Japanese does not specify a standard
for segmentation, researchers define their own crite-
ria before constructing lexical resources. For this
reason, it is difficult to exploit existing external
resources, such as dictionaries and encyclopedias
for human readers, where entry words are not seg-
mented according to the criteria. Among them,
encyclopedias are especially important in that they
contain a lot of terms that a morphological dictio-
nary fails to cover. Most of these terms are noun
phrases and consist of more than one word (mor-
pheme). For example, an encyclopedia has an en-
try “常山城” (tsuneyama-jou, “Tsuneyama Castle”).
According to our segmentation criteria, it consists
of two words “常山” (tsuneyama) and “城” (jou).
However, the morphological analyzer wrongly seg-
ments it into “常” (tsune) and “山城” (yamashiro)
because “常山” (tsuneyama) is an unknown word.
In this paper, we present the first attempt to uti-
lize encyclopedias for word segmentation. We seg-
ment each entry noun phrase into words. To do this,
we examine the main text of the entry, on the as-
sumption that if the noun phrase in question con-
sists of more than one word, its constituents appear
in the main text either freely or as part of other
noun phrases. For “常山城” (tsuneyama-jou), its
constituent “常山” (tsune) appears by itself and as
constituents of other nouns phrases such as “常山山
頂” (peak of Tsuneyama) and “常山駅” (Tsuneyama
Station) while “山城” (yamashiro) does not.
To segment each noun phrase, we use non-
parametric Bayesian language models (Goldwater et
al., 2009; Mochihashi et al., 2009). Our approach
</bodyText>
<page confidence="0.983077">
605
</page>
<note confidence="0.957952">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 605–615,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999812909090909">
is based on two key factors: the bigram model and
type-based block sampling. The bigram model al-
leviates a problem of the unigram model, that is, a
tendency to misidentify a sequence of words in com-
mon collocations as a single word. Type-based sam-
pling (Liang et al., 2010) has the ability to directly
escape a local optimum, making inference very ef-
ficient. However, type-based sampling is not easily
applicable to the bigram model owing to sparsity and
its dependence on latent assignments.
We propose a hybrid type-based sampling proce-
dure, which combines the Metropolis-Hastings al-
gorithm with Gibbs sampling. We circumvent the
sparsity problem by joint sampling of unigram-level
type. Also, instead of calculating the probability of
every possible state of the jointly sampled random
variables, we only compare the current state with
a proposed state. This greatly eases the sampling
procedure while retaining the efficiency of type-
based sampling. Experiments show that the pro-
posed method quickly corrects the initial segmen-
tation given by a morphological analyzer.
</bodyText>
<sectionHeader confidence="0.999963" genericHeader="introduction">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.940877">
Japanese Morphological Analysis and Lexical
</subsectionHeader>
<bodyText confidence="0.999839623188406">
Acquisition Word segmentation for Japanese is
usually solved as the joint task of segmentation and
part-of-speech tagging, which is called morpholog-
ical analysis (Kurohashi et al., 1994; Asahara and
Matsumoto, 2000; Kudo et al., 2004). The stan-
dard approach in Japanese morphological analysis
is lattice-based path selection instead of character-
based IOB tagging. Given a sentence, an analyzer
first builds a lattice of words with dictionary look-up
and then selects an optimal path using pre-defined
parameters. This approach enables fast decoding
and achieves accuracy high enough for practical use.
This success, however, depends on a high-
coverage dictionary, and unknown words are often
misidentified. Although a line of research attempts
to identify unknown words on the fly (Uchimoto et
al., 2001; Asahara and Matsumoto, 2004), it by no
means provides a definitive solution because it suf-
fers from locality of contextual information avail-
able for identification (Nakagawa and Matsumoto,
2006). Therefore we like to perform separate lexical
acquisition processes in which wider context can be
examined.
Our approach in this paper has a complementary
relationship with unknown word acquisition from
text, which we previously proposed (Murawaki and
Kurohashi, 2008). Since, unlike Chinese and Thai,
Japanese is rich in morphology, morphological reg-
ularity can be used to determine if an unknown
word candidate in text is indeed the word to be ac-
quired. In general, this method works pretty well,
but one exception is noun phrases. Noun phrases
can hardly be distinguished from single nouns be-
cause in Japanese, no morphological marker is at-
tached to join nouns to form a noun phrase. We
previously resort to a heuristic measure to segment
noun phrases. The new statistical method provides a
straightforward solution to this problem.
Meanwhile, our language models have their own
problem. The assumption that language is a se-
quence of invariant words fails to capture rich mor-
phology, as our segmentation criteria specify that
each verb or adjective consists of an invariant stem
and an ending that changes its form according to
its grammatical roles. For this reason, we limit our
scope to noun phrases in this paper.
Use of Noun Phrases Named entity recogni-
tion (NER) is a field where encyclopedic knowl-
edge plays an important role. Kazama and Tori-
sawa (2008) encode information extracted from a
gazetteer (e.g. Wikipedia) as features of a CRF-
based Japanese NE tagger. They formalize the NER
task as the character-based labeling of IOB tags.
Noun phrases extracted from a gazetteer are also
straightforwardly represented as IOB tags. How-
ever, this does not fully solve the knowledge bot-
tleneck problem. They also used the output of a
morphological analyzer, which does not utilize en-
cyclopedic knowledge. NER performance may be
affected by segmentation errors in morphological
analysis involving unknown words.
Chinese word segmentation is often formalized as
a character tagging problem (Xue, 2003). In this
setting, it is easy to incorporate external resources
into the model. Low et al. (2005) introduce an exter-
nal dictionary as features of a discriminative model.
However, they only use words up to 4 characters in
length. We conjecture that words in their dictionary
are not noun phrases. External resources used by
</bodyText>
<page confidence="0.998389">
606
</page>
<bodyText confidence="0.992609470588235">
Peng et al. (2004) are also lists of short words and
characters.
Non-parametric Language Models Non-
parametric Bayesian statistics offers an elegant
solution to the task of unsupervised word segmen-
tation, in which the vocabulary size is not known in
advance (Goldwater et al., 2009; Mochihashi et al.,
2009). It does not compete with supervised segmen-
tation, however. Unsupervised word segmentation
is used elsewhere, for example, with theoretical
interest in children’s language acquisition (Johnson,
2008; Johnson and Demuth, 2010) and with the
application to statistical machine translation, in
which segmented text is merely an intermediate rep-
resentation (Xu et al., 2008; Nguyen et al., 2010).
In this paper we demonstrate that non-parametric
models can complement supervised segmentation.
</bodyText>
<sectionHeader confidence="0.996516" genericHeader="method">
3 Japanese Noun Phrase Segmentation
</sectionHeader>
<bodyText confidence="0.999640666666667">
Our goal is to overcome the unknown word prob-
lem in morphological analysis by utilizing existing
resources such as dictionaries and encyclopedias for
human readers. In our settings, we are given a list of
entries from external resources. Almost all of them
are noun phrases and each entry consists of one or
more words.
A naive implementation would be to use noun
phrases as they are. In fact, ipadic1 regards as single
words a large number of long proper nouns like “M
Mg (literally, Kansai Interna-
tional Airport Company Connecting Bridge). How-
ever, this approach has various drawbacks. For ex-
ample, in information retrieval, the query “Kansai
International Airport” does not match the “single”
word for the bridge. So we apply segmentation.
Each entry is associated with text, which is usu-
ally the main text of the entry.2 We assume the text
as the key to segmenting the noun phrase. If the
noun phrase in question consists of more than one
word, its constituents would appear in the text either
freely or as part of other noun phrases.
We obtain the segmentation of an entry noun
phrase by considering the segmentation of the whole
</bodyText>
<footnote confidence="0.999120333333333">
1http://sourceforge.jp/projects/ipadic/
2We may augment the text with related documents if the
main text is not large enough.
</footnote>
<bodyText confidence="0.999635851851852">
text. One may instead consider a pipeline ap-
proach in which we first extract noun phrases in
text and then identify boundaries within these noun
phrases. However, noun phrases in text are not triv-
ially identifiable in the case that they contain un-
known words as their constituents. For example,
the analyzer erroneously segments the word “th.
`-C�5” (chiNsukou) into “th.” (chiN) and “ C
�5” (sukou), and since the latter is misidentified as
a verb, the incorrect noun phrase “th.” (chiN) is
extracted.
We have a morphological analyzer with a dictio-
nary that covers frequent words. Although it often
misidentifies unknown words, the overall accuracy
is reasonably high. For this reason, we like to use
the segmentation given by the analyzer as the ini-
tial state and to make small changes to them to get
a desired output. We also use an annotated corpus,
which was used to build the analyzer. As the an-
notated corpus encodes our segmentation criteria, it
can be used to force the models to stick with our
segmentation criteria.
We concentrate on segmentation in this paper, but
we also need to assign a POS tag to each constituent
word and to incorporate segmented noun phrases
into the dictionary of the morphological analyzer.
We leave them for future work.3
</bodyText>
<sectionHeader confidence="0.963831" genericHeader="method">
4 Non-parametric Bayesian Language
Models
</sectionHeader>
<bodyText confidence="0.99995475">
To correct the initial segmentation given by the an-
alyzer, we use non-parametric Bayesian language
models that have been applied to unsupervised word
segmentation (Goldwater et al., 2009). Specifically,
we adopt unigram and bigram models. We propose
a small modification to these models in order to ex-
ploit an annotated corpus when it is much larger than
raw text.
</bodyText>
<subsectionHeader confidence="0.969409">
4.1 Unigram Model
</subsectionHeader>
<bodyText confidence="0.9999225">
In the unigram model, a word in the corpus wi is
generated as follows:
</bodyText>
<equation confidence="0.5998105">
G|α0, Po — DP(α0, Po)
wi|G — G
</equation>
<bodyText confidence="0.999306666666667">
3Fortunately, the morphological analyzer JUMAN is capa-
ble of handling phrases, each of which consists of more than
one word. All we need to do is POS tagging.
</bodyText>
<page confidence="0.991263">
607
</page>
<bodyText confidence="0.999958666666667">
where G is a distribution over a countably infinite
set of words, and DP(α0, P0) is a Dirichlet pro-
cess (Ferguson, 1973) with the concentration param-
eter α0 and the base distribution P0, for which we
use a zerogram model described in Section 4.3.
Marginalizing out G, we can interpret the model
as a Chinese restaurant process. Suppose that we
have observed i − 1 words w−i = w1, · · · , wi−1,
the probability of wi is given by
</bodyText>
<equation confidence="0.740365">
,
i − 1 + α0
</equation>
<bodyText confidence="0.9974377">
where nw−i
w is the number of word label w observed
in w−i.
The unigram model is known for its tendency to
misidentify a sequence of words in common collo-
cations as a single word (Goldwater et al., 2009). In
preliminary experiments, we found that the unigram
model often interpreted a noun phrase as a single
word, even in the case that its constituents frequently
appeared in text.
</bodyText>
<subsectionHeader confidence="0.959079">
4.2 Bigram Model
</subsectionHeader>
<bodyText confidence="0.999984">
The problem of the unigram model can be alleviated
by the bigram model based on a hierarchical Dirich-
let process (Goldwater et al., 2009). In the bigram
model, word wi is generated as follows:
</bodyText>
<equation confidence="0.98222">
G|α0, P0 ∼ DP(α0, P0)
Hl|α1, G ∼ DP(α1, G)
wi|wi−1 = l,Hl ∼ Hl
</equation>
<bodyText confidence="0.99983675">
Marginalizing out G and Hl, we can again explain
the model with the Chinese restaurant process. Un-
like the unigram model, however, the bigram model
depends on the latent table assignments z−i.
</bodyText>
<equation confidence="0.992560571428571">
h−i
(wi−1,wi) + α1P1(wi|h−i)
nh−i + α
(wi−1,∗)
tw−i
P1(wi|h−i) =i + α0P0(wi) (3)h
t∗ −i + α0
</equation>
<bodyText confidence="0.996111875">
where h−i = (w−i, z−i), th−i
wi is the number of ta-
bles labeled with wi and th−i
∗ is the total number of
tables. Thanks to exchangeability, we do not need to
track the exact seating assignments. Still, we need to
maintain a histogram for each w that consists of fre-
quencies of table customers (Blunsom et al., 2009).
</bodyText>
<subsectionHeader confidence="0.98848">
4.3 Zerogram Model
</subsectionHeader>
<bodyText confidence="0.9996565">
Following Nagata (1996) and Mochihashi et al.
(2009), we model the zerogram distribution P0 with
the word length k and the character sequence w =
c1, · · · , ck. Specifically, we define P0 as the combi-
nation of a Poisson distribution with mean λ and a
bigram distribution over characters.
</bodyText>
<equation confidence="0.999757333333333">
P0(w) = P(k; λ)P(c1, · · · , ck, k|O)
P(k|O)
k
P(k; λ) = e−λ kl
P(c1,··· ,ck,k|O) = k+1∏ P(ci|ci−1)
i=1
</equation>
<bodyText confidence="0.999837666666667">
O is the zerogram model, and c0 and ck+1 are a word
boundary marker. P(k|O) can be estimated by ran-
domly generating words from the model. We use
different λ for different scripts. The Japanese writ-
ing system uses several scripts, and each word can
be classified by script such as hiragana, katakana,
kanji, the mixture of hiragana and kanji, etc. The op-
timal value for λ depends on scripts. For example,
katakana, which predominantly denotes loan words,
is longer on average than hiragana, which is often
used for short function words.
We obtain the parameters and counts from an an-
notated corpus and fix them during noun phrase seg-
mentation. This greatly simplifies inference but may
make the model fragile with unknown words. For
this reason, we set a hierarchical Pitman-Yor process
prior (Teh, 2006; Goldwater et al., 2006) for the bi-
gram probability P(ci|ci−1) with the base distribu-
tion of character unigrams. Note that even character
bigrams are sparse because thousands of characters
are used in Japanese.
</bodyText>
<subsectionHeader confidence="0.999773">
4.4 Mixing an Annotated Corpus
</subsectionHeader>
<bodyText confidence="0.9999035">
An annotated corpus can be used to force the mod-
els to stick with our segmentation criteria. A
straightforward way to do this is to mix it with
raw text while fixing the segmentation during infer-
ence (Mochihashi et al., 2009). A word found in
the annotated corpus is generally preferred because
it has fixed counts obtained from the annotated cor-
pus. We call this method direct mixing.
Direct mixing is problematic when raw text is
much smaller than the annotated corpus. With this
</bodyText>
<equation confidence="0.991857285714286">
n
P1(wi = w|w−i) =
w−i
w + α0P0 (1)
n
P2(wi|h−i) =
(2)
</equation>
<page confidence="0.962681">
608
</page>
<bodyText confidence="0.9988565">
situation, the role of raw text associated with the
noun phrase in question is marginalized by the an-
notated corpus.
As a solution to this problem, we propose another
mixing method called back-off mixing. In back-off
mixing, the annotated corpus is used as part of the
base distribution. In the unigram model, P0 in (1) is
replaced by
</bodyText>
<equation confidence="0.743281666666667">
P BM
0 = AIPP0 + (1 − AIP)PREF
1 ,
where AIP is a parameter for linear interpolation and
PREF
1 is the unigram probability obtained from the
</equation>
<bodyText confidence="0.99442825">
annotated text. The loose coupling makes the mod-
els robust to an imbalanced pair of texts. Similarly,
the back-off mixing bigram model replaces P1 in (2)
with
</bodyText>
<equation confidence="0.957973">
PBM
1 = AIPP1 + (1 − AIP)P REF
2 .
</equation>
<sectionHeader confidence="0.999739" genericHeader="method">
5 Inference
</sectionHeader>
<bodyText confidence="0.9999422">
Collapsed Gibbs sampling is widely used to find
an optimal segmentation (Goldwater et al., 2009).
In this section, we first show that simple collapsed
sampling can hardly escape the initial segmentation.
To address this problem, we apply a block sam-
pling algorithm named type-based sampling (Liang
et al., 2010) to the unigram model. Since type-based
sampling is not applicable to the bigram model, we
propose a novel sampling procedure for the bigram
model, which we call hybrid type-based sampling.
</bodyText>
<subsectionHeader confidence="0.990806">
5.1 Collapsed Sampling
</subsectionHeader>
<bodyText confidence="0.999465454545455">
In collapsed Gibbs sampling, the sampler repeatedly
samples every possible boundary position, condi-
tioned on the current state of the rest of the corpus.
It stochastically decides whether the corresponding
local area consists of a single word w1 or two words
w2w3 (w1 = w2.w3). The conditional probabilities
can be derived from (1).
Collapsed sampling is known for slow conver-
gence. This property is especially problematic in
our settings where the initial segmentation is given
by a morphological analyzer. Since the analyzer de-
terministically segments text using pre-defined pa-
rameters, the resultant segmentation is fairly consis-
tent. Segmentation errors involving unknown words
also occur in a regular way. Intuitively, we start with
a local optimum although it is not too distant from
the global optimum. The collapsed Gibbs sampler is
easily entrapped by this local optimum. For this rea-
son, the initial segmentation is usually chosen at ran-
dom (Goldwater et al., 2009). Sentence-based block
sampling is also susceptible to consistent initializa-
tion (Liang et al., 2010).
</bodyText>
<subsectionHeader confidence="0.993722">
5.2 Type-based Sampling
</subsectionHeader>
<bodyText confidence="0.999987179487179">
To achieve fast convergence, we adopt a block sam-
pling algorithm named type-based sampling (Liang
et al., 2010). For the unigram model, a type-based
sampler jointly samples multiple positions that share
the same type. Two positions have the same type
if the corresponding areas are both of the form w1
or w2w3. Type-based sampling takes advantage of
the exchangeability of multiple positions with the
same type. Given n positions with the same type,
the sampler first samples the number of new bound-
aries m′ (0 &lt; m′ &lt; n), and then uniformly arranges
m′ boundaries out of n positions.
Type-based sampling has the ability to jump from
a local optimum (e.g. consistently segmented) to an-
other stable state (consistently unsegmented). While
Liang et al. (2010) used random initialization, we
take particular note of the possibility of efficiently
correcting the consistent segmentation by the ana-
lyzer.
Type-based sampling is, however, not applicable
to the bigram model for two reasons. The first prob-
lem is sparsity. For the bigram model, we need to
consider adjacent words, wl on the left and wr on
the right. This means that each type consists of
three or four words, wlw1wr or wlw2w3wr. Con-
sequently, few positions share the same type and
we fail to change closely-related areas wl′w1wr′ and
wl′w2w3wr′, making inference inefficient.
The second and more fundamental problem arises
from the hierarchical settings. Since the bigram
model depends on latent table assignments, the joint
distribution of multiple positions is no longer a
closed-form function of counts.
Strictly speaking, we need to update the model
counts even when sampling one position because
the observation of the bigram (wlw1), for exam-
ple, may affect the probability P2(w2|h−, (wlw1)).
Goldwater et al. (2009) approximate the probability
by not updating the model counts in collapsed Gibbs
</bodyText>
<page confidence="0.974594">
609
</page>
<bodyText confidence="0.710993083333333">
sampling (i.e. P2(w2|h−, ⟨wlw1⟩) ≈ P2(w2|h−)). probability 0.4 0.4
They rely on the assumption that repeated bigrams 0.3 0.3
are rare. Obviously this does not hold true for type- 0.2 0.2
based sampling. Hence for type-based sampling, we 0.1 0.1
have to update the model counts whenever we ob- 0 0
serve a new word.
One way to obtain the joint probability is to ex-
plicitly simulate the updates of histograms and other
model counts. This is very cumbersome as we need
to simulate n + 1 ways of model updates.
0 2 4 6 8 10
m’
</bodyText>
<subsectionHeader confidence="0.988895">
5.3 Hybrid Type-based Sampling
</subsectionHeader>
<bodyText confidence="0.999993">
To address these problems, we propose a hybrid
sampler which incorporates the Metropolis-Hastings
algorithm into blocked Gibbs sampling. Metropolis-
Hastings is another technique for sampling from a
Markov chain. It first draws a proposed next state
h′ based on the current state h according to some
proposal distribution Q(h′; h). Then it accepts the
proposal with the probability of
</bodyText>
<equation confidence="0.995405">
f P(h′)Q(h; h′)1l (4)
min l P(h)Q(h′; h) , f
</equation>
<bodyText confidence="0.999919875">
If the proposal is not accepted, the current state is
used as the next state. Metropolis-Hastings is useful
when it is difficult to directly sample from P.
We use the Metropolis-Hastings algorithm within
Gibbs sampling. Instead of calculating the n + 1
probabilities of the number of boundaries, we only
compare the current state with a proposed bound-
ary arrangement. Also, the set of positions sampled
jointly is chosen at unigram-level type instead of
bigram-level type. The positions are no longer ex-
changeable. Therefore we calculate the conditional
probability of one specific boundary arrangement.
When n = 1, the only choice is to flip the cur-
rent state (i.e. (m, m′) ∈ {(0, 1), (1, 0)}). This re-
duces to simple collapsed sampling. Otherwise we
draw a proposed state in two steps. Given the n
positions and the number of current boundaries m,
we first draw the number of proposed boundaries m′
from a probability distribution fn(m′; m). We then
randomly arrange m′ boundaries. The probability
mass is uniformly divided by nC,,t′ arrangements.
One exception is the case when m ∈� {0, n} and
m′ = m. In this case we perform permutation to
obtain h′ ≠ h. To sum up, the proposal distribution
</bodyText>
<figureCaption confidence="0.89338">
Figure 1: Probability of # of boundaries f10(m′; 3).
is defined as follows:
</figureCaption>
<equation confidence="0.9904145">
Q(h′;h) = fn(m′;m) (5)
nCn′ − In(m, m′),
</equation>
<bodyText confidence="0.996529444444444">
where In(m, m′) is 1 if m ∈� {0, n} and m′ = m;
otherwise 0.
We construct fn(m′; m) by discretizing a beta
distribution (α = 0 &lt; 1) and a normal distribution
with mean m, as shown in Figure 1. The former fa-
vors extreme values while the latter prefers smaller
moves.
The sampling of each type is done in the follow-
ing steps.
</bodyText>
<listItem confidence="0.9893495625">
1. Collect n positions that share a unigram-level
type.
2. Propose a new boundary arrangement. In what
follows, we only focus on flipped boundaries
because the rest does not change the likelihood
ratio of the current and proposed states.
3. Calculate the current conditional probability.
This can be done by repeatedly applying (2)
while removing words one-by-one and updat-
ing the model counts accordingly.
4. Calculate the proposed conditional probability
while adding words one-by-one.
5. Decide whether to accept the proposal accord-
ing to (4). If the proposal is accepted, we final-
ize the arrangement; otherwise we revert to the
current state.
</listItem>
<bodyText confidence="0.997912333333333">
We implement skip approximation (Liang et al.,
2010) and sample each type once per iteration. This
is motivated by the observation that although the
</bodyText>
<page confidence="0.973974">
610
</page>
<figure confidence="0.752451333333333">
2. The main text is longer than 1,000 characters.
3. The title appears at least 5 times in the main
text.
</figure>
<bodyText confidence="0.971869666666667">
joint sampling of a large number of positions is com-
putationally expensive, the proposal is accepted very
infrequently.
</bodyText>
<subsectionHeader confidence="0.989002">
5.4 Additional Constraints
</subsectionHeader>
<bodyText confidence="0.99945">
Partial annotations (Tsuboi et al., 2008; Neubig and
Mori, 2010) can be used for inference. If we know in
advance that a certain position is a boundary or non-
boundary, we simply keep it unaltered. As partially-
annotated text, we can use markup. Suppose that the
original text is written with wiki markup as follows:
</bodyText>
<equation confidence="0.9616865">
*JR[[宇野線]][[常山駅]]
[gloss] JR Ube Line Tsuneyama Station
It is clear that the position between “�” (line) and
“&apos;m” (tsune) is a boundary.
</equation>
<bodyText confidence="0.99986475">
Similarly, we can impose our trivial rules of seg-
mentation on the model. For example, we can keep
punctuation markers (Li and Sun, 2009) separate
from others.
</bodyText>
<sectionHeader confidence="0.999416" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.981698">
6.1 Settings
</subsectionHeader>
<bodyText confidence="0.971358318181818">
Data Set We evaluated our approach on Japanese
Wikipedia. For each entry of Wikipedia, we re-
garded the title as a noun phrase and used both the
title and main text for segmentation. We separately
applied our segmentation procedure to each entry.
We constructed the data set as follows. We ex-
tracted each entry from an XML dump of Japanese
Wikipedia.4 We normalized the title by dropping
trailing parentheses that disambiguate entries with
similar names (e.g. “X19 (21&apos;Ha)” for Akagi (aircraft
carrier)). We extracted the main text from wikitext
and used wiki markup as boundary markers. We ap-
plied both the title and main text to the morphologi-
cal analyzer JUMAN5 to get an initial segmentation.
If the resultant segmentation conflicted with markup
information, we overrode the former. The initial seg-
mentation was also used as the baseline.
We only used entries that satisfied all of the fol-
lowing conditions.
1. The (normalized) title is longer than one char-
acter and contains hiragana, katakana and/or
kanji.
</bodyText>
<footnote confidence="0.979738666666667">
4http://download.wikimedia.org/jawiki/
5http://nlp.ist.i.kyoto-u.ac.jp/EN/
index.php?JUMAN
</footnote>
<bodyText confidence="0.999275461538461">
The first condition ensures that there are segmenta-
tion ambiguities. The second and third conditions
exclude entries unsuitable for statistical methods.
14% of the entries satisfied these conditions.
We randomly selected 500 entries and manually
segmented their titles for evaluation. The 2-person
inter-annotator Kappa score was 0.95.
As an annotated corpus, we used Kyoto Text Cor-
pus.6 It contained 1,675,188 characters.
Models We compared the unigram and bigram
models. As for inference procedures, we used col-
lapsed Gibbs sampling (CL) for both models, type-
based sampling (TB) for the unigram model and
hybrid type-based sampling (HTB) for the bigram
model.
We tested two mixing methods of the annotated
corpus, direct mixing (DM) and back-off mixing
(BM).
To investigate the effect of initialization, we also
tried randomly segmented text as the initial state
(RAND). For random initialization, we placed a
boundary with probability 0.5 on each position un-
less it was a fixed boundary.
The unigram model has one Dirichlet process
concentration hyperparameter α0 and the bigram
model has α0 and α1. For each model, we experi-
mented with the following values.
α0: 0.1, 0.5, 1 5 10, 50, 100, 500, 1,000 and 5,000
α1: 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100 and 500
For comparison, we also performed hyperparame-
ter sampling. Following Escobar and West (1995),
we set a gamma prior and introduced auxiliary vari-
ables to infer concentration parameters from data.
For back-off mixing, we used the linear interpola-
tion parameter AJp = 0.5. The zerogram model was
trained on the annotated corpus.
In each run, we performed 10 burn-in iterations.
We then performed another 10 iterations to collect
samples.
</bodyText>
<footnote confidence="0.999039">
6http://nlp.ist.i.kyoto-u.ac.jp/EN/
index.php?Kyoto%20University%20Text%
20Corpus
</footnote>
<page confidence="0.997412">
611
</page>
<tableCaption confidence="0.999902">
Table 1: Results of segmentation of entry titles (F-score (precision/recall)).
</tableCaption>
<table confidence="0.971396166666667">
model best median inferred
unigram + CL 81.35 (77.78/85.27)** 80.09 (75.80/84.89) 80.86 (76.81/85.36)
unigram + TB 55.87 (66.71/48.06) 51.04 (62.64/43.06) 42.63 (54.91/34.84)
bigram + CL 80.65 (76.73/84.99) 79.96 (75.50/84.99) 80.54 (76.84/84.61)
bigram + HTB 83.23 (85.25/81.30)** 74.52 (71.33/78.00) 34.52 (46.69/27.38)
unigram + CL + DM 85.29 (83.14/87.54)** 81.62 (77.93/85.70)** 80.91 (82.87/79.04)
unigram + TB + DM 35.26 (47.74/29.95) 33.81 (46.20/26.66) 31.90 (44.30/24.93)
bigram + CL + DM 80.37 (76.01/85.27) 79.88 (75.42/84.89) 73.77 (78.49/69.59)
bigram + HTB + DM 69.66 (67.68/71.77) 67.39 (64.35/70.73) 31.54 (43.79/24.64)
unigram + CL + BM 81.28 (77.48/85.46) 80.23 (76.06/84.89) 81.42 (77.75/85.46)
unigram + TB + BM 57.22 (68.01/49.39) 52.98 (64.50/44.95) 42.43 (54.69/34.66)
bigram + CL + BM 81.33 (77.34/85.74) 80.07 (75.69/84.99) 81.46 (77.82/85.46)**
bigram + HTB + BM 86.32 (85.67/86.97)** 76.35 (71.89/81.40) 40.81 (53.35/33.05)
unigram + TB + RAND 56.01 (66.93/48.16) 50.89 (62.21/43.06) 42.68 (54.81/34.94)
bigram + HTB + RAND 79.68 (80.13/79.23) 68.16 (63.64/73.37) 34.99 (47.05/27.86)
unigram + TB + BM + RAND 57.44 (67.91/49.76) 50.86 (61.92/43.15) 42.31 (54.55/34.56)
bigram + HTB + BM + RAND 84.03 (83.10/84.99) 70.46 (65.25/76.58) 40.16 (52.60/32.48)
baseline (JUMAN) 80.09 (75.80/84.89)
</table>
<tableCaption confidence="0.559914">
** Statistically significant improvement with P &lt; 0.01.
</tableCaption>
<bodyText confidence="0.9969738125">
Evaluation Metrics We evaluated the segmenta-
tion accuracy of 500 entry titles. Specifically we
evaluated the performance of a model with preci-
sion, recall and the F-score, all of which were based
on tokens. We report the score of the most frequent
segmentation among 10 samples.
Following Lee et al. (2010), we report the best and
median settings of hyperparameters based on the F-
score, in addition to inferred values.
In order to evaluate the degree of difference
between a pair of segmentations, we employed
character-based evaluation. Following Kudo et
al. (2004), we converted a word sequence into
character-based BI labels and examined labeling dis-
agreements. McNemar’s test of significance was
based on this metric.
</bodyText>
<subsectionHeader confidence="0.950563">
6.2 Results
</subsectionHeader>
<bodyText confidence="0.999847435897436">
Table 1 shows segmentation accuracy of various
models. One would notice that the baseline score
is much lower than the score previously reported re-
garding newspaper articles (Kudo et al., 2004). It
is because unlike newspaper articles, the titles of
Wikipedia entries contain an unusually high pro-
portion of unknown words. As suggested by rel-
atively low precision, unknown words tend to be
over-segmented by the morphological analyzer.
In the best hyperparameter settings, the back-off
mixing bigram model with hybrid type-based sam-
pling (bigram + HTB + BM) significantly outper-
formed the baseline and achieved the best F-score.
It did not performed well in the median setting as
it was sensitive to the value of α1. Hyperparameter
estimation led to catastrophic decreases in bigram
models as it made the hyperparameters much larger
than those in the best settings.
Collapsed sampling (+CL) returned scores com-
parable to that of the baseline. It is simply because
it did not change the initial segmentation a lot. In
contrast, type-based sampling (+TB) brought large
moves to the unigram model and significantly hurt
accuracy. As suggested by relatively low recall, the
unigram model prefers under-segmentation.
When combined with (hybrid) type-based sam-
pling (+TB/+HTB), back-off mixing (+BM) in-
creased accuracy from the corresponding non-
mixing models. By contrast, direct mixing (+DM)
drastically decreased accuracy from the non-mixing
models. We can confirm that when the main text
is orders of magnitude smaller than the annotated
text, the role of constituent words in the main text
is underestimated. To our surprise, collapsed sam-
pling with mixing models (+CL, +DM/+BM) out-
performed the baseline. However, the scores of type-
based sampling (+TB) suggest that with much more
iterations, the models would converge to undesired
states.
</bodyText>
<page confidence="0.996054">
612
</page>
<bodyText confidence="0.99995875">
The unigram model with random initialization
was indifferent from that with default initialization.
By contrast, the performance of the bigram model
slightly degenerated with random initialization.
</bodyText>
<subsectionHeader confidence="0.999743">
6.3 Convergence
</subsectionHeader>
<bodyText confidence="0.999937478260869">
Figure 2 shows how segmentations differed from
the initial state in the course of inference.7 A diff
is defined as the number of character-based dis-
agreements between the baseline segmentation and a
model output. Hyperparameters used were those of
the best model with (hybrid) type-based sampling.
We can see that collapsed sampling was almost
unable to escape the initial state. With type-based
sampling (+TB), the unigram model went further
than the bigram model, but to an undesired direc-
tion. The bigram model with hybrid type-based
sampling (bigram + HTB) converged in few itera-
tions. Although the model with random initializa-
tion (+RAND) converged to a nearby point, the ini-
tial segmentation by the morphological analyzer re-
alized a bit faster convergence and better accuracy.
Figure 2 shows how acceptance rates changed
during inference. For comparison, a sample by a
type-based Gibbs sampler was treated as “accepted”
if the number of new boundaries was different from
that of the current boundaries (i.e. m′ 7� m). The
acceptance rates were low and samplers seemingly
stayed around modes.
</bodyText>
<subsectionHeader confidence="0.99753">
6.4 Approximation
</subsectionHeader>
<bodyText confidence="0.999922083333333">
Up to this point, we consider every possible bound-
ary position. However, this seems wasteful, given
that a large portion of text has only marginal influ-
ence on the segmentation of the noun phrase in ques-
tion. For this reason, we implemented approxima-
tion named matching skip. We sampled a boundary
only if the corresponding local area contained a sub-
string of the noun phrase in question.
Table 2 shows the result of approximation. Hy-
perparameters used were those of the best models
with full sampling. Matching skip steadily worsened
performance although not to a large extent. Mean-
</bodyText>
<footnote confidence="0.9902658">
7For a fair comparison, we might need to report changes
over time instead of iterations. However, the difference of con-
vergence speed is obvious in the iteration-based comparison al-
though (hybrid) type-based sampling takes several times longer
than collapsed sampling in the current naive implementation.
</footnote>
<figure confidence="0.9727725">
0 5 10 15 20
iteration
</figure>
<figureCaption confidence="0.993956">
Figure 2: Diffs in the course of iteration. All models were
with back-off mixing (+BM).
</figureCaption>
<figure confidence="0.9946775">
0 5 10 15 20
iteration
</figure>
<figureCaption confidence="0.993499333333333">
Figure 3: Acceptance rates for a noun phrase in the
course of iteration. All models were with back-off mix-
ing (+BM).
</figureCaption>
<bodyText confidence="0.987905">
while it drastically reduced the number of sampled
positions. The median skip rate was 90.87%, with a
standard deviation of 8.5.
</bodyText>
<subsectionHeader confidence="0.853519">
6.5 Discussion
</subsectionHeader>
<bodyText confidence="0.999827307692308">
Figure 4 shows some segmentations corrected by
the back-off mixing bigram model with hybrid type-
based sampling. “市比野” (ichihino) is a rare place
name but can be identified by the model because
it is frequently used in the article. “こなみるく”
(konamiruku in hiragana) seems a pun on “Vミル
ク” (kona miruku, “powdered milk”) and “コナミ”
(konami in katakana, a company). We consider it
as a single word because we cannot reconstruct the
etymology solely based on the main text. Note the
different scripts. In Japanese, people often change
the script to derive a proper noun from a common
noun, which a naive analyzer fails to recognize. It is
</bodyText>
<figure confidence="0.9963416">
300
900
800
600
500
200
100
700
400
0
unigram + CL
unigram + TB
bigram + CL
bigram + HTB
unigram + TB + RAND
bigram + HTB + RAND
acceptance rate
0.14
0.12
0.08
0.06
0.04
0.02
0.1
0
unigram + TB
bigram + HTB
unigram + TB + RAND
bigram + HTB + RAND
diff
</figure>
<page confidence="0.993672">
613
</page>
<tableCaption confidence="0.993923">
Table 2: Effect of matching skip (F-score (precision/recall)).
</tableCaption>
<figure confidence="0.593925714285714">
model full matching skip
bigram + HTB 83.23 (85.25/81.30)** 82.86 (84.27/81.49)
bigram + HTB + BM 86.32 (85.67/86.97)** 83.87 (82.60/85.17)**
bigram + HTB + RAND 79.68 (80.13/79.23) 78.81 (78.64/75.07)
bigram + HTB + BM + RAND 84.03 (83.10/84.99) 81.08 (80.22/81.96)
baseline (JUMAN) 80.09 (75.80/84.89)
** Statistically significant improvement with P &lt; 0.01.
</figure>
<figureCaption confidence="0.999817">
Figure 4: Examples of improved segmentations.
</figureCaption>
<bodyText confidence="0.999972291666667">
very important to identify hiragana words correctly.
As hiragana is mainly used to write function words
and other basic words, segmentation errors concern-
ing hiragana often bring disastrous effects on ap-
plications of morphological analysis. For example,
the analyzer over-segments “tDLZth.” (chiri-
totechiN) into three shorter words among which the
second word “LZ” (tote) is a particle, and this se-
quence of words is transformed into a terrible parse
tree.
Most improvements come from correction of
over-segmentation because the initial segmenta-
tion by the analyzer shows a tendency of over-
segmentation. An example of corrected under-
segmentation is “contra-alto clarinet.” The pres-
ence of “clarinet,” “alto” and “contrabass” and oth-
ers in the main text allowed the model to iden-
tify the constituents. On the other hand, the seg-
mentation failed when our assumption about con-
stituents does not hold. For example, the person
name “*7 MVN` (kikuchi shuNkichi) is two words
but was erroneously combined into a single word by
the model because unfortunately he was always re-
ferred to by the full name.
</bodyText>
<sectionHeader confidence="0.999656" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999974478260869">
In this paper, we proposed a new task of Japanese
noun phrase segmentation. We adopted non-
parametric Bayesian language models and proposed
hybrid type-based sampling that can efficiently cor-
rect segmentation given by the morphological an-
alyzer. Although supervised segmentation is very
competitive, we showed that it can be supplemented
with our unsupervised approach.
We applied the proposed method to encyclopedic
text to segment noun phrases in it. The proposed
method can be applied to other tasks. For example,
in unknown word acquisition (Murawaki and Kuro-
hashi, 2008), noun phrases are often acquired from
text as single words. We can now segment them into
words in a more sophisticated way.
In the future we will assign a POS tag to each
word in order to use segmented noun phrases in mor-
phological analysis. We assume that the meaning
of constituents in a noun phrase rarely depends on
outer context. So it would be helpful to augment
them with rich semantic information in advance in-
stead of disambiguating their meaning every time we
analyze given text.
</bodyText>
<sectionHeader confidence="0.99885" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.963399">
This work was partly supported by JST CREST.
</bodyText>
<sectionHeader confidence="0.993986" genericHeader="references">
References
</sectionHeader>
<copyright confidence="0.5493155">
Masayuki Asahara and Yuji Matsumoto. 2000. Extended
models and tools for high-performance part-of-speech
</copyright>
<figure confidence="0.991085047619048">
(Ichihino, Hiwaki Town, an address)
り + そな + カード ⇒ りそな + カード
risona kando
(Risona Card, a company)
ちり + とて + ちん ⇒ ちりとてちん
chiritotechiN (name of a play)
こな + みる + く ⇒ こなみるく
konamiruku
(a shop affiliated with Konami Corporation)
はい + じぃ ⇒ はいじぃ
haiziI (stage name of a comedian)
ちん + すこう ⇒ ちんすこう
chiNsukou (a traditional sweet)
コントラアルトクラリネット ⇒ コントラ+ アルト
koNtora aruto
+ クラリネット
kurarineQto (Contra-alto clarinet)
樋 + 脇 + 町 + 市 + 比 + 野 ⇒ 樋脇 + 町
hiwaki chou
+ 市比野
ichihino
</figure>
<page confidence="0.981134">
614
</page>
<reference confidence="0.999886254716981">
tagger. In Proc. of COLING 2000, pages 21–27.
Masayuki Asahara and Yuji Matsumoto. 2004.
Japanese unknown word identification by character-
based chunking. In Proc. COLING 2004, pages 459–
465.
Phil Blunsom, Trevor Cohn, Sharon Goldwater, and Mark
Johnson. 2009. A note on the implementation of hier-
archical Dirichlet processes. In Proc. of ACL-IJCNLP
2009: Short Papers, pages 337–340.
Michael D. Escobar and Mike West. 1995. Bayesian
density estimation and inference using mixtures.
Journal of the American Statistical Association,
90(430):577–588.
Thomas S. Ferguson. 1973. A Bayesian analysis of
some nonparametric problems. Annals of Statistics,
1(2):209–230.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2006. Interpolating between types and tokens by
estimating power-law generators. In NIPS 18, pages
459–466.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2009. A Bayesian framework for word segmen-
tation: Exploring the effects of context. Cognition,
112(1):21–54.
Mark Johnson and Katherine Demuth. 2010. Unsu-
pervised phonemic Chinese word segmentation using
adaptor grammars. In Proc. of COLING 2010, pages
528–536.
Mark Johnson. 2008. Using adaptor grammars to iden-
tify synergies in the unsupervised acquisition of lin-
guistic structure. In Proc. of ACL 2008, pages 398–
406.
Jun’ichi Kazama and Kentaro Torisawa. 2008. Inducing
gazetteers for named entity recognition by large-scale
clustering of dependency relations. In Proc. of ACL
2008, pages 407–415, June.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to Japanese
morphological analysis. In Proc. of EMNLP 2004,
pages 230–237.
Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto,
and Makoto Nagao. 1994. Improvements of Japanese
morphological analyzer JUMAN. In Proc. of The In-
ternational Workshop on Sharable Natural Language
Resources, pages 22–38.
Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.
2010. Simple type-level unsupervised POS tagging.
In Proc. of EMNLP 2010, pages 853–861.
Zhongguo Li and Maosong Sun. 2009. Punctuation as
implicit annotations for Chinese word segmentation.
Computational Linguistics, 35(4):505–512.
Percy Liang, Michael I. Jordan, and Dan Klein. 2010.
Type-based MCMC. In Proc. of NAACL 2010, pages
573–581.
Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005. A
maximum entropy approach to Chinese word segmen-
tation. In Proc. of the 4th SIGHAN Workshop, pages
161–164.
Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda.
2009. Bayesian unsupervised word segmentation with
nested Pitman-Yor language modeling. In Proc. of
ACL-IJCNLP 2009, pages 100–108.
Yugo Murawaki and Sadao Kurohashi. 2008. Online
acquisition of Japanese unknown morphemes using
morphological constraints. In Proc. of EMNLP 2008,
pages 429–437.
Masaaki Nagata. 1996. Automatic extraction of new
words from Japanese texts using generalized forward-
backward search. In Proc. of EMNLP 1996, pages 48–
59.
Tetsuji Nakagawa and Yuji Matsumoto. 2006. Guessing
parts-of-speech of unknown words using global infor-
mation. In Proc. of COLING-ACL 2006, pages 705–
712.
Graham Neubig and Shinsuke Mori. 2010. Word-based
partial annotation for efficient corpus construction. In
Proc. of LREC 2010.
ThuyLinh Nguyen, Stephan Vogel, and Noah A. Smith.
2010. Nonparametric word segmentation for machine
translation. In Proc. of COLING 2010, pages 815–
823.
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detection
using conditional random fields. In Proc. of COLING
’04, pages 562–568.
Yee Whye Teh. 2006. A Bayesian interpretation of in-
terpolated Kneser-Ney. Technical Report TRA2/06,
School of Computing, National University of Singa-
pore.
Yuta Tsuboi, Hisashi Kashima, Shinsuke Mori, Hiroki
Oda, and Yuji Matsumoto. 2008. Training conditional
random fields using incomplete annotations. In Proc.
of COLING 2008, pages 897–904.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara.
2001. The unknown word problem: a morphological
analysis of Japanese using maximum entropy aided by
a dictionary. In Proc. of EMNLP 2001, pages 91–99.
Jia Xu, Jianfeng Gao, Kristina Toutanova, and Hermann
Ney. 2008. Bayesian semi-supervised Chinese word
segmentation for statistical machine translation. In
Proc. of COLING 2008, pages 1017–1024.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. Computational Linguistics and
Chinese Language Processing, 8(1):29–48.
Toshio Yokoi. 1995. The EDR electronic dictionary.
Communications of the ACM, 38(11):42–44.
</reference>
<page confidence="0.998558">
615
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.205836">
<title confidence="0.64301575">Non-parametric Bayesian Segmentation of Japanese Noun Phrases Murawaki Graduate School of Kyoto</title>
<abstract confidence="0.987580869565218">A key factor of high quality word segmentation for Japanese is a high-coverage dictionary, but it is costly to manually build such a lexical resource. Although external lexical resources for human readers are potentially good knowledge sources, they have not been utilized due to differences in segmentation criteria. To supplement a morphological dictionary with these resources, we propose a new task of Japanese noun phrase segmentation. We apply non-parametric Bayesian language models to segment each noun phrase in these resources according to the statistical behavior of its supposed constituents in text. For inference, we propose a novel block sampling procedure named hybrid type-based sampling, which has the ability to directly escape a local optimum that is not too distant from the global optimum. Experiments show that the proposed method efficiently corrects the initial segmentation given by a morphological analyzer.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>tagger</author>
</authors>
<date>2000</date>
<booktitle>In Proc. of COLING</booktitle>
<pages>21--27</pages>
<marker>tagger, 2000</marker>
<rawString>tagger. In Proc. of COLING 2000, pages 21–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masayuki Asahara</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese unknown word identification by characterbased chunking.</title>
<date>2004</date>
<booktitle>In Proc. COLING 2004,</booktitle>
<pages>459--465</pages>
<contexts>
<context position="5431" citStr="Asahara and Matsumoto, 2004" startWordPosition="836" endWordPosition="839">moto, 2000; Kudo et al., 2004). The standard approach in Japanese morphological analysis is lattice-based path selection instead of characterbased IOB tagging. Given a sentence, an analyzer first builds a lattice of words with dictionary look-up and then selects an optimal path using pre-defined parameters. This approach enables fast decoding and achieves accuracy high enough for practical use. This success, however, depends on a highcoverage dictionary, and unknown words are often misidentified. Although a line of research attempts to identify unknown words on the fly (Uchimoto et al., 2001; Asahara and Matsumoto, 2004), it by no means provides a definitive solution because it suffers from locality of contextual information available for identification (Nakagawa and Matsumoto, 2006). Therefore we like to perform separate lexical acquisition processes in which wider context can be examined. Our approach in this paper has a complementary relationship with unknown word acquisition from text, which we previously proposed (Murawaki and Kurohashi, 2008). Since, unlike Chinese and Thai, Japanese is rich in morphology, morphological regularity can be used to determine if an unknown word candidate in text is indeed t</context>
</contexts>
<marker>Asahara, Matsumoto, 2004</marker>
<rawString>Masayuki Asahara and Yuji Matsumoto. 2004. Japanese unknown word identification by characterbased chunking. In Proc. COLING 2004, pages 459– 465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
<author>Sharon Goldwater</author>
<author>Mark Johnson</author>
</authors>
<title>A note on the implementation of hierarchical Dirichlet processes.</title>
<date>2009</date>
<booktitle>In Proc. of ACL-IJCNLP 2009: Short Papers,</booktitle>
<pages>337--340</pages>
<contexts>
<context position="13670" citStr="Blunsom et al., 2009" startWordPosition="2219" endWordPosition="2222">i|wi−1 = l,Hl ∼ Hl Marginalizing out G and Hl, we can again explain the model with the Chinese restaurant process. Unlike the unigram model, however, the bigram model depends on the latent table assignments z−i. h−i (wi−1,wi) + α1P1(wi|h−i) nh−i + α (wi−1,∗) tw−i P1(wi|h−i) =i + α0P0(wi) (3)h t∗ −i + α0 where h−i = (w−i, z−i), th−i wi is the number of tables labeled with wi and th−i ∗ is the total number of tables. Thanks to exchangeability, we do not need to track the exact seating assignments. Still, we need to maintain a histogram for each w that consists of frequencies of table customers (Blunsom et al., 2009). 4.3 Zerogram Model Following Nagata (1996) and Mochihashi et al. (2009), we model the zerogram distribution P0 with the word length k and the character sequence w = c1, · · · , ck. Specifically, we define P0 as the combination of a Poisson distribution with mean λ and a bigram distribution over characters. P0(w) = P(k; λ)P(c1, · · · , ck, k|O) P(k|O) k P(k; λ) = e−λ kl P(c1,··· ,ck,k|O) = k+1∏ P(ci|ci−1) i=1 O is the zerogram model, and c0 and ck+1 are a word boundary marker. P(k|O) can be estimated by randomly generating words from the model. We use different λ for different scripts. The Ja</context>
</contexts>
<marker>Blunsom, Cohn, Goldwater, Johnson, 2009</marker>
<rawString>Phil Blunsom, Trevor Cohn, Sharon Goldwater, and Mark Johnson. 2009. A note on the implementation of hierarchical Dirichlet processes. In Proc. of ACL-IJCNLP 2009: Short Papers, pages 337–340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael D Escobar</author>
<author>Mike West</author>
</authors>
<title>Bayesian density estimation and inference using mixtures.</title>
<date>1995</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>90</volume>
<issue>430</issue>
<contexts>
<context position="26615" citStr="Escobar and West (1995)" startWordPosition="4374" endWordPosition="4377">and back-off mixing (BM). To investigate the effect of initialization, we also tried randomly segmented text as the initial state (RAND). For random initialization, we placed a boundary with probability 0.5 on each position unless it was a fixed boundary. The unigram model has one Dirichlet process concentration hyperparameter α0 and the bigram model has α0 and α1. For each model, we experimented with the following values. α0: 0.1, 0.5, 1 5 10, 50, 100, 500, 1,000 and 5,000 α1: 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100 and 500 For comparison, we also performed hyperparameter sampling. Following Escobar and West (1995), we set a gamma prior and introduced auxiliary variables to infer concentration parameters from data. For back-off mixing, we used the linear interpolation parameter AJp = 0.5. The zerogram model was trained on the annotated corpus. In each run, we performed 10 burn-in iterations. We then performed another 10 iterations to collect samples. 6http://nlp.ist.i.kyoto-u.ac.jp/EN/ index.php?Kyoto%20University%20Text% 20Corpus 611 Table 1: Results of segmentation of entry titles (F-score (precision/recall)). model best median inferred unigram + CL 81.35 (77.78/85.27)** 80.09 (75.80/84.89) 80.86 (76.</context>
</contexts>
<marker>Escobar, West, 1995</marker>
<rawString>Michael D. Escobar and Mike West. 1995. Bayesian density estimation and inference using mixtures. Journal of the American Statistical Association, 90(430):577–588.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas S Ferguson</author>
</authors>
<title>A Bayesian analysis of some nonparametric problems.</title>
<date>1973</date>
<journal>Annals of Statistics,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="12091" citStr="Ferguson, 1973" startWordPosition="1926" endWordPosition="1927">mentation (Goldwater et al., 2009). Specifically, we adopt unigram and bigram models. We propose a small modification to these models in order to exploit an annotated corpus when it is much larger than raw text. 4.1 Unigram Model In the unigram model, a word in the corpus wi is generated as follows: G|α0, Po — DP(α0, Po) wi|G — G 3Fortunately, the morphological analyzer JUMAN is capable of handling phrases, each of which consists of more than one word. All we need to do is POS tagging. 607 where G is a distribution over a countably infinite set of words, and DP(α0, P0) is a Dirichlet process (Ferguson, 1973) with the concentration parameter α0 and the base distribution P0, for which we use a zerogram model described in Section 4.3. Marginalizing out G, we can interpret the model as a Chinese restaurant process. Suppose that we have observed i − 1 words w−i = w1, · · · , wi−1, the probability of wi is given by , i − 1 + α0 where nw−i w is the number of word label w observed in w−i. The unigram model is known for its tendency to misidentify a sequence of words in common collocations as a single word (Goldwater et al., 2009). In preliminary experiments, we found that the unigram model often interpre</context>
</contexts>
<marker>Ferguson, 1973</marker>
<rawString>Thomas S. Ferguson. 1973. A Bayesian analysis of some nonparametric problems. Annals of Statistics, 1(2):209–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Interpolating between types and tokens by estimating power-law generators.</title>
<date>2006</date>
<booktitle>In NIPS 18,</booktitle>
<pages>459--466</pages>
<contexts>
<context position="14911" citStr="Goldwater et al., 2006" startWordPosition="2437" endWordPosition="2440">tem uses several scripts, and each word can be classified by script such as hiragana, katakana, kanji, the mixture of hiragana and kanji, etc. The optimal value for λ depends on scripts. For example, katakana, which predominantly denotes loan words, is longer on average than hiragana, which is often used for short function words. We obtain the parameters and counts from an annotated corpus and fix them during noun phrase segmentation. This greatly simplifies inference but may make the model fragile with unknown words. For this reason, we set a hierarchical Pitman-Yor process prior (Teh, 2006; Goldwater et al., 2006) for the bigram probability P(ci|ci−1) with the base distribution of character unigrams. Note that even character bigrams are sparse because thousands of characters are used in Japanese. 4.4 Mixing an Annotated Corpus An annotated corpus can be used to force the models to stick with our segmentation criteria. A straightforward way to do this is to mix it with raw text while fixing the segmentation during inference (Mochihashi et al., 2009). A word found in the annotated corpus is generally preferred because it has fixed counts obtained from the annotated corpus. We call this method direct mixi</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2006. Interpolating between types and tokens by estimating power-law generators. In NIPS 18, pages 459–466.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>A Bayesian framework for word segmentation: Exploring the effects of context.</title>
<date>2009</date>
<journal>Cognition,</journal>
<volume>112</volume>
<issue>1</issue>
<contexts>
<context position="3228" citStr="Goldwater et al., 2009" startWordPosition="502" endWordPosition="505">ncyclopedias for word segmentation. We segment each entry noun phrase into words. To do this, we examine the main text of the entry, on the assumption that if the noun phrase in question consists of more than one word, its constituents appear in the main text either freely or as part of other noun phrases. For “常山城” (tsuneyama-jou), its constituent “常山” (tsune) appears by itself and as constituents of other nouns phrases such as “常山山 頂” (peak of Tsuneyama) and “常山駅” (Tsuneyama Station) while “山城” (yamashiro) does not. To segment each noun phrase, we use nonparametric Bayesian language models (Goldwater et al., 2009; Mochihashi et al., 2009). Our approach 605 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 605–615, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics is based on two key factors: the bigram model and type-based block sampling. The bigram model alleviates a problem of the unigram model, that is, a tendency to misidentify a sequence of words in common collocations as a single word. Type-based sampling (Liang et al., 2010) has the ability to directly escape a local optimum, making inference very efficient. H</context>
<context position="8209" citStr="Goldwater et al., 2009" startWordPosition="1282" endWordPosition="1285">lem (Xue, 2003). In this setting, it is easy to incorporate external resources into the model. Low et al. (2005) introduce an external dictionary as features of a discriminative model. However, they only use words up to 4 characters in length. We conjecture that words in their dictionary are not noun phrases. External resources used by 606 Peng et al. (2004) are also lists of short words and characters. Non-parametric Language Models Nonparametric Bayesian statistics offers an elegant solution to the task of unsupervised word segmentation, in which the vocabulary size is not known in advance (Goldwater et al., 2009; Mochihashi et al., 2009). It does not compete with supervised segmentation, however. Unsupervised word segmentation is used elsewhere, for example, with theoretical interest in children’s language acquisition (Johnson, 2008; Johnson and Demuth, 2010) and with the application to statistical machine translation, in which segmented text is merely an intermediate representation (Xu et al., 2008; Nguyen et al., 2010). In this paper we demonstrate that non-parametric models can complement supervised segmentation. 3 Japanese Noun Phrase Segmentation Our goal is to overcome the unknown word problem </context>
<context position="11510" citStr="Goldwater et al., 2009" startWordPosition="1817" endWordPosition="1820">ed to build the analyzer. As the annotated corpus encodes our segmentation criteria, it can be used to force the models to stick with our segmentation criteria. We concentrate on segmentation in this paper, but we also need to assign a POS tag to each constituent word and to incorporate segmented noun phrases into the dictionary of the morphological analyzer. We leave them for future work.3 4 Non-parametric Bayesian Language Models To correct the initial segmentation given by the analyzer, we use non-parametric Bayesian language models that have been applied to unsupervised word segmentation (Goldwater et al., 2009). Specifically, we adopt unigram and bigram models. We propose a small modification to these models in order to exploit an annotated corpus when it is much larger than raw text. 4.1 Unigram Model In the unigram model, a word in the corpus wi is generated as follows: G|α0, Po — DP(α0, Po) wi|G — G 3Fortunately, the morphological analyzer JUMAN is capable of handling phrases, each of which consists of more than one word. All we need to do is POS tagging. 607 where G is a distribution over a countably infinite set of words, and DP(α0, P0) is a Dirichlet process (Ferguson, 1973) with the concentra</context>
<context position="12949" citStr="Goldwater et al., 2009" startWordPosition="2084" endWordPosition="2087">ed i − 1 words w−i = w1, · · · , wi−1, the probability of wi is given by , i − 1 + α0 where nw−i w is the number of word label w observed in w−i. The unigram model is known for its tendency to misidentify a sequence of words in common collocations as a single word (Goldwater et al., 2009). In preliminary experiments, we found that the unigram model often interpreted a noun phrase as a single word, even in the case that its constituents frequently appeared in text. 4.2 Bigram Model The problem of the unigram model can be alleviated by the bigram model based on a hierarchical Dirichlet process (Goldwater et al., 2009). In the bigram model, word wi is generated as follows: G|α0, P0 ∼ DP(α0, P0) Hl|α1, G ∼ DP(α1, G) wi|wi−1 = l,Hl ∼ Hl Marginalizing out G and Hl, we can again explain the model with the Chinese restaurant process. Unlike the unigram model, however, the bigram model depends on the latent table assignments z−i. h−i (wi−1,wi) + α1P1(wi|h−i) nh−i + α (wi−1,∗) tw−i P1(wi|h−i) =i + α0P0(wi) (3)h t∗ −i + α0 where h−i = (w−i, z−i), th−i wi is the number of tables labeled with wi and th−i ∗ is the total number of tables. Thanks to exchangeability, we do not need to track the exact seating assignments.</context>
<context position="16448" citStr="Goldwater et al., 2009" startWordPosition="2712" endWordPosition="2715">ose another mixing method called back-off mixing. In back-off mixing, the annotated corpus is used as part of the base distribution. In the unigram model, P0 in (1) is replaced by P BM 0 = AIPP0 + (1 − AIP)PREF 1 , where AIP is a parameter for linear interpolation and PREF 1 is the unigram probability obtained from the annotated text. The loose coupling makes the models robust to an imbalanced pair of texts. Similarly, the back-off mixing bigram model replaces P1 in (2) with PBM 1 = AIPP1 + (1 − AIP)P REF 2 . 5 Inference Collapsed Gibbs sampling is widely used to find an optimal segmentation (Goldwater et al., 2009). In this section, we first show that simple collapsed sampling can hardly escape the initial segmentation. To address this problem, we apply a block sampling algorithm named type-based sampling (Liang et al., 2010) to the unigram model. Since type-based sampling is not applicable to the bigram model, we propose a novel sampling procedure for the bigram model, which we call hybrid type-based sampling. 5.1 Collapsed Sampling In collapsed Gibbs sampling, the sampler repeatedly samples every possible boundary position, conditioned on the current state of the rest of the corpus. It stochastically </context>
<context position="17851" citStr="Goldwater et al., 2009" startWordPosition="2932" endWordPosition="2935">is known for slow convergence. This property is especially problematic in our settings where the initial segmentation is given by a morphological analyzer. Since the analyzer deterministically segments text using pre-defined parameters, the resultant segmentation is fairly consistent. Segmentation errors involving unknown words also occur in a regular way. Intuitively, we start with a local optimum although it is not too distant from the global optimum. The collapsed Gibbs sampler is easily entrapped by this local optimum. For this reason, the initial segmentation is usually chosen at random (Goldwater et al., 2009). Sentence-based block sampling is also susceptible to consistent initialization (Liang et al., 2010). 5.2 Type-based Sampling To achieve fast convergence, we adopt a block sampling algorithm named type-based sampling (Liang et al., 2010). For the unigram model, a type-based sampler jointly samples multiple positions that share the same type. Two positions have the same type if the corresponding areas are both of the form w1 or w2w3. Type-based sampling takes advantage of the exchangeability of multiple positions with the same type. Given n positions with the same type, the sampler first sampl</context>
<context position="19758" citStr="Goldwater et al. (2009)" startWordPosition="3235" endWordPosition="3238">our words, wlw1wr or wlw2w3wr. Consequently, few positions share the same type and we fail to change closely-related areas wl′w1wr′ and wl′w2w3wr′, making inference inefficient. The second and more fundamental problem arises from the hierarchical settings. Since the bigram model depends on latent table assignments, the joint distribution of multiple positions is no longer a closed-form function of counts. Strictly speaking, we need to update the model counts even when sampling one position because the observation of the bigram (wlw1), for example, may affect the probability P2(w2|h−, (wlw1)). Goldwater et al. (2009) approximate the probability by not updating the model counts in collapsed Gibbs 609 sampling (i.e. P2(w2|h−, ⟨wlw1⟩) ≈ P2(w2|h−)). probability 0.4 0.4 They rely on the assumption that repeated bigrams 0.3 0.3 are rare. Obviously this does not hold true for type- 0.2 0.2 based sampling. Hence for type-based sampling, we 0.1 0.1 have to update the model counts whenever we ob- 0 0 serve a new word. One way to obtain the joint probability is to explicitly simulate the updates of histograms and other model counts. This is very cumbersome as we need to simulate n + 1 ways of model updates. 0 2 4 6 </context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2009</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2009. A Bayesian framework for word segmentation: Exploring the effects of context. Cognition, 112(1):21–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Katherine Demuth</author>
</authors>
<title>Unsupervised phonemic Chinese word segmentation using adaptor grammars.</title>
<date>2010</date>
<booktitle>In Proc. of COLING</booktitle>
<pages>528--536</pages>
<contexts>
<context position="8461" citStr="Johnson and Demuth, 2010" startWordPosition="1316" endWordPosition="1319">njecture that words in their dictionary are not noun phrases. External resources used by 606 Peng et al. (2004) are also lists of short words and characters. Non-parametric Language Models Nonparametric Bayesian statistics offers an elegant solution to the task of unsupervised word segmentation, in which the vocabulary size is not known in advance (Goldwater et al., 2009; Mochihashi et al., 2009). It does not compete with supervised segmentation, however. Unsupervised word segmentation is used elsewhere, for example, with theoretical interest in children’s language acquisition (Johnson, 2008; Johnson and Demuth, 2010) and with the application to statistical machine translation, in which segmented text is merely an intermediate representation (Xu et al., 2008; Nguyen et al., 2010). In this paper we demonstrate that non-parametric models can complement supervised segmentation. 3 Japanese Noun Phrase Segmentation Our goal is to overcome the unknown word problem in morphological analysis by utilizing existing resources such as dictionaries and encyclopedias for human readers. In our settings, we are given a list of entries from external resources. Almost all of them are noun phrases and each entry consists of </context>
</contexts>
<marker>Johnson, Demuth, 2010</marker>
<rawString>Mark Johnson and Katherine Demuth. 2010. Unsupervised phonemic Chinese word segmentation using adaptor grammars. In Proc. of COLING 2010, pages 528–536.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Using adaptor grammars to identify synergies in the unsupervised acquisition of linguistic structure.</title>
<date>2008</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>398--406</pages>
<contexts>
<context position="8434" citStr="Johnson, 2008" startWordPosition="1314" endWordPosition="1315">n length. We conjecture that words in their dictionary are not noun phrases. External resources used by 606 Peng et al. (2004) are also lists of short words and characters. Non-parametric Language Models Nonparametric Bayesian statistics offers an elegant solution to the task of unsupervised word segmentation, in which the vocabulary size is not known in advance (Goldwater et al., 2009; Mochihashi et al., 2009). It does not compete with supervised segmentation, however. Unsupervised word segmentation is used elsewhere, for example, with theoretical interest in children’s language acquisition (Johnson, 2008; Johnson and Demuth, 2010) and with the application to statistical machine translation, in which segmented text is merely an intermediate representation (Xu et al., 2008; Nguyen et al., 2010). In this paper we demonstrate that non-parametric models can complement supervised segmentation. 3 Japanese Noun Phrase Segmentation Our goal is to overcome the unknown word problem in morphological analysis by utilizing existing resources such as dictionaries and encyclopedias for human readers. In our settings, we are given a list of entries from external resources. Almost all of them are noun phrases </context>
</contexts>
<marker>Johnson, 2008</marker>
<rawString>Mark Johnson. 2008. Using adaptor grammars to identify synergies in the unsupervised acquisition of linguistic structure. In Proc. of ACL 2008, pages 398– 406.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun’ichi Kazama</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Inducing gazetteers for named entity recognition by large-scale clustering of dependency relations.</title>
<date>2008</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>407--415</pages>
<contexts>
<context position="6962" citStr="Kazama and Torisawa (2008)" startWordPosition="1084" endWordPosition="1088">ment noun phrases. The new statistical method provides a straightforward solution to this problem. Meanwhile, our language models have their own problem. The assumption that language is a sequence of invariant words fails to capture rich morphology, as our segmentation criteria specify that each verb or adjective consists of an invariant stem and an ending that changes its form according to its grammatical roles. For this reason, we limit our scope to noun phrases in this paper. Use of Noun Phrases Named entity recognition (NER) is a field where encyclopedic knowledge plays an important role. Kazama and Torisawa (2008) encode information extracted from a gazetteer (e.g. Wikipedia) as features of a CRFbased Japanese NE tagger. They formalize the NER task as the character-based labeling of IOB tags. Noun phrases extracted from a gazetteer are also straightforwardly represented as IOB tags. However, this does not fully solve the knowledge bottleneck problem. They also used the output of a morphological analyzer, which does not utilize encyclopedic knowledge. NER performance may be affected by segmentation errors in morphological analysis involving unknown words. Chinese word segmentation is often formalized as</context>
</contexts>
<marker>Kazama, Torisawa, 2008</marker>
<rawString>Jun’ichi Kazama and Kentaro Torisawa. 2008. Inducing gazetteers for named entity recognition by large-scale clustering of dependency relations. In Proc. of ACL 2008, pages 407–415, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Kaoru Yamamoto</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Applying conditional random fields to Japanese morphological analysis.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>230--237</pages>
<contexts>
<context position="1400" citStr="Kudo et al., 2004" startWordPosition="209" endWordPosition="212">ituents in text. For inference, we propose a novel block sampling procedure named hybrid type-based sampling, which has the ability to directly escape a local optimum that is not too distant from the global optimum. Experiments show that the proposed method efficiently corrects the initial segmentation given by a morphological analyzer. 1 Introduction Word segmentation is the first step of natural language processing for Japanese, Chinese and Thai because they do not delimit words by white-space. Segmentation for Japanese is a successful field of research, achieving the F-score of nearly 99% (Kudo et al., 2004). This success rests on a high-coverage dictionary. Unknown words, or words not covered by the dictionary, are often misidentified. Historically, researchers have devoted extensive human resources to build and maintain highcoverage dictionaries (Yokoi, 1995). Since the orthography of Japanese does not specify a standard for segmentation, researchers define their own criteria before constructing lexical resources. For this reason, it is difficult to exploit existing external resources, such as dictionaries and encyclopedias for human readers, where entry words are not segmented according to the</context>
<context position="4833" citStr="Kudo et al., 2004" startWordPosition="746" endWordPosition="749">sible state of the jointly sampled random variables, we only compare the current state with a proposed state. This greatly eases the sampling procedure while retaining the efficiency of typebased sampling. Experiments show that the proposed method quickly corrects the initial segmentation given by a morphological analyzer. 2 Related Work Japanese Morphological Analysis and Lexical Acquisition Word segmentation for Japanese is usually solved as the joint task of segmentation and part-of-speech tagging, which is called morphological analysis (Kurohashi et al., 1994; Asahara and Matsumoto, 2000; Kudo et al., 2004). The standard approach in Japanese morphological analysis is lattice-based path selection instead of characterbased IOB tagging. Given a sentence, an analyzer first builds a lattice of words with dictionary look-up and then selects an optimal path using pre-defined parameters. This approach enables fast decoding and achieves accuracy high enough for practical use. This success, however, depends on a highcoverage dictionary, and unknown words are often misidentified. Although a line of research attempts to identify unknown words on the fly (Uchimoto et al., 2001; Asahara and Matsumoto, 2004), </context>
<context position="29064" citStr="Kudo et al. (2004)" startWordPosition="4723" endWordPosition="4726">tatistically significant improvement with P &lt; 0.01. Evaluation Metrics We evaluated the segmentation accuracy of 500 entry titles. Specifically we evaluated the performance of a model with precision, recall and the F-score, all of which were based on tokens. We report the score of the most frequent segmentation among 10 samples. Following Lee et al. (2010), we report the best and median settings of hyperparameters based on the Fscore, in addition to inferred values. In order to evaluate the degree of difference between a pair of segmentations, we employed character-based evaluation. Following Kudo et al. (2004), we converted a word sequence into character-based BI labels and examined labeling disagreements. McNemar’s test of significance was based on this metric. 6.2 Results Table 1 shows segmentation accuracy of various models. One would notice that the baseline score is much lower than the score previously reported regarding newspaper articles (Kudo et al., 2004). It is because unlike newspaper articles, the titles of Wikipedia entries contain an unusually high proportion of unknown words. As suggested by relatively low precision, unknown words tend to be over-segmented by the morphological analyz</context>
</contexts>
<marker>Kudo, Yamamoto, Matsumoto, 2004</marker>
<rawString>Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto. 2004. Applying conditional random fields to Japanese morphological analysis. In Proc. of EMNLP 2004, pages 230–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Toshihisa Nakamura</author>
<author>Yuji Matsumoto</author>
<author>Makoto Nagao</author>
</authors>
<title>Improvements of Japanese morphological analyzer JUMAN.</title>
<date>1994</date>
<booktitle>In Proc. of The International Workshop on Sharable Natural Language Resources,</booktitle>
<pages>22--38</pages>
<contexts>
<context position="4784" citStr="Kurohashi et al., 1994" startWordPosition="738" endWordPosition="741">, instead of calculating the probability of every possible state of the jointly sampled random variables, we only compare the current state with a proposed state. This greatly eases the sampling procedure while retaining the efficiency of typebased sampling. Experiments show that the proposed method quickly corrects the initial segmentation given by a morphological analyzer. 2 Related Work Japanese Morphological Analysis and Lexical Acquisition Word segmentation for Japanese is usually solved as the joint task of segmentation and part-of-speech tagging, which is called morphological analysis (Kurohashi et al., 1994; Asahara and Matsumoto, 2000; Kudo et al., 2004). The standard approach in Japanese morphological analysis is lattice-based path selection instead of characterbased IOB tagging. Given a sentence, an analyzer first builds a lattice of words with dictionary look-up and then selects an optimal path using pre-defined parameters. This approach enables fast decoding and achieves accuracy high enough for practical use. This success, however, depends on a highcoverage dictionary, and unknown words are often misidentified. Although a line of research attempts to identify unknown words on the fly (Uchi</context>
</contexts>
<marker>Kurohashi, Nakamura, Matsumoto, Nagao, 1994</marker>
<rawString>Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto, and Makoto Nagao. 1994. Improvements of Japanese morphological analyzer JUMAN. In Proc. of The International Workshop on Sharable Natural Language Resources, pages 22–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoong Keok Lee</author>
<author>Aria Haghighi</author>
<author>Regina Barzilay</author>
</authors>
<title>Simple type-level unsupervised POS tagging.</title>
<date>2010</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>853--861</pages>
<contexts>
<context position="28804" citStr="Lee et al. (2010)" startWordPosition="4682" endWordPosition="4685">/79.23) 68.16 (63.64/73.37) 34.99 (47.05/27.86) unigram + TB + BM + RAND 57.44 (67.91/49.76) 50.86 (61.92/43.15) 42.31 (54.55/34.56) bigram + HTB + BM + RAND 84.03 (83.10/84.99) 70.46 (65.25/76.58) 40.16 (52.60/32.48) baseline (JUMAN) 80.09 (75.80/84.89) ** Statistically significant improvement with P &lt; 0.01. Evaluation Metrics We evaluated the segmentation accuracy of 500 entry titles. Specifically we evaluated the performance of a model with precision, recall and the F-score, all of which were based on tokens. We report the score of the most frequent segmentation among 10 samples. Following Lee et al. (2010), we report the best and median settings of hyperparameters based on the Fscore, in addition to inferred values. In order to evaluate the degree of difference between a pair of segmentations, we employed character-based evaluation. Following Kudo et al. (2004), we converted a word sequence into character-based BI labels and examined labeling disagreements. McNemar’s test of significance was based on this metric. 6.2 Results Table 1 shows segmentation accuracy of various models. One would notice that the baseline score is much lower than the score previously reported regarding newspaper article</context>
</contexts>
<marker>Lee, Haghighi, Barzilay, 2010</marker>
<rawString>Yoong Keok Lee, Aria Haghighi, and Regina Barzilay. 2010. Simple type-level unsupervised POS tagging. In Proc. of EMNLP 2010, pages 853–861.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongguo Li</author>
<author>Maosong Sun</author>
</authors>
<title>Punctuation as implicit annotations for Chinese word segmentation.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>4</issue>
<contexts>
<context position="24104" citStr="Li and Sun, 2009" startWordPosition="3980" endWordPosition="3983">. 5.4 Additional Constraints Partial annotations (Tsuboi et al., 2008; Neubig and Mori, 2010) can be used for inference. If we know in advance that a certain position is a boundary or nonboundary, we simply keep it unaltered. As partiallyannotated text, we can use markup. Suppose that the original text is written with wiki markup as follows: *JR[[宇野線]][[常山駅]] [gloss] JR Ube Line Tsuneyama Station It is clear that the position between “�” (line) and “&apos;m” (tsune) is a boundary. Similarly, we can impose our trivial rules of segmentation on the model. For example, we can keep punctuation markers (Li and Sun, 2009) separate from others. 6 Experiments 6.1 Settings Data Set We evaluated our approach on Japanese Wikipedia. For each entry of Wikipedia, we regarded the title as a noun phrase and used both the title and main text for segmentation. We separately applied our segmentation procedure to each entry. We constructed the data set as follows. We extracted each entry from an XML dump of Japanese Wikipedia.4 We normalized the title by dropping trailing parentheses that disambiguate entries with similar names (e.g. “X19 (21&apos;Ha)” for Akagi (aircraft carrier)). We extracted the main text from wikitext and u</context>
</contexts>
<marker>Li, Sun, 2009</marker>
<rawString>Zhongguo Li and Maosong Sun. 2009. Punctuation as implicit annotations for Chinese word segmentation. Computational Linguistics, 35(4):505–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael I Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Type-based MCMC.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL</booktitle>
<pages>573--581</pages>
<contexts>
<context position="3741" citStr="Liang et al., 2010" startWordPosition="583" endWordPosition="586">es not. To segment each noun phrase, we use nonparametric Bayesian language models (Goldwater et al., 2009; Mochihashi et al., 2009). Our approach 605 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 605–615, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics is based on two key factors: the bigram model and type-based block sampling. The bigram model alleviates a problem of the unigram model, that is, a tendency to misidentify a sequence of words in common collocations as a single word. Type-based sampling (Liang et al., 2010) has the ability to directly escape a local optimum, making inference very efficient. However, type-based sampling is not easily applicable to the bigram model owing to sparsity and its dependence on latent assignments. We propose a hybrid type-based sampling procedure, which combines the Metropolis-Hastings algorithm with Gibbs sampling. We circumvent the sparsity problem by joint sampling of unigram-level type. Also, instead of calculating the probability of every possible state of the jointly sampled random variables, we only compare the current state with a proposed state. This greatly eas</context>
<context position="16663" citStr="Liang et al., 2010" startWordPosition="2746" endWordPosition="2749">e AIP is a parameter for linear interpolation and PREF 1 is the unigram probability obtained from the annotated text. The loose coupling makes the models robust to an imbalanced pair of texts. Similarly, the back-off mixing bigram model replaces P1 in (2) with PBM 1 = AIPP1 + (1 − AIP)P REF 2 . 5 Inference Collapsed Gibbs sampling is widely used to find an optimal segmentation (Goldwater et al., 2009). In this section, we first show that simple collapsed sampling can hardly escape the initial segmentation. To address this problem, we apply a block sampling algorithm named type-based sampling (Liang et al., 2010) to the unigram model. Since type-based sampling is not applicable to the bigram model, we propose a novel sampling procedure for the bigram model, which we call hybrid type-based sampling. 5.1 Collapsed Sampling In collapsed Gibbs sampling, the sampler repeatedly samples every possible boundary position, conditioned on the current state of the rest of the corpus. It stochastically decides whether the corresponding local area consists of a single word w1 or two words w2w3 (w1 = w2.w3). The conditional probabilities can be derived from (1). Collapsed sampling is known for slow convergence. This</context>
<context position="17952" citStr="Liang et al., 2010" startWordPosition="2946" endWordPosition="2949">segmentation is given by a morphological analyzer. Since the analyzer deterministically segments text using pre-defined parameters, the resultant segmentation is fairly consistent. Segmentation errors involving unknown words also occur in a regular way. Intuitively, we start with a local optimum although it is not too distant from the global optimum. The collapsed Gibbs sampler is easily entrapped by this local optimum. For this reason, the initial segmentation is usually chosen at random (Goldwater et al., 2009). Sentence-based block sampling is also susceptible to consistent initialization (Liang et al., 2010). 5.2 Type-based Sampling To achieve fast convergence, we adopt a block sampling algorithm named type-based sampling (Liang et al., 2010). For the unigram model, a type-based sampler jointly samples multiple positions that share the same type. Two positions have the same type if the corresponding areas are both of the form w1 or w2w3. Type-based sampling takes advantage of the exchangeability of multiple positions with the same type. Given n positions with the same type, the sampler first samples the number of new boundaries m′ (0 &lt; m′ &lt; n), and then uniformly arranges m′ boundaries out of n p</context>
<context position="23162" citStr="Liang et al., 2010" startWordPosition="3820" endWordPosition="3823">y arrangement. In what follows, we only focus on flipped boundaries because the rest does not change the likelihood ratio of the current and proposed states. 3. Calculate the current conditional probability. This can be done by repeatedly applying (2) while removing words one-by-one and updating the model counts accordingly. 4. Calculate the proposed conditional probability while adding words one-by-one. 5. Decide whether to accept the proposal according to (4). If the proposal is accepted, we finalize the arrangement; otherwise we revert to the current state. We implement skip approximation (Liang et al., 2010) and sample each type once per iteration. This is motivated by the observation that although the 610 2. The main text is longer than 1,000 characters. 3. The title appears at least 5 times in the main text. joint sampling of a large number of positions is computationally expensive, the proposal is accepted very infrequently. 5.4 Additional Constraints Partial annotations (Tsuboi et al., 2008; Neubig and Mori, 2010) can be used for inference. If we know in advance that a certain position is a boundary or nonboundary, we simply keep it unaltered. As partiallyannotated text, we can use markup. Su</context>
</contexts>
<marker>Liang, Jordan, Klein, 2010</marker>
<rawString>Percy Liang, Michael I. Jordan, and Dan Klein. 2010. Type-based MCMC. In Proc. of NAACL 2010, pages 573–581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin Kiat Low</author>
<author>Hwee Tou Ng</author>
<author>Wenyuan Guo</author>
</authors>
<title>A maximum entropy approach to Chinese word segmentation.</title>
<date>2005</date>
<booktitle>In Proc. of the 4th SIGHAN Workshop,</booktitle>
<pages>161--164</pages>
<contexts>
<context position="7699" citStr="Low et al. (2005)" startWordPosition="1199" endWordPosition="1202">lize the NER task as the character-based labeling of IOB tags. Noun phrases extracted from a gazetteer are also straightforwardly represented as IOB tags. However, this does not fully solve the knowledge bottleneck problem. They also used the output of a morphological analyzer, which does not utilize encyclopedic knowledge. NER performance may be affected by segmentation errors in morphological analysis involving unknown words. Chinese word segmentation is often formalized as a character tagging problem (Xue, 2003). In this setting, it is easy to incorporate external resources into the model. Low et al. (2005) introduce an external dictionary as features of a discriminative model. However, they only use words up to 4 characters in length. We conjecture that words in their dictionary are not noun phrases. External resources used by 606 Peng et al. (2004) are also lists of short words and characters. Non-parametric Language Models Nonparametric Bayesian statistics offers an elegant solution to the task of unsupervised word segmentation, in which the vocabulary size is not known in advance (Goldwater et al., 2009; Mochihashi et al., 2009). It does not compete with supervised segmentation, however. Uns</context>
</contexts>
<marker>Low, Ng, Guo, 2005</marker>
<rawString>Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005. A maximum entropy approach to Chinese word segmentation. In Proc. of the 4th SIGHAN Workshop, pages 161–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daichi Mochihashi</author>
<author>Takeshi Yamada</author>
<author>Naonori Ueda</author>
</authors>
<title>Bayesian unsupervised word segmentation with nested Pitman-Yor language modeling.</title>
<date>2009</date>
<booktitle>In Proc. of ACL-IJCNLP</booktitle>
<pages>100--108</pages>
<contexts>
<context position="3254" citStr="Mochihashi et al., 2009" startWordPosition="506" endWordPosition="509">gmentation. We segment each entry noun phrase into words. To do this, we examine the main text of the entry, on the assumption that if the noun phrase in question consists of more than one word, its constituents appear in the main text either freely or as part of other noun phrases. For “常山城” (tsuneyama-jou), its constituent “常山” (tsune) appears by itself and as constituents of other nouns phrases such as “常山山 頂” (peak of Tsuneyama) and “常山駅” (Tsuneyama Station) while “山城” (yamashiro) does not. To segment each noun phrase, we use nonparametric Bayesian language models (Goldwater et al., 2009; Mochihashi et al., 2009). Our approach 605 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 605–615, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics is based on two key factors: the bigram model and type-based block sampling. The bigram model alleviates a problem of the unigram model, that is, a tendency to misidentify a sequence of words in common collocations as a single word. Type-based sampling (Liang et al., 2010) has the ability to directly escape a local optimum, making inference very efficient. However, type-based samplin</context>
<context position="8235" citStr="Mochihashi et al., 2009" startWordPosition="1286" endWordPosition="1289"> setting, it is easy to incorporate external resources into the model. Low et al. (2005) introduce an external dictionary as features of a discriminative model. However, they only use words up to 4 characters in length. We conjecture that words in their dictionary are not noun phrases. External resources used by 606 Peng et al. (2004) are also lists of short words and characters. Non-parametric Language Models Nonparametric Bayesian statistics offers an elegant solution to the task of unsupervised word segmentation, in which the vocabulary size is not known in advance (Goldwater et al., 2009; Mochihashi et al., 2009). It does not compete with supervised segmentation, however. Unsupervised word segmentation is used elsewhere, for example, with theoretical interest in children’s language acquisition (Johnson, 2008; Johnson and Demuth, 2010) and with the application to statistical machine translation, in which segmented text is merely an intermediate representation (Xu et al., 2008; Nguyen et al., 2010). In this paper we demonstrate that non-parametric models can complement supervised segmentation. 3 Japanese Noun Phrase Segmentation Our goal is to overcome the unknown word problem in morphological analysis </context>
<context position="13743" citStr="Mochihashi et al. (2009)" startWordPosition="2230" endWordPosition="2233">e model with the Chinese restaurant process. Unlike the unigram model, however, the bigram model depends on the latent table assignments z−i. h−i (wi−1,wi) + α1P1(wi|h−i) nh−i + α (wi−1,∗) tw−i P1(wi|h−i) =i + α0P0(wi) (3)h t∗ −i + α0 where h−i = (w−i, z−i), th−i wi is the number of tables labeled with wi and th−i ∗ is the total number of tables. Thanks to exchangeability, we do not need to track the exact seating assignments. Still, we need to maintain a histogram for each w that consists of frequencies of table customers (Blunsom et al., 2009). 4.3 Zerogram Model Following Nagata (1996) and Mochihashi et al. (2009), we model the zerogram distribution P0 with the word length k and the character sequence w = c1, · · · , ck. Specifically, we define P0 as the combination of a Poisson distribution with mean λ and a bigram distribution over characters. P0(w) = P(k; λ)P(c1, · · · , ck, k|O) P(k|O) k P(k; λ) = e−λ kl P(c1,··· ,ck,k|O) = k+1∏ P(ci|ci−1) i=1 O is the zerogram model, and c0 and ck+1 are a word boundary marker. P(k|O) can be estimated by randomly generating words from the model. We use different λ for different scripts. The Japanese writing system uses several scripts, and each word can be classifi</context>
<context position="15354" citStr="Mochihashi et al., 2009" startWordPosition="2512" endWordPosition="2515">s greatly simplifies inference but may make the model fragile with unknown words. For this reason, we set a hierarchical Pitman-Yor process prior (Teh, 2006; Goldwater et al., 2006) for the bigram probability P(ci|ci−1) with the base distribution of character unigrams. Note that even character bigrams are sparse because thousands of characters are used in Japanese. 4.4 Mixing an Annotated Corpus An annotated corpus can be used to force the models to stick with our segmentation criteria. A straightforward way to do this is to mix it with raw text while fixing the segmentation during inference (Mochihashi et al., 2009). A word found in the annotated corpus is generally preferred because it has fixed counts obtained from the annotated corpus. We call this method direct mixing. Direct mixing is problematic when raw text is much smaller than the annotated corpus. With this n P1(wi = w|w−i) = w−i w + α0P0 (1) n P2(wi|h−i) = (2) 608 situation, the role of raw text associated with the noun phrase in question is marginalized by the annotated corpus. As a solution to this problem, we propose another mixing method called back-off mixing. In back-off mixing, the annotated corpus is used as part of the base distributi</context>
</contexts>
<marker>Mochihashi, Yamada, Ueda, 2009</marker>
<rawString>Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda. 2009. Bayesian unsupervised word segmentation with nested Pitman-Yor language modeling. In Proc. of ACL-IJCNLP 2009, pages 100–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yugo Murawaki</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Online acquisition of Japanese unknown morphemes using morphological constraints.</title>
<date>2008</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>429--437</pages>
<contexts>
<context position="5867" citStr="Murawaki and Kurohashi, 2008" startWordPosition="900" endWordPosition="903">ighcoverage dictionary, and unknown words are often misidentified. Although a line of research attempts to identify unknown words on the fly (Uchimoto et al., 2001; Asahara and Matsumoto, 2004), it by no means provides a definitive solution because it suffers from locality of contextual information available for identification (Nakagawa and Matsumoto, 2006). Therefore we like to perform separate lexical acquisition processes in which wider context can be examined. Our approach in this paper has a complementary relationship with unknown word acquisition from text, which we previously proposed (Murawaki and Kurohashi, 2008). Since, unlike Chinese and Thai, Japanese is rich in morphology, morphological regularity can be used to determine if an unknown word candidate in text is indeed the word to be acquired. In general, this method works pretty well, but one exception is noun phrases. Noun phrases can hardly be distinguished from single nouns because in Japanese, no morphological marker is attached to join nouns to form a noun phrase. We previously resort to a heuristic measure to segment noun phrases. The new statistical method provides a straightforward solution to this problem. Meanwhile, our language models h</context>
<context position="36730" citStr="Murawaki and Kurohashi, 2008" startWordPosition="5952" endWordPosition="5956">was always referred to by the full name. 7 Conclusions In this paper, we proposed a new task of Japanese noun phrase segmentation. We adopted nonparametric Bayesian language models and proposed hybrid type-based sampling that can efficiently correct segmentation given by the morphological analyzer. Although supervised segmentation is very competitive, we showed that it can be supplemented with our unsupervised approach. We applied the proposed method to encyclopedic text to segment noun phrases in it. The proposed method can be applied to other tasks. For example, in unknown word acquisition (Murawaki and Kurohashi, 2008), noun phrases are often acquired from text as single words. We can now segment them into words in a more sophisticated way. In the future we will assign a POS tag to each word in order to use segmented noun phrases in morphological analysis. We assume that the meaning of constituents in a noun phrase rarely depends on outer context. So it would be helpful to augment them with rich semantic information in advance instead of disambiguating their meaning every time we analyze given text. Acknowledgments This work was partly supported by JST CREST. References Masayuki Asahara and Yuji Matsumoto. </context>
</contexts>
<marker>Murawaki, Kurohashi, 2008</marker>
<rawString>Yugo Murawaki and Sadao Kurohashi. 2008. Online acquisition of Japanese unknown morphemes using morphological constraints. In Proc. of EMNLP 2008, pages 429–437.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaaki Nagata</author>
</authors>
<title>Automatic extraction of new words from Japanese texts using generalized forwardbackward search.</title>
<date>1996</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>48--59</pages>
<contexts>
<context position="13714" citStr="Nagata (1996)" startWordPosition="2227" endWordPosition="2228">n again explain the model with the Chinese restaurant process. Unlike the unigram model, however, the bigram model depends on the latent table assignments z−i. h−i (wi−1,wi) + α1P1(wi|h−i) nh−i + α (wi−1,∗) tw−i P1(wi|h−i) =i + α0P0(wi) (3)h t∗ −i + α0 where h−i = (w−i, z−i), th−i wi is the number of tables labeled with wi and th−i ∗ is the total number of tables. Thanks to exchangeability, we do not need to track the exact seating assignments. Still, we need to maintain a histogram for each w that consists of frequencies of table customers (Blunsom et al., 2009). 4.3 Zerogram Model Following Nagata (1996) and Mochihashi et al. (2009), we model the zerogram distribution P0 with the word length k and the character sequence w = c1, · · · , ck. Specifically, we define P0 as the combination of a Poisson distribution with mean λ and a bigram distribution over characters. P0(w) = P(k; λ)P(c1, · · · , ck, k|O) P(k|O) k P(k; λ) = e−λ kl P(c1,··· ,ck,k|O) = k+1∏ P(ci|ci−1) i=1 O is the zerogram model, and c0 and ck+1 are a word boundary marker. P(k|O) can be estimated by randomly generating words from the model. We use different λ for different scripts. The Japanese writing system uses several scripts, </context>
</contexts>
<marker>Nagata, 1996</marker>
<rawString>Masaaki Nagata. 1996. Automatic extraction of new words from Japanese texts using generalized forwardbackward search. In Proc. of EMNLP 1996, pages 48– 59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuji Nakagawa</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Guessing parts-of-speech of unknown words using global information.</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL</booktitle>
<pages>705--712</pages>
<contexts>
<context position="5597" citStr="Nakagawa and Matsumoto, 2006" startWordPosition="861" endWordPosition="864"> a sentence, an analyzer first builds a lattice of words with dictionary look-up and then selects an optimal path using pre-defined parameters. This approach enables fast decoding and achieves accuracy high enough for practical use. This success, however, depends on a highcoverage dictionary, and unknown words are often misidentified. Although a line of research attempts to identify unknown words on the fly (Uchimoto et al., 2001; Asahara and Matsumoto, 2004), it by no means provides a definitive solution because it suffers from locality of contextual information available for identification (Nakagawa and Matsumoto, 2006). Therefore we like to perform separate lexical acquisition processes in which wider context can be examined. Our approach in this paper has a complementary relationship with unknown word acquisition from text, which we previously proposed (Murawaki and Kurohashi, 2008). Since, unlike Chinese and Thai, Japanese is rich in morphology, morphological regularity can be used to determine if an unknown word candidate in text is indeed the word to be acquired. In general, this method works pretty well, but one exception is noun phrases. Noun phrases can hardly be distinguished from single nouns becau</context>
</contexts>
<marker>Nakagawa, Matsumoto, 2006</marker>
<rawString>Tetsuji Nakagawa and Yuji Matsumoto. 2006. Guessing parts-of-speech of unknown words using global information. In Proc. of COLING-ACL 2006, pages 705– 712.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
<author>Shinsuke Mori</author>
</authors>
<title>Word-based partial annotation for efficient corpus construction.</title>
<date>2010</date>
<booktitle>In Proc. of LREC</booktitle>
<contexts>
<context position="23580" citStr="Neubig and Mori, 2010" startWordPosition="3889" endWordPosition="3892">de whether to accept the proposal according to (4). If the proposal is accepted, we finalize the arrangement; otherwise we revert to the current state. We implement skip approximation (Liang et al., 2010) and sample each type once per iteration. This is motivated by the observation that although the 610 2. The main text is longer than 1,000 characters. 3. The title appears at least 5 times in the main text. joint sampling of a large number of positions is computationally expensive, the proposal is accepted very infrequently. 5.4 Additional Constraints Partial annotations (Tsuboi et al., 2008; Neubig and Mori, 2010) can be used for inference. If we know in advance that a certain position is a boundary or nonboundary, we simply keep it unaltered. As partiallyannotated text, we can use markup. Suppose that the original text is written with wiki markup as follows: *JR[[宇野線]][[常山駅]] [gloss] JR Ube Line Tsuneyama Station It is clear that the position between “�” (line) and “&apos;m” (tsune) is a boundary. Similarly, we can impose our trivial rules of segmentation on the model. For example, we can keep punctuation markers (Li and Sun, 2009) separate from others. 6 Experiments 6.1 Settings Data Set We evaluated our </context>
</contexts>
<marker>Neubig, Mori, 2010</marker>
<rawString>Graham Neubig and Shinsuke Mori. 2010. Word-based partial annotation for efficient corpus construction. In Proc. of LREC 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ThuyLinh Nguyen</author>
<author>Stephan Vogel</author>
<author>Noah A Smith</author>
</authors>
<title>Nonparametric word segmentation for machine translation.</title>
<date>2010</date>
<booktitle>In Proc. of COLING 2010,</booktitle>
<pages>815--823</pages>
<contexts>
<context position="8626" citStr="Nguyen et al., 2010" startWordPosition="1342" endWordPosition="1345">ic Language Models Nonparametric Bayesian statistics offers an elegant solution to the task of unsupervised word segmentation, in which the vocabulary size is not known in advance (Goldwater et al., 2009; Mochihashi et al., 2009). It does not compete with supervised segmentation, however. Unsupervised word segmentation is used elsewhere, for example, with theoretical interest in children’s language acquisition (Johnson, 2008; Johnson and Demuth, 2010) and with the application to statistical machine translation, in which segmented text is merely an intermediate representation (Xu et al., 2008; Nguyen et al., 2010). In this paper we demonstrate that non-parametric models can complement supervised segmentation. 3 Japanese Noun Phrase Segmentation Our goal is to overcome the unknown word problem in morphological analysis by utilizing existing resources such as dictionaries and encyclopedias for human readers. In our settings, we are given a list of entries from external resources. Almost all of them are noun phrases and each entry consists of one or more words. A naive implementation would be to use noun phrases as they are. In fact, ipadic1 regards as single words a large number of long proper nouns like</context>
</contexts>
<marker>Nguyen, Vogel, Smith, 2010</marker>
<rawString>ThuyLinh Nguyen, Stephan Vogel, and Noah A. Smith. 2010. Nonparametric word segmentation for machine translation. In Proc. of COLING 2010, pages 815– 823.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Fangfang Feng</author>
<author>Andrew McCallum</author>
</authors>
<title>Chinese segmentation and new word detection using conditional random fields.</title>
<date>2004</date>
<booktitle>In Proc. of COLING ’04,</booktitle>
<pages>562--568</pages>
<contexts>
<context position="7947" citStr="Peng et al. (2004)" startWordPosition="1241" endWordPosition="1244">ut of a morphological analyzer, which does not utilize encyclopedic knowledge. NER performance may be affected by segmentation errors in morphological analysis involving unknown words. Chinese word segmentation is often formalized as a character tagging problem (Xue, 2003). In this setting, it is easy to incorporate external resources into the model. Low et al. (2005) introduce an external dictionary as features of a discriminative model. However, they only use words up to 4 characters in length. We conjecture that words in their dictionary are not noun phrases. External resources used by 606 Peng et al. (2004) are also lists of short words and characters. Non-parametric Language Models Nonparametric Bayesian statistics offers an elegant solution to the task of unsupervised word segmentation, in which the vocabulary size is not known in advance (Goldwater et al., 2009; Mochihashi et al., 2009). It does not compete with supervised segmentation, however. Unsupervised word segmentation is used elsewhere, for example, with theoretical interest in children’s language acquisition (Johnson, 2008; Johnson and Demuth, 2010) and with the application to statistical machine translation, in which segmented text </context>
</contexts>
<marker>Peng, Feng, McCallum, 2004</marker>
<rawString>Fuchun Peng, Fangfang Feng, and Andrew McCallum. 2004. Chinese segmentation and new word detection using conditional random fields. In Proc. of COLING ’04, pages 562–568.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
</authors>
<title>A Bayesian interpretation of interpolated Kneser-Ney.</title>
<date>2006</date>
<tech>Technical Report TRA2/06,</tech>
<institution>School of Computing, National University of Singapore.</institution>
<contexts>
<context position="14886" citStr="Teh, 2006" startWordPosition="2435" endWordPosition="2436">writing system uses several scripts, and each word can be classified by script such as hiragana, katakana, kanji, the mixture of hiragana and kanji, etc. The optimal value for λ depends on scripts. For example, katakana, which predominantly denotes loan words, is longer on average than hiragana, which is often used for short function words. We obtain the parameters and counts from an annotated corpus and fix them during noun phrase segmentation. This greatly simplifies inference but may make the model fragile with unknown words. For this reason, we set a hierarchical Pitman-Yor process prior (Teh, 2006; Goldwater et al., 2006) for the bigram probability P(ci|ci−1) with the base distribution of character unigrams. Note that even character bigrams are sparse because thousands of characters are used in Japanese. 4.4 Mixing an Annotated Corpus An annotated corpus can be used to force the models to stick with our segmentation criteria. A straightforward way to do this is to mix it with raw text while fixing the segmentation during inference (Mochihashi et al., 2009). A word found in the annotated corpus is generally preferred because it has fixed counts obtained from the annotated corpus. We cal</context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Yee Whye Teh. 2006. A Bayesian interpretation of interpolated Kneser-Ney. Technical Report TRA2/06, School of Computing, National University of Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuta Tsuboi</author>
<author>Hisashi Kashima</author>
<author>Shinsuke Mori</author>
<author>Hiroki Oda</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Training conditional random fields using incomplete annotations.</title>
<date>2008</date>
<booktitle>In Proc. of COLING</booktitle>
<pages>897--904</pages>
<contexts>
<context position="23556" citStr="Tsuboi et al., 2008" startWordPosition="3885" endWordPosition="3888">s one-by-one. 5. Decide whether to accept the proposal according to (4). If the proposal is accepted, we finalize the arrangement; otherwise we revert to the current state. We implement skip approximation (Liang et al., 2010) and sample each type once per iteration. This is motivated by the observation that although the 610 2. The main text is longer than 1,000 characters. 3. The title appears at least 5 times in the main text. joint sampling of a large number of positions is computationally expensive, the proposal is accepted very infrequently. 5.4 Additional Constraints Partial annotations (Tsuboi et al., 2008; Neubig and Mori, 2010) can be used for inference. If we know in advance that a certain position is a boundary or nonboundary, we simply keep it unaltered. As partiallyannotated text, we can use markup. Suppose that the original text is written with wiki markup as follows: *JR[[宇野線]][[常山駅]] [gloss] JR Ube Line Tsuneyama Station It is clear that the position between “�” (line) and “&apos;m” (tsune) is a boundary. Similarly, we can impose our trivial rules of segmentation on the model. For example, we can keep punctuation markers (Li and Sun, 2009) separate from others. 6 Experiments 6.1 Settings Da</context>
</contexts>
<marker>Tsuboi, Kashima, Mori, Oda, Matsumoto, 2008</marker>
<rawString>Yuta Tsuboi, Hisashi Kashima, Shinsuke Mori, Hiroki Oda, and Yuji Matsumoto. 2008. Training conditional random fields using incomplete annotations. In Proc. of COLING 2008, pages 897–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiyotaka Uchimoto</author>
<author>Satoshi Sekine</author>
<author>Hitoshi Isahara</author>
</authors>
<title>The unknown word problem: a morphological analysis of Japanese using maximum entropy aided by a dictionary.</title>
<date>2001</date>
<booktitle>In Proc. of EMNLP</booktitle>
<pages>91--99</pages>
<contexts>
<context position="5401" citStr="Uchimoto et al., 2001" startWordPosition="832" endWordPosition="835">1994; Asahara and Matsumoto, 2000; Kudo et al., 2004). The standard approach in Japanese morphological analysis is lattice-based path selection instead of characterbased IOB tagging. Given a sentence, an analyzer first builds a lattice of words with dictionary look-up and then selects an optimal path using pre-defined parameters. This approach enables fast decoding and achieves accuracy high enough for practical use. This success, however, depends on a highcoverage dictionary, and unknown words are often misidentified. Although a line of research attempts to identify unknown words on the fly (Uchimoto et al., 2001; Asahara and Matsumoto, 2004), it by no means provides a definitive solution because it suffers from locality of contextual information available for identification (Nakagawa and Matsumoto, 2006). Therefore we like to perform separate lexical acquisition processes in which wider context can be examined. Our approach in this paper has a complementary relationship with unknown word acquisition from text, which we previously proposed (Murawaki and Kurohashi, 2008). Since, unlike Chinese and Thai, Japanese is rich in morphology, morphological regularity can be used to determine if an unknown word</context>
</contexts>
<marker>Uchimoto, Sekine, Isahara, 2001</marker>
<rawString>Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara. 2001. The unknown word problem: a morphological analysis of Japanese using maximum entropy aided by a dictionary. In Proc. of EMNLP 2001, pages 91–99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Xu</author>
<author>Jianfeng Gao</author>
<author>Kristina Toutanova</author>
<author>Hermann Ney</author>
</authors>
<title>Bayesian semi-supervised Chinese word segmentation for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. of COLING</booktitle>
<pages>1017--1024</pages>
<contexts>
<context position="8604" citStr="Xu et al., 2008" startWordPosition="1338" endWordPosition="1341">ers. Non-parametric Language Models Nonparametric Bayesian statistics offers an elegant solution to the task of unsupervised word segmentation, in which the vocabulary size is not known in advance (Goldwater et al., 2009; Mochihashi et al., 2009). It does not compete with supervised segmentation, however. Unsupervised word segmentation is used elsewhere, for example, with theoretical interest in children’s language acquisition (Johnson, 2008; Johnson and Demuth, 2010) and with the application to statistical machine translation, in which segmented text is merely an intermediate representation (Xu et al., 2008; Nguyen et al., 2010). In this paper we demonstrate that non-parametric models can complement supervised segmentation. 3 Japanese Noun Phrase Segmentation Our goal is to overcome the unknown word problem in morphological analysis by utilizing existing resources such as dictionaries and encyclopedias for human readers. In our settings, we are given a list of entries from external resources. Almost all of them are noun phrases and each entry consists of one or more words. A naive implementation would be to use noun phrases as they are. In fact, ipadic1 regards as single words a large number of </context>
</contexts>
<marker>Xu, Gao, Toutanova, Ney, 2008</marker>
<rawString>Jia Xu, Jianfeng Gao, Kristina Toutanova, and Hermann Ney. 2008. Bayesian semi-supervised Chinese word segmentation for statistical machine translation. In Proc. of COLING 2008, pages 1017–1024.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Chinese word segmentation as character tagging.</title>
<date>2003</date>
<booktitle>Computational Linguistics and Chinese Language Processing,</booktitle>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="7602" citStr="Xue, 2003" startWordPosition="1184" endWordPosition="1185">from a gazetteer (e.g. Wikipedia) as features of a CRFbased Japanese NE tagger. They formalize the NER task as the character-based labeling of IOB tags. Noun phrases extracted from a gazetteer are also straightforwardly represented as IOB tags. However, this does not fully solve the knowledge bottleneck problem. They also used the output of a morphological analyzer, which does not utilize encyclopedic knowledge. NER performance may be affected by segmentation errors in morphological analysis involving unknown words. Chinese word segmentation is often formalized as a character tagging problem (Xue, 2003). In this setting, it is easy to incorporate external resources into the model. Low et al. (2005) introduce an external dictionary as features of a discriminative model. However, they only use words up to 4 characters in length. We conjecture that words in their dictionary are not noun phrases. External resources used by 606 Peng et al. (2004) are also lists of short words and characters. Non-parametric Language Models Nonparametric Bayesian statistics offers an elegant solution to the task of unsupervised word segmentation, in which the vocabulary size is not known in advance (Goldwater et al</context>
</contexts>
<marker>Xue, 2003</marker>
<rawString>Nianwen Xue. 2003. Chinese word segmentation as character tagging. Computational Linguistics and Chinese Language Processing, 8(1):29–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toshio Yokoi</author>
</authors>
<title>The EDR electronic dictionary.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="1658" citStr="Yokoi, 1995" startWordPosition="247" endWordPosition="248">ently corrects the initial segmentation given by a morphological analyzer. 1 Introduction Word segmentation is the first step of natural language processing for Japanese, Chinese and Thai because they do not delimit words by white-space. Segmentation for Japanese is a successful field of research, achieving the F-score of nearly 99% (Kudo et al., 2004). This success rests on a high-coverage dictionary. Unknown words, or words not covered by the dictionary, are often misidentified. Historically, researchers have devoted extensive human resources to build and maintain highcoverage dictionaries (Yokoi, 1995). Since the orthography of Japanese does not specify a standard for segmentation, researchers define their own criteria before constructing lexical resources. For this reason, it is difficult to exploit existing external resources, such as dictionaries and encyclopedias for human readers, where entry words are not segmented according to the criteria. Among them, encyclopedias are especially important in that they contain a lot of terms that a morphological dictionary fails to cover. Most of these terms are noun phrases and consist of more than one word (morpheme). For example, an encyclopedia </context>
</contexts>
<marker>Yokoi, 1995</marker>
<rawString>Toshio Yokoi. 1995. The EDR electronic dictionary. Communications of the ACM, 38(11):42–44.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>