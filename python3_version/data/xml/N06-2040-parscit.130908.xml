<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002463">
<title confidence="0.989824">
Sentence Planning for Realtime Navigational Instructions
</title>
<author confidence="0.703965">
Laura Stoia and Donna K. Byron and
</author>
<affiliation confidence="0.639088666666667">
Darla Magdalene Shockley and Eric Fosler-Lussier
The Ohio State University
Computer Science and Engineering
</affiliation>
<address confidence="0.673921">
2015 Neil Ave., Columbus, Ohio 43210
</address>
<email confidence="0.998968">
stoia|dbyron|shockley|fosler@cse.ohio-state.edu
</email>
<sectionHeader confidence="0.995641" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999773470588235">
In the current work, we focus on systems that
provide incremental directions and monitor
the progress of mobile users following those
directions. Such directions are based on dy-
namic quantities like the visibility of reference
points and their distance from the user. An
intelligent navigation assistant might take ad-
vantage of the user’s mobility within the set-
ting to achieve communicative goals, for ex-
ample, by repositioning him to a point from
which a description of the target is easier to
produce. Calculating spatial variables over a
corpus of human-human data developed for
this study, we trained a classifier to detect con-
texts in which a target object can be felici-
tously described. Our algorithm matched the
human subjects with 86% precision.
</bodyText>
<sectionHeader confidence="0.984941" genericHeader="categories and subject descriptors">
1 Introduction and Related Work
</sectionHeader>
<bodyText confidence="0.999888888888889">
Dialog agents have been developed for a variety of
navigation domains such as in-car driving directions
(Dale et al., 2003), tourist information portals (John-
ston et al., 2002) and pedestrian navigation (Muller,
2002). In all these applications, the human partner
receives navigation instructions from a system. For
these domains, contextual features of the physical
setting must be taken into account for the agent to
communicate successfully.
In dialog systems, one misunderstanding can of-
ten lead to additional errors (Moratz and Tenbrink,
2003), so the system must strategically choose in-
structions and referring expressions that can be
clearly understood by the user. Human cognition
studies have found that the in front of/behind axis
is easier to perceive than other relations (Bryant et
al., 1992). In navigation tasks, this suggests that de-
scribing an object when it is in front of the follower
is preferable to using other spatial relations. Studies
on direction-giving language have found that speak-
ers interleave repositioning commands (e.g. “Turn
right 90 degrees”) designating objects of interest
(e.g. “See that chair?”) and action commands (e.g.
“Keep going”)(Tversky and Lee, 1999). The con-
tent planner of a spoken dialog system must decide
which of these dialog moves to produce at each turn.
A route plan is a linked list of arcs between nodes
representing locations and decision-points in the
world. A direction-giving agent must perform sev-
eral content-planning and surface realization steps,
one of which is to decide how much of the route
to describe to the user at once (Dale et al., 2003).
Thus, the system selects the next target destination
and must describe it to the user. In an interactive
system, the generation agent must not only decide
what to say to the user but also when to say it.
</bodyText>
<sectionHeader confidence="0.979148" genericHeader="method">
2 Dialog Collection Procedure
</sectionHeader>
<bodyText confidence="0.95698">
Our task setup employs a virtual-reality (VR) world
in which one partner, the direction-follower (DF),
moves about in the world to perform a series of
tasks, such as pushing buttons to re-arrange ob-
jects in the room, picking up items, etc. The part-
ners communicated through headset microphones.
The simulated world was presented from first-person
perspective on a desk-top computer monitor. The
DF has no knowledge of the world map or tasks.
His partner, the direction-giver (DG), has a paper
2D map of the world and a list of tasks to complete.
During the task, the DG has instant feedback about
</bodyText>
<page confidence="0.976432">
157
</page>
<note confidence="0.909987">
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 157–160,
New York, June 2006. c�2006 Association for Computational Linguistics
</note>
<table confidence="0.709133">
video frame: 00:13:16
video frame: 00:15:12
video frame: 00:17:07
00:13:16 “keep going forward”
00:14:05 “ok, stop”
00:15:20 “turn right”
00:17:19: “and go through that door
[D6]”
</table>
<figureCaption confidence="0.983762">
Figure 1: An example sequence with repositioning
</figureCaption>
<bodyText confidence="0.816524">
DG: ok, yeah, go through that door [D9, locate]
</bodyText>
<subsectionHeader confidence="0.530808">
turn to your right
</subsectionHeader>
<bodyText confidence="0.94774325">
’mkay, and there’s a door [D11, vague]
in there um, go through the one
straight in front of you [D11, locate]
ok, stop... and then turn around and look at
the buttons [B18,B20,B21]
ok, you wanna push the button that’s there
on the left by the door [B18]
ok, and then go through the door [D10]
</bodyText>
<subsectionHeader confidence="0.319503">
look to your left
</subsectionHeader>
<bodyText confidence="0.600847">
there, in that cabinet there [C6, locate]
</bodyText>
<figureCaption confidence="0.990477">
Figure 2: Sample dialog fragment
</figureCaption>
<bodyText confidence="0.999875260869565">
the DF’s location in the VR world, via mirroring of
his partner’s screen on his own computer monitor.
The DF can change his position or orientation within
the virtual world independently of the DG’s direc-
tions, but since the DG knows the task, their collab-
oration is necessary. In this study, we are most inter-
ested in the behavior of the DG, since the algorithm
we develop emulates this role. Our paid participants
were recruited in pairs, and were self-identified na-
tive speakers of North American English.
The video output of DF’s computer was captured
to a camera, along with the audio stream from both
microphones. A logfile created by the VR engine
recorded the DF’s coordinates, gaze angle, and the
position of objects in the world. All 3 data sources
were synchronized using calibration markers. A
technical report is available (Byron, 2005) that de-
scribes the recording equipment and software used.
Figure 2 is a dialog fragment in which the DG
steers his partner to a cabinet, using both a sequence
of target objects and three additional repositioning
commands (in bold) to adjust his partner’s spatial
relationship with the target.
</bodyText>
<subsectionHeader confidence="0.994086">
2.1 Developing the Training Corpus
</subsectionHeader>
<bodyText confidence="0.999993322580645">
We recorded fifteen dialogs containing a total of
221 minutes of speech. The corpus was transcribed
and word-aligned. The dialogs were further anno-
tated using the Anvil tool (Kipp, 2004) to create a
set of target referring expressions. Because we are
interested in the spatial properties of the referents
of these target referring expressions, the items in-
cluded in this experiment were restricted to objects
with a defined spatial position (buttons, doors and
cabinets). We excluded plural referring expressions,
since their spatial properties are more complex, and
also expressions annotated as vague or abandoned.
Overall, the corpus contains 1736 markable items,
of which 87 were annotated as vague, 84 abandoned
and 228 sets.
We annotated each referring expression with a
boolean feature called Locate that indicates whether
the expression is the first one that allowed the fol-
lower to identify the object in the world, in other
words, the point at which joint spatial reference was
achieved. The kappa (Carletta, 1996) obtained on
this feature was 0.93. There were 466 referring ex-
pressions in the 15-dialog corpus that were anno-
tated TRUE for this feature.
The dataset used in the experiments is a consensus
version on which both annotators agreed on the set
of markables. Due to the constraints introduced by
the task, referent annotation achieved almost perfect
agreement. Annotators were allowed to look ahead
in the dialog to assign the referent. The data used in
the current study is only the DG’s language.
</bodyText>
<sectionHeader confidence="0.996302" genericHeader="method">
3 Algorithm Development
</sectionHeader>
<bodyText confidence="0.999705777777778">
The generation module receives as input a route plan
produced by a planning module, composed of a list
of graph nodes that represent the route. As each sub-
sequent target on the list is selected, content plan-
ning considers the tuple of variables ID, LOC
where ID is an identifier for the target and LOC is
the DF’s location (his Cartesian coordinates and ori-
entation angle). Target ID’s are always object id’s
to be visited in performing the task, such as a door
</bodyText>
<page confidence="0.961367">
158
</page>
<figure confidence="0.5366055">
= Visible area( )
= Angle to target
= distance to target
In this scene:
Distractors = 5
B1, B2, B3, C1, D1
VisDistracts = 3 B2, B3, C1
VisSemDistracts = 2 B2, B3
</figure>
<figureCaption confidence="0.991025">
Figure 3: An example configuration with spatial context fea-
tures. The target obje ct is B4 and [B1, B2, B3, B4, C1, D1] are
perceptually accessible.
</figureCaption>
<bodyText confidence="0.999784">
that the DF must pass through. The VR world up-
dates the value of LOC at a rate of 10 frames/sec.
Using these variables, the content planner must de-
cide whether the DF’s current location is appropriate
for producing a referring expression to describe the
object.
The following features are calculated from this in-
formation: absolute Angle between target and fol-
lower’s view direction, which implicitly gives the in
front relation, Distance from target, visible distrac-
tors (VisDistracts), visible distractors of the same
semantic category (VisSemDistracts), whether the
target is visible (boolean Visible), and the target’s
semantic category (Cat: button/door/cabinet). Fig-
ure 3 is an example spatial configuration with these
features identified.
</bodyText>
<subsectionHeader confidence="0.995733">
3.1 Decision Tree Training
</subsectionHeader>
<bodyText confidence="0.999914448275862">
Training examples from the annotation data are tu-
ples containing the ID of the annotated description,
the LOC of the DF at that moment (from the VR en-
gine log), and a class label: either Positive or Nega-
tive. Because we expect some latency between when
the DG judges that a felicity condition is met and
when he begins to speak, rather than using spatial
context features that co-occur with the onset of each
description, we averaged the values over a 0.3 sec-
ond window centered at the onset of the expression.
Negative contexts are difficult to identify since
they often do not manifest linguistically: the DG
may say nothing and allow the user to continue mov-
ing along his current vector, or he may issue a move-
ment command. A minimal criterion for producing
an expression that can achieve joint spatial reference
is that the addressee must have perceptual accessi-
bility to the item. Therefore, negative training exam-
ples for this experiment were selected from the time-
periods that elapsed between the follower achiev-
ing perceptual access to the object (coming into the
same room with it but not necessarily looking at it),
but before the Locating description was spoken. In
these negative examples, we consider the basic felic-
ity conditions for producing a descriptive reference
to the object to be met, yet the DG did not produce
a description. The dataset of 932 training examples
was balanced to contain 50% positive and 50% neg-
ative examples.
</bodyText>
<subsectionHeader confidence="0.999556">
3.2 Decision Tree Performance
</subsectionHeader>
<bodyText confidence="0.999947029411765">
This evaluation is based on our algorithm’s ability
to reproduce the linguistic behavior of our human
subjects, which may not be ideal behavior.
The Weka1 toolkit was used to build a decision
tree classifier (Witten and Frank, 2005). Figure 4
shows the resulting tree. 20% of the examples were
held out as test items, and 80% were used for train-
ing with 10 fold cross validation. Based on training
results, the tree was pruned to a minimum of 30 in-
stances per leaf. The final tree correctly classified
of the test data.
The number of positive and negative examples
was balanced, so the first baseline is 50%. To incor-
porate a more elaborate baseline, we consider that a
description will be made only if the referent is visi-
ble to the DF. Marking all cases where the referent
was visible as describe-id and all the other examples
as delay gives a higher baseline of 70%, still 16%
lower than the result of our tree.2
Previous findings in spatial cognition consider an-
gle, distance and shape as the key factors establish-
ing spatial relationships (Gapp, 1995), the angle de-
viation being the most important feature for projec-
tive spatial relationship. Our algorithm also selects
Angle and Distance as informative features. Vis-
Distracts is selected as the most important feature
by the tree, suggesting that having a large number
of objects to contrast makes the description harder,
which is in sync with human intuition. We note that
Visible is not selected, but that might be due to the
fact that it reduces to Angle . In terms of the
referring expression generation algorithm described
by (Reiter and Dale, 1992), in which the description
which eliminates the most distractors is selected, our
</bodyText>
<footnote confidence="0.9995595">
1http://www.cs.waikato.ac.nz/ml/weka/
2not all positive examples were visible
</footnote>
<page confidence="0.998067">
159
</page>
<bodyText confidence="0.9984932">
results suggest that the human subjects chose to re-
duce the size of the distractor set before producing a
description, presumably in order to reduce the com-
putational load required to calculate the optimal de-
scription.
</bodyText>
<table confidence="0.995309">
VisDistracts &lt;= 3
 |Angle &lt;= 33
  ||Distance &lt;=154: describe-id (308/27)
  ||Distance &gt; 154: delay (60/20)
 |Angle &gt; 33
  ||Distance &lt;= 90
   Angle &lt;=83:describe-id(79/20)
   Angle &gt; 83: delay (53/9)
 ||Distance &gt;90: delay(158/16)
VisDistracts &gt; 3: delay (114/1)
</table>
<figureCaption confidence="0.811274">
Figure 4: The decision tree obtained.
</figureCaption>
<table confidence="0.997955666666667">
Class Precision Recall F-measure
describe-id 0.822 0.925 0.871
delay 0.914 0.8 0.853
</table>
<tableCaption confidence="0.999621">
Table 1: Detailed Performance
</tableCaption>
<bodyText confidence="0.999952375">
The exact values of features shown in our deci-
sion tree are specific to our environment. However,
the features themselves are domain-independent and
are relevant for any spatial direction-giving task, and
their relative influence over the final decision may
transfer to a new domain. To incorporate our find-
ings in a system, we will monitor the user’s context
and plan a description only when our tree predicts it.
</bodyText>
<sectionHeader confidence="0.998805" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9999875">
We describe an experiment in content planning for
spoken dialog agents that provide navigation in-
structions. Navigation requires the system and the
user to achieve joint reference to objects in the envi-
ronment. To accomplish this goal human direction-
givers judge whether their partner is in an appropri-
ate spatial configuration to comprehend a reference
spoken to an object in the scene. If not, one strategy
for accomplishing the communicative goal is to steer
their partner into a position from which the object is
easier to describe.
The algorithm we developed in this study, which
takes into account spatial context features replicates
our human subject’s decision to produce a descrip-
tion with 86%, compared to a 70% baseline based
on the visibility of the object. Although the spatial
details will vary for other spoken dialog domains,
the process developed in this study for producing de-
scription dialog moves only at the appropriate times
should be relevant for spoken dialog agents operat-
ing in other navigation domains.
Building dialog agents for situated tasks provides
a wealth of opportunity to study the interaction be-
tween context and linguistic behavior. In the future,
the generation procedure for our interactive agent
will be further developed in areas such as spatial de-
scriptions and surface realization. We also plan to
investigate whether different object types in the do-
main require differential processing, as prior work
on spatial semantics would suggest.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="acknowledgments">
5 Acknowledgements
</sectionHeader>
<bodyText confidence="0.99904875">
We would like to thank the OSU CSE department for funding
this work, our participants in the study and to M. White and
our reviewers for useful comments on the paper. We also thank
Brad Mellen for building the virtual world.
</bodyText>
<sectionHeader confidence="0.99942" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999931390243903">
D. J. Bryant, B. Tversky, and N. Franklin. 1992. Internal and
external spatial frameworks representing described scenes.
Journal of Memory and Language, 31:74–98.
D. K. Byron. 2005. The OSU Quake 2004 corpus of two-
party situated problem-solving dialogs. Technical Report
OSU-CISRC-805-TR57, The Ohio State University Com-
puter Science and Engineering Department, Sept., 2005.
J. Carletta. 1996. Assessing agreement on classification tasks:
The kappa statistic. Computational Linguistics, 22(2):249–
254.
R. Dale, S. Geldof, and J. Prost. 2003. CORAL: Using natural
language generation for navigational assistance. In M. Oud-
shoorn, editor, Proceedings of the 26th Australasian Com-
puter Science Conference, Adelaide, Australia.
K. Gapp. 1995. Angle, distance, shape, and their relationship
to projective relations. Technical Report 115, Universitat des
Saarlandes.
M. Johnston, S. Bangalore, G. Vasireddy, A. Stent, P. Ehlen,
M. Walker, S. Whittaker, and P. Maloor. 2002. MATCH:
An architecture for multimodal dialogue systems. In Pro-
ceedings of the 40 Annual Meeting of the Association for
Computational Linguistics (ACL ’02), pages 376–383.
M. Kipp. 2004. Gesture Generation by Imitation - From Hu-
man Behavior to Computer Character Animation. Disserta-
tion.com.
R. Moratz and T. Tenbrink. 2003. Instruction modes for
joint spatial reference between naive users and a mobile
robot. In Proc. RISSP 2003 IEEE International Conference
on Robotics, Intelligent Systems and Signal Processing, Spe-
cial Session on New Methods in Human Robot Interaction.
C. Muller. 2002. Multimodal dialog in a pedestrian navi-
gation system. In Proceedings of ISCA Tutorial and Re-
search Workshop on Multi-Modal Dialogue in Mobile En-
vironments.
E. Reiter and R. Dale. 1992. A fast algorithm for the generation
of referring expressions. COLING.
B. Tversky and P. U. Lee. 1999. Pictorial and verbal tools for
conveying routes. Stade, Germany.
I. Witten and E. Frank. 2005. Data Mining: Practical machine
learning tools and techniques, 2nd Edition. Morgan Kauf-
mann, San Francisco.
</reference>
<page confidence="0.997486">
160
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.662111">
<title confidence="0.999778">Sentence Planning for Realtime Navigational Instructions</title>
<author confidence="0.913459">Laura Stoia</author>
<author confidence="0.913459">Donna K Byron Darla Magdalene Shockley</author>
<author confidence="0.913459">Eric</author>
<affiliation confidence="0.9301285">The Ohio State Computer Science and</affiliation>
<address confidence="0.997994">2015 Neil Ave., Columbus, Ohio</address>
<email confidence="0.99989">stoia|dbyron|shockley|fosler@cse.ohio-state.edu</email>
<abstract confidence="0.99742">In the current work, we focus on systems that provide incremental directions and monitor the progress of mobile users following those directions. Such directions are based on dynamic quantities like the visibility of reference points and their distance from the user. An intelligent navigation assistant might take advantage of the user’s mobility within the setting to achieve communicative goals, for example, by repositioning him to a point from which a description of the target is easier to produce. Calculating spatial variables over a corpus of human-human data developed for this study, we trained a classifier to detect contexts in which a target object can be felicitously described. Our algorithm matched the human subjects with 86% precision.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D J Bryant</author>
<author>B Tversky</author>
<author>N Franklin</author>
</authors>
<title>Internal and external spatial frameworks representing described scenes.</title>
<date>1992</date>
<journal>Journal of Memory and Language,</journal>
<pages>31--74</pages>
<contexts>
<context position="1886" citStr="Bryant et al., 1992" startWordPosition="281" endWordPosition="284">strian navigation (Muller, 2002). In all these applications, the human partner receives navigation instructions from a system. For these domains, contextual features of the physical setting must be taken into account for the agent to communicate successfully. In dialog systems, one misunderstanding can often lead to additional errors (Moratz and Tenbrink, 2003), so the system must strategically choose instructions and referring expressions that can be clearly understood by the user. Human cognition studies have found that the in front of/behind axis is easier to perceive than other relations (Bryant et al., 1992). In navigation tasks, this suggests that describing an object when it is in front of the follower is preferable to using other spatial relations. Studies on direction-giving language have found that speakers interleave repositioning commands (e.g. “Turn right 90 degrees”) designating objects of interest (e.g. “See that chair?”) and action commands (e.g. “Keep going”)(Tversky and Lee, 1999). The content planner of a spoken dialog system must decide which of these dialog moves to produce at each turn. A route plan is a linked list of arcs between nodes representing locations and decision-points</context>
</contexts>
<marker>Bryant, Tversky, Franklin, 1992</marker>
<rawString>D. J. Bryant, B. Tversky, and N. Franklin. 1992. Internal and external spatial frameworks representing described scenes. Journal of Memory and Language, 31:74–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D K Byron</author>
</authors>
<title>The OSU Quake</title>
<date>2005</date>
<tech>Technical Report OSU-CISRC-805-TR57,</tech>
<institution>The Ohio State University Computer Science and Engineering Department,</institution>
<contexts>
<context position="5245" citStr="Byron, 2005" startWordPosition="846" endWordPosition="847"> task, their collaboration is necessary. In this study, we are most interested in the behavior of the DG, since the algorithm we develop emulates this role. Our paid participants were recruited in pairs, and were self-identified native speakers of North American English. The video output of DF’s computer was captured to a camera, along with the audio stream from both microphones. A logfile created by the VR engine recorded the DF’s coordinates, gaze angle, and the position of objects in the world. All 3 data sources were synchronized using calibration markers. A technical report is available (Byron, 2005) that describes the recording equipment and software used. Figure 2 is a dialog fragment in which the DG steers his partner to a cabinet, using both a sequence of target objects and three additional repositioning commands (in bold) to adjust his partner’s spatial relationship with the target. 2.1 Developing the Training Corpus We recorded fifteen dialogs containing a total of 221 minutes of speech. The corpus was transcribed and word-aligned. The dialogs were further annotated using the Anvil tool (Kipp, 2004) to create a set of target referring expressions. Because we are interested in the sp</context>
</contexts>
<marker>Byron, 2005</marker>
<rawString>D. K. Byron. 2005. The OSU Quake 2004 corpus of twoparty situated problem-solving dialogs. Technical Report OSU-CISRC-805-TR57, The Ohio State University Computer Science and Engineering Department, Sept., 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: The kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>2</issue>
<pages>254</pages>
<contexts>
<context position="6598" citStr="Carletta, 1996" startWordPosition="1060" endWordPosition="1061">with a defined spatial position (buttons, doors and cabinets). We excluded plural referring expressions, since their spatial properties are more complex, and also expressions annotated as vague or abandoned. Overall, the corpus contains 1736 markable items, of which 87 were annotated as vague, 84 abandoned and 228 sets. We annotated each referring expression with a boolean feature called Locate that indicates whether the expression is the first one that allowed the follower to identify the object in the world, in other words, the point at which joint spatial reference was achieved. The kappa (Carletta, 1996) obtained on this feature was 0.93. There were 466 referring expressions in the 15-dialog corpus that were annotated TRUE for this feature. The dataset used in the experiments is a consensus version on which both annotators agreed on the set of markables. Due to the constraints introduced by the task, referent annotation achieved almost perfect agreement. Annotators were allowed to look ahead in the dialog to assign the referent. The data used in the current study is only the DG’s language. 3 Algorithm Development The generation module receives as input a route plan produced by a planning modu</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>J. Carletta. 1996. Assessing agreement on classification tasks: The kappa statistic. Computational Linguistics, 22(2):249– 254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
<author>S Geldof</author>
<author>J Prost</author>
</authors>
<title>CORAL: Using natural language generation for navigational assistance.</title>
<date>2003</date>
<booktitle>Proceedings of the 26th Australasian Computer Science Conference,</booktitle>
<editor>In M. Oudshoorn, editor,</editor>
<location>Adelaide, Australia.</location>
<contexts>
<context position="1204" citStr="Dale et al., 2003" startWordPosition="178" endWordPosition="181">n assistant might take advantage of the user’s mobility within the setting to achieve communicative goals, for example, by repositioning him to a point from which a description of the target is easier to produce. Calculating spatial variables over a corpus of human-human data developed for this study, we trained a classifier to detect contexts in which a target object can be felicitously described. Our algorithm matched the human subjects with 86% precision. 1 Introduction and Related Work Dialog agents have been developed for a variety of navigation domains such as in-car driving directions (Dale et al., 2003), tourist information portals (Johnston et al., 2002) and pedestrian navigation (Muller, 2002). In all these applications, the human partner receives navigation instructions from a system. For these domains, contextual features of the physical setting must be taken into account for the agent to communicate successfully. In dialog systems, one misunderstanding can often lead to additional errors (Moratz and Tenbrink, 2003), so the system must strategically choose instructions and referring expressions that can be clearly understood by the user. Human cognition studies have found that the in fro</context>
<context position="2694" citStr="Dale et al., 2003" startWordPosition="413" endWordPosition="416">ound that speakers interleave repositioning commands (e.g. “Turn right 90 degrees”) designating objects of interest (e.g. “See that chair?”) and action commands (e.g. “Keep going”)(Tversky and Lee, 1999). The content planner of a spoken dialog system must decide which of these dialog moves to produce at each turn. A route plan is a linked list of arcs between nodes representing locations and decision-points in the world. A direction-giving agent must perform several content-planning and surface realization steps, one of which is to decide how much of the route to describe to the user at once (Dale et al., 2003). Thus, the system selects the next target destination and must describe it to the user. In an interactive system, the generation agent must not only decide what to say to the user but also when to say it. 2 Dialog Collection Procedure Our task setup employs a virtual-reality (VR) world in which one partner, the direction-follower (DF), moves about in the world to perform a series of tasks, such as pushing buttons to re-arrange objects in the room, picking up items, etc. The partners communicated through headset microphones. The simulated world was presented from first-person perspective on a </context>
</contexts>
<marker>Dale, Geldof, Prost, 2003</marker>
<rawString>R. Dale, S. Geldof, and J. Prost. 2003. CORAL: Using natural language generation for navigational assistance. In M. Oudshoorn, editor, Proceedings of the 26th Australasian Computer Science Conference, Adelaide, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Gapp</author>
</authors>
<title>Angle, distance, shape, and their relationship to projective relations.</title>
<date>1995</date>
<tech>Technical Report 115,</tech>
<institution>Universitat des Saarlandes.</institution>
<contexts>
<context position="11204" citStr="Gapp, 1995" startWordPosition="1843" endWordPosition="1844"> instances per leaf. The final tree correctly classified of the test data. The number of positive and negative examples was balanced, so the first baseline is 50%. To incorporate a more elaborate baseline, we consider that a description will be made only if the referent is visible to the DF. Marking all cases where the referent was visible as describe-id and all the other examples as delay gives a higher baseline of 70%, still 16% lower than the result of our tree.2 Previous findings in spatial cognition consider angle, distance and shape as the key factors establishing spatial relationships (Gapp, 1995), the angle deviation being the most important feature for projective spatial relationship. Our algorithm also selects Angle and Distance as informative features. VisDistracts is selected as the most important feature by the tree, suggesting that having a large number of objects to contrast makes the description harder, which is in sync with human intuition. We note that Visible is not selected, but that might be due to the fact that it reduces to Angle . In terms of the referring expression generation algorithm described by (Reiter and Dale, 1992), in which the description which eliminates th</context>
</contexts>
<marker>Gapp, 1995</marker>
<rawString>K. Gapp. 1995. Angle, distance, shape, and their relationship to projective relations. Technical Report 115, Universitat des Saarlandes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnston</author>
<author>S Bangalore</author>
<author>G Vasireddy</author>
<author>A Stent</author>
<author>P Ehlen</author>
<author>M Walker</author>
<author>S Whittaker</author>
<author>P Maloor</author>
</authors>
<title>MATCH: An architecture for multimodal dialogue systems.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40 Annual Meeting of the Association for Computational Linguistics (ACL ’02),</booktitle>
<pages>376--383</pages>
<contexts>
<context position="1257" citStr="Johnston et al., 2002" startWordPosition="185" endWordPosition="189">bility within the setting to achieve communicative goals, for example, by repositioning him to a point from which a description of the target is easier to produce. Calculating spatial variables over a corpus of human-human data developed for this study, we trained a classifier to detect contexts in which a target object can be felicitously described. Our algorithm matched the human subjects with 86% precision. 1 Introduction and Related Work Dialog agents have been developed for a variety of navigation domains such as in-car driving directions (Dale et al., 2003), tourist information portals (Johnston et al., 2002) and pedestrian navigation (Muller, 2002). In all these applications, the human partner receives navigation instructions from a system. For these domains, contextual features of the physical setting must be taken into account for the agent to communicate successfully. In dialog systems, one misunderstanding can often lead to additional errors (Moratz and Tenbrink, 2003), so the system must strategically choose instructions and referring expressions that can be clearly understood by the user. Human cognition studies have found that the in front of/behind axis is easier to perceive than other re</context>
</contexts>
<marker>Johnston, Bangalore, Vasireddy, Stent, Ehlen, Walker, Whittaker, Maloor, 2002</marker>
<rawString>M. Johnston, S. Bangalore, G. Vasireddy, A. Stent, P. Ehlen, M. Walker, S. Whittaker, and P. Maloor. 2002. MATCH: An architecture for multimodal dialogue systems. In Proceedings of the 40 Annual Meeting of the Association for Computational Linguistics (ACL ’02), pages 376–383.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kipp</author>
</authors>
<title>Gesture Generation by Imitation - From Human Behavior to Computer Character Animation.</title>
<date>2004</date>
<publisher>Dissertation.com.</publisher>
<contexts>
<context position="5760" citStr="Kipp, 2004" startWordPosition="929" endWordPosition="930">urces were synchronized using calibration markers. A technical report is available (Byron, 2005) that describes the recording equipment and software used. Figure 2 is a dialog fragment in which the DG steers his partner to a cabinet, using both a sequence of target objects and three additional repositioning commands (in bold) to adjust his partner’s spatial relationship with the target. 2.1 Developing the Training Corpus We recorded fifteen dialogs containing a total of 221 minutes of speech. The corpus was transcribed and word-aligned. The dialogs were further annotated using the Anvil tool (Kipp, 2004) to create a set of target referring expressions. Because we are interested in the spatial properties of the referents of these target referring expressions, the items included in this experiment were restricted to objects with a defined spatial position (buttons, doors and cabinets). We excluded plural referring expressions, since their spatial properties are more complex, and also expressions annotated as vague or abandoned. Overall, the corpus contains 1736 markable items, of which 87 were annotated as vague, 84 abandoned and 228 sets. We annotated each referring expression with a boolean f</context>
</contexts>
<marker>Kipp, 2004</marker>
<rawString>M. Kipp. 2004. Gesture Generation by Imitation - From Human Behavior to Computer Character Animation. Dissertation.com.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Moratz</author>
<author>T Tenbrink</author>
</authors>
<title>Instruction modes for joint spatial reference between naive users and a mobile robot.</title>
<date>2003</date>
<booktitle>In Proc. RISSP 2003 IEEE International Conference on Robotics, Intelligent Systems and Signal Processing, Special Session on New Methods in Human Robot Interaction.</booktitle>
<contexts>
<context position="1629" citStr="Moratz and Tenbrink, 2003" startWordPosition="240" endWordPosition="243">atched the human subjects with 86% precision. 1 Introduction and Related Work Dialog agents have been developed for a variety of navigation domains such as in-car driving directions (Dale et al., 2003), tourist information portals (Johnston et al., 2002) and pedestrian navigation (Muller, 2002). In all these applications, the human partner receives navigation instructions from a system. For these domains, contextual features of the physical setting must be taken into account for the agent to communicate successfully. In dialog systems, one misunderstanding can often lead to additional errors (Moratz and Tenbrink, 2003), so the system must strategically choose instructions and referring expressions that can be clearly understood by the user. Human cognition studies have found that the in front of/behind axis is easier to perceive than other relations (Bryant et al., 1992). In navigation tasks, this suggests that describing an object when it is in front of the follower is preferable to using other spatial relations. Studies on direction-giving language have found that speakers interleave repositioning commands (e.g. “Turn right 90 degrees”) designating objects of interest (e.g. “See that chair?”) and action c</context>
</contexts>
<marker>Moratz, Tenbrink, 2003</marker>
<rawString>R. Moratz and T. Tenbrink. 2003. Instruction modes for joint spatial reference between naive users and a mobile robot. In Proc. RISSP 2003 IEEE International Conference on Robotics, Intelligent Systems and Signal Processing, Special Session on New Methods in Human Robot Interaction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Muller</author>
</authors>
<title>Multimodal dialog in a pedestrian navigation system.</title>
<date>2002</date>
<booktitle>In Proceedings of ISCA Tutorial and Research Workshop on Multi-Modal Dialogue in Mobile Environments.</booktitle>
<contexts>
<context position="1298" citStr="Muller, 2002" startWordPosition="193" endWordPosition="194"> goals, for example, by repositioning him to a point from which a description of the target is easier to produce. Calculating spatial variables over a corpus of human-human data developed for this study, we trained a classifier to detect contexts in which a target object can be felicitously described. Our algorithm matched the human subjects with 86% precision. 1 Introduction and Related Work Dialog agents have been developed for a variety of navigation domains such as in-car driving directions (Dale et al., 2003), tourist information portals (Johnston et al., 2002) and pedestrian navigation (Muller, 2002). In all these applications, the human partner receives navigation instructions from a system. For these domains, contextual features of the physical setting must be taken into account for the agent to communicate successfully. In dialog systems, one misunderstanding can often lead to additional errors (Moratz and Tenbrink, 2003), so the system must strategically choose instructions and referring expressions that can be clearly understood by the user. Human cognition studies have found that the in front of/behind axis is easier to perceive than other relations (Bryant et al., 1992). In navigat</context>
</contexts>
<marker>Muller, 2002</marker>
<rawString>C. Muller. 2002. Multimodal dialog in a pedestrian navigation system. In Proceedings of ISCA Tutorial and Research Workshop on Multi-Modal Dialogue in Mobile Environments.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
<author>R Dale</author>
</authors>
<title>A fast algorithm for the generation of referring expressions.</title>
<date>1992</date>
<publisher>COLING.</publisher>
<contexts>
<context position="11758" citStr="Reiter and Dale, 1992" startWordPosition="1933" endWordPosition="1936">e as the key factors establishing spatial relationships (Gapp, 1995), the angle deviation being the most important feature for projective spatial relationship. Our algorithm also selects Angle and Distance as informative features. VisDistracts is selected as the most important feature by the tree, suggesting that having a large number of objects to contrast makes the description harder, which is in sync with human intuition. We note that Visible is not selected, but that might be due to the fact that it reduces to Angle . In terms of the referring expression generation algorithm described by (Reiter and Dale, 1992), in which the description which eliminates the most distractors is selected, our 1http://www.cs.waikato.ac.nz/ml/weka/ 2not all positive examples were visible 159 results suggest that the human subjects chose to reduce the size of the distractor set before producing a description, presumably in order to reduce the computational load required to calculate the optimal description. VisDistracts &lt;= 3 |Angle &lt;= 33 ||Distance &lt;=154: describe-id (308/27) ||Distance &gt; 154: delay (60/20) |Angle &gt; 33 ||Distance &lt;= 90 Angle &lt;=83:describe-id(79/20) Angle &gt; 83: delay (53/9) ||Distance &gt;90: delay(158/16) V</context>
</contexts>
<marker>Reiter, Dale, 1992</marker>
<rawString>E. Reiter and R. Dale. 1992. A fast algorithm for the generation of referring expressions. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Tversky</author>
<author>P U Lee</author>
</authors>
<title>Pictorial and verbal tools for conveying routes.</title>
<date>1999</date>
<location>Stade, Germany.</location>
<contexts>
<context position="2279" citStr="Tversky and Lee, 1999" startWordPosition="340" endWordPosition="343">ically choose instructions and referring expressions that can be clearly understood by the user. Human cognition studies have found that the in front of/behind axis is easier to perceive than other relations (Bryant et al., 1992). In navigation tasks, this suggests that describing an object when it is in front of the follower is preferable to using other spatial relations. Studies on direction-giving language have found that speakers interleave repositioning commands (e.g. “Turn right 90 degrees”) designating objects of interest (e.g. “See that chair?”) and action commands (e.g. “Keep going”)(Tversky and Lee, 1999). The content planner of a spoken dialog system must decide which of these dialog moves to produce at each turn. A route plan is a linked list of arcs between nodes representing locations and decision-points in the world. A direction-giving agent must perform several content-planning and surface realization steps, one of which is to decide how much of the route to describe to the user at once (Dale et al., 2003). Thus, the system selects the next target destination and must describe it to the user. In an interactive system, the generation agent must not only decide what to say to the user but </context>
</contexts>
<marker>Tversky, Lee, 1999</marker>
<rawString>B. Tversky and P. U. Lee. 1999. Pictorial and verbal tools for conveying routes. Stade, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Witten</author>
<author>E Frank</author>
</authors>
<title>Data Mining: Practical machine learning tools and techniques, 2nd Edition.</title>
<date>2005</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco.</location>
<contexts>
<context position="10380" citStr="Witten and Frank, 2005" startWordPosition="1694" endWordPosition="1697"> but not necessarily looking at it), but before the Locating description was spoken. In these negative examples, we consider the basic felicity conditions for producing a descriptive reference to the object to be met, yet the DG did not produce a description. The dataset of 932 training examples was balanced to contain 50% positive and 50% negative examples. 3.2 Decision Tree Performance This evaluation is based on our algorithm’s ability to reproduce the linguistic behavior of our human subjects, which may not be ideal behavior. The Weka1 toolkit was used to build a decision tree classifier (Witten and Frank, 2005). Figure 4 shows the resulting tree. 20% of the examples were held out as test items, and 80% were used for training with 10 fold cross validation. Based on training results, the tree was pruned to a minimum of 30 instances per leaf. The final tree correctly classified of the test data. The number of positive and negative examples was balanced, so the first baseline is 50%. To incorporate a more elaborate baseline, we consider that a description will be made only if the referent is visible to the DF. Marking all cases where the referent was visible as describe-id and all the other examples as </context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>I. Witten and E. Frank. 2005. Data Mining: Practical machine learning tools and techniques, 2nd Edition. Morgan Kaufmann, San Francisco.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>