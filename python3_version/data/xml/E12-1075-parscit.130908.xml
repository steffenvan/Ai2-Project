<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001059">
<title confidence="0.995273">
Syntax-Based Word Ordering Incorporating a Large-Scale Language
Model
</title>
<author confidence="0.99859">
Yue Zhang Graeme Blackwood Stephen Clark
</author>
<affiliation confidence="0.9749025">
University of Cambridge University of Cambridge University of Cambridge
Computer Laboratory Engineering Department Computer Laboratory
</affiliation>
<email confidence="0.990943">
yz360@cam.ac.uk gwb24@eng.cam.ac.uk sc609@cam.ac.uk
</email>
<sectionHeader confidence="0.997268" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999518318181818">
A fundamental problem in text generation
is word ordering. Word ordering is a com-
putationally difficult problem, which can
be constrained to some extent for particu-
lar applications, for example by using syn-
chronous grammars for statistical machine
translation. There have been some recent
attempts at the unconstrained problem of
generating a sentence from a multi-set of
input words (Wan et al., 2009; Zhang and
Clark, 2011). By using CCG and learn-
ing guided search, Zhang and Clark re-
ported the highest scores on this task. One
limitation of their system is the absence
of an N-gram language model, which has
been used by text generation systems to
improve fluency. We take the Zhang and
Clark system as the baseline, and incor-
porate an N-gram model by applying on-
line large-margin training. Our system sig-
nificantly improved on the baseline by 3.7
BLEU points.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99975684">
One fundamental problem in text generation is
word ordering, which can be abstractly formu-
lated as finding a grammatical order for a multi-
set of words. The word ordering problem can also
include word choice, where only a subset of the
input words are used to produce the output.
Word ordering is a difficult problem. Finding
the best permutation for a set of words accord-
ing to a bigram language model, for example, is
NP-hard, which can be proved by linear reduction
from the traveling salesman problem. In prac-
tice, exploring the whole search space of permu-
tations is often prevented by adding constraints.
In phrase-based machine translation (Koehn et al.,
2003; Koehn et al., 2007), a distortion limit is
used to constrain the position of output phrases.
In syntax-based machine translation systems such
as Wu (1997) and Chiang (2007), synchronous
grammars limit the search space so that poly-
nomial time inference is feasible. In fluency
improvement (Blackwood et al., 2010), parts of
translation hypotheses identified as having high
local confidence are held fixed, so that word or-
dering elsewhere is strictly local.
Some recent work attempts to address the fun-
damental word ordering task directly, using syn-
tactic models and heuristic search. Wan et al.
(2009) uses a dependency grammar to solve word
ordering, and Zhang and Clark (2011) uses CCG
(Steedman, 2000) for word ordering and word
choice. The use of syntax models makes their
search problems harder than word permutation us-
ing an N-gram language model only. Both meth-
ods apply heuristic search. Zhang and Clark de-
veloped a bottom-up best-first algorithm to build
output syntax trees from input words, where
search is guided by learning for both efficiency
and accuracy. The framework is flexible in allow-
ing a large range of constraints to be added for
particular tasks.
We extend the work of Zhang and Clark (2011)
(Z&amp;C) in two ways. First, we apply online large-
margin training to guide search. Compared to the
perceptron algorithm on “constituent level fea-
tures” by Z&amp;C, our training algorithm is theo-
retically more elegant (see Section 3) and con-
verges more smoothly empirically (see Section 5).
Using online large-margin training not only im-
proves the output quality, but also allows the in-
corporation of an N-gram language-model into
</bodyText>
<page confidence="0.973356">
736
</page>
<note confidence="0.9766385">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 736–746,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999451684210526">
the system. N-gram models have been used as a
standard component in statistical machine trans-
lation, but have not been applied to the syntac-
tic model of Z&amp;C. Intuitively, an N-gram model
can improve local fluency when added to a syntax
model. Our experiments show that a four-gram
model trained using the English GigaWord cor-
pus gave improvements when added to the syntax-
based baseline system.
The contributions of this paper are as follows.
First, we improve on the performance of the Z&amp;C
system for the challenging task of the general
word ordering problem. Second, we develop a
novel method for incorporating a large-scale lan-
guage model into a syntax-based generation sys-
tem. Finally, we analyse large-margin training in
the context of learning-guided best-first search,
offering a novel solution to this computationally
hard problem.
</bodyText>
<sectionHeader confidence="0.9828405" genericHeader="introduction">
2 The statistical model and decoding
algorithm
</sectionHeader>
<bodyText confidence="0.999993416666667">
We take Z&amp;C as our baseline system. Given
a multi-set of input words, the baseline system
builds a CCG derivation by choosing and ordering
words from the input set. The scoring model is
trained using CCGBank (Hockenmaier and Steed-
man, 2007), and best-first decoding is applied. We
apply the same decoding framework in this paper,
but apply an improved training process, and incor-
porate an N-gram language model into the syntax
model. In this section, we describe and discuss
the baseline statistical model and decoding frame-
work, motivating our extensions.
</bodyText>
<subsectionHeader confidence="0.894534">
2.1 Combinatory Categorial Grammar
</subsectionHeader>
<bodyText confidence="0.999933761904762">
CCG, and parsing with CCG, has been described
elsewhere (Clark and Curran, 2007; Hockenmaier
and Steedman, 2002); here we provide only a
short description.
CCG (Steedman, 2000) is a lexicalized gram-
mar formalism, which associates each word in a
sentence with a lexical category. There is a small
number of basic lexical categories, such as noun
(N), noun phrase (NP), and prepositional phrase
(PP). Complex lexical categories are formed re-
cursively from basic categories and slashes, which
indicate the directions of arguments. The CCG
grammar used by our system is read off the deriva-
tions in CCGbank, following Hockenmaier and
Steedman (2002), meaning that the CCG combina-
tory rules are encoded as rule instances, together
with a number of additional rules which deal with
punctuation and type-changing. Given a sentence,
its CCG derivation can be produced by first assign-
ing a lexical category to each word, and then re-
cursively applying CCG rules bottom-up.
</bodyText>
<subsectionHeader confidence="0.998668">
2.2 The decoding algorithm
</subsectionHeader>
<bodyText confidence="0.999961166666667">
In the decoding algorithm, a hypothesis is an
edge, which corresponds to a sub-tree in a CCG
derivation. Edges are built bottom-up, starting
from leaf edges, which are generated by assigning
all possible lexical categories to each input word.
Each leaf edge corresponds to an input word with
a particular lexical category. Two existing edges
can be combined if there exists a CCG rule which
combines their category labels, and if they do not
contain the same input word more times than its
total count in the input. The resulting edge is as-
signed a category label according to the combi-
natory rule, and covers the concatenated surface
strings of the two sub-edges in their order or com-
bination. New edges can also be generated by ap-
plying unary rules to a single existing edge. Start-
ing from the leaf edges, the bottom-up process is
repeated until a goal edge is found, and its surface
string is taken as the output.
This derivation-building process is reminiscent
of a bottom-up CCG parser in the edge combina-
tion mechanism. However, it is fundamentally
different from a bottom-up parser. Since, for
the generation problem, the order of two edges
in their combination is flexible, the search prob-
lem is much harder than that of a parser. With
no input order specified, no efficient dynamic-
programming algorithm is available, and less con-
textual information is available for disambigua-
tion due to the lack of an input string.
In order to combat the large search space, best-
first search is applied, where candidate hypothe-
ses are ordered by their scores, and kept in an
agenda, and a limited number of accepted hy-
potheses are recorded in a chart. Here the chart
is essentially a set of beams, each of which con-
tains the highest scored edges covering a particu-
lar number of words. Initially, all leaf edges are
generated and scored, before they are put onto the
agenda. During each step in the decoding process,
the top edge from the agenda is expanded. If it is
a goal edge, it is returned as the output, and the
</bodyText>
<page confidence="0.99097">
737
</page>
<construct confidence="0.528063">
Algorithm 1 The decoding algorithm.
</construct>
<equation confidence="0.98770816">
a +— INITAGENDA( )
c +— INITCHART( )
while not TIMEOUT() do
new +— []
e +— POPBEST(a)
if GOALTEST(e) then
return e
end if
for e′ E UNARY(e, grammar) do
APPEND(new, e)
end for
for e� E c do
if CANCOMBINE(e, �e) then
e′ +— BINARY(e, �e, grammar)
APPEND(new, e′)
end if
if CANCOMBINE(�e, e) then
e′ +— BINARY(�e, e, grammar)
APPEND(new, e′)
end if
end for
for e′ E new do
ADD(a, e′)
end for
ADD(c, e)
</equation>
<bodyText confidence="0.968430933333333">
end while
decoding finishes. Otherwise it is extended with
unary rules, and combined with existing edges in
the chart using binary rules to produce new edges.
The resulting edges are scored and put onto the
agenda, while the original edge is put onto the
chart. The process repeats until a goal edge is
found, or a timeout limit is reached. In the latter
case, a default output is produced using existing
edges in the chart.
Pseudocode for the decoder is shown as Algo-
rithm 1. Again it is reminiscent of a best-first
parser (Caraballo and Charniak, 1998) in the use
of an agenda and a chart, but is fundamentally dif-
ferent due to the fact that there is no input order.
</bodyText>
<subsectionHeader confidence="0.999666">
2.3 Statistical model and feature templates
</subsectionHeader>
<bodyText confidence="0.9998955">
The baseline system uses a linear model to score
hypotheses. For an edge e, its score is defined as:
</bodyText>
<equation confidence="0.996878">
f(e) = 4b(e) · θ,
</equation>
<bodyText confidence="0.9999575">
where 4b(e) represents the feature vector of e and
θ is the parameter vector of the model.
During decoding, feature vectors are computed
incrementally. When an edge is constructed, its
score is computed from the scores of its sub-edges
and the incrementally added structure:
</bodyText>
<equation confidence="0.990938">
f(e) = -b(e) · θ
= �� 1: -b(es)) + φ(e)) · θ
e3Ee
= ( 1: -b(es) · θ) + φ(e) · θ
e3Ee
= (1: f(es)) + φ(e) · θ
e3Ee
</equation>
<bodyText confidence="0.999971617647059">
In the equation, es E e represents a sub-edge of
e. Leaf edges do not have any sub-edges. Unary-
branching edges have one sub-edge, and binary-
branching edges have two sub-edges. The fea-
ture vector φ(e) represents the incremental struc-
ture when e is constructed over its sub-edges.
It is called the “constituent-level feature vector”
by Z&amp;C. For leaf edges, φ(e) includes informa-
tion about the lexical category label; for unary-
branching edges, φ(e) includes information from
the unary rule; for binary-branching edges, φ(e)
includes information from the binary rule, and ad-
ditionally the token, POS and lexical category bi-
grams and trigrams that result from the surface
string concatenation of its sub-edges. The score
f(e) is therefore the sum of f(es) (for all es E e)
plus φ(e)·θ. The feature templates we use are the
same as those in the baseline system.
An important aspect of the scoring model is that
edges with different sizes are compared with each
other during decoding. Edges with different sizes
can have different numbers of features, which can
make the training of a discriminative model more
difficult. For example, a leaf edge with one word
can be compared with an edge over the entire in-
put. One way of reducing the effect of the size dif-
ference is to include the size of the edge as part of
feature definitions, which can improve the compa-
rability of edges of different sizes by reducing the
number of features they have in common. Such
features are applied by Z&amp;C, and we make use of
them here. Even with such features, the question
of whether edges with different sizes are linearly
separable is an empirical one.
</bodyText>
<sectionHeader confidence="0.996328" genericHeader="method">
3 Training
</sectionHeader>
<bodyText confidence="0.998584">
The efficiency of the decoding algorithm is de-
pendent on the statistical model, since the best-
</bodyText>
<page confidence="0.988291">
738
</page>
<bodyText confidence="0.999992490196078">
first search is guided to a solution by the model,
and a good model will lead to a solution being
found more quickly. In the ideal situation for the
best-first decoding algorithm, the model is perfect
and the score of any gold-standard edge is higher
than the score of any non-gold-standard edge. As
a result, the top edge on the agenda is always a
gold-standard edge, and therefore all edges on the
chart are gold-standard before the gold-standard
goal edge is found. In this oracle procedure, the
minimum number of edges is expanded, and the
output is correct. The best-first decoder is perfect
in not only accuracy, but also speed. In practice
this ideal situation is rarely met, but it determines
the goal of the training algorithm: to produce the
perfect model and hence decoder.
If we take gold-standard edges as positive ex-
amples, and non-gold-standard edges as negative
examples, the goal of the training problem can be
viewed as finding a large separating margin be-
tween the scores of positive and negative exam-
ples. However, it is infeasible to generate the full
space of negative examples, which is factorial in
the size of input. Like Z&amp;C, we apply online
learning, and generate negative examples based
on the decoding algorithm.
Our training algorithm is shown as Algo-
rithm 2. The algorithm is based on the decoder,
where an agenda is used as a priority queue of
edges to be expanded, and a set of accepted edges
is kept in a chart. Similar to the decoding algo-
rithm, the agenda is intialized using all possible
leaf edges. During each step, the top of the agenda
e is popped. If it is a gold-standard edge, it is ex-
panded in exactly the same way as the decoder,
with the newly generated edges being put onto
the agenda, and e being inserted into the chart.
If e is not a gold-standard edge, we take it as a
negative example e−, and take the lowest scored
gold-standard edge on the agenda e+ as a positive
example, in order to make an udpate to the model
parameter vector θ. Our parameter update algo-
rithm is different from the baseline perceptron al-
gorithm, as will be discussed later. After updating
the parameters, the scores of agenda edges above
and including e−, together with all chart edges,
are updated, and e− is discarded before the start
of the next processing step. By not putting any
non-gold-standard edges onto the chart, the train-
ing speed is much faster; on the other hand a wide
range of negative examples is pruned. We leave
</bodyText>
<figure confidence="0.763772666666667">
Algorithm 2 The training algorithm.
a ← INITAGENDA( )
c ← INITCHART( )
while not TIMEOUT() do
new ← []
e ← POPBEST(a)
if GOLDSTANDARD(e) and GOALTEST(e)
then return e
end if
</figure>
<bodyText confidence="0.757484">
if not GOLDSTANDARD(e) then
</bodyText>
<equation confidence="0.946093652173913">
e− ← e
e+ ← MINGOLD(a)
UPDATEPARAMETERS(e+, e−)
RECOMPUTESCORES(a, c)
continue
end if
for e′ ∈ UNARY(e, grammar) do
APPEND(new, e)
end for
for e� ∈ c do
if CANCOMBINE(e, �e) then
e′ ← BINARY(e, �e, grammar)
APPEND(new, e′)
end if
if CANCOMBINE(�e, e) then
e′ ← BINARY(�e, e, grammar)
APPEND(new, e′)
end if
end for
for e′ ∈ new do
ADD(a, e′)
end for
ADD(c, e)
</equation>
<bodyText confidence="0.984778375">
end while
for further work possible alternative methods to
generate more negative examples during training.
Another way of viewing the training process is
that it pushes gold-standard edges towards the top
of the agenda, and crucially pushes them above
non-gold-standard edges. This is the view de-
scribed by Z&amp;C. Given a positive example e+ and
a negative example e−, they use the perceptron
algorithm to penalize the score for φ(e−) and re-
ward the score of φ(e+), but do not update pa-
rameters for the sub-edges of e+ and e−. An argu-
ment for not penalizing the sub-edge scores for e−
is that the sub-edges must be gold-standard edges
(since the training process is constructed so that
only gold-standard edges are expanded). From
</bodyText>
<page confidence="0.992803">
739
</page>
<bodyText confidence="0.992168575">
the perspective of correctness, it is unnecessary
to find a margin between the sub-edges of e+ and
those of e−, since both are gold-standard edges.
However, since the score of an edge not only
represents its correctness, but also affects its pri-
ority on the agenda, promoting the sub-edge of
e+ can lead to “easier” edges being constructed
before “harder” ones (i.e. those that are less
likely to be correct), and therefore improve the
output accuracy. This perspective has been ob-
served by other works of learning-guided-search
(Shen et al., 2007; Shen and Joshi, 2008; Gold-
berg and Elhadad, 2010). Intuitively, the score
difference between easy gold-standard and harder
gold-standard edges should not be as great as the
difference between gold-standard and non-gold-
standard edges. The perceptron update cannot
provide such control of separation, because the
amount of update is fixed to 1.
As described earlier, we treat parameter update
as finding a separation between correct and incor-
rect edges, in which the global feature vectors 4b,
rather than φ, are considered. Given a positive ex-
ample e+ and a negative example e−, we make a
minimum update so that the score of e+ is higher
than that of e− with some margin:
have been used as a standard component in statis-
tical machine translation systems to control out-
put fluency. For the syntax-based generation sys-
tem, the incorporation of an N-gram language
model can potentially improve the local fluency
of output sequences. In addition, the N-gram
language model can be trained separately using
a large amount of data, while the syntax-based
model requires manual annotation for training.
The standard method for the combination of
a syntax model and an N-gram model is linear
interpolation. We incorporate fourgram, trigram
and bigram scores into our syntax model, so that
the score of an edge e becomes:
</bodyText>
<equation confidence="0.96631525">
θ ← arg min k θ′ −θ0 k, s.t.-b(e+)θ′−-b(e−)θ′ ≥ 1
0′
F(e) = f(e) + g(e)
= f(e) + α · gfour(e) + β · gtri(e) + γ · gbi(e),
</equation>
<bodyText confidence="0.99929925">
where f is the syntax model score, and g is the
N-gram model score. g consists of three com-
ponents, gfour, gtri and gbi, representing the log-
probabilities of fourgrams, trigrams and bigrams
from the language model, respectively. α, β and
γ are the corresponding weights.
During decoding, F(e) is computed incremen-
tally. Again, denoting the sub-edges of e as es,
</bodyText>
<equation confidence="0.934687333333333">
F(e) = f(e) + g(e)
= ( 1: F(es)) + φ(e)θ + g6(e)
e3∈e
</equation>
<bodyText confidence="0.9996824">
where θ0 and θ denote the parameter vectors be-
fore and after the udpate, respectively. The up-
date is similar to the update of online large-margin
learning algorithms such as 1-best MIRA (Cram-
mer et al., 2006), and has a closed-form solution:
</bodyText>
<equation confidence="0.999151555555555">
f(e−) − f(e+) + 1
θ← θ0+ k `b(e+)
`b(e
)
2 (
−
−
k
�(e+) − `1&apos;(e−))
</equation>
<bodyText confidence="0.9999083">
In this update, the global feature vectors 4b(e+)
and 4b(e−) are used. Unlike Z&amp;C, the scores
of sub-edges of e+ and e− are also udpated, so
that the sub-edges of e− are less prioritized than
those of e+. We show empirically that this train-
ing algorithm significantly outperforms the per-
ceptron training of the baseline system in Sec-
tion 5. An advantage of our new training algo-
rithm is that it enables the accommodation of a
separately trained N-gram model into the system.
</bodyText>
<sectionHeader confidence="0.9955035" genericHeader="method">
4 Incorporating an N-gram language
model
</sectionHeader>
<bodyText confidence="0.999439826086957">
Since the seminal work of the IBM models
(Brown et al., 1993), N-gram language models
Here g6(e) = α·g6four(e)+β·g6tri(e)+γ·g6bi(e)
is the sum of log-probabilities of the new N-
grams resulting from the construction of e. For
leaf edges and unary-branching edges, no new N-
grams result from their construction (i.e. g6 = 0).
For a binary-branching edge, new N-grams result
from the surface-string concatenation of its sub-
edges. The sum of log-probabilities of the new
fourgrams, trigrams and bigrams contribute to g6
with weights α, β and γ, respectively.
For training, there are at least three methods to
tune α, β, γ and θ. One simple method is to train
the syntax model θ independently, and select α,
β, and γ empirically from a range of candidate
values according to development tests. We call
this method test-time interpolation. An alterna-
tive is to select α, β and γ first, initializing the
vector θ as all zeroes, and then run the training
algorithm for θ taking into account the N-gram
language model. In this process, g is considered
when finding a separation between positive and
</bodyText>
<page confidence="0.976759">
740
</page>
<bodyText confidence="0.999957689655172">
negative examples; the training algorithm finds a
value of 0 that best suits the precomputed α, Q
and -y values, together with the N-gram language
model. We call this method g-precomputed in-
terpolation. Yet another method is to initialize α,
Q, -y and 0 as all zeroes, and run the training al-
gorithm taking into account the N-gram language
model. We call this method g-free interpolation.
The incorporation of an N-gram language
model into the syntax-based generation system is
weakly analogous to N-gram model insertion for
syntax-based statistical machine translation sys-
tems, both of which apply a score from the N-
gram model component in a derivation-building
process. As discussed earlier, polynomial-time
decoding is typically feasible for syntax-based
machine translation systems without an N-gram
language model, due to constraints from the
grammar. In these cases, incorporation of N-
gram language models can significantly increase
the complexity of a dynamic-programming de-
coder (Bar-Hillel et al., 1961). Efficient search
has been achieved using chart pruning (Chiang,
2007) and iterative numerical approaches to con-
strained optimization (Rush and Collins, 2011).
In contrast, the incorporation of an N-gram lan-
guage model into our decoder is more straightfor-
ward, and does not add to its asymptotic complex-
ity, due to the heuristic nature of the decoder.
</bodyText>
<sectionHeader confidence="0.999706" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9999413">
We use sections 2–21 of CCGBank to train our
syntax model, section 00 for development and
section 23 for the final test. Derivations from
CCGBank are transformed into inputs by turn-
ing their surface strings into multi-sets of words.
Following Z&amp;C, we treat base noun phrases (i.e.
NPs that do not recursively contain other NPs) as
atomic units for the input. Output sequences are
compared with the original sentences to evaluate
their quality. We follow previous work and use
the BLEU metric (Papineni et al., 2002) to com-
pare outputs with references.
Z&amp;C use two methods to construct leaf edges.
The first is to assign lexical categories according
to a dictionary. There are 26.8 lexical categories
for each word on average using this method, cor-
responding to 26.8 leaf edges. The other method
is to use a pre-processing step — a CCG supertag-
ger (Clark and Curran, 2007) — to prune can-
didate lexical categories according to the gold-
</bodyText>
<table confidence="0.998302666666667">
CCGBank Sentences Tokens
training 39,604 929,552
development 1,913 45,422
GigaWord v4 Sentences Tokens
AFP 30,363,052 684,910,697
XIN 15,982,098 340,666,976
</table>
<tableCaption confidence="0.9976665">
Table 1: Number of sentences and tokens by language
model source.
</tableCaption>
<bodyText confidence="0.999870034482758">
standard sequence, assuming that for some prob-
lems the ambiguities can be reduced (e.g. when
the input is already partly correctly ordered).
Z&amp;C use different probability cutoff levels (the
Q parameter in the supertagger) to control the
pruning. Here we focus mainly on the dictionary
method, which leaves lexical category disam-
biguation entirely to the generation system. For
comparison, we also perform experiments with
lexical category pruning. We chose Q = 0.0001,
which leaves 5.4 leaf edges per word on average.
We used the SRILM Toolkit (Stolcke, 2002)
to build a true-case 4-gram language model es-
timated over the CCGBank training and develop-
ment data and a large additional collection of flu-
ent sentences in the Agence France-Presse (AFP)
and Xinhua News Agency (XIN) subsets of the
English GigaWord Fourth Edition (Parker et al.,
2009), a total of over 1 billion tokens. The Gi-
gaWord data was first pre-processed to replicate
the CCGBank tokenization. The total number
of sentences and tokens in each LM component
is shown in Table 1. The language model vo-
cabulary consists of the 46,574 words that oc-
cur in the concatenation of the CCGBank train-
ing, development, and test sets. The LM proba-
bilities are estimated using modified Kneser-Ney
smoothing (Kneser and Ney, 1995) with interpo-
lation of lower n-gram orders.
</bodyText>
<subsectionHeader confidence="0.99317">
5.1 Development experiments
</subsectionHeader>
<bodyText confidence="0.9999722">
A set of development test results without lexical
category pruning (i.e. using the full dictionary) is
shown in Table 2. We train the baseline system
and our systems under various settings for 10 iter-
ations, and measure the output BLEU scores after
each iteration. The timeout value for each sen-
tence is set to 5 seconds. The highest score (max
BLEU) and averaged score (avg. BLEU) of each
system over the 10 training iterations are shown
in the table.
</bodyText>
<page confidence="0.994128">
741
</page>
<table confidence="0.98552325">
Method max BLEU avg. BLEU
baseline 38.47 37.36
margin 41.20 39.70
margin +LM (g-precomputed) 41.50 40.84
margin +LM (α = 0, Q = 0, -y = 0) 40.83 —
margin +LM (α = 0.08, Q = 0.016, -y = 0.004) 38.99 —
margin +LM (α = 0.4, Q = 0.08, -y = 0.02) 36.17 —
margin +LM (α = 0.8, Q = 0.16, -y = 0.04) 34.74 —
</table>
<tableCaption confidence="0.998841">
Table 2: Development experiments without lexical category pruning.
</tableCaption>
<bodyText confidence="0.999972764705882">
The first three rows represent the baseline sys-
tem, our largin-margin training system (margin),
and our system with the N-gram model incorpo-
rated using g-precomputed interpolation. For in-
terpolation we manually chose α = 0.8, Q = 0.16
and -y = 0.04, respectively. These values could
be optimized by development experiments with
alternative configurations, which may lead to fur-
ther improvements. Our system with large-margin
training gives higher BLEU scores than the base-
line system consistently over all iterations. The
N-gram model led to further improvements.
The last four rows in the table show results
of our system with the N-gram model added us-
ing test-time interpolation. The syntax model is
trained with the optimal number of iterations, and
different α, Q, and -y values are used to integrate
the language model. Compared with the system
using no N-gram model (margin), test-time inter-
polation did not improve the accuracies.
The row with α, Q, -y = 0 represents our system
with the N-gram model loaded, and the scores
gfour, gtri and gbi computed for each N-gram
during decoding, but the scores of edges are com-
puted without using N-gram probabilities. The
scoring model is the same as the syntax model
(margin), but the results are lower than the row
“margin”, because computing N-gram probabil-
ities made the system slower, exploring less hy-
potheses under the same timeout setting.1
The comparison between g-precomputed inter-
polation and test-time interpolation shows that the
system gives better scores when the syntax model
takes into consideration the N-gram model during
</bodyText>
<footnote confidence="0.9712722">
1More decoding time could be given to the slower N-
gram system, but we use 5 seconds as the timeout setting
for all the experiments, giving the methods with the N-gram
language model a slight disadvantage, as shown by the two
rows “margin” and “margin +LM (α,,3, -y = 0).
</footnote>
<figure confidence="0.6874385">
1 2 3 4 5 6 7 8 9 10
training iteration
</figure>
<figureCaption confidence="0.9913215">
Figure 1: Development experiments with lexical cate-
gory pruning (Q = 0.0001).
</figureCaption>
<bodyText confidence="0.99491228">
training. One question that arises is whether g-
free interpolation will outperform g-precomputed
interpolation. g-free interpolation offers the free-
dom of α, Q and -y during training, and can poten-
tially reach a better combination of the parameter
values. However, the training algorithm failed to
converge with g-free interpolation. One possible
explanation is that real-valued features from the
language model made our large-margin training
harder. Another possible reason is that our train-
ing process with heavy pruning does not accom-
modate this complex model.
Figure 1 shows a set of development experi-
ments with lexical category pruning (with the su-
pertagger parameter Q = 0.0001). The scores
of the three different systems are calculated by
varying the number of training iterations. The
large-margin training system (margin) gave con-
sistently better scores than the baseline system,
and adding a language model (margin +LM) im-
proves the scores further.
Table 3 shows some manually chosen examples
for which our system gave significant improve-
ments over the baseline. For most other sentences
the improvements are not as obvious. For each
</bodyText>
<figure confidence="0.889802230769231">
BLEU
45
44
43
42
41
40
39
38
37
baseline
margin
margin +LM
</figure>
<page confidence="0.984127">
742
</page>
<table confidence="0.998621206896552">
baseline margin margin +LM
as a nonexecutive director Pierre Vinken 61 years old , the board will join as a as a nonexecutive director Pierre Vinken
, 61 years old , will join the board . 29 nonexecutive director Nov. 29 , Pierre , 61 years old , will join the board Nov.
Nov. Vinken . 29 .
Lorillard nor smokers were aware of the of any research who studied Neither the Neither Lorillard nor any research on the
Kent cigarettes of any research on the workers were aware of smokers on the workers who studied the Kent cigarettes
workers who studied the researchers Kent cigarettes nor the researchers were aware of smokers of the researchers
.
you But 35 years ago have to recognize recognize But you took place that these But you have to recognize that these
that these events took place . events have to 35 years ago . events took place 35 years ago .
investors to pour cash into money funds Despite investors , yields continue to Despite investors , recent declines in
continue in Despite yields recent declines pour into money funds recent declines in yields continue to pour cash into money
cash . funds .
yielding The top money funds are cur- The top money funds currently are yield- The top money funds are yielding well
rently well over 9 % . ing well over 9 % . over 9 % currently.
where A buffet breakfast, held in the mu- everyday visitors are banned to where A buffet breakfast , everyday visitors are
seum was food and drinks to . everyday A buffet breakfast was held , food and banned to where food and drinks was
visitors banned drinks in the museum . held in the museum.
A Commonwealth Edison spokesman tracking A Commonwealth Edison an administrative nightmare whose ad-
said an administrative nightmare would spokesman said that the two million cus- dresses would be tracking down A Com-
be tracking down the past 3 12 years that tomers whose addresses have changed monwealth Edison spokesman said that
the two million customers have . whose down during the past 3 12 years would the two million customers have changed
changed be an administrative nightmare . during the past 3 12 years .
The $ 2.5 billion Byron 1 plant , Ill. , was The $ 2.5 billion Byron 1 plant was near The $ 2.5 billion Byron 1 plant near
completed. near Rockford in 1985 completed in Rockford , Ill. , 1985 . Rockford , Ill. , was completed in 1985 .
will ( During its centennial year , The as The Wall Street Journal ( During its During its centennial year events will re-
Wall Street Journal report events of the centennial year , milestones stand of port , The Wall Street Journal that stand
past century that stand as milestones of American business history that will re- as milestones of American business his-
American business history. ) port events of the past century. ) tory ( of the past century ) .
</table>
<tableCaption confidence="0.999825">
Table 3: Some chosen examples with significant improvements (supertagger parameter 0 = 0.0001).
</tableCaption>
<bodyText confidence="0.9941496">
method, the examples are chosen from the devel-
opment output with lexical category pruning, af-
ter the optimal number of training iterations, with
the timeout set to 5s. We also tried manually se-
lecting examples without lexical category prun-
ing, but the improvements were not as obvious,
partly because the overall fluency was lower for
all the three systems.
Table 4 shows a set of examples chosen ran-
domly from the development test outputs of our
system with the N-gram model. The optimal
number of training iterations is used, and a time-
out of 1 minute is used in addition to the 5s time-
out for comparison. With more time to decode
each input, the system gave a BLEU score of
44.61, higher than 41.50 with the 5s timout.
While some of the outputs we examined are
reasonably fluent, most are to some extent frag-
mentary.2 In general, the system outputs are
still far below human fluency. Some samples are
</bodyText>
<footnote confidence="0.936683">
2Part of the reason for some fragmentary outputs is the
default output mechanism: partial derivations from the chart
are greedily put together when timeout occurs before a goal
hypothesis is found.
</footnote>
<bodyText confidence="0.99970975">
syntactically grammatical, but are semantically
anomalous. For example, person names are often
confused with company names, verbs often take
unrelated subjects and objects. The problem is
much more severe for long sentences, which have
more ambiguities. For specific tasks, extra infor-
mation (such as the source text for machine trans-
lation) can be available to reduce ambiguities.
</bodyText>
<sectionHeader confidence="0.981398" genericHeader="method">
6 Final results
</sectionHeader>
<bodyText confidence="0.999985307692308">
The final results of our system without lexical cat-
egory pruning are shown in Table 5. Row “W09
CLE” and “W09 AB” show the results of the
maximum spanning tree and assignment-based al-
gorithms of Wan et al. (2009); rows “margin”
and “margin +LM” show the results of our large-
margin training system and our system with the
N-gram model. All these results are directly com-
parable since we do not use any lexical category
pruning for this set of results. For each of our
systems, we fix the number of training iterations
according to development test scores. Consis-
tent with the development experiments, our sys-
</bodyText>
<page confidence="0.956751">
743
</page>
<bodyText confidence="0.996042384615385">
timeout = 5s timeout = 1m
drooled the cars and drivers , like Fortune 500 executives . over After schoolboys drooled over the cars and drivers , the race
the race like Fortune 500 executives.
One big reason: thin margins. One big reason: thin margins.
You or accountants look around ... and at an eye blinks. pro- blinks nobody You or accountants look around ... and at an eye
fessional ballplayers . professional ballplayers
most disturbing And of it , are educators , not students , for the And blamed for the wrongdoing , educators , not students who
wrongdoing is who. are disturbing, much of it is most.
defeat coaching aids the purpose of which is , He and other gauge coaching aids learning progress can and other critics say
critics say can to . standardized tests learning progress the purpose of which is to defeat, standardized tests .
The federal government of government debt because Congress The federal government suspended sales of government debt
has lifted the ceiling on U.S. savings bonds suspended sales because Congress has n’t lifted the ceiling on U.S. savings
bonds.
</bodyText>
<tableCaption confidence="0.998635">
Table 4: Some examples chosen at random from development test outputs without lexical category pruning.
</tableCaption>
<table confidence="0.999508166666667">
System BLEU
W09 CLE 26.8
W09 AB 33.7
Z&amp;C11 40.1
margin 42.5
margin +LM 43.8
</table>
<tableCaption confidence="0.950565">
Table 5: Test results without lexical category pruning.
</tableCaption>
<table confidence="0.9981025">
System BLEU
Z&amp;C11 43.2
margin 44.7
margin +LM 46.1
</table>
<tableCaption confidence="0.994372">
Table 6: Test results with lexical category pruning (su-
</tableCaption>
<bodyText confidence="0.960590384615385">
pertagger parameter 0 = 0.0001).
tem outperforms the baseline methods. The acu-
racies are significantly higher when the N-gram
model is incorporated.
Table 6 compares our system with Z&amp;C using
lexical category pruning (Q = 0.0001) and a 5s
timeout for fair comparison. The results are sim-
ilar to Table 5: our large-margin training systems
outperforms the baseline by 1.5 BLEU points, and
adding the N-gram model gave a further 1.4 point
improvement. The scores could be significantly
increased by using a larger timeout, as shown in
our earlier development experiments.
</bodyText>
<sectionHeader confidence="0.999982" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999952772727273">
There is a recent line of research on text-to-
text generation, which studies the linearization of
dependency structures (Barzilay and McKeown,
2005; Filippova and Strube, 2007; Filippova and
Strube, 2009; Bohnet et al., 2010; Guo et al.,
2011). Unlike our system, and Wan et al. (2009),
input dependencies provide additional informa-
tion to these systems. Although the search space
can be constrained by the assumption of projec-
tivity, permutation of modifiers of the same head
word makes exact inference for tree lineariza-
tion intractable. The above systems typically ap-
ply approximate inference, such as beam-search.
While syntax-based features are commonly used
by these systems for linearization, Filippova and
Strube (2009) apply a trigram model to control
local fluency within constituents. A dependency-
based N-gram model has also been shown effec-
tive for the linearization task (Guo et al., 2011).
The best-first inference and timeout mechanism
of our system is similar to that of White (2004), a
surface realizer from logical forms using CCG.
</bodyText>
<sectionHeader confidence="0.99885" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999998384615385">
We studied the problem of word-ordering using
a syntactic model and allowing permutation. We
took the model of Zhang and Clark (2011) as the
baseline, and extended it with online large-margin
training and an N-gram language model. These
extentions led to improvements in the BLEU eval-
uation. Analyzing the generated sentences sug-
gests that, while highly fluent outputs can be pro-
duced for short sentences (≤ 10 words), the sys-
tem fluency in general is still way below human
standard. Future work remains to apply the sys-
tem as a component for specific text generation
tasks, for example machine translation.
</bodyText>
<sectionHeader confidence="0.998471" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<footnote confidence="0.645646666666667">
Yue Zhang and Stephen Clark are supported by the Eu-
ropean Union Seventh Framework Programme (FP7-
ICT-2009-4) under grant agreement no. 247762.
</footnote>
<page confidence="0.996552">
744
</page>
<sectionHeader confidence="0.996322" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999805919642857">
Yehoshua Bar-Hillel, M. Perles, and E. Shamir. 1961.
On formal properties of simple phrase structure
grammars. Zeitschrift f¨ur Phonetik, Sprachwis-
senschaft und Kommunikationsforschung, 14:143–
172. Reprinted in Y. Bar-Hillel. (1964). Language
and Information: Selected Essays on their Theory
and Application, Addison-Wesley 1964, 116–150.
Regina Barzilay and Kathleen McKeown. 2005. Sen-
tence fusion for multidocument news summariza-
tion. Computational Linguistics, 31(3):297–328.
Graeme Blackwood, Adri`a de Gispert, and William
Byrne. 2010. Fluency constraints for minimum
Bayes-risk decoding of statistical machine trans-
lation lattices. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics
(Coling 2010), pages 71–79, Beijing, China, Au-
gust. Coling 2010 Organizing Committee.
Bernd Bohnet, Leo Wanner, Simon Mill, and Alicia
Burga. 2010. Broad coverage multilingual deep
sentence generation with a stochastic multi-level re-
alizer. In Proceedings of the 23rd International
Conference on Computational Linguistics (Coling
2010), pages 98–106, Beijing, China, August. Col-
ing 2010 Organizing Committee.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter
estimation. Computational Linguistics, 19(2):263–
311.
Sharon A. Caraballo and Eugene Charniak. 1998.
New figures of merit for best-first probabilistic chart
parsing. Comput. Linguist., 24:275–298, June.
David Chiang. 2007. Hierarchical Phrase-
based Translation. Computational Linguistics,
33(2):201–228.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493–552.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 7:551–585.
Katja Filippova and Michael Strube. 2007. Gener-
ating constituent order in german clauses. In Pro-
ceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 320–
327, Prague, Czech Republic, June. Association for
Computational Linguistics.
Katja Filippova and Michael Strube. 2009. Tree lin-
earization in english: Improving language model
based approaches. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, Companion Volume:
Short Papers, pages 225–228, Boulder, Colorado,
June. Association for Computational Linguistics.
Yoav Goldberg and Michael Elhadad. 2010. An effi-
cient algorithm for easy-first non-directional depen-
dency parsing. In Human Language Technologies:
The 2010 Annual Conference ofthe North American
Chapter of the Association for Computational Lin-
guistics, pages 742–750, Los Angeles, California,
June. Association for Computational Linguistics.
Yuqing Guo, Deirdre Hogan, and Josef van Genabith.
2011. Dcu at generation challenges 2011 surface
realisation track. In Proceedings of the Generation
Challenges Session at the 13th European Workshop
on Natural Language Generation, pages 227–229,
Nancy, France, September. Association for Compu-
tational Linguistics.
Julia Hockenmaier and Mark Steedman. 2002. Gen-
erative models for statistical parsing with Combi-
natory Categorial Grammar. In Proceedings of the
40th Meeting of the ACL, pages 335–342, Philadel-
phia, PA.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355–396.
R. Kneser and H. Ney. 1995. Improved backing-off
for m-gram language modeling. In International
Conference on Acoustics, Speech, and Signal Pro-
cessing, 1995. ICASSP-95, volume 1, pages 181–
184.
Philip Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
ofNAACL/HLT, Edmonton, Canada, May.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical ma-
chine translation. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics Companion Volume Proceedings of the
Demo and Poster Sessions, pages 177–180, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: a method for auto-
matic evaluation of machine translation. In Pro-
ceedings of 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311–318,
Philadelphia, Pennsylvania, USA, July. Association
for Computational Linguistics.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2009. English Gigaword Fourth
Edition, Linguistic Data Consortium.
Alexander M. Rush and Michael Collins. 2011. Exact
decoding of syntactic translation models through la-
</reference>
<page confidence="0.980338">
745
</page>
<reference confidence="0.999733634146342">
grangian relaxation. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
72–82, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Libin Shen and Aravind Joshi. 2008. LTAG depen-
dency parsing with bidirectional incremental con-
struction. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 495–504, Honolulu, Hawaii, Octo-
ber. Association for Computational Linguistics.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classi-
fication. In Proceedings of ACL, pages 760–767,
Prague, Czech Republic, June.
Mark Steedman. 2000. The Syntactic Process. The
MIT Press, Cambridge, Mass.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing, pages 901–904.
Stephen Wan, Mark Dras, Robert Dale, and C´ecile
Paris. 2009. Improving grammaticality in statisti-
cal sentence generation: Introducing a dependency
spanning tree algorithm with an argument satisfac-
tion model. In Proceedings of the 12th Conference
of the European Chapter of the ACL (EACL 2009),
pages 852–860, Athens, Greece, March. Associa-
tion for Computational Linguistics.
Michael White. 2004. Reining in CCG chart realiza-
tion. In Proc. INLG-04, pages 182–191.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3).
Yue Zhang and Stephen Clark. 2011. Syntax-
based grammaticality improvement using CCG and
guided search. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language
Processing, pages 1147–1157, Edinburgh, Scot-
land, UK., July. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.998525">
746
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.576648">
<title confidence="0.955578">Syntax-Based Word Ordering Incorporating a Large-Scale Language Model</title>
<author confidence="0.997583">Yue Zhang Graeme Blackwood Stephen Clark</author>
<affiliation confidence="0.999956">University of Cambridge University of Cambridge University of Cambridge Computer Laboratory Engineering Department Computer Laboratory</affiliation>
<email confidence="0.738744">yz360@cam.ac.ukgwb24@eng.cam.ac.uksc609@cam.ac.uk</email>
<abstract confidence="0.992052826086956">A fundamental problem in text generation is word ordering. Word ordering is a computationally difficult problem, which can be constrained to some extent for particular applications, for example by using synchronous grammars for statistical machine translation. There have been some recent attempts at the unconstrained problem of generating a sentence from a multi-set of input words (Wan et al., 2009; Zhang and Clark, 2011). By using CCG and learning guided search, Zhang and Clark reported the highest scores on this task. One limitation of their system is the absence of an N-gram language model, which has been used by text generation systems to improve fluency. We take the Zhang and Clark system as the baseline, and incorporate an N-gram model by applying online large-margin training. Our system significantly improved on the baseline by 3.7 BLEU points.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yehoshua Bar-Hillel</author>
<author>M Perles</author>
<author>E Shamir</author>
</authors>
<title>On formal properties of simple phrase structure grammars. Zeitschrift f¨ur Phonetik, Sprachwissenschaft und Kommunikationsforschung,</title>
<date>1961</date>
<pages>14--143</pages>
<note>Reprinted in</note>
<contexts>
<context position="20665" citStr="Bar-Hillel et al., 1961" startWordPosition="3503" endWordPosition="3506"> The incorporation of an N-gram language model into the syntax-based generation system is weakly analogous to N-gram model insertion for syntax-based statistical machine translation systems, both of which apply a score from the Ngram model component in a derivation-building process. As discussed earlier, polynomial-time decoding is typically feasible for syntax-based machine translation systems without an N-gram language model, due to constraints from the grammar. In these cases, incorporation of Ngram language models can significantly increase the complexity of a dynamic-programming decoder (Bar-Hillel et al., 1961). Efficient search has been achieved using chart pruning (Chiang, 2007) and iterative numerical approaches to constrained optimization (Rush and Collins, 2011). In contrast, the incorporation of an N-gram language model into our decoder is more straightforward, and does not add to its asymptotic complexity, due to the heuristic nature of the decoder. 5 Experiments We use sections 2–21 of CCGBank to train our syntax model, section 00 for development and section 23 for the final test. Derivations from CCGBank are transformed into inputs by turning their surface strings into multi-sets of words. </context>
</contexts>
<marker>Bar-Hillel, Perles, Shamir, 1961</marker>
<rawString>Yehoshua Bar-Hillel, M. Perles, and E. Shamir. 1961. On formal properties of simple phrase structure grammars. Zeitschrift f¨ur Phonetik, Sprachwissenschaft und Kommunikationsforschung, 14:143– 172. Reprinted in Y. Bar-Hillel. (1964). Language and Information: Selected Essays on their Theory and Application, Addison-Wesley 1964, 116–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen McKeown</author>
</authors>
<title>Sentence fusion for multidocument news summarization.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>3</issue>
<contexts>
<context position="34717" citStr="Barzilay and McKeown, 2005" startWordPosition="5887" endWordPosition="5890">igher when the N-gram model is incorporated. Table 6 compares our system with Z&amp;C using lexical category pruning (Q = 0.0001) and a 5s timeout for fair comparison. The results are similar to Table 5: our large-margin training systems outperforms the baseline by 1.5 BLEU points, and adding the N-gram model gave a further 1.4 point improvement. The scores could be significantly increased by using a larger timeout, as shown in our earlier development experiments. 7 Related Work There is a recent line of research on text-totext generation, which studies the linearization of dependency structures (Barzilay and McKeown, 2005; Filippova and Strube, 2007; Filippova and Strube, 2009; Bohnet et al., 2010; Guo et al., 2011). Unlike our system, and Wan et al. (2009), input dependencies provide additional information to these systems. Although the search space can be constrained by the assumption of projectivity, permutation of modifiers of the same head word makes exact inference for tree linearization intractable. The above systems typically apply approximate inference, such as beam-search. While syntax-based features are commonly used by these systems for linearization, Filippova and Strube (2009) apply a trigram mod</context>
</contexts>
<marker>Barzilay, McKeown, 2005</marker>
<rawString>Regina Barzilay and Kathleen McKeown. 2005. Sentence fusion for multidocument news summarization. Computational Linguistics, 31(3):297–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Blackwood</author>
<author>Adri`a de Gispert</author>
<author>William Byrne</author>
</authors>
<title>Fluency constraints for minimum Bayes-risk decoding of statistical machine translation lattices.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>71--79</pages>
<location>Beijing, China,</location>
<marker>Blackwood, de Gispert, Byrne, 2010</marker>
<rawString>Graeme Blackwood, Adri`a de Gispert, and William Byrne. 2010. Fluency constraints for minimum Bayes-risk decoding of statistical machine translation lattices. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 71–79, Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
<author>Leo Wanner</author>
<author>Simon Mill</author>
<author>Alicia Burga</author>
</authors>
<title>Broad coverage multilingual deep sentence generation with a stochastic multi-level realizer.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>98--106</pages>
<location>Beijing, China,</location>
<contexts>
<context position="34794" citStr="Bohnet et al., 2010" startWordPosition="5899" endWordPosition="5902">ing lexical category pruning (Q = 0.0001) and a 5s timeout for fair comparison. The results are similar to Table 5: our large-margin training systems outperforms the baseline by 1.5 BLEU points, and adding the N-gram model gave a further 1.4 point improvement. The scores could be significantly increased by using a larger timeout, as shown in our earlier development experiments. 7 Related Work There is a recent line of research on text-totext generation, which studies the linearization of dependency structures (Barzilay and McKeown, 2005; Filippova and Strube, 2007; Filippova and Strube, 2009; Bohnet et al., 2010; Guo et al., 2011). Unlike our system, and Wan et al. (2009), input dependencies provide additional information to these systems. Although the search space can be constrained by the assumption of projectivity, permutation of modifiers of the same head word makes exact inference for tree linearization intractable. The above systems typically apply approximate inference, such as beam-search. While syntax-based features are commonly used by these systems for linearization, Filippova and Strube (2009) apply a trigram model to control local fluency within constituents. A dependencybased N-gram mod</context>
</contexts>
<marker>Bohnet, Wanner, Mill, Burga, 2010</marker>
<rawString>Bernd Bohnet, Leo Wanner, Simon Mill, and Alicia Burga. 2010. Broad coverage multilingual deep sentence generation with a stochastic multi-level realizer. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 98–106, Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>311</pages>
<contexts>
<context position="18622" citStr="Brown et al., 1993" startWordPosition="3175" endWordPosition="3178"> `b(e+) `b(e ) 2 ( − − k �(e+) − `1&apos;(e−)) In this update, the global feature vectors 4b(e+) and 4b(e−) are used. Unlike Z&amp;C, the scores of sub-edges of e+ and e− are also udpated, so that the sub-edges of e− are less prioritized than those of e+. We show empirically that this training algorithm significantly outperforms the perceptron training of the baseline system in Section 5. An advantage of our new training algorithm is that it enables the accommodation of a separately trained N-gram model into the system. 4 Incorporating an N-gram language model Since the seminal work of the IBM models (Brown et al., 1993), N-gram language models Here g6(e) = α·g6four(e)+β·g6tri(e)+γ·g6bi(e) is the sum of log-probabilities of the new Ngrams resulting from the construction of e. For leaf edges and unary-branching edges, no new Ngrams result from their construction (i.e. g6 = 0). For a binary-branching edge, new N-grams result from the surface-string concatenation of its subedges. The sum of log-probabilities of the new fourgrams, trigrams and bigrams contribute to g6 with weights α, β and γ, respectively. For training, there are at least three methods to tune α, β, γ and θ. One simple method is to train the synt</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263– 311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon A Caraballo</author>
<author>Eugene Charniak</author>
</authors>
<title>New figures of merit for best-first probabilistic chart parsing.</title>
<date>1998</date>
<journal>Comput. Linguist.,</journal>
<pages>24--275</pages>
<contexts>
<context position="9176" citStr="Caraballo and Charniak, 1998" startWordPosition="1511" endWordPosition="1514">PPEND(new, e′) end if end for for e′ E new do ADD(a, e′) end for ADD(c, e) end while decoding finishes. Otherwise it is extended with unary rules, and combined with existing edges in the chart using binary rules to produce new edges. The resulting edges are scored and put onto the agenda, while the original edge is put onto the chart. The process repeats until a goal edge is found, or a timeout limit is reached. In the latter case, a default output is produced using existing edges in the chart. Pseudocode for the decoder is shown as Algorithm 1. Again it is reminiscent of a best-first parser (Caraballo and Charniak, 1998) in the use of an agenda and a chart, but is fundamentally different due to the fact that there is no input order. 2.3 Statistical model and feature templates The baseline system uses a linear model to score hypotheses. For an edge e, its score is defined as: f(e) = 4b(e) · θ, where 4b(e) represents the feature vector of e and θ is the parameter vector of the model. During decoding, feature vectors are computed incrementally. When an edge is constructed, its score is computed from the scores of its sub-edges and the incrementally added structure: f(e) = -b(e) · θ = �� 1: -b(es)) + φ(e)) · θ e3</context>
</contexts>
<marker>Caraballo, Charniak, 1998</marker>
<rawString>Sharon A. Caraballo and Eugene Charniak. 1998. New figures of merit for best-first probabilistic chart parsing. Comput. Linguist., 24:275–298, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical Phrasebased Translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="2024" citStr="Chiang (2007)" startWordPosition="317" endWordPosition="318">et of the input words are used to produce the output. Word ordering is a difficult problem. Finding the best permutation for a set of words according to a bigram language model, for example, is NP-hard, which can be proved by linear reduction from the traveling salesman problem. In practice, exploring the whole search space of permutations is often prevented by adding constraints. In phrase-based machine translation (Koehn et al., 2003; Koehn et al., 2007), a distortion limit is used to constrain the position of output phrases. In syntax-based machine translation systems such as Wu (1997) and Chiang (2007), synchronous grammars limit the search space so that polynomial time inference is feasible. In fluency improvement (Blackwood et al., 2010), parts of translation hypotheses identified as having high local confidence are held fixed, so that word ordering elsewhere is strictly local. Some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search. Wan et al. (2009) uses a dependency grammar to solve word ordering, and Zhang and Clark (2011) uses CCG (Steedman, 2000) for word ordering and word choice. The use of syntax models makes th</context>
<context position="20736" citStr="Chiang, 2007" startWordPosition="3515" endWordPosition="3516">stem is weakly analogous to N-gram model insertion for syntax-based statistical machine translation systems, both of which apply a score from the Ngram model component in a derivation-building process. As discussed earlier, polynomial-time decoding is typically feasible for syntax-based machine translation systems without an N-gram language model, due to constraints from the grammar. In these cases, incorporation of Ngram language models can significantly increase the complexity of a dynamic-programming decoder (Bar-Hillel et al., 1961). Efficient search has been achieved using chart pruning (Chiang, 2007) and iterative numerical approaches to constrained optimization (Rush and Collins, 2011). In contrast, the incorporation of an N-gram language model into our decoder is more straightforward, and does not add to its asymptotic complexity, due to the heuristic nature of the decoder. 5 Experiments We use sections 2–21 of CCGBank to train our syntax model, section 00 for development and section 23 for the final test. Derivations from CCGBank are transformed into inputs by turning their surface strings into multi-sets of words. Following Z&amp;C, we treat base noun phrases (i.e. NPs that do not recursi</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical Phrasebased Translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Widecoverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="5262" citStr="Clark and Curran, 2007" startWordPosition="832" endWordPosition="835">-set of input words, the baseline system builds a CCG derivation by choosing and ordering words from the input set. The scoring model is trained using CCGBank (Hockenmaier and Steedman, 2007), and best-first decoding is applied. We apply the same decoding framework in this paper, but apply an improved training process, and incorporate an N-gram language model into the syntax model. In this section, we describe and discuss the baseline statistical model and decoding framework, motivating our extensions. 2.1 Combinatory Categorial Grammar CCG, and parsing with CCG, has been described elsewhere (Clark and Curran, 2007; Hockenmaier and Steedman, 2002); here we provide only a short description. CCG (Steedman, 2000) is a lexicalized grammar formalism, which associates each word in a sentence with a lexical category. There is a small number of basic lexical categories, such as noun (N), noun phrase (NP), and prepositional phrase (PP). Complex lexical categories are formed recursively from basic categories and slashes, which indicate the directions of arguments. The CCG grammar used by our system is read off the derivations in CCGbank, following Hockenmaier and Steedman (2002), meaning that the CCG combinatory </context>
<context position="21903" citStr="Clark and Curran, 2007" startWordPosition="3709" endWordPosition="3712">we treat base noun phrases (i.e. NPs that do not recursively contain other NPs) as atomic units for the input. Output sequences are compared with the original sentences to evaluate their quality. We follow previous work and use the BLEU metric (Papineni et al., 2002) to compare outputs with references. Z&amp;C use two methods to construct leaf edges. The first is to assign lexical categories according to a dictionary. There are 26.8 lexical categories for each word on average using this method, corresponding to 26.8 leaf edges. The other method is to use a pre-processing step — a CCG supertagger (Clark and Curran, 2007) — to prune candidate lexical categories according to the goldCCGBank Sentences Tokens training 39,604 929,552 development 1,913 45,422 GigaWord v4 Sentences Tokens AFP 30,363,052 684,910,697 XIN 15,982,098 340,666,976 Table 1: Number of sentences and tokens by language model source. standard sequence, assuming that for some problems the ambiguities can be reduced (e.g. when the input is already partly correctly ordered). Z&amp;C use different probability cutoff levels (the Q parameter in the supertagger) to control the pruning. Here we focus mainly on the dictionary method, which leaves lexical c</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007. Widecoverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics, 33(4):493–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="17943" citStr="Crammer et al., 2006" startWordPosition="3049" endWordPosition="3053">re f is the syntax model score, and g is the N-gram model score. g consists of three components, gfour, gtri and gbi, representing the logprobabilities of fourgrams, trigrams and bigrams from the language model, respectively. α, β and γ are the corresponding weights. During decoding, F(e) is computed incrementally. Again, denoting the sub-edges of e as es, F(e) = f(e) + g(e) = ( 1: F(es)) + φ(e)θ + g6(e) e3∈e where θ0 and θ denote the parameter vectors before and after the udpate, respectively. The update is similar to the update of online large-margin learning algorithms such as 1-best MIRA (Crammer et al., 2006), and has a closed-form solution: f(e−) − f(e+) + 1 θ← θ0+ k `b(e+) `b(e ) 2 ( − − k �(e+) − `1&apos;(e−)) In this update, the global feature vectors 4b(e+) and 4b(e−) are used. Unlike Z&amp;C, the scores of sub-edges of e+ and e− are also udpated, so that the sub-edges of e− are less prioritized than those of e+. We show empirically that this training algorithm significantly outperforms the perceptron training of the baseline system in Section 5. An advantage of our new training algorithm is that it enables the accommodation of a separately trained N-gram model into the system. 4 Incorporating an N-gr</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. 2006. Online passive-aggressive algorithms. Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
<author>Michael Strube</author>
</authors>
<title>Generating constituent order in german clauses.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>320--327</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="34745" citStr="Filippova and Strube, 2007" startWordPosition="5891" endWordPosition="5894">is incorporated. Table 6 compares our system with Z&amp;C using lexical category pruning (Q = 0.0001) and a 5s timeout for fair comparison. The results are similar to Table 5: our large-margin training systems outperforms the baseline by 1.5 BLEU points, and adding the N-gram model gave a further 1.4 point improvement. The scores could be significantly increased by using a larger timeout, as shown in our earlier development experiments. 7 Related Work There is a recent line of research on text-totext generation, which studies the linearization of dependency structures (Barzilay and McKeown, 2005; Filippova and Strube, 2007; Filippova and Strube, 2009; Bohnet et al., 2010; Guo et al., 2011). Unlike our system, and Wan et al. (2009), input dependencies provide additional information to these systems. Although the search space can be constrained by the assumption of projectivity, permutation of modifiers of the same head word makes exact inference for tree linearization intractable. The above systems typically apply approximate inference, such as beam-search. While syntax-based features are commonly used by these systems for linearization, Filippova and Strube (2009) apply a trigram model to control local fluency </context>
</contexts>
<marker>Filippova, Strube, 2007</marker>
<rawString>Katja Filippova and Michael Strube. 2007. Generating constituent order in german clauses. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 320– 327, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
<author>Michael Strube</author>
</authors>
<title>Tree linearization in english: Improving language model based approaches.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers,</booktitle>
<pages>225--228</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="34773" citStr="Filippova and Strube, 2009" startWordPosition="5895" endWordPosition="5898">pares our system with Z&amp;C using lexical category pruning (Q = 0.0001) and a 5s timeout for fair comparison. The results are similar to Table 5: our large-margin training systems outperforms the baseline by 1.5 BLEU points, and adding the N-gram model gave a further 1.4 point improvement. The scores could be significantly increased by using a larger timeout, as shown in our earlier development experiments. 7 Related Work There is a recent line of research on text-totext generation, which studies the linearization of dependency structures (Barzilay and McKeown, 2005; Filippova and Strube, 2007; Filippova and Strube, 2009; Bohnet et al., 2010; Guo et al., 2011). Unlike our system, and Wan et al. (2009), input dependencies provide additional information to these systems. Although the search space can be constrained by the assumption of projectivity, permutation of modifiers of the same head word makes exact inference for tree linearization intractable. The above systems typically apply approximate inference, such as beam-search. While syntax-based features are commonly used by these systems for linearization, Filippova and Strube (2009) apply a trigram model to control local fluency within constituents. A depen</context>
</contexts>
<marker>Filippova, Strube, 2009</marker>
<rawString>Katja Filippova and Michael Strube. 2009. Tree linearization in english: Improving language model based approaches. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers, pages 225–228, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Michael Elhadad</author>
</authors>
<title>An efficient algorithm for easy-first non-directional dependency parsing.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference ofthe North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>742--750</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="15935" citStr="Goldberg and Elhadad, 2010" startWordPosition="2701" endWordPosition="2705">dges are expanded). From 739 the perspective of correctness, it is unnecessary to find a margin between the sub-edges of e+ and those of e−, since both are gold-standard edges. However, since the score of an edge not only represents its correctness, but also affects its priority on the agenda, promoting the sub-edge of e+ can lead to “easier” edges being constructed before “harder” ones (i.e. those that are less likely to be correct), and therefore improve the output accuracy. This perspective has been observed by other works of learning-guided-search (Shen et al., 2007; Shen and Joshi, 2008; Goldberg and Elhadad, 2010). Intuitively, the score difference between easy gold-standard and harder gold-standard edges should not be as great as the difference between gold-standard and non-goldstandard edges. The perceptron update cannot provide such control of separation, because the amount of update is fixed to 1. As described earlier, we treat parameter update as finding a separation between correct and incorrect edges, in which the global feature vectors 4b, rather than φ, are considered. Given a positive example e+ and a negative example e−, we make a minimum update so that the score of e+ is higher than that of</context>
</contexts>
<marker>Goldberg, Elhadad, 2010</marker>
<rawString>Yoav Goldberg and Michael Elhadad. 2010. An efficient algorithm for easy-first non-directional dependency parsing. In Human Language Technologies: The 2010 Annual Conference ofthe North American Chapter of the Association for Computational Linguistics, pages 742–750, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuqing Guo</author>
<author>Deirdre Hogan</author>
<author>Josef van Genabith</author>
</authors>
<title>Dcu at generation challenges 2011 surface realisation track.</title>
<date>2011</date>
<booktitle>In Proceedings of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation,</booktitle>
<pages>227--229</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Nancy, France,</location>
<marker>Guo, Hogan, van Genabith, 2011</marker>
<rawString>Yuqing Guo, Deirdre Hogan, and Josef van Genabith. 2011. Dcu at generation challenges 2011 surface realisation track. In Proceedings of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation, pages 227–229, Nancy, France, September. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Generative models for statistical parsing with Combinatory Categorial Grammar.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Meeting of the ACL,</booktitle>
<pages>335--342</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="5295" citStr="Hockenmaier and Steedman, 2002" startWordPosition="836" endWordPosition="839"> baseline system builds a CCG derivation by choosing and ordering words from the input set. The scoring model is trained using CCGBank (Hockenmaier and Steedman, 2007), and best-first decoding is applied. We apply the same decoding framework in this paper, but apply an improved training process, and incorporate an N-gram language model into the syntax model. In this section, we describe and discuss the baseline statistical model and decoding framework, motivating our extensions. 2.1 Combinatory Categorial Grammar CCG, and parsing with CCG, has been described elsewhere (Clark and Curran, 2007; Hockenmaier and Steedman, 2002); here we provide only a short description. CCG (Steedman, 2000) is a lexicalized grammar formalism, which associates each word in a sentence with a lexical category. There is a small number of basic lexical categories, such as noun (N), noun phrase (NP), and prepositional phrase (PP). Complex lexical categories are formed recursively from basic categories and slashes, which indicate the directions of arguments. The CCG grammar used by our system is read off the derivations in CCGbank, following Hockenmaier and Steedman (2002), meaning that the CCG combinatory rules are encoded as rule instanc</context>
</contexts>
<marker>Hockenmaier, Steedman, 2002</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2002. Generative models for statistical parsing with Combinatory Categorial Grammar. In Proceedings of the 40th Meeting of the ACL, pages 335–342, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>CCGbank: A corpus of CCG derivations and dependency structures extracted from the Penn Treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="4831" citStr="Hockenmaier and Steedman, 2007" startWordPosition="765" endWordPosition="769">m for the challenging task of the general word ordering problem. Second, we develop a novel method for incorporating a large-scale language model into a syntax-based generation system. Finally, we analyse large-margin training in the context of learning-guided best-first search, offering a novel solution to this computationally hard problem. 2 The statistical model and decoding algorithm We take Z&amp;C as our baseline system. Given a multi-set of input words, the baseline system builds a CCG derivation by choosing and ordering words from the input set. The scoring model is trained using CCGBank (Hockenmaier and Steedman, 2007), and best-first decoding is applied. We apply the same decoding framework in this paper, but apply an improved training process, and incorporate an N-gram language model into the syntax model. In this section, we describe and discuss the baseline statistical model and decoding framework, motivating our extensions. 2.1 Combinatory Categorial Grammar CCG, and parsing with CCG, has been described elsewhere (Clark and Curran, 2007; Hockenmaier and Steedman, 2002); here we provide only a short description. CCG (Steedman, 2000) is a lexicalized grammar formalism, which associates each word in a sen</context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2007. CCGbank: A corpus of CCG derivations and dependency structures extracted from the Penn Treebank. Computational Linguistics, 33(3):355–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kneser</author>
<author>H Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In International Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<volume>95</volume>
<pages>181--184</pages>
<contexts>
<context position="23470" citStr="Kneser and Ney, 1995" startWordPosition="3959" endWordPosition="3962">nd a large additional collection of fluent sentences in the Agence France-Presse (AFP) and Xinhua News Agency (XIN) subsets of the English GigaWord Fourth Edition (Parker et al., 2009), a total of over 1 billion tokens. The GigaWord data was first pre-processed to replicate the CCGBank tokenization. The total number of sentences and tokens in each LM component is shown in Table 1. The language model vocabulary consists of the 46,574 words that occur in the concatenation of the CCGBank training, development, and test sets. The LM probabilities are estimated using modified Kneser-Ney smoothing (Kneser and Ney, 1995) with interpolation of lower n-gram orders. 5.1 Development experiments A set of development test results without lexical category pruning (i.e. using the full dictionary) is shown in Table 2. We train the baseline system and our systems under various settings for 10 iterations, and measure the output BLEU scores after each iteration. The timeout value for each sentence is set to 5 seconds. The highest score (max BLEU) and averaged score (avg. BLEU) of each system over the 10 training iterations are shown in the table. 741 Method max BLEU avg. BLEU baseline 38.47 37.36 margin 41.20 39.70 margi</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>R. Kneser and H. Ney. 1995. Improved backing-off for m-gram language modeling. In International Conference on Acoustics, Speech, and Signal Processing, 1995. ICASSP-95, volume 1, pages 181– 184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Koehn</author>
<author>Franz Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofNAACL/HLT,</booktitle>
<location>Edmonton, Canada,</location>
<contexts>
<context position="1850" citStr="Koehn et al., 2003" startWordPosition="287" endWordPosition="290">word ordering, which can be abstractly formulated as finding a grammatical order for a multiset of words. The word ordering problem can also include word choice, where only a subset of the input words are used to produce the output. Word ordering is a difficult problem. Finding the best permutation for a set of words according to a bigram language model, for example, is NP-hard, which can be proved by linear reduction from the traveling salesman problem. In practice, exploring the whole search space of permutations is often prevented by adding constraints. In phrase-based machine translation (Koehn et al., 2003; Koehn et al., 2007), a distortion limit is used to constrain the position of output phrases. In syntax-based machine translation systems such as Wu (1997) and Chiang (2007), synchronous grammars limit the search space so that polynomial time inference is feasible. In fluency improvement (Blackwood et al., 2010), parts of translation hypotheses identified as having high local confidence are held fixed, so that word ordering elsewhere is strictly local. Some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search. Wan et al. (200</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philip Koehn, Franz Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings ofNAACL/HLT, Edmonton, Canada, May.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1871" citStr="Koehn et al., 2007" startWordPosition="291" endWordPosition="294"> can be abstractly formulated as finding a grammatical order for a multiset of words. The word ordering problem can also include word choice, where only a subset of the input words are used to produce the output. Word ordering is a difficult problem. Finding the best permutation for a set of words according to a bigram language model, for example, is NP-hard, which can be proved by linear reduction from the traveling salesman problem. In practice, exploring the whole search space of permutations is often prevented by adding constraints. In phrase-based machine translation (Koehn et al., 2003; Koehn et al., 2007), a distortion limit is used to constrain the position of output phrases. In syntax-based machine translation systems such as Wu (1997) and Chiang (2007), synchronous grammars limit the search space so that polynomial time inference is feasible. In fluency improvement (Blackwood et al., 2010), parts of translation hypotheses identified as having high local confidence are held fixed, so that word ordering elsewhere is strictly local. Some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search. Wan et al. (2009) uses a dependency </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177–180, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="21547" citStr="Papineni et al., 2002" startWordPosition="3647" endWordPosition="3650">, and does not add to its asymptotic complexity, due to the heuristic nature of the decoder. 5 Experiments We use sections 2–21 of CCGBank to train our syntax model, section 00 for development and section 23 for the final test. Derivations from CCGBank are transformed into inputs by turning their surface strings into multi-sets of words. Following Z&amp;C, we treat base noun phrases (i.e. NPs that do not recursively contain other NPs) as atomic units for the input. Output sequences are compared with the original sentences to evaluate their quality. We follow previous work and use the BLEU metric (Papineni et al., 2002) to compare outputs with references. Z&amp;C use two methods to construct leaf edges. The first is to assign lexical categories according to a dictionary. There are 26.8 lexical categories for each word on average using this method, corresponding to 26.8 leaf edges. The other method is to use a pre-processing step — a CCG supertagger (Clark and Curran, 2007) — to prune candidate lexical categories according to the goldCCGBank Sentences Tokens training 39,604 929,552 development 1,913 45,422 GigaWord v4 Sentences Tokens AFP 30,363,052 684,910,697 XIN 15,982,098 340,666,976 Table 1: Number of senten</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Parker</author>
<author>David Graff</author>
<author>Junbo Kong</author>
<author>Ke Chen</author>
<author>Kazuaki Maeda</author>
</authors>
<title>English Gigaword Fourth Edition, Linguistic Data Consortium.</title>
<date>2009</date>
<contexts>
<context position="23033" citStr="Parker et al., 2009" startWordPosition="3885" endWordPosition="3888">control the pruning. Here we focus mainly on the dictionary method, which leaves lexical category disambiguation entirely to the generation system. For comparison, we also perform experiments with lexical category pruning. We chose Q = 0.0001, which leaves 5.4 leaf edges per word on average. We used the SRILM Toolkit (Stolcke, 2002) to build a true-case 4-gram language model estimated over the CCGBank training and development data and a large additional collection of fluent sentences in the Agence France-Presse (AFP) and Xinhua News Agency (XIN) subsets of the English GigaWord Fourth Edition (Parker et al., 2009), a total of over 1 billion tokens. The GigaWord data was first pre-processed to replicate the CCGBank tokenization. The total number of sentences and tokens in each LM component is shown in Table 1. The language model vocabulary consists of the 46,574 words that occur in the concatenation of the CCGBank training, development, and test sets. The LM probabilities are estimated using modified Kneser-Ney smoothing (Kneser and Ney, 1995) with interpolation of lower n-gram orders. 5.1 Development experiments A set of development test results without lexical category pruning (i.e. using the full dic</context>
</contexts>
<marker>Parker, Graff, Kong, Chen, Maeda, 2009</marker>
<rawString>Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2009. English Gigaword Fourth Edition, Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>Michael Collins</author>
</authors>
<title>Exact decoding of syntactic translation models through lagrangian relaxation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>72--82</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="20824" citStr="Rush and Collins, 2011" startWordPosition="3525" endWordPosition="3528">l machine translation systems, both of which apply a score from the Ngram model component in a derivation-building process. As discussed earlier, polynomial-time decoding is typically feasible for syntax-based machine translation systems without an N-gram language model, due to constraints from the grammar. In these cases, incorporation of Ngram language models can significantly increase the complexity of a dynamic-programming decoder (Bar-Hillel et al., 1961). Efficient search has been achieved using chart pruning (Chiang, 2007) and iterative numerical approaches to constrained optimization (Rush and Collins, 2011). In contrast, the incorporation of an N-gram language model into our decoder is more straightforward, and does not add to its asymptotic complexity, due to the heuristic nature of the decoder. 5 Experiments We use sections 2–21 of CCGBank to train our syntax model, section 00 for development and section 23 for the final test. Derivations from CCGBank are transformed into inputs by turning their surface strings into multi-sets of words. Following Z&amp;C, we treat base noun phrases (i.e. NPs that do not recursively contain other NPs) as atomic units for the input. Output sequences are compared wit</context>
</contexts>
<marker>Rush, Collins, 2011</marker>
<rawString>Alexander M. Rush and Michael Collins. 2011. Exact decoding of syntactic translation models through lagrangian relaxation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 72–82, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Aravind Joshi</author>
</authors>
<title>LTAG dependency parsing with bidirectional incremental construction.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>495--504</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="15906" citStr="Shen and Joshi, 2008" startWordPosition="2697" endWordPosition="2700">t only gold-standard edges are expanded). From 739 the perspective of correctness, it is unnecessary to find a margin between the sub-edges of e+ and those of e−, since both are gold-standard edges. However, since the score of an edge not only represents its correctness, but also affects its priority on the agenda, promoting the sub-edge of e+ can lead to “easier” edges being constructed before “harder” ones (i.e. those that are less likely to be correct), and therefore improve the output accuracy. This perspective has been observed by other works of learning-guided-search (Shen et al., 2007; Shen and Joshi, 2008; Goldberg and Elhadad, 2010). Intuitively, the score difference between easy gold-standard and harder gold-standard edges should not be as great as the difference between gold-standard and non-goldstandard edges. The perceptron update cannot provide such control of separation, because the amount of update is fixed to 1. As described earlier, we treat parameter update as finding a separation between correct and incorrect edges, in which the global feature vectors 4b, rather than φ, are considered. Given a positive example e+ and a negative example e−, we make a minimum update so that the score</context>
</contexts>
<marker>Shen, Joshi, 2008</marker>
<rawString>Libin Shen and Aravind Joshi. 2008. LTAG dependency parsing with bidirectional incremental construction. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 495–504, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Giorgio Satta</author>
<author>Aravind Joshi</author>
</authors>
<title>Guided learning for bidirectional sequence classification.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>760--767</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="15884" citStr="Shen et al., 2007" startWordPosition="2693" endWordPosition="2696"> constructed so that only gold-standard edges are expanded). From 739 the perspective of correctness, it is unnecessary to find a margin between the sub-edges of e+ and those of e−, since both are gold-standard edges. However, since the score of an edge not only represents its correctness, but also affects its priority on the agenda, promoting the sub-edge of e+ can lead to “easier” edges being constructed before “harder” ones (i.e. those that are less likely to be correct), and therefore improve the output accuracy. This perspective has been observed by other works of learning-guided-search (Shen et al., 2007; Shen and Joshi, 2008; Goldberg and Elhadad, 2010). Intuitively, the score difference between easy gold-standard and harder gold-standard edges should not be as great as the difference between gold-standard and non-goldstandard edges. The perceptron update cannot provide such control of separation, because the amount of update is fixed to 1. As described earlier, we treat parameter update as finding a separation between correct and incorrect edges, in which the global feature vectors 4b, rather than φ, are considered. Given a positive example e+ and a negative example e−, we make a minimum up</context>
</contexts>
<marker>Shen, Satta, Joshi, 2007</marker>
<rawString>Libin Shen, Giorgio Satta, and Aravind Joshi. 2007. Guided learning for bidirectional sequence classification. In Proceedings of ACL, pages 760–767, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context position="2555" citStr="Steedman, 2000" startWordPosition="401" endWordPosition="402">ses. In syntax-based machine translation systems such as Wu (1997) and Chiang (2007), synchronous grammars limit the search space so that polynomial time inference is feasible. In fluency improvement (Blackwood et al., 2010), parts of translation hypotheses identified as having high local confidence are held fixed, so that word ordering elsewhere is strictly local. Some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search. Wan et al. (2009) uses a dependency grammar to solve word ordering, and Zhang and Clark (2011) uses CCG (Steedman, 2000) for word ordering and word choice. The use of syntax models makes their search problems harder than word permutation using an N-gram language model only. Both methods apply heuristic search. Zhang and Clark developed a bottom-up best-first algorithm to build output syntax trees from input words, where search is guided by learning for both efficiency and accuracy. The framework is flexible in allowing a large range of constraints to be added for particular tasks. We extend the work of Zhang and Clark (2011) (Z&amp;C) in two ways. First, we apply online largemargin training to guide search. Compare</context>
<context position="5359" citStr="Steedman, 2000" startWordPosition="848" endWordPosition="849"> input set. The scoring model is trained using CCGBank (Hockenmaier and Steedman, 2007), and best-first decoding is applied. We apply the same decoding framework in this paper, but apply an improved training process, and incorporate an N-gram language model into the syntax model. In this section, we describe and discuss the baseline statistical model and decoding framework, motivating our extensions. 2.1 Combinatory Categorial Grammar CCG, and parsing with CCG, has been described elsewhere (Clark and Curran, 2007; Hockenmaier and Steedman, 2002); here we provide only a short description. CCG (Steedman, 2000) is a lexicalized grammar formalism, which associates each word in a sentence with a lexical category. There is a small number of basic lexical categories, such as noun (N), noun phrase (NP), and prepositional phrase (PP). Complex lexical categories are formed recursively from basic categories and slashes, which indicate the directions of arguments. The CCG grammar used by our system is read off the derivations in CCGbank, following Hockenmaier and Steedman (2002), meaning that the CCG combinatory rules are encoded as rule instances, together with a number of additional rules which deal with p</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. The MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<contexts>
<context position="22747" citStr="Stolcke, 2002" startWordPosition="3840" endWordPosition="3841">mber of sentences and tokens by language model source. standard sequence, assuming that for some problems the ambiguities can be reduced (e.g. when the input is already partly correctly ordered). Z&amp;C use different probability cutoff levels (the Q parameter in the supertagger) to control the pruning. Here we focus mainly on the dictionary method, which leaves lexical category disambiguation entirely to the generation system. For comparison, we also perform experiments with lexical category pruning. We chose Q = 0.0001, which leaves 5.4 leaf edges per word on average. We used the SRILM Toolkit (Stolcke, 2002) to build a true-case 4-gram language model estimated over the CCGBank training and development data and a large additional collection of fluent sentences in the Agence France-Presse (AFP) and Xinhua News Agency (XIN) subsets of the English GigaWord Fourth Edition (Parker et al., 2009), a total of over 1 billion tokens. The GigaWord data was first pre-processed to replicate the CCGBank tokenization. The total number of sentences and tokens in each LM component is shown in Table 1. The language model vocabulary consists of the 46,574 words that occur in the concatenation of the CCGBank training</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Proceedings of the International Conference on Spoken Language Processing, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Wan</author>
<author>Mark Dras</author>
<author>Robert Dale</author>
<author>C´ecile Paris</author>
</authors>
<title>Improving grammaticality in statistical sentence generation: Introducing a dependency spanning tree algorithm with an argument satisfaction model.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</booktitle>
<pages>852--860</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<contexts>
<context position="708" citStr="Wan et al., 2009" startWordPosition="92" endWordPosition="95">wood Stephen Clark University of Cambridge University of Cambridge University of Cambridge Computer Laboratory Engineering Department Computer Laboratory yz360@cam.ac.uk gwb24@eng.cam.ac.uk sc609@cam.ac.uk Abstract A fundamental problem in text generation is word ordering. Word ordering is a computationally difficult problem, which can be constrained to some extent for particular applications, for example by using synchronous grammars for statistical machine translation. There have been some recent attempts at the unconstrained problem of generating a sentence from a multi-set of input words (Wan et al., 2009; Zhang and Clark, 2011). By using CCG and learning guided search, Zhang and Clark reported the highest scores on this task. One limitation of their system is the absence of an N-gram language model, which has been used by text generation systems to improve fluency. We take the Zhang and Clark system as the baseline, and incorporate an N-gram model by applying online large-margin training. Our system significantly improved on the baseline by 3.7 BLEU points. 1 Introduction One fundamental problem in text generation is word ordering, which can be abstractly formulated as finding a grammatical o</context>
<context position="2452" citStr="Wan et al. (2009)" startWordPosition="382" endWordPosition="385">hn et al., 2003; Koehn et al., 2007), a distortion limit is used to constrain the position of output phrases. In syntax-based machine translation systems such as Wu (1997) and Chiang (2007), synchronous grammars limit the search space so that polynomial time inference is feasible. In fluency improvement (Blackwood et al., 2010), parts of translation hypotheses identified as having high local confidence are held fixed, so that word ordering elsewhere is strictly local. Some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search. Wan et al. (2009) uses a dependency grammar to solve word ordering, and Zhang and Clark (2011) uses CCG (Steedman, 2000) for word ordering and word choice. The use of syntax models makes their search problems harder than word permutation using an N-gram language model only. Both methods apply heuristic search. Zhang and Clark developed a bottom-up best-first algorithm to build output syntax trees from input words, where search is guided by learning for both efficiency and accuracy. The framework is flexible in allowing a large range of constraints to be added for particular tasks. We extend the work of Zhang a</context>
<context position="32154" citStr="Wan et al. (2009)" startWordPosition="5460" endWordPosition="5463">found. syntactically grammatical, but are semantically anomalous. For example, person names are often confused with company names, verbs often take unrelated subjects and objects. The problem is much more severe for long sentences, which have more ambiguities. For specific tasks, extra information (such as the source text for machine translation) can be available to reduce ambiguities. 6 Final results The final results of our system without lexical category pruning are shown in Table 5. Row “W09 CLE” and “W09 AB” show the results of the maximum spanning tree and assignment-based algorithms of Wan et al. (2009); rows “margin” and “margin +LM” show the results of our largemargin training system and our system with the N-gram model. All these results are directly comparable since we do not use any lexical category pruning for this set of results. For each of our systems, we fix the number of training iterations according to development test scores. Consistent with the development experiments, our sys743 timeout = 5s timeout = 1m drooled the cars and drivers , like Fortune 500 executives . over After schoolboys drooled over the cars and drivers , the race the race like Fortune 500 executives. One big r</context>
<context position="34855" citStr="Wan et al. (2009)" startWordPosition="5911" endWordPosition="5914">fair comparison. The results are similar to Table 5: our large-margin training systems outperforms the baseline by 1.5 BLEU points, and adding the N-gram model gave a further 1.4 point improvement. The scores could be significantly increased by using a larger timeout, as shown in our earlier development experiments. 7 Related Work There is a recent line of research on text-totext generation, which studies the linearization of dependency structures (Barzilay and McKeown, 2005; Filippova and Strube, 2007; Filippova and Strube, 2009; Bohnet et al., 2010; Guo et al., 2011). Unlike our system, and Wan et al. (2009), input dependencies provide additional information to these systems. Although the search space can be constrained by the assumption of projectivity, permutation of modifiers of the same head word makes exact inference for tree linearization intractable. The above systems typically apply approximate inference, such as beam-search. While syntax-based features are commonly used by these systems for linearization, Filippova and Strube (2009) apply a trigram model to control local fluency within constituents. A dependencybased N-gram model has also been shown effective for the linearization task (</context>
</contexts>
<marker>Wan, Dras, Dale, Paris, 2009</marker>
<rawString>Stephen Wan, Mark Dras, Robert Dale, and C´ecile Paris. 2009. Improving grammaticality in statistical sentence generation: Introducing a dependency spanning tree algorithm with an argument satisfaction model. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 852–860, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael White</author>
</authors>
<title>Reining in CCG chart realization.</title>
<date>2004</date>
<booktitle>In Proc. INLG-04,</booktitle>
<pages>182--191</pages>
<contexts>
<context position="35569" citStr="White (2004)" startWordPosition="6022" endWordPosition="6023"> constrained by the assumption of projectivity, permutation of modifiers of the same head word makes exact inference for tree linearization intractable. The above systems typically apply approximate inference, such as beam-search. While syntax-based features are commonly used by these systems for linearization, Filippova and Strube (2009) apply a trigram model to control local fluency within constituents. A dependencybased N-gram model has also been shown effective for the linearization task (Guo et al., 2011). The best-first inference and timeout mechanism of our system is similar to that of White (2004), a surface realizer from logical forms using CCG. 8 Conclusion We studied the problem of word-ordering using a syntactic model and allowing permutation. We took the model of Zhang and Clark (2011) as the baseline, and extended it with online large-margin training and an N-gram language model. These extentions led to improvements in the BLEU evaluation. Analyzing the generated sentences suggests that, while highly fluent outputs can be produced for short sentences (≤ 10 words), the system fluency in general is still way below human standard. Future work remains to apply the system as a compone</context>
</contexts>
<marker>White, 2004</marker>
<rawString>Michael White. 2004. Reining in CCG chart realization. In Proc. INLG-04, pages 182–191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="2006" citStr="Wu (1997)" startWordPosition="314" endWordPosition="315">re only a subset of the input words are used to produce the output. Word ordering is a difficult problem. Finding the best permutation for a set of words according to a bigram language model, for example, is NP-hard, which can be proved by linear reduction from the traveling salesman problem. In practice, exploring the whole search space of permutations is often prevented by adding constraints. In phrase-based machine translation (Koehn et al., 2003; Koehn et al., 2007), a distortion limit is used to constrain the position of output phrases. In syntax-based machine translation systems such as Wu (1997) and Chiang (2007), synchronous grammars limit the search space so that polynomial time inference is feasible. In fluency improvement (Blackwood et al., 2010), parts of translation hypotheses identified as having high local confidence are held fixed, so that word ordering elsewhere is strictly local. Some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search. Wan et al. (2009) uses a dependency grammar to solve word ordering, and Zhang and Clark (2011) uses CCG (Steedman, 2000) for word ordering and word choice. The use of synt</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Syntaxbased grammaticality improvement using CCG and guided search.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1147--1157</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="732" citStr="Zhang and Clark, 2011" startWordPosition="96" endWordPosition="99"> University of Cambridge University of Cambridge University of Cambridge Computer Laboratory Engineering Department Computer Laboratory yz360@cam.ac.uk gwb24@eng.cam.ac.uk sc609@cam.ac.uk Abstract A fundamental problem in text generation is word ordering. Word ordering is a computationally difficult problem, which can be constrained to some extent for particular applications, for example by using synchronous grammars for statistical machine translation. There have been some recent attempts at the unconstrained problem of generating a sentence from a multi-set of input words (Wan et al., 2009; Zhang and Clark, 2011). By using CCG and learning guided search, Zhang and Clark reported the highest scores on this task. One limitation of their system is the absence of an N-gram language model, which has been used by text generation systems to improve fluency. We take the Zhang and Clark system as the baseline, and incorporate an N-gram model by applying online large-margin training. Our system significantly improved on the baseline by 3.7 BLEU points. 1 Introduction One fundamental problem in text generation is word ordering, which can be abstractly formulated as finding a grammatical order for a multiset of w</context>
<context position="2529" citStr="Zhang and Clark (2011)" startWordPosition="395" endWordPosition="398">train the position of output phrases. In syntax-based machine translation systems such as Wu (1997) and Chiang (2007), synchronous grammars limit the search space so that polynomial time inference is feasible. In fluency improvement (Blackwood et al., 2010), parts of translation hypotheses identified as having high local confidence are held fixed, so that word ordering elsewhere is strictly local. Some recent work attempts to address the fundamental word ordering task directly, using syntactic models and heuristic search. Wan et al. (2009) uses a dependency grammar to solve word ordering, and Zhang and Clark (2011) uses CCG (Steedman, 2000) for word ordering and word choice. The use of syntax models makes their search problems harder than word permutation using an N-gram language model only. Both methods apply heuristic search. Zhang and Clark developed a bottom-up best-first algorithm to build output syntax trees from input words, where search is guided by learning for both efficiency and accuracy. The framework is flexible in allowing a large range of constraints to be added for particular tasks. We extend the work of Zhang and Clark (2011) (Z&amp;C) in two ways. First, we apply online largemargin trainin</context>
<context position="35766" citStr="Zhang and Clark (2011)" startWordPosition="6052" endWordPosition="6055">approximate inference, such as beam-search. While syntax-based features are commonly used by these systems for linearization, Filippova and Strube (2009) apply a trigram model to control local fluency within constituents. A dependencybased N-gram model has also been shown effective for the linearization task (Guo et al., 2011). The best-first inference and timeout mechanism of our system is similar to that of White (2004), a surface realizer from logical forms using CCG. 8 Conclusion We studied the problem of word-ordering using a syntactic model and allowing permutation. We took the model of Zhang and Clark (2011) as the baseline, and extended it with online large-margin training and an N-gram language model. These extentions led to improvements in the BLEU evaluation. Analyzing the generated sentences suggests that, while highly fluent outputs can be produced for short sentences (≤ 10 words), the system fluency in general is still way below human standard. Future work remains to apply the system as a component for specific text generation tasks, for example machine translation. Acknowledgements Yue Zhang and Stephen Clark are supported by the European Union Seventh Framework Programme (FP7- ICT-2009-4</context>
</contexts>
<marker>Zhang, Clark, 2011</marker>
<rawString>Yue Zhang and Stephen Clark. 2011. Syntaxbased grammaticality improvement using CCG and guided search. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1147–1157, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>