<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000025">
<title confidence="0.982967">
Summarizing Search Results using PLSI
</title>
<author confidence="0.969915">
Jun Harashima* and Sadao Kurohashi
</author>
<affiliation confidence="0.972202">
Graduate School of Informatics
Kyoto University
</affiliation>
<address confidence="0.856139">
Yoshida-honmachi, Sakyo-ku,
Kyoto, 606-8501, Japan
</address>
<email confidence="0.999614">
{harashima,kuro}@nlp.kuee.kyoto-u.ac.jp
</email>
<sectionHeader confidence="0.994821" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999912653846154">
In this paper, we investigate generating
a set of query-focused summaries from
search results. Since there may be many
topics related to a given query in the
search results, in order to summarize
these results, they should first be clas-
sified into topics, and then each topic
should be summarized individually. In
this summarization process, two types of
redundancies need to be reduced. First,
each topic summary should not contain
any redundancy (we refer to this prob-
lem as redundancy within a summary).
Second, a topic summary should not be
similar to any other topic summary (we
refer to this problem as redundancy be-
tween summaries). In this paper, we
focus on the document clustering pro-
cess and the reduction of redundancy be-
tween summaries in the summarization
process. We also propose a method using
PLSI to summarize search results. Eval-
uation results confirm that our method
performs well in classifying search re-
sults and reducing the redundancy be-
tween summaries.
</bodyText>
<sectionHeader confidence="0.999163" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.975750625">
Currently, the World Wide Web contains vast
amounts of information. To make efficient use of
this information, search engines are indispens-
able. However, search engines generally return
*Research Fellow of the Japan Society for the Promotion
of Science (JSPS)
only a long list containing the title and a snip-
pet of each of the retrieved documents. While
such lists are effective for navigational queries,
they are not helpful to users with informational
queries. Some systems (e.g., Clusty1) present
keywords related to a given query together with
the search results. It is, however, difficult for
users to understand the relation between the key-
words and the query, as the keywords are merely
words or phrases out of context. To solve this
problem, we address the task of generating a set
of query-focused summaries from search results
to present information about a given query using
natural sentences.
Since there are generally many topics re-
lated to a query in the search results, the task
of summarizing these results is one of, so to
speak, multi-topic multi-document summariza-
tion. Studies on multi-document summariza-
tion typically address summarizing documents
related to a single topic (e.g., TAC2). However
we need to address summarizing documents re-
lated to multiple topics when considering the
summarization of search results.
To summarize documents containing multiple
topics, we first need to classify them into top-
ics. For example, if a set of documents related to
swine flu contains topics such as the outbreaks of
swine flu, the measures to treat swine flu, and so
on, the documents should be divided into these
topics and summarized individually. Note that a
method for soft clustering should be employed
in this process, as one document may belong to
several topics.
</bodyText>
<footnote confidence="0.999139">
1http://clusty.com/
2http://www.nist.gov/tac/
</footnote>
<page confidence="0.957238">
12
</page>
<note confidence="0.7359295">
Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 12–20,
Beijing, August 2010
</note>
<bodyText confidence="0.999652914893617">
In the summarization process, two types of
redundancies need to be addressed. First, each
topic summary should not contain any redun-
dancy. We refer to this problem as redun-
dancy within a summary. This problem is well
known in the field of multi-document summa-
rization (Mani, 2001) and several methods have
been proposed to solve it, such as Maximum
Marginal Relevance (MMR) (Goldstein et al.,
2000) (Mori et al., 2004), using Integer Lin-
ear Programming (ILP) (Filatova and Hatzivas-
siloglou, 2004) (McDonald, 2007) (Takamura
and Okumura, 2009), and so on.
Second, no topic summary should be similar
to any of the other topic summaries. We re-
fer to this problem as redundancy between sum-
maries. For example, to summarize the above-
mentioned documents related to swine flu, the
summary for outbreaks should contain specific
information about outbreaks, whereas the sum-
mary for measures should contain specific infor-
mation about measures. This problem is char-
acteristic of multi-topic multi-document summa-
rization. Some methods have been proposed
to generate topic summaries from documents
(Radev and Fan, 2000) (Haghighi and Vander-
wende, 2009), but to the best of our knowledge,
the redundancy between summaries has not yet
been addressed in any study.
In this paper, we focus on the document clus-
tering process and the reduction of redundancy
between summaries in the summarization pro-
cess. Furthermore, we propose a method using
PLSI (Hofmann, 1999) to summarize search re-
sults. In the proposed method, we employ PLSI
to estimate the membership degree of each doc-
ument to each topic, and then classify the search
results into topics using this information. In the
same way, we employ PLSI to estimate the mem-
bership degree of each keyword to each topic,
and then extract the important sentences spe-
cific to each topic using this information in order
to reduce the redundancy between summaries.
The evaluation results show that our method per-
forms well in classifying search results and suc-
cessfully reduces the redundancy between sum-
maries.
</bodyText>
<figureCaption confidence="0.998082">
Figure 1: Overview of the proposed method.
</figureCaption>
<sectionHeader confidence="0.927237" genericHeader="method">
2 Proposed Method
</sectionHeader>
<subsectionHeader confidence="0.912967">
2.1 Overview
</subsectionHeader>
<bodyText confidence="0.99299165">
Figure 1 gives an overview of the proposed
method, which comprises the following four
steps.
Step 1. Acquisition of Search Results Using a
search engine, obtain the search results for
a given query.
Step 2. Keyword Extraction Extract the key-
words related to the query from the search
results using the method proposed by Shi-
bata et al. (2009).
Step 3. Document Clustering Estimate the
membership degree of each document to
each topic using PLSI, and classify the
search results into topics.
Step 4. Summarization For each topic, gener-
ate a summary by extracting the important
sentences specific to each topic from each
document cluster.
In the following subsections, we describe each
step in detail.
</bodyText>
<subsectionHeader confidence="0.998486">
2.2 Step 1. Acquisition of Search Results
</subsectionHeader>
<bodyText confidence="0.999859">
First, we obtain the search results for a given
query using a search engine. To be more precise,
we obtain the top N&apos; documents of the search en-
gine results. Next, we remove those documents
that should not be included in the summarization,
such as link collections, using a simple filtering
method. For example, we regard any document
that has too many links as a link collection, and
remove it.
In this paper, we write D to denote the search
results after the filtering, and let N = |D|.
</bodyText>
<figure confidence="0.998551928571429">
De
keywords W
transmission, human
oseltamlvlr, Inoculate
prevent avian X ...
search
engine
�
Step 2 s� 1 1 ary -e
Step 1
Inoculate
oseltamlvlr
prevent
�
</figure>
<page confidence="0.991371">
13
</page>
<subsectionHeader confidence="0.989332">
2.3 Step 2. Keyword Extraction
</subsectionHeader>
<bodyText confidence="0.979674142857143">
We extract the keywords related to a query from
D using the method proposed by Shibata et
al. (2009), which comprises the following four
steps.
Step 2-1. Relevant Sentence Extraction For
each document in D, extract the sentences
containing the query and the sentences
around the query as relevant sentences.
Step 2-2. Keyword Candidate Extraction For
each relevant sentence, extract compound
nouns and parenthetic strings as keyword
candidates.
Step 2-3. Synonymous Candidate Unification
Find the paraphrase pairs and the ortho-
graphic variant pairs in the keyword
candidates, and merge them.
Step 2-4. Keyword Selection Score each key-
word candidate, rank them, and select the
best M as the keywords related to the query.
In this paper, we write W to denote the ex-
tracted keywords.
</bodyText>
<subsectionHeader confidence="0.993699">
2.4 Step 3. Document Clustering
</subsectionHeader>
<bodyText confidence="0.999255">
We classify D into topics using PLSI. In PLSI,
a document d and a word w are assumed to be
conditionally independent given a topic z, and
the joint probability p(d, w) is calculated as fol-
lows.
</bodyText>
<equation confidence="0.9642875">
p(d, w) = ∑ p(z) p(d|z) p(w|z) (1)
z
</equation>
<bodyText confidence="0.998688333333333">
p(z), p(d|z), and p(w|z) are estimated by max-
imizing the log-likelihood function L, which is
calculated as
</bodyText>
<equation confidence="0.990797">
∑L = ∑ freq(d, w) log p(d, w), (2)
</equation>
<bodyText confidence="0.8856665">
d w
where freq(d, w) represents the frequency of
word w in document d. L is maximized using
the EM algorithm, in which the E-step and M-
step are given below.
E-step
</bodyText>
<equation confidence="0.9980845">
p(z|d, w) = p(z) p(d|z) p(w|z) (3)
∑z, p(z0) p(d|z0) p(w|z0)
M-step
p(z) = ∑d ∑w freq(d, w) p(z|d, w) (4)
∑d ∑w freq(d, w)
∑w freq(d, w) p(z|d, w)
p(d|z) = ∑d, ∑w freq(d0, w) p(z|d0, w) (5)
∑d freq(d, w) p(z|d, w)
p(w|z) = (6)
∑d ∑w, freq(d, w0) p(z|d, w0)
</equation>
<bodyText confidence="0.999884">
The EM algorithm iterates through these steps
until convergence.
First, we give PLSI the number of topics K,
the search results D, and the keywords W as
input, and estimate p(z), p(d|z), and p(w|z),
where z is a topic related to the query, d is a doc-
ument in D, and w is a keyword in W. There is,
however, no way of knowing the value of K; that
is, we do not know in advance how many topics
related to the query there are in the search results.
Hence, we perform PLSI for several values of K,
and select the K that has the minimum Akaike
Information Criterion (AIC) (Akaike, 1974), cal-
culated as follows.
</bodyText>
<equation confidence="0.997297">
AIC = −2L + 2K(N + M) (7)
</equation>
<bodyText confidence="0.999677571428571">
Furthermore, we select p(z), p(d|z), and p(w|z)
estimated using the selected K as the result of
PLSI.
Next, we calculate the membership degree of
each document to each topic. The membership
degree of document d to topic z, denoted p(z|d),
is calculated as
</bodyText>
<equation confidence="0.999172">
p(z|d) = p(d|z) p(z)
∑ z, p(d|z0)� (8)
</equation>
<bodyText confidence="0.999967">
Finally, for each topic, we collect those docu-
ments whose membership degree to the topic is
larger than the threshold α. If there is a docu-
ment whose membership degree to multiple top-
ics is larger than the threshold, we classify the
document into each topic.
In this paper, Dz denotes the documents clas-
sified into topic z.
</bodyText>
<subsectionHeader confidence="0.998486">
2.5 Step 4. Summarization
</subsectionHeader>
<bodyText confidence="0.990672">
For each topic, we extract the important sen-
tences specific to that topic from each document
</bodyText>
<page confidence="0.994428">
14
</page>
<bodyText confidence="0.9988915">
cluster. Figure 2 gives the algorithm for sum-
marization. When we generate the summary Sz
for topic z, we calculate the importance of sen-
tence s to topic z, denoted as s score(z, s, Sz),
for each sentence in Dz (lines 3-4). Then we ex-
tract the sentence smax with the maximum im-
portance as an important sentence, and include
smax in Sz (lines 5-6). When we extract the
next important sentence, we recalculate the im-
portance s score(z, s, Sz) for each sentence in
Dz except the sentence in Sz (lines 3-4). Then
we extract the sentence smax with the maximum
importance as an important sentence, and add
smax to Sz (lines 5-6). We continue this process
until the number of important sentences compos-
ing the summary, denoted |Sz|, reaches the num-
ber of important sentences extracted for topic z,
denoted num(z) (line 2).
</bodyText>
<equation confidence="0.887269">
s score(z, s, Sz) is calculated as follows:
s score(z, s, Sz)
( )
= E w score(z, w) x c score(w, Sz, s)
wEW3
</equation>
<bodyText confidence="0.998460692307692">
where W3 represents the keywords in sentence s.
w score(z, w) is a function to reduce the re-
dundancy between summaries, and represents
the importance of keyword w to topic z. We can
use the probability of w given z, denoted p(w|z),
as the w score(z, w). This approach fails, how-
ever, because if there are keywords with a high
probability in both topic z and another topic z&apos;,
the sentences containing such keywords are ex-
tracted as the important sentences in both top-
ics, and it follows that the generated summaries
will contain redundancy. To solve this problem,
we use the membership degree of keyword w
</bodyText>
<figureCaption confidence="0.526127">
Figure 2: Algorithm for summarization.
</figureCaption>
<figure confidence="0.5915905">
Input: A set of K document clusters {Dz}(z ∈ Z)
Output: A set of K summaries {Sz}(z ∈ Z)
Procedure:
1: for all z ∈ Z
2: while |Sz |&lt; num(z)
3: for all s ∈ Dz
4: calculates score(z, s, Sz)
5: smax = argmax3EDz\Sz s score(z, s, Sz)
6: Sz = Sz ∪ {smax}
7: return Sz
</figure>
<bodyText confidence="0.916326">
to topic z, denoted p(z|w), as w score(z, w).
We use p(z) and p(w|z) estimated using PLSI
in Section 2.4 to calculate p(z|w).
</bodyText>
<equation confidence="0.995054">
p(z|w) = p(w|z) p(z) (10)
Ez/ p(w|z&apos;)
</equation>
<bodyText confidence="0.999282257142857">
Keywords with high probability in several topics
should have a low membership degree to each
topic. Thus, using p(z|w) as the w score(z, w)
prevents extracting sentences containing such
keywords as important sentences, and it follows
that the similarity between the summaries is re-
duced. Furthermore, the keywords which are
specific to a topic are supposed to have a high
membership degree to that topic. Thus, using
p(z|w) as w score(z, w) makes it easier to ex-
tract sentences containing such keywords as im-
portant sentences, and with the result that each
summary is specific to the particular topic.
c score(w, Sz, s) is a function to reduce the
redundancy within a summary, and represents
the importance of a keyword w in a sentence
s under the condition that there is a set of
extracted important sentences Sz. The value
of c score(w, Sz, s) is determined mainly by
whether or not w is contained in Sz. Ta-
(9) ble 1 gives the values of c score(w, Sz, s).
For example, if w is contained in Sz, we
set c score(w, Sz, s) = 0, else we set
c score(w, Sz, s) = 1. In this way, we can ex-
tract the sentences containing the keywords that
are not contained in Sz as important sentences,
and reduce the redundancy within the summary.
Note that we make some exceptions to generate
a coherent summary. For example, even if w is
contained in Sz, we set c score(w, Sz, s) = 2
as long as w is the subject of s. In the same
way, even if w is not contained in Sz, we set
c score(w, Sz, s) = −2 as long as w is the sub-
ject of s. These values for c score(w, Sz, s) are
empirically determined.
</bodyText>
<figure confidence="0.987291">
w is w is not
contained in Sz contained in Sz
w is the subject of s
otherwise
2 -2
0 1
</figure>
<tableCaption confidence="0.967926">
Table 1: Values of c score(w, Sz, s).
</tableCaption>
<page confidence="0.991127">
15
</page>
<bodyText confidence="0.999838666666667">
Finally, using p(z) we determine the number
of important sentences extracted for topic z, de-
noted as num(z).
</bodyText>
<equation confidence="0.905557666666667">
�
num (z) — L I x p(z) J ( p(z) ? 0 ) (11)
( ) — Imin ( p(z) C 0 )
</equation>
<bodyText confidence="0.998871333333333">
where I represents the parameter that controls
the total number of important sentences ex-
tracted for each topic. The higher the probability
a topic has, the more important sentences we ex-
tract. Note that no matter how low p(z) is, we
extract at least Imin important sentences.
</bodyText>
<sectionHeader confidence="0.999671" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.91614">
3.1 Overview
</subsectionHeader>
<bodyText confidence="0.999656">
To evaluate the proposed method, we recruited
48 subjects, mainly IT workers, and asked them
to fill in a questionnaire. We prepared a sys-
tem implemented according to our method, and
asked the subjects to use our system to evaluate
the following four aspects of our method.
</bodyText>
<listItem confidence="0.9990658">
• Validity of the number of topics
• Precision of document clustering
• Degree of reduction in redundancy between
summaries
• Effectiveness of the method for presenting
</listItem>
<bodyText confidence="0.884969">
information through summaries
We allowed the subjects to create arbitrary
queries for our system.
</bodyText>
<subsectionHeader confidence="0.996722">
3.2 System
</subsectionHeader>
<bodyText confidence="0.999986310344828">
Figure 3 shows the system results for the query
swine flu. Our system presents a separate sum-
mary for each topic related to a given query. In
Fig.3, the colored words in the summaries are
keywords specific to each topic. If a user clicks
on a keyword, the system presents a list of doc-
uments containing that keyword at the bottom of
the browser.
The configuration of our system was as fol-
lows. In the acquisition process, the system ob-
tained the search results for a given query us-
ing the search engine TSUBAKI (Shinzato et al.,
2008b). Setting N&apos; = 1, 000, we obtained the
top 1, 000 documents in the search results for
the query. In the keyword extraction process,
we set M = 100, and extracted 100 keywords
related to the query from the search results. In
the document clustering process, we performed
PLSI for K = 3, 4, 5, and selected the K with
the minimum AIC. We set the initial value of
p(z) = 1/K, and the initial values of p(d|z)
and p(w|z) to random values. The EM algorithm
continued until the increase in L reached just be-
low 1 to achieve convergence. We set α = 1/K.
In the summarization process, we set I = 10,
since the number of important sentences able to
be presented in a browser is about 10. We set
Imin = 2 and 0 = 0.2, and extracted at least two
important sentences, even if p(z) was very low.
</bodyText>
<subsectionHeader confidence="0.998286">
3.3 Validity of the Number of Topics
</subsectionHeader>
<bodyText confidence="0.999992074074074">
First, we investigated how well the proposed
method determined the number of topics. In our
method, the number is determined using AIC.
Ideally, we should have manually counted the
topics in the search results, and compared this
with the number determined using AIC. It was,
however, difficult to count the topics, because the
search results contained 1, 000 documents. Fur-
thermore, it was impossible to count the number
of topics for each query given by each subject.
Thus, in this investigation, we simply asked the
subjects whether they felt the number of topic
summaries presented to them was appropriate or
not, and investigated our method in terms of us-
ability.
Table 2 gives the results. According to Table
2, 60.4% of the subjects agreed that the number
of topic summaries presented by our system was
acceptable. The average of the number of topics
determined by our method was 3.18 per 1 query.
On the other hand, 33.3% of the subjects said
the number of topic summaries was too low or
somewhat too low. According to these results,
it seems that users are satisfied with the system
presenting about 3 or 4 topic summaries, and our
method determined the desirable number of top-
ics in terms of usability.
</bodyText>
<page confidence="0.95157">
16
</page>
<bodyText confidence="0.798880285714286">
sw ine flu
Topic 1
For now transmission from human to human has not been reported and no virus has evolved into swine flu,
but it is warned that there is a high possibility that avian influenza virus mutates in the body of human or birds
and evolves into swine flu virus which transmits from human to human.
If somewhere in the world highly virulent avian influenza evolves into swine flu which can transmit from
human to human (there is some possible reports by now), in the worst case scenario, this novel virus spreads
worldwide within a week and cause about 500 — 600 million death, and 10 million death in Japan.
�
Topic 2
Since the seasonal outbreaks of influenza starts in about January every year and about two weeks are required
until the results of vaccine inoculation are visible, they say the middle of December is better time to inoculate.
Oseltam ivir(Tam iflu) is a drug used to treattype A or type B influenza and preventthem, and is offered in the
form of capsules and dry syrups.
</bodyText>
<figure confidence="0.9660635">
Topic 3
�
</figure>
<figureCaption confidence="0.68718">
Figure 3: System results for the query swine flu.
Table 2: Validity of the number of topics.
</figureCaption>
<figure confidence="0.9975932">
options # subjects ( % )
(a) definitely too many
0 ( 0.0)
(b) somewhat too many
3 ( 6.3)
(c) acceptable 29 (60.4)
(d) somewhat too few
11 (22.9)
(e) definitely too few
5 (10.4)
</figure>
<subsectionHeader confidence="0.807069">
3.4 Precision of Document Clustering
</subsectionHeader>
<bodyText confidence="0.999602184210526">
Second, we investigated how precisely the pro-
posed method classified the search results into
topics. To be more precise, we evaluated the re-
liability of the membership degree p(zld) used
in the document clustering process. It is gen-
erally difficult to evaluate clustering methods.
In our case, we did not have any correct data
and could not even create these since, as men-
tioned previously, the number of topics is not
known. Furthermore, it is not possible to classify
by hand search results containing 1, 000 docu-
ments. Consequently, we did not evaluate our
method directly by comparing correct data with
the clustering result from our method, but instead
evaluated it indirectly by investigating the relia-
bility of the membership degree p(zld) used in
the document clustering process.
The evaluation process was as follows. First,
we presented the subjects with a document d,
which was estimated by our system to have a
high membership degree to a topic z. Strictly
speaking, we selected as d, a document with a
membership degree of about 0.9. Next, we pre-
sented two documents to the subjects. One was
a document d0 whose membership degree to z
was also about 0.9, and another was a document
d00 whose membership degree to z was about
0.1. Finally, we asked them which document was
more similar to d3.
Table 3 gives the results. According to this ta-
ble, 60.5% of the subjects said d0 was more simi-
lar or somewhat more similar. On the other hand,
only 12.6% of the subjects said d00 was more
similar or somewhat more similar. We see from
these results that the ability to recognize topics in
our system is in agreement to some extent with
3Naturally, we did not tell them that d&apos; had a similar
membership degree to d, whereas d&amp;quot; did not.
</bodyText>
<page confidence="0.999642">
17
</page>
<tableCaption confidence="0.999529">
Table 3: Precision of the estimation p(z|d).
</tableCaption>
<figure confidence="0.733122">
options # subjects ( % )
(a) d&apos; is definitely more similar 14 (29.2)
(b) d&apos; is somewhat more similar 15 (31.3)
(c) undecided 13 (27.1)
(d) d&apos;&apos; is somewhat more similar 3 ( 6.3)
(e) d&apos;&apos; is definitely more similar 3 ( 6.3)
</figure>
<bodyText confidence="0.9913284">
the subjects’ ability for recognizing topics; that
is, our method was able to estimate a reliable
membership degree p(z|d). Thus, it seems that
our method using p(z|d) is able to classify search
results into topics to some extent.
</bodyText>
<subsectionHeader confidence="0.9882595">
3.5 Degree of Reduction in Redundancy
between Summaries
</subsectionHeader>
<bodyText confidence="0.993953677419355">
Third, we investigated how well the proposed
method reduced the redundancy between sum-
maries. To be more precise, we used three mea-
sures as w score(z, w) to generate summaries
and investigated which measure generated the
least redundant summaries. Generally, meth-
ods for reducing redundancy are evaluated using
ROUGE (Lin, 2004), BE (Hovy et al., 2005),
or Pyramid (Nenkova and Passonneau, 2004).
However, the use of these methods require that
ideal summaries are created by humans, and this
was not possible for the same reason as men-
tioned previously. Thus, we did not perform
a direct evaluation using the methods such as
ROUGE, but instead evaluated how well our
method performed in reducing redundancy be-
tween summaries using the membership degree
p(z|w) as w score(z, w).
The evaluation process was as follows. We
used three measures as w score(z, w), and gen-
erated three sets of summaries.
Summaries A This set of summaries was gen-
erated using dfidf(w) as w score(z, w),
with dfidf(w) calculated as ldf(w) ×
log(100 million/gdf(w)), ldf(w) repre-
senting the document frequency of keyword
w in the search results, and gdf(w) rep-
resenting the document frequency of key-
word w in the TSUBAKI document col-
lection (Shinzato et al., 2008a) comprising
about 100 million documents.
</bodyText>
<tableCaption confidence="0.988088">
Table 4: Comparison of dfidf(w), p(w|z) and
</tableCaption>
<figure confidence="0.91465147826087">
p(z|w).
options # subjects ( % )
(a) B is definitely less redundant 5 (10.4)
(b) B is somewhat less redundant 16 (33.3)
(c) undecided 15 (31.3)
(d) A is somewhat less redundant 6 (12.5)
(e) A is definitely less redundant 6 (12.5)
options # subjects ( % )
(a) C is definitely less redundant 16 (33.3)
(b) C is somewhat less redundant 14 (29.2)
(c) undecided 6 (12.5)
(d) A is somewhat less redundant 8 (16.7)
(e) A is definitely less redundant 4 ( 8.3)
options # subjects ( % )
(a) C is definitely less redundant 15 (31.3)
(b) C is somewhat less redundant 8 (16.7)
(c) undecided 10 (20.8)
(d) B is somewhat less redundant 6 (12.5)
(e) B is definitely less redundant 9 (18.8)
Summaries B This set of summaries was gen-
erated using p(w|z) as w score(z, w).
Summaries C This set of summaries was gen-
erated using p(z|w) as w score(z, w).
</figure>
<bodyText confidence="0.997950826086956">
We then presented the subjects with three pairs
of summaries, namely a pair from A and B, a
pair from A and C, and a pair from B and C, and
asked them which summaries in each pair was
less redundant4.
The results are given in Tables 4. Firstly, ac-
cording to the comparison of A and B and the
comparison of A and C, A was more redundant
than B and C. The value of dfidf(w) to key-
word w was the same for all topics. Thus, us-
ing dfidf(w) as w score(z, w) made summaries
redundant, as each summary tended to contain
the same keywords with high dfidf(w). On the
other hand, as the value of p(w|z) and p(z|w)
were dependent on the topic, the summaries gen-
erated using these measures were less redundant.
Second, the comparison of B and C shows that
48.0% of the subjects considered C to be less
redundant or somewhat less redundant. p(w|z)
was a better measure than dfidf(w), but even us-
ing p(w|z) generated redundancy between sum-
4Naturally, we did not tell them which summaries were
generated using which measures
</bodyText>
<page confidence="0.997047">
18
</page>
<bodyText confidence="0.9997206">
maries. Because common keywords to a query
have high p(w1z) for several topics, sentences
containing these keywords were extracted as the
important sentences for those topics, and thus
the summaries were similar to one another. On
the other hand, the keywords’ value for p(zlw)
was low, allowing us to extract the important
sentences specific to each topic using p(zlw) as
w score(z, w), thereby reducing the redundancy
between summaries.
</bodyText>
<subsectionHeader confidence="0.892752333333333">
3.6 Effectiveness of the Method for
Presenting Information Using
Summaries
</subsectionHeader>
<bodyText confidence="0.999988631578947">
We also investigated the effectiveness of the
method for presenting information through sum-
maries. We asked the subjects to compare two
different ways of presenting information and to
judge which way was more effective in terms
of usefulness for collecting information about
a query. One of the methods presented the
search results with topic summaries generated by
our system (method X), and while the another
method presented the search results with the key-
words included in each topic summary (method
Y).
Table 5 gives the results. 72.9% of the sub-
jects considered the method using summaries to
be more effective or somewhat more effective.
From these results, it appears that the method of
presenting information through summaries is ef-
fective in terms of usefulness for collecting in-
formation about a query.
</bodyText>
<sectionHeader confidence="0.999428" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999978483870968">
In this paper, we focused on the task of gen-
erating a set of query-focused summaries from
search results. To summarize the search results
for a given query, a process of classifying them
into topics related to the query was needed. In
the proposed method, we employed PLSI to es-
timate the membership degree of each document
to each topic, and then classified search results
into topics using this metric. The evaluation re-
sults showed that our method estimated reliable
degrees of membership. Thus, it seems that our
method is able to some extent to classify search
results into topics.
In the summarization process, redundancy
within a summary and redundancy between sum-
maries needs to be reduced. In this paper, we fo-
cused on the reduction of the latter redundancy.
Our method made use of PLSI to estimate the
membership degree of each keyword to each
topic, and then extracted the important sentences
specific to each topic using this metric. The eval-
uation results showed that our method was able
to reduce the redundancy between summaries us-
ing the membership degree.
In future, we will investigate the use of
more sophisticated topic models. Although our
method detected the topics related to a query us-
ing a simple topic model (i.e., PLSI), we be-
lieve that more sophisticated topic models such
as LDA (Blei et al., 2003) allow us to improve
our method.
</bodyText>
<sectionHeader confidence="0.998979" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999362421052632">
Akaike, Hirotugu. 1974. A new look at the statis-
tical model identification. IEEE Transactions on
Automation Control, 19(6):716–723.
Blei, David M., Andrew Y. Ng, and Michael I. Jor-
dan. 2003. Latent Dirichlet Allocation. Journal
of Machine Learning Research, 3:993–1022.
Filatova, Elena and Vasileios Hatzivassiloglou. 2004.
A formal model for information selection in multi-
sentence text extraction. In Proceedings of COL-
ING 2004, pages 397–403.
Goldstein, Jade, Vibhu Mittal, Jaime Carbonell, and
Mark Kantrowitz. 2000. Multi-document summa-
rization by sentence extraction. In Proceedings of
ANLP/NAACL 2000 Workshop on Automatic Sum-
marization, pages 40–48.
Haghighi, Aria and Lucy Vanderwende. 2009. Ex-
ploring content models for multi-document sum-
marization. In Proceedings of HLT-NAACL 2009,
pages 362–370.
</reference>
<figure confidence="0.992242">
(c) undecided
3 ( 6.3)
</figure>
<tableCaption confidence="0.49659">
Table 5: Comparison of summaries and keywords.
</tableCaption>
<figure confidence="0.992080222222222">
options # subjects ( % )
(a) X is definitely more helpful
(b) X is somewhat more helpful
25 (52.1)
10 (20.8)
(d) Y is somewhat more helpful
(e) Y is definitely more helpful
8 (16.7)
2 ( 4.2)
</figure>
<page confidence="0.978072">
19
</page>
<reference confidence="0.985001354166667">
Hofmann, Thomas. 1999. Probabilistic latent se-
mantic indexing. In Proceedings of SIGIR 1999,
pages 50–57.
Hovy, Eduard, Chin-Yew Lin, and Liang Zhou. 2005.
Evaluating duc 2005 using basic elements. In Pro-
ceedings of DUC 2005.
Lin, Chin-Yew. 2004. Rouge: A package for au-
tomatic evaluation of summaries. In Proceedings
of ACL 2004 Workshop on Text Summarization
Branches Out, pages 74–81.
Mani, Inderjeet. 2001. Automatic Summarization.
John Benjamins Publishing Company.
McDonald, Ryan. 2007. A study of global inference
algorithms in multi-document summarization. In
Proceedings of ECIR 2007, pages 557–564.
Mori, Tatsunori, Masanori Nozawa, and Yoshi-
aki Asada. 2004. Multi-answer-focused
multi-document summarization using a question-
answering engine. In Proceedings of COLING
2004, pages 439–445.
Nenkova, Ani and Rebecca Passonneau. 2004. Eval-
uating content selection in summarization: The
pyramid method. In Proceedings of NAACL-HLT
2004.
Radev, Dragomir R. and Weiguo Fan. 2000. Auto-
matic summarization of search engine hit lists. In
Proceedings ofACL 2000 Workshop on Recent ad-
vances in NLP and IR, pages 1361–1374.
Shibata, Tomohide, Yasuo Bamba, Keiji Shinzato,
and Sadao Kurohashi. 2009. Web information or-
ganization using keyword distillation based clus-
tering. In Proceedings of WI 2009, pages 325–
330.
Shinzato, Keiji, Daisuke Kawahara, Chikara
Hashimoto, and Sadao Kurohashi. 2008a. A
large-scale web data collection as a natural lan-
guage processing infrastructure. In Proceedings
of LREC 2008, pages 2236–2241.
Shinzato, Keiji, Tomohide Shibata, Daisuke Kawa-
hara, Chikara Hashimoto, and Sadao Kurohashi.
2008b. TSUBAKI: An Open Search Engine In-
frastructure for Developing New Information Ac-
cess Methodology. In Proceedings of IJCNLP
2008, pages 189–196.
Takamura, Hiroya and Manabu Okumura. 2009.
Text summarization model based on maximum
coverage problem and its variant. In Proceedings
of EACL 2009, pages 781–789.
</reference>
<page confidence="0.994188">
20
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.162245">
<title confidence="0.894537">Summarizing Search Results using PLSI and Sadao Graduate School of</title>
<author confidence="0.2244">Kyoto</author>
<affiliation confidence="0.420474">Yoshida-honmachi,</affiliation>
<address confidence="0.668845">Kyoto, 606-8501,</address>
<abstract confidence="0.999167111111111">In this paper, we investigate generating a set of query-focused summaries from search results. Since there may be many topics related to a given query in the search results, in order to summarize these results, they should first be classified into topics, and then each topic should be summarized individually. In this summarization process, two types of redundancies need to be reduced. First, each topic summary should not contain any redundancy (we refer to this problem as redundancy within a summary). Second, a topic summary should not be similar to any other topic summary (we refer to this problem as redundancy between summaries). In this paper, we focus on the document clustering process and the reduction of redundancy between summaries in the summarization process. We also propose a method using PLSI to summarize search results. Evaluation results confirm that our method performs well in classifying search results and reducing the redundancy between summaries.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Hirotugu Akaike</author>
</authors>
<title>A new look at the statistical model identification.</title>
<date>1974</date>
<journal>IEEE Transactions on Automation Control,</journal>
<volume>19</volume>
<issue>6</issue>
<contexts>
<context position="8901" citStr="Akaike, 1974" startWordPosition="1479" endWordPosition="1480">6) ∑d ∑w, freq(d, w0) p(z|d, w0) The EM algorithm iterates through these steps until convergence. First, we give PLSI the number of topics K, the search results D, and the keywords W as input, and estimate p(z), p(d|z), and p(w|z), where z is a topic related to the query, d is a document in D, and w is a keyword in W. There is, however, no way of knowing the value of K; that is, we do not know in advance how many topics related to the query there are in the search results. Hence, we perform PLSI for several values of K, and select the K that has the minimum Akaike Information Criterion (AIC) (Akaike, 1974), calculated as follows. AIC = −2L + 2K(N + M) (7) Furthermore, we select p(z), p(d|z), and p(w|z) estimated using the selected K as the result of PLSI. Next, we calculate the membership degree of each document to each topic. The membership degree of document d to topic z, denoted p(z|d), is calculated as p(z|d) = p(d|z) p(z) ∑ z, p(d|z0)� (8) Finally, for each topic, we collect those documents whose membership degree to the topic is larger than the threshold α. If there is a document whose membership degree to multiple topics is larger than the threshold, we classify the document into each to</context>
</contexts>
<marker>Akaike, 1974</marker>
<rawString>Akaike, Hirotugu. 1974. A new look at the statistical model identification. IEEE Transactions on Automation Control, 19(6):716–723.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>Blei, David M., Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Filatova</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>A formal model for information selection in multisentence text extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING</booktitle>
<pages>397--403</pages>
<contexts>
<context position="3679" citStr="Filatova and Hatzivassiloglou, 2004" startWordPosition="572" endWordPosition="576">c/ 12 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 12–20, Beijing, August 2010 In the summarization process, two types of redundancies need to be addressed. First, each topic summary should not contain any redundancy. We refer to this problem as redundancy within a summary. This problem is well known in the field of multi-document summarization (Mani, 2001) and several methods have been proposed to solve it, such as Maximum Marginal Relevance (MMR) (Goldstein et al., 2000) (Mori et al., 2004), using Integer Linear Programming (ILP) (Filatova and Hatzivassiloglou, 2004) (McDonald, 2007) (Takamura and Okumura, 2009), and so on. Second, no topic summary should be similar to any of the other topic summaries. We refer to this problem as redundancy between summaries. For example, to summarize the abovementioned documents related to swine flu, the summary for outbreaks should contain specific information about outbreaks, whereas the summary for measures should contain specific information about measures. This problem is characteristic of multi-topic multi-document summarization. Some methods have been proposed to generate topic summaries from documents (Radev and </context>
</contexts>
<marker>Filatova, Hatzivassiloglou, 2004</marker>
<rawString>Filatova, Elena and Vasileios Hatzivassiloglou. 2004. A formal model for information selection in multisentence text extraction. In Proceedings of COLING 2004, pages 397–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jade Goldstein</author>
<author>Vibhu Mittal</author>
<author>Jaime Carbonell</author>
<author>Mark Kantrowitz</author>
</authors>
<title>Multi-document summarization by sentence extraction.</title>
<date>2000</date>
<booktitle>In Proceedings of ANLP/NAACL 2000 Workshop on Automatic Summarization,</booktitle>
<pages>40--48</pages>
<contexts>
<context position="3581" citStr="Goldstein et al., 2000" startWordPosition="558" endWordPosition="561">ne document may belong to several topics. 1http://clusty.com/ 2http://www.nist.gov/tac/ 12 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 12–20, Beijing, August 2010 In the summarization process, two types of redundancies need to be addressed. First, each topic summary should not contain any redundancy. We refer to this problem as redundancy within a summary. This problem is well known in the field of multi-document summarization (Mani, 2001) and several methods have been proposed to solve it, such as Maximum Marginal Relevance (MMR) (Goldstein et al., 2000) (Mori et al., 2004), using Integer Linear Programming (ILP) (Filatova and Hatzivassiloglou, 2004) (McDonald, 2007) (Takamura and Okumura, 2009), and so on. Second, no topic summary should be similar to any of the other topic summaries. We refer to this problem as redundancy between summaries. For example, to summarize the abovementioned documents related to swine flu, the summary for outbreaks should contain specific information about outbreaks, whereas the summary for measures should contain specific information about measures. This problem is characteristic of multi-topic multi-document sum</context>
</contexts>
<marker>Goldstein, Mittal, Carbonell, Kantrowitz, 2000</marker>
<rawString>Goldstein, Jade, Vibhu Mittal, Jaime Carbonell, and Mark Kantrowitz. 2000. Multi-document summarization by sentence extraction. In Proceedings of ANLP/NAACL 2000 Workshop on Automatic Summarization, pages 40–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Exploring content models for multi-document summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<pages>362--370</pages>
<contexts>
<context position="4322" citStr="Haghighi and Vanderwende, 2009" startWordPosition="671" endWordPosition="675">2007) (Takamura and Okumura, 2009), and so on. Second, no topic summary should be similar to any of the other topic summaries. We refer to this problem as redundancy between summaries. For example, to summarize the abovementioned documents related to swine flu, the summary for outbreaks should contain specific information about outbreaks, whereas the summary for measures should contain specific information about measures. This problem is characteristic of multi-topic multi-document summarization. Some methods have been proposed to generate topic summaries from documents (Radev and Fan, 2000) (Haghighi and Vanderwende, 2009), but to the best of our knowledge, the redundancy between summaries has not yet been addressed in any study. In this paper, we focus on the document clustering process and the reduction of redundancy between summaries in the summarization process. Furthermore, we propose a method using PLSI (Hofmann, 1999) to summarize search results. In the proposed method, we employ PLSI to estimate the membership degree of each document to each topic, and then classify the search results into topics using this information. In the same way, we employ PLSI to estimate the membership degree of each keyword to</context>
</contexts>
<marker>Haghighi, Vanderwende, 2009</marker>
<rawString>Haghighi, Aria and Lucy Vanderwende. 2009. Exploring content models for multi-document summarization. In Proceedings of HLT-NAACL 2009, pages 362–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Probabilistic latent semantic indexing.</title>
<date>1999</date>
<booktitle>In Proceedings of SIGIR</booktitle>
<pages>50--57</pages>
<contexts>
<context position="4630" citStr="Hofmann, 1999" startWordPosition="725" endWordPosition="726">bout outbreaks, whereas the summary for measures should contain specific information about measures. This problem is characteristic of multi-topic multi-document summarization. Some methods have been proposed to generate topic summaries from documents (Radev and Fan, 2000) (Haghighi and Vanderwende, 2009), but to the best of our knowledge, the redundancy between summaries has not yet been addressed in any study. In this paper, we focus on the document clustering process and the reduction of redundancy between summaries in the summarization process. Furthermore, we propose a method using PLSI (Hofmann, 1999) to summarize search results. In the proposed method, we employ PLSI to estimate the membership degree of each document to each topic, and then classify the search results into topics using this information. In the same way, we employ PLSI to estimate the membership degree of each keyword to each topic, and then extract the important sentences specific to each topic using this information in order to reduce the redundancy between summaries. The evaluation results show that our method performs well in classifying search results and successfully reduces the redundancy between summaries. Figure 1</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Hofmann, Thomas. 1999. Probabilistic latent semantic indexing. In Proceedings of SIGIR 1999, pages 50–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Chin-Yew Lin</author>
<author>Liang Zhou</author>
</authors>
<title>Evaluating duc</title>
<date>2005</date>
<booktitle>In Proceedings of DUC</booktitle>
<contexts>
<context position="20958" citStr="Hovy et al., 2005" startWordPosition="3666" endWordPosition="3669"> for recognizing topics; that is, our method was able to estimate a reliable membership degree p(z|d). Thus, it seems that our method using p(z|d) is able to classify search results into topics to some extent. 3.5 Degree of Reduction in Redundancy between Summaries Third, we investigated how well the proposed method reduced the redundancy between summaries. To be more precise, we used three measures as w score(z, w) to generate summaries and investigated which measure generated the least redundant summaries. Generally, methods for reducing redundancy are evaluated using ROUGE (Lin, 2004), BE (Hovy et al., 2005), or Pyramid (Nenkova and Passonneau, 2004). However, the use of these methods require that ideal summaries are created by humans, and this was not possible for the same reason as mentioned previously. Thus, we did not perform a direct evaluation using the methods such as ROUGE, but instead evaluated how well our method performed in reducing redundancy between summaries using the membership degree p(z|w) as w score(z, w). The evaluation process was as follows. We used three measures as w score(z, w), and generated three sets of summaries. Summaries A This set of summaries was generated using d</context>
</contexts>
<marker>Hovy, Lin, Zhou, 2005</marker>
<rawString>Hovy, Eduard, Chin-Yew Lin, and Liang Zhou. 2005. Evaluating duc 2005 using basic elements. In Proceedings of DUC 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL 2004 Workshop on Text Summarization Branches Out,</booktitle>
<pages>74--81</pages>
<contexts>
<context position="20934" citStr="Lin, 2004" startWordPosition="3663" endWordPosition="3664">ubjects’ ability for recognizing topics; that is, our method was able to estimate a reliable membership degree p(z|d). Thus, it seems that our method using p(z|d) is able to classify search results into topics to some extent. 3.5 Degree of Reduction in Redundancy between Summaries Third, we investigated how well the proposed method reduced the redundancy between summaries. To be more precise, we used three measures as w score(z, w) to generate summaries and investigated which measure generated the least redundant summaries. Generally, methods for reducing redundancy are evaluated using ROUGE (Lin, 2004), BE (Hovy et al., 2005), or Pyramid (Nenkova and Passonneau, 2004). However, the use of these methods require that ideal summaries are created by humans, and this was not possible for the same reason as mentioned previously. Thus, we did not perform a direct evaluation using the methods such as ROUGE, but instead evaluated how well our method performed in reducing redundancy between summaries using the membership degree p(z|w) as w score(z, w). The evaluation process was as follows. We used three measures as w score(z, w), and generated three sets of summaries. Summaries A This set of summari</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Lin, Chin-Yew. 2004. Rouge: A package for automatic evaluation of summaries. In Proceedings of ACL 2004 Workshop on Text Summarization Branches Out, pages 74–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
</authors>
<title>Automatic Summarization.</title>
<date>2001</date>
<publisher>John Benjamins Publishing Company.</publisher>
<contexts>
<context position="3463" citStr="Mani, 2001" startWordPosition="541" endWordPosition="542">d summarized individually. Note that a method for soft clustering should be employed in this process, as one document may belong to several topics. 1http://clusty.com/ 2http://www.nist.gov/tac/ 12 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 12–20, Beijing, August 2010 In the summarization process, two types of redundancies need to be addressed. First, each topic summary should not contain any redundancy. We refer to this problem as redundancy within a summary. This problem is well known in the field of multi-document summarization (Mani, 2001) and several methods have been proposed to solve it, such as Maximum Marginal Relevance (MMR) (Goldstein et al., 2000) (Mori et al., 2004), using Integer Linear Programming (ILP) (Filatova and Hatzivassiloglou, 2004) (McDonald, 2007) (Takamura and Okumura, 2009), and so on. Second, no topic summary should be similar to any of the other topic summaries. We refer to this problem as redundancy between summaries. For example, to summarize the abovementioned documents related to swine flu, the summary for outbreaks should contain specific information about outbreaks, whereas the summary for measure</context>
</contexts>
<marker>Mani, 2001</marker>
<rawString>Mani, Inderjeet. 2001. Automatic Summarization. John Benjamins Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>A study of global inference algorithms in multi-document summarization.</title>
<date>2007</date>
<booktitle>In Proceedings of ECIR</booktitle>
<pages>557--564</pages>
<contexts>
<context position="3696" citStr="McDonald, 2007" startWordPosition="577" endWordPosition="578">op on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 12–20, Beijing, August 2010 In the summarization process, two types of redundancies need to be addressed. First, each topic summary should not contain any redundancy. We refer to this problem as redundancy within a summary. This problem is well known in the field of multi-document summarization (Mani, 2001) and several methods have been proposed to solve it, such as Maximum Marginal Relevance (MMR) (Goldstein et al., 2000) (Mori et al., 2004), using Integer Linear Programming (ILP) (Filatova and Hatzivassiloglou, 2004) (McDonald, 2007) (Takamura and Okumura, 2009), and so on. Second, no topic summary should be similar to any of the other topic summaries. We refer to this problem as redundancy between summaries. For example, to summarize the abovementioned documents related to swine flu, the summary for outbreaks should contain specific information about outbreaks, whereas the summary for measures should contain specific information about measures. This problem is characteristic of multi-topic multi-document summarization. Some methods have been proposed to generate topic summaries from documents (Radev and Fan, 2000) (Haghi</context>
</contexts>
<marker>McDonald, 2007</marker>
<rawString>McDonald, Ryan. 2007. A study of global inference algorithms in multi-document summarization. In Proceedings of ECIR 2007, pages 557–564.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tatsunori Mori</author>
<author>Masanori Nozawa</author>
<author>Yoshiaki Asada</author>
</authors>
<title>Multi-answer-focused multi-document summarization using a questionanswering engine.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING</booktitle>
<pages>439--445</pages>
<contexts>
<context position="3601" citStr="Mori et al., 2004" startWordPosition="562" endWordPosition="565"> several topics. 1http://clusty.com/ 2http://www.nist.gov/tac/ 12 Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 12–20, Beijing, August 2010 In the summarization process, two types of redundancies need to be addressed. First, each topic summary should not contain any redundancy. We refer to this problem as redundancy within a summary. This problem is well known in the field of multi-document summarization (Mani, 2001) and several methods have been proposed to solve it, such as Maximum Marginal Relevance (MMR) (Goldstein et al., 2000) (Mori et al., 2004), using Integer Linear Programming (ILP) (Filatova and Hatzivassiloglou, 2004) (McDonald, 2007) (Takamura and Okumura, 2009), and so on. Second, no topic summary should be similar to any of the other topic summaries. We refer to this problem as redundancy between summaries. For example, to summarize the abovementioned documents related to swine flu, the summary for outbreaks should contain specific information about outbreaks, whereas the summary for measures should contain specific information about measures. This problem is characteristic of multi-topic multi-document summarization. Some met</context>
</contexts>
<marker>Mori, Nozawa, Asada, 2004</marker>
<rawString>Mori, Tatsunori, Masanori Nozawa, and Yoshiaki Asada. 2004. Multi-answer-focused multi-document summarization using a questionanswering engine. In Proceedings of COLING 2004, pages 439–445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Rebecca Passonneau</author>
</authors>
<title>Evaluating content selection in summarization: The pyramid method.</title>
<date>2004</date>
<booktitle>In Proceedings of NAACL-HLT</booktitle>
<contexts>
<context position="21001" citStr="Nenkova and Passonneau, 2004" startWordPosition="3672" endWordPosition="3675">, our method was able to estimate a reliable membership degree p(z|d). Thus, it seems that our method using p(z|d) is able to classify search results into topics to some extent. 3.5 Degree of Reduction in Redundancy between Summaries Third, we investigated how well the proposed method reduced the redundancy between summaries. To be more precise, we used three measures as w score(z, w) to generate summaries and investigated which measure generated the least redundant summaries. Generally, methods for reducing redundancy are evaluated using ROUGE (Lin, 2004), BE (Hovy et al., 2005), or Pyramid (Nenkova and Passonneau, 2004). However, the use of these methods require that ideal summaries are created by humans, and this was not possible for the same reason as mentioned previously. Thus, we did not perform a direct evaluation using the methods such as ROUGE, but instead evaluated how well our method performed in reducing redundancy between summaries using the membership degree p(z|w) as w score(z, w). The evaluation process was as follows. We used three measures as w score(z, w), and generated three sets of summaries. Summaries A This set of summaries was generated using dfidf(w) as w score(z, w), with dfidf(w) cal</context>
</contexts>
<marker>Nenkova, Passonneau, 2004</marker>
<rawString>Nenkova, Ani and Rebecca Passonneau. 2004. Evaluating content selection in summarization: The pyramid method. In Proceedings of NAACL-HLT 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Weiguo Fan</author>
</authors>
<title>Automatic summarization of search engine hit lists.</title>
<date>2000</date>
<booktitle>In Proceedings ofACL 2000 Workshop on Recent advances in NLP and IR,</booktitle>
<pages>1361--1374</pages>
<contexts>
<context position="4289" citStr="Radev and Fan, 2000" startWordPosition="667" endWordPosition="670">lou, 2004) (McDonald, 2007) (Takamura and Okumura, 2009), and so on. Second, no topic summary should be similar to any of the other topic summaries. We refer to this problem as redundancy between summaries. For example, to summarize the abovementioned documents related to swine flu, the summary for outbreaks should contain specific information about outbreaks, whereas the summary for measures should contain specific information about measures. This problem is characteristic of multi-topic multi-document summarization. Some methods have been proposed to generate topic summaries from documents (Radev and Fan, 2000) (Haghighi and Vanderwende, 2009), but to the best of our knowledge, the redundancy between summaries has not yet been addressed in any study. In this paper, we focus on the document clustering process and the reduction of redundancy between summaries in the summarization process. Furthermore, we propose a method using PLSI (Hofmann, 1999) to summarize search results. In the proposed method, we employ PLSI to estimate the membership degree of each document to each topic, and then classify the search results into topics using this information. In the same way, we employ PLSI to estimate the mem</context>
</contexts>
<marker>Radev, Fan, 2000</marker>
<rawString>Radev, Dragomir R. and Weiguo Fan. 2000. Automatic summarization of search engine hit lists. In Proceedings ofACL 2000 Workshop on Recent advances in NLP and IR, pages 1361–1374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomohide Shibata</author>
<author>Yasuo Bamba</author>
<author>Keiji Shinzato</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Web information organization using keyword distillation based clustering.</title>
<date>2009</date>
<booktitle>In Proceedings of WI 2009,</booktitle>
<pages>325--330</pages>
<contexts>
<context position="5638" citStr="Shibata et al. (2009)" startWordPosition="892" endWordPosition="896">tion in order to reduce the redundancy between summaries. The evaluation results show that our method performs well in classifying search results and successfully reduces the redundancy between summaries. Figure 1: Overview of the proposed method. 2 Proposed Method 2.1 Overview Figure 1 gives an overview of the proposed method, which comprises the following four steps. Step 1. Acquisition of Search Results Using a search engine, obtain the search results for a given query. Step 2. Keyword Extraction Extract the keywords related to the query from the search results using the method proposed by Shibata et al. (2009). Step 3. Document Clustering Estimate the membership degree of each document to each topic using PLSI, and classify the search results into topics. Step 4. Summarization For each topic, generate a summary by extracting the important sentences specific to each topic from each document cluster. In the following subsections, we describe each step in detail. 2.2 Step 1. Acquisition of Search Results First, we obtain the search results for a given query using a search engine. To be more precise, we obtain the top N&apos; documents of the search engine results. Next, we remove those documents that shoul</context>
</contexts>
<marker>Shibata, Bamba, Shinzato, Kurohashi, 2009</marker>
<rawString>Shibata, Tomohide, Yasuo Bamba, Keiji Shinzato, and Sadao Kurohashi. 2009. Web information organization using keyword distillation based clustering. In Proceedings of WI 2009, pages 325– 330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keiji Shinzato</author>
<author>Daisuke Kawahara</author>
<author>Chikara Hashimoto</author>
<author>Sadao Kurohashi</author>
</authors>
<title>A large-scale web data collection as a natural language processing infrastructure.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC</booktitle>
<pages>2236--2241</pages>
<contexts>
<context position="14958" citStr="Shinzato et al., 2008" startWordPosition="2601" endWordPosition="2604">ummaries We allowed the subjects to create arbitrary queries for our system. 3.2 System Figure 3 shows the system results for the query swine flu. Our system presents a separate summary for each topic related to a given query. In Fig.3, the colored words in the summaries are keywords specific to each topic. If a user clicks on a keyword, the system presents a list of documents containing that keyword at the bottom of the browser. The configuration of our system was as follows. In the acquisition process, the system obtained the search results for a given query using the search engine TSUBAKI (Shinzato et al., 2008b). Setting N&apos; = 1, 000, we obtained the top 1, 000 documents in the search results for the query. In the keyword extraction process, we set M = 100, and extracted 100 keywords related to the query from the search results. In the document clustering process, we performed PLSI for K = 3, 4, 5, and selected the K with the minimum AIC. We set the initial value of p(z) = 1/K, and the initial values of p(d|z) and p(w|z) to random values. The EM algorithm continued until the increase in L reached just below 1 to achieve convergence. We set α = 1/K. In the summarization process, we set I = 10, since </context>
<context position="21842" citStr="Shinzato et al., 2008" startWordPosition="3814" endWordPosition="3817">s such as ROUGE, but instead evaluated how well our method performed in reducing redundancy between summaries using the membership degree p(z|w) as w score(z, w). The evaluation process was as follows. We used three measures as w score(z, w), and generated three sets of summaries. Summaries A This set of summaries was generated using dfidf(w) as w score(z, w), with dfidf(w) calculated as ldf(w) × log(100 million/gdf(w)), ldf(w) representing the document frequency of keyword w in the search results, and gdf(w) representing the document frequency of keyword w in the TSUBAKI document collection (Shinzato et al., 2008a) comprising about 100 million documents. Table 4: Comparison of dfidf(w), p(w|z) and p(z|w). options # subjects ( % ) (a) B is definitely less redundant 5 (10.4) (b) B is somewhat less redundant 16 (33.3) (c) undecided 15 (31.3) (d) A is somewhat less redundant 6 (12.5) (e) A is definitely less redundant 6 (12.5) options # subjects ( % ) (a) C is definitely less redundant 16 (33.3) (b) C is somewhat less redundant 14 (29.2) (c) undecided 6 (12.5) (d) A is somewhat less redundant 8 (16.7) (e) A is definitely less redundant 4 ( 8.3) options # subjects ( % ) (a) C is definitely less redundant 1</context>
</contexts>
<marker>Shinzato, Kawahara, Hashimoto, Kurohashi, 2008</marker>
<rawString>Shinzato, Keiji, Daisuke Kawahara, Chikara Hashimoto, and Sadao Kurohashi. 2008a. A large-scale web data collection as a natural language processing infrastructure. In Proceedings of LREC 2008, pages 2236–2241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keiji Shinzato</author>
<author>Tomohide Shibata</author>
<author>Daisuke Kawahara</author>
<author>Chikara Hashimoto</author>
<author>Sadao Kurohashi</author>
</authors>
<title>TSUBAKI: An Open Search Engine Infrastructure for Developing New Information Access Methodology.</title>
<date>2008</date>
<booktitle>In Proceedings of IJCNLP</booktitle>
<pages>189--196</pages>
<contexts>
<context position="14958" citStr="Shinzato et al., 2008" startWordPosition="2601" endWordPosition="2604">ummaries We allowed the subjects to create arbitrary queries for our system. 3.2 System Figure 3 shows the system results for the query swine flu. Our system presents a separate summary for each topic related to a given query. In Fig.3, the colored words in the summaries are keywords specific to each topic. If a user clicks on a keyword, the system presents a list of documents containing that keyword at the bottom of the browser. The configuration of our system was as follows. In the acquisition process, the system obtained the search results for a given query using the search engine TSUBAKI (Shinzato et al., 2008b). Setting N&apos; = 1, 000, we obtained the top 1, 000 documents in the search results for the query. In the keyword extraction process, we set M = 100, and extracted 100 keywords related to the query from the search results. In the document clustering process, we performed PLSI for K = 3, 4, 5, and selected the K with the minimum AIC. We set the initial value of p(z) = 1/K, and the initial values of p(d|z) and p(w|z) to random values. The EM algorithm continued until the increase in L reached just below 1 to achieve convergence. We set α = 1/K. In the summarization process, we set I = 10, since </context>
<context position="21842" citStr="Shinzato et al., 2008" startWordPosition="3814" endWordPosition="3817">s such as ROUGE, but instead evaluated how well our method performed in reducing redundancy between summaries using the membership degree p(z|w) as w score(z, w). The evaluation process was as follows. We used three measures as w score(z, w), and generated three sets of summaries. Summaries A This set of summaries was generated using dfidf(w) as w score(z, w), with dfidf(w) calculated as ldf(w) × log(100 million/gdf(w)), ldf(w) representing the document frequency of keyword w in the search results, and gdf(w) representing the document frequency of keyword w in the TSUBAKI document collection (Shinzato et al., 2008a) comprising about 100 million documents. Table 4: Comparison of dfidf(w), p(w|z) and p(z|w). options # subjects ( % ) (a) B is definitely less redundant 5 (10.4) (b) B is somewhat less redundant 16 (33.3) (c) undecided 15 (31.3) (d) A is somewhat less redundant 6 (12.5) (e) A is definitely less redundant 6 (12.5) options # subjects ( % ) (a) C is definitely less redundant 16 (33.3) (b) C is somewhat less redundant 14 (29.2) (c) undecided 6 (12.5) (d) A is somewhat less redundant 8 (16.7) (e) A is definitely less redundant 4 ( 8.3) options # subjects ( % ) (a) C is definitely less redundant 1</context>
</contexts>
<marker>Shinzato, Shibata, Kawahara, Hashimoto, Kurohashi, 2008</marker>
<rawString>Shinzato, Keiji, Tomohide Shibata, Daisuke Kawahara, Chikara Hashimoto, and Sadao Kurohashi. 2008b. TSUBAKI: An Open Search Engine Infrastructure for Developing New Information Access Methodology. In Proceedings of IJCNLP 2008, pages 189–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroya Takamura</author>
<author>Manabu Okumura</author>
</authors>
<title>Text summarization model based on maximum coverage problem and its variant.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL</booktitle>
<pages>781--789</pages>
<contexts>
<context position="3725" citStr="Takamura and Okumura, 2009" startWordPosition="579" endWordPosition="582">ges in the Information Explosion Era (NLPIX 2010), pages 12–20, Beijing, August 2010 In the summarization process, two types of redundancies need to be addressed. First, each topic summary should not contain any redundancy. We refer to this problem as redundancy within a summary. This problem is well known in the field of multi-document summarization (Mani, 2001) and several methods have been proposed to solve it, such as Maximum Marginal Relevance (MMR) (Goldstein et al., 2000) (Mori et al., 2004), using Integer Linear Programming (ILP) (Filatova and Hatzivassiloglou, 2004) (McDonald, 2007) (Takamura and Okumura, 2009), and so on. Second, no topic summary should be similar to any of the other topic summaries. We refer to this problem as redundancy between summaries. For example, to summarize the abovementioned documents related to swine flu, the summary for outbreaks should contain specific information about outbreaks, whereas the summary for measures should contain specific information about measures. This problem is characteristic of multi-topic multi-document summarization. Some methods have been proposed to generate topic summaries from documents (Radev and Fan, 2000) (Haghighi and Vanderwende, 2009), b</context>
</contexts>
<marker>Takamura, Okumura, 2009</marker>
<rawString>Takamura, Hiroya and Manabu Okumura. 2009. Text summarization model based on maximum coverage problem and its variant. In Proceedings of EACL 2009, pages 781–789.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>