<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002861">
<title confidence="0.990416">
Better Punctuation Prediction with Dynamic Conditional Random Fields
</title>
<author confidence="0.998943">
Wei Lu and Hwee Tou Ng
</author>
<affiliation confidence="0.9999">
Department of Computer Science
National University of Singapore
</affiliation>
<address confidence="0.846886">
13 Computing Drive, Singapore 117417
</address>
<email confidence="0.999214">
{luwei,nght}@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.997393" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999845882352941">
This paper focuses on the task of insert-
ing punctuation symbols into transcribed con-
versational speech texts, without relying on
prosodic cues. We investigate limitations as-
sociated with previous methods, and propose a
novel approach based on dynamic conditional
random fields. Different from previous work,
our proposed approach is designed to jointly
perform both sentence boundary and sentence
type prediction, and punctuation prediction on
speech utterances.
We performed evaluations on a transcribed
conversational speech domain consisting of
both English and Chinese texts. Empirical re-
sults show that our method outperforms an ap-
proach based on linear-chain conditional ran-
dom fields and other previous approaches.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99963696">
Outputs of standard automatic speech recognition
(ASR) systems typically consist of utterances where
important linguistic and structural information (e.g.,
true case, sentence boundaries, punctuation sym-
bols, etc) is not available. Such information is cru-
cial in improving the readability of the transcribed
speech texts, and plays an important role when fur-
ther processing is required, such as in part-of-speech
(POS) tagging, parsing, information extraction, and
machine translation.
We focus on the punctuation prediction task in
this work. Most previous punctuation prediction
techniques, developed mostly by the speech process-
ing community, exploit both lexical and prosodic
cues. However, in order to fully exploit prosodic fea-
tures such as pitch and pause duration, it is necessary
to have access to the original raw speech waveforms.
In some scenarios where further natural language
processing (NLP) tasks on the transcribed speech
texts become the main concern, speech prosody in-
formation may not be readily available. For exam-
ple, in the recent evaluation campaign of the Inter-
national Workshop on Spoken Language Translation
(IWSLT) (Paul, 2009), only manually transcribed or
automatically recognized speech texts are provided
but the original raw speech waveforms are not avail-
able.
In this paper, we tackle the task of predicting
punctuation symbols from a standard text processing
perspective, where only the speech texts are avail-
able, without relying on additional prosodic fea-
tures such as pitch and pause duration. Specifi-
cally, we perform the punctuation prediction task
on transcribed conversational speech texts, using the
IWSLT corpus (Paul, 2009) as the evaluation data.
Different from many other corpora such as broad-
cast news corpora, a conversational speech corpus
consists of dialogs where informal and short sen-
tences frequently appear. In addition, due to the
nature of conversation, it also contains more ques-
tion sentences compared to other corpora. An ex-
ample English utterance randomly selected from the
IWSLT corpus, along with its punctuated and cased
version, are shown below:
you are quite welcome and by the way we may get
other reservations so could you please call us
as soon as you fix the date
You are quite welcome. And by the way, we may
get other reservations, so could you please call
us as soon as you fix the date ?
</bodyText>
<page confidence="0.965605">
177
</page>
<note confidence="0.817671">
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 177–186,
MIT, Massachusetts, USA, 9-11 October 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.99947425">
The rest of this paper is organized as follows.
We start with surveying related work in Section 2.
One class of widely-used previous techniques is then
studied in detail in Section 3. Next, we investigate
methods for improving existing methods in Section
4 and 5. Empirical evaluation results are presented
and discussed in Section 6. We finally conclude in
Section 7.
</bodyText>
<sectionHeader confidence="0.999928" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999891">
Punctuation prediction has been extensively studied
in the speech processing field. It is also sometimes
studied together with a closely related task – sen-
tence boundary detection.
Much previous work assumes that both lexical
and prosodic cues are available for the task. Kim
and Woodland (2001) performed punctuation inser-
tion during speech recognition. Prosodic features to-
gether with language model probabilities were used
within a decision tree framework. Christensen et
al. (2001) focused on the broadcast news domain
and investigated both finite state and multi-layer per-
ceptron methods for the task, where prosodic and
lexical information was incorporated. Huang and
Zweig (2002) presented a maximum entropy-based
tagging approach to punctuation insertion in spon-
taneous English conversational speech, where both
lexical and prosodic features were exploited. Liu
et al. (2005) focused on the sentence boundary de-
tection task, by making use of conditional random
fields (CRF) (Lafferty et al., 2001). Their method
was shown to improve over a previous method based
on hidden Markov model (HMM).
There is relatively less work that exploited lexical
features only. Beeferman et al. (1998) focused on
comma prediction with a trigram language model. A
joint language model was learned from punctuated
texts, and commas were inserted so as to maximize
the joint probability score. Recent work by Gravano
et al. (2009) presented a purely n-gram based ap-
proach that jointly predicted punctuation and case
information of English.
Stolcke et al. (1998) presented a “hidden event
language model” that treated boundary detection
and punctuation insertion as an interword hidden
event detection task. Their proposed method was
implemented in the handy utility hidden-ngram as
part of the SRILM toolkit (Stolcke, 2002). It was
widely used in many recent spoken language trans-
lation tasks as either a preprocessing (Wang et al.,
2008) or postprocessing (Kirchhoff and Yang, 2007)
step. More details about this model will be given in
the next section.
Recently, there are also several research efforts
that try to optimize some downstream application
after punctuation prediction, rather than the predic-
tion task itself. Examples of such downstream ap-
plications include punctuation prediction for part-of-
speech (POS) tagging and name tagging (Hillard et
al., 2006), statistical machine translation (Matusov
et al., 2006), and information extraction (Favre et
al., 2008).
</bodyText>
<sectionHeader confidence="0.991342" genericHeader="method">
3 Hidden Event Language Model
</sectionHeader>
<bodyText confidence="0.999984866666667">
Many previous research efforts consider the bound-
ary detection and punctuation insertion task as a hid-
den event detection task. One such well-known ap-
proach was introduced by Stolcke et al. (1998).
They adopted a HMM to describe a joint distribu-
tion over words and interword events, where the ob-
servations are the words, and the word/event pairs
are encoded as hidden states. Specifically, in this
task word boundaries and punctuation symbols are
encoded as interword events. The training phase
involves training an n-gram language model over
all observed words and events with smoothing tech-
niques. The learned n-gram probability scores are
then used as the HMM state-transition scores. Dur-
ing testing, the posterior probability of an event
at each word is computed with dynamic program-
ming using the forward-backward algorithm. The
sequence of most probable states thus forms the out-
put which gives the punctuated sentence.
Such a HMM-based approach has several draw-
backs. First, the n-gram language model is only
able to capture surrounding contextual information.
However, we argue that in many cases, modeling of
longer range dependencies is required for punctua-
tion insertion. For example, the method is unable
to effectively capture the long range dependency be-
tween the initial phrase “would you” which strongly
indicates a question sentence, and an ending ques-
tion mark. This hurts the punctuation prediction per-
formance for our task since we are particularly inter-
</bodyText>
<page confidence="0.997081">
178
</page>
<bodyText confidence="0.999744823529412">
ested in conversational speech texts where question
sentences appear frequently.
Thus, in practice, special techniques are usually
required on top of using a hidden event language
model in order to overcome long range dependen-
cies. Examples include relocating or duplicating
punctuation symbols to different positions of a sen-
tence such that they appear closer to the indicative
words (e.g., “how much” indicates a question sen-
tence). One such technique was introduced by the
organizers of the IWSLT evaluation campaign, who
suggested duplicating the ending punctuation sym-
bol to the beginning of each sentence before training
the language model1. Empirically, the technique has
demonstrated its effectiveness in predicting question
marks in English, since most of the indicative words
for English question sentences appear at the begin-
ning of a question. However, such a technique is
specially designed and may not be widely applica-
ble in general or to languages other than English.
Furthermore, a direct application of such a method
may fail in the event of multiple sentences per utter-
ance without clearly annotated sentence boundaries
within an utterance.
Another drawback associated with such an ap-
proach is that the method encodes strong depen-
dency assumptions between the punctuation symbol
to be inserted and its surrounding words. Thus, it
lacks the robustness to handle cases where noisy or
out-of-vocabulary (OOV) words frequently appear,
such as in texts automatically recognized by ASR
systems. In this paper, we devise techniques based
on conditional random fields to tackle the difficulties
due to long range dependencies.
</bodyText>
<sectionHeader confidence="0.9701495" genericHeader="method">
4 Linear-Chain Conditional Random
Fields
</sectionHeader>
<bodyText confidence="0.99603">
One natural approach to relax the strong depen-
dency assumptions encoded by the hidden event lan-
guage model is to adopt an undirected graphical
model, where arbitrary overlapping features can be
exploited.
Conditional random fields (CRF) (Lafferty et al.,
2001) have been widely used in various sequence
labeling and segmentation tasks (Sha and Pereira,
</bodyText>
<footnote confidence="0.991865">
1http://mastarpj.nict.go.jp/IWSLT2008/downloads/
case+punc tool using SRILM.instructions.txt
</footnote>
<bodyText confidence="0.995650375">
2003; Tseng et al., 2005). Unlike a HMM which
models the joint distribution of both the label se-
quence and the observation, a CRF is a discrimi-
native model of the conditional distribution of the
complete label sequence given the observation.
Specifically, a first-order linear-chain CRF which
assumes first-order Markov property is defined by
the following equation:
</bodyText>
<equation confidence="0.99066425">
pλ(y|x) = Z1 1
) exp EEAkfk(x,yt−1,yt,t)
t k
(1)
</equation>
<bodyText confidence="0.997891875">
where x is the observation and y is the label se-
quence. Feature functions fk with time step t are
defined over the entire observation x and two adja-
cent hidden labels. Z(x) is a normalization factor to
ensure a well-formed probability distribution. Fig-
ure 1 gives a simplified graphical representation of
the model, where only the dependencies between la-
bel and observation in the same time step are shown.
</bodyText>
<figureCaption confidence="0.9967405">
Figure 1: A simplified graphical representation for linear-
chain CRF (observations are shaded)
</figureCaption>
<figure confidence="0.358819333333333">
proposed tags
NONE COMMA (,) PERIOD (.)
QMARK (?) EMARK (!)
</figure>
<tableCaption confidence="0.995206">
Table 1: The set of all possible tags for linear-chain CRF
</tableCaption>
<bodyText confidence="0.999989846153846">
We can model the punctuation prediction task as
the process of assigning a tag to each word, where
the set of possible tags is given in Table 1. That
is, we assume each word can be associated with
an event, which tells us which punctuation sym-
bol (possibly NONE) should be inserted after the
word. The training data consists of a set of utter-
ances where punctuation symbols are encoded as
tags that are assigned to the individual words. The
tag NONE means no punctuation symbol is inserted
after the current word. Any other tag refers to insert-
ing the corresponding punctuation symbol. In the
testing phase, the most probable sequence of tags is
</bodyText>
<figure confidence="0.967827090909091">
... yn
xn
y1
x1
x2
y2
x3
y3
179
Sentence: no , please do not. would you save your questions for the end of my talk, when i ask for them ?
no please do not would you ... my talk when ... them
</figure>
<figureCaption confidence="0.8535295">
COMMA NONE NONE PERIOD NONE NONE ... NONE COMMA NONE ... QMARK
Figure 2: An example tagging of a training sentence for the linear-chain CRF
</figureCaption>
<bodyText confidence="0.999942021276596">
predicted and the punctuated text can then be con-
structed from such an output. An example tagging
of an utterance is illustrated in Figure 2.
Following (Sutton et al., 2007), we factorize a
feature of conditional random fields as a product
of a binary function on assignment of the set of
cliques at the current time step (in this case an edge),
and a feature function solely defined on the ob-
servation sequence. n-gram occurrences surround-
ing the current word, together with position infor-
mation, are used as binary feature functions, for
n = 1, 2, 3. All words that appear within 5 words
from the current word are considered when build-
ing the features. Special start and end symbols are
used beyond the utterance boundaries. For example,
for the word do shown in Figure 2, example fea-
tures include unigram features do@0, please@-1,
bigram feature would+you@[2,3], and trigram fea-
ture no+please+do@[-2,0].
Such a linear-chain CRF model is capable of mod-
eling dependencies between words and punctuation
symbols with arbitrary overlapping features, thus
avoiding the strong dependency assumptions in the
hidden event language model. However, the linear-
chain CRF model still exhibits several problems for
the punctuation task. In particular, the dependency
between the punctuation symbols and the indicative
words cannot be captured adequately, if they appear
too far away from each other. For example, in the
sample utterance shown in Figure 2, the long range
dependency between the ending question mark and
the indicative words would you which appear very
far away cannot be directly captured. The problem
arises because a linear-chain CRF only learns a se-
quence of tags at the individual word level but is not
fully aware of sentence level information, such as
the start and end of a complete sentence.
Hence, it would be more reasonable to hypothe-
size that the punctuation symbols are annotated at
the sentence level, rather than relying on a limited
window of surrounding words. A model that can
jointly perform sentence segmentation and sentence
type prediction, together with word level punctu-
ation prediction would be more beneficial for our
task. This motivates us to build a joint model for
performing such a task, to be presented in the next
section.
</bodyText>
<sectionHeader confidence="0.996586" genericHeader="method">
5 Factorial Conditional Random Fields
</sectionHeader>
<bodyText confidence="0.999831666666667">
Extensions to the linear-chain CRF model have been
proposed in previous research efforts to encode long
range dependencies. One such well-known exten-
sion is the semi-Markov CRF (semi-CRF) (Sarawagi
and Cohen, 2005). Motivated by the hidden semi-
Markov model, the semi-CRF is particularly helpful
in text chunking tasks as it allows a state to persist
for a certain interval of time steps. This in practice
often leads to better modeling capability of chunks,
since state transitions within a chunk need not pre-
cisely follow the Markov property as in the case of
linear-chain CRF. However, it is not clear how such
a model can benefit our task, which requires word-
level labeling in addition to sentence boundary de-
tection and sentence type prediction.
The skip-chain CRF (Sutton and McCallum,
2004), another variant of linear-chain CRF, attaches
additional edges on top of a linear-chain CRF for
better modeling of long range dependencies between
states with similar observations. However, such a
model usually requires known long range dependen-
cies in advance and may not be readily applicable to
our task where such clues are not explicit.
As we have discussed above, since we would
like to jointly model both the word-level labeling
task and the sentence-level annotation task (sentence
boundary detection and sentence type prediction),
introducing an additional layer of tags to perform
both tasks together would be desirable. In this sec-
tion, we propose the use of factorial CRF (F-CRF)
(Sutton et al., 2007), which has previously been
shown to be effective for joint labeling of multiple
sequences (McCallum et al., 2003).
</bodyText>
<page confidence="0.986503">
180
</page>
<bodyText confidence="0.999928333333334">
The F-CRF as a specific case of dynamic condi-
tional random fields was originally motivated from
dynamic Bayesian networks, where an identical
structure repeats over different time steps. Analo-
gous to the linear-chain CRF, one can think of the F-
CRF as a framework that provides the capability of
simultaneously labeling multiple layers of tags for a
given sequence. It learns a joint conditional distri-
bution of the tags given the observation. Formally,
dynamic conditional random fields define the con-
ditional probability of a sequence of label vectors y
given the observation x as:
</bodyText>
<equation confidence="0.9407495">
exp E E E λkfk(x, y(c,t), t)/
t cEC k
</equation>
<bodyText confidence="0.986317111111111">
(2)
where cliques are indexed at each time step, C is a set
of clique indices, and y(c,t) is the set of variables in
the unrolled version of a clique with index c at time
t (Sutton et al., 2007). Figure 3 gives a graphical
representation of a two-layer factorial CRF, where
the cliques include the two within-chain edges (e.g.,
z2 − z3 and y2 − y3) and one between-chain edge
(e.g., z3 − y3) at each time step.
</bodyText>
<figureCaption confidence="0.993986">
Figure 3: A two-layer factorial CRF
</figureCaption>
<bodyText confidence="0.729083">
layer proposed tags
word NONE,COMMA,PERIOD,
</bodyText>
<equation confidence="0.517255">
QMARK,EMARK
sentence DEBEG,DEIN,QNBEG,QNIN,
EXBEG,EXIN
</equation>
<tableCaption confidence="0.9702375">
Table 2: The set of all possible tags proposed for each
layer
</tableCaption>
<bodyText confidence="0.999976782608696">
We build two layers of labels for this task, as
listed in Table 2. The word layer tags are respon-
sible for inserting a punctuation symbol (including
NONE) after each word, while the sentence layer
tags are used for annotating sentence boundaries and
identifying the sentence type (declarative, question,
or exclamatory). Tags from the word layer are the
same as those of the linear-chain CRF. The sentence
layer tags are designed for three types of sentences.
DEBEG and DEIN indicate the start and the inner
part of a declarative sentence respectively, likewise
for QNBEG and QNIN (question sentences), as well
as EXBEG and EXIN (exclamatory sentences). The
same example utterance we looked at in the previous
section is now tagged with these two layers of tags,
as shown in Figure 4. Analogous feature factoriza-
tion and the same n-gram feature functions used in
linear-chain CRF are used in F-CRF.
When learning the sentence layer tags together
with the word layer tags, the F-CRF model is capa-
ble of leveraging useful clues learned from the sen-
tence layer about sentence type (e.g., a question sen-
tence, annotated with QNBEG, QNIN, QNIN, ...,
or a declarative sentence, annotated with DEBEG,
DEIN, DEIN, ...), which can be used to guide the
prediction of the punctuation symbol at each word,
hence improving the performance at the word layer.
For example, consider jointly labeling the utterance
shown in Figure 4. Intuitively, when evidences
show that the utterance consists of two sentences –
a declarative sentence followed by a question sen-
tence, the model tends to annotate the second half of
the utterance with the sequence QNBEG QNIN ....
This in turn helps to predict the word level tag at
the end of the utterance as QMARK, given the de-
pendencies between the two layers existing at each
time step. In practice, during the learning process,
the two layers of tags are jointly learned, thus pro-
viding evidences that influence each other’s tagging
process.
In this work, we use the GRMM package (Sutton,
2006) for building both the linear-chain CRF (L-
CRF) and factorial CRF (F-CRF). The tree-based
reparameterization (TRP) schedule for belief propa-
gation (Wainwright et al., 2001) is used for approxi-
mate inference.
</bodyText>
<sectionHeader confidence="0.999843" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999291666666667">
We perform experiments on part of the corpus of the
IWSLT09 evaluation campaign (Paul, 2009), where
both Chinese and English conversational speech
</bodyText>
<figure confidence="0.956257090909091">
z1
y1
y2
z2
y3
z3
x3
x1
x2
. . .
. . .
xn
yn
zn
1
pλ(y|x) =
Z(x)
181
Sentence: no , please do not. would you save your questions for the end of my talk, when i ask for them ?
no please do not would you ... my talk when ... them
COMMA NONE NONE PERIOD NONE NONE ... NONE COMMA NONE ... QMARK
DEBEG DEIN DEIN DEIN QNBEG QNIN ... QNIN QNIN QNIN ... QNIN
</figure>
<figureCaption confidence="0.999512">
Figure 4: An example tagging of a training sentence for the factorial CRF
</figureCaption>
<bodyText confidence="0.999691745454546">
texts are used. Two multilingual datasets are consid-
ered, the BTEC (Basic Travel Expression Corpus)
dataset and the CT (Challenge Task) dataset. The
former consists of tourism-related sentences, and the
latter consists of human-mediated cross-lingual di-
alogs in travel domain. The official IWSLT09 BTEC
training set consists of 19,972 Chinese-English ut-
terance pairs, and the CT training set consists of
10,061 such pairs. We randomly split each of the
two datasets into two portions, where 90% of the ut-
terances are used for training the punctuation predic-
tion models, and the remaining 10% for evaluating
the prediction performance. For all the experiments,
we use the default segmentation of Chinese as pro-
vided, and English texts are preprocessed with the
Penn Treebank tokenizer2. We list the statistics of
the two datasets after processing in Table 3. The
proportions of sentence types in the two datasets are
listed. The majority of the sentences are declarative
sentences. However, question sentences are more
frequent in the BTEC dataset compared to the CT
dataset. Exclamatory sentences contribute less than
1% for all datasets and are not listed. We also count
how often each utterance consists of multiple sen-
tences. The utterances from the CT dataset are much
longer (with more words per utterance), and there-
fore more CT utterances actually consist of multiple
sentences.
to many possible setups. Specifically, these exper-
iments can be divided into two categories: with or
without duplicating the ending punctuation symbol
to the start of a sentence before training. This set-
ting can be used to assess the impact of the proxim-
ity between the punctuation symbol and the indica-
tive words for the prediction task. Under each cat-
egory, two possible approaches are tried. The sin-
gle pass approach performs prediction in one sin-
gle step, where all the punctuation symbols are pre-
dicted sequentially from left to right. In the cas-
caded approach, we format the training sentences
by replacing all sentence-ending punctuation sym-
bols with special sentence boundary symbols first.
A model for sentence boundary prediction is learned
based on such training data. This step is then fol-
lowed by predicting the actual punctuation symbols.
Both trigram and 5-gram language models are tried
for all combinations of the above settings. This gives
us a total of 8 possible combinations based on the
hidden event language model. When training all the
language models, modified Kneser-Ney smoothing
(Chen and Goodman, 1996) for n-grams is used.
To assess the performance of the punctuation pre-
diction task, we compute precision (prec.), recall
(rec.), and F1-measure (Fi), as defined by the fol-
lowing equations:
</bodyText>
<table confidence="0.99198425">
prec.
BTEC CT
CN EN CN EN rec.
declarative sent. 64% 65% 77% 81%
Fl
question sent. 36% 35% 22% 19%
multi.sent./uttr. 14% 17% 29% 39%
avg.words./uttr. 8.59 9.46 10.18 14.33 6.1
</table>
<tableCaption confidence="0.938783">
Table 3: Statistics of the BTEC and CT datasets
</tableCaption>
<figure confidence="0.617454545454545">
Performance on Correctly Recognized
Texts
# Correctly predicted punctuation symbols
=
# predicted punctuation symbols
# Correctly predicted punctuation symbols
=
# expected punctuation symbols
2
=
1/prec. + 1/rec.
</figure>
<footnote confidence="0.521435666666667">
For the methods based on the hidden event lan-
guage model, we design extensive experiments due
2http://www.cis.upenn.edu/—treebank/tokenization.html
</footnote>
<bodyText confidence="0.9361865">
The performance of punctuation prediction on both
Chinese (CN) and English (EN) texts in the correctly
recognized output of the BTEC and CT datasets are
presented in Table 4 and Table 5 respectively. The
</bodyText>
<page confidence="0.994142">
182
</page>
<table confidence="0.999335555555555">
BTEC NO DUPLICATION USE DUPLICATION L-CRF F-CRF
SINGLE PASS CASCADED SINGLE PASS CASCADED
LM ORDER 3 5 3 5 3 5 3 5
Prec. 87.40 86.44 87.72 87.13 76.74 77.58 77.89 78.50 94.82 94.83
CN Rec. 83.01 83.58 82.04 83.76 72.62 73.72 73.02 75.53 87.06 87.94
F1 85.15 84.99 84.79 85.41 74.63 75.60 75.37 76.99 90.78 91.25
Prec. 64.72 62.70 62.39 58.10 85.33 85.74 84.44 81.37 88.37 92.76
EN Rec. 60.76 59.49 58.57 55.28 80.42 80.98 79.43 77.52 80.28 84.73
F1 62.68 61.06 60.42 56.66 82.80 83.29 81.86 79.40 84.13 88.56
</table>
<tableCaption confidence="0.9947035">
Table 4: Punctuation prediction performance on Chinese (CN) and English (EN) texts in the correctly recognized
output of the BTEC dataset. Percentage scores of precision (Prec.), recall (Rec.), and F1 measure (Fl) are reported.
</tableCaption>
<table confidence="0.999934">
CT NO DUPLICATION USE DUPLICATION L-CRF F-CRF
SINGLE PASS CASCADED SINGLE PASS CASCADED
LM ORDER 3 5 3 5 3 5 3 5
Prec. 89.14 87.83 90.97 88.04 74.63 75.42 75.37 76.87 93.14 92.77
CN Rec. 84.71 84.16 77.78 84.08 70.69 70.84 64.62 73.60 83.45 86.92
F1 86.87 85.96 83.86 86.01 72.60 73.06 69.58 75.20 88.03 89.75
Prec. 73.86 73.42 67.02 65.15 75.87 77.78 74.75 74.44 83.07 86.69
EN Rec. 68.94 68.79 62.13 61.23 70.33 72.56 69.28 69.93 76.09 79.62
F1 71.31 71.03 64.48 63.13 72.99 75.08 71.91 72.12 79.43 83.01
</table>
<tableCaption confidence="0.970759">
Table 5: Punctuation prediction performance on Chinese (CN) and English (EN) texts in the correctly recognized
output of the CT dataset. Percentage scores of precision (Prec.), recall (Rec.), and F1 measure (Fl) are reported.
</tableCaption>
<bodyText confidence="0.999975090909091">
performance of the hidden event language model
heavily depends on whether the duplication method
is used and on the actual language under considera-
tion. Specifically, for English, duplicating the end-
ing punctuation symbol to the start of a sentence
before training is shown to be very helpful in im-
proving the overall prediction performance. In con-
trast, applying the same technique to Chinese hurts
the performance.
This observed difference is reasonable and ex-
pected. An English question sentence usually starts
with indicative words such as do you or where that
distinguish it from a declarative sentence. Thus, du-
plicating the ending punctuation symbol to the start
of a sentence so that it is near these indicative words
helps to improve the prediction accuracy. However,
Chinese presents quite different syntactic structures
for question sentences. First, we found that in many
cases, Chinese tends to use semantically vague aux-
iliary words at the end of a sentence to indicate a
question. Such auxiliary words include 吗 and 呢.
Thus, retaining the position of the ending punctu-
ation symbol before training yields better perfor-
mance. Another interesting finding is that, differ-
ent from English, other words that indicate a ques-
tion sentence in Chinese can appear at almost any
position in a Chinese sentence. Examples include
哪里有... (where ... ), ... 是什么 (what ... ), or
... 多少... (how many/much ... ). These pose diffi-
culties for the simple hidden event language model,
which only encodes simple dependencies over sur-
rounding words by means of n-gram language mod-
eling.
By adopting a discriminative model which ex-
ploits non-independent, overlapping features, the L-
CRF model generally outperforms the hidden event
language model. By introducing an additional layer
of tags for performing sentence segmentation and
sentence type prediction, the F-CRF model further
boosts the performance over the L-CRF model. We
perform statistical significance tests using bootstrap
resampling (Efron et al., 1993). The improvements
of F-CRF over L-CRF are statistically significant
(p &lt; 0.01) on Chinese and English texts in the CT
</bodyText>
<page confidence="0.996693">
183
</page>
<table confidence="0.999083333333334">
BTEC NO DUPLICATION USE DUPLICATION L-CRF F-CRF
SINGLE PASS CASCADED SINGLE PASS CASCADED
LM ORDER 3 5 3 5 3 5 3 5
Prec. 85.96 84.80 86.48 85.12 66.86 68.76 68.00 68.75 92.81 93.82
CN Rec. 81.87 82.78 83.15 82.78 63.92 66.12 65.38 66.48 85.16 89.01
F1 83.86 83.78 84.78 83.94 65.36 67.41 66.67 67.60 88.83 91.35
Prec. 62.38 59.29 56.86 54.22 85.23 87.29 84.49 81.32 90.67 93.72
EN Rec. 64.17 60.99 58.76 56.21 88.22 89.65 87.58 84.55 88.22 92.68
F1 63.27 60.13 57.79 55.20 86.70 88.45 86.00 82.90 89.43 93.19
</table>
<tableCaption confidence="0.9887435">
Table 6: Punctuation prediction performance on Chinese (CN) and English (EN) texts in the ASR output of IWSLT08
BTEC evaluation dataset. Percentage scores of precision (Prec.), recall (Rec.), and F1 measure (Fl) are reported.
</tableCaption>
<bodyText confidence="0.999818153846154">
dataset, and on English texts in the BTEC dataset.
The improvements of F-CRF over L-CRF on Chi-
nese texts are smaller, probably because L-CRF
is already performing quite well on Chinese. F1
measures on the CT dataset are lower than those
on BTEC, mainly because the CT dataset consists
of longer utterances and fewer question sentences.
Overall, our proposed F-CRF model is robust and
consistently works well regardless of the language
and dataset it is tested on. This indicates that the
approach is general and relies on minimal linguistic
assumptions, and thus can be readily used on other
languages and datasets.
</bodyText>
<subsectionHeader confidence="0.9987285">
6.2 Performance on Automatically Recognized
Texts
</subsectionHeader>
<bodyText confidence="0.999962916666667">
So far we only evaluated punctuation prediction per-
formance on transcribed texts consisting of correctly
recognized words. We now present the evaluation
results on texts produced by ASR systems.
For evaluation, we use the 1-best ASR outputs of
spontaneous speech of the official IWSLT08 BTEC
evaluation dataset, which is released as part of the
IWSLT09 corpus. The dataset consists of 504 utter-
ances in Chinese, and 498 in English. Unlike the
correctly recognized texts described in Section 6.1,
the ASR outputs contain substantial recognition er-
rors (recognition accuracy is 86% for Chinese, and
80% for English (Paul, 2008)). In the dataset re-
leased by the IWSLT organizers, the correct punctu-
ation symbols are not annotated in the ASR outputs.
To conduct our experimental evaluation, we manu-
ally annotated the correct punctuation symbols on
the ASR outputs.
We used all the learned models in Section 6.1, and
applied them to this dataset. The evaluation results
are shown in Table 6. The results show that F-CRF
still gives higher performance than L-CRF and the
hidden event language model, and the improvements
are statistically significant (p &lt; 0.01).
</bodyText>
<subsectionHeader confidence="0.989306">
6.3 Performance in Translation
</subsectionHeader>
<bodyText confidence="0.999938230769231">
The evaluation process as described in Section 6.2
requires substantial manual efforts to annotate the
correct punctuation symbols. In this section, we in-
stead adopt an indirect approach to automatically
evaluate the performance of punctuation prediction
on ASR output texts by feeding the punctuated ASR
texts to a state-of-the-art machine translation sys-
tem, and evaluate the resulting translation perfor-
mance. The translation performance is in turn mea-
sured by an automatic evaluation metric which cor-
relates well with human judgments. We believe
that such a task-oriented approach for evaluating the
quality of punctuation prediction for ASR output
texts is useful, since it tells us how well the punc-
tuated ASR output texts from each punctuation pre-
diction system can be used for further processing,
such as in statistical machine translation.
In this paper, we use Moses (Koehn et al., 2007),
a state-of-the-art phrase-based statistical machine
translation toolkit, as our translation engine. We
use the entire IWSLT09 BTEC training set for train-
ing the translation system. The state-of-the-art un-
supervised Berkeley aligner3 (Liang et al., 2006) is
used for aligning the training bitext. We use all
the default settings of Moses, except with the lexi-
calized reordering model enabled. This is because
</bodyText>
<footnote confidence="0.960668">
3http://code.google.com/p/berkeleyaligner/
</footnote>
<page confidence="0.987341">
184
</page>
<table confidence="0.9977184">
NO DUPLICATION USE DUPLICATION L-CRF F-CRF
SINGLE PASS CASCADED SINGLE PASS CASCADED
LM ORDER 3 5 3 5 3 5 3 5
CN EN 30.77 30.71 30.98 30.64 30.16 30.26 30.33 30.42 31.27 31.30
EN CN 21.21 21.00 21.16 20.76 23.03 24.04 23.61 23.34 23.44 24.18
</table>
<tableCaption confidence="0.999925">
Table 7: Translation performance on punctuated ASR outputs using Moses (Averaged percentage scores of BLEU)
</tableCaption>
<bodyText confidence="0.999842104166667">
lexicalized reordering gives better performance than
simple distance-based reordering (Koehn et al.,
2005). Specifically, the default lexicalized reorder-
ing model (msd-bidirectional-fe) is used.
For tuning the parameters of Moses, we use the
official IWSLT05 evaluation set where the correct
punctuation symbols are present. Evaluations are
performed on the ASR outputs of the IWSLT08
BTEC evaluation dataset, with punctuation symbols
inserted by each punctuation prediction method.
The tuning set and evaluation set include 7 reference
translations. Following a common practice in statis-
tical machine translation, we report BLEU-4 scores
(Papineni et al., 2002), which were shown to have
good correlation with human judgments, with the
closest reference length as the effective reference
length. The minimum error rate training (MERT)
(Och, 2003) procedure is used for tuning the model
parameters of the translation system. Due to the un-
stable nature of MERT, we perform 10 runs for each
translation task, with a different random initializa-
tion of parameters in each run, and report the BLEU-
4 scores averaged over 10 runs.
The results are reported in Table 7. The best
translation performances for both translation direc-
tions are achieved by applying F-CRF as the punc-
tuation prediction model to the ASR texts. Such im-
provements are observed to be consistent over dif-
ferent runs. The improvement of F-CRF over L-
CRF in translation quality is statistically significant
(p &lt; 0.05) when translating from English to Chi-
nese. In addition, we also assess the translation
performance when the manually annotated punctu-
ation symbols as mentioned in Section 6.2 are used
for translation. The averaged BLEU scores for the
two translation tasks are 31.58 (Chinese to English)
and 24.16 (English to Chinese) respectively, which
show that our punctuation prediction method gives
competitive performance for spoken language trans-
lation.
It is important to note that in this work, we only
focus on optimizing the punctuation prediction per-
formance in the form of F1-measure, without regard
to the subsequent NLP tasks. How to perform punc-
tuation prediction so as to optimize translation per-
formance is an important research topic that is be-
yond the scope of this paper and needs further in-
vestigation in future work.
</bodyText>
<sectionHeader confidence="0.999467" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999985">
In this paper, we have proposed a novel approach
for predicting punctuation symbols for transcribed
conversational speech texts. Our proposed approach
is built on top of a dynamic conditional random
fields framework, which jointly performs punctua-
tion prediction together with sentence boundary and
sentence type prediction on speech utterances. Un-
like most previous work, it tackles the task from a
purely text processing perspective and does not rely
on prosodic cues.
Experimental results have shown that our pro-
posed approach outperforms the widely used ap-
proach based on the hidden event language model,
and also outperforms a method based on linear-chain
conditional random fields. Our proposed approach
has been shown to be general, working well on both
Chinese and English, and on both correctly recog-
nized and automatically recognized texts. Our pro-
posed approach also results in better translation ac-
curacy when the punctuated automatically recog-
nized texts are used in subsequent translation.
</bodyText>
<sectionHeader confidence="0.999332" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9880272">
This research was done for CSIDM Project No.
CSIDM-200804 partially funded by a grant from
the National Research Foundation (NRF) adminis-
tered by the Media Development Authority (MDA)
of Singapore.
</bodyText>
<page confidence="0.998575">
185
</page>
<sectionHeader confidence="0.99832" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999926536082475">
D. Beeferman, A. Berger, and J. Lafferty. 1998.
CYBERPUNC: A lightweight punctuation annotation
system for speech. In Proc. of ICASSP’98.
S.F. Chen and J. Goodman. 1996. An empirical study of
smoothing techniques for language modeling. In Proc.
of ACL’06.
H. Christensen, Y. Gotoh, and S. Renals. 2001. Punctu-
ation annotation using statistical prosody models. In
Proc. of ISCA Workshop on Prosody in Speech Recog-
nition and Understanding.
B. Efron, R. Tibshirani, and R.J. Tibshirani. 1993. An
introduction to the bootstrap. Chapman &amp; Hall/CRC.
B. Favre, R. Grishman, D. Hillard, H. Ji, D. Hakkani-
Tur, and M. Ostendorf. 2008. Punctuating speech for
information extraction. In Proc. of ICASSP’08.
A. Gravano, M. Jansche, and M. Bacchiani. 2009.
Restoring punctuation and capitalization in transcribed
speech. In Proc. of ICASSP’09.
D. Hillard, Z. Huang, H. Ji, R. Grishman, D. Hakkani-
Tur, M. Harper, M. Ostendorf, and W. Wang. 2006.
Impact of automatic comma prediction on POS/name
tagging of speech. In Proc. of SLT’06.
J. Huang and G. Zweig. 2002. Maximum entropy model
for punctuation annotation from speech. In Proc. of
ICSLP’02.
J.H. Kim and P.C. Woodland. 2001. The use of prosody
in a combined system for punctuation generation and
speech recognition. In Proc. of EuroSpeech’01.
K. Kirchhoff and M. Yang. 2007. The University of
Washington machine translation system for the IWSLT
2007 competition. In Proc. of IWSLT’07.
P. Koehn, A. Axelrod, A.B. Mayne, C. Callison-Burch,
M. Osborne, and D. Talbot. 2005. Edinburgh sys-
tem description for the 2005 IWSLT speech translation
evaluation. In Proc. of IWSLT’05.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc. of
ACL’07 (Demo Session).
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of
ICML’01.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In Proc. of HLT/NAACL’06.
Y. Liu, A. Stolcke, E. Shriberg, and M. Harper. 2005.
Using conditional random fields for sentence boundary
detection in speech. In Proc. of ACL’05.
E. Matusov, A. Mauser, and H. Ney. 2006. Automatic
sentence segmentation and punctuation prediction for
spoken language translation. In Proc. of IWSLT’06.
A. McCallum, K. Rohanimanesh, and C. Sutton. 2003.
Dynamic conditional random fields for jointly labeling
multiple sequences. In Proc. of NIPS’03 Workshop on
Syntax, Semantics and Statistics.
F.J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of ACL’03.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL’02.
M. Paul. 2008. Overview of the IWSLT 2008 evaluation
campaign. In Proc. of IWSLT’08.
M. Paul. 2009. Overview of the IWSLT 2009 evaluation
campaign. In Proc. of IWSLT’09.
S. Sarawagi and W.W. Cohen. 2005. Semi-Markov con-
ditional random fields for information extraction. In
Proc. of NIPS’05.
F. Sha and F. Pereira. 2003. Shallow parsing with condi-
tional random fields. In Proc. of HLT-NAACL’03.
A. Stolcke, E. Shriberg, R. Bates, M. Ostendorf,
D. Hakkani, M. Plauche, G. Tur, and Y. Lu. 1998.
Automatic detection of sentence boundaries and dis-
fluencies based on recognized words. In Proc. of IC-
SLP’98.
A. Stolcke. 2002. SRILM–an extensible language mod-
eling toolkit. In Proc. of ICSLP’02.
C. Sutton and A. McCallum. 2004. Collective segmenta-
tion and labeling of distant entities in information ex-
traction. In Proc. of ICML’04 workshop on Statistical
Relational Learning.
C. Sutton, A. McCallum, and K. Rohanimanesh. 2007.
Dynamic conditional random fields: Factorized prob-
abilistic models for labeling and segmenting sequence
data. Journal of Machine Learning Research, 8.
C. Sutton. 2006. GRMM: GRaphical Models in Mallet.
http://mallet.cs.umass.edu/grmm/.
H. Tseng, P. Chang, G. Andrew, D. Jurafsky, and C. Man-
ning. 2005. A conditional random field word seg-
menter for sighan bakeoff 2005. In Proc. of the Fourth
SIGHAN Workshop on Chinese Language Processing.
M. Wainwright, T. Jaakkola, and A. Willsky. 2001. Tree-
based reparameterization for approximate inference on
loopy graphs. In Proc. of NIPS’01.
H. Wang, H. Wu, X. Hu, Z. Liu, J. Li, D. Ren, and
Z. Niu. 2008. The TCH machine translation system
for IWSLT 2008. In Proc. of IWSLT’08.
</reference>
<page confidence="0.998791">
186
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.942956">
<title confidence="0.995292">Better Punctuation Prediction with Dynamic Conditional Random Fields</title>
<author confidence="0.999641">Lu Tou</author>
<affiliation confidence="0.999893">Department of Computer National University of</affiliation>
<address confidence="0.975838">13 Computing Drive, Singapore</address>
<abstract confidence="0.998280944444444">This paper focuses on the task of inserting punctuation symbols into transcribed conversational speech texts, without relying on prosodic cues. We investigate limitations associated with previous methods, and propose a novel approach based on dynamic conditional random fields. Different from previous work, our proposed approach is designed to jointly perform both sentence boundary and sentence type prediction, and punctuation prediction on speech utterances. We performed evaluations on a transcribed conversational speech domain consisting of both English and Chinese texts. Empirical results show that our method outperforms an approach based on linear-chain conditional random fields and other previous approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Beeferman</author>
<author>A Berger</author>
<author>J Lafferty</author>
</authors>
<date>1998</date>
<contexts>
<context position="5107" citStr="Beeferman et al. (1998)" startWordPosition="771" endWordPosition="774">layer perceptron methods for the task, where prosodic and lexical information was incorporated. Huang and Zweig (2002) presented a maximum entropy-based tagging approach to punctuation insertion in spontaneous English conversational speech, where both lexical and prosodic features were exploited. Liu et al. (2005) focused on the sentence boundary detection task, by making use of conditional random fields (CRF) (Lafferty et al., 2001). Their method was shown to improve over a previous method based on hidden Markov model (HMM). There is relatively less work that exploited lexical features only. Beeferman et al. (1998) focused on comma prediction with a trigram language model. A joint language model was learned from punctuated texts, and commas were inserted so as to maximize the joint probability score. Recent work by Gravano et al. (2009) presented a purely n-gram based approach that jointly predicted punctuation and case information of English. Stolcke et al. (1998) presented a “hidden event language model” that treated boundary detection and punctuation insertion as an interword hidden event detection task. Their proposed method was implemented in the handy utility hidden-ngram as part of the SRILM tool</context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1998</marker>
<rawString>D. Beeferman, A. Berger, and J. Lafferty. 1998.</rawString>
</citation>
<citation valid="false">
<title>CYBERPUNC: A lightweight punctuation annotation system for speech.</title>
<booktitle>In Proc. of ICASSP’98.</booktitle>
<marker></marker>
<rawString>CYBERPUNC: A lightweight punctuation annotation system for speech. In Proc. of ICASSP’98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>J Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In Proc. of ACL’06.</booktitle>
<contexts>
<context position="22508" citStr="Chen and Goodman, 1996" startWordPosition="3611" endWordPosition="3614">sequentially from left to right. In the cascaded approach, we format the training sentences by replacing all sentence-ending punctuation symbols with special sentence boundary symbols first. A model for sentence boundary prediction is learned based on such training data. This step is then followed by predicting the actual punctuation symbols. Both trigram and 5-gram language models are tried for all combinations of the above settings. This gives us a total of 8 possible combinations based on the hidden event language model. When training all the language models, modified Kneser-Ney smoothing (Chen and Goodman, 1996) for n-grams is used. To assess the performance of the punctuation prediction task, we compute precision (prec.), recall (rec.), and F1-measure (Fi), as defined by the following equations: prec. BTEC CT CN EN CN EN rec. declarative sent. 64% 65% 77% 81% Fl question sent. 36% 35% 22% 19% multi.sent./uttr. 14% 17% 29% 39% avg.words./uttr. 8.59 9.46 10.18 14.33 6.1 Table 3: Statistics of the BTEC and CT datasets Performance on Correctly Recognized Texts # Correctly predicted punctuation symbols = # predicted punctuation symbols # Correctly predicted punctuation symbols = # expected punctuation sy</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>S.F. Chen and J. Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proc. of ACL’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Christensen</author>
<author>Y Gotoh</author>
<author>S Renals</author>
</authors>
<title>Punctuation annotation using statistical prosody models.</title>
<date>2001</date>
<booktitle>In Proc. of ISCA Workshop on Prosody in Speech Recognition and Understanding.</booktitle>
<contexts>
<context position="4401" citStr="Christensen et al. (2001)" startWordPosition="664" endWordPosition="667">in Section 4 and 5. Empirical evaluation results are presented and discussed in Section 6. We finally conclude in Section 7. 2 Related Work Punctuation prediction has been extensively studied in the speech processing field. It is also sometimes studied together with a closely related task – sentence boundary detection. Much previous work assumes that both lexical and prosodic cues are available for the task. Kim and Woodland (2001) performed punctuation insertion during speech recognition. Prosodic features together with language model probabilities were used within a decision tree framework. Christensen et al. (2001) focused on the broadcast news domain and investigated both finite state and multi-layer perceptron methods for the task, where prosodic and lexical information was incorporated. Huang and Zweig (2002) presented a maximum entropy-based tagging approach to punctuation insertion in spontaneous English conversational speech, where both lexical and prosodic features were exploited. Liu et al. (2005) focused on the sentence boundary detection task, by making use of conditional random fields (CRF) (Lafferty et al., 2001). Their method was shown to improve over a previous method based on hidden Marko</context>
</contexts>
<marker>Christensen, Gotoh, Renals, 2001</marker>
<rawString>H. Christensen, Y. Gotoh, and S. Renals. 2001. Punctuation annotation using statistical prosody models. In Proc. of ISCA Workshop on Prosody in Speech Recognition and Understanding.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Efron</author>
<author>R Tibshirani</author>
<author>R J Tibshirani</author>
</authors>
<title>An introduction to the bootstrap.</title>
<date>1993</date>
<publisher>Chapman &amp; Hall/CRC.</publisher>
<contexts>
<context position="26964" citStr="Efron et al., 1993" startWordPosition="4330" endWordPosition="4333">. (how many/much ... ). These pose difficulties for the simple hidden event language model, which only encodes simple dependencies over surrounding words by means of n-gram language modeling. By adopting a discriminative model which exploits non-independent, overlapping features, the LCRF model generally outperforms the hidden event language model. By introducing an additional layer of tags for performing sentence segmentation and sentence type prediction, the F-CRF model further boosts the performance over the L-CRF model. We perform statistical significance tests using bootstrap resampling (Efron et al., 1993). The improvements of F-CRF over L-CRF are statistically significant (p &lt; 0.01) on Chinese and English texts in the CT 183 BTEC NO DUPLICATION USE DUPLICATION L-CRF F-CRF SINGLE PASS CASCADED SINGLE PASS CASCADED LM ORDER 3 5 3 5 3 5 3 5 Prec. 85.96 84.80 86.48 85.12 66.86 68.76 68.00 68.75 92.81 93.82 CN Rec. 81.87 82.78 83.15 82.78 63.92 66.12 65.38 66.48 85.16 89.01 F1 83.86 83.78 84.78 83.94 65.36 67.41 66.67 67.60 88.83 91.35 Prec. 62.38 59.29 56.86 54.22 85.23 87.29 84.49 81.32 90.67 93.72 EN Rec. 64.17 60.99 58.76 56.21 88.22 89.65 87.58 84.55 88.22 92.68 F1 63.27 60.13 57.79 55.20 86.7</context>
</contexts>
<marker>Efron, Tibshirani, Tibshirani, 1993</marker>
<rawString>B. Efron, R. Tibshirani, and R.J. Tibshirani. 1993. An introduction to the bootstrap. Chapman &amp; Hall/CRC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Favre</author>
<author>R Grishman</author>
<author>D Hillard</author>
<author>H Ji</author>
<author>D HakkaniTur</author>
<author>M Ostendorf</author>
</authors>
<title>Punctuating speech for information extraction.</title>
<date>2008</date>
<booktitle>In Proc. of ICASSP’08.</booktitle>
<contexts>
<context position="6375" citStr="Favre et al., 2008" startWordPosition="964" endWordPosition="967">ecent spoken language translation tasks as either a preprocessing (Wang et al., 2008) or postprocessing (Kirchhoff and Yang, 2007) step. More details about this model will be given in the next section. Recently, there are also several research efforts that try to optimize some downstream application after punctuation prediction, rather than the prediction task itself. Examples of such downstream applications include punctuation prediction for part-ofspeech (POS) tagging and name tagging (Hillard et al., 2006), statistical machine translation (Matusov et al., 2006), and information extraction (Favre et al., 2008). 3 Hidden Event Language Model Many previous research efforts consider the boundary detection and punctuation insertion task as a hidden event detection task. One such well-known approach was introduced by Stolcke et al. (1998). They adopted a HMM to describe a joint distribution over words and interword events, where the observations are the words, and the word/event pairs are encoded as hidden states. Specifically, in this task word boundaries and punctuation symbols are encoded as interword events. The training phase involves training an n-gram language model over all observed words and ev</context>
</contexts>
<marker>Favre, Grishman, Hillard, Ji, HakkaniTur, Ostendorf, 2008</marker>
<rawString>B. Favre, R. Grishman, D. Hillard, H. Ji, D. HakkaniTur, and M. Ostendorf. 2008. Punctuating speech for information extraction. In Proc. of ICASSP’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gravano</author>
<author>M Jansche</author>
<author>M Bacchiani</author>
</authors>
<title>Restoring punctuation and capitalization in transcribed speech.</title>
<date>2009</date>
<booktitle>In Proc. of ICASSP’09.</booktitle>
<contexts>
<context position="5333" citStr="Gravano et al. (2009)" startWordPosition="808" endWordPosition="811">onal speech, where both lexical and prosodic features were exploited. Liu et al. (2005) focused on the sentence boundary detection task, by making use of conditional random fields (CRF) (Lafferty et al., 2001). Their method was shown to improve over a previous method based on hidden Markov model (HMM). There is relatively less work that exploited lexical features only. Beeferman et al. (1998) focused on comma prediction with a trigram language model. A joint language model was learned from punctuated texts, and commas were inserted so as to maximize the joint probability score. Recent work by Gravano et al. (2009) presented a purely n-gram based approach that jointly predicted punctuation and case information of English. Stolcke et al. (1998) presented a “hidden event language model” that treated boundary detection and punctuation insertion as an interword hidden event detection task. Their proposed method was implemented in the handy utility hidden-ngram as part of the SRILM toolkit (Stolcke, 2002). It was widely used in many recent spoken language translation tasks as either a preprocessing (Wang et al., 2008) or postprocessing (Kirchhoff and Yang, 2007) step. More details about this model will be gi</context>
</contexts>
<marker>Gravano, Jansche, Bacchiani, 2009</marker>
<rawString>A. Gravano, M. Jansche, and M. Bacchiani. 2009. Restoring punctuation and capitalization in transcribed speech. In Proc. of ICASSP’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hillard</author>
<author>Z Huang</author>
<author>H Ji</author>
<author>R Grishman</author>
<author>D HakkaniTur</author>
<author>M Harper</author>
<author>M Ostendorf</author>
<author>W Wang</author>
</authors>
<title>Impact of automatic comma prediction on POS/name tagging of speech.</title>
<date>2006</date>
<booktitle>In Proc. of SLT’06.</booktitle>
<contexts>
<context position="6270" citStr="Hillard et al., 2006" startWordPosition="950" endWordPosition="953">n the handy utility hidden-ngram as part of the SRILM toolkit (Stolcke, 2002). It was widely used in many recent spoken language translation tasks as either a preprocessing (Wang et al., 2008) or postprocessing (Kirchhoff and Yang, 2007) step. More details about this model will be given in the next section. Recently, there are also several research efforts that try to optimize some downstream application after punctuation prediction, rather than the prediction task itself. Examples of such downstream applications include punctuation prediction for part-ofspeech (POS) tagging and name tagging (Hillard et al., 2006), statistical machine translation (Matusov et al., 2006), and information extraction (Favre et al., 2008). 3 Hidden Event Language Model Many previous research efforts consider the boundary detection and punctuation insertion task as a hidden event detection task. One such well-known approach was introduced by Stolcke et al. (1998). They adopted a HMM to describe a joint distribution over words and interword events, where the observations are the words, and the word/event pairs are encoded as hidden states. Specifically, in this task word boundaries and punctuation symbols are encoded as inter</context>
</contexts>
<marker>Hillard, Huang, Ji, Grishman, HakkaniTur, Harper, Ostendorf, Wang, 2006</marker>
<rawString>D. Hillard, Z. Huang, H. Ji, R. Grishman, D. HakkaniTur, M. Harper, M. Ostendorf, and W. Wang. 2006. Impact of automatic comma prediction on POS/name tagging of speech. In Proc. of SLT’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Huang</author>
<author>G Zweig</author>
</authors>
<title>Maximum entropy model for punctuation annotation from speech.</title>
<date>2002</date>
<booktitle>In Proc. of ICSLP’02.</booktitle>
<contexts>
<context position="4602" citStr="Huang and Zweig (2002)" startWordPosition="694" endWordPosition="697">h processing field. It is also sometimes studied together with a closely related task – sentence boundary detection. Much previous work assumes that both lexical and prosodic cues are available for the task. Kim and Woodland (2001) performed punctuation insertion during speech recognition. Prosodic features together with language model probabilities were used within a decision tree framework. Christensen et al. (2001) focused on the broadcast news domain and investigated both finite state and multi-layer perceptron methods for the task, where prosodic and lexical information was incorporated. Huang and Zweig (2002) presented a maximum entropy-based tagging approach to punctuation insertion in spontaneous English conversational speech, where both lexical and prosodic features were exploited. Liu et al. (2005) focused on the sentence boundary detection task, by making use of conditional random fields (CRF) (Lafferty et al., 2001). Their method was shown to improve over a previous method based on hidden Markov model (HMM). There is relatively less work that exploited lexical features only. Beeferman et al. (1998) focused on comma prediction with a trigram language model. A joint language model was learned </context>
</contexts>
<marker>Huang, Zweig, 2002</marker>
<rawString>J. Huang and G. Zweig. 2002. Maximum entropy model for punctuation annotation from speech. In Proc. of ICSLP’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Kim</author>
<author>P C Woodland</author>
</authors>
<title>The use of prosody in a combined system for punctuation generation and speech recognition.</title>
<date>2001</date>
<booktitle>In Proc. of EuroSpeech’01.</booktitle>
<contexts>
<context position="4211" citStr="Kim and Woodland (2001)" startWordPosition="638" endWordPosition="641"> with surveying related work in Section 2. One class of widely-used previous techniques is then studied in detail in Section 3. Next, we investigate methods for improving existing methods in Section 4 and 5. Empirical evaluation results are presented and discussed in Section 6. We finally conclude in Section 7. 2 Related Work Punctuation prediction has been extensively studied in the speech processing field. It is also sometimes studied together with a closely related task – sentence boundary detection. Much previous work assumes that both lexical and prosodic cues are available for the task. Kim and Woodland (2001) performed punctuation insertion during speech recognition. Prosodic features together with language model probabilities were used within a decision tree framework. Christensen et al. (2001) focused on the broadcast news domain and investigated both finite state and multi-layer perceptron methods for the task, where prosodic and lexical information was incorporated. Huang and Zweig (2002) presented a maximum entropy-based tagging approach to punctuation insertion in spontaneous English conversational speech, where both lexical and prosodic features were exploited. Liu et al. (2005) focused on </context>
</contexts>
<marker>Kim, Woodland, 2001</marker>
<rawString>J.H. Kim and P.C. Woodland. 2001. The use of prosody in a combined system for punctuation generation and speech recognition. In Proc. of EuroSpeech’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kirchhoff</author>
<author>M Yang</author>
</authors>
<title>The University of Washington machine translation system for the IWSLT</title>
<date>2007</date>
<booktitle>In Proc. of IWSLT’07.</booktitle>
<contexts>
<context position="5886" citStr="Kirchhoff and Yang, 2007" startWordPosition="892" endWordPosition="895">ximize the joint probability score. Recent work by Gravano et al. (2009) presented a purely n-gram based approach that jointly predicted punctuation and case information of English. Stolcke et al. (1998) presented a “hidden event language model” that treated boundary detection and punctuation insertion as an interword hidden event detection task. Their proposed method was implemented in the handy utility hidden-ngram as part of the SRILM toolkit (Stolcke, 2002). It was widely used in many recent spoken language translation tasks as either a preprocessing (Wang et al., 2008) or postprocessing (Kirchhoff and Yang, 2007) step. More details about this model will be given in the next section. Recently, there are also several research efforts that try to optimize some downstream application after punctuation prediction, rather than the prediction task itself. Examples of such downstream applications include punctuation prediction for part-ofspeech (POS) tagging and name tagging (Hillard et al., 2006), statistical machine translation (Matusov et al., 2006), and information extraction (Favre et al., 2008). 3 Hidden Event Language Model Many previous research efforts consider the boundary detection and punctuation </context>
</contexts>
<marker>Kirchhoff, Yang, 2007</marker>
<rawString>K. Kirchhoff and M. Yang. 2007. The University of Washington machine translation system for the IWSLT 2007 competition. In Proc. of IWSLT’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>A Axelrod</author>
<author>A B Mayne</author>
<author>C Callison-Burch</author>
<author>M Osborne</author>
<author>D Talbot</author>
</authors>
<title>Edinburgh system description for the 2005 IWSLT speech translation evaluation.</title>
<date>2005</date>
<booktitle>In Proc. of IWSLT’05.</booktitle>
<contexts>
<context position="31486" citStr="Koehn et al., 2005" startWordPosition="5049" endWordPosition="5052">xt. We use all the default settings of Moses, except with the lexicalized reordering model enabled. This is because 3http://code.google.com/p/berkeleyaligner/ 184 NO DUPLICATION USE DUPLICATION L-CRF F-CRF SINGLE PASS CASCADED SINGLE PASS CASCADED LM ORDER 3 5 3 5 3 5 3 5 CN EN 30.77 30.71 30.98 30.64 30.16 30.26 30.33 30.42 31.27 31.30 EN CN 21.21 21.00 21.16 20.76 23.03 24.04 23.61 23.34 23.44 24.18 Table 7: Translation performance on punctuated ASR outputs using Moses (Averaged percentage scores of BLEU) lexicalized reordering gives better performance than simple distance-based reordering (Koehn et al., 2005). Specifically, the default lexicalized reordering model (msd-bidirectional-fe) is used. For tuning the parameters of Moses, we use the official IWSLT05 evaluation set where the correct punctuation symbols are present. Evaluations are performed on the ASR outputs of the IWSLT08 BTEC evaluation dataset, with punctuation symbols inserted by each punctuation prediction method. The tuning set and evaluation set include 7 reference translations. Following a common practice in statistical machine translation, we report BLEU-4 scores (Papineni et al., 2002), which were shown to have good correlation </context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>P. Koehn, A. Axelrod, A.B. Mayne, C. Callison-Burch, M. Osborne, and D. Talbot. 2005. Edinburgh system description for the 2005 IWSLT speech translation evaluation. In Proc. of IWSLT’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL’07</booktitle>
<location>(Demo Session).</location>
<contexts>
<context position="30573" citStr="Koehn et al., 2007" startWordPosition="4912" endWordPosition="4915">feeding the punctuated ASR texts to a state-of-the-art machine translation system, and evaluate the resulting translation performance. The translation performance is in turn measured by an automatic evaluation metric which correlates well with human judgments. We believe that such a task-oriented approach for evaluating the quality of punctuation prediction for ASR output texts is useful, since it tells us how well the punctuated ASR output texts from each punctuation prediction system can be used for further processing, such as in statistical machine translation. In this paper, we use Moses (Koehn et al., 2007), a state-of-the-art phrase-based statistical machine translation toolkit, as our translation engine. We use the entire IWSLT09 BTEC training set for training the translation system. The state-of-the-art unsupervised Berkeley aligner3 (Liang et al., 2006) is used for aligning the training bitext. We use all the default settings of Moses, except with the lexicalized reordering model enabled. This is because 3http://code.google.com/p/berkeleyaligner/ 184 NO DUPLICATION USE DUPLICATION L-CRF F-CRF SINGLE PASS CASCADED SINGLE PASS CASCADED LM ORDER 3 5 3 5 3 5 3 5 CN EN 30.77 30.71 30.98 30.64 30.</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, et al. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. of ACL’07 (Demo Session).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proc. of ICML’01.</booktitle>
<contexts>
<context position="4921" citStr="Lafferty et al., 2001" startWordPosition="741" endWordPosition="744">ith language model probabilities were used within a decision tree framework. Christensen et al. (2001) focused on the broadcast news domain and investigated both finite state and multi-layer perceptron methods for the task, where prosodic and lexical information was incorporated. Huang and Zweig (2002) presented a maximum entropy-based tagging approach to punctuation insertion in spontaneous English conversational speech, where both lexical and prosodic features were exploited. Liu et al. (2005) focused on the sentence boundary detection task, by making use of conditional random fields (CRF) (Lafferty et al., 2001). Their method was shown to improve over a previous method based on hidden Markov model (HMM). There is relatively less work that exploited lexical features only. Beeferman et al. (1998) focused on comma prediction with a trigram language model. A joint language model was learned from punctuated texts, and commas were inserted so as to maximize the joint probability score. Recent work by Gravano et al. (2009) presented a purely n-gram based approach that jointly predicted punctuation and case information of English. Stolcke et al. (1998) presented a “hidden event language model” that treated b</context>
<context position="9824" citStr="Lafferty et al., 2001" startWordPosition="1499" endWordPosition="1502">its surrounding words. Thus, it lacks the robustness to handle cases where noisy or out-of-vocabulary (OOV) words frequently appear, such as in texts automatically recognized by ASR systems. In this paper, we devise techniques based on conditional random fields to tackle the difficulties due to long range dependencies. 4 Linear-Chain Conditional Random Fields One natural approach to relax the strong dependency assumptions encoded by the hidden event language model is to adopt an undirected graphical model, where arbitrary overlapping features can be exploited. Conditional random fields (CRF) (Lafferty et al., 2001) have been widely used in various sequence labeling and segmentation tasks (Sha and Pereira, 1http://mastarpj.nict.go.jp/IWSLT2008/downloads/ case+punc tool using SRILM.instructions.txt 2003; Tseng et al., 2005). Unlike a HMM which models the joint distribution of both the label sequence and the observation, a CRF is a discriminative model of the conditional distribution of the complete label sequence given the observation. Specifically, a first-order linear-chain CRF which assumes first-order Markov property is defined by the following equation: pλ(y|x) = Z1 1 ) exp EEAkfk(x,yt−1,yt,t) t k (1</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. of ICML’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>B Taskar</author>
<author>D Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proc. of HLT/NAACL’06.</booktitle>
<contexts>
<context position="30828" citStr="Liang et al., 2006" startWordPosition="4947" endWordPosition="4950">ents. We believe that such a task-oriented approach for evaluating the quality of punctuation prediction for ASR output texts is useful, since it tells us how well the punctuated ASR output texts from each punctuation prediction system can be used for further processing, such as in statistical machine translation. In this paper, we use Moses (Koehn et al., 2007), a state-of-the-art phrase-based statistical machine translation toolkit, as our translation engine. We use the entire IWSLT09 BTEC training set for training the translation system. The state-of-the-art unsupervised Berkeley aligner3 (Liang et al., 2006) is used for aligning the training bitext. We use all the default settings of Moses, except with the lexicalized reordering model enabled. This is because 3http://code.google.com/p/berkeleyaligner/ 184 NO DUPLICATION USE DUPLICATION L-CRF F-CRF SINGLE PASS CASCADED SINGLE PASS CASCADED LM ORDER 3 5 3 5 3 5 3 5 CN EN 30.77 30.71 30.98 30.64 30.16 30.26 30.33 30.42 31.27 31.30 EN CN 21.21 21.00 21.16 20.76 23.03 24.04 23.61 23.34 23.44 24.18 Table 7: Translation performance on punctuated ASR outputs using Moses (Averaged percentage scores of BLEU) lexicalized reordering gives better performance </context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>P. Liang, B. Taskar, and D. Klein. 2006. Alignment by agreement. In Proc. of HLT/NAACL’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Liu</author>
<author>A Stolcke</author>
<author>E Shriberg</author>
<author>M Harper</author>
</authors>
<title>Using conditional random fields for sentence boundary detection in speech.</title>
<date>2005</date>
<booktitle>In Proc. of ACL’05.</booktitle>
<contexts>
<context position="4799" citStr="Liu et al. (2005)" startWordPosition="721" endWordPosition="724">task. Kim and Woodland (2001) performed punctuation insertion during speech recognition. Prosodic features together with language model probabilities were used within a decision tree framework. Christensen et al. (2001) focused on the broadcast news domain and investigated both finite state and multi-layer perceptron methods for the task, where prosodic and lexical information was incorporated. Huang and Zweig (2002) presented a maximum entropy-based tagging approach to punctuation insertion in spontaneous English conversational speech, where both lexical and prosodic features were exploited. Liu et al. (2005) focused on the sentence boundary detection task, by making use of conditional random fields (CRF) (Lafferty et al., 2001). Their method was shown to improve over a previous method based on hidden Markov model (HMM). There is relatively less work that exploited lexical features only. Beeferman et al. (1998) focused on comma prediction with a trigram language model. A joint language model was learned from punctuated texts, and commas were inserted so as to maximize the joint probability score. Recent work by Gravano et al. (2009) presented a purely n-gram based approach that jointly predicted p</context>
</contexts>
<marker>Liu, Stolcke, Shriberg, Harper, 2005</marker>
<rawString>Y. Liu, A. Stolcke, E. Shriberg, and M. Harper. 2005. Using conditional random fields for sentence boundary detection in speech. In Proc. of ACL’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Matusov</author>
<author>A Mauser</author>
<author>H Ney</author>
</authors>
<title>Automatic sentence segmentation and punctuation prediction for spoken language translation.</title>
<date>2006</date>
<booktitle>In Proc. of IWSLT’06.</booktitle>
<contexts>
<context position="6326" citStr="Matusov et al., 2006" startWordPosition="957" endWordPosition="960">olkit (Stolcke, 2002). It was widely used in many recent spoken language translation tasks as either a preprocessing (Wang et al., 2008) or postprocessing (Kirchhoff and Yang, 2007) step. More details about this model will be given in the next section. Recently, there are also several research efforts that try to optimize some downstream application after punctuation prediction, rather than the prediction task itself. Examples of such downstream applications include punctuation prediction for part-ofspeech (POS) tagging and name tagging (Hillard et al., 2006), statistical machine translation (Matusov et al., 2006), and information extraction (Favre et al., 2008). 3 Hidden Event Language Model Many previous research efforts consider the boundary detection and punctuation insertion task as a hidden event detection task. One such well-known approach was introduced by Stolcke et al. (1998). They adopted a HMM to describe a joint distribution over words and interword events, where the observations are the words, and the word/event pairs are encoded as hidden states. Specifically, in this task word boundaries and punctuation symbols are encoded as interword events. The training phase involves training an n-g</context>
</contexts>
<marker>Matusov, Mauser, Ney, 2006</marker>
<rawString>E. Matusov, A. Mauser, and H. Ney. 2006. Automatic sentence segmentation and punctuation prediction for spoken language translation. In Proc. of IWSLT’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>K Rohanimanesh</author>
<author>C Sutton</author>
</authors>
<title>Dynamic conditional random fields for jointly labeling multiple sequences.</title>
<date>2003</date>
<booktitle>In Proc. of NIPS’03 Workshop on Syntax, Semantics and Statistics.</booktitle>
<contexts>
<context position="15946" citStr="McCallum et al., 2003" startWordPosition="2510" endWordPosition="2513">ually requires known long range dependencies in advance and may not be readily applicable to our task where such clues are not explicit. As we have discussed above, since we would like to jointly model both the word-level labeling task and the sentence-level annotation task (sentence boundary detection and sentence type prediction), introducing an additional layer of tags to perform both tasks together would be desirable. In this section, we propose the use of factorial CRF (F-CRF) (Sutton et al., 2007), which has previously been shown to be effective for joint labeling of multiple sequences (McCallum et al., 2003). 180 The F-CRF as a specific case of dynamic conditional random fields was originally motivated from dynamic Bayesian networks, where an identical structure repeats over different time steps. Analogous to the linear-chain CRF, one can think of the FCRF as a framework that provides the capability of simultaneously labeling multiple layers of tags for a given sequence. It learns a joint conditional distribution of the tags given the observation. Formally, dynamic conditional random fields define the conditional probability of a sequence of label vectors y given the observation x as: exp E E E λ</context>
</contexts>
<marker>McCallum, Rohanimanesh, Sutton, 2003</marker>
<rawString>A. McCallum, K. Rohanimanesh, and C. Sutton. 2003. Dynamic conditional random fields for jointly labeling multiple sequences. In Proc. of NIPS’03 Workshop on Syntax, Semantics and Statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL’03.</booktitle>
<contexts>
<context position="32227" citStr="Och, 2003" startWordPosition="5158" endWordPosition="5159"> the official IWSLT05 evaluation set where the correct punctuation symbols are present. Evaluations are performed on the ASR outputs of the IWSLT08 BTEC evaluation dataset, with punctuation symbols inserted by each punctuation prediction method. The tuning set and evaluation set include 7 reference translations. Following a common practice in statistical machine translation, we report BLEU-4 scores (Papineni et al., 2002), which were shown to have good correlation with human judgments, with the closest reference length as the effective reference length. The minimum error rate training (MERT) (Och, 2003) procedure is used for tuning the model parameters of the translation system. Due to the unstable nature of MERT, we perform 10 runs for each translation task, with a different random initialization of parameters in each run, and report the BLEU4 scores averaged over 10 runs. The results are reported in Table 7. The best translation performances for both translation directions are achieved by applying F-CRF as the punctuation prediction model to the ASR texts. Such improvements are observed to be consistent over different runs. The improvement of F-CRF over LCRF in translation quality is stati</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F.J. Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of ACL’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL’02.</booktitle>
<contexts>
<context position="32042" citStr="Papineni et al., 2002" startWordPosition="5128" endWordPosition="5131">ormance than simple distance-based reordering (Koehn et al., 2005). Specifically, the default lexicalized reordering model (msd-bidirectional-fe) is used. For tuning the parameters of Moses, we use the official IWSLT05 evaluation set where the correct punctuation symbols are present. Evaluations are performed on the ASR outputs of the IWSLT08 BTEC evaluation dataset, with punctuation symbols inserted by each punctuation prediction method. The tuning set and evaluation set include 7 reference translations. Following a common practice in statistical machine translation, we report BLEU-4 scores (Papineni et al., 2002), which were shown to have good correlation with human judgments, with the closest reference length as the effective reference length. The minimum error rate training (MERT) (Och, 2003) procedure is used for tuning the model parameters of the translation system. Due to the unstable nature of MERT, we perform 10 runs for each translation task, with a different random initialization of parameters in each run, and report the BLEU4 scores averaged over 10 runs. The results are reported in Table 7. The best translation performances for both translation directions are achieved by applying F-CRF as t</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. of ACL’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Paul</author>
</authors>
<title>Overview of the IWSLT</title>
<date>2008</date>
<booktitle>In Proc. of IWSLT’08.</booktitle>
<contexts>
<context position="29113" citStr="Paul, 2008" startWordPosition="4683" endWordPosition="4684"> evaluated punctuation prediction performance on transcribed texts consisting of correctly recognized words. We now present the evaluation results on texts produced by ASR systems. For evaluation, we use the 1-best ASR outputs of spontaneous speech of the official IWSLT08 BTEC evaluation dataset, which is released as part of the IWSLT09 corpus. The dataset consists of 504 utterances in Chinese, and 498 in English. Unlike the correctly recognized texts described in Section 6.1, the ASR outputs contain substantial recognition errors (recognition accuracy is 86% for Chinese, and 80% for English (Paul, 2008)). In the dataset released by the IWSLT organizers, the correct punctuation symbols are not annotated in the ASR outputs. To conduct our experimental evaluation, we manually annotated the correct punctuation symbols on the ASR outputs. We used all the learned models in Section 6.1, and applied them to this dataset. The evaluation results are shown in Table 6. The results show that F-CRF still gives higher performance than L-CRF and the hidden event language model, and the improvements are statistically significant (p &lt; 0.01). 6.3 Performance in Translation The evaluation process as described i</context>
</contexts>
<marker>Paul, 2008</marker>
<rawString>M. Paul. 2008. Overview of the IWSLT 2008 evaluation campaign. In Proc. of IWSLT’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Paul</author>
</authors>
<title>Overview of the IWSLT</title>
<date>2009</date>
<booktitle>In Proc. of IWSLT’09.</booktitle>
<contexts>
<context position="2124" citStr="Paul, 2009" startWordPosition="304" endWordPosition="305">ost previous punctuation prediction techniques, developed mostly by the speech processing community, exploit both lexical and prosodic cues. However, in order to fully exploit prosodic features such as pitch and pause duration, it is necessary to have access to the original raw speech waveforms. In some scenarios where further natural language processing (NLP) tasks on the transcribed speech texts become the main concern, speech prosody information may not be readily available. For example, in the recent evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) (Paul, 2009), only manually transcribed or automatically recognized speech texts are provided but the original raw speech waveforms are not available. In this paper, we tackle the task of predicting punctuation symbols from a standard text processing perspective, where only the speech texts are available, without relying on additional prosodic features such as pitch and pause duration. Specifically, we perform the punctuation prediction task on transcribed conversational speech texts, using the IWSLT corpus (Paul, 2009) as the evaluation data. Different from many other corpora such as broadcast news corpo</context>
<context position="19511" citStr="Paul, 2009" startWordPosition="3109" endWordPosition="3110">rance as QMARK, given the dependencies between the two layers existing at each time step. In practice, during the learning process, the two layers of tags are jointly learned, thus providing evidences that influence each other’s tagging process. In this work, we use the GRMM package (Sutton, 2006) for building both the linear-chain CRF (LCRF) and factorial CRF (F-CRF). The tree-based reparameterization (TRP) schedule for belief propagation (Wainwright et al., 2001) is used for approximate inference. 6 Experiments We perform experiments on part of the corpus of the IWSLT09 evaluation campaign (Paul, 2009), where both Chinese and English conversational speech z1 y1 y2 z2 y3 z3 x3 x1 x2 . . . . . . xn yn zn 1 pλ(y|x) = Z(x) 181 Sentence: no , please do not. would you save your questions for the end of my talk, when i ask for them ? no please do not would you ... my talk when ... them COMMA NONE NONE PERIOD NONE NONE ... NONE COMMA NONE ... QMARK DEBEG DEIN DEIN DEIN QNBEG QNIN ... QNIN QNIN QNIN ... QNIN Figure 4: An example tagging of a training sentence for the factorial CRF texts are used. Two multilingual datasets are considered, the BTEC (Basic Travel Expression Corpus) dataset and the CT (</context>
</contexts>
<marker>Paul, 2009</marker>
<rawString>M. Paul. 2009. Overview of the IWSLT 2009 evaluation campaign. In Proc. of IWSLT’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sarawagi</author>
<author>W W Cohen</author>
</authors>
<title>Semi-Markov conditional random fields for information extraction.</title>
<date>2005</date>
<booktitle>In Proc. of NIPS’05.</booktitle>
<contexts>
<context position="14535" citStr="Sarawagi and Cohen, 2005" startWordPosition="2281" endWordPosition="2284">annotated at the sentence level, rather than relying on a limited window of surrounding words. A model that can jointly perform sentence segmentation and sentence type prediction, together with word level punctuation prediction would be more beneficial for our task. This motivates us to build a joint model for performing such a task, to be presented in the next section. 5 Factorial Conditional Random Fields Extensions to the linear-chain CRF model have been proposed in previous research efforts to encode long range dependencies. One such well-known extension is the semi-Markov CRF (semi-CRF) (Sarawagi and Cohen, 2005). Motivated by the hidden semiMarkov model, the semi-CRF is particularly helpful in text chunking tasks as it allows a state to persist for a certain interval of time steps. This in practice often leads to better modeling capability of chunks, since state transitions within a chunk need not precisely follow the Markov property as in the case of linear-chain CRF. However, it is not clear how such a model can benefit our task, which requires wordlevel labeling in addition to sentence boundary detection and sentence type prediction. The skip-chain CRF (Sutton and McCallum, 2004), another variant </context>
</contexts>
<marker>Sarawagi, Cohen, 2005</marker>
<rawString>S. Sarawagi and W.W. Cohen. 2005. Semi-Markov conditional random fields for information extraction. In Proc. of NIPS’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sha</author>
<author>F Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAACL’03.</booktitle>
<marker>Sha, Pereira, 2003</marker>
<rawString>F. Sha and F. Pereira. 2003. Shallow parsing with conditional random fields. In Proc. of HLT-NAACL’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
<author>E Shriberg</author>
<author>R Bates</author>
<author>M Ostendorf</author>
<author>D Hakkani</author>
<author>M Plauche</author>
<author>G Tur</author>
<author>Y Lu</author>
</authors>
<title>Automatic detection of sentence boundaries and disfluencies based on recognized words.</title>
<date>1998</date>
<booktitle>In Proc. of ICSLP’98.</booktitle>
<contexts>
<context position="5464" citStr="Stolcke et al. (1998)" startWordPosition="828" endWordPosition="831">task, by making use of conditional random fields (CRF) (Lafferty et al., 2001). Their method was shown to improve over a previous method based on hidden Markov model (HMM). There is relatively less work that exploited lexical features only. Beeferman et al. (1998) focused on comma prediction with a trigram language model. A joint language model was learned from punctuated texts, and commas were inserted so as to maximize the joint probability score. Recent work by Gravano et al. (2009) presented a purely n-gram based approach that jointly predicted punctuation and case information of English. Stolcke et al. (1998) presented a “hidden event language model” that treated boundary detection and punctuation insertion as an interword hidden event detection task. Their proposed method was implemented in the handy utility hidden-ngram as part of the SRILM toolkit (Stolcke, 2002). It was widely used in many recent spoken language translation tasks as either a preprocessing (Wang et al., 2008) or postprocessing (Kirchhoff and Yang, 2007) step. More details about this model will be given in the next section. Recently, there are also several research efforts that try to optimize some downstream application after p</context>
</contexts>
<marker>Stolcke, Shriberg, Bates, Ostendorf, Hakkani, Plauche, Tur, Lu, 1998</marker>
<rawString>A. Stolcke, E. Shriberg, R. Bates, M. Ostendorf, D. Hakkani, M. Plauche, G. Tur, and Y. Lu. 1998. Automatic detection of sentence boundaries and disfluencies based on recognized words. In Proc. of ICSLP’98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM–an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. of ICSLP’02.</booktitle>
<contexts>
<context position="5726" citStr="Stolcke, 2002" startWordPosition="868" endWordPosition="869">sed on comma prediction with a trigram language model. A joint language model was learned from punctuated texts, and commas were inserted so as to maximize the joint probability score. Recent work by Gravano et al. (2009) presented a purely n-gram based approach that jointly predicted punctuation and case information of English. Stolcke et al. (1998) presented a “hidden event language model” that treated boundary detection and punctuation insertion as an interword hidden event detection task. Their proposed method was implemented in the handy utility hidden-ngram as part of the SRILM toolkit (Stolcke, 2002). It was widely used in many recent spoken language translation tasks as either a preprocessing (Wang et al., 2008) or postprocessing (Kirchhoff and Yang, 2007) step. More details about this model will be given in the next section. Recently, there are also several research efforts that try to optimize some downstream application after punctuation prediction, rather than the prediction task itself. Examples of such downstream applications include punctuation prediction for part-ofspeech (POS) tagging and name tagging (Hillard et al., 2006), statistical machine translation (Matusov et al., 2006)</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM–an extensible language modeling toolkit. In Proc. of ICSLP’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sutton</author>
<author>A McCallum</author>
</authors>
<title>Collective segmentation and labeling of distant entities in information extraction.</title>
<date>2004</date>
<booktitle>In Proc. of ICML’04 workshop on Statistical Relational Learning.</booktitle>
<contexts>
<context position="15117" citStr="Sutton and McCallum, 2004" startWordPosition="2379" endWordPosition="2382">ov CRF (semi-CRF) (Sarawagi and Cohen, 2005). Motivated by the hidden semiMarkov model, the semi-CRF is particularly helpful in text chunking tasks as it allows a state to persist for a certain interval of time steps. This in practice often leads to better modeling capability of chunks, since state transitions within a chunk need not precisely follow the Markov property as in the case of linear-chain CRF. However, it is not clear how such a model can benefit our task, which requires wordlevel labeling in addition to sentence boundary detection and sentence type prediction. The skip-chain CRF (Sutton and McCallum, 2004), another variant of linear-chain CRF, attaches additional edges on top of a linear-chain CRF for better modeling of long range dependencies between states with similar observations. However, such a model usually requires known long range dependencies in advance and may not be readily applicable to our task where such clues are not explicit. As we have discussed above, since we would like to jointly model both the word-level labeling task and the sentence-level annotation task (sentence boundary detection and sentence type prediction), introducing an additional layer of tags to perform both ta</context>
</contexts>
<marker>Sutton, McCallum, 2004</marker>
<rawString>C. Sutton and A. McCallum. 2004. Collective segmentation and labeling of distant entities in information extraction. In Proc. of ICML’04 workshop on Statistical Relational Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sutton</author>
<author>A McCallum</author>
<author>K Rohanimanesh</author>
</authors>
<title>Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data.</title>
<date>2007</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>8</volume>
<contexts>
<context position="12196" citStr="Sutton et al., 2007" startWordPosition="1904" endWordPosition="1907">fers to inserting the corresponding punctuation symbol. In the testing phase, the most probable sequence of tags is ... yn xn y1 x1 x2 y2 x3 y3 179 Sentence: no , please do not. would you save your questions for the end of my talk, when i ask for them ? no please do not would you ... my talk when ... them COMMA NONE NONE PERIOD NONE NONE ... NONE COMMA NONE ... QMARK Figure 2: An example tagging of a training sentence for the linear-chain CRF predicted and the punctuated text can then be constructed from such an output. An example tagging of an utterance is illustrated in Figure 2. Following (Sutton et al., 2007), we factorize a feature of conditional random fields as a product of a binary function on assignment of the set of cliques at the current time step (in this case an edge), and a feature function solely defined on the observation sequence. n-gram occurrences surrounding the current word, together with position information, are used as binary feature functions, for n = 1, 2, 3. All words that appear within 5 words from the current word are considered when building the features. Special start and end symbols are used beyond the utterance boundaries. For example, for the word do shown in Figure 2</context>
<context position="15832" citStr="Sutton et al., 2007" startWordPosition="2492" endWordPosition="2495">or better modeling of long range dependencies between states with similar observations. However, such a model usually requires known long range dependencies in advance and may not be readily applicable to our task where such clues are not explicit. As we have discussed above, since we would like to jointly model both the word-level labeling task and the sentence-level annotation task (sentence boundary detection and sentence type prediction), introducing an additional layer of tags to perform both tasks together would be desirable. In this section, we propose the use of factorial CRF (F-CRF) (Sutton et al., 2007), which has previously been shown to be effective for joint labeling of multiple sequences (McCallum et al., 2003). 180 The F-CRF as a specific case of dynamic conditional random fields was originally motivated from dynamic Bayesian networks, where an identical structure repeats over different time steps. Analogous to the linear-chain CRF, one can think of the FCRF as a framework that provides the capability of simultaneously labeling multiple layers of tags for a given sequence. It learns a joint conditional distribution of the tags given the observation. Formally, dynamic conditional random </context>
</contexts>
<marker>Sutton, McCallum, Rohanimanesh, 2007</marker>
<rawString>C. Sutton, A. McCallum, and K. Rohanimanesh. 2007. Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data. Journal of Machine Learning Research, 8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sutton</author>
</authors>
<date>2006</date>
<note>GRMM: GRaphical Models in Mallet. http://mallet.cs.umass.edu/grmm/.</note>
<contexts>
<context position="19198" citStr="Sutton, 2006" startWordPosition="3061" endWordPosition="3062">in Figure 4. Intuitively, when evidences show that the utterance consists of two sentences – a declarative sentence followed by a question sentence, the model tends to annotate the second half of the utterance with the sequence QNBEG QNIN .... This in turn helps to predict the word level tag at the end of the utterance as QMARK, given the dependencies between the two layers existing at each time step. In practice, during the learning process, the two layers of tags are jointly learned, thus providing evidences that influence each other’s tagging process. In this work, we use the GRMM package (Sutton, 2006) for building both the linear-chain CRF (LCRF) and factorial CRF (F-CRF). The tree-based reparameterization (TRP) schedule for belief propagation (Wainwright et al., 2001) is used for approximate inference. 6 Experiments We perform experiments on part of the corpus of the IWSLT09 evaluation campaign (Paul, 2009), where both Chinese and English conversational speech z1 y1 y2 z2 y3 z3 x3 x1 x2 . . . . . . xn yn zn 1 pλ(y|x) = Z(x) 181 Sentence: no , please do not. would you save your questions for the end of my talk, when i ask for them ? no please do not would you ... my talk when ... them COMM</context>
</contexts>
<marker>Sutton, 2006</marker>
<rawString>C. Sutton. 2006. GRMM: GRaphical Models in Mallet. http://mallet.cs.umass.edu/grmm/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Tseng</author>
<author>P Chang</author>
<author>G Andrew</author>
<author>D Jurafsky</author>
<author>C Manning</author>
</authors>
<title>A conditional random field word segmenter for sighan bakeoff</title>
<date>2005</date>
<booktitle>In Proc. of the Fourth SIGHAN Workshop on Chinese Language Processing.</booktitle>
<contexts>
<context position="10035" citStr="Tseng et al., 2005" startWordPosition="1523" endWordPosition="1526">e techniques based on conditional random fields to tackle the difficulties due to long range dependencies. 4 Linear-Chain Conditional Random Fields One natural approach to relax the strong dependency assumptions encoded by the hidden event language model is to adopt an undirected graphical model, where arbitrary overlapping features can be exploited. Conditional random fields (CRF) (Lafferty et al., 2001) have been widely used in various sequence labeling and segmentation tasks (Sha and Pereira, 1http://mastarpj.nict.go.jp/IWSLT2008/downloads/ case+punc tool using SRILM.instructions.txt 2003; Tseng et al., 2005). Unlike a HMM which models the joint distribution of both the label sequence and the observation, a CRF is a discriminative model of the conditional distribution of the complete label sequence given the observation. Specifically, a first-order linear-chain CRF which assumes first-order Markov property is defined by the following equation: pλ(y|x) = Z1 1 ) exp EEAkfk(x,yt−1,yt,t) t k (1) where x is the observation and y is the label sequence. Feature functions fk with time step t are defined over the entire observation x and two adjacent hidden labels. Z(x) is a normalization factor to ensure </context>
</contexts>
<marker>Tseng, Chang, Andrew, Jurafsky, Manning, 2005</marker>
<rawString>H. Tseng, P. Chang, G. Andrew, D. Jurafsky, and C. Manning. 2005. A conditional random field word segmenter for sighan bakeoff 2005. In Proc. of the Fourth SIGHAN Workshop on Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wainwright</author>
<author>T Jaakkola</author>
<author>A Willsky</author>
</authors>
<title>Treebased reparameterization for approximate inference on loopy graphs.</title>
<date>2001</date>
<booktitle>In Proc. of NIPS’01.</booktitle>
<contexts>
<context position="19369" citStr="Wainwright et al., 2001" startWordPosition="3084" endWordPosition="3087">ends to annotate the second half of the utterance with the sequence QNBEG QNIN .... This in turn helps to predict the word level tag at the end of the utterance as QMARK, given the dependencies between the two layers existing at each time step. In practice, during the learning process, the two layers of tags are jointly learned, thus providing evidences that influence each other’s tagging process. In this work, we use the GRMM package (Sutton, 2006) for building both the linear-chain CRF (LCRF) and factorial CRF (F-CRF). The tree-based reparameterization (TRP) schedule for belief propagation (Wainwright et al., 2001) is used for approximate inference. 6 Experiments We perform experiments on part of the corpus of the IWSLT09 evaluation campaign (Paul, 2009), where both Chinese and English conversational speech z1 y1 y2 z2 y3 z3 x3 x1 x2 . . . . . . xn yn zn 1 pλ(y|x) = Z(x) 181 Sentence: no , please do not. would you save your questions for the end of my talk, when i ask for them ? no please do not would you ... my talk when ... them COMMA NONE NONE PERIOD NONE NONE ... NONE COMMA NONE ... QMARK DEBEG DEIN DEIN DEIN QNBEG QNIN ... QNIN QNIN QNIN ... QNIN Figure 4: An example tagging of a training sentence </context>
</contexts>
<marker>Wainwright, Jaakkola, Willsky, 2001</marker>
<rawString>M. Wainwright, T. Jaakkola, and A. Willsky. 2001. Treebased reparameterization for approximate inference on loopy graphs. In Proc. of NIPS’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Wang</author>
<author>H Wu</author>
<author>X Hu</author>
<author>Z Liu</author>
<author>J Li</author>
<author>D Ren</author>
<author>Z Niu</author>
</authors>
<title>The TCH machine translation system for IWSLT</title>
<date>2008</date>
<booktitle>In Proc. of IWSLT’08.</booktitle>
<contexts>
<context position="5841" citStr="Wang et al., 2008" startWordPosition="886" endWordPosition="889">, and commas were inserted so as to maximize the joint probability score. Recent work by Gravano et al. (2009) presented a purely n-gram based approach that jointly predicted punctuation and case information of English. Stolcke et al. (1998) presented a “hidden event language model” that treated boundary detection and punctuation insertion as an interword hidden event detection task. Their proposed method was implemented in the handy utility hidden-ngram as part of the SRILM toolkit (Stolcke, 2002). It was widely used in many recent spoken language translation tasks as either a preprocessing (Wang et al., 2008) or postprocessing (Kirchhoff and Yang, 2007) step. More details about this model will be given in the next section. Recently, there are also several research efforts that try to optimize some downstream application after punctuation prediction, rather than the prediction task itself. Examples of such downstream applications include punctuation prediction for part-ofspeech (POS) tagging and name tagging (Hillard et al., 2006), statistical machine translation (Matusov et al., 2006), and information extraction (Favre et al., 2008). 3 Hidden Event Language Model Many previous research efforts con</context>
</contexts>
<marker>Wang, Wu, Hu, Liu, Li, Ren, Niu, 2008</marker>
<rawString>H. Wang, H. Wu, X. Hu, Z. Liu, J. Li, D. Ren, and Z. Niu. 2008. The TCH machine translation system for IWSLT 2008. In Proc. of IWSLT’08.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>