<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002455">
<title confidence="0.995995">
A comparison of models of word meaning in context
</title>
<author confidence="0.983925">
Georgiana Dinu Stefan Thater Sören Laue
</author>
<affiliation confidence="0.942331">
Universität des Saarlandes Universität des Saarlandes Friedrich-Schiller Universität
Saarbrücken, Germany Saarbrücken, Germany Jena, Germany
</affiliation>
<email confidence="0.960051">
dinu@coli.uni-saarland.de stth@coli.uni-saarland.de soeren.laue@uni-jena.de
</email>
<sectionHeader confidence="0.996976" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999789">
This paper compares a number of recently pro-
posed models for computing context sensitive
word similarity. We clarify the connections
between these models, simplify their formula-
tion and evaluate them in a unified setting. We
show that the models are essentially equivalent
if syntactic information is ignored, and that the
substantial performance differences previously
reported disappear to a large extent when these
simplified variants are evaluated under identi-
cal conditions. Furthermore, our reformulation
allows for the design of a straightforward and
fast implementation.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99987745">
The computation of semantic similarity scores be-
tween words is an important sub-task for a variety
of NLP applications (Turney and Pantel, 2010). One
standard approach is to exploit the so-called distribu-
tional hypothesis that similar words tend to appear
in similar contexts: Word meaning is represented by
the contexts in which a word occurs, and semantic
similarity is computed by comparing these contexts
in a high-dimensional vector space.
Such distributional models of word meaning are
attractive because they are simple, have wide cover-
age, and can be easily acquired in an unsupervised
way. Ambiguity, however, is a fundamental problem:
when encountering a word in context, we want a dis-
tributional representation which reflects its meaning
in this specific context. For instance, while buy and
acquire are similar when we consider them in iso-
lation, they do not convey the same meaning when
acquire occurs in students acquire knowledge. This
is particularly difficult for vector space models which
compute a single type vector summing up over all
occurrences of a word. This vector mixes all of a
word’s usages and makes no distinctions between
its—potentially very diverse—senses.
Several proposals have been made in the recent
literature to address this problem. Type-based meth-
ods combine the (type) vector of the target with the
vectors of the surrounding context words to obtain
a disambiguated representation. In recent work, this
has been proposed by Mitchell and Lapata (2008),
Erk and Padó (2008) and Thater et al. (2010; 2011),
which differ in the choice of input vector representa-
tion and in the combination operation they propose.
A different approach has been taken by Erk and
Padó (2010), Reisinger and Mooney (2010) and
Reddy et al. (2011), who make use of token vectors
for individual occurrences of a word, rather than us-
ing the already mixed type vectors. Generally speak-
ing, these methods “select” a set of token vectors
of the target, which are similar to the current con-
text, and use only these to obtain a disambiguated
representation.
Yet another approach has been taken by Dinu and
Lapata (2010), Ó Séaghdha and Korhonen (2011)
and Van de Cruys et al. (2011), who propose to use
latent variable models. Conceptually, this comes
close to token-based models, however their approach
is more unitary as they attempt to recover a hidden
layer which best explains the observation data.
In this paper, we focus on the first group of ap-
proaches and investigate the precise differences be-
tween the three models of Erk and Padó and Thater et
al., out of which (Thater et al., 2011) achieves state of
the art results on a standard data set. Despite the fact
that these models exploit similar intuitions, both their
formal presentations and the results obtained vary to
a great extent. The answer given in this paper is sur-
prising: the three models are essentially equivalent if
syntactic information is ignored; in a syntactic space
the three methods implement only slightly different
</bodyText>
<page confidence="0.904724333333333">
611
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 611–615,
Montr´eal, Canada, June 3-8, 2012. c�2012 Association for Computational Linguistics
</page>
<bodyText confidence="0.999966857142857">
intuitions. We clarify these connections, simplify
the syntactic variants originally proposed and reduce
them to straightforward matrix operations, and evalu-
ate them in a unified experimental setting. We obtain
significantly better results than originally reported in
the literature. Our reformulation also also supports
efficient implementations for these methods.
</bodyText>
<sectionHeader confidence="0.989625" genericHeader="method">
2 Models for meaning in context
</sectionHeader>
<bodyText confidence="0.999565333333333">
We consider the following problem: we are given
an occurrence of a target word and want to obtain a
vector that reflects its meaning in the given context.
To simplify the presentation, we restrict ourselves to
contexts consisting of a single word, and use acquire
in context knowledge as a running example.
</bodyText>
<listItem confidence="0.944376666666667">
EP08. Erk and Padó (2008) compute a contextu-
alized vector for acquire by combining its type vec-
tor (~w) with the inverse selectional preference vector
of knowledge (c). This is simply the centroid of the
vectors of all words that take knowledge as direct
object (r):
</listItem>
<equation confidence="0.978132">
(1)
!f (w0,r,c) · ~w0×~w
</equation>
<bodyText confidence="0.99983">
where f (w0,r,c) denotes the co-occurrence associa-
tion between the context word c and words w0 related
to c by grammatical relation r in a training corpus;
n is the number of words w0 and × denotes a vector
composition operation. In this paper, we take × to be
point-wise multiplication, which is reported to work
best in many studies in the literature.
TFP10. Thater et al. (2010) also compute contex-
tualized vectors by combing the vectors of the target
word and of its context. In contrast to EP08, however,
they use second order vectors as basic representation
for the target word.
</bodyText>
<equation confidence="0.923941266666667">
)
f (w
)
r
r
,
(2)
0
·
0,
0,w00
!~e
,
0
w00
</equation>
<bodyText confidence="0.978198428571429">
That is, the vector for a target word w has components
for all combinations of two grammatical roles r,r0 and
a context word w0; the inner sum gives the value for
each component.
The contextualized vector for acquire is obtained
through pointwise multiplication with the (1st-order)
vector for knowledge
</bodyText>
<equation confidence="0.8294658">
=
v(w,r,c)
w~×Lr(~c) (3)
=
f
is a first order vector
for the context word; the
map&amp;quot;
maps
this vector to
c~
∑r0,w0
(c,r0,w0)~e(r0,w0)
“lifting
Lr(~c)
</equation>
<bodyText confidence="0.9246218">
∑r0,w0 f (c,r0,w0)~e(r,r0,w0) to make it com-
patible with ~w.
Thater et al. (2011) take a slightly different
perspective on contextualization. Instead of comb-
ing vector representations for the target word and its
context directly, they propose to re-weight the vector
components of the target word, based on distribu-
tional similari
TFP11.
ty with the context word:
</bodyText>
<equation confidence="0.996982966666667">
v(w,r,c) = ∑ a(r,c,r0,w0)· f(w,r0,w0)·~e(r0,w0) (4)
r0,w0
E
(w, c) = n
f
,
v
P08
∑~
(w0
c) · ~w0~ ×~w
w0
1 h&lt;~c, ~w1&gt;,...,&lt;~c, ~wn&gt;i ×~w
n
1
n
� f (w0,c) · h f (w0,w1),...i� ×~w
f (w0,c) · f (w0,w1),...i ×~w
n1∑
w0
1 h∑
nw0
1
v(w,r,c) = ∑
n
w0
w~= ∑
r,r0,w00
∑f (w, r,
w0
</equation>
<bodyText confidence="0.913835">
(~c), which has to be “lifted” first
to make the two vectors comparable:
where
</bodyText>
<figure confidence="0.970229714285714">
is simply
if r an
a(r,c,r0,w0)
cos(~c,~w0)
d r0
denote the same grammatical function, else 0.
3 Comparison
</figure>
<bodyText confidence="0.808013666666667">
The models presented above have a number of things
in common: they all use syntactic information and
vectors to represent word meaning in
context. Yet, their formal presentations differ substan-
tially. We now show that the models are essentially
equivalent if we ignore syntax: they component-wise
multiply the second order vector of one word (target
or context) with the first order vector of the other
word. Specifically, we obtain the following deriva-
tions, where W =
denotes the vocabu-
lary, and V the symmetric n
n input matrix, where
= f(wi,wj) gives the co-occurrence association
between words wi and wj
</bodyText>
<figure confidence="0.915523833333333">
“secondorder”
{w1,...,wn}
×
Vij
:
~c V ×~w
</figure>
<page confidence="0.939556">
612
</page>
<equation confidence="0.992486846153846">
vTFP10(w,c) = E E �f (w,w&apos;) · f (w&apos;,w&apos;&apos;) ew&apos;&apos; xe Model GAP A Literature
w&apos;&apos;EW w&apos;EW
EP08 46.6 + 14.4 (32.2)*
= ( E f(w,w&apos;)f(w&apos;,w1),...) x�c TFP10 48.3 + 3.9 (44.4)
w&apos;EW TFP11 51.8 f0.0
= (&lt;W, IV1&gt;,...,&lt;W, IVn&gt;) xe TFP10+11 52.1 N/A
= w�V xe
vTFP11(w,c) = E a(c,w&apos;) · f(w,w&apos;) ·ew&apos;
w&apos;EW
= (a(w1,c)· f(w,w1),...)
= (a(w1,c),...) xw (*)
= (&lt;W1,e&gt;,...,&lt;In,e&gt;) xW
= e V xw
</equation>
<bodyText confidence="0.999983090909091">
where &lt;V,w&gt; denotes scalar product. In step (*), we
assume that a(w,c) denotes the scalar product of w�
ande, instead of cosine similarity, as TFP11. This is
justified if we assume that all vectors are normalized,
in which case the two are identical.
As it can be observed the syntax-free variants of
EP08 and TFP11 are identical up to the choice in
normalization. TFP10 proposes an identical model to
that of TFP11, however with a different interpretation,
in which the roles of the context word and of the
target word are swapped.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.99994352631579">
We have just shown that EP08, TFP10 and TFP11
are essentially equivalent to each other if syntactic
information is ignored, hence it is a bit surprising that
performance results reported in the literature vary
to such a great extent. In this section we consider
syntactic variants of these methods and we show that
performance differences previously reported can only
partly be explained by the different ways syntactic
information is used: when we simplify these models
and evaluate them under identical conditions, the
differences between them disappear to a large extent.
To evaluate the three models, we reimplemented
them using matrix operations similar to the ones used
in Section 3, where we made few simplifications
to the TFP10 and EP08 models: we follow TFP11
and we use component-wise multiplication to com-
bine the target with one context word, and add the
resulting composed vectors when given more con-
text words1. Furthermore for TFP10, we change the
</bodyText>
<footnote confidence="0.861336">
1Note that some of the parameters in the EP08 method (omit-
</footnote>
<tableCaption confidence="0.997203">
Table 1: GAP scores LST data.
</tableCaption>
<bodyText confidence="0.680372833333333">
* The best available GAP score for this model (from Erk and
Padó (2010)) is reported only on a subset of the data - this subset
is however judged by the authors to be “easier” than the entire
data; all other methods are tested on the entire dataset.
treatment of syntax in the line of the much simpler
proposal of TFP11. Specifically:
</bodyText>
<equation confidence="0.9997695">
v(w,r,c) = Lr (VVT )w,: xVc,: (TFP10)
v(w,r,c) = Vw,: x Lr(VVT )c,: (TFP11)
</equation>
<bodyText confidence="0.970947241379311">
where V is a I x J syntactic input matrix, i.e. the
columns are (word, relation) pairs. For simplification,
the columns of V are reordered such that syntactic
relations form continuous regions. Lr is a lifting map
similar to that of Equation (3) as it maps I- into J-
dimensional vectors: the resulting vector is equal to
the original one in the column region of relation r,
while everything else is 0. In the above equations we
use the standard Matlab notation, Vw,: denoting a row
vector in matrix V.
We evaluate these models on a paraphrase ranking
task, using the SemEval 2007 Lexical Substitution
Task (LST) dataset: the models are given a target
word in context plus a list of potential synonyms
(substitution candidates) ranging over all senses of
the target word. The models have to decide to what
extent each substitution candidate is a synonym of
the target in the given context. We omit the precise de-
scription of the evaluation setting here, as we follow
the methodology described in Thater et al. (2011).
Results are shown in Table 1, where the first col-
umn gives the GAP (Generalized Average Precision)
score of the model and the second column gives
the difference to the result reported in the literature.
TFP10 and EP08 perform much better than the origi-
nal proposals, as we obtain very significant gains of
4 and 14 GAP points.
ted in the brief presentation in Section 2), which are difficult to
tune (Erk and Padó (2009)), disappear this way.
</bodyText>
<page confidence="0.99828">
613
</page>
<bodyText confidence="0.99998474">
We can observe that the differences between the
three methods, when simplified and tested in an uni-
fied setting, largely disappear. This is to be expected
as all three methods implement very similar, all moti-
vated intuitions: TFP11 reweights the vector of the
target acquire with the second order vector of the
context knowledge, i.e. with the vector of similarities
of knowledge to all other words in the vocabulary.
TFP10 takes a complementary approach: it reweights
the vector of knowledge with the second order vector
of acquire. In both these methods, anything outside
the object (object−1 respectively) region of the space,
is set to 0. The variant of EP08 that we implement is
very similar to TFP11, however it compares knowl-
edge to all other words in the vocabulary only using
occurrences as objects while TFP11 takes all syntac-
tic relations into account.
Note that TFP10 and TFP11 operate on comple-
mentary syntactic regions of the vectors. For this
reason the two models can be trivially combined.
The combined model (TFP10+11) achieves even bet-
ter results: the difference to TFP11 is small, however
statistically significant at level p &lt; 0.05.
Implementation details. Straightforward imple-
mentations of the three models are computationally
expensive, as they all use “second order” vectors to
implement contextualization of a target word. Our re-
formulation in terms of matrix operations allows for
efficient implementations, which take advantage of
the sparsity of the input matrix V: contextualization
of a target word runs in O(nnz(V)), where nnz is the
number of non-zero entries. Note that ranking not
only a small set of predefined substitution candidates,
as in the experiment above, but also ranking the en-
tire vocabulary runs in O(nnz(V)). On this task, this
overall running time is in fact identical to that of sim-
pler methods such as those of Mitchell and Lapata
(2008).
In our experiments, we use GigaWord to extract
a syntactic input matrix V of size Pz� 2M x 7M. V is
only 4.5x10−06 dense. Note that because of the sim-
ple operations involved, we do not need to compute
or store the entire VVT matrix, which is much denser
than V (we have estimated order of 1010 entries). The
sparsity of V allows for very efficient computations
in practice: the best single model, TFP11, runs in
less than 0.2s/0.4s per LST instance, for ranking the
candidate list/entire vocabulary in a Python imple-
mentation using scipy.sparse, on a standard 1GHz
processor.
</bodyText>
<sectionHeader confidence="0.999413" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999913851851852">
In this paper, we have compared three related vec-
tor space models of word meaning in context. We
have reformulated the models and showed that they
are in fact very similar. We also showed that the
different performances reported in the literature are
only to some extent due to the differences in the
models: We evaluated simplified variants of these
and obtained results which are (much) better than
previously reported, bringing the three models much
closer together in terms of performance. Aside from
clarifying the precise relationship between the three
models under consideration, our reformulation has
the additional benefit of allowing the design of a
straightforward and efficient implementation.
Finally, our focus on these methods is justified by
their clear advantages over other classes of models:
unlike token-based or latent variable methods, they
are much simpler and require no parameter tuning.
Furthermore, they also obtain state of the art results
on the paraphrase ranking task, outperforming other
simple type-based methods (see (Van de Cruys et
al., 2011) and (Ó Séaghdha and Korhonen, 2011) for
results of other methods on this data).
Acknowledgments. This work was partially sup-
ported by the Cluster of Excellence “Multimodal
Computing and Interaction&amp;quot;, funded by the German
Excellence Initiative.
</bodyText>
<sectionHeader confidence="0.999461" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997263538461539">
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings of
EMNLP 2010, Cambridge, MA.
Katrin Erk and Sebastian Padó. 2008. A structured vector
space model for word meaning in context. In Proceed-
ings of EMNLP 2008, Honolulu, HI, USA.
Katrin Erk and Sebastian Padó. 2009. Paraphrase assess-
ment in structured vector space: Exploring parameters
and datasets. In Proceedings of the Workshop on Geo-
metrical Models of Natural Language Semantics.
Katrin Erk and Sebastian Padó. 2010. Exemplar-based
models for word meaning in context. In Proceedings
of ACL 2010 Short Papers, Uppsala, Sweden.
</reference>
<page confidence="0.984019">
614
</page>
<reference confidence="0.999846846153846">
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, Columbus, OH, USA.
Diarmuid Ó Séaghdha and Anna Korhonen. 2011. Prob-
abilistic models of similarity in syntactic context. In
Proceedings of EMNLP 2011.
Siva Reddy, Ioannis Klapaftis, Diana McCarthy, and
Suresh Manandhar. 2011. Dynamic and static pro-
totype vectors for semantic composition. In Proc. of
IJCNLP 2011.
Joseph Reisinger and Raymond J. Mooney. 2010. Multi-
prototype vector-space models of word meaning. In
Proceedings of NAACL 2010, Los Angeles, California.
Stefan Thater, Hagen Fürstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations using
syntactically enriched vector models. In Proceedings
of ACL 2010, Uppsala, Sweden.
Stefan Thater, Hagen Fürstenau, and Manfred Pinkal.
2011. Word meaning in context: A simple and effective
vector model. In Proceedings of IJCNLP 2011.
Peter D. Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space modes of semantics. Journal
of Artificial Intelligence Research, 37:141–188.
Tim Van de Cruys, Thierry Poibeau, and Anna Korhonen.
2011. Latent vector weighting for word meaning in
context. In Proceedings of EMNLP 2011.
</reference>
<page confidence="0.99856">
615
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.877537">
<title confidence="0.992803">A comparison of models of word meaning in context</title>
<author confidence="0.990551">Georgiana Dinu Stefan Thater Sören Laue</author>
<affiliation confidence="0.998074">Universität des Saarlandes Universität des Saarlandes Friedrich-Schiller Universität</affiliation>
<address confidence="0.973295">Saarbrücken, Germany Saarbrücken, Germany Jena, Germany</address>
<email confidence="0.953041">dinu@coli.uni-saarland.destth@coli.uni-saarland.desoeren.laue@uni-jena.de</email>
<abstract confidence="0.997158285714286">This paper compares a number of recently proposed models for computing context sensitive word similarity. We clarify the connections between these models, simplify their formulation and evaluate them in a unified setting. We show that the models are essentially equivalent if syntactic information is ignored, and that the substantial performance differences previously reported disappear to a large extent when these simplified variants are evaluated under identical conditions. Furthermore, our reformulation allows for the design of a straightforward and fast implementation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Mirella Lapata</author>
</authors>
<title>Measuring distributional similarity in context.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP 2010,</booktitle>
<location>Cambridge, MA.</location>
<contexts>
<context position="3038" citStr="Dinu and Lapata (2010)" startWordPosition="459" endWordPosition="462">ó (2008) and Thater et al. (2010; 2011), which differ in the choice of input vector representation and in the combination operation they propose. A different approach has been taken by Erk and Padó (2010), Reisinger and Mooney (2010) and Reddy et al. (2011), who make use of token vectors for individual occurrences of a word, rather than using the already mixed type vectors. Generally speaking, these methods “select” a set of token vectors of the target, which are similar to the current context, and use only these to obtain a disambiguated representation. Yet another approach has been taken by Dinu and Lapata (2010), Ó Séaghdha and Korhonen (2011) and Van de Cruys et al. (2011), who propose to use latent variable models. Conceptually, this comes close to token-based models, however their approach is more unitary as they attempt to recover a hidden layer which best explains the observation data. In this paper, we focus on the first group of approaches and investigate the precise differences between the three models of Erk and Padó and Thater et al., out of which (Thater et al., 2011) achieves state of the art results on a standard data set. Despite the fact that these models exploit similar intuitions, bo</context>
</contexts>
<marker>Dinu, Lapata, 2010</marker>
<rawString>Georgiana Dinu and Mirella Lapata. 2010. Measuring distributional similarity in context. In Proceedings of EMNLP 2010, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Padó</author>
</authors>
<title>A structured vector space model for word meaning in context.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP 2008,</booktitle>
<location>Honolulu, HI, USA.</location>
<contexts>
<context position="2424" citStr="Erk and Padó (2008)" startWordPosition="353" endWordPosition="356">when acquire occurs in students acquire knowledge. This is particularly difficult for vector space models which compute a single type vector summing up over all occurrences of a word. This vector mixes all of a word’s usages and makes no distinctions between its—potentially very diverse—senses. Several proposals have been made in the recent literature to address this problem. Type-based methods combine the (type) vector of the target with the vectors of the surrounding context words to obtain a disambiguated representation. In recent work, this has been proposed by Mitchell and Lapata (2008), Erk and Padó (2008) and Thater et al. (2010; 2011), which differ in the choice of input vector representation and in the combination operation they propose. A different approach has been taken by Erk and Padó (2010), Reisinger and Mooney (2010) and Reddy et al. (2011), who make use of token vectors for individual occurrences of a word, rather than using the already mixed type vectors. Generally speaking, these methods “select” a set of token vectors of the target, which are similar to the current context, and use only these to obtain a disambiguated representation. Yet another approach has been taken by Dinu and</context>
<context position="4875" citStr="Erk and Padó (2008)" startWordPosition="746" endWordPosition="749">o straightforward matrix operations, and evaluate them in a unified experimental setting. We obtain significantly better results than originally reported in the literature. Our reformulation also also supports efficient implementations for these methods. 2 Models for meaning in context We consider the following problem: we are given an occurrence of a target word and want to obtain a vector that reflects its meaning in the given context. To simplify the presentation, we restrict ourselves to contexts consisting of a single word, and use acquire in context knowledge as a running example. EP08. Erk and Padó (2008) compute a contextualized vector for acquire by combining its type vector (~w) with the inverse selectional preference vector of knowledge (c). This is simply the centroid of the vectors of all words that take knowledge as direct object (r): (1) !f (w0,r,c) · ~w0×~w where f (w0,r,c) denotes the co-occurrence association between the context word c and words w0 related to c by grammatical relation r in a training corpus; n is the number of words w0 and × denotes a vector composition operation. In this paper, we take × to be point-wise multiplication, which is reported to work best in many studie</context>
</contexts>
<marker>Erk, Padó, 2008</marker>
<rawString>Katrin Erk and Sebastian Padó. 2008. A structured vector space model for word meaning in context. In Proceedings of EMNLP 2008, Honolulu, HI, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Padó</author>
</authors>
<title>Paraphrase assessment in structured vector space: Exploring parameters and datasets.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics.</booktitle>
<contexts>
<context position="11513" citStr="Erk and Padó (2009)" startWordPosition="1908" endWordPosition="1911">ubstitution candidate is a synonym of the target in the given context. We omit the precise description of the evaluation setting here, as we follow the methodology described in Thater et al. (2011). Results are shown in Table 1, where the first column gives the GAP (Generalized Average Precision) score of the model and the second column gives the difference to the result reported in the literature. TFP10 and EP08 perform much better than the original proposals, as we obtain very significant gains of 4 and 14 GAP points. ted in the brief presentation in Section 2), which are difficult to tune (Erk and Padó (2009)), disappear this way. 613 We can observe that the differences between the three methods, when simplified and tested in an unified setting, largely disappear. This is to be expected as all three methods implement very similar, all motivated intuitions: TFP11 reweights the vector of the target acquire with the second order vector of the context knowledge, i.e. with the vector of similarities of knowledge to all other words in the vocabulary. TFP10 takes a complementary approach: it reweights the vector of knowledge with the second order vector of acquire. In both these methods, anything outside</context>
</contexts>
<marker>Erk, Padó, 2009</marker>
<rawString>Katrin Erk and Sebastian Padó. 2009. Paraphrase assessment in structured vector space: Exploring parameters and datasets. In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Padó</author>
</authors>
<title>Exemplar-based models for word meaning in context.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL 2010 Short Papers,</booktitle>
<location>Uppsala,</location>
<contexts>
<context position="2620" citStr="Erk and Padó (2010)" startWordPosition="387" endWordPosition="390">mixes all of a word’s usages and makes no distinctions between its—potentially very diverse—senses. Several proposals have been made in the recent literature to address this problem. Type-based methods combine the (type) vector of the target with the vectors of the surrounding context words to obtain a disambiguated representation. In recent work, this has been proposed by Mitchell and Lapata (2008), Erk and Padó (2008) and Thater et al. (2010; 2011), which differ in the choice of input vector representation and in the combination operation they propose. A different approach has been taken by Erk and Padó (2010), Reisinger and Mooney (2010) and Reddy et al. (2011), who make use of token vectors for individual occurrences of a word, rather than using the already mixed type vectors. Generally speaking, these methods “select” a set of token vectors of the target, which are similar to the current context, and use only these to obtain a disambiguated representation. Yet another approach has been taken by Dinu and Lapata (2010), Ó Séaghdha and Korhonen (2011) and Van de Cruys et al. (2011), who propose to use latent variable models. Conceptually, this comes close to token-based models, however their approa</context>
<context position="9737" citStr="Erk and Padó (2010)" startWordPosition="1595" endWordPosition="1598"> conditions, the differences between them disappear to a large extent. To evaluate the three models, we reimplemented them using matrix operations similar to the ones used in Section 3, where we made few simplifications to the TFP10 and EP08 models: we follow TFP11 and we use component-wise multiplication to combine the target with one context word, and add the resulting composed vectors when given more context words1. Furthermore for TFP10, we change the 1Note that some of the parameters in the EP08 method (omitTable 1: GAP scores LST data. * The best available GAP score for this model (from Erk and Padó (2010)) is reported only on a subset of the data - this subset is however judged by the authors to be “easier” than the entire data; all other methods are tested on the entire dataset. treatment of syntax in the line of the much simpler proposal of TFP11. Specifically: v(w,r,c) = Lr (VVT )w,: xVc,: (TFP10) v(w,r,c) = Vw,: x Lr(VVT )c,: (TFP11) where V is a I x J syntactic input matrix, i.e. the columns are (word, relation) pairs. For simplification, the columns of V are reordered such that syntactic relations form continuous regions. Lr is a lifting map similar to that of Equation (3) as it maps I- </context>
</contexts>
<marker>Erk, Padó, 2010</marker>
<rawString>Katrin Erk and Sebastian Padó. 2010. Exemplar-based models for word meaning in context. In Proceedings of ACL 2010 Short Papers, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<location>Columbus, OH, USA.</location>
<contexts>
<context position="2403" citStr="Mitchell and Lapata (2008)" startWordPosition="349" endWordPosition="352">not convey the same meaning when acquire occurs in students acquire knowledge. This is particularly difficult for vector space models which compute a single type vector summing up over all occurrences of a word. This vector mixes all of a word’s usages and makes no distinctions between its—potentially very diverse—senses. Several proposals have been made in the recent literature to address this problem. Type-based methods combine the (type) vector of the target with the vectors of the surrounding context words to obtain a disambiguated representation. In recent work, this has been proposed by Mitchell and Lapata (2008), Erk and Padó (2008) and Thater et al. (2010; 2011), which differ in the choice of input vector representation and in the combination operation they propose. A different approach has been taken by Erk and Padó (2010), Reisinger and Mooney (2010) and Reddy et al. (2011), who make use of token vectors for individual occurrences of a word, rather than using the already mixed type vectors. Generally speaking, these methods “select” a set of token vectors of the target, which are similar to the current context, and use only these to obtain a disambiguated representation. Yet another approach has b</context>
<context position="13429" citStr="Mitchell and Lapata (2008)" startWordPosition="2219" endWordPosition="2222"> they all use “second order” vectors to implement contextualization of a target word. Our reformulation in terms of matrix operations allows for efficient implementations, which take advantage of the sparsity of the input matrix V: contextualization of a target word runs in O(nnz(V)), where nnz is the number of non-zero entries. Note that ranking not only a small set of predefined substitution candidates, as in the experiment above, but also ranking the entire vocabulary runs in O(nnz(V)). On this task, this overall running time is in fact identical to that of simpler methods such as those of Mitchell and Lapata (2008). In our experiments, we use GigaWord to extract a syntactic input matrix V of size Pz� 2M x 7M. V is only 4.5x10−06 dense. Note that because of the simple operations involved, we do not need to compute or store the entire VVT matrix, which is much denser than V (we have estimated order of 1010 entries). The sparsity of V allows for very efficient computations in practice: the best single model, TFP11, runs in less than 0.2s/0.4s per LST instance, for ranking the candidate list/entire vocabulary in a Python implementation using scipy.sparse, on a standard 1GHz processor. 5 Conclusions In this </context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL-08: HLT, Columbus, OH, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid Ó Séaghdha</author>
<author>Anna Korhonen</author>
</authors>
<title>Probabilistic models of similarity in syntactic context.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context position="3070" citStr="Séaghdha and Korhonen (2011)" startWordPosition="464" endWordPosition="467"> (2010; 2011), which differ in the choice of input vector representation and in the combination operation they propose. A different approach has been taken by Erk and Padó (2010), Reisinger and Mooney (2010) and Reddy et al. (2011), who make use of token vectors for individual occurrences of a word, rather than using the already mixed type vectors. Generally speaking, these methods “select” a set of token vectors of the target, which are similar to the current context, and use only these to obtain a disambiguated representation. Yet another approach has been taken by Dinu and Lapata (2010), Ó Séaghdha and Korhonen (2011) and Van de Cruys et al. (2011), who propose to use latent variable models. Conceptually, this comes close to token-based models, however their approach is more unitary as they attempt to recover a hidden layer which best explains the observation data. In this paper, we focus on the first group of approaches and investigate the precise differences between the three models of Erk and Padó and Thater et al., out of which (Thater et al., 2011) achieves state of the art results on a standard data set. Despite the fact that these models exploit similar intuitions, both their formal presentations an</context>
</contexts>
<marker>Séaghdha, Korhonen, 2011</marker>
<rawString>Diarmuid Ó Séaghdha and Anna Korhonen. 2011. Probabilistic models of similarity in syntactic context. In Proceedings of EMNLP 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siva Reddy</author>
<author>Ioannis Klapaftis</author>
<author>Diana McCarthy</author>
<author>Suresh Manandhar</author>
</authors>
<title>Dynamic and static prototype vectors for semantic composition.</title>
<date>2011</date>
<booktitle>In Proc. of IJCNLP</booktitle>
<contexts>
<context position="2673" citStr="Reddy et al. (2011)" startWordPosition="396" endWordPosition="399">s between its—potentially very diverse—senses. Several proposals have been made in the recent literature to address this problem. Type-based methods combine the (type) vector of the target with the vectors of the surrounding context words to obtain a disambiguated representation. In recent work, this has been proposed by Mitchell and Lapata (2008), Erk and Padó (2008) and Thater et al. (2010; 2011), which differ in the choice of input vector representation and in the combination operation they propose. A different approach has been taken by Erk and Padó (2010), Reisinger and Mooney (2010) and Reddy et al. (2011), who make use of token vectors for individual occurrences of a word, rather than using the already mixed type vectors. Generally speaking, these methods “select” a set of token vectors of the target, which are similar to the current context, and use only these to obtain a disambiguated representation. Yet another approach has been taken by Dinu and Lapata (2010), Ó Séaghdha and Korhonen (2011) and Van de Cruys et al. (2011), who propose to use latent variable models. Conceptually, this comes close to token-based models, however their approach is more unitary as they attempt to recover a hidde</context>
</contexts>
<marker>Reddy, Klapaftis, McCarthy, Manandhar, 2011</marker>
<rawString>Siva Reddy, Ioannis Klapaftis, Diana McCarthy, and Suresh Manandhar. 2011. Dynamic and static prototype vectors for semantic composition. In Proc. of IJCNLP 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Reisinger</author>
<author>Raymond J Mooney</author>
</authors>
<title>Multiprototype vector-space models of word meaning.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL 2010,</booktitle>
<location>Los Angeles, California.</location>
<contexts>
<context position="2649" citStr="Reisinger and Mooney (2010)" startWordPosition="391" endWordPosition="394"> usages and makes no distinctions between its—potentially very diverse—senses. Several proposals have been made in the recent literature to address this problem. Type-based methods combine the (type) vector of the target with the vectors of the surrounding context words to obtain a disambiguated representation. In recent work, this has been proposed by Mitchell and Lapata (2008), Erk and Padó (2008) and Thater et al. (2010; 2011), which differ in the choice of input vector representation and in the combination operation they propose. A different approach has been taken by Erk and Padó (2010), Reisinger and Mooney (2010) and Reddy et al. (2011), who make use of token vectors for individual occurrences of a word, rather than using the already mixed type vectors. Generally speaking, these methods “select” a set of token vectors of the target, which are similar to the current context, and use only these to obtain a disambiguated representation. Yet another approach has been taken by Dinu and Lapata (2010), Ó Séaghdha and Korhonen (2011) and Van de Cruys et al. (2011), who propose to use latent variable models. Conceptually, this comes close to token-based models, however their approach is more unitary as they at</context>
</contexts>
<marker>Reisinger, Mooney, 2010</marker>
<rawString>Joseph Reisinger and Raymond J. Mooney. 2010. Multiprototype vector-space models of word meaning. In Proceedings of NAACL 2010, Los Angeles, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Thater</author>
<author>Hagen Fürstenau</author>
<author>Manfred Pinkal</author>
</authors>
<title>Contextualizing semantic representations using syntactically enriched vector models.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL 2010,</booktitle>
<location>Uppsala,</location>
<contexts>
<context position="2448" citStr="Thater et al. (2010" startWordPosition="358" endWordPosition="361">tudents acquire knowledge. This is particularly difficult for vector space models which compute a single type vector summing up over all occurrences of a word. This vector mixes all of a word’s usages and makes no distinctions between its—potentially very diverse—senses. Several proposals have been made in the recent literature to address this problem. Type-based methods combine the (type) vector of the target with the vectors of the surrounding context words to obtain a disambiguated representation. In recent work, this has been proposed by Mitchell and Lapata (2008), Erk and Padó (2008) and Thater et al. (2010; 2011), which differ in the choice of input vector representation and in the combination operation they propose. A different approach has been taken by Erk and Padó (2010), Reisinger and Mooney (2010) and Reddy et al. (2011), who make use of token vectors for individual occurrences of a word, rather than using the already mixed type vectors. Generally speaking, these methods “select” a set of token vectors of the target, which are similar to the current context, and use only these to obtain a disambiguated representation. Yet another approach has been taken by Dinu and Lapata (2010), Ó Séaghd</context>
<context position="5523" citStr="Thater et al. (2010)" startWordPosition="860" endWordPosition="863">ctor for acquire by combining its type vector (~w) with the inverse selectional preference vector of knowledge (c). This is simply the centroid of the vectors of all words that take knowledge as direct object (r): (1) !f (w0,r,c) · ~w0×~w where f (w0,r,c) denotes the co-occurrence association between the context word c and words w0 related to c by grammatical relation r in a training corpus; n is the number of words w0 and × denotes a vector composition operation. In this paper, we take × to be point-wise multiplication, which is reported to work best in many studies in the literature. TFP10. Thater et al. (2010) also compute contextualized vectors by combing the vectors of the target word and of its context. In contrast to EP08, however, they use second order vectors as basic representation for the target word. ) f (w ) r r , (2) 0 · 0, 0,w00 !~e , 0 w00 That is, the vector for a target word w has components for all combinations of two grammatical roles r,r0 and a context word w0; the inner sum gives the value for each component. The contextualized vector for acquire is obtained through pointwise multiplication with the (1st-order) vector for knowledge = v(w,r,c) w~×Lr(~c) (3) = f is a first order ve</context>
</contexts>
<marker>Thater, Fürstenau, Pinkal, 2010</marker>
<rawString>Stefan Thater, Hagen Fürstenau, and Manfred Pinkal. 2010. Contextualizing semantic representations using syntactically enriched vector models. In Proceedings of ACL 2010, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Thater</author>
<author>Hagen Fürstenau</author>
<author>Manfred Pinkal</author>
</authors>
<title>Word meaning in context: A simple and effective vector model.</title>
<date>2011</date>
<booktitle>In Proceedings of IJCNLP</booktitle>
<contexts>
<context position="3514" citStr="Thater et al., 2011" startWordPosition="542" endWordPosition="545"> the current context, and use only these to obtain a disambiguated representation. Yet another approach has been taken by Dinu and Lapata (2010), Ó Séaghdha and Korhonen (2011) and Van de Cruys et al. (2011), who propose to use latent variable models. Conceptually, this comes close to token-based models, however their approach is more unitary as they attempt to recover a hidden layer which best explains the observation data. In this paper, we focus on the first group of approaches and investigate the precise differences between the three models of Erk and Padó and Thater et al., out of which (Thater et al., 2011) achieves state of the art results on a standard data set. Despite the fact that these models exploit similar intuitions, both their formal presentations and the results obtained vary to a great extent. The answer given in this paper is surprising: the three models are essentially equivalent if syntactic information is ignored; in a syntactic space the three methods implement only slightly different 611 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 611–615, Montr´eal, Canada, June 3-8, 2012. c�2012 Association</context>
<context position="6305" citStr="Thater et al. (2011)" startWordPosition="999" endWordPosition="1002">basic representation for the target word. ) f (w ) r r , (2) 0 · 0, 0,w00 !~e , 0 w00 That is, the vector for a target word w has components for all combinations of two grammatical roles r,r0 and a context word w0; the inner sum gives the value for each component. The contextualized vector for acquire is obtained through pointwise multiplication with the (1st-order) vector for knowledge = v(w,r,c) w~×Lr(~c) (3) = f is a first order vector for the context word; the map&amp;quot; maps this vector to c~ ∑r0,w0 (c,r0,w0)~e(r0,w0) “lifting Lr(~c) ∑r0,w0 f (c,r0,w0)~e(r,r0,w0) to make it compatible with ~w. Thater et al. (2011) take a slightly different perspective on contextualization. Instead of combing vector representations for the target word and its context directly, they propose to re-weight the vector components of the target word, based on distributional similari TFP11. ty with the context word: v(w,r,c) = ∑ a(r,c,r0,w0)· f(w,r0,w0)·~e(r0,w0) (4) r0,w0 E (w, c) = n f , v P08 ∑~ (w0 c) · ~w0~ ×~w w0 1 h&lt;~c, ~w1&gt;,...,&lt;~c, ~wn&gt;i ×~w n 1 n � f (w0,c) · h f (w0,w1),...i� ×~w f (w0,c) · f (w0,w1),...i ×~w n1∑ w0 1 h∑ nw0 1 v(w,r,c) = ∑ n w0 w~= ∑ r,r0,w00 ∑f (w, r, w0 (~c), which has to be “lifted” first to make </context>
<context position="11091" citStr="Thater et al. (2011)" startWordPosition="1833" endWordPosition="1836"> is 0. In the above equations we use the standard Matlab notation, Vw,: denoting a row vector in matrix V. We evaluate these models on a paraphrase ranking task, using the SemEval 2007 Lexical Substitution Task (LST) dataset: the models are given a target word in context plus a list of potential synonyms (substitution candidates) ranging over all senses of the target word. The models have to decide to what extent each substitution candidate is a synonym of the target in the given context. We omit the precise description of the evaluation setting here, as we follow the methodology described in Thater et al. (2011). Results are shown in Table 1, where the first column gives the GAP (Generalized Average Precision) score of the model and the second column gives the difference to the result reported in the literature. TFP10 and EP08 perform much better than the original proposals, as we obtain very significant gains of 4 and 14 GAP points. ted in the brief presentation in Section 2), which are difficult to tune (Erk and Padó (2009)), disappear this way. 613 We can observe that the differences between the three methods, when simplified and tested in an unified setting, largely disappear. This is to be expec</context>
</contexts>
<marker>Thater, Fürstenau, Pinkal, 2011</marker>
<rawString>Stefan Thater, Hagen Fürstenau, and Manfred Pinkal. 2011. Word meaning in context: A simple and effective vector model. In Proceedings of IJCNLP 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space modes of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>37--141</pages>
<contexts>
<context position="1054" citStr="Turney and Pantel, 2010" startWordPosition="137" endWordPosition="140">fy the connections between these models, simplify their formulation and evaluate them in a unified setting. We show that the models are essentially equivalent if syntactic information is ignored, and that the substantial performance differences previously reported disappear to a large extent when these simplified variants are evaluated under identical conditions. Furthermore, our reformulation allows for the design of a straightforward and fast implementation. 1 Introduction The computation of semantic similarity scores between words is an important sub-task for a variety of NLP applications (Turney and Pantel, 2010). One standard approach is to exploit the so-called distributional hypothesis that similar words tend to appear in similar contexts: Word meaning is represented by the contexts in which a word occurs, and semantic similarity is computed by comparing these contexts in a high-dimensional vector space. Such distributional models of word meaning are attractive because they are simple, have wide coverage, and can be easily acquired in an unsupervised way. Ambiguity, however, is a fundamental problem: when encountering a word in context, we want a distributional representation which reflects its mea</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space modes of semantics. Journal of Artificial Intelligence Research, 37:141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Van de Cruys</author>
<author>Thierry Poibeau</author>
<author>Anna Korhonen</author>
</authors>
<title>Latent vector weighting for word meaning in context.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<marker>Van de Cruys, Poibeau, Korhonen, 2011</marker>
<rawString>Tim Van de Cruys, Thierry Poibeau, and Anna Korhonen. 2011. Latent vector weighting for word meaning in context. In Proceedings of EMNLP 2011.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>