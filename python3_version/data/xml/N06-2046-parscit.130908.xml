<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000227">
<title confidence="0.985842">
Improved Affinity Graph Based Multi-Document Summarization
</title>
<author confidence="0.969834">
Xiaojun Wan, Jianwu Yang
</author>
<affiliation confidence="0.645479">
Institute of Computer Science and Technology, Peking University
Beijing 100871, China
</affiliation>
<email confidence="0.931434">
lwanxiaojun, yangjianwu}@icst.pku.edu.cn
</email>
<sectionHeader confidence="0.992839" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999944333333334">
This paper describes an affinity graph
based approach to multi-document sum-
marization. We incorporate a diffusion
process to acquire semantic relationships
between sentences, and then compute in-
formation richness of sentences by a
graph rank algorithm on differentiated in-
tra-document links and inter-document
links between sentences. A greedy algo-
rithm is employed to impose diversity
penalty on sentences and the sentences
with both high information richness and
high information novelty are chosen into
the summary. Experimental results on
task 2 of DUC 2002 and task 2 of DUC
2004 demonstrate that the proposed ap-
proach outperforms existing state-of-the-
art systems.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999903583333334">
Automated multi-document summarization has
drawn much attention in recent years. Multi-
document summary is usually used to provide con-
cise topic description about a cluster of documents
and facilitate the users to browse the document
cluster. A particular challenge for multi-document
summarization is that the information stored in
different documents inevitably overlaps with each
other, and hence we need effective summarization
methods to merge information stored in different
documents, and if possible, contrast their differ-
ences.
A variety of multi-document summarization
methods have been developed recently. In this
study, we focus on extractive summarization,
which involves assigning saliency scores to some
units (e.g. sentences, paragraphs) of the documents
and extracting the sentences with highest scores.
MEAD is an implementation of the centroid-based
method (Radev et al., 2004) that scores sentences
based on sentence-level and inter-sentence features,
including cluster centroids, position, TF*IDF, etc.
NeATS (Lin and Hovy, 2002) selects important
content using sentence position, term frequency,
topic signature and term clustering, and then uses
MMR (Goldstein et al., 1999) to remove redun-
dancy. XDoX (Hardy et al., 1998) identifies the
most salient themes within the set by passage clus-
tering and then composes an extraction summary,
which reflects these main themes. Harabagiu and
Lacatusu (2005) investigate different topic repre-
sentations and extraction methods.
Graph-based methods have been proposed to
rank sentences or passages. Websumm (Mani and
Bloedorn, 2000) uses a graph-connectivity model
and operates under the assumption that nodes
which are connected to many other nodes are likely
to carry salient information. LexPageRank (Erkan
and Radev, 2004) is an approach for computing
sentence importance based on the concept of ei-
genvector centrality. Mihalcea and Tarau (2005)
also propose similar algorithms based on PageR-
ank and HITS to compute sentence importance for
document summarization.
In this study, we extend the above graph-based
works by proposing an integrated framework for
considering both information richness and infor-
mation novelty of a sentence based on sentence
affinity graph. First, a diffusion process is imposed
on sentence affinity graph in order to make the af-
finity graph reflect true semantic relationships be-
tween sentences. Second, intra-document links and
inter-document links between sentences are differ-
entiated to attach more importance to inter-
document links for sentence information richness
computation. Lastly, a diversity penalty process is
imposed on sentences to penalize redundant sen-
tences. Experiments on DUC 2002 and DUC 2004
data are performed and we obtain encouraging re-
sults and conclusions.
</bodyText>
<page confidence="0.970329">
181
</page>
<note confidence="0.9285505">
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 181–184,
New York, June 2006. c�2006 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.788527" genericHeader="method">
2 The Affinity Graph Based Approach
</sectionHeader>
<bodyText confidence="0.999918916666667">
The proposed affinity graph based summarization
method consists of three steps: (1) an affinity graph
is built to reflect the semantic relationship between
sentences in the document set; (2) information
richness of each sentence is computed based on the
affinity graph; (3) based on the affinity graph and
the information richness scores, diversity penalty is
imposed to sentences and the affinity rank score
for each sentence is obtained to reflect both infor-
mation richness and information novelty of the
sentence. The sentences with high affinity rank
scores are chosen to produce the summary.
</bodyText>
<subsectionHeader confidence="0.998395">
2.1 Affinity Graph Building
</subsectionHeader>
<bodyText confidence="0.999945782608695">
Given a sentence collection S={si  |1&lt;i&lt;n}, the af-
finity weight aff(si, sj) between a sentence pair of si
and sj is calculated using the cosine measure. The
weight associated with term t is calculated with the
tft*isft formula, where tft is the frequency of term t
in the corresponding sentence and isft is the inverse
sentence frequency of term t, i.e. 1+log(N/nt),
where N is the total number of sentences and nt is
the number of sentences containing term t. If sen-
tences are considered as nodes, the sentence collec-
tion can be modeled as an undirected graph by
generating the link between two sentences if their
affinity weight exceeds 0, i.e. an undirected link
between si and sj (ij) with affinity weight aff(si,sj)
is constructed if aff(si,sj)&gt;0; otherwise no link is
constructed. Thus, we construct an undirected
graph G reflecting the semantic relationship be-
tween sentences by their content similarity. The
graph is called as Affinity Graph. We use an adja-
cency (affinity) matrix M to describe the affinity
graph with each entry corresponding to the weight
of a link in the graph. M = (Mi,j)nxn is defined as
follows:
</bodyText>
<subsectionHeader confidence="0.559883">
Mi, j aff(si, sj) (1)
</subsectionHeader>
<bodyText confidence="0.999976904761905">
Then M is normalized to make the sum of each
row equal to 1. Note that we use the same notation
to denote a matrix and its normalized matrix.
However, the affinity weight between two sen-
tences in the affinity graph is currently computed
simply based on their own content similarity and
ignore the affinity diffusion process on the graph.
Other than the direct link between two sentences,
the possible paths with more than two steps be-
tween the sentences in the graph also convey more
or less semantic relationship. In order to acquire
the implicit semantic relationship between sen-
tences, we apply a diffusion process (Kandola et
al., 2002) on the graph to obtain a more appropri-
ate affinity matrix. Though the number of possible
paths between any two given nodes can grow ex-
ponentially, recent spectral graph theory (Kondor
and Lafferty, 2002) shows that it is possible to
compute the affinity between any two given nodes
efficiently without examining all possible paths.
The diffusion process on the graph is as follows:
</bodyText>
<equation confidence="0.993364666666667">
t 1
- (2)
t
M � J
¦f M
t 1
</equation>
<bodyText confidence="0.999964">
where Y(0&lt;Y&lt;1) is the decay factor set to 0.9.
is the t-th power of the initial affinity matrix
and the entry in it is given by
</bodyText>
<equation confidence="0.973311833333333">
) = d
InfoRich(s j)
M j
+ (1—d) (4)
�
i
¦

i
allz
j
i
</equation>
<bodyText confidence="0.944543785714285">
And the matrix form is:
that is the sum of the products of the weights over
all paths of length t that start at node i and finish at
node jin the graph on the examples. If the entries
satisfy that they are all positive and for each node
the sum of the connections is 1, we can view the
entry as the probability that a random walk begin-
ning at node i reaches node jafter t steps. The ma-
tri
x M is normalized to make the sum of each row
equal to 1. t is limited to 5 in this study.
The computation of information richness of sen-
tences is based on the following three intuitions: 1)
the more neighbors a sentence has, the more in-
formative it is; 2) the more informative a sen-
tence&apos;s neighbors are, the more informative it is; 3)
the more heavily a sentence is linked with other
informative sentences, the more informative it is.
Based on the above intuitions, the information
richness score
for a sentence
can
InfoRich(si)
si
be
deduced from those of all other sentences linked
with it and it can be formulated in a recursive form
as follows:
</bodyText>
<equation confidence="0.919325142857143">
InfoRich(s
n
&amp; d T &amp; � &amp;
� (1 d)
O O M � e(5)
n
t 1
</equation>
<figure confidence="0.9710253">
�
¦ 
u {1,..., n} t &amp;quot; 1

u i , u j
1 t
2.2 Information Richness Computation
t
M
j
i,
1
(3)
�
Mu
&amp;quot; &amp;quot;
,u
t
M
M
</figure>
<page confidence="0.980924">
182
</page>
<bodyText confidence="0.981111">
where A= [InfoRich(si )]nx1 is the eigenvector of
</bodyText>
<equation confidence="0.754988">
M e&amp;
</equation>
<bodyText confidence="0.99786625">
� T . is a unit vector with all elements equaling
to 1. d is the damping factor set to 0.85.
Note that given a link between a sentence pair of
si and sj, if si and sj comes from the same document,
the link is an intra-document link; and if si and sj
comes from different documents, the link is an in-
ter-document link. We believe that inter-document
links are more important than intra-document links
for information richness computation. Different
weights are assigned to intra-document links and
inter-document links respectively, and the new af-
finity matrix is:
</bodyText>
<equation confidence="0.83343925">
M� = aM intra + PAM inter (6)
�
replaced by in Equations (4) and (5).
M
</equation>
<subsectionHeader confidence="0.99713">
2.3 Diversity Penalty Imposition
</subsectionHeader>
<bodyText confidence="0.849128875">
Based on the affinity graph and obtained informa-
tion richness scores, a greedy algorithm is applied
to impose the diversity penalty and compute the
final affinity rank scores of sentences as follows:
1. Initialize two sets A=O, B={si  |i=1,2,...,n}, and
each sentence&apos;s affinity rank score is initialized to
its information richness score, i.e. ARScore(si) =
InfoRich(si), i=1,2,...n.
</bodyText>
<listItem confidence="0.93615">
2. Sort the sentences in B by their current affinity rank
scores in descending order.
3. Suppose si is the highest ranked sentence, i.e. the
</listItem>
<bodyText confidence="0.909777">
first sentence in the ranked list. Move sentence si
from B to A, and then a diversity penalty is im-
posed to the affinity rank score of each sentence
linked with si as follows:
For each sentence sj in B, we have
</bodyText>
<equation confidence="0.994003">
ARScore(s) = ARScore(s) - Ȧ • Mj i • InfoRich(si) (7)
</equation>
<bodyText confidence="0.9032589">
where Ȧ&gt;0 is the penalty degree factor. The larger
Ȧ is, the greater penalty is imposed to the affinity
rank score. If Ȧ=0, no diversity penalty is imposed
at all.
4. Go to step 2 and iterate until B= 0 or the iteration
count reaches a predefined maximum number.
After the affinity rank scores are obtained for all
sentences, the sentences with highest affinity rank
scores are chosen to produce the summary accord-
ing to the summary length limit.
</bodyText>
<sectionHeader confidence="0.990313" genericHeader="conclusions">
3 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999442470588235">
We compare our system with top 3 performing
systems and two baseline systems on task 2 of
DUC 2002 and task 4 of DUC 2004 respectively.
ROUGE (Lin and Hovy, 2003) metrics is used for
evaluation1 and we mainly concern about ROUGE-
1. The parameters of our system are tuned on DUC
2001 as follows: Ȧ=7, Į=0.3 and ȕ=1.
We can see from the tables that our system out-
performs the top performing systems and baseline
systems on both DUC 2002 and DUC 2004 tasks
over all three metrics. The performance improve-
ment achieved by our system results from three
factors: diversity penalty imposition, intra-
document and inter-document link differentiation
and diffusion process incorporation. The ROUGE-
1 contributions of the above three factors are
0.02200, 0.00268 and 0.00043 respectively.
</bodyText>
<table confidence="0.993493857142857">
System ROUGE-1 ROUGE-2 ROUGE-W
Our System 0.38125 0.08196 0.12390
S26 0.35151 0.07642 0.11448
S19 0.34504 0.07936 0.11332
S28 0.34355 0.07521 0.10956
Coverage Baseline 0.32894 0.07148 0.10847
Lead Baseline 0.28684 0.05283 0.09525
</table>
<tableCaption confidence="0.998261">
Table 1. System comparison on task 2 of DUC 2002
</tableCaption>
<table confidence="0.998972285714286">
System ROUGE-1 ROUGE-2 ROUGE-W
Our System 0.41102 0.09738 0.12560
S65 0.38232 0.09219 0.11528
S104 0.37436 0.08544 0.11305
S35 0.37427 0.08364 0.11561
Coverage Baseline 0.34882 0.07189 0.10622
Lead Baseline 0.32420 0.06409 0.09905
</table>
<tableCaption confidence="0.999405">
Table 2. System comparison on task 2 of DUC 2004
</tableCaption>
<bodyText confidence="0.973062125">
Figures 1-4 show the influence of the parameters
in our system. Note that Į: ȕ denotes the real val-
ues Į and ȕ are set to. &amp;quot;w/ diffusion&amp;quot; is the system
with the diffusion process (our system) and &amp;quot;w/o
diffusion&amp;quot; is the system without the diffusion proc-
�
where intra
M is the affinity matrix containing only
the intra-document links (the entries of inter-
document links are set to 0) and inter
M is the affin-
ity matrix containing only the inter-document links
(the entries of intra-document links are set to 0). Į,
ȕ are weighting parameters and we let 0Į, ȕ1.
�
The matrix is normalized and now the matrix is
</bodyText>
<equation confidence="0.615484">
M
</equation>
<footnote confidence="0.876961">
1 We use ROUGEeval-1.4.2 with &amp;quot;-l&amp;quot; or &amp;quot;-b&amp;quot; option for trun-
cating longer summaries, and &amp;quot;-m&amp;quot; option for word stemming.
</footnote>
<page confidence="0.998567">
183
</page>
<bodyText confidence="0.999867357142857">
ess. The observations demonstrate that &amp;quot;w/ diffu-
sion&amp;quot; performs better than &amp;quot;w/o diffusion&amp;quot; for most
parameter settings. Meanwhile, &amp;quot;w/ diffusion&amp;quot; is
more robust than &amp;quot;w/o diffusion&amp;quot; because the
ROUGE-1 value of &amp;quot;w/ diffusion&amp;quot; changes less
when the parameter values vary. Note that in Fig-
ures 3 and 4 the performance decreases sharply
with the decrease of the weight 0 of inter-
document links and it is the worst case when inter-
document links are not taken into account (i.e. a:
0=1:0), while if intra-document links are not taken
into account (i.e. a:0=0:1), the performance is still
good, which demonstrates the great importance of
inter-document links.
</bodyText>
<figureCaption confidence="0.999813333333333">
Figure 1. Penalty factor tuning on task 2 of DUC 2002
Figure 2. Penalty factor tuning on task 2 of DUC 2004
Figure3. Intra- &amp; Inter-document link weight tuning on
task 2 of DUC 2002
Figure 4. Intra- &amp; Inter-document link weight tuning on
task 2 of DUC 2004
</figureCaption>
<sectionHeader confidence="0.994856" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998240142857143">
G. Erkan and D. Radev. LexPageRank: prestige in multi-
document text summarization. In Proceedings of
EMNLP&apos;04
J. Goldstein, M. Kantrowitz, V. Mittal, and J. Carbonell.
Summarizing Text Documents: Sentence Selection and
Evaluation Metrics. Proceedings of SIGIR-99.
S. Harabagiu and F. Lacatusu. Topic themes for multi-
document summarization. In Proceedings of SIGIR&apos;05,
Salvador, Brazil, 202-209, 2005.
H. Hardy, N. Shimizu, T. Strzalkowski, L. Ting, G. B. Wise,
and X. Zhang. Cross-document summarization by con-
cept classification. In Proceedings of SIGIR&apos;02, Tampere,
Finland, 2002.
J. Kandola, J. Shawe-Taylor, N. Cristianini. Learning semantic
similarity. In Proceedings of NIPS&apos;2002.
K. Knight and D. Marcu. Summarization beyond sentence
extraction: a probabilistic approach to sentence compres-
sion, Artificial Intelligence, 139(1), 2002.
R. I. Kondor and J. Lafferty. Diffusion kernels on graphs and
other discrete structures. In Proceedings of ICML&apos;2002.
C.-Y. Lin and E.H. Hovy. Automatic Evaluation of Summa-
ries Using N-gram Co-occurrence Statistics. In Proceed-
ings of HLT-NAACL 2003.
C.-Y. Lin and E.H. Hovy. From Single to Multi-document
Summarization: A Prototype System and its Evaluation.
In Proceedings of ACL-2002.
I. Mani and E. Bloedorn. Summarizing Similarities and Dif-
ferences Among Related Documents. Information Re-
trieval, 1(1), 2000.
R. Mihalcea and P. Tarau. A language independent algorithm
for single and multiple document summarization. In Pro-
ceedings of IJCNLP&apos;2005.
D. R. Radev, H. Y. Jing, M. Stys and D. Tam. Centroid-based
summarization of multiple documents. Information Proc-
essing and Management, 40: 919-938, 2004.
</reference>
<figure confidence="0.999539901639344">
ROUGE-1
0.385
0.375
0.365
0.355
0.38
0.37
0.36
012345 67 89 10 11 12 13 14 15
Z
w/o diffusion
w/ diffusion
ROUGE-1
0.42
0.41
0.39
0.38
0.37
0.36
0.4
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Z
w/o diffusion
w/ diffusion
ROUGE-1
0.39
0.37
0.35
0.33
0.31
0.29
0.27
0:1
0.1:1
w/o diffusion
w/ diffusion
0.3:1
0.5:1
0.7:1
0.9:1
D E
:
1:1
1:0.9
1:0.7
1:0.5
1:0.3
1:0.1
1:0
ROUGE-1
0.43
0.41
0.39
0.37
0.35
0.33
0.31
w/o diffusion
w/ diffusion
D E
:
</figure>
<page confidence="0.966945">
184
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.921213">
<title confidence="0.999657">Improved Affinity Graph Based Multi-Document Summarization</title>
<author confidence="0.993708">Xiaojun Wan</author>
<author confidence="0.993708">Jianwu</author>
<affiliation confidence="0.997676">Institute of Computer Science and Technology, Peking</affiliation>
<address confidence="0.994214">Beijing 100871,</address>
<email confidence="0.954089">lwanxiaojun,yangjianwu}@icst.pku.edu.cn</email>
<abstract confidence="0.998857473684211">This paper describes an affinity graph based approach to multi-document summarization. We incorporate a diffusion process to acquire semantic relationships between sentences, and then compute information richness of sentences by a graph rank algorithm on differentiated intra-document links and inter-document links between sentences. A greedy algorithm is employed to impose diversity penalty on sentences and the sentences with both high information richness and high information novelty are chosen into the summary. Experimental results on task 2 of DUC 2002 and task 2 of DUC 2004 demonstrate that the proposed approach outperforms existing state-of-theart systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>G Erkan</author>
<author>D Radev</author>
</authors>
<title>LexPageRank: prestige in multidocument text summarization.</title>
<booktitle>In Proceedings of EMNLP&apos;04</booktitle>
<marker>Erkan, Radev, </marker>
<rawString>G. Erkan and D. Radev. LexPageRank: prestige in multidocument text summarization. In Proceedings of EMNLP&apos;04</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Goldstein</author>
<author>M Kantrowitz</author>
<author>V Mittal</author>
<author>J Carbonell</author>
</authors>
<title>Summarizing Text Documents: Sentence Selection and Evaluation Metrics.</title>
<booktitle>Proceedings of SIGIR-99.</booktitle>
<marker>Goldstein, Kantrowitz, Mittal, Carbonell, </marker>
<rawString>J. Goldstein, M. Kantrowitz, V. Mittal, and J. Carbonell. Summarizing Text Documents: Sentence Selection and Evaluation Metrics. Proceedings of SIGIR-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>F Lacatusu</author>
</authors>
<title>Topic themes for multidocument summarization.</title>
<date>2005</date>
<booktitle>In Proceedings of SIGIR&apos;05,</booktitle>
<location>Salvador,</location>
<contexts>
<context position="2326" citStr="Harabagiu and Lacatusu (2005)" startWordPosition="326" endWordPosition="329">tences with highest scores. MEAD is an implementation of the centroid-based method (Radev et al., 2004) that scores sentences based on sentence-level and inter-sentence features, including cluster centroids, position, TF*IDF, etc. NeATS (Lin and Hovy, 2002) selects important content using sentence position, term frequency, topic signature and term clustering, and then uses MMR (Goldstein et al., 1999) to remove redundancy. XDoX (Hardy et al., 1998) identifies the most salient themes within the set by passage clustering and then composes an extraction summary, which reflects these main themes. Harabagiu and Lacatusu (2005) investigate different topic representations and extraction methods. Graph-based methods have been proposed to rank sentences or passages. Websumm (Mani and Bloedorn, 2000) uses a graph-connectivity model and operates under the assumption that nodes which are connected to many other nodes are likely to carry salient information. LexPageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality. Mihalcea and Tarau (2005) also propose similar algorithms based on PageRank and HITS to compute sentence importance for document summari</context>
</contexts>
<marker>Harabagiu, Lacatusu, 2005</marker>
<rawString>S. Harabagiu and F. Lacatusu. Topic themes for multidocument summarization. In Proceedings of SIGIR&apos;05, Salvador, Brazil, 202-209, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hardy</author>
<author>N Shimizu</author>
<author>T Strzalkowski</author>
<author>L Ting</author>
<author>G B Wise</author>
<author>X Zhang</author>
</authors>
<title>Cross-document summarization by concept classification.</title>
<date>2002</date>
<booktitle>In Proceedings of SIGIR&apos;02,</booktitle>
<location>Tampere, Finland,</location>
<marker>Hardy, Shimizu, Strzalkowski, Ting, Wise, Zhang, 2002</marker>
<rawString>H. Hardy, N. Shimizu, T. Strzalkowski, L. Ting, G. B. Wise, and X. Zhang. Cross-document summarization by concept classification. In Proceedings of SIGIR&apos;02, Tampere, Finland, 2002.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Kandola</author>
<author>J Shawe-Taylor</author>
<author>N Cristianini</author>
</authors>
<title>Learning semantic similarity.</title>
<booktitle>In Proceedings of NIPS&apos;2002.</booktitle>
<marker>Kandola, Shawe-Taylor, Cristianini, </marker>
<rawString>J. Kandola, J. Shawe-Taylor, N. Cristianini. Learning semantic similarity. In Proceedings of NIPS&apos;2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>Summarization beyond sentence extraction: a probabilistic approach to sentence compression,</title>
<date>2002</date>
<journal>Artificial Intelligence,</journal>
<volume>139</volume>
<issue>1</issue>
<marker>Knight, Marcu, 2002</marker>
<rawString>K. Knight and D. Marcu. Summarization beyond sentence extraction: a probabilistic approach to sentence compression, Artificial Intelligence, 139(1), 2002.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R I Kondor</author>
<author>J Lafferty</author>
</authors>
<title>Diffusion kernels on graphs and other discrete structures.</title>
<booktitle>In Proceedings of ICML&apos;2002.</booktitle>
<marker>Kondor, Lafferty, </marker>
<rawString>R. I. Kondor and J. Lafferty. Diffusion kernels on graphs and other discrete structures. In Proceedings of ICML&apos;2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
<author>E H Hovy</author>
</authors>
<title>Automatic Evaluation of Summaries Using N-gram Co-occurrence Statistics.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<contexts>
<context position="10268" citStr="Lin and Hovy, 2003" startWordPosition="1713" endWordPosition="1716"> penalty degree factor. The larger Ȧ is, the greater penalty is imposed to the affinity rank score. If Ȧ=0, no diversity penalty is imposed at all. 4. Go to step 2 and iterate until B= 0 or the iteration count reaches a predefined maximum number. After the affinity rank scores are obtained for all sentences, the sentences with highest affinity rank scores are chosen to produce the summary according to the summary length limit. 3 Experiments and Results We compare our system with top 3 performing systems and two baseline systems on task 2 of DUC 2002 and task 4 of DUC 2004 respectively. ROUGE (Lin and Hovy, 2003) metrics is used for evaluation1 and we mainly concern about ROUGE1. The parameters of our system are tuned on DUC 2001 as follows: Ȧ=7, Į=0.3 and ȕ=1. We can see from the tables that our system outperforms the top performing systems and baseline systems on both DUC 2002 and DUC 2004 tasks over all three metrics. The performance improvement achieved by our system results from three factors: diversity penalty imposition, intradocument and inter-document link differentiation and diffusion process incorporation. The ROUGE1 contributions of the above three factors are 0.02200, 0.00268 and 0.00043 </context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>C.-Y. Lin and E.H. Hovy. Automatic Evaluation of Summaries Using N-gram Co-occurrence Statistics. In Proceedings of HLT-NAACL 2003.</rawString>
</citation>
<citation valid="false">
<authors>
<author>C-Y Lin</author>
<author>E H Hovy</author>
</authors>
<title>From Single to Multi-document Summarization: A Prototype System and its Evaluation.</title>
<booktitle>In Proceedings of ACL-2002.</booktitle>
<marker>Lin, Hovy, </marker>
<rawString>C.-Y. Lin and E.H. Hovy. From Single to Multi-document Summarization: A Prototype System and its Evaluation. In Proceedings of ACL-2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>E Bloedorn</author>
</authors>
<title>Summarizing Similarities and Differences Among Related Documents.</title>
<date>2000</date>
<journal>Information Retrieval,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="2498" citStr="Mani and Bloedorn, 2000" startWordPosition="349" endWordPosition="352">, including cluster centroids, position, TF*IDF, etc. NeATS (Lin and Hovy, 2002) selects important content using sentence position, term frequency, topic signature and term clustering, and then uses MMR (Goldstein et al., 1999) to remove redundancy. XDoX (Hardy et al., 1998) identifies the most salient themes within the set by passage clustering and then composes an extraction summary, which reflects these main themes. Harabagiu and Lacatusu (2005) investigate different topic representations and extraction methods. Graph-based methods have been proposed to rank sentences or passages. Websumm (Mani and Bloedorn, 2000) uses a graph-connectivity model and operates under the assumption that nodes which are connected to many other nodes are likely to carry salient information. LexPageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality. Mihalcea and Tarau (2005) also propose similar algorithms based on PageRank and HITS to compute sentence importance for document summarization. In this study, we extend the above graph-based works by proposing an integrated framework for considering both information richness and information novelty of a sen</context>
</contexts>
<marker>Mani, Bloedorn, 2000</marker>
<rawString>I. Mani and E. Bloedorn. Summarizing Similarities and Differences Among Related Documents. Information Retrieval, 1(1), 2000.</rawString>
</citation>
<citation valid="false">
<authors>
<author>R Mihalcea</author>
<author>P Tarau</author>
</authors>
<title>A language independent algorithm for single and multiple document summarization.</title>
<booktitle>In Proceedings of IJCNLP&apos;2005.</booktitle>
<marker>Mihalcea, Tarau, </marker>
<rawString>R. Mihalcea and P. Tarau. A language independent algorithm for single and multiple document summarization. In Proceedings of IJCNLP&apos;2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Radev</author>
<author>H Y Jing</author>
<author>M Stys</author>
<author>D Tam</author>
</authors>
<title>Centroid-based summarization of multiple documents.</title>
<date>2004</date>
<journal>Information Processing and Management,</journal>
<volume>40</volume>
<pages>919--938</pages>
<contexts>
<context position="1800" citStr="Radev et al., 2004" startWordPosition="249" endWordPosition="252">ocument summarization is that the information stored in different documents inevitably overlaps with each other, and hence we need effective summarization methods to merge information stored in different documents, and if possible, contrast their differences. A variety of multi-document summarization methods have been developed recently. In this study, we focus on extractive summarization, which involves assigning saliency scores to some units (e.g. sentences, paragraphs) of the documents and extracting the sentences with highest scores. MEAD is an implementation of the centroid-based method (Radev et al., 2004) that scores sentences based on sentence-level and inter-sentence features, including cluster centroids, position, TF*IDF, etc. NeATS (Lin and Hovy, 2002) selects important content using sentence position, term frequency, topic signature and term clustering, and then uses MMR (Goldstein et al., 1999) to remove redundancy. XDoX (Hardy et al., 1998) identifies the most salient themes within the set by passage clustering and then composes an extraction summary, which reflects these main themes. Harabagiu and Lacatusu (2005) investigate different topic representations and extraction methods. Graph</context>
</contexts>
<marker>Radev, Jing, Stys, Tam, 2004</marker>
<rawString>D. R. Radev, H. Y. Jing, M. Stys and D. Tam. Centroid-based summarization of multiple documents. Information Processing and Management, 40: 919-938, 2004.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>