<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000499">
<title confidence="0.9693035">
Data nonlinearity in exploratory multivariate analysis of language
corpora
</title>
<author confidence="0.791203">
Hermann Moisl
</author>
<affiliation confidence="0.6022955">
School of English Literature, Language, and Linguistics
University of Newcastle
</affiliation>
<bodyText confidence="0.525345333333333">
Newcastle upon Tyne NE 1 7 RU
United Kingdom
hermann .moisl@ nc l.ac .uk
</bodyText>
<sectionHeader confidence="0.978199" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997986058823529">
Data nonlinearity has historically not been
and currently is not an issue in work o n
exploratory multivariate analysis of
language corpora. However, the presence
of nonlinearity in data has a fundamental
bearing on the conduct of exploratory
analysis. The first part of the discussion
explains why this is so in principle, and
the second exemplifies the explanation via
exploratory analysis of the Newcastle
Electronic Corpus of Tyneside English
(NECTE), an historical speech corpus.
The conclusion is that data should b e
screened for nonlinearity prior to analysis
and, if a substantial degree of it is found, a
nonlinear analytical method should b e
used.
</bodyText>
<sectionHeader confidence="0.981318" genericHeader="method">
1 . Introduction
</sectionHeader>
<bodyText confidence="0.996510911764706">
Exploratory multivariate analysis methods
are used across a wide range of research disciplines
to identify interesting structure in multidimensional
data whose characteristics are not well known, and,
if structure is found, to generate hypotheses about
the domain which the data describes (Andrienko
and Andrienko, 2 0 0 5 ). Corpus-based linguistics
has long been among these disciplines, and, a s
computational power has increased and ever-larger
natural language corpora have become available,
the application of exploratory analysis in empirical
linguistic research has grown. When one surveys
the relevant linguistics literature, it becomes clear
that data nonlinearity has historically not been and
is not currently an issue. An exhaustive review
cannot b e undertaken here, but a snapshot of recent
literature is symptomatic: neither the relevant
papers in the Literary and Linguistic Computing
journal&apos;s special issue on &apos;Progress in
Dialectometry &apos; (2 0 0 6 ) nor Manning and S c h ütz e&apos;s
discussion of clustering in their subject-standard
Foundations of Statistical Natural Language
Processing (2 0 0 0 ) refer to it, except perhaps in
passing. However, the presence of nonlinearity in
data has a fundamental bearing on the conduct of
exploratory analysis. The first part of the
discussion explains why this is so in principle, and
the second exemplifies the explanation via
exploratory analysis of the Newcastle Electronic
Corpus of Tyneside English (NECTE), an historical
speech corpus. The conclusion is that data should
b e screened for nonlinearity prior to analysis and, if
a substantial degree of it is found, a nonlinear
analytical method should b e used.
</bodyText>
<sectionHeader confidence="0.728407" genericHeader="method">
2 . Nonlinearity and exploratory analysis
</sectionHeader>
<bodyText confidence="0.99994365">
In physical systems, nonlinearity is the
breakdown of proportionality between cause and
effect, and it manifests itself in a variety of
complex and often unexpected --including chaotic-
- behaviours. Since nonlinearity pervades the
physical world (see for example B ertuglia , 2 0 0 5 ),
data that describes it is likely to contain
nonlinearity a s well. If the data is in vector space
representation, such nonlinearity manifests itself a s
curvature in the data manifold, which can range
from simple curves and surfaces to highly
convoluted fractals.
Many of the commonly used exploratory
multivariate methods, henceforth called &apos;linear
methods&apos;, are insensitive to nonlinearity, and a s
such can generate results that misrepresent the
structure of a nonlinear data manifold. This
insensitivity stems from the way in which the linear
methods measure distance between pairs of vectors
in the manifold --a s the shortest straight-line
</bodyText>
<page confidence="0.982777">
93
</page>
<bodyText confidence="0.975584857142857">
Proceedings of Ninth Meeting of the ACL Special Interest Group in Computational Morphology and Phonology, pages 93–100,
Prague, June 2007. c�2007 Association for Computational Linguistics
distance between them. This is not, however, the
only possible measure. This distance between two
cities can b e measured linearly a s in figure 1 a or
nonlinearly along the curve of the earth&apos;s surface,
a s in figure 1 b :
</bodyText>
<figureCaption confidence="0.869636">
Figure 1 : Linear and nonlinear distance measure
</figureCaption>
<bodyText confidence="0.990092">
Linear distance in this case seriously misrepresents
the true distance. The same applies to nonlinear
data manifolds. Figure 2 shows an extreme
example frequently used in discussions of
nonlinear dimensionality reduction (i.e.
Tenenbaum et al., 2 0 0 0 ), in which linear distance
and distance along the surface of the manifold
differ markedly.
</bodyText>
<figureCaption confidence="0.748934">
Figure 2 : Linear and nonlinear distance in a
nonlinear manifold
</figureCaption>
<bodyText confidence="0.999817958333333">
Linear exploratory methods base their
representation of data structure on linear distance
between vectors in the data space. If the manifold
diverges significantly from linearity, linear distance
measures can give distorted results.
The classic response to the discovery of
nonlinearity in data is to remove it using well
established methods like log-transformation (i.e.
Clarke and Cooke, 1 9 9 8 :5 7 1 -4 ), and then to
analyze the linearized data using a linear method.
This risks throwing the proverbial baby out with
the bathwater. Nonlinearity is not always just a
nuisance to b e eliminated, but may reflect a
fundamental aspect of the thing being studied; in
fact, the study of nonlinearity in natural systems is
now well established across a range of disciplines
(Scott, 2 0 04 ). If nonlinearity is found in natural
language corpus data, the default should b e to
retain it on the grounds that it might reflect a
scientifically interesting aspect of corpus structure.
If it is retained, however, linear analytical methods
become inapplicable in principle, and nonlinear
ones which measure distance along the curvature of
the manifold must b e used.
</bodyText>
<sectionHeader confidence="0.8524095" genericHeader="method">
3 . Exploratory analysis of the NECTE data
3 .1 The NECTE data
</sectionHeader>
<subsectionHeader confidence="0.77031">
The Newcastle Electronic Corpus of
</subsectionHeader>
<bodyText confidence="0.999929714285714">
Tyneside English (NECTE) is a corpus of dialect
speech from Tyneside in North-East England
(Allen et al., 2 0 0 5 ). It includes phonetic
transcriptions of 6 3 interviews together with social
data about the speakers, and a s such offers an
opportunity to study the sociophonetics of
Tyneside speech of the late 1 9 6 0 s. Moisl et al.
(2 0 0 6 ) and Moisl and Maguire (2 0 07 ) have begun
that study using exploratory analysis of the
transcriptions with the aim of generating
hypotheses about phonetic variation among
speakers in the Tyneside dialect area. These studies
were based on comparison of profiles associated
with each of the informants. A profile for any
speaker S is the number of times S uses each of the
phonetic segments in the NECTE transcription
scheme in his or her interview. More specifically,
the profile P associated with S is a vector having a s
many elements a s there are segments such that
each vector element Pj represents the j’th segment,
where j is in the range 1 ..numb er of segments in
the NECTE phonetic transcription scheme, and the
value stored at Pj is an integer representing the
number of times S uses the j’th segment. There are
1 5 6 segments, and so a speaker profile is a length-
156 vector. There are 6 3 TLS speakers, and their
profiles are represented in a matrix M having 6 3
rows, one for each profile.
</bodyText>
<sectionHeader confidence="0.949889" genericHeader="method">
3 .2 Identifying nonlinearity
</sectionHeader>
<bodyText confidence="0.999892571428571">
Where the data dimensionality is 3 or less,
nonlinearity can b e identified b y creating a
scatterplot of the manifold and looking for
curvature. Visual interpretation is subjective,
however. It can b e unreliable when the shape of the
manifold is not a s clear cut a s, say, in figure 2 , and
needs to b e supplemented with some quantitative
</bodyText>
<figure confidence="0.9990715">
a
b
</figure>
<page confidence="0.994579">
94
</page>
<bodyText confidence="0.990116166666667">
measure of nonlinearity; for high-dimensional data
direct graphical representation is impossible
(Andrienko and Andrienko, 2 0 0 5 , ch . 4 ), and
quantitative measurement is the only alternative.
The most straightforward measures are based the
residuals in linear and nonlinear regression: the
sum of squares of residuals, or SSR, gives the total
divergence of the data variables from the line of
best fit, and the standard error their average
dispersion around the line in a way analogous to
univariate standard deviation.
Figure 3 : Lines of best fit in linear and nonlinear
regression
For a given pair of variables, if the SSR and
standard error from a nonlinear regression are less
than those from a linear one, then a curve fits the
data better than a straight line and the relationship
of the two variables is nonlinear.
In applications where the dimensionality
of the data can b e in the hundreds or even
thousands, pairwise regression-based testing of
nonlinearity can quickly become onerous since,
for any given dimensionality n ,
= n(n −1)
2
For n = 1 0 0 , there would b e 4 9 5 0 different variable
pairs to consider. The situation can b e salvaged in
cases where some variables are more important
than others relative to the research question by
examining only a tractable subset of important
variables. Several criteria for variable importance
are available, such a s variance, term frequency /
inverse document frequency (Robertson, 2 0 04 ) and
Poisson distribution (Church and Gale, 1 9 9 5 a ,
1 9 9 5 b ); the use of variance for this purpose is
exemplified below.
With a dimensionality of 1 5 6 , 1 2 0 9 0
variable pairs would have to b e tested for
nonlinearity, which is not impossible but certainly
onerous. The number of pairs to b e considered was
therefore reduced to a manageable level using the
relative variances of the 1 5 6 variables a s a
selection criterion. The justification for using
variance for this purpose is a s follows.
Classification of objects in any domain of study
depends on there being variation in their
characteristics. When the objects to b e classified
are described by variables, then a variable is only
useful for the purpose if there is significant
variation in the values that it takes; those with little
or no variation can b e disregarded. The variances
of the column vectors of M were calculated, sorted
in descending order of magnitude, and plotted in
figure 4 .
</bodyText>
<figureCaption confidence="0.664494">
Figure 4 : Variances of column vectors of N
</figureCaption>
<bodyText confidence="0.9995844">
The highest-variance dozen variables were
selected and linear, quadratic, and cubic regression
were applied to all 6 6 distinct pairings of them, in
each case calculating SSR and standard error.
Three examples are given: figure 5 a is
representative of the linearly-related pairs, figure
5 b of moderately nonlinear pairs, and figure 5 c of
strongly nonlinear ones. The frequencies of these
are 1 2 linear, 2 5 moderately nonlinear, and 2 9
strongly nonlinear.
</bodyText>
<figure confidence="0.872469714285714">
a
b
p n
95
Regression plots Quantifications
a
b
</figure>
<table confidence="0.980798392857143">
c
Linear
SSR 2 6 6 8 2 .0 0
Standard Error 2 0 .7 4
Quadratic
SSR 2 6 6 2 2 .4 0
Standard Error 2 0 .8 9
Cubic
SSR 2 6 5 9 8 .2 0
Standard error 2 1 .0 5
Linear
SSR 5 3 7 0 3 .4 3
Standard Error 2 9 .67
Quadratic
SSR 5 3 4 9 6 .5 8
Standard Error 2 9 .8 6
Cubic
SSR 3 8 8 8 0 .4 0
Standard error 2 5 .67
Linear
SSR 9 5 07 1 .2 1
Standard error 3 9 .1 6
Quadratic
SSR 4 9 2 8 1 .2 0
Standard error 2 8 .4 2
Cubic
SSR 2 2 2 0 6 .8 8
Standard error 1 9 .24
</table>
<figureCaption confidence="0.917491">
Figure 5 : Sample regressions of variable pairs from data matrix M
</figureCaption>
<bodyText confidence="0.998791866666667">
The essentially linear relationship of v 1 and v2 is
clear both visually and in the uniformity of SSR
and standard error measures, where the nonlinear
regressions yield no meaningful improvement over
the linear. For v 1 and v9 cubic regression shows
some improvement over linear and quadratic both
visually and quantitatively. For v6 and v 1 2 the
quadratic regression line is visually a much better
fit to the data than the linear one, and the cubic
one is even better; correspondingly, the quadratic
quantifications show a substantial improvement
over the linear ones, and the cubic ones even more
so . The relationships between the highest-variance
variables in M can, therefore, b e said to range
from linear to strongly nonlinear.
</bodyText>
<page confidence="0.962949">
96
</page>
<bodyText confidence="0.971509162790698">
3 .3 Linear and nonlinear analysis o f the NECTE
data
Moisl et al. (2 0 0 6 ) analyzed the NECTE
data with what is probably the most widely used of
the linear exploratory methods: hierarchical cluster
analysis (Everitt et al., 2 0 0 1 ). This is actually a
class of methods each of which defines clusters
differently, but all of which represent cluster
structure a s nested constituency trees. Infamously -
-and unsurprisingly, given that each uses a different
definition of what constitutes a cluster-- the variant
methods can and often do assign different tree
structures to the same data, and it is not usually
clear which is to b e preferred (Everitt et al., 2 0 0 1 ,
ch . 4 ). In the NECTE case, however, a range of
variants (single link, complete link, average link,
Ward&apos;s Method) converged on a stable structure of
four main clusters exemplified b y the Ward tree
shown in figure 6 .
Figure 6 : Ward&apos;s Method cluster tree for data
matrix M
When interpreted in terms of the social data that
NECTE provides for the speakers, a clear
correlation between phonetic usage and social
factors emerged. The main distinction is between
middle class, well educated speakers from
Newcastle on the north side of the river Tyne,
labelled N , and working class, less well educated
speakers from Gateshead on the south side of the
Tyne, labelled G . The Gateshead speakers are
categorized into G2 (exclusively male), and G 1
(mainly through not exclusively female); G 1 is
subcategorized into G 1 a (working class males and
females) and G 1 b (males and females with
relatively higher socioeconomic status). Moisl and
Maguire (2 0 07 ) subsequently used the centroids of
these clusters to identify the phonetic features most
characteristic of each. Three sets of vowels were
found to b e of particular importance. Although all
of these had been commented on before, their
relative (and cumulative) sociolinguistic
importance had hitherto escaped attention. They
are:
</bodyText>
<listItem confidence="0.9989895">
• various types of [ ].
• [ ] and [ ], which correspond to RP [ ],
</listItem>
<bodyText confidence="0.796105">
and are found in words of the GOAT
lexical set a s defined b y Wells (1 9 8 2 :1 4 6 -
7 ).
</bodyText>
<listItem confidence="0.960903">
• [a ], [ ], and [e ], which correspond to RP
[a ], and are found in words belonging to
</listItem>
<bodyText confidence="0.992126">
the PRICE lexical set a s defined b y Wells
(1 9 8 2 :1 4 9 -5 0 ).
For nonlinear analysis the self-organizing
map, or SOM, was selected from among the
various available nonlinear exploratory methods
because it has been successfully used in a very
wide range of applications (Kaski et al., 1 9 9 8 ; Oja
et al., 2 0 0 1 ). The standard SOM (Kohonen, 2 0 0 1 )
projects the topology of a data manifold in a space
of arbitrary dimensionality n onto a two-
dimensional lattice, where the structure of the
manifold can b e visually inspected. It does this by
partitioning the vectors on the manifold surface
into a Voronoi tesselation (Aurenhammer and
Klein, 2 0 0 0 ), thereby assigning all the data vectors
within a defined topological neighborhood to the
same cell of the tesselation, a s shown in figure 7 .
</bodyText>
<figureCaption confidence="0.996185">
Figure 7 : Voronoi tesselation of a manifold surface
</figureCaption>
<page confidence="0.99866">
97
</page>
<bodyText confidence="0.99779">
For example, the doughnut shape on the left of
figure 7 is a manifold in 3-dimensional space, and
the square on the right represents the way in which
a SOM partitions its surface: each dot represents a
quantized vector and the lines enclosing a dot
represent the boundaries of the area of the
tesselation cell containing the k vectors within the
specified topological neighborhood. All the vectors
in a given cell are mapped to the same lattice unit,
and the vectors in adjoining cells are mapped to
adjacent lattice units. The result of this topology
preservation is that all vectors close to one another
in the input space in the sense that they are in the
same or adjoining topological neighbourhoods will
b e close on the SOM output lattice (for further
discussion see Ritter et al., (1 9 9 2 ), ch . 4 ). The
topology preservation is, moreover, nonlinear
because the tesselation is based not on a global
distance measure between vectors on the manifold
but on local neighborhood distance, and a s such the
tesselation follows the manifold surface: if the
surface is nonlinear, so is the topology-preserving
representation of it.
The NECTE data was analyzed using a
range of SOM parameters for output lattice size
and shape and various initializations such a s
starting neighborhood, learning rate, and rate of
neighborhood decrease. The results converged on a
stable analysis of which the following map is
representative.
</bodyText>
<figureCaption confidence="0.753629">
Figure 8 : SOM analysis of the NECTE data
</figureCaption>
<bodyText confidence="0.9999935">
The speaker labels were positioned automatically
on the lattice b y the SOM&apos;s input-to-lattice
mapping function, and the shading was generated
using the U-matrix method (Ultsch, 1 9 9 3 ). This
shading must b e understood in order to interpret the
above SOM correctly, so a brief explanation is
given here. It has already been noted that the SOM
preserves the topology of the n-dimensional input
manifold in the sense that vectors which are close
in the input space are also close in the two-
dimensional output space. The converse is not true,
however: just because vectors are close in the
output space does not necessarily mean that they
are close on the input manifold. This apparently-
paradoxical situation arises because the SOM
mapping function does not use a global distance
measure but only local neighborhood distance, and
it consequently cannot and does not represent
proportionality of distance between vector pairs in
the input space. Instead, is squeezes its
representation of the input topology onto the lattice
in such a way that closely adjacent lattice cells may
represent vectors which are far apart on the input
manifold. Because, therefore, spatial distance is a
delphic guide to interpretation of the SOM, some
way must b e found of demarcating the shape of the
manifold representation given b y the lattice. The
U-matrix is a way of doing this. How it works can
only b e explained in terms of the details of SOM
architecture, which cannot b e given here on
</bodyText>
<page confidence="0.991954">
98
</page>
<bodyText confidence="0.999914555555556">
account of space constraints. It is, however,
important to understand that lighter regions of the
map represent manifold boundaries and darker ones
the manifold surface; metaphorically, the darker
areas are islands representing the shape of the
manifold, and the lighter areas the sea separating
them. The remaining annotations in figure 8 ,
finally, were added b y hand to facilitate discussion
in the next subsection, and are explained there.
</bodyText>
<sectionHeader confidence="0.995249" genericHeader="method">
3 .4 Discussion
</sectionHeader>
<bodyText confidence="0.999945781818182">
Associated with each speaker on the SOM
is a label which shows that speaker&apos;s place in the
hierarchical cluster tree --tlsg0 8 on the SOM is in
cluster G 1 a in the tree, for example. In addition,
solid-line curves have been added to the SOM
which show the approximate areas of the map that
correspond to the main hierarchical clusters and,
for each region, the relevant hierarchical cluster
label has been shown surrounded by a square --the
upper left corner of the SOM, for example, is
bounded by a solid curve and labelled N to show
that the speaker vectors found there correspond to
those in the N hierarchical cluster. Using these
annotations, it might appear that the hierarchical
and SOM analyses are similar: the hierarchical
analysis shows four main clusters, and the SOM
has four disjoint regions corresponding to those
clusters. This perception of correspondence is,
however, based on spatial placement of the speaker
vectors on the SOM, and, a s w e have seen, relative
spatial distance on a SOM can b e misleading. If
one looks instead at the U-matrix shading that
demarcates the manifold boundaries, the Newcastle
group is a s clearly distinguished from the
Gateshead speakers b y the SOM a s by the
hierarchical analysis, but the Gateshead speakers
are grouped in a way that differs subtly from the
hierarchical analysis. The hierarchical analysis says
that there are three distinct Gateshead groups: G 1 a
consists of working class men and women, G 1 b of
lower middle class men and women, and G2 of
working class men. The SOM, on the other hand,
says that the Gateshead speakers fall into only two
main groups the boundary between which is shown
in figure 8 a s a dotted-line curve. The one above
and to the right of the dotted line (and excluding
the Newcastle group) consists of lower middle
class men and women and working class women.
The other, below and to the left of the dotted line,
comprises working class men together with two
women (tlsg37 and tlsg40) who are classified with
men both here and in the hierarchical analysis.
The linear and nonlinear methods,
therefore, offer results that differ substantively.
From a methodological point of view, the SOM
result must b e preferred because the data contains
nonlinearity, and a nonlinear method can b e
expected to give a more accurate analysis of
nonlinear data than a linear one. A sociolinguist
might find the SOM analysis preferable on grounds
of simplicity: there is no obvious distinction in the
social data between the working class men that the
hierarchical analysis assigns to separate clusters.
The present paper is, however, a methodological
one, and no further comment is ventured on this.
</bodyText>
<sectionHeader confidence="0.999343" genericHeader="conclusions">
5 . Conclusion
</sectionHeader>
<bodyText confidence="0.999866470588235">
The discussion began with the observation
that existing work on exploratory analysis of
linguistic corpora does not take the possibility of
data nonlinearity into account, and claimed that the
presence of nonlinearity in data has a fundamental
bearing on the conduct of exploratory analysis. The
first part of the discussion explained why this is so
in principle, and the second exemplified the
explanation via exploratory analysis of the
Newcastle Electronic Corpus of Tyneside English
using both linear and nonlinear methods. That the
two types of method gave substantively different
results supports the case in principle that data
should b e screened for nonlinearity prior to
exploratory analysis and that, if substantial degree
of it is found, a nonlinear analytical method should
b e used.
</bodyText>
<sectionHeader confidence="0.998113" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.982394083333333">
Will Allen, Joan Beal, Karen Corrigan, Warren
Maguire, and Hermann Moisl. 2 0 07 . A
Linguistic Time Capsule: the Newcastle
Electronic Corpus of Tyneside English. In : Joan
Beal, Karen Corrigan, and Hermann Moisl
(eds.). 2 0 07 . Using Unconventional Digital
Language Corpora: Diachronic Corpora.
P algrave Macmillan, Basingstoke, 1 6 -4 8 .
Natalie Andrienko and Gennady Andrienko. 2 0 0 5 .
Exploratory Analysis of Spatial and Temporal
Data: a Systematic Approach. Springer-Verlag,
Berlin.
</reference>
<page confidence="0.978661">
99
</page>
<reference confidence="0.928309220588235">
Franz Aurenhammer and R . Klein. 2 0 0 0 .
Voronoi Diagrams. In : J.-R . Sack and J.
Urrutia (ed s.) Handbook of Computational
Geometry. North-Holland, Amsterdam, 2 0 1 -
2 9 0 .
Cristoforo B ertuglia and Franco Vaio . 2 0 0 5 .
Nonlinearity, Chaos, and Complexity: The
Dynamics of Natural and Social Systems.
Oxford University Press, Oxford.
Kenneth Church and William Gale. 1 9 9 5 a . Poisson
mixtures. Natural Language Engineering, 1 :
1 6 3 -1 9 0 .
Kenneth Church and William Gale. 1 9 9 5 b . Inverse
Document Frequency (IDF): A Measure of
Deviation from Poisson. Proceedings of the
Third Workshop on Very Large Corpora.
Association for Computational Linguistics. Reed
Elsevier, 1 2 1 -1 3 0 .
G . Clarke and D . Cooke. 1 9 9 8 . A Basic Course in
Statistics. 4th ed . Arnold, London.
Brian Everitt, Sabine Landau, and Morven Leese.
2 0 0 1 . Cluster Analysis. 4th ed . Arnold, London.
Samuel Kaski, J. Kangas, and Teuvo Kohonen.
1 9 9 8 . Bibliography of Self-Organizing Map
(S OM ) Papers: 1 9 8 1 -1 9 97 . Neural Computing
Surveys, 1 :1 0 2 -3 5 0 .
Teuvo Kohonen. 2 0 0 1 . Self-Organizing Maps. 3 rd
ed . Springer-Verlag, Berlin.
Christopher Manning and Hinrich S c hütz e. 2 0 0 0 .
Foundations of Statistical Natural Language
Processing. MIT Press, Cambridge, Mass.
Hermann Moisl, Warren Maguire, and Will Allen.
2 0 0 6 . Phonetic variation in Tyneside:
exploratory multivariate analysis of the
Newcastle Electronic Corpus of Tyneside
English. In : Frans Hinskens (ed .). Language
Variation-European Perspectives. John
Benjamins Publishing, Amsterdam, 1 27 -1 4 1 .
Hermann Moisl and Warren Maguire. 2 0 07 .
Identifying the Main Determinants of Phonetic
Variation in the Newcastle Electronic Corpus of
Tyneside English. Journal of Quantitative
Linguistics 20 07 , 1 4 : to appear.
M . Oja, Samuel Kaski, and Teuvo Kohonen. 2 0 0 1 .
Bibliography of self-organizing map (S OM )
papers: 1 9 9 8 -2 0 0 1 . Neural Computing Surveys,
3 :1 -1 5 6 .
Helge Ritter, T . Martinetz, and K . Schulten. 1 9 9 2 .
Neural computation and self -organizing maps.
Addison-Wesley, Wokingham, UK .
Stephen Robertson. 2 0 04 . Understanding inverse
document frequency: on theoretical arguments
for IDF. Journal of Documentation, 6 0 :5 0 3 -
5 2 0 .
Alwyn Scott. 2 0 04 . Encyclopedia of Nonlinear
Science. Fitzroy Dearborn Publishers. .
Josh Tenenbaum, V . de Silva, and John Langford.
2 0 0 0 . A global framework for nonlinear
dimensionality reduction. Science, 2 9 0 :2 3 1 9 --
2 3 2 3 .
Alfred Ultsch. 1 9 9 3 . Self-organizing neural
networks for visualization and classification. In :
O.Opitz, B . Lausen, and R . Klar, (eds.),
Information and classification :concepts,
methods, and applications. Springer-Verlag,
Berlin, 3 07 -3 1 3 .
John Wells. 1 9 8 2 . Accents of English. Cambridge:
Cambridge University Press, Cambridge, UK .
</reference>
<page confidence="0.98987">
100
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.110193">
<title confidence="0.8554495">Data nonlinearity in exploratory multivariate analysis of language corpora</title>
<author confidence="0.991172">Hermann</author>
<affiliation confidence="0.977">School of English Literature, Language, and University of</affiliation>
<note confidence="0.306353333333333">Newcastle upon Tyne NE 1 7 United Kingdom hermann .moisl@ nc l.ac .uk</note>
<abstract confidence="0.9978555">Data nonlinearity has historically not been and currently is not an issue in work o n exploratory multivariate analysis of language corpora. However, the presence of nonlinearity in data has a fundamental bearing on the conduct of exploratory analysis. The first part of the discussion explains why this is so in principle, and the second exemplifies the explanation via analysis of the Electronic Corpus of Tyneside English (NECTE), an historical speech corpus. The conclusion is that data should b e screened for nonlinearity prior to analysis and, if a substantial degree of it is found, a nonlinear analytical method should b e used.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Will Allen</author>
<author>Joan Beal</author>
<author>Karen Corrigan</author>
<author>Warren Maguire</author>
<author>Hermann Moisl</author>
</authors>
<title>07 . A Linguistic Time Capsule: the Newcastle Electronic Corpus of Tyneside English.</title>
<booktitle>07 . Using Unconventional Digital Language Corpora: Diachronic Corpora. P algrave Macmillan, Basingstoke,</booktitle>
<volume>2</volume>
<pages>.</pages>
<editor>In : Joan Beal, Karen Corrigan, and Hermann Moisl (eds.).</editor>
<marker>Allen, Beal, Corrigan, Maguire, Moisl, </marker>
<rawString>Will Allen, Joan Beal, Karen Corrigan, Warren Maguire, and Hermann Moisl. 2 0 07 . A Linguistic Time Capsule: the Newcastle Electronic Corpus of Tyneside English. In : Joan Beal, Karen Corrigan, and Hermann Moisl (eds.). 2 0 07 . Using Unconventional Digital Language Corpora: Diachronic Corpora. P algrave Macmillan, Basingstoke, 1 6 -4 8 .</rawString>
</citation>
<citation valid="false">
<authors>
<author>Natalie Andrienko</author>
<author>Gennady Andrienko</author>
</authors>
<title>Exploratory Analysis of Spatial and Temporal Data: a Systematic Approach.</title>
<volume>2</volume>
<publisher>Springer-Verlag,</publisher>
<location>Berlin.</location>
<marker>Andrienko, Andrienko, </marker>
<rawString>Natalie Andrienko and Gennady Andrienko. 2 0 0 5 . Exploratory Analysis of Spatial and Temporal Data: a Systematic Approach. Springer-Verlag, Berlin.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Franz Aurenhammer</author>
<author>R</author>
</authors>
<title>Voronoi Diagrams.</title>
<booktitle>In : J.-R . Sack and J. Urrutia (ed s.) Handbook of Computational Geometry. North-Holland,</booktitle>
<volume>2</volume>
<pages>.</pages>
<location>Amsterdam,</location>
<marker>Aurenhammer, R, </marker>
<rawString>Franz Aurenhammer and R . Klein. 2 0 0 0 . Voronoi Diagrams. In : J.-R . Sack and J. Urrutia (ed s.) Handbook of Computational Geometry. North-Holland, Amsterdam, 2 0 1 -2 9 0 .</rawString>
</citation>
<citation valid="false">
<authors>
<author>Cristoforo B ertuglia</author>
<author>Franco Vaio</author>
</authors>
<title>Chaos, and Complexity: The Dynamics of Natural and Social Systems.</title>
<volume>2</volume>
<publisher>Oxford University Press,</publisher>
<location>Oxford.</location>
<marker>ertuglia, Vaio, </marker>
<rawString>Cristoforo B ertuglia and Franco Vaio . 2 0 0 5 . Nonlinearity, Chaos, and Complexity: The Dynamics of Natural and Social Systems. Oxford University Press, Oxford.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kenneth Church</author>
<author>William Gale</author>
</authors>
<title>a . Poisson mixtures.</title>
<journal>Natural Language Engineering,</journal>
<volume>1</volume>
<pages>.</pages>
<marker>Church, Gale, </marker>
<rawString>Kenneth Church and William Gale. 1 9 9 5 a . Poisson mixtures. Natural Language Engineering, 1 : 1 6 3 -1 9 0 .</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kenneth Church</author>
<author>William Gale</author>
</authors>
<title>b . Inverse Document Frequency (IDF): A Measure of Deviation from Poisson.</title>
<booktitle>Proceedings of the Third Workshop on Very Large Corpora. Association for Computational Linguistics. Reed Elsevier,</booktitle>
<volume>1</volume>
<pages>.</pages>
<marker>Church, Gale, </marker>
<rawString>Kenneth Church and William Gale. 1 9 9 5 b . Inverse Document Frequency (IDF): A Measure of Deviation from Poisson. Proceedings of the Third Workshop on Very Large Corpora. Association for Computational Linguistics. Reed Elsevier, 1 2 1 -1 3 0 .</rawString>
</citation>
<citation valid="false">
<authors>
<author>Clarke</author>
<author>D</author>
</authors>
<booktitle>A Basic Course in Statistics. 4th ed .</booktitle>
<volume>1</volume>
<location>Arnold, London.</location>
<marker>Clarke, D, </marker>
<rawString>G . Clarke and D . Cooke. 1 9 9 8 . A Basic Course in Statistics. 4th ed . Arnold, London.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Brian Everitt</author>
<author>Sabine Landau</author>
<author>Morven Leese</author>
</authors>
<title>Cluster Analysis.</title>
<journal>Bibliography of Self-Organizing Map (S OM ) Papers:</journal>
<booktitle>4th</booktitle>
<volume>2</volume>
<pages>.</pages>
<marker>Everitt, Landau, Leese, </marker>
<rawString>Brian Everitt, Sabine Landau, and Morven Leese. 2 0 0 1 . Cluster Analysis. 4th ed . Arnold, London. Samuel Kaski, J. Kangas, and Teuvo Kohonen. 1 9 9 8 . Bibliography of Self-Organizing Map (S OM ) Papers: 1 9 8 1 -1 9 97 . Neural Computing Surveys, 1 :1 0 2 -3 5 0 .</rawString>
</citation>
<citation valid="false">
<authors>
<author>Teuvo Kohonen</author>
</authors>
<title>Self-Organizing Maps. 3 rd ed .</title>
<volume>2</volume>
<publisher>Springer-Verlag,</publisher>
<location>Berlin.</location>
<marker>Kohonen, </marker>
<rawString>Teuvo Kohonen. 2 0 0 1 . Self-Organizing Maps. 3 rd ed . Springer-Verlag, Berlin.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Christopher Manning</author>
</authors>
<title>and Hinrich S c hütz e.</title>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<volume>2</volume>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<marker>Manning, </marker>
<rawString>Christopher Manning and Hinrich S c hütz e. 2 0 0 0 . Foundations of Statistical Natural Language Processing. MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Hermann Moisl</author>
<author>Warren Maguire</author>
<author>Will Allen</author>
</authors>
<title>Phonetic variation in Tyneside: exploratory multivariate analysis of the Newcastle Electronic Corpus of Tyneside English. In : Frans Hinskens (ed .).</title>
<volume>2</volume>
<publisher>Language</publisher>
<marker>Moisl, Maguire, Allen, </marker>
<rawString>Hermann Moisl, Warren Maguire, and Will Allen. 2 0 0 6 . Phonetic variation in Tyneside: exploratory multivariate analysis of the Newcastle Electronic Corpus of Tyneside English. In : Frans Hinskens (ed .). Language</rawString>
</citation>
<citation valid="false">
<authors>
<author>Variation-European Perspectives</author>
</authors>
<volume>1</volume>
<pages>.</pages>
<publisher>John Benjamins Publishing,</publisher>
<location>Amsterdam,</location>
<marker>Perspectives, </marker>
<rawString>Variation-European Perspectives. John Benjamins Publishing, Amsterdam, 1 27 -1 4 1 .</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hermann Moisl</author>
<author>Warren Maguire</author>
</authors>
<title>Identifying the Main Determinants of Phonetic Variation in the Newcastle Electronic Corpus of Tyneside English.</title>
<date></date>
<journal>Journal of Quantitative Linguistics</journal>
<volume>2</volume>
<pages>.</pages>
<note>to</note>
<marker>Moisl, Maguire, </marker>
<rawString>Hermann Moisl and Warren Maguire. 2 0 07 . Identifying the Main Determinants of Phonetic Variation in the Newcastle Electronic Corpus of Tyneside English. Journal of Quantitative Linguistics 20 07 , 1 4 : to appear. M . Oja, Samuel Kaski, and Teuvo Kohonen. 2 0 0 1 . Bibliography of self-organizing map (S OM ) papers: 1 9 9 8 -2 0 0 1 . Neural Computing Surveys, 3 :1 -1 5 6 .</rawString>
</citation>
<citation valid="false">
<authors>
<author>Martinetz</author>
<author>K</author>
</authors>
<title>Neural computation and self -organizing maps.</title>
<volume>1</volume>
<publisher>Addison-Wesley,</publisher>
<location>Wokingham, UK .</location>
<marker>Martinetz, K, </marker>
<rawString>Helge Ritter, T . Martinetz, and K . Schulten. 1 9 9 2 . Neural computation and self -organizing maps. Addison-Wesley, Wokingham, UK .</rawString>
</citation>
<citation valid="false">
<authors>
<author>Stephen Robertson</author>
</authors>
<title>04 . Understanding inverse document frequency: on theoretical arguments for IDF.</title>
<journal>Journal of Documentation,</journal>
<volume>2</volume>
<pages>.</pages>
<marker>Robertson, </marker>
<rawString>Stephen Robertson. 2 0 04 . Understanding inverse document frequency: on theoretical arguments for IDF. Journal of Documentation, 6 0 :5 0 3 -5 2 0 .</rawString>
</citation>
<citation valid="false">
<authors>
<author>Alwyn Scott</author>
</authors>
<title>04 . Encyclopedia of Nonlinear Science. Fitzroy Dearborn Publishers.</title>
<volume>2</volume>
<publisher></publisher>
<marker>Scott, </marker>
<rawString>Alwyn Scott. 2 0 04 . Encyclopedia of Nonlinear Science. Fitzroy Dearborn Publishers. .</rawString>
</citation>
<citation valid="false">
<authors>
<author>de Silva</author>
<author>John Langford</author>
</authors>
<title>A global framework for nonlinear dimensionality reduction.</title>
<journal>Science,</journal>
<volume>2</volume>
<pages>.</pages>
<marker>de Silva, Langford, </marker>
<rawString>Josh Tenenbaum, V . de Silva, and John Langford. 2 0 0 0 . A global framework for nonlinear dimensionality reduction. Science, 2 9 0 :2 3 1 9 --2 3 2 3 .</rawString>
</citation>
<citation valid="false">
<authors>
<author>Alfred Ultsch</author>
</authors>
<title>Self-organizing neural networks for visualization and classification.</title>
<volume>1</volume>
<pages>.</pages>
<editor>In : O.Opitz, B . Lausen, and R . Klar, (eds.),</editor>
<publisher>Springer-Verlag,</publisher>
<location>Berlin,</location>
<marker>Ultsch, </marker>
<rawString>Alfred Ultsch. 1 9 9 3 . Self-organizing neural networks for visualization and classification. In : O.Opitz, B . Lausen, and R . Klar, (eds.), Information and classification :concepts, methods, and applications. Springer-Verlag, Berlin, 3 07 -3 1 3 .</rawString>
</citation>
<citation valid="false">
<authors>
<author>John Wells</author>
</authors>
<title>Accents of English. Cambridge:</title>
<volume>1</volume>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK .</location>
<marker>Wells, </marker>
<rawString>John Wells. 1 9 8 2 . Accents of English. Cambridge: Cambridge University Press, Cambridge, UK .</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>