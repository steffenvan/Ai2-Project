<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.095788">
<title confidence="0.996186">
A Computer Assisted Speech Transcription System
</title>
<author confidence="0.999015">
Alejandro Revuelta-Martinez, Luis Rodriguez, Ismael Garcia-Varea
</author>
<affiliation confidence="0.9917055">
Computer Systems Department
University of Castilla-La Mancha
</affiliation>
<address confidence="0.642381">
Albacete, Spain
</address>
<email confidence="0.998315">
{Alejandro.Revuelta,Luis.RRuiz,Ismael.Garcia}@uclm.es
</email>
<sectionHeader confidence="0.997369" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9994481875">
Current automatic speech transcription sys-
tems can achieve a high accuracy although
they still make mistakes. In some scenar-
ios, high quality transcriptions are needed
and, therefore, fully automatic systems are
not suitable for them. These high accuracy
tasks require a human transcriber. How-
ever, we consider that automatic techniques
could improve the transcriber’s efficiency.
With this idea we present an interactive
speech recognition system integrated with
a word processor in order to assists users
when transcribing speech. This system au-
tomatically recognizes speech while allow-
ing the user to interactively modify the tran-
scription.
</bodyText>
<sectionHeader confidence="0.999519" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999978214285714">
Speech has been the main mean of communica-
tion for thousands of years and, hence, is the most
natural human interaction mode. For this reason,
Automatic Speech Recognition (ASR) has been
one of the major research interests within the Nat-
ural Language Processing (NLP) community.
Although current speech recognition ap-
proaches (which are based on statistical learning
theory (Jelinek, 1998)) are speaker independent
and achieve high accuracy, ASR systems are not
perfect and transcription errors rise drastically
when considering large vocabularies, dealing
with noise environments or spontaneous speech.
In those tasks (as for example, automatic tran-
scription of parliaments proceedings) where
perfect recognition results are required, ASR
can not be fully reliable so far and, a human
transcriber has to check and supervise the
automatically generated transcriptions.
In the last years, cooperative systems, where
a human user and an automatic system work to-
gether, have gain growing attention. Here we
present a system that interactively assists a human
transcriber when using an ASR software. The
proposed tool is fully embedded into a widely
used and open source word processor and it relies
on an ASR system that is proposing suggestions to
the user in the form of practical transcriptions for
the input speech. The user is allowed to introduce
corrections at any moment of the discourse and,
each time an amendment is performed, the sys-
tem will take it into account in order to propose a
new transcription (always preserving the decision
made by the user, as can be seen in Fig. 1). The
rationale behind this idea is to reduce the human
user’s effort and increase efficiency.
Our proposal’s main contribution is that it car-
ries out an interactive ASR process, continually
proposing new transcriptions that take into ac-
count user amendments to increase their useful-
ness. To our knowledge, no current transcription
package provides such an interactive process.
</bodyText>
<sectionHeader confidence="0.954734" genericHeader="method">
2 Theoretical Background
</sectionHeader>
<bodyText confidence="0.993520875">
Computer Assisted Speech Recognition (CAST)
can be addressed by extending the statistical ap-
proach to ASR. Specifically, we have an input
signal to be transcribed x and the user feedback
in the form of a fully correct transcription pre-
fix p (an example of a CAST session is shown
in Fig. 1). From this, the recognition system has
to search for the optimal completion (suffix) s� as:
</bodyText>
<equation confidence="0.9986514">
s� = arg max
S
= arg max Pr(x  |p, s) · Pr(s  |p) (1)
S
Pr(s  |x, p)
</equation>
<page confidence="0.897593">
41
</page>
<bodyText confidence="0.979594444444444">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 41–45,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
where, as in traditional ASR, we have an acous-
tic model Pr(x  |p, s) and a language model
Pr(s  |p). The main difference is that, here,
part of the correct transcription is available (pre-
fix) and we can use this information to improve
the suffix recognition. This can be achieved by
properly adapting the language model to account
for the user validated prefix as it is detailed in
(Rodr´ıguez et al., 2007; Toselli et al., 2011).
As was commented before, the main goal of
this approach is to improve the efficiency of the
transcription process by saving user keystrokes.
Off-line experiments have shown that this ap-
proach can save about 30% of typing effort when
compared to the traditional approach of off-line
post-editing results from an ASR system.
</bodyText>
<sectionHeader confidence="0.99377" genericHeader="method">
3 Prototype Description
</sectionHeader>
<bodyText confidence="0.999996818181818">
A fully functional prototype, which implements
the CAST techniques described in section 2, has
been developed. The main goal is to provide a
completely usable tool. To this end, we have im-
plemented a tool that easily allows for organiz-
ing and accessing different transcription projects.
Besides, the prototype has been embedded into a
widely used office suite. This way, the transcribed
document can be properly formatted since all the
features provided by a word processor are avail-
able during the transcription process.
</bodyText>
<subsectionHeader confidence="0.997406">
3.1 Implementation Issues
</subsectionHeader>
<bodyText confidence="0.990531">
The system has been implemented following a
modular architecture consisting of several compo-
nents:
</bodyText>
<listItem confidence="0.982432928571429">
• User interface. Manages the graphical fea-
tures of the prototype user interface.
• Project management: Allows the user to
define and deal with transcription projects.
These projects are stored in XML files con-
taining parameters such as input files to be
transcribed, output documents, etc.
• System controller. Manages communication
among all the components.
• OpenOffice integration: This subsystem pro-
vides an appropriate integration between the
CAST tool and the OpenOffice1 software
suite. The transcriber has, therefore, full ac-
cess to a word processor functionality.
</listItem>
<footnote confidence="0.882505">
1www.openoffice.org
</footnote>
<listItem confidence="0.988919">
• Speech manager: Implements audio play-
back and synchronization with the ASR out-
comes.
• CAST engine: Provides the interactive ASR
suggestion mechanism.
</listItem>
<bodyText confidence="0.999790529411765">
This architecture is oriented to be flexible and
portable so that different scenarios, word proces-
sor software or ASR engines can be adopted with-
out requiring big changes in the current imple-
mentation. Although this initial prototype works
as a standalone application the followed design
should allow for a future “in the cloud” tool,
where the CAST engine is located in a server and
the user can employ a mobile device to carry out
the transcription process.
With the purpose of providing a real-time sys-
tem response, CAST is actually performed over
a set of word lattices. A lattice, representing a
huge set of hypotheses for the current utterance,
is initially used to parse the user validated prefix
and then to search for the best completion (sug-
gestion).
</bodyText>
<subsectionHeader confidence="0.999782">
3.2 System Interface and Usage
</subsectionHeader>
<bodyText confidence="0.999958428571428">
The prototype has been designed to be intuitive
for professional speech transcribers and general
users; we expect most users to quickly get used
to the system without any previous experience or
external assistance.
The prototype features and operation mode are
described in the following items:
</bodyText>
<listItem confidence="0.997937941176471">
• The initial screen (Fig. 2) guides the user on
how to address a transcription project. Here,
the transcriber can select one of the three
main tasks that have to be performed to ob-
tain the final result.
• In the project management screen (Fig. 3),
the user can interact with the current projects
or create a new one. A project is a set of
input audio files to be transcribed along with
the partial transcription achieved and some
other related parameters.
• Once the current project has been selected, a
transcription session is started (Fig. 4). Dur-
ing this session, the application looks like a
standard OpenOffice word processor incor-
porating CAST features. Specifically, the
user can perform the following operations:
</listItem>
<page confidence="0.995218">
42
</page>
<table confidence="0.996076384615385">
utterance
ITER-0 prefix ( )
ITER-1 suffix (Nine extra soul are planned half beam discovered these years)
validated (Nine)
correction (extrasolar)
prefix (Nine extrasolar)
ITER-2 suffix (planets have been discovered these years)
validated (planets have been discovered)
correction (this)
prefix (Nine extrasolar planets have been discovered this)
FINAL suffix (year)
validated (#)
prefix (Nine extrasolar planets have been discovered this year)
</table>
<figureCaption confidence="0.988598">
Figure 1: Example of a CAST session. In each iteration, the system suggests a suffix based on the input utterance
and the previous prefix. After this, the user can validate part of the suggestion and type a correction to generate
a new prefix that can be used in the next iteration. This process is iterated until the full utterance is transcribed.
</figureCaption>
<bodyText confidence="0.933290454545454">
The user can move between audio segments
by pressing the “fast forward” and “rewind”
buttons. Once the a segment to be tran-
scribed has been chosen, the “play” button
starts the audio replay and transcription. The
system produces the text in synchrony with
the audio so that the user can check in “real
time” the proposed transcription. As soon as
a mistake is produced, the transcriber can use
the “pause” button to interrupt the process.
Then, the error is corrected and by pressing
“play” again the process is continued. At
this point, the CAST engine will use the user
amendment to improve the rest of the tran-
scription.
• When all the segments have been tran-
scribed, the final task in the initial screen al-
lows the user to open the OpenOffice’s PDF
export dialog to generate the final document.
A video, showing the prototype operation
mode, can be found on the following website:
www.youtube.com/watch?v=vc6bQCtYVR4.
</bodyText>
<sectionHeader confidence="0.999385" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999552103448276">
In this paper we have presented a CAST system
which has been fully implemented and integrated
into the OpenOffice word processing software.
The implemented techniques have been tested of-
fline and the prototype has been presented to a re-
duced number of real users.
Preliminary results suggest that the system
could be useful for transcribers when high qual-
ity transcriptions are needed. It is expected to
save effort, increase efficiency and allow inexperi-
enced users to take advantage of ASR systems all
along the transcription process. However, these
results should be corroborated by performing a
formal usability evaluation.
Currently, we are in the process of carrying out
a formal usability evaluation with real users that
has been designed following the ISO/IEC 9126-4
(2004) standard according to the efficiency, effec-
tiveness and satisfaction characteristics.
As future work, it will be interesting to consider
concurrent collaborative work at both, project and
transcription levels. Other promising line is to
consider a multimodal user interface in order to
allow users to control the playback and transcrip-
tion features using their own speech. This has
been explored in the literature (Rodr´ıguez et al.,
2010) and would allow the system to be used in
devices with constrained interfaces such as mo-
bile phones or tablet PCs.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999172">
Work supported by the EC (ERDF/ESF) and
the Spanish government under the MIPRCV
“Consolider Ingenio 2010” program (CSD2007-
00018), and the Spanish Junta de Comunidades
de Castilla-La Mancha regional government un-
der projects PBI08-0210-7127 and PPII11-0309-
6935.
</bodyText>
<page confidence="0.99983">
43
</page>
<figureCaption confidence="0.909442444444444">
Figure 2: Main window prototype. The three stages of a transcription project are shown.
Figure 3: Screenshot of the project management window showing a loaded project. A project consists of several
audio segments, each of them is stored in a file so that the user can easily add or remove files when needed. In
this screen the user can choose the current working segments.
Figure 4: Screenshot of a transcription session. This shows the process of transcribing one audio segment. In this
figure, all the text but the last incomplete sentence has already been transcribed and validated. The last partial
sentence, shown in italics, is being produced by the ASR system while the transcriber listen to the audio. As
soon as an error is detected the user momentarily interrupts the process to correct the mistake. Then, the system
will continue transcribing the audio according to the new user feedback (prefix).
</figureCaption>
<page confidence="0.998483">
44
</page>
<sectionHeader confidence="0.99833" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998049">
ISO/IEC 9126-4. 2004. Software engineering —
Product quality — Part 4: Quality in use metrics.
F. Jelinek. 1998. Statistical Methods for Speech
Recognition. The MIT Press, Cambridge, Mas-
sachusetts, USA.
Luis Rodriguez, Francisco Casacuberta, and Enrique
Vidal. 2007. Computer assisted transcription of
speech. In Proceedings of the 3rd Iberian confer-
ence on Pattern Recognition and Image Analysis,
Part I, IbPRIA ’07, pages 241–248, Berlin, Heidel-
berg. Springer-Verlag.
Luis Rodriguez, Ismael Garcia-Varea, and Enrique Vi-
dal. 2010. Multi-modal computer assisted speech
transcription. In Proceedings of the 12th Interna-
tional Conference on Multimodal Interfaces and the
7th International Workshop on Machine Learning
for Multimodal Interaction, ICMI-MLMI.
A.H. Toselli, E. Vidal, and F. Casacuberta. 2011. Mul-
timodal Interactive Pattern Recognition and Appli-
cations. Springer.
</reference>
<page confidence="0.999391">
45
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.669461">
<title confidence="0.999964">A Computer Assisted Speech Transcription System</title>
<author confidence="0.999499">Alejandro Revuelta-Martinez</author>
<author confidence="0.999499">Luis Rodriguez</author>
<author confidence="0.999499">Ismael</author>
<affiliation confidence="0.9998125">Computer Systems University of Castilla-La</affiliation>
<address confidence="0.757833">Albacete,</address>
<abstract confidence="0.993134588235294">Current automatic speech transcription systems can achieve a high accuracy although they still make mistakes. In some scenarios, high quality transcriptions are needed and, therefore, fully automatic systems are not suitable for them. These high accuracy tasks require a human transcriber. However, we consider that automatic techniques could improve the transcriber’s efficiency. With this idea we present an interactive speech recognition system integrated with a word processor in order to assists users when transcribing speech. This system automatically recognizes speech while allowing the user to interactively modify the transcription.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>ISOIEC</author>
</authors>
<date>2004</date>
<journal>Software engineering — Product quality — Part</journal>
<volume>4</volume>
<note>Quality in use metrics.</note>
<marker>ISOIEC, 2004</marker>
<rawString>ISO/IEC 9126-4. 2004. Software engineering — Product quality — Part 4: Quality in use metrics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
</authors>
<title>Statistical Methods for Speech Recognition.</title>
<date>1998</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts, USA.</location>
<contexts>
<context position="1301" citStr="Jelinek, 1998" startWordPosition="178" endWordPosition="179">nition system integrated with a word processor in order to assists users when transcribing speech. This system automatically recognizes speech while allowing the user to interactively modify the transcription. 1 Introduction Speech has been the main mean of communication for thousands of years and, hence, is the most natural human interaction mode. For this reason, Automatic Speech Recognition (ASR) has been one of the major research interests within the Natural Language Processing (NLP) community. Although current speech recognition approaches (which are based on statistical learning theory (Jelinek, 1998)) are speaker independent and achieve high accuracy, ASR systems are not perfect and transcription errors rise drastically when considering large vocabularies, dealing with noise environments or spontaneous speech. In those tasks (as for example, automatic transcription of parliaments proceedings) where perfect recognition results are required, ASR can not be fully reliable so far and, a human transcriber has to check and supervise the automatically generated transcriptions. In the last years, cooperative systems, where a human user and an automatic system work together, have gain growing atte</context>
</contexts>
<marker>Jelinek, 1998</marker>
<rawString>F. Jelinek. 1998. Statistical Methods for Speech Recognition. The MIT Press, Cambridge, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis Rodriguez</author>
<author>Francisco Casacuberta</author>
<author>Enrique Vidal</author>
</authors>
<title>Computer assisted transcription of speech.</title>
<date>2007</date>
<booktitle>In Proceedings of the 3rd Iberian conference on Pattern Recognition and Image Analysis, Part I, IbPRIA ’07,</booktitle>
<pages>241--248</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<marker>Rodriguez, Casacuberta, Vidal, 2007</marker>
<rawString>Luis Rodriguez, Francisco Casacuberta, and Enrique Vidal. 2007. Computer assisted transcription of speech. In Proceedings of the 3rd Iberian conference on Pattern Recognition and Image Analysis, Part I, IbPRIA ’07, pages 241–248, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis Rodriguez</author>
<author>Ismael Garcia-Varea</author>
<author>Enrique Vidal</author>
</authors>
<title>Multi-modal computer assisted speech transcription.</title>
<date>2010</date>
<booktitle>In Proceedings of the 12th International Conference on Multimodal Interfaces and the 7th International Workshop on Machine Learning for Multimodal Interaction, ICMI-MLMI.</booktitle>
<marker>Rodriguez, Garcia-Varea, Vidal, 2010</marker>
<rawString>Luis Rodriguez, Ismael Garcia-Varea, and Enrique Vidal. 2010. Multi-modal computer assisted speech transcription. In Proceedings of the 12th International Conference on Multimodal Interfaces and the 7th International Workshop on Machine Learning for Multimodal Interaction, ICMI-MLMI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A H Toselli</author>
<author>E Vidal</author>
<author>F Casacuberta</author>
</authors>
<title>Multimodal Interactive Pattern Recognition and Applications.</title>
<date>2011</date>
<publisher>Springer.</publisher>
<contexts>
<context position="3989" citStr="Toselli et al., 2011" startWordPosition="621" endWordPosition="624"> the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 41–45, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics where, as in traditional ASR, we have an acoustic model Pr(x |p, s) and a language model Pr(s |p). The main difference is that, here, part of the correct transcription is available (prefix) and we can use this information to improve the suffix recognition. This can be achieved by properly adapting the language model to account for the user validated prefix as it is detailed in (Rodr´ıguez et al., 2007; Toselli et al., 2011). As was commented before, the main goal of this approach is to improve the efficiency of the transcription process by saving user keystrokes. Off-line experiments have shown that this approach can save about 30% of typing effort when compared to the traditional approach of off-line post-editing results from an ASR system. 3 Prototype Description A fully functional prototype, which implements the CAST techniques described in section 2, has been developed. The main goal is to provide a completely usable tool. To this end, we have implemented a tool that easily allows for organizing and accessin</context>
</contexts>
<marker>Toselli, Vidal, Casacuberta, 2011</marker>
<rawString>A.H. Toselli, E. Vidal, and F. Casacuberta. 2011. Multimodal Interactive Pattern Recognition and Applications. Springer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>