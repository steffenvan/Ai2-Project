<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.041503">
<title confidence="0.966014">
NTNU-CORE: Combining strong features for semantic similarity
</title>
<author confidence="0.999855">
Erwin Marsi, Hans Moen, Lars Bungum, Gleb Sizov, Bj¨orn Gamb¨ack, Andr´e Lynum
</author>
<affiliation confidence="0.9988675">
Norwegian University of Science and Technology
Department of Computer and Information and Science
</affiliation>
<address confidence="0.9589455">
Sem Sælands vei 7-9
NO-7491 Trondheim, Norway
</address>
<email confidence="0.999599">
{emarsi,hansmoe,bungum,sizov,gamback,andrely}@idi.ntnu.no
</email>
<sectionHeader confidence="0.998605" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.947396266666667">
The paper outlines the work carried out at
NTNU as part of the *SEM’13 shared task
on Semantic Textual Similarity, using an ap-
proach which combines shallow textual, dis-
tributional and knowledge-based features by
a support vector regression model. Feature
sets include (1) aggregated similarity based
on named entity recognition with WordNet
and Levenshtein distance through the calcula-
tion of maximum weighted bipartite graphs;
(2) higher order word co-occurrence simi-
larity using a novel method called “Multi-
sense Random Indexing”; (3) deeper seman-
tic relations based on the RelEx semantic
dependency relationship extraction system;
</bodyText>
<listItem confidence="0.7268285">
(4) graph edit-distance on dependency trees;
(5) reused features of the TakeLab and DKPro
</listItem>
<bodyText confidence="0.924266333333333">
systems from the STS’12 shared task. The
NTNU systems obtained 9th place overall (5th
best team) and 1st place on the SMT data set.
</bodyText>
<sectionHeader confidence="0.999415" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999274">
Intuitively, two texts are semantically similar if they
roughly mean the same thing. The task of formally
establishing semantic textual similarity clearly is
more complex. For a start, it implies that we have
a way to formally represent the intended meaning of
all texts in all possible contexts, and furthermore a
way to measure the degree of equivalence between
two such representations. This goes far beyond the
state-of-the-art for arbitrary sentence pairs, and sev-
eral restrictions must be imposed. The Semantic
Textual Similarity (STS) task (Agirre et al., 2012,
2013) limits the comparison to isolated sentences
only (rather than complete texts), and defines sim-
ilarity of a pair of sentences as the one assigned by
human judges on a 0–5 scale (with 0 implying no
relation and 5 complete semantic equivalence). It is
unclear, however, to what extent two judges would
agree on the level of similarity between sentences;
Agirre et al. (2012) report figures on the agreement
between the authors themselves of about 87–89%.
As in most language processing tasks, there are
two overall ways to measure sentence similarity, ei-
ther by data-driven (distributional) methods or by
knowledge-driven methods; in the STS’12 task the
two approaches were used nearly equally much.
Distributional models normally measure similarity
in terms of word or word co-occurrence statistics, or
through concept relations extracted from a corpus.
The basic strategy taken by NTNU in the STS’13
task was to use something of a “feature carpet bomb-
ing approach” in the way of first automatically ex-
tracting as many potentially useful features as possi-
ble, using both knowledge and data-driven methods,
and then evaluating feature combinations on the data
sets provided by the organisers of the shared task.
To this end, four different types of features were
extracted. The first (Section 2) aggregates similar-
ity based on named entity recognition with WordNet
and Levenshtein distance by calculating maximum
weighted bipartite graphs. The second set of features
(Section 3) models higher order co-occurrence sim-
ilarity relations using Random Indexing (Kanerva
et al., 2000), both in the form of a (standard) sliding
window approach and through a novel method called
“Multi-sense Random Indexing” which aims to sep-
arate the representation of different senses of a term
</bodyText>
<page confidence="0.915614">
66
</page>
<note confidence="0.8982515">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 66–73, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999750588235294">
from each other. The third feature set (Section 4)
aims to capture deeper semantic relations using ei-
ther the output of the RelEx semantic dependency
relationship extraction system (Fundel et al., 2007)
or an in-house graph edit-distance matching system.
The final set (Section 5) is a straight-forward gath-
ering of features from the systems that fared best in
STS’12: TakeLab from University of Zagreb ( ˇSari´c
et al., 2012) and DKPro from Darmstadt’s Ubiqui-
tous Knowledge Processing Lab (B¨ar et al., 2012).
As described in Section 6, Support Vector Regres-
sion (Vapnik et al., 1997) was used for solving the
multi-dimensional regression problem of combining
all the extracted feature values. Three different sys-
tems were created based on feature performance on
the supplied development data. Section 7 discusses
scores on the STS’12 and STS’13 test data.
</bodyText>
<sectionHeader confidence="0.985753" genericHeader="introduction">
2 Compositional Word Matching
</sectionHeader>
<bodyText confidence="0.999474692307692">
Compositional word matching similarity is based
on a one-to-one alignment of words from the two
sentences. The alignment is obtained by maximal
weighted bipartite matching using several word sim-
ilarity measures. In addition, we utilise named entity
recognition and matching tools. In general, the ap-
proach is similar to the one described by Karnick
et al. (2012), with a different set of tools used. Our
implementation relies on the ANNIE components in
GATE (Cunningham et al., 2002) and will thus be
referred to as GateWordMatch.
The processing pipeline for GateWordMatch
is: (1) tokenization by ANNIE English Tokeniser,
</bodyText>
<listItem confidence="0.9944634">
(2) part-of-speech tagging by ANNIE POS Tagger,
(3) lemmatization by GATE Morphological Anal-
yser, (4) stopword removal, (5) named entity recog-
nition based on lists by ANNIE Gazetteer, (6) named
entity recognition based on the JAPE grammar by
the ANNIE NE Transducer, (7) matching of named
entities by ANNIE Ortho Matcher, (8) computing
WordNet and Levenstein similarity between words,
(9) calculation of a maximum weighted bipartite
graph matching based on similarities from 7 and 8.
</listItem>
<bodyText confidence="0.99992052631579">
Steps 1–4 are standard preprocessing routines.
In step 5, named entities are recognised based on
lists that contain locations, organisations, compa-
nies, newspapers, and person names, as well as date,
time and currency units. In step 6, JAPE grammar
rules are applied to recognise entities such as ad-
dresses, emails, dates, job titles, and person names
based on basic syntactic and morphological features.
Matching of named entities in step 7 is based on
matching rules that check the type of named entity,
and lists with aliases to identify entities as “US”,
“United State”, and “USA” as the same entity.
In step 8, similarity is computed for each pair
of words from the two sentences. Words that are
matched as entities in step 7 get a similarity value
of 1.0. For the rest of the entities and non-entity
words we use LCH (Leacock and Chodorow, 1998)
similarity, which is based on a shortest path between
the corresponding senses in WordNet. Since word
sense disambiguation is not used, we take the simi-
larity between the nearest senses of two words. For
cases when the WordNet-based similarity cannot be
obtained, a similarity based on the Levenshtein dis-
tance (Levenshtein, 1966) is used instead. It is nor-
malised by the length of the longest word in the pair.
For the STS’13 test data set, named entity matching
contributed to 4% of all matched word pairs; LCH
similarity to 61%, and Levenshtein distance to 35%.
In step 9, maximum weighted bipartite matching
is computed using the Hungarian Algorithm (Kuhn,
1955). Nodes in the bipartite graph represent words
from the sentences, and edges have weights that cor-
respond to similarities between tokens obtained in
step 8. Weighted bipartite matching finds the one-to-
one alignment that maximizes the sum of similarities
between aligned tokens. Total similarity normalised
by the number of words in both sentences is used as
the final sentence similarity measure.
</bodyText>
<sectionHeader confidence="0.987672" genericHeader="method">
3 Distributional Similarity
</sectionHeader>
<bodyText confidence="0.999779333333333">
Our distributional similarity features use Random
Indexing (RI; Kanerva et al., 2000; Sahlgren, 2005),
also employed in STS’12 by Tovar et al. (2012);
Sokolov (2012); Semeraro et al. (2012). It is an
efficient method for modelling higher order co-
occurrence similarities among terms, comparable to
Latent Semantic Analysis (LSA; Deerwester et al.,
1990). It incrementally builds a term co-occurrence
matrix of reduced dimensionality through the use of
a sliding window and fixed size index vectors used
for training context vectors, one per unique term.
A novel variant, which we have called “Multi-
</bodyText>
<page confidence="0.996971">
67
</page>
<bodyText confidence="0.999831585714286">
sense Random Indexing” (MSRI), inspired by
Reisinger and Mooney (2010), attempts to capture
one or more “senses” per unique term in an unsu-
pervised manner, each sense represented as an indi-
vidual vector in the model. The method is similar to
classical sliding window RI, but each term can have
multiple context vectors (referred to as “sense vec-
tors” here) which are updated individually. When
updating a term vector, instead of directly adding the
index vectors of the neighbouring terms in the win-
dow to its context vector, the system first computes a
separate window vector consisting of the sum of the
index vectors. Then cosine similarity is calculated
between the window vector and each of the term’s
sense vectors. Each similarity score is in turn com-
pared to a set similarity threshold: if no score ex-
ceeds the threshold, the sentence vector is added as
a new separate sense vector for the term; if exactly
one score is above the threshold, the window vector
is added to that sense vector; and if multiple scores
are above the threshold, all the involved senses are
merged into one sense vector, together with the win-
dow vector. This accomplishes an incremental clus-
tering of senses in an unsupervised manner while re-
taining the efficiency of classical RI.
As data for training the models we used the
CLEF 2004–2008 English corpus (approx. 130M
words). Our implementation of RI and MSRI is
based on JavaSDM (Hassel, 2004). For classical
RI, we used stopword removal (using a customised
versions of the English stoplist from the Lucene
project), window size of 4+4, dimensionality set to
1800, 4 non-zeros, and unweighted index vector in
the sliding window. For MSRI, we used a simi-
larity threshold of 0.2, a vector dimensionality of
800, a non-zero count of 4, and window size of
5+5. The index vectors in the sliding window were
shifted to create direction vectors (Sahlgren et al.,
2008), and weighted by distance to the target term.
Rare senses with a frequency below 10 were ex-
cluded. Other sliding-window schemes, including
unweighted non-shifted vectors and Random Permu-
tation (Sahlgren et al., 2008), were tested, but none
outperformed the sliding-window schemes used.
Similarity between sentence pairs was calcu-
lated as the normalised maximal bipartite similar-
ity between term pairs in each sentence, resulting
in the following features: (1) MSRI-Centroid:
each term is represented as the sum of its sense
vectors; (2) MSRI-MaxSense: for each term
pair, the sense-pair with max similarity is used;
(3) MSRI-Context: for each term, its neigh-
bouring terms within a window of 2+2 is used as
context for picking a single, max similar, sense
from the target term to be used as its represen-
tation; (4) MSRI-HASenses: similarity between
two terms is computed by applying the Hungarian
Algorithm to all their possible sense pair mappings;
(5) RI-Avg: classical RI, each term is represented
as a single context vector; (6) RI-Hungarian:
similarity between two sentences is calculated us-
ing the Hungarian Algorithm. Alternatively, sen-
tence level similarity was computed as the cosine
similarity between sentence vectors composed of
their terms’ vectors. The corresponding features
are (1) RI-SentVectors-Norm: sentence vec-
tors are created by summing their constituent terms
(i.e., context vectors), which have first been normal-
ized; (2) RI-SentVectors-TFIDF: same as be-
fore, but TF*IDF weights are added.
</bodyText>
<sectionHeader confidence="0.995773" genericHeader="method">
4 Deeper Semantic Relations
</sectionHeader>
<bodyText confidence="0.9995018">
Two deep strategies were employed to accompany
the shallow-processed feature sets. Two existing
systems were used to provide the basis for these fea-
tures, namely the RelEx system (Fundel et al., 2007)
from the OpenCog initiative (Hart and Goertzel,
2008), and an in-house graph-edit distance system
developed for plagiarism detection (Røkenes, 2013).
RelEx outputs syntactic trees, dependency graphs,
and semantic frames as this one for the sentence
“Indian air force to buy 126 Rafale fighter jets”:
</bodyText>
<equation confidence="0.987385625">
Commerce buy:Goods(buy,jet)
Entity:Entity(jet,jet)
Entity:Name(jet,Rafale)
Entity:Name(jet,fighter)
Possibilities:Event(hyp,buy)
Request:Addressee(air,you)
Request:Message(air,air)
Transitive action:Beneficiary(buy,jet)
</equation>
<bodyText confidence="0.9999484">
Three features were extracted from this: first, if
there was an exact match of the frame found in s1
with s2; second, if there was a partial match until the
first argument (Commerce buy:Goods(buy);
and third if there was a match of the frame category
</bodyText>
<page confidence="0.996914">
68
</page>
<bodyText confidence="0.998876470588235">
(Commerce buy:Goods).
In STS’12, Singh et al. (2012) matched Universal
Networking Language (UNL) graphs against each
other by counting matches of relations and univer-
sal words, while Bhagwani et al. (2012) calculated
WordNet-based word-level similarities and created
a weighted bipartite graph (see Section 2). The
method employed here instead looked at the graph
edit distance between dependency graphs obtained
with the Maltparser dependency parser (Nivre et al.,
2006). Edit distance is the defined as the minimum
of the sum of the costs of the edit operations (in-
sertion, deletion and substitution of nodes) required
to transform one graph into the other. It is approx-
imated with a fast but suboptimal algorithm based
on bipartite graph matching through the Hungarian
algorithm (Riesen and Bunke, 2009).
</bodyText>
<sectionHeader confidence="0.998861" genericHeader="method">
5 Reused Features
</sectionHeader>
<bodyText confidence="0.999415590909091">
The TakeLab ‘simple’ system (ˇSari´c et al., 2012) ob-
tained 3rd place in overall Pearson correlation and
1st for normalized Pearson in STS’12. The source
code1 was used to generate all its features, that is,
n-gram overlap, WordNet-augmented word overlap,
vector space sentence similarity, normalized differ-
ence, shallow NE similarity, numbers overlap, and
stock index features.2 This required the full LSA
vector space models, which were kindly provided
by the TakeLab team. The word counts required for
computing Information Content were obtained from
Google Books Ngrams.3
The DKPro system (B¨ar et al., 2012) obtained first
place in STS’12 with the second run. We used the
source code4 to generate features for the STS’12
and STS’13 data. Of the string-similarity features,
we reused the Longest Common Substring, Longest
Common Subsequence (with and without normaliza-
tion), and Greedy String Tiling measures. From the
character/word n-grams features, we used Charac-
ter n-grams (n = 2, 3, 4), Word n-grams by Con-
tainment w/o Stopwords (n = 1, 2), Word n-grams
</bodyText>
<footnote confidence="0.993572571428571">
1http://takelab.fer.hr/sts/
2We did not use content n-gram overlap or skip n-grams.
3http://storage.googleapis.com/books/
ngrams/books/datasetsv2.html, version 20120701,
with 468,491,999,592 words
4http://code.google.com/p/
dkpro-similarity-asl/
</footnote>
<bodyText confidence="0.993079142857143">
by Jaccard (n = 1, 3, 4), and Word n-grams by Jac-
card w/o Stopwords (n = 2, 4). Semantic similarity
measures include WordNet Similarity based on the
Resnik measure (two variants) and Explicit Seman-
tic Similarity based on WordNet, Wikipedia or Wik-
tionary. This means that we reused all features from
DKPro run 1 except for Distributional Thesaurus.
</bodyText>
<sectionHeader confidence="0.998683" genericHeader="method">
6 Systems
</sectionHeader>
<bodyText confidence="0.999983081081081">
Our systems follow previous submissions to the STS
task (e.g., ˇSari´c et al., 2012; Banea et al., 2012) in
that feature values are extracted for each sentence
pair and combined with a gold standard score in or-
der to train a Support Vector Regressor on the result-
ing regression task. A postprocessing step guaran-
tees that all scores are in the [0, 5] range and equal 5
if the two sentences are identical. SVR has been
shown to be a powerful technique for predictive data
analysis when the primary goal is to approximate a
function, since the learning algorithm is applicable
to continuous classes. Hence support vector regres-
sion differs from support vector machine classifica-
tion where the goal rather is to take a binary deci-
sion. The key idea in SVR is to use a cost function
for building the model which tries to ignore noise in
training data (i.e., data which is too close to the pre-
diction), so that the produced model in essence only
depends on a more robust subset of the extracted fea-
tures.
Three systems were created using the supplied
annotated data based on Microsoft Research Para-
phrase and Video description corpora (MSRpar and
MSvid), statistical machine translation system out-
put (SMTeuroparl and SMTnews), and sense map-
pings between OntoNotes and WordNet (OnWN).
The first system (NTNU1) includes all TakeLab and
DKPro features plus the GateWordMatch feature
with the SVR in its default setting.5 The training
material consisted of all annotated data available,
except for the SMT test set, where it was limited to
SMTeuroparl and SMTnews. The NTNU2 system is
similar to NTNU1, except that the training material
for OnWN and FNWN excluded MSRvid and that
the SVR parameter C was set to 200. NTNU3 is
similar to NTNU1 except that all features available
are included.
</bodyText>
<footnote confidence="0.5986445">
5RBF kernel, e = 0.1, C = #samples, -y = 1
#fe���res
</footnote>
<page confidence="0.995231">
69
</page>
<table confidence="0.999897571428571">
Data NTNU1 NTNU2 NTNU3
MSRpar 0.7262 0.7507 0.7221
MSRvid 0.8660 0.8882 0.8662
SMTeuroparl 0.5843 0.3386 0.5503
SMTnews 0.5840 0.5592 0.5306
OnWN 0.7503 0.6365 0.7200
mean 0.7022 0.6346 0.6779
</table>
<tableCaption confidence="0.999972">
Table 1: Correlation score on 2012 test data
</tableCaption>
<sectionHeader confidence="0.998332" genericHeader="evaluation">
7 Results
</sectionHeader>
<bodyText confidence="0.999959638888889">
System performance is evaluated using the Pearson
product-moment correlation coefficient (r) between
the system scores and the human scores. Results on
the 2012 test data (i.e., 2013 development data) are
listed in Table 1. This basically shows that except
for the GateWordMatch, adding our other fea-
tures tends to give slightly lower scores (cf. NTNU1
vs NTNU3). In addition, the table illustrates that op-
timizing the SVR according to cross-validated grid
search on 2012 training data (here C = 200), rarely
pays off when testing on unseen data (cf. NTNU1
vs NTNU2).
Table 2 shows the official results on the test data.
These are generally in agreement with the scores on
the development data, although substantially lower.
Our systems did particularly well on SMT, holding
first and second position, reasonably good on head-
lines, but not so well on the ontology alignment data,
resulting in overall 9th (NTNU1) and 12th (NTNU3)
system positions (5th best team). Table 3 lists the
correlation score and rank of the ten best individual
features per STS’13 test data set, and those among
the top-20 overall, resulting from linear regression
on a single feature. Features in boldface are gen-
uinely new (i.e., described in Sections 2–4).
Overall the character n-gram features are the most
informative, particularly for HeadLine and SMT.
The reason may be that these not only capture word
overlap (Ahn, 2011), but also inflectional forms and
spelling variants.
The (weighted) distributional similarity features
based on NYT are important for HeadLine and SMT,
which obviously contain sentence pairs from the
news genre, whereas the Wikipedia based feature is
more important for OnWN and FNWN. WordNet-
based measures are highly relevant too, with variants
</bodyText>
<table confidence="0.999818857142857">
Data NTNU1 n NTNU2 n NTNU3
r r r n
Head 0.7279 11 0.5909 59 0.7274 12
OnWN 0.5952 31 0.1634 86 0.5882 32
FNWN 0.3215 45 0.3650 27 0.3115 49
SMT 0.4015 2 0.3786 9 0.4035 1
mean 0.5519 9 0.3946 68 0.5498 12
</table>
<tableCaption confidence="0.999908">
Table 2: Correlation score and rank on 2013 test data
</tableCaption>
<bodyText confidence="0.999691842105263">
relying on path length outperforming those based on
Resnik similarity, except for SMT.
As is to be expected, basic word and lemma uni-
gram overlap prove to be informative, with overall
unweighted variants resulting in higher correlation.
Somewhat surprisingly, higher order n-gram over-
laps (n &gt; 1) seem to be less relevant. Longest com-
mon subsequence and substring appear to work par-
ticularly well for OnWN and FNWN, respectively.
GateWordMatch is highly relevant too, in
agreement with earlier results on the development
data. Although treated as a single feature, it is ac-
tually a combination of similarity features where an
appropriate feature is selected for each word pair.
This “vertical” way of combining features can po-
tentially provide a more fine-grained feature selec-
tion, resulting in less noise. Indeed, if two words are
matching as named entities or as close synonyms,
less precise types of features such as character-based
and data-driven similarity should not dominate the
overall similarity score.
It is interesting to find that MSRI outper-
forms both classical RI and ESA (Gabrilovich and
Markovitch, 2007) on this task. Still, the more ad-
vanced features, such as MSRT-Context, gave in-
ferior results compared to MSRT-Centroid. This
suggests that more research on MSRI is needed
to understand how both training and retrieval can
be optimised. Also, LSA-based features (see
tl.weight-dist-sim-wiki) achieve better
results than both MSRI, RI and ESA. Then again,
larger corpora were used for training the LSA mod-
els. RI has been shown to be comparable to LSA
(Karlgren and Sahlgren, 2001), and since a relatively
small corpus was used for training the RI/MSRI
models, there are reasons to believe that better
scores can be achieved by both RI- and MSRI-based
features by using more training data.
</bodyText>
<page confidence="0.995212">
70
</page>
<table confidence="0.997923484848485">
HeadLine OnWN FNWN SMT Mean
Features r n r n r n r n r n
CharacterNGramMeasure-3 0.72 2 0.39 2 0.44 3 0.70 1 0.56 1
CharacterNGramMeasure-4 0.69 3 0.38 5 0.45 2 0.67 6 0.55 2
CharacterNGramMeasure-2 0.73 1 0.37 9 0.34 10 0.69 2 0.53 3
tl.weight-dist-sim-wiki 0.58 14 0.39 3 0.45 1 0.67 5 0.52 4
tl.wn-sim-lem 0.69 4 0.40 1 0.41 5 0.59 10 0.52 5
GateWordMatch 0.67 8 0.37 11 0.34 11 0.60 9 0.50 6
tl.dist-sim-nyt 0.69 5 0.34 28 0.26 23 0.65 8 0.49 7
tl.n-gram-match-lem-1 0.68 6 0.36 16 0.37 8 0.51 14 0.48 8
tl.weight-dist-sim-nyt 0.57 17 0.37 14 0.29 18 0.66 7 0.47 9
tl.n-gram-match-lc-1 0.68 7 0.37 10 0.32 13 0.50 17 0.47 10
MCS06-Resnik-WordNet 0.49 26 0.36 22 0.28 19 0.68 3 0.45 11
TWSI-Resnik-WordNet 0.49 27 0.36 23 0.28 20 0.68 4 0.45 12
tl.weight-word-match-lem 0.56 18 0.37 16 0.37 7 0.50 16 0.45 13
MSRI-Centroid 0.60 13 0.36 17 0.37 9 0.45 19 0.45 14
tl.weight-word-match-olc 0.56 19 0.38 8 0.32 12 0.51 15 0.44 15
MSRI-MaxSense 0.58 15 0.36 15 0.31 14 0.45 20 0.42 16
GreedyStringTiling-3 0.67 9 0.38 6 0.31 15 0.34 29 0.43 17
ESA-Wikipedia 0.50 25 0.30 38 0.32 14 0.54 12 0.42 18
WordNGramJaccard-1 0.64 10 0.37 12 0.25 25 0.33 30 0.40 19
WordNGramContainment-1-stopword 0.64 25 0.38 7 0.25 24 0.32 31 0.40 20
RI-Hungarian 0.58 16 0.33 31 0.10 34 0.42 22 0.36 24
RI-AvgTermTerm 0.56 20 0.33 32 0.11 33 0.37 28 0.34 25
LongestCommonSubstring 0.40 29 0.30 39 0.42 4 0.37 27 0.37 26
ESA-WordNet 0.11 43 0.30 40 0.41 6 0.49 18 0.33 29
LongestCommonSubsequenceNorm 0.53 21 0.39 4 0.19 27 0.18 37 0.32 30
MultisenseRI-ContextTermTerm 0.39 31 0.33 33 0.28 21 0.15 38 0.29 33
MultisenseRI-HASensesTermTerm 0.39 32 0.33 34 0.28 22 0.15 39 0.29 34
RI-SentVectors-Norm 0.34 35 0.35 26 -0.01 51 0.24 35 0.23 39
RelationSimilarity 0.31 39 0.35 27 0.24 26 0.02 41 0.23 40
RI-SentVectors-TFIDF 0.27 40 0.15 50 0.08 40 0.23 36 0.18 41
GraphEditDistance 0.33 38 0.25 46 0.13 31 -0.11 49 0.15 42
</table>
<tableCaption confidence="0.999914">
Table 3: Correlation score and rank of the best features
</tableCaption>
<sectionHeader confidence="0.960887" genericHeader="conclusions">
8 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999971523809524">
The NTNU system can be regarded as continuation
of the most successful systems from the STS’12
shared task, combining shallow textual, distribu-
tional and knowledge-based features into a support
vector regression model. It reuses features from the
TakeLab and DKPro systems, resulting in a very
strong baseline.
Adding new features to further improve
performance turned out to be hard: only
GateWordMatch yielded improved perfor-
mance. Similarity features based on both classical
and innovative variants of Random Indexing were
shown to correlate with semantic textual similarity,
but did not complement the existing distributional
features. Likewise, features designed to reveal
deeper syntactic (graph edit distance) and semantic
relations (RelEx) did not add to the score.
As future work, we would aim to explore a
vertical feature composition approach similar to
GateWordMatch and contrast it with the “flat”
composition currently used in our systems.
</bodyText>
<sectionHeader confidence="0.997752" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9997505">
Thanks to TakeLab for source code of their ‘simple’
system and the full-scale LSA models. Thanks to the
team from Ubiquitous Knowledge Processing Lab
for source code of their DKPro Similarity system.
</bodyText>
<page confidence="0.998138">
71
</page>
<sectionHeader confidence="0.995678" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.982937068181818">
Agirre, E., Cer, D., Diab, M., and Gonzalez-Agirre,
A. (2012). SemEval-2012 Task 6: A pilot on se-
mantic textual similarity. In *SEM (2012), pages
385–393.
Agirre, E., Cer, D., Diab, M., Gonzalez-Agirre, A.,
and Guo, W. (2013). *SEM 2013 Shared Task:
Semantic textual similarity, including a pilot on
typed-similarity. In *SEM 2013: The Second
Joint Conference on Lexical and Computational
Semantics. Association for Computational Lin-
guistics.
Ahn, C. S. (2011). Automatically detecting authors’
native language. PhD thesis, Monterey, Califor-
nia. Naval Postgraduate School.
Banea, C., Hassan, S., Mohler, M., and Mihalcea, R.
(2012). UNT: a supervised synergistic approach
to semantic text similarity. In *SEM (2012),
pages 635–642.
B¨ar, D., Biemann, C., Gurevych, I., and Zesch, T.
(2012). UKP: Computing semantic textual sim-
ilarity by combining multiple content similarity
measures. In *SEM (2012), pages 435–440.
Bhagwani, S., Satapathy, S., and Karnick, H. (2012).
sranjans : Semantic textual similarity using maxi-
mal weighted bipartite graph matching. In *SEM
2012: The First Joint Conference on Lexical and
Computational Semantics – Volume 1: Proceed-
ings of the main conference and the shared task,
and Volume 2: Proceedings of the Sixth Interna-
tional Workshop on Semantic Evaluation (Sem-
Eval 2012), pages 579–585, Montr´eal, Canada.
Association for Computational Linguistics.
Cunningham, H., Maynard, D., Bontcheva, K., and
Tablan, V. (2002). GATE: A framework and
graphical development environment for robust
NLP tools and applications. In Proceedings of the
40th Annual Meeting of the Association for Com-
putational Linguistics, pages 168–175, Philadel-
phia, Pennsylvania. ACL.
Deerwester, S., Dumais, S., Furnas, G., Landauer,
T., and Harshman, R. (1990). Indexing by latent
semantic analysis. Journal of the American Soci-
ety for Information Science, 41(6):391–407.
Fundel, K., K¨uffner, R., and Zimmer, R. (2007).
RelEx - Relation extraction using dependency
parse trees. Bioinformatics, 23(3):365–371.
Gabrilovich, E. and Markovitch, S. (2007). Comput-
ing semantic relatedness using wikipedia-based
explicit semantic analysis. In Proceedings of The
Twentieth International Joint Conference for Ar-
tificial Intelligence., pages 1606–1611.
Hart, D. and Goertzel, B. (2008). Opencog: A soft-
ware framework for integrative artificial general
intelligence. In Proceedings of the 2008 confer-
ence on Artificial General Intelligence 2008: Pro-
ceedings of the First AGI Conference, pages 468–
472, Amsterdam, The Netherlands, The Nether-
lands. IOS Press.
Hassel, M. (2004). JavaSDM package.
Kanerva, P., Kristoferson, J., and Holst, A. (2000).
Random indexing of text samples for latent se-
mantic analysis. In Gleitman, L. and Josh, A.,
editors, Proceedings of the 22nd Annual Confer-
ence of the Cognitive Science Society, page 1036.
Erlbaum.
Karlgren, J. and Sahlgren, M. (2001). From Words
to Understanding. In Uesaka, Y., Kanerva, P., and
Asoh, H., editors, Foundations of real-world in-
telligence, chapter 26, pages 294–311. Stanford:
CSLI Publications.
Karnick, H., Satapathy, S., and Bhagwani, S. (2012).
sranjans: Semantic textual similarity using max-
imal bipartite graph matching. In *SEM (2012),
pages 579–585.
Kuhn, H. (1955). The Hungarian method for the as-
signment problem. Naval research logistics quar-
terly, 2:83–97.
Leacock, C. and Chodorow, M. (1998). Combin-
ing local context and WordNet similarity for word
sense identification. WordNet: An electronic lexi-
cal....
Levenshtein, V. I. (1966). Binary codes capable of
correcting deletions, insertions and reversals. So-
viet Physics Doklady, 10(8):707–710.
Nivre, J., Hall, J., and Nilsson, J. (2006). Malt-
parser: A data-driven parser-generator for depen-
dency parsing. In In Proc. of LREC-2006, pages
2216–2219.
</reference>
<page confidence="0.977637">
72
</page>
<reference confidence="0.99344215">
Reisinger, J. and Mooney, R. (2010). Multi-
prototype vector-space models of word meaning.
In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter
of the Association for Computational Linguistics,
number June, pages 109–117.
Riesen, K. and Bunke, H. (2009). Approximate
graph edit distance computation by means of bi-
partite graph matching. Image and Vision Com-
puting, 27(7):950–959.
Røkenes, H. (2013). Graph-Edit Distance Applied to
the Task of Detecting Plagiarism. Master’s the-
sis, Norwegian University of Science and Tech-
nology.
Sahlgren, M. (2005). An introduction to random in-
dexing. In Methods and Applications of Seman-
tic Indexing Workshop at the 7th International
Conference on Terminology and Knowledge En-
gineering, TKE, volume 5.
Sahlgren, M., Holst, A., and Kanerva, P. (2008). Per-
mutations as a Means to Encode Order in Word
Space. Proceedings of the 30th Conference of the
Cognitive Science Society.
ˇSari´c, F., Glavaˇs, G., Karan, M., ˇSnajder, J., and
Baˇsi´c, B. D. (2012). TakeLab: systems for mea-
suring semantic text similarity. In *SEM (2012),
pages 441–448.
*SEM (2012). Proceedings of the First Joint Con-
ference on Lexical and Computational Seman-
tics (*SEM), volume 2: Proceedings of the Sixth
International Workshop on Semantic Evaluation,
Montreal, Canada. Association for Computational
Linguistics.
Semeraro, G., Aldo, B., and Orabona, V. E. (2012).
UNIBA: Distributional semantics for textual sim-
ilarity. In *SEM (2012), pages 591–596.
Singh, J., Bhattacharya, A., and Bhattacharyya, P.
(2012). janardhan: Semantic textual similarity us-
ing universal networking language graph match-
ing. In *SEM 2012: The First Joint Conference
on Lexical and Computational Semantics – Vol-
ume 1: Proceedings of the main conference and
the shared task, and Volume 2: Proceedings of the
Sixth International Workshop on Semantic Evalu-
ation (SemEval 2012), pages 662–666, Montr´eal,
Canada. Association for Computational Linguis-
tics.
Sokolov, A. (2012). LIMSI: learning semantic simi-
larity by selecting random word subsets. In *SEM
(2012), pages 543–546.
Tovar, M., Reyes, J., and Montes, A. (2012). BUAP:
a first approximation to relational similarity mea-
suring. In *SEM (2012), pages 502–505.
Vapnik, V., Golowich, S. E., and Smola, A. (1997).
Support vector method for function approxima-
tion, regression estimation, and signal process-
ing. In Mozer, M. C., Jordan, M. I., and Petsche,
T., editors, Advances in Neural Information Pro-
cessing Systems, volume 9, pages 281–287. MIT
Press, Cambridge, Massachusetts.
</reference>
<page confidence="0.999299">
73
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.599082">
<title confidence="0.999347">NTNU-CORE: Combining strong features for semantic similarity</title>
<author confidence="0.997056">Erwin Marsi</author>
<author confidence="0.997056">Hans Moen</author>
<author confidence="0.997056">Lars Bungum</author>
<author confidence="0.997056">Gleb Sizov</author>
<author confidence="0.997056">Bj¨orn Gamb¨ack</author>
<author confidence="0.997056">Andr´e</author>
<affiliation confidence="0.996237">Norwegian University of Science and Department of Computer and Information and</affiliation>
<note confidence="0.669753">Sem Sælands vei NO-7491 Trondheim,</note>
<abstract confidence="0.994378761904762">The paper outlines the work carried out at NTNU as part of the *SEM’13 shared task on Semantic Textual Similarity, using an approach which combines shallow textual, distributional and knowledge-based features by a support vector regression model. Feature sets include (1) aggregated similarity based on named entity recognition with WordNet and Levenshtein distance through the calculation of maximum weighted bipartite graphs; (2) higher order word co-occurrence similarity using a novel method called “Multisense Random Indexing”; (3) deeper semantic relations based on the RelEx semantic dependency relationship extraction system; (4) graph edit-distance on dependency trees; (5) reused features of the TakeLab and DKPro systems from the STS’12 shared task. The NTNU systems obtained 9th place overall (5th best team) and 1st place on the SMT data set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>D Cer</author>
<author>M Diab</author>
<author>A Gonzalez-Agirre</author>
</authors>
<title>SemEval-2012 Task 6: A pilot on semantic textual similarity. In *SEM</title>
<date>2012</date>
<pages>385--393</pages>
<contexts>
<context position="1789" citStr="Agirre et al., 2012" startWordPosition="260" endWordPosition="263">) and 1st place on the SMT data set. 1 Introduction Intuitively, two texts are semantically similar if they roughly mean the same thing. The task of formally establishing semantic textual similarity clearly is more complex. For a start, it implies that we have a way to formally represent the intended meaning of all texts in all possible contexts, and furthermore a way to measure the degree of equivalence between two such representations. This goes far beyond the state-of-the-art for arbitrary sentence pairs, and several restrictions must be imposed. The Semantic Textual Similarity (STS) task (Agirre et al., 2012, 2013) limits the comparison to isolated sentences only (rather than complete texts), and defines similarity of a pair of sentences as the one assigned by human judges on a 0–5 scale (with 0 implying no relation and 5 complete semantic equivalence). It is unclear, however, to what extent two judges would agree on the level of similarity between sentences; Agirre et al. (2012) report figures on the agreement between the authors themselves of about 87–89%. As in most language processing tasks, there are two overall ways to measure sentence similarity, either by data-driven (distributional) meth</context>
</contexts>
<marker>Agirre, Cer, Diab, Gonzalez-Agirre, 2012</marker>
<rawString>Agirre, E., Cer, D., Diab, M., and Gonzalez-Agirre, A. (2012). SemEval-2012 Task 6: A pilot on semantic textual similarity. In *SEM (2012), pages 385–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>D Cer</author>
<author>M Diab</author>
<author>A Gonzalez-Agirre</author>
<author>W Guo</author>
</authors>
<title>Shared Task: Semantic textual similarity, including a pilot on typed-similarity.</title>
<date>2013</date>
<booktitle>In *SEM 2013: The Second Joint Conference on Lexical and Computational Semantics. Association for Computational Linguistics.</booktitle>
<publisher>SEM</publisher>
<marker>Agirre, Cer, Diab, Gonzalez-Agirre, Guo, 2013</marker>
<rawString>Agirre, E., Cer, D., Diab, M., Gonzalez-Agirre, A., and Guo, W. (2013). *SEM 2013 Shared Task: Semantic textual similarity, including a pilot on typed-similarity. In *SEM 2013: The Second Joint Conference on Lexical and Computational Semantics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C S Ahn</author>
</authors>
<title>Automatically detecting authors’ native language.</title>
<date>2011</date>
<tech>PhD thesis,</tech>
<institution>Monterey, California. Naval Postgraduate School.</institution>
<contexts>
<context position="18741" citStr="Ahn, 2011" startWordPosition="2944" endWordPosition="2945">sition, reasonably good on headlines, but not so well on the ontology alignment data, resulting in overall 9th (NTNU1) and 12th (NTNU3) system positions (5th best team). Table 3 lists the correlation score and rank of the ten best individual features per STS’13 test data set, and those among the top-20 overall, resulting from linear regression on a single feature. Features in boldface are genuinely new (i.e., described in Sections 2–4). Overall the character n-gram features are the most informative, particularly for HeadLine and SMT. The reason may be that these not only capture word overlap (Ahn, 2011), but also inflectional forms and spelling variants. The (weighted) distributional similarity features based on NYT are important for HeadLine and SMT, which obviously contain sentence pairs from the news genre, whereas the Wikipedia based feature is more important for OnWN and FNWN. WordNetbased measures are highly relevant too, with variants Data NTNU1 n NTNU2 n NTNU3 r r r n Head 0.7279 11 0.5909 59 0.7274 12 OnWN 0.5952 31 0.1634 86 0.5882 32 FNWN 0.3215 45 0.3650 27 0.3115 49 SMT 0.4015 2 0.3786 9 0.4035 1 mean 0.5519 9 0.3946 68 0.5498 12 Table 2: Correlation score and rank on 2013 test </context>
</contexts>
<marker>Ahn, 2011</marker>
<rawString>Ahn, C. S. (2011). Automatically detecting authors’ native language. PhD thesis, Monterey, California. Naval Postgraduate School.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Banea</author>
<author>S Hassan</author>
<author>M Mohler</author>
<author>R Mihalcea</author>
</authors>
<title>UNT: a supervised synergistic approach to semantic text similarity.</title>
<date>2012</date>
<booktitle>In *SEM</booktitle>
<pages>635--642</pages>
<contexts>
<context position="15354" citStr="Banea et al., 2012" startWordPosition="2378" endWordPosition="2381">://storage.googleapis.com/books/ ngrams/books/datasetsv2.html, version 20120701, with 468,491,999,592 words 4http://code.google.com/p/ dkpro-similarity-asl/ by Jaccard (n = 1, 3, 4), and Word n-grams by Jaccard w/o Stopwords (n = 2, 4). Semantic similarity measures include WordNet Similarity based on the Resnik measure (two variants) and Explicit Semantic Similarity based on WordNet, Wikipedia or Wiktionary. This means that we reused all features from DKPro run 1 except for Distributional Thesaurus. 6 Systems Our systems follow previous submissions to the STS task (e.g., ˇSari´c et al., 2012; Banea et al., 2012) in that feature values are extracted for each sentence pair and combined with a gold standard score in order to train a Support Vector Regressor on the resulting regression task. A postprocessing step guarantees that all scores are in the [0, 5] range and equal 5 if the two sentences are identical. SVR has been shown to be a powerful technique for predictive data analysis when the primary goal is to approximate a function, since the learning algorithm is applicable to continuous classes. Hence support vector regression differs from support vector machine classification where the goal rather i</context>
</contexts>
<marker>Banea, Hassan, Mohler, Mihalcea, 2012</marker>
<rawString>Banea, C., Hassan, S., Mohler, M., and Mihalcea, R. (2012). UNT: a supervised synergistic approach to semantic text similarity. In *SEM (2012), pages 635–642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D B¨ar</author>
<author>C Biemann</author>
<author>I Gurevych</author>
<author>T Zesch</author>
</authors>
<title>UKP: Computing semantic textual similarity by combining multiple content similarity measures. In *SEM</title>
<date>2012</date>
<pages>435--440</pages>
<marker>B¨ar, Biemann, Gurevych, Zesch, 2012</marker>
<rawString>B¨ar, D., Biemann, C., Gurevych, I., and Zesch, T. (2012). UKP: Computing semantic textual similarity by combining multiple content similarity measures. In *SEM (2012), pages 435–440.</rawString>
</citation>
<citation valid="false">
<authors>
<author>S Bhagwani</author>
<author>S Satapathy</author>
<author>H Karnick</author>
</authors>
<title>sranjans : Semantic textual similarity using maximal weighted bipartite graph matching.</title>
<date>2012</date>
<booktitle>In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>579--585</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="12961" citStr="Bhagwani et al. (2012)" startWordPosition="2016" endWordPosition="2019">:Name(jet,Rafale) Entity:Name(jet,fighter) Possibilities:Event(hyp,buy) Request:Addressee(air,you) Request:Message(air,air) Transitive action:Beneficiary(buy,jet) Three features were extracted from this: first, if there was an exact match of the frame found in s1 with s2; second, if there was a partial match until the first argument (Commerce buy:Goods(buy); and third if there was a match of the frame category 68 (Commerce buy:Goods). In STS’12, Singh et al. (2012) matched Universal Networking Language (UNL) graphs against each other by counting matches of relations and universal words, while Bhagwani et al. (2012) calculated WordNet-based word-level similarities and created a weighted bipartite graph (see Section 2). The method employed here instead looked at the graph edit distance between dependency graphs obtained with the Maltparser dependency parser (Nivre et al., 2006). Edit distance is the defined as the minimum of the sum of the costs of the edit operations (insertion, deletion and substitution of nodes) required to transform one graph into the other. It is approximated with a fast but suboptimal algorithm based on bipartite graph matching through the Hungarian algorithm (Riesen and Bunke, 2009</context>
</contexts>
<marker>Bhagwani, Satapathy, Karnick, 2012</marker>
<rawString>Bhagwani, S., Satapathy, S., and Karnick, H. (2012). sranjans : Semantic textual similarity using maximal weighted bipartite graph matching. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 579–585, Montr´eal, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cunningham</author>
<author>D Maynard</author>
<author>K Bontcheva</author>
<author>V Tablan</author>
</authors>
<title>GATE: A framework and graphical development environment for robust NLP tools and applications.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>168--175</pages>
<publisher>ACL.</publisher>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="5176" citStr="Cunningham et al., 2002" startWordPosition="790" endWordPosition="793">e performance on the supplied development data. Section 7 discusses scores on the STS’12 and STS’13 test data. 2 Compositional Word Matching Compositional word matching similarity is based on a one-to-one alignment of words from the two sentences. The alignment is obtained by maximal weighted bipartite matching using several word similarity measures. In addition, we utilise named entity recognition and matching tools. In general, the approach is similar to the one described by Karnick et al. (2012), with a different set of tools used. Our implementation relies on the ANNIE components in GATE (Cunningham et al., 2002) and will thus be referred to as GateWordMatch. The processing pipeline for GateWordMatch is: (1) tokenization by ANNIE English Tokeniser, (2) part-of-speech tagging by ANNIE POS Tagger, (3) lemmatization by GATE Morphological Analyser, (4) stopword removal, (5) named entity recognition based on lists by ANNIE Gazetteer, (6) named entity recognition based on the JAPE grammar by the ANNIE NE Transducer, (7) matching of named entities by ANNIE Ortho Matcher, (8) computing WordNet and Levenstein similarity between words, (9) calculation of a maximum weighted bipartite graph matching based on simi</context>
</contexts>
<marker>Cunningham, Maynard, Bontcheva, Tablan, 2002</marker>
<rawString>Cunningham, H., Maynard, D., Bontcheva, K., and Tablan, V. (2002). GATE: A framework and graphical development environment for robust NLP tools and applications. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 168–175, Philadelphia, Pennsylvania. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S Dumais</author>
<author>G Furnas</author>
<author>T Landauer</author>
<author>R Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="8094" citStr="Deerwester et al., 1990" startWordPosition="1257" endWordPosition="1260"> 8. Weighted bipartite matching finds the one-toone alignment that maximizes the sum of similarities between aligned tokens. Total similarity normalised by the number of words in both sentences is used as the final sentence similarity measure. 3 Distributional Similarity Our distributional similarity features use Random Indexing (RI; Kanerva et al., 2000; Sahlgren, 2005), also employed in STS’12 by Tovar et al. (2012); Sokolov (2012); Semeraro et al. (2012). It is an efficient method for modelling higher order cooccurrence similarities among terms, comparable to Latent Semantic Analysis (LSA; Deerwester et al., 1990). It incrementally builds a term co-occurrence matrix of reduced dimensionality through the use of a sliding window and fixed size index vectors used for training context vectors, one per unique term. A novel variant, which we have called “Multi67 sense Random Indexing” (MSRI), inspired by Reisinger and Mooney (2010), attempts to capture one or more “senses” per unique term in an unsupervised manner, each sense represented as an individual vector in the model. The method is similar to classical sliding window RI, but each term can have multiple context vectors (referred to as “sense vectors” h</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Deerwester, S., Dumais, S., Furnas, G., Landauer, T., and Harshman, R. (1990). Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Fundel</author>
<author>R K¨uffner</author>
<author>R Zimmer</author>
</authors>
<title>RelEx - Relation extraction using dependency parse trees.</title>
<date>2007</date>
<journal>Bioinformatics,</journal>
<volume>23</volume>
<issue>3</issue>
<marker>Fundel, K¨uffner, Zimmer, 2007</marker>
<rawString>Fundel, K., K¨uffner, R., and Zimmer, R. (2007). RelEx - Relation extraction using dependency parse trees. Bioinformatics, 23(3):365–371.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gabrilovich</author>
<author>S Markovitch</author>
</authors>
<title>Computing semantic relatedness using wikipedia-based explicit semantic analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of The Twentieth International Joint Conference for Artificial Intelligence.,</booktitle>
<pages>1606--1611</pages>
<contexts>
<context position="20468" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="3225" endWordPosition="3228">lier results on the development data. Although treated as a single feature, it is actually a combination of similarity features where an appropriate feature is selected for each word pair. This “vertical” way of combining features can potentially provide a more fine-grained feature selection, resulting in less noise. Indeed, if two words are matching as named entities or as close synonyms, less precise types of features such as character-based and data-driven similarity should not dominate the overall similarity score. It is interesting to find that MSRI outperforms both classical RI and ESA (Gabrilovich and Markovitch, 2007) on this task. Still, the more advanced features, such as MSRT-Context, gave inferior results compared to MSRT-Centroid. This suggests that more research on MSRI is needed to understand how both training and retrieval can be optimised. Also, LSA-based features (see tl.weight-dist-sim-wiki) achieve better results than both MSRI, RI and ESA. Then again, larger corpora were used for training the LSA models. RI has been shown to be comparable to LSA (Karlgren and Sahlgren, 2001), and since a relatively small corpus was used for training the RI/MSRI models, there are reasons to believe that better </context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Gabrilovich, E. and Markovitch, S. (2007). Computing semantic relatedness using wikipedia-based explicit semantic analysis. In Proceedings of The Twentieth International Joint Conference for Artificial Intelligence., pages 1606–1611.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hart</author>
<author>B Goertzel</author>
</authors>
<title>Opencog: A software framework for integrative artificial general intelligence.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 conference on Artificial General Intelligence 2008: Proceedings of the First AGI Conference,</booktitle>
<pages>468--472</pages>
<publisher>IOS Press.</publisher>
<location>Amsterdam, The</location>
<contexts>
<context position="12035" citStr="Hart and Goertzel, 2008" startWordPosition="1893" endWordPosition="1896">mputed as the cosine similarity between sentence vectors composed of their terms’ vectors. The corresponding features are (1) RI-SentVectors-Norm: sentence vectors are created by summing their constituent terms (i.e., context vectors), which have first been normalized; (2) RI-SentVectors-TFIDF: same as before, but TF*IDF weights are added. 4 Deeper Semantic Relations Two deep strategies were employed to accompany the shallow-processed feature sets. Two existing systems were used to provide the basis for these features, namely the RelEx system (Fundel et al., 2007) from the OpenCog initiative (Hart and Goertzel, 2008), and an in-house graph-edit distance system developed for plagiarism detection (Røkenes, 2013). RelEx outputs syntactic trees, dependency graphs, and semantic frames as this one for the sentence “Indian air force to buy 126 Rafale fighter jets”: Commerce buy:Goods(buy,jet) Entity:Entity(jet,jet) Entity:Name(jet,Rafale) Entity:Name(jet,fighter) Possibilities:Event(hyp,buy) Request:Addressee(air,you) Request:Message(air,air) Transitive action:Beneficiary(buy,jet) Three features were extracted from this: first, if there was an exact match of the frame found in s1 with s2; second, if there was a </context>
</contexts>
<marker>Hart, Goertzel, 2008</marker>
<rawString>Hart, D. and Goertzel, B. (2008). Opencog: A software framework for integrative artificial general intelligence. In Proceedings of the 2008 conference on Artificial General Intelligence 2008: Proceedings of the First AGI Conference, pages 468– 472, Amsterdam, The Netherlands, The Netherlands. IOS Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hassel</author>
</authors>
<date>2004</date>
<note>JavaSDM package.</note>
<contexts>
<context position="9771" citStr="Hassel, 2004" startWordPosition="1542" endWordPosition="1543">he threshold, the sentence vector is added as a new separate sense vector for the term; if exactly one score is above the threshold, the window vector is added to that sense vector; and if multiple scores are above the threshold, all the involved senses are merged into one sense vector, together with the window vector. This accomplishes an incremental clustering of senses in an unsupervised manner while retaining the efficiency of classical RI. As data for training the models we used the CLEF 2004–2008 English corpus (approx. 130M words). Our implementation of RI and MSRI is based on JavaSDM (Hassel, 2004). For classical RI, we used stopword removal (using a customised versions of the English stoplist from the Lucene project), window size of 4+4, dimensionality set to 1800, 4 non-zeros, and unweighted index vector in the sliding window. For MSRI, we used a similarity threshold of 0.2, a vector dimensionality of 800, a non-zero count of 4, and window size of 5+5. The index vectors in the sliding window were shifted to create direction vectors (Sahlgren et al., 2008), and weighted by distance to the target term. Rare senses with a frequency below 10 were excluded. Other sliding-window schemes, in</context>
</contexts>
<marker>Hassel, 2004</marker>
<rawString>Hassel, M. (2004). JavaSDM package.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Kanerva</author>
<author>J Kristoferson</author>
<author>A Holst</author>
</authors>
<title>Random indexing of text samples for latent semantic analysis.</title>
<date>2000</date>
<booktitle>Proceedings of the 22nd Annual Conference of the Cognitive Science Society,</booktitle>
<pages>1036</pages>
<editor>In Gleitman, L. and Josh, A., editors,</editor>
<publisher>Erlbaum.</publisher>
<contexts>
<context position="3372" citStr="Kanerva et al., 2000" startWordPosition="508" endWordPosition="511">approach” in the way of first automatically extracting as many potentially useful features as possible, using both knowledge and data-driven methods, and then evaluating feature combinations on the data sets provided by the organisers of the shared task. To this end, four different types of features were extracted. The first (Section 2) aggregates similarity based on named entity recognition with WordNet and Levenshtein distance by calculating maximum weighted bipartite graphs. The second set of features (Section 3) models higher order co-occurrence similarity relations using Random Indexing (Kanerva et al., 2000), both in the form of a (standard) sliding window approach and through a novel method called “Multi-sense Random Indexing” which aims to separate the representation of different senses of a term 66 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task, pages 66–73, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics from each other. The third feature set (Section 4) aims to capture deeper semantic relations using either the output of the RelEx semantic dependency relationship extr</context>
<context position="7826" citStr="Kanerva et al., 2000" startWordPosition="1216" endWordPosition="1219">e to 35%. In step 9, maximum weighted bipartite matching is computed using the Hungarian Algorithm (Kuhn, 1955). Nodes in the bipartite graph represent words from the sentences, and edges have weights that correspond to similarities between tokens obtained in step 8. Weighted bipartite matching finds the one-toone alignment that maximizes the sum of similarities between aligned tokens. Total similarity normalised by the number of words in both sentences is used as the final sentence similarity measure. 3 Distributional Similarity Our distributional similarity features use Random Indexing (RI; Kanerva et al., 2000; Sahlgren, 2005), also employed in STS’12 by Tovar et al. (2012); Sokolov (2012); Semeraro et al. (2012). It is an efficient method for modelling higher order cooccurrence similarities among terms, comparable to Latent Semantic Analysis (LSA; Deerwester et al., 1990). It incrementally builds a term co-occurrence matrix of reduced dimensionality through the use of a sliding window and fixed size index vectors used for training context vectors, one per unique term. A novel variant, which we have called “Multi67 sense Random Indexing” (MSRI), inspired by Reisinger and Mooney (2010), attempts to </context>
</contexts>
<marker>Kanerva, Kristoferson, Holst, 2000</marker>
<rawString>Kanerva, P., Kristoferson, J., and Holst, A. (2000). Random indexing of text samples for latent semantic analysis. In Gleitman, L. and Josh, A., editors, Proceedings of the 22nd Annual Conference of the Cognitive Science Society, page 1036. Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Karlgren</author>
<author>M Sahlgren</author>
</authors>
<title>From Words to Understanding.</title>
<date>2001</date>
<booktitle>Foundations of real-world intelligence, chapter 26,</booktitle>
<pages>294--311</pages>
<editor>In Uesaka, Y., Kanerva, P., and Asoh, H., editors,</editor>
<publisher>CSLI Publications.</publisher>
<location>Stanford:</location>
<contexts>
<context position="20947" citStr="Karlgren and Sahlgren, 2001" startWordPosition="3302" endWordPosition="3305">t dominate the overall similarity score. It is interesting to find that MSRI outperforms both classical RI and ESA (Gabrilovich and Markovitch, 2007) on this task. Still, the more advanced features, such as MSRT-Context, gave inferior results compared to MSRT-Centroid. This suggests that more research on MSRI is needed to understand how both training and retrieval can be optimised. Also, LSA-based features (see tl.weight-dist-sim-wiki) achieve better results than both MSRI, RI and ESA. Then again, larger corpora were used for training the LSA models. RI has been shown to be comparable to LSA (Karlgren and Sahlgren, 2001), and since a relatively small corpus was used for training the RI/MSRI models, there are reasons to believe that better scores can be achieved by both RI- and MSRI-based features by using more training data. 70 HeadLine OnWN FNWN SMT Mean Features r n r n r n r n r n CharacterNGramMeasure-3 0.72 2 0.39 2 0.44 3 0.70 1 0.56 1 CharacterNGramMeasure-4 0.69 3 0.38 5 0.45 2 0.67 6 0.55 2 CharacterNGramMeasure-2 0.73 1 0.37 9 0.34 10 0.69 2 0.53 3 tl.weight-dist-sim-wiki 0.58 14 0.39 3 0.45 1 0.67 5 0.52 4 tl.wn-sim-lem 0.69 4 0.40 1 0.41 5 0.59 10 0.52 5 GateWordMatch 0.67 8 0.37 11 0.34 11 0.60 9</context>
</contexts>
<marker>Karlgren, Sahlgren, 2001</marker>
<rawString>Karlgren, J. and Sahlgren, M. (2001). From Words to Understanding. In Uesaka, Y., Kanerva, P., and Asoh, H., editors, Foundations of real-world intelligence, chapter 26, pages 294–311. Stanford: CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Karnick</author>
<author>S Satapathy</author>
<author>S Bhagwani</author>
</authors>
<title>sranjans: Semantic textual similarity using maximal bipartite graph matching.</title>
<date>2012</date>
<booktitle>In *SEM</booktitle>
<pages>579--585</pages>
<contexts>
<context position="5055" citStr="Karnick et al. (2012)" startWordPosition="770" endWordPosition="773">regression problem of combining all the extracted feature values. Three different systems were created based on feature performance on the supplied development data. Section 7 discusses scores on the STS’12 and STS’13 test data. 2 Compositional Word Matching Compositional word matching similarity is based on a one-to-one alignment of words from the two sentences. The alignment is obtained by maximal weighted bipartite matching using several word similarity measures. In addition, we utilise named entity recognition and matching tools. In general, the approach is similar to the one described by Karnick et al. (2012), with a different set of tools used. Our implementation relies on the ANNIE components in GATE (Cunningham et al., 2002) and will thus be referred to as GateWordMatch. The processing pipeline for GateWordMatch is: (1) tokenization by ANNIE English Tokeniser, (2) part-of-speech tagging by ANNIE POS Tagger, (3) lemmatization by GATE Morphological Analyser, (4) stopword removal, (5) named entity recognition based on lists by ANNIE Gazetteer, (6) named entity recognition based on the JAPE grammar by the ANNIE NE Transducer, (7) matching of named entities by ANNIE Ortho Matcher, (8) computing Word</context>
</contexts>
<marker>Karnick, Satapathy, Bhagwani, 2012</marker>
<rawString>Karnick, H., Satapathy, S., and Bhagwani, S. (2012). sranjans: Semantic textual similarity using maximal bipartite graph matching. In *SEM (2012), pages 579–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kuhn</author>
</authors>
<title>The Hungarian method for the assignment problem. Naval research logistics quarterly,</title>
<date>1955</date>
<pages>2--83</pages>
<contexts>
<context position="7317" citStr="Kuhn, 1955" startWordPosition="1142" endWordPosition="1143">he corresponding senses in WordNet. Since word sense disambiguation is not used, we take the similarity between the nearest senses of two words. For cases when the WordNet-based similarity cannot be obtained, a similarity based on the Levenshtein distance (Levenshtein, 1966) is used instead. It is normalised by the length of the longest word in the pair. For the STS’13 test data set, named entity matching contributed to 4% of all matched word pairs; LCH similarity to 61%, and Levenshtein distance to 35%. In step 9, maximum weighted bipartite matching is computed using the Hungarian Algorithm (Kuhn, 1955). Nodes in the bipartite graph represent words from the sentences, and edges have weights that correspond to similarities between tokens obtained in step 8. Weighted bipartite matching finds the one-toone alignment that maximizes the sum of similarities between aligned tokens. Total similarity normalised by the number of words in both sentences is used as the final sentence similarity measure. 3 Distributional Similarity Our distributional similarity features use Random Indexing (RI; Kanerva et al., 2000; Sahlgren, 2005), also employed in STS’12 by Tovar et al. (2012); Sokolov (2012); Semeraro</context>
</contexts>
<marker>Kuhn, 1955</marker>
<rawString>Kuhn, H. (1955). The Hungarian method for the assignment problem. Naval research logistics quarterly, 2:83–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>M Chodorow</author>
</authors>
<title>Combining local context and WordNet similarity for word sense identification. WordNet: An electronic lexical....</title>
<date>1998</date>
<contexts>
<context position="6650" citStr="Leacock and Chodorow, 1998" startWordPosition="1030" endWordPosition="1033"> In step 6, JAPE grammar rules are applied to recognise entities such as addresses, emails, dates, job titles, and person names based on basic syntactic and morphological features. Matching of named entities in step 7 is based on matching rules that check the type of named entity, and lists with aliases to identify entities as “US”, “United State”, and “USA” as the same entity. In step 8, similarity is computed for each pair of words from the two sentences. Words that are matched as entities in step 7 get a similarity value of 1.0. For the rest of the entities and non-entity words we use LCH (Leacock and Chodorow, 1998) similarity, which is based on a shortest path between the corresponding senses in WordNet. Since word sense disambiguation is not used, we take the similarity between the nearest senses of two words. For cases when the WordNet-based similarity cannot be obtained, a similarity based on the Levenshtein distance (Levenshtein, 1966) is used instead. It is normalised by the length of the longest word in the pair. For the STS’13 test data set, named entity matching contributed to 4% of all matched word pairs; LCH similarity to 61%, and Levenshtein distance to 35%. In step 9, maximum weighted bipart</context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>Leacock, C. and Chodorow, M. (1998). Combining local context and WordNet similarity for word sense identification. WordNet: An electronic lexical....</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions and reversals.</title>
<date>1966</date>
<journal>Soviet Physics Doklady,</journal>
<volume>10</volume>
<issue>8</issue>
<contexts>
<context position="6981" citStr="Levenshtein, 1966" startWordPosition="1084" endWordPosition="1085">ed State”, and “USA” as the same entity. In step 8, similarity is computed for each pair of words from the two sentences. Words that are matched as entities in step 7 get a similarity value of 1.0. For the rest of the entities and non-entity words we use LCH (Leacock and Chodorow, 1998) similarity, which is based on a shortest path between the corresponding senses in WordNet. Since word sense disambiguation is not used, we take the similarity between the nearest senses of two words. For cases when the WordNet-based similarity cannot be obtained, a similarity based on the Levenshtein distance (Levenshtein, 1966) is used instead. It is normalised by the length of the longest word in the pair. For the STS’13 test data set, named entity matching contributed to 4% of all matched word pairs; LCH similarity to 61%, and Levenshtein distance to 35%. In step 9, maximum weighted bipartite matching is computed using the Hungarian Algorithm (Kuhn, 1955). Nodes in the bipartite graph represent words from the sentences, and edges have weights that correspond to similarities between tokens obtained in step 8. Weighted bipartite matching finds the one-toone alignment that maximizes the sum of similarities between al</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>Levenshtein, V. I. (1966). Binary codes capable of correcting deletions, insertions and reversals. Soviet Physics Doklady, 10(8):707–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
</authors>
<title>Maltparser: A data-driven parser-generator for dependency parsing. In</title>
<date>2006</date>
<booktitle>In Proc. of LREC-2006,</booktitle>
<pages>2216--2219</pages>
<contexts>
<context position="13227" citStr="Nivre et al., 2006" startWordPosition="2053" endWordPosition="2056">s2; second, if there was a partial match until the first argument (Commerce buy:Goods(buy); and third if there was a match of the frame category 68 (Commerce buy:Goods). In STS’12, Singh et al. (2012) matched Universal Networking Language (UNL) graphs against each other by counting matches of relations and universal words, while Bhagwani et al. (2012) calculated WordNet-based word-level similarities and created a weighted bipartite graph (see Section 2). The method employed here instead looked at the graph edit distance between dependency graphs obtained with the Maltparser dependency parser (Nivre et al., 2006). Edit distance is the defined as the minimum of the sum of the costs of the edit operations (insertion, deletion and substitution of nodes) required to transform one graph into the other. It is approximated with a fast but suboptimal algorithm based on bipartite graph matching through the Hungarian algorithm (Riesen and Bunke, 2009). 5 Reused Features The TakeLab ‘simple’ system (ˇSari´c et al., 2012) obtained 3rd place in overall Pearson correlation and 1st for normalized Pearson in STS’12. The source code1 was used to generate all its features, that is, n-gram overlap, WordNet-augmented wor</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2006</marker>
<rawString>Nivre, J., Hall, J., and Nilsson, J. (2006). Maltparser: A data-driven parser-generator for dependency parsing. In In Proc. of LREC-2006, pages 2216–2219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Reisinger</author>
<author>R Mooney</author>
</authors>
<title>Multiprototype vector-space models of word meaning.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>109--117</pages>
<location>number</location>
<contexts>
<context position="8412" citStr="Reisinger and Mooney (2010)" startWordPosition="1307" endWordPosition="1310">Random Indexing (RI; Kanerva et al., 2000; Sahlgren, 2005), also employed in STS’12 by Tovar et al. (2012); Sokolov (2012); Semeraro et al. (2012). It is an efficient method for modelling higher order cooccurrence similarities among terms, comparable to Latent Semantic Analysis (LSA; Deerwester et al., 1990). It incrementally builds a term co-occurrence matrix of reduced dimensionality through the use of a sliding window and fixed size index vectors used for training context vectors, one per unique term. A novel variant, which we have called “Multi67 sense Random Indexing” (MSRI), inspired by Reisinger and Mooney (2010), attempts to capture one or more “senses” per unique term in an unsupervised manner, each sense represented as an individual vector in the model. The method is similar to classical sliding window RI, but each term can have multiple context vectors (referred to as “sense vectors” here) which are updated individually. When updating a term vector, instead of directly adding the index vectors of the neighbouring terms in the window to its context vector, the system first computes a separate window vector consisting of the sum of the index vectors. Then cosine similarity is calculated between the </context>
</contexts>
<marker>Reisinger, Mooney, 2010</marker>
<rawString>Reisinger, J. and Mooney, R. (2010). Multiprototype vector-space models of word meaning. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, number June, pages 109–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Riesen</author>
<author>H Bunke</author>
</authors>
<title>Approximate graph edit distance computation by means of bipartite graph matching.</title>
<date>2009</date>
<journal>Image and Vision Computing,</journal>
<volume>27</volume>
<issue>7</issue>
<contexts>
<context position="13562" citStr="Riesen and Bunke, 2009" startWordPosition="2109" endWordPosition="2112">Bhagwani et al. (2012) calculated WordNet-based word-level similarities and created a weighted bipartite graph (see Section 2). The method employed here instead looked at the graph edit distance between dependency graphs obtained with the Maltparser dependency parser (Nivre et al., 2006). Edit distance is the defined as the minimum of the sum of the costs of the edit operations (insertion, deletion and substitution of nodes) required to transform one graph into the other. It is approximated with a fast but suboptimal algorithm based on bipartite graph matching through the Hungarian algorithm (Riesen and Bunke, 2009). 5 Reused Features The TakeLab ‘simple’ system (ˇSari´c et al., 2012) obtained 3rd place in overall Pearson correlation and 1st for normalized Pearson in STS’12. The source code1 was used to generate all its features, that is, n-gram overlap, WordNet-augmented word overlap, vector space sentence similarity, normalized difference, shallow NE similarity, numbers overlap, and stock index features.2 This required the full LSA vector space models, which were kindly provided by the TakeLab team. The word counts required for computing Information Content were obtained from Google Books Ngrams.3 The </context>
</contexts>
<marker>Riesen, Bunke, 2009</marker>
<rawString>Riesen, K. and Bunke, H. (2009). Approximate graph edit distance computation by means of bipartite graph matching. Image and Vision Computing, 27(7):950–959.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Røkenes</author>
</authors>
<title>Graph-Edit Distance Applied to the Task of Detecting Plagiarism.</title>
<date>2013</date>
<tech>Master’s thesis,</tech>
<institution>Norwegian University of Science and Technology.</institution>
<contexts>
<context position="12130" citStr="Røkenes, 2013" startWordPosition="1907" endWordPosition="1908">ing features are (1) RI-SentVectors-Norm: sentence vectors are created by summing their constituent terms (i.e., context vectors), which have first been normalized; (2) RI-SentVectors-TFIDF: same as before, but TF*IDF weights are added. 4 Deeper Semantic Relations Two deep strategies were employed to accompany the shallow-processed feature sets. Two existing systems were used to provide the basis for these features, namely the RelEx system (Fundel et al., 2007) from the OpenCog initiative (Hart and Goertzel, 2008), and an in-house graph-edit distance system developed for plagiarism detection (Røkenes, 2013). RelEx outputs syntactic trees, dependency graphs, and semantic frames as this one for the sentence “Indian air force to buy 126 Rafale fighter jets”: Commerce buy:Goods(buy,jet) Entity:Entity(jet,jet) Entity:Name(jet,Rafale) Entity:Name(jet,fighter) Possibilities:Event(hyp,buy) Request:Addressee(air,you) Request:Message(air,air) Transitive action:Beneficiary(buy,jet) Three features were extracted from this: first, if there was an exact match of the frame found in s1 with s2; second, if there was a partial match until the first argument (Commerce buy:Goods(buy); and third if there was a match</context>
</contexts>
<marker>Røkenes, 2013</marker>
<rawString>Røkenes, H. (2013). Graph-Edit Distance Applied to the Task of Detecting Plagiarism. Master’s thesis, Norwegian University of Science and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sahlgren</author>
</authors>
<title>An introduction to random indexing.</title>
<date>2005</date>
<booktitle>In Methods and Applications of Semantic Indexing Workshop at the 7th International Conference on Terminology and Knowledge Engineering, TKE,</booktitle>
<volume>5</volume>
<contexts>
<context position="7843" citStr="Sahlgren, 2005" startWordPosition="1220" endWordPosition="1221">aximum weighted bipartite matching is computed using the Hungarian Algorithm (Kuhn, 1955). Nodes in the bipartite graph represent words from the sentences, and edges have weights that correspond to similarities between tokens obtained in step 8. Weighted bipartite matching finds the one-toone alignment that maximizes the sum of similarities between aligned tokens. Total similarity normalised by the number of words in both sentences is used as the final sentence similarity measure. 3 Distributional Similarity Our distributional similarity features use Random Indexing (RI; Kanerva et al., 2000; Sahlgren, 2005), also employed in STS’12 by Tovar et al. (2012); Sokolov (2012); Semeraro et al. (2012). It is an efficient method for modelling higher order cooccurrence similarities among terms, comparable to Latent Semantic Analysis (LSA; Deerwester et al., 1990). It incrementally builds a term co-occurrence matrix of reduced dimensionality through the use of a sliding window and fixed size index vectors used for training context vectors, one per unique term. A novel variant, which we have called “Multi67 sense Random Indexing” (MSRI), inspired by Reisinger and Mooney (2010), attempts to capture one or mo</context>
</contexts>
<marker>Sahlgren, 2005</marker>
<rawString>Sahlgren, M. (2005). An introduction to random indexing. In Methods and Applications of Semantic Indexing Workshop at the 7th International Conference on Terminology and Knowledge Engineering, TKE, volume 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sahlgren</author>
<author>A Holst</author>
<author>P Kanerva</author>
</authors>
<title>Permutations as a Means to Encode Order in Word Space.</title>
<date>2008</date>
<booktitle>Proceedings of the 30th Conference of the Cognitive Science Society.</booktitle>
<contexts>
<context position="10239" citStr="Sahlgren et al., 2008" startWordPosition="1619" endWordPosition="1622">r training the models we used the CLEF 2004–2008 English corpus (approx. 130M words). Our implementation of RI and MSRI is based on JavaSDM (Hassel, 2004). For classical RI, we used stopword removal (using a customised versions of the English stoplist from the Lucene project), window size of 4+4, dimensionality set to 1800, 4 non-zeros, and unweighted index vector in the sliding window. For MSRI, we used a similarity threshold of 0.2, a vector dimensionality of 800, a non-zero count of 4, and window size of 5+5. The index vectors in the sliding window were shifted to create direction vectors (Sahlgren et al., 2008), and weighted by distance to the target term. Rare senses with a frequency below 10 were excluded. Other sliding-window schemes, including unweighted non-shifted vectors and Random Permutation (Sahlgren et al., 2008), were tested, but none outperformed the sliding-window schemes used. Similarity between sentence pairs was calculated as the normalised maximal bipartite similarity between term pairs in each sentence, resulting in the following features: (1) MSRI-Centroid: each term is represented as the sum of its sense vectors; (2) MSRI-MaxSense: for each term pair, the sense-pair with max sim</context>
</contexts>
<marker>Sahlgren, Holst, Kanerva, 2008</marker>
<rawString>Sahlgren, M., Holst, A., and Kanerva, P. (2008). Permutations as a Means to Encode Order in Word Space. Proceedings of the 30th Conference of the Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F ˇSari´c</author>
<author>G Glavaˇs</author>
<author>M Karan</author>
<author>J ˇSnajder</author>
<author>B D Baˇsi´c</author>
</authors>
<title>TakeLab: systems for measuring semantic text similarity.</title>
<date>2012</date>
<booktitle>In *SEM</booktitle>
<pages>441--448</pages>
<marker>ˇSari´c, Glavaˇs, Karan, ˇSnajder, Baˇsi´c, 2012</marker>
<rawString>ˇSari´c, F., Glavaˇs, G., Karan, M., ˇSnajder, J., and Baˇsi´c, B. D. (2012). TakeLab: systems for measuring semantic text similarity. In *SEM (2012), pages 441–448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>SEM</author>
</authors>
<date>2012</date>
<booktitle>Proceedings of the First Joint Conference on Lexical and Computational Semantics (*SEM),</booktitle>
<volume>2</volume>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Montreal, Canada.</location>
<marker>SEM, 2012</marker>
<rawString>*SEM (2012). Proceedings of the First Joint Conference on Lexical and Computational Semantics (*SEM), volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, Montreal, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Semeraro</author>
<author>B Aldo</author>
<author>V E Orabona</author>
</authors>
<title>UNIBA: Distributional semantics for textual similarity. In *SEM</title>
<date>2012</date>
<pages>591--596</pages>
<contexts>
<context position="7931" citStr="Semeraro et al. (2012)" startWordPosition="1233" endWordPosition="1236">n, 1955). Nodes in the bipartite graph represent words from the sentences, and edges have weights that correspond to similarities between tokens obtained in step 8. Weighted bipartite matching finds the one-toone alignment that maximizes the sum of similarities between aligned tokens. Total similarity normalised by the number of words in both sentences is used as the final sentence similarity measure. 3 Distributional Similarity Our distributional similarity features use Random Indexing (RI; Kanerva et al., 2000; Sahlgren, 2005), also employed in STS’12 by Tovar et al. (2012); Sokolov (2012); Semeraro et al. (2012). It is an efficient method for modelling higher order cooccurrence similarities among terms, comparable to Latent Semantic Analysis (LSA; Deerwester et al., 1990). It incrementally builds a term co-occurrence matrix of reduced dimensionality through the use of a sliding window and fixed size index vectors used for training context vectors, one per unique term. A novel variant, which we have called “Multi67 sense Random Indexing” (MSRI), inspired by Reisinger and Mooney (2010), attempts to capture one or more “senses” per unique term in an unsupervised manner, each sense represented as an indi</context>
</contexts>
<marker>Semeraro, Aldo, Orabona, 2012</marker>
<rawString>Semeraro, G., Aldo, B., and Orabona, V. E. (2012). UNIBA: Distributional semantics for textual similarity. In *SEM (2012), pages 591–596.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Singh</author>
<author>A Bhattacharya</author>
<author>P Bhattacharyya</author>
</authors>
<title>janardhan: Semantic textual similarity using universal networking language graph matching.</title>
<date>2012</date>
<booktitle>In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>662--666</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="12808" citStr="Singh et al. (2012)" startWordPosition="1993" endWordPosition="1996">antic frames as this one for the sentence “Indian air force to buy 126 Rafale fighter jets”: Commerce buy:Goods(buy,jet) Entity:Entity(jet,jet) Entity:Name(jet,Rafale) Entity:Name(jet,fighter) Possibilities:Event(hyp,buy) Request:Addressee(air,you) Request:Message(air,air) Transitive action:Beneficiary(buy,jet) Three features were extracted from this: first, if there was an exact match of the frame found in s1 with s2; second, if there was a partial match until the first argument (Commerce buy:Goods(buy); and third if there was a match of the frame category 68 (Commerce buy:Goods). In STS’12, Singh et al. (2012) matched Universal Networking Language (UNL) graphs against each other by counting matches of relations and universal words, while Bhagwani et al. (2012) calculated WordNet-based word-level similarities and created a weighted bipartite graph (see Section 2). The method employed here instead looked at the graph edit distance between dependency graphs obtained with the Maltparser dependency parser (Nivre et al., 2006). Edit distance is the defined as the minimum of the sum of the costs of the edit operations (insertion, deletion and substitution of nodes) required to transform one graph into the</context>
</contexts>
<marker>Singh, Bhattacharya, Bhattacharyya, 2012</marker>
<rawString>Singh, J., Bhattacharya, A., and Bhattacharyya, P. (2012). janardhan: Semantic textual similarity using universal networking language graph matching. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 662–666, Montr´eal, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sokolov</author>
</authors>
<title>LIMSI: learning semantic similarity by selecting random word subsets.</title>
<date>2012</date>
<booktitle>In *SEM</booktitle>
<pages>543--546</pages>
<contexts>
<context position="7907" citStr="Sokolov (2012)" startWordPosition="1231" endWordPosition="1232">n Algorithm (Kuhn, 1955). Nodes in the bipartite graph represent words from the sentences, and edges have weights that correspond to similarities between tokens obtained in step 8. Weighted bipartite matching finds the one-toone alignment that maximizes the sum of similarities between aligned tokens. Total similarity normalised by the number of words in both sentences is used as the final sentence similarity measure. 3 Distributional Similarity Our distributional similarity features use Random Indexing (RI; Kanerva et al., 2000; Sahlgren, 2005), also employed in STS’12 by Tovar et al. (2012); Sokolov (2012); Semeraro et al. (2012). It is an efficient method for modelling higher order cooccurrence similarities among terms, comparable to Latent Semantic Analysis (LSA; Deerwester et al., 1990). It incrementally builds a term co-occurrence matrix of reduced dimensionality through the use of a sliding window and fixed size index vectors used for training context vectors, one per unique term. A novel variant, which we have called “Multi67 sense Random Indexing” (MSRI), inspired by Reisinger and Mooney (2010), attempts to capture one or more “senses” per unique term in an unsupervised manner, each sens</context>
</contexts>
<marker>Sokolov, 2012</marker>
<rawString>Sokolov, A. (2012). LIMSI: learning semantic similarity by selecting random word subsets. In *SEM (2012), pages 543–546.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tovar</author>
<author>J Reyes</author>
<author>A Montes</author>
</authors>
<title>BUAP: a first approximation to relational similarity measuring.</title>
<date>2012</date>
<booktitle>In *SEM</booktitle>
<pages>502--505</pages>
<contexts>
<context position="7891" citStr="Tovar et al. (2012)" startWordPosition="1227" endWordPosition="1230">ed using the Hungarian Algorithm (Kuhn, 1955). Nodes in the bipartite graph represent words from the sentences, and edges have weights that correspond to similarities between tokens obtained in step 8. Weighted bipartite matching finds the one-toone alignment that maximizes the sum of similarities between aligned tokens. Total similarity normalised by the number of words in both sentences is used as the final sentence similarity measure. 3 Distributional Similarity Our distributional similarity features use Random Indexing (RI; Kanerva et al., 2000; Sahlgren, 2005), also employed in STS’12 by Tovar et al. (2012); Sokolov (2012); Semeraro et al. (2012). It is an efficient method for modelling higher order cooccurrence similarities among terms, comparable to Latent Semantic Analysis (LSA; Deerwester et al., 1990). It incrementally builds a term co-occurrence matrix of reduced dimensionality through the use of a sliding window and fixed size index vectors used for training context vectors, one per unique term. A novel variant, which we have called “Multi67 sense Random Indexing” (MSRI), inspired by Reisinger and Mooney (2010), attempts to capture one or more “senses” per unique term in an unsupervised m</context>
</contexts>
<marker>Tovar, Reyes, Montes, 2012</marker>
<rawString>Tovar, M., Reyes, J., and Montes, A. (2012). BUAP: a first approximation to relational similarity measuring. In *SEM (2012), pages 502–505.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vapnik</author>
<author>S E Golowich</author>
<author>A Smola</author>
</authors>
<title>Support vector method for function approximation, regression estimation, and signal processing.</title>
<date>1997</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<volume>9</volume>
<pages>281--287</pages>
<editor>In Mozer, M. C., Jordan, M. I., and Petsche, T., editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="4390" citStr="Vapnik et al., 1997" startWordPosition="668" endWordPosition="671">tion for Computational Linguistics from each other. The third feature set (Section 4) aims to capture deeper semantic relations using either the output of the RelEx semantic dependency relationship extraction system (Fundel et al., 2007) or an in-house graph edit-distance matching system. The final set (Section 5) is a straight-forward gathering of features from the systems that fared best in STS’12: TakeLab from University of Zagreb ( ˇSari´c et al., 2012) and DKPro from Darmstadt’s Ubiquitous Knowledge Processing Lab (B¨ar et al., 2012). As described in Section 6, Support Vector Regression (Vapnik et al., 1997) was used for solving the multi-dimensional regression problem of combining all the extracted feature values. Three different systems were created based on feature performance on the supplied development data. Section 7 discusses scores on the STS’12 and STS’13 test data. 2 Compositional Word Matching Compositional word matching similarity is based on a one-to-one alignment of words from the two sentences. The alignment is obtained by maximal weighted bipartite matching using several word similarity measures. In addition, we utilise named entity recognition and matching tools. In general, the </context>
</contexts>
<marker>Vapnik, Golowich, Smola, 1997</marker>
<rawString>Vapnik, V., Golowich, S. E., and Smola, A. (1997). Support vector method for function approximation, regression estimation, and signal processing. In Mozer, M. C., Jordan, M. I., and Petsche, T., editors, Advances in Neural Information Processing Systems, volume 9, pages 281–287. MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>