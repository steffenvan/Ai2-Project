<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002285">
<title confidence="0.988802">
Comparison of Similarity Models for the Relation Discovery Task
</title>
<author confidence="0.998832">
Ben Hachey
</author>
<affiliation confidence="0.898546">
School of Informatics
University of Edinburgh
2 Buccleuch Place, Edinburgh EH8 9LW
</affiliation>
<email confidence="0.99746">
bhachey@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.993877" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999953611111111">
We present results on the relation discov-
ery task, which addresses some of the
shortcomings of supervised relation ex-
traction by applying minimally supervised
methods. We describe a detailed experi-
mental design that compares various con-
figurations of conceptual representations
and similarity measures across six differ-
ent subsets of the ACE relation extraction
data. Previous work on relation discovery
used a semantic space based on a term-by-
document matrix. We find that represen-
tations based on term co-occurrence per-
form significantly better. We also observe
further improvements when reducing the
dimensionality of the term co-occurrence
matrix using probabilistic topic models,
though these are not significant.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.96203825">
This paper describes work that aims to improve
upon previous approaches to identifying relation-
ships between named objects in text (e.g., people,
organisations, locations). Figure 1 contains sev-
eral example sentences from the ACE 2005 cor-
pus that contain relations and Figure 2 summarises
the relations occurring in these sentences. So, for
example, sentence 1 contains an employment rela-
tion between Lebron James and Nike, sentence 2
contains a sports-affiliation relation between Stig
Toefting and Bolton and sentence 4 contains a
business relation between Martha Stewart (she)
and the board of directors (of Martha Stewart Liv-
ing Omnimedia).
Possible applications include identifying com-
panies taking part in mergers/acquisitions from
</bodyText>
<footnote confidence="0.925635125">
1 As for that $90 million shoe contract with Nike,
it may be a good deal for James.
2 Toefting transferred to Bolton in February 2002
from German club Hamburg.
3 Toyoda founded the automaker in 1937 ... .
4 In a statement, she says she’s stepping aside in
the best interest of the company, but she will
stay on the board of directors.
</footnote>
<figureCaption confidence="0.996897">
Figure 1: Example sentences from ACE 2005.
</figureCaption>
<table confidence="0.629461666666667">
Sent Entity1 Entity2 Relation
1 Lebron James Nike Employ
2 Stig Toefting Bolton Sports-Aff
2 Stig Toefting Hamburg Sports-Aff
3 Kiichiro Toyoda Toyota Corp Founder
4 Martha Stewart board Business
</table>
<figureCaption confidence="0.996138">
Figure 2: Example entity pairs and relation types.
</figureCaption>
<bodyText confidence="0.999241761904762">
business newswire, which could be inserted into a
corporate intelligence database. In the biomedical
domain, we may want to identify relationships be-
tween genes and proteins from biomedical publi-
cations, e.g. Hirschman et al. (2004), to help scien-
tists keep up-to-date on the literature. Or, we may
want to identify disease and treatment relations in
publications and textbooks, which can be used to
help formalise medical knowledge and assist gen-
eral practitioners in diagnosis, treatment and prog-
nosis (Rosario and Hearst, 2004).
Another application scenario involves building
networks of relationships from text collections that
indicate the important entities in a domain and
can be used to visualise interactions. The net-
works could provide an alternative to searching
when interacting with a document collection. This
could prove beneficial, for example, in investiga-
tive journalism. It might also be used for social
science research using techniques from social net-
work analysis (Marsden and Lin, 1982). In previ-
</bodyText>
<page confidence="0.98063">
25
</page>
<note confidence="0.69349">
Proceedings of the Workshop on Linguistic Distances, pages 25–34,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999878666666667">
ous work, relations have been used for automatic
text summarisation as a conceptual representation
of sentence content in a sentence extraction frame-
work (Filatova and Hatzivassiloglou, 2004).
In the next section, we motivate and introduce
the relation discovery task, which addresses some
of the shortcomings of conventional approaches to
relation extraction (i.e. supervised learning or rule
engineering) by applying minimally supervised
methods.1 A critical part of the relation discov-
ery task is grouping entity pairs by their relation
type. This is a clustering task and requires a ro-
bust conceptual representation of relation seman-
tics and a measure of similarity between relations.
In previous work (Hasegawa et al., 2004; Chen et
al., 2005), the conceptual representation has been
limited to term-by-document (TxD) models of re-
lation semantics. The current work introduces a
term co-occurrence (TxT) representation for the
relation discovery task and shows that it performs
significantly better than the TxD representation.
We also explore dimensionality reduction tech-
niques, which show a further improvement.
Section 3 presents a parameterisation of similar-
ity models for relation discovery. For the purposes
of the current work, this consists of the semantic
representation for terms (i.e. how a term’s context
is modelled), dimensionality reduction technique
(e.g. singular value decomposition, latent Dirich-
let allocation), and the measure used to compute
similarity.
We also build on the evaluation paradigm for
relation discovery with a detailed, controlled ex-
perimental setup. Section 4 describes the experi-
ment design, which compares the various system
configurations across six different subsets of the
relation extraction data from the automatic con-
tent extraction (ACE) evaluation. Finally, Section
5 presents results and statistical analysis.
</bodyText>
<sectionHeader confidence="0.975557" genericHeader="method">
2 The Relation Discovery Task
</sectionHeader>
<bodyText confidence="0.9996405">
Conventionally, relation extraction is considered
to be part of information extraction and has been
approached through supervised learning or rule
engineering (e.g., Blaschke and Valencia (2002),
Bunescu and Mooney (2005)). However, tradi-
tional approaches have several shortcomings. First
</bodyText>
<footnote confidence="0.995783">
1The relation discovery task is minimally supervised in
the sense that it relies on having certain resources such as
named entity recognition. The focus of the current paper is
the unsupervised task of clustering relations.
</footnote>
<bodyText confidence="0.999613727272727">
and foremost, they are generally based on pre-
defined templates of what types of relations ex-
ist in the data and thus only capture information
whose importance was anticipated by the template
designers. This poses reliability problems when
predicting new data in the same domain as the
training data will be from a certain epoch in the
past. Due to language change and topical varia-
tion, as time passes, it is likely that the new data
will deviate more and more from the trained mod-
els. Additionally, there are cost problems asso-
ciated with the conventional supervised approach
when updating templates or transferring to a new
domain, both of which require substantial effort in
re-engineering rules or re-annotating training data.
The goal of the relation discovery task is to
identify the existence of associations between en-
tities, to identify the kinds of relations that oc-
cur in a corpus and to annotate particular associ-
ations with relation types. These goals correspond
to the three main steps in a generalised algorithm
(Hasegawa et al., 2004):
</bodyText>
<listItem confidence="0.999194333333333">
1. Identify co-occurring pairs of named entities
2. Group entity pairs using the textual context
3. Label each cluster of entity pairs
</listItem>
<bodyText confidence="0.999965357142857">
The first step is the relation identification task.
In the current work, this is assumed to have been
done already. We use the gold standard relations
in the ACE data in order to isolate the performance
of the second step. The second step is a clustering
task and as such it is necessary to compute simi-
larity between the co-occurring pairs of named en-
tities (relations). In order to do this, a model of re-
lation similarity is required, which is the focus of
the current work.
We also assume that it is possible to perform the
third step.2 The evaluation we present here looks
just at the quality of the clustering and does not
attempt to assess the labelling task.
</bodyText>
<sectionHeader confidence="0.967439" genericHeader="method">
3 Modelling Relation Similarity
</sectionHeader>
<bodyText confidence="0.988459">
The possible space of models for relation similar-
ity can be explored in a principled manner by pa-
rameterisation. In this section, we discuss several
</bodyText>
<footnote confidence="0.9906514">
2Previous approaches select labels from the collection of
context words for a relation cluster (Hasegawa et al., 2004;
Zhang et al., 2005). Chen et al. (2005) use discriminative
category matching to make sure that selected labels are also
able to differentiate between clusters.
</footnote>
<page confidence="0.998673">
26
</page>
<bodyText confidence="0.994412">
parameters including the term context representa-
tion, whether or not we apply dimensionality re-
duction, and what similarity measure we use.
</bodyText>
<subsectionHeader confidence="0.994907">
3.1 Term Context
</subsectionHeader>
<bodyText confidence="0.999901078947369">
Representing texts in such a way that they can be
compared is a familiar problem from the fields
of information retrieval (IR), text mining (TM),
textual data analysis (TDA) and natural language
processing (NLP) (Lebart and Rajman, 2000).
The traditional model for IR and TM is based
on a term-by-document (TxD) vector representa-
tion. Previous approaches to relation discovery
(Hasegawa et al., 2004; Chen et al., 2005) have
been limited to TxD representations, using tf*idf
weighting and the cosine similarity measure. In
information retrieval, the weighted term represen-
tation works well as the comparison is generally
between pieces of text with large context vectors.
In the relation discovery task, though, the term
contexts (as we will define them in Section 4) can
be very small, often consisting of only one or two
words. This means that a term-based similarity
matrix between entity pairs is very sparse, which
may pose problems for performing reliable clus-
tering.
An alternative method widely used in NLP
and cognitive science is to represent a term con-
text by its neighbouring words as opposed to the
documents in which it occurs. This term co-
occurrence (TxT) model is based on the intu-
ition that two words are semantically similar if
they appear in a similar set of contexts (see e.g.
Pado and Lapata (2003)). The current work ex-
plores such a term co-occurrence (TxT) represen-
tation based on the hypothesis that it will provide
a more robust representation of relation contexts
and help overcome the sparsity problems asso-
ciated with weighted term representations in the
relation discovery task. This is compared to a
baseline term-by-document (TxD) representation
which is a re-implementation of the approach used
by Hasegawa et al. (2004) and Chen et al. (2005).
</bodyText>
<subsectionHeader confidence="0.999321">
3.2 Dimensionality Reduction
</subsectionHeader>
<bodyText confidence="0.999186724137932">
Dimensionality reduction techniques for docu-
ment and corpus modelling aim to reduce descrip-
tion length and model a type of semantic similar-
ity that is more linguistic in nature (e.g., see Lan-
dauer et al.’s (1998) discussion of LSA and syn-
onym tests). In the current work, we explore sin-
gular value decomposition (Berry et al., 1994), a
technique from linear algebra that has been ap-
plied to a number of tasks from NLP and cogni-
tive modelling. We also explore latent Dirichlet
allocation, a probabilistic technique analogous to
singular value decomposition whose contribution
to NLP has not been as thoroughly explored.
Singular value decomposition (SVD) has been
used extensively for the analysis of lexical seman-
tics under the name of latent semantic analysis
(Landauer et al., 1998). Here, a rectangular matrix
is decomposed into the product of three matrices
(Xwxp = WwxnSnxn(Ppxn)T ) with n ’latent se-
mantic’ dimensions. The resulting decomposition
can be viewed as a rotation of the n-dimensional
axes such that the first axis runs along the direction
of largest variation among the documents (Man-
ning and Sch¨utze, 1999). W and P represent
terms and documents in the new space. And S is
a diagonal matrix of singular values in decreasing
order.
Taking the product WwxkSkxk(Ppxk)T over
the first D columns gives the best least square ap-
proximation of the original matrix X by a matrix
of rank D, i.e. a reduction of the original matrix to
D dimensions. SVD can equally be applied to the
word co-occurrence matrices obtained in the TxT
representation presented in Section 2, in which
case we can think of the original matrix as being a
term x co-occurring term feature matrix.
While SVD has proved successful and has been
adapted for tasks such as word sense discrimi-
nation (Sch¨utze, 1998), its behaviour is not easy
to interpret. Probabilistic LSA (pLSA) is a gen-
erative probabilistic version of LSA (Hofmann,
2001). This models each word in a document as
a sample from a mixture model, but does not pro-
vide a probabilistic model at the document level.
Latent Dirichlet Allocation (LDA) addresses this
by representing documents as random mixtures
over latent topics (Blei et al., 2003). Besides hav-
ing a clear probabilistic interpretation, an addi-
tional advantage of these models is that they have
intuitive graphical representations.
Figure 3 contains a graphical representation
of the LDA model as applied to TxT word
co-occurrence matrices in standard plate nota-
tion. This models the word features f in the
co-occurrence context (size N) of each word w
(where w E W and |W |= W) with a mixture of
topics z. In its generative mode, the LDA model
samples a topic from the word-specific multino-
</bodyText>
<page confidence="0.997182">
27
</page>
<figureCaption confidence="0.99818">
Figure 3: Graphical representation of LDA.
</figureCaption>
<bodyText confidence="0.999949875">
mial distribution 0. Then, each context feature is
generated by sampling from a topic-specific multi-
nomial distribution 0z.3 In a manner analogous to
the SVD model, we use the distribution over top-
ics for a word w to represent its semantics and we
use the average topic distribution over all context
words to represent the conceptual content of an en-
tity pair context.
</bodyText>
<subsectionHeader confidence="0.999864">
3.3 Measuring Similarity
</subsectionHeader>
<bodyText confidence="0.9864045">
Cosine (Cos) is commonly used in the literature to
compute similarities between tf*idf vectors:
</bodyText>
<equation confidence="0.9723665">
Cos(p,q) = V E p2V E q2
�i piqi
</equation>
<bodyText confidence="0.995938">
In the current work, we use cosine over term
and SVD representations of entity pair context.
However, it is not clear which similarity measure
should be used for the probabilistic topic models.
Dagan et al. (1997) find that the symmetric infor-
mation radius measure performs best on a pseudo-
word sense disambiguation task, while Lee (1999)
find that the asymmetric skew divergence – a gen-
eralisation of Kullback-Leibler divergence – per-
forms best for improving probability estimates for
unseen word co-occurrences.
In the current work, we compare KL divergence
with two methods for deriving a symmetric mea-
3The hyperparameters α and Q are Dirichlet priors on the
multinomial distributions for word features (0 — Dir(Q))
and topics (0 — Dir(α)). The choice of the Dirichlet is
explained by its conjugacy to the multinomial distribution,
meaning that if the parameter (e.g. 0, 0) for a multinomial
distribution is endowed with a Dirichlet prior then the poste-
rior will also be a Dirichlet. Intuitively, it is a distribution over
distributions used to encode prior knowledge about the pa-
rameters (0 and 0) of the multinomial distributions for word
features and topics. Practically, it allows efficient estimation
of the joint distribution over word features and topics P( f, ��)
by integrating out 0 and 0.
sure. The KL divergence of two probability dis-
tributions (p and q) over the same event space is
defined as:
</bodyText>
<equation confidence="0.972335666666667">
�
KL(p||q) =
i
</equation>
<bodyText confidence="0.9992366">
In information-theoretic terms, KL divergence is
the average number of bits wasted by encoding
events from a distribution p with a code based on
distribution q. The symmetric measures are de-
fined as:
</bodyText>
<equation confidence="0.910406">
1
Sym(p, q) = 2 [KL(p||q) + KL(q||p)]
JS(p, q) = 2 I KL (p ||p 2 q) + KL (q||p 2 q)J
</equation>
<bodyText confidence="0.999954761904762">
The first is termed symmetrised KL divergence
(Sym) and the second is termed Jensen-Shannon
(JS) divergence. We explore KL divergence as
well as the symmetric measures as it is not known
in advance whether a domain is symmetric or not.
Technically, the divergence measures are dis-
similarity measures as they calculate the differ-
ence between two distributions. However, they
can be converted to increasing measures of simi-
larity through various transformations. We treated
this as a parameter to be tuned during develop-
ment and considered two approaches. The first is
from Dagan et al. (1997). For KL divergence, this
function is defined as Sim(p, q) = 10−,3KL(p||q),
where Q is a free parameter, which is tuned on the
development set (as described in Section 4.2). The
same procedure is applied for symmetric KL di-
vergence and JS divergence. The second approach
is from Lee (1999). Here similarity for KL is de-
fined as Sim(p, q) = C − KL(p||q), where C is
a free parameter to be tuned.
</bodyText>
<sectionHeader confidence="0.998681" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.862846">
4.1 Materials
</subsectionHeader>
<bodyText confidence="0.999252">
Following Chen et al. (2005), we derive our rela-
tion discovery data from the automatic content ex-
traction (ACE) 2004 and 2005 materials for eval-
uation of information extraction.4 This is prefer-
able to using the New York Times data used by
Hasegawa et al. (2004) as it has gold standard an-
notation, which can be used for unbiased evalua-
tion.
The relation clustering data is based on the gold
standard relations in the information extraction
</bodyText>
<footnote confidence="0.737602">
4http://www.nist.gov/speech/tests/ace/
</footnote>
<figure confidence="0.9948644">
R
f
T
a
q
z
f Nd
W
pi log pi
qi
</figure>
<page confidence="0.995469">
28
</page>
<bodyText confidence="0.999922847826087">
data. We only consider data from newswire or
broadcast news sources. We constructed six data
subsets from the ACE corpus based on four of the
ACE entities: persons (PER), organisations (ORG),
geographical/social/political entities (GPE) and fa-
cilities (FAC). The six data subsets were chosen
during development based on a lower limit of 50
for the data subset size (i.e. the number of entity
pairs in the domain), ensuring that there is a rea-
sonable amount of data. We also set a lower limit
of 3 for the number of classes (relation types) in a
data subset, ensuring that the clustering task is not
too simple.
The entity pair instances for clustering were
chosen based on several criteria. First, we do not
use ACE’s discourse relations, which are relations
in which the entity referred to is not an official en-
tity according to world knowledge. Second, we
only use pairs with one or more non-stop words
in the intervening context, that is the context be-
tween the two entity heads.5 Finally, we only keep
relation classes with 3 or more members. Table
4.1 contains the full list of relation types from the
subsets of ACE that we used. (Refer to Table 4.2
for definition of the relation type abbreviations.)
We use the Infomap tool6 for singular value
decomposition of TxT matrices and compute the
conceptual content of an entity pair context as the
average over the reduced D-dimensional represen-
tation of the co-occurrence vector of the terms in
the relation context. For LDA, we use Steyvers
and Griffiths’ Topic Modeling Toolbox7). The in-
put is produced by a version of Infomap which
was modified to output the TxT matrix. Again, we
compute the conceptual content of an entity pair
as the average over the topic vectors for the con-
text words. As documents are explicitly modelled
in the LDA model, we input a matrix with raw fre-
quencies. In the TxD, unreduced TxT and SVD
models we use tf*idf term weighting.
We use the same preprocessing when prepar-
ing the text for building the SVD and probabilistic
topic models as we use for processing the interven-
ing context of entity pairs. This consisted of Mx-
Terminator (Reynar and Ratnaparkhi., 1997) for
sentence boundary detection, the Penn Treebank
</bodyText>
<footnote confidence="0.794045857142857">
5Following results reported by Chen et al. (2005), who
tried unsuccessfully to incorporate words from the surround-
ing context to represent a relation’s semantics, we use only
intervening words.
6http://infomap.stanford.edu/
7http://psiexp.ss.uci.edu/research/
programs_data/toolbox.htm
</footnote>
<bodyText confidence="0.942546666666667">
sed script8 for tokenisation, and the Infomap stop
word list. We also use an implementation of the
Porter algorithm (Porter, 1980) for stemming.9
</bodyText>
<subsectionHeader confidence="0.993693">
4.2 Model Selection
</subsectionHeader>
<bodyText confidence="0.999977536585366">
We used the ACE 2004 relation data to perform
model selection. Firstly, dimensionality (D) needs
to be optimised for SVD and LDA. SVD was
found to perform best with the number of dimen-
sions set to 10. For LDA, dimensionality inter-
acts with the divergence-to-similarity conversion
so they were tuned jointly. The optimal con-
figuration varies by the divergence measure with
D = 50 and C = 14 for KL divergence, D = 200
and C = 4 for symmetrised KL, and D = 150
and C = 2 for JS divergence. For all divergence
measures, Lee’s (1999) method outperformed Da-
gan et al.’s (1997) method. Also for all divergence
measures, the model hyper-parameter Q was found
to be optimal at 0.0001. The α hyper-parameter
was always set to 50/T following Griffiths and
Steyvers (2004).
Clustering is performed with the CLUTO soft-
ware10 and the technique used is identical across
models. Agglomerative clustering is used for
comparability with the original relation discovery
work of Hasegawa et al. (2004). This choice was
motivated because as it is not known in advance
how many clusters there should be in a new do-
main.
One way to view the clustering problem is as
an optimisation process where an optimal cluster-
ing is chosen with respect to a criterion function
over the entire solution. The criterion function
used here was chosen based on performance on
the development data. We compared a number of
criterion functions including single link, complete
link, group average, I1, I2, £1 and H1. I1 is a
criterion function that maximises sum of pairwise
similarities between relation instances assigned to
each cluster, I2 is an internal criterion function
that maximises the similarity between each rela-
tion instance and the centroid of the cluster it is as-
signed to, £1 is an external criterion function that
minimises the similarity between the centroid vec-
tor of each cluster and the centroid vector of the
</bodyText>
<footnote confidence="0.996304333333333">
8http://www.cis.upenn.edu/-treebank/
tokenizer.sed
9http://www.ldc.usb.ve/-vdaniel/
porter.pm
10http://glaros.dtc.umn.edu/gkhome/
cluto/cluto/overview
</footnote>
<page confidence="0.994493">
29
</page>
<table confidence="0.979304181818182">
ORG-GPE ORG-ORG PER-FAC PER-GPE PER-ORG PER-PER
basedin 54 subsidiary 36 located 127 located 222 staff 121 business 81
subsidiary 27 emporgothr 14 owner 14 resident 79 executive 100 family 20
located 15 partner 8 near 4 executive 42 member 44 persocothr 16
gpeaffothr 3 member 6 staff 30 emporgothr 27 perorgothr 9
employgen 7 employgen 9 near 7
located 4 ethnic 5
executive 3
ideology 3
member 3
Total 99 Total 64 Total 145 Total 380 Total 305 Total 147
</table>
<tableCaption confidence="0.999818">
Table 1: Relation distributions for entity pair domains.
</tableCaption>
<figure confidence="0.98493645">
Type Subtype Abbr
AGENT-ARTIFACT User-or-Owner owner
EMPLOY/MEMBER Employ-Executive executive
Employ-Staff staff
Employ-Undet’d employgen
Member-of-Group member
Other artothr
Partner partner
Subsidiary subsidiary
GPE AFFILIATION Based-In basedin
Citizen-or-Resdent resident
Other gpeaffothr
PER/ORG AFFIL’N Ethnic ethnic
Ideology ideology
Other perorgothr
PERSONAL-SOC’L Business business
Family family
Other persocothr
PHYSICAL Located located
Near near
</figure>
<tableCaption confidence="0.8744605">
Table 2: Overview of ACE relations with abbrevi-
ations used here.
</tableCaption>
<bodyText confidence="0.997962875">
entire collection, and x1 is a combined criterion
function that consists of the ration of I1 over 91.
The I2, x1 and x2 criterion functions outper-
formed single link, complete link and group aver-
age on the development data. We use I2, which
performed as well as x1 and x2 and is superior
in terms of computational complexity (Zhao and
Karypis, 2004).
</bodyText>
<sectionHeader confidence="0.997888" genericHeader="method">
5 Experiment
</sectionHeader>
<subsectionHeader confidence="0.988255">
5.1 Method
</subsectionHeader>
<bodyText confidence="0.993631166666667">
This section describes experimental setup, which
uses relation extraction data from ACE 2005 to an-
swer four questions concerning the effectiveness
of similarity models based on term co-occurrence
and dimensionality reduction for the relation dis-
covery task:
</bodyText>
<listItem confidence="0.976892090909091">
1. Do term co-occurrence models provide a bet-
ter representation of relation semantics than
standard term-by-document vector space?
2. Do textual dimensionality reduction tech-
niques provide any further improvements?
3. How do probabilistic topic models perform
with respect to SVD on the relation discovery
task?
4. Does one similarity measure (for probability
distributions) outperform the others on the re-
lation discovery task?
</listItem>
<bodyText confidence="0.999575545454546">
System configurations are compared across
six different data subsets (entity type pairs, i.e.,
organisation-geopolitical entity, organisation-
organisation, person-facility, person-geopolitical
entity, person-organisation, person-person)
and evaluated following suggestions by
Demˇsar (2006) for statistical comparison of
classifiers over multiple data sets.
The dependent variable is the clustering perfor-
mance as measured by the F-score. F-score ac-
counts for both the amount of predictions made
that are true (Precision) and the amount of true
classes that are predicted (Recall). We use the
CLUTO implementation of this measure for eval-
uating hierarchical clustering. Based on (Larsen
and Aone, 1999), this is a balanced F-score
(F = 2�P �+P ) that computes the maximum per-class
score over all possible alignments of gold stan-
dard classes with nodes in the hierarchical tree.
The average F-score for the entire hierarchical tree
is a micro-average over the class-specific scores
weighted according to the relative size of the class.
</bodyText>
<subsectionHeader confidence="0.88565">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999715">
Table 3 contains F-score performance on the test
set (ACE 2005). The columns contain results from
the different system configurations. The column
labels in the top row indicate the different repre-
sentations of relation similarity. The column la-
bels in the second row indicate the dimensional-
</bodyText>
<page confidence="0.992301">
30
</page>
<table confidence="0.999357166666667">
Sem Space TxD TxT TxT TxT TxT TxT
Dim Red’n None None SVD LDA LDA LDA
Similarity Cos Cos Cos KL Sym JS
ORG-GPE 0.644 0.673 0.645 0.680 0.670 0.673
ORG-ORG 0.879 0.922 0.879 0.904 0.900 0.904
PER-FAC 0.811 0.827 0.831 0.832 0.826 0.820
PER-GPE 0.595 0.637 0.627 0.664 0.642 0.670
PER-ORG 0.520 0.551 0.532 0.569 0.552 0.569
PER-PER 0.534 0.572 0.593 0.633 0.553 0.618
Micro Ave 0.627 0.661 0.652 0.683 0.658 0.681
Macro Ave 0.664 0.697 0.684 0.714 0.689 0.709
RankAve 5.917 3.083 4.250 1.500 4.000 2.250
</table>
<tableCaption confidence="0.937919">
Table 3: F-score performance on the test data (ACE 2005) using agglomerative clustering with the 12
criterion function.
</tableCaption>
<bodyText confidence="0.999192032258065">
ity reduction technique used. The column labels
in the third row indicated the similarity measure
used, i.e. cosine (Cos) and KL (KL), symmetrised
KL (Sym) and JS (JS) divergence. The rows con-
tain results for the different data subsets. While
we do not use them for analysis of statistical sig-
nificance, we include micro and macro averages
over the data subsets.11 We also include the aver-
age ranks, which show that the LDA system using
KL divergence performed best.
Initial inspection of the table shows that all sys-
tems that use the term co-occurrence semantic
space outperform the baseline system that uses the
term-by-document semantic space. To test for sta-
tistical significance, we use non-parametric tests
proposed by Demˇsar (2006) for comparing clas-
sifiers across multiple data sets. The use of non-
parametric tests is safer here as they do not as-
sume normality and outliers have less effect. The
first test we perform is a Friedman test (Friedman,
1940), a multiple comparisons technique which
is the non-parametric equivalent of the repeated-
measures ANOVA. The null hypothesis is that all
models perform the same and observed differences
are random. With a Friedman statistic (X2F ) of
21.238, we reject the null hypothesis at p &lt; 0.01.
The first question we wanted to address is
whether term co-occurrence models outperform
the term-by-document representation of relation
semantics. To address this question, we continue
with post-hoc analysis. The objective here is to
</bodyText>
<footnote confidence="0.56341675">
11Averages over data sets are unreliable where it is not
clear whether the domains are commensurable (Webb, 2000).
We present averages in our results but avoid drawing conclu-
sions based on them.
</footnote>
<bodyText confidence="0.999605257142857">
compare several conditions to a control (i.e., com-
pare the term co-occurrence systems to the term-
by-document baseline) so we use a Bonferroni-
Dunn test. At a significance level of p &lt; 0.05,
the critical difference for the Bonferroni-Dunn test
for comparing 6 systems across 6 data sets is
2.782. We conclude that the unreduced term co-
occurrence system and the LDA systems with KL
and JS divergence all perform significantly better
than baseline, while the SVD system and the LDA
system with symmetrised KL divergence do not.
The second question asks whether SVD and
LDA dimensionality reduction techniques provide
any further improvement. We observe that the sys-
tems using KL and JS divergence both outperform
the unreduced term co-occurrence system, though
the difference is not significant.
The third question asks how the probabilistic
topic models perform with respect to the SVD
models. Here, Holm-correct Wilcoxon signed-
ranks tests show that the KL divergence system
performs significantly better than SVD while the
symmetrised KL divergence and JS divergence
systems do not.
The final question is whether one of the diver-
gence measures (KL, symmetrised KL or JS) out-
performs the others. With a statistic of X2F �
9.336, we reject the null hypothesis that all sys-
tems are the same at p &lt; 0.01. Post-hoc analysis
with Holm-corrected Wilcoxon signed-ranks tests
show that the KL divergence system and the JS
divergence system both perform significantly bet-
ter than the symmetrised KL system at p &lt; 0.05,
while there is no significant difference between the
KL and JS systems.
</bodyText>
<page confidence="0.999877">
31
</page>
<sectionHeader confidence="0.998291" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999978361702128">
An interesting aspect of using the ACE corpus is
the wealth of linguistic knowledge encoded. With
respect to named entities, this includes class infor-
mation describing the kind of reference the entity
makes to something in the world (i.e., specific ref-
erential, generic referential, under-specified ref-
erential) and it includes mention type informa-
tion (i.e., names, quantified nominal construc-
tions, pronouns). It also includes information de-
scribing the lexical condition of a relation (i.e.,
possessive, preposition, pre-modifier, formulaic, ,
verbal). Based on a mapping between gold stan-
dard and predicted clusters, we assigned each case
a value of 1 or 0 to indicate whether it is a correct
or incorrect classification. We then carried out de-
tailed statistical analysis12 to test for effects of the
entity and relation information described above on
each system in each domain.
Overall, the effects were fairly small and do not
generalise across domains or systems very well.
However, there were some observable tendencies.
With respect to entity class, relations with specific
referential entities tend to correlate positively with
correct classifications while under-specified refer-
ential entities tend to correlate negatively with cor-
rect classifications. With respect to entity men-
tion type, relations entities that consist of names
tend to correlate positively with correct classifica-
tions while pronouns tend to correlate negatively
with correct classifications. Though, this is only
reliably observed in the PER-GPE domain. Fi-
nally, with respect to lexical condition, we observe
that possessive conditioned relations tend to cor-
relate negatively, especially in the PER-GPE and
PER-ORG domains with the PER-PER domain also
showing some effect. Pre-modifier conditioned re-
lations also tend to correlate negatively in the PER-
GPE domain. The effect with verbally conditioned
relations is mixed. This is probably due to the
fact that verbal relations tend to have more words
occurring between the entity pair, which provides
more context but can also be misleading when the
key terms describing the relation do not occur be-
tween the entity pair (e.g., the first sentence in Fig-
ure 1).
It is also informative to look at overall proper-
ties of the entity pair domains and compare this
</bodyText>
<footnote confidence="0.990922">
12For this analysis, we used the Phi coefficient, which is
a measure of relatedness for binomial variables that is inter-
preted like correlation.
</footnote>
<table confidence="0.999945142857143">
Domain Score TTR Entrpy
ORG-GPE 0.680 0.893 1.554
ORG-ORG 0.904 0.720 1.642
PER-FAC 0.832 0.933 0.636
PER-GPE 0.664 0.933 1.671
PER-ORG 0.569 0.973 2.001
PER-PER 0.633 0.867 2.179
</table>
<tableCaption confidence="0.993958">
Table 4: System score, type-to-token ratio (TTR)
</tableCaption>
<bodyText confidence="0.990912523809524">
and relation type entropy (Entrpy) for entity pair
domains.
to the system performance. Table 6 contains, for
each domain, the F-score of the LDA+KL system,
the type-to-token ratio, and the entropy of the re-
lation type distribution for each domain. Type-to-
token ratio (TTR) is the number of words divided
by the number of word instances and indicates
how much repetition there is in word use. Since
TTR can vary depending on the size of the text,
we compute it on a random sample of 75 tokens
from each domain. Entropy can be interpreted as
a measure of the uniformity of a distribution. Low
entropy indicates a more spiked distribution while
high entropy indicates a more uniform distribu-
tion. Though there is not enough data to make a
reliable conclusion, it seems that the system does
poorly on domains that have both a high type-to-
token ratio and a high entropy (uniform relation
type distribution), while it performs very well on
domains that have low TTR or low entropy.
</bodyText>
<sectionHeader confidence="0.997274" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9999855">
This paper presented work on the relation dis-
covery task. We tested several systems for the
clustering subtask that use different models of the
conceptual/semantic similarity of relations. These
models included a baseline system based on a
term-by-document representation of term context,
which is equivalent to the representation used in
previous work by Hasegawa et al. (Hasegawa et
al., 2004) and Chen et al. (Chen et al., 2005). We
hypothesised that this representation suffers from
a sparsity problem and showed that models that
use a term co-occurrence representation perform
significantly better.
Furthermore, we investigated the use of singular
value decomposition and latent Dirichlet alloca-
tion for dimensionality reduction. It has been sug-
gested that representations using these techniques
are able to model a similarity that is less reliant on
</bodyText>
<page confidence="0.997409">
32
</page>
<bodyText confidence="0.999985290322581">
specific word forms and therefore more semantic
in nature. Our experiments showed an improve-
ment over a term co-occurrence baseline when us-
ing LDA with KL and JS divergence, though it
was not significant. We also found that LDA with
KL divergence performs significantly better than
SVD.
Comparing the different divergence measures
for LDA, we found that KL and JS perform sig-
nificantly better than symmetrised KL divergence.
Interestingly, the performance of the asymmetric
KL divergence and the symmetric JS divergence
is very close, which makes it difficult to con-
clude whether the relation discovery domain is a
symmetric domain or an asymmetric domain like
Lee’s (1999) task of improving probability esti-
mates for unseen word co-occurrences.
A shortcoming of all the models we will de-
scribe here is that they are derived from the basic
bag-of-words models and as such do not account
for word order or other notions of syntax. Related
work on relation discovery by Zhang et al. (2005)
addresses this shortcoming by using tree kernels to
compute similarity between entity pairs. In future
work we will extend our experiment to explore the
use of syntactic and semantic features following
the frame work of Pado and Lapata (2003). We
are also planning to look at non-parametric ver-
sions of LDA that address the model order selec-
tion problem and perform an extrinsic evaluation
of the relation discovery task.
</bodyText>
<sectionHeader confidence="0.99495" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999832571428571">
This work was supported by Scottish Enterprise
Edinburgh-Stanford Link grant R37588 as part of
the EASIE project. I would like to thank Claire
Grover, Mirella Lapata, Gabriel Murray and Se-
bastian Riedell for very useful comments and dis-
cussion on this work. I would also like to thank
the anonymous reviewers for their comments.
</bodyText>
<sectionHeader confidence="0.999474" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999639433333333">
Michael W. Berry, Susan T. Dumais, and Gavin W.
O’Brien. 1994. Using linear algebra for intelligent
information retrieval. SIAMReview, 37(4):573–595.
Christian Blaschke and Alfonso Valencia. 2002. The
frame-based module of the suiseki information ex-
traction system. IEEE Intelligent Systems, 17:14–
20.
David Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. Journal of Ma-
chine Learning Research, 3.
Razvan C. Bunescu and Raymond J. Mooney. 2005.
Subsequence kernels for relation extraction. In Pro-
ceedings of the 19th Conference on Neural Informa-
tion Processing Systems, Vancouver, BC, Canada.
Jinxiu Chen, Donghong Ji, Chew Lim Tan, and
Zhengyu Niu. 2005. Automatic relation extraction
with model order selection and discriminative label
identification. In Proceedings of the 2nd Interna-
tional Joint Conference on Natural Language Pro-
cessing.
Ido Dagan, Lillian Lee, and Fernando Pereira. 1997.
Similarity-based methods for word sense disam-
biguation. In Proceedings of the 35th Annual Meet-
ing of the Association for Computational Linguis-
tics, Madrid, Spain.
Janez Demˇsar. 2006. Statistical comparisons of clas-
sifiers over multiple data sets. Journal of Machine
Learning Research, 7:1–30, Jan.
Elena Filatova and Vasileios Hatzivassiloglou. 2004.
Event-based extractive summarization. In Proceed-
ings of the ACL-2004 Text Summarization Branches
Out Workshop, Barcelona, Spain.
Milton Friedman. 1940. A comparison of alternative
tests of significance for the problem of m rankings.
The Annals ofMathematical Statistics, 11:86–92.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101:5228–5235.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grish-
man. 2004. Discovering relations among named
entities from large corpora. In Proceedings of the
42nd Annual Meeting of Association of Computa-
tional Linguistics.
Lynette Hirschman, Alexander Yeh, Christian
Blaschke, and Alfonso Valencia. 2004. Overview
of BioCreAtIvE: Critical assessment of information
extraction for biology. In Proceedings of Critical
Assessment of Information Extraction Systems in
Biology Workshop (BioCreAtIvE), Granada, Spain.
Thomas Hofmann. 2001. Unsupervised learning
by probabilistic latent semantic analysis. Machine
Learning, 42:177–196.
Thomas K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. An introduction to latent semantic anal-
ysis. Discourse Processes, 25:259–284.
Buornar Larsen and Chinatsu Aone. 1999. Fast and ef-
fective text mining using linear-time document clus-
tering. In Proceedings of the 5th ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining, San Diego, CA, USA.
</reference>
<page confidence="0.985537">
33
</page>
<reference confidence="0.999733782608695">
Ludovic Lebart and Martin Rajman. 2000. Comput-
ing similarity. In Robert Dale, Hermann Moisl, and
Harold Somers, editors, Handbook of Natural Lan-
guage Processing, pages 477–505. Marcel Dekker,
New York.
Lillian Lee. 1999. Measures of distributional similar-
ity. In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics, Col-
lege Park, MD, USA.
Christopher D. Manning and Hinrich Sch¨utze. 1999.
Foundations of Statistical Natural Language Pro-
cessing. MIT Press.
Peter V. Marsden and Nan Lin, editors. 1982. So-
cial Structure and Network Analysis. Sage, Beverly
Hills.
Sebastian Pado and Mirella Lapata. 2003. Construct-
ing semantic space models from parsed corpora. In
Proceedings of the 41st Annual Meeting of the As-
sociation for Computational Linguistics, Sapporo,
Japan.
Martin F. Porter. 1980. An algorithm for suffix strip-
ping. Program, 14(3):130–137.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A
maximum entropy approach to identifying sentence
boundaries. In Proceedings of the 5th Conference on
Applied Natural Language Processing, Washington,
D.C., USA.
Barbara Rosario and Marti Hearst. 2004. Classifying
semantic relations in bioscience text. In Proceed-
ings of the 42nd Annual Meeting of the Association
for Computational Linguistics, Barcelona, Spain.
Hinrich Sch¨utze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):91–
124.
Geoffrey I. Webb. 2000. Multiboosting: A tech-
nique for combining boosting and wagging. Ma-
chine Learning, 40(2):159–196.
Min Zhang, Jian Su, Danmei Wang, Guodong Zhou,
and Chew Lim Tan. 2005. Discovering relations
from a large raw corpus using tree similarity-based
clustering. In Proceedings of the 2nd International
Joint Conference on Natural Language Processing.
Ying Zhao and George Karypis. 2004. Empirical and
theoretical comparisons of selected criterion func-
tions for document clustering. Machine Learning,
55:311–331.
</reference>
<page confidence="0.99932">
34
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.479638">
<title confidence="0.999692">Comparison of Similarity Models for the Relation Discovery Task</title>
<author confidence="0.998755">Ben</author>
<affiliation confidence="0.831286333333333">School of University of 2 Buccleuch Place, Edinburgh EH8</affiliation>
<email confidence="0.997447">bhachey@inf.ed.ac.uk</email>
<abstract confidence="0.998532263157895">We present results on the relation discovery task, which addresses some of the shortcomings of supervised relation extraction by applying minimally supervised methods. We describe a detailed experimental design that compares various configurations of conceptual representations and similarity measures across six different subsets of the ACE relation extraction data. Previous work on relation discovery used a semantic space based on a term-bydocument matrix. We find that representations based on term co-occurrence perform significantly better. We also observe further improvements when reducing the dimensionality of the term co-occurrence matrix using probabilistic topic models, though these are not significant.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael W Berry</author>
<author>Susan T Dumais</author>
<author>Gavin W O’Brien</author>
</authors>
<title>Using linear algebra for intelligent information retrieval.</title>
<date>1994</date>
<journal>SIAMReview,</journal>
<volume>37</volume>
<issue>4</issue>
<marker>Berry, Dumais, O’Brien, 1994</marker>
<rawString>Michael W. Berry, Susan T. Dumais, and Gavin W. O’Brien. 1994. Using linear algebra for intelligent information retrieval. SIAMReview, 37(4):573–595.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Blaschke</author>
<author>Alfonso Valencia</author>
</authors>
<title>The frame-based module of the suiseki information extraction system.</title>
<date>2002</date>
<journal>IEEE Intelligent Systems,</journal>
<volume>17</volume>
<pages>20</pages>
<contexts>
<context position="5529" citStr="Blaschke and Valencia (2002)" startWordPosition="821" endWordPosition="824"> compute similarity. We also build on the evaluation paradigm for relation discovery with a detailed, controlled experimental setup. Section 4 describes the experiment design, which compares the various system configurations across six different subsets of the relation extraction data from the automatic content extraction (ACE) evaluation. Finally, Section 5 presents results and statistical analysis. 2 The Relation Discovery Task Conventionally, relation extraction is considered to be part of information extraction and has been approached through supervised learning or rule engineering (e.g., Blaschke and Valencia (2002), Bunescu and Mooney (2005)). However, traditional approaches have several shortcomings. First 1The relation discovery task is minimally supervised in the sense that it relies on having certain resources such as named entity recognition. The focus of the current paper is the unsupervised task of clustering relations. and foremost, they are generally based on predefined templates of what types of relations exist in the data and thus only capture information whose importance was anticipated by the template designers. This poses reliability problems when predicting new data in the same domain as </context>
</contexts>
<marker>Blaschke, Valencia, 2002</marker>
<rawString>Christian Blaschke and Alfonso Valencia. 2002. The frame-based module of the suiseki information extraction system. IEEE Intelligent Systems, 17:14– 20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>3</volume>
<contexts>
<context position="12318" citStr="Blei et al., 2003" startWordPosition="1939" endWordPosition="1942">on 2, in which case we can think of the original matrix as being a term x co-occurring term feature matrix. While SVD has proved successful and has been adapted for tasks such as word sense discrimination (Sch¨utze, 1998), its behaviour is not easy to interpret. Probabilistic LSA (pLSA) is a generative probabilistic version of LSA (Hofmann, 2001). This models each word in a document as a sample from a mixture model, but does not provide a probabilistic model at the document level. Latent Dirichlet Allocation (LDA) addresses this by representing documents as random mixtures over latent topics (Blei et al., 2003). Besides having a clear probabilistic interpretation, an additional advantage of these models is that they have intuitive graphical representations. Figure 3 contains a graphical representation of the LDA model as applied to TxT word co-occurrence matrices in standard plate notation. This models the word features f in the co-occurrence context (size N) of each word w (where w E W and |W |= W) with a mixture of topics z. In its generative mode, the LDA model samples a topic from the word-specific multino27 Figure 3: Graphical representation of LDA. mial distribution 0. Then, each context featu</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research, 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>Subsequence kernels for relation extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of the 19th Conference on Neural Information Processing Systems,</booktitle>
<location>Vancouver, BC,</location>
<contexts>
<context position="5556" citStr="Bunescu and Mooney (2005)" startWordPosition="825" endWordPosition="828">uild on the evaluation paradigm for relation discovery with a detailed, controlled experimental setup. Section 4 describes the experiment design, which compares the various system configurations across six different subsets of the relation extraction data from the automatic content extraction (ACE) evaluation. Finally, Section 5 presents results and statistical analysis. 2 The Relation Discovery Task Conventionally, relation extraction is considered to be part of information extraction and has been approached through supervised learning or rule engineering (e.g., Blaschke and Valencia (2002), Bunescu and Mooney (2005)). However, traditional approaches have several shortcomings. First 1The relation discovery task is minimally supervised in the sense that it relies on having certain resources such as named entity recognition. The focus of the current paper is the unsupervised task of clustering relations. and foremost, they are generally based on predefined templates of what types of relations exist in the data and thus only capture information whose importance was anticipated by the template designers. This poses reliability problems when predicting new data in the same domain as the training data will be f</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan C. Bunescu and Raymond J. Mooney. 2005. Subsequence kernels for relation extraction. In Proceedings of the 19th Conference on Neural Information Processing Systems, Vancouver, BC, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinxiu Chen</author>
<author>Donghong Ji</author>
<author>Chew Lim Tan</author>
<author>Zhengyu Niu</author>
</authors>
<title>Automatic relation extraction with model order selection and discriminative label identification.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2nd International Joint Conference on Natural Language Processing.</booktitle>
<contexts>
<context position="4189" citStr="Chen et al., 2005" startWordPosition="632" endWordPosition="635">ce extraction framework (Filatova and Hatzivassiloglou, 2004). In the next section, we motivate and introduce the relation discovery task, which addresses some of the shortcomings of conventional approaches to relation extraction (i.e. supervised learning or rule engineering) by applying minimally supervised methods.1 A critical part of the relation discovery task is grouping entity pairs by their relation type. This is a clustering task and requires a robust conceptual representation of relation semantics and a measure of similarity between relations. In previous work (Hasegawa et al., 2004; Chen et al., 2005), the conceptual representation has been limited to term-by-document (TxD) models of relation semantics. The current work introduces a term co-occurrence (TxT) representation for the relation discovery task and shows that it performs significantly better than the TxD representation. We also explore dimensionality reduction techniques, which show a further improvement. Section 3 presents a parameterisation of similarity models for relation discovery. For the purposes of the current work, this consists of the semantic representation for terms (i.e. how a term’s context is modelled), dimensionali</context>
<context position="8040" citStr="Chen et al. (2005)" startWordPosition="1240" endWordPosition="1243">this, a model of relation similarity is required, which is the focus of the current work. We also assume that it is possible to perform the third step.2 The evaluation we present here looks just at the quality of the clustering and does not attempt to assess the labelling task. 3 Modelling Relation Similarity The possible space of models for relation similarity can be explored in a principled manner by parameterisation. In this section, we discuss several 2Previous approaches select labels from the collection of context words for a relation cluster (Hasegawa et al., 2004; Zhang et al., 2005). Chen et al. (2005) use discriminative category matching to make sure that selected labels are also able to differentiate between clusters. 26 parameters including the term context representation, whether or not we apply dimensionality reduction, and what similarity measure we use. 3.1 Term Context Representing texts in such a way that they can be compared is a familiar problem from the fields of information retrieval (IR), text mining (TM), textual data analysis (TDA) and natural language processing (NLP) (Lebart and Rajman, 2000). The traditional model for IR and TM is based on a term-by-document (TxD) vector </context>
<context position="10096" citStr="Chen et al. (2005)" startWordPosition="1571" endWordPosition="1574">urrence (TxT) model is based on the intuition that two words are semantically similar if they appear in a similar set of contexts (see e.g. Pado and Lapata (2003)). The current work explores such a term co-occurrence (TxT) representation based on the hypothesis that it will provide a more robust representation of relation contexts and help overcome the sparsity problems associated with weighted term representations in the relation discovery task. This is compared to a baseline term-by-document (TxD) representation which is a re-implementation of the approach used by Hasegawa et al. (2004) and Chen et al. (2005). 3.2 Dimensionality Reduction Dimensionality reduction techniques for document and corpus modelling aim to reduce description length and model a type of semantic similarity that is more linguistic in nature (e.g., see Landauer et al.’s (1998) discussion of LSA and synonym tests). In the current work, we explore singular value decomposition (Berry et al., 1994), a technique from linear algebra that has been applied to a number of tasks from NLP and cognitive modelling. We also explore latent Dirichlet allocation, a probabilistic technique analogous to singular value decomposition whose contrib</context>
<context position="16155" citStr="Chen et al. (2005)" startWordPosition="2592" endWordPosition="2595">ough various transformations. We treated this as a parameter to be tuned during development and considered two approaches. The first is from Dagan et al. (1997). For KL divergence, this function is defined as Sim(p, q) = 10−,3KL(p||q), where Q is a free parameter, which is tuned on the development set (as described in Section 4.2). The same procedure is applied for symmetric KL divergence and JS divergence. The second approach is from Lee (1999). Here similarity for KL is defined as Sim(p, q) = C − KL(p||q), where C is a free parameter to be tuned. 4 Experimental Setup 4.1 Materials Following Chen et al. (2005), we derive our relation discovery data from the automatic content extraction (ACE) 2004 and 2005 materials for evaluation of information extraction.4 This is preferable to using the New York Times data used by Hasegawa et al. (2004) as it has gold standard annotation, which can be used for unbiased evaluation. The relation clustering data is based on the gold standard relations in the information extraction 4http://www.nist.gov/speech/tests/ace/ R f T a q z f Nd W pi log pi qi 28 data. We only consider data from newswire or broadcast news sources. We constructed six data subsets from the ACE </context>
<context position="18889" citStr="Chen et al. (2005)" startWordPosition="3062" endWordPosition="3065">in, we compute the conceptual content of an entity pair as the average over the topic vectors for the context words. As documents are explicitly modelled in the LDA model, we input a matrix with raw frequencies. In the TxD, unreduced TxT and SVD models we use tf*idf term weighting. We use the same preprocessing when preparing the text for building the SVD and probabilistic topic models as we use for processing the intervening context of entity pairs. This consisted of MxTerminator (Reynar and Ratnaparkhi., 1997) for sentence boundary detection, the Penn Treebank 5Following results reported by Chen et al. (2005), who tried unsuccessfully to incorporate words from the surrounding context to represent a relation’s semantics, we use only intervening words. 6http://infomap.stanford.edu/ 7http://psiexp.ss.uci.edu/research/ programs_data/toolbox.htm sed script8 for tokenisation, and the Infomap stop word list. We also use an implementation of the Porter algorithm (Porter, 1980) for stemming.9 4.2 Model Selection We used the ACE 2004 relation data to perform model selection. Firstly, dimensionality (D) needs to be optimised for SVD and LDA. SVD was found to perform best with the number of dimensions set to </context>
<context position="32765" citStr="Chen et al., 2005" startWordPosition="5243" endWordPosition="5246"> a high type-totoken ratio and a high entropy (uniform relation type distribution), while it performs very well on domains that have low TTR or low entropy. 7 Conclusions and Future Work This paper presented work on the relation discovery task. We tested several systems for the clustering subtask that use different models of the conceptual/semantic similarity of relations. These models included a baseline system based on a term-by-document representation of term context, which is equivalent to the representation used in previous work by Hasegawa et al. (Hasegawa et al., 2004) and Chen et al. (Chen et al., 2005). We hypothesised that this representation suffers from a sparsity problem and showed that models that use a term co-occurrence representation perform significantly better. Furthermore, we investigated the use of singular value decomposition and latent Dirichlet allocation for dimensionality reduction. It has been suggested that representations using these techniques are able to model a similarity that is less reliant on 32 specific word forms and therefore more semantic in nature. Our experiments showed an improvement over a term co-occurrence baseline when using LDA with KL and JS divergence</context>
</contexts>
<marker>Chen, Ji, Tan, Niu, 2005</marker>
<rawString>Jinxiu Chen, Donghong Ji, Chew Lim Tan, and Zhengyu Niu. 2005. Automatic relation extraction with model order selection and discriminative label identification. In Proceedings of the 2nd International Joint Conference on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Lillian Lee</author>
<author>Fernando Pereira</author>
</authors>
<title>Similarity-based methods for word sense disambiguation.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Madrid,</location>
<contexts>
<context position="13608" citStr="Dagan et al. (1997)" startWordPosition="2158" endWordPosition="2161">ion 0z.3 In a manner analogous to the SVD model, we use the distribution over topics for a word w to represent its semantics and we use the average topic distribution over all context words to represent the conceptual content of an entity pair context. 3.3 Measuring Similarity Cosine (Cos) is commonly used in the literature to compute similarities between tf*idf vectors: Cos(p,q) = V E p2V E q2 �i piqi In the current work, we use cosine over term and SVD representations of entity pair context. However, it is not clear which similarity measure should be used for the probabilistic topic models. Dagan et al. (1997) find that the symmetric information radius measure performs best on a pseudoword sense disambiguation task, while Lee (1999) find that the asymmetric skew divergence – a generalisation of Kullback-Leibler divergence – performs best for improving probability estimates for unseen word co-occurrences. In the current work, we compare KL divergence with two methods for deriving a symmetric mea3The hyperparameters α and Q are Dirichlet priors on the multinomial distributions for word features (0 — Dir(Q)) and topics (0 — Dir(α)). The choice of the Dirichlet is explained by its conjugacy to the mult</context>
<context position="15697" citStr="Dagan et al. (1997)" startWordPosition="2509" endWordPosition="2512">p ||p 2 q) + KL (q||p 2 q)J The first is termed symmetrised KL divergence (Sym) and the second is termed Jensen-Shannon (JS) divergence. We explore KL divergence as well as the symmetric measures as it is not known in advance whether a domain is symmetric or not. Technically, the divergence measures are dissimilarity measures as they calculate the difference between two distributions. However, they can be converted to increasing measures of similarity through various transformations. We treated this as a parameter to be tuned during development and considered two approaches. The first is from Dagan et al. (1997). For KL divergence, this function is defined as Sim(p, q) = 10−,3KL(p||q), where Q is a free parameter, which is tuned on the development set (as described in Section 4.2). The same procedure is applied for symmetric KL divergence and JS divergence. The second approach is from Lee (1999). Here similarity for KL is defined as Sim(p, q) = C − KL(p||q), where C is a free parameter to be tuned. 4 Experimental Setup 4.1 Materials Following Chen et al. (2005), we derive our relation discovery data from the automatic content extraction (ACE) 2004 and 2005 materials for evaluation of information extr</context>
</contexts>
<marker>Dagan, Lee, Pereira, 1997</marker>
<rawString>Ido Dagan, Lillian Lee, and Fernando Pereira. 1997. Similarity-based methods for word sense disambiguation. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janez Demˇsar</author>
</authors>
<title>Statistical comparisons of classifiers over multiple data sets.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>7</volume>
<marker>Demˇsar, 2006</marker>
<rawString>Janez Demˇsar. 2006. Statistical comparisons of classifiers over multiple data sets. Journal of Machine Learning Research, 7:1–30, Jan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Filatova</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Event-based extractive summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL-2004 Text Summarization Branches Out Workshop,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="3632" citStr="Filatova and Hatzivassiloglou, 2004" startWordPosition="546" endWordPosition="549">ctions. The networks could provide an alternative to searching when interacting with a document collection. This could prove beneficial, for example, in investigative journalism. It might also be used for social science research using techniques from social network analysis (Marsden and Lin, 1982). In previ25 Proceedings of the Workshop on Linguistic Distances, pages 25–34, Sydney, July 2006. c�2006 Association for Computational Linguistics ous work, relations have been used for automatic text summarisation as a conceptual representation of sentence content in a sentence extraction framework (Filatova and Hatzivassiloglou, 2004). In the next section, we motivate and introduce the relation discovery task, which addresses some of the shortcomings of conventional approaches to relation extraction (i.e. supervised learning or rule engineering) by applying minimally supervised methods.1 A critical part of the relation discovery task is grouping entity pairs by their relation type. This is a clustering task and requires a robust conceptual representation of relation semantics and a measure of similarity between relations. In previous work (Hasegawa et al., 2004; Chen et al., 2005), the conceptual representation has been li</context>
</contexts>
<marker>Filatova, Hatzivassiloglou, 2004</marker>
<rawString>Elena Filatova and Vasileios Hatzivassiloglou. 2004. Event-based extractive summarization. In Proceedings of the ACL-2004 Text Summarization Branches Out Workshop, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Milton Friedman</author>
</authors>
<title>A comparison of alternative tests of significance for the problem of m rankings. The Annals ofMathematical Statistics,</title>
<date>1940</date>
<pages>11--86</pages>
<contexts>
<context position="26355" citStr="Friedman, 1940" startWordPosition="4219" endWordPosition="4220">e data subsets.11 We also include the average ranks, which show that the LDA system using KL divergence performed best. Initial inspection of the table shows that all systems that use the term co-occurrence semantic space outperform the baseline system that uses the term-by-document semantic space. To test for statistical significance, we use non-parametric tests proposed by Demˇsar (2006) for comparing classifiers across multiple data sets. The use of nonparametric tests is safer here as they do not assume normality and outliers have less effect. The first test we perform is a Friedman test (Friedman, 1940), a multiple comparisons technique which is the non-parametric equivalent of the repeatedmeasures ANOVA. The null hypothesis is that all models perform the same and observed differences are random. With a Friedman statistic (X2F ) of 21.238, we reject the null hypothesis at p &lt; 0.01. The first question we wanted to address is whether term co-occurrence models outperform the term-by-document representation of relation semantics. To address this question, we continue with post-hoc analysis. The objective here is to 11Averages over data sets are unreliable where it is not clear whether the domain</context>
</contexts>
<marker>Friedman, 1940</marker>
<rawString>Milton Friedman. 1940. A comparison of alternative tests of significance for the problem of m rankings. The Annals ofMathematical Statistics, 11:86–92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<pages>101--5228</pages>
<contexts>
<context position="20053" citStr="Griffiths and Steyvers (2004)" startWordPosition="3247" endWordPosition="3250">D was found to perform best with the number of dimensions set to 10. For LDA, dimensionality interacts with the divergence-to-similarity conversion so they were tuned jointly. The optimal configuration varies by the divergence measure with D = 50 and C = 14 for KL divergence, D = 200 and C = 4 for symmetrised KL, and D = 150 and C = 2 for JS divergence. For all divergence measures, Lee’s (1999) method outperformed Dagan et al.’s (1997) method. Also for all divergence measures, the model hyper-parameter Q was found to be optimal at 0.0001. The α hyper-parameter was always set to 50/T following Griffiths and Steyvers (2004). Clustering is performed with the CLUTO software10 and the technique used is identical across models. Agglomerative clustering is used for comparability with the original relation discovery work of Hasegawa et al. (2004). This choice was motivated because as it is not known in advance how many clusters there should be in a new domain. One way to view the clustering problem is as an optimisation process where an optimal clustering is chosen with respect to a criterion function over the entire solution. The criterion function used here was chosen based on performance on the development data. We</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences, 101:5228–5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takaaki Hasegawa</author>
<author>Satoshi Sekine</author>
<author>Ralph Grishman</author>
</authors>
<title>Discovering relations among named entities from large corpora.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of Association of Computational Linguistics.</booktitle>
<contexts>
<context position="4169" citStr="Hasegawa et al., 2004" startWordPosition="628" endWordPosition="631">nce content in a sentence extraction framework (Filatova and Hatzivassiloglou, 2004). In the next section, we motivate and introduce the relation discovery task, which addresses some of the shortcomings of conventional approaches to relation extraction (i.e. supervised learning or rule engineering) by applying minimally supervised methods.1 A critical part of the relation discovery task is grouping entity pairs by their relation type. This is a clustering task and requires a robust conceptual representation of relation semantics and a measure of similarity between relations. In previous work (Hasegawa et al., 2004; Chen et al., 2005), the conceptual representation has been limited to term-by-document (TxD) models of relation semantics. The current work introduces a term co-occurrence (TxT) representation for the relation discovery task and shows that it performs significantly better than the TxD representation. We also explore dimensionality reduction techniques, which show a further improvement. Section 3 presents a parameterisation of similarity models for relation discovery. For the purposes of the current work, this consists of the semantic representation for terms (i.e. how a term’s context is mod</context>
<context position="6898" citStr="Hasegawa et al., 2004" startWordPosition="1042" endWordPosition="1045"> data will deviate more and more from the trained models. Additionally, there are cost problems associated with the conventional supervised approach when updating templates or transferring to a new domain, both of which require substantial effort in re-engineering rules or re-annotating training data. The goal of the relation discovery task is to identify the existence of associations between entities, to identify the kinds of relations that occur in a corpus and to annotate particular associations with relation types. These goals correspond to the three main steps in a generalised algorithm (Hasegawa et al., 2004): 1. Identify co-occurring pairs of named entities 2. Group entity pairs using the textual context 3. Label each cluster of entity pairs The first step is the relation identification task. In the current work, this is assumed to have been done already. We use the gold standard relations in the ACE data in order to isolate the performance of the second step. The second step is a clustering task and as such it is necessary to compute similarity between the co-occurring pairs of named entities (relations). In order to do this, a model of relation similarity is required, which is the focus of the </context>
<context position="8720" citStr="Hasegawa et al., 2004" startWordPosition="1345" endWordPosition="1348">ected labels are also able to differentiate between clusters. 26 parameters including the term context representation, whether or not we apply dimensionality reduction, and what similarity measure we use. 3.1 Term Context Representing texts in such a way that they can be compared is a familiar problem from the fields of information retrieval (IR), text mining (TM), textual data analysis (TDA) and natural language processing (NLP) (Lebart and Rajman, 2000). The traditional model for IR and TM is based on a term-by-document (TxD) vector representation. Previous approaches to relation discovery (Hasegawa et al., 2004; Chen et al., 2005) have been limited to TxD representations, using tf*idf weighting and the cosine similarity measure. In information retrieval, the weighted term representation works well as the comparison is generally between pieces of text with large context vectors. In the relation discovery task, though, the term contexts (as we will define them in Section 4) can be very small, often consisting of only one or two words. This means that a term-based similarity matrix between entity pairs is very sparse, which may pose problems for performing reliable clustering. An alternative method wid</context>
<context position="10073" citStr="Hasegawa et al. (2004)" startWordPosition="1566" endWordPosition="1569"> it occurs. This term cooccurrence (TxT) model is based on the intuition that two words are semantically similar if they appear in a similar set of contexts (see e.g. Pado and Lapata (2003)). The current work explores such a term co-occurrence (TxT) representation based on the hypothesis that it will provide a more robust representation of relation contexts and help overcome the sparsity problems associated with weighted term representations in the relation discovery task. This is compared to a baseline term-by-document (TxD) representation which is a re-implementation of the approach used by Hasegawa et al. (2004) and Chen et al. (2005). 3.2 Dimensionality Reduction Dimensionality reduction techniques for document and corpus modelling aim to reduce description length and model a type of semantic similarity that is more linguistic in nature (e.g., see Landauer et al.’s (1998) discussion of LSA and synonym tests). In the current work, we explore singular value decomposition (Berry et al., 1994), a technique from linear algebra that has been applied to a number of tasks from NLP and cognitive modelling. We also explore latent Dirichlet allocation, a probabilistic technique analogous to singular value deco</context>
<context position="16388" citStr="Hasegawa et al. (2004)" startWordPosition="2633" endWordPosition="2636">p||q), where Q is a free parameter, which is tuned on the development set (as described in Section 4.2). The same procedure is applied for symmetric KL divergence and JS divergence. The second approach is from Lee (1999). Here similarity for KL is defined as Sim(p, q) = C − KL(p||q), where C is a free parameter to be tuned. 4 Experimental Setup 4.1 Materials Following Chen et al. (2005), we derive our relation discovery data from the automatic content extraction (ACE) 2004 and 2005 materials for evaluation of information extraction.4 This is preferable to using the New York Times data used by Hasegawa et al. (2004) as it has gold standard annotation, which can be used for unbiased evaluation. The relation clustering data is based on the gold standard relations in the information extraction 4http://www.nist.gov/speech/tests/ace/ R f T a q z f Nd W pi log pi qi 28 data. We only consider data from newswire or broadcast news sources. We constructed six data subsets from the ACE corpus based on four of the ACE entities: persons (PER), organisations (ORG), geographical/social/political entities (GPE) and facilities (FAC). The six data subsets were chosen during development based on a lower limit of 50 for the</context>
<context position="20274" citStr="Hasegawa et al. (2004)" startWordPosition="3280" endWordPosition="3283">easure with D = 50 and C = 14 for KL divergence, D = 200 and C = 4 for symmetrised KL, and D = 150 and C = 2 for JS divergence. For all divergence measures, Lee’s (1999) method outperformed Dagan et al.’s (1997) method. Also for all divergence measures, the model hyper-parameter Q was found to be optimal at 0.0001. The α hyper-parameter was always set to 50/T following Griffiths and Steyvers (2004). Clustering is performed with the CLUTO software10 and the technique used is identical across models. Agglomerative clustering is used for comparability with the original relation discovery work of Hasegawa et al. (2004). This choice was motivated because as it is not known in advance how many clusters there should be in a new domain. One way to view the clustering problem is as an optimisation process where an optimal clustering is chosen with respect to a criterion function over the entire solution. The criterion function used here was chosen based on performance on the development data. We compared a number of criterion functions including single link, complete link, group average, I1, I2, £1 and H1. I1 is a criterion function that maximises sum of pairwise similarities between relation instances assigned </context>
<context position="32729" citStr="Hasegawa et al., 2004" startWordPosition="5235" endWordPosition="5238">em does poorly on domains that have both a high type-totoken ratio and a high entropy (uniform relation type distribution), while it performs very well on domains that have low TTR or low entropy. 7 Conclusions and Future Work This paper presented work on the relation discovery task. We tested several systems for the clustering subtask that use different models of the conceptual/semantic similarity of relations. These models included a baseline system based on a term-by-document representation of term context, which is equivalent to the representation used in previous work by Hasegawa et al. (Hasegawa et al., 2004) and Chen et al. (Chen et al., 2005). We hypothesised that this representation suffers from a sparsity problem and showed that models that use a term co-occurrence representation perform significantly better. Furthermore, we investigated the use of singular value decomposition and latent Dirichlet allocation for dimensionality reduction. It has been suggested that representations using these techniques are able to model a similarity that is less reliant on 32 specific word forms and therefore more semantic in nature. Our experiments showed an improvement over a term co-occurrence baseline when</context>
</contexts>
<marker>Hasegawa, Sekine, Grishman, 2004</marker>
<rawString>Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman. 2004. Discovering relations among named entities from large corpora. In Proceedings of the 42nd Annual Meeting of Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynette Hirschman</author>
<author>Alexander Yeh</author>
<author>Christian Blaschke</author>
<author>Alfonso Valencia</author>
</authors>
<title>Overview of BioCreAtIvE: Critical assessment of information extraction for biology.</title>
<date>2004</date>
<booktitle>In Proceedings of Critical Assessment of Information Extraction Systems in Biology Workshop (BioCreAtIvE),</booktitle>
<location>Granada,</location>
<contexts>
<context position="2517" citStr="Hirschman et al. (2004)" startWordPosition="381" endWordPosition="384">e’s stepping aside in the best interest of the company, but she will stay on the board of directors. Figure 1: Example sentences from ACE 2005. Sent Entity1 Entity2 Relation 1 Lebron James Nike Employ 2 Stig Toefting Bolton Sports-Aff 2 Stig Toefting Hamburg Sports-Aff 3 Kiichiro Toyoda Toyota Corp Founder 4 Martha Stewart board Business Figure 2: Example entity pairs and relation types. business newswire, which could be inserted into a corporate intelligence database. In the biomedical domain, we may want to identify relationships between genes and proteins from biomedical publications, e.g. Hirschman et al. (2004), to help scientists keep up-to-date on the literature. Or, we may want to identify disease and treatment relations in publications and textbooks, which can be used to help formalise medical knowledge and assist general practitioners in diagnosis, treatment and prognosis (Rosario and Hearst, 2004). Another application scenario involves building networks of relationships from text collections that indicate the important entities in a domain and can be used to visualise interactions. The networks could provide an alternative to searching when interacting with a document collection. This could pr</context>
</contexts>
<marker>Hirschman, Yeh, Blaschke, Valencia, 2004</marker>
<rawString>Lynette Hirschman, Alexander Yeh, Christian Blaschke, and Alfonso Valencia. 2004. Overview of BioCreAtIvE: Critical assessment of information extraction for biology. In Proceedings of Critical Assessment of Information Extraction Systems in Biology Workshop (BioCreAtIvE), Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Hofmann</author>
</authors>
<title>Unsupervised learning by probabilistic latent semantic analysis.</title>
<date>2001</date>
<booktitle>Machine Learning,</booktitle>
<pages>42--177</pages>
<contexts>
<context position="12048" citStr="Hofmann, 2001" startWordPosition="1896" endWordPosition="1897">mns gives the best least square approximation of the original matrix X by a matrix of rank D, i.e. a reduction of the original matrix to D dimensions. SVD can equally be applied to the word co-occurrence matrices obtained in the TxT representation presented in Section 2, in which case we can think of the original matrix as being a term x co-occurring term feature matrix. While SVD has proved successful and has been adapted for tasks such as word sense discrimination (Sch¨utze, 1998), its behaviour is not easy to interpret. Probabilistic LSA (pLSA) is a generative probabilistic version of LSA (Hofmann, 2001). This models each word in a document as a sample from a mixture model, but does not provide a probabilistic model at the document level. Latent Dirichlet Allocation (LDA) addresses this by representing documents as random mixtures over latent topics (Blei et al., 2003). Besides having a clear probabilistic interpretation, an additional advantage of these models is that they have intuitive graphical representations. Figure 3 contains a graphical representation of the LDA model as applied to TxT word co-occurrence matrices in standard plate notation. This models the word features f in the co-oc</context>
</contexts>
<marker>Hofmann, 2001</marker>
<rawString>Thomas Hofmann. 2001. Unsupervised learning by probabilistic latent semantic analysis. Machine Learning, 42:177–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Peter W Foltz</author>
<author>Darrell Laham</author>
</authors>
<title>An introduction to latent semantic analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<pages>25--259</pages>
<contexts>
<context position="10911" citStr="Landauer et al., 1998" startWordPosition="1702" endWordPosition="1705">stic in nature (e.g., see Landauer et al.’s (1998) discussion of LSA and synonym tests). In the current work, we explore singular value decomposition (Berry et al., 1994), a technique from linear algebra that has been applied to a number of tasks from NLP and cognitive modelling. We also explore latent Dirichlet allocation, a probabilistic technique analogous to singular value decomposition whose contribution to NLP has not been as thoroughly explored. Singular value decomposition (SVD) has been used extensively for the analysis of lexical semantics under the name of latent semantic analysis (Landauer et al., 1998). Here, a rectangular matrix is decomposed into the product of three matrices (Xwxp = WwxnSnxn(Ppxn)T ) with n ’latent semantic’ dimensions. The resulting decomposition can be viewed as a rotation of the n-dimensional axes such that the first axis runs along the direction of largest variation among the documents (Manning and Sch¨utze, 1999). W and P represent terms and documents in the new space. And S is a diagonal matrix of singular values in decreasing order. Taking the product WwxkSkxk(Ppxk)T over the first D columns gives the best least square approximation of the original matrix X by a m</context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>Thomas K. Landauer, Peter W. Foltz, and Darrell Laham. 1998. An introduction to latent semantic analysis. Discourse Processes, 25:259–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Buornar Larsen</author>
<author>Chinatsu Aone</author>
</authors>
<title>Fast and effective text mining using linear-time document clustering.</title>
<date>1999</date>
<booktitle>In Proceedings of the 5th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<location>San Diego, CA, USA.</location>
<contexts>
<context position="24130" citStr="Larsen and Aone, 1999" startWordPosition="3847" endWordPosition="3850">pe pairs, i.e., organisation-geopolitical entity, organisationorganisation, person-facility, person-geopolitical entity, person-organisation, person-person) and evaluated following suggestions by Demˇsar (2006) for statistical comparison of classifiers over multiple data sets. The dependent variable is the clustering performance as measured by the F-score. F-score accounts for both the amount of predictions made that are true (Precision) and the amount of true classes that are predicted (Recall). We use the CLUTO implementation of this measure for evaluating hierarchical clustering. Based on (Larsen and Aone, 1999), this is a balanced F-score (F = 2�P �+P ) that computes the maximum per-class score over all possible alignments of gold standard classes with nodes in the hierarchical tree. The average F-score for the entire hierarchical tree is a micro-average over the class-specific scores weighted according to the relative size of the class. 5.2 Results Table 3 contains F-score performance on the test set (ACE 2005). The columns contain results from the different system configurations. The column labels in the top row indicate the different representations of relation similarity. The column labels in th</context>
</contexts>
<marker>Larsen, Aone, 1999</marker>
<rawString>Buornar Larsen and Chinatsu Aone. 1999. Fast and effective text mining using linear-time document clustering. In Proceedings of the 5th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Diego, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ludovic Lebart</author>
<author>Martin Rajman</author>
</authors>
<title>Computing similarity.</title>
<date>2000</date>
<booktitle>Handbook of Natural Language Processing,</booktitle>
<pages>477--505</pages>
<editor>In Robert Dale, Hermann Moisl, and Harold Somers, editors,</editor>
<publisher>Marcel Dekker,</publisher>
<location>New York.</location>
<contexts>
<context position="8558" citStr="Lebart and Rajman, 2000" startWordPosition="1320" endWordPosition="1323">on of context words for a relation cluster (Hasegawa et al., 2004; Zhang et al., 2005). Chen et al. (2005) use discriminative category matching to make sure that selected labels are also able to differentiate between clusters. 26 parameters including the term context representation, whether or not we apply dimensionality reduction, and what similarity measure we use. 3.1 Term Context Representing texts in such a way that they can be compared is a familiar problem from the fields of information retrieval (IR), text mining (TM), textual data analysis (TDA) and natural language processing (NLP) (Lebart and Rajman, 2000). The traditional model for IR and TM is based on a term-by-document (TxD) vector representation. Previous approaches to relation discovery (Hasegawa et al., 2004; Chen et al., 2005) have been limited to TxD representations, using tf*idf weighting and the cosine similarity measure. In information retrieval, the weighted term representation works well as the comparison is generally between pieces of text with large context vectors. In the relation discovery task, though, the term contexts (as we will define them in Section 4) can be very small, often consisting of only one or two words. This me</context>
</contexts>
<marker>Lebart, Rajman, 2000</marker>
<rawString>Ludovic Lebart and Martin Rajman. 2000. Computing similarity. In Robert Dale, Hermann Moisl, and Harold Somers, editors, Handbook of Natural Language Processing, pages 477–505. Marcel Dekker, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lillian Lee</author>
</authors>
<title>Measures of distributional similarity.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>College Park, MD, USA.</location>
<contexts>
<context position="13733" citStr="Lee (1999)" startWordPosition="2180" endWordPosition="2181">e the average topic distribution over all context words to represent the conceptual content of an entity pair context. 3.3 Measuring Similarity Cosine (Cos) is commonly used in the literature to compute similarities between tf*idf vectors: Cos(p,q) = V E p2V E q2 �i piqi In the current work, we use cosine over term and SVD representations of entity pair context. However, it is not clear which similarity measure should be used for the probabilistic topic models. Dagan et al. (1997) find that the symmetric information radius measure performs best on a pseudoword sense disambiguation task, while Lee (1999) find that the asymmetric skew divergence – a generalisation of Kullback-Leibler divergence – performs best for improving probability estimates for unseen word co-occurrences. In the current work, we compare KL divergence with two methods for deriving a symmetric mea3The hyperparameters α and Q are Dirichlet priors on the multinomial distributions for word features (0 — Dir(Q)) and topics (0 — Dir(α)). The choice of the Dirichlet is explained by its conjugacy to the multinomial distribution, meaning that if the parameter (e.g. 0, 0) for a multinomial distribution is endowed with a Dirichlet pr</context>
<context position="15986" citStr="Lee (1999)" startWordPosition="2561" endWordPosition="2562">es are dissimilarity measures as they calculate the difference between two distributions. However, they can be converted to increasing measures of similarity through various transformations. We treated this as a parameter to be tuned during development and considered two approaches. The first is from Dagan et al. (1997). For KL divergence, this function is defined as Sim(p, q) = 10−,3KL(p||q), where Q is a free parameter, which is tuned on the development set (as described in Section 4.2). The same procedure is applied for symmetric KL divergence and JS divergence. The second approach is from Lee (1999). Here similarity for KL is defined as Sim(p, q) = C − KL(p||q), where C is a free parameter to be tuned. 4 Experimental Setup 4.1 Materials Following Chen et al. (2005), we derive our relation discovery data from the automatic content extraction (ACE) 2004 and 2005 materials for evaluation of information extraction.4 This is preferable to using the New York Times data used by Hasegawa et al. (2004) as it has gold standard annotation, which can be used for unbiased evaluation. The relation clustering data is based on the gold standard relations in the information extraction 4http://www.nist.go</context>
</contexts>
<marker>Lee, 1999</marker>
<rawString>Lillian Lee. 1999. Measures of distributional similarity. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, College Park, MD, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<publisher>MIT Press.</publisher>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Christopher D. Manning and Hinrich Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter V Marsden</author>
<author>Nan Lin</author>
<author>editors</author>
</authors>
<title>Social Structure and Network Analysis.</title>
<date>1982</date>
<location>Sage, Beverly Hills.</location>
<marker>Marsden, Lin, editors, 1982</marker>
<rawString>Peter V. Marsden and Nan Lin, editors. 1982. Social Structure and Network Analysis. Sage, Beverly Hills.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pado</author>
<author>Mirella Lapata</author>
</authors>
<title>Constructing semantic space models from parsed corpora.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="9640" citStr="Pado and Lapata (2003)" startWordPosition="1500" endWordPosition="1503"> task, though, the term contexts (as we will define them in Section 4) can be very small, often consisting of only one or two words. This means that a term-based similarity matrix between entity pairs is very sparse, which may pose problems for performing reliable clustering. An alternative method widely used in NLP and cognitive science is to represent a term context by its neighbouring words as opposed to the documents in which it occurs. This term cooccurrence (TxT) model is based on the intuition that two words are semantically similar if they appear in a similar set of contexts (see e.g. Pado and Lapata (2003)). The current work explores such a term co-occurrence (TxT) representation based on the hypothesis that it will provide a more robust representation of relation contexts and help overcome the sparsity problems associated with weighted term representations in the relation discovery task. This is compared to a baseline term-by-document (TxD) representation which is a re-implementation of the approach used by Hasegawa et al. (2004) and Chen et al. (2005). 3.2 Dimensionality Reduction Dimensionality reduction techniques for document and corpus modelling aim to reduce description length and model </context>
</contexts>
<marker>Pado, Lapata, 2003</marker>
<rawString>Sebastian Pado and Mirella Lapata. 2003. Constructing semantic space models from parsed corpora. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin F Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="19256" citStr="Porter, 1980" startWordPosition="3109" endWordPosition="3110">tic topic models as we use for processing the intervening context of entity pairs. This consisted of MxTerminator (Reynar and Ratnaparkhi., 1997) for sentence boundary detection, the Penn Treebank 5Following results reported by Chen et al. (2005), who tried unsuccessfully to incorporate words from the surrounding context to represent a relation’s semantics, we use only intervening words. 6http://infomap.stanford.edu/ 7http://psiexp.ss.uci.edu/research/ programs_data/toolbox.htm sed script8 for tokenisation, and the Infomap stop word list. We also use an implementation of the Porter algorithm (Porter, 1980) for stemming.9 4.2 Model Selection We used the ACE 2004 relation data to perform model selection. Firstly, dimensionality (D) needs to be optimised for SVD and LDA. SVD was found to perform best with the number of dimensions set to 10. For LDA, dimensionality interacts with the divergence-to-similarity conversion so they were tuned jointly. The optimal configuration varies by the divergence measure with D = 50 and C = 14 for KL divergence, D = 200 and C = 4 for symmetrised KL, and D = 150 and C = 2 for JS divergence. For all divergence measures, Lee’s (1999) method outperformed Dagan et al.’s</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Martin F. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey C Reynar</author>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy approach to identifying sentence boundaries.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th Conference on Applied Natural Language Processing,</booktitle>
<location>Washington, D.C., USA.</location>
<marker>Reynar, Ratnaparkhi, 1997</marker>
<rawString>Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A maximum entropy approach to identifying sentence boundaries. In Proceedings of the 5th Conference on Applied Natural Language Processing, Washington, D.C., USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Rosario</author>
<author>Marti Hearst</author>
</authors>
<title>Classifying semantic relations in bioscience text.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="2815" citStr="Rosario and Hearst, 2004" startWordPosition="428" endWordPosition="431">orp Founder 4 Martha Stewart board Business Figure 2: Example entity pairs and relation types. business newswire, which could be inserted into a corporate intelligence database. In the biomedical domain, we may want to identify relationships between genes and proteins from biomedical publications, e.g. Hirschman et al. (2004), to help scientists keep up-to-date on the literature. Or, we may want to identify disease and treatment relations in publications and textbooks, which can be used to help formalise medical knowledge and assist general practitioners in diagnosis, treatment and prognosis (Rosario and Hearst, 2004). Another application scenario involves building networks of relationships from text collections that indicate the important entities in a domain and can be used to visualise interactions. The networks could provide an alternative to searching when interacting with a document collection. This could prove beneficial, for example, in investigative journalism. It might also be used for social science research using techniques from social network analysis (Marsden and Lin, 1982). In previ25 Proceedings of the Workshop on Linguistic Distances, pages 25–34, Sydney, July 2006. c�2006 Association for </context>
</contexts>
<marker>Rosario, Hearst, 2004</marker>
<rawString>Barbara Rosario and Marti Hearst. 2004. Classifying semantic relations in bioscience text. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<pages>124</pages>
<marker>Sch¨utze, 1998</marker>
<rawString>Hinrich Sch¨utze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):91– 124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey I Webb</author>
</authors>
<title>Multiboosting: A technique for combining boosting and wagging.</title>
<date>2000</date>
<booktitle>Machine Learning,</booktitle>
<volume>40</volume>
<issue>2</issue>
<contexts>
<context position="26987" citStr="Webb, 2000" startWordPosition="4316" endWordPosition="4317">ons technique which is the non-parametric equivalent of the repeatedmeasures ANOVA. The null hypothesis is that all models perform the same and observed differences are random. With a Friedman statistic (X2F ) of 21.238, we reject the null hypothesis at p &lt; 0.01. The first question we wanted to address is whether term co-occurrence models outperform the term-by-document representation of relation semantics. To address this question, we continue with post-hoc analysis. The objective here is to 11Averages over data sets are unreliable where it is not clear whether the domains are commensurable (Webb, 2000). We present averages in our results but avoid drawing conclusions based on them. compare several conditions to a control (i.e., compare the term co-occurrence systems to the termby-document baseline) so we use a BonferroniDunn test. At a significance level of p &lt; 0.05, the critical difference for the Bonferroni-Dunn test for comparing 6 systems across 6 data sets is 2.782. We conclude that the unreduced term cooccurrence system and the LDA systems with KL and JS divergence all perform significantly better than baseline, while the SVD system and the LDA system with symmetrised KL divergence do</context>
</contexts>
<marker>Webb, 2000</marker>
<rawString>Geoffrey I. Webb. 2000. Multiboosting: A technique for combining boosting and wagging. Machine Learning, 40(2):159–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Jian Su</author>
<author>Danmei Wang</author>
<author>Guodong Zhou</author>
<author>Chew Lim Tan</author>
</authors>
<title>Discovering relations from a large raw corpus using tree similarity-based clustering.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2nd International Joint Conference on Natural Language Processing.</booktitle>
<contexts>
<context position="8020" citStr="Zhang et al., 2005" startWordPosition="1236" endWordPosition="1239">ons). In order to do this, a model of relation similarity is required, which is the focus of the current work. We also assume that it is possible to perform the third step.2 The evaluation we present here looks just at the quality of the clustering and does not attempt to assess the labelling task. 3 Modelling Relation Similarity The possible space of models for relation similarity can be explored in a principled manner by parameterisation. In this section, we discuss several 2Previous approaches select labels from the collection of context words for a relation cluster (Hasegawa et al., 2004; Zhang et al., 2005). Chen et al. (2005) use discriminative category matching to make sure that selected labels are also able to differentiate between clusters. 26 parameters including the term context representation, whether or not we apply dimensionality reduction, and what similarity measure we use. 3.1 Term Context Representing texts in such a way that they can be compared is a familiar problem from the fields of information retrieval (IR), text mining (TM), textual data analysis (TDA) and natural language processing (NLP) (Lebart and Rajman, 2000). The traditional model for IR and TM is based on a term-by-do</context>
<context position="34180" citStr="Zhang et al. (2005)" startWordPosition="5467" endWordPosition="5470">orm significantly better than symmetrised KL divergence. Interestingly, the performance of the asymmetric KL divergence and the symmetric JS divergence is very close, which makes it difficult to conclude whether the relation discovery domain is a symmetric domain or an asymmetric domain like Lee’s (1999) task of improving probability estimates for unseen word co-occurrences. A shortcoming of all the models we will describe here is that they are derived from the basic bag-of-words models and as such do not account for word order or other notions of syntax. Related work on relation discovery by Zhang et al. (2005) addresses this shortcoming by using tree kernels to compute similarity between entity pairs. In future work we will extend our experiment to explore the use of syntactic and semantic features following the frame work of Pado and Lapata (2003). We are also planning to look at non-parametric versions of LDA that address the model order selection problem and perform an extrinsic evaluation of the relation discovery task. Acknowledgements This work was supported by Scottish Enterprise Edinburgh-Stanford Link grant R37588 as part of the EASIE project. I would like to thank Claire Grover, Mirella L</context>
</contexts>
<marker>Zhang, Su, Wang, Zhou, Tan, 2005</marker>
<rawString>Min Zhang, Jian Su, Danmei Wang, Guodong Zhou, and Chew Lim Tan. 2005. Discovering relations from a large raw corpus using tree similarity-based clustering. In Proceedings of the 2nd International Joint Conference on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Zhao</author>
<author>George Karypis</author>
</authors>
<title>Empirical and theoretical comparisons of selected criterion functions for document clustering.</title>
<date>2004</date>
<booktitle>Machine Learning,</booktitle>
<pages>55--311</pages>
<contexts>
<context position="22716" citStr="Zhao and Karypis, 2004" startWordPosition="3650" endWordPosition="3653"> Citizen-or-Resdent resident Other gpeaffothr PER/ORG AFFIL’N Ethnic ethnic Ideology ideology Other perorgothr PERSONAL-SOC’L Business business Family family Other persocothr PHYSICAL Located located Near near Table 2: Overview of ACE relations with abbreviations used here. entire collection, and x1 is a combined criterion function that consists of the ration of I1 over 91. The I2, x1 and x2 criterion functions outperformed single link, complete link and group average on the development data. We use I2, which performed as well as x1 and x2 and is superior in terms of computational complexity (Zhao and Karypis, 2004). 5 Experiment 5.1 Method This section describes experimental setup, which uses relation extraction data from ACE 2005 to answer four questions concerning the effectiveness of similarity models based on term co-occurrence and dimensionality reduction for the relation discovery task: 1. Do term co-occurrence models provide a better representation of relation semantics than standard term-by-document vector space? 2. Do textual dimensionality reduction techniques provide any further improvements? 3. How do probabilistic topic models perform with respect to SVD on the relation discovery task? 4. D</context>
</contexts>
<marker>Zhao, Karypis, 2004</marker>
<rawString>Ying Zhao and George Karypis. 2004. Empirical and theoretical comparisons of selected criterion functions for document clustering. Machine Learning, 55:311–331.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>