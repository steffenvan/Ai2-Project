<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000108">
<title confidence="0.9991815">
Logistic Online Learning Methods and Their Application to
Incremental Dependency Parsing
</title>
<author confidence="0.98891">
Richard Johansson
</author>
<affiliation confidence="0.977402">
Department of Computer Science
Lund University
</affiliation>
<address confidence="0.805844">
Lund, Sweden
</address>
<email confidence="0.997473">
richard@cs.lth.se
</email>
<sectionHeader confidence="0.997371" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998544">
We investigate a family of update methods
for online machine learning algorithms for
cost-sensitive multiclass and structured clas-
sification problems. The update rules are
based on multinomial logistic models. The
most interesting question for such an ap-
proach is how to integrate the cost function
into the learning paradigm. We propose a
number of solutions to this problem.
To demonstrate the applicability of the al-
gorithms, we evaluated them on a number
of classification tasks related to incremental
dependency parsing. These tasks were con-
ventional multiclass classification, hiearchi-
cal classification, and a structured classifica-
tion task: complete labeled dependency tree
prediction. The performance figures of the
logistic algorithms range from slightly lower
to slightly higher than margin-based online
algorithms.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999641469387755">
Natural language consists of complex structures,
such as sequences of phonemes, parse trees, and dis-
course or temporal graphs. Researchers in NLP have
started to realize that this complexity should be re-
flected in their statistical models. This intuition has
spurred a growing interest of related research in the
machine learning community, which in turn has led
to improved results in a wide range of applications
in NLP, including sequence labeling (Lafferty et al.,
2001; Taskar et al., 2006), constituent and depen-
dency parsing (Collins and Duffy, 2002; McDon-
ald et al., 2005), and logical form extraction (Zettle-
moyer and Collins, 2005).
Machine learning research for structured prob-
lems have generally used margin-based formula-
tions. These include global batch methods such as
Max-margin Markov Networks (M3N) (Taskar et al.,
2006) and SVMstruct (Tsochantaridis et al., 2005)
as well as online methods such as Margin Infused
Relaxed Algorithm (MIRA) (Crammer and Singer,
2003) and the Online Passive-Aggressive Algorithm
(OPA) (Crammer et al., 2006). Although the batch
methods are formulated very elegantly, they do not
seem to scale well to the large training sets prevalent
in NLP contexts. The online methods on the other
hand, although less theoretically appealing, can han-
dle realistically sized data sets.
In this work, we investigate whether logistic
online learning performs as well as margin-based
methods. Logistic models are easily extended to us-
ing kernels; that this is theoretically well-justified
was shown by Zhu and Hastie (2005), who also
made an elegant argument that margin-based meth-
ods are in fact related to regularized logistic models.
For batch learning, there exist several learning algo-
rithms in a logistic framework for conventional mul-
ticlass classification but few for structured problems.
Prediction of complex structures is conventionally
treated as a cost-sensitive multiclass classification
problem, although special care has to be taken to
handle the large space of possible outputs. The in-
tegration of the cost function into the logistic frame-
work leads to two distinct (although related) update
methods: the Scaled Prior Variance (SPV) and the
Minimum Expected Cost (MEC) updates.
Apart from its use in structured prediction, cost-
sensitive classification is useful for hierachical clas-
sification, which we briefly consider here in an ex-
periment. This type of classification has useful ap-
</bodyText>
<page confidence="0.99487">
49
</page>
<bodyText confidence="0.941824727272727">
Proceedings of the ACL 2007 Student Research Workshop, pages 49–54,
Prague, June 2007. c�2007 Association for Computational Linguistics
plications in NLP. Apart from the obvious use in
classification of concepts in an ontology, it is also
useful for prediction of complex morphological or
named-entity tags. Cost-sensitive learning is also
required in the SEARN algorithm (Daumé III et al.,
2006), which is a method to decompose the predic-
tion problem of a complex structure into a sequence
of actions, and train the search in the space of action
sequences to maximize global performance.
</bodyText>
<sectionHeader confidence="0.989936" genericHeader="introduction">
2 Algorithm
</sectionHeader>
<bodyText confidence="0.999913833333333">
We model the learning problem as finding a discrim-
inant function F that assigns a score to each possible
output y given an input x. Classification in this set-
ting is done by finding the y� that maximizes F(x, y).
In this work, we consider linear discriminants of the
following form:
</bodyText>
<equation confidence="0.936988">
F(x, y) = (w, *(x, y))
</equation>
<bodyText confidence="0.9427061">
Here, *(x, y) is a numeric feature representation of
the pair (x, y) and w a vector of feature weights.
Learning in this case is equivalent to assigning ap-
propriate weights in the vector w.
In the online learning framework, the weight vec-
tor is constructed incrementally. Algorithm 1 shows
the general form of the algorithm. It proceeds a
number of times through the training set. In each
step, it computes an update to the weight vector
based on the current example. The resulting weight
vector tends to be overfit to the last few examples;
one way to reduce overfitting is to use the average
of all successive weight vectors as the result of the
training (Freund and Schapire, 1999).
Algorithm 1 General form of online algorithms
input Training set T = {(xt, yt)}Tt=1
Number of iterations N
for n in L.N
for (xt, yt) in T
Compute update vector Sw for (xt, yt)
www+Sw
return waverage
Following earlier online learning methods such as
the Perceptron, we assume that in each update step,
we adjust the weight vector by incrementally adding
feature vectors. For stability, we impose the con-
straint that the sum of the updates in each step should
be zero. We assume that the possible output values
are {yi}mi=0 and, for convenience, that y0 is the cor-
rect value. This leads to the following ansatz:
</bodyText>
<equation confidence="0.9884995">
Sw = �m αj(*(x, y0) − *(x, yj))
j=1
</equation>
<bodyText confidence="0.9999065">
Here, αj defines how much F is shifted to favor y0
instead of yj. This is also the approach (implicitly)
used by other algorithms such as MIRA and OPA.
The following two subsections present two ways
of creating the weight update Sw, differing in how
the cost function is integrated into the model. Both
are based on a multinomial logistic framework,
where we model the probability of the class y being
assigned to an input x using a “soft-max” function
as follows:
</bodyText>
<equation confidence="0.961351333333333">
eF(x,y)
�m 0 eF(x,yj )
j=
</equation>
<subsectionHeader confidence="0.964181">
2.1 Scaled Prior Variance Approach
</subsectionHeader>
<bodyText confidence="0.999824230769231">
The first update method, Scaled Prior Variance
(SPV), directly uses the probability of the correct
output. It uses a maximum a posteriori approach,
where the cost function is used by the prior.
Naïvely, the update could be done by maximizing
the likelihood with respect to α in each step. How-
ever, this would lead to overfitting – in the case of
separability, a maximum does not even exist. We
thus introduce a regularizing prior that penalizes
large values of α. We introduce variance-controlling
hyperparameters sj for each αj, and with a Gaussian
prior we obtain (disregarding constants) the follow-
ing log posterior:
</bodyText>
<equation confidence="0.968708666666667">
αj(K00 − Kj0) −
�m efk+Ej&amp;quot;_1 αj(K0k−Kjk)
k=0
</equation>
<bodyText confidence="0.99995">
where Kij = (*(x, yi), *(x, yj)) and fk =
F(x, yk) (i.e. the output before w is updated).
As usual, the feature vectors occur only in inner
products, allowing us to use kernels if appropriate.
</bodyText>
<equation confidence="0.994761">
P(y|x) =
L(α) = �m
j=1
− log
�m
j=1
sjα?j
</equation>
<page confidence="0.943439">
50
</page>
<bodyText confidence="0.962986192307692">
We could have used any prior; however, in prac-
tice we will require it to be log-concave to avoid
suboptimal local maxima. A Laplacian prior (i.e.
− Em j=1 sj|αj|) will also be considered in this work
– the discontinuity of its gradient at the origin seems
to pose no problem in practice.
Costs are incorporated into the model by as-
sociating them to the prior variances. We tried
two variants of variance scaling. In the first case,
we let the variance be directly proportional to the
cost (C-SPV):
-y
c(yj)
where -y is a tradeoff parameter controlling the rel-
ative weight of the prior with respect to the likeli-
hood. Intuitively, this model allows the algorithm
more freedom to adjust an αj associated with a yj
with a high cost.
In the second case, inspired by margin-based
learning we instead scaled the variance by the loss,
i.e. the scoring error plus the cost (L-SPV):
-y
sj _ max(0, fj − f0) + c(yj)
Here, the intuition is instead that the algorithm is
allowed more freedom for “dangerous” outputs that
are ranked high but have high costs.
</bodyText>
<subsectionHeader confidence="0.997107">
2.2 Minimum Expected Cost Approach
</subsectionHeader>
<bodyText confidence="0.9999386">
In the second approach to integrating the cost func-
tion, the Minimum Expected Cost (MEC) update,
the method seeks to minimize the expected cost in
each step. Once again using the soft-max probabil-
ity, we get the following expectation of the cost:
</bodyText>
<equation confidence="0.9877536">
E(c(y)|x) _ �m c(yk)P(yk|x)
k=0
Emk=0 c(yk)efk+Emj=1 αj(K0k−Kjk)
Em fk+Em 1 αj(K0k−Kjk)
k=0 e j=
</equation>
<bodyText confidence="0.999337583333333">
This quantity is easily minimized in the same way
as the SPV posterior was maximized, although
we had to add a constant 1 to the expectation to
avoid numerical instability. To avoid overfitting, we
added a quadratic regularizer -y Emj=1 α2j to log(1 +
E(c(y)|x)) just like the prior in the SPV method,
although this regularizer does not have an interpre-
tation as a prior.
The MEC update is closely related to SPV: for
cost-insensitive classification (i.e. the cost of every
misclassified instance is 1), the expectation is equal
to one minus the likelihood in the SPV model.
</bodyText>
<subsectionHeader confidence="0.999533">
2.3 Handling Complex Prediction Problems
</subsectionHeader>
<bodyText confidence="0.999885375">
The algorithm can thus be used for any cost-
sensitive classification problem. This class of prob-
lems includes prediction of complex structures such
as trees or graphs. However, for those problems the
set of possible outputs is typically very large. Two
broad categories of solutions to this problem have
been common in literature, both of which rely on
the structure of the domain:
</bodyText>
<listItem confidence="0.876007714285714">
• Subset selection: instead of working with the
complete range of outputs, only an “interest-
ing” subset is used, for instance by repeatedly
finding the most violated constraints (Tsochan-
taridis et al., 2005) or by using N-best search
(McDonald et al., 2005).
• Decomposition: the inherent structure of the
</listItem>
<bodyText confidence="0.967881636363636">
problem is used to factorize the optimiza-
tion problem. Examples include Markov de-
compositions in M3N (Taskar et al., 2006)
and dependency-based factorization for MIRA
(McDonald et al., 2005).
In principle, both methods could be used in our
framework. In this work, we use subset selec-
tion since it is easy to implement for many do-
mains (in the form of an N-best search) and al-
lows a looser coupling between the domain and the
learning algorithm.
</bodyText>
<subsectionHeader confidence="0.98861">
2.4 Implementation Issues
</subsectionHeader>
<bodyText confidence="0.954717583333333">
Since we typically work with only a few variables in
each iteration, maximizing the log posterior or mini-
mizing the expectation is easy (assuming, of course,
that we chose a log-concave prior). We used gra-
dient ascent and did not try to use more sophisti-
cated optimization procedures like BFGS or New-
ton’s method. Typically, only a few iterations were
needed to reach the optimum. The running time of
the update step is almost identical to that of MIRA,
which solves a small quadratic program in each step,
but longer than for the Perceptron algorithm or OPA.
sj _
</bodyText>
<page confidence="0.976208">
51
</page>
<sectionHeader confidence="0.627969" genericHeader="method">
Actions Parser actions Conditions
</sectionHeader>
<equation confidence="0.905567333333333">
Initialize (nil, W, ∅)
Terminate (S, nil, A)
Left-arc (n|S, n′|I, A) → (S, n′|I, A ∪ {(n′, n)}) ¬∃n′′(n′′, n) ∈ A
Right-arc (n|S, n′|I, A) → (n′|n|S, I, A ∪ {(n, n′)}) ¬∃n′′(n′′, n′) ∈ A
Reduce (n|S, I, A) → (S, I, A) ∃n′(n′, n) ∈ A
Shift (S, n|I, A) → (n|S, I, A)
</equation>
<tableCaption confidence="0.86855">
Table 1: Nivre’s parser transitions where W is the initial word list; I, the current input word list; A, the
</tableCaption>
<bodyText confidence="0.7861815">
graph of dependencies; and S, the stack. (n′, n) denotes a dependency relations between n′ and n, where n′
is the head and n the dependent.
</bodyText>
<sectionHeader confidence="0.999535" genericHeader="evaluation">
3 Experiments
</sectionHeader>
<bodyText confidence="0.9999498125">
To compare the logistic online algorithms against
other learning algorithms, we performed a set of ex-
periments in incremental dependency parsing using
the Nivre algorithm (Nivre, 2003).
The algorithm is a variant of the shift–reduce al-
gorithm and creates a projective and acyclic graph.
As with the regular shift–reduce, it uses a stack S
and a list of input words W, and builds the parse
tree incrementally using a set of parsing actions (see
Table 1). However, instead of finding constituents,
it builds a set of arcs representing the graph of de-
pendencies. It can be shown that every projective
dependency graph can be produced by a sequence
of parser actions, and that the worst-case number of
actions is linear with respect to the number of words
in the sentence.
</bodyText>
<subsectionHeader confidence="0.997097">
3.1 Multiclass Classification
</subsectionHeader>
<bodyText confidence="0.999218766666667">
In the first experiment, we trained multiclass clas-
sifiers to choose an action in a given parser state
(see (Nivre, 2003) for a description of the feature
set). We stress that this is true multiclass classifica-
tion rather than a decomposed method (such as one-
versus-all or pairwise binarization).
As a training set, we randomly selected 50,000
instances of state–action pairs generated for a
dependency-converted version of Penn Treebank.
This training set contained 22 types of actions (such
as SHIFT, REDUCE, LEFT-ARC(SUBJECT), and
RIGHT-ARC(OBJECT). The test set was also ran-
domly selected and contained 10,000 instances.
We trained classifiers using the logistic updates
(C-SPV, L-SPV, and MEC) with Gaussian and
Laplacian priors. Additionally, we trained OPA
and MIRA classifiers, as well as an Additive Ultra-
conservative (AU) classifier (Crammer and Singer,
2003), a variant of the Perceptron.
For all algorithms, we tried to find the best val-
ues of the respective regularization parameter using
cross-validation. All training algorithms iterated five
times through the training set and used an expanded
quadratic kernel.
Table 2 shows the classification error for all algo-
rithms. As can be seen, the performance was lower
for the logistic algorithms, although the difference
was slight. Both the logistic (MEC and SPV) and
the margin-based classifiers (OPA and MIRA) out-
performed the AU classifier.
</bodyText>
<table confidence="0.9991701">
Method Test error
MIRA 6.05%
OPA 6.17%
C-SPV, Laplace 6.20%
MEC, Laplace 6.21%
C-SPV, Gauss 6.22%
MEC, Gauss 6.23%
L-SPV, Laplace 6.25%
L-SPV, Gauss 6.26%
AU 6.39%
</table>
<tableCaption confidence="0.99283">
Table 2: Multiclass classification results.
</tableCaption>
<subsectionHeader confidence="0.99691">
3.2 Hierarchical Classification
</subsectionHeader>
<bodyText confidence="0.999757333333333">
In the second experiment, we used the same train-
ing and test set, but considered the selection of the
parsing action as a hierarchical classficiation task,
i.e. the predicted value has a main type (SHIFT,
REDUCE, LEFT-ARC, and RIGHT-ARC) and possi-
bly also a subtype (such as LEFT-ARC(SUBJECT) or
</bodyText>
<page confidence="0.996294">
52
</page>
<bodyText confidence="0.997627583333333">
RIGHT-ARC(OBJECT)).
To predict the class in this experiment, we used
the same feature function but a new cost function:
the cost of misclassification was 1 for an incorrect
parsing action, and 0.5 if the action was correct but
the arc label incorrect.
We used the same experimental setup as in the
multiclass experiment. Table 3 shows the average
cost on the test set for all algorithms. Here, the
MEC update outperformed the margin-based ones
by a negligible difference. We did not use AU in
this experiment since it does not optimize for cost.
</bodyText>
<table confidence="0.999637444444444">
Method Average cost
MEC, Gauss 0.0573
MEC, Laplace 0.0576
OPA 0.0577
C-SPV, Gauss 0.0582
C-SPV, Laplace 0.0587
MIRA 0.0590
L-SPV, Gauss 0.0590
L-SPV, Laplace 0.0632
</table>
<tableCaption confidence="0.996071">
Table 3: Hierarchical classification results.
</tableCaption>
<subsectionHeader confidence="0.999711">
3.3 Prediction of Complex Structures
</subsectionHeader>
<bodyText confidence="0.999887357142857">
Finally, we made an experiment in prediction of de-
pendency trees. We created a global model where
the discriminant function was trained to assign high
scores to the correct parse tree. A similar model was
previously used by McDonald et al. (2005), with the
difference that we here represent the parse tree as
a sequence of actions in the incremental algorithm
rather than using the dependency links directly.
For a sentence x and a parse tree y, we defined
the feature representation by finding the sequence
((S1, I1) , a1) , ((S2, I2), a2) ... of states and their
corresponding actions, and creating a feature vector
for each state/action pair. The discriminant function
was thus written
</bodyText>
<equation confidence="0.5979635">
�y), w) � (ψ((Si, Ii) , ai), w)
i
</equation>
<bodyText confidence="0.999732227272727">
where ψ is the feature function from the previous
two experiments, which assigns a feature vector to a
state (Si, Ii) and the action ai taken in that state.
The cost function was defined as the sum of link
costs, where the link cost was 0 for a correct depen-
dency link with a correct label, 0.5 for a correct link
with an incorrect label, and 1 for an incorrect link.
Since the history-based feature set used in the
parsing algorithm makes it impossible to use inde-
pendence to factorize the scoring function, an exact
search to find the best-scoring action sequence is not
possible. We used a beam search of width 2 in this
experiment.
We trained models on a 5000-word subset of the
Basque Treebank (Aduriz et al., 2003) and evalu-
ated them on a 8000-word subset of the same cor-
pus. As before, we used an expanded quadratic ker-
nel, and all algorithms iterated five times through the
training set.
Table 4 shows the results of this experiment. We
show labeled accuracy instead of cost for ease of in-
terpretation. Here, the loss-based SPV outperformed
</bodyText>
<table confidence="0.922302">
Method Labeled Accuracy
L-SPV, Gauss 66.24
MIRA 66.19
MEC, Gauss 65.99
C-SPV, Gauss 65.84
OPA 65.45
MEC, Laplace 64.81
C-SPV, Laplace 64.73
L-SPV, Laplace 64.50
</table>
<tableCaption confidence="0.998754">
Table 4: Results for dependency tree prediction.
</tableCaption>
<bodyText confidence="0.999959333333333">
MIRA, and two other logistic updates also outper-
formed OPA. The differences between the first four
scores are however not statistically significant. In-
terestingly, all updates with Laplacian prior resulted
in low performance. The reason for this may be that
Laplacian priors tend to promote sparse solutions
(see Krishnapuram et al. (2005), inter alia), and that
this sparsity is detrimental for this highly lexicalized
feature set.
</bodyText>
<sectionHeader confidence="0.998874" genericHeader="conclusions">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9996924">
This paper presented new update methods for online
machine learning algorithms. The update methods
are based on a multinomial logistic model. Their
performance is on par with other state-of-the-art on-
line learning algorithms for cost-sensitive problems.
</bodyText>
<page confidence="0.995605">
53
</page>
<bodyText confidence="0.999931628571429">
We investigated two main approaches to integrat-
ing the cost function into the logistic model. In the
first method, the cost was linked to the prior vari-
ances, while in the second method, the update rule
sets the weights to minimize the expected cost. We
tried a few different priors. Which update method
and which prior was the best varied between exper-
iments. For instance, the update where the prior
variances were scaled by the costs was the best-
performing in the multiclass experiment but the
worst-performing in the dependency tree prediction
experiment.
In the SPV update, the cost was incorporated into
the MAP model in a rather ad-hoc fashion. Al-
though this seems to work well, we would like to
investigate this further and possibly devise a cost-
based prior that is both theoretically well-grounded
and performs well in practice.
To achieve a good classification performance us-
ing the updates presented in this article, there is a
considerable need for cross-validation to find the
best value for the regularization parameter. This is
true for most other classification methods as well,
including SVM, MIRA, and OPA. There has been
some work on machine learning methods where this
parameter is tuned automatically (Tipping, 2001),
and a possible extension to our work could be to
adapt those models to the multinomial and cost-
sensitive setting.
We applied the learning models to three problems
in incremental dependency parsing, the last of which
being prediction of full labeled dependency trees.
Our system can be seen as a unification of the two
best-performing parsers presented at the CoNLL-X
Shared Task (Buchholz and Marsi, 2006).
</bodyText>
<sectionHeader confidence="0.999461" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.992318766666667">
Itzair Aduriz, Maria Jesus Aranzabe, Jose Mari Arriola,
Aitziber Atutxa, Arantza Diaz de Ilarraza, Aitzpea
Garmendia, and Maite Oronoz. 2003. Construction
of a Basque dependency treebank. In Proceedings of
the TLT, pages 201–204.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the CoNLL-X.
Michael Collins and Nigel Duffy. 2002. New ranking
algorithms for parsing and tagging: Kernels over dis-
crete structures, and the voted perceptron. In Proceed-
ings of the ACL.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. Jour-
nal of Machine Learning Research, 2003(3):951–991.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Schwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 2006(7):551–585.
Hal Daumé III, John Langford, and Daniel Marcu. 2006.
Search-based structured prediction. Submitted.
Yoav Freund and Robert E. Schapire. 1999. Large mar-
gin classification using the perceptron algorithm. Ma-
chine Learning, 37(3):277–296.
Balaji Krishnapuram, Lawrence Carin, Mário A. T.
Figueiredo, and Alexander J. Hartemink. 2005.
Sparse multinomial logistic regression: Fast algo-
rithms and generalization bounds. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 27(6).
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the 18th International Conference on Ma-
chine Learning.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajiˇc. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT-EMNLP-2005.
Joakim Nivre. 2003. An efficient algorithm for projec-
tive dependency parsing. In Proceedings of the 8th In-
ternational Workshop on Parsing Technologies (IWPT
03), pages 149–160, Nancy, France, 23-25 April.
Ben Taskar, Carlos Guestrin, Vassil Chatalbashev, and
Daphne Koller. 2006. Max-margin Markov networks.
Journal of Machine Learning Research, to appear.
Michael E. Tipping. 2001. Sparse Bayesian learning
and the relevance vector machine. Journal of Machine
Learning Research, 1:211 – 244.
Iannis Tsochantaridis, Thorsten Joachims, Thomas Hof-
mann, and Yasemin Altun. 2005. Large margin meth-
ods for structured and interdependent output variables.
Journal of Machine Learning Research, 6(Sep):1453–
1484.
Luke S. Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form: Structured clas-
sification with probabilistic categorial grammars. In
Proceedings of UAI 2005.
Ji Zhu and Trevor Hastie. 2005. Kernel logistic regres-
sion and the import vector machine. Journal of Com-
putational and Graphical Statistics, 14(1):185–205.
</reference>
<page confidence="0.998884">
54
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.908748">
<title confidence="0.9990125">Logistic Online Learning Methods and Their Application to Incremental Dependency Parsing</title>
<author confidence="0.999994">Richard Johansson</author>
<affiliation confidence="0.9999735">Department of Computer Science Lund University</affiliation>
<address confidence="0.976402">Lund, Sweden</address>
<email confidence="0.991574">richard@cs.lth.se</email>
<abstract confidence="0.997074142857143">We investigate a family of update methods for online machine learning algorithms for cost-sensitive multiclass and structured classification problems. The update rules are based on multinomial logistic models. The most interesting question for such an approach is how to integrate the cost function into the learning paradigm. We propose a number of solutions to this problem. To demonstrate the applicability of the algorithms, we evaluated them on a number of classification tasks related to incremental dependency parsing. These tasks were conventional multiclass classification, hiearchical classification, and a structured classification task: complete labeled dependency tree prediction. The performance figures of the logistic algorithms range from slightly lower to slightly higher than margin-based online algorithms.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Itzair Aduriz</author>
<author>Maria Jesus Aranzabe</author>
<author>Jose Mari Arriola</author>
</authors>
<title>Aitziber Atutxa, Arantza Diaz de Ilarraza, Aitzpea Garmendia, and Maite Oronoz.</title>
<date>2003</date>
<booktitle>In Proceedings of the TLT,</booktitle>
<pages>201--204</pages>
<contexts>
<context position="16449" citStr="Aduriz et al., 2003" startWordPosition="2724" endWordPosition="2727"> a state (Si, Ii) and the action ai taken in that state. The cost function was defined as the sum of link costs, where the link cost was 0 for a correct dependency link with a correct label, 0.5 for a correct link with an incorrect label, and 1 for an incorrect link. Since the history-based feature set used in the parsing algorithm makes it impossible to use independence to factorize the scoring function, an exact search to find the best-scoring action sequence is not possible. We used a beam search of width 2 in this experiment. We trained models on a 5000-word subset of the Basque Treebank (Aduriz et al., 2003) and evaluated them on a 8000-word subset of the same corpus. As before, we used an expanded quadratic kernel, and all algorithms iterated five times through the training set. Table 4 shows the results of this experiment. We show labeled accuracy instead of cost for ease of interpretation. Here, the loss-based SPV outperformed Method Labeled Accuracy L-SPV, Gauss 66.24 MIRA 66.19 MEC, Gauss 65.99 C-SPV, Gauss 65.84 OPA 65.45 MEC, Laplace 64.81 C-SPV, Laplace 64.73 L-SPV, Laplace 64.50 Table 4: Results for dependency tree prediction. MIRA, and two other logistic updates also outperformed OPA. T</context>
</contexts>
<marker>Aduriz, Aranzabe, Arriola, 2003</marker>
<rawString>Itzair Aduriz, Maria Jesus Aranzabe, Jose Mari Arriola, Aitziber Atutxa, Arantza Diaz de Ilarraza, Aitzpea Garmendia, and Maite Oronoz. 2003. Construction of a Basque dependency treebank. In Proceedings of the TLT, pages 201–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the CoNLL-X.</booktitle>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceedings of the CoNLL-X.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL.</booktitle>
<contexts>
<context position="1592" citStr="Collins and Duffy, 2002" startWordPosition="228" endWordPosition="231">lightly higher than margin-based online algorithms. 1 Introduction Natural language consists of complex structures, such as sequences of phonemes, parse trees, and discourse or temporal graphs. Researchers in NLP have started to realize that this complexity should be reflected in their statistical models. This intuition has spurred a growing interest of related research in the machine learning community, which in turn has led to improved results in a wide range of applications in NLP, including sequence labeling (Lafferty et al., 2001; Taskar et al., 2006), constituent and dependency parsing (Collins and Duffy, 2002; McDonald et al., 2005), and logical form extraction (Zettlemoyer and Collins, 2005). Machine learning research for structured problems have generally used margin-based formulations. These include global batch methods such as Max-margin Markov Networks (M3N) (Taskar et al., 2006) and SVMstruct (Tsochantaridis et al., 2005) as well as online methods such as Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003) and the Online Passive-Aggressive Algorithm (OPA) (Crammer et al., 2006). Although the batch methods are formulated very elegantly, they do not seem to scale well to the lar</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Michael Collins and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In Proceedings of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>2003</volume>
<issue>3</issue>
<contexts>
<context position="2018" citStr="Crammer and Singer, 2003" startWordPosition="292" endWordPosition="295">s led to improved results in a wide range of applications in NLP, including sequence labeling (Lafferty et al., 2001; Taskar et al., 2006), constituent and dependency parsing (Collins and Duffy, 2002; McDonald et al., 2005), and logical form extraction (Zettlemoyer and Collins, 2005). Machine learning research for structured problems have generally used margin-based formulations. These include global batch methods such as Max-margin Markov Networks (M3N) (Taskar et al., 2006) and SVMstruct (Tsochantaridis et al., 2005) as well as online methods such as Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003) and the Online Passive-Aggressive Algorithm (OPA) (Crammer et al., 2006). Although the batch methods are formulated very elegantly, they do not seem to scale well to the large training sets prevalent in NLP contexts. The online methods on the other hand, although less theoretically appealing, can handle realistically sized data sets. In this work, we investigate whether logistic online learning performs as well as margin-based methods. Logistic models are easily extended to using kernels; that this is theoretically well-justified was shown by Zhu and Hastie (2005), who also made an elegant ar</context>
<context position="13136" citStr="Crammer and Singer, 2003" startWordPosition="2171" endWordPosition="2174">od (such as oneversus-all or pairwise binarization). As a training set, we randomly selected 50,000 instances of state–action pairs generated for a dependency-converted version of Penn Treebank. This training set contained 22 types of actions (such as SHIFT, REDUCE, LEFT-ARC(SUBJECT), and RIGHT-ARC(OBJECT). The test set was also randomly selected and contained 10,000 instances. We trained classifiers using the logistic updates (C-SPV, L-SPV, and MEC) with Gaussian and Laplacian priors. Additionally, we trained OPA and MIRA classifiers, as well as an Additive Ultraconservative (AU) classifier (Crammer and Singer, 2003), a variant of the Perceptron. For all algorithms, we tried to find the best values of the respective regularization parameter using cross-validation. All training algorithms iterated five times through the training set and used an expanded quadratic kernel. Table 2 shows the classification error for all algorithms. As can be seen, the performance was lower for the logistic algorithms, although the difference was slight. Both the logistic (MEC and SPV) and the margin-based classifiers (OPA and MIRA) outperformed the AU classifier. Method Test error MIRA 6.05% OPA 6.17% C-SPV, Laplace 6.20% MEC</context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>Koby Crammer and Yoram Singer. 2003. Ultraconservative online algorithms for multiclass problems. Journal of Machine Learning Research, 2003(3):951–991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai ShalevSchwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passiveaggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>2006</volume>
<issue>7</issue>
<contexts>
<context position="2091" citStr="Crammer et al., 2006" startWordPosition="302" endWordPosition="305">equence labeling (Lafferty et al., 2001; Taskar et al., 2006), constituent and dependency parsing (Collins and Duffy, 2002; McDonald et al., 2005), and logical form extraction (Zettlemoyer and Collins, 2005). Machine learning research for structured problems have generally used margin-based formulations. These include global batch methods such as Max-margin Markov Networks (M3N) (Taskar et al., 2006) and SVMstruct (Tsochantaridis et al., 2005) as well as online methods such as Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003) and the Online Passive-Aggressive Algorithm (OPA) (Crammer et al., 2006). Although the batch methods are formulated very elegantly, they do not seem to scale well to the large training sets prevalent in NLP contexts. The online methods on the other hand, although less theoretically appealing, can handle realistically sized data sets. In this work, we investigate whether logistic online learning performs as well as margin-based methods. Logistic models are easily extended to using kernels; that this is theoretically well-justified was shown by Zhu and Hastie (2005), who also made an elegant argument that margin-based methods are in fact related to regularized logis</context>
</contexts>
<marker>Crammer, Dekel, Keshet, ShalevSchwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai ShalevSchwartz, and Yoram Singer. 2006. Online passiveaggressive algorithms. Journal of Machine Learning Research, 2006(7):551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daumé John Langford</author>
<author>Daniel Marcu</author>
</authors>
<date>2006</date>
<note>Search-based structured prediction. Submitted.</note>
<marker>Langford, Marcu, 2006</marker>
<rawString>Hal Daumé III, John Langford, and Daniel Marcu. 2006. Search-based structured prediction. Submitted.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>Large margin classification using the perceptron algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="5061" citStr="Freund and Schapire, 1999" startWordPosition="785" endWordPosition="788">air (x, y) and w a vector of feature weights. Learning in this case is equivalent to assigning appropriate weights in the vector w. In the online learning framework, the weight vector is constructed incrementally. Algorithm 1 shows the general form of the algorithm. It proceeds a number of times through the training set. In each step, it computes an update to the weight vector based on the current example. The resulting weight vector tends to be overfit to the last few examples; one way to reduce overfitting is to use the average of all successive weight vectors as the result of the training (Freund and Schapire, 1999). Algorithm 1 General form of online algorithms input Training set T = {(xt, yt)}Tt=1 Number of iterations N for n in L.N for (xt, yt) in T Compute update vector Sw for (xt, yt) www+Sw return waverage Following earlier online learning methods such as the Perceptron, we assume that in each update step, we adjust the weight vector by incrementally adding feature vectors. For stability, we impose the constraint that the sum of the updates in each step should be zero. We assume that the possible output values are {yi}mi=0 and, for convenience, that y0 is the correct value. This leads to the follow</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Yoav Freund and Robert E. Schapire. 1999. Large margin classification using the perceptron algorithm. Machine Learning, 37(3):277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Balaji Krishnapuram</author>
<author>Lawrence Carin</author>
<author>Mário A T Figueiredo</author>
<author>Alexander J Hartemink</author>
</authors>
<title>Sparse multinomial logistic regression: Fast algorithms and generalization bounds.</title>
<date>2005</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>27</volume>
<issue>6</issue>
<contexts>
<context position="17327" citStr="Krishnapuram et al. (2005)" startWordPosition="2865" endWordPosition="2868">ad of cost for ease of interpretation. Here, the loss-based SPV outperformed Method Labeled Accuracy L-SPV, Gauss 66.24 MIRA 66.19 MEC, Gauss 65.99 C-SPV, Gauss 65.84 OPA 65.45 MEC, Laplace 64.81 C-SPV, Laplace 64.73 L-SPV, Laplace 64.50 Table 4: Results for dependency tree prediction. MIRA, and two other logistic updates also outperformed OPA. The differences between the first four scores are however not statistically significant. Interestingly, all updates with Laplacian prior resulted in low performance. The reason for this may be that Laplacian priors tend to promote sparse solutions (see Krishnapuram et al. (2005), inter alia), and that this sparsity is detrimental for this highly lexicalized feature set. 4 Conclusion and Future Work This paper presented new update methods for online machine learning algorithms. The update methods are based on a multinomial logistic model. Their performance is on par with other state-of-the-art online learning algorithms for cost-sensitive problems. 53 We investigated two main approaches to integrating the cost function into the logistic model. In the first method, the cost was linked to the prior variances, while in the second method, the update rule sets the weights </context>
</contexts>
<marker>Krishnapuram, Carin, Figueiredo, Hartemink, 2005</marker>
<rawString>Balaji Krishnapuram, Lawrence Carin, Mário A. T. Figueiredo, and Alexander J. Hartemink. 2005. Sparse multinomial logistic regression: Fast algorithms and generalization bounds. IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the 18th International Conference on Machine Learning.</booktitle>
<contexts>
<context position="1509" citStr="Lafferty et al., 2001" startWordPosition="215" endWordPosition="218">The performance figures of the logistic algorithms range from slightly lower to slightly higher than margin-based online algorithms. 1 Introduction Natural language consists of complex structures, such as sequences of phonemes, parse trees, and discourse or temporal graphs. Researchers in NLP have started to realize that this complexity should be reflected in their statistical models. This intuition has spurred a growing interest of related research in the machine learning community, which in turn has led to improved results in a wide range of applications in NLP, including sequence labeling (Lafferty et al., 2001; Taskar et al., 2006), constituent and dependency parsing (Collins and Duffy, 2002; McDonald et al., 2005), and logical form extraction (Zettlemoyer and Collins, 2005). Machine learning research for structured problems have generally used margin-based formulations. These include global batch methods such as Max-margin Markov Networks (M3N) (Taskar et al., 2006) and SVMstruct (Tsochantaridis et al., 2005) as well as online methods such as Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003) and the Online Passive-Aggressive Algorithm (OPA) (Crammer et al., 2006). Although the bat</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP-2005.</booktitle>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajiˇc. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of HLT-EMNLP-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>An efficient algorithm for projective dependency parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT 03),</booktitle>
<pages>149--160</pages>
<location>Nancy,</location>
<contexts>
<context position="11652" citStr="Nivre, 2003" startWordPosition="1935" endWordPosition="1936">′(n′′, n) ∈ A Right-arc (n|S, n′|I, A) → (n′|n|S, I, A ∪ {(n, n′)}) ¬∃n′′(n′′, n′) ∈ A Reduce (n|S, I, A) → (S, I, A) ∃n′(n′, n) ∈ A Shift (S, n|I, A) → (n|S, I, A) Table 1: Nivre’s parser transitions where W is the initial word list; I, the current input word list; A, the graph of dependencies; and S, the stack. (n′, n) denotes a dependency relations between n′ and n, where n′ is the head and n the dependent. 3 Experiments To compare the logistic online algorithms against other learning algorithms, we performed a set of experiments in incremental dependency parsing using the Nivre algorithm (Nivre, 2003). The algorithm is a variant of the shift–reduce algorithm and creates a projective and acyclic graph. As with the regular shift–reduce, it uses a stack S and a list of input words W, and builds the parse tree incrementally using a set of parsing actions (see Table 1). However, instead of finding constituents, it builds a set of arcs representing the graph of dependencies. It can be shown that every projective dependency graph can be produced by a sequence of parser actions, and that the worst-case number of actions is linear with respect to the number of words in the sentence. 3.1 Multiclass </context>
</contexts>
<marker>Nivre, 2003</marker>
<rawString>Joakim Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT 03), pages 149–160, Nancy, France, 23-25 April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Carlos Guestrin</author>
<author>Vassil Chatalbashev</author>
<author>Daphne Koller</author>
</authors>
<title>Max-margin Markov networks.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<note>to appear.</note>
<contexts>
<context position="1531" citStr="Taskar et al., 2006" startWordPosition="219" endWordPosition="222"> of the logistic algorithms range from slightly lower to slightly higher than margin-based online algorithms. 1 Introduction Natural language consists of complex structures, such as sequences of phonemes, parse trees, and discourse or temporal graphs. Researchers in NLP have started to realize that this complexity should be reflected in their statistical models. This intuition has spurred a growing interest of related research in the machine learning community, which in turn has led to improved results in a wide range of applications in NLP, including sequence labeling (Lafferty et al., 2001; Taskar et al., 2006), constituent and dependency parsing (Collins and Duffy, 2002; McDonald et al., 2005), and logical form extraction (Zettlemoyer and Collins, 2005). Machine learning research for structured problems have generally used margin-based formulations. These include global batch methods such as Max-margin Markov Networks (M3N) (Taskar et al., 2006) and SVMstruct (Tsochantaridis et al., 2005) as well as online methods such as Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003) and the Online Passive-Aggressive Algorithm (OPA) (Crammer et al., 2006). Although the batch methods are formula</context>
<context position="9988" citStr="Taskar et al., 2006" startWordPosition="1636" endWordPosition="1639">blems the set of possible outputs is typically very large. Two broad categories of solutions to this problem have been common in literature, both of which rely on the structure of the domain: • Subset selection: instead of working with the complete range of outputs, only an “interesting” subset is used, for instance by repeatedly finding the most violated constraints (Tsochantaridis et al., 2005) or by using N-best search (McDonald et al., 2005). • Decomposition: the inherent structure of the problem is used to factorize the optimization problem. Examples include Markov decompositions in M3N (Taskar et al., 2006) and dependency-based factorization for MIRA (McDonald et al., 2005). In principle, both methods could be used in our framework. In this work, we use subset selection since it is easy to implement for many domains (in the form of an N-best search) and allows a looser coupling between the domain and the learning algorithm. 2.4 Implementation Issues Since we typically work with only a few variables in each iteration, maximizing the log posterior or minimizing the expectation is easy (assuming, of course, that we chose a log-concave prior). We used gradient ascent and did not try to use more soph</context>
</contexts>
<marker>Taskar, Guestrin, Chatalbashev, Koller, 2006</marker>
<rawString>Ben Taskar, Carlos Guestrin, Vassil Chatalbashev, and Daphne Koller. 2006. Max-margin Markov networks. Journal of Machine Learning Research, to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael E Tipping</author>
</authors>
<title>Sparse Bayesian learning and the relevance vector machine.</title>
<date>2001</date>
<journal>Journal of Machine Learning Research, 1:211 –</journal>
<pages>244</pages>
<marker>Tipping, 2001</marker>
<rawString>Michael E. Tipping. 2001. Sparse Bayesian learning and the relevance vector machine. Journal of Machine Learning Research, 1:211 – 244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iannis Tsochantaridis</author>
<author>Thorsten Joachims</author>
<author>Thomas Hofmann</author>
<author>Yasemin Altun</author>
</authors>
<title>Large margin methods for structured and interdependent output variables.</title>
<date>2005</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>6</volume>
<pages>1484</pages>
<contexts>
<context position="1917" citStr="Tsochantaridis et al., 2005" startWordPosition="276" endWordPosition="279">n has spurred a growing interest of related research in the machine learning community, which in turn has led to improved results in a wide range of applications in NLP, including sequence labeling (Lafferty et al., 2001; Taskar et al., 2006), constituent and dependency parsing (Collins and Duffy, 2002; McDonald et al., 2005), and logical form extraction (Zettlemoyer and Collins, 2005). Machine learning research for structured problems have generally used margin-based formulations. These include global batch methods such as Max-margin Markov Networks (M3N) (Taskar et al., 2006) and SVMstruct (Tsochantaridis et al., 2005) as well as online methods such as Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003) and the Online Passive-Aggressive Algorithm (OPA) (Crammer et al., 2006). Although the batch methods are formulated very elegantly, they do not seem to scale well to the large training sets prevalent in NLP contexts. The online methods on the other hand, although less theoretically appealing, can handle realistically sized data sets. In this work, we investigate whether logistic online learning performs as well as margin-based methods. Logistic models are easily extended to using kernels; that</context>
<context position="9767" citStr="Tsochantaridis et al., 2005" startWordPosition="1599" endWordPosition="1603">.3 Handling Complex Prediction Problems The algorithm can thus be used for any costsensitive classification problem. This class of problems includes prediction of complex structures such as trees or graphs. However, for those problems the set of possible outputs is typically very large. Two broad categories of solutions to this problem have been common in literature, both of which rely on the structure of the domain: • Subset selection: instead of working with the complete range of outputs, only an “interesting” subset is used, for instance by repeatedly finding the most violated constraints (Tsochantaridis et al., 2005) or by using N-best search (McDonald et al., 2005). • Decomposition: the inherent structure of the problem is used to factorize the optimization problem. Examples include Markov decompositions in M3N (Taskar et al., 2006) and dependency-based factorization for MIRA (McDonald et al., 2005). In principle, both methods could be used in our framework. In this work, we use subset selection since it is easy to implement for many domains (in the form of an N-best search) and allows a looser coupling between the domain and the learning algorithm. 2.4 Implementation Issues Since we typically work with </context>
</contexts>
<marker>Tsochantaridis, Joachims, Hofmann, Altun, 2005</marker>
<rawString>Iannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin Altun. 2005. Large margin methods for structured and interdependent output variables. Journal of Machine Learning Research, 6(Sep):1453– 1484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of UAI</booktitle>
<contexts>
<context position="1677" citStr="Zettlemoyer and Collins, 2005" startWordPosition="241" endWordPosition="245">guage consists of complex structures, such as sequences of phonemes, parse trees, and discourse or temporal graphs. Researchers in NLP have started to realize that this complexity should be reflected in their statistical models. This intuition has spurred a growing interest of related research in the machine learning community, which in turn has led to improved results in a wide range of applications in NLP, including sequence labeling (Lafferty et al., 2001; Taskar et al., 2006), constituent and dependency parsing (Collins and Duffy, 2002; McDonald et al., 2005), and logical form extraction (Zettlemoyer and Collins, 2005). Machine learning research for structured problems have generally used margin-based formulations. These include global batch methods such as Max-margin Markov Networks (M3N) (Taskar et al., 2006) and SVMstruct (Tsochantaridis et al., 2005) as well as online methods such as Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003) and the Online Passive-Aggressive Algorithm (OPA) (Crammer et al., 2006). Although the batch methods are formulated very elegantly, they do not seem to scale well to the large training sets prevalent in NLP contexts. The online methods on the other hand, alt</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Proceedings of UAI 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ji Zhu</author>
<author>Trevor Hastie</author>
</authors>
<title>Kernel logistic regression and the import vector machine.</title>
<date>2005</date>
<journal>Journal of Computational and Graphical Statistics,</journal>
<volume>14</volume>
<issue>1</issue>
<contexts>
<context position="2589" citStr="Zhu and Hastie (2005)" startWordPosition="380" endWordPosition="383">elaxed Algorithm (MIRA) (Crammer and Singer, 2003) and the Online Passive-Aggressive Algorithm (OPA) (Crammer et al., 2006). Although the batch methods are formulated very elegantly, they do not seem to scale well to the large training sets prevalent in NLP contexts. The online methods on the other hand, although less theoretically appealing, can handle realistically sized data sets. In this work, we investigate whether logistic online learning performs as well as margin-based methods. Logistic models are easily extended to using kernels; that this is theoretically well-justified was shown by Zhu and Hastie (2005), who also made an elegant argument that margin-based methods are in fact related to regularized logistic models. For batch learning, there exist several learning algorithms in a logistic framework for conventional multiclass classification but few for structured problems. Prediction of complex structures is conventionally treated as a cost-sensitive multiclass classification problem, although special care has to be taken to handle the large space of possible outputs. The integration of the cost function into the logistic framework leads to two distinct (although related) update methods: the S</context>
</contexts>
<marker>Zhu, Hastie, 2005</marker>
<rawString>Ji Zhu and Trevor Hastie. 2005. Kernel logistic regression and the import vector machine. Journal of Computational and Graphical Statistics, 14(1):185–205.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>