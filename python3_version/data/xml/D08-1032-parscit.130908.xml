<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000145">
<title confidence="0.998169">
Selecting Sentences for Answering Complex Questions
</title>
<author confidence="0.995464">
Yllias Chali
</author>
<affiliation confidence="0.9933225">
University of Lethbridge
4401 University Drive
</affiliation>
<address confidence="0.560321">
Lethbridge, Alberta, Canada, T1K 3M4
</address>
<email confidence="0.992887">
chali@cs.uleth.ca
</email>
<author confidence="0.973254">
Shafiq R. Joty
</author>
<affiliation confidence="0.99498">
University of British Columbia
</affiliation>
<address confidence="0.571502">
2366 Main Mall
Vancouver, B.C. Canada V6T 1Z4
</address>
<email confidence="0.995067">
rjoty@cs.ubc.ca
</email>
<sectionHeader confidence="0.995486" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997343086956522">
Complex questions that require inferencing
and synthesizing information from multiple
documents can be seen as a kind of topic-
oriented, informative multi-document summa-
rization. In this paper, we have experimented
with one empirical and two unsupervised
statistical machine learning techniques: k-
means and Expectation Maximization (EM),
for computing relative importance of the sen-
tences. However, the performance of these ap-
proaches depends entirely on the feature set
used and the weighting of these features. We
extracted different kinds of features (i.e. lex-
ical, lexical semantic, cosine similarity, ba-
sic element, tree kernel based syntactic and
shallow-semantic) for each of the document
sentences in order to measure its importance
and relevancy to the user query. We used a
local search technique to learn the weights of
the features. For all our methods of generating
summaries, we have shown the effects of syn-
tactic and shallow-semantic features over the
bag of words (BOW) features.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999793863636364">
After having made substantial headway in factoid
and list questions, researchers have turned their at-
tention to more complex information needs that can-
not be answered by simply extracting named enti-
ties (persons, organizations, locations, dates, etc.)
from documents. For example, the question: “De-
scribe the after-effects of cyclone Sidr-Nov 2007 in
Bangladesh” requires inferencing and synthesizing
information from multiple documents. This infor-
mation synthesis in NLP can be seen as a kind of
topic-oriented, informative multi-document summa-
rization, where the goal is to produce a single text as
a compressed version of a set of documents with a
minimum loss of relevant information.
In this paper, we experimented with one em-
pirical and two well-known unsupervised statisti-
cal machine learning techniques: k-means and EM
and evaluated their performance in generating topic-
oriented summaries. However, the performance of
these approaches depends entirely on the feature set
used and the weighting of these features. We ex-
tracted different kinds of features (i.e. lexical, lexi-
cal semantic, cosine similarity, basic element, tree
kernel based syntactic and shallow-semantic) for
each of the document sentences in order to measure
its importance and relevancy to the user query. We
have used a gradient descent local search technique
to learn the weights of the features. Traditionally,
information extraction techniques are based on the
BOW approach augmented by language modeling.
But when the task requires the use of more com-
plex semantics, the approaches based on only BOW
are often inadequate to perform fine-level textual
analysis. Some improvements on BOW are given
by the use of dependency trees and syntactic parse
trees (Hirao et al., 2004), (Punyakanok et al., 2004),
(Zhang and Lee, 2003), but these, too are not ade-
quate when dealing with complex questions whose
answers are expressed by long and articulated sen-
tences or even paragraphs. Shallow semantic rep-
resentations, bearing a more compact information,
could prevent the sparseness of deep structural ap-
proaches and the weakness of BOW models (Mos-
chitti et al., 2007). Attempting an application of
</bodyText>
<page confidence="0.985848">
304
</page>
<note confidence="0.962084">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 304–313,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999898809523809">
syntactic and semantic information to complex QA
hence seems natural, as pinpointing the answer to a
question relies on a deep understanding of the se-
mantics of both. In more complex tasks such as
computing the relatedness between the query sen-
tences and the document sentences in order to gen-
erate query-focused summaries (or answers to com-
plex questions), to our knowledge no study uses tree
kernel functions to encode syntactic/semantic infor-
mation. For all our methods of generating sum-
maries (i.e. empirical, k-means and EM), we have
shown the effects of syntactic and shallow-semantic
features over the BOW features.
This paper is organized as follows: Section 2 fo-
cuses on the related work, Section 3 describes how
the features are extracted, Section 4 discusses the
scoring approaches, Section 5 discusses how we re-
move the redundant sentences before adding them
to the summary, Section 6 describes our experimen-
tal study. We conclude and give future directions in
Section 7.
</bodyText>
<sectionHeader confidence="0.999745" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999927137254902">
Researchers all over the world working on query-
based summarization are trying different direc-
tions to see which methods provide the best re-
sults. The LexRank method addressed in (Erkan
and Radev, 2004) was very successful in generic
multi-document summarization. A topic-sensitive
LexRank is proposed in (Otterbacher et al., 2005).
As in LexRank, the set of sentences in a document
cluster is represented as a graph, where nodes are
sentences and links between the nodes are induced
by a similarity relation between the sentences. Then
the system ranked the sentences according to a ran-
dom walk model defined in terms of both the inter-
sentence similarities and the similarities of the sen-
tences to the topic description or question.
The summarization methods based on lexical
chain first extract the nouns, compound nouns and
named entities as candidate words (Li et al., 2007).
Then using WordNet, the systems find the semantic
similarity between the nouns and compound nouns.
After that, lexical chains are built in two steps: 1)
Building single document strong chains while dis-
ambiguating the senses of the words and, 2) build-
ing multi-chain by merging the strongest chains of
the single documents into one chain. The systems
rank sentences using a formula that involves a) the
lexical chain, b) keywords from query and c) named
entities.
(Harabagiu et al., 2006) introduce a new paradigm
for processing complex questions that relies on a
combination of (a) question decompositions; (b) fac-
toid QA techniques; and (c) Multi-Document Sum-
marization (MDS) techniques. The question decom-
position procedure operates on a Marcov chain, by
following a random walk with mixture model on a
bipartite graph of relations established between con-
cepts related to the topic of a complex question and
subquestions derived from topic-relevant passages
that manifest these relations. Decomposed questions
are then submitted to a state-of-the-art QA system
in order to retrieve a set of passages that can later be
merged into a comprehensive answer by a MDS sys-
tem. They show that question decompositions using
this method can significantly enhance the relevance
and comprehensiveness of summary-length answers
to complex questions.
There are approaches that are based on probabilis-
tic models (Pingali et al., 2007) (Toutanova et al.,
2007). (Pingali et al., 2007) rank the sentences based
on a mixture model where each component of the
model is a statistical model:
</bodyText>
<equation confidence="0.995672">
Score(s) = αxQIScore(s)+(1−α)xQFocus(s, Q)
</equation>
<bodyText confidence="0.964150095238095">
Where, Score(s) is the score for sentence s. Query-
independent score (QIScore) and query-dependent score
(QFocus) are calculated based on probabilistic models.
(Toutanova et al., 2007) learns a log-linear sentence rank-
ing model by maximizing three metrics of sentence good-
ness: (a) ROUGE oracle, (b) Pyramid-derived, and (c)
Model Frequency. The scoring function is learned by fit-
ting weights for a set of feature functions of sentences
in the document set and is trained to optimize a sentence
pair-wise ranking criterion. The scoring function is fur-
ther adapted to apply to summaries rather than sentences
and to take into account redundancy among sentences.
There are approaches in “Recognizing Textual Entail-
ment”, “Sentence Alignment” and “Question Answering”
that use syntactic and/or semantic information in order to
measure the similarity between two textual units. (Mac-
Cartney et al., 2006) use typed dependency graphs (same
as dependency trees) to represent the text and the hypoth-
esis. Then they try to find a good partial alignment be-
tween the typed dependency graphs representing the hy-
pothesis and the text in a search space of O((m + 1)n)
</bodyText>
<page confidence="0.998661">
305
</page>
<bodyText confidence="0.99885294117647">
where hypothesis graph contains n nodes and a text graph
contains m nodes. (Hirao et al., 2004) represent the sen-
tences using Dependency Tree Path (DTP) to incorporate
syntactic information. They apply String Subsequence
Kernel (SSK) to measure the similarity between the DTPs
of two sentences. They also introduce Extended String
Subsequence Kernel (ESK) to incorporate semantics in
DTPs. (Kouylekov and Magnini, 2005) use the tree edit
distance algorithms on the dependency trees of the text
and the hypothesis to recognize the textual entailment.
According to this approach, a text T entails a hypothesis
H if there exists a sequence of transformations (i.e. dele-
tion, insertion and substitution) applied to T such that
we can obtain H with an overall cost below a certain
threshold. (Punyakanok et al., 2004) represent the ques-
tion and the sentence containing answer with their depen-
dency trees. They add semantic information (i.e. named
entity, synonyms and other related words) in the depen-
dency trees. They apply the approximate tree matching
in order to decide how similar any given pair of trees are.
They also use the edit distance as the matching criteria in
the approximate tree matching. All these methods show
the improvement over the BOW scoring methods.
Our Basic Element (BE)-based feature used the depen-
dency tree to extract the BEs (i.e. head-modifier-relation)
and ranked the BEs based on their log-likelihood ratios.
For syntactic feature, we extracted the syntactic trees for
the sentence as well as for the query using the Charniak
parser and measured the similarity between the two trees
using the tree kernel function. We used the ASSERT se-
mantic role labeler system to parse the sentence as well
as the query semantically and used the shallow seman-
tic tree kernel to measure the similarity between the two
shallow-semantic trees.
</bodyText>
<sectionHeader confidence="0.9909" genericHeader="method">
3 Feature Extraction
</sectionHeader>
<bodyText confidence="0.99984">
The sentences in the document collection are analyzed
in various levels and each of the document-sentences is
represented as a vector of feature-values. The features
can be divided into several categories:
</bodyText>
<subsectionHeader confidence="0.9972485">
3.1 Lexical Features
3.1.1 N-gram Overlap
</subsectionHeader>
<bodyText confidence="0.59907925">
N-gram overlap measures the overlapping word se-
quences between the candidate sentence and the query
sentence. With the view to measure the N-gram
(N=1,2,3,4) overlap scores, a query pool and a sentence
pool are created. In order to create the query (or sentence)
pool, we took the query (or document) sentence and cre-
ated a set of related sentences by replacing its important
words1 by their first-sense synonyms. For example given
</bodyText>
<footnote confidence="0.974917">
1hence forth important words are the nouns, verbs, adverbs
and adjectives
</footnote>
<bodyText confidence="0.9991248">
a stemmed document-sentence: “John write a poem”, the
sentence pool contains: “John compose a poem”, “John
write a verse form” along with the given sentence. We
measured the recall based n-gram scores for a sentence P
using the following formula:
</bodyText>
<equation confidence="0.9994465">
n-gramScore(P) = maxi(maxj N-gram(si, qj))
N-gram(S,Q) = EE9 amn ESoCount(gra mn) n)
</equation>
<bodyText confidence="0.994912333333333">
Where, n stands for the length of the n-gram (n =
1, 2, 3, 4) and Countmat h (gramn) is the number
of n-grams co-occurring in the query and the candi-
date sentence, qj is the jth sentence in the query
pool and si is the ith sentence in the sentence pool
of sentence P.
</bodyText>
<subsectionHeader confidence="0.943882">
3.1.2 LCS, WLCS and Skip-Bigram
</subsectionHeader>
<bodyText confidence="0.999983533333333">
A sequence W = [w1, w2, ..., wn] is a subse-
quence of another sequence X = [x1, x2, ..., xm], if
there exists a strict increasing sequence [i1, i2, ..., id
of indices of X such that for all j =
1,2, ..., k we have xis = wj. Given two sequences,
51 and 52, the Longest Common Subsequence
(LCS) of 51 and 52 is a common subsequence with
maximum length. The longer the LCS of two sen-
tences is, the more similar the two sentences are.
The basic LCS has a problem that it does not dif-
ferentiate LCSes of different spatial relations within
their embedding sequences (Lin, 2004). To improve
the basic LCS method, we can remember the length
of consecutive matches encountered so far to a reg-
ular two dimensional dynamic program table com-
puting LCS. We call this weighted LCS (WLCS)
and use k to indicate the length of the current con-
secutive matches ending at words xi and yj. Given
two sentences X and Y, the WLCS score of X and
Y can be computed using the similar dynamic pro-
gramming procedure as stated in (Lin, 2004). We
computed the LCS and WLCS-based F-measure fol-
lowing (Lin, 2004) using both the query pool and the
sentence pool as in the previous section.
Skip-bigram is any pair of words in their sentence
order, allowing for arbitrary gaps. Skip-bigram mea-
sures the overlap of skip-bigrams between a candi-
date sentence and a query sentence. Following (Lin,
2004), we computed the skip bi-gram score using
both the sentence pool and the query pool.
</bodyText>
<page confidence="0.99549">
306
</page>
<subsectionHeader confidence="0.627899">
3.1.3 Head and Head Related-words Overlap
</subsectionHeader>
<bodyText confidence="0.999891923076923">
The number of head words common in between
two sentences can indicate how much they are rel-
evant to each other. In order to extract the heads
from the sentence (or query), the sentence (or query)
is parsed by Minipar 2 and from the dependency
tree we extract the heads which we call exact head
words. For example, the head word of the sentence:
“John eats rice” is “eat”.
We take the synonyms, hyponyms and hyper-
nyms3 of both the query-head words and the
sentence-head words and form a set of words which
we call head-related words. We measured the exact
head score and the head-related score as follows:
</bodyText>
<equation confidence="0.995758333333333">
ExactHeadScore =
HeadRelatedScore =
Pw1EHeadSet Countmatch(w1)
PPw1EHeadSet Count(w1)
w1EHeadRe�Set Countmatch(w1)
Pw1EHeadReLSet Count(w1)
</equation>
<bodyText confidence="0.998449">
Where HeadSet is the set of head words in the sen-
tence and Count,,,,at h is the number of matches
between the HeadSet of the query and the sen-
tence. HeadRelSet is the set of synonyms, hy-
ponyms and hypernyms of head words in the sen-
tence and Count,,tat h is the number of matches
between the head-related words of the query and the
sentence.
</bodyText>
<subsectionHeader confidence="0.999775">
3.2 Lexical Semantic Features
</subsectionHeader>
<bodyText confidence="0.999468">
We form a set of words which we call QueryRe-
latedWords by taking the important words from the
query, their first-sense synonyms, the nouns’ hy-
pernyms/hyponyms and important words from the
nouns’ gloss definitions.
Synonym overlap measure is the overlap be-
tween the list of synonyms of the important words
extracted from the candidate sentence and the
QueryRelatedWords. Hypernym/hyponym overlap
measure is the overlap between the list of hypernyms
and hyponyms of the nouns extracted from the sen-
tence and the QueryRelatedWords, and gloss overlap
measure is the overlap between the list of important
words that are extracted from the gloss definitions
of the nouns of the sentence and the QueryRelated-
Words.
</bodyText>
<footnote confidence="0.976702666666667">
2http://www.cs.ualberta.ca/ lindek/minipar.htm
3hypernym and hyponym levels are restricted to 2 and 3 re-
spectively
</footnote>
<subsectionHeader confidence="0.979426">
3.3 Statistical Similarity Measures
</subsectionHeader>
<listItem confidence="0.7241674">
Statistical similarity measures are based on the
co-occurance of similar words in a corpus. We
have used two statistical similarity measures:
1. Dependency-based similarity measure and 2.
Proximity-based similarity measure.
</listItem>
<bodyText confidence="0.9938942">
Dependency-based similarity measure uses the
dependency relations among words in order to mea-
sure the similarity. It extracts the dependency triples
then uses statistical approach to measure the similar-
ity. Proximity-based similarity measure is computed
based on the linear proximity relationship between
words only. It uses the information theoretic defini-
tion of similarity to measure the similarity.
We used the data provided by Dr. Dekang Lin4.
Using the data, one can retrieve most similar words
for a given word. The similar words are grouped into
clusters. Note that, for a word there can be more than
one cluster. Each cluster represents the sense of the
word and its similar words for that sense.
For each query word, we extract all of its clus-
ters from the data. Now, in order to determine the
right cluster for a query word, we measure the over-
lap score between the QueryRelatedWords and the
clusters of words. The hypothesis is that, the cluster
that has more words common with the QueryRelat-
edWords is the right cluster. We chose the cluster for
a word which has the highest overlap score.
Once we get the clusters for the query words, we
measured the overlap between the cluster words and
the sentence words as follows:
</bodyText>
<equation confidence="0.9985355">
Pw1ESenW ords Countmatch(w1)
Measure = P
Count(w1)
w1 E S enW ords
</equation>
<bodyText confidence="0.98670425">
Where, SenWords is the set of important words ex-
tracted from the sentence and Countmat,h is the number
of matches between the sentence words and the clusters
of similar words of the query words.
</bodyText>
<subsectionHeader confidence="0.989141">
3.4 Graph-based Similarity Measure
</subsectionHeader>
<bodyText confidence="0.999265444444445">
In LexRank (Erkan and Radev, 2004), the concept of
graph-based centrality is used to rank a set of sentences,
in producing generic multi-document summaries. A sim-
ilarity graph is produced for the sentences in the docu-
ment collection. In the graph, each node represents a
sentence. The edges between the nodes measure the co-
sine similarity between the respective pair of sentences.
The degree of a given node is an indication of how much
important the sentence is. Once the similarity graph is
</bodyText>
<footnote confidence="0.997657">
4http://www.cs.ualberta.ca/ lindek/downloads.htm
</footnote>
<page confidence="0.995595">
307
</page>
<bodyText confidence="0.99967675">
constructed, the sentences are then ranked according to
their eigenvector centrality. To apply LexRank to query-
focused context, a topic-sensitive version of LexRank is
proposed in (Otterbacher et al., 2005). We followed a
similar approach in order to calculate this feature. The
score of a sentence is determined by a mixture model of
the relevance of the sentence to the query and the similar-
ity of the sentence to other high-scoring sentences.
</bodyText>
<subsectionHeader confidence="0.971083">
3.5 Syntactic and Semantic Features:
</subsectionHeader>
<bodyText confidence="0.991947285714286">
So far, we have included the features of type Bag of
Words (BOW). The task like query-based summarization
that requires the use of more complex syntactic and se-
mantics, the approaches with only BOW are often inade-
quate to perform fine-level textual analysis. We extracted
three features that incorporate syntactic/semantic infor-
mation.
</bodyText>
<subsectionHeader confidence="0.663413">
3.5.1 Basic Element (BE) Overlap Measure
</subsectionHeader>
<bodyText confidence="0.999746789473684">
The “head-modifier-relation” triples, extracted from
the dependency trees are considered as BEs in our exper-
iment. The triples encode some syntactic/semantic infor-
mation and one can quite easily decide whether any two
units match or not- considerably more easily than with
longer units (Zhou et al., 2005). We used the BE package
distributed by ISI5 to extract the BEs for the sentences.
Once we get the BEs for a sentence, we computed the
Likelihood Ratio (LR) for each BE following (Zhou et
al., 2005). Sorting BEs according to their LR scores pro-
duced a BE-ranked list. Our goal is to generate a sum-
mary that will answer the user questions. The ranked
list of BEs in this way contains important BEs at the top
which may or may not be relevant to the user questions.
We filter those BEs by checking whether they contain any
word which is a query word or a QueryRelatedWords (de-
fined in Section 3.2). The score of a sentence is the sum
of its BE scores divided by the number of BEs in the sen-
tence.
</bodyText>
<subsectionHeader confidence="0.907738">
3.5.2 Syntactic Feature
</subsectionHeader>
<bodyText confidence="0.990488">
Encoding syntactic structure is easier and straight for-
ward. Given a sentence (or query), we first parse it into
a syntactic tree using a syntactic parser (i.e. Charniak
parser) and then we calculate the similarity between the
two trees using the tree kernel defined in (Collins and
Duffy, 2001).
</bodyText>
<subsectionHeader confidence="0.687939">
3.5.3 Shallow-semantic Feature
</subsectionHeader>
<bodyText confidence="0.9986878">
Though introducing BE and syntactic information
gives an improvement on BOW by the use of depen-
dency/syntactic parses, but these, too are not adequate
when dealing with complex questions whose answers
are expressed by long and articulated sentences or even
</bodyText>
<footnote confidence="0.985957">
5BE website:http://www.isi.edu/ cyl/BE
</footnote>
<figureCaption confidence="0.9568449">
Figure 1: Example of semantic trees
paragraphs. Shallow semantic representations, bearing a
more compact information, could prevent the sparseness
of deep structural approaches and the weakness of BOW
models (Moschitti et al., 2007).
Initiatives such as PropBank (PB) (Kingsbury and
Palmer, 2002) have made possible the design of accurate
automatic Semantic Role Labeling (SRL) systems like
ASSERT (Hacioglu et al., 2003). For example, consider
the PB annotation:
</figureCaption>
<bodyText confidence="0.929794161290323">
[ARG0 all][TARGET use][ARG1 the french
franc][ARG2 as their currency]
Such annotation can be used to design a shallow se-
mantic representation that can be matched against other
semantically similar sentences, e.g.
[ARG0 the Vatican][TARGET use][ARG1 the Italian
lira][ARG2 as their currency]
In order to calculate the semantic similarity between
the sentences, we first represent the annotated sentence
using the tree structures like Figure 1 which we call Se-
mantic Tree (ST). In the semantic tree, arguments are re-
placed with the most important word-often referred to as
the semantic head.
The sentences may contain one or more subordinate
clauses. For example the sentence, “the Vatican, located
wholly within Italy uses the Italian lira as their currency.”
gives the STs as in Figure 2. As we can see in Fig-
ure 2(A), when an argument node corresponds to an en-
tire subordinate clause, we label its leaf with ST, e.g.
the leaf of ARG0. Such ST node is actually the root of
the subordinate clause in Figure 2(B). If taken separately,
such STs do not express the whole meaning of the sen-
tence, hence it is more accurate to define a single struc-
ture encoding the dependency between the two predicates
as in Figure 2(C). We refer to this kind of nested STs as
STNs.
Note that, the tree kernel (TK) function defined in
(Collins and Duffy, 2001) computes the number of com-
mon subtrees between two trees. Such subtrees are sub-
ject to the constraint that their nodes are taken with all
or none of the children they have in the original tree.
</bodyText>
<page confidence="0.998261">
308
</page>
<figureCaption confidence="0.999745">
Figure 2: Two STs composing a STN
</figureCaption>
<bodyText confidence="0.99853375">
Though, this definition of subtrees makes the TK func-
tion appropriate for syntactic trees but at the same time
makes it not well suited for the semantic trees (ST) de-
fined above. For instance, although the two STs of Fig-
ure 1 share most of the subtrees rooted in the ST node,
the kernel defined above computes only one match (ST
ARG0 TARGET ARG1 ARG2) which is not useful.
The critical aspect of the TK function is that the pro-
ductions of two evaluated nodes have to be identical to
allow the match of further descendants. This means that
common substructures cannot be composed by a node
with only some of its children as an effective ST represen-
tation would require. (Moschitti et al., 2007) solve this
problem by designing the Shallow Semantic Tree Kernel
(SSTK) which allows to match portions of a ST. We fol-
lowed the similar approach to compute the SSTK.
</bodyText>
<sectionHeader confidence="0.985826" genericHeader="method">
4 Ranking Sentences
</sectionHeader>
<bodyText confidence="0.997721">
In this section, we describe the scoring techniques in de-
tail.
</bodyText>
<subsectionHeader confidence="0.9593985">
4.1 Learning Feature-weights: A Local Search
Strategy
</subsectionHeader>
<bodyText confidence="0.678728380952381">
In order to fine-tune the weights of the features, we used
a local search technique with simulated annealing to find
the global maximum. Initially, we set all the feature-
weights, w1, · · · , w, as equal values (i.e. 0.5) (see Al-
gorithm 1). Based on the current weights we score the
sentences and generate summaries accordingly. We eval-
uate the summaries using the automatic evaluation tool
ROUGE (Lin, 2004) (described in Section 6) and the
ROUGE value works as the feedback to our learning
loop. Our learning system tries to maximize the ROUGE
score in every step by changing the weights individually
by a specific step size (i.e. 0.01). That means, to learn
weight wi, we change the value of wi keeping all other
weight values (wjdj#i) stagnant. For each weight wi,
the algorithm achieves the local maximum of ROUGE
value. In order to find the global maximum we ran this
algorithm multiple times with different random choices
of initial values (i.e. simulated annealing).
Input: Stepsize l, Weight Initial Value v
Output: A vector w� of learned weights
Initialize the weight values wi to v.
</bodyText>
<equation confidence="0.983935833333333">
for i +— 1 to n do
rg1 = rg2 = prev = 0
while (true) do
scoreSentences(w—)
generateSummaries()
rg2 = evaluateROUGE()
if rg1 G rg2 then
prev = wi
wi+ = l
rg1 = rg2
else
break
end
end
end
return w�
Algorithm 1: Tuning weights using Local Search
technique
</equation>
<bodyText confidence="0.997577666666667">
Once we have learned the feature-weights, our empir-
ical method computes the final scores for the sentences
using the formula:
</bodyText>
<equation confidence="0.992626">
scorei = xi.w (1)
</equation>
<bodyText confidence="0.9769625">
Where, xi is the feature vector for i-th sentence, w� is
the weight vector and scorei is the score of i-th sentence.
</bodyText>
<subsectionHeader confidence="0.96522">
4.2 K-means Learning
</subsectionHeader>
<bodyText confidence="0.998898333333333">
We start with a set of initial cluster centers and go through
several iterations of assigning each object to the cluster
whose center is closest. After all objects have been as-
signed, we recompute the center of each cluster as the
centroid or mean (p) of its members.
Once we have learned the means of the clusters using
the k-means algorithm, our next task is to rank the sen-
tences according to a probability model. We have used
Bayesian model in order to do so. Bayes’ law says:
</bodyText>
<equation confidence="0.9965875">
P(qk|x, o) = p(�x|qk, o)P(qk|o) (2)
EKk=1 p(x|qk, o)p(qk|o)
</equation>
<bodyText confidence="0.9997115">
where qk is a class, x� is a feature vector repre-
senting a sentence and o is the parameter set of all
class models. We set the weights of the clusters as
equiprobable (i.e. P(qk|o) = 1/K). We calculated
</bodyText>
<page confidence="0.998398">
309
</page>
<bodyText confidence="0.999849333333333">
p(x|qk, O) using the gaussian probability distribu-
tion. The gaussian probability density function (pdf)
for the d-dimensional random variable x� is given by:
</bodyText>
<equation confidence="0.996000666666667">
−1
2 (~x−µ)T r−1(~x−µ)
√27rd�&apos;Idet(E)
</equation>
<bodyText confidence="0.9998382">
where µ, the mean vector and E, the covariance
matrix are the parameters of the gaussian distribu-
tion. We get the means (µ) from the k-means algo-
rithm and we calculate the covariance matrix using
the unbiased covariance estimation:
</bodyText>
<equation confidence="0.56669">
(xj − µj)(xi − µi)T (4)
</equation>
<subsectionHeader confidence="0.996219">
4.3 EM Learning
</subsectionHeader>
<bodyText confidence="0.998265272727273">
EM is an iterative two step procedure:
1. Expectation-step and 2. Maximization-step.
In the expectation step, we compute expected values
for the hidden variables hi,j which are cluster mem-
bership probabilities. Given the current parameters,
we compute how likely an object belongs to any
of the clusters. The maximization step computes
the most likely parameters of the model given the
cluster membership probabilities. The data-points
are considered to be generated by a mixture model
of k-gaussians of the form:
</bodyText>
<equation confidence="0.995424">
P(C = i)P(x|µi, Ei) (5)
</equation>
<bodyText confidence="0.999265">
Where the total likelihood of model O with k
components given the observed data points, X =
</bodyText>
<equation confidence="0.939701">
x1,···,xn is:
P(C = j)P(xi|oj)
wjP(xi|µj, Ej)
wjP(xi|µj,Ej)
</equation>
<bodyText confidence="0.999746944444445">
where P is the probability density function (i.e.
eq 3). µj and Ej are the mean and covariance ma-
trix of component j, respectively. Each component
contributes a proportion, wj, of the total population,
such that: K1 wj = 1.
However, a significant problem with the EM al-
gorithm is that it converges to a local maximum
of the likelihood function and hence the quality of
the result depends on the initialization. In order
to get good results from using random starting val-
ues, we can run the EM algorithm several times
and choose the initial configuration for which we
get the maximum log likelihood among all con-
figurations. Choosing the best one among several
runs is very computer intensive process. So, to im-
prove the outcome of the EM algorithm on gaus-
sian mixture models it is necessary to find a better
method of estimating initial means for the compo-
nents. To achieve this aim we explored the widely
used “k-means” algorithm as a cluster (means) find-
ing method. That means, the means found by k-
means clustering above will be utilized as the initial
means for EM and we calculate the initial covari-
ance matrices using the unbiased covariance estima-
tion procedure (eq:4).
Once the sentences are clustered by EM al-
gorithm, we filter out the sentences which are
not query-relevant by checking their probabilities,
P(qr|xi, 0) where, qr denotes the cluster “query-
relevant”. If for a sentence xi, P(qr|xi, 0) &gt; 0.5
then xi is considered to be query-relevant.
Our next task is to rank the query-relevant sen-
tences in order to include them in the summary. This
can be done easily by multiplying the feature vector
xi with the weight vector w� that we learned by the
local search technique (eq:1).
</bodyText>
<sectionHeader confidence="0.973117" genericHeader="method">
5 Redundancy Checking
</sectionHeader>
<bodyText confidence="0.999960333333333">
When many of the competing sentences are included
in the summary, the issue of information overlap be-
tween parts of the output comes up, and a mecha-
nism for addressing redundancy is needed. There-
fore, our summarization systems employ a final level
of analysis: before being added to the final output,
the sentences deemed to be important are compared
to each other and only those that are not too simi-
lar to other candidates are included in the final an-
swer or summary. Following (Zhou et al., 2005), we
modeled this by BE overlap between an intermedi-
ate summary and a to-be-added candidate summary
</bodyText>
<equation confidence="0.9886834">
e
p(µ,r)(x) =
(3)
E� = 1
N − 1
EN
i=1
Ek
i=1
P(x) =
L(E)|X) = �n k
= i=1 E
⇔ �n j=1
i=1 k
En E
i=1 j=1
log
k
E
j=1
</equation>
<page confidence="0.988929">
310
</page>
<bodyText confidence="0.9999018">
sentence. We call this overlap ratio R, where R is
between 0 and 1 inclusively. Setting R = 0.7 means
that a candidate summary sentence, s, can be added
to an intermediate summary, S, if the sentence has a
BE overlap ratio less than or equal to 0.7.
</bodyText>
<sectionHeader confidence="0.984909" genericHeader="method">
6 Experimental Evaluation
</sectionHeader>
<subsectionHeader confidence="0.99701">
6.1 Evaluation Setup
</subsectionHeader>
<bodyText confidence="0.999976844444444">
We used the main task of Document Understanding
Conference (DUC) 2007 for evaluation. The task
was: “Given a complex question (topic description)
and a collection of relevant documents, the task is to
synthesize a fluent, well-organized 250-word sum-
mary of the documents that answers the question(s)
in the topic.”
NIST assessors developed topics of interest to
them and choose a set of 25 documents relevant
(document cluster) to each topic. Each topic and its
document cluster were given to 4 different NIST as-
sessors. The assessor created a 250-word summary
of the document cluster that satisfies the information
need expressed in the topic statement. These multi-
ple “reference summaries” are used in the evaluation
of summary content.
We carried out automatic evaluation of our sum-
maries using ROUGE (Lin, 2004) toolkit, which
has been widely adopted by DUC for automatic
summarization evaluation. It measures summary
quality by counting overlapping units such as the
n-grams (ROUGE-N), word sequences (ROUGE-L
and ROUGE-W) and word pairs (ROUGE-S and
ROUGE-SU) between the candidate summary and
the reference summary. ROUGE parameters were
set as the same as DUC 2007 evaluation setup.
One purpose of our experiments is to study the
impact of different features for complex question
answering task. To accomplish this, we generated
summaries for the topics of DUC 2007 by each of
our seven systems defined as below:
The LEX system generates summaries based on
only lexical features: n-gram (n=1,2,3,4), LCS,
WLCS, skip bi-gram, head, head synonym. The
LSEM system considers only lexical semantic
features: synonym, hypernym/hyponym, gloss,
dependency-based and proximity-based similarity.
The COS system generates summary based on the
graph-based method. The SYS1 system considers
all the features except the BE, syntactic and seman-
tic features. The SYS2 system considers all the fea-
tures except the syntactic and semantic features. The
SYS3 considers all the features except the semantic
and the ALL6 system generates summaries taking
all the features into account.
</bodyText>
<subsectionHeader confidence="0.998102">
6.2 Evaluation Results
</subsectionHeader>
<bodyText confidence="0.99786075">
Table 17 to Table 3, Table 4 to Table 6 and Table 7 to
Table 9 show the evaluation measures for k-means,
EM and empirical approaches respectively. As Ta-
ble 1 shows, in k-means, SYS2 gets 0-21%, SYS3
gets 4-32% and ALL gets 3-36% improvement in
ROUGE-2 scores over the SYS1 system. We get best
ROUGE-W (Table 2) scores for SYS2 (i.e. includ-
ing BE) but SYS3 and ALL do not perform well in
this case. SYS2 improves the ROUGE-W F-score by
1% over SYS1. We do not get any improvement in
ROUGE-SU (Table 3) scores when we include any
kind of syntactic/semantic structures.
The case is different for EM and empirical ap-
proaches. Here, in every case we get a significant
amount of improvement when we include the syn-
tactic and/or semantic features. For EM (Table 4 to
Table 6), the ratio of improvement in F-scores over
SYS1 is: 1-3% for SYS2, 3-15% for SYS3 and 2-
24% for ALL. In our empirical approach (Table 7
to Table 9), SYS2, SYS3 and ALL improve the F-
scores by 3-11%, 7-15% and 8-19% over SYS1 re-
spectively. These results clearly indicate the positive
impact of the syntactic/semantic features for com-
plex question answering task.
</bodyText>
<table confidence="0.9966095">
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.074 0.077 0.086 0.075 0.075 0.078 0.077
P 0.081 0.084 0.093 0.081 0.098 0.107 0.110
F 0.078 0.080 0.089 0.078 0.085 0.090 0.090
</table>
<tableCaption confidence="0.99978">
Table 1: ROUGE-2 measures in k-means learning
</tableCaption>
<bodyText confidence="0.9177045">
Table 10 shows the F-scores of the ROUGE mea-
sures for one baseline system, the best system in
DUC 2007 and our three scoring techniques con-
sidering all features. The baseline system gener-
</bodyText>
<footnote confidence="0.8825785">
6SYS2, SYS3 and ALL systems show the impact of BE,
syntactic and semantic features respectively
7R stands for Recall, P stands for Precision and F stands for
F-score
</footnote>
<page confidence="0.990015">
311
</page>
<table confidence="0.99340125">
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.098 0.097 0.101 0.099 0.101 0.097 0.097
P 0.195 0.194 0.200 0.237 0.233 0.241 0.237
F 0.130 0.129 0.134 0.140 0.141 0.139 0.138
</table>
<tableCaption confidence="0.765154">
Table 2: ROUGE-W measures in k-means learning
</tableCaption>
<table confidence="0.999447">
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.131 0.127 0.139 0.136 0.135 0.135 0.135
P 0.155 0.152 0.162 0.176 0.171 0.174 0.174
F 0.142 0.139 0.150 0.153 0.151 0.152 0.152
</table>
<tableCaption confidence="0.995217">
Table 3: ROUGE-SU in k-means learning
</tableCaption>
<table confidence="0.999144">
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.089 0.080 0.087 0.085 0.085 0.089 0.091
P 0.096 0.087 0.094 0.092 0.095 0.116 0.138
F 0.092 0.083 0.090 0.088 0.090 0.101 0.109
</table>
<tableCaption confidence="0.991312">
Table 4: ROUGE-2 measures in EM learning
</tableCaption>
<table confidence="0.99916525">
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.103 0.096 0.101 0.102 0.101 0.102 0.101
P 0.205 0.193 0.200 0.203 0.218 0.222 0.223
F 0.137 0.128 0.134 0.136 0.138 0.139 0.139
</table>
<tableCaption confidence="0.982542">
Table 5: ROUGE-W measures in EM learning
</tableCaption>
<table confidence="0.9991515">
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.146 0.128 0.138 0.143 0.144 0.145 0.144
P 0.171 0.153 0.162 0.168 0.177 0.186 0.185
F 0.157 0.140 0.149 0.154 0.159 0.163 0.162
</table>
<tableCaption confidence="0.936643">
Table 6: ROUGE-SU measures in EM learning
</tableCaption>
<table confidence="0.9993695">
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.086 0.080 0.087 0.087 0.090 0.095 0.099
P 0.093 0.087 0.094 0.094 0.112 0.115 0.116
F 0.089 0.083 0.090 0.090 0.100 0.104 0.107
</table>
<tableCaption confidence="0.993822">
Table 7: ROUGE-2 in empirical approach
</tableCaption>
<table confidence="0.99891075">
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.102 0.096 0.101 0.102 0.102 0.104 0.105
P 0.203 0.193 0.200 0.204 0.239 0.246 0.247
F 0.135 0.128 0.134 0.137 0.143 0.147 0.148
</table>
<tableCaption confidence="0.995426">
Table 8: ROUGE-W in empirical approach
</tableCaption>
<table confidence="0.9986845">
Score LEX LSEM COS SYS1 SYS2 SYS3 ALL
R 0.144 0.129 0.138 0.145 0.146 0.149 0.150
P 0.169 0.153 0.162 0.171 0.182 0.195 0.197
F 0.155 0.140 0.150 0.157 0.162 0.169 0.170
</table>
<tableCaption confidence="0.999691">
Table 9: ROUGE-SU in empirical approach
</tableCaption>
<bodyText confidence="0.996916846153846">
ates summaries by returning all the leading sen-
tences (up to 250 words) in the (TEXT) field of
the most recent document(s). It shows that the em-
pirical approach outperforms the other two learning
techniques and EM performs better than k-means al-
gorithm. EM improves the F-scores over k-means
by 0.7-22.5%. Empirical approach improves the F-
scores over k-means and EM by 5.9-20.2% and 3.5-
6.5% respectively. Comparing with the DUC 2007
participants our systems achieve top scores and for
some ROUGE measures there is no statistically sig-
nificant difference between our system and the best
DUC 2007 system.
</bodyText>
<table confidence="0.984130142857143">
System ROUGE- ROUGE- ROUGE- ROUGE-
1 2 W SU
Baseline 0.335 0.065 0.114 0.113
Best 0.438 0.122 0.153 0.174
k-means 0.390 0.090 0.138 0.152
EM 0.399 0.109 0.139 0.162
Empirical 0.413 0.107 0.148 0.170
</table>
<tableCaption confidence="0.923847">
Table 10: F-measures for different systems
</tableCaption>
<sectionHeader confidence="0.985812" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999975">
Our experiments show the following: (a) our ap-
proaches achieve promising results, (b) empirical
approach outperforms the other two learning and
EM performs better than the k-means algorithm for
this particular task, and (c) our systems achieve bet-
ter results when we include BE, syntactic and se-
mantic features.
In future, we have the plan to decompose the com-
plex questions into several simple questions before
measuring the similarity between the document sen-
tence and the query sentence. We expect that by de-
composing complex questions into the sets of sub-
questions that they entail, systems can improve the
average quality of answers returned and achieve bet-
ter coverage for the question as a whole.
</bodyText>
<page confidence="0.998015">
312
</page>
<sectionHeader confidence="0.995889" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999966586666667">
M. Collins and N. Duffy. 2001. Convolution Kernels for
Natural Language. In Proceedings of Neural Informa-
tion Processing Systems, pages 625–632, Vancouver,
Canada.
G. Erkan and D. R. Radev. 2004. LexRank: Graph-
based Lexical Centrality as Salience in Text Summa-
rization. Journal of Artificial Intelligence Research,
22:457–479.
K. Hacioglu, S. Pradhan, W. Ward, J. H. Martin, and
D. Jurafsky. 2003. Shallow Semantic Parsing Using
Support Vector Machines. In Technical Report TR-
CSLR-2003-03, University of Colorado.
S. Harabagiu, F. Lacatusu, and A. Hickl. 2006. Answer-
ing complex questions with random walk models. In
Proceedings of the 29th annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval, pages 220 – 227. ACM.
T. Hirao, , J. Suzuki, H. Isozaki, and E. Maeda. 2004.
Dependency-based sentence alignment for multiple
document summarization. In Proceedings of Coling
2004, pages 446–452, Geneva, Switzerland. COLING.
P. Kingsbury and M. Palmer. 2002. From Treebank to
PropBank. In Proceedings of the international con-
ference on Language Resources and Evaluation, Las
Palmas, Spain.
M. Kouylekov and B. Magnini. 2005. Recognizing
textual entailment with tree edit distance algorithms.
In Proceedings of the PASCAL Challenges Workshop:
Recognising Textual Entailment Challenge.
J. Li, L. Sun, C. Kit, and J. Webster. 2007. A Query-
Focused Multi-Document Summarizer Based on Lex-
ical Chains. In Proceedings of the Document Under-
standing Conference, Rochester. NIST.
C. Y. Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. In Proceedings of
Workshop on Text Summarization Branches Out, Post-
Conference Workshop of Association for Computa-
tional Linguistics, pages 74–81, Barcelona, Spain.
B. MacCartney, T. Grenager, M.C. de Marneffe, D. Cer,
and C. D. Manning. 2006. Learning to recognize fea-
tures of valid textual entailments. In Proceedings of
the Human Language Technology Conference of the
North American Chapter of the ACL, page 4148, New
York, USA.
A. Moschitti, S. Quarteroni, R. Basili, and S. Manand-
har. 2007. Exploiting Syntactic and Shallow Seman-
tic Kernels for Question/Answer Classificaion. In Pro-
ceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 776–783, Prague,
Czech Republic. ACL.
J. Otterbacher, G. Erkan, and D. R. Radev. 2005. Us-
ing Random Walks for Question-focused Sentence Re-
trieval. In Proceedings of Human Language Technol-
ogy Conference and Conference on Empirical Meth-
ods in Natural Language Processing, pages 915–922,
Vancouver, Canada.
P. Pingali, Rahul K., and V. Varma. 2007. IIIT Hyder-
abad at DUC 2007. In Proceedings of the Document
Understanding Conference, Rochester. NIST.
V. Punyakanok, D. Roth, and W. Yih. 2004. Mapping de-
pendencies trees: An application to question answer-
ing. In Proceedings ofAI &amp; Math, Florida, USA.
K. Toutanova, C. Brockett, M. Gamon, J. Jagarlamudi,
H. Suzuki, and L. Vanderwende. 2007. The PYTHY
Summarization System: Microsoft Research at DUC
2007 . In proceedings of the Document Understanding
Conference, Rochester. NIST.
D. Zhang and W. S. Lee. 2003. A Language Mod-
eling Approach to Passage Question Answering. In
Proceedings of the Twelfth Text REtreival Conference,
pages 489–495, Gaithersburg, Maryland.
L. Zhou, C. Y. Lin, and E. Hovy. 2005. A BE-based
Multi-dccument Summarizer with Query Interpreta-
tion. In Proceedings of Document Understanding
Conference, Vancouver, B.C., Canada.
</reference>
<page confidence="0.999514">
313
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.806984">
<title confidence="0.999661">Selecting Sentences for Answering Complex Questions</title>
<author confidence="0.943319">Yllias</author>
<affiliation confidence="0.9860305">University of 4401 University</affiliation>
<address confidence="0.999722">Lethbridge, Alberta, Canada, T1K</address>
<email confidence="0.989302">chali@cs.uleth.ca</email>
<author confidence="0.94043">R Shafiq</author>
<affiliation confidence="0.999861">University of British</affiliation>
<address confidence="0.9832465">2366 Main Vancouver, B.C. Canada V6T</address>
<email confidence="0.99252">rjoty@cs.ubc.ca</email>
<abstract confidence="0.999296875">Complex questions that require inferencing and synthesizing information from multiple documents can be seen as a kind of topicoriented, informative multi-document summarization. In this paper, we have experimented with one empirical and two unsupervised statistical machine learning techniques: kmeans and Expectation Maximization (EM), for computing relative importance of the sentences. However, the performance of these approaches depends entirely on the feature set used and the weighting of these features. We extracted different kinds of features (i.e. lexical, lexical semantic, cosine similarity, basic element, tree kernel based syntactic and shallow-semantic) for each of the document sentences in order to measure its importance and relevancy to the user query. We used a local search technique to learn the weights of the features. For all our methods of generating summaries, we have shown the effects of syntactic and shallow-semantic features over the bag of words (BOW) features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>N Duffy</author>
</authors>
<title>Convolution Kernels for Natural Language.</title>
<date>2001</date>
<booktitle>In Proceedings of Neural Information Processing Systems,</booktitle>
<pages>625--632</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="19510" citStr="Collins and Duffy, 2001" startWordPosition="3153" endWordPosition="3156">nt BEs at the top which may or may not be relevant to the user questions. We filter those BEs by checking whether they contain any word which is a query word or a QueryRelatedWords (defined in Section 3.2). The score of a sentence is the sum of its BE scores divided by the number of BEs in the sentence. 3.5.2 Syntactic Feature Encoding syntactic structure is easier and straight forward. Given a sentence (or query), we first parse it into a syntactic tree using a syntactic parser (i.e. Charniak parser) and then we calculate the similarity between the two trees using the tree kernel defined in (Collins and Duffy, 2001). 3.5.3 Shallow-semantic Feature Though introducing BE and syntactic information gives an improvement on BOW by the use of dependency/syntactic parses, but these, too are not adequate when dealing with complex questions whose answers are expressed by long and articulated sentences or even 5BE website:http://www.isi.edu/ cyl/BE Figure 1: Example of semantic trees paragraphs. Shallow semantic representations, bearing a more compact information, could prevent the sparseness of deep structural approaches and the weakness of BOW models (Moschitti et al., 2007). Initiatives such as PropBank (PB) (Ki</context>
<context position="21642" citStr="Collins and Duffy, 2001" startWordPosition="3493" endWordPosition="3496">y uses the Italian lira as their currency.” gives the STs as in Figure 2. As we can see in Figure 2(A), when an argument node corresponds to an entire subordinate clause, we label its leaf with ST, e.g. the leaf of ARG0. Such ST node is actually the root of the subordinate clause in Figure 2(B). If taken separately, such STs do not express the whole meaning of the sentence, hence it is more accurate to define a single structure encoding the dependency between the two predicates as in Figure 2(C). We refer to this kind of nested STs as STNs. Note that, the tree kernel (TK) function defined in (Collins and Duffy, 2001) computes the number of common subtrees between two trees. Such subtrees are subject to the constraint that their nodes are taken with all or none of the children they have in the original tree. 308 Figure 2: Two STs composing a STN Though, this definition of subtrees makes the TK function appropriate for syntactic trees but at the same time makes it not well suited for the semantic trees (ST) defined above. For instance, although the two STs of Figure 1 share most of the subtrees rooted in the ST node, the kernel defined above computes only one match (ST ARG0 TARGET ARG1 ARG2) which is not us</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>M. Collins and N. Duffy. 2001. Convolution Kernels for Natural Language. In Proceedings of Neural Information Processing Systems, pages 625–632, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Erkan</author>
<author>D R Radev</author>
</authors>
<title>LexRank: Graphbased Lexical Centrality as Salience in Text Summarization.</title>
<date>2004</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>22--457</pages>
<contexts>
<context position="4845" citStr="Erkan and Radev, 2004" startWordPosition="740" endWordPosition="743">low-semantic features over the BOW features. This paper is organized as follows: Section 2 focuses on the related work, Section 3 describes how the features are extracted, Section 4 discusses the scoring approaches, Section 5 discusses how we remove the redundant sentences before adding them to the summary, Section 6 describes our experimental study. We conclude and give future directions in Section 7. 2 Related Work Researchers all over the world working on querybased summarization are trying different directions to see which methods provide the best results. The LexRank method addressed in (Erkan and Radev, 2004) was very successful in generic multi-document summarization. A topic-sensitive LexRank is proposed in (Otterbacher et al., 2005). As in LexRank, the set of sentences in a document cluster is represented as a graph, where nodes are sentences and links between the nodes are induced by a similarity relation between the sentences. Then the system ranked the sentences according to a random walk model defined in terms of both the intersentence similarities and the similarities of the sentences to the topic description or question. The summarization methods based on lexical chain first extract the n</context>
<context position="16819" citStr="Erkan and Radev, 2004" startWordPosition="2705" endWordPosition="2708">t, the cluster that has more words common with the QueryRelatedWords is the right cluster. We chose the cluster for a word which has the highest overlap score. Once we get the clusters for the query words, we measured the overlap between the cluster words and the sentence words as follows: Pw1ESenW ords Countmatch(w1) Measure = P Count(w1) w1 E S enW ords Where, SenWords is the set of important words extracted from the sentence and Countmat,h is the number of matches between the sentence words and the clusters of similar words of the query words. 3.4 Graph-based Similarity Measure In LexRank (Erkan and Radev, 2004), the concept of graph-based centrality is used to rank a set of sentences, in producing generic multi-document summaries. A similarity graph is produced for the sentences in the document collection. In the graph, each node represents a sentence. The edges between the nodes measure the cosine similarity between the respective pair of sentences. The degree of a given node is an indication of how much important the sentence is. Once the similarity graph is 4http://www.cs.ualberta.ca/ lindek/downloads.htm 307 constructed, the sentences are then ranked according to their eigenvector centrality. To</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G. Erkan and D. R. Radev. 2004. LexRank: Graphbased Lexical Centrality as Salience in Text Summarization. Journal of Artificial Intelligence Research, 22:457–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hacioglu</author>
<author>S Pradhan</author>
<author>W Ward</author>
<author>J H Martin</author>
<author>D Jurafsky</author>
</authors>
<title>Shallow Semantic Parsing Using Support Vector Machines. In</title>
<date>2003</date>
<tech>Technical Report TRCSLR-2003-03,</tech>
<institution>University of Colorado.</institution>
<contexts>
<context position="20260" citStr="Hacioglu et al., 2003" startWordPosition="3260" endWordPosition="3263">dency/syntactic parses, but these, too are not adequate when dealing with complex questions whose answers are expressed by long and articulated sentences or even 5BE website:http://www.isi.edu/ cyl/BE Figure 1: Example of semantic trees paragraphs. Shallow semantic representations, bearing a more compact information, could prevent the sparseness of deep structural approaches and the weakness of BOW models (Moschitti et al., 2007). Initiatives such as PropBank (PB) (Kingsbury and Palmer, 2002) have made possible the design of accurate automatic Semantic Role Labeling (SRL) systems like ASSERT (Hacioglu et al., 2003). For example, consider the PB annotation: [ARG0 all][TARGET use][ARG1 the french franc][ARG2 as their currency] Such annotation can be used to design a shallow semantic representation that can be matched against other semantically similar sentences, e.g. [ARG0 the Vatican][TARGET use][ARG1 the Italian lira][ARG2 as their currency] In order to calculate the semantic similarity between the sentences, we first represent the annotated sentence using the tree structures like Figure 1 which we call Semantic Tree (ST). In the semantic tree, arguments are replaced with the most important word-often r</context>
</contexts>
<marker>Hacioglu, Pradhan, Ward, Martin, Jurafsky, 2003</marker>
<rawString>K. Hacioglu, S. Pradhan, W. Ward, J. H. Martin, and D. Jurafsky. 2003. Shallow Semantic Parsing Using Support Vector Machines. In Technical Report TRCSLR-2003-03, University of Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>F Lacatusu</author>
<author>A Hickl</author>
</authors>
<title>Answering complex questions with random walk models.</title>
<date>2006</date>
<booktitle>In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>220--227</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6010" citStr="Harabagiu et al., 2006" startWordPosition="929" endWordPosition="932">ization methods based on lexical chain first extract the nouns, compound nouns and named entities as candidate words (Li et al., 2007). Then using WordNet, the systems find the semantic similarity between the nouns and compound nouns. After that, lexical chains are built in two steps: 1) Building single document strong chains while disambiguating the senses of the words and, 2) building multi-chain by merging the strongest chains of the single documents into one chain. The systems rank sentences using a formula that involves a) the lexical chain, b) keywords from query and c) named entities. (Harabagiu et al., 2006) introduce a new paradigm for processing complex questions that relies on a combination of (a) question decompositions; (b) factoid QA techniques; and (c) Multi-Document Summarization (MDS) techniques. The question decomposition procedure operates on a Marcov chain, by following a random walk with mixture model on a bipartite graph of relations established between concepts related to the topic of a complex question and subquestions derived from topic-relevant passages that manifest these relations. Decomposed questions are then submitted to a state-of-the-art QA system in order to retrieve a s</context>
</contexts>
<marker>Harabagiu, Lacatusu, Hickl, 2006</marker>
<rawString>S. Harabagiu, F. Lacatusu, and A. Hickl. 2006. Answering complex questions with random walk models. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 220 – 227. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Suzuki</author>
<author>H Isozaki</author>
<author>E Maeda</author>
</authors>
<title>Dependency-based sentence alignment for multiple document summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of Coling</booktitle>
<pages>446--452</pages>
<location>Geneva, Switzerland. COLING.</location>
<marker>Suzuki, Isozaki, Maeda, 2004</marker>
<rawString>T. Hirao, , J. Suzuki, H. Isozaki, and E. Maeda. 2004. Dependency-based sentence alignment for multiple document summarization. In Proceedings of Coling 2004, pages 446–452, Geneva, Switzerland. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Kingsbury</author>
<author>M Palmer</author>
</authors>
<title>From Treebank to PropBank.</title>
<date>2002</date>
<booktitle>In Proceedings of the international conference on Language Resources and Evaluation,</booktitle>
<location>Las Palmas,</location>
<contexts>
<context position="20135" citStr="Kingsbury and Palmer, 2002" startWordPosition="3241" endWordPosition="3244">1). 3.5.3 Shallow-semantic Feature Though introducing BE and syntactic information gives an improvement on BOW by the use of dependency/syntactic parses, but these, too are not adequate when dealing with complex questions whose answers are expressed by long and articulated sentences or even 5BE website:http://www.isi.edu/ cyl/BE Figure 1: Example of semantic trees paragraphs. Shallow semantic representations, bearing a more compact information, could prevent the sparseness of deep structural approaches and the weakness of BOW models (Moschitti et al., 2007). Initiatives such as PropBank (PB) (Kingsbury and Palmer, 2002) have made possible the design of accurate automatic Semantic Role Labeling (SRL) systems like ASSERT (Hacioglu et al., 2003). For example, consider the PB annotation: [ARG0 all][TARGET use][ARG1 the french franc][ARG2 as their currency] Such annotation can be used to design a shallow semantic representation that can be matched against other semantically similar sentences, e.g. [ARG0 the Vatican][TARGET use][ARG1 the Italian lira][ARG2 as their currency] In order to calculate the semantic similarity between the sentences, we first represent the annotated sentence using the tree structures like</context>
</contexts>
<marker>Kingsbury, Palmer, 2002</marker>
<rawString>P. Kingsbury and M. Palmer. 2002. From Treebank to PropBank. In Proceedings of the international conference on Language Resources and Evaluation, Las Palmas, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kouylekov</author>
<author>B Magnini</author>
</authors>
<title>Recognizing textual entailment with tree edit distance algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of the PASCAL Challenges Workshop: Recognising Textual Entailment Challenge.</booktitle>
<contexts>
<context position="8715" citStr="Kouylekov and Magnini, 2005" startWordPosition="1348" endWordPosition="1351">rees) to represent the text and the hypothesis. Then they try to find a good partial alignment between the typed dependency graphs representing the hypothesis and the text in a search space of O((m + 1)n) 305 where hypothesis graph contains n nodes and a text graph contains m nodes. (Hirao et al., 2004) represent the sentences using Dependency Tree Path (DTP) to incorporate syntactic information. They apply String Subsequence Kernel (SSK) to measure the similarity between the DTPs of two sentences. They also introduce Extended String Subsequence Kernel (ESK) to incorporate semantics in DTPs. (Kouylekov and Magnini, 2005) use the tree edit distance algorithms on the dependency trees of the text and the hypothesis to recognize the textual entailment. According to this approach, a text T entails a hypothesis H if there exists a sequence of transformations (i.e. deletion, insertion and substitution) applied to T such that we can obtain H with an overall cost below a certain threshold. (Punyakanok et al., 2004) represent the question and the sentence containing answer with their dependency trees. They add semantic information (i.e. named entity, synonyms and other related words) in the dependency trees. They apply</context>
</contexts>
<marker>Kouylekov, Magnini, 2005</marker>
<rawString>M. Kouylekov and B. Magnini. 2005. Recognizing textual entailment with tree edit distance algorithms. In Proceedings of the PASCAL Challenges Workshop: Recognising Textual Entailment Challenge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Li</author>
<author>L Sun</author>
<author>C Kit</author>
<author>J Webster</author>
</authors>
<title>A QueryFocused Multi-Document Summarizer Based on Lexical Chains.</title>
<date>2007</date>
<booktitle>In Proceedings of the Document Understanding Conference,</booktitle>
<location>Rochester. NIST.</location>
<contexts>
<context position="5521" citStr="Li et al., 2007" startWordPosition="849" endWordPosition="852">. A topic-sensitive LexRank is proposed in (Otterbacher et al., 2005). As in LexRank, the set of sentences in a document cluster is represented as a graph, where nodes are sentences and links between the nodes are induced by a similarity relation between the sentences. Then the system ranked the sentences according to a random walk model defined in terms of both the intersentence similarities and the similarities of the sentences to the topic description or question. The summarization methods based on lexical chain first extract the nouns, compound nouns and named entities as candidate words (Li et al., 2007). Then using WordNet, the systems find the semantic similarity between the nouns and compound nouns. After that, lexical chains are built in two steps: 1) Building single document strong chains while disambiguating the senses of the words and, 2) building multi-chain by merging the strongest chains of the single documents into one chain. The systems rank sentences using a formula that involves a) the lexical chain, b) keywords from query and c) named entities. (Harabagiu et al., 2006) introduce a new paradigm for processing complex questions that relies on a combination of (a) question decompo</context>
</contexts>
<marker>Li, Sun, Kit, Webster, 2007</marker>
<rawString>J. Li, L. Sun, C. Kit, and J. Webster. 2007. A QueryFocused Multi-Document Summarizer Based on Lexical Chains. In Proceedings of the Document Understanding Conference, Rochester. NIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Y Lin</author>
</authors>
<title>ROUGE: A Package for Automatic Evaluation of Summaries.</title>
<date>2004</date>
<booktitle>In Proceedings of Workshop on Text Summarization Branches Out, PostConference Workshop of Association for Computational Linguistics,</booktitle>
<pages>74--81</pages>
<location>Barcelona,</location>
<contexts>
<context position="12129" citStr="Lin, 2004" startWordPosition="1931" endWordPosition="1932">ence P. 3.1.2 LCS, WLCS and Skip-Bigram A sequence W = [w1, w2, ..., wn] is a subsequence of another sequence X = [x1, x2, ..., xm], if there exists a strict increasing sequence [i1, i2, ..., id of indices of X such that for all j = 1,2, ..., k we have xis = wj. Given two sequences, 51 and 52, the Longest Common Subsequence (LCS) of 51 and 52 is a common subsequence with maximum length. The longer the LCS of two sentences is, the more similar the two sentences are. The basic LCS has a problem that it does not differentiate LCSes of different spatial relations within their embedding sequences (Lin, 2004). To improve the basic LCS method, we can remember the length of consecutive matches encountered so far to a regular two dimensional dynamic program table computing LCS. We call this weighted LCS (WLCS) and use k to indicate the length of the current consecutive matches ending at words xi and yj. Given two sentences X and Y, the WLCS score of X and Y can be computed using the similar dynamic programming procedure as stated in (Lin, 2004). We computed the LCS and WLCS-based F-measure following (Lin, 2004) using both the query pool and the sentence pool as in the previous section. Skip-bigram is</context>
<context position="23279" citStr="Lin, 2004" startWordPosition="3785" endWordPosition="3786">ns of a ST. We followed the similar approach to compute the SSTK. 4 Ranking Sentences In this section, we describe the scoring techniques in detail. 4.1 Learning Feature-weights: A Local Search Strategy In order to fine-tune the weights of the features, we used a local search technique with simulated annealing to find the global maximum. Initially, we set all the featureweights, w1, · · · , w, as equal values (i.e. 0.5) (see Algorithm 1). Based on the current weights we score the sentences and generate summaries accordingly. We evaluate the summaries using the automatic evaluation tool ROUGE (Lin, 2004) (described in Section 6) and the ROUGE value works as the feedback to our learning loop. Our learning system tries to maximize the ROUGE score in every step by changing the weights individually by a specific step size (i.e. 0.01). That means, to learn weight wi, we change the value of wi keeping all other weight values (wjdj#i) stagnant. For each weight wi, the algorithm achieves the local maximum of ROUGE value. In order to find the global maximum we ran this algorithm multiple times with different random choices of initial values (i.e. simulated annealing). Input: Stepsize l, Weight Initial</context>
<context position="29953" citStr="Lin, 2004" startWordPosition="4946" endWordPosition="4947">hesize a fluent, well-organized 250-word summary of the documents that answers the question(s) in the topic.” NIST assessors developed topics of interest to them and choose a set of 25 documents relevant (document cluster) to each topic. Each topic and its document cluster were given to 4 different NIST assessors. The assessor created a 250-word summary of the document cluster that satisfies the information need expressed in the topic statement. These multiple “reference summaries” are used in the evaluation of summary content. We carried out automatic evaluation of our summaries using ROUGE (Lin, 2004) toolkit, which has been widely adopted by DUC for automatic summarization evaluation. It measures summary quality by counting overlapping units such as the n-grams (ROUGE-N), word sequences (ROUGE-L and ROUGE-W) and word pairs (ROUGE-S and ROUGE-SU) between the candidate summary and the reference summary. ROUGE parameters were set as the same as DUC 2007 evaluation setup. One purpose of our experiments is to study the impact of different features for complex question answering task. To accomplish this, we generated summaries for the topics of DUC 2007 by each of our seven systems defined as b</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>C. Y. Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Proceedings of Workshop on Text Summarization Branches Out, PostConference Workshop of Association for Computational Linguistics, pages 74–81, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B MacCartney</author>
<author>T Grenager</author>
<author>M C de Marneffe</author>
<author>D Cer</author>
<author>C D Manning</author>
</authors>
<title>Learning to recognize features of valid textual entailments.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL,</booktitle>
<pages>4148</pages>
<location>New York, USA.</location>
<marker>MacCartney, Grenager, de Marneffe, Cer, Manning, 2006</marker>
<rawString>B. MacCartney, T. Grenager, M.C. de Marneffe, D. Cer, and C. D. Manning. 2006. Learning to recognize features of valid textual entailments. In Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, page 4148, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
<author>S Quarteroni</author>
<author>R Basili</author>
<author>S Manandhar</author>
</authors>
<title>Exploiting Syntactic and Shallow Semantic Kernels for Question/Answer Classificaion.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>776--783</pages>
<publisher>ACL.</publisher>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3435" citStr="Moschitti et al., 2007" startWordPosition="517" endWordPosition="521">use of more complex semantics, the approaches based on only BOW are often inadequate to perform fine-level textual analysis. Some improvements on BOW are given by the use of dependency trees and syntactic parse trees (Hirao et al., 2004), (Punyakanok et al., 2004), (Zhang and Lee, 2003), but these, too are not adequate when dealing with complex questions whose answers are expressed by long and articulated sentences or even paragraphs. Shallow semantic representations, bearing a more compact information, could prevent the sparseness of deep structural approaches and the weakness of BOW models (Moschitti et al., 2007). Attempting an application of 304 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 304–313, Honolulu, October 2008.c�2008 Association for Computational Linguistics syntactic and semantic information to complex QA hence seems natural, as pinpointing the answer to a question relies on a deep understanding of the semantics of both. In more complex tasks such as computing the relatedness between the query sentences and the document sentences in order to generate query-focused summaries (or answers to complex questions), to our knowledge no study uses t</context>
<context position="20071" citStr="Moschitti et al., 2007" startWordPosition="3232" endWordPosition="3235">ees using the tree kernel defined in (Collins and Duffy, 2001). 3.5.3 Shallow-semantic Feature Though introducing BE and syntactic information gives an improvement on BOW by the use of dependency/syntactic parses, but these, too are not adequate when dealing with complex questions whose answers are expressed by long and articulated sentences or even 5BE website:http://www.isi.edu/ cyl/BE Figure 1: Example of semantic trees paragraphs. Shallow semantic representations, bearing a more compact information, could prevent the sparseness of deep structural approaches and the weakness of BOW models (Moschitti et al., 2007). Initiatives such as PropBank (PB) (Kingsbury and Palmer, 2002) have made possible the design of accurate automatic Semantic Role Labeling (SRL) systems like ASSERT (Hacioglu et al., 2003). For example, consider the PB annotation: [ARG0 all][TARGET use][ARG1 the french franc][ARG2 as their currency] Such annotation can be used to design a shallow semantic representation that can be matched against other semantically similar sentences, e.g. [ARG0 the Vatican][TARGET use][ARG1 the Italian lira][ARG2 as their currency] In order to calculate the semantic similarity between the sentences, we first</context>
<context position="22568" citStr="Moschitti et al., 2007" startWordPosition="3662" endWordPosition="3665">for syntactic trees but at the same time makes it not well suited for the semantic trees (ST) defined above. For instance, although the two STs of Figure 1 share most of the subtrees rooted in the ST node, the kernel defined above computes only one match (ST ARG0 TARGET ARG1 ARG2) which is not useful. The critical aspect of the TK function is that the productions of two evaluated nodes have to be identical to allow the match of further descendants. This means that common substructures cannot be composed by a node with only some of its children as an effective ST representation would require. (Moschitti et al., 2007) solve this problem by designing the Shallow Semantic Tree Kernel (SSTK) which allows to match portions of a ST. We followed the similar approach to compute the SSTK. 4 Ranking Sentences In this section, we describe the scoring techniques in detail. 4.1 Learning Feature-weights: A Local Search Strategy In order to fine-tune the weights of the features, we used a local search technique with simulated annealing to find the global maximum. Initially, we set all the featureweights, w1, · · · , w, as equal values (i.e. 0.5) (see Algorithm 1). Based on the current weights we score the sentences and </context>
</contexts>
<marker>Moschitti, Quarteroni, Basili, Manandhar, 2007</marker>
<rawString>A. Moschitti, S. Quarteroni, R. Basili, and S. Manandhar. 2007. Exploiting Syntactic and Shallow Semantic Kernels for Question/Answer Classificaion. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 776–783, Prague, Czech Republic. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Otterbacher</author>
<author>G Erkan</author>
<author>D R Radev</author>
</authors>
<title>Using Random Walks for Question-focused Sentence Retrieval.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>915--922</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="4974" citStr="Otterbacher et al., 2005" startWordPosition="757" endWordPosition="760">n 3 describes how the features are extracted, Section 4 discusses the scoring approaches, Section 5 discusses how we remove the redundant sentences before adding them to the summary, Section 6 describes our experimental study. We conclude and give future directions in Section 7. 2 Related Work Researchers all over the world working on querybased summarization are trying different directions to see which methods provide the best results. The LexRank method addressed in (Erkan and Radev, 2004) was very successful in generic multi-document summarization. A topic-sensitive LexRank is proposed in (Otterbacher et al., 2005). As in LexRank, the set of sentences in a document cluster is represented as a graph, where nodes are sentences and links between the nodes are induced by a similarity relation between the sentences. Then the system ranked the sentences according to a random walk model defined in terms of both the intersentence similarities and the similarities of the sentences to the topic description or question. The summarization methods based on lexical chain first extract the nouns, compound nouns and named entities as candidate words (Li et al., 2007). Then using WordNet, the systems find the semantic s</context>
<context position="17537" citStr="Otterbacher et al., 2005" startWordPosition="2815" endWordPosition="2818">ric multi-document summaries. A similarity graph is produced for the sentences in the document collection. In the graph, each node represents a sentence. The edges between the nodes measure the cosine similarity between the respective pair of sentences. The degree of a given node is an indication of how much important the sentence is. Once the similarity graph is 4http://www.cs.ualberta.ca/ lindek/downloads.htm 307 constructed, the sentences are then ranked according to their eigenvector centrality. To apply LexRank to queryfocused context, a topic-sensitive version of LexRank is proposed in (Otterbacher et al., 2005). We followed a similar approach in order to calculate this feature. The score of a sentence is determined by a mixture model of the relevance of the sentence to the query and the similarity of the sentence to other high-scoring sentences. 3.5 Syntactic and Semantic Features: So far, we have included the features of type Bag of Words (BOW). The task like query-based summarization that requires the use of more complex syntactic and semantics, the approaches with only BOW are often inadequate to perform fine-level textual analysis. We extracted three features that incorporate syntactic/semantic </context>
</contexts>
<marker>Otterbacher, Erkan, Radev, 2005</marker>
<rawString>J. Otterbacher, G. Erkan, and D. R. Radev. 2005. Using Random Walks for Question-focused Sentence Retrieval. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 915–922, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pingali</author>
<author>K Rahul</author>
<author>V Varma</author>
</authors>
<title>IIIT Hyderabad at DUC</title>
<date>2007</date>
<booktitle>In Proceedings of the Document Understanding Conference,</booktitle>
<location>Rochester. NIST.</location>
<contexts>
<context position="6944" citStr="Pingali et al., 2007" startWordPosition="1071" endWordPosition="1074">on a bipartite graph of relations established between concepts related to the topic of a complex question and subquestions derived from topic-relevant passages that manifest these relations. Decomposed questions are then submitted to a state-of-the-art QA system in order to retrieve a set of passages that can later be merged into a comprehensive answer by a MDS system. They show that question decompositions using this method can significantly enhance the relevance and comprehensiveness of summary-length answers to complex questions. There are approaches that are based on probabilistic models (Pingali et al., 2007) (Toutanova et al., 2007). (Pingali et al., 2007) rank the sentences based on a mixture model where each component of the model is a statistical model: Score(s) = αxQIScore(s)+(1−α)xQFocus(s, Q) Where, Score(s) is the score for sentence s. Queryindependent score (QIScore) and query-dependent score (QFocus) are calculated based on probabilistic models. (Toutanova et al., 2007) learns a log-linear sentence ranking model by maximizing three metrics of sentence goodness: (a) ROUGE oracle, (b) Pyramid-derived, and (c) Model Frequency. The scoring function is learned by fitting weights for a set of </context>
</contexts>
<marker>Pingali, Rahul, Varma, 2007</marker>
<rawString>P. Pingali, Rahul K., and V. Varma. 2007. IIIT Hyderabad at DUC 2007. In Proceedings of the Document Understanding Conference, Rochester. NIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>Mapping dependencies trees: An application to question answering.</title>
<date>2004</date>
<booktitle>In Proceedings ofAI &amp; Math,</booktitle>
<location>Florida, USA.</location>
<contexts>
<context position="3076" citStr="Punyakanok et al., 2004" startWordPosition="460" endWordPosition="463">and shallow-semantic) for each of the document sentences in order to measure its importance and relevancy to the user query. We have used a gradient descent local search technique to learn the weights of the features. Traditionally, information extraction techniques are based on the BOW approach augmented by language modeling. But when the task requires the use of more complex semantics, the approaches based on only BOW are often inadequate to perform fine-level textual analysis. Some improvements on BOW are given by the use of dependency trees and syntactic parse trees (Hirao et al., 2004), (Punyakanok et al., 2004), (Zhang and Lee, 2003), but these, too are not adequate when dealing with complex questions whose answers are expressed by long and articulated sentences or even paragraphs. Shallow semantic representations, bearing a more compact information, could prevent the sparseness of deep structural approaches and the weakness of BOW models (Moschitti et al., 2007). Attempting an application of 304 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 304–313, Honolulu, October 2008.c�2008 Association for Computational Linguistics syntactic and semantic informat</context>
<context position="9108" citStr="Punyakanok et al., 2004" startWordPosition="1414" endWordPosition="1417">n. They apply String Subsequence Kernel (SSK) to measure the similarity between the DTPs of two sentences. They also introduce Extended String Subsequence Kernel (ESK) to incorporate semantics in DTPs. (Kouylekov and Magnini, 2005) use the tree edit distance algorithms on the dependency trees of the text and the hypothesis to recognize the textual entailment. According to this approach, a text T entails a hypothesis H if there exists a sequence of transformations (i.e. deletion, insertion and substitution) applied to T such that we can obtain H with an overall cost below a certain threshold. (Punyakanok et al., 2004) represent the question and the sentence containing answer with their dependency trees. They add semantic information (i.e. named entity, synonyms and other related words) in the dependency trees. They apply the approximate tree matching in order to decide how similar any given pair of trees are. They also use the edit distance as the matching criteria in the approximate tree matching. All these methods show the improvement over the BOW scoring methods. Our Basic Element (BE)-based feature used the dependency tree to extract the BEs (i.e. head-modifier-relation) and ranked the BEs based on the</context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2004</marker>
<rawString>V. Punyakanok, D. Roth, and W. Yih. 2004. Mapping dependencies trees: An application to question answering. In Proceedings ofAI &amp; Math, Florida, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>C Brockett</author>
<author>M Gamon</author>
<author>J Jagarlamudi</author>
<author>H Suzuki</author>
<author>L Vanderwende</author>
</authors>
<title></title>
<date>2007</date>
<booktitle>The PYTHY Summarization System: Microsoft Research at DUC</booktitle>
<location>Rochester. NIST.</location>
<contexts>
<context position="6969" citStr="Toutanova et al., 2007" startWordPosition="1075" endWordPosition="1078"> relations established between concepts related to the topic of a complex question and subquestions derived from topic-relevant passages that manifest these relations. Decomposed questions are then submitted to a state-of-the-art QA system in order to retrieve a set of passages that can later be merged into a comprehensive answer by a MDS system. They show that question decompositions using this method can significantly enhance the relevance and comprehensiveness of summary-length answers to complex questions. There are approaches that are based on probabilistic models (Pingali et al., 2007) (Toutanova et al., 2007). (Pingali et al., 2007) rank the sentences based on a mixture model where each component of the model is a statistical model: Score(s) = αxQIScore(s)+(1−α)xQFocus(s, Q) Where, Score(s) is the score for sentence s. Queryindependent score (QIScore) and query-dependent score (QFocus) are calculated based on probabilistic models. (Toutanova et al., 2007) learns a log-linear sentence ranking model by maximizing three metrics of sentence goodness: (a) ROUGE oracle, (b) Pyramid-derived, and (c) Model Frequency. The scoring function is learned by fitting weights for a set of feature functions of sent</context>
</contexts>
<marker>Toutanova, Brockett, Gamon, Jagarlamudi, Suzuki, Vanderwende, 2007</marker>
<rawString>K. Toutanova, C. Brockett, M. Gamon, J. Jagarlamudi, H. Suzuki, and L. Vanderwende. 2007. The PYTHY Summarization System: Microsoft Research at DUC 2007 . In proceedings of the Document Understanding Conference, Rochester. NIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zhang</author>
<author>W S Lee</author>
</authors>
<title>A Language Modeling Approach to Passage Question Answering.</title>
<date>2003</date>
<booktitle>In Proceedings of the Twelfth Text REtreival Conference,</booktitle>
<pages>489--495</pages>
<location>Gaithersburg, Maryland.</location>
<contexts>
<context position="3099" citStr="Zhang and Lee, 2003" startWordPosition="464" endWordPosition="467">ach of the document sentences in order to measure its importance and relevancy to the user query. We have used a gradient descent local search technique to learn the weights of the features. Traditionally, information extraction techniques are based on the BOW approach augmented by language modeling. But when the task requires the use of more complex semantics, the approaches based on only BOW are often inadequate to perform fine-level textual analysis. Some improvements on BOW are given by the use of dependency trees and syntactic parse trees (Hirao et al., 2004), (Punyakanok et al., 2004), (Zhang and Lee, 2003), but these, too are not adequate when dealing with complex questions whose answers are expressed by long and articulated sentences or even paragraphs. Shallow semantic representations, bearing a more compact information, could prevent the sparseness of deep structural approaches and the weakness of BOW models (Moschitti et al., 2007). Attempting an application of 304 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 304–313, Honolulu, October 2008.c�2008 Association for Computational Linguistics syntactic and semantic information to complex QA hence</context>
</contexts>
<marker>Zhang, Lee, 2003</marker>
<rawString>D. Zhang and W. S. Lee. 2003. A Language Modeling Approach to Passage Question Answering. In Proceedings of the Twelfth Text REtreival Conference, pages 489–495, Gaithersburg, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zhou</author>
<author>C Y Lin</author>
<author>E Hovy</author>
</authors>
<title>A BE-based Multi-dccument Summarizer with Query Interpretation.</title>
<date>2005</date>
<booktitle>In Proceedings of Document Understanding Conference,</booktitle>
<location>Vancouver, B.C.,</location>
<contexts>
<context position="18496" citStr="Zhou et al., 2005" startWordPosition="2967" endWordPosition="2970">. The task like query-based summarization that requires the use of more complex syntactic and semantics, the approaches with only BOW are often inadequate to perform fine-level textual analysis. We extracted three features that incorporate syntactic/semantic information. 3.5.1 Basic Element (BE) Overlap Measure The “head-modifier-relation” triples, extracted from the dependency trees are considered as BEs in our experiment. The triples encode some syntactic/semantic information and one can quite easily decide whether any two units match or not- considerably more easily than with longer units (Zhou et al., 2005). We used the BE package distributed by ISI5 to extract the BEs for the sentences. Once we get the BEs for a sentence, we computed the Likelihood Ratio (LR) for each BE following (Zhou et al., 2005). Sorting BEs according to their LR scores produced a BE-ranked list. Our goal is to generate a summary that will answer the user questions. The ranked list of BEs in this way contains important BEs at the top which may or may not be relevant to the user questions. We filter those BEs by checking whether they contain any word which is a query word or a QueryRelatedWords (defined in Section 3.2). The</context>
<context position="28622" citStr="Zhou et al., 2005" startWordPosition="4707" endWordPosition="4710">eature vector xi with the weight vector w� that we learned by the local search technique (eq:1). 5 Redundancy Checking When many of the competing sentences are included in the summary, the issue of information overlap between parts of the output comes up, and a mechanism for addressing redundancy is needed. Therefore, our summarization systems employ a final level of analysis: before being added to the final output, the sentences deemed to be important are compared to each other and only those that are not too similar to other candidates are included in the final answer or summary. Following (Zhou et al., 2005), we modeled this by BE overlap between an intermediate summary and a to-be-added candidate summary e p(µ,r)(x) = (3) E� = 1 N − 1 EN i=1 Ek i=1 P(x) = L(E)|X) = �n k = i=1 E ⇔ �n j=1 i=1 k En E i=1 j=1 log k E j=1 310 sentence. We call this overlap ratio R, where R is between 0 and 1 inclusively. Setting R = 0.7 means that a candidate summary sentence, s, can be added to an intermediate summary, S, if the sentence has a BE overlap ratio less than or equal to 0.7. 6 Experimental Evaluation 6.1 Evaluation Setup We used the main task of Document Understanding Conference (DUC) 2007 for evaluation</context>
</contexts>
<marker>Zhou, Lin, Hovy, 2005</marker>
<rawString>L. Zhou, C. Y. Lin, and E. Hovy. 2005. A BE-based Multi-dccument Summarizer with Query Interpretation. In Proceedings of Document Understanding Conference, Vancouver, B.C., Canada.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>