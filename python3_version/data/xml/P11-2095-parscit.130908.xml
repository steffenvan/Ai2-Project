<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.067841">
<title confidence="0.990166">
Fully Unsupervised Word Segmentation with BVE and MDL
</title>
<author confidence="0.99861">
Daniel Hewlett and Paul Cohen
</author>
<affiliation confidence="0.9986525">
Department of Computer Science
University of Arizona
</affiliation>
<address confidence="0.688504">
Tucson, AZ 85721
</address>
<email confidence="0.998808">
{dhewlett,cohen}@cs.arizona.edu
</email>
<sectionHeader confidence="0.99387" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999399470588235">
Several results in the word segmentation liter-
ature suggest that description length provides
a useful estimate of segmentation quality in
fully unsupervised settings. However, since
the space of potential segmentations grows ex-
ponentially with the length of the corpus, no
tractable algorithm follows directly from the
Minimum Description Length (MDL) princi-
ple. Therefore, it is necessary to generate
a set of candidate segmentations and select
between them according to the MDL princi-
ple. We evaluate several algorithms for gen-
erating these candidate segmentations on a
range of natural language corpora, and show
that the Bootstrapped Voting Experts algo-
rithm consistently outperforms other methods
when paired with MDL.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999970470588235">
The goal of unsupervised word segmentation is to
discover correct word boundaries in natural lan-
guage corpora where explicit boundaries are absent.
Often, unsupervised word segmentation algorithms
rely heavily on parameterization to produce the cor-
rect segmentation for a given language. The goal
of fully unsupervised word segmentation, then, is to
recover the correct boundaries for arbitrary natural
language corpora without explicit human parameter-
ization. This means that a fully unsupervised algo-
rithm would have to set its own parameters based
only on the corpus provided to it.
In principle, this goal can be achieved by creat-
ing a function that measures the quality of a seg-
mentation in a language-independent way, and ap-
plying this function to all possible segmentations of
the corpora to select the best one. Evidence from the
word segmentation literature suggests that descrip-
tion length provides a good approximation to this
segmentation quality function. We discuss the Min-
imum Description Length (MDL) principle in more
detail in the next section. Unfortunately, evaluating
all possible segmentations is intractable, since a cor-
pus of length n has 2n−1 possible segmentations. As
a result, MDL methods have to rely on an efficient
algorithm to generate a relatively small number of
candidate segmentations to choose between. It is
an empirical question which algorithm will generate
the most effective set of candidate segmentations.
In this work, we compare a variety of unsupervised
word segmentation algorithms operating in conjunc-
tion with MDL for fully unsupervised segmentation,
and find that the Bootstrapped Voting Experts (BVE)
algorithm generally achieves the best performance.
</bodyText>
<sectionHeader confidence="0.976087" genericHeader="method">
2 Minimum Description Length
</sectionHeader>
<bodyText confidence="0.992561916666667">
At a formal level, a segmentation algorithm is a
function SEGMENT(c, θ) that maps a corpus c and
a vector of parameters θ E O to one of the
possible segmentations of that corpus. The goal
of fully unsupervised segmentation is to reduce
SEGMENT(c, θ) to SEGMENT(c) by removing the
need for a human to specify a particular θ. One way
to achieve this goal is to generate a set of candidate
segmentations by evaluating the algorithm for mul-
tiple values of θ, and then choose the segmentation
that minimizes some cost function. Thus, we can
define SEGMENT(c) in terms of SEGMENT(c, θ):
</bodyText>
<equation confidence="0.996304">
SEGMENT(c) = argmin COST(SEGMENT(c, θ))
BEO
(1)
</equation>
<page confidence="0.87477">
540
</page>
<note confidence="0.547271">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 540–545,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.9999939">
Now, selecting the best segmentation is treated as a
model selection problem, where each segmentation
provides a different model of the corpus. Intuitively,
a general approach is to choose the simplest model
that explains the data, a principle known as Occam’s
Razor. In information theory, this intuitive princi-
ple of simplicity or parsimony has been formalized
as the Minimum Description Length (MDL) princi-
ple, which states that the most likely model of the
data is the one that requires the fewest bits to en-
code (Rissanen, 1983). The number of bits required
to represent a model is called its description length.
Previous work applying the MDL principle to seg-
mentation (Yu, 2000; Argamon et al., 2004; Zhikov
et al., 2010) is motivated by the observation that ev-
ery segmentation of a corpus implicitly defines a lex-
icon, or set of words.
More formally, the segmented corpus S is a list
of words s1s2 ... sN. L(S), the lexicon implicitly
defined by S, is simply the set of unique words in S.
The description length of S can then be broken into
two components, the description length of the lex-
icon and the description length of the corpus given
the lexicon. If we consider S as being generated
by sampling words from a probability distribution
over words in the lexicon, the number of bits re-
quired to represent each word si in S is simply its
surprisal, − log P(si). The information cost of the
corpus given the lexicon is then computed by sum-
ming the surprisal of each word si in the corpus:
</bodyText>
<equation confidence="0.990712">
�N
CODE(S|L(S)) _ − i�1 log P(si) (2)
</equation>
<bodyText confidence="0.999322181818182">
To properly compute the description length of the
segmentation, we must also include the cost of the
lexicon. Adding in the description length of the lex-
icon forces a trade-off between the lexicon size and
the size of the compressed corpus. For purposes of
the description length calculation, the lexicon is sim-
ply treated as a separate corpus consisting of char-
acters rather than words. The description length can
then be computed in the usual manner, by summing
the surprisal of each character in each word in the
lexicon:
</bodyText>
<equation confidence="0.952198">
CODE(L(S)) _ − EwEL(S) EkEw log P(k) (3)
</equation>
<bodyText confidence="0.967389882352941">
where k E w refers to the characters in word w
in the lexicon. As noted by Zhikov et al. (Zhikov
et al., 2010), an additional term is needed for the
information required to encode the parameters of the
lexicon model. This quantity is normally estimated
by (k/2) log n, where k is the degrees of freedom in
the model and n is the length of the data (Rissanen,
1983). Substituting the appropriate values for the
lexicon model yields:
|L(S) |− 1 * log N (4)
2
The full description length calculation is simply the
sum of three terms shown in 2, 3, and 4. From this
definition, it follows that a low description length
will be achieved by a segmentation that defines a
small lexicon, which nonetheless reduces the corpus
to a short series of mostly high-frequency words.
</bodyText>
<sectionHeader confidence="0.991614" genericHeader="method">
3 Generating Candidate Segmentations
</sectionHeader>
<bodyText confidence="0.999933772727273">
Recent unsupervised MDL algorithms rely on
heuristic methods to generate candidate segmenta-
tions. Yu (2000) makes simplifying assumptions
about the nature of the lexicon, and then performs an
Expectation-Maximization (EM) search over this re-
duced hypothesis space. Zhikov et al. (2010) present
an algorithm called EntropyMDL that generates a
candidate segmentation based on branching entropy,
and then iteratively refines the segmentation in an
attempt to greedily minimize description length.
We selected three entropy-based algorithms for
generating candidate segmentations, because such
algorithms do not depend on the details of any par-
ticular language. By “unsupervised,” we mean op-
erating on a single unbroken sequence of characters
without any boundary information; Excluded from
consideration are a class of algorithms that are semi-
supervised because they require sentence boundaries
to be provided. Such algorithms include MBDP-1
(Brent, 1999), HDP (Goldwater et al., 2009), and
WordEnds (Fleck, 2008), each of which is discussed
in Section 5.
</bodyText>
<subsectionHeader confidence="0.999428">
3.1 Phoneme to Morpheme
</subsectionHeader>
<bodyText confidence="0.999870555555555">
Tanaka-Ishii and Jin (2006) developed Phoneme to
Morpheme (PtM) to implement ideas originally de-
veloped by Harris (1955). Harris noticed that if
one proceeds incrementally through a sequence of
phonemes and asks speakers of the language to
count the letters that could appear next in the se-
quence (today called the successor count), the points
where the number increases often correspond to
morpheme boundaries. Tanaka-Ishii and Jin cor-
</bodyText>
<page confidence="0.986287">
541
</page>
<bodyText confidence="0.999983">
rectly recognized that this idea was an early ver-
sion of branching entropy, given by HB(seq) =
− EKES P(c|seq) log P(c|seq), where 5 is the set
of successors to seq. They designed their PtM algo-
rithm based on branching entropy in both directions,
and it was able to achieve scores near the state of the
art on word segmentation in phonetically-encoded
English and Chinese. PtM posits a boundary when-
ever the increase in the branching entropy exceeds
a threshold. This threshold provides an adjustable
parameter for PtM, which we exploit to generate 41
candidate segmentations by trying every threshold in
the range [0.0, 2.0], in steps of 0.05.
</bodyText>
<subsectionHeader confidence="0.999674">
3.2 Voting Experts
</subsectionHeader>
<bodyText confidence="0.99994647826087">
The Voting Experts (VE) algorithm (Cohen and
Adams, 2001) is based on the premise that words
may be identified by an information theoretic signa-
ture: Entropy within a word is relatively low, en-
tropy at word boundaries is relatively high. The
name Voting Experts refers to the “experts” that vote
on possible boundary locations. VE has two ex-
perts: One votes to place boundaries after sequences
that have low internal entropy (surprisal), given by
HI(seq) = − log P(seq), the other votes after se-
quences that have high branching entropy. All se-
quences are evaluated locally, within a sliding win-
dow, so the algorithm is very efficient. A boundary
is generated whenever the vote total at a given loca-
tion exceeds a threshold, and in some cases only if
the vote total is a local maximum. VE thus has three
parameters that can be manipulated to generate po-
tential segmentations: Window size, threshold, and
local maximum. Pairing VE with MDL was first ex-
amined by Hewlett and Cohen (2009). We generated
a set of 104 segmentations by trying every viable
threshold and local max setting for each window size
between 2 and 9.
</bodyText>
<subsectionHeader confidence="0.99736">
3.3 Bootstrapped Voting Experts
</subsectionHeader>
<bodyText confidence="0.999980727272727">
The Bootstrapped Voting Experts (BVE) algorithm
(Hewlett and Cohen, 2009) is an extension to VE.
BVE works by segmenting the corpus repeatedly,
with each new segmentation incorporating knowl-
edge gained from previous segmentations. As with
many bootstrapping methods, three essential com-
ponents are required: some initial seed knowledge,
a way to represent knowledge, and a way to lever-
age that knowledge to improve future performance.
For BVE, the seed knowledge consists of a high-
precision segmentation generated by VE. After this
seed segmentation, BVE segments the corpus re-
peatedly, lowering the vote threshold with each iter-
ation. Knowledge gained from prior segmentations
is represented in a data structure called the knowl-
edge trie. During voting, this knowledge trie pro-
vides statistics for a third expert that places votes in
contexts where boundaries were most frequently ob-
served during the previous iteration. Each iteration
of BVE provides a candidate segmentation, and ex-
ecuting BVE for window sizes 2-8 and both local
max settings generated a total of 126 segmentations.
</bodyText>
<sectionHeader confidence="0.999799" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9999892">
There are two ways to evaluate the quality of a seg-
mentation algorithm in the MDL framework. The
first is to directly measure the quantity of the seg-
mentation chosen by MDL. For word segmentation,
this is typically done by computing the F-score,
where F = (2 * Precision * Recall)/(Precision +
Recall), for both boundaries (BF) and words (WF)
found by the algorithm. The second is to com-
pare the minimal description length among the can-
didates to the true description length of the corpus.
</bodyText>
<sectionHeader confidence="0.681559" genericHeader="evaluation">
4.1 Results
</sectionHeader>
<bodyText confidence="0.999669166666667">
We chose a diverse set of natural language cor-
pora, including some widely-used corpora to facil-
itate comparison. For each corpus, we generated a
set of candidate segmentations with PtM, VE, and
BVE, as described in the previous section. From
each set of candidates, results for the segmentation
with minimal description length are presented in the
tables below. Where possible, results for other algo-
rithms are presented in italics, with semi-supervised
algorithms set apart. Source code for all algorithms
evaluated here, as well as data files for all corpora,
are available online1.
One of the most commonly-used benchmark cor-
pora for unsupervised word segmentation is the
BR87 corpus. This corpus is a phonemic encod-
ing of the Bernstein Ratner corpus (Bernstein Rat-
ner, 1987) from the CHILDES database of child-
directed speech (MacWhinney, 2000). The perfor-
</bodyText>
<footnote confidence="0.982909">
1http://code.google.com/p/voting-experts
</footnote>
<page confidence="0.996429">
542
</page>
<bodyText confidence="0.982743142857143">
mance of the algorithms on BR87 is shown in Ta-
ble 1 below. As with all experiments in this work,
the input was presented as one continuous sequence
of characters with no word or sentence boundaries.
Published results for two unsupervised algorithms,
the MDL-based algorithm of Yu (2000) and the
EntropyMDL (EMDL) algorithm of Zhikov et al.
(2010), on this widely-used benchmark corpus are
shown in italics. Set apart in the table are pub-
lished results for three semi-supervised algorithms,
MBDP-1 (Brent, 1999), HDP (Goldwater, 2007),
and WordEnds (Fleck, 2008), described in Section
5. These algorithms operate on a version of the cor-
pus that includes sentence boundaries.
</bodyText>
<table confidence="0.999476888888889">
Algorithm BP BR BF WP WR WF
PtM+MDL 0.861 0.897 0.879 0.676 0.704 0.690
VE+MDL 0.875 0.803 0.838 0.614 0.563 0.587
BVE+MDL 0.949 0.879 0.913 0.793 0.734 0.762
Yu 0.722 0.724 0.723 NR NR NR
EMDL NR NR 0.907 NR NR 0.750
MBDP-1 0.803 0.843 0.823 0.670 0.694 0.682
HDP 0.903 0.808 0.852 0.752 0.696 0.723
WordEnds 0.946 0.737 0.829 NR NR 0.707
</table>
<tableCaption confidence="0.9995">
Table 1: Results for the BR87 corpus.
</tableCaption>
<bodyText confidence="0.9972594">
Results for one corpus, the first 50,000 charac-
ters of George Orwell’s 1984, have been reported
in nearly every VE-related paper. It thus provides
a good opportunity to compare to the other VE-
derived algorithms: Hierarchical Voting Experts –
3 Experts (Miller and Stoytchev, 2008) and Markov
Experts (Cheng and Mitzenmacher, 2005). Table 2
shows the results for candidate algorithms as well as
the two other VE-derived algorithms, HVE-3E and
ME.
</bodyText>
<table confidence="0.999323833333333">
Algorithm BP BR BF WP WR WF
PtM+MDL 0.694 0.833 0.758 0.421 0.505 0.459
VE+MDL 0.788 0.774 0.781 0.498 0.489 0.493
BVE+MDL 0.841 0.828 0.834 0.585 0.577 0.581
HVE-3E 0.796 0.771 0.784 0.512 0.496 0.504
ME 0.809 0.787 0.798 NR 0.542 NR
</table>
<tableCaption confidence="0.997344">
Table 2: Results for the first 50,000 characters of 1984.
</tableCaption>
<bodyText confidence="0.997442705882353">
Chinese and Thai are both commonly written
without spaces between words, though some punc-
tuation is often included. Because of this, these
languages provide an excellent real-world challenge
for unsupervised segmentation. The results shown
in Table 3 were obtained using the first 100,000
words of the Chinese Gigaword corpus (Huang,
2007), written in Chinese characters. The word
boundaries specified in the Chinese Gigaword Cor-
pus were used as a gold standard. Table 4 shows
results for a roughly 100,000 word subset of a cor-
pus of Thai novels written in the Thai script, taken
from a recent Thai word segmentation competition,
InterBEST 2009. Working with a similar but much
larger corpus of Thai text, Zhikov et al. were able
to achieve slightly better performance (BF=0.934,
WF=0.822).
</bodyText>
<table confidence="0.99911125">
Algorithm BP BR BF WP WR WF
PtM+MDL 0.894 0.610 0.725 0.571 0.390 0.463
VE+MDL 0.871 0.847 0.859 0.657 0.639 0.648
BVE+MDL 0.834 0.914 0.872 0.654 0.717 0.684
</table>
<tableCaption confidence="0.951829">
Table 3: Results for a corpus of orthographic Chinese.
</tableCaption>
<table confidence="0.99983725">
Algorithm BP BR BF WP WR WF
PtM+MDL 0.863 0.934 0.897 0.702 0.760 0.730
VE+MDL 0.916 0.837 0.874 0.702 0.642 0.671
BVE+MDL 0.889 0.969 0.927 0.767 0.836 0.800
</table>
<tableCaption confidence="0.9986">
Table 4: Results for a corpus of orthographic Thai.
</tableCaption>
<bodyText confidence="0.997821333333333">
The Switchboard corpus (Godfrey and Holli-
man, 1993) was created by transcribing sponta-
neous speech, namely telephone conversations be-
tween English speakers. Results in Table 5 are for
a roughly 64,000 word section of the corpus, tran-
scribed orthographically.
</bodyText>
<table confidence="0.999572428571429">
Algorithm BP BR BF WP WR WF
PtM+MDL 0.761 0.837 0.797 0.499 0.549 0.523
VE+MDL 0.779 0.855 0.815 0.530 0.582 0.555
BVE+MDL 0.890 0.818 0.853 0.644 0.592 0.617
Yu 0.674 0.665 0.669 NR NR NR
WordEnds 0.900 0.755 0.821 NR NR 0.663
HDP 0.731 0.924 0.816 NR NR 0.636
</table>
<tableCaption confidence="0.995696">
Table 5: Results for a subset of the Switchboard corpus.
</tableCaption>
<subsectionHeader confidence="0.997299">
4.2 Description Length
</subsectionHeader>
<bodyText confidence="0.993530666666667">
Table 6 shows the best description length achieved
by each algorithm for each of the test corpora. In
most cases, BVE compressed the corpus more than
VE, which in turn achieved better compression than
PtM. In Chinese, the two VE-algorithms were able
to compress the corpus beyond the gold standard
</bodyText>
<page confidence="0.996628">
543
</page>
<bodyText confidence="0.957231">
size, which may mean that these algorithms are
sometimes finding repeated units larger than words,
such as phrases.
word boundaries to improve segmentation (Fleck,
2008), which is analogous to the role of the knowl-
edge trie in BVE.
</bodyText>
<table confidence="0.99883">
Algorithm BR87 Orwell SWB CGW Thai
PtM+MDL 3.43e5 6.10e5 8.79e5 1.80e6 1,23e6
VE+MDL 3.41e5 5.75e5 8.24e5 1.54e6 1.23e6
BVE+MDL 3.13e5 5.29e5 7.64e5 1.56e6 1.13e6
Gold Standard 2.99e5 5.07e5 7.06e5 1.62e6 1.11e6
</table>
<tableCaption confidence="0.973626">
Table 6: Best description length achieved by each algo-
rithm compared to the actual description length of the
corpus.
</tableCaption>
<sectionHeader confidence="0.999529" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999994242424243">
The algorithms described in Section 3 are all rela-
tively recent algorithms based on entropy. Many al-
gorithms for computational morphology make use
of concepts similar to branching entropy, such as
successor count. The HubMorph algorithm (John-
son and Martin, 2003) adds all known words to a
trie and then performs DFA minimization (Hopcroft
and Ullman, 1979) to convert the trie to a finite state
machine. In this DFA, it searches for sequences of
states (stretched hubs) with low branching factor in-
ternally and high branching factor at the boundaries,
which is analogous to the chunk signature that drives
VE and BVE, as well as the role of branching en-
tropy in PtM.
MDL is analogous to Bayesian inference, where
the information cost of the model CODE(M) acts
as the prior distribution over models P(M), and
CODE(DIM), the information cost of the data given
the model, acts as the likelihood function P(DIM).
Thus, Bayesian word segmentation methods may
be considered related as well. Indeed, one of the
early Bayesian methods, MBDP-1 (Brent, 1999)
was adapted from an earlier MDL-based method.
Venkataraman (2001) simplified MBDP-1, relaxed
some of its assumptions while preserving the same
level of performance. Recently, Bayesian methods
with more sophisticated language models have been
developed, including one that models language gen-
eration as a hierarchical Dirichlet process (HDP),
in order to incorporate the effects of syntax into
word segmentation (Goldwater et al., 2009). An-
other recent algorithm, WordEnds, generalizes in-
formation about the distribution of characters near
</bodyText>
<sectionHeader confidence="0.999519" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.9999916">
For the five corpora tested above, BVE achieved
the best performance in conjunction with MDL, and
also achieved the lowest description length. We have
shown that the combination of BVE and MDL pro-
vides an effective approach to unsupervised word
segmentation, and that it can equal or surpass semi-
supervised algorithms such as MBDP-1, HDP, and
WordEnds in some cases.
All of the languages tested here have relatively
few morphemes per word. One area for future work
is a full investigation of the performance of these al-
gorithms in polysynthetic languages such as Inukti-
tut, where each word contains many morphemes. It
is likely that in such languages, the algorithms will
find morphs rather than words.
</bodyText>
<sectionHeader confidence="0.994727" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999893333333333">
This work was supported by the Office of Naval Re-
search under contract ONR N00141010117. Any
opinions, findings, and conclusions or recommen-
dations expressed in this publication are those of the
authors and do not necessarily reflect the views of
the ONR.
</bodyText>
<sectionHeader confidence="0.998585" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998434842105263">
Shlomo Argamon, Navot Akiva, Amihood Amir, and
Oren Kapah. 2004. Efficient Unsupervised Recur-
sive Word Segmentation Using Minimum Description
Length. In Proceedings of the 20th International Con-
ference on Computational Linguistics, Morristown,
NJ, USA. Association for Computational Linguistics.
Nan Bernstein Ratner, 1987. The phonology of parent-
child speech, pages 159–174. Erlbaum, Hillsdale, NJ.
Michael R. Brent. 1999. An Efficient, Probabilistically
Sound Algorithm for Segmentation and Word Discov-
ery. Machine Learning, (34):71–105.
Jimming Cheng and Michael Mitzenmacher. 2005. The
Markov Expert for Finding Episodes in Time Series.
In Proceedings of the Data Compression Conference,
pages 454–454. IEEE.
Paul Cohen and Niall Adams. 2001. An algorithm
for segmenting categorical time series into meaning-
ful episodes. In Proceedings of the Fourth Symposium
on Intelligent Data Analysis.
</reference>
<page confidence="0.982865">
544
</page>
<reference confidence="0.999884114754098">
Margaret M. Fleck. 2008. Lexicalized phonotactic word
segmentation. In Proceedings of The 49th Annual
Meeting of the Association for Computational Linguis-
tics: Human Language Technologies, pages 130–138,
Columbus, Ohio, USA. Association for Computational
Linguistics.
John J. Godfrey and Ed Holliman. 1993. Switchboard- 1
Transcripts.
Sharon Goldwater, Thomas L Griffiths, and Mark John-
son. 2009. A Bayesian Framework for Word Segmen-
tation: Exploring the Effects of Context. Cognition,
112(1):21–54.
Sharon Goldwater. 2007. Nonparametric Bayesian mod-
els of lexical acquisition. Ph.D. dissertation, Brown
University.
Zellig S. Harris. 1955. From Phoneme to Morpheme.
Language, 31(2):190–222.
Daniel Hewlett and Paul Cohen. 2009. Bootstrap Voting
Experts. In Proceedings of the Twenty-first Interna-
tional Joint Conference on Artificial Intelligence.
J. E. Hopcroft and J. D. Ullman. 1979. Introduction
to Automata Theory, Languages, and Computation.
Addison-Wesley.
Chu-Ren Huang. 2007. Tagged Chinese Gigaword
(Catalog LDC2007T03). Linguistic Data Consortium,
Philadephia.
Howard Johnson and Joel Martin. 2003. Unsupervised
learning of morphology for English and Inuktitut. Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology (HLT-
NAACL 2003), pages 43–45.
Brian MacWhinney. 2000. The CHILDES project: Tools
for analyzing talk. Lawrence Erlbaum Associates,
Mahwah, NJ, 3rd editio edition.
Matthew Miller and Alexander Stoytchev. 2008. Hierar-
chical Voting Experts: An Unsupervised Algorithm for
Hierarchical Sequence Segmentation. In Proceedings
of the 7th IEEE International Conference on Develop-
ment and Learning, pages 186–191.
Jorma Rissanen. 1983. A Universal Prior for Integers
and Estimation by Minimum Description Length. The
Annals of Statistics, 11(2):416–431.
Kumiko Tanaka-Ishii and Zhihui Jin. 2006. From
Phoneme to Morpheme: Another Verification Using
a Corpus. In Proceedings of the 21st International
Conference on Computer Processing of Oriental Lan-
guages, pages 234–244.
Anand Venkataraman. 2001. A procedure for unsuper-
vised lexicon learning. In Proceedings of the Eigh-
teenth International Conference on Machine Learning.
Hua Yu. 2000. Unsupervised Word Induction using
MDL Criterion. In Proceedings of the International
Symposium of Chinese Spoken Language Processing,
Beijing, China.
Valentin Zhikov, Hiroya Takamura, and Manabu Oku-
mura. 2010. An Efficient Algorithm for Unsuper-
vised Word Segmentation with Branching Entropy and
MDL. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 832–842, Cambridge, MA. MIT Press.
</reference>
<page confidence="0.99853">
545
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.860748">
<title confidence="0.990182">Fully Unsupervised Word Segmentation with BVE and MDL</title>
<author confidence="0.973413">Hewlett</author>
<affiliation confidence="0.9998385">Department of Computer University of</affiliation>
<address confidence="0.92704">Tucson, AZ</address>
<abstract confidence="0.997563722222222">Several results in the word segmentation literature suggest that description length provides a useful estimate of segmentation quality in fully unsupervised settings. However, since the space of potential segmentations grows exponentially with the length of the corpus, no tractable algorithm follows directly from the Minimum Description Length (MDL) principle. Therefore, it is necessary to generate a set of candidate segmentations and select between them according to the MDL principle. We evaluate several algorithms for generating these candidate segmentations on a range of natural language corpora, and show that the Bootstrapped Voting Experts algorithm consistently outperforms other methods when paired with MDL.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shlomo Argamon</author>
<author>Navot Akiva</author>
<author>Amihood Amir</author>
<author>Oren Kapah</author>
</authors>
<title>Efficient Unsupervised Recursive Word Segmentation Using Minimum Description Length.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="4206" citStr="Argamon et al., 2004" startWordPosition="644" endWordPosition="647">ch segmentation provides a different model of the corpus. Intuitively, a general approach is to choose the simplest model that explains the data, a principle known as Occam’s Razor. In information theory, this intuitive principle of simplicity or parsimony has been formalized as the Minimum Description Length (MDL) principle, which states that the most likely model of the data is the one that requires the fewest bits to encode (Rissanen, 1983). The number of bits required to represent a model is called its description length. Previous work applying the MDL principle to segmentation (Yu, 2000; Argamon et al., 2004; Zhikov et al., 2010) is motivated by the observation that every segmentation of a corpus implicitly defines a lexicon, or set of words. More formally, the segmented corpus S is a list of words s1s2 ... sN. L(S), the lexicon implicitly defined by S, is simply the set of unique words in S. The description length of S can then be broken into two components, the description length of the lexicon and the description length of the corpus given the lexicon. If we consider S as being generated by sampling words from a probability distribution over words in the lexicon, the number of bits required to</context>
</contexts>
<marker>Argamon, Akiva, Amir, Kapah, 2004</marker>
<rawString>Shlomo Argamon, Navot Akiva, Amihood Amir, and Oren Kapah. 2004. Efficient Unsupervised Recursive Word Segmentation Using Minimum Description Length. In Proceedings of the 20th International Conference on Computational Linguistics, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nan Bernstein Ratner</author>
</authors>
<title>The phonology of parentchild speech,</title>
<date>1987</date>
<pages>159--174</pages>
<publisher>Erlbaum,</publisher>
<location>Hillsdale, NJ.</location>
<contexts>
<context position="12119" citStr="Ratner, 1987" startWordPosition="1956" endWordPosition="1958">mentations with PtM, VE, and BVE, as described in the previous section. From each set of candidates, results for the segmentation with minimal description length are presented in the tables below. Where possible, results for other algorithms are presented in italics, with semi-supervised algorithms set apart. Source code for all algorithms evaluated here, as well as data files for all corpora, are available online1. One of the most commonly-used benchmark corpora for unsupervised word segmentation is the BR87 corpus. This corpus is a phonemic encoding of the Bernstein Ratner corpus (Bernstein Ratner, 1987) from the CHILDES database of childdirected speech (MacWhinney, 2000). The perfor1http://code.google.com/p/voting-experts 542 mance of the algorithms on BR87 is shown in Table 1 below. As with all experiments in this work, the input was presented as one continuous sequence of characters with no word or sentence boundaries. Published results for two unsupervised algorithms, the MDL-based algorithm of Yu (2000) and the EntropyMDL (EMDL) algorithm of Zhikov et al. (2010), on this widely-used benchmark corpus are shown in italics. Set apart in the table are published results for three semi-supervi</context>
</contexts>
<marker>Ratner, 1987</marker>
<rawString>Nan Bernstein Ratner, 1987. The phonology of parentchild speech, pages 159–174. Erlbaum, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael R Brent</author>
</authors>
<title>An Efficient, Probabilistically Sound Algorithm for Segmentation and Word Discovery.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--71</pages>
<contexts>
<context position="7359" citStr="Brent, 1999" startWordPosition="1176" endWordPosition="1177">a candidate segmentation based on branching entropy, and then iteratively refines the segmentation in an attempt to greedily minimize description length. We selected three entropy-based algorithms for generating candidate segmentations, because such algorithms do not depend on the details of any particular language. By “unsupervised,” we mean operating on a single unbroken sequence of characters without any boundary information; Excluded from consideration are a class of algorithms that are semisupervised because they require sentence boundaries to be provided. Such algorithms include MBDP-1 (Brent, 1999), HDP (Goldwater et al., 2009), and WordEnds (Fleck, 2008), each of which is discussed in Section 5. 3.1 Phoneme to Morpheme Tanaka-Ishii and Jin (2006) developed Phoneme to Morpheme (PtM) to implement ideas originally developed by Harris (1955). Harris noticed that if one proceeds incrementally through a sequence of phonemes and asks speakers of the language to count the letters that could appear next in the sequence (today called the successor count), the points where the number increases often correspond to morpheme boundaries. Tanaka-Ishii and Jin cor541 rectly recognized that this idea wa</context>
<context position="12755" citStr="Brent, 1999" startWordPosition="2054" endWordPosition="2055">se of childdirected speech (MacWhinney, 2000). The perfor1http://code.google.com/p/voting-experts 542 mance of the algorithms on BR87 is shown in Table 1 below. As with all experiments in this work, the input was presented as one continuous sequence of characters with no word or sentence boundaries. Published results for two unsupervised algorithms, the MDL-based algorithm of Yu (2000) and the EntropyMDL (EMDL) algorithm of Zhikov et al. (2010), on this widely-used benchmark corpus are shown in italics. Set apart in the table are published results for three semi-supervised algorithms, MBDP-1 (Brent, 1999), HDP (Goldwater, 2007), and WordEnds (Fleck, 2008), described in Section 5. These algorithms operate on a version of the corpus that includes sentence boundaries. Algorithm BP BR BF WP WR WF PtM+MDL 0.861 0.897 0.879 0.676 0.704 0.690 VE+MDL 0.875 0.803 0.838 0.614 0.563 0.587 BVE+MDL 0.949 0.879 0.913 0.793 0.734 0.762 Yu 0.722 0.724 0.723 NR NR NR EMDL NR NR 0.907 NR NR 0.750 MBDP-1 0.803 0.843 0.823 0.670 0.694 0.682 HDP 0.903 0.808 0.852 0.752 0.696 0.723 WordEnds 0.946 0.737 0.829 NR NR 0.707 Table 1: Results for the BR87 corpus. Results for one corpus, the first 50,000 characters of Geo</context>
<context position="17780" citStr="Brent, 1999" startWordPosition="2888" endWordPosition="2889">uences of states (stretched hubs) with low branching factor internally and high branching factor at the boundaries, which is analogous to the chunk signature that drives VE and BVE, as well as the role of branching entropy in PtM. MDL is analogous to Bayesian inference, where the information cost of the model CODE(M) acts as the prior distribution over models P(M), and CODE(DIM), the information cost of the data given the model, acts as the likelihood function P(DIM). Thus, Bayesian word segmentation methods may be considered related as well. Indeed, one of the early Bayesian methods, MBDP-1 (Brent, 1999) was adapted from an earlier MDL-based method. Venkataraman (2001) simplified MBDP-1, relaxed some of its assumptions while preserving the same level of performance. Recently, Bayesian methods with more sophisticated language models have been developed, including one that models language generation as a hierarchical Dirichlet process (HDP), in order to incorporate the effects of syntax into word segmentation (Goldwater et al., 2009). Another recent algorithm, WordEnds, generalizes information about the distribution of characters near 6 Discussion For the five corpora tested above, BVE achieved</context>
</contexts>
<marker>Brent, 1999</marker>
<rawString>Michael R. Brent. 1999. An Efficient, Probabilistically Sound Algorithm for Segmentation and Word Discovery. Machine Learning, (34):71–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jimming Cheng</author>
<author>Michael Mitzenmacher</author>
</authors>
<title>The Markov Expert for Finding Episodes in Time Series.</title>
<date>2005</date>
<booktitle>In Proceedings of the Data Compression Conference,</booktitle>
<pages>454--454</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="13627" citStr="Cheng and Mitzenmacher, 2005" startWordPosition="2199" endWordPosition="2202">0.875 0.803 0.838 0.614 0.563 0.587 BVE+MDL 0.949 0.879 0.913 0.793 0.734 0.762 Yu 0.722 0.724 0.723 NR NR NR EMDL NR NR 0.907 NR NR 0.750 MBDP-1 0.803 0.843 0.823 0.670 0.694 0.682 HDP 0.903 0.808 0.852 0.752 0.696 0.723 WordEnds 0.946 0.737 0.829 NR NR 0.707 Table 1: Results for the BR87 corpus. Results for one corpus, the first 50,000 characters of George Orwell’s 1984, have been reported in nearly every VE-related paper. It thus provides a good opportunity to compare to the other VEderived algorithms: Hierarchical Voting Experts – 3 Experts (Miller and Stoytchev, 2008) and Markov Experts (Cheng and Mitzenmacher, 2005). Table 2 shows the results for candidate algorithms as well as the two other VE-derived algorithms, HVE-3E and ME. Algorithm BP BR BF WP WR WF PtM+MDL 0.694 0.833 0.758 0.421 0.505 0.459 VE+MDL 0.788 0.774 0.781 0.498 0.489 0.493 BVE+MDL 0.841 0.828 0.834 0.585 0.577 0.581 HVE-3E 0.796 0.771 0.784 0.512 0.496 0.504 ME 0.809 0.787 0.798 NR 0.542 NR Table 2: Results for the first 50,000 characters of 1984. Chinese and Thai are both commonly written without spaces between words, though some punctuation is often included. Because of this, these languages provide an excellent real-world challenge </context>
</contexts>
<marker>Cheng, Mitzenmacher, 2005</marker>
<rawString>Jimming Cheng and Michael Mitzenmacher. 2005. The Markov Expert for Finding Episodes in Time Series. In Proceedings of the Data Compression Conference, pages 454–454. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Cohen</author>
<author>Niall Adams</author>
</authors>
<title>An algorithm for segmenting categorical time series into meaningful episodes.</title>
<date>2001</date>
<booktitle>In Proceedings of the Fourth Symposium on Intelligent Data Analysis.</booktitle>
<contexts>
<context position="8645" citStr="Cohen and Adams, 2001" startWordPosition="1385" endWordPosition="1388">ES P(c|seq) log P(c|seq), where 5 is the set of successors to seq. They designed their PtM algorithm based on branching entropy in both directions, and it was able to achieve scores near the state of the art on word segmentation in phonetically-encoded English and Chinese. PtM posits a boundary whenever the increase in the branching entropy exceeds a threshold. This threshold provides an adjustable parameter for PtM, which we exploit to generate 41 candidate segmentations by trying every threshold in the range [0.0, 2.0], in steps of 0.05. 3.2 Voting Experts The Voting Experts (VE) algorithm (Cohen and Adams, 2001) is based on the premise that words may be identified by an information theoretic signature: Entropy within a word is relatively low, entropy at word boundaries is relatively high. The name Voting Experts refers to the “experts” that vote on possible boundary locations. VE has two experts: One votes to place boundaries after sequences that have low internal entropy (surprisal), given by HI(seq) = − log P(seq), the other votes after sequences that have high branching entropy. All sequences are evaluated locally, within a sliding window, so the algorithm is very efficient. A boundary is generate</context>
</contexts>
<marker>Cohen, Adams, 2001</marker>
<rawString>Paul Cohen and Niall Adams. 2001. An algorithm for segmenting categorical time series into meaningful episodes. In Proceedings of the Fourth Symposium on Intelligent Data Analysis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret M Fleck</author>
</authors>
<title>Lexicalized phonotactic word segmentation.</title>
<date>2008</date>
<booktitle>In Proceedings of The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>130--138</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio, USA.</location>
<contexts>
<context position="7417" citStr="Fleck, 2008" startWordPosition="1185" endWordPosition="1186">hen iteratively refines the segmentation in an attempt to greedily minimize description length. We selected three entropy-based algorithms for generating candidate segmentations, because such algorithms do not depend on the details of any particular language. By “unsupervised,” we mean operating on a single unbroken sequence of characters without any boundary information; Excluded from consideration are a class of algorithms that are semisupervised because they require sentence boundaries to be provided. Such algorithms include MBDP-1 (Brent, 1999), HDP (Goldwater et al., 2009), and WordEnds (Fleck, 2008), each of which is discussed in Section 5. 3.1 Phoneme to Morpheme Tanaka-Ishii and Jin (2006) developed Phoneme to Morpheme (PtM) to implement ideas originally developed by Harris (1955). Harris noticed that if one proceeds incrementally through a sequence of phonemes and asks speakers of the language to count the letters that could appear next in the sequence (today called the successor count), the points where the number increases often correspond to morpheme boundaries. Tanaka-Ishii and Jin cor541 rectly recognized that this idea was an early version of branching entropy, given by HB(seq) </context>
<context position="12806" citStr="Fleck, 2008" startWordPosition="2061" endWordPosition="2062">perfor1http://code.google.com/p/voting-experts 542 mance of the algorithms on BR87 is shown in Table 1 below. As with all experiments in this work, the input was presented as one continuous sequence of characters with no word or sentence boundaries. Published results for two unsupervised algorithms, the MDL-based algorithm of Yu (2000) and the EntropyMDL (EMDL) algorithm of Zhikov et al. (2010), on this widely-used benchmark corpus are shown in italics. Set apart in the table are published results for three semi-supervised algorithms, MBDP-1 (Brent, 1999), HDP (Goldwater, 2007), and WordEnds (Fleck, 2008), described in Section 5. These algorithms operate on a version of the corpus that includes sentence boundaries. Algorithm BP BR BF WP WR WF PtM+MDL 0.861 0.897 0.879 0.676 0.704 0.690 VE+MDL 0.875 0.803 0.838 0.614 0.563 0.587 BVE+MDL 0.949 0.879 0.913 0.793 0.734 0.762 Yu 0.722 0.724 0.723 NR NR NR EMDL NR NR 0.907 NR NR 0.750 MBDP-1 0.803 0.843 0.823 0.670 0.694 0.682 HDP 0.903 0.808 0.852 0.752 0.696 0.723 WordEnds 0.946 0.737 0.829 NR NR 0.707 Table 1: Results for the BR87 corpus. Results for one corpus, the first 50,000 characters of George Orwell’s 1984, have been reported in nearly eve</context>
<context position="16324" citStr="Fleck, 2008" startWordPosition="2649" endWordPosition="2650">00 0.755 0.821 NR NR 0.663 HDP 0.731 0.924 0.816 NR NR 0.636 Table 5: Results for a subset of the Switchboard corpus. 4.2 Description Length Table 6 shows the best description length achieved by each algorithm for each of the test corpora. In most cases, BVE compressed the corpus more than VE, which in turn achieved better compression than PtM. In Chinese, the two VE-algorithms were able to compress the corpus beyond the gold standard 543 size, which may mean that these algorithms are sometimes finding repeated units larger than words, such as phrases. word boundaries to improve segmentation (Fleck, 2008), which is analogous to the role of the knowledge trie in BVE. Algorithm BR87 Orwell SWB CGW Thai PtM+MDL 3.43e5 6.10e5 8.79e5 1.80e6 1,23e6 VE+MDL 3.41e5 5.75e5 8.24e5 1.54e6 1.23e6 BVE+MDL 3.13e5 5.29e5 7.64e5 1.56e6 1.13e6 Gold Standard 2.99e5 5.07e5 7.06e5 1.62e6 1.11e6 Table 6: Best description length achieved by each algorithm compared to the actual description length of the corpus. 5 Related Work The algorithms described in Section 3 are all relatively recent algorithms based on entropy. Many algorithms for computational morphology make use of concepts similar to branching entropy, such</context>
</contexts>
<marker>Fleck, 2008</marker>
<rawString>Margaret M. Fleck. 2008. Lexicalized phonotactic word segmentation. In Proceedings of The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 130–138, Columbus, Ohio, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John J Godfrey</author>
<author>Ed Holliman</author>
</authors>
<date>1993</date>
<note>Switchboard- 1 Transcripts.</note>
<contexts>
<context position="15303" citStr="Godfrey and Holliman, 1993" startWordPosition="2477" endWordPosition="2481">2009. Working with a similar but much larger corpus of Thai text, Zhikov et al. were able to achieve slightly better performance (BF=0.934, WF=0.822). Algorithm BP BR BF WP WR WF PtM+MDL 0.894 0.610 0.725 0.571 0.390 0.463 VE+MDL 0.871 0.847 0.859 0.657 0.639 0.648 BVE+MDL 0.834 0.914 0.872 0.654 0.717 0.684 Table 3: Results for a corpus of orthographic Chinese. Algorithm BP BR BF WP WR WF PtM+MDL 0.863 0.934 0.897 0.702 0.760 0.730 VE+MDL 0.916 0.837 0.874 0.702 0.642 0.671 BVE+MDL 0.889 0.969 0.927 0.767 0.836 0.800 Table 4: Results for a corpus of orthographic Thai. The Switchboard corpus (Godfrey and Holliman, 1993) was created by transcribing spontaneous speech, namely telephone conversations between English speakers. Results in Table 5 are for a roughly 64,000 word section of the corpus, transcribed orthographically. Algorithm BP BR BF WP WR WF PtM+MDL 0.761 0.837 0.797 0.499 0.549 0.523 VE+MDL 0.779 0.855 0.815 0.530 0.582 0.555 BVE+MDL 0.890 0.818 0.853 0.644 0.592 0.617 Yu 0.674 0.665 0.669 NR NR NR WordEnds 0.900 0.755 0.821 NR NR 0.663 HDP 0.731 0.924 0.816 NR NR 0.636 Table 5: Results for a subset of the Switchboard corpus. 4.2 Description Length Table 6 shows the best description length achieved</context>
</contexts>
<marker>Godfrey, Holliman, 1993</marker>
<rawString>John J. Godfrey and Ed Holliman. 1993. Switchboard- 1 Transcripts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>A Bayesian Framework for Word Segmentation: Exploring the Effects of Context.</title>
<date>2009</date>
<journal>Cognition,</journal>
<volume>112</volume>
<issue>1</issue>
<contexts>
<context position="7389" citStr="Goldwater et al., 2009" startWordPosition="1179" endWordPosition="1182">ation based on branching entropy, and then iteratively refines the segmentation in an attempt to greedily minimize description length. We selected three entropy-based algorithms for generating candidate segmentations, because such algorithms do not depend on the details of any particular language. By “unsupervised,” we mean operating on a single unbroken sequence of characters without any boundary information; Excluded from consideration are a class of algorithms that are semisupervised because they require sentence boundaries to be provided. Such algorithms include MBDP-1 (Brent, 1999), HDP (Goldwater et al., 2009), and WordEnds (Fleck, 2008), each of which is discussed in Section 5. 3.1 Phoneme to Morpheme Tanaka-Ishii and Jin (2006) developed Phoneme to Morpheme (PtM) to implement ideas originally developed by Harris (1955). Harris noticed that if one proceeds incrementally through a sequence of phonemes and asks speakers of the language to count the letters that could appear next in the sequence (today called the successor count), the points where the number increases often correspond to morpheme boundaries. Tanaka-Ishii and Jin cor541 rectly recognized that this idea was an early version of branchin</context>
<context position="18216" citStr="Goldwater et al., 2009" startWordPosition="2948" endWordPosition="2951"> model, acts as the likelihood function P(DIM). Thus, Bayesian word segmentation methods may be considered related as well. Indeed, one of the early Bayesian methods, MBDP-1 (Brent, 1999) was adapted from an earlier MDL-based method. Venkataraman (2001) simplified MBDP-1, relaxed some of its assumptions while preserving the same level of performance. Recently, Bayesian methods with more sophisticated language models have been developed, including one that models language generation as a hierarchical Dirichlet process (HDP), in order to incorporate the effects of syntax into word segmentation (Goldwater et al., 2009). Another recent algorithm, WordEnds, generalizes information about the distribution of characters near 6 Discussion For the five corpora tested above, BVE achieved the best performance in conjunction with MDL, and also achieved the lowest description length. We have shown that the combination of BVE and MDL provides an effective approach to unsupervised word segmentation, and that it can equal or surpass semisupervised algorithms such as MBDP-1, HDP, and WordEnds in some cases. All of the languages tested here have relatively few morphemes per word. One area for future work is a full investig</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2009</marker>
<rawString>Sharon Goldwater, Thomas L Griffiths, and Mark Johnson. 2009. A Bayesian Framework for Word Segmentation: Exploring the Effects of Context. Cognition, 112(1):21–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
</authors>
<title>Nonparametric Bayesian models of lexical acquisition.</title>
<date>2007</date>
<institution>Brown University.</institution>
<note>Ph.D. dissertation,</note>
<contexts>
<context position="12778" citStr="Goldwater, 2007" startWordPosition="2057" endWordPosition="2058"> speech (MacWhinney, 2000). The perfor1http://code.google.com/p/voting-experts 542 mance of the algorithms on BR87 is shown in Table 1 below. As with all experiments in this work, the input was presented as one continuous sequence of characters with no word or sentence boundaries. Published results for two unsupervised algorithms, the MDL-based algorithm of Yu (2000) and the EntropyMDL (EMDL) algorithm of Zhikov et al. (2010), on this widely-used benchmark corpus are shown in italics. Set apart in the table are published results for three semi-supervised algorithms, MBDP-1 (Brent, 1999), HDP (Goldwater, 2007), and WordEnds (Fleck, 2008), described in Section 5. These algorithms operate on a version of the corpus that includes sentence boundaries. Algorithm BP BR BF WP WR WF PtM+MDL 0.861 0.897 0.879 0.676 0.704 0.690 VE+MDL 0.875 0.803 0.838 0.614 0.563 0.587 BVE+MDL 0.949 0.879 0.913 0.793 0.734 0.762 Yu 0.722 0.724 0.723 NR NR NR EMDL NR NR 0.907 NR NR 0.750 MBDP-1 0.803 0.843 0.823 0.670 0.694 0.682 HDP 0.903 0.808 0.852 0.752 0.696 0.723 WordEnds 0.946 0.737 0.829 NR NR 0.707 Table 1: Results for the BR87 corpus. Results for one corpus, the first 50,000 characters of George Orwell’s 1984, have</context>
</contexts>
<marker>Goldwater, 2007</marker>
<rawString>Sharon Goldwater. 2007. Nonparametric Bayesian models of lexical acquisition. Ph.D. dissertation, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig S Harris</author>
</authors>
<title>From Phoneme to Morpheme.</title>
<date>1955</date>
<journal>Language,</journal>
<volume>31</volume>
<issue>2</issue>
<contexts>
<context position="7604" citStr="Harris (1955)" startWordPosition="1215" endWordPosition="1216"> such algorithms do not depend on the details of any particular language. By “unsupervised,” we mean operating on a single unbroken sequence of characters without any boundary information; Excluded from consideration are a class of algorithms that are semisupervised because they require sentence boundaries to be provided. Such algorithms include MBDP-1 (Brent, 1999), HDP (Goldwater et al., 2009), and WordEnds (Fleck, 2008), each of which is discussed in Section 5. 3.1 Phoneme to Morpheme Tanaka-Ishii and Jin (2006) developed Phoneme to Morpheme (PtM) to implement ideas originally developed by Harris (1955). Harris noticed that if one proceeds incrementally through a sequence of phonemes and asks speakers of the language to count the letters that could appear next in the sequence (today called the successor count), the points where the number increases often correspond to morpheme boundaries. Tanaka-Ishii and Jin cor541 rectly recognized that this idea was an early version of branching entropy, given by HB(seq) = − EKES P(c|seq) log P(c|seq), where 5 is the set of successors to seq. They designed their PtM algorithm based on branching entropy in both directions, and it was able to achieve scores</context>
</contexts>
<marker>Harris, 1955</marker>
<rawString>Zellig S. Harris. 1955. From Phoneme to Morpheme. Language, 31(2):190–222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Hewlett</author>
<author>Paul Cohen</author>
</authors>
<title>Bootstrap Voting Experts.</title>
<date>2009</date>
<booktitle>In Proceedings of the Twenty-first International Joint Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="9572" citStr="Hewlett and Cohen (2009)" startWordPosition="1545" endWordPosition="1548"> place boundaries after sequences that have low internal entropy (surprisal), given by HI(seq) = − log P(seq), the other votes after sequences that have high branching entropy. All sequences are evaluated locally, within a sliding window, so the algorithm is very efficient. A boundary is generated whenever the vote total at a given location exceeds a threshold, and in some cases only if the vote total is a local maximum. VE thus has three parameters that can be manipulated to generate potential segmentations: Window size, threshold, and local maximum. Pairing VE with MDL was first examined by Hewlett and Cohen (2009). We generated a set of 104 segmentations by trying every viable threshold and local max setting for each window size between 2 and 9. 3.3 Bootstrapped Voting Experts The Bootstrapped Voting Experts (BVE) algorithm (Hewlett and Cohen, 2009) is an extension to VE. BVE works by segmenting the corpus repeatedly, with each new segmentation incorporating knowledge gained from previous segmentations. As with many bootstrapping methods, three essential components are required: some initial seed knowledge, a way to represent knowledge, and a way to leverage that knowledge to improve future performance</context>
</contexts>
<marker>Hewlett, Cohen, 2009</marker>
<rawString>Daniel Hewlett and Paul Cohen. 2009. Bootstrap Voting Experts. In Proceedings of the Twenty-first International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J E Hopcroft</author>
<author>J D Ullman</author>
</authors>
<title>Introduction to Automata Theory, Languages, and Computation.</title>
<date>1979</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="17088" citStr="Hopcroft and Ullman, 1979" startWordPosition="2770" endWordPosition="2773">e6 VE+MDL 3.41e5 5.75e5 8.24e5 1.54e6 1.23e6 BVE+MDL 3.13e5 5.29e5 7.64e5 1.56e6 1.13e6 Gold Standard 2.99e5 5.07e5 7.06e5 1.62e6 1.11e6 Table 6: Best description length achieved by each algorithm compared to the actual description length of the corpus. 5 Related Work The algorithms described in Section 3 are all relatively recent algorithms based on entropy. Many algorithms for computational morphology make use of concepts similar to branching entropy, such as successor count. The HubMorph algorithm (Johnson and Martin, 2003) adds all known words to a trie and then performs DFA minimization (Hopcroft and Ullman, 1979) to convert the trie to a finite state machine. In this DFA, it searches for sequences of states (stretched hubs) with low branching factor internally and high branching factor at the boundaries, which is analogous to the chunk signature that drives VE and BVE, as well as the role of branching entropy in PtM. MDL is analogous to Bayesian inference, where the information cost of the model CODE(M) acts as the prior distribution over models P(M), and CODE(DIM), the information cost of the data given the model, acts as the likelihood function P(DIM). Thus, Bayesian word segmentation methods may be</context>
</contexts>
<marker>Hopcroft, Ullman, 1979</marker>
<rawString>J. E. Hopcroft and J. D. Ullman. 1979. Introduction to Automata Theory, Languages, and Computation. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chu-Ren Huang</author>
</authors>
<date>2007</date>
<booktitle>Tagged Chinese Gigaword (Catalog LDC2007T03). Linguistic Data Consortium,</booktitle>
<location>Philadephia.</location>
<contexts>
<context position="14375" citStr="Huang, 2007" startWordPosition="2323" endWordPosition="2324">P WR WF PtM+MDL 0.694 0.833 0.758 0.421 0.505 0.459 VE+MDL 0.788 0.774 0.781 0.498 0.489 0.493 BVE+MDL 0.841 0.828 0.834 0.585 0.577 0.581 HVE-3E 0.796 0.771 0.784 0.512 0.496 0.504 ME 0.809 0.787 0.798 NR 0.542 NR Table 2: Results for the first 50,000 characters of 1984. Chinese and Thai are both commonly written without spaces between words, though some punctuation is often included. Because of this, these languages provide an excellent real-world challenge for unsupervised segmentation. The results shown in Table 3 were obtained using the first 100,000 words of the Chinese Gigaword corpus (Huang, 2007), written in Chinese characters. The word boundaries specified in the Chinese Gigaword Corpus were used as a gold standard. Table 4 shows results for a roughly 100,000 word subset of a corpus of Thai novels written in the Thai script, taken from a recent Thai word segmentation competition, InterBEST 2009. Working with a similar but much larger corpus of Thai text, Zhikov et al. were able to achieve slightly better performance (BF=0.934, WF=0.822). Algorithm BP BR BF WP WR WF PtM+MDL 0.894 0.610 0.725 0.571 0.390 0.463 VE+MDL 0.871 0.847 0.859 0.657 0.639 0.648 BVE+MDL 0.834 0.914 0.872 0.654 0</context>
</contexts>
<marker>Huang, 2007</marker>
<rawString>Chu-Ren Huang. 2007. Tagged Chinese Gigaword (Catalog LDC2007T03). Linguistic Data Consortium, Philadephia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Howard Johnson</author>
<author>Joel Martin</author>
</authors>
<title>Unsupervised learning of morphology for English and Inuktitut.</title>
<date>2003</date>
<booktitle>Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (HLTNAACL</booktitle>
<pages>43--45</pages>
<contexts>
<context position="16994" citStr="Johnson and Martin, 2003" startWordPosition="2753" endWordPosition="2757">edge trie in BVE. Algorithm BR87 Orwell SWB CGW Thai PtM+MDL 3.43e5 6.10e5 8.79e5 1.80e6 1,23e6 VE+MDL 3.41e5 5.75e5 8.24e5 1.54e6 1.23e6 BVE+MDL 3.13e5 5.29e5 7.64e5 1.56e6 1.13e6 Gold Standard 2.99e5 5.07e5 7.06e5 1.62e6 1.11e6 Table 6: Best description length achieved by each algorithm compared to the actual description length of the corpus. 5 Related Work The algorithms described in Section 3 are all relatively recent algorithms based on entropy. Many algorithms for computational morphology make use of concepts similar to branching entropy, such as successor count. The HubMorph algorithm (Johnson and Martin, 2003) adds all known words to a trie and then performs DFA minimization (Hopcroft and Ullman, 1979) to convert the trie to a finite state machine. In this DFA, it searches for sequences of states (stretched hubs) with low branching factor internally and high branching factor at the boundaries, which is analogous to the chunk signature that drives VE and BVE, as well as the role of branching entropy in PtM. MDL is analogous to Bayesian inference, where the information cost of the model CODE(M) acts as the prior distribution over models P(M), and CODE(DIM), the information cost of the data given the </context>
</contexts>
<marker>Johnson, Martin, 2003</marker>
<rawString>Howard Johnson and Joel Martin. 2003. Unsupervised learning of morphology for English and Inuktitut. Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology (HLTNAACL 2003), pages 43–45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian MacWhinney</author>
</authors>
<title>The CHILDES project: Tools for analyzing talk. Lawrence Erlbaum Associates, Mahwah, NJ, 3rd editio edition.</title>
<date>2000</date>
<contexts>
<context position="12188" citStr="MacWhinney, 2000" startWordPosition="1967" endWordPosition="1968">ction. From each set of candidates, results for the segmentation with minimal description length are presented in the tables below. Where possible, results for other algorithms are presented in italics, with semi-supervised algorithms set apart. Source code for all algorithms evaluated here, as well as data files for all corpora, are available online1. One of the most commonly-used benchmark corpora for unsupervised word segmentation is the BR87 corpus. This corpus is a phonemic encoding of the Bernstein Ratner corpus (Bernstein Ratner, 1987) from the CHILDES database of childdirected speech (MacWhinney, 2000). The perfor1http://code.google.com/p/voting-experts 542 mance of the algorithms on BR87 is shown in Table 1 below. As with all experiments in this work, the input was presented as one continuous sequence of characters with no word or sentence boundaries. Published results for two unsupervised algorithms, the MDL-based algorithm of Yu (2000) and the EntropyMDL (EMDL) algorithm of Zhikov et al. (2010), on this widely-used benchmark corpus are shown in italics. Set apart in the table are published results for three semi-supervised algorithms, MBDP-1 (Brent, 1999), HDP (Goldwater, 2007), and Word</context>
</contexts>
<marker>MacWhinney, 2000</marker>
<rawString>Brian MacWhinney. 2000. The CHILDES project: Tools for analyzing talk. Lawrence Erlbaum Associates, Mahwah, NJ, 3rd editio edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Miller</author>
<author>Alexander Stoytchev</author>
</authors>
<title>Hierarchical Voting Experts: An Unsupervised Algorithm for Hierarchical Sequence Segmentation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 7th IEEE International Conference on Development and Learning,</booktitle>
<pages>186--191</pages>
<contexts>
<context position="13577" citStr="Miller and Stoytchev, 2008" startWordPosition="2192" endWordPosition="2195">+MDL 0.861 0.897 0.879 0.676 0.704 0.690 VE+MDL 0.875 0.803 0.838 0.614 0.563 0.587 BVE+MDL 0.949 0.879 0.913 0.793 0.734 0.762 Yu 0.722 0.724 0.723 NR NR NR EMDL NR NR 0.907 NR NR 0.750 MBDP-1 0.803 0.843 0.823 0.670 0.694 0.682 HDP 0.903 0.808 0.852 0.752 0.696 0.723 WordEnds 0.946 0.737 0.829 NR NR 0.707 Table 1: Results for the BR87 corpus. Results for one corpus, the first 50,000 characters of George Orwell’s 1984, have been reported in nearly every VE-related paper. It thus provides a good opportunity to compare to the other VEderived algorithms: Hierarchical Voting Experts – 3 Experts (Miller and Stoytchev, 2008) and Markov Experts (Cheng and Mitzenmacher, 2005). Table 2 shows the results for candidate algorithms as well as the two other VE-derived algorithms, HVE-3E and ME. Algorithm BP BR BF WP WR WF PtM+MDL 0.694 0.833 0.758 0.421 0.505 0.459 VE+MDL 0.788 0.774 0.781 0.498 0.489 0.493 BVE+MDL 0.841 0.828 0.834 0.585 0.577 0.581 HVE-3E 0.796 0.771 0.784 0.512 0.496 0.504 ME 0.809 0.787 0.798 NR 0.542 NR Table 2: Results for the first 50,000 characters of 1984. Chinese and Thai are both commonly written without spaces between words, though some punctuation is often included. Because of this, these la</context>
</contexts>
<marker>Miller, Stoytchev, 2008</marker>
<rawString>Matthew Miller and Alexander Stoytchev. 2008. Hierarchical Voting Experts: An Unsupervised Algorithm for Hierarchical Sequence Segmentation. In Proceedings of the 7th IEEE International Conference on Development and Learning, pages 186–191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorma Rissanen</author>
</authors>
<title>A Universal Prior for Integers and Estimation by Minimum Description Length. The Annals of Statistics,</title>
<date>1983</date>
<contexts>
<context position="4033" citStr="Rissanen, 1983" startWordPosition="617" endWordPosition="618">tland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics Now, selecting the best segmentation is treated as a model selection problem, where each segmentation provides a different model of the corpus. Intuitively, a general approach is to choose the simplest model that explains the data, a principle known as Occam’s Razor. In information theory, this intuitive principle of simplicity or parsimony has been formalized as the Minimum Description Length (MDL) principle, which states that the most likely model of the data is the one that requires the fewest bits to encode (Rissanen, 1983). The number of bits required to represent a model is called its description length. Previous work applying the MDL principle to segmentation (Yu, 2000; Argamon et al., 2004; Zhikov et al., 2010) is motivated by the observation that every segmentation of a corpus implicitly defines a lexicon, or set of words. More formally, the segmented corpus S is a list of words s1s2 ... sN. L(S), the lexicon implicitly defined by S, is simply the set of unique words in S. The description length of S can then be broken into two components, the description length of the lexicon and the description length of </context>
<context position="5967" citStr="Rissanen, 1983" startWordPosition="968" endWordPosition="969">treated as a separate corpus consisting of characters rather than words. The description length can then be computed in the usual manner, by summing the surprisal of each character in each word in the lexicon: CODE(L(S)) _ − EwEL(S) EkEw log P(k) (3) where k E w refers to the characters in word w in the lexicon. As noted by Zhikov et al. (Zhikov et al., 2010), an additional term is needed for the information required to encode the parameters of the lexicon model. This quantity is normally estimated by (k/2) log n, where k is the degrees of freedom in the model and n is the length of the data (Rissanen, 1983). Substituting the appropriate values for the lexicon model yields: |L(S) |− 1 * log N (4) 2 The full description length calculation is simply the sum of three terms shown in 2, 3, and 4. From this definition, it follows that a low description length will be achieved by a segmentation that defines a small lexicon, which nonetheless reduces the corpus to a short series of mostly high-frequency words. 3 Generating Candidate Segmentations Recent unsupervised MDL algorithms rely on heuristic methods to generate candidate segmentations. Yu (2000) makes simplifying assumptions about the nature of th</context>
</contexts>
<marker>Rissanen, 1983</marker>
<rawString>Jorma Rissanen. 1983. A Universal Prior for Integers and Estimation by Minimum Description Length. The Annals of Statistics, 11(2):416–431.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kumiko Tanaka-Ishii</author>
<author>Zhihui Jin</author>
</authors>
<title>From Phoneme to Morpheme: Another Verification Using a Corpus.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computer Processing of Oriental Languages,</booktitle>
<pages>234--244</pages>
<contexts>
<context position="7511" citStr="Tanaka-Ishii and Jin (2006)" startWordPosition="1199" endWordPosition="1202">cription length. We selected three entropy-based algorithms for generating candidate segmentations, because such algorithms do not depend on the details of any particular language. By “unsupervised,” we mean operating on a single unbroken sequence of characters without any boundary information; Excluded from consideration are a class of algorithms that are semisupervised because they require sentence boundaries to be provided. Such algorithms include MBDP-1 (Brent, 1999), HDP (Goldwater et al., 2009), and WordEnds (Fleck, 2008), each of which is discussed in Section 5. 3.1 Phoneme to Morpheme Tanaka-Ishii and Jin (2006) developed Phoneme to Morpheme (PtM) to implement ideas originally developed by Harris (1955). Harris noticed that if one proceeds incrementally through a sequence of phonemes and asks speakers of the language to count the letters that could appear next in the sequence (today called the successor count), the points where the number increases often correspond to morpheme boundaries. Tanaka-Ishii and Jin cor541 rectly recognized that this idea was an early version of branching entropy, given by HB(seq) = − EKES P(c|seq) log P(c|seq), where 5 is the set of successors to seq. They designed their P</context>
</contexts>
<marker>Tanaka-Ishii, Jin, 2006</marker>
<rawString>Kumiko Tanaka-Ishii and Zhihui Jin. 2006. From Phoneme to Morpheme: Another Verification Using a Corpus. In Proceedings of the 21st International Conference on Computer Processing of Oriental Languages, pages 234–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anand Venkataraman</author>
</authors>
<title>A procedure for unsupervised lexicon learning.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning.</booktitle>
<contexts>
<context position="17846" citStr="Venkataraman (2001)" startWordPosition="2897" endWordPosition="2898"> internally and high branching factor at the boundaries, which is analogous to the chunk signature that drives VE and BVE, as well as the role of branching entropy in PtM. MDL is analogous to Bayesian inference, where the information cost of the model CODE(M) acts as the prior distribution over models P(M), and CODE(DIM), the information cost of the data given the model, acts as the likelihood function P(DIM). Thus, Bayesian word segmentation methods may be considered related as well. Indeed, one of the early Bayesian methods, MBDP-1 (Brent, 1999) was adapted from an earlier MDL-based method. Venkataraman (2001) simplified MBDP-1, relaxed some of its assumptions while preserving the same level of performance. Recently, Bayesian methods with more sophisticated language models have been developed, including one that models language generation as a hierarchical Dirichlet process (HDP), in order to incorporate the effects of syntax into word segmentation (Goldwater et al., 2009). Another recent algorithm, WordEnds, generalizes information about the distribution of characters near 6 Discussion For the five corpora tested above, BVE achieved the best performance in conjunction with MDL, and also achieved t</context>
</contexts>
<marker>Venkataraman, 2001</marker>
<rawString>Anand Venkataraman. 2001. A procedure for unsupervised lexicon learning. In Proceedings of the Eighteenth International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Yu</author>
</authors>
<title>Unsupervised Word Induction using MDL Criterion.</title>
<date>2000</date>
<booktitle>In Proceedings of the International Symposium of Chinese Spoken Language Processing,</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="4184" citStr="Yu, 2000" startWordPosition="642" endWordPosition="643">, where each segmentation provides a different model of the corpus. Intuitively, a general approach is to choose the simplest model that explains the data, a principle known as Occam’s Razor. In information theory, this intuitive principle of simplicity or parsimony has been formalized as the Minimum Description Length (MDL) principle, which states that the most likely model of the data is the one that requires the fewest bits to encode (Rissanen, 1983). The number of bits required to represent a model is called its description length. Previous work applying the MDL principle to segmentation (Yu, 2000; Argamon et al., 2004; Zhikov et al., 2010) is motivated by the observation that every segmentation of a corpus implicitly defines a lexicon, or set of words. More formally, the segmented corpus S is a list of words s1s2 ... sN. L(S), the lexicon implicitly defined by S, is simply the set of unique words in S. The description length of S can then be broken into two components, the description length of the lexicon and the description length of the corpus given the lexicon. If we consider S as being generated by sampling words from a probability distribution over words in the lexicon, the numb</context>
<context position="6514" citStr="Yu (2000)" startWordPosition="1056" endWordPosition="1057"> in the model and n is the length of the data (Rissanen, 1983). Substituting the appropriate values for the lexicon model yields: |L(S) |− 1 * log N (4) 2 The full description length calculation is simply the sum of three terms shown in 2, 3, and 4. From this definition, it follows that a low description length will be achieved by a segmentation that defines a small lexicon, which nonetheless reduces the corpus to a short series of mostly high-frequency words. 3 Generating Candidate Segmentations Recent unsupervised MDL algorithms rely on heuristic methods to generate candidate segmentations. Yu (2000) makes simplifying assumptions about the nature of the lexicon, and then performs an Expectation-Maximization (EM) search over this reduced hypothesis space. Zhikov et al. (2010) present an algorithm called EntropyMDL that generates a candidate segmentation based on branching entropy, and then iteratively refines the segmentation in an attempt to greedily minimize description length. We selected three entropy-based algorithms for generating candidate segmentations, because such algorithms do not depend on the details of any particular language. By “unsupervised,” we mean operating on a single </context>
<context position="12531" citStr="Yu (2000)" startWordPosition="2019" endWordPosition="2020">e1. One of the most commonly-used benchmark corpora for unsupervised word segmentation is the BR87 corpus. This corpus is a phonemic encoding of the Bernstein Ratner corpus (Bernstein Ratner, 1987) from the CHILDES database of childdirected speech (MacWhinney, 2000). The perfor1http://code.google.com/p/voting-experts 542 mance of the algorithms on BR87 is shown in Table 1 below. As with all experiments in this work, the input was presented as one continuous sequence of characters with no word or sentence boundaries. Published results for two unsupervised algorithms, the MDL-based algorithm of Yu (2000) and the EntropyMDL (EMDL) algorithm of Zhikov et al. (2010), on this widely-used benchmark corpus are shown in italics. Set apart in the table are published results for three semi-supervised algorithms, MBDP-1 (Brent, 1999), HDP (Goldwater, 2007), and WordEnds (Fleck, 2008), described in Section 5. These algorithms operate on a version of the corpus that includes sentence boundaries. Algorithm BP BR BF WP WR WF PtM+MDL 0.861 0.897 0.879 0.676 0.704 0.690 VE+MDL 0.875 0.803 0.838 0.614 0.563 0.587 BVE+MDL 0.949 0.879 0.913 0.793 0.734 0.762 Yu 0.722 0.724 0.723 NR NR NR EMDL NR NR 0.907 NR NR </context>
</contexts>
<marker>Yu, 2000</marker>
<rawString>Hua Yu. 2000. Unsupervised Word Induction using MDL Criterion. In Proceedings of the International Symposium of Chinese Spoken Language Processing, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin Zhikov</author>
<author>Hiroya Takamura</author>
<author>Manabu Okumura</author>
</authors>
<title>An Efficient Algorithm for Unsupervised Word Segmentation with Branching Entropy and MDL.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>832--842</pages>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="4228" citStr="Zhikov et al., 2010" startWordPosition="648" endWordPosition="651">es a different model of the corpus. Intuitively, a general approach is to choose the simplest model that explains the data, a principle known as Occam’s Razor. In information theory, this intuitive principle of simplicity or parsimony has been formalized as the Minimum Description Length (MDL) principle, which states that the most likely model of the data is the one that requires the fewest bits to encode (Rissanen, 1983). The number of bits required to represent a model is called its description length. Previous work applying the MDL principle to segmentation (Yu, 2000; Argamon et al., 2004; Zhikov et al., 2010) is motivated by the observation that every segmentation of a corpus implicitly defines a lexicon, or set of words. More formally, the segmented corpus S is a list of words s1s2 ... sN. L(S), the lexicon implicitly defined by S, is simply the set of unique words in S. The description length of S can then be broken into two components, the description length of the lexicon and the description length of the corpus given the lexicon. If we consider S as being generated by sampling words from a probability distribution over words in the lexicon, the number of bits required to represent each word s</context>
<context position="5713" citStr="Zhikov et al., 2010" startWordPosition="920" endWordPosition="923">tation, we must also include the cost of the lexicon. Adding in the description length of the lexicon forces a trade-off between the lexicon size and the size of the compressed corpus. For purposes of the description length calculation, the lexicon is simply treated as a separate corpus consisting of characters rather than words. The description length can then be computed in the usual manner, by summing the surprisal of each character in each word in the lexicon: CODE(L(S)) _ − EwEL(S) EkEw log P(k) (3) where k E w refers to the characters in word w in the lexicon. As noted by Zhikov et al. (Zhikov et al., 2010), an additional term is needed for the information required to encode the parameters of the lexicon model. This quantity is normally estimated by (k/2) log n, where k is the degrees of freedom in the model and n is the length of the data (Rissanen, 1983). Substituting the appropriate values for the lexicon model yields: |L(S) |− 1 * log N (4) 2 The full description length calculation is simply the sum of three terms shown in 2, 3, and 4. From this definition, it follows that a low description length will be achieved by a segmentation that defines a small lexicon, which nonetheless reduces the </context>
<context position="12591" citStr="Zhikov et al. (2010)" startWordPosition="2027" endWordPosition="2030">ra for unsupervised word segmentation is the BR87 corpus. This corpus is a phonemic encoding of the Bernstein Ratner corpus (Bernstein Ratner, 1987) from the CHILDES database of childdirected speech (MacWhinney, 2000). The perfor1http://code.google.com/p/voting-experts 542 mance of the algorithms on BR87 is shown in Table 1 below. As with all experiments in this work, the input was presented as one continuous sequence of characters with no word or sentence boundaries. Published results for two unsupervised algorithms, the MDL-based algorithm of Yu (2000) and the EntropyMDL (EMDL) algorithm of Zhikov et al. (2010), on this widely-used benchmark corpus are shown in italics. Set apart in the table are published results for three semi-supervised algorithms, MBDP-1 (Brent, 1999), HDP (Goldwater, 2007), and WordEnds (Fleck, 2008), described in Section 5. These algorithms operate on a version of the corpus that includes sentence boundaries. Algorithm BP BR BF WP WR WF PtM+MDL 0.861 0.897 0.879 0.676 0.704 0.690 VE+MDL 0.875 0.803 0.838 0.614 0.563 0.587 BVE+MDL 0.949 0.879 0.913 0.793 0.734 0.762 Yu 0.722 0.724 0.723 NR NR NR EMDL NR NR 0.907 NR NR 0.750 MBDP-1 0.803 0.843 0.823 0.670 0.694 0.682 HDP 0.903 0</context>
</contexts>
<marker>Zhikov, Takamura, Okumura, 2010</marker>
<rawString>Valentin Zhikov, Hiroya Takamura, and Manabu Okumura. 2010. An Efficient Algorithm for Unsupervised Word Segmentation with Branching Entropy and MDL. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 832–842, Cambridge, MA. MIT Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>