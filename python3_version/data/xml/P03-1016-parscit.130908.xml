<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.984594">
Synonymous Collocation Extraction Using Translation Information
</title>
<author confidence="0.947643">
Hua WU, Ming ZHOU
</author>
<affiliation confidence="0.943621">
Microsoft Research Asia
</affiliation>
<address confidence="0.8867895">
5F Sigma Center, No.49 Zhichun Road, Haidian District
Beijing, 100080, China
</address>
<email confidence="0.972251">
wu_hua_@msn.com, mingzhou@microsoft.com
</email>
<sectionHeader confidence="0.993625" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999887433333333">
Automatically acquiring synonymous col-
location pairs such as &lt;turn on, OBJ, light&gt;
and &lt;switch on, OBJ, light&gt; from corpora
is a challenging task. For this task, we can,
in general, have a large monolingual corpus
and/or a very limited bilingual corpus.
Methods that use monolingual corpora
alone or use bilingual corpora alone are
apparently inadequate because of low pre-
cision or low coverage. In this paper, we
propose a method that uses both these re-
sources to get an optimal compromise of
precision and coverage. This method first
gets candidates of synonymous collocation
pairs based on a monolingual corpus and a
word thesaurus, and then selects the ap-
propriate pairs from the candidates using
their translations in a second language. The
translations of the candidates are obtained
with a statistical translation model which is
trained with a small bilingual corpus and a
large monolingual corpus. The translation
information is proved as effective to select
synonymous collocation pairs. Experi-
mental results indicate that the average
precision and recall of our approach are
74% and 64% respectively, which outper-
form those methods that only use mono-
lingual corpora and those that only use bi-
lingual corpora.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999861386363637">
This paper addresses the problem of automatically
extracting English synonymous collocation pairs
using translation information. A synonymous col-
location pair includes two collocations which are
similar in meaning, but not identical in wording.
Throughout this paper, the term collocation refers
to a lexically restricted word pair with a certain
syntactic relation. For instance, &lt;turn on, OBJ,
light&gt; is a collocation with a syntactic relation
verb-object, and &lt;turn on, OBJ, light&gt; and &lt;switch
on, OBJ, light&gt; are a synonymous collocation pair.
In this paper, translation information means trans-
lations of collocations and their translation prob-
abilities.
Synonymous collocations can be considered as
an extension of the concept of synonymous ex-
pressions which conventionally include synony-
mous words, phrases and sentence patterns. Syn-
onymous expressions are very useful in a number of
NLP applications. They are used in information
retrieval and question answering (Kiyota et al.,
2002; Dragomia et al., 2001) to bridge the expres-
sion gap between the query space and the document
space. For instance, “buy book” extracted from the
users’ query should also in some way match “order
book” indexed in the documents. Besides, the
synonymous expressions are also important in
language generation (Langkilde and Knight, 1998)
and computer assisted authoring to produce vivid
texts.
Up to now, there have been few researches
which directly address the problem of extracting
synonymous collocations. However, a number of
studies investigate the extraction of synonymous
words from monolingual corpora (Carolyn et al.,
1992; Grefenstatte, 1994; Lin, 1998; Gasperin et al.,
2001). The methods used the contexts around the
investigated words to discover synonyms. The
problem of the methods is that the precision of the
extracted synonymous words is low because it
extracts many word pairs such as “cat” and “dog”,
which are similar but not synonymous. In addition,
some studies investigate the extraction of synony-
mous words and/or patterns from bilingual corpora
(Barzilay and Mckeown, 2001; Shimohata and
Sumita, 2002). However, these methods can only
extract synonymous expressions which occur in the
bilingual corpus. Due to the limited size of the
bilingual corpus, the coverage of the extracted
expressions is very low.
Given the fact that we usually have large mono-
lingual corpora (unlimited in some sense) and very
limited bilingual corpora, this paper proposes a
method that tries to make full use of these different
resources to get an optimal compromise of preci-
sion and coverage for synonymous collocation
extraction. We first obtain candidates of synony-
mous collocation pairs based on a monolingual
corpus and a word thesaurus. We then select those
appropriate candidates using their translations in a
second language. Each translation of the candidates
is assigned a probability with a statistical translation
model that is trained with a small bilingual corpus
and a large monolingual corpus. The similarity of
two collocations is estimated by computing the
similarity of their vectors constructed with their
corresponding translations. Those candidates with
larger similarity scores are extracted as synony-
mous collocations. The basic assumption behind
this method is that two collocations are synony-
mous if their translations are similar. For example,
&lt;turn on, OBJ, light&gt; and &lt;switch on, OBJ, light&gt;
are synonymous because both of them are translated
into &lt;ff, OBJ, �T&gt; (&lt;kai1, OBJ, deng1&gt;) and &lt;#Tff,
OBJ, �T&gt; (&lt;da3 kai1, OBJ, deng1&gt;) in Chinese.
In order to evaluate the performance of our
method, we conducted experiments on extracting
three typical types of synonymous collocations.
Experimental results indicate that our approach
achieves 74% average precision and 64% recall
respectively, which considerably outperform those
methods that only use monolingual corpora or only
use bilingual corpora.
The remainder of this paper is organized as fol-
lows. Section 2 describes our synonymous colloca-
tion extraction method. Section 3 evaluates the
proposed method, and the last section draws our
conclusion and presents the future work.
</bodyText>
<sectionHeader confidence="0.955304" genericHeader="method">
2 Our Approach
</sectionHeader>
<bodyText confidence="0.9998545">
Our method for synonymous collocation extraction
comprises of three steps: (1) extract collocations
from large monolingual corpora; (2) generate can-
didates of synonymous collocation pairs with a
word thesaurus WordNet; (3) select synonymous
collocation candidates using their translations.
</bodyText>
<subsectionHeader confidence="0.997091">
2.1 Collocation Extraction
</subsectionHeader>
<bodyText confidence="0.9995904">
This section describes how to extract English col-
locations. Since Chinese collocations will be used
to train the language model in Section 2.3, they are
also extracted in the same way.
Collocations in this paper take some syntactical
relations (dependency relations), such as &lt;verb,
OBJ, noun&gt;, &lt;noun, ATTR, adj&gt;, and &lt;verb, MOD,
adv&gt;. These dependency triples, which embody the
syntactic relationship between words in a sentence,
are generated with a parser—we use NLPWIN in
this paper1. For example, the sentence “She owned
this red coat” is transformed to the following four
triples after parsing: &lt;own, SUBJ, she&gt;, &lt;own, OBJ,
coat&gt;, &lt;coat, DET, this&gt;, and &lt;coat, ATTR, red&gt;.
These triples are generally represented in the form
of &lt;Head, Relation Type, Modifier&gt;.
The measure we use to extract collocations
from the parsed triples is weighted mutual infor-
mation (WMI) (Fung and Mckeown, 1997), as
described as
</bodyText>
<equation confidence="0.994756">
WMI(w1,r,w2) = p(w1,r,
p
</equation>
<bodyText confidence="0.997384">
Those triples whose WMI values are larger than a
given threshold are taken as collocations. We do not
use the point-wise mutual information because it
tends to overestimate the association between two
words with low frequencies. Weighted mutual
information meliorates this effect by add-
ing p(w1, r, w2).
For expository purposes, we will only look into
three kinds of collocations for synonymous collo-
cation extraction: &lt;verb, OBJ, noun&gt;, &lt;noun,
ATTR, adj&gt; and &lt;verb, MOD, adv&gt;.
</bodyText>
<tableCaption confidence="0.998855">
Table 1. English Collocations
</tableCaption>
<table confidence="0.99094225">
Class #Type #Token
verb, OBJ, noun 506,628 7,005,455
noun, ATTR, adj 333,234 4,747,970
verb, Mod, adv 40,748 483,911
</table>
<tableCaption confidence="0.998109">
Table 2. Chinese Collocations
</tableCaption>
<bodyText confidence="0.914431571428571">
Class #Type #Token
verb, OBJ, noun 1,579,783 19,168,229
noun, ATTR, adj 311,560 5,383,200
verb, Mod, adv 546,054 9,467,103
The English collocations are extracted from
Wall Street Journal (1987-1992) and Association
Press (1988-1990), and the Chinese collocations are
</bodyText>
<footnote confidence="0.9688054">
1 The NLPWIN parser is developed at Microsoft Re-
search, which parses several languages including Chi-
nese and English. Its output can be a phrase structure
parse tree or a logical form which is represented with
dependency triples.
</footnote>
<equation confidence="0.997517375">
w2
)log
)
p(w1,r,
w2
|
r)p(w2  |r)p(r)
(w1
</equation>
<bodyText confidence="0.999596833333333">
extracted from People’s Daily (1980-1998). The
statistics of the extracted collocations are shown in
Table 1 and 2. The thresholds are set as 5 for both
English and Chinese. Token refers to the total
number of collocation occurrences and Type refers
to the number of unique collocations in the corpus.
</bodyText>
<subsectionHeader confidence="0.999768">
2.2 Candidate Generation
</subsectionHeader>
<bodyText confidence="0.998790770833333">
Candidate generation is based on the following
assumption: For a collocation &lt;Head, Relation
Type, Modifier&gt;, its synonymous expressions also
take the form of &lt;Head, Relation Type, Modifier&gt;
although sometimes they may also be a single word
or a sentence pattern.
The synonymous candidates of a collocation are
obtained by expanding a collocation &lt;Head, Rela-
tion Type, Modifier&gt; using the synonyms of Head
and Modifier. The synonyms of a word are obtained
from WordNet 1.6. In WordNet, one synset consists
of several synonyms which represent a single sense.
Therefore, polysemous words occur in more than
one synsets. The synonyms of a given word are
obtained from all the synsets including it. For ex-
ample, the word “turn on” is a polysemous word
and is included in several synsets. For the sense
“cause to operate by flipping a switch”, “switch on”
is one of its synonyms. For the sense “be contingent
on”, “depend on” is one of its synonyms. We take
both of them as the synonyms of “turn on” regard-
less of its meanings since we do not have sense tags
for words in collocations.
If we use Cw to indicate the synonym set of a
word w and U to denote the English collocation set
generated in Section 2.1. The detail algorithm on
generating candidates of synonymous collocation
pairs is described in Figure 1. For example, given a
collocation &lt;turn on, OBJ, light&gt;, we expand “turn
on” to “switch on”, “depend on”, and then expand
“light” to “lump”, “illumination”. With these
synonyms and the relation type OBJ, we generate
synonymous collocation candidates of &lt;turn on,
OBJ, light&gt;. The candidates are &lt;switch on, OBJ,
light&gt;, &lt;turn on, OBJ, lump&gt;, &lt;depend on, OBJ,
illumination&gt;, &lt;depend on, OBJ, light&gt; etc. Both
these candidates and the original collocation &lt;turn
on, OBJ, light&gt; are used to generate the synony-
mous collocation pairs.
With the above method, we obtained candidates
of synonymous collocation pairs. For example,
&lt;switch on, OBJ, light&gt; and &lt;turn on, OBJ, light&gt;
are a synonymous collocation pair. However, this
method also produces wrong synonymous colloca-
tion candidates. For example, &lt;depend on, OBJ,
illumination&gt; and &lt;turn on, OBJ, light&gt; is not a
synonymous pair. Thus, it is important to filter out
these inappropriate candidates.
</bodyText>
<listItem confidence="0.9062875">
(1) For each collocation (Co1i=&lt;Head, R, Modi-
fier&gt;)EU, do the following:
a. Use the synonyms in WordNet 1.6 to expand
Head and Modifier and get their synonym
sets CHead and CModifier
b. Generate the candidate set of its synonymous
collocations Si={&lt;w1, R, w2&gt;  |w1E{Head}
U CHead &amp; w2 E{Modifier}U CModifier &amp;
&lt;w1, R, w2&gt;EU &amp; &lt;w1, R, w2&gt; # Co1i }
(2) Generate the candidate set of synonymous
</listItem>
<figure confidence="0.5929285">
collocation pairs SC= {(Co1i, Co1j) |Co1i E
U &amp; Co1j ESi 1
</figure>
<figureCaption confidence="0.998605">
Figure 1. Candidate Set Generation Algorithm
</figureCaption>
<subsectionHeader confidence="0.996485">
2.3 Candidate Selection
</subsectionHeader>
<bodyText confidence="0.998200870967742">
In synonymous word extraction, the similarity of
two words can be estimated based on the similarity
of their contexts. However, this method cannot be
effectively extended to collocation similarity esti-
mation. For example, in sentences “They turned on
the lights” and “They depend on the illumination”,
the meaning of two collocations &lt;turn on, OBJ,
light&gt; and &lt;depend on, OBJ, illumination&gt; are
different although their contexts are the same.
Therefore, monolingual information is not enough
to estimate the similarity of two collocations.
However, the meanings of the above two colloca-
tions can be distinguished if they are translated into
a second language (e.g., Chinese). For example,
&lt;turn on, OBJ, light&gt; is translated into &lt;3f, OBJ, �
&gt; (&lt;kai1, OBJ, deng1) and &lt;4T3f, OBJ, �T&gt; (&lt;da3
kai1, OBJ, deng1&gt;) in Chinese while &lt;depend on,
OBJ, illumination&gt; is translated into &lt;IR&amp;-T, OBJ,
ARM&gt; (qu3 jue2 yu2, OBJ, guang1 zhao4 du4&gt;).
Thus, they are not synonymous pairs because their
translations are completely different.
In this paper, we select the synonymous collo-
cation pairs from the candidates in the following
way. First, given a candidate of synonymous col-
location pair generated in section 2.2, we translate
the two collocations into Chinese with a simple
statistical translation model. Second, we calculate
the similarity of two collocations with the feature
vectors constructed with their translations. A can-
didate is selected as a synonymous collocation pair
if its similarity exceeds a certain threshold.
</bodyText>
<subsubsectionHeader confidence="0.966846">
2.3.1 Collocation Translation
</subsubsectionHeader>
<bodyText confidence="0.999116142857143">
collocation can only be translated to the same type
of Chinese collocations3. Thus, p(re |rc) =1 in our
case. Equation (3) is rewritten as:
For an English collocation ecol=&lt;e1, re, e2&gt;, we
translate it into Chinese collocations 2 using an
English-Chinese dictionary. If the translation sets of
e1 and e2 are represented as CS1 and CS2 respec-
tively, the Chinese translations can be represented
as S={&lt;c1, rc, c2&gt; |c1ECS1 , c2 ECS2 , rc ER }, with R
denoting the relation set.
Given an English collocation ecol=&lt;e1, re, e2&gt;
and one of its Chinese collocation ccol=&lt;c1, rc,
c2&gt;ES, the probability that ecol is translated into ccol
is calculated as in Equation (1).
</bodyText>
<equation confidence="0.989597875">
p e r e c r c p c r c
( , ,  |, , ) ( , , )
1 e 2 1 c 2 1 c 2
p c e =
(  |) (1)
col col p e
( )
col
</equation>
<bodyText confidence="0.999898142857143">
According to Equation (1), we need to calculate the
translation probability p(ecol|ccol) and the target
language probability p(ccol). Calculating the trans-
lation probability needs a bilingual corpus. If the
above equation is used directly, we will run into the
data sparseness problem. Thus, model simplifica-
tion is necessary.
</bodyText>
<subsubsectionHeader confidence="0.57134">
2.3.2 Translation Model
</subsubsectionHeader>
<bodyText confidence="0.998312166666667">
Our simplification is made according to the fol-
lowing three assumptions.
Assumption 1: For a Chinese collocation ccol and re,
we assume that e1 and e2 are conditionally inde-
pendent. The translation model is rewritten as:
= p e r e c
</bodyText>
<equation confidence="0.9803432">
( , ,  |)
1 e 2 col
r c
 |)
e col
</equation>
<bodyText confidence="0.990119">
Assumption 2: Given a Chinese collocation &lt;c1, rc,
c2&gt;, we assume that the translation probability
p(ei|ccol) only depends on ei and ci (i=1,2), and
p(re|ccol) only depends on re and rc. Equation (2) is
rewritten as:
</bodyText>
<equation confidence="0.9582835">
pe c
(  |) (  |) (
</equation>
<bodyText confidence="0.877934">
= p e c p e
col col 1 col 2
It is equal to a word translation model if we take
the relation type in the collocations as an element
like a word, which is similar to Model 1 in (Brown
et al., 1993).
</bodyText>
<footnote confidence="0.604003">
Assumption 3: We assume that one type of English
2 Some English collocations can be translated into Chi-
nese words, phrases or patterns. Here we only consider
the case of being translated into collocations.
</footnote>
<equation confidence="0.996201">
p e c
(  |) (  |) (  |) (  |)
= p e c p e c p r r
col col 1 1 2 2 e c
)p(e2  |c2)
</equation>
<subsectionHeader confidence="0.39872">
2.3.3 Language Model
</subsectionHeader>
<bodyText confidence="0.999943571428571">
The language model p(ccol) is calculated with the
Chinese collocation database extracted in section
2.1. In order to tackle with the data sparseness
problem, we smooth the language model with an
interpolation method.
When the given Chinese collocation occurs in
the corpus, we calculate it as in (5).
</bodyText>
<equation confidence="0.8884058">
count c
( )
p c
( ) = col (5)
col N
</equation>
<bodyText confidence="0.999976625">
where count( ccol) represents the count of the Chi-
nese collocation ccol. N represents the total counts
of all the Chinese collocations in the training cor-
pus.
For a collocation &lt;c1, rc, c2&gt;, if we assume that
two words c1 and c2 are conditionally independent
given the relation rc, Equation (5) can be rewritten
as in (6).
</bodyText>
<equation confidence="0.9385226">
p ccol = p c r c p c r c p r c
( ) ( 1  |) ( 2  |) ( ) (6)
,*)
rc
count r c
(*, , )
c 2
p c r =
(  |) , p r
( ) =
2 c c
count r
(*, ,*) N
c
count(c1, rc,*) : frequency of the collocations with c1
</equation>
<bodyText confidence="0.992854142857143">
as the head and rc as the relation type.
count(*, rc , c2 ) : frequency of the collocations with
c2 as the modifier and rc as the relation type
count(*, rc ,*) : frequency of the collocations with rc
as the relation type.
With Equation (5) and (6), we get the interpolated
language model as shown in (7).
</bodyText>
<equation confidence="0.99622175">
p ( )
c =  + 
col 1 c 2 c c
count c
N ( )
col ( 1 - ) (  |) (  |) (
p c r p c r p r )
(7)
</equation>
<bodyText confidence="0.776515666666667">
where 0 &lt;  &lt; 1.  is a constant so that the prob-
abilities sum to 1.
, c
</bodyText>
<footnote confidence="0.890301666666667">
3 Zhou et al. (2001) found that about 70% of the Chinese
translations have the same relation type as the source
English collocations.
</footnote>
<equation confidence="0.954447470588235">
(
=
p
 |c
( 2
) (  |)
p r r
e c
e1  |c1)p(e2
p e
( col
=
p(e1 |
) (  |, ) (
p e r c p
2 e col
r c
</equation>
<figure confidence="0.981855875">
,
e col
)
 |ccol
(2)
r
 |c
 |c
col
) p(
col
e
)
(3)
p
=
(e1  |c1
(4)
where
c r
1p c r =
(  |)
1 c
count
count
(
(*,
,*)
*)
count
(* r c
, ,
</figure>
<subsubsectionHeader confidence="0.841757">
2.3.4 Word Translation Probability Estimation
</subsubsectionHeader>
<bodyText confidence="0.999817818181818">
Many methods are used to estimate word translation
probabilities from unparallel or parallel bilingual
corpora (Koehn and Knight, 2000; Brown et al.,
1993). In this paper, we use a parallel bilingual
corpus to train the word translation probabilities
based on the result of word alignment with a bi-
lingual Chinese-English dictionary. The alignment
method is described in (Wang et al., 2001). In order
to deal with the problem of data sparseness, we
conduct a simple smoothing by adding 0.5 to the
counts of each translation pair as in (8).
</bodyText>
<equation confidence="0.768464">
( ) 0.5* |
c + trans e
_
</equation>
<bodyText confidence="0.997827">
where  |trans _ e  |represents the number of Eng-
lish translations for a given Chinese word c.
</bodyText>
<subsubsectionHeader confidence="0.981886">
2.3.5 Collocation Similarity Calculation
</subsubsectionHeader>
<bodyText confidence="0.999600142857143">
For each synonymous collocation pair, we get its
corresponding Chinese translations and calculate
the translation probabilities as in section 2.3.1.
These Chinese collocations with their correspond-
ing translation probabilities are taken as feature
vectors of the English collocations, which can be
represented as:
</bodyText>
<equation confidence="0.9453105">
i im
=&lt; ( 1 , 1 ) , ( 2 , 2 ), ... , ( ,)
i c p im
c p c p
i i &gt;
col col col col col col
</equation>
<bodyText confidence="0.999970333333333">
The similarity of two collocations is defined as in
(9). The candidate pairs whose similarity scores
exceed a given threshold are selected.
</bodyText>
<equation confidence="0.98147775">
( 1 2 1 2
ecol , ecol) = cos( Fecol , Fecol
(9)
=
</equation>
<bodyText confidence="0.9996872">
For example, given a synonymous collocation
pair &lt;turn on, OBJ, light&gt; and &lt;switch on, OBJ,
light&gt;, we first get their corresponding feature
vectors.
The feature vector of &lt;turn on, OBJ, light&gt;:
</bodyText>
<construct confidence="0.618112666666667">
&lt; (&lt;ᓔ, OBJ, ♃&gt;, 0.04692), (&lt;ᠧᓔ, OBJ, ♃&gt;,
0.01602), ... , (&lt;ձ䌪, OBJ, ܝ&gt;, 0.0002710), (&lt;ձ䌪,
OBJ, ܝ✻ᑺ&gt;, 0.0000305) &gt;
</construct>
<bodyText confidence="0.908184">
The feature vector of &lt;switch on, OBJ, light&gt;:
</bodyText>
<construct confidence="0.644934">
&lt; (&lt;ᠧᓔ, OBJ, ♃&gt;, 0.04238), (&lt;ᓔ, OBJ, ♃&gt;,
0.01257), (&lt;ᠧᓔ, OBJ, ♃ܝ&gt;, 0.002531), ... , (&lt;ᓔ,
OBJ, ֵো♃&gt;, 0.00003542) &gt;
</construct>
<bodyText confidence="0.99725075">
The values in the feature vector are translation
probabilities. With these two vectors, we get the
similarity of &lt;turn on, OBJ, light&gt; and &lt;switch on,
OBJ, light&gt;, which is 0.2348.
</bodyText>
<subsectionHeader confidence="0.998058">
2.4 Implementation of our Approach
</subsectionHeader>
<bodyText confidence="0.999963692307692">
We use an English-Chinese dictionary to get the
Chinese translations of collocations, which includes
219,404 English words. Each source word has 3
translation words on average. The word translation
probabilities are estimated from a bilingual corpus
that obtains 170,025 pairs of Chinese-English sen-
tences, including about 2.1 million English words
and about 2.5 million Chinese words.
With these data and the collocations in section
2.1, we produced 93,523 synonymous collocation
pairs and filtered out 1,060,788 candidate pairs with
our translation method if we set the similarity
threshold to 0.01.
</bodyText>
<sectionHeader confidence="0.998796" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999759333333333">
To evaluate the effectiveness of our methods, two
experiments have been conducted. The first one is
designed to compare our method with two methods
that use monolingual corpora. The second one is
designed to compare our method with a method that
uses a bilingual corpus.
</bodyText>
<subsectionHeader confidence="0.9997925">
3.1 Comparison with Methods using
Monolingual Corpora
</subsectionHeader>
<bodyText confidence="0.999992285714286">
We compared our approach with two methods that
use monolingual corpora. These two methods also
employed the candidate generation described in
section 2.2. The difference is that the two methods
use different strategies to select appropriate candi-
dates. The training corpus for these two methods is
the same English one as in Section 2.1.
</bodyText>
<subsectionHeader confidence="0.889839">
3.1.1 Method Description
</subsectionHeader>
<bodyText confidence="0.999389090909091">
Method 1: This method uses monolingual contexts
to select synonymous candidates. The purpose of
this experiment is to see whether the context
method for synonymous word extraction can be
effectively extended to synonymous collocation
extraction.
The similarity of two collocations is calculated
with their feature vectors. The feature vector of a
collocation is constructed by all words in sentences
which surround the given collocation. The context
vector for collocation i is represented as in (10).
</bodyText>
<equation confidence="0.945995358974359">
( , ) 0.5
e c +
=
count
)
 |c
(e
p
count
 |(8)
Fei
col
sim
)
col
pol p
1i * 2 j
1
col
c
i
j
2
c
=
col
I (p 1il )2 * E (p of )
j
2
i
Fe i =&lt; ( 1 , 1 ) , ( 2 , 2 ),..., ( , )
w p w p w p &gt;(10)
col i i i i im im
where pij N
wij: context word j of collocation i.
pij : probability of wij co-occurring with i
ecol .
count wij ecol : frequency of the context word wij
( , i )
</equation>
<bodyText confidence="0.616255">
co-occurring with the collocation i
</bodyText>
<subsubsectionHeader confidence="0.296379">
ecol
</subsubsectionHeader>
<bodyText confidence="0.982405166666667">
N: all counts of the words in the training corpus.
With the feature vectors, the similarity of two col-
locations is calculated as in (11). Those candidates
whose similarities exceed a given threshold are
selected as synonymous collocations.
collocations, from which we randomly selected
1,300 pairs to construct a test set. Each pair was
evaluated independently by two judges to see if it is
synonymous. Only those agreed upon by two judges
are considered as synonymous pairs. The statistics
of the test set is shown in Table 3. We evaluated
three types of synonymous collocations: &lt;verb,
OBJ, noun&gt;, &lt;noun, ATTR, adj&gt;, &lt;verb, MOD,
adv&gt;. For the type &lt;verb, OBJ, noun&gt;, among the
630 synonymous collocation candidate pairs, 197
pairs are correct. For &lt;noun, ATTR, adj&gt;, 163 pairs
(among 324 pairs) are correct, and for &lt;verb, MOD,
adv&gt;, 124 pairs (among 346 pairs) are correct.
</bodyText>
<tableCaption confidence="0.998266">
Table 3. The Test Set
</tableCaption>
<table confidence="0.953986181818182">
i
count
)
, ecol
=
(wij
Type #total #correct
verb, OBJ, noun 630 197
noun, ATTR, adj 324 163
verb, MOD, adv 346 124
1 2 1 2
</table>
<equation confidence="0.977763823529412">
ecol,ecol) = cos(Fecol , Fecol
(
sim
)
=
� ( ) � ( )
2 2
p* p
1 i 2 j
j
i
(11)
w
1i=
2
w
j
</equation>
<bodyText confidence="0.9825296">
Method 2: Instead of using contexts to calculate the
similarity of two words, this method calculates the
similarity of collocations with the similarity of their
components. The formula is described in Equation
(12).
</bodyText>
<equation confidence="0.92722425">
sim e col e col
( , )
1 2
* sim(rel1 , rel 2 )
</equation>
<bodyText confidence="0.799806">
where ( 1 , , 2 )
</bodyText>
<equation confidence="0.828412">
e col = e rel e . We assume that the rela-
i i i i
</equation>
<bodyText confidence="0.995092428571429">
tion type keeps the same, so sim(rel1 , rel2) =1.
The similarity of the words is calculated with the
same method as described in (Lin, 1998), which is
rewritten in Equation (13). The similarity of the
words is calculated through the surrounding context
words which have dependency relationships with
the investigated words.
</bodyText>
<equation confidence="0.753124">
(13)
</equation>
<bodyText confidence="0.999752">
where T(ei) denotes the set of words which have the
dependency relation rel with ei.
</bodyText>
<subsectionHeader confidence="0.946446">
3.1.2 Test Set
</subsectionHeader>
<bodyText confidence="0.999983">
With the candidate generation method as depicted
in section 2.2, we generated 1,154,311 candidates
of synonymous collocations pairs for 880,600
</bodyText>
<subsectionHeader confidence="0.951185">
3.1.3 Evaluation Results
</subsectionHeader>
<bodyText confidence="0.999401363636364">
With the test set, we evaluate the performance of
each method. The evaluation metrics are precision,
recall, and f-measure.
A development set including 500 synonymous
pairs is used to determine the thresholds of each
method. For each method, the thresholds for getting
highest f-measure scores on the development set are
selected. As the result, the thresholds for Method 1,
Method 2 and our approach are 0.02, 0.02, and 0.01
respectively. With these thresholds, the experi-
mental results on the test set in Table 3 are shown in
</bodyText>
<tableCaption confidence="0.7587655">
Table 4, Table 5 and Table 6.
Table 4. Results for &lt;verb, OBJ, noun&gt;
</tableCaption>
<table confidence="0.999947">
Method Precision Recall F-measure
Method 1 0.3148 0.8934 0.4656
Method 2 0.3886 0.7614 0.5146
Ours 0.6811 0.6396 0.6597
</table>
<tableCaption confidence="0.876336">
Table 5. Results for &lt;noun, ATTR, adj&gt;
</tableCaption>
<table confidence="0.976008">
Method Precision Recall F-measure
Method 1 0.5161 0.9816 0.6765
Method 2 0.5673 0.8282 0.6733
Ours 0.8739 0.6380 0.7376
p1i p2j
Table 6. Results for &lt;verb, MOD, adv&gt;
Method Precision Recall F-measure
Method 1 0.3662 0.9597 0.5301
Method 2 0.4163 0.7339 0.5291
Ours 0.6641 0.7016 0.6824
</table>
<equation confidence="0.935405461538462">
E ( ( , , ) (
w e rel e w e
+
1 2
) + Y_ w(e2
(rel,e)T(e2 )
T e
( 1
) (
�T e
2 )
)
=
</equation>
<figure confidence="0.96302185106383">
(
rel,e
)
E
, rel, e
w e
( 1
)
( ,
rel e
)
T e
( 1
))
, rel, e
)
, rel, e
Sim(e1, e2
,
w
)
(
ej
ei , rel
,
(
)
p
ej
ei , rel
,
=
p
 |rel
(ei , rel
) log
) p(
 |rel)p(rel)
ej
p e
( i
ej
= sim ( e1 , ) * ( e1 , e2 )
1 e sim 2 2
2
1
(12)
</figure>
<bodyText confidence="0.97294325">
It can be seen that our approach gets the highest
precision (74% on average) for all the three types of
synonymous collocations. Although the recall (64%
on average) of our approach is below other methods,
the f-measure scores, which combine both precision
and recall, are the highest. In order to compare our
methods with other methods under the same recall
value, we conduct another experiment on the type
&lt;verb, OBJ, noun&gt;4. We set the recalls of the two
methods to the same value of our method, which is
0.6396 in Table 4. The precisions are 0.3190,
0.4922, and 0.6811 for Method 1, Method 2, and
our method, respectively. Thus, the precisions of
our approach are higher than the other two methods
even when their recalls are the same. It proves that
our method of using translation information to
select the candidates is effective for synonymous
collocation extraction.
The results of Method 1 show that it is difficult
to extract synonymous collocations with monolin-
gual contexts. Although Method 1 gets higher re-
calls than the other methods, it brings a large
number of wrong candidates, which results in lower
precision. If we set higher thresholds to get com-
parable precision, the recall is much lower than that
of our approach. This indicates that the contexts of
collocations are not discriminative to extract syn-
onymous collocations.
The results also show that Model 2 is not suit-
able for the task. The main reason is that both high
scores of sim(e; , e;) and sim(e1 , eZ ) does not mean
the high similarity of the two collocations.
The reason that our method outperforms the
other two methods is that when one collocation is
translated into another language, its translations
indirectly disambiguate the words’ senses in the
collocation. For example, the probability of &lt;turn
on, OBJ, light&gt; being translated into &lt;#T3f, OBJ, �
&gt; (&lt;da3 kai1, OBJ, deng1&gt;) is much higher than
that of it being translated into &lt;IR&amp;-T, OBJ, Afft
&gt; (&lt;qu3 jue2 yu2, OBJ, guang1 zhao4 du4&gt;) while
the situation is reversed for &lt;depend on, OBJ, il-
lumination&gt;. Thus, the similarity between &lt;turn on,
OBJ, light&gt; and &lt;depend on, OBJ, illumination&gt; is
low and, therefore, this candidate is filtered out.
4 The results of the other two types of collocations are the
same as &lt;verb, OBJ, noun&gt;. We omit them because of
the space limit.
</bodyText>
<subsectionHeader confidence="0.99962">
3.2 Comparison with Methods using
Bilingual Corpora
</subsectionHeader>
<bodyText confidence="0.986745216666667">
Barzilay and Mckeown (2001), and Shimohata and
Sumita (2002) used a bilingual corpus to extract
synonymous expressions. If the same source ex-
pression has more than one different translation in
the second language, these different translations are
extracted as synonymous expressions. In order to
compare our method with these methods that only
use a bilingual corpus, we implement a method that
is similar to the above two studies. The detail proc-
ess is described in Method 3.
Method 3: The method is described as follows:
(1) All the source and target sentences (here Chi-
nese and English, respectively) are parsed; (2)
extract the Chinese and English collocations in the
bilingual corpus; (3) align Chinese collocations
ccol=&lt;c1, rc, c2&gt; and English collocations ecol=&lt;e1, re,
e2&gt; if c1 is aligned with e1 and c2 is aligned with e2;
(4) obtain two English synonymous collocations if
two different English collocations are aligned with
the same Chinese collocation and if they occur more
than once in the corpus.
The training bilingual corpus is the same one
described in Section 2. With Method 3, we get
9,368 synonymous collocation pairs in total. The
number is only 10% of that extracted by our ap-
proach, which extracts 93,523 pairs with the same
bilingual corpus. In order to evaluate Method 3 and
our approach on the same test set. We randomly
select 100 collocations which have synonymous
collocations in the bilingual corpus. For these 100
collocations, Method 3 extracts 121 synonymous
collocation pairs, where 83% (100 among 121) are
correct 5. Our method described in Section 2 gen-
erates 556 synonymous collocation pairs with a
threshold set in the above section, where 75% (417
among 556) are correct.
If we set a higher threshold (0.08) for our
method, we get 360 pairs where 295 are correct
(82%). If we use |A|, |B|, |C |to denote correct pairs
extracted by Method 3, our method, both Method 3
and our method respectively, we get |A|=100,
|B|=295, and C = |A  |n  |B |= 7 8 . Thus, the syn-
onymous collocation pairs extracted by our method
cover 78% (  |C   ||A |) of those extracted by Method
5 These synonymous collocation pairs are evaluated by
two judges and only those agreed on by both are selected
as correct pairs.
3 while those extracted by Method 3 only cover
26% (  |C   ||B |) of those extracted by our method.
It can be seen that the coverage of Method 3 is
much lower than that of our method even when their
precisions are set to the same value. This is mainly
because Method 3 can only extract synonymous
collocations which occur in the bilingual corpus. In
contrast, our method uses the bilingual corpus to
train the translation probabilities, where the trans-
lations are not necessary to occur in the bilingual
corpus. The advantage of our method is that it can
extract synonymous collocations not occurring in
the bilingual corpus.
</bodyText>
<sectionHeader confidence="0.999416" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.99998744">
This paper proposes a novel method to automati-
cally extract synonymous collocations by using
translation information. Our contribution is that,
given a large monolingual corpus and a very limited
bilingual corpus, we can make full use of these
resources to get an optimal compromise of preci-
sion and recall. Especially, with a small bilingual
corpus, a statistical translation model is trained for
the translations of synonymous collocation candi-
dates. The translation information is used to select
synonymous collocation pairs from the candidates
obtained with a monolingual corpus. Experimental
results indicate that our approach extracts syn-
onymous collocations with an average precision of
74% and recall of 64%. This result significantly
outperforms those of the methods that only use
monolingual corpora, and that only use a bilingual
corpus.
Our future work will extend synonymous ex-
pressions of the collocations to words and patterns
besides collocations. In addition, we are also inter-
ested in extending this method to the extraction of
synonymous words so that “black” and “white”,
“dog” and “cat” can be classified into different
synsets.
</bodyText>
<sectionHeader confidence="0.997595" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.963490666666667">
We thank Jianyun Nie, Dekang Lin, Jianfeng Gao,
Changning Huang, and Ashley Chang for their
valuable comments on an early draft of this paper.
</bodyText>
<sectionHeader confidence="0.995728" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999863517857143">
Barzilay R. and McKeown K. (2001). Extracting Para-
phrases from a Parallel Corpus. In Proc. of
ACL/EACL.
Brown P.F., S.A. Della Pietra, V.J. Della Pietra, and R.L.
Mercer (1993). The mathematics of statistical machine
translation: Parameter estimation. Computational
Linguistics, 19(2), pp263- 311.
Carolyn J. Crouch and Bokyung Yang (1992). Experi-
ments in automatic statistical thesaurus construction.
In Proc. of the Fifteenth Annual International ACM
SIGIR conference on Research and Development in
Information Retrieval, pp77-88.
Dragomir R. Radev, Hong Qi, Zhiping Zheng, Sasha
Blair-Goldensohn, Zhu Zhang, Waiguo Fan, and John
Prager (2001). Mining the web for answers to natural
language questions. In ACM CIKM 2001: Tenth In-
ternational Conference on Information and Knowledge
Management, Atlanta, GA.
Fung P. and Mckeown K. (1997). A Technical Word- and
Term- Translation Aid Using Noisy Parallel Corpora
across Language Groups. In: Machine Translation,
Vol.1-2 (special issue), pp53-87.
Gasperin C., Gamallo P, Agustini A., Lopes G., and Vera
de Lima (2001) Using Syntactic Contexts for Meas-
uring Word Similarity. Workshop on Knowledge
Acquisition &amp; Categorization, ESSLLI.
Grefenstette G. (1994) Explorations in Automatic The-
saurus Discovery. Kluwer Academic Press, Boston.
Kiyota Y., Kurohashi S., and Kido F. (2002) &amp;quot;Dialog
Navigator&amp;quot;: A Question Answering System based on
Large Text Knowledge Base. In Proc. of the 19th In-
ternational Conference on Computational Linguistics,
Taiwan.
Koehn. P and Knight K. (2000). Estimating Word
Translation Probabilities from Unrelated Monolin-
gual Corpora using the EM Algorithm. National
Conference on Artificial Intelligence (AAAI 2000)
Langkilde I. and Knight K. (1998). Generation that
Exploits Corpus-based Statistical Knowledge. In Proc.
of the COLING-ACL 1998.
Lin D. (1998) Automatic Retrieval and Clustering of
Similar Words. In Proc. of the 36th Annual Meeting of
the Association for Computational Linguistics.
Shimohata M. and Sumita E.(2002). Automatic Para-
phrasing Based on Parallel Corpus for Normalization.
In Proc. of the Third International Conference on
Language Resources and Evaluation.
Wang W., Huang J., Zhou M., and Huang C.N. (2001).
Finding Target Language Correspondence for Lexi-
calized EBMT System. In Proc. of the Sixth Natural
Language Processing Pacific Rim Symposium.
Zhou M., Ding Y., and Huang C.N. (2001). Improving
Translation Selection with a New Translation Model
Trained by Independent Monolingual Corpora.
Computational Linguistics &amp; Chinese Language
Processing. Vol. 6 No, 1, pp1-26.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000106">
<title confidence="0.999829">Synonymous Collocation Extraction Using Translation Information</title>
<author confidence="0.996339">Hua WU</author>
<author confidence="0.996339">Ming ZHOU</author>
<affiliation confidence="0.999789">Microsoft Research Asia</affiliation>
<address confidence="0.9792715">5F Sigma Center, No.49 Zhichun Road, Haidian District Beijing, 100080, China</address>
<email confidence="0.999794">wu_hua_@msn.com,mingzhou@microsoft.com</email>
<abstract confidence="0.995152848101267">Automatically acquiring synonymous collocation pairs such as &lt;turn on, OBJ, light&gt; and &lt;switch on, OBJ, light&gt; from corpora is a challenging task. For this task, we can, in general, have a large monolingual corpus and/or a very limited bilingual corpus. Methods that use monolingual corpora alone or use bilingual corpora alone are apparently inadequate because of low precision or low coverage. In this paper, we propose a method that uses both these resources to get an optimal compromise of precision and coverage. This method first gets candidates of synonymous collocation pairs based on a monolingual corpus and a word thesaurus, and then selects the appropriate pairs from the candidates using their translations in a second language. The translations of the candidates are obtained with a statistical translation model which is trained with a small bilingual corpus and a large monolingual corpus. The translation information is proved as effective to select synonymous collocation pairs. Experimental results indicate that the average precision and recall of our approach are 74% and 64% respectively, which outperform those methods that only use monolingual corpora and those that only use bilingual corpora. This paper addresses the problem of automatically extracting English synonymous collocation pairs using translation information. A synonymous collocation pair includes two collocations which are similar in meaning, but not identical in wording. this paper, the term to a lexically restricted word pair with a certain syntactic relation. For instance, &lt;turn on, OBJ, light&gt; is a collocation with a syntactic relation verb-object, and &lt;turn on, OBJ, light&gt; and &lt;switch on, OBJ, light&gt; are a synonymous collocation pair. In this paper, translation information means translations of collocations and their translation probabilities. Synonymous collocations can be considered as an extension of the concept of synonymous expressions which conventionally include synonymous words, phrases and sentence patterns. Synonymous expressions are very useful in a number of NLP applications. They are used in information retrieval and question answering (Kiyota et al., 2002; Dragomia et al., 2001) to bridge the expression gap between the query space and the document space. For instance, “buy book” extracted from the users’ query should also in some way match “order book” indexed in the documents. Besides, the synonymous expressions are also important in language generation (Langkilde and Knight, 1998) and computer assisted authoring to produce vivid texts. Up to now, there have been few researches which directly address the problem of extracting synonymous collocations. However, a number of studies investigate the extraction of synonymous words from monolingual corpora (Carolyn et al., 1992; Grefenstatte, 1994; Lin, 1998; Gasperin et al., 2001). The methods used the contexts around the investigated words to discover synonyms. The problem of the methods is that the precision of the extracted synonymous words is low because it extracts many word pairs such as “cat” and “dog”, which are similar but not synonymous. In addition, some studies investigate the extraction of synonymous words and/or patterns from bilingual corpora (Barzilay and Mckeown, 2001; Shimohata and Sumita, 2002). However, these methods can only extract synonymous expressions which occur in the bilingual corpus. Due to the limited size of the bilingual corpus, the coverage of the extracted expressions is very low. the fact that we usually have large monolingual corpora (unlimited in some sense) and very limited bilingual corpora, this paper proposes a method that tries to make full use of these different resources to get an optimal compromise of precision and coverage for synonymous collocation extraction. We first obtain candidates of synonymous collocation pairs based on a monolingual corpus and a word thesaurus. We then select those appropriate candidates using their translations in a second language. Each translation of the candidates is assigned a probability with a statistical translation model that is trained with a small bilingual corpus and a large monolingual corpus. The similarity of two collocations is estimated by computing the similarity of their vectors constructed with their corresponding translations. Those candidates with larger similarity scores are extracted as synonymous collocations. The basic assumption behind this method is that two collocations are synonymous if their translations are similar. For example, &lt;turn on, OBJ, light&gt; and &lt;switch on, OBJ, light&gt; are synonymous because both of them are translated OBJ, (&lt;kai1, OBJ, deng1&gt;) (&lt;da3 kai1, OBJ, deng1&gt;) Chinese. In order to evaluate the performance of our method, we conducted experiments on extracting three typical types of synonymous collocations. Experimental results indicate that our approach achieves 74% average precision and 64% recall respectively, which considerably outperform those methods that only use monolingual corpora or only use bilingual corpora. The remainder of this paper is organized as follows. Section 2 describes our synonymous collocation extraction method. Section 3 evaluates the proposed method, and the last section draws our conclusion and presents the future work. Approach Our method for synonymous collocation extraction comprises of three steps: (1) extract collocations from large monolingual corpora; (2) generate candidates of synonymous collocation pairs with a word thesaurus WordNet; (3) select synonymous collocation candidates using their translations. 2.1 Collocation Extraction This section describes how to extract English collocations. Since Chinese collocations will be used to train the language model in Section 2.3, they are also extracted in the same way. Collocations in this paper take some syntactical relations (dependency relations), such as &lt;verb, OBJ, noun&gt;, &lt;noun, ATTR, adj&gt;, and &lt;verb, MOD, adv&gt;. These dependency triples, which embody the syntactic relationship between words in a sentence, are generated with a parser—we use NLPWIN in For example, the sentence “She owned this red coat” is transformed to the following four triples after parsing: &lt;own, SUBJ, she&gt;, &lt;own, OBJ, coat&gt;, &lt;coat, DET, this&gt;, and &lt;coat, ATTR, red&gt;. These triples are generally represented in the form of &lt;Head, Relation Type, Modifier&gt;. The measure we use to extract collocations from the parsed triples is weighted mutual information (WMI) (Fung and Mckeown, 1997), as described as p Those triples whose WMI values are larger than a given threshold are taken as collocations. We do not use the point-wise mutual information because it tends to overestimate the association between two words with low frequencies. Weighted mutual information meliorates this effect by add- For expository purposes, we will only look into three kinds of collocations for synonymous collocation extraction: &lt;verb, OBJ, noun&gt;, &lt;noun, ATTR, adj&gt; and &lt;verb, MOD, adv&gt;.</abstract>
<note confidence="0.968189076923077">Table 1. English Collocations Class #Type #Token verb, OBJ, noun 506,628 7,005,455 noun, ATTR, adj 333,234 4,747,970 verb, Mod, adv 40,748 483,911 Table 2. Chinese Collocations Class #Type #Token verb, OBJ, noun 1,579,783 19,168,229 noun, ATTR, adj 311,560 5,383,200 verb, Mod, adv 546,054 9,467,103 The English collocations are extracted from Wall Street Journal (1987-1992) and Association Press (1988-1990), and the Chinese collocations are</note>
<abstract confidence="0.994854606837607">NLPWIN parser is developed at Microsoft Research, which parses several languages including Chinese and English. Its output can be a phrase structure parse tree or a logical form which is represented with dependency triples. )log ) | | extracted from People’s Daily (1980-1998). The statistics of the extracted collocations are shown in Table 1 and 2. The thresholds are set as 5 for both and Chinese. to the total of collocation occurrences and to the number of unique collocations in the corpus. 2.2 Candidate Generation Candidate generation is based on the following assumption: For a collocation &lt;Head, Relation Type, Modifier&gt;, its synonymous expressions also take the form of &lt;Head, Relation Type, Modifier&gt; although sometimes they may also be a single word or a sentence pattern. The synonymous candidates of a collocation are obtained by expanding a collocation &lt;Head, Rela- Type, Modifier&gt; using the synonyms of The synonyms of a word are obtained from WordNet 1.6. In WordNet, one synset consists of several synonyms which represent a single sense. Therefore, polysemous words occur in more than one synsets. The synonyms of a given word are obtained from all the synsets including it. For example, the word “turn on” is a polysemous word and is included in several synsets. For the sense “cause to operate by flipping a switch”, “switch on” is one of its synonyms. For the sense “be contingent on”, “depend on” is one of its synonyms. We take both of them as the synonyms of “turn on” regardless of its meanings since we do not have sense tags for words in collocations. we use to indicate the synonym set of a U to denote the English collocation set generated in Section 2.1. The detail algorithm on generating candidates of synonymous collocation pairs is described in Figure 1. For example, given a collocation &lt;turn on, OBJ, light&gt;, we expand “turn on” to “switch on”, “depend on”, and then expand “light” to “lump”, “illumination”. With these synonyms and the relation type OBJ, we generate synonymous collocation candidates of &lt;turn on, OBJ, light&gt;. The candidates are &lt;switch on, OBJ, light&gt;, &lt;turn on, OBJ, lump&gt;, &lt;depend on, OBJ, illumination&gt;, &lt;depend on, OBJ, light&gt; etc. Both these candidates and the original collocation &lt;turn on, OBJ, light&gt; are used to generate the synonymous collocation pairs. With the above method, we obtained candidates of synonymous collocation pairs. For example, &lt;switch on, OBJ, light&gt; and &lt;turn on, OBJ, light&gt; are a synonymous collocation pair. However, this method also produces wrong synonymous collocation candidates. For example, &lt;depend on, OBJ, illumination&gt; and &lt;turn on, OBJ, light&gt; is not a synonymous pair. Thus, it is important to filter out these inappropriate candidates. For each collocation R, Modithe following: a. Use the synonyms in WordNet 1.6 to expand get their synonym and b. Generate the candidate set of its synonymous | &amp; (2) Generate the candidate set of synonymous pairs SC= &amp; Figure 1. Candidate Set Generation Algorithm 2.3 Candidate Selection In synonymous word extraction, the similarity of two words can be estimated based on the similarity of their contexts. However, this method cannot be effectively extended to collocation similarity estimation. For example, in sentences “They turned on the lights” and “They depend on the illumination”, the meaning of two collocations &lt;turn on, OBJ, light&gt; and &lt;depend on, OBJ, illumination&gt; are different although their contexts are the same. Therefore, monolingual information is not enough to estimate the similarity of two collocations. However, the meanings of the above two collocations can be distinguished if they are translated into a second language (e.g., Chinese). For example, on, OBJ, light&gt; is translated into OBJ, (&lt;kai1, OBJ, deng1) and OBJ, (&lt;da3 OBJ, deng1&gt;) Chinese while &lt;depend on, illumination&gt; is translated into OBJ, (qu3 jue2 yu2, OBJ, guang1 zhao4 du4&gt;). Thus, they are not synonymous pairs because their translations are completely different. In this paper, we select the synonymous collocation pairs from the candidates in the following way. First, given a candidate of synonymous collocation pair generated in section 2.2, we translate the two collocations into Chinese with a simple statistical translation model. Second, we calculate the similarity of two collocations with the feature vectors constructed with their translations. A candidate is selected as a synonymous collocation pair if its similarity exceeds a certain threshold. Translation collocation can only be translated to the same type Chinese Thus, =1 in our case. Equation (3) is rewritten as: an English collocation we it into Chinese collocations 2using an English-Chinese dictionary. If the translation sets of represented as respectively, the Chinese translations can be represented , ER with denoting the relation set. an English collocation one of its Chinese collocation the probability that is translated into is calculated as in Equation (1). p e r e c r c p c r c ( , ,  |, , ) ( , , ) 1e 1 c2 1c c e  |) col e ( ) col According to Equation (1), we need to calculate the probability and the target probability Calculating the translation probability needs a bilingual corpus. If the above equation is used directly, we will run into the data sparseness problem. Thus, model simplification is necessary. Model Our simplification is made according to the following three assumptions. 1: a Chinese collocation and assume that conditionally independent. The translation model is rewritten as: e r e c ( , ,  |) r c  |) e col 2: a Chinese collocation we assume that the translation probability only depends on and only depends on and Equation (2) is rewritten as: pe c (  |) (  |) ( e c p e col It is equal to a word translation model if we take the relation type in the collocations as an element like a word, which is similar to Model 1 in (Brown et al., 1993). 3: assume that one type of English English collocations can be translated into Chinese words, phrases or patterns. Here we only consider the case of being translated into collocations. p e c (  |) (  |) (  |) (  |) e c p e c p r r col 1 2 2 c | Model language model is calculated with the Chinese collocation database extracted in section 2.1. In order to tackle with the data sparseness problem, we smooth the language model with an interpolation method. When the given Chinese collocation occurs in the corpus, we calculate it as in (5). count c ( ) p c ) col(5) represents the count of the Chicollocation N represents the total counts of all the Chinese collocations in the training corpus. a collocation if we assume that words conditionally independent the relation Equation (5) can be rewritten as in (6). = c r cp c r cp r ) ( ) ( ) ( ) ,*) count r c (*, , ) c2 c r  |) r ) c count r ,*) c frequency of the collocations with the head and as the relation type. , frequency of the collocations with the modifier and as the relation type ,*) frequency of the collocations with as the relation type. With Equation (5) and (6), we get the interpolated language model as shown in (7). ) c count c ) 1 - ) (  |) (  |) ( c r p c r p r (7) a constant so that the probabilities sum to 1. , c et al. (2001) found that about 70% of the Chinese translations have the same relation type as the source English collocations. ( = p ) (  |) p r r e c | p e = | ) (  |, ) ( p e r c p col r c , e col ) (2) r col col e ) (3) p = | (4) where c r c r (  |) count count ( (*, ,*) *) count , , Translation Probability Estimation Many methods are used to estimate word translation probabilities from unparallel or parallel bilingual corpora (Koehn and Knight, 2000; Brown et al., 1993). In this paper, we use a parallel bilingual corpus to train the word translation probabilities based on the result of word alignment with a bilingual Chinese-English dictionary. The alignment method is described in (Wang et al., 2001). In order to deal with the problem of data sparseness, we conduct a simple smoothing by adding 0.5 to the counts of each translation pair as in (8). ( ) 0.5* | c e _ the number of Engtranslations for a given Chinese word Similarity Calculation For each synonymous collocation pair, we get its corresponding Chinese translations and calculate the translation probabilities as in section 2.3.1. These Chinese collocations with their corresponding translation probabilities are taken as feature vectors of the English collocations, which can be represented as: i im , ( ... , ( ,) p im c p c p i col col col col col col The similarity of two collocations is defined as in (9). The candidate pairs whose similarity scores exceed a given threshold are selected. 2 1 2 (9) = For example, given a synonymous collocation pair &lt;turn on, OBJ, light&gt; and &lt;switch on, OBJ, light&gt;, we first get their corresponding feature vectors. The feature vector of &lt;turn on, OBJ, light&gt;: OBJ, 0.04692), OBJ, ... , OBJ, 0.0002710), 0.0000305) &gt; The feature vector of &lt;switch on, OBJ, light&gt;: OBJ, 0.04238), OBJ, OBJ, 0.002531), ... , 0.00003542) &gt; The values in the feature vector are translation probabilities. With these two vectors, we get the similarity of &lt;turn on, OBJ, light&gt; and &lt;switch on, OBJ, light&gt;, which is 0.2348. 2.4 Implementation of our Approach We use an English-Chinese dictionary to get the Chinese translations of collocations, which includes 219,404 English words. Each source word has 3 translation words on average. The word translation probabilities are estimated from a bilingual corpus that obtains 170,025 pairs of Chinese-English sentences, including about 2.1 million English words and about 2.5 million Chinese words. With these data and the collocations in section 2.1, we produced 93,523 synonymous collocation pairs and filtered out 1,060,788 candidate pairs with our translation method if we set the similarity threshold to 0.01. To evaluate the effectiveness of our methods, two experiments have been conducted. The first one is designed to compare our method with two methods that use monolingual corpora. The second one is designed to compare our method with a method that uses a bilingual corpus. 3.1 Comparison with Methods using Monolingual Corpora We compared our approach with two methods that use monolingual corpora. These two methods also employed the candidate generation described in section 2.2. The difference is that the two methods use different strategies to select appropriate candidates. The training corpus for these two methods is the same English one as in Section 2.1. Description 1: method uses monolingual contexts to select synonymous candidates. The purpose of this experiment is to see whether the context method for synonymous word extraction can be effectively extended to synonymous collocation extraction. The similarity of two collocations is calculated with their feature vectors. The feature vector of a collocation is constructed by all words in sentences which surround the given collocation. The context for collocation represented as in (10). ( , ) 0.5 c = count ) p count col sim ) col 1 col c i j 2 c = col j 2 i 1, , ( 2, ( , ) p w p w p col i i i i im im context word collocation probability of co-occurring with i frequency of the context word , i) with the collocation i all counts of the words in the training corpus. With the feature vectors, the similarity of two collocations is calculated as in (11). Those candidates whose similarities exceed a given threshold are selected as synonymous collocations. collocations, from which we randomly selected 1,300 pairs to construct a test set. Each pair was evaluated independently by two judges to see if it is synonymous. Only those agreed upon by two judges are considered as synonymous pairs. The statistics of the test set is shown in Table 3. We evaluated three types of synonymous collocations: &lt;verb, OBJ, noun&gt;, &lt;noun, ATTR, adj&gt;, &lt;verb, MOD, adv&gt;. For the type &lt;verb, OBJ, noun&gt;, among the 630 synonymous collocation candidate pairs, 197 pairs are correct. For &lt;noun, ATTR, adj&gt;, 163 pairs (among 324 pairs) are correct, and for &lt;verb, MOD, adv&gt;, 124 pairs (among 346 pairs) are correct. Table 3. The Test Set i count ) = Type #total #correct verb, OBJ, noun 630 197 noun, ATTR, adj 324 163 verb, MOD, adv 346 124 1 2 1 2 ( sim ) = ) ) 2 2 j i (11) 1i= 2 2: of using contexts to calculate the similarity of two words, this method calculates the similarity of collocations with the similarity of their components. The formula is described in Equation (12). e cole ( , ) 1 2 , 2) 1, , 2) rel e We assume that the relai i i i type keeps the same, so , The similarity of the words is calculated with the same method as described in (Lin, 1998), which is rewritten in Equation (13). The similarity of the words is calculated through the surrounding context words which have dependency relationships with the investigated words. (13) the set of words which have the relation Set With the candidate generation method as depicted in section 2.2, we generated 1,154,311 candidates of synonymous collocations pairs for 880,600 Results With the test set, we evaluate the performance of each method. The evaluation metrics are precision, recall, and f-measure. A development set including 500 synonymous pairs is used to determine the thresholds of each method. For each method, the thresholds for getting highest f-measure scores on the development set are selected. As the result, the thresholds for Method 1, Method 2 and our approach are 0.02, 0.02, and 0.01 respectively. With these thresholds, the experimental results on the test set in Table 3 are shown in Table 4, Table 5 and Table 6.</abstract>
<note confidence="0.7868394375">Table 4. Results for &lt;verb, OBJ, noun&gt; Method Precision Recall F-measure Method 1 0.3148 0.8934 0.4656 Method 2 0.3886 0.7614 0.5146 Ours 0.6811 0.6396 0.6597 Table 5. Results for &lt;noun, ATTR, adj&gt; Method Precision Recall F-measure Method 1 0.5161 0.9816 0.6765 Method 2 0.5673 0.8282 0.6733 Ours 0.8739 0.6380 0.7376 p1i p2j Table 6. Results for &lt;verb, MOD, adv&gt; Method Precision Recall F-measure Method 1 0.3662 0.9597 0.5301 Method 2 0.4163 0.7339 0.5291 Ours 0.6641 0.7016 0.6824</note>
<abstract confidence="0.997706830601093">( , , ) ( w e rel e w e + 1 2 ) T e ( 1 ) ( e 2 ) ) = ( E w e ) ( , rel e T e ( 1 )) ) , w ) ( , , ( ) p , , = p , ) log p e ( 1 , ) * ( 2 , 2 ) e sim 2 1 (12) It can be seen that our approach gets the highest precision (74% on average) for all the three types of synonymous collocations. Although the recall (64% on average) of our approach is below other methods, the f-measure scores, which combine both precision and recall, are the highest. In order to compare our methods with other methods under the same recall value, we conduct another experiment on the type OBJ, We set the recalls of the two methods to the same value of our method, which is 0.6396 in Table 4. The precisions are 0.3190, 0.4922, and 0.6811 for Method 1, Method 2, and our method, respectively. Thus, the precisions of our approach are higher than the other two methods even when their recalls are the same. It proves that our method of using translation information to select the candidates is effective for synonymous collocation extraction. The results of Method 1 show that it is difficult to extract synonymous collocations with monolingual contexts. Although Method 1 gets higher recalls than the other methods, it brings a large number of wrong candidates, which results in lower precision. If we set higher thresholds to get comparable precision, the recall is much lower than that of our approach. This indicates that the contexts of collocations are not discriminative to extract synonymous collocations. The results also show that Model 2 is not suitable for the task. The main reason is that both high of , not mean the high similarity of the two collocations. The reason that our method outperforms the other two methods is that when one collocation is translated into another language, its translations indirectly disambiguate the words’ senses in the collocation. For example, the probability of &lt;turn OBJ, light&gt; being translated into OBJ, kai1, OBJ, deng1&gt;) is much higher than of it being translated into OBJ, (&lt;qu3 jue2 yu2, OBJ, guang1 zhao4 du4&gt;) the situation is reversed for &lt;depend on, OBJ, illumination&gt;. Thus, the similarity between &lt;turn on, OBJ, light&gt; and &lt;depend on, OBJ, illumination&gt; is low and, therefore, this candidate is filtered out. results of the other two types of collocations are the same as &lt;verb, OBJ, noun&gt;. We omit them because of the space limit. 3.2 Comparison with Methods using Bilingual Corpora Barzilay and Mckeown (2001), and Shimohata and Sumita (2002) used a bilingual corpus to extract synonymous expressions. If the same source expression has more than one different translation in the second language, these different translations are extracted as synonymous expressions. In order to compare our method with these methods that only use a bilingual corpus, we implement a method that is similar to the above two studies. The detail process is described in Method 3. 3: method is described as follows: (1) All the source and target sentences (here Chinese and English, respectively) are parsed; (2) extract the Chinese and English collocations in the bilingual corpus; (3) align Chinese collocations and English collocations if aligned with aligned with (4) obtain two English synonymous collocations if two different English collocations are aligned with the same Chinese collocation and if they occur more than once in the corpus. The training bilingual corpus is the same one described in Section 2. With Method 3, we get 9,368 synonymous collocation pairs in total. The number is only 10% of that extracted by our approach, which extracts 93,523 pairs with the same bilingual corpus. In order to evaluate Method 3 and our approach on the same test set. We randomly select 100 collocations which have synonymous collocations in the bilingual corpus. For these 100 collocations, Method 3 extracts 121 synonymous collocation pairs, where 83% (100 among 121) are Our method described in Section 2 generates 556 synonymous collocation pairs with a threshold set in the above section, where 75% (417 among 556) are correct. If we set a higher threshold (0.08) for our method, we get 360 pairs where 295 are correct (82%). If we use |A|, |B|, |C |to denote correct pairs extracted by Method 3, our method, both Method 3 and our method respectively, we get |A|=100, and 8 Thus, the synonymous collocation pairs extracted by our method 78% (  |of those extracted by Method synonymous collocation pairs are evaluated by two judges and only those agreed on by both are selected as correct pairs. 3 while those extracted by Method 3 only cover (  |of those extracted by our method. It can be seen that the coverage of Method 3 is much lower than that of our method even when their precisions are set to the same value. This is mainly because Method 3 can only extract synonymous collocations which occur in the bilingual corpus. In contrast, our method uses the bilingual corpus to train the translation probabilities, where the translations are not necessary to occur in the bilingual corpus. The advantage of our method is that it can extract synonymous collocations not occurring in the bilingual corpus. and Future Work This paper proposes a novel method to automatically extract synonymous collocations by using translation information. Our contribution is that, given a large monolingual corpus and a very limited bilingual corpus, we can make full use of these resources to get an optimal compromise of precision and recall. Especially, with a small bilingual corpus, a statistical translation model is trained for the translations of synonymous collocation candidates. The translation information is used to select synonymous collocation pairs from the candidates obtained with a monolingual corpus. Experimental results indicate that our approach extracts synonymous collocations with an average precision of 74% and recall of 64%. This result significantly outperforms those of the methods that only use monolingual corpora, and that only use a bilingual corpus. Our future work will extend synonymous expressions of the collocations to words and patterns besides collocations. In addition, we are also interested in extending this method to the extraction of synonymous words so that “black” and “white”, “dog” and “cat” can be classified into different synsets. Acknowledgements We thank Jianyun Nie, Dekang Lin, Jianfeng Gao, Changning Huang, and Ashley Chang for their valuable comments on an early draft of this paper.</abstract>
<note confidence="0.872770947368421">References R. and McKeown K. (2001). Parafrom a Parallel Corpus. Proc. of ACL/EACL. Brown P.F., S.A. Della Pietra, V.J. Della Pietra, and R.L. (1993). mathematics of statistical machine Parameter estimation. Linguistics, 19(2), pp263- 311. J. Crouch and Bokyung Yang (1992). Experiments in automatic statistical thesaurus construction. In Proc. of the Fifteenth Annual International ACM SIGIR conference on Research and Development in Information Retrieval, pp77-88. Dragomir R. Radev, Hong Qi, Zhiping Zheng, Sasha Blair-Goldensohn, Zhu Zhang, Waiguo Fan, and John (2001). the web for answers to natural questions. In CIKM 2001: Tenth International Conference on Information and Knowledge GA. P. and Mckeown K. (1997). Technical Wordand Term- Translation Aid Using Noisy Parallel Corpora Language Groups. Machine Translation, Vol.1-2 (special issue), pp53-87. Gasperin C., Gamallo P, Agustini A., Lopes G., and Vera Lima (2001) Syntactic Contexts for Meas- Word Similarity. on Knowledge Acquisition &amp; Categorization, ESSLLI. G. (1994) in Automatic The- Discovery. Academic Press, Boston. Y., Kurohashi S., and Kido F. (2002) Navigator&amp;quot;: A Question Answering System based on Text Knowledge Base. Proc. of the 19th International Conference on Computational Linguistics, Taiwan. P and Knight K. (2000). Word Translation Probabilities from Unrelated Monolin- Corpora using the EM Algorithm. Conference on Artificial Intelligence (AAAI 2000) I. and Knight K. (1998). that Corpus-based Statistical Knowledge. Proc. of the COLING-ACL 1998. D. (1998) Retrieval and Clustering of Words. Proc. of the 36th Annual Meeting of the Association for Computational Linguistics. M. and Sumita E.(2002). Paraphrasing Based on Parallel Corpus for Normalization. In Proc. of the Third International Conference on Language Resources and Evaluation. Wang W., Huang J., Zhou M., and Huang C.N. (2001). Finding Target Language Correspondence for Lexi- EBMT System. Proc. of the Sixth Natural Language Processing Pacific Rim Symposium. M., Ding Y., and Huang C.N. (2001). Translation Selection with a New Translation Model Trained by Independent Monolingual Corpora. Computational Linguistics &amp; Chinese Language Processing. Vol. 6 No, 1, pp1-26.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>K McKeown</author>
</authors>
<title>Extracting Paraphrases from a Parallel Corpus.</title>
<date>2001</date>
<booktitle>In Proc. of ACL/EACL.</booktitle>
<marker>Barzilay, McKeown, 2001</marker>
<rawString>Barzilay R. and McKeown K. (2001). Extracting Paraphrases from a Parallel Corpus. In Proc. of ACL/EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>263--311</pages>
<contexts>
<context position="14535" citStr="Brown et al., 1993" startWordPosition="2324" endWordPosition="2327">sumption 1: For a Chinese collocation ccol and re, we assume that e1 and e2 are conditionally independent. The translation model is rewritten as: = p e r e c ( , , |) 1 e 2 col r c |) e col Assumption 2: Given a Chinese collocation &lt;c1, rc, c2&gt;, we assume that the translation probability p(ei|ccol) only depends on ei and ci (i=1,2), and p(re|ccol) only depends on re and rc. Equation (2) is rewritten as: pe c ( |) ( |) ( = p e c p e col col 1 col 2 It is equal to a word translation model if we take the relation type in the collocations as an element like a word, which is similar to Model 1 in (Brown et al., 1993). Assumption 3: We assume that one type of English 2 Some English collocations can be translated into Chinese words, phrases or patterns. Here we only consider the case of being translated into collocations. p e c ( |) ( |) ( |) ( |) = p e c p e c p r r col col 1 1 2 2 e c )p(e2 |c2) 2.3.3 Language Model The language model p(ccol) is calculated with the Chinese collocation database extracted in section 2.1. In order to tackle with the data sparseness problem, we smooth the language model with an interpolation method. When the given Chinese collocation occurs in the corpus, we calculate it as i</context>
<context position="16726" citStr="Brown et al., 1993" startWordPosition="2826" endWordPosition="2829"> is a constant so that the probabilities sum to 1. , c 3 Zhou et al. (2001) found that about 70% of the Chinese translations have the same relation type as the source English collocations. ( = p |c ( 2 ) ( |) p r r e c e1 |c1)p(e2 p e ( col = p(e1 | ) ( |, ) ( p e r c p 2 e col r c , e col ) |ccol (2) r |c |c col ) p( col e ) (3) p = (e1 |c1 (4) where c r 1p c r = ( |) 1 c count count ( (*, ,*) *) count (* r c , , 2.3.4 Word Translation Probability Estimation Many methods are used to estimate word translation probabilities from unparallel or parallel bilingual corpora (Koehn and Knight, 2000; Brown et al., 1993). In this paper, we use a parallel bilingual corpus to train the word translation probabilities based on the result of word alignment with a bilingual Chinese-English dictionary. The alignment method is described in (Wang et al., 2001). In order to deal with the problem of data sparseness, we conduct a simple smoothing by adding 0.5 to the counts of each translation pair as in (8). ( ) 0.5* | c + trans e _ where |trans _ e |represents the number of English translations for a given Chinese word c. 2.3.5 Collocation Similarity Calculation For each synonymous collocation pair, we get its correspo</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Brown P.F., S.A. Della Pietra, V.J. Della Pietra, and R.L. Mercer (1993). The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2), pp263- 311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carolyn J Crouch</author>
<author>Bokyung Yang</author>
</authors>
<title>Experiments in automatic statistical thesaurus construction.</title>
<date>1992</date>
<booktitle>In Proc. of the Fifteenth Annual International ACM SIGIR conference on Research and Development in Information Retrieval,</booktitle>
<pages>77--88</pages>
<marker>Crouch, Yang, 1992</marker>
<rawString>Carolyn J. Crouch and Bokyung Yang (1992). Experiments in automatic statistical thesaurus construction. In Proc. of the Fifteenth Annual International ACM SIGIR conference on Research and Development in Information Retrieval, pp77-88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Hong Qi</author>
<author>Zhiping Zheng</author>
<author>Sasha Blair-Goldensohn</author>
<author>Zhu Zhang</author>
<author>Waiguo Fan</author>
<author>John Prager</author>
</authors>
<title>Mining the web for answers to natural language questions.</title>
<date>2001</date>
<booktitle>In ACM CIKM 2001: Tenth International Conference on Information and Knowledge Management,</booktitle>
<location>Atlanta, GA.</location>
<marker>Radev, Qi, Zheng, Blair-Goldensohn, Zhang, Fan, Prager, 2001</marker>
<rawString>Dragomir R. Radev, Hong Qi, Zhiping Zheng, Sasha Blair-Goldensohn, Zhu Zhang, Waiguo Fan, and John Prager (2001). Mining the web for answers to natural language questions. In ACM CIKM 2001: Tenth International Conference on Information and Knowledge Management, Atlanta, GA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Fung</author>
<author>K Mckeown</author>
</authors>
<title>A Technical Word- and Term- Translation Aid Using Noisy Parallel Corpora across Language Groups. In:</title>
<date>1997</date>
<booktitle>Machine Translation, Vol.1-2 (special issue),</booktitle>
<pages>53--87</pages>
<contexts>
<context position="6843" citStr="Fung and Mckeown, 1997" startWordPosition="1032" endWordPosition="1035"> as &lt;verb, OBJ, noun&gt;, &lt;noun, ATTR, adj&gt;, and &lt;verb, MOD, adv&gt;. These dependency triples, which embody the syntactic relationship between words in a sentence, are generated with a parser—we use NLPWIN in this paper1. For example, the sentence “She owned this red coat” is transformed to the following four triples after parsing: &lt;own, SUBJ, she&gt;, &lt;own, OBJ, coat&gt;, &lt;coat, DET, this&gt;, and &lt;coat, ATTR, red&gt;. These triples are generally represented in the form of &lt;Head, Relation Type, Modifier&gt;. The measure we use to extract collocations from the parsed triples is weighted mutual information (WMI) (Fung and Mckeown, 1997), as described as WMI(w1,r,w2) = p(w1,r, p Those triples whose WMI values are larger than a given threshold are taken as collocations. We do not use the point-wise mutual information because it tends to overestimate the association between two words with low frequencies. Weighted mutual information meliorates this effect by adding p(w1, r, w2). For expository purposes, we will only look into three kinds of collocations for synonymous collocation extraction: &lt;verb, OBJ, noun&gt;, &lt;noun, ATTR, adj&gt; and &lt;verb, MOD, adv&gt;. Table 1. English Collocations Class #Type #Token verb, OBJ, noun 506,628 7,005,</context>
</contexts>
<marker>Fung, Mckeown, 1997</marker>
<rawString>Fung P. and Mckeown K. (1997). A Technical Word- and Term- Translation Aid Using Noisy Parallel Corpora across Language Groups. In: Machine Translation, Vol.1-2 (special issue), pp53-87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Gasperin</author>
<author>P Gamallo</author>
<author>A Agustini</author>
<author>G Lopes</author>
<author>Vera de Lima</author>
</authors>
<title>Using Syntactic Contexts for Measuring Word Similarity.</title>
<date>2001</date>
<booktitle>Workshop on Knowledge Acquisition &amp; Categorization, ESSLLI.</booktitle>
<marker>Gasperin, Gamallo, Agustini, Lopes, de Lima, 2001</marker>
<rawString>Gasperin C., Gamallo P, Agustini A., Lopes G., and Vera de Lima (2001) Using Syntactic Contexts for Measuring Word Similarity. Workshop on Knowledge Acquisition &amp; Categorization, ESSLLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic Press,</publisher>
<location>Boston.</location>
<marker>Grefenstette, 1994</marker>
<rawString>Grefenstette G. (1994) Explorations in Automatic Thesaurus Discovery. Kluwer Academic Press, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Kiyota</author>
<author>S Kurohashi</author>
<author>F Kido</author>
</authors>
<title>Dialog Navigator&amp;quot;: A Question Answering System based on Large Text Knowledge Base.</title>
<date>2002</date>
<booktitle>In Proc. of the 19th International Conference on Computational Linguistics,</booktitle>
<contexts>
<context position="2456" citStr="Kiyota et al., 2002" startWordPosition="364" endWordPosition="367">instance, &lt;turn on, OBJ, light&gt; is a collocation with a syntactic relation verb-object, and &lt;turn on, OBJ, light&gt; and &lt;switch on, OBJ, light&gt; are a synonymous collocation pair. In this paper, translation information means translations of collocations and their translation probabilities. Synonymous collocations can be considered as an extension of the concept of synonymous expressions which conventionally include synonymous words, phrases and sentence patterns. Synonymous expressions are very useful in a number of NLP applications. They are used in information retrieval and question answering (Kiyota et al., 2002; Dragomia et al., 2001) to bridge the expression gap between the query space and the document space. For instance, “buy book” extracted from the users’ query should also in some way match “order book” indexed in the documents. Besides, the synonymous expressions are also important in language generation (Langkilde and Knight, 1998) and computer assisted authoring to produce vivid texts. Up to now, there have been few researches which directly address the problem of extracting synonymous collocations. However, a number of studies investigate the extraction of synonymous words from monolingual </context>
</contexts>
<marker>Kiyota, Kurohashi, Kido, 2002</marker>
<rawString>Kiyota Y., Kurohashi S., and Kido F. (2002) &amp;quot;Dialog Navigator&amp;quot;: A Question Answering System based on Large Text Knowledge Base. In Proc. of the 19th International Conference on Computational Linguistics, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P</author>
<author>K Knight</author>
</authors>
<title>Estimating Word Translation Probabilities from Unrelated Monolingual Corpora using the EM Algorithm.</title>
<date>2000</date>
<booktitle>National Conference on Artificial Intelligence (AAAI</booktitle>
<marker>P, Knight, 2000</marker>
<rawString>Koehn. P and Knight K. (2000). Estimating Word Translation Probabilities from Unrelated Monolingual Corpora using the EM Algorithm. National Conference on Artificial Intelligence (AAAI 2000)</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde</author>
<author>K Knight</author>
</authors>
<title>Generation that Exploits Corpus-based Statistical Knowledge.</title>
<date>1998</date>
<booktitle>In Proc. of the COLING-ACL</booktitle>
<contexts>
<context position="2790" citStr="Langkilde and Knight, 1998" startWordPosition="417" endWordPosition="420">red as an extension of the concept of synonymous expressions which conventionally include synonymous words, phrases and sentence patterns. Synonymous expressions are very useful in a number of NLP applications. They are used in information retrieval and question answering (Kiyota et al., 2002; Dragomia et al., 2001) to bridge the expression gap between the query space and the document space. For instance, “buy book” extracted from the users’ query should also in some way match “order book” indexed in the documents. Besides, the synonymous expressions are also important in language generation (Langkilde and Knight, 1998) and computer assisted authoring to produce vivid texts. Up to now, there have been few researches which directly address the problem of extracting synonymous collocations. However, a number of studies investigate the extraction of synonymous words from monolingual corpora (Carolyn et al., 1992; Grefenstatte, 1994; Lin, 1998; Gasperin et al., 2001). The methods used the contexts around the investigated words to discover synonyms. The problem of the methods is that the precision of the extracted synonymous words is low because it extracts many word pairs such as “cat” and “dog”, which are simil</context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>Langkilde I. and Knight K. (1998). Generation that Exploits Corpus-based Statistical Knowledge. In Proc. of the COLING-ACL 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic Retrieval and Clustering of Similar Words.</title>
<date>1998</date>
<booktitle>In Proc. of the 36th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3116" citStr="Lin, 1998" startWordPosition="466" endWordPosition="467">ap between the query space and the document space. For instance, “buy book” extracted from the users’ query should also in some way match “order book” indexed in the documents. Besides, the synonymous expressions are also important in language generation (Langkilde and Knight, 1998) and computer assisted authoring to produce vivid texts. Up to now, there have been few researches which directly address the problem of extracting synonymous collocations. However, a number of studies investigate the extraction of synonymous words from monolingual corpora (Carolyn et al., 1992; Grefenstatte, 1994; Lin, 1998; Gasperin et al., 2001). The methods used the contexts around the investigated words to discover synonyms. The problem of the methods is that the precision of the extracted synonymous words is low because it extracts many word pairs such as “cat” and “dog”, which are similar but not synonymous. In addition, some studies investigate the extraction of synonymous words and/or patterns from bilingual corpora (Barzilay and Mckeown, 2001; Shimohata and Sumita, 2002). However, these methods can only extract synonymous expressions which occur in the bilingual corpus. Due to the limited size of the bi</context>
<context position="22346" citStr="Lin, 1998" startWordPosition="3853" endWordPosition="3854">ATTR, adj 324 163 verb, MOD, adv 346 124 1 2 1 2 ecol,ecol) = cos(Fecol , Fecol ( sim ) = � ( ) � ( ) 2 2 p* p 1 i 2 j j i (11) w 1i= 2 w j Method 2: Instead of using contexts to calculate the similarity of two words, this method calculates the similarity of collocations with the similarity of their components. The formula is described in Equation (12). sim e col e col ( , ) 1 2 * sim(rel1 , rel 2 ) where ( 1 , , 2 ) e col = e rel e . We assume that the relai i i i tion type keeps the same, so sim(rel1 , rel2) =1. The similarity of the words is calculated with the same method as described in (Lin, 1998), which is rewritten in Equation (13). The similarity of the words is calculated through the surrounding context words which have dependency relationships with the investigated words. (13) where T(ei) denotes the set of words which have the dependency relation rel with ei. 3.1.2 Test Set With the candidate generation method as depicted in section 2.2, we generated 1,154,311 candidates of synonymous collocations pairs for 880,600 3.1.3 Evaluation Results With the test set, we evaluate the performance of each method. The evaluation metrics are precision, recall, and f-measure. A development set </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin D. (1998) Automatic Retrieval and Clustering of Similar Words. In Proc. of the 36th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>M Shimohata</author>
</authors>
<title>and Sumita E.(2002). Automatic Paraphrasing Based on Parallel Corpus for Normalization.</title>
<booktitle>In Proc. of the Third International Conference on Language Resources and Evaluation.</booktitle>
<marker>Shimohata, </marker>
<rawString>Shimohata M. and Sumita E.(2002). Automatic Paraphrasing Based on Parallel Corpus for Normalization. In Proc. of the Third International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wang</author>
<author>J Huang</author>
<author>M Zhou</author>
<author>C N Huang</author>
</authors>
<title>Finding Target Language Correspondence for Lexicalized EBMT System.</title>
<date>2001</date>
<booktitle>In Proc. of the Sixth Natural Language Processing Pacific Rim Symposium.</booktitle>
<contexts>
<context position="16961" citStr="Wang et al., 2001" startWordPosition="2864" endWordPosition="2867"> col = p(e1 | ) ( |, ) ( p e r c p 2 e col r c , e col ) |ccol (2) r |c |c col ) p( col e ) (3) p = (e1 |c1 (4) where c r 1p c r = ( |) 1 c count count ( (*, ,*) *) count (* r c , , 2.3.4 Word Translation Probability Estimation Many methods are used to estimate word translation probabilities from unparallel or parallel bilingual corpora (Koehn and Knight, 2000; Brown et al., 1993). In this paper, we use a parallel bilingual corpus to train the word translation probabilities based on the result of word alignment with a bilingual Chinese-English dictionary. The alignment method is described in (Wang et al., 2001). In order to deal with the problem of data sparseness, we conduct a simple smoothing by adding 0.5 to the counts of each translation pair as in (8). ( ) 0.5* | c + trans e _ where |trans _ e |represents the number of English translations for a given Chinese word c. 2.3.5 Collocation Similarity Calculation For each synonymous collocation pair, we get its corresponding Chinese translations and calculate the translation probabilities as in section 2.3.1. These Chinese collocations with their corresponding translation probabilities are taken as feature vectors of the English collocations, which c</context>
</contexts>
<marker>Wang, Huang, Zhou, Huang, 2001</marker>
<rawString>Wang W., Huang J., Zhou M., and Huang C.N. (2001). Finding Target Language Correspondence for Lexicalized EBMT System. In Proc. of the Sixth Natural Language Processing Pacific Rim Symposium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Zhou</author>
<author>Y Ding</author>
<author>C N Huang</author>
</authors>
<title>Improving Translation Selection with a New Translation Model Trained by Independent Monolingual Corpora.</title>
<date>2001</date>
<journal>Computational Linguistics &amp; Chinese Language Processing.</journal>
<volume>6</volume>
<pages>1--26</pages>
<contexts>
<context position="16182" citStr="Zhou et al. (2001)" startWordPosition="2698" endWordPosition="2701"> count r c (*, , ) c 2 p c r = ( |) , p r ( ) = 2 c c count r (*, ,*) N c count(c1, rc,*) : frequency of the collocations with c1 as the head and rc as the relation type. count(*, rc , c2 ) : frequency of the collocations with c2 as the modifier and rc as the relation type count(*, rc ,*) : frequency of the collocations with rc as the relation type. With Equation (5) and (6), we get the interpolated language model as shown in (7). p ( ) c =  +  col 1 c 2 c c count c N ( ) col ( 1 - ) ( |) ( |) ( p c r p c r p r ) (7) where 0 &lt;  &lt; 1.  is a constant so that the probabilities sum to 1. , c 3 Zhou et al. (2001) found that about 70% of the Chinese translations have the same relation type as the source English collocations. ( = p |c ( 2 ) ( |) p r r e c e1 |c1)p(e2 p e ( col = p(e1 | ) ( |, ) ( p e r c p 2 e col r c , e col ) |ccol (2) r |c |c col ) p( col e ) (3) p = (e1 |c1 (4) where c r 1p c r = ( |) 1 c count count ( (*, ,*) *) count (* r c , , 2.3.4 Word Translation Probability Estimation Many methods are used to estimate word translation probabilities from unparallel or parallel bilingual corpora (Koehn and Knight, 2000; Brown et al., 1993). In this paper, we use a parallel bilingual corpus to t</context>
</contexts>
<marker>Zhou, Ding, Huang, 2001</marker>
<rawString>Zhou M., Ding Y., and Huang C.N. (2001). Improving Translation Selection with a New Translation Model Trained by Independent Monolingual Corpora. Computational Linguistics &amp; Chinese Language Processing. Vol. 6 No, 1, pp1-26.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>