<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000007">
<title confidence="0.9977295">
Clustering Technique in Multi-Document Personal Name Disambigu-
ation
</title>
<author confidence="0.976447">
Chen Chen
</author>
<affiliation confidence="0.8273765">
Key Laboratory of Computa-
tional Linguistics (Peking
University),
Ministry of Education, China
</affiliation>
<email confidence="0.981792">
chenchen@pku.edu.cn
</email>
<author confidence="0.597999">
Hu Junfeng
</author>
<affiliation confidence="0.65848925">
Key Laboratory of Computa-
tional Linguistics (Peking
University),
Ministry of Education, China
</affiliation>
<email confidence="0.994158">
hujf@pku.edu.cn
</email>
<author confidence="0.972123">
Wang Houfeng
</author>
<affiliation confidence="0.82560875">
Key Laboratory of Computa-
tional Linguistics (Peking
University),
Ministry of Education, China
</affiliation>
<email confidence="0.992033">
wanghf@pku.edu.cn
</email>
<sectionHeader confidence="0.998554" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9990604">
Focusing on multi-document personal name
disambiguation, this paper develops an agglo-
merative clustering approach to resolving this
problem. We start from an analysis of point-
wise mutual information between feature and
the ambiguous name, which brings about a
novel weight computing method for feature in
clustering. Then a trade-off measure between
within-cluster compactness and among-cluster
separation is proposed for stopping clustering.
After that, we apply a labeling method to find
representative feature for each cluster. Finally,
experiments are conducted on word-based
clustering in Chinese dataset and the result
shows a good effect.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999954405405405">
Multi-document named entity co-reference reso-
lution is the process of determining whether an
identical name occurring in different texts refers
to the same entity in the real world. With the rap-
id development of multi-document applications
like multi-document summarization and informa-
tion fusion, there is an increasing need for multi-
document named entity co-reference resolution.
This paper focuses on multi-document personal
name disambiguation, which seeks to determine
if the same name from different documents refers
to the same person.
This paper develops an agglomerative cluster-
ing approach to resolving multi-document per-
sonal name disambiguation. In order to represent
texts better, a novel weight computing method
for clustering features is presented. It is based on
the pointwise mutual information between the
ambiguous name and features. This paper also
develops a trade-off point based cluster-stopping
measure and a labeling algorithm for each clus-
ters. Finally, experiments are conducted on
word-based clustering in Chinese dataset. The
dataset contains eleven different personal names
with varying-sized datasets, and has 1669 texts in
all.
The rest of this paper is organized as follows:
in Section 2 we review the related work; Section
3 describes the framework; section 4 introduces
our methodologies including feature weight
computing with pointwise mutual information,
cluster-stopping measure based on trade-off
point, and cluster labeling algorithm. These are
the main contribution of this paper; Section 5
discusses our experimental result. Finally, the
conclusion and suggestions for further extension
of the work are given in Section 6.
</bodyText>
<sectionHeader confidence="0.999931" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999680666666666">
Due to the varying ambiguity of personal names
in a corpus, existing approaches typically cast it
as an unsupervised clustering problem based on
vector space model. The main difference among
these approaches lies in the features, which are
used to create a similarity space. Bagga &amp; Bald-
win (1998) first performed within-document co-
reference resolution, and then explored features
in local context. Mann &amp; Yarowsky (2003) ex-
tracted local biographical information as features.
Al-Kamha and Embley (2004) clustered search
results with feature set including attributes, links
and page similarities. Chen and Martin (2007)
explored the use of a range of syntactic and se-
mantic features in unsupervised clustering of
documents. Song (2007) learned the PLSA and
LDA model as feature sets. Ono et al. (2008)
used mixture features including co-occurrences
</bodyText>
<page confidence="0.989026">
88
</page>
<note confidence="0.9969995">
Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, pages 88–95,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.99997416">
of named entities, key compound words, and top-
ic information. Previous works usually focus on
feature identification and feature selection. The
method to assign appropriate weight to each fea-
ture has not been discussed widely.
A major challenge in clustering analysis is de-
termining the number of ‘clusters’. Therefore,
clustering based approaches to this problem still
require estimating the number of clusters. In Hie-
rarchy clustering, it equates to determine the
stopping step of clustering. The measure to find
the “knee” in the criterion function curve is a
well known cluster-stopping measure. Pedersen
and Kulkarni had studied this problem (Pedersen
and Kulkarni, 2006). They developed cluster-
stopping measures named PK1, PK2, PK3, and
presented the Adapted Gap Statistics.
After estimating the number of ‘clusters’, we
obtain the clustering result. In order to label the
‘clusters’, the method that finding representative
features for each ‘cluster’ is needed. For example,
the captain John Smith can be labeled as captain.
Pedersen and Kulkarni (2006) selected the top N
non-stopping word features from texts grouped
in a cluster as label.
</bodyText>
<sectionHeader confidence="0.997992" genericHeader="method">
3 Framework
</sectionHeader>
<bodyText confidence="0.909552796296296">
On the assumption of “one person per document”
(i.e. all mentions of an ambiguous personal name
in one document refer to the same personal enti-
ty), the task of disambiguating personal name in
text set intends to partition the set into subsets,
where each subset refer to one particular entity.
Suppose the set of texts containing the ambi-
guous name is denoted by D= {d1,d2,...,dn}, and
di (0&lt;i&lt;n+1) stands for one text. The entities
with the ambiguous name are denoted by a set
E= {e1,e2,...,em}, where the number of entities ‘m’
is unknown. The ambiguous name in each text di
indicates only one entity ek. The aim of the work
is to map an ambiguous name appearing in each
text to an entity. Therefore, those texts indicating
the same entity need to be clustered together.
In determining whether a personal name refers
to a specific entity, the personal information, so-
cial network information and related topics play
important roles, all of which are expressed by
words in texts,. Extracting words as features, this
paper applies an agglomerative clustering ap-
proach to resolving name co-reference. The
framework of our approach consists of the fol-
lowing seven main steps:
Step 1: Pre-process each text with Chinese
word segmentation tool;
Step 2: Extract words as features from the
set of texts D;.
Step 3: Represent texts d1,...,dn by features
vectors;
Step 4: Calculate similarity between texts;
Step 5: Cluster the set D step by step until
only one cluster exists;
Step 6: Estimate the number of entities in
accordance with cluster-stopping
measure;
Step 7: Assign each cluster a discriminating
label.
This paper focuses on the Step 4, Step 6 and
Step 7, i.e., feature weight computing method,
clustering stopping measure and cluster labeling
method. They will be described in the next sec-
tion in detail.
Step1 and Step3 are simple, and there is no
further description here. In Step 2, we use co-
occurrence words of the ambiguous name in
texts as features. In the process of agglomerative
clustering (see Step 5), each text is viewed as one
cluster at first, and the most similar two clusters
are merged together as a new cluster at each
round. After replacing the former two clusters
with the new one, we use average linked method
to update similarity between clusters.
</bodyText>
<sectionHeader confidence="0.998052" genericHeader="method">
4 Methodology
</sectionHeader>
<subsectionHeader confidence="0.979867">
4.1 Feature weight
</subsectionHeader>
<bodyText confidence="0.999986045454545">
Each text is represented as a feature vector, and
each item of the vector represents the weight
value for corresponding feature in the text. Since
our approach is completely unsupervised we
cannot use supervised methods to select
significant features. Since the weight of feature
will be adjusted well instead of feature selection,
all words in set D are used as feature in our
approach.
The problem of computing feature weight is
involved in both text clustering and text classifi-
cation. By comparing the supervised text classi-
fication and unsupervised text clustering, we find
that the former one has a better performance ow-
ing to the selection of features and the computing
method of feature weight. Firstly, in the applica-
tion of supervised text classification, features can
be selected by many methods, such as, Mutual
Information (MI) and Expected Cross Entropy
(ECE) feature selection methods. Secondly,
model training methods, such as SVM model, are
generally adopted by programs when to find the
</bodyText>
<page confidence="0.999587">
89
</page>
<bodyText confidence="0.999966034482759">
optimal feature weight. There is no training data
for unsupervised tasks, so above-mentioned me-
thods are unsuitable for text clustering.
In addition, we find that the text clustering for
personal name disambiguation is different from
common text clustering. System can easily judge
whether a text contains the ambiguous personal
name or not. Thus the whole collection of texts
can be easily divided into two classes: texts with
or without the name. As a result, we can easily
calculate the pointwise mutual information
between feature words and the personal name.
To a certain extent, it represents the correlative
degree between feature words and the underlying
entity corresponding to the personal name.
For these reasons, our feature weight
computing method calculates the pointwise
mutual information between personal name and
feature word. And the value of pointwise mutual
information will be used to expresse feature
word’s weight by combining the feature‘s tf (the
abbreviation for term-frequency) in text and idf
(the abbreviation for inverse document frequency)
in dataset. The formula of feature weight compu-
ting proposed in this paper is as below, and it is
need both texts containing and not containing the
ambiguous personal name to form dataset D. For
each tk in di that contains name, its mi_weight is
computed as follow:
</bodyText>
<equation confidence="0.906692941176471">
× log( 1 MI( , )) log(|
+ t name × D
k
(1)
And
p name , t k )
(
p name p t
( ) (
× k )
dfname
( ,tk) /  |D
(name) × df (tk) /  |D |
2
name, tk )×  |D |
df
(name) × df (tk)
</equation>
<bodyText confidence="0.997668833333334">
Where tk is a feature; name is the ambiguous
name; di is the ith text in dataset; tf(tk,di)
represents term frequency of feature tk in text di;
df(tk), df(name) is the number of the texts con-
taining tk or name in dataset D respectively;
df(tk,name) is the number of texts containing both
tk and name; |D |is the number of all the texts.
Formula (2) can be comprehended as: if word
tk occurs much more times in texts containing the
ambiguous name than in texts not containing the
name, it must have some information about the
name.
A widely used approach for computing feature
weight is tf*idf scheme as formula (3) (Salton
and Buckley. 1998), which only uses the texts
containing the ambiguous name. We denote it by
old_weight . For each tk in di containing name,
the old_weight is computed as follow:
</bodyText>
<equation confidence="0.9885351">
old_weight(
=+
(1 log(tf (tk, di))) (3)
× log( (
df name
d
)))
,
i
× log( |D  |df (tk ))
</equation>
<bodyText confidence="0.970261">
Before clustering, the similarity between texts
is computed by cosine value of the angle
between vectors (such as dx, dy in formula (5)):
d d
</bodyText>
<equation confidence="0.991381333333333">
⋅
cos( ) = x y
d , d (5)
x y d d
⋅
x
</equation>
<bodyText confidence="0.99975725">
Each item of the vector (i.e. dx, dy) represents
the weight value for corresponding feature in the
text.
The first term on the right side is tf, and the
second term is idf. If the idf scheme is computed
in the whole dataset D for reducing noise, the
weight computing formula can be expressed as
follow, and is denoted by imp_weight:
</bodyText>
<subsectionHeader confidence="0.995829">
4.2 Cluster-stopping measure
</subsectionHeader>
<bodyText confidence="0.999958923076923">
The process of clustering will produce n cluster
results, one for each step. Independent of
clustering algorithm, the cluster stopping meas-
ure should choose the cluster results which can
represent the structure of data.
A fundamental and difficult problem in cluster
analysis is to measure the structure of clustering
result. The geometric structure is a representative
method. It defines that a “good” clustering re-
sults should make data points from one cluster
“compact”, while data points from different clus-
ter are “separate” as far as possible. The indica-
tors should quantify the “compactness” and “se-
paration” for clusters, and combine both. In the
study of cluster stopping measures by Pedersen
and Kulkarni (2006), the criterion functions de-
fines text similarity based on cosine value of the
angle between vectors. Their cluster-stopping
measures focused on finding the ‘knee’ of crite-
rion function.
Our cluster-stopping measure is also based on
the geometric structure of dataset. The measure
aims to find the trade-off point between within-
cluster compactness and among-cluster
separation. Both the within-cluster compactness
(Internal critical function) and among-cluster
</bodyText>
<equation confidence="0.892311666666667">
mi _weight(
, name d
, i
) (1 log( ( , )))
= + tf t d
k i
tk
imp _weight(tk,
i
</equation>
<figure confidence="0.929858684210526">
)
|( ))
df tk
MI( t name
k , )
(2)
df
df
(
tk , name d )
, i
) df
t name
k, ))
(
(1 log( (
+ tf t k
(4)
y
</figure>
<page confidence="0.981405">
90
</page>
<bodyText confidence="0.999637769230769">
separation (External critical function) are defined
by Euclidean distance. The hybrid critical
function (Hybrid critical function) combines
internal and external criterion functions.
Suppose that the given dataset contains N ref-
erences, which are denoted as: d1,d2,...,dN; the
data have been repeatedly clustered into k clus-
ters, where k=N,...,1; and clusters are denoted as
Cr, r=1,...k; and the number of references in
each cluster is nr, so nr=|Cr|. We introduce Incrf
(Internal critical function), Excrf (External
critical function) and Hycrf (Hybrid critical
function) to measure it as follows.
</bodyText>
<equation confidence="0.935700571428571">
(7)
1
Hycrf( )
k = × (Incrf( ) Excrf( ))
k + k (8)
M
Where M=Incrf(1)=Excrf(N)
</equation>
<table confidence="0.903220909090909">
1.5 Hycrf(t)
1
0.5
0
1
8
15
22
29
36
43
50
57
64
71
78
85
92
99
106
113
120
</table>
<figureCaption confidence="0.998059">
Figure 1 Hycrf vs. t (N-k)
</figureCaption>
<bodyText confidence="0.999918190476191">
Chen proved the existence of the minimum
value between (0,1) in Hycrf(k) (see Chen et al.
2008). The Hycrf value in a typical Hycrf(t)
curve is shown as Figure 1, where t=N-k.
Function Hycrf based on Incrf and Excrf is
used as the Hybrid criterion function. The Hycrf
curve will rise sharply after the minimum, indi-
cating that the cluster of several optimal parti-
tions’ subsets will lead to drastic drop in cluster
quality. Thus cluster partition can be determined.
Using the attributes of the Hycrf(k) curve, we put
forward a new cluster-stopping measure named
trade-off point based cluster-stopping measure
(TO_CSM).
Trade-off point based cluster-stopping meas-
ure (TO_CSM) selects the k value which max-
imizes TO_CSM(k), and indicates the number of
cluster. The first term on the right side of formu-
la (9) is used to minimize the value of Hycrf(k),
and the second one is used to find the ‘knee’ ris-
ing sharply.
</bodyText>
<subsectionHeader confidence="0.998701">
4.3 Labeling
</subsectionHeader>
<bodyText confidence="0.999983055555556">
Once the clusters are created, we label each
entity to represent the underlying entity with
some important information. A label is
represented as a list of feature words, which
summarize the information about cluster’s
underlying entity.
The algorithm is outlined as follows: after
clustering N references into m clusters, for each
cluster Ck in {C1, C2, ..., Cm}, we calculate the
score of each feature for Ck and choose features
as the label of Ck whose scores rank top N. In
particular, the score caculated in this paper is
different from Pedersen and Kulkarni’s (2006).
We combine pointwise mutual information
computing method with term frequency in cluster
to compute the score.
The formula of feature scoring for labeling is
shown as follows:
</bodyText>
<equation confidence="0.9735165">
Score(tk, Ci) = MI(tk, name) × MIname
× +
(1 log(
(10)
</equation>
<bodyText confidence="0.9283986">
The calculation of MI(tk,name) is shown as
formula (2) in subsection 4.1. tf(tk,Ci) represents
the total occurrence frequency of feature tk in
cluster Ci . The MIname(tk,Ci) is computed as for-
mula (11):
</bodyText>
<equation confidence="0.99510075">
tk,Ci) /  |D |
df k
(t
(11)
</equation>
<bodyText confidence="0.999843666666667">
In formula (10), the weight of stopping words
can be reduced by the first item. The second item
can increase the weight of words with high dis-
tinguishing ability for a certain ambiguous name.
The third item of formula (10) gives higher
scores to features whose frequency are higher.
</bodyText>
<equation confidence="0.963260739130435">
j
C
2
y
Incrf(k
k
∑∑dx, dy∈Ci dx −
i=1
)
d
y
Excrf(k
1
∑ ∑ ∑dx∈Ci,dy∈
i=1 j=1,j≠i ninj
dx d
−
)
k k
2
(6)
,
)
C i
p t
( k
)
( )
k
p t p
(Ci
×
(
df
C;
tk, )×  |D
)
df k × df
(t )
(Ci
1 Hycrf(k)
×
Hycrf( 1)
k + Hycrf( 1
k +
)
</equation>
<page confidence="0.42965">
(9)
</page>
<figure confidence="0.946648032258064">
TO_CSM(k
)
,
)
(
tk
Ci
,
)))
tf
(
tk
Ci
,
MI
)
(
tk
name
C i
(
df
)
×df /ID|
(C;
2
91
5 Experiment precision 1 (12)
5.1 Data =∑ ∈
d Dprecisiond
N
</figure>
<bodyText confidence="0.9998835">
The dataset is from WWW, and contains 1,669
texts with eleven real ambiguous personal names.
Such raw texts containing ambiguous names are
collected via search engine1, and most of them
are news. The eleven person-names are, &amp;quot;刘易斯
Liu-Yi-si ‘Lewis’&amp;quot;, &amp;quot;刘淑珍 Liu-Shu-zhen &amp;quot;, &amp;quot;李
强 Li-Qiang&amp;quot;, &amp;quot;李娜 Li-Na&amp;quot;, &amp;quot;李桂英 Li-Gui-
ying&amp;quot;, &amp;quot;米歇尔 Mi-xie-er ‘Michelle’&amp;quot;, &amp;quot;玛丽
Ma-Li ‘Mary’&amp;quot;, &amp;quot;约翰逊 Yue-han-xun ‘John-
son’&amp;quot;, &amp;quot;王涛 Wang-Tao&amp;quot;, &amp;quot;王刚 Wang-Gang&amp;quot;, &amp;quot;
陈志强 Chen-Zhi-qiang&amp;quot;. Names like “Michelle”,
“Johnson” are transliterated from English to Chi-
nese, while names like “Liu –Shu-zhen”, “Chen-
Zhi-qiang” are original Chinese personal names.
Some of these names only have a few persons,
while others have more persons.
Table 1 shows our data set. “#text” presents
the number of texts with the personal name.
“#per” presents the number of entities with the
personal name in text dataset. “#max” presents
the maximum of texts for an entity with the per-
sonal name, and “#min” presents the minimum.
</bodyText>
<table confidence="0.999448666666667">
#text #per #max #min
Lewis 120 6 25 10
Liu-Shu-zhen 149 15 28 3
Li-Qiang 122 7 25 9
Li-Na 149 5 39 21
Li-Gui-ying 150 7 30 10
Michelle 144 7 25 12
Mary 127 7 35 10
Johnson 279 19 26 1
Wang-Gang 125 18 26 1
Wang-Tao 182 10 38 5
Chen-Zhi-qiang 122 4 52 13
</table>
<tableCaption confidence="0.998817">
Table 1 Statistics of the test dataset
</tableCaption>
<bodyText confidence="0.9997962">
We first convert all the downloaded docu-
ments into plain text format to facilitate the test
process, and pre-process them by using the seg-
mentation toolkit ICTCLAS2.
In testing and evaluating, we adopt B-Cubed
definition for Precision, Recall and F-Measure
as indicators (Bagga, Amit and Baldwin. 1998).
F-Measure is the harmonic mean of Precision
and Recall.
The definitions are presented as below:
</bodyText>
<footnote confidence="0.88975">
1 April.2008
2 http://ictclas.org/
</footnote>
<equation confidence="0.981932">
recall 1
= (13)
∑ ∈ recall d
d D
N
2 × precision
precision + recall
</equation>
<bodyText confidence="0.999697166666667">
where precisiond is the precision for a text d.
Suppose the text d is in subset A, precisiond is
the percentage of texts in A which indicates the
same entity as d. Recalld is the recall ratio for a
text d. Recalld is the ratio of number of texts
which indicates the same entity as d in A to that
in corpus D. n =  |D |, D refers to a collection of
texts containing a particular name (such as Wang
Tao, e.g. a set of 200 texts, n = 200). Subset A is
a set formed after clustering (text included in
class), and d refers to a certain text that contain-
ing &amp;quot;Wang Tao&amp;quot;.
</bodyText>
<subsectionHeader confidence="0.982554">
5.2 Result
</subsectionHeader>
<bodyText confidence="0.999989857142857">
All the 1669 texts in the dataset are employed
during experiment. Each personal name disam-
biguation process only clusters the texts contain-
ing the ambiguous name. After pre-processing, in
order to verify the mi_weight method for feature
weight computing, all the words in texts are used
as features.
Using formula (1), (3) and (4) as feature
weight computing formula, we can get the evalu-
ation of cluster result shown as table 2. In this
step, cluster-stopping measure is not used. In-
stead, the highest F-measure during clustering is
highlighted to represent the efficiency of the fea-
ture weight computing method.
Further more, we carry out the experiment on
the trade-off point based cluster-stopping
measure, and compare its cluster result with
highest F-measure and cluster result determined
by cluster-stopping measure PK3 proposed by
Pedersen and Kulkarni’s. Based on the
experiment in Table 2, a structure tree is
constructed in the clustering process. Cluster-
stopping measures are used to determine where
to stop cutting the dendrogram. As shown in
Table 3, the TO-CMS method predicts the
optimal results of four names in eleven, while
PK3 method predicts the optimal result of one
name, which are marked in a bold type.
</bodyText>
<figure confidence="0.94386525">
F − measure
×
recall
(14)
</figure>
<page confidence="0.855535">
92
</page>
<table confidence="0.997014357142857">
old weight imp weight mi weight
#pre #rec #F #pre #rec #F #pre #rec #F
Lewis 0.9488 0.8668. 0.9059 1 1 1 1 1 1
Liu-Shu-zhen 0.8004 0.7381 0.7680 0.8409 0.8004 0.8201 0.9217 0.7940 0.8531
Li-Qiang 0.8057 0.6886 0.7426 0.9412 0.7968 0.8630 0.8962 0.8208 0.8569
Li-Na 0.9487 0.7719 0.8512 0.9870 0.8865 0.9340 0.9870 0.9870 0.9870
Li-Gui-ying 0.8871 0.9124 0.8996 0.9879 0.8938 0.9385 0.9778 0.8813 0.9271
Michelle 0.9769 0.7205 0.8293 0.9549 0.8146 0.8792 0.9672 0.9498 0.9584
Mary 0.9520 0.6828 0.7953 1 0.9290 0.9632 1 0.9001 0.9474
Johnson 0.9620 0.8120 0.8807 0.9573 0.8083 0.8765 0.9593 0.8595 0.9067
Wang-Gang 0.8130 0.8171 0.8150 0.7804 0.9326 0.8498 0.8143 0.9185 0.8633
Wang-Tao 1 0.9323 0.9650 0.9573 0.9485 0.9529 0.9897 0.9768 0.9832
Chen-Zhi-qiang 0.9732 0.8401 0.9017 0.9891 0.9403 0.9641 0.9891 0.9564 0.9725
Average 0.9153 0.7916 0.8504 0.9451 0.8864 0.9128 0.9548 0.9131 0.9323
</table>
<tableCaption confidence="0.996806">
Table 2 comparison of feature weight computing method (highest F-measure)
</tableCaption>
<table confidence="0.999925714285714">
Optimal TO-CMS PK3
#pre #rec #F #pre #rec #F #pre #rec #F
Lewis 1 1 1 1 1 1 0.8575 1 0.9233
Liu-Shuzhen 0.9217 0.7940 0.8531 0.8466 0.8433 0.8450 0.5451 0.9503 0.6928
Li-Qiang 0.8962 0.8208 0.8569 0.8962 0.8208 0.8569 0.7897 0.9335 0.8556
Li-Na 0.9870 0.9870 0.9870 0.9870 0.9870 0.9870 0.9870 0.9016 0.9424
Li-Gui-ying 0.9778 0.8813 0.9271 0.9778 0.8813 0.9271 0.8750 0.9427 0.9076
Michelle 0.9672 0.9498 0.9584 0.9482 0.9498 0.9490 0.9672 0.9498 0.9584
Mary 1 0.9001 0.9474 0.8545 0.9410 0.8957 0.8698 0.9410 0.9040
Johnson 0.9593 0.8595 0.9067 0.9524 0.8648 0.9066 0.2423 0.9802 0.3885
Wang-Gang 0.8143 0.9185 0.8633 0.9255 0.7102 0.8036 0.5198 0.9550 0.6732
Wang-Tao 0.9897 0.9768 0.9832 0.8594 0.9767 0.9144 0.9700 0.9768 0.9734
Chen-Zhi-qiang 0.9891 0.9564 0.9725 0.8498 1 0.9188 0.8499 1 0.9188
Average 0.9548 0.9131 0.9323 0.9179 0.9068 0.9095 0.7703 0.9574 0.8307
</table>
<tableCaption confidence="0.99807">
Table 3 comparison of cluster-stopping measures’ performance
</tableCaption>
<table confidence="0.987404153846154">
name Entity Created Labels
Lewis Person-1 E, L�(Babbitt),-:�-:A ·SIJ-iWf(Sinclair Lewis),R7�&apos;��Wf(Arrow smith),3C
�?_�4(Literature Prize),f&amp; (Dresser), IA�,j,, (Howells),AA3C�? c
(Swedish Academy),*ff&amp;·#cf&amp;�(Sherwood Anderson), i V·-R f J
(Elmer Gan Hartley),)�,-47(street),,-&apos;v&apos;---�P,(award),Xg3C�?-E*-P (American
Literature and Arts Association)
Person-2 XMTT(Bank of America),XVfjVTT(Bank of America),VTT(bank),&amp; j-
(investors),f_jn-A(credit card),rPTT(Bank of China),rL*(Citibank),#-)-
(mergers and acquisitions), TT(Construction Bank),#,TTV(executive officer),
VTT1L(banking),jj*(stock),F4·�kIJ-i�(Ken Lewis)
Person-3 �FM(Single),pffPp (Liana), (album),pff#c (Liana),pff#cg·�kIJ-iWT(Liana
Lewis),f JPp (Liana), (airborne),M:K(sales),�.,F (Music Awards), pff
R·M (Maria Kelly), (List),t_f(debut),
Person-4 ��·�-i�(Carl Lewis),P)LU-(long jump),-�/T,%(Carl),V&amp;3C (Owens),[B
(track and field),TN ji1:(Burrell),XgA (the U.S. Olympic Committee),w
,&apos;(sprint),**A(Taylors),Jn�,%4 f&amp;(Belgrade),�fff&amp;· A (Verde Exxon),
AA (Exxon)
93
Person-5 泰森(Tyson),拳王(King of Boxer),击倒(knock down),重量级(heavyweight),唐
金(Don King),拳击(boxing),腰带(belt),拳手(Boxing),拳(fist),回合(bout),拳台
(Ring),WBC
Person-6 丹尼尔(Daniel),戴·刘易斯(Day Lewis),血色(Blood),丹尼尔·戴·刘易斯(Daniel
Day Lewis),黑金(There Will Be Blood),左脚(left crus),影帝(movie king),纽约
影评人协会(New York Film Critics Circles),小金人(the Gold Oscar statues),主
角奖(Best Actor in a Leading Role),奥斯卡(Oscar),未血绸缪(There Will Be
Blood)
</table>
<tableCaption confidence="0.986148">
Table 4 Labels for “Lewis” clusters
</tableCaption>
<bodyText confidence="0.9999422">
On the basis of text clustering result that
obtained from the Trade-off based cluster-
stopping measure experiment in Table 3, we try
our labelling method mentioned in subsection 4.3.
For each cluster, we choose 12 words with
highest score as its label. The experiment result
demonstrates that the created label is able to
represent the category. Take name “刘易斯 Liu-
Yi-si ‘Lewis’” for example, the labeling result
shown as Table 4.
</bodyText>
<subsectionHeader confidence="0.886868">
5.3 Discussion
</subsectionHeader>
<bodyText confidence="0.99999397368421">
From the test result in table 2, we find that our
feature weight computing method can improve
the Chinese personal name clustering disambigu-
ation performance effectively. For each personal
name in test dataset, the performance is im-
proved obviously. The average value of optimal
F-measures for eleven names rises from 85.04%
to 91.28% by using the whole dataset D for cal-
culated idf, and rises from 91.28% to 93.23% by
using mi_weight. Therefore, in the application of
Chinese text clustering with constraints, we can
compute pointwise mutual information between
constraints and feature, and it can be merged
with feature weight value to improve the cluster-
ing performance.
We can see from table 3 that trade-off point
based cluster-stopping measure (TO_CSM) per-
forms much better than PK3. According to the
experimental results, PK3 measure is not that
robust. The optimal number of clusters can be
determined for certain data. However, we found
that it did not apply to all cases. For example, it
obtains the optimal estimation result for data
“Michelle”, as for “Liu Shuzhen”, “Wang Gang”
and “Johnson”, the results are extremely bad.
The better result is achieved by using TO_CSM
measure, and the selected results are closer to the
optimal value. The PK3 measure uses the mean
and the standard deviation to deduce, and its
processes are more complicated than TO_CSM’s.
Our cluster labeling method computes the fea-
tures’ score with formula (10). From the labeling
results sample shown in Table 4, we can see that
all of the labels are representative. Most of them
are person and organizations’ name, and the rest
are key compound words. Therefore, when the
clustering performance is good, the quality of
cluster labels created by our method is also good.
</bodyText>
<sectionHeader confidence="0.999941" genericHeader="discussions">
6 Future Work
</sectionHeader>
<bodyText confidence="0.999990777777778">
This paper developed a clustering algorithm of
multi-document personal name disambiguation,
and put forward a novel feature weight compu-
ting method for vector space model. This method
computes weight with the pointwise mutual in-
formation between the personal name and feature.
We also study a hybrid criterion function based
on trade-off point and put forward the trade-off
point cluster-stopping measure. At last, we expe-
riment on our score computing method for clus-
ter labeling.
Unsupervised personal name disambiguation
techniques can be extended to address the prob-
lem of unsupervised Entity Resolution and unsu-
pervised word sense discrimination. We will at-
tempt to apply the feature weight computing me-
thod to these fields.
One of the main directions of our future work
will be how to improve the performance of per-
sonal name disambiguation. Computing weight
based on a window around names may be helpful.
Moreover, word-based text features haven’t
solved two difficult problems of natural language
problems: Synonym and Polysemy, which se-
riously affect the precision and efficiency of
clustering algorithms. Text representation based
on concept and topic may solve the problem.
</bodyText>
<sectionHeader confidence="0.998892" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.63556425">
This research is supported by National Natural
Science Foundation of Chinese (No.60675035)
and Beijing Natural Science Foundation
(No.4072012)
</bodyText>
<page confidence="0.997909">
94
</page>
<note confidence="0.7213725">
References 18–23, 2007, Vancouver, British Columbia, Cana-
da.
</note>
<reference confidence="0.999693779661017">
Al-Kamha. R. and D. W. Embley. 2004. Grouping
search-engine returned citations for person-name
queries. In Proceedings of WIDM’04, 96-103,
Washington, DC, USA.
Bagga and B. Baldwin. 1998. Entity-based cross-
document coreferencing using the vector space
model. In Proceedings of 17th International Con-
ference on Computational Linguistics, 79–85.
Bagga, Amit and B. Baldwin. 1998. Algorithms for
scoring co-reference chains. In Proceedings of the
First International Conference on Language Re-
sources and Evaluation Workshop on Linguistic
co-reference.
Chen Ying and James Martin. 2007. Towards Robust
Unsupervised Personal Name Disambiguation,
EMNLP 2007.
Chen Lifei, Jiang Qingshan, and Wang Shengrui.
2008. A Hierarchical Method for Determining the
Number of Clusters. Journal of Software, 19(1). [in
Chinese]
Chung Heong Gooi and James Allan. 2004. Cross-
document co-reference on a large scale corpus. In S.
Dumais, D. Marcu, and S. Roukos, editors, HLT-
NAACL 2004: Main Proceedings, 9–16, Boston,
Massachusetts, USA, May 2 - May 7 2004. Asso-
ciation for Computational Linguistics.
Gao Huixian. Applied Multivariate Statistical Analy-
sis. Peking Univ. Press. 2004.
G. Salton and C. Buckley. 1988. Term-weighting ap-
proaches in automatic text retrieval. Information
Processing and Management,
Kulkarni Anagha and Ted Pedersen. 2006. How Many
Different “John Smiths”, and Who are They? In
Proceedings of the Student Abstract and Poster
Session of the 21st National Conference on Artifi-
cial Intelligence, Boston, Massachusetts.
Mann G. and D. Yarowsky. 2003. Unsupervised per-
sonal name disambiguation. In W. Daelemans and
M. Osborne, editors, Proceedings of CoNLL-2003,
33–40, Edmonton, Canada.
Niu Cheng, Wei Li, and Rohini K. Srihari. 2004.
Weakly Supervised Learning for Cross-document
Person Name Disambiguation Supported by Infor-
mation Extraction. In Proceedings of ACL 2004.
Ono. Shingo, Issei Sato, Minoru Yoshida, and Hiroshi
Nakagawa2. 2008. Person Name Disambiguation
in Web Pages Using Social Network, Compound
Words and Latent Topics. T. Washio et al. (Eds.):
PAKDD 2008, LNAI 5012, 260–271.
Song Yang, Jian Huang, Isaac G. Councill, Jia Li, and
C. Lee Giles. 2007. Efficient Topic-based Unsu-
pervised Name Disambiguation. JCDL’07, June
Ted Pedersen and Kulkarni Anagha. 2006. Automatic
Cluster Stopping with Criterion Functions and the
Gap Statistic. In Proceedings of the Demonstration
Session of the Human Language Technology Con-
ference and the Sixth Annual Meeting of the North
American Chapter of the Association for Computa-
tional Linguistic, New York City, NY.
</reference>
<page confidence="0.999071">
95
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.161757">
<title confidence="0.9959585">Technique in Multi-Document Personal Name Disambiguation</title>
<author confidence="0.999979">Chen Chen</author>
<affiliation confidence="0.982109666666667">Laboratory of tional Linguistics (Peking University),</affiliation>
<address confidence="0.608024">Ministry of Education, China</address>
<email confidence="0.910038">chenchen@pku.edu.cn</email>
<author confidence="0.973794">Hu Junfeng</author>
<affiliation confidence="0.979376666666667">Laboratory of tional Linguistics (Peking University),</affiliation>
<address confidence="0.601662">Ministry of Education, China</address>
<email confidence="0.936973">hujf@pku.edu.cn</email>
<author confidence="0.998936">Wang Houfeng</author>
<affiliation confidence="0.979865666666667">Laboratory of tional Linguistics (Peking University),</affiliation>
<address confidence="0.62408">Ministry of Education, China</address>
<email confidence="0.987386">wanghf@pku.edu.cn</email>
<abstract confidence="0.999103">Focusing on multi-document personal name disambiguation, this paper develops an agglomerative clustering approach to resolving this problem. We start from an analysis of pointwise mutual information between feature and the ambiguous name, which brings about a novel weight computing method for feature in clustering. Then a trade-off measure between within-cluster compactness and among-cluster separation is proposed for stopping clustering. After that, we apply a labeling method to find representative feature for each cluster. Finally, experiments are conducted on word-based clustering in Chinese dataset and the result shows a good effect.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R</author>
<author>D W Embley</author>
</authors>
<title>Grouping search-engine returned citations for person-name queries.</title>
<date>2004</date>
<booktitle>In Proceedings of WIDM’04,</booktitle>
<pages>96--103</pages>
<location>Washington, DC, USA.</location>
<marker>R, Embley, 2004</marker>
<rawString>Al-Kamha. R. and D. W. Embley. 2004. Grouping search-engine returned citations for person-name queries. In Proceedings of WIDM’04, 96-103, Washington, DC, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bagga</author>
<author>B Baldwin</author>
</authors>
<title>Entity-based crossdocument coreferencing using the vector space model.</title>
<date>1998</date>
<booktitle>In Proceedings of 17th International Conference on Computational Linguistics,</booktitle>
<pages>79--85</pages>
<contexts>
<context position="3086" citStr="Bagga &amp; Baldwin (1998)" startWordPosition="438" endWordPosition="442">g with pointwise mutual information, cluster-stopping measure based on trade-off point, and cluster labeling algorithm. These are the main contribution of this paper; Section 5 discusses our experimental result. Finally, the conclusion and suggestions for further extension of the work are given in Section 6. 2 Related Work Due to the varying ambiguity of personal names in a corpus, existing approaches typically cast it as an unsupervised clustering problem based on vector space model. The main difference among these approaches lies in the features, which are used to create a similarity space. Bagga &amp; Baldwin (1998) first performed within-document coreference resolution, and then explored features in local context. Mann &amp; Yarowsky (2003) extracted local biographical information as features. Al-Kamha and Embley (2004) clustered search results with feature set including attributes, links and page similarities. Chen and Martin (2007) explored the use of a range of syntactic and semantic features in unsupervised clustering of documents. Song (2007) learned the PLSA and LDA model as feature sets. Ono et al. (2008) used mixture features including co-occurrences 88 Proceedings of the ACL-IJCNLP 2009 Student Res</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Bagga and B. Baldwin. 1998. Entity-based crossdocument coreferencing using the vector space model. In Proceedings of 17th International Conference on Computational Linguistics, 79–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>B Baldwin</author>
</authors>
<title>Algorithms for scoring co-reference chains.</title>
<date>1998</date>
<booktitle>In Proceedings of the First International Conference on Language Resources and Evaluation Workshop on Linguistic co-reference.</booktitle>
<contexts>
<context position="3086" citStr="Bagga &amp; Baldwin (1998)" startWordPosition="438" endWordPosition="442">g with pointwise mutual information, cluster-stopping measure based on trade-off point, and cluster labeling algorithm. These are the main contribution of this paper; Section 5 discusses our experimental result. Finally, the conclusion and suggestions for further extension of the work are given in Section 6. 2 Related Work Due to the varying ambiguity of personal names in a corpus, existing approaches typically cast it as an unsupervised clustering problem based on vector space model. The main difference among these approaches lies in the features, which are used to create a similarity space. Bagga &amp; Baldwin (1998) first performed within-document coreference resolution, and then explored features in local context. Mann &amp; Yarowsky (2003) extracted local biographical information as features. Al-Kamha and Embley (2004) clustered search results with feature set including attributes, links and page similarities. Chen and Martin (2007) explored the use of a range of syntactic and semantic features in unsupervised clustering of documents. Song (2007) learned the PLSA and LDA model as feature sets. Ono et al. (2008) used mixture features including co-occurrences 88 Proceedings of the ACL-IJCNLP 2009 Student Res</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Bagga, Amit and B. Baldwin. 1998. Algorithms for scoring co-reference chains. In Proceedings of the First International Conference on Language Resources and Evaluation Workshop on Linguistic co-reference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Ying</author>
<author>James Martin</author>
</authors>
<title>Towards Robust Unsupervised Personal Name Disambiguation,</title>
<date>2007</date>
<location>EMNLP</location>
<marker>Ying, Martin, 2007</marker>
<rawString>Chen Ying and James Martin. 2007. Towards Robust Unsupervised Personal Name Disambiguation, EMNLP 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chen Lifei</author>
<author>Jiang Qingshan</author>
<author>Wang Shengrui</author>
</authors>
<title>A Hierarchical Method for Determining the Number of Clusters.</title>
<date>2008</date>
<journal>Journal of Software,</journal>
<volume>19</volume>
<issue>1</issue>
<note>[in Chinese]</note>
<marker>Lifei, Qingshan, Shengrui, 2008</marker>
<rawString>Chen Lifei, Jiang Qingshan, and Wang Shengrui. 2008. A Hierarchical Method for Determining the Number of Clusters. Journal of Software, 19(1). [in Chinese]</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chung Heong Gooi</author>
<author>James Allan</author>
</authors>
<title>Crossdocument co-reference on a large scale corpus.</title>
<date>2004</date>
<booktitle>HLTNAACL 2004: Main Proceedings, 9–16,</booktitle>
<volume>2</volume>
<pages>-</pages>
<editor>In S. Dumais, D. Marcu, and S. Roukos, editors,</editor>
<location>Boston, Massachusetts, USA,</location>
<marker>Gooi, Allan, 2004</marker>
<rawString>Chung Heong Gooi and James Allan. 2004. Crossdocument co-reference on a large scale corpus. In S. Dumais, D. Marcu, and S. Roukos, editors, HLTNAACL 2004: Main Proceedings, 9–16, Boston, Massachusetts, USA, May 2 - May 7 2004. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gao Huixian</author>
</authors>
<title>Applied Multivariate Statistical Analysis. Peking Univ.</title>
<date>2004</date>
<publisher>Press.</publisher>
<marker>Huixian, 2004</marker>
<rawString>Gao Huixian. Applied Multivariate Statistical Analysis. Peking Univ. Press. 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C Buckley</author>
</authors>
<title>Term-weighting approaches in automatic text retrieval.</title>
<date>1988</date>
<booktitle>Information Processing and Management,</booktitle>
<marker>Salton, Buckley, 1988</marker>
<rawString>G. Salton and C. Buckley. 1988. Term-weighting approaches in automatic text retrieval. Information Processing and Management,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kulkarni Anagha</author>
<author>Ted Pedersen</author>
</authors>
<title>How Many Different “John Smiths”, and Who are They?</title>
<date>2006</date>
<booktitle>In Proceedings of the Student Abstract and Poster Session of the 21st National Conference on Artificial Intelligence,</booktitle>
<location>Boston, Massachusetts.</location>
<marker>Anagha, Pedersen, 2006</marker>
<rawString>Kulkarni Anagha and Ted Pedersen. 2006. How Many Different “John Smiths”, and Who are They? In Proceedings of the Student Abstract and Poster Session of the 21st National Conference on Artificial Intelligence, Boston, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Mann</author>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised personal name disambiguation.</title>
<date>2003</date>
<booktitle>Proceedings of CoNLL-2003,</booktitle>
<editor>In W. Daelemans and M. Osborne, editors,</editor>
<location>33–40, Edmonton, Canada.</location>
<contexts>
<context position="3210" citStr="Mann &amp; Yarowsky (2003)" startWordPosition="456" endWordPosition="459">e are the main contribution of this paper; Section 5 discusses our experimental result. Finally, the conclusion and suggestions for further extension of the work are given in Section 6. 2 Related Work Due to the varying ambiguity of personal names in a corpus, existing approaches typically cast it as an unsupervised clustering problem based on vector space model. The main difference among these approaches lies in the features, which are used to create a similarity space. Bagga &amp; Baldwin (1998) first performed within-document coreference resolution, and then explored features in local context. Mann &amp; Yarowsky (2003) extracted local biographical information as features. Al-Kamha and Embley (2004) clustered search results with feature set including attributes, links and page similarities. Chen and Martin (2007) explored the use of a range of syntactic and semantic features in unsupervised clustering of documents. Song (2007) learned the PLSA and LDA model as feature sets. Ono et al. (2008) used mixture features including co-occurrences 88 Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, pages 88–95, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP of named entities, key compound words, a</context>
</contexts>
<marker>Mann, Yarowsky, 2003</marker>
<rawString>Mann G. and D. Yarowsky. 2003. Unsupervised personal name disambiguation. In W. Daelemans and M. Osborne, editors, Proceedings of CoNLL-2003, 33–40, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niu Cheng</author>
<author>Wei Li</author>
<author>Rohini K Srihari</author>
</authors>
<title>Weakly Supervised Learning for Cross-document Person Name Disambiguation Supported by Information Extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL</booktitle>
<marker>Cheng, Li, Srihari, 2004</marker>
<rawString>Niu Cheng, Wei Li, and Rohini K. Srihari. 2004. Weakly Supervised Learning for Cross-document Person Name Disambiguation Supported by Information Extraction. In Proceedings of ACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Issei Sato Shingo</author>
<author>Minoru Yoshida</author>
<author>Hiroshi Nakagawa2</author>
</authors>
<title>Person Name Disambiguation in Web Pages Using Social Network, Compound Words and Latent Topics.</title>
<date>2008</date>
<booktitle>T. Washio et al. (Eds.): PAKDD 2008, LNAI 5012,</booktitle>
<pages>260--271</pages>
<marker>Shingo, Yoshida, Nakagawa2, 2008</marker>
<rawString>Ono. Shingo, Issei Sato, Minoru Yoshida, and Hiroshi Nakagawa2. 2008. Person Name Disambiguation in Web Pages Using Social Network, Compound Words and Latent Topics. T. Washio et al. (Eds.): PAKDD 2008, LNAI 5012, 260–271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Song Yang</author>
<author>Jian Huang</author>
<author>Isaac G Councill</author>
<author>Jia Li</author>
<author>C Lee Giles</author>
</authors>
<title>Efficient Topic-based Unsupervised Name Disambiguation.</title>
<date>2007</date>
<tech>JCDL’07,</tech>
<marker>Yang, Huang, Councill, Li, Giles, 2007</marker>
<rawString>Song Yang, Jian Huang, Isaac G. Councill, Jia Li, and C. Lee Giles. 2007. Efficient Topic-based Unsupervised Name Disambiguation. JCDL’07, June</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Kulkarni Anagha</author>
</authors>
<title>Automatic Cluster Stopping with Criterion Functions and the Gap Statistic.</title>
<date>2006</date>
<booktitle>In Proceedings of the Demonstration Session of the Human Language Technology Conference and the Sixth Annual Meeting of the North American Chapter of the Association for Computational Linguistic,</booktitle>
<location>New York City, NY.</location>
<marker>Pedersen, Anagha, 2006</marker>
<rawString>Ted Pedersen and Kulkarni Anagha. 2006. Automatic Cluster Stopping with Criterion Functions and the Gap Statistic. In Proceedings of the Demonstration Session of the Human Language Technology Conference and the Sixth Annual Meeting of the North American Chapter of the Association for Computational Linguistic, New York City, NY.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>