<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<author confidence="0.442571">
Identifying and Following Expert Investors in Stock Microblogs
</author>
<affiliation confidence="0.651310333333333">
1Roy Bar-Haim, 1Elad Dinur, 1,2Ronen Feldman, 1Moshe Fresko and 1Guy Goldstein
1Digital Trowel, Airport City, Israel
2School of Business Administration, The Hebrew University of Jerusalem, Jerusalem, Israel
</affiliation>
<email confidence="0.9861">
{roy, moshe}@digitaltrowel.com, ronen.feldman@huji.ac.il
</email>
<sectionHeader confidence="0.996524" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99856852631579">
Information published in online stock invest-
ment message boards, and more recently in
stock microblogs, is considered highly valu-
able by many investors. Previous work fo-
cused on aggregation of sentiment from all
users. However, in this work we show that it
is beneficial to distinguish expert users from
non-experts. We propose a general framework
for identifying expert investors, and use it as a
basis for several models that predict stock rise
from stock microblogging messages (stock
tweets). In particular, we present two methods
that combine expert identification and per-user
unsupervised learning. These methods were
shown to achieve relatively high precision in
predicting stock rise, and significantly outper-
form our baseline. In addition, our work pro-
vides an in-depth analysis of the content and
potential usefulness of stock tweets.
</bodyText>
<sectionHeader confidence="0.998878" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9997115625">
Online investment message boards such as Yahoo!
Finance and Raging Bull allow investors to share
trading ideas, advice and opinions on public com-
panies. Recently, stock microblogging services such
as StockTwits (which started as a filtering service
over the Twitter platform) have become very popu-
lar. These forums are considered by many investors
as highly valuable sources for making their trading
decisions.
This work aims to mine useful investment in-
formation from messages published in stock mi-
croblogs. We shall henceforth refer to these mes-
sages as stock tweets. Ultimately, we would like to
transform those tweets into buy and sell decisions.
Given a set of stock-related messages, this process
typically comprises two steps:
</bodyText>
<listItem confidence="0.9714578">
1. Classify each message as “bullish” (having a
positive outlook on the stock), “bearish” (hav-
ing a negative outlook on the stock), or neutral.
2. Make trading decisions based on these message
classifications.
</listItem>
<bodyText confidence="0.9990078">
Previous work on stock investment forums and
microblogs usually regarded the first step (message
classification) as a sentiment analysis problem, and
aligned bullish with positive sentiment and bearish
with negative sentiment. Messages were classified
by matching positive and negative terms from sen-
timent lexicons, learning from a hand-labeled set of
messages, or some combination of the two (Das and
Chen, 2007; Antweiler and Frank, 2004; Chua et al.,
2009; Zhang and Skiena, 2010; Sprenger and Welpe,
2010). Trading decisions were made by aggregating
the sentiment for a given stock over all the tweets,
and picking stocks with strongest sentiment signal
(buying the most bullish stocks and short-selling the
most bearish ones).
Sentiment aggregation reflects the opinion of the
investors community as a whole, but overlooks the
variability in user expertise. Clearly, not all investors
are born equal, and if we could tell experts from non-
experts, we would reduce the noise in these forums
and obtain high-quality signals to follow. This pa-
per presents a framework for identifying experts in
stock microblogs by monitoring their performance
in a training period. We show that following the ex-
perts results in more precise predictions.
</bodyText>
<page confidence="0.912063">
1310
</page>
<note confidence="0.9577375">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1310–1319,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.999793454545454">
Based on the expert identification framework, we
experiment with different methods for deriving pre-
dictions from stock tweets. While previous work
largely aligned bullishness with message sentiment,
our in-depth content analysis of stock tweets (to be
presented in section 2.2) suggests that this view is
too simplistic. To start with, one important dif-
ference between bullishness/bearishness and posi-
tive/negative sentiment is that while the former rep-
resents belief about the future, the latter may also
refer to the past or present. For example, a user re-
porting on making profit from a buying stock yester-
day and selling it today is clearly positive about the
stock, but does not express any prediction about its
future performance. Furthermore, messages that do
refer to the future differ considerably in their signif-
icance. A tweet reporting on buying a stock by the
user conveys a much stronger bullishness signal than
a tweet that merely expresses an opinion. Overall, it
would seem that judging bullishness is far more elu-
sive than judging sentiment.
We therefore propose and compare two alterna-
tive approaches that sidestep the complexities of as-
sessing tweets bullishness. These two approaches
can be viewed as representing two extremes. The
first approach restricts our attention to the most ex-
plicit signals of bullishness and bearishness, namely,
tweets that report actual buy and sell transactions
performed by the user. In the second approach we
learn directly the relation between tweets content
and stock prices, following previous work on pre-
dicting stock price movement from factual sources
such as news articles (Lavrenko et al., 2000; Koppel
and Shtrimberg, 2004; Schumaker and Chen, 2010).
This approach poses no restrictions on the tweets
content and avoids any stipulated tweet classifica-
tion. However, user-generated messages are largely
subjective, and their correlation with the stock prices
depends on user’s expertise. This introduces much
noise into the learning process. We show that by
making the learning user-sensitive we can improve
the results substantially. Overall, our work illus-
trates the feasibility of finding expert investors, and
the utility of following them.
</bodyText>
<sectionHeader confidence="0.984356" genericHeader="introduction">
2 Stock Tweets
</sectionHeader>
<subsectionHeader confidence="0.999323">
2.1 Stock Tweets Language
</subsectionHeader>
<bodyText confidence="0.999962407407407">
Stock tweets, as Twitter messages in general, are
short textual messages of up to 140 characters. They
are distinguished by having one or more references
to stock symbols (tickers), prefixed by a dollar sign.
For instance, the stock of Apple, Inc. is referenced
as $AAPL. Two other noteworthy Twitter conven-
tions that are also found in stock tweets are hashtags,
user-defined labels starting with ‘#’, and references
to other users, starting with ‘@’. Table 1 lists some
examples of stock tweets.
As common with Twitter messages, stock tweets
are typically abbreviated and ungrammatical utter-
ances. The language is informal and includes many
slang expressions, many of which are unique to the
stock tweets community. Thus, many positive and
negative expressions common to stock tweets are not
found in standard sentiment lexicons. Their unique
language and terminology often make stock tweets
hard to understand for an outsider. Many words
are abbreviated and appear in several non-standard
forms. For example, the word bought may also ap-
pear as bot or bght, and today may appear as 2day.
Stock tweets also contain many sentiment expres-
sions which may appear in many variations, e.g.
wow, woooow, woooooooow and so on. These char-
acteristics make the analysis of stock tweets a par-
ticularly challenging task.
</bodyText>
<subsectionHeader confidence="0.999794">
2.2 Content Analysis
</subsectionHeader>
<bodyText confidence="0.9999796">
A preliminary step of this research was an exten-
sive data analysis, aimed to gain better understand-
ing of the major types of content conveyed in stock
tweets. First, we developed a taxonomy of tweet
categories while reading a few thousands of tweets.
Based on this taxonomy we then tagged a sample
of 350 tweets to obtain statistics on the frequency
of each category. The sample contained only tweets
that mention exactly one ticker. The following types
of tweets were considered irrelevant:
</bodyText>
<listItem confidence="0.9954542">
• Tweets that express question. These tweets
were labeled as Question.
• Obscure tweets, e.g. “$AAPL fat”, tweets
that contain insufficient information (e.g.
“http://url.com $AAPL”) and tweets that seem
</listItem>
<page confidence="0.966797">
1311
</page>
<table confidence="0.9995253125">
Example %
Fact News $KFRC: Deutsche Bank starts at Buy 14.3%
Chart Pattern $C (Citigroup Inc) $3.81 crossed its 2nd Pivot Point Support 10.9%
http://empirasign.com/s/x4c
Trade bot back some $AXP this morning 12.9%
Trade Outcome Sold $CELG at 55.80 for day-trade, +0.90 (+1.6%)X 2.9%
Opinion Speculation thinking of hedging my shorts by buying some oil. thinking of 4.0%
buying as much $goog as i can in my IRA. but i need more doing,
less thinking.
Chart Prediction http://chart.ly/wsy5ny $GS - not looking good for this one - 12.9%
breaks this support line on volume will nibble a few short
Recommendation $WFC if you have to own financials, WFC would be my choice. 1.7%
http://fsc.bz/448 #WORDEN
Sentiment $ivn is rocking 8.6%
Question $aapl breaking out but in this mkt should wait till close? 7.1%
Irrelevant $CLNE follow Mr. Clean $$ 24.9%
</table>
<tableCaption confidence="0.999894">
Table 1: Tweets categories and their relative frequencies
</tableCaption>
<bodyText confidence="0.934772571428572">
to contain no useful information (e.g “Even
Steve Jobs is wrong sometimes... $AAPL
http://ow.ly/1Tw0Z”). These tweets were la-
beled Irrelevant.
The rest of the tweets were classified into two major
categories: Facts and Opinions.
Facts can be divided into four main subcategories:
</bodyText>
<listItem confidence="0.881980553191489">
1. News: such tweets are generally in the form of
a tweeted headline describing news or a current
event generally drawn from mass media. As
such they are reliable but, since the information
is available in far greater detail elsewhere, their
added value is limited.
2. Chart Pattern: technical analysis aims to pro-
vide insight into trends and emerging patterns
in a stock’s price. These tweets describe pat-
terns in the stock’s chart without the inclusion
of any predicted or projected movement, an im-
portant contrast to Chart Prediction, which is
an opinion tweet described below. Chart pat-
tern tweets, like news, are a condensed form of
information already available through more in-
depth sources and as such their added value is
limited.
3. Trade: reports an actual purchase or sale of a
stock by the user. We consider this as the most
valuable form of tweet.
4. Trade Outcome: provides details of an “inverse
trade”, the secondary trade to exit the initial
position along with the outcome of the over-
all trade (profit/loss). The value of these tweets
is debatable since although they provide details
of a trade, they generally describe the “exit”
transaction. This creates a dilemma for ana-
lysts since traders will often exit not because
of a perceived change in the stock’s potential
but as a result of many short-term trading ac-
tivities. For this reason trade outcome provides
a moderate insight into a user’s position which
should be viewed with some degree of caution.
Opinions can also be divided into four main subcat-
egories:
1. Speculation: provides individual predictions of
future events relating to a company or actions
of the company. These are amongst the least
reliable categories, as the individual user is typ-
ically unable to justify his or her insight into the
predicted action.
2. Chart Prediction: describes a user’s prediction
of a future chart movement based on technical
analysis of the stock’s chart.
3. Recommendation: As with analyst recommen-
dations, this category represents users who
summarize their understanding and insight into
</listItem>
<page confidence="0.983234">
1312
</page>
<bodyText confidence="0.8967609">
a stock with a simple and effective recommen-
dation to take a certain course of action with
regard to a particular share. Recommendation
is the less determinate counterpart to Trade.
4. Sentiment: These tweets express pure senti-
ment toward the stock, rather than any factual
content.
Table 1 shows examples for each of the tweet cate-
gories, as well as their relative frequency in the ana-
lyzed sample.
</bodyText>
<sectionHeader confidence="0.961048" genericHeader="method">
3 An Expert Finding Framework
</sectionHeader>
<bodyText confidence="0.999958090909091">
In this section we present a general procedure for
finding experts in stock microblogs. Based on this
procedure, we will develop in the next sections sev-
eral models for extracting reliable trading signals
from tweets.
We assume that a stock tweet refers to exactly one
stock, and therefore there is a one-to-one mapping
between tweets and stocks. Other tweets are dis-
carded. We define expertise as the ability to pre-
dict stock rise with high precision. Thus, a user is
an expert if a high percentage of his or her bullish
tweets is followed by a stock rise. In principle, we
could analogously follow bearish tweets, and see if
they are followed by a stock fall. However, bearish
tweets are somewhat more difficult to interpret: for
example, selling a share may indicate a negative out-
look on the stock, but it may also result from other
considerations, e.g. following a trading strategy that
holds the stock for a fixed period (cf. the discussion
on Trade Outcome tweets in the previous section).
We now describe a procedure that determines
whether a user u is an expert. The procedure re-
ceives a training set T of tweets posted by u, where
each tweet is annotated with its posting time. It is
also given a classifier C, which classifies each tweet
as bullish or not bullish (either bearish or neutral).
The procedure first applies the classifier C to iden-
tify the bullish tweets in T . It then determines the
correctness of each bullish tweet. Given a tweet t,
we observe the price change of the stock referenced
by t over a one day period starting at the next trading
day. The exact definition of mapping tweets to stock
prices is given in section 5.1. A one-day holding
period was chosen as it was found to perform well
in previous works on tweet-based trading (Zhang
and Skiena, 2010; Sprenger and Welpe, 2010), in
particular for long positions (buy transactions). A
bullish tweet is considered correct if it is followed
by a stock rise, and as incorrect otherwise1. Given a
set of tweets, we define its precision as the percent-
age of correct tweets in the set. Let Cu, Iu denote
the number of correct and incorrect bullish tweets
of user u, respectively. The precision of u’s bullish
tweets is therefore:
</bodyText>
<equation confidence="0.9681055">
Cu
Pu = Cu + Iu
</equation>
<bodyText confidence="0.999055090909091">
Let Pbl be the baseline precision. In this work we
chose the baseline precision to be the proportion of
tweets that are followed by a stock rise in the whole
training set (including all the users). This represents
the expected precision when picking tweets at ran-
dom. Clearly, if Pu G Pbl then u is not an expert.
If Pu &gt; Pbl, we apply the following statistical test
to assess whether the difference is statistically sig-
nificant. First, we compute the expected number of
correct and incorrect transactions Cbl, Ibl according
to the baseline:
</bodyText>
<equation confidence="0.96922">
Cbl = Pbl X (Cu + Iu)
Ibl = (1 − Pbl) X (Cu + Iu)
</equation>
<bodyText confidence="0.999982529411765">
We then compare the observed counts (Cu, Iu) to
the expected counts (Cbl, Ibl), using Pearson’s Chi-
square test. Since it is required for this test that
Cbl and Ibl are at least 5, cases that do not meet
this requirement are discarded. If the resulting p-
value satisfies the required significance level α, then
u is considered an expert. In this work we take
α = 0.05. Note that since the statistical test takes
into account the number of observations, it will re-
ject cases where the number of the observations is
very small, even if the precision is very high. The
output of the procedure is a classification of u as
expert/non-expert, as well as the p-value (for ex-
perts). The expert finding procedure is summarized
in Algorithm 1.
In the next two sections we propose several alter-
natives for the classifier C.
</bodyText>
<footnote confidence="0.994525666666667">
1For about 1% of the tweets the stock price did not change
in the next trading day. These tweets are also considered correct
throughout this work.
</footnote>
<page confidence="0.919494">
1313
</page>
<construct confidence="0.424547">
Algorithm 1 Determine if a user u is an expert
</construct>
<bodyText confidence="0.998898666666667">
Input: set of tweets T posted by u, bullishness
classifier C, baseline probability Pbl, significance
level α
</bodyText>
<equation confidence="0.896746">
Output: NON-EXPERT/(EXPERT, p-value)
Tbullish +-- tweets in T classified by C as bullish
Cu +-- 0 ; Iu +-- 0
for each t E Tbullish do
</equation>
<bodyText confidence="0.327867">
if t is followed by a stock rise then
</bodyText>
<figure confidence="0.657680947368421">
Cu++
else
Iu++
end if
end for
Pu = Cu
Cu+Iu
if Pu G Pbl then
return NON-EXPERT
else
Cbl +-- Pbl X (Cu + Iu)
Ibl +-- (1 − Pbl) X (Cu + Iu)
p +-- ChiSquareTest(Cu, Iu, Cbl, Ibl)
if p &gt; α then
return NON-EXPERT
else
return (EXPERT, p)
end if
end if
</figure>
<sectionHeader confidence="0.984786" genericHeader="method">
4 Following Explicit Transactions
</sectionHeader>
<bodyText confidence="0.998029625">
The first approach we attempt for classifying bullish
(and bearish) tweets aims to identify only tweets that
report buy and sell transactions (that is, tweets in
the Trade category). According to our data analysis
(reported in section 2.2), about 13% of the tweets
belong to this category. There are two reasons to
focus on these tweets. First, as we already noted,
actual transactions are clearly the strongest signal
of bulishness/bearishness. Second, the buy and sell
actions are usually reported using a closed set of
expressions, making these tweets relatively easy to
identify. A few examples for buy and sell tweets are
shown in Table 2.
While buy and sell transactions can be captured
reasonably well by a relatively small set of patterns,
the examples in Table 2 show that stock tweets have
</bodyText>
<table confidence="0.9994929">
sell sold sum $OMNI 2.14 +12%
buy bot $MSPD for earnings testing
new indicator as well.
sell Out 1/2 $RIMM calls @ 1.84
(+0.81)
buy added to $joez 2.56
buy I picked up some $X JUL 50 Puts @
3.20 for gap fill play about an hour
ago.
buy long $BIDU 74.01
buy $$ Anxiously sitting at the bid on
$CWCO @ 11.85 It seems the ask
and I are at an impasse. 20 min of
this so far. Who will budge? (not
me)
buy In 300 $GOOG @ 471.15.
sell sold $THOR 41.84 for $400 the
FreeFactory is rocking
sell That was quick stopped out $ICE
sell Initiated a short position in $NEM.
</table>
<tableCaption confidence="0.998804">
Table 2: Buy and sell tweets
</tableCaption>
<bodyText confidence="0.995767142857143">
their unique language for reporting these transac-
tions, which must be investigated in order to come
by these patterns. Thus, in order to develop a clas-
sifier for these tweets, we created a training and test
corpora as follows. Based on our preliminary anal-
ysis of several thousand tweets, we composed a vo-
cabulary of keywords which trade tweets must in-
clude2. This vocabulary contained words such as in,
out, bot, bght, sld and so on. Filtering out tweets that
match none of the keywords removed two thirds of
the tweets. Out of the remaining tweets, about 5700
tweets were tagged. The training set contains about
3700 tweets, 700 of which are transactions. The test
set contains about 2000 tweets, 350 of which are
transactions.
Since the transaction tweets can be characterized
by a closed set of recurring patterns, we developed
a classifier that is based on a few dozens of man-
ually composed pattern matching rules, formulated
as regular expressions. The classifier works in three
stages:
</bodyText>
<listItem confidence="0.7099994">
1. Normalization: The tweet is transformed into
a canonical form. For example, user name
2That is, we did not come across any trade tweet that does
not include at least one of the keywords in the large sample we
analyzed, so we assume that such tweets are negligible.
</listItem>
<page confidence="0.858023">
1314
</page>
<table confidence="0.9998596">
Dataset Transaction P R F1
Train Buy 94.0% 84.0% 0.89
Sell 96.0% 83.0% 0.89
Test Buy 85.0% 70.0% 0.77
Sell 88.5% 79.0% 0.84
</table>
<tableCaption confidence="0.9850895">
Table 3: Results for buy/sell transactition classifier. Pre-
cision (P), Recall (R), and F-measure (F1) are reported.
</tableCaption>
<bodyText confidence="0.94205575">
is transformed into USERNAME; ticker name
is transformed into TICKER; buy, buying,
bought, bot, bght are transformed into BUY,
and so on.
</bodyText>
<listItem confidence="0.985984333333333">
2. Matching: Trying to match one of the buy/sell
patterns in the normalized tweet.
3. Filtering: Filtering out tweets that match “dis-
qualifying” patterns. The simplest examples
are a tweet starting with an “if” or a tweet con-
taining a question mark.
</listItem>
<bodyText confidence="0.99983675">
The results of the classifier on the train and test set
are summarized in Table 3. The results show that
our classifier identifies buy/sell transactions with a
good precision and a reasonable recall.
</bodyText>
<sectionHeader confidence="0.9117045" genericHeader="method">
5 Unsupervised Learning from Stock
Prices
</sectionHeader>
<bodyText confidence="0.999993428571429">
The drawback of the method presented in the pre-
vious section is that it only considers a small part
of the available tweets. In this section we propose
an alternative method, which considers all the avail-
able tweets, and does not require any tagged corpus
of tweets. Instead, we use actual stock price move-
ments as our labels.
</bodyText>
<subsectionHeader confidence="0.99867">
5.1 Associating Tweets with Stock Prices
</subsectionHeader>
<bodyText confidence="0.999997888888889">
We used stock prices to label tweets as follows. Each
tweet message has a time stamp (eastern time), indi-
cating when it was published. Our policy is to buy
in the opening price of the next trading day (PB),
and sell on the opening price of the following trad-
ing day (PS). Tweets that are posted until 9:25 in the
morning (market hours begin at 9:30) are associated
with the same day, while those are posted after that
time are associated with the next trading date.
</bodyText>
<subsectionHeader confidence="0.998046">
5.2 Training
</subsectionHeader>
<bodyText confidence="0.996253857142857">
Given the buy and sell prices associated with each
tweet, we construct positive and negative training
examples as follows: positive examples are tweets
where PS−PB PB≥ 3%, and negative examples are
tweets where PS−PB PB≤ −3%.
We used the SVM-light package (Joachims,
1999), with the following features:
</bodyText>
<listItem confidence="0.9977814">
• The existence of the following elements in the
message text:
– Reference to a ticker
– Reference to a user
– URL
– Number
– Hashtag
– Question mark
• The case-insensitive words in the message after
dropping the above elements.
• The 3, 4, 5 letter prefixes of each word.
• The name of the user who authored the tweet,
if it is a frequent user (at least 50 messages in
the training data). Otherwise, the user name is
taken to be “anonymous”.
• Whether the stock price was up or down 1% or
more in the previous trading day.
• 2, 3, 4-word expressions which are typical to
tweets (that is, their relative frequency in tweets
is much higher than in general news text).
</listItem>
<sectionHeader confidence="0.983641" genericHeader="method">
6 Empirical Evaluation
</sectionHeader>
<bodyText confidence="0.99993475">
In this section we focus on the empirical task of
tweet ranking: ordering the tweets in the test set ac-
cording to their likelihood to be followed by a stock
rise. This is similar to the common IR task of rank-
ing documents according to their relevance. A per-
fect ranking would place all the correct tweets before
all the incorrect ones.
We present several ranking models that use the
expert finding framework and the bullishness classi-
fication methods discussed in the previous sections
as building blocks. The performance of these mod-
els is evaluated on the test set. By considering the
</bodyText>
<page confidence="0.981667">
1315
</page>
<bodyText confidence="0.999933333333333">
precision at various points along the list of ranked
tweets, we can compare the precision-recall trade-
offs achieved by each model.
Before we discuss the ranking models and the em-
pirical results, we describe the datasets used to train
and test these models.
</bodyText>
<subsectionHeader confidence="0.986899">
6.1 Datasets
</subsectionHeader>
<bodyText confidence="0.9999725">
Stock tweets were downloaded from the StockTwits
website3, during two periods: from April 25, 2010
to November 1, 2011, and from December 14, 2010
to February 3, 2011. A total of 700K tweets mes-
sages were downloaded. Tweets that do not contain
exactly one stock ticker (traded in NYSE or NAS-
DAQ) were filtered out. The remaining 340K tweets
were divided as follows:
</bodyText>
<listItem confidence="0.999415333333333">
• Development set: April 25, 2010 to August 31,
2010: 124K messages
• Held out set: September 1, 2010 to November
1, 2010: 110K messages
• Test set: December 14, 2010 to February 3,
2011: 106K messages
</listItem>
<bodyText confidence="0.995751">
We consider the union of the development and held
out sets as our training set.
</bodyText>
<subsectionHeader confidence="0.932988">
6.2 Ranking Models
6.2.1 Joint-All Model
</subsectionHeader>
<bodyText confidence="0.999763">
This is our baseline model, as it does not attempt
to identify experts. It learns a single SVM model
as described in Section 5 from all the tweets in the
training set. It then applies the SVM model to each
tweet in the test set, and ranks them according to the
SVM classification score.
</bodyText>
<subsubsectionHeader confidence="0.65907">
6.2.2 Transaction Model
</subsubsectionHeader>
<bodyText confidence="0.94372">
This model finds expert users in the training set
(Algorithm 1), using the buy/sell classifier described
in Section 4. Tweets classified as buy are considered
bullish, and the rest are considered non-bullish. Ex-
pert users are ranked according to their p value (in
ascending order). The same classifier is then applied
to the tweets of the expert users in the test set. The
tweets classified as bullish are ordered according to
the ranking of their author (first all the bullish tweets
3stocktwits.com
of the highest-ranked expert user, then all the bullish
tweets of the expert ranked second, and so on).
</bodyText>
<subsubsectionHeader confidence="0.630057">
6.2.3 Per-User Model
</subsubsectionHeader>
<bodyText confidence="0.999983483870968">
The joint all model suffers from the tweets of
non-experts twice: at training time, these tweets in-
troduce much noise into the training of the SVM
model. At test time, we follow these unreliable
tweets along with the more reliable tweets of the ex-
perts. The per-user model addresses both problems.
This model learns from the development set a sep-
arate SVM model Cu for each user u, based solely
on the user’s tweets. We then optimize the clas-
sification threshold of the learnt SVM model Cu
as follows. Setting the threshold to 0 results in a
new classifier Cu,B. Algorithm 1 is applied to u’s
tweets in the held-out set (denoted Wu), using the
classifier Cu,B. For the ease of presentation, we de-
fine ExpertPValue(Wu, Cu,B,Pbl,α) as a function that
calls Algorithm 1 with the given parameters, and re-
turns the obtained p-value if u is an expert and 1
otherwise. We search exhaustively for the thresh-
old 0ˆ for which this function is minimized (in other
words, the threshold that results in the best p-value).
The threshold of Cu is then set to ˆ0, and the user’s
p-value is set to the best p-value found. If u is a
non-expert for all of the attempted 0 values then u is
discarded. Otherwise, u is identified as an expert.
The rest of the process is similar to the transac-
tion model: the tweets of each expert u in the test
set are classified using the optimized per-user clas-
sifier Cu. The final ranking is obtained by sorting
the tweets that were classified as bullish according
to the p-value of their author. The per-user ranking
procedure is summarized in Algorithm 2.
</bodyText>
<subsubsectionHeader confidence="0.379754">
6.2.4 Joint-Experts Model
</subsubsectionHeader>
<bodyText confidence="0.9999667">
The joint experts model makes use of the experts
identified by the per-user model, and builds a sin-
gle joint SVM model from the tweets of these users.
This results in a model that is trained on more exam-
ples than in the previous per-user method, but unlike
the joint all method, it learns only from high-quality
users. As with the joint all model, test tweets are
ranked according to the SVM’s score. However, the
model considers only the tweets of expert users in
the test set.
</bodyText>
<page confidence="0.994635">
1316
</page>
<figureCaption confidence="0.998163">
Figure 1: Empirical model comparison
</figureCaption>
<figure confidence="0.920135233333333">
70
65
60
55
50
45
Precision
random
joint all
per user
joint experts
transaction
0 200 400 600 800 1000 1200 1400
Tweets
Algorithm 2 Per-user ranking model
Input: dev. set D, held-out set W, test set S, base-
line probability Pbl, significance level α
Output: A ranked list R of tweets in S
// Learning from the training set
E ∅ // set of expert users
for each user u do
Du u’s tweets in D
Cu SVM classifier learnt from Du
Wu u’s tweets in W
Bˆ = arg minθ ExpertPValue(Wu, Cu,θ,Pbl,α)
Cu Cu,ˆθ
pu ExpertPValue(Wu, Cu,ˆθ,Pbl,α)
if pu G α then
add u to E
end if
</figure>
<subsectionHeader confidence="0.649667">
end for
</subsectionHeader>
<bodyText confidence="0.846021">
// Classifying and ranking the test set
for each user u E E do
Sbullish,u u’s tweets in S that were classified
as bullish by Cu
end for
</bodyText>
<equation confidence="0.652951">
R tweets in Uu Sbullish,u sorted by pu
return R
</equation>
<subsectionHeader confidence="0.8075">
6.3 Results
</subsectionHeader>
<bodyText confidence="0.990900222222222">
Figure 1 summarizes the results obtained for the
various models. Each model was used to rank the
tweets according to the confidence that they predict
a positive stock price movement. Each data point
corresponds to the precision obtained for the first k
tweets ranked by the model, and the results for vary-
ing k values illustrate the precision/recall tradeoff of
the model. These data points were obtained as fol-
lows:
</bodyText>
<listItem confidence="0.853402142857143">
• For methods that learn a single SVM model
(joint all and joint experts), the graph was ob-
tained by decreasing the threshold of the SVM
classifier, at fixed intervals of 0.05. For each
threshold value, k is the number of tweets clas-
sified as bullish by the model.
• For methods that rank the users by their p value
</listItem>
<bodyText confidence="0.994667">
and order the tweets accordingly (transaction
and per user), the i-th data point corresponds
to the cumulative precision for the tweets clas-
sified as bullish by the first i users. For the per
user method we show the cumulative results for
the first 20 users. For the transaction method
we show all the users that were identified as ex-
perts.
The random line is our baseline. It shows the ex-
pected results for randomly ordering the tweets in
the test set. The expected precision at any point is
equal to the percentage of tweets in the test set that
were followed by a stock rise, which was found to
be 51.4%.
We first consider the joint all method, which
learns a single model from all the tweets. The only
</bodyText>
<page confidence="0.977216">
1317
</page>
<table confidence="0.999961285714286">
Correct Incorrect P p
87 46 65.4 0.001
142 86 62.3 0.001
162 103 61.1 0.002
220 158 58.2 0.008
232 168 58.0 0.008
244 176 58.1 0.006
299 229 56.6 0.016
335 255 56.8 0.009
338 268 55.8 0.031
344 269 56.1 0.019
419 346 54.8 0.062
452 387 53.9 0.152
455 389 53.9 0.145
479 428 52.8 0.395
481 430 52.8 0.398
487 435 52.8 0.388
675 564 54.5 0.030
683 569 54.6 0.026
690 573 54.6 0.022
720 591 54.9 0.011
</table>
<tableCaption confidence="0.979174666666667">
Table 4: Per user model: cumulative results for first 20
users. The table lists the number of correct and incorrect
tweets, the precision P and the significance level p.
</tableCaption>
<bodyText confidence="0.9999691">
per-user information available to this model is a fea-
ture fed to the SVM classifier, which, as we found,
does not contribute to the results. Except for the
first 58 tweets, which achieved precision of 55%,
the precision quickly dropped to a level of around
52%, which is just a little better than the random
baseline. Next, we consider the transaction configu-
ration, which is based on detecting buy transactions.
Only 10 users were found to be experts according to
this method, and in the test period these users had a
total of 173 tweets. These 173 tweets achieve good
precision (57.1% for the first 161 tweets, and 54.9%
for the first 173 tweets). However this method re-
sulted in a low number of transactions. This happens
because it is able to utilize only a small fraction of
the tweets (explicit buy transactions).
Remarkably, per user and joint experts, the two
methods which rely on identifying the experts via
unsupervised learning are by far the best methods.
Both models seem to have comparable performance,
where the results of the join experts model are some-
what smoother, as expected. Table 4 shows cumu-
lative results for the first 20 users in the per-user
model. The results show that this model achieves
good precision for a relatively large number of
tweets, and for most of the data points reported in the
table the results significantly outperform the base-
line (as indicated by the p value). Overall, these re-
sults show the effectiveness of our methods for find-
ing experts through unsupervised learning.
</bodyText>
<sectionHeader confidence="0.999935" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999884333333333">
A growing body of work aims at extracting senti-
ment and opinions from tweets, and exploit this in-
formation in a variety of application domains. Davi-
dov et al. (2010) propose utilizing twitter hash-
tag and smileys to learn enhanced sentiment types.
O’Connor et al. (2010) propose a sentiment detec-
tor based on Twitter data that may be used as a re-
placement for public opinion polls. Bollen et al.
(2011) measure six different dimensions of public
mood from a very large tweet collection, and show
that some of these dimensions improve the predica-
tion of changes in the Dow Jones Industrial Average
(DJIA).
Sentiment analysis of news articles and financial
blogs and their application for stock prediction were
the subject of several studies in recent years. Some
of these works focus on document-level sentiment
classification (Devitt and Ahmad, 2007; O’Hare et
al., 2009). Other works also aimed at predicting
stock movement (Lavrenko et al., 2000; Koppel
and Shtrimberg, 2004; Schumaker and Chen, 2010).
All these methods rely on predefined sentiment lex-
icons, manually classified training texts, or their
combination. Lavrenko et al. (2000), Koppel and
Shtrimberg (2004), and Schumaker and Chen (2010)
exploit stock prices for training, and thus save the
need in supervised learning.
Previous work on stock message boards include
(Das and Chen, 2007; Antweiler and Frank, 2004;
Chua et al., 2009). (Sprenger and Welpe, 2010) is, to
the best of our knowledge, the first work to address
specifically stock microblogs. All these works take
a similar approach for classifying message bullish-
ness: they train a classifier (Naive Bayes, which Das
and Chen combined with additional classifiers and
a sentiment lexicon, and Chua et al. presented im-
provement for) on a collection of manually labeled
messages (classified into Buy, Sell, Hold). Interest-
ingly, Chua et al. made use of an Australian mes-
</bodyText>
<page confidence="0.968278">
1318
</page>
<bodyText confidence="0.999910882352941">
sage board (HotCopper), where, unlike most of the
stock message boards, these labels are added by the
message author. Another related work is (Zhang and
Skiena, 2010), who apply lexicon-based sentiment
analysis to several sources of news and blogs, in-
cluding tweets. However, their data set does not in-
clude stock microblogs, but tweets mentioning the
official company name.
Our work differs from previous work on stock
messages in two vital aspects. Firstly, these works
did not attempt to distinguish between experts and
non-expert users, but aggregated the sentiment over
all the users when studying the relation between sen-
timent and the stock market. Secondly, unlike these
works, our best-performing methods are completely
unsupervised, and require no manually tagged train-
ing data or sentiment lexicons.
</bodyText>
<sectionHeader confidence="0.998906" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999995764705882">
This paper investigated the novel task of finding ex-
pert investors in online stock forums. In particular,
we focused on stock microblogs. We proposed a
framework for finding expert investors, and exper-
imented with several methods for tweet classifica-
tion using this framework. We found that combin-
ing our framework with user-specific unsupervised
learning allows us to predict stock price movement
with high precision, and the results were shown to be
statistically significant. Our results illustrate the im-
portance of distinguishing experts from non-experts.
An additional contribution of this work is an in-
depth analysis of stock tweets, which sheds light on
their content and its potential utility.
In future work we plan to improve the features of
the SVM classifier, and further investigate the use-
fulness of our approach for trading.
</bodyText>
<sectionHeader confidence="0.998986" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999183508196721">
Werner Antweiler and Murray Z. Frank. 2004. Is all
that talk just noise? the information content of in-
ternet stock message boards. Journal of Finance,
59(3):1259–1294.
Johan Bollen, Huina Mao, and Xiaojun Zeng. 2011.
Twitter mood predicts the stock market. Journal of
Computational Science, 2(1):1 – 8.
Christopher Chua, Maria Milosavljevic, and James R.
Curran. 2009. A sentiment detection engine for in-
ternet stock message boards. In Proceedings of the
Australasian Language Technology Association Work-
shop 2009.
Sanjiv R. Das and Mike Y. Chen. 2007. Yahoo! for
Amazon: Sentiment extraction from small talk on the
Web. Management Science, 53(9):1375–1388.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Enhanced sentiment learning using twitter hashtags
and smileys. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
COLING ’10. Association for Computational Linguis-
tics.
Ann Devitt and Khurshid Ahmad. 2007. Sentiment
polarity identification in financial news: A cohesion-
based approach. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics, pages 984–991.
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Scholkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vector
Learning. MIT-Press.
Moshe Koppel and Itai Shtrimberg. 2004. Good news
or bad news? Let the market decide. In Proceedings
of the AAAI Spring Symposium on Exploring Attitude
and Affect in Text: Theories and Applications.
Victor Lavrenko, Matt Schmill, Dawn Lawrie, Paul
Ogilvie, David Jensen, and James Allan. 2000. Min-
ing of concurrent text and time series. In Proceedings
of the 6th ACM SIGKDD Int’l Conference on Knowl-
edge Discovery and Data Mining Workshop on Text
Mining.
Brendan O’Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010. From
tweets to polls: Linking text sentiment to public opin-
ion time series. In Proceedings of the International
AAAI Conference on Weblogs and Social Media.
Neil O’Hare, Michael Davy, Adam Bermingham, Paul
Ferguson, Pvraic Sheridan, Cathal Gurrin, and Alan F
Smeaton. 2009. Topic-dependent sentiment analysis
of financial blogs. In TSA’09 - 1st International CIKM
Workshop on Topic-Sentiment Analysis for Mass Opin-
ion Measurement.
Robert P. Schumaker and Hsinchun Chen. 2010. A dis-
crete stock price prediction engine based on financial
news. Computer, 43:51–56.
Timm O. Sprenger and Isabell M. Welpe. 2010. Tweets
and trades: The information content of stock mi-
croblogs. Technical report, TUM School of Manage-
ment, December. working paper.
Wenbin Zhang and Steven Skiena. 2010. Trading
strategies to exploit blog and news sentiment. In
ICWSM’10.
</reference>
<page confidence="0.995892">
1319
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.342549">
<title confidence="0.998296">Identifying and Following Expert Investors in Stock Microblogs</title>
<author confidence="0.6713465">Dinur Bar-Haim</author>
<author confidence="0.6713465">Fresko Trowel Feldman</author>
<author confidence="0.6713465">Airport City</author>
<affiliation confidence="0.826288">of Business Administration, The Hebrew University of Jerusalem, Jerusalem,</affiliation>
<email confidence="0.998705">ronen.feldman@huji.ac.il</email>
<abstract confidence="0.99892985">Information published in online stock investment message boards, and more recently in stock microblogs, is considered highly valuable by many investors. Previous work focused on aggregation of sentiment from all users. However, in this work we show that it is beneficial to distinguish expert users from non-experts. We propose a general framework for identifying expert investors, and use it as a basis for several models that predict stock rise from stock microblogging messages (stock tweets). In particular, we present two methods that combine expert identification and per-user unsupervised learning. These methods were shown to achieve relatively high precision in predicting stock rise, and significantly outperform our baseline. In addition, our work provides an in-depth analysis of the content and potential usefulness of stock tweets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Werner Antweiler</author>
<author>Murray Z Frank</author>
</authors>
<title>Is all that talk just noise? the information content of internet stock message boards.</title>
<date>2004</date>
<journal>Journal of Finance,</journal>
<volume>59</volume>
<issue>3</issue>
<contexts>
<context position="2581" citStr="Antweiler and Frank, 2004" startWordPosition="378" endWordPosition="381">as “bullish” (having a positive outlook on the stock), “bearish” (having a negative outlook on the stock), or neutral. 2. Make trading decisions based on these message classifications. Previous work on stock investment forums and microblogs usually regarded the first step (message classification) as a sentiment analysis problem, and aligned bullish with positive sentiment and bearish with negative sentiment. Messages were classified by matching positive and negative terms from sentiment lexicons, learning from a hand-labeled set of messages, or some combination of the two (Das and Chen, 2007; Antweiler and Frank, 2004; Chua et al., 2009; Zhang and Skiena, 2010; Sprenger and Welpe, 2010). Trading decisions were made by aggregating the sentiment for a given stock over all the tweets, and picking stocks with strongest sentiment signal (buying the most bullish stocks and short-selling the most bearish ones). Sentiment aggregation reflects the opinion of the investors community as a whole, but overlooks the variability in user expertise. Clearly, not all investors are born equal, and if we could tell experts from nonexperts, we would reduce the noise in these forums and obtain high-quality signals to follow. Th</context>
<context position="31585" citStr="Antweiler and Frank, 2004" startWordPosition="5360" endWordPosition="5363">rs. Some of these works focus on document-level sentiment classification (Devitt and Ahmad, 2007; O’Hare et al., 2009). Other works also aimed at predicting stock movement (Lavrenko et al., 2000; Koppel and Shtrimberg, 2004; Schumaker and Chen, 2010). All these methods rely on predefined sentiment lexicons, manually classified training texts, or their combination. Lavrenko et al. (2000), Koppel and Shtrimberg (2004), and Schumaker and Chen (2010) exploit stock prices for training, and thus save the need in supervised learning. Previous work on stock message boards include (Das and Chen, 2007; Antweiler and Frank, 2004; Chua et al., 2009). (Sprenger and Welpe, 2010) is, to the best of our knowledge, the first work to address specifically stock microblogs. All these works take a similar approach for classifying message bullishness: they train a classifier (Naive Bayes, which Das and Chen combined with additional classifiers and a sentiment lexicon, and Chua et al. presented improvement for) on a collection of manually labeled messages (classified into Buy, Sell, Hold). Interestingly, Chua et al. made use of an Australian mes1318 sage board (HotCopper), where, unlike most of the stock message boards, these la</context>
</contexts>
<marker>Antweiler, Frank, 2004</marker>
<rawString>Werner Antweiler and Murray Z. Frank. 2004. Is all that talk just noise? the information content of internet stock message boards. Journal of Finance, 59(3):1259–1294.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bollen</author>
<author>Huina Mao</author>
<author>Xiaojun Zeng</author>
</authors>
<title>Twitter mood predicts the stock market.</title>
<date>2011</date>
<journal>Journal of Computational Science,</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="30609" citStr="Bollen et al. (2011)" startWordPosition="5210" endWordPosition="5213"> the table the results significantly outperform the baseline (as indicated by the p value). Overall, these results show the effectiveness of our methods for finding experts through unsupervised learning. 7 Related Work A growing body of work aims at extracting sentiment and opinions from tweets, and exploit this information in a variety of application domains. Davidov et al. (2010) propose utilizing twitter hashtag and smileys to learn enhanced sentiment types. O’Connor et al. (2010) propose a sentiment detector based on Twitter data that may be used as a replacement for public opinion polls. Bollen et al. (2011) measure six different dimensions of public mood from a very large tweet collection, and show that some of these dimensions improve the predication of changes in the Dow Jones Industrial Average (DJIA). Sentiment analysis of news articles and financial blogs and their application for stock prediction were the subject of several studies in recent years. Some of these works focus on document-level sentiment classification (Devitt and Ahmad, 2007; O’Hare et al., 2009). Other works also aimed at predicting stock movement (Lavrenko et al., 2000; Koppel and Shtrimberg, 2004; Schumaker and Chen, 2010</context>
</contexts>
<marker>Bollen, Mao, Zeng, 2011</marker>
<rawString>Johan Bollen, Huina Mao, and Xiaojun Zeng. 2011. Twitter mood predicts the stock market. Journal of Computational Science, 2(1):1 – 8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Chua</author>
<author>Maria Milosavljevic</author>
<author>James R Curran</author>
</authors>
<title>A sentiment detection engine for internet stock message boards.</title>
<date>2009</date>
<booktitle>In Proceedings of the Australasian Language Technology Association Workshop</booktitle>
<contexts>
<context position="2600" citStr="Chua et al., 2009" startWordPosition="382" endWordPosition="385">tive outlook on the stock), “bearish” (having a negative outlook on the stock), or neutral. 2. Make trading decisions based on these message classifications. Previous work on stock investment forums and microblogs usually regarded the first step (message classification) as a sentiment analysis problem, and aligned bullish with positive sentiment and bearish with negative sentiment. Messages were classified by matching positive and negative terms from sentiment lexicons, learning from a hand-labeled set of messages, or some combination of the two (Das and Chen, 2007; Antweiler and Frank, 2004; Chua et al., 2009; Zhang and Skiena, 2010; Sprenger and Welpe, 2010). Trading decisions were made by aggregating the sentiment for a given stock over all the tweets, and picking stocks with strongest sentiment signal (buying the most bullish stocks and short-selling the most bearish ones). Sentiment aggregation reflects the opinion of the investors community as a whole, but overlooks the variability in user expertise. Clearly, not all investors are born equal, and if we could tell experts from nonexperts, we would reduce the noise in these forums and obtain high-quality signals to follow. This paper presents a</context>
<context position="31605" citStr="Chua et al., 2009" startWordPosition="5364" endWordPosition="5367">us on document-level sentiment classification (Devitt and Ahmad, 2007; O’Hare et al., 2009). Other works also aimed at predicting stock movement (Lavrenko et al., 2000; Koppel and Shtrimberg, 2004; Schumaker and Chen, 2010). All these methods rely on predefined sentiment lexicons, manually classified training texts, or their combination. Lavrenko et al. (2000), Koppel and Shtrimberg (2004), and Schumaker and Chen (2010) exploit stock prices for training, and thus save the need in supervised learning. Previous work on stock message boards include (Das and Chen, 2007; Antweiler and Frank, 2004; Chua et al., 2009). (Sprenger and Welpe, 2010) is, to the best of our knowledge, the first work to address specifically stock microblogs. All these works take a similar approach for classifying message bullishness: they train a classifier (Naive Bayes, which Das and Chen combined with additional classifiers and a sentiment lexicon, and Chua et al. presented improvement for) on a collection of manually labeled messages (classified into Buy, Sell, Hold). Interestingly, Chua et al. made use of an Australian mes1318 sage board (HotCopper), where, unlike most of the stock message boards, these labels are added by th</context>
</contexts>
<marker>Chua, Milosavljevic, Curran, 2009</marker>
<rawString>Christopher Chua, Maria Milosavljevic, and James R. Curran. 2009. A sentiment detection engine for internet stock message boards. In Proceedings of the Australasian Language Technology Association Workshop 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjiv R Das</author>
<author>Mike Y Chen</author>
</authors>
<title>Yahoo! for Amazon: Sentiment extraction from small talk on the Web.</title>
<date>2007</date>
<journal>Management Science,</journal>
<volume>53</volume>
<issue>9</issue>
<contexts>
<context position="2554" citStr="Das and Chen, 2007" startWordPosition="374" endWordPosition="377">assify each message as “bullish” (having a positive outlook on the stock), “bearish” (having a negative outlook on the stock), or neutral. 2. Make trading decisions based on these message classifications. Previous work on stock investment forums and microblogs usually regarded the first step (message classification) as a sentiment analysis problem, and aligned bullish with positive sentiment and bearish with negative sentiment. Messages were classified by matching positive and negative terms from sentiment lexicons, learning from a hand-labeled set of messages, or some combination of the two (Das and Chen, 2007; Antweiler and Frank, 2004; Chua et al., 2009; Zhang and Skiena, 2010; Sprenger and Welpe, 2010). Trading decisions were made by aggregating the sentiment for a given stock over all the tweets, and picking stocks with strongest sentiment signal (buying the most bullish stocks and short-selling the most bearish ones). Sentiment aggregation reflects the opinion of the investors community as a whole, but overlooks the variability in user expertise. Clearly, not all investors are born equal, and if we could tell experts from nonexperts, we would reduce the noise in these forums and obtain high-qu</context>
<context position="31558" citStr="Das and Chen, 2007" startWordPosition="5356" endWordPosition="5359">tudies in recent years. Some of these works focus on document-level sentiment classification (Devitt and Ahmad, 2007; O’Hare et al., 2009). Other works also aimed at predicting stock movement (Lavrenko et al., 2000; Koppel and Shtrimberg, 2004; Schumaker and Chen, 2010). All these methods rely on predefined sentiment lexicons, manually classified training texts, or their combination. Lavrenko et al. (2000), Koppel and Shtrimberg (2004), and Schumaker and Chen (2010) exploit stock prices for training, and thus save the need in supervised learning. Previous work on stock message boards include (Das and Chen, 2007; Antweiler and Frank, 2004; Chua et al., 2009). (Sprenger and Welpe, 2010) is, to the best of our knowledge, the first work to address specifically stock microblogs. All these works take a similar approach for classifying message bullishness: they train a classifier (Naive Bayes, which Das and Chen combined with additional classifiers and a sentiment lexicon, and Chua et al. presented improvement for) on a collection of manually labeled messages (classified into Buy, Sell, Hold). Interestingly, Chua et al. made use of an Australian mes1318 sage board (HotCopper), where, unlike most of the sto</context>
</contexts>
<marker>Das, Chen, 2007</marker>
<rawString>Sanjiv R. Das and Mike Y. Chen. 2007. Yahoo! for Amazon: Sentiment extraction from small talk on the Web. Management Science, 53(9):1375–1388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Enhanced sentiment learning using twitter hashtags and smileys.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="30373" citStr="Davidov et al. (2010)" startWordPosition="5168" endWordPosition="5172">er, as expected. Table 4 shows cumulative results for the first 20 users in the per-user model. The results show that this model achieves good precision for a relatively large number of tweets, and for most of the data points reported in the table the results significantly outperform the baseline (as indicated by the p value). Overall, these results show the effectiveness of our methods for finding experts through unsupervised learning. 7 Related Work A growing body of work aims at extracting sentiment and opinions from tweets, and exploit this information in a variety of application domains. Davidov et al. (2010) propose utilizing twitter hashtag and smileys to learn enhanced sentiment types. O’Connor et al. (2010) propose a sentiment detector based on Twitter data that may be used as a replacement for public opinion polls. Bollen et al. (2011) measure six different dimensions of public mood from a very large tweet collection, and show that some of these dimensions improve the predication of changes in the Dow Jones Industrial Average (DJIA). Sentiment analysis of news articles and financial blogs and their application for stock prediction were the subject of several studies in recent years. Some of t</context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Enhanced sentiment learning using twitter hashtags and smileys. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Devitt</author>
<author>Khurshid Ahmad</author>
</authors>
<title>Sentiment polarity identification in financial news: A cohesionbased approach.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>984--991</pages>
<contexts>
<context position="31056" citStr="Devitt and Ahmad, 2007" startWordPosition="5279" endWordPosition="5282">ced sentiment types. O’Connor et al. (2010) propose a sentiment detector based on Twitter data that may be used as a replacement for public opinion polls. Bollen et al. (2011) measure six different dimensions of public mood from a very large tweet collection, and show that some of these dimensions improve the predication of changes in the Dow Jones Industrial Average (DJIA). Sentiment analysis of news articles and financial blogs and their application for stock prediction were the subject of several studies in recent years. Some of these works focus on document-level sentiment classification (Devitt and Ahmad, 2007; O’Hare et al., 2009). Other works also aimed at predicting stock movement (Lavrenko et al., 2000; Koppel and Shtrimberg, 2004; Schumaker and Chen, 2010). All these methods rely on predefined sentiment lexicons, manually classified training texts, or their combination. Lavrenko et al. (2000), Koppel and Shtrimberg (2004), and Schumaker and Chen (2010) exploit stock prices for training, and thus save the need in supervised learning. Previous work on stock message boards include (Das and Chen, 2007; Antweiler and Frank, 2004; Chua et al., 2009). (Sprenger and Welpe, 2010) is, to the best of our</context>
</contexts>
<marker>Devitt, Ahmad, 2007</marker>
<rawString>Ann Devitt and Khurshid Ahmad. 2007. Sentiment polarity identification in financial news: A cohesionbased approach. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 984–991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods - Support Vector Learning. MIT-Press.</booktitle>
<editor>In B. Scholkopf, C. Burges, and A. Smola, editors,</editor>
<contexts>
<context position="20568" citStr="Joachims, 1999" startWordPosition="3412" endWordPosition="3413">ur policy is to buy in the opening price of the next trading day (PB), and sell on the opening price of the following trading day (PS). Tweets that are posted until 9:25 in the morning (market hours begin at 9:30) are associated with the same day, while those are posted after that time are associated with the next trading date. 5.2 Training Given the buy and sell prices associated with each tweet, we construct positive and negative training examples as follows: positive examples are tweets where PS−PB PB≥ 3%, and negative examples are tweets where PS−PB PB≤ −3%. We used the SVM-light package (Joachims, 1999), with the following features: • The existence of the following elements in the message text: – Reference to a ticker – Reference to a user – URL – Number – Hashtag – Question mark • The case-insensitive words in the message after dropping the above elements. • The 3, 4, 5 letter prefixes of each word. • The name of the user who authored the tweet, if it is a frequent user (at least 50 messages in the training data). Otherwise, the user name is taken to be “anonymous”. • Whether the stock price was up or down 1% or more in the previous trading day. • 2, 3, 4-word expressions which are typical </context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>T. Joachims. 1999. Making large-scale SVM learning practical. In B. Scholkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods - Support Vector Learning. MIT-Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moshe Koppel</author>
<author>Itai Shtrimberg</author>
</authors>
<title>Good news or bad news? Let the market decide.</title>
<date>2004</date>
<booktitle>In Proceedings of the AAAI Spring Symposium on Exploring Attitude and Affect in Text: Theories and Applications.</booktitle>
<contexts>
<context position="5267" citStr="Koppel and Shtrimberg, 2004" startWordPosition="796" endWordPosition="799">t. We therefore propose and compare two alternative approaches that sidestep the complexities of assessing tweets bullishness. These two approaches can be viewed as representing two extremes. The first approach restricts our attention to the most explicit signals of bullishness and bearishness, namely, tweets that report actual buy and sell transactions performed by the user. In the second approach we learn directly the relation between tweets content and stock prices, following previous work on predicting stock price movement from factual sources such as news articles (Lavrenko et al., 2000; Koppel and Shtrimberg, 2004; Schumaker and Chen, 2010). This approach poses no restrictions on the tweets content and avoids any stipulated tweet classification. However, user-generated messages are largely subjective, and their correlation with the stock prices depends on user’s expertise. This introduces much noise into the learning process. We show that by making the learning user-sensitive we can improve the results substantially. Overall, our work illustrates the feasibility of finding expert investors, and the utility of following them. 2 Stock Tweets 2.1 Stock Tweets Language Stock tweets, as Twitter messages in </context>
<context position="31183" citStr="Koppel and Shtrimberg, 2004" startWordPosition="5299" endWordPosition="5302">cement for public opinion polls. Bollen et al. (2011) measure six different dimensions of public mood from a very large tweet collection, and show that some of these dimensions improve the predication of changes in the Dow Jones Industrial Average (DJIA). Sentiment analysis of news articles and financial blogs and their application for stock prediction were the subject of several studies in recent years. Some of these works focus on document-level sentiment classification (Devitt and Ahmad, 2007; O’Hare et al., 2009). Other works also aimed at predicting stock movement (Lavrenko et al., 2000; Koppel and Shtrimberg, 2004; Schumaker and Chen, 2010). All these methods rely on predefined sentiment lexicons, manually classified training texts, or their combination. Lavrenko et al. (2000), Koppel and Shtrimberg (2004), and Schumaker and Chen (2010) exploit stock prices for training, and thus save the need in supervised learning. Previous work on stock message boards include (Das and Chen, 2007; Antweiler and Frank, 2004; Chua et al., 2009). (Sprenger and Welpe, 2010) is, to the best of our knowledge, the first work to address specifically stock microblogs. All these works take a similar approach for classifying me</context>
</contexts>
<marker>Koppel, Shtrimberg, 2004</marker>
<rawString>Moshe Koppel and Itai Shtrimberg. 2004. Good news or bad news? Let the market decide. In Proceedings of the AAAI Spring Symposium on Exploring Attitude and Affect in Text: Theories and Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Lavrenko</author>
<author>Matt Schmill</author>
<author>Dawn Lawrie</author>
<author>Paul Ogilvie</author>
<author>David Jensen</author>
<author>James Allan</author>
</authors>
<title>Mining of concurrent text and time series.</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th ACM SIGKDD Int’l Conference on Knowledge Discovery and Data Mining Workshop on Text Mining.</booktitle>
<contexts>
<context position="5238" citStr="Lavrenko et al., 2000" startWordPosition="792" endWordPosition="795">e than judging sentiment. We therefore propose and compare two alternative approaches that sidestep the complexities of assessing tweets bullishness. These two approaches can be viewed as representing two extremes. The first approach restricts our attention to the most explicit signals of bullishness and bearishness, namely, tweets that report actual buy and sell transactions performed by the user. In the second approach we learn directly the relation between tweets content and stock prices, following previous work on predicting stock price movement from factual sources such as news articles (Lavrenko et al., 2000; Koppel and Shtrimberg, 2004; Schumaker and Chen, 2010). This approach poses no restrictions on the tweets content and avoids any stipulated tweet classification. However, user-generated messages are largely subjective, and their correlation with the stock prices depends on user’s expertise. This introduces much noise into the learning process. We show that by making the learning user-sensitive we can improve the results substantially. Overall, our work illustrates the feasibility of finding expert investors, and the utility of following them. 2 Stock Tweets 2.1 Stock Tweets Language Stock tw</context>
<context position="31154" citStr="Lavrenko et al., 2000" startWordPosition="5295" endWordPosition="5298"> may be used as a replacement for public opinion polls. Bollen et al. (2011) measure six different dimensions of public mood from a very large tweet collection, and show that some of these dimensions improve the predication of changes in the Dow Jones Industrial Average (DJIA). Sentiment analysis of news articles and financial blogs and their application for stock prediction were the subject of several studies in recent years. Some of these works focus on document-level sentiment classification (Devitt and Ahmad, 2007; O’Hare et al., 2009). Other works also aimed at predicting stock movement (Lavrenko et al., 2000; Koppel and Shtrimberg, 2004; Schumaker and Chen, 2010). All these methods rely on predefined sentiment lexicons, manually classified training texts, or their combination. Lavrenko et al. (2000), Koppel and Shtrimberg (2004), and Schumaker and Chen (2010) exploit stock prices for training, and thus save the need in supervised learning. Previous work on stock message boards include (Das and Chen, 2007; Antweiler and Frank, 2004; Chua et al., 2009). (Sprenger and Welpe, 2010) is, to the best of our knowledge, the first work to address specifically stock microblogs. All these works take a simila</context>
</contexts>
<marker>Lavrenko, Schmill, Lawrie, Ogilvie, Jensen, Allan, 2000</marker>
<rawString>Victor Lavrenko, Matt Schmill, Dawn Lawrie, Paul Ogilvie, David Jensen, and James Allan. 2000. Mining of concurrent text and time series. In Proceedings of the 6th ACM SIGKDD Int’l Conference on Knowledge Discovery and Data Mining Workshop on Text Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O’Connor</author>
<author>Ramnath Balasubramanyan</author>
<author>Bryan R Routledge</author>
<author>Noah A Smith</author>
</authors>
<title>From tweets to polls: Linking text sentiment to public opinion time series.</title>
<date>2010</date>
<booktitle>In Proceedings of the International AAAI Conference on Weblogs and Social Media.</booktitle>
<marker>O’Connor, Balasubramanyan, Routledge, Smith, 2010</marker>
<rawString>Brendan O’Connor, Ramnath Balasubramanyan, Bryan R. Routledge, and Noah A. Smith. 2010. From tweets to polls: Linking text sentiment to public opinion time series. In Proceedings of the International AAAI Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Neil O’Hare</author>
<author>Michael Davy</author>
<author>Adam Bermingham</author>
<author>Paul Ferguson</author>
<author>Pvraic Sheridan</author>
<author>Cathal Gurrin</author>
<author>Alan F Smeaton</author>
</authors>
<title>Topic-dependent sentiment analysis of financial blogs.</title>
<date>2009</date>
<booktitle>In TSA’09 - 1st International CIKM Workshop on Topic-Sentiment Analysis for Mass Opinion Measurement.</booktitle>
<marker>O’Hare, Davy, Bermingham, Ferguson, Sheridan, Gurrin, Smeaton, 2009</marker>
<rawString>Neil O’Hare, Michael Davy, Adam Bermingham, Paul Ferguson, Pvraic Sheridan, Cathal Gurrin, and Alan F Smeaton. 2009. Topic-dependent sentiment analysis of financial blogs. In TSA’09 - 1st International CIKM Workshop on Topic-Sentiment Analysis for Mass Opinion Measurement.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert P Schumaker</author>
<author>Hsinchun Chen</author>
</authors>
<title>A discrete stock price prediction engine based on financial news.</title>
<date>2010</date>
<journal>Computer,</journal>
<pages>43--51</pages>
<contexts>
<context position="5294" citStr="Schumaker and Chen, 2010" startWordPosition="800" endWordPosition="803">ompare two alternative approaches that sidestep the complexities of assessing tweets bullishness. These two approaches can be viewed as representing two extremes. The first approach restricts our attention to the most explicit signals of bullishness and bearishness, namely, tweets that report actual buy and sell transactions performed by the user. In the second approach we learn directly the relation between tweets content and stock prices, following previous work on predicting stock price movement from factual sources such as news articles (Lavrenko et al., 2000; Koppel and Shtrimberg, 2004; Schumaker and Chen, 2010). This approach poses no restrictions on the tweets content and avoids any stipulated tweet classification. However, user-generated messages are largely subjective, and their correlation with the stock prices depends on user’s expertise. This introduces much noise into the learning process. We show that by making the learning user-sensitive we can improve the results substantially. Overall, our work illustrates the feasibility of finding expert investors, and the utility of following them. 2 Stock Tweets 2.1 Stock Tweets Language Stock tweets, as Twitter messages in general, are short textual </context>
<context position="31210" citStr="Schumaker and Chen, 2010" startWordPosition="5303" endWordPosition="5306">ls. Bollen et al. (2011) measure six different dimensions of public mood from a very large tweet collection, and show that some of these dimensions improve the predication of changes in the Dow Jones Industrial Average (DJIA). Sentiment analysis of news articles and financial blogs and their application for stock prediction were the subject of several studies in recent years. Some of these works focus on document-level sentiment classification (Devitt and Ahmad, 2007; O’Hare et al., 2009). Other works also aimed at predicting stock movement (Lavrenko et al., 2000; Koppel and Shtrimberg, 2004; Schumaker and Chen, 2010). All these methods rely on predefined sentiment lexicons, manually classified training texts, or their combination. Lavrenko et al. (2000), Koppel and Shtrimberg (2004), and Schumaker and Chen (2010) exploit stock prices for training, and thus save the need in supervised learning. Previous work on stock message boards include (Das and Chen, 2007; Antweiler and Frank, 2004; Chua et al., 2009). (Sprenger and Welpe, 2010) is, to the best of our knowledge, the first work to address specifically stock microblogs. All these works take a similar approach for classifying message bullishness: they tra</context>
</contexts>
<marker>Schumaker, Chen, 2010</marker>
<rawString>Robert P. Schumaker and Hsinchun Chen. 2010. A discrete stock price prediction engine based on financial news. Computer, 43:51–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timm O Sprenger</author>
<author>Isabell M Welpe</author>
</authors>
<title>Tweets and trades: The information content of stock microblogs.</title>
<date>2010</date>
<tech>Technical report, TUM</tech>
<institution>School of Management,</institution>
<note>working paper.</note>
<contexts>
<context position="2651" citStr="Sprenger and Welpe, 2010" startWordPosition="390" endWordPosition="393">ng a negative outlook on the stock), or neutral. 2. Make trading decisions based on these message classifications. Previous work on stock investment forums and microblogs usually regarded the first step (message classification) as a sentiment analysis problem, and aligned bullish with positive sentiment and bearish with negative sentiment. Messages were classified by matching positive and negative terms from sentiment lexicons, learning from a hand-labeled set of messages, or some combination of the two (Das and Chen, 2007; Antweiler and Frank, 2004; Chua et al., 2009; Zhang and Skiena, 2010; Sprenger and Welpe, 2010). Trading decisions were made by aggregating the sentiment for a given stock over all the tweets, and picking stocks with strongest sentiment signal (buying the most bullish stocks and short-selling the most bearish ones). Sentiment aggregation reflects the opinion of the investors community as a whole, but overlooks the variability in user expertise. Clearly, not all investors are born equal, and if we could tell experts from nonexperts, we would reduce the noise in these forums and obtain high-quality signals to follow. This paper presents a framework for identifying experts in stock microbl</context>
<context position="13350" citStr="Sprenger and Welpe, 2010" startWordPosition="2126" endWordPosition="2129">t is also given a classifier C, which classifies each tweet as bullish or not bullish (either bearish or neutral). The procedure first applies the classifier C to identify the bullish tweets in T . It then determines the correctness of each bullish tweet. Given a tweet t, we observe the price change of the stock referenced by t over a one day period starting at the next trading day. The exact definition of mapping tweets to stock prices is given in section 5.1. A one-day holding period was chosen as it was found to perform well in previous works on tweet-based trading (Zhang and Skiena, 2010; Sprenger and Welpe, 2010), in particular for long positions (buy transactions). A bullish tweet is considered correct if it is followed by a stock rise, and as incorrect otherwise1. Given a set of tweets, we define its precision as the percentage of correct tweets in the set. Let Cu, Iu denote the number of correct and incorrect bullish tweets of user u, respectively. The precision of u’s bullish tweets is therefore: Cu Pu = Cu + Iu Let Pbl be the baseline precision. In this work we chose the baseline precision to be the proportion of tweets that are followed by a stock rise in the whole training set (including all th</context>
<context position="31633" citStr="Sprenger and Welpe, 2010" startWordPosition="5368" endWordPosition="5371">sentiment classification (Devitt and Ahmad, 2007; O’Hare et al., 2009). Other works also aimed at predicting stock movement (Lavrenko et al., 2000; Koppel and Shtrimberg, 2004; Schumaker and Chen, 2010). All these methods rely on predefined sentiment lexicons, manually classified training texts, or their combination. Lavrenko et al. (2000), Koppel and Shtrimberg (2004), and Schumaker and Chen (2010) exploit stock prices for training, and thus save the need in supervised learning. Previous work on stock message boards include (Das and Chen, 2007; Antweiler and Frank, 2004; Chua et al., 2009). (Sprenger and Welpe, 2010) is, to the best of our knowledge, the first work to address specifically stock microblogs. All these works take a similar approach for classifying message bullishness: they train a classifier (Naive Bayes, which Das and Chen combined with additional classifiers and a sentiment lexicon, and Chua et al. presented improvement for) on a collection of manually labeled messages (classified into Buy, Sell, Hold). Interestingly, Chua et al. made use of an Australian mes1318 sage board (HotCopper), where, unlike most of the stock message boards, these labels are added by the message author. Another re</context>
</contexts>
<marker>Sprenger, Welpe, 2010</marker>
<rawString>Timm O. Sprenger and Isabell M. Welpe. 2010. Tweets and trades: The information content of stock microblogs. Technical report, TUM School of Management, December. working paper.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Zhang</author>
<author>Steven Skiena</author>
</authors>
<title>Trading strategies to exploit blog and news sentiment.</title>
<date>2010</date>
<booktitle>In ICWSM’10.</booktitle>
<contexts>
<context position="2624" citStr="Zhang and Skiena, 2010" startWordPosition="386" endWordPosition="389"> stock), “bearish” (having a negative outlook on the stock), or neutral. 2. Make trading decisions based on these message classifications. Previous work on stock investment forums and microblogs usually regarded the first step (message classification) as a sentiment analysis problem, and aligned bullish with positive sentiment and bearish with negative sentiment. Messages were classified by matching positive and negative terms from sentiment lexicons, learning from a hand-labeled set of messages, or some combination of the two (Das and Chen, 2007; Antweiler and Frank, 2004; Chua et al., 2009; Zhang and Skiena, 2010; Sprenger and Welpe, 2010). Trading decisions were made by aggregating the sentiment for a given stock over all the tweets, and picking stocks with strongest sentiment signal (buying the most bullish stocks and short-selling the most bearish ones). Sentiment aggregation reflects the opinion of the investors community as a whole, but overlooks the variability in user expertise. Clearly, not all investors are born equal, and if we could tell experts from nonexperts, we would reduce the noise in these forums and obtain high-quality signals to follow. This paper presents a framework for identifyi</context>
<context position="13323" citStr="Zhang and Skiena, 2010" startWordPosition="2122" endWordPosition="2125">with its posting time. It is also given a classifier C, which classifies each tweet as bullish or not bullish (either bearish or neutral). The procedure first applies the classifier C to identify the bullish tweets in T . It then determines the correctness of each bullish tweet. Given a tweet t, we observe the price change of the stock referenced by t over a one day period starting at the next trading day. The exact definition of mapping tweets to stock prices is given in section 5.1. A one-day holding period was chosen as it was found to perform well in previous works on tweet-based trading (Zhang and Skiena, 2010; Sprenger and Welpe, 2010), in particular for long positions (buy transactions). A bullish tweet is considered correct if it is followed by a stock rise, and as incorrect otherwise1. Given a set of tweets, we define its precision as the percentage of correct tweets in the set. Let Cu, Iu denote the number of correct and incorrect bullish tweets of user u, respectively. The precision of u’s bullish tweets is therefore: Cu Pu = Cu + Iu Let Pbl be the baseline precision. In this work we chose the baseline precision to be the proportion of tweets that are followed by a stock rise in the whole tra</context>
<context position="32271" citStr="Zhang and Skiena, 2010" startWordPosition="5472" endWordPosition="5475">t of our knowledge, the first work to address specifically stock microblogs. All these works take a similar approach for classifying message bullishness: they train a classifier (Naive Bayes, which Das and Chen combined with additional classifiers and a sentiment lexicon, and Chua et al. presented improvement for) on a collection of manually labeled messages (classified into Buy, Sell, Hold). Interestingly, Chua et al. made use of an Australian mes1318 sage board (HotCopper), where, unlike most of the stock message boards, these labels are added by the message author. Another related work is (Zhang and Skiena, 2010), who apply lexicon-based sentiment analysis to several sources of news and blogs, including tweets. However, their data set does not include stock microblogs, but tweets mentioning the official company name. Our work differs from previous work on stock messages in two vital aspects. Firstly, these works did not attempt to distinguish between experts and non-expert users, but aggregated the sentiment over all the users when studying the relation between sentiment and the stock market. Secondly, unlike these works, our best-performing methods are completely unsupervised, and require no manually</context>
</contexts>
<marker>Zhang, Skiena, 2010</marker>
<rawString>Wenbin Zhang and Steven Skiena. 2010. Trading strategies to exploit blog and news sentiment. In ICWSM’10.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>