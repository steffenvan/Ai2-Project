<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.026354">
<title confidence="0.99632">
Parser Evaluation over
Local and Non-Local Deep Dependencies in a Large Corpus
</title>
<author confidence="0.999853">
Emily M. Bender♠, Dan Flickinger♥, Stephan Oepen♣, Yi Zhang♦♠Dept of Linguistics, University of Washington, ♥CSLI, Stanford University
</author>
<affiliation confidence="0.999531">
♣Dept of Informatics, Universitetet i Oslo, ♦Dept of Computational Linguistics, Saarland University
</affiliation>
<email confidence="0.987968">
ebender@uw.edu, danf@stanford.edu, oe@ifi.uio.no, yzhang@coli.uni-sb.de
</email>
<sectionHeader confidence="0.993819" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999819916666667">
In order to obtain a fine-grained evaluation of
parser accuracy over naturally occurring text,
we study 100 examples each of ten reason-
ably frequent linguistic phenomena, randomly
selected from a parsed version of the En-
glish Wikipedia. We construct a correspond-
ing set of gold-standard target dependencies
for these 1000 sentences, operationalize map-
pings to these targets from seven state-of-the-
art parsers, and evaluate the parsers against
this data to measure their level of success in
identifying these dependencies.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999970081632653">
The terms “deep” and “shallow” are frequently used
to characterize or contrast different approaches to
parsing. Inevitably, such informal notions lack a
clear definition, and there is little evidence of com-
munity consensus on the relevant dimension(s) of
depth, let alone agreement on applicable metrics. At
its core, the implied dichotomy of approaches al-
ludes to differences in the interpretation of the pars-
ing task. Its abstract goal, on the one hand, could
be pre-processing of the linguistic signal, to enable
subsequent stages of analysis. On the other hand, it
could be making explicit the (complete) contribution
that the grammatical form of the linguistic signal
makes to interpretation, working out who did what
to whom. Stereotypically, one expects correspond-
ing differences in the choice of interface representa-
tions, ranging from various levels of syntactic anal-
ysis to logical-form representations of semantics.
In this paper, we seek to probe aspects of variation
in automated linguistic analysis. We make the as-
sumption that an integral part of many (albeit not all)
applications of parsing technology is the recovery of
structural relations, i.e. dependencies at the level of
interpretation. We suggest a selection of ten linguis-
tic phenomena that we believe (a) occur with reason-
ably high frequency in running text and (b) have the
potential to shed some light on the depths of linguis-
tic analysis. We quantify the frequency of these con-
structions in the English Wikipedia, then annotate
100 example sentences for each phenomenon with
gold-standard dependencies reflecting core proper-
ties of the phenomena of interest. This gold standard
is then used to estimate the recall of these dependen-
cies by seven commonly used parsers, providing the
basis for a qualitative discussion of the state of the
art in parsing for English.
In this work, we answer the call by Rimell et
al. (2009) for “construction-focused parser evalua-
tion”, extending and complementing their work in
several respects: (i) we investigate both local and
non-local dependencies which prove to be challeng-
ing for many existing state-of-the-art parsers; (ii) we
investigate a wider range of linguistic phenomena,
each accompanied with an in-depth discussion of
relevant properties; and (iii) we draw our data from
the 50-million sentence English Wikipedia, which
is more varied and a thousand times larger than the
venerable WSJ corpus, to explore a more level and
ambitious playing field for parser comparison.
</bodyText>
<sectionHeader confidence="0.989175" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.9998458">
All parsing systems embody knowledge about possi-
ble and probable pairings of strings and correspond-
ing linguistic structure. Such linguistic and proba-
bilistic knowledge can be hand-coded (e.g., as gram-
mar rules) or automatically acquired from labeled or
</bodyText>
<page confidence="0.970426">
397
</page>
<note confidence="0.957915">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 397–408,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<bodyText confidence="0.9996948">
unlabeled training data. A related dimension of vari-
ation is the type of representations manipulated by
the parser. We briefly review some representative
examples along these dimensions, as these help to
position the parsers we subsequently evaluate.1
</bodyText>
<subsectionHeader confidence="0.981115">
2.1 Approaches to parsing
</subsectionHeader>
<bodyText confidence="0.999907742857143">
Source of linguistic knowledge At one end of this
dimension, we find systems whose linguistic knowl-
edge is encoded in hand-crafted rules and lexical en-
tries; for English, the ParGram XLE system (Rie-
zler et al., 2002) and DELPH-IN English Resource
Grammar (ERG; Flickinger (2000))—each reflect-
ing decades of continuous development—achieve
broad coverage of open-domain running text, for ex-
ample. At the other end of this dimension, we find
fully unsupervised approaches (Clark, 2001; Klein
and Manning, 2004), where the primary source of
linguistic knowledge is co-occurrence patterns of
words in unannotated text. As Haghighi and Klein
(2006) show, augmenting this knowledge with hand-
crafted prototype “seeds” can bring strong improve-
ments. Somewhere between these poles, a broad
class of parsers take some or all of their linguistic
knowledge from annotated treebanks, e.g. the Penn
Treebank (PTB), which encodes “surface grammati-
cal analysis” (Marcus et al., 1993). Such approaches
include those that directly (and exclusively) use the
information in the treebank (e.g. Charniak (1997),
Collins (1999), Petrov et al. (2006), inter alios) as
well as those that complement treebank structures
with some amount of hand-coded linguistic knowl-
edge (e.g. O’Donovan et al. (2004), Miyao et al.
(2004), Hockenmaier and Steedman (2007), inter
alios). Another hybrid in terms of its acquisition of
linguistic knowledge is the RASP system of Briscoe
et al. (2006), combining a hand-coded grammar over
PoS tag sequences with a probabilistic tagger and
statistical syntactic disambiguation.
Design of representations Approaches to parsing
also differ fundamentally in the style of represen-
tation assigned to strings. These vary both in their
</bodyText>
<footnote confidence="0.985672">
1Additional sources of variation among extant parsing tech-
nologies include (a) the behavior with respect to ungrammatical
inputs and (b) the relationship between probabilistic and sym-
bolic knowledge in the parser, where parsers with a hand-coded
grammar at their core typically also incorporate an automati-
cally trained probabilistic disambiguation component.
</footnote>
<bodyText confidence="0.999972861111111">
formal nature and the “granularity” of linguistic in-
formation (i.e. the number of distinctions assumed),
encompassing variants of constituent structure, syn-
tactic dependencies, or logical-form representations
of semantics. Parser interface representations range
between the relatively simple (e.g. phrase structure
trees with a limited vocabulary of node labels as in
the PTB, or syntactic dependency structures with a
limited vocabulary of relation labels as in Johansson
and Nugues (2007)) and the relatively complex, as
for example elaborate syntactico-semantic analyses
produced by the ParGram or DELPH-IN grammars.
There tends to be a correlation between the
methodology used in the acquisition of linguistic
knowledge and the complexity of representations: in
the creation of a mostly hand-crafted treebank like
the PTB, representations have to be simple enough
for human annotators to reliably manipulate. Deriv-
ing more complex representations typically presup-
poses further computational support, often involv-
ing some hand-crafted linguistic knowledge—which
can take the form of mappings from PTB-like repre-
sentations to “richer” grammatical frameworks (as
in the line of work by O’Donovan et al. (2004), and
others; see above), or can be rules for creating the
parse structures in the first place (i.e. a computa-
tional grammar), as for example in the treebanks of
van der Beek et al. (2002) or Oepen et al. (2004).2
In principle, one might expect that richer repre-
sentations allow parsers to capture complex syntac-
tic or semantic dependencies more explicitly. At the
same time, such “deeper” relations may still be re-
coverable (to some degree) from comparatively sim-
ple parser outputs, as demonstrated for unbounded
dependency extraction from strictly local syntactic
dependency trees by Nivre et al. (2010).
</bodyText>
<subsectionHeader confidence="0.999694">
2.2 An armada of parsers
</subsectionHeader>
<bodyText confidence="0.99717425">
Stanford Parser (Klein and Manning, 2003) is a
probabilistic parser which can produce both phrase
structure trees and grammatical relations (syntactic
dependencies). The parsing model we evaluate is the
</bodyText>
<footnote confidence="0.8801145">
2A noteworthy exception to this correlation is the annotated
corpus of Zettlemoyer and Collins (2005), which pairs sur-
face strings from the realm of natural language database inter-
faces directly with semantic representations in lambda calculus.
These were hand-written on the basis of database query state-
ments distributed with the original datasets.
</footnote>
<page confidence="0.993567">
398
</page>
<bodyText confidence="0.989062901960785">
English factored model which combines the prefer- or syntactic disambiguation components.
ences of unlexicalized PCFG phrase structures and MSTParser (McDonald et al., 2005) is a data-
of lexical dependencies, trained on sections 02–21 driven dependency parser. The parser uses an edge-
of the WSJ portion of the PTB. We chose Stanford factored model and searches for a maximal span-
Parser from among the state-of-the-art PTB-derived ning tree that connects all the words in a sentence
parsers for its support for grammatical relations as into a dependency tree. The model we evaluate
an alternate interface representation. is the second-order projective model trained on the
Charniak&amp;Johnson Reranking Parser (Char- same WSJ corpus, where the original PTB phrase
niak and Johnson, 2005) is a two-stage PCFG parser structure annotations were first converted into de-
with a lexicalized generative model for the first- pendencies, as established in the CoNLL shared task
stage, and a discriminative MaxEnt reranker for the 2009 (Johansson and Nugues, 2007).
second-stage. The models we evaluate are also XLE/ParGram (Riezler et al., 2002, see also
trained on sections 02–21 of the WSJ. Top-50 read- Cahill et al., 2008) applies a hand-built Lexical
ings were used for the reranking stage. The output Functional Grammar for English and a stochastic
constituent trees were then converted into Stanford parse selection model. For our evaluation, we used
Dependencies. According to Cer et al. (2010), this the Nov 4, 2010 release of XLE and the Nov 25,
combination gives the best parsing accuracy in terms 2009 release of the ParGram English grammar, with
of Stanford dependencies on the PTB. c-structure pruning turned off and resource limita-
Enju (Miyao et al., 2004) is a probabilistic HPSG tions set to the maximum possible to allow for ex-
parser, combining a hand-crafted core grammar with haustive search. In particular, we are evaluating the
automatically acquired lexical types from the PTB.3 f-structures output by the system.
The model we evaluate is trained on the same ma- Each parser, of course, has its own requirements
terial from the WSJ sections of the PTB, but the regarding preprocessing of text, especially tokeniza-
treebank is first semi-automatically converted into tion. We customized the tokenization to each parser,
HPSG derivations, and the annotation is enriched by using the parser’s own internal tokenization or
with typed feature structures for each constituent. pre-tokenizing to match the parser’s desired input.
In addition to HPSG derivation trees, Enju also pro- The evaluation script is robust to variations in tok-
duces predicate argument structures. enization across parsers.
C&amp;C (Clark and Curran, 2007) is a statistical 3 Phenomena
CCG parser. Abstractly similar to the approach of In this section we summarize the ten phenomena we
Enju, the grammar and lexicon are automatically explore and our motivations for choosing them. Our
induced from CCGBank (Hockenmaier and Steed- goal was to find phenomena where the relevant de-
man, 2007), a largely automatic projection of (the pendencies are relatively subtle, such that more lin-
WSJ portion of) PTB trees into the CCG framework. guistic knowledge is beneficial in order to retrieve
In addition to CCG derivations, the C&amp;C parser can them. Though this set is of course only a sampling,
directly output a variant of grammatical relations. these phenomena illustrate the richness of structure,
RASP (Briscoe et al., 2006) is an unlexicalized both local and non-local, involved in the mapping
robust parsing system, with a hand-crafted “tag se- from English strings to their meanings. We discuss
quence” grammar at its core. The parser thus anal- the phenomena in four sets and then briefly review
yses a lattice of PoS tags, building a parse forest their representation in the Penn Treebank.
from which the most probable syntactic trees and 3.1 Long distance dependencies
sets of corresponding grammatical relations can be Three of our phenomena can be classified as involv-
extracted. Unlike other parsers in our mix, RASP ing long-distance dependencies: finite that-less rel-
did not build on PTB data in either its PoS tagging atives clauses (‘barerel’), tough adjectives (‘tough’)
and right node raising (‘rnr’). These are illustrated
3This hand-crafted grammar is distinct from the ERG, de-
spite sharing the general framework of HPSG. The ERG is not
included in our evaluation, since it was used in the extraction of
the original examples and thus cannot be fairly evaluated.
399
in the following examples:4
</bodyText>
<listItem confidence="0.9974724">
(1) barerel: This is the second time in a row Aus-
tralia has lost their home tri-nations’ series.
(2) tough: Original copies are very hard to find.
(3) rnr: Il´uvatar, as his names imply, exists before
and independently of all else.
</listItem>
<bodyText confidence="0.999878909090909">
While the majority of our phenomena involve lo-
cal dependencies, we include these long-distance
dependency types because they are challenging for
parsers and enable more direct comparison with the
work of Rimell et al. (2009), who also address right
node raising and bare relatives. Our barerel category
corresponds to their “object reduced relative” cate-
gory with the difference that we also include adverb
relatives, where the head noun functions as a modi-
fier within the relative clause, as does time in (1). In
contrast, our rnr category is somewhat narrower than
Rimell et al. (2009)’s “right node raising” category:
where they include raised modifiers, we restrict our
category to raised complements.
Part of the difficulty in retrieving long-distance
dependencies is that the so-called extraction site is
not overtly marked in the string. In addition to this
baseline level of complication, these three construc-
tion types present further difficulties: Bare relatives,
unlike other relative clauses, do not carry any lexi-
cal cues to their presence (i.e., no relative pronouns).
Tough adjective constructions require the presence
of specific lexical items which form a subset of a
larger open class. They are rendered more difficult
by two sources of ambiguity: alternative subcatego-
rization frames for the adjectives and the purposive
adjunct analysis (akin to in order to) for the infiniti-
val VP. Finally, right node raising often involves co-
ordination where one of the conjuncts is in fact not
a well-formed phrase (e.g., independently of in (3)),
making it potentially difficult to construct the correct
coordination structure, let alone associate the raised
element with the correct position in each conjunct.
</bodyText>
<subsectionHeader confidence="0.99899">
3.2 Non-dependencies
</subsectionHeader>
<bodyText confidence="0.741471">
Two of our phenomena crucially look for the lack of
dependencies. These are it expletives (‘itexpl’) and
verb-particle constructions (‘vpart’):
4All examples are from our data. Words involved in the rel-
evant dependencies are highlighted in italics (dependents) and
boldface (heads).
</bodyText>
<listItem confidence="0.99577425">
(4) itexpl: Crew negligence is blamed, and it is sug-
gested that the flight crew were drunk.
(5) vpart: He once threw out two baserunners at
home in the same inning.
</listItem>
<bodyText confidence="0.99993508">
The English pronoun it can be used as an ordi-
nary personal pronoun or as an expletive: a place-
holder for when the language demands a subject (or
occasionally object) NP but there is no semantic role
for that NP. The expletive it only appears when it
is licensed by a specific construction (such as ex-
traposition, (4)) or selecting head. If the goal of
parsing is to recover from the surface string the de-
pendencies capturing who did what to whom, exple-
tive it should not feature in any of those dependen-
cies. Likewise, instances of expletive it should be
detected and discarded in reference resolution. We
hypothesize that detecting expletive it requires en-
coding linguistic knowledge about its licensers.
The other non-dependency we explore is between
the particle in verb-particle constructions and the
direct object. Since English particles are almost
always homophonous with prepositions, when the
object of the verb-particle pair follows the par-
ticle, there will always be a competing analysis
which analyses the sequence as V+PP rather than
V+particle+NP. Furthermore, since verb-particle
pairs often have non-compositional semantics (Sag
et al., 2002), misanalyzing these constructions could
be costly to downstream components.
</bodyText>
<subsectionHeader confidence="0.998238">
3.3 Phrasal modifiers
</subsectionHeader>
<bodyText confidence="0.997401">
Our next category concerns modifier phrases:
</bodyText>
<listItem confidence="0.9940845">
(6) ned: Light colored glazes also have softening
effects when painted over dark or bright images.
(7) absol: The format consisted of 12 games, each
team facing the other teams twice.
</listItem>
<bodyText confidence="0.999913363636364">
The first, (‘ned’), is a pattern which to our knowl-
edge has not been named in the literature, where a
noun takes the typically verbal -ed ending, is modi-
fied by another noun or adjective, and functions as a
modifier or a predicate. We believe this phenomenon
to be interesting because its unusual morphology is
likely to lead PoS-taggers astray, and because the
often-hyphenated Adj+N-ed constituent has produc-
tive internal structure constraining its interpretation.
The second phrasal modifier we investigate is the
absolutive construction. An absolutive consists of an
</bodyText>
<page confidence="0.980088">
400
</page>
<bodyText confidence="0.999970333333333">
NP followed by a non-finite predicate (such as could
appear after the copula be). The whole phrase mod-
ifies a verbal projection that it attaches to. Absolu-
tives may be marked with with or unmarked. Here,
we focus on the unmarked type as this lack of lexical
cue can make the construction harder to detect.
</bodyText>
<subsectionHeader confidence="0.998171">
3.4 Subtle arguments
</subsectionHeader>
<bodyText confidence="0.994558666666667">
Our final three phenomena involve ways in which
verbal arguments can be more difficult to identify
than in ordinary finite clauses. These include de-
tecting the arguments of verbal gerunds (‘vger’), the
interleaving of arguments and adjuncts (‘argadj’) and
raising/control (‘control’) constructions.
</bodyText>
<listItem confidence="0.980523857142857">
(8) vger: Accessing the website without the “www”
subdomain returned a copy of the main site for
“EP.net”.
(9) argadj: The story shows, through flashbacks, the
different histories of the characters.
(10) control: Alfred “retired” in 1957 at age 60 but
continued to paint full time.
</listItem>
<bodyText confidence="0.999980828571428">
In a verbal gerund, the -ing form a verb retains
verbal properties (e.g., being able to take NP com-
plements, rather than only PP complements) but
heads a phrase that fills an NP position in the syn-
tax (Malouf, 2000). Since gerunds have the same
morphology as present participle VPs, their role in
the larger clause is susceptible to misanalysis.
The argadj examples are of interest because En-
glish typically prefers to have direct objects directly
adjacent to the selecting verb. Nonetheless, phe-
nomena such as parentheticals and heavy-NP shift
(Arnold et al., 2000), in which “heavy” constituents
appear further to the right in the string, allow for
adjunct-argument order in a minority of cases. We
hypothesize that the relative infrequency of this con-
struction will lead parsers to prefer incorrect analy-
ses (wherein the adjunct is picked up as a comple-
ment, the complement as an adjunct or the structure
differs entirely) unless they have access to linguis-
tic knowledge providing constraints on possible and
probable complementation patterns for the head.
Finally, we turn to raising and control verbs (‘con-
trol’) (e.g., Huddleston and Pullum (2002, ch. 14)).
These verbs select for an infinitival VP complement
and stipulate that another of their arguments (sub-
ject or direct object in the examples we explore) is
identified with the unrealized subject position of the
infinitival VP. Here it is the dependency between
the infinitval VP and the NP argument of the “up-
stairs” verb which we expect to be particularly sub-
tle. Getting this right requires specific lexical knowl-
edge about which verbs take these complementation
patterns. This lexical knowledge needs to be repre-
sented in such a way that it can be used robustly even
in the case of passives, relative clauses, etc.5
</bodyText>
<subsectionHeader confidence="0.971143">
3.5 Penn Treebank representations
</subsectionHeader>
<bodyText confidence="0.999979121212121">
We investigated the representation of these 10 phe-
nomena in the PTB (Marcus et al., 1993) in two
steps: First we explored the PTB’s annotation guide-
lines (Bies et al., 1995) to determine how the rele-
vant dependencies were intended to be represented.
We then used Ghodke and Bird’s (2010) Treebank
Search to find examples of the intended annotations
as well as potential examples of the phenomena an-
notated differently, to get a sense of the consistency
of the annotation from both precision and recall per-
spectives. In this study, we take the phrase structure
trees of the PTB to represent dependencies based on
reasonable identification of heads.
The barerel, vpart, and absol phenomena are com-
pletely unproblematic, with their relevant dependen-
cies explicitly and reliably represented. In addition,
the tough construction is reliably annotated, though
one of the dependencies we take to be central is not
directly represented: The missing argument is linked
to a null wh head at the left edge of the comple-
ment of the tough predicate, rather than to its sub-
ject. Two further phenomena (rnr and vger) are es-
sentially correctly represented: the representations
of the dependencies are explicit and mostly but not
entirely consistently applied. Two out of a sample of
20 examples annotated as containing rnr did not, and
two out of a sample of 35 non-rnr-annotated coordi-
nations actually contained rnr. For vger the primary
problem is with the PoS tagging, where the gerund
is sometimes given a nominal tag, contrary to PTB
guidelines, though the structure above it conforms.
The remaining four constructions are more prob-
lematic. In the case of object control, while the guide-
</bodyText>
<footnote confidence="0.85693725">
5Distinguishing between raising and control requires fur-
ther lexical knowledge and is another example of a “non-
dependency” (in the raising examples). We do not draw that
distinction in our annotations.
</footnote>
<page confidence="0.996267">
401
</page>
<bodyText confidence="0.999772236842105">
lines specify an analysis in which the shared NP is
attached as the object of the higher verb, the PTB
includes not only structures conforming to that anal-
ysis but also “small clause” structures, with the latter
obscuring the relationship of the shared argument to
the higher verb. In the case of itexpl, the adjoined
(S(-NONE- *EXP*)) indicating an expletive use of
it is applied consistently for extraposition (as pre-
scribed in the guidelines). However, the set of lex-
ical licensers of the expletive is incomplete. For ar-
gadj we run into the problem that the PTB does not
explicitly distinguish between post-verbal modifiers
and verbal complements in the way that they are at-
tached. The guidelines suggest that the function tags
(e.g., PP-LOC, etc.) should allow one to distinguish
these, but examination of the PTB itself suggests
that they are not consistently applied. Finally, the
ned construction is not mentioned in the PTB guide-
lines nor is its internal structure represented in the
treebank. Rather, strings like gritty-eyed are left un-
segmented and tagged as JJ.
We note that the PTB representations of many of
these phenomena (barerel, tough, rnr, argadj, control,
itexpl) involve empty elements and/or function tags.
Systems that strip these out before training, as is
common practice, will not benefit from the informa-
tion that is in the PTB.
Our purpose here is not to criticize the PTB,
which has been a tremendously important resource
to the field. Rather, we have two aims: The first is
to provide context for the evaluation of PTB-derived
parsers on these phenomena. The second is to high-
light the difficulty of producing consistent annota-
tions of any complexity as well as the hurdles faced
by a hand-annotation approach when attempting to
scale a resource to more complex representations
and/or additional phenomena (though cf. Vadas and
Curran (2008) on improving PTB representations).
</bodyText>
<sectionHeader confidence="0.999297" genericHeader="method">
4 Methodology
</sectionHeader>
<subsectionHeader confidence="0.983823">
4.1 Data extraction
</subsectionHeader>
<bodyText confidence="0.999585833333333">
We processed 900 million tokens of Wikipedia text
using the October 2010 release of the ERG, follow-
ing the work of the WikiWoods project (Flickinger
et al., 2010). Using the top-ranked ERG deriva-
tion trees as annotations over this corpus and sim-
ple patterns using names of ERG-specific construc-
</bodyText>
<table confidence="0.999897909090909">
Phenomenon Frequency Candidates
barerel 2.12% 546
tough 0.07% 175
rnr 0.69% 1263
itexpl 0.13% 402
vpart 4.07% 765
ned 1.18% 349
absol 0.51% 963
vger 5.16% 679
argadj 3.60% 1346
control 3.78% 124
</table>
<tableCaption confidence="0.982577">
Table 1: Relative frequencies of phenomena matches in
Wikipedia, and number of candidate strings vetted.
</tableCaption>
<bodyText confidence="0.999893454545455">
tions or lexical types, we randomly selected a set
of candidate sentences for each of our ten phenom-
ena. These candidates were then hand-vetted in se-
quence by two annotators to identify, for each phe-
nomenon, 100 examples that do in fact involve the
phenomenon in question and which are both gram-
matical and free of typos. Examples that were ei-
ther deemed overly basic (e.g. plain V + V coordi-
nation, which the ERG treats as rnr) or inappropri-
ately complex (e.g. non-constituent coordination ob-
scuring the interleaving of arguments and adjuncts)
were also discarded at this step. Table 1 summarizes
relative frequencies of each phenomenon in about
47 million parsed Wikipedia sentences, as well as
the total size of the candidate sets inspected. For
the control and tough phenomena hardly any filtering
for complexity was applied, hence these can serve
as indicators of the rate of genuine false positives.
For phenomena that partially overlap with those of
Rimell et al. (2009), it appears our frequency es-
timates are comparable to what they report for the
Brown Corpus (but not the WSJ portion of the PTB).
</bodyText>
<subsectionHeader confidence="0.995442">
4.2 Annotation format
</subsectionHeader>
<bodyText confidence="0.999948714285714">
We annotated up to two dependency triples per phe-
nomenon instance, identifying the heads and depen-
dents by the surface form of the head words in the
sentence suffixed with a number indicating word po-
sition (see Table 2).6 Some strings contain more
than one instance of the phenomenon they illustrate;
in these cases, multiple sets of dependencies are
</bodyText>
<footnote confidence="0.9457865">
6As the parsers differ in tokenization strategies, our evalua-
tion script treats these position IDs as approximate indicators.
</footnote>
<page confidence="0.99357">
402
</page>
<table confidence="0.99307125">
Item ID Phenomenon Polarity
1011079100200 absol 1
1011079100200 absol 1
1011079100200 absol 1
Dependency
having-2|been-3|passed-4 ARG act-1
withdrew-9 MOD having-2|been-3|passed-4
carried+on-12 MOD having-2|been-3|passed-4
</table>
<tableCaption confidence="0.996661">
Table 2: Sample annotations for sentence # 1011079100200: The-0 act-1 having-2 been-3 passed-4 in-5 that-6 year-7
</tableCaption>
<figure confidence="0.986949181818182">
Jessop-8 withdrew-9 and-10 Whitworth-11 carried-12 on-13 with-14 the-15 assistance-16 of-17 his-18 son-19.
Phenomenon Head Type Dependent Distance
Bare relatives gapped predicate in relative ARG2/MOD modified noun 3.0 (8)
(barerel) modified noun MOD top predicate of relative 3.3 (8)
Tough adjectives tough adjective ARG2 to-VP complement 1.7 (5)
(tough) gapped predicate in to-VP ARG2 subject/modifiee of adjective 6.4 (21)
Right Node Raising verb/prep2 ARG2 shared noun 2.8 (9)
(rnr) verb/prep1 ARG2 shared noun 6.1 (12)
Expletive It it-subject taking verb !ARG1 it 1.2 (3)
(itexpl) raising-to-object verb !ARG2 it –
Absolutives absolutive predicate ARG1 subject of absolutive 1.7 (12)
(absol) main clause predicate MOD absolutive predicate 9.8 (26)
Verbal gerunds selecting head ARG[1,2] gerund 1.9 (13)
(vger) gerund ARG2/MOD first complement/modifier of gerund 2.3 (8)
Interleaved arg/adj selecting verb MOD interleaved adjunct 1.2 (7)
(argadj) selecting verb ARG[2,3] displaced complement 5.9 (26)
Control “upstairs” verb ARG[2,3] “downstairs” verb 2.4 (23)
(control) “downstairs” verb ARG1 shared argument 4.8 (17)
Verb+particle constructions particle !ARG2 complement 2.7 (9)
(vpart) verb+particle ARG2 complement 3.7 (10)
Adj/Noun2 + Noun1-ed head noun MOD Noun1-ed 2.4 (17)
(ned) Noun1-ed ARG1/MOD Adj/Noun2 1.0 (1.5)
</figure>
<tableCaption confidence="0.994873">
Table 3: Dependencies labeled for each phenomenon type, including average and maximum surface distances.
</tableCaption>
<bodyText confidence="0.998147285714286">
recorded. In addition, some strings evince more than
one of the phenomena we are studying. However,
we only annotate the dependencies associated with
the phenomenon the string was selected to repre-
sent. Finally, in examples with coordinated heads or
dependents, we recorded separate dependencies for
each conjunct. In total, we annotated 2127 depen-
dency triples for the 1000 sentences, including 253
negative dependencies (see below). Table 3 outlines
the dependencies annotated for each phenomenon.
To allow for multiple plausible attachment sites,
we give disjunctive values for heads or dependents
in several cases: (i) with auxiliaries, (ii) with com-
plementizers (that or to, as in Table 2), (iii) in cases
of measure or classifier nouns or partitives, (iv) with
multi-word proper names and (v) where there is
genuine attachment ambiguity for modifiers. As
these sets of targets are disjunctive, these conven-
tions should have the effect of increasing measured
parser performance. 580 (27%) of the annotated de-
pendencies had at least one disjunction.
</bodyText>
<subsectionHeader confidence="0.998572">
4.3 Annotation and reconciliation process
</subsectionHeader>
<bodyText confidence="0.99997505">
The entire data set was annotated independently by
two annotators. Both annotators were familiar with
the ERG, used to identify these sentences in the
WikiWoods corpus, but the annotation was done
without reference to the ERG parses. Before begin-
ning annotation on each phenomenon, we agreed on
which dependencies to annotate. We also communi-
cated with each other about annotation conventions
as the need for each convention became clear. The
annotation conventions address how to handle co-
ordination, semantically empty auxiliaries, passives
and similar orthogonal phenomena.
Once the entire data set was dual-annotated, we
compared annotations, identifying the following
sources of mismatch: typographical errors, incom-
pletely specified annotation conventions, inconsis-
tent application of conventions (101 items, dropping
in frequency as the annotation proceeded), and gen-
uine disagreement about what to annotate, either dif-
ferent numbers of dependencies of interest identified
</bodyText>
<page confidence="0.998561">
403
</page>
<bodyText confidence="0.999993785714286">
in an item (59 items) or conflicting elements in a de-
pendency (54 items).7 Overall, our initial annotation
pass led to agreement on 79% of the items, and a
higher per-dependency level of agreement. Agree-
ment could be expected to approach 90% with more
experience in applying annotation conventions.
We then reconciled the annotations, using the
comparison to address all sources of difference. In
most cases, we readily agreed which annotation was
correct and which was in error. In a few cases, we
decided that both annotations were plausible alter-
natives (e.g., in terms of alternative attachment sites
for modifiers) and so created a single merged anno-
tation expressing the disjunction of both (cf. § 4.2).
</bodyText>
<sectionHeader confidence="0.99866" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999805384615385">
With the test data consisting of 100 items for each of
our ten selected phenomena, we ran all seven pars-
ing systems and recorded their dependency-style
outputs for each sentence. While these outputs
are not directly comparable with each other, we
were able to associate our manually-annotated tar-
get dependencies with parser-specific dependencies,
by defining sets of phenomenon-specific regular ex-
pressions for each parser. In principle, we allow this
mapping to be somewhat complex (and forgiving to
non-contentful variation), though we require that it
work deterministically and not involve specific lexi-
cal information. An example set is given in Fig. 2.
</bodyText>
<table confidence="0.790420454545455">
&amp;quot;absol&amp;quot; =&gt; [
{’ARG1’ =&gt;
’\(ncsubj \W*{W1]\W*_(\d+) \W*{W2]\W*_(\d+) _\)’,
’\(ncmod _ \W*{W2]\W*_(\d+) \W*{W1]\W*_(\d+)\)’],
’ARG’ =&gt; [
’\(ncsubj \W*{W1]\W*_(\d+) \W*{W2]\W*_(\d+) _\)’,
’\(ncmod _ \W*{W1]\W*_(\d+) \W*{W2]\W*_(\d+)\)’],
’MOD’ =&gt; [
’\(xmod _ \W*{W1]\W*_(\d+) \W*{W2]\W*_(\d+)\)’,
’\(ncmod _ \W*{W1]\W*_(\d+) \W*{W2]\W*_(\d+)\)’,
’\(cmod _ \W*{W1]\W*_(\d+) \W*{W2]\W*_(\d+)\)’]]
</table>
<figureCaption confidence="0.986235">
Figure 2: Regexp set to evaluate C&amp;C for absol.
</figureCaption>
<bodyText confidence="0.9998776">
These expressions fit the output that we got from the
C&amp;C parser, illustrated in Fig. 3 with a relevant por-
tion of the dependencies produced for the example
in Table 2. Here the C&amp;C dependency (ncsubj
passed 4 Act 1 ) matches the first target in the
</bodyText>
<footnote confidence="0.9530345">
7We do not count typographical errors or incompletely spec-
ified conventions as failures of inter-annotator agreement.
</footnote>
<bodyText confidence="0.886543">
gold-standard (Table 2), but no matching C&amp;C de-
pendency is found for the other two targets.
</bodyText>
<equation confidence="0.972737">
(xmod _ Act_1 passed_4)
(ncsubj passed_4 Act_1 _)
(ncmod _ withdrew,_9 Jessop_8)
(dobj year,_7 withdrew,_9)
</equation>
<figureCaption confidence="0.997857">
Figure 3: Excerpts of C&amp;C output for item in Table 2.
</figureCaption>
<bodyText confidence="0.996463243902439">
The regular expressions operate solely on the de-
pendency labels and are not lexically-specific. They
are specific to each phenomenon, as we did not at-
tempt to write a general dependency converter, but
rather to discover what patterns of dependency rela-
tions describe the phenomenon when it is correctly
identified by each parser. Thus, though we did not
hold out a test set, we believe that they would gener-
alize to additional gold standard material annotated
in the same way for the same phenomena.8
In total, we wrote 364 regular expressions to han-
dle the output of the seven parsers, allowing some
leeway in the role labels used by a parser for any
given target dependency. The supplementary mate-
rials for this paper include the test data, parser out-
puts, target annotations, and evaluation script.
Fig. 1 provides a visualization of the results of our
evaluation. Each column of points represents one
dependency type. Dependency types for the same
phenomenon are represented by adjacent columns.
The order of the columns within a phenomenon fol-
lows the order of the dependency descriptions in
Table 3: For each pair, the dependency type with
the higher score for the majority of the parsers is
shown first (to the left). The phenomena them-
selves are also arranged according to increasing (av-
erage) difficulty. itexpl only has one column, as we
annotated just one dependency per instance here.
(The two descriptions in Table 3 reflect different,
mutually-incompatible instance types.) Since ex-
pletive it should not be the semantic dependent of
any head, the targets are generalized for this phe-
nomenon and the evaluation script counts as incor-
8In the case of the XLE, our simplistic regular-expression
approach to the interpretation of parser outputs calls for much
more complex patterns than for the other parsers. This is owed
to the rich internal structure of LFG f-structures and higher
granularity of linguistic analysis, where feature annotations on
nodes as well as reentrancies need to be taken into account.
Therefore, our current results for the XLE admit small amounts
of both over- and under-counting.
</bodyText>
<page confidence="0.997057">
404
</page>
<figure confidence="0.999774909090909">
100 enju
90 xle
80 c&amp;j
70 c&amp;c
60 stanford
50 mst
40 rasp
30
20
10
0
</figure>
<figureCaption confidence="0.999984">
Figure 1: Individual dependency recall for seven parsers over ten phenomena.
</figureCaption>
<bodyText confidence="0.99979505">
rect any dependency involving referential it.
We observe fairly high recall of the dependencies
for vpart and vger (with the exception of RASP), and
high recall for both dependencies representing con-
trol for five systems. While Enju, Stanford, MST,
and RASP all found between 70 and 85% of the de-
pendency between the adjective and its complement
in the tough construction, only Enju and XLE rep-
resented the dependency between the subject of the
adjective and the gap inside the adjective’s comple-
ment. For the remaining phenomena, each parser
performed markedly worse on one dependency type,
compared to the other. The only exceptions here
are XLE and C&amp;C’s (and to a lesser extent, C&amp;J’s)
scores for barerel. No system scored higher than
33% on the harder of the two dependencies in rnror
absol, and Stanford, MST, and RASP all scored be-
low 25% on the harder dependency in barerel. Only
XLE scored higher than 10% on the second depen-
dency for ned and higher than 50% for itexpl.
</bodyText>
<sectionHeader confidence="0.998975" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.945554161290322">
From the results in Fig. 1, it is clear that even the best
of these parsers fail to correctly identify a large num-
ber of relevant dependencies associated with linguis-
tic phenomena that occur with reasonable frequency
in the Wikipedia. Each of the parsers attempts
with some success to analyze each of these phe-
nomena, reinforcing the claim of relevance, but they
vary widely across phenomena. For the two long-
distance phenomena that overlap with those studied
in Rimell et al. (2009), our results are comparable.9
Our evaluation over Wikipedia examples thus shows
the same relative lack of success in recovering long-
distance dependencies that they found for WSJ sen-
tences. The systems did better on relatively well-
studied phenomena including control, vger, and vpart,
but had less success with the rest, even though all but
two of those remaining phenomena involve syntac-
tically local dependencies (as indicated in Table 3).
Successful identification of the dependencies in
these phenomena would, we hypothesize, benefit
from richer (or deeper) linguistic information when
parsing, whether it is lexical (tough, control, itexpl,
and vpart), or structural (rnr, absol, vger, argadj, and
barerel), or somewhere in between, as with ned. In
the case of treebank-trained parsers, for the informa-
tion to be available, it must be consistently encoded
in the treebank and attended to during training. As
9Other than Enju, which scores 16 points higher in the eval-
uation of Rimell et al., our average scores for each parser across
the dependencies for these phenomena are within 12 points of
those reported by Rimell et al. (2009) and Nivre et al. (2010).
</bodyText>
<page confidence="0.983941">
405
</page>
<bodyText confidence="0.995558978723404">
noted in Sections 2.1 and 3.5, there is tension be- previously thought.
tween developing sufficiently complex representa- 8 Conclusion
tions to capture linguistic phenomena and keeping We have presented a detailed construction-focused
an annotation scheme simple enough that it can be evaluation of seven parsers over 10 phenomena,
reliably produced by humans, in the case of hand- with 1000 examples drawn from English Wikipedia.
annotation. Gauging recall of such “deep” dependencies, in our
7 Related Work view, can serve as a proxy for downstream pro-
This paper builds on a growing body of work which cessing involving semantic interpretation of parser
goes beyond (un)labeled bracketing in parser evalua- outputs. Our annotations and automated evaluation
tion, including Lin (1995), Carroll et al. (1998), Ka- script are provided in the supplementary materials,
plan et al. (2004), Rimell et al. (2009), and Nivre et for full replicability. Our results demonstrate that
al. (2010). Most closely related are the latter two of significant opportunities remain for parser improve-
the above, as we adopt their “construction-focused ment, and highlight specific challenges that remain
parser evaluation methodology”. invisible in aggregate parser evaluation (e.g. Parse-
There are several methodological differences be- val or overall dependency accuracy). These results
tween our work and that of Rimell et al. First, we suggest that further progress will depend on train-
draw our evaluation data from a much larger and ing data that is both more extensive and more richly
more varied corpus. Second, we automate the com- annotated than what is typically used today (seeing,
parison of parser output to the gold standard, and we for example, that a large part of more detailed PTB
distribute the evaluation scripts along with the anno- annotation remains ignored in much parsing work).
tated corpus, enhancing replicability. Third, where There are obvious reasons calling for diversity in
Rimell et al. extract evaluation targets on the basis approaches to parsing and for different trade-offs
of PTB annotations, we make use of a linguistically in, for example, the granularity of linguistic analy-
precise broad-coverage grammar to identify candi- sis, average accuracy, cost of computation, or ease
date examples. This allows us to include both local of adaptation. Our proposal is not to substitute
and non-local dependencies not represented or not construction-focused evaluation on Wikipedia data
reliably encoded in the PTB, enabling us to evalu- for widely used aggregate metrics and reference cor-
ate parser performance with more precision over a pora, but rather to augment such best practices in
wider range of linguistic phenomena. the spirit of Rimell et al. (2009) and expand the
These methodological innovations bring two em- range of phenomena considered in such evaluations.
pirical results. The first is qualitative: Where previ- Across frameworks and traditions (and in principle
ous work showed that overall Parseval numbers hide languages), it is of vital importance to be able to
difficulties with long-distance dependencies, our re- evaluate the quality of parsing (and grammar induc-
sults show that there are multiple kinds of reason- tion) algorithms in a maximally informative manner.
ably frequent local dependencies which are also dif- Acknowledgments
ficult for the current standard approaches to pars- We are grateful to Tracy King for her assistance in
ing. The second is quantitative: Where Rimell et setting up the XLE system and to three anonymous
al. found two phenomena which were virtually un- reviewers for helpful comments. The fourth author
analyzed (recall below 10%) for one or two parsers thanks DFKI and the DFG funded Excellence Clus-
each, we found eight phenomena which were vir- ter on MMCI for their support of the work. Data
tually unanalyzed by at least one system, includ- preparation on the scale of Wikipedia was made pos-
ing two unanalyzed by five and one by six. Every sible through access to large-scale HPC facilities,
system had at least one virtually unanalyzed phe- and we are grateful to the Scientific Computing staff
nomenon. Thus we have shown that the dependen- at UiO and the Norwegian Metacenter for Computa-
cies being missed by typical modern approaches to tional Science.
parsing are more varied and more numerous than
406
</bodyText>
<sectionHeader confidence="0.988433" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999646695238095">
Jennifer E. Arnold, Thomas Wasow, Anthony Losongco,
and Ryan Ginstrom. 2000. Heaviness vs. newness:
The effects of structural complexity and discourse sta-
tus on constituent ordering. Language, 76(1):28–55.
Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-
Intyre. 1995. Bracketing guidelines for treebank II
style Penn treebank project. Technical report, Univer-
sity of Pennsylvania.
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In Proceed-
ings of the COLING/ACL 2006 Interactive Presenta-
tion Sessions, pages 77–80, Sydney, Australia.
Aoife Cahill, John T. Maxwell III, Paul Meurer, Chris-
tian Rohrer, and Victoria Ros´en. 2008. Speeding
up LFG parsing using c-structure pruning. In Coling
2008: Proceedings of the workshop on Grammar En-
gineering Across Frameworks, pages 33–40, Manch-
ester, England, August. Coling 2008 Organizing Com-
mittee.
John Carroll, Ted Briscoe, and Antonio Sanfilippo. 1998.
Parser evaluation: A survey and a new proposal.
In Proceedings of the International Conference on
Language Resources and Evaluation, pages 447–454,
Granada.
Daniel Cer, Marie-Catherine de Marneffe, Daniel Juraf-
sky, and Christopher D. Manning. 2010. Parsing to
Stanford dependencies: Trade-offs between speed and
accuracy. In 7th International Conference on Lan-
guage Resources and Evaluation (LREC 2010), Malta.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL’05),
pages 173–180, Ann Arbor, Michigan.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Proceed-
ings of the Fourteenth National Conference on Artifi-
cialIntelligence, pages 598–603, Providence, RI.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG and
log-linear models. Computational Linguistics,
33(4):493–552.
Alexander Clark. 2001. Unsupervised induction
of stochastic context-free grammars using distribu-
tional clustering. In Proceedings of the 5th Confer-
ence on Natural Language Learning, pages 105–112.
Toulouse, France.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Dan Flickinger, Stephan Oepen, and Gisle Ytrestøl.
2010. WikiWoods. Syntacto-semantic annotation for
English Wikipedia. In Proceedings of the 6th Interna-
tional Conference on Language Resources and Evalu-
ation, Valletta, Malta.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language En-
gineering, 6 (1) (Special Issue on Efficient Processing
with HPSG):15 – 28.
Sumukh Ghodke and Steven Bird. 2010. Fast query for
large treebanks. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pages 267–275, Los Angeles, California, June.
Association for Computational Linguistics.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
grammar induction. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 881–888, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Dependency
Structures Extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355–396.
Rodney Huddleston and Geoffrey K. Pullum. 2002. The
Cambridge Grammar of the English Language. Cam-
bridge University Press, Cambridge, UK.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
In Proceedings of NODALIDA 2007, pages 105–112,
Tartu, Estonia.
Ron Kaplan, Stefan Riezler, Tracy H King, John T
Maxwell III, Alex Vasserman, and Richard Crouch.
2004. Speed and accuracy in shallow and deep
stochastic parsing. In Susan Dumais, Daniel Marcu,
and Salim Roukos, editors, HLT-NAACL 2004: Main
Proceedings, pages 97–104, Boston, Massachusetts,
USA, May. Association for Computational Linguis-
tics.
Dan Klein and Christopher D. Manning. 2003. Fast exact
inference with a factored model for natural language
parsing. In Advances in Neural Information Process-
ing Systems 15, pages 3–10, Cambridge, MA. MIT
Press.
Dan Klein and Christopher Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of the
42nd Meeting of the Association for Computational
Linguistics, pages 478–485, Barcelona, Spain.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In Proceedings of
IJCAI-95, pages 1420–1425, Montreal, Canada.
Robert Malouf. 2000. Verbal gerunds as mixed cate-
gories in HPSG. In Robert Borsley, editor, The Nature
</reference>
<page confidence="0.979743">
407
</page>
<reference confidence="0.999744552631579">
and Function of Syntactic Categories, pages 133–166.
Academic Press, New York.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19:313–330.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-Projective Dependency Pars-
ing using Spanning Tree Algorithms. In Proceedings
of the 2005 Conference on Empirical Methods in Natu-
ral Language Processing, pages 523–530, Vancouver,
Canada.
Yusuke Miyao, Takashi Ninomiya, and Jun’ichi Tsujii.
2004. Corpus-oriented grammar development for ac-
quiring a Head-driven Phrase Structure Grammar from
the Penn Treebank. In Proceedings of the 1st Interna-
tional Joint Conference on Natural Language Process-
ing, pages 684–693, Hainan Island, China.
Joakim Nivre, Laura Rimell, Ryan McDonald, and Carlos
G´omez Rodr´ıguez. 2010. Evaluation of dependency
parsers on unbounded dependencies. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics, pages 833–841, Beijing, China.
Ruth O’Donovan, Michael Burke, Aoife Cahill, Josef
Van Genabith, and Andy Way. 2004. Large-scale in-
duction and evaluation of lexical resources from the
penn-ii treebank. In Proceedings of the 42nd Meet-
ing of the Association for Computational Linguistics,
pages 367–374, Barcelona, Spain.
Stephan Oepen, Daniel Flickinger, Kristina Toutanova,
and Christopher D. Manning. 2004. LinGO Red-
woods. A rich and dynamic treebank for HPSG.
Journal of Research on Language and Computation,
2(4):575–596.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 433–440, Sydney, Aus-
tralia.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell III, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proceedings of the 40th Meet-
ing of the Association for Computational Linguistics,
Philadelphia, PA.
Laura Rimell, Stephen Clark, and Mark Steedman. 2009.
Unbounded dependency recovery for parser evalua-
tion. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 813–821, Singapore. Association for Computa-
tional Linguistics.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-
take, and Dan Flickinger. 2002. Multiword expres-
sions. A pain in the neck for NLP. In Alexander Gel-
bukh, editor, Computational Linguistics and Intelli-
gent Text Processing, volume 2276 of Lecture Notes in
Computer Science, pages 189–206. Springer, Berlin,
Germany.
David Vadas and James R. Curran. 2008. Parsing noun
phrase structure with CCG. In Proceedings of ACL-
08: HLT, pages 335–343, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
L. van der Beek, Gosse Bouma, Robert Malouf, and Gert-
jan van Noord. 2002. The Alpino Dependency Tree-
bank. In Mariet Theune, Anton Nijholt, and Hen-
dri Hondorp, editors, Computational Linguistics in the
Netherlands, Amsterdam, The Netherlands. Rodopi.
Luke Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form: Structured clas-
sification with probabilistic categorial grammars. In
Proceedings of the Twenty-FirstAnnual Conference on
Uncertainty in Artificial Intelligence, pages 658–666,
Arlington, Virginia. AUAI Press.
</reference>
<page confidence="0.997893">
408
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.765387">
<title confidence="0.9987315">Parser Evaluation over Local and Non-Local Deep Dependencies in a Large Corpus</title>
<author confidence="0.986538">M Dan Stephan Yi of Linguistics</author>
<author confidence="0.986538">University of Washington</author>
<author confidence="0.986538">Stanford</author>
<address confidence="0.781824">of Informatics, Universitetet i Oslo, of Computational Linguistics, Saarland</address>
<email confidence="0.997654">ebender@uw.edu,danf@stanford.edu,oe@ifi.uio.no,yzhang@coli.uni-sb.de</email>
<abstract confidence="0.999669230769231">In order to obtain a fine-grained evaluation of parser accuracy over naturally occurring text, we study 100 examples each of ten reasonably frequent linguistic phenomena, randomly selected from a parsed version of the English Wikipedia. We construct a corresponding set of gold-standard target dependencies for these 1000 sentences, operationalize mappings to these targets from seven state-of-theart parsers, and evaluate the parsers against this data to measure their level of success in identifying these dependencies.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jennifer E Arnold</author>
<author>Thomas Wasow</author>
<author>Anthony Losongco</author>
<author>Ryan Ginstrom</author>
</authors>
<title>Heaviness vs. newness: The effects of structural complexity and discourse status on constituent ordering.</title>
<date>2000</date>
<journal>Language,</journal>
<volume>76</volume>
<issue>1</issue>
<contexts>
<context position="19195" citStr="Arnold et al., 2000" startWordPosition="2957" endWordPosition="2960">” in 1957 at age 60 but continued to paint full time. In a verbal gerund, the -ing form a verb retains verbal properties (e.g., being able to take NP complements, rather than only PP complements) but heads a phrase that fills an NP position in the syntax (Malouf, 2000). Since gerunds have the same morphology as present participle VPs, their role in the larger clause is susceptible to misanalysis. The argadj examples are of interest because English typically prefers to have direct objects directly adjacent to the selecting verb. Nonetheless, phenomena such as parentheticals and heavy-NP shift (Arnold et al., 2000), in which “heavy” constituents appear further to the right in the string, allow for adjunct-argument order in a minority of cases. We hypothesize that the relative infrequency of this construction will lead parsers to prefer incorrect analyses (wherein the adjunct is picked up as a complement, the complement as an adjunct or the structure differs entirely) unless they have access to linguistic knowledge providing constraints on possible and probable complementation patterns for the head. Finally, we turn to raising and control verbs (‘control’) (e.g., Huddleston and Pullum (2002, ch. 14)). Th</context>
</contexts>
<marker>Arnold, Wasow, Losongco, Ginstrom, 2000</marker>
<rawString>Jennifer E. Arnold, Thomas Wasow, Anthony Losongco, and Ryan Ginstrom. 2000. Heaviness vs. newness: The effects of structural complexity and discourse status on constituent ordering. Language, 76(1):28–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Bies</author>
<author>Mark Ferguson</author>
<author>Karen Katz</author>
<author>Robert MacIntyre</author>
</authors>
<title>Bracketing guidelines for treebank II style Penn treebank project.</title>
<date>1995</date>
<tech>Technical report,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="20618" citStr="Bies et al., 1995" startWordPosition="3191" endWordPosition="3194">he infinitival VP. Here it is the dependency between the infinitval VP and the NP argument of the “upstairs” verb which we expect to be particularly subtle. Getting this right requires specific lexical knowledge about which verbs take these complementation patterns. This lexical knowledge needs to be represented in such a way that it can be used robustly even in the case of passives, relative clauses, etc.5 3.5 Penn Treebank representations We investigated the representation of these 10 phenomena in the PTB (Marcus et al., 1993) in two steps: First we explored the PTB’s annotation guidelines (Bies et al., 1995) to determine how the relevant dependencies were intended to be represented. We then used Ghodke and Bird’s (2010) Treebank Search to find examples of the intended annotations as well as potential examples of the phenomena annotated differently, to get a sense of the consistency of the annotation from both precision and recall perspectives. In this study, we take the phrase structure trees of the PTB to represent dependencies based on reasonable identification of heads. The barerel, vpart, and absol phenomena are completely unproblematic, with their relevant dependencies explicitly and reliabl</context>
</contexts>
<marker>Bies, Ferguson, Katz, MacIntyre, 1995</marker>
<rawString>Ann Bies, Mark Ferguson, Karen Katz, and Robert MacIntyre. 1995. Bracketing guidelines for treebank II style Penn treebank project. Technical report, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
<author>Rebecca Watson</author>
</authors>
<title>The second release of the RASP system.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions,</booktitle>
<pages>77--80</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="5626" citStr="Briscoe et al. (2006)" startWordPosition="844" endWordPosition="847"> knowledge from annotated treebanks, e.g. the Penn Treebank (PTB), which encodes “surface grammatical analysis” (Marcus et al., 1993). Such approaches include those that directly (and exclusively) use the information in the treebank (e.g. Charniak (1997), Collins (1999), Petrov et al. (2006), inter alios) as well as those that complement treebank structures with some amount of hand-coded linguistic knowledge (e.g. O’Donovan et al. (2004), Miyao et al. (2004), Hockenmaier and Steedman (2007), inter alios). Another hybrid in terms of its acquisition of linguistic knowledge is the RASP system of Briscoe et al. (2006), combining a hand-coded grammar over PoS tag sequences with a probabilistic tagger and statistical syntactic disambiguation. Design of representations Approaches to parsing also differ fundamentally in the style of representation assigned to strings. These vary both in their 1Additional sources of variation among extant parsing technologies include (a) the behavior with respect to ungrammatical inputs and (b) the relationship between probabilistic and symbolic knowledge in the parser, where parsers with a hand-coded grammar at their core typically also incorporate an automatically trained pro</context>
<context position="12153" citStr="Briscoe et al., 2006" startWordPosition="1837" endWordPosition="1840">mmar and lexicon are automatically explore and our motivations for choosing them. Our induced from CCGBank (Hockenmaier and Steed- goal was to find phenomena where the relevant deman, 2007), a largely automatic projection of (the pendencies are relatively subtle, such that more linWSJ portion of) PTB trees into the CCG framework. guistic knowledge is beneficial in order to retrieve In addition to CCG derivations, the C&amp;C parser can them. Though this set is of course only a sampling, directly output a variant of grammatical relations. these phenomena illustrate the richness of structure, RASP (Briscoe et al., 2006) is an unlexicalized both local and non-local, involved in the mapping robust parsing system, with a hand-crafted “tag se- from English strings to their meanings. We discuss quence” grammar at its core. The parser thus anal- the phenomena in four sets and then briefly review yses a lattice of PoS tags, building a parse forest their representation in the Penn Treebank. from which the most probable syntactic trees and 3.1 Long distance dependencies sets of corresponding grammatical relations can be Three of our phenomena can be classified as involvextracted. Unlike other parsers in our mix, RASP</context>
</contexts>
<marker>Briscoe, Carroll, Watson, 2006</marker>
<rawString>Ted Briscoe, John Carroll, and Rebecca Watson. 2006. The second release of the RASP system. In Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 77–80, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>John T Maxwell Paul Meurer</author>
<author>Christian Rohrer</author>
<author>Victoria Ros´en</author>
</authors>
<title>Speeding up LFG parsing using c-structure pruning.</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In Coling 2008: Proceedings of the workshop on Grammar Engineering Across Frameworks,</booktitle>
<pages>33--40</pages>
<location>Manchester, England,</location>
<marker>Cahill, Meurer, Rohrer, Ros´en, 2008</marker>
<rawString>Aoife Cahill, John T. Maxwell III, Paul Meurer, Christian Rohrer, and Victoria Ros´en. 2008. Speeding up LFG parsing using c-structure pruning. In Coling 2008: Proceedings of the workshop on Grammar Engineering Across Frameworks, pages 33–40, Manchester, England, August. Coling 2008 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Ted Briscoe</author>
<author>Antonio Sanfilippo</author>
</authors>
<title>Parser evaluation: A survey and a new proposal.</title>
<date>1998</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation,</booktitle>
<pages>447--454</pages>
<location>Granada.</location>
<contexts>
<context position="38486" citStr="Carroll et al. (1998)" startWordPosition="6003" endWordPosition="6006"> have presented a detailed construction-focused an annotation scheme simple enough that it can be evaluation of seven parsers over 10 phenomena, reliably produced by humans, in the case of hand- with 1000 examples drawn from English Wikipedia. annotation. Gauging recall of such “deep” dependencies, in our 7 Related Work view, can serve as a proxy for downstream proThis paper builds on a growing body of work which cessing involving semantic interpretation of parser goes beyond (un)labeled bracketing in parser evalua- outputs. Our annotations and automated evaluation tion, including Lin (1995), Carroll et al. (1998), Ka- script are provided in the supplementary materials, plan et al. (2004), Rimell et al. (2009), and Nivre et for full replicability. Our results demonstrate that al. (2010). Most closely related are the latter two of significant opportunities remain for parser improvethe above, as we adopt their “construction-focused ment, and highlight specific challenges that remain parser evaluation methodology”. invisible in aggregate parser evaluation (e.g. ParseThere are several methodological differences be- val or overall dependency accuracy). These results tween our work and that of Rimell et al. </context>
</contexts>
<marker>Carroll, Briscoe, Sanfilippo, 1998</marker>
<rawString>John Carroll, Ted Briscoe, and Antonio Sanfilippo. 1998. Parser evaluation: A survey and a new proposal. In Proceedings of the International Conference on Language Resources and Evaluation, pages 447–454, Granada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Cer</author>
<author>Marie-Catherine de Marneffe</author>
<author>Daniel Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing to Stanford dependencies: Trade-offs between speed and accuracy.</title>
<date>2010</date>
<booktitle>In 7th International Conference on Language Resources and Evaluation (LREC</booktitle>
<marker>Cer, de Marneffe, Jurafsky, Manning, 2010</marker>
<rawString>Daniel Cer, Marie-Catherine de Marneffe, Daniel Jurafsky, and Christopher D. Manning. 2010. Parsing to Stanford dependencies: Trade-offs between speed and accuracy. In 7th International Conference on Language Resources and Evaluation (LREC 2010), Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-tofine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>173--180</pages>
<location>Ann Arbor, Michigan.</location>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-tofine n-best parsing and maxent discriminative reranking. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 173–180, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical parsing with a context-free grammar and word statistics.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fourteenth National Conference on ArtificialIntelligence,</booktitle>
<pages>598--603</pages>
<location>Providence, RI.</location>
<contexts>
<context position="5259" citStr="Charniak (1997)" startWordPosition="788" endWordPosition="789">ark, 2001; Klein and Manning, 2004), where the primary source of linguistic knowledge is co-occurrence patterns of words in unannotated text. As Haghighi and Klein (2006) show, augmenting this knowledge with handcrafted prototype “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all of their linguistic knowledge from annotated treebanks, e.g. the Penn Treebank (PTB), which encodes “surface grammatical analysis” (Marcus et al., 1993). Such approaches include those that directly (and exclusively) use the information in the treebank (e.g. Charniak (1997), Collins (1999), Petrov et al. (2006), inter alios) as well as those that complement treebank structures with some amount of hand-coded linguistic knowledge (e.g. O’Donovan et al. (2004), Miyao et al. (2004), Hockenmaier and Steedman (2007), inter alios). Another hybrid in terms of its acquisition of linguistic knowledge is the RASP system of Briscoe et al. (2006), combining a hand-coded grammar over PoS tag sequences with a probabilistic tagger and statistical syntactic disambiguation. Design of representations Approaches to parsing also differ fundamentally in the style of representation as</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>Eugene Charniak. 1997. Statistical parsing with a context-free grammar and word statistics. In Proceedings of the Fourteenth National Conference on ArtificialIntelligence, pages 598–603, Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Widecoverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="11389" citStr="Clark and Curran, 2007" startWordPosition="1714" endWordPosition="1717">own requirements terial from the WSJ sections of the PTB, but the regarding preprocessing of text, especially tokenizatreebank is first semi-automatically converted into tion. We customized the tokenization to each parser, HPSG derivations, and the annotation is enriched by using the parser’s own internal tokenization or with typed feature structures for each constituent. pre-tokenizing to match the parser’s desired input. In addition to HPSG derivation trees, Enju also pro- The evaluation script is robust to variations in tokduces predicate argument structures. enization across parsers. C&amp;C (Clark and Curran, 2007) is a statistical 3 Phenomena CCG parser. Abstractly similar to the approach of In this section we summarize the ten phenomena we Enju, the grammar and lexicon are automatically explore and our motivations for choosing them. Our induced from CCGBank (Hockenmaier and Steed- goal was to find phenomena where the relevant deman, 2007), a largely automatic projection of (the pendencies are relatively subtle, such that more linWSJ portion of) PTB trees into the CCG framework. guistic knowledge is beneficial in order to retrieve In addition to CCG derivations, the C&amp;C parser can them. Though this set</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007. Widecoverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics, 33(4):493–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Unsupervised induction of stochastic context-free grammars using distributional clustering.</title>
<date>2001</date>
<booktitle>In Proceedings of the 5th Conference on Natural Language Learning,</booktitle>
<pages>105--112</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="4653" citStr="Clark, 2001" startWordPosition="698" endWordPosition="699">ative examples along these dimensions, as these help to position the parsers we subsequently evaluate.1 2.1 Approaches to parsing Source of linguistic knowledge At one end of this dimension, we find systems whose linguistic knowledge is encoded in hand-crafted rules and lexical entries; for English, the ParGram XLE system (Riezler et al., 2002) and DELPH-IN English Resource Grammar (ERG; Flickinger (2000))—each reflecting decades of continuous development—achieve broad coverage of open-domain running text, for example. At the other end of this dimension, we find fully unsupervised approaches (Clark, 2001; Klein and Manning, 2004), where the primary source of linguistic knowledge is co-occurrence patterns of words in unannotated text. As Haghighi and Klein (2006) show, augmenting this knowledge with handcrafted prototype “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all of their linguistic knowledge from annotated treebanks, e.g. the Penn Treebank (PTB), which encodes “surface grammatical analysis” (Marcus et al., 1993). Such approaches include those that directly (and exclusively) use the information in the treebank (e.g. Charniak </context>
</contexts>
<marker>Clark, 2001</marker>
<rawString>Alexander Clark. 2001. Unsupervised induction of stochastic context-free grammars using distributional clustering. In Proceedings of the 5th Conference on Natural Language Learning, pages 105–112. Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="5275" citStr="Collins (1999)" startWordPosition="790" endWordPosition="791">and Manning, 2004), where the primary source of linguistic knowledge is co-occurrence patterns of words in unannotated text. As Haghighi and Klein (2006) show, augmenting this knowledge with handcrafted prototype “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all of their linguistic knowledge from annotated treebanks, e.g. the Penn Treebank (PTB), which encodes “surface grammatical analysis” (Marcus et al., 1993). Such approaches include those that directly (and exclusively) use the information in the treebank (e.g. Charniak (1997), Collins (1999), Petrov et al. (2006), inter alios) as well as those that complement treebank structures with some amount of hand-coded linguistic knowledge (e.g. O’Donovan et al. (2004), Miyao et al. (2004), Hockenmaier and Steedman (2007), inter alios). Another hybrid in terms of its acquisition of linguistic knowledge is the RASP system of Briscoe et al. (2006), combining a hand-coded grammar over PoS tag sequences with a probabilistic tagger and statistical syntactic disambiguation. Design of representations Approaches to parsing also differ fundamentally in the style of representation assigned to string</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Flickinger</author>
<author>Stephan Oepen</author>
<author>Gisle Ytrestøl</author>
</authors>
<title>WikiWoods. Syntacto-semantic annotation for English Wikipedia.</title>
<date>2010</date>
<booktitle>In Proceedings of the 6th International Conference on Language Resources and Evaluation,</booktitle>
<location>Valletta, Malta.</location>
<contexts>
<context position="24431" citStr="Flickinger et al., 2010" startWordPosition="3812" endWordPosition="3815">e two aims: The first is to provide context for the evaluation of PTB-derived parsers on these phenomena. The second is to highlight the difficulty of producing consistent annotations of any complexity as well as the hurdles faced by a hand-annotation approach when attempting to scale a resource to more complex representations and/or additional phenomena (though cf. Vadas and Curran (2008) on improving PTB representations). 4 Methodology 4.1 Data extraction We processed 900 million tokens of Wikipedia text using the October 2010 release of the ERG, following the work of the WikiWoods project (Flickinger et al., 2010). Using the top-ranked ERG derivation trees as annotations over this corpus and simple patterns using names of ERG-specific construcPhenomenon Frequency Candidates barerel 2.12% 546 tough 0.07% 175 rnr 0.69% 1263 itexpl 0.13% 402 vpart 4.07% 765 ned 1.18% 349 absol 0.51% 963 vger 5.16% 679 argadj 3.60% 1346 control 3.78% 124 Table 1: Relative frequencies of phenomena matches in Wikipedia, and number of candidate strings vetted. tions or lexical types, we randomly selected a set of candidate sentences for each of our ten phenomena. These candidates were then hand-vetted in sequence by two annot</context>
</contexts>
<marker>Flickinger, Oepen, Ytrestøl, 2010</marker>
<rawString>Dan Flickinger, Stephan Oepen, and Gisle Ytrestøl. 2010. WikiWoods. Syntacto-semantic annotation for English Wikipedia. In Proceedings of the 6th International Conference on Language Resources and Evaluation, Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Flickinger</author>
</authors>
<title>On building a more efficient grammar by exploiting types.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<booktitle>(Special Issue on Efficient Processing with HPSG):15 –</booktitle>
<volume>6</volume>
<issue>1</issue>
<pages>28</pages>
<contexts>
<context position="4450" citStr="Flickinger (2000)" startWordPosition="669" endWordPosition="670">7–31, 2011. c�2011 Association for Computational Linguistics unlabeled training data. A related dimension of variation is the type of representations manipulated by the parser. We briefly review some representative examples along these dimensions, as these help to position the parsers we subsequently evaluate.1 2.1 Approaches to parsing Source of linguistic knowledge At one end of this dimension, we find systems whose linguistic knowledge is encoded in hand-crafted rules and lexical entries; for English, the ParGram XLE system (Riezler et al., 2002) and DELPH-IN English Resource Grammar (ERG; Flickinger (2000))—each reflecting decades of continuous development—achieve broad coverage of open-domain running text, for example. At the other end of this dimension, we find fully unsupervised approaches (Clark, 2001; Klein and Manning, 2004), where the primary source of linguistic knowledge is co-occurrence patterns of words in unannotated text. As Haghighi and Klein (2006) show, augmenting this knowledge with handcrafted prototype “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all of their linguistic knowledge from annotated treebanks, e.g. the</context>
</contexts>
<marker>Flickinger, 2000</marker>
<rawString>Dan Flickinger. 2000. On building a more efficient grammar by exploiting types. Natural Language Engineering, 6 (1) (Special Issue on Efficient Processing with HPSG):15 – 28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sumukh Ghodke</author>
<author>Steven Bird</author>
</authors>
<title>Fast query for large treebanks. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>267--275</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<marker>Ghodke, Bird, 2010</marker>
<rawString>Sumukh Ghodke and Steven Bird. 2010. Fast query for large treebanks. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 267–275, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Prototype-driven grammar induction.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>881--888</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="4814" citStr="Haghighi and Klein (2006)" startWordPosition="720" endWordPosition="723">tic knowledge At one end of this dimension, we find systems whose linguistic knowledge is encoded in hand-crafted rules and lexical entries; for English, the ParGram XLE system (Riezler et al., 2002) and DELPH-IN English Resource Grammar (ERG; Flickinger (2000))—each reflecting decades of continuous development—achieve broad coverage of open-domain running text, for example. At the other end of this dimension, we find fully unsupervised approaches (Clark, 2001; Klein and Manning, 2004), where the primary source of linguistic knowledge is co-occurrence patterns of words in unannotated text. As Haghighi and Klein (2006) show, augmenting this knowledge with handcrafted prototype “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all of their linguistic knowledge from annotated treebanks, e.g. the Penn Treebank (PTB), which encodes “surface grammatical analysis” (Marcus et al., 1993). Such approaches include those that directly (and exclusively) use the information in the treebank (e.g. Charniak (1997), Collins (1999), Petrov et al. (2006), inter alios) as well as those that complement treebank structures with some amount of hand-coded linguistic knowled</context>
</contexts>
<marker>Haghighi, Klein, 2006</marker>
<rawString>Aria Haghighi and Dan Klein. 2006. Prototype-driven grammar induction. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 881–888, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="5500" citStr="Hockenmaier and Steedman (2007)" startWordPosition="823" endWordPosition="826">type “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all of their linguistic knowledge from annotated treebanks, e.g. the Penn Treebank (PTB), which encodes “surface grammatical analysis” (Marcus et al., 1993). Such approaches include those that directly (and exclusively) use the information in the treebank (e.g. Charniak (1997), Collins (1999), Petrov et al. (2006), inter alios) as well as those that complement treebank structures with some amount of hand-coded linguistic knowledge (e.g. O’Donovan et al. (2004), Miyao et al. (2004), Hockenmaier and Steedman (2007), inter alios). Another hybrid in terms of its acquisition of linguistic knowledge is the RASP system of Briscoe et al. (2006), combining a hand-coded grammar over PoS tag sequences with a probabilistic tagger and statistical syntactic disambiguation. Design of representations Approaches to parsing also differ fundamentally in the style of representation assigned to strings. These vary both in their 1Additional sources of variation among extant parsing technologies include (a) the behavior with respect to ungrammatical inputs and (b) the relationship between probabilistic and symbolic knowledg</context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2007. CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank. Computational Linguistics, 33(3):355–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rodney Huddleston</author>
<author>Geoffrey K Pullum</author>
</authors>
<title>The Cambridge Grammar of the English Language.</title>
<date>2002</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="19781" citStr="Huddleston and Pullum (2002" startWordPosition="3050" endWordPosition="3053"> and heavy-NP shift (Arnold et al., 2000), in which “heavy” constituents appear further to the right in the string, allow for adjunct-argument order in a minority of cases. We hypothesize that the relative infrequency of this construction will lead parsers to prefer incorrect analyses (wherein the adjunct is picked up as a complement, the complement as an adjunct or the structure differs entirely) unless they have access to linguistic knowledge providing constraints on possible and probable complementation patterns for the head. Finally, we turn to raising and control verbs (‘control’) (e.g., Huddleston and Pullum (2002, ch. 14)). These verbs select for an infinitival VP complement and stipulate that another of their arguments (subject or direct object in the examples we explore) is identified with the unrealized subject position of the infinitival VP. Here it is the dependency between the infinitval VP and the NP argument of the “upstairs” verb which we expect to be particularly subtle. Getting this right requires specific lexical knowledge about which verbs take these complementation patterns. This lexical knowledge needs to be represented in such a way that it can be used robustly even in the case of pass</context>
</contexts>
<marker>Huddleston, Pullum, 2002</marker>
<rawString>Rodney Huddleston and Geoffrey K. Pullum. 2002. The Cambridge Grammar of the English Language. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Extended constituent-to-dependency conversion for English. In</title>
<date>2007</date>
<booktitle>In Proceedings of NODALIDA 2007,</booktitle>
<pages>105--112</pages>
<location>Tartu, Estonia.</location>
<contexts>
<context position="6753" citStr="Johansson and Nugues (2007)" startWordPosition="1001" endWordPosition="1004">sers with a hand-coded grammar at their core typically also incorporate an automatically trained probabilistic disambiguation component. formal nature and the “granularity” of linguistic information (i.e. the number of distinctions assumed), encompassing variants of constituent structure, syntactic dependencies, or logical-form representations of semantics. Parser interface representations range between the relatively simple (e.g. phrase structure trees with a limited vocabulary of node labels as in the PTB, or syntactic dependency structures with a limited vocabulary of relation labels as in Johansson and Nugues (2007)) and the relatively complex, as for example elaborate syntactico-semantic analyses produced by the ParGram or DELPH-IN grammars. There tends to be a correlation between the methodology used in the acquisition of linguistic knowledge and the complexity of representations: in the creation of a mostly hand-crafted treebank like the PTB, representations have to be simple enough for human annotators to reliably manipulate. Deriving more complex representations typically presupposes further computational support, often involving some hand-crafted linguistic knowledge—which can take the form of mapp</context>
<context position="9711" citStr="Johansson and Nugues, 2007" startWordPosition="1451" endWordPosition="1454">derived ning tree that connects all the words in a sentence parsers for its support for grammatical relations as into a dependency tree. The model we evaluate an alternate interface representation. is the second-order projective model trained on the Charniak&amp;Johnson Reranking Parser (Char- same WSJ corpus, where the original PTB phrase niak and Johnson, 2005) is a two-stage PCFG parser structure annotations were first converted into dewith a lexicalized generative model for the first- pendencies, as established in the CoNLL shared task stage, and a discriminative MaxEnt reranker for the 2009 (Johansson and Nugues, 2007). second-stage. The models we evaluate are also XLE/ParGram (Riezler et al., 2002, see also trained on sections 02–21 of the WSJ. Top-50 read- Cahill et al., 2008) applies a hand-built Lexical ings were used for the reranking stage. The output Functional Grammar for English and a stochastic constituent trees were then converted into Stanford parse selection model. For our evaluation, we used Dependencies. According to Cer et al. (2010), this the Nov 4, 2010 release of XLE and the Nov 25, combination gives the best parsing accuracy in terms 2009 release of the ParGram English grammar, with of S</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Richard Johansson and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for English. In In Proceedings of NODALIDA 2007, pages 105–112, Tartu, Estonia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ron Kaplan</author>
<author>Stefan Riezler</author>
<author>Tracy H King</author>
<author>John T Maxwell Alex Vasserman</author>
<author>Richard Crouch</author>
</authors>
<title>Speed and accuracy in shallow and deep stochastic parsing.</title>
<date>2004</date>
<booktitle>HLT-NAACL 2004: Main Proceedings,</booktitle>
<pages>97--104</pages>
<editor>In Susan Dumais, Daniel Marcu, and Salim Roukos, editors,</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Boston, Massachusetts, USA,</location>
<marker>Kaplan, Riezler, King, Vasserman, Crouch, 2004</marker>
<rawString>Ron Kaplan, Stefan Riezler, Tracy H King, John T Maxwell III, Alex Vasserman, and Richard Crouch. 2004. Speed and accuracy in shallow and deep stochastic parsing. In Susan Dumais, Daniel Marcu, and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings, pages 97–104, Boston, Massachusetts, USA, May. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2003</date>
<booktitle>In Advances in Neural Information Processing Systems 15,</booktitle>
<pages>3--10</pages>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="8146" citStr="Klein and Manning, 2003" startWordPosition="1215" endWordPosition="1218">ating the parse structures in the first place (i.e. a computational grammar), as for example in the treebanks of van der Beek et al. (2002) or Oepen et al. (2004).2 In principle, one might expect that richer representations allow parsers to capture complex syntactic or semantic dependencies more explicitly. At the same time, such “deeper” relations may still be recoverable (to some degree) from comparatively simple parser outputs, as demonstrated for unbounded dependency extraction from strictly local syntactic dependency trees by Nivre et al. (2010). 2.2 An armada of parsers Stanford Parser (Klein and Manning, 2003) is a probabilistic parser which can produce both phrase structure trees and grammatical relations (syntactic dependencies). The parsing model we evaluate is the 2A noteworthy exception to this correlation is the annotated corpus of Zettlemoyer and Collins (2005), which pairs surface strings from the realm of natural language database interfaces directly with semantic representations in lambda calculus. These were hand-written on the basis of database query statements distributed with the original datasets. 398 English factored model which combines the prefer- or syntactic disambiguation compo</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Fast exact inference with a factored model for natural language parsing. In Advances in Neural Information Processing Systems 15, pages 3–10, Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher Manning</author>
</authors>
<title>Corpusbased induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics,</booktitle>
<pages>478--485</pages>
<location>Barcelona,</location>
<contexts>
<context position="4679" citStr="Klein and Manning, 2004" startWordPosition="700" endWordPosition="703">s along these dimensions, as these help to position the parsers we subsequently evaluate.1 2.1 Approaches to parsing Source of linguistic knowledge At one end of this dimension, we find systems whose linguistic knowledge is encoded in hand-crafted rules and lexical entries; for English, the ParGram XLE system (Riezler et al., 2002) and DELPH-IN English Resource Grammar (ERG; Flickinger (2000))—each reflecting decades of continuous development—achieve broad coverage of open-domain running text, for example. At the other end of this dimension, we find fully unsupervised approaches (Clark, 2001; Klein and Manning, 2004), where the primary source of linguistic knowledge is co-occurrence patterns of words in unannotated text. As Haghighi and Klein (2006) show, augmenting this knowledge with handcrafted prototype “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all of their linguistic knowledge from annotated treebanks, e.g. the Penn Treebank (PTB), which encodes “surface grammatical analysis” (Marcus et al., 1993). Such approaches include those that directly (and exclusively) use the information in the treebank (e.g. Charniak (1997), Collins (1999), Pe</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>Dan Klein and Christopher Manning. 2004. Corpusbased induction of syntactic structure: Models of dependency and constituency. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics, pages 478–485, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>A dependency-based method for evaluating broad-coverage parsers.</title>
<date>1995</date>
<booktitle>In Proceedings of IJCAI-95,</booktitle>
<pages>1420--1425</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="38463" citStr="Lin (1995)" startWordPosition="6001" endWordPosition="6002">d keeping We have presented a detailed construction-focused an annotation scheme simple enough that it can be evaluation of seven parsers over 10 phenomena, reliably produced by humans, in the case of hand- with 1000 examples drawn from English Wikipedia. annotation. Gauging recall of such “deep” dependencies, in our 7 Related Work view, can serve as a proxy for downstream proThis paper builds on a growing body of work which cessing involving semantic interpretation of parser goes beyond (un)labeled bracketing in parser evalua- outputs. Our annotations and automated evaluation tion, including Lin (1995), Carroll et al. (1998), Ka- script are provided in the supplementary materials, plan et al. (2004), Rimell et al. (2009), and Nivre et for full replicability. Our results demonstrate that al. (2010). Most closely related are the latter two of significant opportunities remain for parser improvethe above, as we adopt their “construction-focused ment, and highlight specific challenges that remain parser evaluation methodology”. invisible in aggregate parser evaluation (e.g. ParseThere are several methodological differences be- val or overall dependency accuracy). These results tween our work and</context>
</contexts>
<marker>Lin, 1995</marker>
<rawString>Dekang Lin. 1995. A dependency-based method for evaluating broad-coverage parsers. In Proceedings of IJCAI-95, pages 1420–1425, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
</authors>
<title>Verbal gerunds as mixed categories</title>
<date>2000</date>
<booktitle>The Nature and Function of Syntactic Categories,</booktitle>
<pages>133--166</pages>
<editor>in HPSG. In Robert Borsley, editor,</editor>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<contexts>
<context position="18844" citStr="Malouf, 2000" startWordPosition="2905" endWordPosition="2906">unds (‘vger’), the interleaving of arguments and adjuncts (‘argadj’) and raising/control (‘control’) constructions. (8) vger: Accessing the website without the “www” subdomain returned a copy of the main site for “EP.net”. (9) argadj: The story shows, through flashbacks, the different histories of the characters. (10) control: Alfred “retired” in 1957 at age 60 but continued to paint full time. In a verbal gerund, the -ing form a verb retains verbal properties (e.g., being able to take NP complements, rather than only PP complements) but heads a phrase that fills an NP position in the syntax (Malouf, 2000). Since gerunds have the same morphology as present participle VPs, their role in the larger clause is susceptible to misanalysis. The argadj examples are of interest because English typically prefers to have direct objects directly adjacent to the selecting verb. Nonetheless, phenomena such as parentheticals and heavy-NP shift (Arnold et al., 2000), in which “heavy” constituents appear further to the right in the string, allow for adjunct-argument order in a minority of cases. We hypothesize that the relative infrequency of this construction will lead parsers to prefer incorrect analyses (whe</context>
</contexts>
<marker>Malouf, 2000</marker>
<rawString>Robert Malouf. 2000. Verbal gerunds as mixed categories in HPSG. In Robert Borsley, editor, The Nature and Function of Syntactic Categories, pages 133–166. Academic Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="5138" citStr="Marcus et al., 1993" startWordPosition="769" endWordPosition="772">verage of open-domain running text, for example. At the other end of this dimension, we find fully unsupervised approaches (Clark, 2001; Klein and Manning, 2004), where the primary source of linguistic knowledge is co-occurrence patterns of words in unannotated text. As Haghighi and Klein (2006) show, augmenting this knowledge with handcrafted prototype “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all of their linguistic knowledge from annotated treebanks, e.g. the Penn Treebank (PTB), which encodes “surface grammatical analysis” (Marcus et al., 1993). Such approaches include those that directly (and exclusively) use the information in the treebank (e.g. Charniak (1997), Collins (1999), Petrov et al. (2006), inter alios) as well as those that complement treebank structures with some amount of hand-coded linguistic knowledge (e.g. O’Donovan et al. (2004), Miyao et al. (2004), Hockenmaier and Steedman (2007), inter alios). Another hybrid in terms of its acquisition of linguistic knowledge is the RASP system of Briscoe et al. (2006), combining a hand-coded grammar over PoS tag sequences with a probabilistic tagger and statistical syntactic di</context>
<context position="20534" citStr="Marcus et al., 1993" startWordPosition="3176" endWordPosition="3179">ct in the examples we explore) is identified with the unrealized subject position of the infinitival VP. Here it is the dependency between the infinitval VP and the NP argument of the “upstairs” verb which we expect to be particularly subtle. Getting this right requires specific lexical knowledge about which verbs take these complementation patterns. This lexical knowledge needs to be represented in such a way that it can be used robustly even in the case of passives, relative clauses, etc.5 3.5 Penn Treebank representations We investigated the representation of these 10 phenomena in the PTB (Marcus et al., 1993) in two steps: First we explored the PTB’s annotation guidelines (Bies et al., 1995) to determine how the relevant dependencies were intended to be represented. We then used Ghodke and Bird’s (2010) Treebank Search to find examples of the intended annotations as well as potential examples of the phenomena annotated differently, to get a sense of the consistency of the annotation from both precision and recall perspectives. In this study, we take the phrase structure trees of the PTB to represent dependencies based on reasonable identification of heads. The barerel, vpart, and absol phenomena a</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajic</author>
</authors>
<title>Non-Projective Dependency Parsing using Spanning Tree Algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2005 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>523--530</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="8836" citStr="McDonald et al., 2005" startWordPosition="1314" endWordPosition="1317">rees and grammatical relations (syntactic dependencies). The parsing model we evaluate is the 2A noteworthy exception to this correlation is the annotated corpus of Zettlemoyer and Collins (2005), which pairs surface strings from the realm of natural language database interfaces directly with semantic representations in lambda calculus. These were hand-written on the basis of database query statements distributed with the original datasets. 398 English factored model which combines the prefer- or syntactic disambiguation components. ences of unlexicalized PCFG phrase structures and MSTParser (McDonald et al., 2005) is a dataof lexical dependencies, trained on sections 02–21 driven dependency parser. The parser uses an edgeof the WSJ portion of the PTB. We chose Stanford factored model and searches for a maximal spanParser from among the state-of-the-art PTB-derived ning tree that connects all the words in a sentence parsers for its support for grammatical relations as into a dependency tree. The model we evaluate an alternate interface representation. is the second-order projective model trained on the Charniak&amp;Johnson Reranking Parser (Char- same WSJ corpus, where the original PTB phrase niak and Johns</context>
</contexts>
<marker>McDonald, Pereira, Ribarov, Hajic, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajic. 2005. Non-Projective Dependency Parsing using Spanning Tree Algorithms. In Proceedings of the 2005 Conference on Empirical Methods in Natural Language Processing, pages 523–530, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Takashi Ninomiya</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Corpus-oriented grammar development for acquiring a Head-driven Phrase Structure Grammar from the Penn Treebank.</title>
<date>2004</date>
<booktitle>In Proceedings of the 1st International Joint Conference on Natural Language Processing,</booktitle>
<pages>684--693</pages>
<location>Hainan Island, China.</location>
<contexts>
<context position="5467" citStr="Miyao et al. (2004)" startWordPosition="819" endWordPosition="822">ith handcrafted prototype “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all of their linguistic knowledge from annotated treebanks, e.g. the Penn Treebank (PTB), which encodes “surface grammatical analysis” (Marcus et al., 1993). Such approaches include those that directly (and exclusively) use the information in the treebank (e.g. Charniak (1997), Collins (1999), Petrov et al. (2006), inter alios) as well as those that complement treebank structures with some amount of hand-coded linguistic knowledge (e.g. O’Donovan et al. (2004), Miyao et al. (2004), Hockenmaier and Steedman (2007), inter alios). Another hybrid in terms of its acquisition of linguistic knowledge is the RASP system of Briscoe et al. (2006), combining a hand-coded grammar over PoS tag sequences with a probabilistic tagger and statistical syntactic disambiguation. Design of representations Approaches to parsing also differ fundamentally in the style of representation assigned to strings. These vary both in their 1Additional sources of variation among extant parsing technologies include (a) the behavior with respect to ungrammatical inputs and (b) the relationship between pr</context>
<context position="10419" citStr="Miyao et al., 2004" startWordPosition="1566" endWordPosition="1569">so trained on sections 02–21 of the WSJ. Top-50 read- Cahill et al., 2008) applies a hand-built Lexical ings were used for the reranking stage. The output Functional Grammar for English and a stochastic constituent trees were then converted into Stanford parse selection model. For our evaluation, we used Dependencies. According to Cer et al. (2010), this the Nov 4, 2010 release of XLE and the Nov 25, combination gives the best parsing accuracy in terms 2009 release of the ParGram English grammar, with of Stanford dependencies on the PTB. c-structure pruning turned off and resource limitaEnju (Miyao et al., 2004) is a probabilistic HPSG tions set to the maximum possible to allow for exparser, combining a hand-crafted core grammar with haustive search. In particular, we are evaluating the automatically acquired lexical types from the PTB.3 f-structures output by the system. The model we evaluate is trained on the same ma- Each parser, of course, has its own requirements terial from the WSJ sections of the PTB, but the regarding preprocessing of text, especially tokenizatreebank is first semi-automatically converted into tion. We customized the tokenization to each parser, HPSG derivations, and the anno</context>
</contexts>
<marker>Miyao, Ninomiya, Tsujii, 2004</marker>
<rawString>Yusuke Miyao, Takashi Ninomiya, and Jun’ichi Tsujii. 2004. Corpus-oriented grammar development for acquiring a Head-driven Phrase Structure Grammar from the Penn Treebank. In Proceedings of the 1st International Joint Conference on Natural Language Processing, pages 684–693, Hainan Island, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Laura Rimell</author>
<author>Ryan McDonald</author>
<author>Carlos G´omez Rodr´ıguez</author>
</authors>
<title>Evaluation of dependency parsers on unbounded dependencies.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>833--841</pages>
<location>Beijing, China.</location>
<marker>Nivre, Rimell, McDonald, Rodr´ıguez, 2010</marker>
<rawString>Joakim Nivre, Laura Rimell, Ryan McDonald, and Carlos G´omez Rodr´ıguez. 2010. Evaluation of dependency parsers on unbounded dependencies. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 833–841, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruth O’Donovan</author>
<author>Michael Burke</author>
<author>Aoife Cahill</author>
<author>Josef Van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Large-scale induction and evaluation of lexical resources from the penn-ii treebank.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics,</booktitle>
<pages>367--374</pages>
<location>Barcelona,</location>
<marker>O’Donovan, Burke, Cahill, Van Genabith, Way, 2004</marker>
<rawString>Ruth O’Donovan, Michael Burke, Aoife Cahill, Josef Van Genabith, and Andy Way. 2004. Large-scale induction and evaluation of lexical resources from the penn-ii treebank. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics, pages 367–374, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Oepen</author>
<author>Daniel Flickinger</author>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
</authors>
<title>LinGO Redwoods. A rich and dynamic treebank for HPSG.</title>
<date>2004</date>
<journal>Journal of Research on Language and Computation,</journal>
<volume>2</volume>
<issue>4</issue>
<contexts>
<context position="7684" citStr="Oepen et al. (2004)" startWordPosition="1145" endWordPosition="1148">d treebank like the PTB, representations have to be simple enough for human annotators to reliably manipulate. Deriving more complex representations typically presupposes further computational support, often involving some hand-crafted linguistic knowledge—which can take the form of mappings from PTB-like representations to “richer” grammatical frameworks (as in the line of work by O’Donovan et al. (2004), and others; see above), or can be rules for creating the parse structures in the first place (i.e. a computational grammar), as for example in the treebanks of van der Beek et al. (2002) or Oepen et al. (2004).2 In principle, one might expect that richer representations allow parsers to capture complex syntactic or semantic dependencies more explicitly. At the same time, such “deeper” relations may still be recoverable (to some degree) from comparatively simple parser outputs, as demonstrated for unbounded dependency extraction from strictly local syntactic dependency trees by Nivre et al. (2010). 2.2 An armada of parsers Stanford Parser (Klein and Manning, 2003) is a probabilistic parser which can produce both phrase structure trees and grammatical relations (syntactic dependencies). The parsing m</context>
</contexts>
<marker>Oepen, Flickinger, Toutanova, Manning, 2004</marker>
<rawString>Stephan Oepen, Daniel Flickinger, Kristina Toutanova, and Christopher D. Manning. 2004. LinGO Redwoods. A rich and dynamic treebank for HPSG. Journal of Research on Language and Computation, 2(4):575–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>433--440</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="5297" citStr="Petrov et al. (2006)" startWordPosition="792" endWordPosition="795">4), where the primary source of linguistic knowledge is co-occurrence patterns of words in unannotated text. As Haghighi and Klein (2006) show, augmenting this knowledge with handcrafted prototype “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all of their linguistic knowledge from annotated treebanks, e.g. the Penn Treebank (PTB), which encodes “surface grammatical analysis” (Marcus et al., 1993). Such approaches include those that directly (and exclusively) use the information in the treebank (e.g. Charniak (1997), Collins (1999), Petrov et al. (2006), inter alios) as well as those that complement treebank structures with some amount of hand-coded linguistic knowledge (e.g. O’Donovan et al. (2004), Miyao et al. (2004), Hockenmaier and Steedman (2007), inter alios). Another hybrid in terms of its acquisition of linguistic knowledge is the RASP system of Briscoe et al. (2006), combining a hand-coded grammar over PoS tag sequences with a probabilistic tagger and statistical syntactic disambiguation. Design of representations Approaches to parsing also differ fundamentally in the style of representation assigned to strings. These vary both in </context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 433–440, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Tracy H King</author>
<author>Ronald M Kaplan</author>
<author>Richard Crouch</author>
<author>John T Maxwell</author>
<author>Mark Johnson</author>
</authors>
<title>Parsing the Wall Street Journal using a Lexical-Functional Grammar and discriminative estimation techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Meeting of the Association for Computational Linguistics,</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="4388" citStr="Riezler et al., 2002" startWordPosition="658" endWordPosition="662">anguage Processing, pages 397–408, Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics unlabeled training data. A related dimension of variation is the type of representations manipulated by the parser. We briefly review some representative examples along these dimensions, as these help to position the parsers we subsequently evaluate.1 2.1 Approaches to parsing Source of linguistic knowledge At one end of this dimension, we find systems whose linguistic knowledge is encoded in hand-crafted rules and lexical entries; for English, the ParGram XLE system (Riezler et al., 2002) and DELPH-IN English Resource Grammar (ERG; Flickinger (2000))—each reflecting decades of continuous development—achieve broad coverage of open-domain running text, for example. At the other end of this dimension, we find fully unsupervised approaches (Clark, 2001; Klein and Manning, 2004), where the primary source of linguistic knowledge is co-occurrence patterns of words in unannotated text. As Haghighi and Klein (2006) show, augmenting this knowledge with handcrafted prototype “seeds” can bring strong improvements. Somewhere between these poles, a broad class of parsers take some or all of</context>
<context position="9792" citStr="Riezler et al., 2002" startWordPosition="1463" endWordPosition="1466">grammatical relations as into a dependency tree. The model we evaluate an alternate interface representation. is the second-order projective model trained on the Charniak&amp;Johnson Reranking Parser (Char- same WSJ corpus, where the original PTB phrase niak and Johnson, 2005) is a two-stage PCFG parser structure annotations were first converted into dewith a lexicalized generative model for the first- pendencies, as established in the CoNLL shared task stage, and a discriminative MaxEnt reranker for the 2009 (Johansson and Nugues, 2007). second-stage. The models we evaluate are also XLE/ParGram (Riezler et al., 2002, see also trained on sections 02–21 of the WSJ. Top-50 read- Cahill et al., 2008) applies a hand-built Lexical ings were used for the reranking stage. The output Functional Grammar for English and a stochastic constituent trees were then converted into Stanford parse selection model. For our evaluation, we used Dependencies. According to Cer et al. (2010), this the Nov 4, 2010 release of XLE and the Nov 25, combination gives the best parsing accuracy in terms 2009 release of the ParGram English grammar, with of Stanford dependencies on the PTB. c-structure pruning turned off and resource limi</context>
</contexts>
<marker>Riezler, King, Kaplan, Crouch, Maxwell, Johnson, 2002</marker>
<rawString>Stefan Riezler, Tracy H. King, Ronald M. Kaplan, Richard Crouch, John T. Maxwell III, and Mark Johnson. 2002. Parsing the Wall Street Journal using a Lexical-Functional Grammar and discriminative estimation techniques. In Proceedings of the 40th Meeting of the Association for Computational Linguistics, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Rimell</author>
<author>Stephen Clark</author>
<author>Mark Steedman</author>
</authors>
<title>Unbounded dependency recovery for parser evaluation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>813--821</pages>
<institution>Singapore. Association for Computational Linguistics.</institution>
<contexts>
<context position="2832" citStr="Rimell et al. (2009)" startWordPosition="427" endWordPosition="430"> occur with reasonably high frequency in running text and (b) have the potential to shed some light on the depths of linguistic analysis. We quantify the frequency of these constructions in the English Wikipedia, then annotate 100 example sentences for each phenomenon with gold-standard dependencies reflecting core properties of the phenomena of interest. This gold standard is then used to estimate the recall of these dependencies by seven commonly used parsers, providing the basis for a qualitative discussion of the state of the art in parsing for English. In this work, we answer the call by Rimell et al. (2009) for “construction-focused parser evaluation”, extending and complementing their work in several respects: (i) we investigate both local and non-local dependencies which prove to be challenging for many existing state-of-the-art parsers; (ii) we investigate a wider range of linguistic phenomena, each accompanied with an in-depth discussion of relevant properties; and (iii) we draw our data from the 50-million sentence English Wikipedia, which is more varied and a thousand times larger than the venerable WSJ corpus, to explore a more level and ambitious playing field for parser comparison. 2 Ba</context>
<context position="13696" citStr="Rimell et al. (2009)" startWordPosition="2087" endWordPosition="2090">s not included in our evaluation, since it was used in the extraction of the original examples and thus cannot be fairly evaluated. 399 in the following examples:4 (1) barerel: This is the second time in a row Australia has lost their home tri-nations’ series. (2) tough: Original copies are very hard to find. (3) rnr: Il´uvatar, as his names imply, exists before and independently of all else. While the majority of our phenomena involve local dependencies, we include these long-distance dependency types because they are challenging for parsers and enable more direct comparison with the work of Rimell et al. (2009), who also address right node raising and bare relatives. Our barerel category corresponds to their “object reduced relative” category with the difference that we also include adverb relatives, where the head noun functions as a modifier within the relative clause, as does time in (1). In contrast, our rnr category is somewhat narrower than Rimell et al. (2009)’s “right node raising” category: where they include raised modifiers, we restrict our category to raised complements. Part of the difficulty in retrieving long-distance dependencies is that the so-called extraction site is not overtly m</context>
<context position="25840" citStr="Rimell et al. (2009)" startWordPosition="4044" endWordPosition="4047">erly basic (e.g. plain V + V coordination, which the ERG treats as rnr) or inappropriately complex (e.g. non-constituent coordination obscuring the interleaving of arguments and adjuncts) were also discarded at this step. Table 1 summarizes relative frequencies of each phenomenon in about 47 million parsed Wikipedia sentences, as well as the total size of the candidate sets inspected. For the control and tough phenomena hardly any filtering for complexity was applied, hence these can serve as indicators of the rate of genuine false positives. For phenomena that partially overlap with those of Rimell et al. (2009), it appears our frequency estimates are comparable to what they report for the Brown Corpus (but not the WSJ portion of the PTB). 4.2 Annotation format We annotated up to two dependency triples per phenomenon instance, identifying the heads and dependents by the surface form of the head words in the sentence suffixed with a number indicating word position (see Table 2).6 Some strings contain more than one instance of the phenomenon they illustrate; in these cases, multiple sets of dependencies are 6As the parsers differ in tokenization strategies, our evaluation script treats these position I</context>
<context position="36510" citStr="Rimell et al. (2009)" startWordPosition="5695" endWordPosition="5698">der dependency in barerel. Only XLE scored higher than 10% on the second dependency for ned and higher than 50% for itexpl. 6 Discussion From the results in Fig. 1, it is clear that even the best of these parsers fail to correctly identify a large number of relevant dependencies associated with linguistic phenomena that occur with reasonable frequency in the Wikipedia. Each of the parsers attempts with some success to analyze each of these phenomena, reinforcing the claim of relevance, but they vary widely across phenomena. For the two longdistance phenomena that overlap with those studied in Rimell et al. (2009), our results are comparable.9 Our evaluation over Wikipedia examples thus shows the same relative lack of success in recovering longdistance dependencies that they found for WSJ sentences. The systems did better on relatively wellstudied phenomena including control, vger, and vpart, but had less success with the rest, even though all but two of those remaining phenomena involve syntactically local dependencies (as indicated in Table 3). Successful identification of the dependencies in these phenomena would, we hypothesize, benefit from richer (or deeper) linguistic information when parsing, w</context>
<context position="38584" citStr="Rimell et al. (2009)" startWordPosition="6019" endWordPosition="6022">valuation of seven parsers over 10 phenomena, reliably produced by humans, in the case of hand- with 1000 examples drawn from English Wikipedia. annotation. Gauging recall of such “deep” dependencies, in our 7 Related Work view, can serve as a proxy for downstream proThis paper builds on a growing body of work which cessing involving semantic interpretation of parser goes beyond (un)labeled bracketing in parser evalua- outputs. Our annotations and automated evaluation tion, including Lin (1995), Carroll et al. (1998), Ka- script are provided in the supplementary materials, plan et al. (2004), Rimell et al. (2009), and Nivre et for full replicability. Our results demonstrate that al. (2010). Most closely related are the latter two of significant opportunities remain for parser improvethe above, as we adopt their “construction-focused ment, and highlight specific challenges that remain parser evaluation methodology”. invisible in aggregate parser evaluation (e.g. ParseThere are several methodological differences be- val or overall dependency accuracy). These results tween our work and that of Rimell et al. First, we suggest that further progress will depend on traindraw our evaluation data from a much l</context>
<context position="40449" citStr="Rimell et al. (2009)" startWordPosition="6307" endWordPosition="6310">xample, the granularity of linguistic analyprecise broad-coverage grammar to identify candi- sis, average accuracy, cost of computation, or ease date examples. This allows us to include both local of adaptation. Our proposal is not to substitute and non-local dependencies not represented or not construction-focused evaluation on Wikipedia data reliably encoded in the PTB, enabling us to evalu- for widely used aggregate metrics and reference corate parser performance with more precision over a pora, but rather to augment such best practices in wider range of linguistic phenomena. the spirit of Rimell et al. (2009) and expand the These methodological innovations bring two em- range of phenomena considered in such evaluations. pirical results. The first is qualitative: Where previ- Across frameworks and traditions (and in principle ous work showed that overall Parseval numbers hide languages), it is of vital importance to be able to difficulties with long-distance dependencies, our re- evaluate the quality of parsing (and grammar inducsults show that there are multiple kinds of reason- tion) algorithms in a maximally informative manner. ably frequent local dependencies which are also dif- Acknowledgments</context>
</contexts>
<marker>Rimell, Clark, Steedman, 2009</marker>
<rawString>Laura Rimell, Stephen Clark, and Mark Steedman. 2009. Unbounded dependency recovery for parser evaluation. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 813–821, Singapore. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Sag</author>
<author>Timothy Baldwin</author>
<author>Francis Bond</author>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
</authors>
<title>Multiword expressions. A pain in the neck for NLP.</title>
<date>2002</date>
<booktitle>Computational Linguistics and Intelligent Text Processing,</booktitle>
<volume>2276</volume>
<pages>189--206</pages>
<editor>In Alexander Gelbukh, editor,</editor>
<publisher>Springer,</publisher>
<location>Berlin, Germany.</location>
<contexts>
<context position="16818" citStr="Sag et al., 2002" startWordPosition="2578" endWordPosition="2581"> it should be detected and discarded in reference resolution. We hypothesize that detecting expletive it requires encoding linguistic knowledge about its licensers. The other non-dependency we explore is between the particle in verb-particle constructions and the direct object. Since English particles are almost always homophonous with prepositions, when the object of the verb-particle pair follows the particle, there will always be a competing analysis which analyses the sequence as V+PP rather than V+particle+NP. Furthermore, since verb-particle pairs often have non-compositional semantics (Sag et al., 2002), misanalyzing these constructions could be costly to downstream components. 3.3 Phrasal modifiers Our next category concerns modifier phrases: (6) ned: Light colored glazes also have softening effects when painted over dark or bright images. (7) absol: The format consisted of 12 games, each team facing the other teams twice. The first, (‘ned’), is a pattern which to our knowledge has not been named in the literature, where a noun takes the typically verbal -ed ending, is modified by another noun or adjective, and functions as a modifier or a predicate. We believe this phenomenon to be interes</context>
</contexts>
<marker>Sag, Baldwin, Bond, Copestake, Flickinger, 2002</marker>
<rawString>Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copestake, and Dan Flickinger. 2002. Multiword expressions. A pain in the neck for NLP. In Alexander Gelbukh, editor, Computational Linguistics and Intelligent Text Processing, volume 2276 of Lecture Notes in Computer Science, pages 189–206. Springer, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vadas</author>
<author>James R Curran</author>
</authors>
<title>Parsing noun phrase structure with CCG.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL08: HLT,</booktitle>
<pages>335--343</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="24199" citStr="Vadas and Curran (2008)" startWordPosition="3775" endWordPosition="3778">these out before training, as is common practice, will not benefit from the information that is in the PTB. Our purpose here is not to criticize the PTB, which has been a tremendously important resource to the field. Rather, we have two aims: The first is to provide context for the evaluation of PTB-derived parsers on these phenomena. The second is to highlight the difficulty of producing consistent annotations of any complexity as well as the hurdles faced by a hand-annotation approach when attempting to scale a resource to more complex representations and/or additional phenomena (though cf. Vadas and Curran (2008) on improving PTB representations). 4 Methodology 4.1 Data extraction We processed 900 million tokens of Wikipedia text using the October 2010 release of the ERG, following the work of the WikiWoods project (Flickinger et al., 2010). Using the top-ranked ERG derivation trees as annotations over this corpus and simple patterns using names of ERG-specific construcPhenomenon Frequency Candidates barerel 2.12% 546 tough 0.07% 175 rnr 0.69% 1263 itexpl 0.13% 402 vpart 4.07% 765 ned 1.18% 349 absol 0.51% 963 vger 5.16% 679 argadj 3.60% 1346 control 3.78% 124 Table 1: Relative frequencies of phenomen</context>
</contexts>
<marker>Vadas, Curran, 2008</marker>
<rawString>David Vadas and James R. Curran. 2008. Parsing noun phrase structure with CCG. In Proceedings of ACL08: HLT, pages 335–343, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L van der Beek</author>
<author>Gosse Bouma</author>
<author>Robert Malouf</author>
<author>Gertjan van Noord</author>
</authors>
<title>The Alpino Dependency Treebank.</title>
<date>2002</date>
<booktitle>In Mariet Theune, Anton Nijholt, and Hendri Hondorp, editors, Computational Linguistics in the Netherlands,</booktitle>
<location>Amsterdam, The Netherlands. Rodopi.</location>
<marker>van der Beek, Bouma, Malouf, van Noord, 2002</marker>
<rawString>L. van der Beek, Gosse Bouma, Robert Malouf, and Gertjan van Noord. 2002. The Alpino Dependency Treebank. In Mariet Theune, Anton Nijholt, and Hendri Hondorp, editors, Computational Linguistics in the Netherlands, Amsterdam, The Netherlands. Rodopi.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of the Twenty-FirstAnnual Conference on Uncertainty in Artificial Intelligence,</booktitle>
<pages>658--666</pages>
<publisher>AUAI Press.</publisher>
<location>Arlington, Virginia.</location>
<contexts>
<context position="8409" citStr="Zettlemoyer and Collins (2005)" startWordPosition="1253" endWordPosition="1256">syntactic or semantic dependencies more explicitly. At the same time, such “deeper” relations may still be recoverable (to some degree) from comparatively simple parser outputs, as demonstrated for unbounded dependency extraction from strictly local syntactic dependency trees by Nivre et al. (2010). 2.2 An armada of parsers Stanford Parser (Klein and Manning, 2003) is a probabilistic parser which can produce both phrase structure trees and grammatical relations (syntactic dependencies). The parsing model we evaluate is the 2A noteworthy exception to this correlation is the annotated corpus of Zettlemoyer and Collins (2005), which pairs surface strings from the realm of natural language database interfaces directly with semantic representations in lambda calculus. These were hand-written on the basis of database query statements distributed with the original datasets. 398 English factored model which combines the prefer- or syntactic disambiguation components. ences of unlexicalized PCFG phrase structures and MSTParser (McDonald et al., 2005) is a dataof lexical dependencies, trained on sections 02–21 driven dependency parser. The parser uses an edgeof the WSJ portion of the PTB. We chose Stanford factored model</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Luke Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Proceedings of the Twenty-FirstAnnual Conference on Uncertainty in Artificial Intelligence, pages 658–666, Arlington, Virginia. AUAI Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>