<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004104">
<title confidence="0.9550875">
Importance of Pronominal Anaphora resolution in Question
Answering systems
</title>
<author confidence="0.651595">
Jose L. Vicedo and Antonio Ferrdndez
</author>
<affiliation confidence="0.466514">
Departamento de Lenguajes y Sistemas Informaticos
Universidad de Alicante
</affiliation>
<address confidence="0.761942">
Apartado 99. 03080 Alicante, Spain
</address>
<email confidence="0.997017">
{vicedo,antonio}@dlsi.ua.es
</email>
<sectionHeader confidence="0.981865" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998913">
The main aim of this paper is
to analyse the effects of applying
pronominal anaphora resolution to
Question Answering (QA) systems.
For this task a complete QA system
has been implemented. System eval-
uation measures performance im-
provements obtained when informa-
tion that is referenced anaphorically
in documents is not ignored.
</bodyText>
<sectionHeader confidence="0.996247" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99983244117647">
Open domain QA systems are defined as
tools capable of extracting the answer to
user queries directly from unrestricted do-
main documents. Or at least, systems that
can extract text snippets from texts, from
whose content it is possible to infer the an-
swer to a specific question. In both cases,
these systems try to reduce the amount of
time users spend to locate a concrete infor-
mation.
This work is intended to achieve two princi-
pal objectives. First, we analyse several docu-
ment collections to determine the level of in-
formation referenced pronominally in them.
This study gives us an overview about the
amount of information that is discarded when
these references are not solved. As second ob-
jective, we try to measure improvements of
solving this kind of references in QA systems.
With this purpose in mind, a full QA system
has been implemented. Benefits obtained by
solving pronominal references are measured
by comparing system performance with and
without taking into account information ref-
erenced pronominally. Evaluation shows that
solving these references improves QA perfor-
mance.
In the following section, the state-of-the-
art of open domain QA systems will be sum-
marised. Afterwards, importance of pronom-
inal references in documents is analysed.
Next, our approach and system components
are described. Finally, evaluation results are
presented and discussed.
</bodyText>
<sectionHeader confidence="0.917261" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.997911745901639">
Interest in open domain QA systems is quite
recent. We had little information about this
kind of systems until the First Question An-
swering Track was held in last TREC confer-
ence (TRE, 1999). In this conference, nearly
twenty different systems were evaluated with
very different success rates. We can clas-
sify current approaches into two groups: text-
snippet extraction systems and noun-phrase
extraction systems.
Text-snippet extraction approaches are
based on locating and extracting the most rel-
evant sentences or paragraphs to the query by
supposing that this text will contain the cor-
rect answer to the query. This approach has
been the most commonly used by participants
in last TREC QA Track. Examples of these
systems are (Moldovan et al., 1999) (Singhal
et al., 1999) (Prager et al., 1999) (Takaki,
1999) (Hull, 1999) (Cormack et al., 1999).
After reviewing these approaches, we can
notice that there is a general agreement
about the importance of several Natural Lan-
guage Processing (NLP) techniques for QA
task. Pos-tagging, parsing and Name En-
tity recognition are used by most of the sys-
tems. However, few systems apply other NLP
techniques. Particularly, only four systems
model some coreference relations between en-
tities in the query and documents (Morton,
1999)(Breck et al., 1999) (Gard et al., 1999)
(Humphreys et al., 1999). As example, Mor-
ton approach models identity, definite noun-
phrases and non-possessive third person pro-
nouns. Nevertheless, benefits of applying
these coreference techniques have not been
analysed and measured separately.
The second group includes noun-phrase ex-
traction systems. These approaches try to
find the precise information requested by
questions whose answer is defined typically by
a noun phrase.
MURAX is one of these systems (Kupiec,
1999). It can use information from different
sentences, paragraphs and even different doc-
uments to determine the answer (the most rel-
evant noun-phrase) to the question. However,
this system does not take into account the
information referenced pronominally in docu-
ments. Simply, it is ignored.
With our system, we want to determine the
benefits of applying pronominal anaphora res-
olution techniques to QA systems. Therefore,
we apply the developed computational sys-
tem, Slot Unification Parser for Anaphora res-
olution (SUPAR) over documents and queries
(Ferrandez et al., 1999). SUPAR&apos;s architec-
ture consists of three independent modules:
lexical analysis, syntactic analysis, and a reso-
lution module for natural language processing
problems, such as pronominal anaphora.
For evaluation, a standard based IR system
and a sentence-extraction QA system have
been implemented. Both are based on Salton
approach (1989). After IR system retrieves
relevant documents, our QA system processes
these documents with and without solving
pronominal references in order to compare fi-
nal performance.
As results will show, pronominal anaphora
resolution improves greatly QA systems per-
formance. So, we think that this NLP tech-
nique should be considered as part of any
open domain QA system.
3 Importance of pronominal
information in documents
Trying to measure the importance of informa-
tion referenced pronominally in documents,
we have analysed several text collections used
for QA task in TREC-8 Conference as well
as others used frequently for IR system test-
ing. These collections were the following: Los
Angeles Times (LAT), Federal Register (FR),
Financial Times (FT), Federal Bureau Infor-
mation Service (FBIS), TIME, CRANFIELD,
CISI, CACM, MED and LISA. This analy-
sis consists on determining the amount and
type of pronouns used, as well as the number
of sentences containing pronouns in each of
them. As average measure of pronouns used
in a collection, we use the ratio between the
quantity of pronouns and the number of sen-
tences containing pronouns. This measure ap-
proximates the level of information that is ig-
nored if these references are not solved. Fig-
ure 1 shows the results obtained in this anal-
ysis.
As we can see, the amount and type of pro-
nouns used in analysed collections vary de-
pending on the subject the documents talk
about. LAT, FBIS, TIME and FT collections
are composed from news published in differ-
ent newspapers. The ratio of pronominal ref-
erence used in this kind of documents is very
high (from 35,96% to 55,20%). These doc-
uments contain a great number of pronomi-
nal references in third person (he, she, they,
his, her, their) whose antecedents are mainly
people&apos;s names. In this type of documents,
pronominal anaphora resolution seems to be
very necessary for a correct modelling of rela-
tions between entities. CISI and MED collec-
tions appear ranked next in decreasing ratio
level order. These collections are composed
by general comments about document man-
aging, classification and indexing and doc-
uments extracted from medical journals re-
spectively. Although the ratio presented by
these collections (24,94% and 22,16%) is also
high, the most important group of pronominal
references used in these collections is formed
by &amp;quot;it&amp;quot; and &amp;quot;its&amp;quot; pronouns. In this case,
</bodyText>
<table confidence="0.997707">
TEXT COLLECTION LAT FBIS TIME FT CISI MED CACM LISA FR CRANFIELD
Pronoun type
HE, SHE, THEY 38,59% 29,15% 31,20% 26,20% 15,38% 15,07% 8,59% 12,24% 13,31% 6,54%
HIS, HER, THEIR 25,84% 21,54% 35,01% 20,52% 22,96% 21,46% 15,69% 31,03% 20,70% 10,35%
IT, ITS 26,92% 39,60% 22,43% 46,68% 52,11% 57,41% 67,61% 47,86% 61,06% 79,76%
HIM, THEM 7,04% 7,08% 7,82% 4,44% 6,38% 3,96% 4,87% 6,30% 3,45% 1,60%
HIM, HER,IT(SELF), THEMSELVES 1,61% 2,63% 3,54% 2,17% 3,17% 2,10% 3,25% 2,57% 1,48% 1,75%
Pronouns in Sentences
Containing 0 pronouns 44,80% 48,09% 51,37% 64,04% 75,06% 77,84% 79,06% 83,79% 84,92% 90,95%
Containing 1 pronoun 30,40% 31,37% 29,46% 23,07% 17,17% 15,02% 17,54% 13,01% 11,64% 8,10%
Containing 2 pronouns 14,94% 12,99% 12,26% 8,54% 5,27% 4,75% 2,79% 2,56% 2,57% 0,85%
Containing +2 pronouns 9,86% 7,55% 6,90% 4,34% 2,51% 2,39% 0,60% 0,64% 0,88% 0,09%
Ratio of pronominal reference 55,20% 51,91% 48,63% 35,96% 24,94% 22,16% 20,94% 16,21% 15,08% 9,05%
</table>
<figureCaption confidence="0.96621">
Figure 1: Pronominal references in text collections
</figureCaption>
<bodyText confidence="0.99996776">
antecedents of these pronominal references
are mainly concepts represented typically by
noun phrases. It seems again important solv-
ing these references for a correct modelling
of relations between concepts expressed by
noun-phrases. The lowest ratio results are
presented by CRANFIELD collection with a
9,05%. The reason of this level of pronominal
use is due to text contents. This collection is
composed by extracts of very high technical
subjects. Between the described percentages
we find the CACM, LISA and FR collections.
These collections are formed by abstracts and
documents extracted from the Federal Regis-
ter, from the CACM journal and from Library
and Information Science Abstracts, respec-
tively. As general behaviour, we can notice
that as more technical document contents be-
come, the pronouns &amp;quot;it&amp;quot; and &amp;quot;its&amp;quot; become the
most appearing in documents and the ratio
of pronominal references used decreases. An-
other observation can be extracted from this
analysis. Distribution of pronouns within sen-
tences is similar in all collections. Pronouns
appear scattered through sentences contain-
ing one or two pronouns. Using more than
two pronouns in the same sentence is quite
infrequent.
After analysing these results an important
question may arise. Is it worth enough to
solve pronominal references in documents? It
would seem reasonable to think that resolu-
tion of pronominal anaphora would only be
accomplished when the ratio of pronominal
occurrence exceeds a minimum level. How-
ever, we have to take into account that the
cost of solving these references is proportional
to the number of pronouns analysed and con-
sequently, proportional to the amount of in-
formation a system will ignore if these refer-
ences are not solved.
As results above state, it seems reason-
able to solve pronominal references in queries
and documents for QA tasks. At least, when
the ratio of pronouns used in documents rec-
ommend it. Anyway, evaluation and later
analysis (section 5) contribute with empiri-
cal data to conclude that applying pronom-
inal anaphora resolution techniques improve
QA systems performance.
</bodyText>
<sectionHeader confidence="0.985209" genericHeader="method">
4 Our Approach
</sectionHeader>
<bodyText confidence="0.999992870967742">
Our system is made up of three modules. The
first one is a standard IR system that retrieves
relevant documents for queries. The second
module will manage with anaphora resolution
in both, queries and retrieved documents. For
this purpose we use SUPAR computational
system (section 4.1). And the third one is
a sentence-extraction QA system that inter-
acts with SUPAR module and ranks sentences
from retrieved documents to locate the an-
swer where the correct answer appears (sec-
tion 4.2).
For the purpose of evaluation an IR sys-
tem has been implemented. This system is
based on the standard information retrieval
approach to document ranking described in
Salton (1989). For QA task, the same ap-
proach has been used as baseline but using
sentences as text unit. Each term in the query
and documents is assigned an inverse docu-
ment frequency (idf ) score based on the same
corpus. This measure is computed as:
where N is the total number of documents
in the collection and df(t) is the number of
documents which contains term t. Query ex-
pansion consists of stemming terms using a
version of the Porter stemmer. Document and
sentence similarity to the query was computed
using the cosine similarity measure. The LAT
corpus has been selected as test collection due
to his high level of pronominal references.
</bodyText>
<subsectionHeader confidence="0.999132">
4.1 Solving pronominal anaphora
</subsectionHeader>
<bodyText confidence="0.997796558441558">
In this section, the NLP Slot Unification
Parser for Anaphora Resolution (SUPAR)
is briefly described (Ferrandez et al., 1999;
Ferrandez et al., 1998). SUPAR&apos;s architec-
ture consists of three independent modules
that interact with one other. These modules
are lexical analysis, syntactic analysis, and a
resolution module for Natural Language Pro-
cessing problems.
Lexical analysis module. This module
takes each sentence to parse as input, along
with a tool that provides the system with all
the lexical information for each word of the
sentence. This tool may be either a dictio-
nary or a part-of-speech tagger. In addition,
this module returns a list with all the neces-
sary information for the remaining modules
as output. SUPAR works sentence by sen-
tence from the input text, but stores informa-
tion from previous sentences, which it uses in
other modules, (e.g. the list of antecedents of
previous sentences for anaphora resolution).
Syntactic analysis module. This mod-
ule takes as input the output of lexical analy-
sis module and the syntactic information rep-
resented by means of grammatical formalism
Slot Unification Grammar (SUG). It returns
what is called slot structure, which stores all
necessary information for following modules.
One of the main advantages of this system is
that it allows carrying out either partial or
full parsing of the text.
Module of resolution of NLP prob-
lems. In this module, NLP problems
(e.g. anaphora, extra-position, ellipsis or PP-
attachment) are dealt with. It takes the slot
structure (SS) that corresponds to the parsed
sentence as input. The output is an SS in
which all the anaphors have been resolved. In
this paper, only pronominal anaphora resolu-
tion has been applied.
The kinds of knowledge that are going to
be used in pronominal anaphora resolution in
this paper are: pos-tagger, partial parsing,
statistical knowledge, c-command and mor-
phologic agreement as restrictions and several
heuristics such as syntactic parallelism, pref-
erence for noun-phrases in same sentence as
the pronoun preference for proper nouns.
We should remark that when we work with
unrestricted texts (as it occurs in this paper)
we do not use semantic knowledge (i.e. a
tool such as WorNet). Presently, SUPAR re-
solves both Spanish and English pronominal
anaphora with a success rate of 87% and 84%
respectively.
SUPAR pronominal anaphora resolution
differs from those based on restrictions and
preferences, since the aim of our preferences
is not to sort candidates, but rather to dis-
card candidates. That is to say, preferences
are considered in a similar way to restrictions,
except when no candidate satisfies a prefer-
ence, in which case no candidate is discarded.
For example in sentence: &amp;quot;Rob was asking us
about John. I replied that Peter saw John yes-
terday. James also saw him.&amp;quot; After applying
the restrictions, the following list of candi-
dates is obtained for the pronoun him: [John,
Peter, Rob], which are then sorted according
to their proximity to the anaphora. If pref-
erence for candidates in same sentence as the
anaphora is applied, then no candidate satis-
fies it, so the following preference is applied on
the same list of candidates. Next, preference
for candidates in the previous sentence is ap-
plied and the list is reduced to the following
</bodyText>
<equation confidence="0.998765">
idf(t) = log( N
df( t)) (1)
</equation>
<bodyText confidence="0.960116272727273">
candidates: [John, Peter]. If syntactic par-
allelism preference is then applied, only one
candidate remains, [John], which will be the
antecedent chosen.
Each kind of anaphora has its own set of
restrictions and preferences, although they all
follow the same general algorithm: first come
the restrictions, after which the preferences
are applied. For pronominal anaphora, the
set of restrictions and preferences that apply
are described in Figure 2.
</bodyText>
<figure confidence="0.718048625">
Procedure SelectingAntecedent ( INPUT L: ListOfCandidates,
OUTPUT Solution: Antecedent )
Apply restrictions to L with a result of L1
Morphologic agreement
C-command constraints
Semantic consistency
Case of:
NumberOfElements (L1) = 1
Solution = TheFirstOne (L1)
NumberOfElements (L1) = 0
Exophora or cataphora
NumberOfElements (L1) &gt; 1
Apply preferences to L1 with a result of L2
1) Candidates in the same sentence as anaphor.
2) Candidates in the previous sentence
3) Preference for proper nouns.
4) Candidates in the same position as the anaphor
with reference to the verb (before or after).
5) Candidates with the same number of parsed
constituents as the anaphora
6) Candidates that have appeared with the verb of
the anaphor more than once
7) Preference for indefinite NPs.
Case of:
NumberOfElements (L2) = 1
Solution = TheFirstOne (L2)
NumberOfElements (L2) &gt; 1
Extract from L2 in L3 those candidates that have
been repeated most in the text
If NumberOfElements (L3) &gt; 1
Extract from L3 in L4 those candidates that
have appeared most with the verb of the
anaphora
Solution = TheFirstOne (L4)
Else
Solution = TheFirstOne (L3)
EndIf
EndCase
EndCase
EndProcedure
</figure>
<figureCaption confidence="0.842755">
Figure 2: Pronominal anaphora resolution al-
gorithm
</figureCaption>
<bodyText confidence="0.999974130434783">
The following restrictions are first applied
to the list of candidates: morphologic agree-
ment, c-command constraints and semantic
consistency. This list is sorted by proximity to
the anaphor. Next, if after applying restric-
tions there is still more than one candidate,
the preferences are then applied, in the order
shown in this figure. This sequence of prefer-
ences (from 1 to 7) stops when, after having
applied a preference, only one candidate re-
mains. If after applying preferences there is
still more than one candidate, then the most
repeated candidates&apos; in the text are extracted
from the list after applying preferences. After
this is done, if there is still more than one can-
didate, then those candidates that have ap-
peared most frequently with the verb of the
anaphor are extracted from the previous list.
Finally, if after having applied all the previ-
ous preferences, there is still more than one
candidate left, the first candidate of the re-
sulting list, (the closest one to the anaphor),
is selected.
</bodyText>
<subsectionHeader confidence="0.996357">
4.2 Anaphora resolution and QA
</subsectionHeader>
<bodyText confidence="0.99941">
Our QA approach provides a second level of
processing for relevant documents: Analysing
matching documents and Sentence ranking.
</bodyText>
<subsectionHeader confidence="0.449606">
Analysing Matching Documents. This
</subsectionHeader>
<bodyText confidence="0.999797409090909">
step is applied over the best matching docu-
ments retrieved from the IR system. These
documents are analysed by SUPAR module
and pronominal references are solved. As re-
sult, each pronoun is associated with the noun
phrase it refers to in the documents. Then,
documents are split into sentences as basic
text unit for QA purposes. This set of sen-
tences is sent to the sentence ranking stage.
Sentence Ranking. Each term in the
query is assigned a weight. This weight is
the sum of inverse document frequency mea-
sure of terms based on its occurrence in the
LAT collection described earlier. Each docu-
ment sentence is weighted the same way. The
only difference with baseline is that pronouns
are given the weight of the entity they refer
to. As we only want to analyse the effects
of pronominal reference resolution, no more
changes are introduced in weighting scheme.
For sentence ranking, cosine similarity is used
between query and document sentences.
</bodyText>
<sectionHeader confidence="0.970688" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.966435129032258">
For this evaluation, several people unac-
quainted with this work proposed 150 queries
Here, we mean that firstly we obtain the maxi-
mum number of repetitions for an antecedent in the
remaining list. After that, we extract from that list
the antecedents that have this value of repetition.
whose correct answer appeared at least once
into the analysed collection. These queries
were also selected based on their expressing
the user&apos;s information need clearly and their
being likely answered in a single sentence.
First, relevant documents for each query
were retrieved using the IR system described
earlier. Only the best 50 matching docu-
ments were selected for QA evaluation. As
the document containing the correct answer
was included into the retrieved sets for only
93 queries (a 62% of the proposed queries),
the remaining 57 queries were excluded for
this evaluation.
Once retrieval of relevant document sets
was accomplished for each query, the sys-
tem applied anaphora resolution algorithm to
these documents. Finally, sentence matching
and ranking was accomplished as described in
section 4.2 and the system presented a ranked
list containing the 10 most relevant sentences
to each query.
For a better understanding of evaluation re-
sults, queries were classified into three groups
depending on the following characteristics:
</bodyText>
<listItem confidence="0.989005555555555">
• Group A. There are no pronominal ref-
erences in the target sentence (sentence
containing the correct answer).
• Group B. The information required as
answer is referenced via pronominal
anaphora in the target sentence.
• Group C. Any term in the query is ref-
erenced pronominally in the target sen-
tence.
</listItem>
<bodyText confidence="0.99854425">
Group A was made up by 37 questions.
Groups B and C contained 25 and 31 queries
respectively. Figure 3 shows examples of
queries classified into groups B and C.
Evaluation results are presented in Figure
4 as the number of target sentences appear-
ing into the 10 most relevant sentences re-
turned by the system for each query and also,
the number of these sentences that are con-
sidered a correct answer. An answer is con-
sidered correct if it can be obtained by sim-
ply looking at the target sentence. Results
</bodyText>
<figureCaption confidence="0.969924">
Figure 3: Group B and C query examples
</figureCaption>
<bodyText confidence="0.9991962">
are classified based on question type intro-
duced above. The number of queries pertain-
ing to each group appears in the second col-
umn. Third and fourth columns show base-
line results (without solving anaphora). Fifth
and sixth columns show results obtained when
pronominal references have been solved.
Results show several aspects we have to
take into account. Benefits obtained from ap-
plying pronominal anaphora resolution vary
depending on question type. Results for
group A and B queries show us that relevance
to the query is the same as baseline system.
So, it seems that pronominal anaphora res-
olution does not achieve any improvement.
This is true only for group A questions. Al-
though target sentences are ranked similarly,
for group B questions, target sentences re-
turned by baseline can not be considered as
correct because we do not obtain the an-
swer by simply looking at returned sentences.
The correct answer is displayed only when
pronominal anaphora is solved and pronom-
inal references are substituted by the noun
phrase they refer to. Only if pronominal ref-
erences are solved, the user will not need to
read more text to obtain the correct answer.
For noun-phrase extraction QA systems the
improvement is greater. If pronominal ref-
erences are not solved, this information will
</bodyText>
<table confidence="0.914981263157895">
Group B Example
Question: “Who is the village head man of Digha ?”
Answer: “He is the sarpanch, or village head man of
Digha, a hamlet or mud-and-straw huts 10
miles from ...”
Anaphora resolution: Ram Bahadu
Group C Example
Question: “What did Democrats propose for low-income
families?”
Answer: “They also want to provide small subsidies for
low-income families in which both parents work
at outside jobs.”
Anaphora resolution: Democrats
Baseline Anaphora solved
Answer Type Number Target included Correct answer Target included Correct answer
A 37 (39,78%) 18 (48,65%) 18 (48,65%) 18 (48,65%) 18 (48,65%)
B 25 (26,88%) 12 (48,00%) 0 (0,00%) 12 (48,00%) 12 (48,00%)
C 31 (33,33%) 9 (29,03%) 9 (29,03%) 21 (67,74%) 21 (67,74%)
A+B+C 93 (100,00°/x) 39 (41,94°/x) 27 (29,03°/x) 51 (54,84°/x) 51 (54,84°/x)
</table>
<figureCaption confidence="0.996802">
Figure 4: Evaluation results
</figureCaption>
<bodyText confidence="0.998895415384616">
not be analysed and probably a wrong noun-
phrase will be given as answer to the query.
Results improve again if we analyse group
C queries performance. These queries have
the following characteristic: some of the
query terms were referenced via pronominal
anaphora in the relevant sentence. When
this situation occurs, target sentences are re-
trieved earlier in the final ranked list than in
the baseline list. This improvement is because
similarity increases between query and target
sentence when pronouns are weighted with
the same score as their referring terms. The
percentage of target sentences obtained in-
creases 38,71 points (from 29,03% to 67,74%).
Aggregate results presented in Figure 4
measure improvement obtained considering
the system as a whole. General percentage
of target sentences obtained increases 12,90
points (from 41,94% to 54,84%) and the level
of correct answers returned by the system in-
creases 25,81 points (from 29,03% to 54,84%).
At this point we need to consider the follow-
ing question: Will these results be the same
for any other question set? We have analysed
test questions in order to determine if results
obtained depend on question test set. We ar-
gue that a well-balanced query set would have
a percentage of target sentences that contain
pronouns (PTSC) similar to the pronominal
reference ratio of the text collection that is
being queried. Besides, we suppose that the
probability of finding an answer in a sentence
is the same for all sentences in the collec-
tion. Comparing LAT ratio of pronominal
reference (55,20%) with the question test set
PTSC we can measure how a question set can
affect results. Our question set PTSC value
is a 60,22%. We obtain as target sentences
containing pronouns only a 5,02% more than
expected when test queries are randomly se-
lected. In order to obtain results according to
a well-balanced question set, we discarded five
questions from both groups B and C. Figure 5
shows that results for this well-balanced ques-
tion set are similar to previous results. Aggre-
gate results show that general percentage of
target sentences increases 10,84 points when
solving pronominal anaphora and the level
of correct answers retrieved increases 22,89
points (instead of 12,90 and 25,81 obtained
in previous evaluation respectively).
As results show, we can say that pronom-
inal anaphora resolution improves QA sys-
tems performance in several aspects. First,
precision increases when query terms are ref-
erenced anaphorically in the target sentence.
Second, pronominal anaphora resolution re-
duces the amount of text a user has to read
when the answer sentence is displayed and
pronominal references are substituted with
their coreferent noun phrases. And third,
for noun phrase extraction QA systems it is
essential to solve pronominal references if a
good performance is pursued.
</bodyText>
<sectionHeader confidence="0.990881" genericHeader="conclusions">
6 Conclusions and future research
</sectionHeader>
<bodyText confidence="0.998118230769231">
The analysis of information referenced
pronominally in documents has revealed to
be important to tasks where high level of
recall is required. We have analysed and
measured the effects of applying pronominal
anaphora resolution in QA systems. As
results show, its application improves greatly
QA performance and seems to be essential in
some cases.
Three main areas of future work have ap-
peared while investigation has been devel-
oped. First, IR system used for retrieving
relevant documents has to be adapted for QA
</bodyText>
<figure confidence="0.983395">
Baseline Anaphora solved
Answer Type Number Target included Correct answer Target included Correct answer
A 37 (39,78%) 18 (48,65%) 18 (48,65%) 18 (48,65%) 18 (48,65%)
B 20 (21,51%) 10 (50,00%) 0 (0,00%) 10 (50,00%) 10 (50,00%)
C 26 (27,96%) 9 (34,62%) 9 (34,62%) 18 (69,23%) 18 (69,23%)
A+B+C 83 (89,25%) 37 (44,58%) 27 (32,53%) 46 (55,42%) 46 (55,42%)
</figure>
<figureCaption confidence="0.999937">
Figure 5: Well-balanced question set results
</figureCaption>
<bodyText confidence="0.999162333333333">
tasks. The IR used, obtained the document
containing the target sentence only for 93 of
the 150 proposed queries. Therefore, its preci-
sion needs to be improved. Second, anaphora
resolution algorithm has to be extended to
different types of anaphora such as definite
descriptions, surface count, verbal phrase and
one-anaphora. And third, sentence ranking
approach has to be analysed to maximise the
percentage of target sentences included into
the 10 answer sentences presented by the sys-
tem.
</bodyText>
<sectionHeader confidence="0.998069" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999864442622951">
Eric Breck, John Burger, Lisa Ferro, David House,
Marc Light, and Inderjeet Mani. 1999. A Sys
Called Quanda. In Eighth Text REtrieval Con-
ference (TRE, 1999).
Gordon V. Cormack, Charles L. A. Clarke,
Christopher R. Palmer, and Derek I. E.
Kisman. 1999. Fast Automatic Passage Rank-
ing (MultiText Experiments for TREC-8). In
Eighth Text REtrieval Conference (TRE, 1999).
Antonio Ferrandez, Manuel Palomar, and Lidia
Moreno. 1998. Anaphora resolution in unre-
striced texts with partial parsing. In 36th An-
nual Meeting of the Association for Computa-
tional Linguistics and 17th International Con-
ference on Computational Lingustics COLING-
ACL.
Antonio Ferrandez, Manuel Palomar, and Lidia
Moreno. 1999. An empirical approach to Span-
ish anaphora resolution. To appear in Machine
Translation.
David A. Hull. 1999. Xerox TREC-8 Question
Answering Track Report. In Eighth Text RE-
trieval Conference (TRE, 1999).
Kevin Humphreys, Robert Gaizauskas, Mark
Hepple, and Mark Sanderson. 1999. University
of Sheffield TREC-8 Q&amp;A System. In Eighth
Text REtrieval Conference (TRE, 1999).
Julian Kupiec, 1999. MURAX: Finding and Or-
ganising Answers from Text Search, pages 311{
331. Kluwer Academic, New York.
Dan Moldovan, Sanda Harabagiu, Marius Pasca,
Rada Mihalcea, Richard Goodrum, Roxana
Girju, and Vasile Rus. 1999. LASSO: A Tool
for Surfing the Answer Net. In Eighth Text RE-
trieval Conference (TRE, 1999).
Thomas S. Morton. 1999. Using Coreference in
Question Answering. In Eighth Text REtrieval
Conference (TRE, 1999).
Douglas W. Oard, Jianqiang Wang, Dekang Lin,
and Ian Soboroff. 1999. TREC-8 Experiments
at Maryland: CLIR, QA and Routing. In
Eighth Text REtrieval Conference (TRE, 1999).
John Prager, Dragomir Radev, Eric Brown, Anni
Coden, and Valerie Samn. 1999. The Use of
Predictive Annotation for Question Answering.
In Eighth Text REtrieval Conference (TRE,
1999).
Gerard A. Salton. 1989. Automatic Text Process-
ing: The Transformation, Analysis, and Re-
trieval of Information by Computer. Addison
Wesley, New York.
Amit Singhal, Steve Abney, Michiel Bacchiani,
Michael Collins, Donald Hindle, and Fernando
Pereira. 1999. ATT at TREC-8. In Eighth Text
REtrieval Conference (TRE, 1999).
Toru Takaki. 1999. NTT DATA: Overview of sys-
tem approach at TREC-8 ad-hoc and question
answering. In Eighth Text REtrieval Confer-
ence (TRE, 1999).
TREC-8. 1999. Eighth Text REtrieval Confer-
ence.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.882554">
<title confidence="0.999418">Importance of Pronominal Anaphora resolution in Question Answering systems</title>
<author confidence="0.965814">L Vicedo Ferrdndez</author>
<affiliation confidence="0.990956">Departamento de Lenguajes y Sistemas Informaticos Universidad de Alicante</affiliation>
<address confidence="0.947447">Apartado 99. 03080 Alicante, Spain</address>
<abstract confidence="0.997132">The main aim of this paper is to analyse the effects of applying pronominal anaphora resolution to Question Answering (QA) systems. For this task a complete QA system has been implemented. System evaluation measures performance improvements obtained when information that is referenced anaphorically in documents is not ignored.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eric Breck</author>
<author>John Burger</author>
<author>Lisa Ferro</author>
<author>David House</author>
<author>Marc Light</author>
<author>Inderjeet Mani</author>
</authors>
<title>A Sys Called Quanda.</title>
<date>1999</date>
<booktitle>In Eighth Text REtrieval Conference (TRE,</booktitle>
<contexts>
<context position="3285" citStr="Breck et al., 1999" startWordPosition="509" endWordPosition="512">n last TREC QA Track. Examples of these systems are (Moldovan et al., 1999) (Singhal et al., 1999) (Prager et al., 1999) (Takaki, 1999) (Hull, 1999) (Cormack et al., 1999). After reviewing these approaches, we can notice that there is a general agreement about the importance of several Natural Language Processing (NLP) techniques for QA task. Pos-tagging, parsing and Name Entity recognition are used by most of the systems. However, few systems apply other NLP techniques. Particularly, only four systems model some coreference relations between entities in the query and documents (Morton, 1999)(Breck et al., 1999) (Gard et al., 1999) (Humphreys et al., 1999). As example, Morton approach models identity, definite nounphrases and non-possessive third person pronouns. Nevertheless, benefits of applying these coreference techniques have not been analysed and measured separately. The second group includes noun-phrase extraction systems. These approaches try to find the precise information requested by questions whose answer is defined typically by a noun phrase. MURAX is one of these systems (Kupiec, 1999). It can use information from different sentences, paragraphs and even different documents to determine</context>
</contexts>
<marker>Breck, Burger, Ferro, House, Light, Mani, 1999</marker>
<rawString>Eric Breck, John Burger, Lisa Ferro, David House, Marc Light, and Inderjeet Mani. 1999. A Sys Called Quanda. In Eighth Text REtrieval Conference (TRE, 1999).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gordon V Cormack</author>
<author>Charles L A Clarke</author>
<author>Christopher R Palmer</author>
<author>Derek I E Kisman</author>
</authors>
<title>Fast Automatic Passage Ranking (MultiText Experiments for TREC-8).</title>
<date>1999</date>
<booktitle>In Eighth Text REtrieval Conference (TRE,</booktitle>
<contexts>
<context position="2837" citStr="Cormack et al., 1999" startWordPosition="439" endWordPosition="442">ifferent systems were evaluated with very different success rates. We can classify current approaches into two groups: textsnippet extraction systems and noun-phrase extraction systems. Text-snippet extraction approaches are based on locating and extracting the most relevant sentences or paragraphs to the query by supposing that this text will contain the correct answer to the query. This approach has been the most commonly used by participants in last TREC QA Track. Examples of these systems are (Moldovan et al., 1999) (Singhal et al., 1999) (Prager et al., 1999) (Takaki, 1999) (Hull, 1999) (Cormack et al., 1999). After reviewing these approaches, we can notice that there is a general agreement about the importance of several Natural Language Processing (NLP) techniques for QA task. Pos-tagging, parsing and Name Entity recognition are used by most of the systems. However, few systems apply other NLP techniques. Particularly, only four systems model some coreference relations between entities in the query and documents (Morton, 1999)(Breck et al., 1999) (Gard et al., 1999) (Humphreys et al., 1999). As example, Morton approach models identity, definite nounphrases and non-possessive third person pronoun</context>
</contexts>
<marker>Cormack, Clarke, Palmer, Kisman, 1999</marker>
<rawString>Gordon V. Cormack, Charles L. A. Clarke, Christopher R. Palmer, and Derek I. E. Kisman. 1999. Fast Automatic Passage Ranking (MultiText Experiments for TREC-8). In Eighth Text REtrieval Conference (TRE, 1999).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Ferrandez</author>
<author>Manuel Palomar</author>
<author>Lidia Moreno</author>
</authors>
<title>Anaphora resolution in unrestriced texts with partial parsing.</title>
<date>1998</date>
<booktitle>In 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Lingustics COLINGACL.</booktitle>
<contexts>
<context position="11665" citStr="Ferrandez et al., 1998" startWordPosition="1837" endWordPosition="1840">e same corpus. This measure is computed as: where N is the total number of documents in the collection and df(t) is the number of documents which contains term t. Query expansion consists of stemming terms using a version of the Porter stemmer. Document and sentence similarity to the query was computed using the cosine similarity measure. The LAT corpus has been selected as test collection due to his high level of pronominal references. 4.1 Solving pronominal anaphora In this section, the NLP Slot Unification Parser for Anaphora Resolution (SUPAR) is briefly described (Ferrandez et al., 1999; Ferrandez et al., 1998). SUPAR&apos;s architecture consists of three independent modules that interact with one other. These modules are lexical analysis, syntactic analysis, and a resolution module for Natural Language Processing problems. Lexical analysis module. This module takes each sentence to parse as input, along with a tool that provides the system with all the lexical information for each word of the sentence. This tool may be either a dictionary or a part-of-speech tagger. In addition, this module returns a list with all the necessary information for the remaining modules as output. SUPAR works sentence by sen</context>
</contexts>
<marker>Ferrandez, Palomar, Moreno, 1998</marker>
<rawString>Antonio Ferrandez, Manuel Palomar, and Lidia Moreno. 1998. Anaphora resolution in unrestriced texts with partial parsing. In 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Lingustics COLINGACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Ferrandez</author>
<author>Manuel Palomar</author>
<author>Lidia Moreno</author>
</authors>
<title>An empirical approach to Spanish anaphora resolution.</title>
<date>1999</date>
<note>To appear in Machine Translation.</note>
<contexts>
<context position="4354" citStr="Ferrandez et al., 1999" startWordPosition="670" endWordPosition="673">hrase. MURAX is one of these systems (Kupiec, 1999). It can use information from different sentences, paragraphs and even different documents to determine the answer (the most relevant noun-phrase) to the question. However, this system does not take into account the information referenced pronominally in documents. Simply, it is ignored. With our system, we want to determine the benefits of applying pronominal anaphora resolution techniques to QA systems. Therefore, we apply the developed computational system, Slot Unification Parser for Anaphora resolution (SUPAR) over documents and queries (Ferrandez et al., 1999). SUPAR&apos;s architecture consists of three independent modules: lexical analysis, syntactic analysis, and a resolution module for natural language processing problems, such as pronominal anaphora. For evaluation, a standard based IR system and a sentence-extraction QA system have been implemented. Both are based on Salton approach (1989). After IR system retrieves relevant documents, our QA system processes these documents with and without solving pronominal references in order to compare final performance. As results will show, pronominal anaphora resolution improves greatly QA systems performa</context>
<context position="11640" citStr="Ferrandez et al., 1999" startWordPosition="1833" endWordPosition="1836">(idf ) score based on the same corpus. This measure is computed as: where N is the total number of documents in the collection and df(t) is the number of documents which contains term t. Query expansion consists of stemming terms using a version of the Porter stemmer. Document and sentence similarity to the query was computed using the cosine similarity measure. The LAT corpus has been selected as test collection due to his high level of pronominal references. 4.1 Solving pronominal anaphora In this section, the NLP Slot Unification Parser for Anaphora Resolution (SUPAR) is briefly described (Ferrandez et al., 1999; Ferrandez et al., 1998). SUPAR&apos;s architecture consists of three independent modules that interact with one other. These modules are lexical analysis, syntactic analysis, and a resolution module for Natural Language Processing problems. Lexical analysis module. This module takes each sentence to parse as input, along with a tool that provides the system with all the lexical information for each word of the sentence. This tool may be either a dictionary or a part-of-speech tagger. In addition, this module returns a list with all the necessary information for the remaining modules as output. SU</context>
</contexts>
<marker>Ferrandez, Palomar, Moreno, 1999</marker>
<rawString>Antonio Ferrandez, Manuel Palomar, and Lidia Moreno. 1999. An empirical approach to Spanish anaphora resolution. To appear in Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Hull</author>
</authors>
<title>Xerox TREC-8 Question Answering Track Report.</title>
<date>1999</date>
<booktitle>In Eighth Text REtrieval Conference (TRE,</booktitle>
<contexts>
<context position="2814" citStr="Hull, 1999" startWordPosition="437" endWordPosition="438">arly twenty different systems were evaluated with very different success rates. We can classify current approaches into two groups: textsnippet extraction systems and noun-phrase extraction systems. Text-snippet extraction approaches are based on locating and extracting the most relevant sentences or paragraphs to the query by supposing that this text will contain the correct answer to the query. This approach has been the most commonly used by participants in last TREC QA Track. Examples of these systems are (Moldovan et al., 1999) (Singhal et al., 1999) (Prager et al., 1999) (Takaki, 1999) (Hull, 1999) (Cormack et al., 1999). After reviewing these approaches, we can notice that there is a general agreement about the importance of several Natural Language Processing (NLP) techniques for QA task. Pos-tagging, parsing and Name Entity recognition are used by most of the systems. However, few systems apply other NLP techniques. Particularly, only four systems model some coreference relations between entities in the query and documents (Morton, 1999)(Breck et al., 1999) (Gard et al., 1999) (Humphreys et al., 1999). As example, Morton approach models identity, definite nounphrases and non-possessi</context>
</contexts>
<marker>Hull, 1999</marker>
<rawString>David A. Hull. 1999. Xerox TREC-8 Question Answering Track Report. In Eighth Text REtrieval Conference (TRE, 1999).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Humphreys</author>
<author>Robert Gaizauskas</author>
<author>Mark Hepple</author>
<author>Mark Sanderson</author>
</authors>
<date>1999</date>
<booktitle>University of Sheffield TREC-8 Q&amp;A System. In Eighth Text REtrieval Conference (TRE,</booktitle>
<contexts>
<context position="3330" citStr="Humphreys et al., 1999" startWordPosition="517" endWordPosition="520">ystems are (Moldovan et al., 1999) (Singhal et al., 1999) (Prager et al., 1999) (Takaki, 1999) (Hull, 1999) (Cormack et al., 1999). After reviewing these approaches, we can notice that there is a general agreement about the importance of several Natural Language Processing (NLP) techniques for QA task. Pos-tagging, parsing and Name Entity recognition are used by most of the systems. However, few systems apply other NLP techniques. Particularly, only four systems model some coreference relations between entities in the query and documents (Morton, 1999)(Breck et al., 1999) (Gard et al., 1999) (Humphreys et al., 1999). As example, Morton approach models identity, definite nounphrases and non-possessive third person pronouns. Nevertheless, benefits of applying these coreference techniques have not been analysed and measured separately. The second group includes noun-phrase extraction systems. These approaches try to find the precise information requested by questions whose answer is defined typically by a noun phrase. MURAX is one of these systems (Kupiec, 1999). It can use information from different sentences, paragraphs and even different documents to determine the answer (the most relevant noun-phrase) t</context>
</contexts>
<marker>Humphreys, Gaizauskas, Hepple, Sanderson, 1999</marker>
<rawString>Kevin Humphreys, Robert Gaizauskas, Mark Hepple, and Mark Sanderson. 1999. University of Sheffield TREC-8 Q&amp;A System. In Eighth Text REtrieval Conference (TRE, 1999).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
</authors>
<title>MURAX: Finding and Organising Answers from Text Search,</title>
<date>1999</date>
<pages>311--331</pages>
<publisher>Kluwer Academic,</publisher>
<location>New York.</location>
<contexts>
<context position="3782" citStr="Kupiec, 1999" startWordPosition="585" endWordPosition="586">tems model some coreference relations between entities in the query and documents (Morton, 1999)(Breck et al., 1999) (Gard et al., 1999) (Humphreys et al., 1999). As example, Morton approach models identity, definite nounphrases and non-possessive third person pronouns. Nevertheless, benefits of applying these coreference techniques have not been analysed and measured separately. The second group includes noun-phrase extraction systems. These approaches try to find the precise information requested by questions whose answer is defined typically by a noun phrase. MURAX is one of these systems (Kupiec, 1999). It can use information from different sentences, paragraphs and even different documents to determine the answer (the most relevant noun-phrase) to the question. However, this system does not take into account the information referenced pronominally in documents. Simply, it is ignored. With our system, we want to determine the benefits of applying pronominal anaphora resolution techniques to QA systems. Therefore, we apply the developed computational system, Slot Unification Parser for Anaphora resolution (SUPAR) over documents and queries (Ferrandez et al., 1999). SUPAR&apos;s architecture consi</context>
</contexts>
<marker>Kupiec, 1999</marker>
<rawString>Julian Kupiec, 1999. MURAX: Finding and Organising Answers from Text Search, pages 311{ 331. Kluwer Academic, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Moldovan</author>
<author>Sanda Harabagiu</author>
<author>Marius Pasca</author>
<author>Rada Mihalcea</author>
<author>Richard Goodrum</author>
<author>Roxana Girju</author>
<author>Vasile Rus</author>
</authors>
<title>LASSO: A Tool for Surfing the Answer Net.</title>
<date>1999</date>
<booktitle>In Eighth Text REtrieval Conference (TRE,</booktitle>
<contexts>
<context position="2741" citStr="Moldovan et al., 1999" startWordPosition="423" endWordPosition="426">Answering Track was held in last TREC conference (TRE, 1999). In this conference, nearly twenty different systems were evaluated with very different success rates. We can classify current approaches into two groups: textsnippet extraction systems and noun-phrase extraction systems. Text-snippet extraction approaches are based on locating and extracting the most relevant sentences or paragraphs to the query by supposing that this text will contain the correct answer to the query. This approach has been the most commonly used by participants in last TREC QA Track. Examples of these systems are (Moldovan et al., 1999) (Singhal et al., 1999) (Prager et al., 1999) (Takaki, 1999) (Hull, 1999) (Cormack et al., 1999). After reviewing these approaches, we can notice that there is a general agreement about the importance of several Natural Language Processing (NLP) techniques for QA task. Pos-tagging, parsing and Name Entity recognition are used by most of the systems. However, few systems apply other NLP techniques. Particularly, only four systems model some coreference relations between entities in the query and documents (Morton, 1999)(Breck et al., 1999) (Gard et al., 1999) (Humphreys et al., 1999). As exampl</context>
</contexts>
<marker>Moldovan, Harabagiu, Pasca, Mihalcea, Goodrum, Girju, Rus, 1999</marker>
<rawString>Dan Moldovan, Sanda Harabagiu, Marius Pasca, Rada Mihalcea, Richard Goodrum, Roxana Girju, and Vasile Rus. 1999. LASSO: A Tool for Surfing the Answer Net. In Eighth Text REtrieval Conference (TRE, 1999).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas S Morton</author>
</authors>
<title>Using Coreference in Question Answering.</title>
<date>1999</date>
<booktitle>In Eighth Text REtrieval Conference (TRE,</booktitle>
<contexts>
<context position="3265" citStr="Morton, 1999" startWordPosition="508" endWordPosition="509">participants in last TREC QA Track. Examples of these systems are (Moldovan et al., 1999) (Singhal et al., 1999) (Prager et al., 1999) (Takaki, 1999) (Hull, 1999) (Cormack et al., 1999). After reviewing these approaches, we can notice that there is a general agreement about the importance of several Natural Language Processing (NLP) techniques for QA task. Pos-tagging, parsing and Name Entity recognition are used by most of the systems. However, few systems apply other NLP techniques. Particularly, only four systems model some coreference relations between entities in the query and documents (Morton, 1999)(Breck et al., 1999) (Gard et al., 1999) (Humphreys et al., 1999). As example, Morton approach models identity, definite nounphrases and non-possessive third person pronouns. Nevertheless, benefits of applying these coreference techniques have not been analysed and measured separately. The second group includes noun-phrase extraction systems. These approaches try to find the precise information requested by questions whose answer is defined typically by a noun phrase. MURAX is one of these systems (Kupiec, 1999). It can use information from different sentences, paragraphs and even different do</context>
</contexts>
<marker>Morton, 1999</marker>
<rawString>Thomas S. Morton. 1999. Using Coreference in Question Answering. In Eighth Text REtrieval Conference (TRE, 1999).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas W Oard</author>
<author>Jianqiang Wang</author>
<author>Dekang Lin</author>
<author>Ian Soboroff</author>
</authors>
<date>1999</date>
<booktitle>TREC-8 Experiments at Maryland: CLIR, QA and Routing. In Eighth Text REtrieval Conference (TRE,</booktitle>
<marker>Oard, Wang, Lin, Soboroff, 1999</marker>
<rawString>Douglas W. Oard, Jianqiang Wang, Dekang Lin, and Ian Soboroff. 1999. TREC-8 Experiments at Maryland: CLIR, QA and Routing. In Eighth Text REtrieval Conference (TRE, 1999).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Prager</author>
<author>Dragomir Radev</author>
<author>Eric Brown</author>
<author>Anni Coden</author>
<author>Valerie Samn</author>
</authors>
<title>The Use of Predictive Annotation for Question Answering.</title>
<date>1999</date>
<booktitle>In Eighth Text REtrieval Conference (TRE,</booktitle>
<contexts>
<context position="2786" citStr="Prager et al., 1999" startWordPosition="431" endWordPosition="434">e (TRE, 1999). In this conference, nearly twenty different systems were evaluated with very different success rates. We can classify current approaches into two groups: textsnippet extraction systems and noun-phrase extraction systems. Text-snippet extraction approaches are based on locating and extracting the most relevant sentences or paragraphs to the query by supposing that this text will contain the correct answer to the query. This approach has been the most commonly used by participants in last TREC QA Track. Examples of these systems are (Moldovan et al., 1999) (Singhal et al., 1999) (Prager et al., 1999) (Takaki, 1999) (Hull, 1999) (Cormack et al., 1999). After reviewing these approaches, we can notice that there is a general agreement about the importance of several Natural Language Processing (NLP) techniques for QA task. Pos-tagging, parsing and Name Entity recognition are used by most of the systems. However, few systems apply other NLP techniques. Particularly, only four systems model some coreference relations between entities in the query and documents (Morton, 1999)(Breck et al., 1999) (Gard et al., 1999) (Humphreys et al., 1999). As example, Morton approach models identity, definite </context>
</contexts>
<marker>Prager, Radev, Brown, Coden, Samn, 1999</marker>
<rawString>John Prager, Dragomir Radev, Eric Brown, Anni Coden, and Valerie Samn. 1999. The Use of Predictive Annotation for Question Answering. In Eighth Text REtrieval Conference (TRE, 1999).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard A Salton</author>
</authors>
<title>Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer.</title>
<date>1989</date>
<publisher>Addison Wesley,</publisher>
<location>New York.</location>
<contexts>
<context position="10846" citStr="Salton (1989)" startWordPosition="1700" endWordPosition="1701">is a standard IR system that retrieves relevant documents for queries. The second module will manage with anaphora resolution in both, queries and retrieved documents. For this purpose we use SUPAR computational system (section 4.1). And the third one is a sentence-extraction QA system that interacts with SUPAR module and ranks sentences from retrieved documents to locate the answer where the correct answer appears (section 4.2). For the purpose of evaluation an IR system has been implemented. This system is based on the standard information retrieval approach to document ranking described in Salton (1989). For QA task, the same approach has been used as baseline but using sentences as text unit. Each term in the query and documents is assigned an inverse document frequency (idf ) score based on the same corpus. This measure is computed as: where N is the total number of documents in the collection and df(t) is the number of documents which contains term t. Query expansion consists of stemming terms using a version of the Porter stemmer. Document and sentence similarity to the query was computed using the cosine similarity measure. The LAT corpus has been selected as test collection due to his </context>
</contexts>
<marker>Salton, 1989</marker>
<rawString>Gerard A. Salton. 1989. Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer. Addison Wesley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Singhal</author>
<author>Steve Abney</author>
<author>Michiel Bacchiani</author>
<author>Michael Collins</author>
<author>Donald Hindle</author>
<author>Fernando Pereira</author>
</authors>
<date>1999</date>
<booktitle>ATT at TREC-8. In Eighth Text REtrieval Conference (TRE,</booktitle>
<contexts>
<context position="2764" citStr="Singhal et al., 1999" startWordPosition="427" endWordPosition="430"> in last TREC conference (TRE, 1999). In this conference, nearly twenty different systems were evaluated with very different success rates. We can classify current approaches into two groups: textsnippet extraction systems and noun-phrase extraction systems. Text-snippet extraction approaches are based on locating and extracting the most relevant sentences or paragraphs to the query by supposing that this text will contain the correct answer to the query. This approach has been the most commonly used by participants in last TREC QA Track. Examples of these systems are (Moldovan et al., 1999) (Singhal et al., 1999) (Prager et al., 1999) (Takaki, 1999) (Hull, 1999) (Cormack et al., 1999). After reviewing these approaches, we can notice that there is a general agreement about the importance of several Natural Language Processing (NLP) techniques for QA task. Pos-tagging, parsing and Name Entity recognition are used by most of the systems. However, few systems apply other NLP techniques. Particularly, only four systems model some coreference relations between entities in the query and documents (Morton, 1999)(Breck et al., 1999) (Gard et al., 1999) (Humphreys et al., 1999). As example, Morton approach mode</context>
</contexts>
<marker>Singhal, Abney, Bacchiani, Collins, Hindle, Pereira, 1999</marker>
<rawString>Amit Singhal, Steve Abney, Michiel Bacchiani, Michael Collins, Donald Hindle, and Fernando Pereira. 1999. ATT at TREC-8. In Eighth Text REtrieval Conference (TRE, 1999).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toru Takaki</author>
</authors>
<title>NTT DATA: Overview of system approach at TREC-8 ad-hoc and question answering.</title>
<date>1999</date>
<booktitle>In Eighth Text REtrieval Conference (TRE,</booktitle>
<contexts>
<context position="2801" citStr="Takaki, 1999" startWordPosition="435" endWordPosition="436"> conference, nearly twenty different systems were evaluated with very different success rates. We can classify current approaches into two groups: textsnippet extraction systems and noun-phrase extraction systems. Text-snippet extraction approaches are based on locating and extracting the most relevant sentences or paragraphs to the query by supposing that this text will contain the correct answer to the query. This approach has been the most commonly used by participants in last TREC QA Track. Examples of these systems are (Moldovan et al., 1999) (Singhal et al., 1999) (Prager et al., 1999) (Takaki, 1999) (Hull, 1999) (Cormack et al., 1999). After reviewing these approaches, we can notice that there is a general agreement about the importance of several Natural Language Processing (NLP) techniques for QA task. Pos-tagging, parsing and Name Entity recognition are used by most of the systems. However, few systems apply other NLP techniques. Particularly, only four systems model some coreference relations between entities in the query and documents (Morton, 1999)(Breck et al., 1999) (Gard et al., 1999) (Humphreys et al., 1999). As example, Morton approach models identity, definite nounphrases and</context>
</contexts>
<marker>Takaki, 1999</marker>
<rawString>Toru Takaki. 1999. NTT DATA: Overview of system approach at TREC-8 ad-hoc and question answering. In Eighth Text REtrieval Conference (TRE, 1999).</rawString>
</citation>
<citation valid="true">
<authors>
<author>TREC-8</author>
</authors>
<date>1999</date>
<booktitle>Eighth Text REtrieval Conference.</booktitle>
<marker>TREC-8, 1999</marker>
<rawString>TREC-8. 1999. Eighth Text REtrieval Conference.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>