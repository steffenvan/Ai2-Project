<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000143">
<title confidence="0.998873">
Massively Parallel Suffix Array Queries and On-Demand Phrase Extraction
for Statistical Machine Translation Using GPUs
</title>
<author confidence="0.999286">
Hua He
</author>
<affiliation confidence="0.998021">
Dept. of Computer Science
University of Maryland
</affiliation>
<address confidence="0.673922">
College Park, Maryland
</address>
<email confidence="0.998478">
huah@cs.umd.edu
</email>
<author confidence="0.98528">
Jimmy Lin
</author>
<affiliation confidence="0.9848185">
iSchool and UMIACS
University of Maryland
</affiliation>
<address confidence="0.674124">
College Park, Maryland
</address>
<email confidence="0.998558">
jimmylin@umd.edu
</email>
<note confidence="0.389245">
Adam Lopez
HLTCOE
</note>
<address confidence="0.655475">
Johns Hopkins University
Baltimore, Maryland
</address>
<email confidence="0.999162">
alopez@cs.jhu.edu
</email>
<sectionHeader confidence="0.995582" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998298545454546">
Translation models in statistical machine
translation can be scaled to large corpora
and arbitrarily-long phrases by looking up
translations of source phrases “on the fly”
in an indexed parallel corpus using suffix
arrays. However, this can be slow because
on-demand extraction of phrase tables is
computationally expensive. We address this
problem by developing novel algorithms for
general purpose graphics processing units
(GPUs), which enable suffix array queries
for phrase lookup and phrase extraction to
be massively parallelized. Compared to
a highly-optimized, state-of-the-art serial
CPU-based implementation, our techniques
achieve at least an order of magnitude
improvement in terms of throughput. This
work demonstrates the promise of massively
parallel architectures and the potential
of GPUs for tackling computationally-
demanding problems in statistical machine
translation and language processing.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999867590909091">
Efficiently handling large translation models is a
perennial problem in statistical machine translation.
One particularly promising solution (§2) is to use
the parallel text itself as an implicit representation
of the translation model and extract translation units
“on the fly” when they are needed to decode new
input (Brown, 2004). This idea has been applied
to phrase-based (Callison-Burch et al., 2005; Zhang
and Vogel, 2005), hierarchical (Lopez, 2007; Lopez,
2008b; Lopez, 2008a), and syntax-based (Cromieres
and Kurohashi, 2011) models. A benefit of this
technique is that it scales to arbitrarily large models
with very little pre-processing. For instance, Lopez
(2008b) showed that a translation model trained on
a large corpus with sparse word alignments and
loose extraction heuristics substantially improved
Chinese-English translation. An explicit represen-
tation of the model would have required nearly a
terabyte of memory, but its implicit representation
using the parallel text required only a few gigabytes.
Unfortunately, there is substantial computational
cost in searching a parallel corpus for source
phrases, extracting their translations, and scoring
them on the fly. Since the number of possible
translation units may be quite large (for example,
all substrings of a source sentence) and their
translations are numerous, both phrase lookup and
extraction are performance bottlenecks. Despite
considerable research and the use of efficient
indexes like suffix arrays (Manber and Myers,
1990), this problem remains not fully solved.
We show how to exploit the massive parallelism
offered by modern general purpose graphics pro-
cessing units (GPUs) to eliminate the computational
bottlenecks associated with “on the fly” phrase ex-
traction. GPUs have previously been applied to
DNA sequence matching using suffix trees (Schatz
et al., 2007) and suffix arrays (Gharaibeh and Ri-
peanu, 2010). Building on this work, we present
two novel contributions: First, we describe improved
GPU algorithms for suffix array queries that achieve
greater parallelism (§3). Second, we propose novel
data structures and algorithms for phrase extraction
(§4) and scoring (§5) that are amenable to GPU par-
</bodyText>
<page confidence="0.986701">
325
</page>
<note confidence="0.4705865">
Proceedings of NAACL-HLT 2013, pages 325–334,
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999960733333333">
allelization. The resulting implementation achieves
at least an order of magnitude higher throughput
than a state-of-the-art single-threaded CPU imple-
mentation (§6). Since our experiments verify that
the GPU implementation produces exactly the same
results as a CPU reference implementation on a full
extraction, we can simply replace that component
and reap significant performance advantages with no
impact on translation quality. To the best of our
knowledge, this is the first reported application of
GPU acceleration techniques for statistical machine
translation. We believe these results reveal a promis-
ing yet unexplored future direction in exploiting par-
allelism to tackle perennial performance bottlenecks
in state-of-the-art translation models.
</bodyText>
<sectionHeader confidence="0.966571" genericHeader="introduction">
2 Phrase Extraction On Demand
</sectionHeader>
<bodyText confidence="0.999622666666667">
Lopez (2008b) provides the following recipe for
“translation by pattern matching”, which we use as
a guide for the remainder of this paper:
</bodyText>
<listItem confidence="0.939506666666667">
Algorithm 1 Translation by pattern matching
1: for each input sentence do
2: for each possible phrase in the sentence do
3: Find its occurrences in the source text
4: for each occurrence do
5: Extract its aligned target phrase (if any)
6: for each extracted phrase pair do
7: Compute feature values
8: Decode as usual using the scored rules
</listItem>
<bodyText confidence="0.9996714">
The computational bottleneck occurs in lines 2–7:
there are vast numbers of query phrases, matching
occurrences, and extracted phrase pairs to process in
the loops. In the next three sections, we attack each
problem in turn.
</bodyText>
<sectionHeader confidence="0.903649" genericHeader="method">
3 Finding Every Phrase
</sectionHeader>
<bodyText confidence="0.99982325">
First, we must find all occurrences of each source
phrase in the input (line 3, Algorithm 1). This
is a classic application of string pattern matching:
given a short query pattern, the task is to find all
occurrences in a much larger text. Solving the
problem efficiently is crucial: for an input sentence
F of length |F|, each of its O(|F |2) substrings is a
potential query pattern.
</bodyText>
<subsectionHeader confidence="0.996033">
3.1 Pattern Matching with Suffix Arrays
</subsectionHeader>
<bodyText confidence="0.999494043478261">
Although there are many algorithms for pattern
matching, all of the examples that we are aware
of for machine translation rely on suffix arrays.
We briefly review the classic algorithms of Manber
and Myers (1990) here since they form the basis
of our techniques and analysis, but readers who
are familiar with them can safely skip ahead to
additional optimizations (§3.2).
A suffix array represents all suffixes of a corpus
in lexicographical order. Formally, for a text T, the
ith suffix of T is the substring of the text beginning
at position i and continuing to the end of T. Each
suffix can therefore be uniquely identified by the
index i of its first word. A suffix array 5(T)
of T is a permutation of these suffix identifiers
[1, |T|] arranged by the lexicographical order of the
corresponding suffixes—in other words, the suffix
array represents a sorted list of all suffixes in T.
With both T and 5(T) in memory, we can find any
query pattern Q in O(|Q |log |T |) time by compar-
ing pattern Q against the first |Q |characters of up to
log |T |different suffixes using binary search.
An inefficiency in this solution is that each com-
parison in the binary search algorithm requires com-
paring all |Q |characters of the query pattern against
some suffix of text T. We can improve on this using
an observation about the longest common prefix
(LCP) of the query pattern and the suffix against
which it is compared. Suppose we search for a query
pattern Q in the span of the suffix array beginning at
suffix L and ending at suffix R. For any suffix M
which falls lexicographically between those at L and
R, the LCP of Q and M will be at least as long as
the LCP of Q and L or Q and R. Hence if we know
the quantity h = MIN(LCP(Q, L), LCP(Q, R)) we
can skip comparisons of the first h symbols between
Q and the suffix M, since they must be the same.
The solution of Manber and Myers (1990) ex-
ploits this fact along with the observation that each
comparison in binary search is carried out accord-
ing to a fixed recursion scheme: a query is only
ever compared against a specific suffix M for a
single range of suffixes bounded by some fixed L
and R. Hence if we know the longest common
prefix between M and each of its corresponding
L and R according to the fixed recursions in the
</bodyText>
<page confidence="0.998442">
326
</page>
<bodyText confidence="0.999947866666667">
algorithm, we can maintain a bound on h and reduce
the aggregate number of symbol comparisons to
O(|Q |+ log |T|). To accomplish this, in addition
to the suffix array, we pre-compute two other arrays
of size |T |for both left and right recursions (called
the LCP arrays).
Memory use is an important consideration, since
GPUs have less memory than CPUs. For the algo-
rithms described here, we require four arrays: the
original text T, the suffix array 5(T), and the two
LCP arrays. We use a representation of T in which
each word has been converted to a unique integer
identifier; with 32-bit integers the total number of
bytes is 16|T |. As we will show, this turns out to be
quite modest, even for large parallel corpora (§6).
</bodyText>
<subsectionHeader confidence="0.998676">
3.2 Suffix Array Efficiency Tricks
</subsectionHeader>
<bodyText confidence="0.983301">
Previous work on translation by pattern matching
using suffix arrays on serial architectures has pro-
duced a number of efficiency optimizations:
</bodyText>
<listItem confidence="0.977203833333333">
1. Binary search bounds for longer substrings are
initialized to the bounds of their longest prefix.
Substrings are queried only if their longest
prefix string was matched in the text.
2. In addition to conditioning on the longest pre-
fix, Zhang and Vogel (2005) and Lopez (2007)
condition on a successful query for the longest
proper suffix.
3. Lopez (2007) queries each unique substring
of a sentence exactly once, regardless of how
many times it appears in an input sentence.
4. Lopez (2007) directly indexes one-word sub-
strings with a small auxiliary array, so that
their positions in the suffix array can be found
in constant time. For longer substrings, this
optimization reduces the log |T |term of query
complexity to log(count(a)), where a is the
first word of the query string.
</listItem>
<bodyText confidence="0.997771888888889">
Although these efficiency tricks are important in the
serial algorithms that serve as our baseline, not all
of them are applicable to parallel architectures. In
particular, optimizations (1), (2), and (3) introduce
order dependencies between queries; they are disre-
garded in our GPU implementation so that we can
fully exploit parallelization opportunities. We have
not yet fully implemented (4), which is orthogonal
to parallelization: this is left for future work.
</bodyText>
<subsectionHeader confidence="0.999814">
3.3 Finding Every Phrase on a GPU
</subsectionHeader>
<bodyText confidence="0.999983434782609">
Recent work in computational biology has shown
that suffix arrays are particularly amenable to GPU
acceleration: the suffix-array-based DNA sequence
matching system MummurGPU++ (Gharaibeh and
Ripeanu, 2010) has been reported to outperform the
already fast MummurGPU 2 (Trapnell and Schatz,
2009), based on suffix trees (an alternative indexing
structure). Here, we apply the same ideas to ma-
chine translation, introducing some novel improve-
ments to their algorithms in the process.
A natural approach to parallelism is to perform
all substring queries in parallel (Gharaibeh and Ri-
peanu, 2010). There are no dependencies between
iterations of the loop beginning on line 2 of Algo-
rithm 1, so for input sentence F, we can parallelize
by searching for all O(|F |2) substrings concurrently.
We adopt this approach here.
However, naive application of query-level paral-
lelism leads to a large number of wasted threads,
since most long substrings of an input sentence will
not be found in the text. Therefore, we employ
a novel two-pass strategy: in the first pass, we
simply compute, for each position i in the input
sentence, the length j of the longest substring in F
that appears in T. These computations are carried
out concurrently for every position i. During this
pass, we also compute the suffix array bounds of the
one-word substring F[i], to be used as input to the
second pass—a variant of optimizations (1) and (4)
discussed in §3.2. On the second pass, we search
for all substrings F[i, k] for all k E [i + 1, i + j].
These computations are carried out concurrently for
all substrings longer than one word.
Even more parallelization is possible. As we saw
in §3.1, each query in a suffix array actually requires
two binary searches: one each for the first and last
match in 5(T). The abundance of inexpensive
threads on a GPU permits us to perform both queries
concurrently on separate threads. By doing this in
both passes we utilize more of the GPU’s processing
power and obtain further speedups.
As a simple example, consider an input sentence
“The government puts more tax on its citizens”, and
suppose that substrings “The government”, “gov-
ernment puts”, and “puts more tax” are found in
the training text, while none of the words in “on
</bodyText>
<page confidence="0.941015">
327
</page>
<bodyText confidence="0.911758636363636">
Initial Word Longest Match Substrings Threads
1st pass 2nd pass
The 2 The, The government 2 2
government 2 government, government puts 2 2
puts 3 puts, puts more, puts more tax 2 4
more 2 more, more tax 2 2
tax 1 tax 2 0
on 0 – 2 0
its 0 – 2 0
citizens 0 – 2 0
Total Threads: 16 10
</bodyText>
<tableCaption confidence="0.950463">
Table 1: Example of how large numbers of suffix array queries can be factored across two highly parallel passes on a
GPU with a total of 26 threads to perform all queries for this sample input sentence.
</tableCaption>
<bodyText confidence="0.999962214285714">
its citizens” are found. The number of threads
spawned is shown in Table 1: all threads during a
pass execute in parallel, and each thread performs a
binary search which takes no more than O(|Q |+
log |T |) time. While spawning so many threads
may seem wasteful, this degree of parallelization
still under-utilizes the GPU; the hardware we use
(§6) can manage up to 21,504 concurrent threads
in its resident occupancy. To fully take advantage
of the processing power, we process multiple input
sentences in parallel. Compared with previous
algorithms, our two-pass approach and our strategy
of thread assignment to increase the amount of
parallelism represent novel contributions.
</bodyText>
<sectionHeader confidence="0.948766" genericHeader="method">
4 Extracting Aligned Target Phrases
</sectionHeader>
<bodyText confidence="0.999916235294118">
The problem at line 5 of Algorithm 1 is to extract the
target phrase aligned to each matching source phrase
instance. Efficiency is crucial since some source
phrases occur hundreds of thousands of times.
Phrase extraction from word alignments typically
uses the consistency check of Och et al. (1999). A
consistent phrase is one for which no words inside
the phrase pair are aligned to words outside the
phrase pair. Usually, consistent pairs are computed
offline via dynamic programming over the align-
ment grid, from which we extract all consistent
phrase pairs up to a heuristic bound on phrase length.
The online extraction algorithm of Lopez (2008a)
checks for consistent phrases in a different manner.
Rather than finding all consistent phrase pairs in
a sentence, the algorithm asks: given a specific
source phrase, is there a consistent phrase pair
</bodyText>
<figureCaption confidence="0.9751015">
Figure 1: Source phrase f2f3f4 and target phrase
e2e3e4 are extracted as a consistent pair, since the back-
projection is contained within the original source span.
Figure 2: Source phrase f2f3f4 and target phrase e2e3e4
should not be extracted, since the back-projection is not
contained within the original source span.
</figureCaption>
<bodyText confidence="0.999897071428571">
of which it is one side? To answer this, it first
computes the projection of the source phrase in the
target sentence: the minimum span containing all
words that are aligned to any word of the source
span. It then computes the projection of the target
span back into the source; if this back-projection
is contained within the original source span, the
phrase pair is consistent, and the target span is
extracted as the translation of the source. Figure 1
shows a “good” pair for source phrase f2f3f4, since
the back-projection is contained within the original
source span, whereas Figure 2 shows a “bad” pair
for source phrase f2f3f4 since the back-projection
is not contained within the original source span.
</bodyText>
<page confidence="0.992992">
328
</page>
<subsectionHeader confidence="0.996924">
4.1 Sampling Consistent Phrases
</subsectionHeader>
<bodyText confidence="0.999668928571428">
Regardless of how efficient the extraction of a single
target phrase is made, the fact remains that there
are many phrases to extract. For example, in our
Chinese Xinhua dataset (see §6), from 8,000 input
query sentences, about 20 million source substrings
can be extracted. The standard solution to this
problem is to sample a set of occurrences of each
source phrase, and only extract translations for those
occurrences (Callison-Burch et al., 2005; Zhang and
Vogel, 2005). As a practical matter, this can be done
by sampling at uniform intervals from the matching
span of a suffix array. Lopez (2008a) reports a
sample size of 300; for phrases occurring fewer than
300 times, all translations are extracted.
</bodyText>
<subsectionHeader confidence="0.756651">
4.2 GPU Implementation
</subsectionHeader>
<bodyText confidence="0.9999605">
We present novel data structures and an algorithm
for efficient phrase extraction, which together are
amenable to massive parallelization on GPUs. The
basic insight is to pre-compute data structures for
the source-to-target alignment projection and back-
projection procedure described by Lopez (2008a)
for checking consistent alignments.
Let us consider a single matching substring (from
the output of the suffix array queries), span [i, j] in
the source text T. For each k, we need to know the
leftmost and rightmost positions that it aligns to in
the target T&apos;. For this purpose we can define the
target span [i&apos;, j&apos;], along with leftmost and rightmost
arrays L and R as follows:
</bodyText>
<equation confidence="0.99282125">
i&apos; := min
kE[i,j]
j&apos; :=max
kE[i,j]
</equation>
<bodyText confidence="0.9916825">
The arrays L and R are each of length |T|, in-
dexed by absolute corpus position. Each array
element contains the leftmost and rightmost extents
of the source-to-target alignments (in the target),
respectively. Note that in order to save space,
the values stored in the arrays are sentence-relative
positions (e.g., token count from the beginning of
each sentence), so that we only need one byte per
array entry. Thus, i&apos; and j&apos; are sentence-relative
positions (in the target).
Similarly, for the back-projection, we use two
arrays L&apos; and R&apos; on the target side (length |T&apos;|) to
keep track of the leftmost and rightmost positions
that k&apos; in the target training text align to, as below:
</bodyText>
<equation confidence="0.9556625">
i&apos;&apos; := min
k0E[s0+i0,s0+j0]
j&apos;&apos; :=max
k0E[s0+i0,s0+j0]
</equation>
<bodyText confidence="0.99542432">
The arrays L&apos; and R&apos; are indexed by absolute corpus
positions, but their contents are sentence relative
positions (on the source side). To index the arrays
L&apos; and R&apos;, we also need to obtain the corresponding
target sentence start position s&apos;. Note that the back-
projected span [i&apos;&apos;, j&apos;&apos;] may or may not be the same
as the original span [i, j]. In fact, this is exactly what
we must check for to ensure a consistent alignment.
The suffix array gives us i, which is an ab-
solute corpus position, but we need to know the
sentence-relative position, since the spans computed
by R, L, R&apos;, L&apos; are all sentence relative. To solve
this, we introduce an array P (length |T |) that gives
the relative sentence position of each source word.
We then pack the three source side arrays (R, L,
and P) into a single RLP array of 32-bit integers
(note that we are actually wasting one byte per array
element). Finally, since the end-of-sentence special
token is not used in any of R, L, or P, its position
in RLP can be used to store an index to the start
of the corresponding target sentence in the target
array T&apos;. Now, given a source phrase spanning
[i, j] (recall, these are absolute corpus positions), our
phrase extraction algorithm is as follows:
Algorithm 2 Efficient Phrase Extraction Algorithm
</bodyText>
<listItem confidence="0.9841285">
1: for each source span [i, j] do
2: Compute [i&apos;, j&apos;]
3: s := i − P[i] − 1
4: s&apos; := RLP[s]
5: i&apos;&apos; := mink0E[s0+i0,s0+j0] L&apos;(k&apos;)
6: j&apos;&apos; :=maxk0E[s0+i0,s0+j0] R&apos;(k&apos;)
7: If i − s = i&apos;&apos; and j − s = j&apos;&apos; then
8: Extract T [i, j] with T&apos;[s&apos; + i&apos;, s&apos; + j&apos;]
</listItem>
<bodyText confidence="0.989837714285714">
where s is the source sentence start position of a
given source phrase and s&apos; is the target sentence
start position. If the back-projected spans match the
original spans, the phrase pair T[i, j] and T&apos;[s&apos; +
i&apos;, s&apos; + j&apos;] is extracted.
In total, the data structures RLP, R&apos;, and L&apos;
require 4|T  |+ �|T&apos; |bytes. Not only is this phrase
</bodyText>
<equation confidence="0.57179775">
L(k)
R(k)
L&apos;(k&apos;)
R&apos;(k&apos;)
</equation>
<page confidence="0.995695">
329
</page>
<bodyText confidence="0.999911684210526">
extraction algorithm fast—requiring only a few in-
direct array references—the space requirements for
the auxiliary data structures are quite modest.
Given sufficient resources, we would ideally par-
allelize the phrase table creation process for each
occurrence of the matched source substring. How-
ever, the typical number of source substring matches
for an input sentence is even larger than the number
of threads available on GPUs, so this strategy does
not make sense due to context switching overhead.
Instead, GPU thread blocks (groups of 512 threads)
are used to process each source substring. This
means that for substrings with large numbers of
matches, one thread in the GPU block would process
multiple occurrences. This strategy is widely used,
and according to GPU programming best practices
from NVIDIA, allocating more work to a single
thread maintains high GPU utilization and reduces
the cost of context switches.
</bodyText>
<sectionHeader confidence="0.975099" genericHeader="method">
5 Computing Every Feature
</sectionHeader>
<bodyText confidence="0.9999772">
Finally, we arrive at line 7 in Algorithm 3, where
we must compute feature values for each extracted
phrase pair. Following the implementation of gram-
mar extraction used in cdec (Lopez, 2008a), we
compute several widely-used features:
</bodyText>
<listItem confidence="0.949478076923077">
1. Pair count feature, c(e, f).
2. The joint probability of all target-to-source
phrase translation probabilities, p(elf)
= c(e, f)/c(f), where e is target phrase, f is
the source phrase.
3. The logarithm of the target-to-source lexical
weighting feature.
4. The logarithm of the source-to-target lexical
weighting feature.
5. The coherence probability, defined as the ratio
between the number of successful extractions
of a source phrase to the total count of the
source phrase in the suffix array.
</listItem>
<bodyText confidence="0.999518">
The output of our phrase extraction is a large
collection of phrase pairs. To extract the above fea-
tures, aggregate statistics need to be computed over
phrase pairs. To make the solution both compact
and efficient, we first sort the unordered collection
of phrases from the GPU into an array, then the
aggregate statistics can be obtained in a single pass
over the array, since identical phrase pairs are now
grouped together.
</bodyText>
<sectionHeader confidence="0.99783" genericHeader="method">
6 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999952465116279">
We tested our GPU-based grammar extraction im-
plementation under the conditions in which it would
be used for a Chinese-to-English machine transla-
tion task, in particular, replicating the data condi-
tions of Lopez (2008b). Experiments were per-
formed on two data sets. First, we used the source
(Chinese) side of news articles collected from the
Xinhua Agency, with around 27 million words of
Chinese in around one million sentences (totaling
137 MB). Second, we added source-side parallel text
from the United Nations, with around 81 million
words of Chinese in around four million sentences
(totaling 561 MB). In a pre-processing phase, we
mapped every word to a unique integer, with two
special integers representing end-of-sentence and
end-of-corpus, respectively.
Input query data consisted of all sentences from
the NIST 2002–2006 translation campaigns, tok-
enized and integerized identically to the training
data. On average, sentences contained around 29
words. In order to fully stress our GPU algorithms,
we ran tests on batches of 2,000, 4,000, 6,000,
8,000, and 16,000 sentences. Since there are only
around 8,000 test sentences in the NIST data, we
simply duplicated the test data as necessary.
Our experiments used NVIDIA’s Tesla C2050
GPU (Fermi Generation), which has 448 CUDA
cores with a peak memory bandwidth 144 GB/s.
Note that the GPU was released in early 2010
and represents previous generation technology.
NVIDIA’s current GPUs (Kepler) boasts raw
processing power in the 1.3 TFlops (double
precision) range, which is approximately three
times the GPU we used. Our CPU is a 3.33 GHz
Intel Xeon X5260 processor, which has two cores.
As a baseline, we compared against the publicly
available implementation of the CPU-based algo-
rithms described by Lopez (2008a) found in the
pycdec (Chahuneau et al., 2012) extension of the
cdec machine translation system (Dyer et al., 2010).
Note that we only tested grammar extraction for
continuous pairs of phrases, and we did not test the
slower and more complex queries for hierarchical
</bodyText>
<page confidence="0.994696">
330
</page>
<table confidence="0.9998979">
Input Sentences 2,000 4,000 6,000 8,000 16,000
Number of Words 57,868 117,854 161,883 214,246 428,492
Xinhua
With Sampling (s300) GPU (words/second) 3811 4723 5496 6391 12405
(21.9) (20.4) (32.1) (29.7) (36.0)
CPU (words/second) 200 (1.5)
Speedup 19× 24× 27× 32× 62×
No Sampling (sem) GPU (words/second) 1917 2859 3496 4171 8186
(8.5) (11.1) (19.9) (23.2) (27.6)
CPU (words/second) 1.13 (0.02)
Speedup 1690× 2520× 3082× 3677× 7217×
Xinhua + UN
With Sampling (s300) GPU (words/second) 2021 2558 2933 3439 6737
(5.3) (10.7) (13.9) (15.2) (29.0)
CPU (words/second) 157 (1.8)
Speedup 13× 16× 19× 22× 43×
No Sampling (sem) GPU (words/second) 500.5 770.1 984.6 1243.8 2472.3
(2.5) (3.9) (5.8) (5.4) (12.0)
CPU (words/second) 0.23 (0.002)
Speedup 2194× 3375× 4315× 5451× 10836×
</table>
<tableCaption confidence="0.988176333333333">
Table 2: Comparing the GPU and CPU implementations for phrase extraction on two different corpora. Throughput
is measured in words per second under different test set sizes; the 95% confidence intervals across five trials are given
in parentheses, along with relative speedups comparing the two implementations.
</tableCaption>
<bodyText confidence="0.99952">
(gappy) patterns described by Lopez (2007). Both
our implementation and the baseline are written
primarily in C/C++.1
Our source corpora and test data are the same
as that presented in Lopez (2008b), and using the
CPU implementation as a reference enabled us to
confirm that our extracted grammars and features
are identical (modulo sampling). We timed our
GPU implementation as follows: from the loading
of query sentences, extractions of substrings and
grammar rules, until all grammars for all sentences
are generated in memory. Timing does not include
offline preparations such as the construction of the
suffix array on source texts and the I/O costs for
writing the per-sentence grammar files to disk. This
timing procedure is exactly the same for the CPU
</bodyText>
<footnote confidence="0.43065">
1The Chahuneau et al. (2012) implementation is in Cython,
a language for building Python applications with performance-
critical components in C. In particular, all of the suffix array
code that we instrumented for these experiments are compiled
to C/C++. The implementation is a port of the original code
written by Lopez (2008a) in Pyrex, a precursor to Cython.
Much of the code is unchanged from the original version.
</footnote>
<bodyText confidence="0.999827166666667">
baseline. We are confident that our results represent
a fair comparison between the GPU and CPU, and
are not attributed to misconfigurations or other flaws
in experimental procedures. Note that the CPU
implementation runs in a single thread, on the same
machine that hosts the GPU (described above).
</bodyText>
<sectionHeader confidence="0.999726" genericHeader="evaluation">
7 Results
</sectionHeader>
<bodyText confidence="0.999754">
Table 2 shows performance results comparing our
GPU implementation against the reference CPU
implementation for phrase extraction. In one ex-
perimental condition, the sampling parameter for
frequently-matching phrases is set to 300, per Lopez
(2008a), denoted s300. The experimental condition
without sampling is denoted sem. Following stan-
dard settings, the maximum length of the source
phrase is set to 5 and the maximum length of the
target phrase is set to 15 (same for both GPU
and CPU implementations). The table is divided
into two sections: the top shows results on the
Xinhua data, and the bottom on Xinhua + UN
data. Columns report results for different numbers
</bodyText>
<page confidence="0.995303">
331
</page>
<table confidence="0.998066666666667">
# Sent. 2000 4000 6000 8000 16000
Speedup 9.6x 14.3x 17.5x 20.9x 40.9x
Phrases 2.1x 1.8x 1.7x 1.6x 1.6x
</table>
<tableCaption confidence="0.94269025">
Table 3: Comparing no sampling on the GPU with sam-
pling on the CPU in terms of performance improvements
(GPU over CPU) and increases in the number of phrase
pairs extracted (GPU over CPU).
</tableCaption>
<bodyText confidence="0.999821586206897">
of input sentences. Performance is reported in terms
of throughput: the number of processed words per
second on average (i.e., total time divided by the
batch size in words). The results are averaged over
five trials, with 95% confidence intervals shown in
parentheses. Note that as the batch size increases,
we achieve higher throughput on the GPU since
we are better saturating its full processing power.
In contrast, performance is constant on the CPU
regardless of the number of sentences processed.
The CPU throughput on the Xinhua data is 1.13
words per second without sampling and 200 words
per second with sampling. On 16,000 test sentences,
we have mostly saturated the GPU’s processing
power, and observe a 7217x speedup over the CPU
implementation without sampling and 62x speedup
with sampling. On the larger (Xinhua + UN)
corpus, we observe 43x and 10836x speedup with
sampling and no sampling, respectively.
Interestingly, a run without sampling on the GPU
is still substantially faster than a run with sampling
on the CPU. On the Xinhua corpus, we observe
speedups ranging from nine times to forty times, as
shown in Table 3. Without sampling, we are able to
extract up to twice as many phrases.
In previous CPU implementations of on-the-fly
phrase extraction, restrictions were placed on the
maximum length of the source and target phrases
due to computational constraints (in addition to sam-
pling). Given the massive parallelism afforded by
the GPU, might we be able to lift these restrictions
and construct the complete phrase table? To answer
this question, we performed an experiment without
sampling and without any restrictions on the length
of the extracted phrases. The complete phrase
table contained about 0.5% more distinct pairs, with
negligible impact on performance.
When considering these results, an astute reader
might note that we are comparing performance
of a single-threaded implementation with a fully-
saturated GPU. To address this concern, we
conducted an experiment using a multi-threaded
version of the CPU reference implementation to
take full advantage of multiple cores on the CPU (by
specifying the -j option in cdec); we experimented
with up to four threads to fully saturate the
dual-core CPU. In terms of throughput, the CPU
implementation scales linearly, i.e., running on four
threads achieves roughly 4x throughput. Note that
the CPU and GPU implementations take advantage
of parallelism in completely different ways: cdec
can be characterized as embarrassingly parallel, with
different threads processing each complete sentence
in isolation, whereas our GPU implementation
achieves intra-sentential parallelism by exploiting
many threads to concurrently process each sentence.
In terms of absolute performance figures, even
with the 4x throughput improvement from fully
saturating the CPU, our GPU implementation
remains faster by a wide margin. Note that neither
our GPU nor CPU represents state-of-the-art
hardware, and we would expect the performance
advantage of GPUs to be even greater with latest
generation hardware, since the number of available
threads on a GPU is increasing faster than the
number of threads available on a CPU.
Since phrase extraction is only one part of an
end-to-end machine translation system, it makes
sense to examine the overall performance of the
entire translation pipeline. For this experiment, we
used our GPU implementation for phrase extrac-
tion, serialized the grammar files to disk, and used
cdec for decoding (on the CPU). The comparison
condition used cdec for all three stages. We used
standard phrase length constraints (5 on source side,
15 on target side) with sampling of frequent phrases.
Finally, we replicated the data conditions in Lopez
(2008a), where our source corpora was the Xinhua
data set and our development/test sets were the
NIST03/NIST05 data; the NIST05 test set contains
1,082 sentences.
Performance results for end-to-end translation are
shown in Table 4, broken down in terms of total
amount of time for each of the processing stages
for the entire test set under different conditions.
In the decoding stage, we varied the number of
CPU threads (note here we do not observe linear
</bodyText>
<page confidence="0.991767">
332
</page>
<table confidence="0.999848">
Phrase Extraction I/O Decoding
1 thread 55.7
GPU: 11.0 2 threads 35.3
3.7 3 threads 31.5
CPU: 166.5 4 threads 26.2
</table>
<tableCaption confidence="0.949806">
Table 4: End-to-end machine translation performance:
time to process the NIST05 test set in seconds, broken
down in terms of the three processing stages.
</tableCaption>
<bodyText confidence="0.999669888888889">
speedup). In terms of end-to-end results, complete
translation of the test set takes 41 seconds with the
GPU for phrase extraction and CPU for decoding,
compared to 196 seconds using the CPU for both
(with four decoding threads in both cases). This rep-
resents a speedup of 4.8x, which suggests that even
selective optimizations of individual components in
the MT pipeline using GPUs can make a substantial
difference in overall performance.
</bodyText>
<sectionHeader confidence="0.99925" genericHeader="discussions">
8 Future Work
</sectionHeader>
<bodyText confidence="0.999978307692308">
There are a number of directions that we have
identified for future work. For computational ef-
ficiency reasons, previous implementations of the
“translation by pattern matching” approach have
had to introduce approximations, e.g., sampling and
constraints on phrase lengths. Our results show that
the massive amounts of parallelism available in the
GPU make these approximations unnecessary, but
it is unclear to what extent they impact translation
quality. For example, Table 3 shows that we extract
up to twice as many phrase pairs without sampling,
but do these pairs actually matter? We have begun to
examine the impact of various settings on translation
quality and have observed small improvements in
some cases (which, note, come for “free”), but so
far the results have not been conclusive.
The experiments in this paper focus primarily
on throughput, but for large classes of applications
latency is also important. One current limitation of
our work is that large batch sizes are necessary to
fully utilize the available processing power of the
GPU. This and other properties of the GPU, such as
the high latency involved in transferring data from
main memory to GPU memory, make low-latency
processing a challenge, which we hope to address.
Another broad future direction is to “GPU-ify”
other machine translation models and other com-
ponents in the machine translation pipeline. An
obvious next step is to extend our work to the
hierarchical phrase-based translation model (Chi-
ang, 2007), which would involve extracting “gappy”
phrases. Lopez (2008a) has tackled this problem
on the CPU, but it is unclear to what extent the
same types of algorithms he proposed can execute
efficiently in the GPU environment. Beyond phrase
extraction, it might be possible to perform decoding
itself in the GPU—not only will this exploit massive
amounts of parallelism, but also reduce costs in
moving data to and from the GPU memory.
</bodyText>
<sectionHeader confidence="0.995904" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999979133333333">
GPU parallelism offers many promises for practical
and efficient implementations of language process-
ing systems. This promise has been demonstrated
for speech recognition (Chong et al., 2008; Chong
et al., 2009) and parsing (Yi et al., 2011), and we
have demonstrated here that it extends to machine
translation as well. We believe that explorations of
modern parallel hardware architectures is a fertile
area of research: the field has only begun to exam-
ine the possibilities and there remain many more
interesting questions to tackle. Parallelism is critical
not only from the perspective of building real-world
applications, but for overcoming fundamental com-
putational bottlenecks associated with models that
researchers are developing today.
</bodyText>
<sectionHeader confidence="0.998226" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999533625">
This research was supported in part by the BOLT
program of the Defense Advanced Research Projects
Agency, Contract No. HR0011-12-C-0015; NSF
under award IIS-1144034. Any opinions, findings,
conclusions, or recommendations expressed in this
paper are those of the authors and do not necessarily
reflect views of the sponsors. The second author is
grateful to Esther and Kiri for their loving support
and dedicates this work to Joshua and Jacob. We
would like to thank three anonymous reviewers for
providing helpful suggestions and also acknowledge
Benjamin Van Durme and CLIP labmates for useful
discussions. We also thank UMIACS for providing
hardware resources via the NVIDIA CUDA Center
of Excellence, UMIACS IT staff, especially Joe
Webster, for excellent support.
</bodyText>
<page confidence="0.999008">
333
</page>
<sectionHeader confidence="0.995875" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999718777777778">
R. D. Brown. 2004. A modified Burrows-Wheeler
Transform for highly-scalable example-based transla-
tion. In Proceedings of the 6th Conference of the
Association for Machine Translation in the Americas
(AMTA 2004), pages 27–36.
C. Callison-Burch, C. Bannard, and J. Schroeder. 2005.
Scaling phrase-based statistical machine translation to
larger corpora and longer phrases. In Proceedings
of the 43rd Annual Meeting on Association for
Computational Linguistics (ACL 2005), pages 255–
262.
V. Chahuneau, N. A. Smith, and C. Dyer. 2012. pycdec:
A Python interface to cdec. In Proceedings of the 7th
Machine Translation Marathon (MTM 2012).
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201–228.
J. Chong, Y. Yi, A. Faria, N. R. Satish, and K. Keutzer.
2008. Data-parallel large vocabulary continuous
speech recognition on graphics processors. In Pro-
ceedings of the Workshop on Emerging Applications
and Manycore Architectures.
J. Chong, E. Gonina, Y. Yi, and K. Keutzer. 2009. A fully
data parallel WFST-based large vocabulary continuous
speech recognition on a graphics processing unit.
In Proceedings of the 10th Annual Conference of
the International Speech Communication Association
(INTERSPEECH 2009), pages 1183–1186.
F. Cromieres and S. Kurohashi. 2011. Efficient retrieval
of tree translation examples for syntax-based machine
translation. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
EMNLP 2011, pages 508–518.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,
P. Blunsom, H. Setiawan, V. Eidelman, and P. Resnik.
2010. cdec: A decoder, alignment, and learning
framework for finite-state and context-free translation
models. In Proceedings of the ACL 2010 System
Demonstrations, pages 7–12.
A. Gharaibeh and M. Ripeanu. 2010. Size matters:
Space/time tradeoffs to improve GPGPU applications
performance. In Proceedings of the 2010 ACM/IEEE
International Conference for High Performance Com-
puting, Networking, Storage and Analysis (SC 2010),
pages 1–12.
A. Lopez. 2007. Hierarchical phrase-based translation
with suffix arrays. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 976–985.
A. Lopez. 2008a. Machine translation by pattern
matching. Ph.D. dissertation, University of Maryland,
College Park, Maryland, USA.
A. Lopez. 2008b. Tera-scale translation models via
pattern matching. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics
(COLING 2008), pages 505–512.
U. Manber and G. Myers. 1990. Suffix arrays: a new
method for on-line string searches. In Proceedings of
the First Annual ACM-SIAM Symposium on Discrete
Algorithms (SODA ’90), pages 319–327.
F. J. Och, C. Tillmann, and H. Ney. 1999. Improved
alignment models for statistical machine translation.
In Proceedings of the 1999 Joint SIGDAT Conference
on Empirical Methods in Natural Language Process-
ing and Very Large Corpora, pages 20–28.
M. Schatz, C. Trapnell, A. Delcher, and A. Varshney.
2007. High-throughput sequence alignment using
graphics processing units. BMC Bioinformatics,
8(1):474.
C. Trapnell and M. C. Schatz. 2009. Optimizing data
intensive GPGPU computations for DNA sequence
alignment. Parallel Computing, 35(8-9):429–440.
Y. Yi, C.-Y. Lai, S. Petrov, and K. Keutzer. 2011.
Efficient parallel CKY parsing on GPUs. In
Proceedings of the 12th International Conference on
Parsing Technologies, pages 175–185.
Y. Zhang and S. Vogel. 2005. An efficient phrase-to-
phrase alignment model for arbitrarily long phrase and
large corpora. In Proceedings of the Tenth Conference
of the European Association for Machine Translation
(EAMT-05).
</reference>
<page confidence="0.999026">
334
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.121791">
<title confidence="0.9996015">Massively Parallel Suffix Array Queries and On-Demand Phrase Extraction for Statistical Machine Translation Using GPUs</title>
<author confidence="0.988143">Hua</author>
<affiliation confidence="0.9998705">Dept. of Computer University of</affiliation>
<address confidence="0.620673">College Park,</address>
<email confidence="0.999709">huah@cs.umd.edu</email>
<author confidence="0.999796">Jimmy Lin</author>
<affiliation confidence="0.815804">iSchool and UMIACS University of Maryland</affiliation>
<address confidence="0.997361">College Park, Maryland</address>
<email confidence="0.999799">jimmylin@umd.edu</email>
<author confidence="0.686428">Adam Johns Hopkins</author>
<affiliation confidence="0.520815">Baltimore,</affiliation>
<email confidence="0.999824">alopez@cs.jhu.edu</email>
<abstract confidence="0.999336782608696">Translation models in statistical machine translation can be scaled to large corpora and arbitrarily-long phrases by looking up translations of source phrases “on the fly” in an indexed parallel corpus using suffix arrays. However, this can be slow because on-demand extraction of phrase tables is computationally expensive. We address this problem by developing novel algorithms for general purpose graphics processing units (GPUs), which enable suffix array queries for phrase lookup and phrase extraction to be massively parallelized. Compared a highly-optimized, state-of-the-art serial CPU-based implementation, our techniques achieve at least an order of magnitude improvement in terms of throughput. This work demonstrates the promise of massively parallel architectures and the potential of GPUs for tackling computationallydemanding problems in statistical machine translation and language processing.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R D Brown</author>
</authors>
<title>A modified Burrows-Wheeler Transform for highly-scalable example-based translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 6th Conference of the Association for Machine Translation in the Americas (AMTA 2004),</booktitle>
<pages>27--36</pages>
<contexts>
<context position="1658" citStr="Brown, 2004" startWordPosition="225" endWordPosition="226">t an order of magnitude improvement in terms of throughput. This work demonstrates the promise of massively parallel architectures and the potential of GPUs for tackling computationallydemanding problems in statistical machine translation and language processing. 1 Introduction Efficiently handling large translation models is a perennial problem in statistical machine translation. One particularly promising solution (§2) is to use the parallel text itself as an implicit representation of the translation model and extract translation units “on the fly” when they are needed to decode new input (Brown, 2004). This idea has been applied to phrase-based (Callison-Burch et al., 2005; Zhang and Vogel, 2005), hierarchical (Lopez, 2007; Lopez, 2008b; Lopez, 2008a), and syntax-based (Cromieres and Kurohashi, 2011) models. A benefit of this technique is that it scales to arbitrarily large models with very little pre-processing. For instance, Lopez (2008b) showed that a translation model trained on a large corpus with sparse word alignments and loose extraction heuristics substantially improved Chinese-English translation. An explicit representation of the model would have required nearly a terabyte of me</context>
</contexts>
<marker>Brown, 2004</marker>
<rawString>R. D. Brown. 2004. A modified Burrows-Wheeler Transform for highly-scalable example-based translation. In Proceedings of the 6th Conference of the Association for Machine Translation in the Americas (AMTA 2004), pages 27–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>C Bannard</author>
<author>J Schroeder</author>
</authors>
<title>Scaling phrase-based statistical machine translation to larger corpora and longer phrases.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL</booktitle>
<pages>255--262</pages>
<contexts>
<context position="1731" citStr="Callison-Burch et al., 2005" startWordPosition="234" endWordPosition="237">. This work demonstrates the promise of massively parallel architectures and the potential of GPUs for tackling computationallydemanding problems in statistical machine translation and language processing. 1 Introduction Efficiently handling large translation models is a perennial problem in statistical machine translation. One particularly promising solution (§2) is to use the parallel text itself as an implicit representation of the translation model and extract translation units “on the fly” when they are needed to decode new input (Brown, 2004). This idea has been applied to phrase-based (Callison-Burch et al., 2005; Zhang and Vogel, 2005), hierarchical (Lopez, 2007; Lopez, 2008b; Lopez, 2008a), and syntax-based (Cromieres and Kurohashi, 2011) models. A benefit of this technique is that it scales to arbitrarily large models with very little pre-processing. For instance, Lopez (2008b) showed that a translation model trained on a large corpus with sparse word alignments and loose extraction heuristics substantially improved Chinese-English translation. An explicit representation of the model would have required nearly a terabyte of memory, but its implicit representation using the parallel text required on</context>
<context position="15908" citStr="Callison-Burch et al., 2005" startWordPosition="2595" endWordPosition="2598">an, whereas Figure 2 shows a “bad” pair for source phrase f2f3f4 since the back-projection is not contained within the original source span. 328 4.1 Sampling Consistent Phrases Regardless of how efficient the extraction of a single target phrase is made, the fact remains that there are many phrases to extract. For example, in our Chinese Xinhua dataset (see §6), from 8,000 input query sentences, about 20 million source substrings can be extracted. The standard solution to this problem is to sample a set of occurrences of each source phrase, and only extract translations for those occurrences (Callison-Burch et al., 2005; Zhang and Vogel, 2005). As a practical matter, this can be done by sampling at uniform intervals from the matching span of a suffix array. Lopez (2008a) reports a sample size of 300; for phrases occurring fewer than 300 times, all translations are extracted. 4.2 GPU Implementation We present novel data structures and an algorithm for efficient phrase extraction, which together are amenable to massive parallelization on GPUs. The basic insight is to pre-compute data structures for the source-to-target alignment projection and backprojection procedure described by Lopez (2008a) for checking co</context>
</contexts>
<marker>Callison-Burch, Bannard, Schroeder, 2005</marker>
<rawString>C. Callison-Burch, C. Bannard, and J. Schroeder. 2005. Scaling phrase-based statistical machine translation to larger corpora and longer phrases. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL 2005), pages 255– 262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Chahuneau</author>
<author>N A Smith</author>
<author>C Dyer</author>
</authors>
<title>pycdec: A Python interface to cdec.</title>
<date>2012</date>
<booktitle>In Proceedings of the 7th Machine Translation</booktitle>
<location>Marathon (MTM</location>
<contexts>
<context position="23500" citStr="Chahuneau et al., 2012" startWordPosition="3864" endWordPosition="3867">y. Our experiments used NVIDIA’s Tesla C2050 GPU (Fermi Generation), which has 448 CUDA cores with a peak memory bandwidth 144 GB/s. Note that the GPU was released in early 2010 and represents previous generation technology. NVIDIA’s current GPUs (Kepler) boasts raw processing power in the 1.3 TFlops (double precision) range, which is approximately three times the GPU we used. Our CPU is a 3.33 GHz Intel Xeon X5260 processor, which has two cores. As a baseline, we compared against the publicly available implementation of the CPU-based algorithms described by Lopez (2008a) found in the pycdec (Chahuneau et al., 2012) extension of the cdec machine translation system (Dyer et al., 2010). Note that we only tested grammar extraction for continuous pairs of phrases, and we did not test the slower and more complex queries for hierarchical 330 Input Sentences 2,000 4,000 6,000 8,000 16,000 Number of Words 57,868 117,854 161,883 214,246 428,492 Xinhua With Sampling (s300) GPU (words/second) 3811 4723 5496 6391 12405 (21.9) (20.4) (32.1) (29.7) (36.0) CPU (words/second) 200 (1.5) Speedup 19× 24× 27× 32× 62× No Sampling (sem) GPU (words/second) 1917 2859 3496 4171 8186 (8.5) (11.1) (19.9) (23.2) (27.6) CPU (words/s</context>
<context position="25598" citStr="Chahuneau et al. (2012)" startWordPosition="4193" endWordPosition="4196">that presented in Lopez (2008b), and using the CPU implementation as a reference enabled us to confirm that our extracted grammars and features are identical (modulo sampling). We timed our GPU implementation as follows: from the loading of query sentences, extractions of substrings and grammar rules, until all grammars for all sentences are generated in memory. Timing does not include offline preparations such as the construction of the suffix array on source texts and the I/O costs for writing the per-sentence grammar files to disk. This timing procedure is exactly the same for the CPU 1The Chahuneau et al. (2012) implementation is in Cython, a language for building Python applications with performancecritical components in C. In particular, all of the suffix array code that we instrumented for these experiments are compiled to C/C++. The implementation is a port of the original code written by Lopez (2008a) in Pyrex, a precursor to Cython. Much of the code is unchanged from the original version. baseline. We are confident that our results represent a fair comparison between the GPU and CPU, and are not attributed to misconfigurations or other flaws in experimental procedures. Note that the CPU impleme</context>
</contexts>
<marker>Chahuneau, Smith, Dyer, 2012</marker>
<rawString>V. Chahuneau, N. A. Smith, and C. Dyer. 2012. pycdec: A Python interface to cdec. In Proceedings of the 7th Machine Translation Marathon (MTM 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="33693" citStr="Chiang, 2007" startWordPosition="5488" endWordPosition="5490">applications latency is also important. One current limitation of our work is that large batch sizes are necessary to fully utilize the available processing power of the GPU. This and other properties of the GPU, such as the high latency involved in transferring data from main memory to GPU memory, make low-latency processing a challenge, which we hope to address. Another broad future direction is to “GPU-ify” other machine translation models and other components in the machine translation pipeline. An obvious next step is to extend our work to the hierarchical phrase-based translation model (Chiang, 2007), which would involve extracting “gappy” phrases. Lopez (2008a) has tackled this problem on the CPU, but it is unclear to what extent the same types of algorithms he proposed can execute efficiently in the GPU environment. Beyond phrase extraction, it might be possible to perform decoding itself in the GPU—not only will this exploit massive amounts of parallelism, but also reduce costs in moving data to and from the GPU memory. 9 Conclusion GPU parallelism offers many promises for practical and efficient implementations of language processing systems. This promise has been demonstrated for spe</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>D. Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chong</author>
<author>Y Yi</author>
<author>A Faria</author>
<author>N R Satish</author>
<author>K Keutzer</author>
</authors>
<title>Data-parallel large vocabulary continuous speech recognition on graphics processors.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on Emerging Applications and Manycore Architectures.</booktitle>
<contexts>
<context position="34328" citStr="Chong et al., 2008" startWordPosition="5587" endWordPosition="5590">olve extracting “gappy” phrases. Lopez (2008a) has tackled this problem on the CPU, but it is unclear to what extent the same types of algorithms he proposed can execute efficiently in the GPU environment. Beyond phrase extraction, it might be possible to perform decoding itself in the GPU—not only will this exploit massive amounts of parallelism, but also reduce costs in moving data to and from the GPU memory. 9 Conclusion GPU parallelism offers many promises for practical and efficient implementations of language processing systems. This promise has been demonstrated for speech recognition (Chong et al., 2008; Chong et al., 2009) and parsing (Yi et al., 2011), and we have demonstrated here that it extends to machine translation as well. We believe that explorations of modern parallel hardware architectures is a fertile area of research: the field has only begun to examine the possibilities and there remain many more interesting questions to tackle. Parallelism is critical not only from the perspective of building real-world applications, but for overcoming fundamental computational bottlenecks associated with models that researchers are developing today. Acknowledgments This research was supported</context>
</contexts>
<marker>Chong, Yi, Faria, Satish, Keutzer, 2008</marker>
<rawString>J. Chong, Y. Yi, A. Faria, N. R. Satish, and K. Keutzer. 2008. Data-parallel large vocabulary continuous speech recognition on graphics processors. In Proceedings of the Workshop on Emerging Applications and Manycore Architectures.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chong</author>
<author>E Gonina</author>
<author>Y Yi</author>
<author>K Keutzer</author>
</authors>
<title>A fully data parallel WFST-based large vocabulary continuous speech recognition on a graphics processing unit.</title>
<date>2009</date>
<booktitle>In Proceedings of the 10th Annual Conference of the International Speech Communication Association (INTERSPEECH</booktitle>
<pages>1183--1186</pages>
<contexts>
<context position="34349" citStr="Chong et al., 2009" startWordPosition="5591" endWordPosition="5594">py” phrases. Lopez (2008a) has tackled this problem on the CPU, but it is unclear to what extent the same types of algorithms he proposed can execute efficiently in the GPU environment. Beyond phrase extraction, it might be possible to perform decoding itself in the GPU—not only will this exploit massive amounts of parallelism, but also reduce costs in moving data to and from the GPU memory. 9 Conclusion GPU parallelism offers many promises for practical and efficient implementations of language processing systems. This promise has been demonstrated for speech recognition (Chong et al., 2008; Chong et al., 2009) and parsing (Yi et al., 2011), and we have demonstrated here that it extends to machine translation as well. We believe that explorations of modern parallel hardware architectures is a fertile area of research: the field has only begun to examine the possibilities and there remain many more interesting questions to tackle. Parallelism is critical not only from the perspective of building real-world applications, but for overcoming fundamental computational bottlenecks associated with models that researchers are developing today. Acknowledgments This research was supported in part by the BOLT </context>
</contexts>
<marker>Chong, Gonina, Yi, Keutzer, 2009</marker>
<rawString>J. Chong, E. Gonina, Y. Yi, and K. Keutzer. 2009. A fully data parallel WFST-based large vocabulary continuous speech recognition on a graphics processing unit. In Proceedings of the 10th Annual Conference of the International Speech Communication Association (INTERSPEECH 2009), pages 1183–1186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Cromieres</author>
<author>S Kurohashi</author>
</authors>
<title>Efficient retrieval of tree translation examples for syntax-based machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, EMNLP</booktitle>
<pages>508--518</pages>
<contexts>
<context position="1861" citStr="Cromieres and Kurohashi, 2011" startWordPosition="251" endWordPosition="254">demanding problems in statistical machine translation and language processing. 1 Introduction Efficiently handling large translation models is a perennial problem in statistical machine translation. One particularly promising solution (§2) is to use the parallel text itself as an implicit representation of the translation model and extract translation units “on the fly” when they are needed to decode new input (Brown, 2004). This idea has been applied to phrase-based (Callison-Burch et al., 2005; Zhang and Vogel, 2005), hierarchical (Lopez, 2007; Lopez, 2008b; Lopez, 2008a), and syntax-based (Cromieres and Kurohashi, 2011) models. A benefit of this technique is that it scales to arbitrarily large models with very little pre-processing. For instance, Lopez (2008b) showed that a translation model trained on a large corpus with sparse word alignments and loose extraction heuristics substantially improved Chinese-English translation. An explicit representation of the model would have required nearly a terabyte of memory, but its implicit representation using the parallel text required only a few gigabytes. Unfortunately, there is substantial computational cost in searching a parallel corpus for source phrases, extr</context>
</contexts>
<marker>Cromieres, Kurohashi, 2011</marker>
<rawString>F. Cromieres and S. Kurohashi. 2011. Efficient retrieval of tree translation examples for syntax-based machine translation. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, EMNLP 2011, pages 508–518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Dyer</author>
<author>A Lopez</author>
<author>J Ganitkevitch</author>
<author>J Weese</author>
<author>F Ture</author>
<author>P Blunsom</author>
<author>H Setiawan</author>
<author>V Eidelman</author>
<author>P Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 System Demonstrations,</booktitle>
<pages>7--12</pages>
<contexts>
<context position="23569" citStr="Dyer et al., 2010" startWordPosition="3875" endWordPosition="3878"> has 448 CUDA cores with a peak memory bandwidth 144 GB/s. Note that the GPU was released in early 2010 and represents previous generation technology. NVIDIA’s current GPUs (Kepler) boasts raw processing power in the 1.3 TFlops (double precision) range, which is approximately three times the GPU we used. Our CPU is a 3.33 GHz Intel Xeon X5260 processor, which has two cores. As a baseline, we compared against the publicly available implementation of the CPU-based algorithms described by Lopez (2008a) found in the pycdec (Chahuneau et al., 2012) extension of the cdec machine translation system (Dyer et al., 2010). Note that we only tested grammar extraction for continuous pairs of phrases, and we did not test the slower and more complex queries for hierarchical 330 Input Sentences 2,000 4,000 6,000 8,000 16,000 Number of Words 57,868 117,854 161,883 214,246 428,492 Xinhua With Sampling (s300) GPU (words/second) 3811 4723 5496 6391 12405 (21.9) (20.4) (32.1) (29.7) (36.0) CPU (words/second) 200 (1.5) Speedup 19× 24× 27× 32× 62× No Sampling (sem) GPU (words/second) 1917 2859 3496 4171 8186 (8.5) (11.1) (19.9) (23.2) (27.6) CPU (words/second) 1.13 (0.02) Speedup 1690× 2520× 3082× 3677× 7217× Xinhua + UN </context>
</contexts>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Ture, Blunsom, Setiawan, Eidelman, Resnik, 2010</marker>
<rawString>C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture, P. Blunsom, H. Setiawan, V. Eidelman, and P. Resnik. 2010. cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models. In Proceedings of the ACL 2010 System Demonstrations, pages 7–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gharaibeh</author>
<author>M Ripeanu</author>
</authors>
<title>Size matters: Space/time tradeoffs to improve GPGPU applications performance.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 ACM/IEEE International Conference for High Performance Computing, Networking, Storage and Analysis (SC</booktitle>
<pages>1--12</pages>
<contexts>
<context position="3233" citStr="Gharaibeh and Ripeanu, 2010" startWordPosition="454" endWordPosition="458">trings of a source sentence) and their translations are numerous, both phrase lookup and extraction are performance bottlenecks. Despite considerable research and the use of efficient indexes like suffix arrays (Manber and Myers, 1990), this problem remains not fully solved. We show how to exploit the massive parallelism offered by modern general purpose graphics processing units (GPUs) to eliminate the computational bottlenecks associated with “on the fly” phrase extraction. GPUs have previously been applied to DNA sequence matching using suffix trees (Schatz et al., 2007) and suffix arrays (Gharaibeh and Ripeanu, 2010). Building on this work, we present two novel contributions: First, we describe improved GPU algorithms for suffix array queries that achieve greater parallelism (§3). Second, we propose novel data structures and algorithms for phrase extraction (§4) and scoring (§5) that are amenable to GPU par325 Proceedings of NAACL-HLT 2013, pages 325–334, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics allelization. The resulting implementation achieves at least an order of magnitude higher throughput than a state-of-the-art single-threaded CPU implementation (§6). Since</context>
<context position="10285" citStr="Gharaibeh and Ripeanu, 2010" startWordPosition="1641" endWordPosition="1644">rve as our baseline, not all of them are applicable to parallel architectures. In particular, optimizations (1), (2), and (3) introduce order dependencies between queries; they are disregarded in our GPU implementation so that we can fully exploit parallelization opportunities. We have not yet fully implemented (4), which is orthogonal to parallelization: this is left for future work. 3.3 Finding Every Phrase on a GPU Recent work in computational biology has shown that suffix arrays are particularly amenable to GPU acceleration: the suffix-array-based DNA sequence matching system MummurGPU++ (Gharaibeh and Ripeanu, 2010) has been reported to outperform the already fast MummurGPU 2 (Trapnell and Schatz, 2009), based on suffix trees (an alternative indexing structure). Here, we apply the same ideas to machine translation, introducing some novel improvements to their algorithms in the process. A natural approach to parallelism is to perform all substring queries in parallel (Gharaibeh and Ripeanu, 2010). There are no dependencies between iterations of the loop beginning on line 2 of Algorithm 1, so for input sentence F, we can parallelize by searching for all O(|F |2) substrings concurrently. We adopt this appro</context>
</contexts>
<marker>Gharaibeh, Ripeanu, 2010</marker>
<rawString>A. Gharaibeh and M. Ripeanu. 2010. Size matters: Space/time tradeoffs to improve GPGPU applications performance. In Proceedings of the 2010 ACM/IEEE International Conference for High Performance Computing, Networking, Storage and Analysis (SC 2010), pages 1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lopez</author>
</authors>
<title>Hierarchical phrase-based translation with suffix arrays.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>976--985</pages>
<contexts>
<context position="1782" citStr="Lopez, 2007" startWordPosition="243" endWordPosition="244">tures and the potential of GPUs for tackling computationallydemanding problems in statistical machine translation and language processing. 1 Introduction Efficiently handling large translation models is a perennial problem in statistical machine translation. One particularly promising solution (§2) is to use the parallel text itself as an implicit representation of the translation model and extract translation units “on the fly” when they are needed to decode new input (Brown, 2004). This idea has been applied to phrase-based (Callison-Burch et al., 2005; Zhang and Vogel, 2005), hierarchical (Lopez, 2007; Lopez, 2008b; Lopez, 2008a), and syntax-based (Cromieres and Kurohashi, 2011) models. A benefit of this technique is that it scales to arbitrarily large models with very little pre-processing. For instance, Lopez (2008b) showed that a translation model trained on a large corpus with sparse word alignments and loose extraction heuristics substantially improved Chinese-English translation. An explicit representation of the model would have required nearly a terabyte of memory, but its implicit representation using the parallel text required only a few gigabytes. Unfortunately, there is substan</context>
<context position="9069" citStr="Lopez (2007)" startWordPosition="1454" endWordPosition="1455">ifier; with 32-bit integers the total number of bytes is 16|T |. As we will show, this turns out to be quite modest, even for large parallel corpora (§6). 3.2 Suffix Array Efficiency Tricks Previous work on translation by pattern matching using suffix arrays on serial architectures has produced a number of efficiency optimizations: 1. Binary search bounds for longer substrings are initialized to the bounds of their longest prefix. Substrings are queried only if their longest prefix string was matched in the text. 2. In addition to conditioning on the longest prefix, Zhang and Vogel (2005) and Lopez (2007) condition on a successful query for the longest proper suffix. 3. Lopez (2007) queries each unique substring of a sentence exactly once, regardless of how many times it appears in an input sentence. 4. Lopez (2007) directly indexes one-word substrings with a small auxiliary array, so that their positions in the suffix array can be found in constant time. For longer substrings, this optimization reduces the log |T |term of query complexity to log(count(a)), where a is the first word of the query string. Although these efficiency tricks are important in the serial algorithms that serve as our b</context>
<context position="24850" citStr="Lopez (2007)" startWordPosition="4074" endWordPosition="4075">) (10.7) (13.9) (15.2) (29.0) CPU (words/second) 157 (1.8) Speedup 13× 16× 19× 22× 43× No Sampling (sem) GPU (words/second) 500.5 770.1 984.6 1243.8 2472.3 (2.5) (3.9) (5.8) (5.4) (12.0) CPU (words/second) 0.23 (0.002) Speedup 2194× 3375× 4315× 5451× 10836× Table 2: Comparing the GPU and CPU implementations for phrase extraction on two different corpora. Throughput is measured in words per second under different test set sizes; the 95% confidence intervals across five trials are given in parentheses, along with relative speedups comparing the two implementations. (gappy) patterns described by Lopez (2007). Both our implementation and the baseline are written primarily in C/C++.1 Our source corpora and test data are the same as that presented in Lopez (2008b), and using the CPU implementation as a reference enabled us to confirm that our extracted grammars and features are identical (modulo sampling). We timed our GPU implementation as follows: from the loading of query sentences, extractions of substrings and grammar rules, until all grammars for all sentences are generated in memory. Timing does not include offline preparations such as the construction of the suffix array on source texts and </context>
</contexts>
<marker>Lopez, 2007</marker>
<rawString>A. Lopez. 2007. Hierarchical phrase-based translation with suffix arrays. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 976–985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lopez</author>
</authors>
<title>Machine translation by pattern matching.</title>
<date>2008</date>
<institution>University of Maryland, College Park,</institution>
<location>Maryland, USA.</location>
<note>Ph.D. dissertation,</note>
<contexts>
<context position="1795" citStr="Lopez, 2008" startWordPosition="245" endWordPosition="246"> potential of GPUs for tackling computationallydemanding problems in statistical machine translation and language processing. 1 Introduction Efficiently handling large translation models is a perennial problem in statistical machine translation. One particularly promising solution (§2) is to use the parallel text itself as an implicit representation of the translation model and extract translation units “on the fly” when they are needed to decode new input (Brown, 2004). This idea has been applied to phrase-based (Callison-Burch et al., 2005; Zhang and Vogel, 2005), hierarchical (Lopez, 2007; Lopez, 2008b; Lopez, 2008a), and syntax-based (Cromieres and Kurohashi, 2011) models. A benefit of this technique is that it scales to arbitrarily large models with very little pre-processing. For instance, Lopez (2008b) showed that a translation model trained on a large corpus with sparse word alignments and loose extraction heuristics substantially improved Chinese-English translation. An explicit representation of the model would have required nearly a terabyte of memory, but its implicit representation using the parallel text required only a few gigabytes. Unfortunately, there is substantial computat</context>
<context position="4459" citStr="Lopez (2008" startWordPosition="631" endWordPosition="632">s verify that the GPU implementation produces exactly the same results as a CPU reference implementation on a full extraction, we can simply replace that component and reap significant performance advantages with no impact on translation quality. To the best of our knowledge, this is the first reported application of GPU acceleration techniques for statistical machine translation. We believe these results reveal a promising yet unexplored future direction in exploiting parallelism to tackle perennial performance bottlenecks in state-of-the-art translation models. 2 Phrase Extraction On Demand Lopez (2008b) provides the following recipe for “translation by pattern matching”, which we use as a guide for the remainder of this paper: Algorithm 1 Translation by pattern matching 1: for each input sentence do 2: for each possible phrase in the sentence do 3: Find its occurrences in the source text 4: for each occurrence do 5: Extract its aligned target phrase (if any) 6: for each extracted phrase pair do 7: Compute feature values 8: Decode as usual using the scored rules The computational bottleneck occurs in lines 2–7: there are vast numbers of query phrases, matching occurrences, and extracted phr</context>
<context position="14186" citStr="Lopez (2008" startWordPosition="2317" endWordPosition="2318">to extract the target phrase aligned to each matching source phrase instance. Efficiency is crucial since some source phrases occur hundreds of thousands of times. Phrase extraction from word alignments typically uses the consistency check of Och et al. (1999). A consistent phrase is one for which no words inside the phrase pair are aligned to words outside the phrase pair. Usually, consistent pairs are computed offline via dynamic programming over the alignment grid, from which we extract all consistent phrase pairs up to a heuristic bound on phrase length. The online extraction algorithm of Lopez (2008a) checks for consistent phrases in a different manner. Rather than finding all consistent phrase pairs in a sentence, the algorithm asks: given a specific source phrase, is there a consistent phrase pair Figure 1: Source phrase f2f3f4 and target phrase e2e3e4 are extracted as a consistent pair, since the backprojection is contained within the original source span. Figure 2: Source phrase f2f3f4 and target phrase e2e3e4 should not be extracted, since the back-projection is not contained within the original source span. of which it is one side? To answer this, it first computes the projection o</context>
<context position="16060" citStr="Lopez (2008" startWordPosition="2624" endWordPosition="2625"> Phrases Regardless of how efficient the extraction of a single target phrase is made, the fact remains that there are many phrases to extract. For example, in our Chinese Xinhua dataset (see §6), from 8,000 input query sentences, about 20 million source substrings can be extracted. The standard solution to this problem is to sample a set of occurrences of each source phrase, and only extract translations for those occurrences (Callison-Burch et al., 2005; Zhang and Vogel, 2005). As a practical matter, this can be done by sampling at uniform intervals from the matching span of a suffix array. Lopez (2008a) reports a sample size of 300; for phrases occurring fewer than 300 times, all translations are extracted. 4.2 GPU Implementation We present novel data structures and an algorithm for efficient phrase extraction, which together are amenable to massive parallelization on GPUs. The basic insight is to pre-compute data structures for the source-to-target alignment projection and backprojection procedure described by Lopez (2008a) for checking consistent alignments. Let us consider a single matching substring (from the output of the suffix array queries), span [i, j] in the source text T. For ea</context>
<context position="20683" citStr="Lopez, 2008" startWordPosition="3420" endWordPosition="3421">roups of 512 threads) are used to process each source substring. This means that for substrings with large numbers of matches, one thread in the GPU block would process multiple occurrences. This strategy is widely used, and according to GPU programming best practices from NVIDIA, allocating more work to a single thread maintains high GPU utilization and reduces the cost of context switches. 5 Computing Every Feature Finally, we arrive at line 7 in Algorithm 3, where we must compute feature values for each extracted phrase pair. Following the implementation of grammar extraction used in cdec (Lopez, 2008a), we compute several widely-used features: 1. Pair count feature, c(e, f). 2. The joint probability of all target-to-source phrase translation probabilities, p(elf) = c(e, f)/c(f), where e is target phrase, f is the source phrase. 3. The logarithm of the target-to-source lexical weighting feature. 4. The logarithm of the source-to-target lexical weighting feature. 5. The coherence probability, defined as the ratio between the number of successful extractions of a source phrase to the total count of the source phrase in the suffix array. The output of our phrase extraction is a large collecti</context>
<context position="23453" citStr="Lopez (2008" startWordPosition="3858" endWordPosition="3859">uplicated the test data as necessary. Our experiments used NVIDIA’s Tesla C2050 GPU (Fermi Generation), which has 448 CUDA cores with a peak memory bandwidth 144 GB/s. Note that the GPU was released in early 2010 and represents previous generation technology. NVIDIA’s current GPUs (Kepler) boasts raw processing power in the 1.3 TFlops (double precision) range, which is approximately three times the GPU we used. Our CPU is a 3.33 GHz Intel Xeon X5260 processor, which has two cores. As a baseline, we compared against the publicly available implementation of the CPU-based algorithms described by Lopez (2008a) found in the pycdec (Chahuneau et al., 2012) extension of the cdec machine translation system (Dyer et al., 2010). Note that we only tested grammar extraction for continuous pairs of phrases, and we did not test the slower and more complex queries for hierarchical 330 Input Sentences 2,000 4,000 6,000 8,000 16,000 Number of Words 57,868 117,854 161,883 214,246 428,492 Xinhua With Sampling (s300) GPU (words/second) 3811 4723 5496 6391 12405 (21.9) (20.4) (32.1) (29.7) (36.0) CPU (words/second) 200 (1.5) Speedup 19× 24× 27× 32× 62× No Sampling (sem) GPU (words/second) 1917 2859 3496 4171 8186</context>
<context position="25004" citStr="Lopez (2008" startWordPosition="4100" endWordPosition="4101"> (2.5) (3.9) (5.8) (5.4) (12.0) CPU (words/second) 0.23 (0.002) Speedup 2194× 3375× 4315× 5451× 10836× Table 2: Comparing the GPU and CPU implementations for phrase extraction on two different corpora. Throughput is measured in words per second under different test set sizes; the 95% confidence intervals across five trials are given in parentheses, along with relative speedups comparing the two implementations. (gappy) patterns described by Lopez (2007). Both our implementation and the baseline are written primarily in C/C++.1 Our source corpora and test data are the same as that presented in Lopez (2008b), and using the CPU implementation as a reference enabled us to confirm that our extracted grammars and features are identical (modulo sampling). We timed our GPU implementation as follows: from the loading of query sentences, extractions of substrings and grammar rules, until all grammars for all sentences are generated in memory. Timing does not include offline preparations such as the construction of the suffix array on source texts and the I/O costs for writing the per-sentence grammar files to disk. This timing procedure is exactly the same for the CPU 1The Chahuneau et al. (2012) imple</context>
<context position="26546" citStr="Lopez (2008" startWordPosition="4343" endWordPosition="4344">h of the code is unchanged from the original version. baseline. We are confident that our results represent a fair comparison between the GPU and CPU, and are not attributed to misconfigurations or other flaws in experimental procedures. Note that the CPU implementation runs in a single thread, on the same machine that hosts the GPU (described above). 7 Results Table 2 shows performance results comparing our GPU implementation against the reference CPU implementation for phrase extraction. In one experimental condition, the sampling parameter for frequently-matching phrases is set to 300, per Lopez (2008a), denoted s300. The experimental condition without sampling is denoted sem. Following standard settings, the maximum length of the source phrase is set to 5 and the maximum length of the target phrase is set to 15 (same for both GPU and CPU implementations). The table is divided into two sections: the top shows results on the Xinhua data, and the bottom on Xinhua + UN data. Columns report results for different numbers 331 # Sent. 2000 4000 6000 8000 16000 Speedup 9.6x 14.3x 17.5x 20.9x 40.9x Phrases 2.1x 1.8x 1.7x 1.6x 1.6x Table 3: Comparing no sampling on the GPU with sampling on the CPU i</context>
<context position="31014" citStr="Lopez (2008" startWordPosition="5057" endWordPosition="5058"> faster than the number of threads available on a CPU. Since phrase extraction is only one part of an end-to-end machine translation system, it makes sense to examine the overall performance of the entire translation pipeline. For this experiment, we used our GPU implementation for phrase extraction, serialized the grammar files to disk, and used cdec for decoding (on the CPU). The comparison condition used cdec for all three stages. We used standard phrase length constraints (5 on source side, 15 on target side) with sampling of frequent phrases. Finally, we replicated the data conditions in Lopez (2008a), where our source corpora was the Xinhua data set and our development/test sets were the NIST03/NIST05 data; the NIST05 test set contains 1,082 sentences. Performance results for end-to-end translation are shown in Table 4, broken down in terms of total amount of time for each of the processing stages for the entire test set under different conditions. In the decoding stage, we varied the number of CPU threads (note here we do not observe linear 332 Phrase Extraction I/O Decoding 1 thread 55.7 GPU: 11.0 2 threads 35.3 3.7 3 threads 31.5 CPU: 166.5 4 threads 26.2 Table 4: End-to-end machine </context>
<context position="33754" citStr="Lopez (2008" startWordPosition="5497" endWordPosition="5498">of our work is that large batch sizes are necessary to fully utilize the available processing power of the GPU. This and other properties of the GPU, such as the high latency involved in transferring data from main memory to GPU memory, make low-latency processing a challenge, which we hope to address. Another broad future direction is to “GPU-ify” other machine translation models and other components in the machine translation pipeline. An obvious next step is to extend our work to the hierarchical phrase-based translation model (Chiang, 2007), which would involve extracting “gappy” phrases. Lopez (2008a) has tackled this problem on the CPU, but it is unclear to what extent the same types of algorithms he proposed can execute efficiently in the GPU environment. Beyond phrase extraction, it might be possible to perform decoding itself in the GPU—not only will this exploit massive amounts of parallelism, but also reduce costs in moving data to and from the GPU memory. 9 Conclusion GPU parallelism offers many promises for practical and efficient implementations of language processing systems. This promise has been demonstrated for speech recognition (Chong et al., 2008; Chong et al., 2009) and </context>
</contexts>
<marker>Lopez, 2008</marker>
<rawString>A. Lopez. 2008a. Machine translation by pattern matching. Ph.D. dissertation, University of Maryland, College Park, Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lopez</author>
</authors>
<title>Tera-scale translation models via pattern matching.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (COLING</booktitle>
<pages>505--512</pages>
<marker>Lopez, 2008</marker>
<rawString>A. Lopez. 2008b. Tera-scale translation models via pattern matching. In Proceedings of the 22nd International Conference on Computational Linguistics (COLING 2008), pages 505–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Manber</author>
<author>G Myers</author>
</authors>
<title>Suffix arrays: a new method for on-line string searches.</title>
<date>1990</date>
<booktitle>In Proceedings of the First Annual ACM-SIAM Symposium on Discrete Algorithms (SODA ’90),</booktitle>
<pages>319--327</pages>
<contexts>
<context position="2840" citStr="Manber and Myers, 1990" startWordPosition="394" endWordPosition="397">uld have required nearly a terabyte of memory, but its implicit representation using the parallel text required only a few gigabytes. Unfortunately, there is substantial computational cost in searching a parallel corpus for source phrases, extracting their translations, and scoring them on the fly. Since the number of possible translation units may be quite large (for example, all substrings of a source sentence) and their translations are numerous, both phrase lookup and extraction are performance bottlenecks. Despite considerable research and the use of efficient indexes like suffix arrays (Manber and Myers, 1990), this problem remains not fully solved. We show how to exploit the massive parallelism offered by modern general purpose graphics processing units (GPUs) to eliminate the computational bottlenecks associated with “on the fly” phrase extraction. GPUs have previously been applied to DNA sequence matching using suffix trees (Schatz et al., 2007) and suffix arrays (Gharaibeh and Ripeanu, 2010). Building on this work, we present two novel contributions: First, we describe improved GPU algorithms for suffix array queries that achieve greater parallelism (§3). Second, we propose novel data structure</context>
<context position="5814" citStr="Manber and Myers (1990)" startWordPosition="860" endWordPosition="863"> find all occurrences of each source phrase in the input (line 3, Algorithm 1). This is a classic application of string pattern matching: given a short query pattern, the task is to find all occurrences in a much larger text. Solving the problem efficiently is crucial: for an input sentence F of length |F|, each of its O(|F |2) substrings is a potential query pattern. 3.1 Pattern Matching with Suffix Arrays Although there are many algorithms for pattern matching, all of the examples that we are aware of for machine translation rely on suffix arrays. We briefly review the classic algorithms of Manber and Myers (1990) here since they form the basis of our techniques and analysis, but readers who are familiar with them can safely skip ahead to additional optimizations (§3.2). A suffix array represents all suffixes of a corpus in lexicographical order. Formally, for a text T, the ith suffix of T is the substring of the text beginning at position i and continuing to the end of T. Each suffix can therefore be uniquely identified by the index i of its first word. A suffix array 5(T) of T is a permutation of these suffix identifiers [1, |T|] arranged by the lexicographical order of the corresponding suffixes—in </context>
<context position="7490" citStr="Manber and Myers (1990)" startWordPosition="1175" endWordPosition="1178">T. We can improve on this using an observation about the longest common prefix (LCP) of the query pattern and the suffix against which it is compared. Suppose we search for a query pattern Q in the span of the suffix array beginning at suffix L and ending at suffix R. For any suffix M which falls lexicographically between those at L and R, the LCP of Q and M will be at least as long as the LCP of Q and L or Q and R. Hence if we know the quantity h = MIN(LCP(Q, L), LCP(Q, R)) we can skip comparisons of the first h symbols between Q and the suffix M, since they must be the same. The solution of Manber and Myers (1990) exploits this fact along with the observation that each comparison in binary search is carried out according to a fixed recursion scheme: a query is only ever compared against a specific suffix M for a single range of suffixes bounded by some fixed L and R. Hence if we know the longest common prefix between M and each of its corresponding L and R according to the fixed recursions in the 326 algorithm, we can maintain a bound on h and reduce the aggregate number of symbol comparisons to O(|Q |+ log |T|). To accomplish this, in addition to the suffix array, we pre-compute two other arrays of si</context>
</contexts>
<marker>Manber, Myers, 1990</marker>
<rawString>U. Manber and G. Myers. 1990. Suffix arrays: a new method for on-line string searches. In Proceedings of the First Annual ACM-SIAM Symposium on Discrete Algorithms (SODA ’90), pages 319–327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>C Tillmann</author>
<author>H Ney</author>
</authors>
<title>Improved alignment models for statistical machine translation.</title>
<date>1999</date>
<booktitle>In Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>20--28</pages>
<contexts>
<context position="13835" citStr="Och et al. (1999)" startWordPosition="2257" endWordPosition="2260">esident occupancy. To fully take advantage of the processing power, we process multiple input sentences in parallel. Compared with previous algorithms, our two-pass approach and our strategy of thread assignment to increase the amount of parallelism represent novel contributions. 4 Extracting Aligned Target Phrases The problem at line 5 of Algorithm 1 is to extract the target phrase aligned to each matching source phrase instance. Efficiency is crucial since some source phrases occur hundreds of thousands of times. Phrase extraction from word alignments typically uses the consistency check of Och et al. (1999). A consistent phrase is one for which no words inside the phrase pair are aligned to words outside the phrase pair. Usually, consistent pairs are computed offline via dynamic programming over the alignment grid, from which we extract all consistent phrase pairs up to a heuristic bound on phrase length. The online extraction algorithm of Lopez (2008a) checks for consistent phrases in a different manner. Rather than finding all consistent phrase pairs in a sentence, the algorithm asks: given a specific source phrase, is there a consistent phrase pair Figure 1: Source phrase f2f3f4 and target ph</context>
</contexts>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>F. J. Och, C. Tillmann, and H. Ney. 1999. Improved alignment models for statistical machine translation. In Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 20–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Schatz</author>
<author>C Trapnell</author>
<author>A Delcher</author>
<author>A Varshney</author>
</authors>
<title>High-throughput sequence alignment using graphics processing units.</title>
<date>2007</date>
<journal>BMC Bioinformatics,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="3185" citStr="Schatz et al., 2007" startWordPosition="447" endWordPosition="450">ay be quite large (for example, all substrings of a source sentence) and their translations are numerous, both phrase lookup and extraction are performance bottlenecks. Despite considerable research and the use of efficient indexes like suffix arrays (Manber and Myers, 1990), this problem remains not fully solved. We show how to exploit the massive parallelism offered by modern general purpose graphics processing units (GPUs) to eliminate the computational bottlenecks associated with “on the fly” phrase extraction. GPUs have previously been applied to DNA sequence matching using suffix trees (Schatz et al., 2007) and suffix arrays (Gharaibeh and Ripeanu, 2010). Building on this work, we present two novel contributions: First, we describe improved GPU algorithms for suffix array queries that achieve greater parallelism (§3). Second, we propose novel data structures and algorithms for phrase extraction (§4) and scoring (§5) that are amenable to GPU par325 Proceedings of NAACL-HLT 2013, pages 325–334, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics allelization. The resulting implementation achieves at least an order of magnitude higher throughput than a state-of-the-ar</context>
</contexts>
<marker>Schatz, Trapnell, Delcher, Varshney, 2007</marker>
<rawString>M. Schatz, C. Trapnell, A. Delcher, and A. Varshney. 2007. High-throughput sequence alignment using graphics processing units. BMC Bioinformatics, 8(1):474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Trapnell</author>
<author>M C Schatz</author>
</authors>
<title>Optimizing data intensive GPGPU computations for DNA sequence alignment.</title>
<date>2009</date>
<journal>Parallel Computing,</journal>
<pages>35--8</pages>
<contexts>
<context position="10374" citStr="Trapnell and Schatz, 2009" startWordPosition="1655" endWordPosition="1658">r, optimizations (1), (2), and (3) introduce order dependencies between queries; they are disregarded in our GPU implementation so that we can fully exploit parallelization opportunities. We have not yet fully implemented (4), which is orthogonal to parallelization: this is left for future work. 3.3 Finding Every Phrase on a GPU Recent work in computational biology has shown that suffix arrays are particularly amenable to GPU acceleration: the suffix-array-based DNA sequence matching system MummurGPU++ (Gharaibeh and Ripeanu, 2010) has been reported to outperform the already fast MummurGPU 2 (Trapnell and Schatz, 2009), based on suffix trees (an alternative indexing structure). Here, we apply the same ideas to machine translation, introducing some novel improvements to their algorithms in the process. A natural approach to parallelism is to perform all substring queries in parallel (Gharaibeh and Ripeanu, 2010). There are no dependencies between iterations of the loop beginning on line 2 of Algorithm 1, so for input sentence F, we can parallelize by searching for all O(|F |2) substrings concurrently. We adopt this approach here. However, naive application of query-level parallelism leads to a large number o</context>
</contexts>
<marker>Trapnell, Schatz, 2009</marker>
<rawString>C. Trapnell and M. C. Schatz. 2009. Optimizing data intensive GPGPU computations for DNA sequence alignment. Parallel Computing, 35(8-9):429–440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yi</author>
<author>C-Y Lai</author>
<author>S Petrov</author>
<author>K Keutzer</author>
</authors>
<title>Efficient parallel CKY parsing on GPUs.</title>
<date>2011</date>
<booktitle>In Proceedings of the 12th International Conference on Parsing Technologies,</booktitle>
<pages>175--185</pages>
<contexts>
<context position="34379" citStr="Yi et al., 2011" startWordPosition="5597" endWordPosition="5600">ckled this problem on the CPU, but it is unclear to what extent the same types of algorithms he proposed can execute efficiently in the GPU environment. Beyond phrase extraction, it might be possible to perform decoding itself in the GPU—not only will this exploit massive amounts of parallelism, but also reduce costs in moving data to and from the GPU memory. 9 Conclusion GPU parallelism offers many promises for practical and efficient implementations of language processing systems. This promise has been demonstrated for speech recognition (Chong et al., 2008; Chong et al., 2009) and parsing (Yi et al., 2011), and we have demonstrated here that it extends to machine translation as well. We believe that explorations of modern parallel hardware architectures is a fertile area of research: the field has only begun to examine the possibilities and there remain many more interesting questions to tackle. Parallelism is critical not only from the perspective of building real-world applications, but for overcoming fundamental computational bottlenecks associated with models that researchers are developing today. Acknowledgments This research was supported in part by the BOLT program of the Defense Advance</context>
</contexts>
<marker>Yi, Lai, Petrov, Keutzer, 2011</marker>
<rawString>Y. Yi, C.-Y. Lai, S. Petrov, and K. Keutzer. 2011. Efficient parallel CKY parsing on GPUs. In Proceedings of the 12th International Conference on Parsing Technologies, pages 175–185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>S Vogel</author>
</authors>
<title>An efficient phrase-tophrase alignment model for arbitrarily long phrase and large corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of the Tenth Conference of the European Association for Machine Translation (EAMT-05).</booktitle>
<contexts>
<context position="1755" citStr="Zhang and Vogel, 2005" startWordPosition="238" endWordPosition="241">promise of massively parallel architectures and the potential of GPUs for tackling computationallydemanding problems in statistical machine translation and language processing. 1 Introduction Efficiently handling large translation models is a perennial problem in statistical machine translation. One particularly promising solution (§2) is to use the parallel text itself as an implicit representation of the translation model and extract translation units “on the fly” when they are needed to decode new input (Brown, 2004). This idea has been applied to phrase-based (Callison-Burch et al., 2005; Zhang and Vogel, 2005), hierarchical (Lopez, 2007; Lopez, 2008b; Lopez, 2008a), and syntax-based (Cromieres and Kurohashi, 2011) models. A benefit of this technique is that it scales to arbitrarily large models with very little pre-processing. For instance, Lopez (2008b) showed that a translation model trained on a large corpus with sparse word alignments and loose extraction heuristics substantially improved Chinese-English translation. An explicit representation of the model would have required nearly a terabyte of memory, but its implicit representation using the parallel text required only a few gigabytes. Unfo</context>
<context position="9052" citStr="Zhang and Vogel (2005)" startWordPosition="1449" endWordPosition="1452">d to a unique integer identifier; with 32-bit integers the total number of bytes is 16|T |. As we will show, this turns out to be quite modest, even for large parallel corpora (§6). 3.2 Suffix Array Efficiency Tricks Previous work on translation by pattern matching using suffix arrays on serial architectures has produced a number of efficiency optimizations: 1. Binary search bounds for longer substrings are initialized to the bounds of their longest prefix. Substrings are queried only if their longest prefix string was matched in the text. 2. In addition to conditioning on the longest prefix, Zhang and Vogel (2005) and Lopez (2007) condition on a successful query for the longest proper suffix. 3. Lopez (2007) queries each unique substring of a sentence exactly once, regardless of how many times it appears in an input sentence. 4. Lopez (2007) directly indexes one-word substrings with a small auxiliary array, so that their positions in the suffix array can be found in constant time. For longer substrings, this optimization reduces the log |T |term of query complexity to log(count(a)), where a is the first word of the query string. Although these efficiency tricks are important in the serial algorithms th</context>
<context position="15932" citStr="Zhang and Vogel, 2005" startWordPosition="2599" endWordPosition="2602">“bad” pair for source phrase f2f3f4 since the back-projection is not contained within the original source span. 328 4.1 Sampling Consistent Phrases Regardless of how efficient the extraction of a single target phrase is made, the fact remains that there are many phrases to extract. For example, in our Chinese Xinhua dataset (see §6), from 8,000 input query sentences, about 20 million source substrings can be extracted. The standard solution to this problem is to sample a set of occurrences of each source phrase, and only extract translations for those occurrences (Callison-Burch et al., 2005; Zhang and Vogel, 2005). As a practical matter, this can be done by sampling at uniform intervals from the matching span of a suffix array. Lopez (2008a) reports a sample size of 300; for phrases occurring fewer than 300 times, all translations are extracted. 4.2 GPU Implementation We present novel data structures and an algorithm for efficient phrase extraction, which together are amenable to massive parallelization on GPUs. The basic insight is to pre-compute data structures for the source-to-target alignment projection and backprojection procedure described by Lopez (2008a) for checking consistent alignments. Let</context>
</contexts>
<marker>Zhang, Vogel, 2005</marker>
<rawString>Y. Zhang and S. Vogel. 2005. An efficient phrase-tophrase alignment model for arbitrarily long phrase and large corpora. In Proceedings of the Tenth Conference of the European Association for Machine Translation (EAMT-05).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>