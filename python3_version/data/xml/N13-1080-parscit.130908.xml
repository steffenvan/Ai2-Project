<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.019692">
<title confidence="0.990258">
What’s in a Domain? Multi-Domain Learning for Multi-Attribute Data
</title>
<author confidence="0.993946">
Mahesh Joshi* Mark Dredze† William W. Cohen* Carolyn P. Ros´e*
</author>
<affiliation confidence="0.743969">
* School of Computer Science, Carnegie Mellon University
Pittsburgh, PA, 15213, USA
† Human Language Technology Center of Excellence, Johns Hopkins University
</affiliation>
<address confidence="0.956779">
Baltimore, MD, 21211, USA
</address>
<email confidence="0.9960225">
maheshj@cs.cmu.edu,mdredze@cs.jhu.edu
wcohen@cs.cmu.edu,cprose@cs.cmu.edu
</email>
<sectionHeader confidence="0.998577" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999784066666667">
Multi-Domain learning assumes that a sin-
gle metadata attribute is used in order to di-
vide the data into so-called domains. How-
ever, real-world datasets often have multi-
ple metadata attributes that can divide the
data into domains. It is not always apparent
which single attribute will lead to the best do-
mains, and more than one attribute might im-
pact classification. We propose extensions to
two multi-domain learning techniques for our
multi-attribute setting, enabling them to si-
multaneously learn from several metadata at-
tributes. Experimentally, they outperform the
multi-domain learning baseline, even when it
selects the single “best” attribute.
</bodyText>
<sectionHeader confidence="0.998982" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994175644444445">
Multi-Domain Learning (Evgeniou and Pontil,
2004; Daum´e III, 2007; Dredze and Crammer, 2008;
Finkel and Manning, 2009; Zhang and Yeung, 2010;
Saha et al., 2011) algorithms learn when training in-
stances are spread across many domains, which im-
pact model parameters. These algorithms use exam-
ples from each domain to learn a general model that
is also sensitive to individual domain differences.
However, many data sets include a host of meta-
data attributes, many of which can potentially define
the domains to use. Consider the case of restaurant
reviews, which can be categorized into domains cor-
responding to the cuisine, location, price range, or
several other factors. For multi-domain learning, we
should use the metadata attribute most likely to char-
acterize a domain: a change in vocabulary (i.e. fea-
tures) that most impacts the classification decision
(Ben-David et al., 2009). This choice is not easy.
First, we may not know which metadata attribute is
most likely to fit this role. Perhaps the location most
impacts the review language, but it could easily be
the price of the meal. Second, multiple metadata
attributes could impact the classification decision,
and picking a single one might reduce classification
accuracy. Therefore, we seek multi-domain learn-
ing algorithms which can simultaneously learn from
many types of domains (metadata attributes).
We introduce the multi-attribute multi-domain
(MAMD) learning problem, in which each learning
instance is associated with multiple metadata at-
tributes, each of which may impact feature behavior.
We present extensions to two popular multi-domain
learning algorithms, FEDA (Daum´e III, 2007) and
MDR (Dredze et al., 2009). Rather than selecting
a single domain division, our algorithms consider
all attributes as possible distinctions and discover
changes in features across attributes. We evaluate
our algorithms using two different data sets – a data
set of restaurant reviews (Chahuneau et al., 2012),
and a dataset of transcribed speech segments from
floor debates in the United States Congress (Thomas
et al., 2006). We demonstrate that multi-attribute al-
gorithms improve over their multi-domain counter-
parts, which can learn distinctions from only a single
attribute.
</bodyText>
<sectionHeader confidence="0.989809" genericHeader="method">
2 MAMD Learning
</sectionHeader>
<bodyText confidence="0.9998026">
In multi-domain learning, each instance x is drawn
from a domain d with distribution x - Dd over a
vectors space RD and labeled with a domain spe-
cific function fd with label y E {-1,+1} (for bi-
nary classification). In multi-attribute multi-domain
</bodyText>
<page confidence="0.989039">
685
</page>
<subsectionHeader confidence="0.293051">
Proceedings of NAACL-HLT 2013, pages 685–690,
</subsectionHeader>
<bodyText confidence="0.991082204545455">
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
(MAMD) learning, we have M metadata attributes in
a data set, where the mth metadata attribute has Km
possible unique values which represent the domains
induced by that metadata attribute. Each instance xi
is drawn from a distribution xi — Da specific to a
set of attribute values Ai associated with each in-
stance. Additionally, each unique set of attributes
indexes a function fA.1 Ai could contain a value for
each attribute, or no values for any attribute (which
would index a domain-agnostic “background” distri-
bution and labeling function). Just as a domain can
change a feature’s probability and behavior, so can
each metadata attribute.
Examples of data for MAMD learning abound. The
commonly used Amazon product reviews data set
(Blitzer et al., 2007) only includes product types, but
the original reviews can be attributed with author,
product price, brand, and so on. Additional exam-
ples include congressional floor debate records (e.g.
political party, speaker, bill) (Joshi et al., 2012). In
this paper, we use restaurant reviews (Chahuneau et
al., 2012), which have upto 20 metadata attributes
that define domains, and congressional floor de-
bates, with two attributes that define domains.
It is difficult to apply multi-domain learning algo-
rithms when it is unclear which metadata attribute
to choose for defining the “domains”. It is possible
that there is a single “best” attribute to use for defin-
ing domains, one that when used in multi-domain
learning will yield the best classifier. To find this
attribute, one must rely on one’s intuition about the
problem,2 or perform an exhaustive empirical search
over all attributes using some validation set. Both
these strategies can be brittle, because as the nature
of data changes over time so may the “best” do-
main distinction. Additionally, multi-domain learn-
ing was not designed to benefit from multiple helpful
attributes.
We note here that Eisenstein et al. (2011), as well
as Wang et al. (2012), worked with a “multifaceted
topic model” using the framework of sparse addi-
tive generative models (SAGE). Both those models
capture interactions between topics and multiple as-
</bodyText>
<footnote confidence="0.858631666666667">
1Distributions and functions that share attributes could share
parameters.
2Intuition is often critical for learning and in some cases can
help, such as in the Amazon product reviews data set, where
product type clearly corresponds to domain. However, for other
data sets the choice may be less clear.
</footnote>
<bodyText confidence="0.999966466666667">
pects, and can be adapted to the case of MAMD. While
our problem formulation has significant conceptual
overlap with the SAGE–like multifaceted topic mod-
els framework, our proposed methods are motivated
from a fast online learning perspective.
A naive approach for MAMD would be to treat ev-
ery unique set of attributes as a domain, including
unique proper subsets of different attributes to ac-
count for the case of missing attributes in some in-
stances.3 However, introducing an exponential num-
ber of domains requires a similar increase in train-
ing data, clearly an infeasible requirement. Instead,
we develop multi-attribute extensions for two multi-
domain learning algorithms, such that the increase
in parameters is linear in the number of metadata at-
tributes, and no special handling is required for the
case where some metadata attributes might be miss-
ing from an instance.
Multi-Attribute FEDA The key idea behind
FEDA (Daum´e III, 2007) is to encode each domain
using its own parameters, one per feature. FEDA
maps a feature vector x in RD to RD(K+1). This
provides a separate parameter sub-space for every
domain k E 1... K, and also maintains a domain-
agnostic shared sub-space. Essentially, each feature
is duplicated for every instance in the appropriate
sub-space of RD(K+1) that corresponds to the in-
stance’s domain. We extend this idea to the MAMD
setting by using one parameter per attribute value.
The original instance x E RD is now mapped into
RD(1+Em Km); a separate parameter for each at-
tribute value and a shared set of parameters. In ef-
fect, for every metadata attribute a E Ai, the original
features are copied into the appropriate sub-space.
This grows linearly with the number of metadata at-
tribute values, as opposed to exponentially in our
naive solution. While this is still substantial growth,
each instance retains the same feature sparsity as in
the original input space. In this new setup, FEDA al-
lows an instance to contribute towards learning the
shared parameters, and the attribute-specific param-
eters for all the attributes present on an instance. Just
like multi-domain FEDA, any supervised learning al-
gorithm can be applied to the transformed represen-
tation.
</bodyText>
<footnote confidence="0.904819">
3While we used a similar setup for formulating our problem,
we did not rule out the potential for factoring the distributions.
</footnote>
<page confidence="0.997545">
686
</page>
<bodyText confidence="0.999974957446809">
Multi-Attribute MDR We make a similar change
to MDR (Dredze et al., 2009) to extend it for
the MAMD setting. In the original formulation,
Dredze et al. used confidence-weighted (CW)
learning (Dredze et al., 2008) for learning shared
and domain-specific classifiers, which are combined
based on the confidence scores associated with the
feature weights. For training the MDR approaches in
a multi-domain learning setup, they found that com-
puting updates for the combined classifier and then
equally distributing them to the shared and domain-
specific classifiers was the best strategy, although it
approximated the true objective that they aimed to
optimize. In our multi-attribute setup confidence-
weighted (CW) classifiers are learned for each of the
Em Km attribute values in addition to a shared CW
classifier. At classification time, a combined clas-
sifier is computed for every instance. However, in-
stead of combining the shared classifier and a single
domain-specific classifier, we combine the shared
CW classifier and |Ai |different attribute value-
specific CW classifiers associated with xi. The
combined classifier is found by minimizing the KL-
divergence of the combined classifier with respect to
each of the underlying classifiers.4
When learning the shared and domain-specific
classifiers, we follow the best result in Dredze et
al. and use the “averaged update” strategy (�7.3 in
Dredze et al.), where updates are computed for the
combined classifier, and are then distributed to the
shared and domain-specific classifiers. MDR-U will
indicate that the updates to the combined classifiers
are uniformly distributed to the underlying shared
and domain-specific classifiers.
Dredze et al. also used another scheme called
“variance” to distribute the combined update to the
underlying classifiers (�4, last paragraph in Dredze
et al.) Their idea was to give a lower portion
of the update to the underlying classifier that has
higher variance (or in their terminology, “less con-
fidence”) since it contributed less to the combined
classifier. We refer to this as MDR-V. However, this
conflicts with the original CW intuition that features
with higher variance (lower confidence) should re-
ceive higher updates; since they are more in need
of change. Therefore, we implemented a modi-
fied “variance” scheme, where the updates are dis-
</bodyText>
<footnote confidence="0.950812">
4We also tried the 12 distance method of Dredze et al. (2009)
but it gave consistently worse results.
</footnote>
<bodyText confidence="0.9992515">
tributed to the underlying classifiers such that higher
variance features receive the larger updates. We re-
fer to this as MDR-NV. We observed significant im-
provements with this modified scheme.
</bodyText>
<sectionHeader confidence="0.999463" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.9998759">
To evaluate our multi-attribute algorithms we con-
sider two datasets. First, we use two subsets of the
restaurant reviews dataset (1,180,308 reviews) intro-
duced by Chahuneau et al. (2012) with the goal of
labeling reviews as positive or negative. The first
subset (50K-RND) randomly selects 50,000 reviews
while the second (50K-BAL) is a class-balanced
sample. Following the approach of Blitzer et al.
(2007), scores above and below 3-stars indicated
positive and negative reviews, while 3-star reviews
were discarded. Second, we use the transcribed seg-
ments of speech from the United States Congress
floor debates (Convote), introduced by Thomas
et al. (2006). The binary classification task on this
dataset is that of predicting whether a given speech
segment supports or opposes a bill under discussion
in the floor debate.
In the WordSalad datasets, each restaurant re-
view can have many metadata attributes, including a
unique identifier, name (which may not be unique),
address (we extract the zipcode), and type (Italian,
Chinese, etc.). We select the 20 most common meta-
data attributes (excluding latitude, longitude, and the
average rating). 5 In the Convote dataset, each
speech segment is associated with the political party
affiliation of the speaker (democrat, independent, or
republican) and the speaker identifier (we use bill
identifiers for creating folds in our 10-fold cross-
validation setup).
In addition to our new algorithms, we evalu-
ate several baselines. All methods use confidence-
weighted (CW) learning (Crammer et al., 2012).
BASE A single classifier trained on all the data,
and which ignores metadata attributes and uses uni-
gram features. For CW, we use the best-performing
setting from Dredze et al. (2008) — the “variance”
algorithm, which computes approximate but closed–
form updates, which also lead to faster learning. Pa-
rameters are tuned over a validation set within each
training fold.
</bodyText>
<footnote confidence="0.993068">
5Our method requires categorical metadata attributes, al-
though real-valued attributes can be discretized.
</footnote>
<page confidence="0.992134">
687
</page>
<table confidence="0.999910818181818">
metadata 1-META FEDA MDR-U MDR-V MDR-NV
50K-RND NONE (BASE) 92.29 (±0.14)
ALL (META) † 92.69 (±0.10)
CATEGORY † 92.48 (±0.11) 92.47 (±0.10) †t 92.99 (±0.12) 91.16 (±0.16) †t 93.24 (±0.13)
ZIPCODE 92.40 (±0.09) † 92.73 (±0.09) †t 92.99 (±0.12) 91.19 (±0.20) †t 93.22 (±0.11)
NEIGHBORHOOD 92.42 (±0.11) † 92.65 (±0.13) †t 93.02 (±0.13) 91.17 (±0.21) †t 93.21 (±0.12)
50K-BAL NONE (BASE) 89.95 (±0.10)
ALL (META) † 90.39 (±0.09)
CATEGORY 90.09 (±0.11) † 90.50 (±0.11) † 90.60 (±0.11) 87.89 (±0.13) †t 91.33 (±0.08)
ZIPCODE 89.97 (±0.12) † 90.42 (±0.13) † 90.56 (±0.09) 87.78 (±0.16) †t 91.30 (±0.10)
ID † 90.42 (±0.11) †t 90.64 (±0.11) † 90.50 (±0.11) 87.78 (±0.25) †t 91.27 (±0.09)
</table>
<tableCaption confidence="0.90378375">
Table 1: Average accuracy (± standard error) for the best three metadata attributes, when using a single attribute at
a time. Results that are numerically the best within a row are in bold. Results significantly better than BASE are
marked with t, and better than META are marked with t. Significance is measured using a two-tailed paired t-test with
α = 0.05.
</tableCaption>
<table confidence="0.999889777777778">
#attributes FEDA MDR-U MDR-V MDR-NV
50K-RND MAMD †t 93.07 (±0.19) †t 93.12 (±0.11) 87.08 (±1.72) †t 93.19 (±0.12)
1-ORCL †t 93.06 (±0.11) †t 93.17 (±0.11) 92.37 (±0.11) †t 93.39 (±0.12)
1-TUNE † 92.64 (±0.12) † 92.81 (±0.16) 92.15 (±0.17) †t 93.07 (±0.14)
1-MEAN † 92.61 (±0.09) † 92.59 (±0.10) 91.41 (±0.12) † 92.58 (±0.10)
50K-BAL MAMD †t 91.42 (±0.09) †t 91.06 (±0.04) 81.43 (±2.79) †t 91.40 (±0.08)
1-ORCL †t 90.89 (±0.10) †t 90.87 (±0.11) 89.33 (±0.13) †t 91.45 (±0.07)
1-TUNE † 90.33 (±0.10) †t 90.70 (±0.14) 89.13 (±0.16) †t 91.26 (±0.08)
1-MEAN † 90.30 (±0.06) 89.92 (±0.07) 88.25 (±0.07) 90.06 (±0.08)
</table>
<tableCaption confidence="0.991808">
Table 2: Average accuracy (± standard error) using 10-fold cross-validation for methods that use all attributes, either
directly (our proposed methods) or for selecting the “best” single attribute using one of the strategies described earlier.
Formatting and significance symbols are the same as in Table 1.
</tableCaption>
<bodyText confidence="0.999053823529412">
META Identical to BASE with a unique bias feature
added for each attribute value (Joshi et al., 2012).
1-META A special case of META where a unique
bias feature is added only for a single attribute.
To use multi-domain learning directly, we could
select a single attribute as the domain. We consider
several strategies for picking this attribute and eval-
uate both FEDA and MDR in this setting.
1-MEAN Choose an attribute randomly, equivalent
to the expected (mean) error over all attributes.
1-TUNE Select the best performing attribute on a
validation set.
1-ORCL Select the best performing attribute on
the test set. Though impossible in practice, this gives
the oracle upper bound on multi-domain learning.
All experiments use ten-fold cross-validation. We
report the mean accuracy, along with standard error.
</bodyText>
<sectionHeader confidence="0.999905" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999488774193548">
Table 1 shows the results of single-attribute multi-
domain learning methods for the WordSalad
datasets. The table shows the three best-performing
metadata attributes (as decided by the highest accu-
racy among all the methods across all 20 metadata
attributes). Clearly, several of the attributes can pro-
vide meaningful domains, which demonstrates that
methods that can select multiple attributes at once
are desirable. We also see that our modification to
MDR (MDR-NV) works the best.
Table 3 shows the results of single-attribute multi-
domain learning methods for the Convote dataset.
The first observation to be made on this dataset is
that neither the PARTY, nor the SPEAKER attribute
individually achieve significant improvement over
the META baseline, which uses both these attributes
as features. This is in contrast with the results on
the WordSalad dataset, where some attributes by
themselves showed an improvement over the META
baseline. Thus, this dataset represents a more chal-
lenging setup for our multi–attribute multi–domain
learning methods — they need to exploit the two
weak attributes simultaneously.
We next demonstrate multi-attribute improve-
ments over the multi-domain baselines (Tables 2
and 4). For WordSalad datasets, our exten-
sions that can use all metadata attributes simul-
taneously are consistently better than both the
1-MEAN and the 1-TUNE strategies (except for
the case of the old variance scheme used by
(Dredze et al., 2009)). For the skewed subset
</bodyText>
<page confidence="0.995628">
688
</page>
<table confidence="0.9998446">
metadata 1-META FEDA MDR-U MDR-V MDR-NV
NONE (BASE) 67.08 (±1.74)
ALL (META) † 82.60 (±1.95)
PARTY † 78.81 (±1.47) † 84.19 (±2.44) † 83.23 (±2.48) † 81.38 (±2.22) † 83.92 (±2.31)
SPEAKER † 77.49 (±1.75) † 82.88 (±2.43) † 78.32 (±1.91) 62.43 (±2.20) † 72.26 (±1.37)
</table>
<tableCaption confidence="0.991604333333333">
Table 3: Convote: Average accuracy (± standard error) when using a single attribute at a time. Results that are
numerically the best within a row are in bold. Results significantly better than BASE are marked with t, and better
than META are marked with t. Significance is measured using a two-tailed paired t-test with α = 0.05.
</tableCaption>
<table confidence="0.9998828">
#attributes FEDA MDR-U MDR-V MDR-NV
MAMD †t 85.71 (±2.74) † 84.12 (±2.56) 50.44 (±1.78) †t 86.19 (±2.49)
1-ORCL † 84.77 (±2.47) † 83.88 (±2.27) † 81.38 (±2.22) † 83.92 (±2.31)
1-TUNE † 84.19 (±2.44) † 83.23 (±2.48) † 81.38 (±2.22) † 83.92 (±2.31)
1-MEAN † 83.53 (±2.40) † 80.77 (±1.92) † 71.91 (±1.82) † 78.09 (±1.69)
</table>
<tableCaption confidence="0.986088">
Table 4: Convote: Average accuracy (± standard error) using 10-fold cross-validation for methods that use all
attributes, either directly (our proposed methods) or for selecting the “best” single attribute using one of the strategies
described earlier. Formatting and significance symbols are the same as in Table 3.
</tableCaption>
<bodyText confidence="0.98941692">
50K-RND, MAMD+FEDA is significantly better than
1-TUNE+FEDA; MAMD+MDR-U is significantly bet-
ter than 1-TUNE+MDR-U; MAMD+MDR-NV is not
significantly different from 1-TUNE+MDR-U. For
the balanced subset 50K-BAL, a similar pattern
holds, except that MAMD+MDR-NV is significantly
better than 1-TUNE+MDR-NV. Clearly, our multi-
attribute algorithms provide a benefit over existing
approaches. Even with oracle knowledge of the test
performance using multi-domain learning, we can
still obtain improvements (FEDA and MDR-U in the
50K-BAL set, and all the Convote results, except
MDR-V).
Although MAMD+MDR-NV is not significantly bet-
ter than 1-TUNE+MDR-NV on the 50K-RND set,
we found that in every single fold in our ten-
fold cross-validation experiments, the “best” single
metadata attribute decided using a validation set did
not match the best-performing single metadata at-
tribute on the corresponding test set. This shows
the potential instability of choosing a single best at-
tribute. Also, note that MDR-NV is a variant that we
have proposed in the current work, and in fact for
the earlier variant of MDR (MDR-U), as well as for
FEDA, we do see significant improvements when us-
ing all metadata attributes. Furthermore, the compu-
tational cost of evaluating every metadata attribute
independently to tune the single best metadata at-
tribute can be high and often impractical. Our ap-
proach requires no such tuning. Finally, observe
that for FEDA, the 1-TUNE strategy is not signifi-
cantly different from 1-MEAN, which just randomly
picks a single best metadata attribute. For MDR-U,
1-TUNE is significantly better than 1-MEAN on the
balanced subset 50K-BAL, but not on the skewed
subset 50K-RND.
As mentioned earlier, the Convote dataset is a
challenging setting for our methods due to the fact
that no single attribute is strong enough to yield im-
provements over the META baseline. In this setting,
both MAMD+FEDA and MAMD+MDR-NV achieve a
significant improvement over the META baseline,
with MDR-NV being the best (though not signif-
icantly better than FEDA). Additionally, both of
them are significantly better than their correspond-
ing 1-TUNE strategies. This result further supports
our claim that using multiple attributes in combi-
nation for defining domains (even when any single
one of them is not particularly beneficial for multi–
domain learning) is important.
</bodyText>
<sectionHeader confidence="0.999503" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999927777777778">
We propose multi-attribute multi-domain learning
methods that can utilize multiple metadata attributes
simultaneously for defining domains. Using these
methods, the definition of “domains” does not have
to be restricted to a single metadata attribute. Our
methods achieve a better performance on two multi-
attribute datasets as compared to traditional multi-
domain learning methods that are tuned to use a sin-
gle “best” attribute.
</bodyText>
<sectionHeader confidence="0.998461" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.715464">
This research is supported by the Office of Naval
Research grant number N000141110221.
</bodyText>
<page confidence="0.998489">
689
</page>
<sectionHeader confidence="0.995981" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999803575342466">
Shai Ben-David, John Blitzer, Koby Crammer, Alex
Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. 2009. A theory of learning from different
domains. Machine Learning.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, Bollywood, Boom-boxes and Blenders:
Domain Adaptation for Sentiment Classification. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 440–447.
Association for Computational Linguistics.
Victor Chahuneau, Kevin Gimpel, Bryan R. Routledge,
Lily Scherlis, and Noah A. Smith. 2012. Word
Salad: Relating Food Prices and Descriptions. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing and Natural Language
Learning (EMNLP 2012).
Koby Crammer, Mark Dredze, and Fernando Pereira.
2012. Confidence-weighted linear classification for
text categorization. Journal of Machine Learning Re-
search (JMLR).
Hal Daum´e III. 2007. Frustratingly Easy Domain Adap-
tation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256–263. Association for Computational Linguistics.
Mark Dredze and Koby Crammer. 2008. Online meth-
ods for multi-domain learning and adaptation. Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing - EMNLP ’08.
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification. Pro-
ceedings of the 25th international conference on Ma-
chine learning - ICML ’08.
Mark Dredze, Alex Kulesza, and Koby Crammer. 2009.
Multi-domain learning by confidence-weighted pa-
rameter combination. Machine Learning, 79(1–
2):123–149.
Jacob Eisenstein, Amr Ahmed, and Eric P. Xing. 2011.
Sparse Additive Generative Models of Text. In Pro-
ceedings of the 28th International Conference on Ma-
chine Learning (ICML).
Theodoros Evgeniou and Massimiliano Pontil. 2004.
Regularized multi–task learning. In Proceedings of
the 2004 ACM SIGKDD international conference on
Knowledge discovery and data mining - KDD ’04.
Jenny R Finkel and Christopher D Manning. 2009. Hier-
archical Bayesian Domain Adaptation. In Proceedings
of Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 602–
610. Association for Computational Linguistics.
Mahesh Joshi, Mark Dredze, William W. Cohen, and Car-
olyn P. Ros´e. 2012. Multi-domain learning: When do
domains matter? In Proceedings of EMNLP-CoNLL
2012, pages 1302–1312.
Avishek Saha, Piyush Rai, Hal Daum´e III, and Suresh
Venkatasubramanian. 2011. Online learning of mul-
tiple tasks and their relationships. In Proceedings of
AISTATS 2011.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get
out the vote: Determining support or opposition from
Congressional floor-debate transcripts. In Proceed-
ings of EMNLP, pages 327–335.
William Yang Wang, Elijah Mayfield, Suresh Naidu, and
Jeremiah Dittmar. 2012. Historical Analysis of Legal
Opinions with a Sparse Mixed-Effects Latent Variable
Model. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics (ACL
2012).
Yu Zhang and Dit-Yan Yeung. 2010. A Convex Formu-
lation for Learning Task Relationships in Multi-Task
Learning. In Proceedings of the Proceedings of the
Twenty-Sixth Conference Annual Conference on Un-
certainty in Artificial Intelligence (UAI-10).
</reference>
<page confidence="0.997569">
690
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.624481">
<title confidence="0.99972">What’s in a Domain? Multi-Domain Learning for Multi-Attribute Data</title>
<author confidence="0.999316">Mark W Carolyn P</author>
<affiliation confidence="0.994265">of Computer Science, Carnegie Mellon</affiliation>
<address confidence="0.983053">Pittsburgh, PA, 15213,</address>
<affiliation confidence="0.655712">Language Technology Center of Excellence, Johns Hopkins</affiliation>
<address confidence="0.995484">Baltimore, MD, 21211,</address>
<email confidence="0.999869">wcohen@cs.cmu.edu,cprose@cs.cmu.edu</email>
<abstract confidence="0.9989626875">Multi-Domain learning assumes that a single metadata attribute is used in order to divide the data into so-called domains. However, real-world datasets often have multiple metadata attributes that can divide the data into domains. It is not always apparent which single attribute will lead to the best domains, and more than one attribute might impact classification. We propose extensions to two multi-domain learning techniques for our enabling them to simultaneously learn from several metadata attributes. Experimentally, they outperform the multi-domain learning baseline, even when it selects the single “best” attribute.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shai Ben-David</author>
<author>John Blitzer</author>
<author>Koby Crammer</author>
<author>Alex Kulesza</author>
<author>Fernando Pereira</author>
<author>Jennifer Wortman Vaughan</author>
</authors>
<title>A theory of learning from different domains.</title>
<date>2009</date>
<journal>Machine Learning.</journal>
<contexts>
<context position="1950" citStr="Ben-David et al., 2009" startWordPosition="286" endWordPosition="289">ers. These algorithms use examples from each domain to learn a general model that is also sensitive to individual domain differences. However, many data sets include a host of metadata attributes, many of which can potentially define the domains to use. Consider the case of restaurant reviews, which can be categorized into domains corresponding to the cuisine, location, price range, or several other factors. For multi-domain learning, we should use the metadata attribute most likely to characterize a domain: a change in vocabulary (i.e. features) that most impacts the classification decision (Ben-David et al., 2009). This choice is not easy. First, we may not know which metadata attribute is most likely to fit this role. Perhaps the location most impacts the review language, but it could easily be the price of the meal. Second, multiple metadata attributes could impact the classification decision, and picking a single one might reduce classification accuracy. Therefore, we seek multi-domain learning algorithms which can simultaneously learn from many types of domains (metadata attributes). We introduce the multi-attribute multi-domain (MAMD) learning problem, in which each learning instance is associated</context>
</contexts>
<marker>Ben-David, Blitzer, Crammer, Kulesza, Pereira, Vaughan, 2009</marker>
<rawString>Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. 2009. A theory of learning from different domains. Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>440--447</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4461" citStr="Blitzer et al., 2007" startWordPosition="675" endWordPosition="678">nt the domains induced by that metadata attribute. Each instance xi is drawn from a distribution xi — Da specific to a set of attribute values Ai associated with each instance. Additionally, each unique set of attributes indexes a function fA.1 Ai could contain a value for each attribute, or no values for any attribute (which would index a domain-agnostic “background” distribution and labeling function). Just as a domain can change a feature’s probability and behavior, so can each metadata attribute. Examples of data for MAMD learning abound. The commonly used Amazon product reviews data set (Blitzer et al., 2007) only includes product types, but the original reviews can be attributed with author, product price, brand, and so on. Additional examples include congressional floor debate records (e.g. political party, speaker, bill) (Joshi et al., 2012). In this paper, we use restaurant reviews (Chahuneau et al., 2012), which have upto 20 metadata attributes that define domains, and congressional floor debates, with two attributes that define domains. It is difficult to apply multi-domain learning algorithms when it is unclear which metadata attribute to choose for defining the “domains”. It is possible th</context>
<context position="11522" citStr="Blitzer et al. (2007)" startWordPosition="1797" endWordPosition="1800">ults. tributed to the underlying classifiers such that higher variance features receive the larger updates. We refer to this as MDR-NV. We observed significant improvements with this modified scheme. 3 Experiments To evaluate our multi-attribute algorithms we consider two datasets. First, we use two subsets of the restaurant reviews dataset (1,180,308 reviews) introduced by Chahuneau et al. (2012) with the goal of labeling reviews as positive or negative. The first subset (50K-RND) randomly selects 50,000 reviews while the second (50K-BAL) is a class-balanced sample. Following the approach of Blitzer et al. (2007), scores above and below 3-stars indicated positive and negative reviews, while 3-star reviews were discarded. Second, we use the transcribed segments of speech from the United States Congress floor debates (Convote), introduced by Thomas et al. (2006). The binary classification task on this dataset is that of predicting whether a given speech segment supports or opposes a bill under discussion in the floor debate. In the WordSalad datasets, each restaurant review can have many metadata attributes, including a unique identifier, name (which may not be unique), address (we extract the zipcode),</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 440–447. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Chahuneau</author>
<author>Kevin Gimpel</author>
<author>Bryan R Routledge</author>
<author>Lily Scherlis</author>
<author>Noah A Smith</author>
</authors>
<title>Word Salad: Relating Food Prices and Descriptions.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing and Natural Language Learning (EMNLP</booktitle>
<contexts>
<context position="3035" citStr="Chahuneau et al., 2012" startWordPosition="448" endWordPosition="451">metadata attributes). We introduce the multi-attribute multi-domain (MAMD) learning problem, in which each learning instance is associated with multiple metadata attributes, each of which may impact feature behavior. We present extensions to two popular multi-domain learning algorithms, FEDA (Daum´e III, 2007) and MDR (Dredze et al., 2009). Rather than selecting a single domain division, our algorithms consider all attributes as possible distinctions and discover changes in features across attributes. We evaluate our algorithms using two different data sets – a data set of restaurant reviews (Chahuneau et al., 2012), and a dataset of transcribed speech segments from floor debates in the United States Congress (Thomas et al., 2006). We demonstrate that multi-attribute algorithms improve over their multi-domain counterparts, which can learn distinctions from only a single attribute. 2 MAMD Learning In multi-domain learning, each instance x is drawn from a domain d with distribution x - Dd over a vectors space RD and labeled with a domain specific function fd with label y E {-1,+1} (for binary classification). In multi-attribute multi-domain 685 Proceedings of NAACL-HLT 2013, pages 685–690, Atlanta, Georgia</context>
<context position="4768" citStr="Chahuneau et al., 2012" startWordPosition="722" endWordPosition="725">r any attribute (which would index a domain-agnostic “background” distribution and labeling function). Just as a domain can change a feature’s probability and behavior, so can each metadata attribute. Examples of data for MAMD learning abound. The commonly used Amazon product reviews data set (Blitzer et al., 2007) only includes product types, but the original reviews can be attributed with author, product price, brand, and so on. Additional examples include congressional floor debate records (e.g. political party, speaker, bill) (Joshi et al., 2012). In this paper, we use restaurant reviews (Chahuneau et al., 2012), which have upto 20 metadata attributes that define domains, and congressional floor debates, with two attributes that define domains. It is difficult to apply multi-domain learning algorithms when it is unclear which metadata attribute to choose for defining the “domains”. It is possible that there is a single “best” attribute to use for defining domains, one that when used in multi-domain learning will yield the best classifier. To find this attribute, one must rely on one’s intuition about the problem,2 or perform an exhaustive empirical search over all attributes using some validation set</context>
<context position="11301" citStr="Chahuneau et al. (2012)" startWordPosition="1763" endWordPosition="1766">dates; since they are more in need of change. Therefore, we implemented a modified “variance” scheme, where the updates are dis4We also tried the 12 distance method of Dredze et al. (2009) but it gave consistently worse results. tributed to the underlying classifiers such that higher variance features receive the larger updates. We refer to this as MDR-NV. We observed significant improvements with this modified scheme. 3 Experiments To evaluate our multi-attribute algorithms we consider two datasets. First, we use two subsets of the restaurant reviews dataset (1,180,308 reviews) introduced by Chahuneau et al. (2012) with the goal of labeling reviews as positive or negative. The first subset (50K-RND) randomly selects 50,000 reviews while the second (50K-BAL) is a class-balanced sample. Following the approach of Blitzer et al. (2007), scores above and below 3-stars indicated positive and negative reviews, while 3-star reviews were discarded. Second, we use the transcribed segments of speech from the United States Congress floor debates (Convote), introduced by Thomas et al. (2006). The binary classification task on this dataset is that of predicting whether a given speech segment supports or opposes a bil</context>
</contexts>
<marker>Chahuneau, Gimpel, Routledge, Scherlis, Smith, 2012</marker>
<rawString>Victor Chahuneau, Kevin Gimpel, Bryan R. Routledge, Lily Scherlis, and Noah A. Smith. 2012. Word Salad: Relating Food Prices and Descriptions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing and Natural Language Learning (EMNLP 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Confidence-weighted linear classification for text categorization.</title>
<date>2012</date>
<journal>Journal of Machine Learning Research (JMLR).</journal>
<contexts>
<context position="12662" citStr="Crammer et al., 2012" startWordPosition="1972" endWordPosition="1975">que identifier, name (which may not be unique), address (we extract the zipcode), and type (Italian, Chinese, etc.). We select the 20 most common metadata attributes (excluding latitude, longitude, and the average rating). 5 In the Convote dataset, each speech segment is associated with the political party affiliation of the speaker (democrat, independent, or republican) and the speaker identifier (we use bill identifiers for creating folds in our 10-fold crossvalidation setup). In addition to our new algorithms, we evaluate several baselines. All methods use confidenceweighted (CW) learning (Crammer et al., 2012). BASE A single classifier trained on all the data, and which ignores metadata attributes and uses unigram features. For CW, we use the best-performing setting from Dredze et al. (2008) — the “variance” algorithm, which computes approximate but closed– form updates, which also lead to faster learning. Parameters are tuned over a validation set within each training fold. 5Our method requires categorical metadata attributes, although real-valued attributes can be discretized. 687 metadata 1-META FEDA MDR-U MDR-V MDR-NV 50K-RND NONE (BASE) 92.29 (±0.14) ALL (META) † 92.69 (±0.10) CATEGORY † 92.48</context>
</contexts>
<marker>Crammer, Dredze, Pereira, 2012</marker>
<rawString>Koby Crammer, Mark Dredze, and Fernando Pereira. 2012. Confidence-weighted linear classification for text categorization. Journal of Machine Learning Research (JMLR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly Easy Domain Adaptation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>256--263</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e III. 2007. Frustratingly Easy Domain Adaptation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 256–263. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>Koby Crammer</author>
</authors>
<title>Online methods for multi-domain learning and adaptation.</title>
<date>2008</date>
<booktitle>Proceedings of the Conference on Empirical Methods in Natural Language Processing - EMNLP ’08.</booktitle>
<contexts>
<context position="1158" citStr="Dredze and Crammer, 2008" startWordPosition="159" endWordPosition="162">-world datasets often have multiple metadata attributes that can divide the data into domains. It is not always apparent which single attribute will lead to the best domains, and more than one attribute might impact classification. We propose extensions to two multi-domain learning techniques for our multi-attribute setting, enabling them to simultaneously learn from several metadata attributes. Experimentally, they outperform the multi-domain learning baseline, even when it selects the single “best” attribute. 1 Introduction Multi-Domain Learning (Evgeniou and Pontil, 2004; Daum´e III, 2007; Dredze and Crammer, 2008; Finkel and Manning, 2009; Zhang and Yeung, 2010; Saha et al., 2011) algorithms learn when training instances are spread across many domains, which impact model parameters. These algorithms use examples from each domain to learn a general model that is also sensitive to individual domain differences. However, many data sets include a host of metadata attributes, many of which can potentially define the domains to use. Consider the case of restaurant reviews, which can be categorized into domains corresponding to the cuisine, location, price range, or several other factors. For multi-domain le</context>
</contexts>
<marker>Dredze, Crammer, 2008</marker>
<rawString>Mark Dredze and Koby Crammer. 2008. Online methods for multi-domain learning and adaptation. Proceedings of the Conference on Empirical Methods in Natural Language Processing - EMNLP ’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Confidence-weighted linear classification.</title>
<date>2008</date>
<booktitle>Proceedings of the 25th international conference on Machine learning - ICML ’08.</booktitle>
<contexts>
<context position="8682" citStr="Dredze et al., 2008" startWordPosition="1358" endWordPosition="1361"> FEDA allows an instance to contribute towards learning the shared parameters, and the attribute-specific parameters for all the attributes present on an instance. Just like multi-domain FEDA, any supervised learning algorithm can be applied to the transformed representation. 3While we used a similar setup for formulating our problem, we did not rule out the potential for factoring the distributions. 686 Multi-Attribute MDR We make a similar change to MDR (Dredze et al., 2009) to extend it for the MAMD setting. In the original formulation, Dredze et al. used confidence-weighted (CW) learning (Dredze et al., 2008) for learning shared and domain-specific classifiers, which are combined based on the confidence scores associated with the feature weights. For training the MDR approaches in a multi-domain learning setup, they found that computing updates for the combined classifier and then equally distributing them to the shared and domainspecific classifiers was the best strategy, although it approximated the true objective that they aimed to optimize. In our multi-attribute setup confidenceweighted (CW) classifiers are learned for each of the Em Km attribute values in addition to a shared CW classifier. </context>
<context position="12847" citStr="Dredze et al. (2008)" startWordPosition="2003" endWordPosition="2006"> longitude, and the average rating). 5 In the Convote dataset, each speech segment is associated with the political party affiliation of the speaker (democrat, independent, or republican) and the speaker identifier (we use bill identifiers for creating folds in our 10-fold crossvalidation setup). In addition to our new algorithms, we evaluate several baselines. All methods use confidenceweighted (CW) learning (Crammer et al., 2012). BASE A single classifier trained on all the data, and which ignores metadata attributes and uses unigram features. For CW, we use the best-performing setting from Dredze et al. (2008) — the “variance” algorithm, which computes approximate but closed– form updates, which also lead to faster learning. Parameters are tuned over a validation set within each training fold. 5Our method requires categorical metadata attributes, although real-valued attributes can be discretized. 687 metadata 1-META FEDA MDR-U MDR-V MDR-NV 50K-RND NONE (BASE) 92.29 (±0.14) ALL (META) † 92.69 (±0.10) CATEGORY † 92.48 (±0.11) 92.47 (±0.10) †t 92.99 (±0.12) 91.16 (±0.16) †t 93.24 (±0.13) ZIPCODE 92.40 (±0.09) † 92.73 (±0.09) †t 92.99 (±0.12) 91.19 (±0.20) †t 93.22 (±0.11) NEIGHBORHOOD 92.42 (±0.11) †</context>
</contexts>
<marker>Dredze, Crammer, Pereira, 2008</marker>
<rawString>Mark Dredze, Koby Crammer, and Fernando Pereira. 2008. Confidence-weighted linear classification. Proceedings of the 25th international conference on Machine learning - ICML ’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>Alex Kulesza</author>
<author>Koby Crammer</author>
</authors>
<title>Multi-domain learning by confidence-weighted parameter combination.</title>
<date>2009</date>
<booktitle>Machine Learning,</booktitle>
<volume>79</volume>
<issue>1</issue>
<pages>2--123</pages>
<contexts>
<context position="2753" citStr="Dredze et al., 2009" startWordPosition="406" endWordPosition="409">be the price of the meal. Second, multiple metadata attributes could impact the classification decision, and picking a single one might reduce classification accuracy. Therefore, we seek multi-domain learning algorithms which can simultaneously learn from many types of domains (metadata attributes). We introduce the multi-attribute multi-domain (MAMD) learning problem, in which each learning instance is associated with multiple metadata attributes, each of which may impact feature behavior. We present extensions to two popular multi-domain learning algorithms, FEDA (Daum´e III, 2007) and MDR (Dredze et al., 2009). Rather than selecting a single domain division, our algorithms consider all attributes as possible distinctions and discover changes in features across attributes. We evaluate our algorithms using two different data sets – a data set of restaurant reviews (Chahuneau et al., 2012), and a dataset of transcribed speech segments from floor debates in the United States Congress (Thomas et al., 2006). We demonstrate that multi-attribute algorithms improve over their multi-domain counterparts, which can learn distinctions from only a single attribute. 2 MAMD Learning In multi-domain learning, each </context>
<context position="8543" citStr="Dredze et al., 2009" startWordPosition="1336" endWordPosition="1339"> While this is still substantial growth, each instance retains the same feature sparsity as in the original input space. In this new setup, FEDA allows an instance to contribute towards learning the shared parameters, and the attribute-specific parameters for all the attributes present on an instance. Just like multi-domain FEDA, any supervised learning algorithm can be applied to the transformed representation. 3While we used a similar setup for formulating our problem, we did not rule out the potential for factoring the distributions. 686 Multi-Attribute MDR We make a similar change to MDR (Dredze et al., 2009) to extend it for the MAMD setting. In the original formulation, Dredze et al. used confidence-weighted (CW) learning (Dredze et al., 2008) for learning shared and domain-specific classifiers, which are combined based on the confidence scores associated with the feature weights. For training the MDR approaches in a multi-domain learning setup, they found that computing updates for the combined classifier and then equally distributing them to the shared and domainspecific classifiers was the best strategy, although it approximated the true objective that they aimed to optimize. In our multi-att</context>
<context position="10866" citStr="Dredze et al. (2009)" startWordPosition="1696" endWordPosition="1699"> the underlying classifiers (�4, last paragraph in Dredze et al.) Their idea was to give a lower portion of the update to the underlying classifier that has higher variance (or in their terminology, “less confidence”) since it contributed less to the combined classifier. We refer to this as MDR-V. However, this conflicts with the original CW intuition that features with higher variance (lower confidence) should receive higher updates; since they are more in need of change. Therefore, we implemented a modified “variance” scheme, where the updates are dis4We also tried the 12 distance method of Dredze et al. (2009) but it gave consistently worse results. tributed to the underlying classifiers such that higher variance features receive the larger updates. We refer to this as MDR-NV. We observed significant improvements with this modified scheme. 3 Experiments To evaluate our multi-attribute algorithms we consider two datasets. First, we use two subsets of the restaurant reviews dataset (1,180,308 reviews) introduced by Chahuneau et al. (2012) with the goal of labeling reviews as positive or negative. The first subset (50K-RND) randomly selects 50,000 reviews while the second (50K-BAL) is a class-balanced</context>
<context position="17382" citStr="Dredze et al., 2009" startWordPosition="2721" endWordPosition="2724">s on the WordSalad dataset, where some attributes by themselves showed an improvement over the META baseline. Thus, this dataset represents a more challenging setup for our multi–attribute multi–domain learning methods — they need to exploit the two weak attributes simultaneously. We next demonstrate multi-attribute improvements over the multi-domain baselines (Tables 2 and 4). For WordSalad datasets, our extensions that can use all metadata attributes simultaneously are consistently better than both the 1-MEAN and the 1-TUNE strategies (except for the case of the old variance scheme used by (Dredze et al., 2009)). For the skewed subset 688 metadata 1-META FEDA MDR-U MDR-V MDR-NV NONE (BASE) 67.08 (±1.74) ALL (META) † 82.60 (±1.95) PARTY † 78.81 (±1.47) † 84.19 (±2.44) † 83.23 (±2.48) † 81.38 (±2.22) † 83.92 (±2.31) SPEAKER † 77.49 (±1.75) † 82.88 (±2.43) † 78.32 (±1.91) 62.43 (±2.20) † 72.26 (±1.37) Table 3: Convote: Average accuracy (± standard error) when using a single attribute at a time. Results that are numerically the best within a row are in bold. Results significantly better than BASE are marked with t, and better than META are marked with t. Significance is measured using a two-tailed paire</context>
</contexts>
<marker>Dredze, Kulesza, Crammer, 2009</marker>
<rawString>Mark Dredze, Alex Kulesza, and Koby Crammer. 2009. Multi-domain learning by confidence-weighted parameter combination. Machine Learning, 79(1– 2):123–149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Amr Ahmed</author>
<author>Eric P Xing</author>
</authors>
<title>Sparse Additive Generative Models of Text.</title>
<date>2011</date>
<booktitle>In Proceedings of the 28th International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="5634" citStr="Eisenstein et al. (2011)" startWordPosition="861" endWordPosition="864">hoose for defining the “domains”. It is possible that there is a single “best” attribute to use for defining domains, one that when used in multi-domain learning will yield the best classifier. To find this attribute, one must rely on one’s intuition about the problem,2 or perform an exhaustive empirical search over all attributes using some validation set. Both these strategies can be brittle, because as the nature of data changes over time so may the “best” domain distinction. Additionally, multi-domain learning was not designed to benefit from multiple helpful attributes. We note here that Eisenstein et al. (2011), as well as Wang et al. (2012), worked with a “multifaceted topic model” using the framework of sparse additive generative models (SAGE). Both those models capture interactions between topics and multiple as1Distributions and functions that share attributes could share parameters. 2Intuition is often critical for learning and in some cases can help, such as in the Amazon product reviews data set, where product type clearly corresponds to domain. However, for other data sets the choice may be less clear. pects, and can be adapted to the case of MAMD. While our problem formulation has significa</context>
</contexts>
<marker>Eisenstein, Ahmed, Xing, 2011</marker>
<rawString>Jacob Eisenstein, Amr Ahmed, and Eric P. Xing. 2011. Sparse Additive Generative Models of Text. In Proceedings of the 28th International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theodoros Evgeniou</author>
<author>Massimiliano Pontil</author>
</authors>
<title>Regularized multi–task learning.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 ACM SIGKDD international conference on Knowledge discovery and data mining - KDD ’04.</booktitle>
<contexts>
<context position="1114" citStr="Evgeniou and Pontil, 2004" startWordPosition="152" endWordPosition="155">he data into so-called domains. However, real-world datasets often have multiple metadata attributes that can divide the data into domains. It is not always apparent which single attribute will lead to the best domains, and more than one attribute might impact classification. We propose extensions to two multi-domain learning techniques for our multi-attribute setting, enabling them to simultaneously learn from several metadata attributes. Experimentally, they outperform the multi-domain learning baseline, even when it selects the single “best” attribute. 1 Introduction Multi-Domain Learning (Evgeniou and Pontil, 2004; Daum´e III, 2007; Dredze and Crammer, 2008; Finkel and Manning, 2009; Zhang and Yeung, 2010; Saha et al., 2011) algorithms learn when training instances are spread across many domains, which impact model parameters. These algorithms use examples from each domain to learn a general model that is also sensitive to individual domain differences. However, many data sets include a host of metadata attributes, many of which can potentially define the domains to use. Consider the case of restaurant reviews, which can be categorized into domains corresponding to the cuisine, location, price range, o</context>
</contexts>
<marker>Evgeniou, Pontil, 2004</marker>
<rawString>Theodoros Evgeniou and Massimiliano Pontil. 2004. Regularized multi–task learning. In Proceedings of the 2004 ACM SIGKDD international conference on Knowledge discovery and data mining - KDD ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny R Finkel</author>
<author>Christopher D Manning</author>
</authors>
<title>Hierarchical Bayesian Domain Adaptation.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>602--610</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1184" citStr="Finkel and Manning, 2009" startWordPosition="163" endWordPosition="166"> multiple metadata attributes that can divide the data into domains. It is not always apparent which single attribute will lead to the best domains, and more than one attribute might impact classification. We propose extensions to two multi-domain learning techniques for our multi-attribute setting, enabling them to simultaneously learn from several metadata attributes. Experimentally, they outperform the multi-domain learning baseline, even when it selects the single “best” attribute. 1 Introduction Multi-Domain Learning (Evgeniou and Pontil, 2004; Daum´e III, 2007; Dredze and Crammer, 2008; Finkel and Manning, 2009; Zhang and Yeung, 2010; Saha et al., 2011) algorithms learn when training instances are spread across many domains, which impact model parameters. These algorithms use examples from each domain to learn a general model that is also sensitive to individual domain differences. However, many data sets include a host of metadata attributes, many of which can potentially define the domains to use. Consider the case of restaurant reviews, which can be categorized into domains corresponding to the cuisine, location, price range, or several other factors. For multi-domain learning, we should use the </context>
</contexts>
<marker>Finkel, Manning, 2009</marker>
<rawString>Jenny R Finkel and Christopher D Manning. 2009. Hierarchical Bayesian Domain Adaptation. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 602– 610. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mahesh Joshi</author>
<author>Mark Dredze</author>
<author>William W Cohen</author>
<author>Carolyn P Ros´e</author>
</authors>
<title>Multi-domain learning: When do domains matter?</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP-CoNLL 2012,</booktitle>
<pages>1302--1312</pages>
<marker>Joshi, Dredze, Cohen, Ros´e, 2012</marker>
<rawString>Mahesh Joshi, Mark Dredze, William W. Cohen, and Carolyn P. Ros´e. 2012. Multi-domain learning: When do domains matter? In Proceedings of EMNLP-CoNLL 2012, pages 1302–1312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avishek Saha</author>
<author>Piyush Rai</author>
<author>Hal Daum´e</author>
<author>Suresh Venkatasubramanian</author>
</authors>
<title>Online learning of multiple tasks and their relationships.</title>
<date>2011</date>
<booktitle>In Proceedings of AISTATS</booktitle>
<marker>Saha, Rai, Daum´e, Venkatasubramanian, 2011</marker>
<rawString>Avishek Saha, Piyush Rai, Hal Daum´e III, and Suresh Venkatasubramanian. 2011. Online learning of multiple tasks and their relationships. In Proceedings of AISTATS 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Thomas</author>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Get out the vote: Determining support or opposition from Congressional floor-debate transcripts.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>327--335</pages>
<contexts>
<context position="3152" citStr="Thomas et al., 2006" startWordPosition="467" endWordPosition="470">ance is associated with multiple metadata attributes, each of which may impact feature behavior. We present extensions to two popular multi-domain learning algorithms, FEDA (Daum´e III, 2007) and MDR (Dredze et al., 2009). Rather than selecting a single domain division, our algorithms consider all attributes as possible distinctions and discover changes in features across attributes. We evaluate our algorithms using two different data sets – a data set of restaurant reviews (Chahuneau et al., 2012), and a dataset of transcribed speech segments from floor debates in the United States Congress (Thomas et al., 2006). We demonstrate that multi-attribute algorithms improve over their multi-domain counterparts, which can learn distinctions from only a single attribute. 2 MAMD Learning In multi-domain learning, each instance x is drawn from a domain d with distribution x - Dd over a vectors space RD and labeled with a domain specific function fd with label y E {-1,+1} (for binary classification). In multi-attribute multi-domain 685 Proceedings of NAACL-HLT 2013, pages 685–690, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics (MAMD) learning, we have M metadata attributes in </context>
<context position="11774" citStr="Thomas et al. (2006)" startWordPosition="1835" endWordPosition="1838">hms we consider two datasets. First, we use two subsets of the restaurant reviews dataset (1,180,308 reviews) introduced by Chahuneau et al. (2012) with the goal of labeling reviews as positive or negative. The first subset (50K-RND) randomly selects 50,000 reviews while the second (50K-BAL) is a class-balanced sample. Following the approach of Blitzer et al. (2007), scores above and below 3-stars indicated positive and negative reviews, while 3-star reviews were discarded. Second, we use the transcribed segments of speech from the United States Congress floor debates (Convote), introduced by Thomas et al. (2006). The binary classification task on this dataset is that of predicting whether a given speech segment supports or opposes a bill under discussion in the floor debate. In the WordSalad datasets, each restaurant review can have many metadata attributes, including a unique identifier, name (which may not be unique), address (we extract the zipcode), and type (Italian, Chinese, etc.). We select the 20 most common metadata attributes (excluding latitude, longitude, and the average rating). 5 In the Convote dataset, each speech segment is associated with the political party affiliation of the speake</context>
</contexts>
<marker>Thomas, Pang, Lee, 2006</marker>
<rawString>Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out the vote: Determining support or opposition from Congressional floor-debate transcripts. In Proceedings of EMNLP, pages 327–335.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Yang Wang</author>
<author>Elijah Mayfield</author>
<author>Suresh Naidu</author>
<author>Jeremiah Dittmar</author>
</authors>
<title>Historical Analysis of Legal Opinions with a Sparse Mixed-Effects Latent Variable Model.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="5665" citStr="Wang et al. (2012)" startWordPosition="868" endWordPosition="871">is possible that there is a single “best” attribute to use for defining domains, one that when used in multi-domain learning will yield the best classifier. To find this attribute, one must rely on one’s intuition about the problem,2 or perform an exhaustive empirical search over all attributes using some validation set. Both these strategies can be brittle, because as the nature of data changes over time so may the “best” domain distinction. Additionally, multi-domain learning was not designed to benefit from multiple helpful attributes. We note here that Eisenstein et al. (2011), as well as Wang et al. (2012), worked with a “multifaceted topic model” using the framework of sparse additive generative models (SAGE). Both those models capture interactions between topics and multiple as1Distributions and functions that share attributes could share parameters. 2Intuition is often critical for learning and in some cases can help, such as in the Amazon product reviews data set, where product type clearly corresponds to domain. However, for other data sets the choice may be less clear. pects, and can be adapted to the case of MAMD. While our problem formulation has significant conceptual overlap with the </context>
</contexts>
<marker>Wang, Mayfield, Naidu, Dittmar, 2012</marker>
<rawString>William Yang Wang, Elijah Mayfield, Suresh Naidu, and Jeremiah Dittmar. 2012. Historical Analysis of Legal Opinions with a Sparse Mixed-Effects Latent Variable Model. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu Zhang</author>
<author>Dit-Yan Yeung</author>
</authors>
<title>A Convex Formulation for Learning Task Relationships in Multi-Task Learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the Proceedings of the Twenty-Sixth Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI-10).</booktitle>
<contexts>
<context position="1207" citStr="Zhang and Yeung, 2010" startWordPosition="167" endWordPosition="170">tes that can divide the data into domains. It is not always apparent which single attribute will lead to the best domains, and more than one attribute might impact classification. We propose extensions to two multi-domain learning techniques for our multi-attribute setting, enabling them to simultaneously learn from several metadata attributes. Experimentally, they outperform the multi-domain learning baseline, even when it selects the single “best” attribute. 1 Introduction Multi-Domain Learning (Evgeniou and Pontil, 2004; Daum´e III, 2007; Dredze and Crammer, 2008; Finkel and Manning, 2009; Zhang and Yeung, 2010; Saha et al., 2011) algorithms learn when training instances are spread across many domains, which impact model parameters. These algorithms use examples from each domain to learn a general model that is also sensitive to individual domain differences. However, many data sets include a host of metadata attributes, many of which can potentially define the domains to use. Consider the case of restaurant reviews, which can be categorized into domains corresponding to the cuisine, location, price range, or several other factors. For multi-domain learning, we should use the metadata attribute most</context>
</contexts>
<marker>Zhang, Yeung, 2010</marker>
<rawString>Yu Zhang and Dit-Yan Yeung. 2010. A Convex Formulation for Learning Task Relationships in Multi-Task Learning. In Proceedings of the Proceedings of the Twenty-Sixth Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI-10).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>