<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000008">
<title confidence="0.999087">
Refined Lexicon Models for Statistical Machine Translation using a
Maximum Entropy Approach
</title>
<author confidence="0.821105">
Ismael Garcia Varea
</author>
<affiliation confidence="0.6544015">
Dpto. de Inform´atica
Univ. de Castilla-La Mancha
</affiliation>
<address confidence="0.751105">
Campus Universitario s/n
02071 Albacete, Spain
</address>
<email confidence="0.85537">
ivarea@info-ab.uclm.es
</email>
<note confidence="0.58047325">
Franz J. Och and
Hermann Ney
Lehrstuhl f¨ur Inf. VI
RWTH Aachen
</note>
<address confidence="0.896688">
Ahornstr., 55
D-52056 Aachen, Germany
</address>
<email confidence="0.995126">
och|ney@cs.rwth-aachen.de
</email>
<author confidence="0.660316">
Francisco Casacuberta
</author>
<affiliation confidence="0.482368">
Dpto. de Sist. Inf. y Comp.
Inst. Tecn. de Inf. (UPV)
Avda. de Los Naranjos, s/n
</affiliation>
<address confidence="0.913913">
46071 Valencia, Spain
</address>
<email confidence="0.996984">
fcn@iti.upv.es
</email>
<sectionHeader confidence="0.993841" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999750714285714">
Typically, the lexicon models used in
statistical machine translation systems
do not include any kind of linguistic
or contextual information, which often
leads to problems in performing a cor-
rect word sense disambiguation. One
way to deal with this problem within
the statistical framework is to use max-
imum entropy methods. In this paper,
we present how to use this type of in-
formation within a statistical machine
translation system. We show that it is
possible to significantly decrease train-
ing and test corpus perplexity of the
translation models. In addition, we per-
form a rescoring of-Best lists us-
ing our maximum entropy model and
thereby yield an improvement in trans-
lation quality. Experimental results are
presented on the so-called “Verbmobil
Task”.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9536758">
Typically, the lexicon models used in statistical
machine translation systems are only single-word
based, that is one word in the source language cor-
responds to only one word in the target language.
Those lexicon models lack from context infor-
mation that can be extracted from the same paral-
lel corpus. This additional information could be:
Simple context information: information of
the words surrounding the word pair;
Syntactic information: part-of-speech in-
formation, syntactic constituent, sentence
mood;
Semantic information: disambiguation in-
formation (e.g. from WordNet), cur-
rent/previous speech or dialog act.
To include this additional information within the
statistical framework we use the maximum en-
tropy approach. This approach has been applied
in natural language processing to a variety of
tasks. (Berger et al., 1996) applies this approach
to the so-called IBM Candide system to build con-
text dependent models, compute automatic sen-
tence splitting and to improve word reordering in
translation. Similar techniques are used in (Pap-
ineni et al., 1996; Papineni et al., 1998) for so-
called direct translation models instead of those
proposed in (Brown et al., 1993). (Foster, 2000)
describes two methods for incorporating informa-
tion about the relative position of bilingual word
pairs into a maximum entropy translation model.
Other authors have applied this approach to lan-
guage modeling (Rosenfeld, 1996; Martin et al.,
1999; Peters and Klakow, 1999). A short review
of the maximum entropy approach is outlined in
Section 3.
</bodyText>
<sectionHeader confidence="0.987221" genericHeader="introduction">
2 Statistical Machine Translation
</sectionHeader>
<bodyText confidence="0.996981860465116">
The goal of the translation process in statisti-
cal machine translation can be formulated as fol-
lows: A source language string
is to be translated into a target language string
. In the experiments reported in
this paper, the source language is German and the
target language is English. Every target string is
considered as a possible translation for the input.
If we assign a probability to each pair
of strings , then according to Bayes’ de-
cision rule, we have to choose the target string
that maximizes the product of the target language
model and the string translation model
.
Many existing systems for statistical machine
translation (Berger et al., 1994; Wang and Waibel,
1997; Tillmann et al., 1997; Nießen et al., 1998)
make use of a special way of structuring the string
translation model like proposed by (Brown et al.,
1993): The correspondence between the words in
the source and the target string is described by
alignments that assign one target word position
to each source word position. The lexicon prob-
ability of a certain target wordto occur
in the target string is assumed to depend basically
only on the source wordaligned to it.
These alignment models are similar to the con-
cept of Hidden Markov models (HMM) in speech
recognition. The alignment mapping is
from source positionto target position
. The alignmentmay contain align-
ments with the ‘empty’ word to ac-
count for source words that are not aligned to
any target word. In (statistical) alignment models
, the alignmentis introduced as
a hidden variable.
Typically, the search is performed using the so-
called maximum approximation:
The search space consists of the set of all possible
target language strings and all possible align-
ments.
The overall architecture of the statistical trans-
lation approach is depicted in Figure 1.
</bodyText>
<sectionHeader confidence="0.985086" genericHeader="method">
3 Maximum entropy modeling
</sectionHeader>
<bodyText confidence="0.986846">
The translation probability can be
rewritten as follows:
</bodyText>
<subsectionHeader confidence="0.917584">
Source Language Text
</subsectionHeader>
<table confidence="0.946896">
Transformation Pr(f1J  |e1I )
J
f1
Global Search: Lexicon Model
maximize Pr( e1
I) Pr(f1J  |e1I)
over e1I
I )
Pr( e1
Alignment Model
Language Model
Transformation
Target Language Text
</table>
<figureCaption confidence="0.9770585">
Figure 1: Architecture of the translation approach
based on Bayes’ decision rule.
</figureCaption>
<bodyText confidence="0.992437523809524">
Typically, the probability
, , and.
Obviously, this simplification is not true for a lot
of natural language phenomena. The straightfor-
ward approach to include more dependencies in
the lexicon model would be to add additional de-
pendencies(e.g. ). This approach
would yield a significant data sparseness problem.
Here, the role of maximum entropy (ME) is to
build a stochastic model that efficiently takes a
larger context into account. In the following, we
will use to denote the probability that the
ME model assigns toin the contextin order
to distinguish this model from the basic lexicon
model .
In the maximum entropy approach we describe
all properties that we feel are useful by so-called
feature functions . For example, if we
want to model the existence or absence of a spe-
cific word in the context of an English word
which has the translationwe can express this
dependency using the following feature function:
The ME principle suggests that the optimal
is
approximated by a lexicon model by
dropping the dependencies on
if and (1)
otherwise
parametric form of a model taking into
account only the feature functions
is given by:
Here is a normalization factor. The re-
sulting model has an exponential form with free
parameters . The parameter
values which maximize the likelihood for a given
training corpus can be computed with the so-
called GIS algorithm (general iterative scaling)
or its improved version IIS (Pietra et al., 1997;
Berger et al., 1996).
It is important to notice that we will have to ob-
tain one ME model for each target word observed
in the training data.
</bodyText>
<sectionHeader confidence="0.914535" genericHeader="method">
4 Contextual information and training
events
</sectionHeader>
<bodyText confidence="0.999616377777778">
In order to train the ME model associated
to a target word, we need to construct a corre-
sponding training sample from the whole bilin-
gual corpus depending on the contextual informa-
tion that we want to use. To construct this sample,
we need to know the word-to-word alignment be-
tween each sentence pair within the corpus. That
is obtained using the Viterbi alignment provided
by a translation model as described in (Brown et
al., 1993). Specifically, we use the Viterbi align-
ment that was produced by Model 5. We use the
program GIZA++ (Och and Ney, 2000b; Och and
Ney, 2000a), which is an extension of the training
program available in EGYPT (Al-Onaizan et al.,
1999).
Berger et al. (1996) use the words that sur-
round a specific word pairas contextual in-
formation. The authors propose as context the 3
words to the left and the 3 words to the right of
the target word. In this work we use the following
contextual information:
Target context: As in (Berger et al., 1996) we
consider a window of 3 words to the left and
to the right of the target word considered.
wordwhich is connected toaccording to
the Viterbi alignment.
Word classes: Instead of using a dependency
on the word identity we include also a de-
pendency on word classes. By doing this, we
improve the generalization of the models and
include some semantic and syntactic infor-
mation with. The word classes are computed
automatically using another statistical train-
ing procedure (Och, 1999) which often pro-
duces word classes including words with the
same semantic meaning in the same class.
A training event, for a specific target word, is
composed by three items:
The source wordaligned to.
The context in which the aligned pair
appears.
The number of occurrences of the event in
the training corpus.
Table 1 shows some examples of training events
for the target word “which”.
</bodyText>
<sectionHeader confidence="0.998836" genericHeader="method">
5 Features
</sectionHeader>
<bodyText confidence="0.999898">
Once we have a set of training events for each tar-
get word we need to describe our feature func-
tions. We do this by first specifying a large pool
of possible features and then by selecting a subset
of “good” features from this pool.
</bodyText>
<subsectionHeader confidence="0.966027">
5.1 Features definition
</subsectionHeader>
<bodyText confidence="0.979332428571429">
All the features we consider form a triple
(✛ label-1label-2) where:
pos: is the position that label-2 has in a spe-
cific context.
label-1: is the source wordof the aligned
word pairor the word class of the
source word( ).
</bodyText>
<footnote confidence="0.6413705">
label-2: is one word of the aligned word pair
or the word class to which these words
</footnote>
<table confidence="0.857483666666667">
belong ( ).
Source context: In addition, we consider a Using this notation and given a context:
window of 3 words to the left of the source
</table>
<tableCaption confidence="0.998036">
Table 1: Some training events for the English word “which”. The symbol “ ” is the placeholder of the
</tableCaption>
<bodyText confidence="0.96315">
English word “which” in the English context. In the German part the placeholder (“ ”) corresponds
to the word aligned to “which”, in the first example the German word “die”, the word “das” in the
second and the word “was” in the third. The considered English and German contexts are separated by
the double bar “ ”.The last number in the rightmost position is the number of occurrences of the event
in the whole corpus.
</bodyText>
<table confidence="0.993347714285714">
Alig. word () Context () # of occur.
die 2
das 1
was 1
bar there I just already nette Bar
hotel best is very centrally ein Hotel
now one do we jetzt ,
</table>
<tableCaption confidence="0.9605615">
Table 2: Meaning of different feature categories whererepresents a specific target word andrepre-
sents a specific source word.
</tableCaption>
<table confidence="0.9070985">
Category if and only if ...
1
</table>
<sectionHeader confidence="0.992734666666667" genericHeader="method">
2 and
2 and
3 and
3 and
6 and
7 and
</sectionHeader>
<bodyText confidence="0.998972">
for the word pair , we use the following
</bodyText>
<listItem confidence="0.9740481">
categories of features:
1. ()
2. () and
3. ( ) and
4. ( ) and
5. ( ) and
6. () and
7. ( ) and
8. ( ) and
9. ( ) and
</listItem>
<bodyText confidence="0.97548595">
Category 1 features depend only on the source
wordand the target word . A ME model that
uses only those, predicts each source translation
with the probability determined by the
empirical data. This is exactly the standard lex-
icon probability employed in the transla-
tion model described in (Brown et al., 1993) and
in Section 2.
Category 2 describes features which depend in
addition on the wordone position to the left or
to the right of . The same explanation is valid
for category 3 but in this casecould appears in
any position of the context. Categories 4 and
5 are the analogous categories to 2 and 3 using
word classes instead of words. In the categories
6, 7, 8 and 9 the source context is used instead of
the target context. Table 2 gives an overview of
the different feature categories.
Examples of specific features and their respec-
tive category are shown in Table 3.
</bodyText>
<tableCaption confidence="0.608649">
Table 3: The 10 most important features and their
respective category andvalues for the English
word “which”.
</tableCaption>
<table confidence="0.996012">
Category Feature
1 (0,was,) 1.20787
1 (0,das,) 1.19333
5 (3,F35,E15) 1.17612
4 (1,F35,E15) 1.15916
3 (3,das,is) 1.12869
2 (1,das,is) 1.12596
1 (0,die,) 1.12596
5 (-3,was,@@) 1.12052
6 (-1,was,@@) 1.11511
9 (-3,F26,F18) 1.11242
</table>
<subsectionHeader confidence="0.988041">
5.2 Feature selection
</subsectionHeader>
<bodyText confidence="0.998444742857143">
The number of possible features that can be used
according to the German and English vocabular-
ies and word classes is huge. In order to re-
duce the number of features we perform a thresh-
old based feature selection, that is every feature
which occurs less than times is not used. The
aim of the feature selection is two-fold. Firstly,
we obtain smaller models by using less features,
and secondly, we hope to avoid overfitting on the
training data.
In order to obtain the threshold we compare
the test corpus perplexity for various thresholds.
The different threshold used in the experiments
range from 0 to 512. The threshold is used as a
cut-off for the number of occurrences that a spe-
cific feature must appear. So a cut-off of 0 means
that all features observed in the training data are
used. A cut-off of 32 means those features that
appear 32 times or more are considered to train
the maximum entropy models.
We select the English words that appear at least
150 times in the training sample which are in total
348 of the 4673 words contained in the English
vocabulary. Table 4 shows the different number
of features considered for the 348 English words
selected using different thresholds.
In choosing a reasonable threshold we have to
balance the number of features and observed per-
plexity.
Table 4: Number of features used according to
different cut-off threshold. In the second column
of the table are shown the number of features used
when only the English context is considered. The
third column correspond to English, German and
Word-Classes contexts.
</bodyText>
<figure confidence="0.938847">
# features used
English English+German
0 846121 1581529
2 240053 500285
4 153225 330077
8 96983 210795
16 61329 131323
32 40441 80769
64 28147 49509
128 21469 31805
256 18511 22947
512 17193 19027
</figure>
<sectionHeader confidence="0.817149" genericHeader="evaluation">
6 Experimental results
</sectionHeader>
<subsectionHeader confidence="0.993971">
6.1 Training and test corpus
</subsectionHeader>
<bodyText confidence="0.962253142857143">
The “Verbmobil Task” is a speech translation task
in the domain of appointment scheduling, travel
planning, and hotel reservation. The task is dif-
ficult because it consists of spontaneous speech
and the syntactic structures of the sentences are
less restricted and highly variable. For the rescor-
ing experiments we use the corpus described in
</bodyText>
<tableCaption confidence="0.986186333333333">
Table 5.
Table 5: Corpus characteristics for translation
task.
</tableCaption>
<table confidence="0.998624857142857">
German English
Train Sentences 58 332
Words 519 523 549 921
Vocabulary 7 940 4 673
Test Sentences 147
Words 1968 2 173
PP (trigr. LM) (40.3) 28.8
</table>
<bodyText confidence="0.889479">
To train the maximum entropy models we used
the “Ristad ME Toolkit” described in (Ristad,
1997). We performed 100 iteration of the Im-
proved Iterative Scaling algorithm (Pietra et al.,
1997) using the corpus described in Table 6,
</bodyText>
<tableCaption confidence="0.920079">
Table 6: Corpus characteristics for perplexity
quality experiments.
</tableCaption>
<table confidence="0.999346">
German English
Train Sentences 50 000
Words 454 619 482 344
Vocabulary 7 456 4 420
Test Sentences 8073
Words 64 875 65 547
Vocabulary 2 579 1666
</table>
<bodyText confidence="0.475063">
which is a subset of the corpus shown in Table 5.
</bodyText>
<subsectionHeader confidence="0.994806">
6.2 Training and test perplexities
</subsectionHeader>
<bodyText confidence="0.982443105263158">
In order to compute the training and test perplex-
ities, we split the whole aligned training corpus
in two parts as shown in Table 6. The training
and test perplexities are shown in Table 7. As
expected, the perplexity reduction in the test cor-
pus is lower than in the training corpus, but in
both cases better perplexities are obtained using
the ME models. The best value is obtained when
a threshold of 4 is used.
We expected to observe strong overfitting ef-
fects when a too small cut-off for features gets
used. Yet, for most words the best test corpus
perplexity is observed when we use all features
including those that occur only once.
Table 7: Training and Test perplexities us-
ing different contextual information and different
thresholds. The reference perplexities obtained
with the basic translation model 5 are TrainPP =
10.38 and TestPP = 13.22.
</bodyText>
<table confidence="0.997501333333333">
English English+German
TrainPP TestPP TrainPP TestPP
0 5.03 11.39 4.60 9.28
2 6.59 10.37 5.70 8.94
4 7.09 10.28 6.17 8.92
8 7.50 10.39 6.63 9.03
16 7.95 10.64 7.07 9.30
32 8.38 11.04 7.55 9.73
64 9.68 11.56 8.05 10.26
128 9.31 12.09 8.61 10.94
256 9.70 12.62 9.20 11.80
512 10.07 13.12 9.69 12.45
</table>
<subsectionHeader confidence="0.997726">
6.3 Translation results
</subsectionHeader>
<bodyText confidence="0.973063276923077">
In order to make use of the ME models in a statis-
tical translation system we implemented a rescor-
ing algorithm. This algorithm take as input the
standard lexicon model (not using maximum en-
tropy) and the 348 models obtained with the ME
training. For an hypothesis sentenceand a cor-
responding alignmentthe algorithm modifies
the score according to the refined
maximum entropy lexicon model.
We carried out some preliminary experiments
with the-best lists of hypotheses provided by
the translation system in order to make a rescor-
ing of each i-th hypothesis and reorder the list ac-
cording to the new score computed with the re-
fined lexicon model. Unfortunately, our-best
extraction algorithm is sub-optimal, i.e. not the
true best translations are extracted. In addition,
so far we had to use a limit of onlytranslations
per sentence. Therefore, the results of the transla-
tion experiments are only preliminary.
For the evaluation of the translation quality
we use the automatically computable Word Er-
ror Rate (WER). The WER corresponds to the
edit distance between the produced translation
and one predefined reference translation. A short-
coming of the WER is the fact that it requires a
perfect word order. This is particularly a prob-
lem for the Verbmobil task, where the word or-
der of the German-English sentence pair can be
quite different. As a result, the word order of
the automatically generated target sentence can
be different from that of the target sentence, but
nevertheless acceptable so that the WER measure
alone can be misleading. In order to overcome
this problem, we introduce as additional measure
the position-independent word error rate (PER).
This measure compares the words in the two sen-
tences without taking the word order into account.
Depending on whether the translated sentence is
longer or shorter than the target translation, the
remaining words result in either insertion or dele-
tion errors in addition to substitution errors. The
PER is guaranteed to be less than or equal to the
WER.
We use the top-10 list of hypothesis provided
by the translation system described in (Tillmann
and Ney, 2000) for rescoring the hypothesis us-
ing the ME models and sort them according to the
new maximum entropy score. The translation re-
sults in terms of error rates are shown in Table 8.
We use Model 4 in order to perform the transla-
tion experiments because Model 4 typically gives
better translation results than Model 5.
We see that the translation quality improves
slightly with respect to the WER and PER. The
translation quality improvements so far are quite
small compared to the perplexity measure im-
provements. We attribute this to the fact that the
algorithm for computing the-best lists is sub-
optimal.
Table 8: Preliminary translation results for the
Verbmobil Test-147 for different contextual infor-
mation and different thresholds using the top-10
translations. The baseline translation results for
model 4 are WER=54.80 and PER=43.07.
</bodyText>
<table confidence="0.995431666666667">
English English+German
WER PER WER PER
0 54.57 42.98 54.02 42.48
2 54.16 42.43 54.07 42.71
4 54.53 42.71 54.11 42.75
8 54.76 43.21 54.39 43.07
16 54.76 43.53 54.02 42.75
32 54.80 43.12 54.53 42.94
64 54.21 42.89 54.53 42.89
128 54.57 42.98 54.67 43.12
256 54.99 43.12 54.57 42.89
512 55.08 43.30 54.85 43.21
</table>
<tableCaption confidence="0.526613">
Table 9 shows some examples where the trans-
</tableCaption>
<bodyText confidence="0.976239333333333">
lation obtained with the rescoring procedure is
better than the best hypothesis provided by the
translation system.
</bodyText>
<sectionHeader confidence="0.999413" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999955785714286">
We have developed refined lexicon models for
statistical machine translation by using maximum
entropy models. We have been able to obtain a
significant better test corpus perplexity and also a
slight improvement in translation quality. We be-
lieve that by performing a rescoring on translation
word graphs we will obtain a more significant im-
provement in translation quality.
For the future we plan to investigate more re-
fined feature selection methods in order to make
the maximum entropy models smaller and better
generalizing. In addition, we want to investigate
more syntactic, semantic features and to include
features that go beyond sentence boundaries.
</bodyText>
<sectionHeader confidence="0.989406" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999506458333334">
Yaser Al-Onaizan, Jan Curin, Michael Jahr,
Kevin Knight, John Lafferty, Dan Melamed,
David Purdy, Franz J. Och, Noah A. Smith,
and David Yarowsky. 1999. Statistical ma-
chine translation, final report, JHU workshop.
http://www.clsp.jhu.edu/ws99/pro-
jects/mt/final report/mt-final-
report.ps.
A. L. Berger, P. F. Brown, S. A. Della Pietra, et al.
1994. The candide system for machine translation.
In Proc. , ARPA Workshop on Human Language
Technology, pages 157–162.
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Compu-
tational Linguistics, 22(1):39–72, March.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263–311.
George Foster. 2000. Incorporating position informa-
tion into a maximum entropy/minimum divergence
translation model. In Proc. of CoNNL-2000 and
LLL-2000, pages 37–52, Lisbon, Portugal.
Sven Martin, Christoph Hamacher, J¨org Liermann,
Frank Wessel, and Hermann Ney. 1999. Assess-
ment of smoothing methods and complex stochas-
tic language modeling. In IEEE International Con-
ference on Acoustics, Speech and Signal Process-
ing, volume I, pages 1939–1942, Budapest, Hun-
gary, September.
Sonja Nießen, Stephan Vogel, Hermann Ney, and
Christoph Tillmann. 1998. A DP-based search
algorithm for statistical machine translation. In
COLING-ACL ’98: 36th Annual Meeting of the As-
sociation for Computational Linguistics and 17th
Int. Conf. on Computational Linguistics, pages
960–967, Montreal, Canada, August.
Franz J. Och and Hermann Ney. 2000a. Giza++:
Training of statistical translation models.
http://www-i6.Informatik.RWTH-
Aachen.DE/˜och/software/GIZA++.html.
Franz J. Och and Hermann Ney. 2000b. Improved sta-
tistical alignment models. In Proc. of the 38th An-
nual Meeting of the Association for Computational
Linguistics, pages 440–447, Hongkong, China, Oc-
tober.
</reference>
<tableCaption confidence="0.8174105">
Table 9: Four examples showing the translation obtained with the Model 4 and the ME model for a
given German source sentence.
</tableCaption>
<table confidence="0.974308416666667">
SRC: Danach wollten wir eigentlich noch Abendessen gehen.
M4: We actually concluding dinner together.
ME: Afterwards we wanted to go to dinner.
SRC: Bei mir oder bei Ihnen?
M4: For me or for you?
ME: At your or my place?
SRC: Das w¨are genau das richtige.
M4: That is exactly it spirit.
ME: That is the right thing.
SRC: Ja, das sieht bei mir eigentlich im Januar ziemlich gut aus.
M4: Yes, that does not suit me in January looks pretty good.
ME: Yes, that looks pretty good for me actually in January.
</table>
<reference confidence="0.999608780487805">
Franz J. Och. 1999. An efficient method for deter-
mining bilingual word classes. In EACL ’99: Ninth
Conf. of the Europ. Chapter of the Association for
Computational Linguistics, pages 71–76, Bergen,
Norway, June.
K.A. Papineni, S. Roukos, and R.T. Ward. 1996.
Feature-based language understanding. In ESCA,
Eurospeech, pages 1435–1438, Rhodes, Greece.
K.A. Papineni, S. Roukos, and R.T. Ward. 1998.
Maximum likelihood and discriminative training of
direct translation models. In Proc. Int. Conf. on
Acoustics, Speech, and Signal Processing, pages
189–192.
Jochen Peters and Dietrich Klakow. 1999. Compact
maximum entropy language models. In Proceed-
ings of the IEEE Workshop on Automatic Speech
Recognition and Understanding, Keystone, CO,
December.
Stephen Della Pietra, Vincent Della Pietra, and John
Lafferty. 1997. Inducing features in random fields.
IEEE Trans. on Pattern Analysis and Machine In-
teligence, 19(4):380–393, July.
Eric S. Ristad. 1997. Maximum entropy modelling
toolkit. Technical report, Princeton Univesity.
R. Rosenfeld. 1996. A maximum entropy approach to
adaptive statistical language modeling. Computer,
Speech and Language, 10:187–228.
Christoph Tillmann and Hermann Ney. 2000. Word
re-ordering and dp-based search in statistical ma-
chine translation. In 8th International Confer-
ence on Computational Linguistics (CoLing 2000),
pages 850–856, Saarbr¨ucken, Germany, July.
C. Tillmann, S. Vogel, H. Ney, and A. Zubiaga. 1997.
A DP-based search using monotone alignments in
statistical translation. In Proc. 35th Annual Conf.
of the Association for Computational Linguistics,
pages 289–296, Madrid, Spain, July.
Ye-Yi Wang and Alex Waibel. 1997. Decoding algo-
rithm in statistical translation. In Proc. 35th Annual
Conf. of the Association for Computational Linguis-
tics, pages 366–372, Madrid, Spain, July.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.089907">
<title confidence="0.994309">Refined Lexicon Models for Statistical Machine Translation using a Maximum Entropy Approach</title>
<author confidence="0.994892">Ismael Garcia Varea</author>
<affiliation confidence="0.958530333333333">Dpto. de Inform´atica Univ. de Castilla-La Mancha Campus Universitario s/n</affiliation>
<address confidence="0.998141">02071 Albacete, Spain</address>
<email confidence="0.787195">ivarea@info-ab.uclm.es</email>
<author confidence="0.9813955">J Och</author>
<author confidence="0.9813955">Hermann Ney</author>
<affiliation confidence="0.692967">Lehrstuhl f¨ur Inf. VI RWTH Aachen</affiliation>
<address confidence="0.998495">Ahornstr., 55 D-52056 Aachen, Germany</address>
<email confidence="0.997554">och|ney@cs.rwth-aachen.de</email>
<author confidence="0.98853">Francisco Casacuberta</author>
<affiliation confidence="0.944266">Dpto. de Sist. Inf. y Comp. Inst. Tecn. de Inf. (UPV)</affiliation>
<address confidence="0.8457795">Avda. de Los Naranjos, s/n 46071 Valencia, Spain</address>
<email confidence="0.99473">fcn@iti.upv.es</email>
<abstract confidence="0.998698476190476">Typically, the lexicon models used in statistical machine translation systems do not include any kind of linguistic or contextual information, which often leads to problems in performing a correct word sense disambiguation. One way to deal with this problem within the statistical framework is to use maximum entropy methods. In this paper, we present how to use this type of information within a statistical machine translation system. We show that it is possible to significantly decrease training and test corpus perplexity of the translation models. In addition, we perform a rescoring of-Best lists using our maximum entropy model and thereby yield an improvement in translation quality. Experimental results are presented on the so-called “Verbmobil</abstract>
<intro confidence="0.50207">Task”.</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Jan Curin</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>John Lafferty</author>
<author>Dan Melamed</author>
<author>David Purdy</author>
<author>Franz J Och</author>
<author>Noah A Smith</author>
<author>David Yarowsky</author>
</authors>
<title>Statistical machine translation, final report, JHU workshop. http://www.clsp.jhu.edu/ws99/projects/mt/final report/mt-finalreport.ps.</title>
<date>1999</date>
<contexts>
<context position="7333" citStr="Al-Onaizan et al., 1999" startWordPosition="1178" endWordPosition="1181">o a target word, we need to construct a corresponding training sample from the whole bilingual corpus depending on the contextual information that we want to use. To construct this sample, we need to know the word-to-word alignment between each sentence pair within the corpus. That is obtained using the Viterbi alignment provided by a translation model as described in (Brown et al., 1993). Specifically, we use the Viterbi alignment that was produced by Model 5. We use the program GIZA++ (Och and Ney, 2000b; Och and Ney, 2000a), which is an extension of the training program available in EGYPT (Al-Onaizan et al., 1999). Berger et al. (1996) use the words that surround a specific word pairas contextual information. The authors propose as context the 3 words to the left and the 3 words to the right of the target word. In this work we use the following contextual information: Target context: As in (Berger et al., 1996) we consider a window of 3 words to the left and to the right of the target word considered. wordwhich is connected toaccording to the Viterbi alignment. Word classes: Instead of using a dependency on the word identity we include also a dependency on word classes. By doing this, we improve the ge</context>
</contexts>
<marker>Al-Onaizan, Curin, Jahr, Knight, Lafferty, Melamed, Purdy, Och, Smith, Yarowsky, 1999</marker>
<rawString>Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin Knight, John Lafferty, Dan Melamed, David Purdy, Franz J. Och, Noah A. Smith, and David Yarowsky. 1999. Statistical machine translation, final report, JHU workshop. http://www.clsp.jhu.edu/ws99/projects/mt/final report/mt-finalreport.ps.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>P F Brown</author>
<author>S A Della Pietra</author>
</authors>
<title>The candide system for machine translation.</title>
<date>1994</date>
<booktitle>In Proc. , ARPA Workshop on Human Language Technology,</booktitle>
<pages>157--162</pages>
<contexts>
<context position="3522" citStr="Berger et al., 1994" startWordPosition="541" endWordPosition="544">atistical machine translation can be formulated as follows: A source language string is to be translated into a target language string . In the experiments reported in this paper, the source language is German and the target language is English. Every target string is considered as a possible translation for the input. If we assign a probability to each pair of strings , then according to Bayes’ decision rule, we have to choose the target string that maximizes the product of the target language model and the string translation model . Many existing systems for statistical machine translation (Berger et al., 1994; Wang and Waibel, 1997; Tillmann et al., 1997; Nießen et al., 1998) make use of a special way of structuring the string translation model like proposed by (Brown et al., 1993): The correspondence between the words in the source and the target string is described by alignments that assign one target word position to each source word position. The lexicon probability of a certain target wordto occur in the target string is assumed to depend basically only on the source wordaligned to it. These alignment models are similar to the concept of Hidden Markov models (HMM) in speech recognition. The a</context>
</contexts>
<marker>Berger, Brown, Pietra, 1994</marker>
<rawString>A. L. Berger, P. F. Brown, S. A. Della Pietra, et al. 1994. The candide system for machine translation. In Proc. , ARPA Workshop on Human Language Technology, pages 157–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="2121" citStr="Berger et al., 1996" startWordPosition="313" endWordPosition="316">odels lack from context information that can be extracted from the same parallel corpus. This additional information could be: Simple context information: information of the words surrounding the word pair; Syntactic information: part-of-speech information, syntactic constituent, sentence mood; Semantic information: disambiguation information (e.g. from WordNet), current/previous speech or dialog act. To include this additional information within the statistical framework we use the maximum entropy approach. This approach has been applied in natural language processing to a variety of tasks. (Berger et al., 1996) applies this approach to the so-called IBM Candide system to build context dependent models, compute automatic sentence splitting and to improve word reordering in translation. Similar techniques are used in (Papineni et al., 1996; Papineni et al., 1998) for socalled direct translation models instead of those proposed in (Brown et al., 1993). (Foster, 2000) describes two methods for incorporating information about the relative position of bilingual word pairs into a maximum entropy translation model. Other authors have applied this approach to language modeling (Rosenfeld, 1996; Martin et al.</context>
<context position="6500" citStr="Berger et al., 1996" startWordPosition="1031" endWordPosition="1034">anslationwe can express this dependency using the following feature function: The ME principle suggests that the optimal is approximated by a lexicon model by dropping the dependencies on if and (1) otherwise parametric form of a model taking into account only the feature functions is given by: Here is a normalization factor. The resulting model has an exponential form with free parameters . The parameter values which maximize the likelihood for a given training corpus can be computed with the socalled GIS algorithm (general iterative scaling) or its improved version IIS (Pietra et al., 1997; Berger et al., 1996). It is important to notice that we will have to obtain one ME model for each target word observed in the training data. 4 Contextual information and training events In order to train the ME model associated to a target word, we need to construct a corresponding training sample from the whole bilingual corpus depending on the contextual information that we want to use. To construct this sample, we need to know the word-to-word alignment between each sentence pair within the corpus. That is obtained using the Viterbi alignment provided by a translation model as described in (Brown et al., 1993)</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–72, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="2465" citStr="Brown et al., 1993" startWordPosition="369" endWordPosition="372">(e.g. from WordNet), current/previous speech or dialog act. To include this additional information within the statistical framework we use the maximum entropy approach. This approach has been applied in natural language processing to a variety of tasks. (Berger et al., 1996) applies this approach to the so-called IBM Candide system to build context dependent models, compute automatic sentence splitting and to improve word reordering in translation. Similar techniques are used in (Papineni et al., 1996; Papineni et al., 1998) for socalled direct translation models instead of those proposed in (Brown et al., 1993). (Foster, 2000) describes two methods for incorporating information about the relative position of bilingual word pairs into a maximum entropy translation model. Other authors have applied this approach to language modeling (Rosenfeld, 1996; Martin et al., 1999; Peters and Klakow, 1999). A short review of the maximum entropy approach is outlined in Section 3. 2 Statistical Machine Translation The goal of the translation process in statistical machine translation can be formulated as follows: A source language string is to be translated into a target language string . In the experiments report</context>
<context position="3698" citStr="Brown et al., 1993" startWordPosition="572" endWordPosition="575">er, the source language is German and the target language is English. Every target string is considered as a possible translation for the input. If we assign a probability to each pair of strings , then according to Bayes’ decision rule, we have to choose the target string that maximizes the product of the target language model and the string translation model . Many existing systems for statistical machine translation (Berger et al., 1994; Wang and Waibel, 1997; Tillmann et al., 1997; Nießen et al., 1998) make use of a special way of structuring the string translation model like proposed by (Brown et al., 1993): The correspondence between the words in the source and the target string is described by alignments that assign one target word position to each source word position. The lexicon probability of a certain target wordto occur in the target string is assumed to depend basically only on the source wordaligned to it. These alignment models are similar to the concept of Hidden Markov models (HMM) in speech recognition. The alignment mapping is from source positionto target position . The alignmentmay contain alignments with the ‘empty’ word to account for source words that are not aligned to any t</context>
<context position="7100" citStr="Brown et al., 1993" startWordPosition="1137" endWordPosition="1140">erger et al., 1996). It is important to notice that we will have to obtain one ME model for each target word observed in the training data. 4 Contextual information and training events In order to train the ME model associated to a target word, we need to construct a corresponding training sample from the whole bilingual corpus depending on the contextual information that we want to use. To construct this sample, we need to know the word-to-word alignment between each sentence pair within the corpus. That is obtained using the Viterbi alignment provided by a translation model as described in (Brown et al., 1993). Specifically, we use the Viterbi alignment that was produced by Model 5. We use the program GIZA++ (Och and Ney, 2000b; Och and Ney, 2000a), which is an extension of the training program available in EGYPT (Al-Onaizan et al., 1999). Berger et al. (1996) use the words that surround a specific word pairas contextual information. The authors propose as context the 3 words to the left and the 3 words to the right of the target word. In this work we use the following contextual information: Target context: As in (Berger et al., 1996) we consider a window of 3 words to the left and to the right of</context>
<context position="10551" citStr="Brown et al., 1993" startWordPosition="1775" endWordPosition="1778">ategories whererepresents a specific target word andrepresents a specific source word. Category if and only if ... 1 2 and 2 and 3 and 3 and 6 and 7 and for the word pair , we use the following categories of features: 1. () 2. () and 3. ( ) and 4. ( ) and 5. ( ) and 6. () and 7. ( ) and 8. ( ) and 9. ( ) and Category 1 features depend only on the source wordand the target word . A ME model that uses only those, predicts each source translation with the probability determined by the empirical data. This is exactly the standard lexicon probability employed in the translation model described in (Brown et al., 1993) and in Section 2. Category 2 describes features which depend in addition on the wordone position to the left or to the right of . The same explanation is valid for category 3 but in this casecould appears in any position of the context. Categories 4 and 5 are the analogous categories to 2 and 3 using word classes instead of words. In the categories 6, 7, 8 and 9 the source context is used instead of the target context. Table 2 gives an overview of the different feature categories. Examples of specific features and their respective category are shown in Table 3. Table 3: The 10 most important </context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
</authors>
<title>Incorporating position information into a maximum entropy/minimum divergence translation model.</title>
<date>2000</date>
<booktitle>In Proc. of CoNNL-2000 and LLL-2000,</booktitle>
<pages>37--52</pages>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="2481" citStr="Foster, 2000" startWordPosition="373" endWordPosition="374">urrent/previous speech or dialog act. To include this additional information within the statistical framework we use the maximum entropy approach. This approach has been applied in natural language processing to a variety of tasks. (Berger et al., 1996) applies this approach to the so-called IBM Candide system to build context dependent models, compute automatic sentence splitting and to improve word reordering in translation. Similar techniques are used in (Papineni et al., 1996; Papineni et al., 1998) for socalled direct translation models instead of those proposed in (Brown et al., 1993). (Foster, 2000) describes two methods for incorporating information about the relative position of bilingual word pairs into a maximum entropy translation model. Other authors have applied this approach to language modeling (Rosenfeld, 1996; Martin et al., 1999; Peters and Klakow, 1999). A short review of the maximum entropy approach is outlined in Section 3. 2 Statistical Machine Translation The goal of the translation process in statistical machine translation can be formulated as follows: A source language string is to be translated into a target language string . In the experiments reported in this paper</context>
</contexts>
<marker>Foster, 2000</marker>
<rawString>George Foster. 2000. Incorporating position information into a maximum entropy/minimum divergence translation model. In Proc. of CoNNL-2000 and LLL-2000, pages 37–52, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sven Martin</author>
<author>Christoph Hamacher</author>
<author>J¨org Liermann</author>
<author>Frank Wessel</author>
<author>Hermann Ney</author>
</authors>
<title>Assessment of smoothing methods and complex stochastic language modeling.</title>
<date>1999</date>
<booktitle>In IEEE International Conference on Acoustics, Speech and Signal Processing, volume I,</booktitle>
<pages>1939--1942</pages>
<location>Budapest, Hungary,</location>
<contexts>
<context position="2727" citStr="Martin et al., 1999" startWordPosition="408" endWordPosition="411">et al., 1996) applies this approach to the so-called IBM Candide system to build context dependent models, compute automatic sentence splitting and to improve word reordering in translation. Similar techniques are used in (Papineni et al., 1996; Papineni et al., 1998) for socalled direct translation models instead of those proposed in (Brown et al., 1993). (Foster, 2000) describes two methods for incorporating information about the relative position of bilingual word pairs into a maximum entropy translation model. Other authors have applied this approach to language modeling (Rosenfeld, 1996; Martin et al., 1999; Peters and Klakow, 1999). A short review of the maximum entropy approach is outlined in Section 3. 2 Statistical Machine Translation The goal of the translation process in statistical machine translation can be formulated as follows: A source language string is to be translated into a target language string . In the experiments reported in this paper, the source language is German and the target language is English. Every target string is considered as a possible translation for the input. If we assign a probability to each pair of strings , then according to Bayes’ decision rule, we have to</context>
</contexts>
<marker>Martin, Hamacher, Liermann, Wessel, Ney, 1999</marker>
<rawString>Sven Martin, Christoph Hamacher, J¨org Liermann, Frank Wessel, and Hermann Ney. 1999. Assessment of smoothing methods and complex stochastic language modeling. In IEEE International Conference on Acoustics, Speech and Signal Processing, volume I, pages 1939–1942, Budapest, Hungary, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Nießen</author>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>A DP-based search algorithm for statistical machine translation.</title>
<date>1998</date>
<booktitle>In COLING-ACL ’98: 36th Annual Meeting of the Association for Computational Linguistics and 17th Int. Conf. on Computational Linguistics,</booktitle>
<pages>960--967</pages>
<location>Montreal, Canada,</location>
<contexts>
<context position="3590" citStr="Nießen et al., 1998" startWordPosition="553" endWordPosition="556">e language string is to be translated into a target language string . In the experiments reported in this paper, the source language is German and the target language is English. Every target string is considered as a possible translation for the input. If we assign a probability to each pair of strings , then according to Bayes’ decision rule, we have to choose the target string that maximizes the product of the target language model and the string translation model . Many existing systems for statistical machine translation (Berger et al., 1994; Wang and Waibel, 1997; Tillmann et al., 1997; Nießen et al., 1998) make use of a special way of structuring the string translation model like proposed by (Brown et al., 1993): The correspondence between the words in the source and the target string is described by alignments that assign one target word position to each source word position. The lexicon probability of a certain target wordto occur in the target string is assumed to depend basically only on the source wordaligned to it. These alignment models are similar to the concept of Hidden Markov models (HMM) in speech recognition. The alignment mapping is from source positionto target position . The ali</context>
</contexts>
<marker>Nießen, Vogel, Ney, Tillmann, 1998</marker>
<rawString>Sonja Nießen, Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1998. A DP-based search algorithm for statistical machine translation. In COLING-ACL ’98: 36th Annual Meeting of the Association for Computational Linguistics and 17th Int. Conf. on Computational Linguistics, pages 960–967, Montreal, Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>Giza++: Training of statistical translation models.</title>
<date>2000</date>
<note>http://www-i6.Informatik.RWTHAachen.DE/˜och/software/GIZA++.html.</note>
<contexts>
<context position="7219" citStr="Och and Ney, 2000" startWordPosition="1159" endWordPosition="1162">the training data. 4 Contextual information and training events In order to train the ME model associated to a target word, we need to construct a corresponding training sample from the whole bilingual corpus depending on the contextual information that we want to use. To construct this sample, we need to know the word-to-word alignment between each sentence pair within the corpus. That is obtained using the Viterbi alignment provided by a translation model as described in (Brown et al., 1993). Specifically, we use the Viterbi alignment that was produced by Model 5. We use the program GIZA++ (Och and Ney, 2000b; Och and Ney, 2000a), which is an extension of the training program available in EGYPT (Al-Onaizan et al., 1999). Berger et al. (1996) use the words that surround a specific word pairas contextual information. The authors propose as context the 3 words to the left and the 3 words to the right of the target word. In this work we use the following contextual information: Target context: As in (Berger et al., 1996) we consider a window of 3 words to the left and to the right of the target word considered. wordwhich is connected toaccording to the Viterbi alignment. Word classes: Instead of usin</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz J. Och and Hermann Ney. 2000a. Giza++: Training of statistical translation models. http://www-i6.Informatik.RWTHAachen.DE/˜och/software/GIZA++.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proc. of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>440--447</pages>
<location>Hongkong, China,</location>
<contexts>
<context position="7219" citStr="Och and Ney, 2000" startWordPosition="1159" endWordPosition="1162">the training data. 4 Contextual information and training events In order to train the ME model associated to a target word, we need to construct a corresponding training sample from the whole bilingual corpus depending on the contextual information that we want to use. To construct this sample, we need to know the word-to-word alignment between each sentence pair within the corpus. That is obtained using the Viterbi alignment provided by a translation model as described in (Brown et al., 1993). Specifically, we use the Viterbi alignment that was produced by Model 5. We use the program GIZA++ (Och and Ney, 2000b; Och and Ney, 2000a), which is an extension of the training program available in EGYPT (Al-Onaizan et al., 1999). Berger et al. (1996) use the words that surround a specific word pairas contextual information. The authors propose as context the 3 words to the left and the 3 words to the right of the target word. In this work we use the following contextual information: Target context: As in (Berger et al., 1996) we consider a window of 3 words to the left and to the right of the target word considered. wordwhich is connected toaccording to the Viterbi alignment. Word classes: Instead of usin</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz J. Och and Hermann Ney. 2000b. Improved statistical alignment models. In Proc. of the 38th Annual Meeting of the Association for Computational Linguistics, pages 440–447, Hongkong, China, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>An efficient method for determining bilingual word classes.</title>
<date>1999</date>
<booktitle>In EACL ’99: Ninth Conf. of the Europ. Chapter of the Association for Computational Linguistics,</booktitle>
<pages>71--76</pages>
<location>Bergen, Norway,</location>
<contexts>
<context position="8118" citStr="Och, 1999" startWordPosition="1316" endWordPosition="1317"> right of the target word. In this work we use the following contextual information: Target context: As in (Berger et al., 1996) we consider a window of 3 words to the left and to the right of the target word considered. wordwhich is connected toaccording to the Viterbi alignment. Word classes: Instead of using a dependency on the word identity we include also a dependency on word classes. By doing this, we improve the generalization of the models and include some semantic and syntactic information with. The word classes are computed automatically using another statistical training procedure (Och, 1999) which often produces word classes including words with the same semantic meaning in the same class. A training event, for a specific target word, is composed by three items: The source wordaligned to. The context in which the aligned pair appears. The number of occurrences of the event in the training corpus. Table 1 shows some examples of training events for the target word “which”. 5 Features Once we have a set of training events for each target word we need to describe our feature functions. We do this by first specifying a large pool of possible features and then by selecting a subset of </context>
</contexts>
<marker>Och, 1999</marker>
<rawString>Franz J. Och. 1999. An efficient method for determining bilingual word classes. In EACL ’99: Ninth Conf. of the Europ. Chapter of the Association for Computational Linguistics, pages 71–76, Bergen, Norway, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K A Papineni</author>
<author>S Roukos</author>
<author>R T Ward</author>
</authors>
<title>Feature-based language understanding.</title>
<date>1996</date>
<booktitle>In ESCA, Eurospeech,</booktitle>
<pages>1435--1438</pages>
<location>Rhodes, Greece.</location>
<contexts>
<context position="2352" citStr="Papineni et al., 1996" startWordPosition="349" endWordPosition="353">part-of-speech information, syntactic constituent, sentence mood; Semantic information: disambiguation information (e.g. from WordNet), current/previous speech or dialog act. To include this additional information within the statistical framework we use the maximum entropy approach. This approach has been applied in natural language processing to a variety of tasks. (Berger et al., 1996) applies this approach to the so-called IBM Candide system to build context dependent models, compute automatic sentence splitting and to improve word reordering in translation. Similar techniques are used in (Papineni et al., 1996; Papineni et al., 1998) for socalled direct translation models instead of those proposed in (Brown et al., 1993). (Foster, 2000) describes two methods for incorporating information about the relative position of bilingual word pairs into a maximum entropy translation model. Other authors have applied this approach to language modeling (Rosenfeld, 1996; Martin et al., 1999; Peters and Klakow, 1999). A short review of the maximum entropy approach is outlined in Section 3. 2 Statistical Machine Translation The goal of the translation process in statistical machine translation can be formulated a</context>
</contexts>
<marker>Papineni, Roukos, Ward, 1996</marker>
<rawString>K.A. Papineni, S. Roukos, and R.T. Ward. 1996. Feature-based language understanding. In ESCA, Eurospeech, pages 1435–1438, Rhodes, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K A Papineni</author>
<author>S Roukos</author>
<author>R T Ward</author>
</authors>
<title>Maximum likelihood and discriminative training of direct translation models.</title>
<date>1998</date>
<booktitle>In Proc. Int. Conf. on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>189--192</pages>
<contexts>
<context position="2376" citStr="Papineni et al., 1998" startWordPosition="354" endWordPosition="357">ion, syntactic constituent, sentence mood; Semantic information: disambiguation information (e.g. from WordNet), current/previous speech or dialog act. To include this additional information within the statistical framework we use the maximum entropy approach. This approach has been applied in natural language processing to a variety of tasks. (Berger et al., 1996) applies this approach to the so-called IBM Candide system to build context dependent models, compute automatic sentence splitting and to improve word reordering in translation. Similar techniques are used in (Papineni et al., 1996; Papineni et al., 1998) for socalled direct translation models instead of those proposed in (Brown et al., 1993). (Foster, 2000) describes two methods for incorporating information about the relative position of bilingual word pairs into a maximum entropy translation model. Other authors have applied this approach to language modeling (Rosenfeld, 1996; Martin et al., 1999; Peters and Klakow, 1999). A short review of the maximum entropy approach is outlined in Section 3. 2 Statistical Machine Translation The goal of the translation process in statistical machine translation can be formulated as follows: A source lang</context>
</contexts>
<marker>Papineni, Roukos, Ward, 1998</marker>
<rawString>K.A. Papineni, S. Roukos, and R.T. Ward. 1998. Maximum likelihood and discriminative training of direct translation models. In Proc. Int. Conf. on Acoustics, Speech, and Signal Processing, pages 189–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jochen Peters</author>
<author>Dietrich Klakow</author>
</authors>
<title>Compact maximum entropy language models.</title>
<date>1999</date>
<booktitle>In Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding,</booktitle>
<location>Keystone, CO,</location>
<contexts>
<context position="2753" citStr="Peters and Klakow, 1999" startWordPosition="412" endWordPosition="415"> this approach to the so-called IBM Candide system to build context dependent models, compute automatic sentence splitting and to improve word reordering in translation. Similar techniques are used in (Papineni et al., 1996; Papineni et al., 1998) for socalled direct translation models instead of those proposed in (Brown et al., 1993). (Foster, 2000) describes two methods for incorporating information about the relative position of bilingual word pairs into a maximum entropy translation model. Other authors have applied this approach to language modeling (Rosenfeld, 1996; Martin et al., 1999; Peters and Klakow, 1999). A short review of the maximum entropy approach is outlined in Section 3. 2 Statistical Machine Translation The goal of the translation process in statistical machine translation can be formulated as follows: A source language string is to be translated into a target language string . In the experiments reported in this paper, the source language is German and the target language is English. Every target string is considered as a possible translation for the input. If we assign a probability to each pair of strings , then according to Bayes’ decision rule, we have to choose the target string </context>
</contexts>
<marker>Peters, Klakow, 1999</marker>
<rawString>Jochen Peters and Dietrich Klakow. 1999. Compact maximum entropy language models. In Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding, Keystone, CO, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
<author>John Lafferty</author>
</authors>
<title>Inducing features in random fields.</title>
<date>1997</date>
<journal>IEEE Trans. on Pattern Analysis and Machine Inteligence,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="6478" citStr="Pietra et al., 1997" startWordPosition="1027" endWordPosition="1030">word which has the translationwe can express this dependency using the following feature function: The ME principle suggests that the optimal is approximated by a lexicon model by dropping the dependencies on if and (1) otherwise parametric form of a model taking into account only the feature functions is given by: Here is a normalization factor. The resulting model has an exponential form with free parameters . The parameter values which maximize the likelihood for a given training corpus can be computed with the socalled GIS algorithm (general iterative scaling) or its improved version IIS (Pietra et al., 1997; Berger et al., 1996). It is important to notice that we will have to obtain one ME model for each target word observed in the training data. 4 Contextual information and training events In order to train the ME model associated to a target word, we need to construct a corresponding training sample from the whole bilingual corpus depending on the contextual information that we want to use. To construct this sample, we need to know the word-to-word alignment between each sentence pair within the corpus. That is obtained using the Viterbi alignment provided by a translation model as described i</context>
<context position="14031" citStr="Pietra et al., 1997" startWordPosition="2364" endWordPosition="2367">ervation. The task is difficult because it consists of spontaneous speech and the syntactic structures of the sentences are less restricted and highly variable. For the rescoring experiments we use the corpus described in Table 5. Table 5: Corpus characteristics for translation task. German English Train Sentences 58 332 Words 519 523 549 921 Vocabulary 7 940 4 673 Test Sentences 147 Words 1968 2 173 PP (trigr. LM) (40.3) 28.8 To train the maximum entropy models we used the “Ristad ME Toolkit” described in (Ristad, 1997). We performed 100 iteration of the Improved Iterative Scaling algorithm (Pietra et al., 1997) using the corpus described in Table 6, Table 6: Corpus characteristics for perplexity quality experiments. German English Train Sentences 50 000 Words 454 619 482 344 Vocabulary 7 456 4 420 Test Sentences 8073 Words 64 875 65 547 Vocabulary 2 579 1666 which is a subset of the corpus shown in Table 5. 6.2 Training and test perplexities In order to compute the training and test perplexities, we split the whole aligned training corpus in two parts as shown in Table 6. The training and test perplexities are shown in Table 7. As expected, the perplexity reduction in the test corpus is lower than i</context>
</contexts>
<marker>Pietra, Pietra, Lafferty, 1997</marker>
<rawString>Stephen Della Pietra, Vincent Della Pietra, and John Lafferty. 1997. Inducing features in random fields. IEEE Trans. on Pattern Analysis and Machine Inteligence, 19(4):380–393, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric S Ristad</author>
</authors>
<title>Maximum entropy modelling toolkit.</title>
<date>1997</date>
<tech>Technical report,</tech>
<institution>Princeton Univesity.</institution>
<contexts>
<context position="13937" citStr="Ristad, 1997" startWordPosition="2351" endWordPosition="2352">ranslation task in the domain of appointment scheduling, travel planning, and hotel reservation. The task is difficult because it consists of spontaneous speech and the syntactic structures of the sentences are less restricted and highly variable. For the rescoring experiments we use the corpus described in Table 5. Table 5: Corpus characteristics for translation task. German English Train Sentences 58 332 Words 519 523 549 921 Vocabulary 7 940 4 673 Test Sentences 147 Words 1968 2 173 PP (trigr. LM) (40.3) 28.8 To train the maximum entropy models we used the “Ristad ME Toolkit” described in (Ristad, 1997). We performed 100 iteration of the Improved Iterative Scaling algorithm (Pietra et al., 1997) using the corpus described in Table 6, Table 6: Corpus characteristics for perplexity quality experiments. German English Train Sentences 50 000 Words 454 619 482 344 Vocabulary 7 456 4 420 Test Sentences 8073 Words 64 875 65 547 Vocabulary 2 579 1666 which is a subset of the corpus shown in Table 5. 6.2 Training and test perplexities In order to compute the training and test perplexities, we split the whole aligned training corpus in two parts as shown in Table 6. The training and test perplexities </context>
</contexts>
<marker>Ristad, 1997</marker>
<rawString>Eric S. Ristad. 1997. Maximum entropy modelling toolkit. Technical report, Princeton Univesity.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rosenfeld</author>
</authors>
<title>A maximum entropy approach to adaptive statistical language modeling.</title>
<date>1996</date>
<journal>Computer, Speech and Language,</journal>
<pages>10--187</pages>
<contexts>
<context position="2706" citStr="Rosenfeld, 1996" startWordPosition="406" endWordPosition="407">f tasks. (Berger et al., 1996) applies this approach to the so-called IBM Candide system to build context dependent models, compute automatic sentence splitting and to improve word reordering in translation. Similar techniques are used in (Papineni et al., 1996; Papineni et al., 1998) for socalled direct translation models instead of those proposed in (Brown et al., 1993). (Foster, 2000) describes two methods for incorporating information about the relative position of bilingual word pairs into a maximum entropy translation model. Other authors have applied this approach to language modeling (Rosenfeld, 1996; Martin et al., 1999; Peters and Klakow, 1999). A short review of the maximum entropy approach is outlined in Section 3. 2 Statistical Machine Translation The goal of the translation process in statistical machine translation can be formulated as follows: A source language string is to be translated into a target language string . In the experiments reported in this paper, the source language is German and the target language is English. Every target string is considered as a possible translation for the input. If we assign a probability to each pair of strings , then according to Bayes’ deci</context>
</contexts>
<marker>Rosenfeld, 1996</marker>
<rawString>R. Rosenfeld. 1996. A maximum entropy approach to adaptive statistical language modeling. Computer, Speech and Language, 10:187–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Word re-ordering and dp-based search in statistical machine translation.</title>
<date>2000</date>
<booktitle>In 8th International Conference on Computational Linguistics (CoLing</booktitle>
<pages>850--856</pages>
<location>Saarbr¨ucken, Germany,</location>
<contexts>
<context position="17672" citStr="Tillmann and Ney, 2000" startWordPosition="2977" endWordPosition="2980">t the WER measure alone can be misleading. In order to overcome this problem, we introduce as additional measure the position-independent word error rate (PER). This measure compares the words in the two sentences without taking the word order into account. Depending on whether the translated sentence is longer or shorter than the target translation, the remaining words result in either insertion or deletion errors in addition to substitution errors. The PER is guaranteed to be less than or equal to the WER. We use the top-10 list of hypothesis provided by the translation system described in (Tillmann and Ney, 2000) for rescoring the hypothesis using the ME models and sort them according to the new maximum entropy score. The translation results in terms of error rates are shown in Table 8. We use Model 4 in order to perform the translation experiments because Model 4 typically gives better translation results than Model 5. We see that the translation quality improves slightly with respect to the WER and PER. The translation quality improvements so far are quite small compared to the perplexity measure improvements. We attribute this to the fact that the algorithm for computing the-best lists is suboptima</context>
</contexts>
<marker>Tillmann, Ney, 2000</marker>
<rawString>Christoph Tillmann and Hermann Ney. 2000. Word re-ordering and dp-based search in statistical machine translation. In 8th International Conference on Computational Linguistics (CoLing 2000), pages 850–856, Saarbr¨ucken, Germany, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Tillmann</author>
<author>S Vogel</author>
<author>H Ney</author>
<author>A Zubiaga</author>
</authors>
<title>A DP-based search using monotone alignments in statistical translation.</title>
<date>1997</date>
<booktitle>In Proc. 35th Annual Conf. of the Association for Computational Linguistics,</booktitle>
<pages>289--296</pages>
<location>Madrid, Spain,</location>
<contexts>
<context position="3568" citStr="Tillmann et al., 1997" startWordPosition="549" endWordPosition="552">ted as follows: A source language string is to be translated into a target language string . In the experiments reported in this paper, the source language is German and the target language is English. Every target string is considered as a possible translation for the input. If we assign a probability to each pair of strings , then according to Bayes’ decision rule, we have to choose the target string that maximizes the product of the target language model and the string translation model . Many existing systems for statistical machine translation (Berger et al., 1994; Wang and Waibel, 1997; Tillmann et al., 1997; Nießen et al., 1998) make use of a special way of structuring the string translation model like proposed by (Brown et al., 1993): The correspondence between the words in the source and the target string is described by alignments that assign one target word position to each source word position. The lexicon probability of a certain target wordto occur in the target string is assumed to depend basically only on the source wordaligned to it. These alignment models are similar to the concept of Hidden Markov models (HMM) in speech recognition. The alignment mapping is from source positionto tar</context>
</contexts>
<marker>Tillmann, Vogel, Ney, Zubiaga, 1997</marker>
<rawString>C. Tillmann, S. Vogel, H. Ney, and A. Zubiaga. 1997. A DP-based search using monotone alignments in statistical translation. In Proc. 35th Annual Conf. of the Association for Computational Linguistics, pages 289–296, Madrid, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ye-Yi Wang</author>
<author>Alex Waibel</author>
</authors>
<title>Decoding algorithm in statistical translation.</title>
<date>1997</date>
<booktitle>In Proc. 35th Annual Conf. of the Association for Computational Linguistics,</booktitle>
<pages>366--372</pages>
<location>Madrid, Spain,</location>
<contexts>
<context position="3545" citStr="Wang and Waibel, 1997" startWordPosition="545" endWordPosition="548">nslation can be formulated as follows: A source language string is to be translated into a target language string . In the experiments reported in this paper, the source language is German and the target language is English. Every target string is considered as a possible translation for the input. If we assign a probability to each pair of strings , then according to Bayes’ decision rule, we have to choose the target string that maximizes the product of the target language model and the string translation model . Many existing systems for statistical machine translation (Berger et al., 1994; Wang and Waibel, 1997; Tillmann et al., 1997; Nießen et al., 1998) make use of a special way of structuring the string translation model like proposed by (Brown et al., 1993): The correspondence between the words in the source and the target string is described by alignments that assign one target word position to each source word position. The lexicon probability of a certain target wordto occur in the target string is assumed to depend basically only on the source wordaligned to it. These alignment models are similar to the concept of Hidden Markov models (HMM) in speech recognition. The alignment mapping is fro</context>
</contexts>
<marker>Wang, Waibel, 1997</marker>
<rawString>Ye-Yi Wang and Alex Waibel. 1997. Decoding algorithm in statistical translation. In Proc. 35th Annual Conf. of the Association for Computational Linguistics, pages 366–372, Madrid, Spain, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>