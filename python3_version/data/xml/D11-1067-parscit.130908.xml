<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.99862">
Multiword Expression Identification with Tree Substitution Grammars:
A Parsing tour de force with French
</title>
<author confidence="0.831196">
Spence Green*, Marie-Catherine de Marneffe†, John Bauer*, and Christopher D. Manning*†
</author>
<affiliation confidence="0.7425995">
*Computer Science Department, Stanford University
†Linguistics Department, Stanford University
</affiliation>
<email confidence="0.999216">
{spenceg,mcdm,horatio,manning}@stanford.edu
</email>
<sectionHeader confidence="0.989527" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.479080764705882">
Multiword expressions (MWE), a known nui-
sance for both linguistics and NLP, blur the
lines between syntax and semantics. Previous
work on MWE identification has relied primar-
ily on surface statistics, which perform poorly
for longer MWEs and cannot model discontin-
uous expressions. To address these problems,
we show that even the simplest parsing mod-
els can effectively identify MWEs of arbitrary
length, and that Tree Substitution Grammars
achieve the best results. Our experiments show
a 36.4% F1 absolute improvement for French
over an n-gram surface statistics baseline, cur-
rently the predominant method for MWE iden-
tification. Our models are useful for several
NLP tasks in which MWE pre-grouping has
improved accuracy.
</bodyText>
<sectionHeader confidence="0.99727" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9990555">
Multiword expressions (MWE) have long been a
challenge for linguistic theory and NLP. There is
no universally accepted definition of the term, but
MWEs can be characterized as “idiosyncratic inter-
pretations that cross word boundaries (or spaces)”
(Sag et al., 2002) such as traffic light, or as “fre-
quently occurring phrasal units which are subject
to a certain level of semantic opaqueness, or non-
compositionality” (Rayson et al., 2010).
MWEs are often opaque fixed expressions, al-
though the degree to which they are fixed can vary.
Some MWEs do not allow morphosyntactic varia-
tion or internal modification (e.g., in short, but *in
shorter or *in very short). Other MWEs are “semi-
fixed,” meaning that they can be inflected or undergo
internal modification. The type of modification is of-
ten limited, but not predictable, so it is not possible
to enumerate all variants (Table 1).
</bodyText>
<subsectionHeader confidence="0.588516">
French English
</subsectionHeader>
<bodyText confidence="0.3917145">
in the near term
in the short term
à tr8s court terme in the very short term
à moyen terme
à long terme
à tr8s long terme in the very long term
</bodyText>
<tableCaption confidence="0.862372">
Table 1: Semi-fixed MWEs in French and English. The
French adverb à terme ‘in the end’ can be modified by
a small set of adjectives, and in turn some of these ad-
jectives can be modified by an adverb such as très ‘very’.
Similar restrictions appear in English.
</tableCaption>
<bodyText confidence="0.965641642857143">
Merging known MWEs into single tokens has
been shown to improve accuracy for a variety of
NLP tasks: dependency parsing (Nivre and Nilsson,
2004), constituency parsing (Arun and Keller, 2005),
sentence generation (Hogan et al., 2007), and ma-
chine translation (Carpuat and Diab, 2010). Most ex-
periments use gold MWE pre-grouping or language-
specific resources like WordNet. For unlabeled text,
the best MWE identification methods, which are
based on surface statistics (Pecina, 2010), suffer
from sparsity induced by longer n-grams (Ramisch
et al., 2010). A dilemma thus exists: MWE knowl-
edge is useful, but MWEs are hard to identify.
In this paper, we show the effectiveness of statis-
tical parsers for MWE identification. Specifically,
Tree Substitution Grammars (TSG) can achieve a
36.4% F1 absolute improvement over a state-of-the-
art surface statistics method. We choose French,
which has pervasive MWEs, for our experiments.
Parsing models naturally accommodate discontinu-
ous MWEs like phrasal verbs, and provide syntac-
tic subcategorization. By contrast, surface statistics
methods are usually limited to binary judgements for
contiguous n-grams or dependency bigrams.
à terme
à court terme
in the mediumterm
in the long term
</bodyText>
<page confidence="0.969458">
725
</page>
<note confidence="0.973225">
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 725–735,
Edinburgh, Scotland, UK, July 27–31, 2011. c�2011 Association for Computational Linguistics
</note>
<table confidence="0.997366833333333">
FTB (train) WSJ (train)
Sentences 13,449 39,832
Tokens 398,248 950,028
#Word Types 28,842 44,389
#Tag Types 30 45
#Phrasal Types 24 27
Per Sentence
Depth (µ/v2) 4.03 / 0.360 4.18 / 0.730
Breadth (µ/v2) 13.5 / 6.79 10.7 / 4.59
Length (µ/v2) 29.6 / 17.3 23.9 / 11.2
Constituents (µ) 20.3 19.6
µ Constituents / µ Length 0.686 0.820
</table>
<tableCaption confidence="0.972503">
Table 2: Gross corpus statistics for the pre-processed FTB
(training set) and WSJ (sec. 2-21). The FTB sentences are
longer with broader syntactic trees. The FTB POS tag set
has 33% fewer types than the WSJ. The FTB dev set OOV
rate is 17.77% vs. 12.78% for the WSJ.
</tableCaption>
<table confidence="0.999702769230769">
Type #Total #Single %Single %Total
MWN noun 9,680 2,737 28.3 49.7
MWADV adverb 3,852 449 11.7 19.8
MWP prep. 3,526 342 9.70 18.1
MWC conj. 814 73 8.97 4.18
MWV verb. 585 243 41.5 3.01
MWD det. 328 69 21.0 1.69
MWA adj. 324 126 38.9 1.66
MWPRO pron. 266 33 12.4 1.37
MWCL clitic 59 1 1.69 0.30
MWET foreign 24 18 0.75 0.12
MWI interj. 4 2 0.50 0.02
19,462 4,093 21.0% 100.0%
</table>
<tableCaption confidence="0.999266">
Table 3: Frequency distribution of the 11 MWE subcate-
</tableCaption>
<bodyText confidence="0.86407575">
gories in the FTB (training set). MWEs account for 7.08%
of the bracketings and 13.0% of the tokens in the treebank.
Only 21% of the MWEs occur once (“single”).
We first introduce a new instantiation of the
French Treebank that, unlike previous work, does not
use gold MWE pre-grouping. Consequently, our ex-
perimental results also provide a better baseline for
parsing raw French text.
</bodyText>
<sectionHeader confidence="0.97519" genericHeader="introduction">
2 French Treebank Setup
</sectionHeader>
<bodyText confidence="0.996749727272727">
The corpus used in our experiments is the French
Treebank (Abeillé et al. (2003), version from June
2010, hereafter FTB). In French, there is a linguis-
tic tradition of lexicography which compiles lists
of MWEs occurring in the language. For exam-
ple, Gross (1986) shows that dictionaries contain
about 1,500 single-word adverbs but that French con-
tains over 5,000 multiword adverbs. MWEs occur
in every part-of-speech (POS) category (e.g., noun
trousse de secours ‘first-aid kit’; verb faire main-
basse [do hand-low] ‘seize’; adverb comme dans du
beurre [as in butter] ‘easily’; adjective ‘à part en-
tière’ ‘wholly’).
The FTB explicitly annotates MWEs (also called
compounds in prior work). We used the subset of
the corpus with functional annotations, not for those
annotations but because this subset is known to be
more consistently annotated. POS tags for MWEs
are given not only at the MWE level, but also inter-
nally: most tokens that constitute an MWE also have
a POS tag. Table 2 compares this part of the FTB to
the WSJ portion of the Penn Treebank.
</bodyText>
<subsectionHeader confidence="0.982864">
2.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.92996325">
The FTB requires significant pre-processing prior to
parsing.
Tokenization We changed the default tokenization
for numbers by fusing adjacent digit tokens. For ex-
ample, 500 000 is tagged as an MWE composed of
two words 500 and 000. We made this 500000 and
retained the MWE POS, although we did not mark
the new token as an MWE. For consistency, we used
one token for punctuated numbers like “17,9”.
MWE Tagging We marked MWEs with a flat
bracketing in which the phrasal label is the MWE-
level POS tag with an “MW” prefix, and the preter-
minals are the internal POS tags for each terminal.
The resulting POS sequences are not always unique
to MWEs: they appear in abundance elsewhere in
the corpus. However, some MWEs contain normally
ungrammatical POS sequences (e.g., adverb à la va
vite ‘in a hurry’: P D V ADV [at the goes quick]), and
some words appear only as part of an MWE, such as
insu in à l’insu de ‘to the ignorance of’.
Labels We augmented the basic FTB label set—
which contains 14 POS tags and 19 phrasal tags—in
two ways. First, we added 16 finer-grained POS tags
for punctuation.1 Second, we added the 11 MWE
1Punctuation tag clusters—as used in the WSJ—did not im-
prove accuracy. Enriched tag sets like that of Crabbé and Can-
dito (2008) could also be investigated and compared to our re-
sults since Evalb is insensitive to POS tags.
</bodyText>
<page confidence="0.997367">
726
</page>
<bodyText confidence="0.9924178">
correction of annotation errors. Like ARUN-CONT,
MFT contains concatenated MWEs.
labels shown in Table 3, resulting in 24 total phrasal
categories.
Corrections Historically, the FTB suffered from
annotation errors such as missing POS and phrasal
tags (Arun and Keller, 2005). We found that this
problem has been largely resolved in the current re-
lease. However, 1,949 tokens and 36 MWE spans
still lacked tags. We restored the labels by first as-
signing each token its most frequent POS tag else-
where in the treebank, and then assigning the most
frequent MWE phrasal category for the resulting
POS sequence.2
Split We used the 80/10/10 split described by
Crabbé and Candito (2008). However, they used a
previous release of the treebank with 12,531 trees.
3,391 trees have been added to the present version.
We appended these extra trees to the training set, thus
retaining the same development and test sets.
</bodyText>
<subsectionHeader confidence="0.999694">
2.2 Comparison to Prior FTB Representations
</subsectionHeader>
<bodyText confidence="0.999827">
Our pre-processing approach is simple and auto-
matic3 unlike the three major instantiations of the
FTB that have been used in previous work:
</bodyText>
<sectionHeader confidence="0.651715" genericHeader="method">
ARUN-CONT and ARUN-EXP (Arun and Keller,
</sectionHeader>
<bodyText confidence="0.999522133333333">
2005): Two instantiations of the full 20,000 sentence
treebank that differed principally in their treatment of
MWEs: (1) CONT, in which the tokens of each MWE
were concatenated into a single token (en moyenne
→ en_moyenne); (2) Exp, in which they were marked
with a flat structure. For both representations, they
also gave results in which coordinated phrase struc-
tures were flattened. In the published experiments,
they mistakenly removed half of the corpus, believ-
ing that the multi-terminal (per POS tag) annotations
of MWEs were XML errors (Schluter and Genabith,
2007).
MFT (Schluter and Genabith, 2007): Manual revi-
sion to 3,800 sentences. Major changes included co-
ordination raising, an expanded POS tag set, and the
</bodyText>
<footnote confidence="0.844424">
273 of the unlabeled word types did not appear elsewhere
in the treebank. All but 11 of these were nouns. We manually
assigned the correct tags, but we would not expect a negative
effect by deterministically labeling all of them as nouns.
3We automate tree manipulation with Tregex/Tsurgeon
(Levy and Andrew, 2006). Our pre-processing package is avail-
able at http://nlp.stanford.edu/software/lex-parser.shtml.
</footnote>
<bodyText confidence="0.998051863636364">
FTB-UC (Candito and Crabbé, 2009): An in-
stantiation of the functionally annotated section that
makes a distinction between MWEs that are “syn-
tactically regular” and those that are not. Syntacti-
cally regular MWEs were given internal structure,
while all other MWEs were concatenated into sin-
gle tokens. For example, nouns followed by ad-
jectives, such as loi agraire ‘land law’ or Union
monétaire et économique ‘monetary and economic
Union’ were considered syntactically regular. They
are MWEs because the choice of adjective is arbi-
trary (loi agraire and not *loi agricole, similarly to
‘coal black’ but not *‘crow black’ for example), but
their syntactic structure is not intrinsic to MWEs.
In such cases, FTB-UC gives the MWE a conven-
tional analysis of an NP with internal structure. Such
analysis is indeed sufficient to recover the mean-
ing of these semantically compositional MWEs that
are extremely productive. On the other hand, the
FTB-UC loses information about MWEs with non-
compositional semantics.
Almost all work on the FTB has followed ARUN-
CONT and used gold MWE pre-grouping. As a result,
most results for French parsing are analogous to early
results for Chinese, which used gold word segmen-
tation, and Arabic, which used gold clitic segmenta-
tion. Candito et al. (2010) were the first to acknowl-
edge and address this issue, but they still used FTB-
UC (with some pre-grouped MWEs). Since the syn-
tax and definition of MWEs is a contentious issue,
we take a more agnostic view—which is consistent
with that of the FTB annotators—and leave them to-
kenized. This permits a data-oriented approach to
MWE identification that is more robust to changes
to the status of specific MWE instances.
To set a baseline prior to grammar development,
we trained the Stanford parser (Klein and Manning,
2003) with no grammar features, achieving 74.2%
labeled F1 on the development set (sentences ≤ 40
words). This is lower than the most recent results ob-
tained by Seddah (2010). However, the results are
not comparable: the data split was different, they
made use of morphological information, and more
importantly they concatenated MWEs. The focus of
</bodyText>
<page confidence="0.981357">
727
</page>
<bodyText confidence="0.97777">
our work is on models and data representations that
enable MWE identification.
</bodyText>
<sectionHeader confidence="0.885412" genericHeader="method">
3 MWEs in Lexicon-Grammar
</sectionHeader>
<bodyText confidence="0.999915896551724">
The MWE representation in the FTB is close to
the one proposed in the Lexicon-Grammar (Gross,
1986). In the Lexicon-Grammar, MWEs are classi-
fied according to their global POS tags (noun, verb,
adverb, adjective), and described in terms of the se-
quence of the POS tags of the words that constitute
the MWE (e.g., “N de N” garde d’enfant [guard of
child] ‘daycare’, pied de guerre [foot of war] ‘at the
ready’). In other words, MWEs are represented by a
flat structure. The Lexicon-Grammar distinguishes
between units that are fixed and have to appear as is
(en tout et pour tout [in all and for all] ‘in total’) and
units that accept some syntactic variation such as ad-
mitting the insertion of an adverb or adjective, or the
variation of one of the words in the expression (e.g.,
a possessive as in ‘from the top of one’s hat’). It also
notes whether the MWE displays some selectional
preferences (e.g., it has to be preceded by a verb or
by an adjective).
Our FTB instantiation is largely consistent with
the Lexicon-Grammar. Recall that we defined differ-
ent MWE categories based on the global POS. We
now detail three of the categories.
MWN The MWN category consists of proper
nouns (1a), foreign common nouns (1b), as well as
common nouns. The common nouns appear in sev-
eral syntactically regular sequences of POS tags (2).
Multiword nouns allow inflection (singular vs. plu-
ral) but no insertion.
</bodyText>
<listItem confidence="0.987450666666667">
(1) a. London Sunday Times, Los Angeles
b. week - end, mea culpa, joint - venture
(2) a. N A: corps médical ‘medical staff’, dette
publique ‘public debt’
b. N P N: mode d’emploi ‘instruction man-
ual’
c. N N: numéro deux ‘number two’, mai-
son mère [house mother] ‘headquarters’,
grève surprise ‘sudden strike’
</listItem>
<bodyText confidence="0.963493333333333">
d. N P D N: impôt sur le revenu ‘income
tax’, ministre de l’économie ‘finance
minister’
MWA Multiword adjectives appear with different
POS sequences (3). They include numbers such as
vingt et unième ‘21st’. Some items in (3b) allow in-
ternal variation: some adverbs or adjectives can be
added to both examples given (à très haut risque, de
toute dernière minute).
</bodyText>
<listItem confidence="0.803121875">
(3) a. P N: d’antan [from before] ‘old’, en
question ‘under discussion’
b. P A N: à haut risque ‘high-risk’, de
dernière minute [from the last minute]
‘at the eleventh hour’
c. A C A: pur et simple [pure and simple]
‘straightforward’, noir et blanc ‘black
and white’
</listItem>
<bodyText confidence="0.9703697">
MWV Multiword verbs also appear in several POS
sequences (4). All verbs allow number and tense in-
flections. Some MWVs containing a noun or an ad-
jective allow the insertion of a modifier (e.g., don-
ner grande satisfication ‘give great satisfaction’),
whereas others do not. When an adverb intervenes
between the main verb and its complement, the FTB
marks the two parts of the MWV discontinuously
(e.g., [MWV [V prennent]] [ADV déjà] [MWV [P en] [N
cause]] ‘already take into account’).
(4) a. V N: avoir lieu ‘take place’, donner sat-
isfaction ‘give satisfaction’
b. V P N: mettre en place ‘put in place’,
entrer en vigueur ‘to come into effect’
c. V P ADV: mettre à mal [put at bad]
‘harm’, être à même [be at same] ‘be
able’
d. V D N P N: tirer la sonnette d’alarme
‘ring the alarm bell’, avoir le vent en
poupe ‘to have the wind astern’
</bodyText>
<sectionHeader confidence="0.982934" genericHeader="method">
4 Parsing Models
</sectionHeader>
<bodyText confidence="0.999963222222222">
We develop two parsers for French with the goal
of improving MWE identification. The first is a
manually-annotated grammar that we incorporate
into the Stanford parser. Manual annotation results in
human interpretable grammars that can inform future
treebank annotation decisions. Moreover, the gram-
mar can be used as the base distribution in our sec-
ond model, a Probabilistic Tree Substitution Gram-
mar (PTSG) parser. PTSGs learn parameters for tree
</bodyText>
<page confidence="0.985334">
728
</page>
<table confidence="0.9341074">
Feature States Tags F1 AF1
— 4325 31 74.21
tagPA 4509 215 76.94 +2.73
markInf 4510 216 77.42 +0.48
markPart 4511 217 77.73 +0.31
markVN 5986 217 78.32 +0.59
markCoord 7361 217 78.45 +0.13
markDe 7521 233 79.11 +0.66
markP 7523 235 79.34 +0.23
markMWE 7867 235 79.23 −0.11
</table>
<tableCaption confidence="0.9554298">
Table 4: Effects on grammar size and labeled F1 for each
of the manual state splits (development set, sentences ≤
40 words). markMWE decreases overall accuracy, but
increases both the number of correctly parsed trees (by
0.30%) and per category MWE accuracy.
</tableCaption>
<bodyText confidence="0.9205655">
fragments larger than basic CFG rules. PTSG rules
may also be lexicalized. This means that commonly
observed collocations—some of which are MWEs—
can be stored in the grammar.
</bodyText>
<subsectionHeader confidence="0.999572">
4.1 Stanford Parser
</subsectionHeader>
<bodyText confidence="0.962591947368421">
We configure the Stanford parser with settings that
are effective for other languages: selective parent an-
notation, lexicon smoothing, and factored parsing.
We use the head-finding rules of Dybro-Johansen
(2004), which we find to yield an approximately
1.0% F1 development set improvement over those of
Arun (2004). Finally, we include a simple unknown
word model consisting entirely of surface features:
- Nominal, adjectival, verbal, adverbial, and plu-
ral suffixes
- Contains a digit or punctuation
- Is capitalized (except the first word in a sen-
tence)
- Consists entirely of capital letters
- If none of the above, add a one- or two-character
suffix
Combined with the grammar features, this unknown
word model yields 97.3% tagging accuracy on the
development set.
</bodyText>
<subsubsectionHeader confidence="0.675219">
4.1.1 Grammar Development
</subsubsectionHeader>
<bodyText confidence="0.999857545454546">
Table 4 lists the symbol refinements used in our
grammar. Most of the features are POS splits as
many phrasal tag splits did not lead to any improve-
ment. Parent annotation of POS tags (tagPA) cap-
tures information about the external context. mark-
Inf and markPart accomplish a finite/nonfinite dis-
tinction: they respectively specify whether the verb
is an infinitive or a participle based on the type of
the grandparent node. markVN captures the notion
of verbal distance as in Klein and Manning (2003).
We opted to keep the COORD phrasal tag, and
to capture parallelism in coordination, we mark CO-
ORD with the type of its child (NP, AP, VPinf, etc.).
markDe identifies the preposition de and its variants
(du, des, d’) which is very frequent and appears in
several different contexts. markP identifies preposi-
tions which introduce PPs modifying a noun. Mark-
ing other kinds of prepositional modifiers (e.g., verb)
did not help. markMWE adds an annotation to sev-
eral MWE categories for frequently occuring POS
sequences. For example, we mark MWNs that occur
more than 600 times (e.g., “N P N” and “N N”).
</bodyText>
<subsectionHeader confidence="0.602435">
4.2 DP-TSG Parser
</subsectionHeader>
<bodyText confidence="0.999841333333333">
A shortcoming of CFG-based grammars is that they
do not explicitly capture idiomatic usage. For exam-
ple, consider the two utterances:
</bodyText>
<listItem confidence="0.488115">
(5) a. He [MWV kicked the bucket] .
b. He [VP kicked [NP the pail]].
</listItem>
<bodyText confidence="0.9985503">
The examples in (5) may be equally probable and re-
ceive the same analysis under a PCFG; words are
generated independently. However, recall that in
our representation, (5a) should receive a flat analysis
as MWV, whereas (5b) should have a conventional
analysis of the verb kicked and its two arguments.
An alternate view of parsing is one in which new
utterances are built from previously observed frag-
ments. This is the original motivation for data ori-
ented parsing (DOP) (Bod, 1992), in which “id-
iomaticity is the rule rather than the exception”
(Scha, 1990). If we have seen the collocation kicked
the bucket several times before, we should store that
whole fragment for later use.
We consider a variant of the non-parametric PTSG
model of Cohn et al. (2009) in which tree fragments
are drawn from a Dirichlet process (DP) prior.4
The DP-TSG can be viewed as a DOP model with
Bayesian parameter estimation. A PTSG is a 5-tuple
(V, Σ, R, Q, θ) where c E V are non-terminals;
</bodyText>
<footnote confidence="0.801754">
4Similar models were developed independently by
O’Donnell et al. (2009) and Post and Gildea (2009).
</footnote>
<page confidence="0.986721">
729
</page>
<figure confidence="0.562504272727273">
αc DP concentration parameter for each c E V
P0(e|c) CFG base distribution
x Set of non-terminal nodes in the treebank
S Set of sampling sites (one for each x E x)
S A block of sampling sites, where S ⊆ S
b = {bs}s∈S Binary variables to be sampled (bs = 1 →
frontier node)
z Latent state of the segmented treebank
m Number of sites s E S s.t. bS = 1
n = {nc,e} Sufficient statistics of z
OnS:m Change in counts by setting m sites in S
</figure>
<tableCaption confidence="0.9938305">
Table 5: DP-TSG model notation. For consistency, we
largely follow the notation of Liang et al. (2010). Note
</tableCaption>
<equation confidence="0.915742666666667">
that z = (b, x), and as such z = (c, e).
t E E are terminals; e E R are elementary trees;5
Q E V is a unique start symbol; and θc,e E 0 are
</equation>
<bodyText confidence="0.999864875">
parameters for each tree fragment. A PTSG deriva-
tion is created by successively applying the substitu-
tion operator to the leftmost frontier node (denoted
by c+). All other nodes are internal (denoted by c−).
In the supervised setting, DP-TSG grammar ex-
traction reduces to a segmentation problem. We have
a treebank T that we segment into the set R, a pro-
cess that we model with Bayes’ rule:
</bodyText>
<equation confidence="0.891216">
p(R  |T) a p(T  |R) p(R) (1)
</equation>
<bodyText confidence="0.999118333333333">
Since the tree fragments completely specify each
tree, p(T  |R) is either 0 or 1, so all work is per-
formed by the prior over the set of elementary trees.
The DP-TSG contains a DP prior for each c E V
(Table 5 defines further notation). We generate (c, e)
tuples as follows:
</bodyText>
<equation confidence="0.9195425">
θc|c, αc, P0(�|c) - DP(αc, P0)
e|θc - θc
</equation>
<bodyText confidence="0.940421">
The data likelihood is given by the latent state z and
the parameters 0: p(z|0) = 11zEz θnc,e(z)
c,e . Integrat-
ing out the parameters, we have:
</bodyText>
<equation confidence="0.916555333333333">
He(αcP0(e|c))nc,e(z)
nc,· (z)
αc
</equation>
<bodyText confidence="0.9974165">
where xn = x(x + 1) ... (x + n − 1) is the rising
factorial. (§A.1 contains ancillary details.)
Base Distribution The base distribution P0 is the
same maximum likelihood PCFG used in the Stan-
</bodyText>
<footnote confidence="0.767309">
5We use the terms tree fragment and elementary tree inter-
changeably.
</footnote>
<figureCaption confidence="0.983816375">
Figure 1: Example of two conflicting sites of the same
type. Define the type of a site t(z, s) def = (Ans:0, Ans:1).
Sites (1) and (2) above have the same type since t(z, s1) =
t(z, s2). However, the two sites conflict since the prob-
abilities of setting bs1 and bs2 both depend on counts for
the tree fragment rooted at NP. Consequently, sites (1) and
(2) are not exchangeable: the probabilities of their assign-
ments depend on the order in which they are sampled.
</figureCaption>
<bodyText confidence="0.9889455">
ford parser.6,7 After applying the manual state splits,
we perform simple right binarization, collapse unary
rules, and replace rare words with their signatures
(Petrov et al., 2006).
For each non-terminal type c, we learn a stop prob-
ability sc - Beta(1,1). Under P0, the probability of
generating a rule A+ -+ B− C+ composed of non-
terminals is
</bodyText>
<equation confidence="0.5617415">
P0(A+ -+ B− C+) = pMLE(A -+ B C)sB(1−sC)
(3)
</equation>
<bodyText confidence="0.9929745">
For lexical insertion rules, we add a penalty propor-
tional to the frequency of the lexical item:
</bodyText>
<equation confidence="0.998787">
P0(c -+ t) = pMLE(c -+ t)p(t) (4)
</equation>
<bodyText confidence="0.9998975">
where p(t) is equal to the MLE unigram probabil-
ity of t in the treebank. Lexicalizing a rule makes it
very specific, so we generally want to avoid lexical-
ization with rare words. Empirically, we found that
this penalty reduces overfitting.
Type-based Inference Algorithm To learn the pa-
rameters 0 we use the collapsed, block Gibbs sam-
pler of Liang et al. (2010). We sample binary vari-
ables bs associated with each non-terminal node/site
in the treebank. The key idea is to select a block
of exchangeable sites S of the same type that do not
conflict (Figure 1). Since the sites in S are exchange-
able, we can set bS randomly so long as we know m,
the number of sites with bs = 1. Because this algo-
rithm is a not a contribution of this paper, we refer
the reader to Liang et al. (2010).
</bodyText>
<footnote confidence="0.70964625">
6The Stanford parser is a product model, so the results in §5.1
include the contribution of a dependency parser.
7Bansal and Klein (2010) also experimented with symbol re-
finement in an all-fragments (parametric) TSG for English.
</footnote>
<figure confidence="0.994065384615385">
NP+
PUNC-(1)
11
N+
Jacques
N-
Chirac
PUNC+(2)
11
�
p(z) =
cEV
(2)
</figure>
<page confidence="0.98979">
730
</page>
<bodyText confidence="0.999708833333333">
After each Gibbs iteration, we sample each sc di-
rectly using binomial-Beta conjugacy. We re-sample
the DP concentration parameters αc with the auxil-
iary variable procedure of West (1995).
Decoding We compute the rule score of each tree
fragment from a single grammar sample as follows:
</bodyText>
<equation confidence="0.999912">
nc,e(z) + αcP0(e|c)
θc,e = (5)
nc,·(z) + αc
</equation>
<bodyText confidence="0.999925454545455">
To make the grammar more robust, we also include
all CFG rules in Po with zero counts in n. Scores for
these rules follow from (5) with nc,e(z) = 0.
For decoding, we note that the derivations of a
TSG are a CFG parse forest (Vijay-Shanker and Weir,
1993). As such, we can use a Synchronous Context
Free Grammar (SCFG) to translate the 1-best parse
to its derivation. Consider a unique tree fragment ez
rooted at X with frontier &apos;y, which is a sequence of
terminals and non-terminals. We encode this frag-
ment as an SCFG rule of the form
</bodyText>
<equation confidence="0.974501">
[X -+ &apos;y , X -+ i, Yl, ... , Yn] (6)
</equation>
<bodyText confidence="0.999664">
where Yl, ... , Yn is the sequence of non-terminal
nodes in &apos;y.8 During decoding, the input is re-
written as a sequence of tree fragment (rule) indices
{i, j, k,... }. Because the TSG substitution operator
always applies to the leftmost frontier node, we can
deterministically recover the monolingual parse with
top-down re-writes of Q.
The SCFG formulation has a practical benefit: we
can take advantage of the heavily-optimized SCFG
decoders for machine translation. We use cdec
(Dyer et al., 2010) to recover the Viterbi derivation
under a DP-TSG grammar sample.
</bodyText>
<sectionHeader confidence="0.999876" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999392">
5.1 Standard Parsing Experiments
</subsectionHeader>
<bodyText confidence="0.999927666666667">
We evaluate parsing accuracy of the Stanford and
DP-TSG models (Table 6). For comparison, we also
include the Berkeley parser (Petrov et al., 2006).9
For the DP-TSG, we initialized all bs with fair coin
tosses and ran for 400 iterations, after which likeli-
hood stopped improving.
</bodyText>
<footnote confidence="0.745432333333333">
8This formulation is due to Chris Dyer.
9Training settings: right binarization, no parent annotation,
six split-merge cycles, and random initialization.
</footnote>
<table confidence="0.999554">
Leaf Ancestor LP Evalb EX%
Corpus Sent LR F1
PA-PCFG 0.793 0.812 68.1 67.0 67.6 10.5
DP-TSG 0.823 0.842 75.6 76.0 75.8 15.1
Stanford 0.843 0.861 77.8 79.0 78.4 17.5
Berkeley 0.880 0.891 82.4 82.0 82.2 21.4
</table>
<tableCaption confidence="0.93952325">
Table 6: Standard parsing experiments (test set, sentences
&lt; 40 words). All parsers exceed 96% tagging accuracy.
Berkeley and DP-TSG results are the average of three in-
dependentruns.
</tableCaption>
<bodyText confidence="0.999987142857143">
We report two different parsing metrics. Evalb
is the standard labeled precision/recall metric.10
Leaf Ancestor measures the cost of transforming
guess trees to the reference (Sampson and Babar-
czy, 2003). It was developed in response to the non-
terminal/terminal ratio bias of Evalb, which penal-
izes flat treebanks like the FTB. The range of the
score is between 0 and 1 (higher is better). We report
micro-averaged (whole corpus) and macro-averaged
(per sentence) scores.
In terms of parsing accuracy, the Berkeley parser
exceeds both Stanford and DP-TSG. This is consis-
tent with previous experiments for French by Sed-
dah et al. (2009), who show that the Berkeley parser
outperforms other models. It also matches the or-
dering for English (Cohn et al., 2010; Liang et al.,
2010). However, the standard baseline for TSG mod-
els is a simple parent-annotated PCFG (PA-PCFG).
For English, Liang et al. (2010) showed that a similar
DP-TSG improved over PA-PCFG by 4.2% F1. For
French, our gain is a more substantial 8.2% F1.
</bodyText>
<subsectionHeader confidence="0.987553">
5.2 MWE Identification Experiments
</subsectionHeader>
<bodyText confidence="0.997553">
Table 7 lists overall and per-category MWE identifi-
cation results for the parsing models. Although DP-
TSG is less accurate as a general parsing model, it is
more effective at identifying MWEs.
The predominant approach to MWE identification
is the combination of lexical association measures
(surface statistics) with a binary classifier (Pecina,
2010). A state-of-the-art, language independent
package that implements this approach for higher
order n-grams is mwetoolkit (Ramisch et al.,
2010).11 In Table 8 we compare DP-TSG to both
</bodyText>
<footnote confidence="0.999667">
10Available at http://nlp.cs.nyu.edu/evalb/ (v.20080701).
11Available at http://multiword.sourceforge.net/. See §A.2 for
</footnote>
<page confidence="0.990329">
731
</page>
<table confidence="0.999527818181818">
#gold Stanford DP-TSG Berkeley
MWET 3 0.0 0.0 0.0
MWV 26 64.0 57.7 50.7
MWA 8 26.1 32.2 29.8
MWN 456 64.1 67.6 67.1
MWD 15 70.3 65.5 70.1
MWPRO 17 73.7 78.0 76.2
MWADV 220 74.6 72.7 70.4
MWP 162 81.3 80.5 77.7
MWC 47 83.5 83.5 80.8
954 70.1 71.1 69.6
</table>
<tableCaption confidence="0.998347666666667">
Table 7: MWE identification per category and overall re-
sults (test set, sentences &lt; 40 words). MWI and MWCL
do not occur in the test set.
</tableCaption>
<figure confidence="0.99531226">
tour
de
passe
- passe
tour
P
NP
de
MWN
(a) Reference
N
-
N
MWN
MWV
sous - V
faire N
V les moyens
VdeN
VenN
MWP
de l’ordre de
y compris
au N de
en N de
ADV de
Table 9: Sample of the TSG rules learned.
MWN
-
P
N
N
N
NP
N
PP
sociétés de N
prix de N
coup de N
N d’état
NdeN
N à N
Model F1
mwetoolkit All 15.4
PA-PCFG 32.6
mwetoolkit Filter 34.7
PA-PCFG+Features 63.1
DP-TSG 71.1
passe - passe
(b) DP-TSG
</figure>
<figureCaption confidence="0.985164">
Figure 2: Example of an MWE error for tour de passe-
passe ‘magic trick’. (dev set)
</figureCaption>
<tableCaption confidence="0.83991275">
Table 8: MWE identification F1 of the best parsing model
vs. the mwetoolkit baseline (test set, sentences &lt; 40
words). PA-PCFG+Features includes the grammar fea-
tures in Table 4, which is the CFG from which the TSG is
extracted. For mwetoolkit, All indicates the inclusion
of all n-grams in the training corpus. Filter indicates pre-
filtering of the training corpus by removing rare n-grams
(see §A.2 for details).
</tableCaption>
<bodyText confidence="0.99984225">
mwetoolkit and the CFG from the which the TSG
is extracted. The TSG-based parsing model outper-
forms mwetoolkit by 36.4% F1 while providing
syntactic subcategory information.
</bodyText>
<sectionHeader confidence="0.998504" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.988263162162162">
Automatic learning methods run the risk of produc-
ing uninterpretable models. However, the DP-TSG
model learns useful generalizations over MWEs. A
sample of the rules is given in Table 9. Some spe-
cific sequences like “[MWN [coup de N]]” are part of
the grammar: such rules can indeed generate quite
a few MWEs, e.g., coup de pied ‘kick’, coup de
coeur, coup de foudre ‘love at first sight’, coup de
main ‘help’, coup d’état, coup de grâce (note that
only some of these MWEs are seen in the training
configuration details.
data). For MWV, “V de N” as in avoir de cesse ‘give
no peace’, perdre de vue [lose from sight] ‘forget’,
prendre de vitesse [take from speed] ‘outpace’), is
learned. For prepositions, the grammar stores full
subtrees of MWPs, but can also generalize the struc-
ture of very frequent sequences: “en N de” occurs in
many multiword prepositions (e.g., en compagnie de,
en face de, en matière de, en terme de, en cours de,
en faveur de, en raison de, en fonction de). The TSG
grammar thus provides a categorization of MWEs
consistent with the Lexicon-Grammar. It also learns
verbal phrases which contain discontinuous MWVs
due to the insertion of an adverb or negation such as
“[VN [MWV va] [MWADV d’ailleurs] [MWV bon train]]”
[go indeed well], “[VN [MWV a] [ADV jamais] [MWV
été question d’]]” [has never been in question].
A significant fraction of errors for MWNs occur
with adjectives that are not recognized as part of the
MWE. For example, since établissements privés ‘pri-
vate corporation’ is unseen in the training data, it is
not found. Sometimes the parser did not recognize
the whole structure of an MWE. Figure 2 shows an
example where the parser only found a subpart of the
MWN tour de passe-passe ‘magic trick’.
Other DP-TSG errors are due to inconsistencies in
the FTB annotation. For example, sous prétexte que
</bodyText>
<page confidence="0.982143">
732
</page>
<figure confidence="0.867040166666667">
PP
P NP
sous N Ssub
prétexte C
que
(b) Reference
</figure>
<figureCaption confidence="0.987082">
Figure 3: Example of an inconsistent FTB annotation for
sous prétexte que ‘on the pretext of’.
</figureCaption>
<bodyText confidence="0.9999194">
‘on the pretext of’ is tagged as both MWC and as a
regular PP structure (Figure 3). However, the parser
always assigns a MWC structure, which is a better
analysis than the gold annotation. We expect that
more consistent annotation would help the DP-TSG
more than the CFG-based parsers.
The DP-TSG is not immune to false positives: in
Le marché national, fait-on remarquer, est enfin en
régression ... ‘The national economy, people at last
note, is going down’ the parser tags marché national
as MWN. As noted, the boundary of what should and
should not count as an MWE can be fuzzy, and it is
therefore hard to assess whether or not this should be
an MWE. The FTB does not mark it as one.
There are multiple examples were the DP-TSG
found the MWE whereas Stanford (its base distribu-
tion) did not, such as in Figure 4. Note that the “N
P N” structure is quite frequent for MWNs, but the
TSG correctly identifies the MWADV in emplois à
domicile [jobs at home] ‘homeworking’.
</bodyText>
<sectionHeader confidence="0.999812" genericHeader="method">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999841818181818">
There is a voluminous literature on MWE identi-
fication. Here we review closely related syntax-
based methods.12 The linguistic and computa-
tional attractiveness of lexicalized grammars for
modeling idiosyncratic constructions in French was
identified by Abeillé (1988) and Abeillé and Sch-
abes (1989). They manually developed a small
Tree Adjoining Grammar (TAG) of 1,200 elemen-
tary trees and 4,000 lexical items that included
MWEs. The classic statistical approach to MWE
identification, Xtract (Smadja, 1993), used an in-
</bodyText>
<footnote confidence="0.553084333333333">
12See Seretan (2011) for a comprehensive survey of syntax-
based methods for MWE identification. For an overview of n-
gram methods like mwetoolkit, see Pecina (2010).
</footnote>
<figureCaption confidence="0.990938">
Figure 4: Correct analyses by DP-TSG. (dev set)
</figureCaption>
<bodyText confidence="0.999903888888889">
cremental parser in the third stage of its pipeline
to identify predicate-argument relationships. Lin
(1999) applied information-theoretic measures to
automatically-extracted dependency relations to find
MWEs. To our knowledge, Wehrli (2000) was the
first to use syntactically annotated corpora to im-
prove a parser for MWE identification. He pro-
posed to rank analyses of a symbolic parser based
on the presence of collocations, although details of
the ranking function were not provided.
The most similar work to ours is that of Nivre
and Nilsson (2004), who converted a Swedish cor-
pus into two versions: one in which MWEs were
left as tokens, and one in which they were merged.
On the first version, they showed that a deterministic
dependency parser could identify MWEs at 71.1%
F1, albeit without subcategory information. On
the second version—which simulated perfect MWE
identification—they showed that labeled attachment
improved by about 1%.
Recent statistical parsing work on French has in-
cluded Stochastic Tree Insertion Grammars (STIGs),
which are related to TAGs, but with a restricted ad-
junction operation.13 Seddah et al. (2009) and Sed-
dah (2010) showed that STIGs underperform CFG-
based parsers on the FTB. In their experiments,
MWEs were concatenated.
</bodyText>
<tableCaption confidence="0.4260965">
13TSGs differ from TAGs and STIGs in that they do not in-
clude an adjunction operator.
</tableCaption>
<figure confidence="0.999492135135135">
MWN NP
NP NP
(c) DP-TSG (d) Stanford
N
PP
de
promotion
campagne
(a) DP-TSG
P
N
N
NP
N
promotion
(b) Stanford
campagne
P
de
N MWADV
P
emplois
N
à domicile
PP
N
NP
P
emplois
N
à domicile
MWC
C
P
N
sous prétexte que
(a) Reference
</figure>
<page confidence="0.995654">
733
</page>
<sectionHeader confidence="0.996312" genericHeader="method">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999863333333333">
The main result of this paper is that an existing sta-
tistical parser can achieve a 36.4% F1 absolute im-
provement for MWE identification over a state-of-
the-art n-gram surface statistics package. Parsers
also provide syntactic subcategorization, and do not
require pre-filtering of the training data. We have
also demonstrated that TSGs can capture idiomatic
usage better than a PCFG. While the DP-TSG, which
is a relatively new parsing model, still lags state-of-
the-art parsers in terms of overall labeling accuracy,
we have shown that it is already very effective for
other tasks like MWE identification. We plan to im-
prove the DP-TSG by experimenting with alternate
parsing objectives (Cohn et al., 2010), lexical rep-
resentations, and parameterizations of the base dis-
tribution. A particularly promising base distribution
is the latent variable PCFG learned by the Berkeley
parser. However, initial experiments with this distri-
bution were negative, so we leave further develop-
ment to future work.
We chose French for these experiments due to the
pervasiveness of MWEs and the availability of an an-
notated corpus. However, MWE lists and syntactic
treebanks exist for many of the world’s major lan-
guages. We will investigate automatic conversion of
these treebanks (by flattening MWE bracketings) for
MWE identification.
</bodyText>
<sectionHeader confidence="0.635344" genericHeader="method">
A Appendix
</sectionHeader>
<bodyText confidence="0.994570142857143">
A.1 Notes on the Rising Factorial
The rising factorial—also known as the ascending
factorial or Pochhammer symbol—arises in the con-
text of samples from a Dirichlet process (see Prop.
3 of Antoniak (1974) for details). For a positive in-
teger n and a complex number x, the rising factorial
xn is defined14 by
</bodyText>
<equation confidence="0.98362425">
xn = x(x + 1)...(x + n − 1)
n
= H (x + j − 1) (7)
j=1
</equation>
<bodyText confidence="0.999887">
The rising factorial can be generalized to a com-
plex number α with the gamma function:
</bodyText>
<equation confidence="0.968559">
Γ(x + α)
xα = (8)
Γ(x)
</equation>
<bodyText confidence="0.9485592">
14We adopt the notation of Knuth (1992).
where x0 ≡ 1.
In our type-based sampler, we computed (7) di-
rectly in a dynamic program. We found that (8) was
prohibitively slow for sampling.
</bodyText>
<sectionHeader confidence="0.566151" genericHeader="method">
A.2 mwetoolkit Configuration
</sectionHeader>
<bodyText confidence="0.999004558823529">
We configured mwetoolkit15 with the four stan-
dard lexical features: the maximum likelihood esti-
mator, Dice’s coefficient, pointwise mutual informa-
tion (PMI), and Student’s t-score. We added the POS
sequence for each n-gram as a single feature. We re-
moved the web counts features to make the experi-
ments comparable. To compensate for the absence
of web counts, we computed the lexical features us-
ing the gold lemmas from the FTB instead of using
an automatic lemmatizer.
Since MWE n-grams only account for a small
fraction of the n-grams in the corpus, we filtered the
training and test sets by removing all n-grams that
occurred once. To further balance the proportion of
MWEs, we trained on all valid MWEs plus 10x ran-
domly selected non-MWE n-grams. This proportion
matches the fraction of MWE/non-MWE tokens in
the FTB. Since we generated a random training set,
we reported the average of three independent runs.
We created feature vectors for the training n-
grams and trained a binary Support Vector Machine
(SVM) classifier with Weka (Hall et al., 2009). Al-
though mwetoolkit defaults to a linear kernel,
we achieved higher accuracy on the development set
with an RBF kernel.
The FTB is sufficiently large for the corpus-based
methods implemented in mwetoolkit. Ramisch
et al. (2010)’s experiments were on Genia, which
contains 18k sentences and 490k tokens, similar to
the FTB. Their test set had 895 sentences, smaller
than ours. They reported 30.6% F1 for their task
against an Xtract baseline, which only obtained 7.3%
F1. These results are comparable in magnitude to our
FTB results.
</bodyText>
<sectionHeader confidence="0.771405" genericHeader="method">
Acknowledgments We thank Marie Candito, Chris Dyer,
</sectionHeader>
<reference confidence="0.806698">
Dan Flickinger, Percy Liang, Carlos Ramisch, Djamé
Seddah, and Val Spitkovsky for their helpful comments.
The first author is supported by a National Defense Sci-
ence and Engineering Graduate (NDSEG) fellowship.
15We re-implemented mwetoolkit in Java for compatibil-
ity with Weka and our pre-processing routines.
</reference>
<page confidence="0.998626">
734
</page>
<sectionHeader confidence="0.995852" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99978100952381">
A. Abeillé and Y. Schabes. 1989. Parsing idioms in lexicalized
TAGs. In EACL.
A. Abeillé, L. Clément, and A. Kinyon, 2003. Building a tree-
bank for French, chapter 10. Kluwer.
A. Abeillé. 1988. Parsing French with Tree Adjoining Grammar:
some linguistic accounts. In COLING.
C. E. Antoniak. 1974. Mixtures of Dirichlet processes with ap-
plications to Bayesian nonparametric problems. The Annals
of Statistics, 2(6):1152–1174.
A. Arun and F. Keller. 2005. Lexicalization in crosslinguistic
probabilistic parsing: The case of French. In ACL.
A. Arun. 2004. Statistical parsing of the French treebank. Tech-
nical report, University of Edinburgh.
M. Bansal and D. Klein. 2010. Simple, accurate parsing with
an all-fragments grammar. In ACL.
R. Bod. 1992. A computation model of language performance:
Data-Oriented Parsing. In COLING.
M. Candito and B. Crabbé. 2009. Improving generative statisti-
cal parsing with semi-supervised word clustering. In IWPT.
M. Candito, B. Crabbé, and P. Denis. 2010. Statistical French
dependency parsing: treebank conversion and first results. In
LREC.
M. Carpuat and M. Diab. 2010. Task-based evaluation of mul-
tiword expressions: a pilot study in statistical machine trans-
lation. In HLT-NAACL.
T. Cohn, S. Goldwater, and P. Blunsom. 2009. Inducing compact
but accurate tree-substitution grammars. In HLT-NAACL.
T. Cohn, P. Blunsom, and S. Goldwater. 2010. Inducing tree-
substitution grammars. JMLR, 11:3053–3096, Nov.
B. Crabbé and M. Candito. 2008. Expériences d’analyse syn-
taxique statistique du français. In TALN.
A. Dybro-Johansen. 2004. Extraction automatique de gram-
maires à partir d’un corpus français. Master’s thesis, Univer-
sité Paris 7.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture, et al. 2010.
cdec: A decoder, alignment, and learning framework for
finite-state and context-free translation models. In ACL Sys-
tem Demonstrations.
M. Gross. 1986. Lexicon-Grammar: the representation of com-
pound words. In COLING.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and
I. H. Witten. 2009. The WEKA data mining software: an
update. SIGKDD Explorations Newsletter, 11:10–18.
D. Hogan, C. Cafferkey, A. Cahill, and J. van Genabith. 2007.
Exploiting multi-word units in history-based probabilistic
generation. In EMNLP-CoNLL.
D. Klein and C. D. Manning. 2003. Accurate unlexicalized
parsing. In ACL.
D. E. Knuth. 1992. Two notes on notation. American Mathe-
matical Monthly, 99:403–422, May.
R. Levy and G. Andrew. 2006. Tregex and Tsurgeon: tools for
querying and manipulating tree data structures. In LREC.
P. Liang, M. I. Jordan, and D. Klein. 2010. Type-based MCMC.
In HLT-NAACL.
D. Lin. 1999. Automatic identification of non-compositional
phrases. In ACL.
J. Nivre and J. Nilsson. 2004. Multiword units in syntactic pars-
ing. In Methodologies and Evaluation of Multiword Units in
Real-World Applications (MEMURA).
T. J. O’Donnell, J. B. Tenenbaum, and N. D. Goodman. 2009.
Fragment grammars: Exploring computation and reuse in
language. Technical report, MIT Computer Science and Arti-
ficial Intelligence Laboratory Technical Report Series, MIT-
CSAIL-TR-2009-013.
P. Pecina. 2010. Lexical association measures and collocation
extraction. Language Resources and Evaluation, 44:137–
158.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. Learning
accurate, compact, and interpretable tree annotation. In ACL.
M. Post and D. Gildea. 2009. Bayesian learning of a tree sub-
stitution grammar. In ACL-IJCNLP, Short Papers.
C. Ramisch, A. Villavicencio, and C. Boitet. 2010. mwe-
toolkit: a framework for multiword expression identifi-
cation. In LREC.
P. Rayson, S. Piao, S. Sharoff, S. Evert, and B. Moirón. 2010.
Multiword expressions: hard going or plain sailing? Lan-
guage Resources and Evaluation, 44:1–5.
I. A. Sag, T. Baldwin, F. Bond, A. Copestake, and D. Flickinger.
2002. Multiword expressions: A pain in the neck for NLP.
In CICLing.
G. Sampson and A. Babarczy. 2003. A test of the leaf-ancestor
metric for parse accuracy. Natural Language Engineering,
9:365–380.
R. Scha, 1990. Taaltheorie en taaltechnologie: competence en
performance, pages 7–22. Landelijke Vereniging van Neer-
landici (LVVNjaarboek).
N. Schluter and J. Genabith. 2007. Preparing, restructuring,
and augmenting a French treebank: Lexicalised parsers or
coherent treebanks? In Pacling.
D. Seddah, M. Candito, and B. Crabbé. 2009. Cross parser
evaluation and tagset variation: a French treebank study. In
IWPT.
D. Seddah. 2010. Exploring the Spinal-STIG model for parsing
French. In LREC.
V. Seretan. 2011. Syntax-Based Collocation Extraction, vol-
ume 44 of Text, Speech, and Language Technology. Springer.
F. Smadja. 1993. Retrieving collocations from text: Xtract.
Computational Linguistics, 19:143–177.
K. Vijay-Shanker and D. J. Weir. 1993. The use of shared forests
in tree adjoining grammar parsing. In EACL.
E. Wehrli. 2000. Parsing and collocations. In Natural Lan-
guage Processing—NLP 2000, volume 1835 of Lecture Notes
in Computer Science, pages 272–282. Springer.
M. West. 1995. Hyperparameter estimation in Dirichlet process
mixture models. Technical report, Duke University.
</reference>
<page confidence="0.998567">
735
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.789257">
<title confidence="0.987089">Multiword Expression Identification with Tree Substitution Parsing de force French</title>
<author confidence="0.938656">Marie-Catherine de_John</author>
<author confidence="0.938656">D Christopher</author>
<affiliation confidence="0.953091">Science Department, Stanford</affiliation>
<address confidence="0.924669">Department, Stanford</address>
<email confidence="0.999774">spenceg@stanford.edu</email>
<email confidence="0.999774">mcdm@stanford.edu</email>
<email confidence="0.999774">horatio@stanford.edu</email>
<email confidence="0.999774">manning@stanford.edu</email>
<abstract confidence="0.997130888888889">Multiword expressions (MWE), a known nuisance for both linguistics and NLP, blur the lines between syntax and semantics. Previous work on MWE identification has relied primarily on surface statistics, which perform poorly for longer MWEs and cannot model discontinuous expressions. To address these problems, we show that even the simplest parsing models can effectively identify MWEs of arbitrary length, and that Tree Substitution Grammars achieve the best results. Our experiments show a 36.4% F1 absolute improvement for French an surface statistics baseline, currently the predominant method for MWE identification. Our models are useful for several NLP tasks in which MWE pre-grouping has improved accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Dan Flickinger</author>
<author>Percy Liang</author>
<author>Carlos Ramisch</author>
</authors>
<title>Djamé Seddah, and Val Spitkovsky for their helpful comments. The first author is supported by a National Defense Science and Engineering Graduate (NDSEG) fellowship. 15We re-implemented mwetoolkit in Java for compatibility with Weka and our pre-processing routines.</title>
<marker>Flickinger, Liang, Ramisch, </marker>
<rawString>Dan Flickinger, Percy Liang, Carlos Ramisch, Djamé Seddah, and Val Spitkovsky for their helpful comments. The first author is supported by a National Defense Science and Engineering Graduate (NDSEG) fellowship. 15We re-implemented mwetoolkit in Java for compatibility with Weka and our pre-processing routines.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Abeillé</author>
<author>Y Schabes</author>
</authors>
<title>Parsing idioms in lexicalized TAGs.</title>
<date>1989</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="32466" citStr="Abeillé and Schabes (1989)" startWordPosition="5525" endWordPosition="5529">e FTB does not mark it as one. There are multiple examples were the DP-TSG found the MWE whereas Stanford (its base distribution) did not, such as in Figure 4. Note that the “N P N” structure is quite frequent for MWNs, but the TSG correctly identifies the MWADV in emplois à domicile [jobs at home] ‘homeworking’. 7 Related Work There is a voluminous literature on MWE identification. Here we review closely related syntaxbased methods.12 The linguistic and computational attractiveness of lexicalized grammars for modeling idiosyncratic constructions in French was identified by Abeillé (1988) and Abeillé and Schabes (1989). They manually developed a small Tree Adjoining Grammar (TAG) of 1,200 elementary trees and 4,000 lexical items that included MWEs. The classic statistical approach to MWE identification, Xtract (Smadja, 1993), used an in12See Seretan (2011) for a comprehensive survey of syntaxbased methods for MWE identification. For an overview of ngram methods like mwetoolkit, see Pecina (2010). Figure 4: Correct analyses by DP-TSG. (dev set) cremental parser in the third stage of its pipeline to identify predicate-argument relationships. Lin (1999) applied information-theoretic measures to automatically-e</context>
</contexts>
<marker>Abeillé, Schabes, 1989</marker>
<rawString>A. Abeillé and Y. Schabes. 1989. Parsing idioms in lexicalized TAGs. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Abeillé</author>
<author>L Clément</author>
<author>A Kinyon</author>
</authors>
<title>Building a treebank for French, chapter 10.</title>
<date>2003</date>
<publisher>Kluwer.</publisher>
<contexts>
<context position="5321" citStr="Abeillé et al. (2003)" startWordPosition="858" endWordPosition="861">T foreign 24 18 0.75 0.12 MWI interj. 4 2 0.50 0.02 19,462 4,093 21.0% 100.0% Table 3: Frequency distribution of the 11 MWE subcategories in the FTB (training set). MWEs account for 7.08% of the bracketings and 13.0% of the tokens in the treebank. Only 21% of the MWEs occur once (“single”). We first introduce a new instantiation of the French Treebank that, unlike previous work, does not use gold MWE pre-grouping. Consequently, our experimental results also provide a better baseline for parsing raw French text. 2 French Treebank Setup The corpus used in our experiments is the French Treebank (Abeillé et al. (2003), version from June 2010, hereafter FTB). In French, there is a linguistic tradition of lexicography which compiles lists of MWEs occurring in the language. For example, Gross (1986) shows that dictionaries contain about 1,500 single-word adverbs but that French contains over 5,000 multiword adverbs. MWEs occur in every part-of-speech (POS) category (e.g., noun trousse de secours ‘first-aid kit’; verb faire mainbasse [do hand-low] ‘seize’; adverb comme dans du beurre [as in butter] ‘easily’; adjective ‘à part entière’ ‘wholly’). The FTB explicitly annotates MWEs (also called compounds in prior</context>
</contexts>
<marker>Abeillé, Clément, Kinyon, 2003</marker>
<rawString>A. Abeillé, L. Clément, and A. Kinyon, 2003. Building a treebank for French, chapter 10. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Abeillé</author>
</authors>
<title>Parsing French with Tree Adjoining Grammar: some linguistic accounts.</title>
<date>1988</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="32435" citStr="Abeillé (1988)" startWordPosition="5522" endWordPosition="5523">hould be an MWE. The FTB does not mark it as one. There are multiple examples were the DP-TSG found the MWE whereas Stanford (its base distribution) did not, such as in Figure 4. Note that the “N P N” structure is quite frequent for MWNs, but the TSG correctly identifies the MWADV in emplois à domicile [jobs at home] ‘homeworking’. 7 Related Work There is a voluminous literature on MWE identification. Here we review closely related syntaxbased methods.12 The linguistic and computational attractiveness of lexicalized grammars for modeling idiosyncratic constructions in French was identified by Abeillé (1988) and Abeillé and Schabes (1989). They manually developed a small Tree Adjoining Grammar (TAG) of 1,200 elementary trees and 4,000 lexical items that included MWEs. The classic statistical approach to MWE identification, Xtract (Smadja, 1993), used an in12See Seretan (2011) for a comprehensive survey of syntaxbased methods for MWE identification. For an overview of ngram methods like mwetoolkit, see Pecina (2010). Figure 4: Correct analyses by DP-TSG. (dev set) cremental parser in the third stage of its pipeline to identify predicate-argument relationships. Lin (1999) applied information-theore</context>
</contexts>
<marker>Abeillé, 1988</marker>
<rawString>A. Abeillé. 1988. Parsing French with Tree Adjoining Grammar: some linguistic accounts. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C E Antoniak</author>
</authors>
<title>Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems.</title>
<date>1974</date>
<journal>The Annals of Statistics,</journal>
<volume>2</volume>
<issue>6</issue>
<contexts>
<context position="36029" citStr="Antoniak (1974)" startWordPosition="6101" endWordPosition="6102">is distribution were negative, so we leave further development to future work. We chose French for these experiments due to the pervasiveness of MWEs and the availability of an annotated corpus. However, MWE lists and syntactic treebanks exist for many of the world’s major languages. We will investigate automatic conversion of these treebanks (by flattening MWE bracketings) for MWE identification. A Appendix A.1 Notes on the Rising Factorial The rising factorial—also known as the ascending factorial or Pochhammer symbol—arises in the context of samples from a Dirichlet process (see Prop. 3 of Antoniak (1974) for details). For a positive integer n and a complex number x, the rising factorial xn is defined14 by xn = x(x + 1)...(x + n − 1) n = H (x + j − 1) (7) j=1 The rising factorial can be generalized to a complex number α with the gamma function: Γ(x + α) xα = (8) Γ(x) 14We adopt the notation of Knuth (1992). where x0 ≡ 1. In our type-based sampler, we computed (7) directly in a dynamic program. We found that (8) was prohibitively slow for sampling. A.2 mwetoolkit Configuration We configured mwetoolkit15 with the four standard lexical features: the maximum likelihood estimator, Dice’s coefficien</context>
</contexts>
<marker>Antoniak, 1974</marker>
<rawString>C. E. Antoniak. 1974. Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems. The Annals of Statistics, 2(6):1152–1174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Arun</author>
<author>F Keller</author>
</authors>
<title>Lexicalization in crosslinguistic probabilistic parsing: The case of French.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2572" citStr="Arun and Keller, 2005" startWordPosition="402" endWordPosition="405"> (Table 1). French English in the near term in the short term à tr8s court terme in the very short term à moyen terme à long terme à tr8s long terme in the very long term Table 1: Semi-fixed MWEs in French and English. The French adverb à terme ‘in the end’ can be modified by a small set of adjectives, and in turn some of these adjectives can be modified by an adverb such as très ‘very’. Similar restrictions appear in English. Merging known MWEs into single tokens has been shown to improve accuracy for a variety of NLP tasks: dependency parsing (Nivre and Nilsson, 2004), constituency parsing (Arun and Keller, 2005), sentence generation (Hogan et al., 2007), and machine translation (Carpuat and Diab, 2010). Most experiments use gold MWE pre-grouping or languagespecific resources like WordNet. For unlabeled text, the best MWE identification methods, which are based on surface statistics (Pecina, 2010), suffer from sparsity induced by longer n-grams (Ramisch et al., 2010). A dilemma thus exists: MWE knowledge is useful, but MWEs are hard to identify. In this paper, we show the effectiveness of statistical parsers for MWE identification. Specifically, Tree Substitution Grammars (TSG) can achieve a 36.4% F1 </context>
<context position="7938" citStr="Arun and Keller, 2005" startWordPosition="1305" endWordPosition="1308">and 19 phrasal tags—in two ways. First, we added 16 finer-grained POS tags for punctuation.1 Second, we added the 11 MWE 1Punctuation tag clusters—as used in the WSJ—did not improve accuracy. Enriched tag sets like that of Crabbé and Candito (2008) could also be investigated and compared to our results since Evalb is insensitive to POS tags. 726 correction of annotation errors. Like ARUN-CONT, MFT contains concatenated MWEs. labels shown in Table 3, resulting in 24 total phrasal categories. Corrections Historically, the FTB suffered from annotation errors such as missing POS and phrasal tags (Arun and Keller, 2005). We found that this problem has been largely resolved in the current release. However, 1,949 tokens and 36 MWE spans still lacked tags. We restored the labels by first assigning each token its most frequent POS tag elsewhere in the treebank, and then assigning the most frequent MWE phrasal category for the resulting POS sequence.2 Split We used the 80/10/10 split described by Crabbé and Candito (2008). However, they used a previous release of the treebank with 12,531 trees. 3,391 trees have been added to the present version. We appended these extra trees to the training set, thus retaining th</context>
</contexts>
<marker>Arun, Keller, 2005</marker>
<rawString>A. Arun and F. Keller. 2005. Lexicalization in crosslinguistic probabilistic parsing: The case of French. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Arun</author>
</authors>
<title>Statistical parsing of the French treebank.</title>
<date>2004</date>
<tech>Technical report,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="16870" citStr="Arun (2004)" startWordPosition="2794" endWordPosition="2795">y, but increases both the number of correctly parsed trees (by 0.30%) and per category MWE accuracy. fragments larger than basic CFG rules. PTSG rules may also be lexicalized. This means that commonly observed collocations—some of which are MWEs— can be stored in the grammar. 4.1 Stanford Parser We configure the Stanford parser with settings that are effective for other languages: selective parent annotation, lexicon smoothing, and factored parsing. We use the head-finding rules of Dybro-Johansen (2004), which we find to yield an approximately 1.0% F1 development set improvement over those of Arun (2004). Finally, we include a simple unknown word model consisting entirely of surface features: - Nominal, adjectival, verbal, adverbial, and plural suffixes - Contains a digit or punctuation - Is capitalized (except the first word in a sentence) - Consists entirely of capital letters - If none of the above, add a one- or two-character suffix Combined with the grammar features, this unknown word model yields 97.3% tagging accuracy on the development set. 4.1.1 Grammar Development Table 4 lists the symbol refinements used in our grammar. Most of the features are POS splits as many phrasal tag splits</context>
</contexts>
<marker>Arun, 2004</marker>
<rawString>A. Arun. 2004. Statistical parsing of the French treebank. Technical report, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bansal</author>
<author>D Klein</author>
</authors>
<title>Simple, accurate parsing with an all-fragments grammar.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="23513" citStr="Bansal and Klein (2010)" startWordPosition="3990" endWordPosition="3993"> use the collapsed, block Gibbs sampler of Liang et al. (2010). We sample binary variables bs associated with each non-terminal node/site in the treebank. The key idea is to select a block of exchangeable sites S of the same type that do not conflict (Figure 1). Since the sites in S are exchangeable, we can set bS randomly so long as we know m, the number of sites with bs = 1. Because this algorithm is a not a contribution of this paper, we refer the reader to Liang et al. (2010). 6The Stanford parser is a product model, so the results in §5.1 include the contribution of a dependency parser. 7Bansal and Klein (2010) also experimented with symbol refinement in an all-fragments (parametric) TSG for English. NP+ PUNC-(1) 11 N+ Jacques NChirac PUNC+(2) 11 � p(z) = cEV (2) 730 After each Gibbs iteration, we sample each sc directly using binomial-Beta conjugacy. We re-sample the DP concentration parameters αc with the auxiliary variable procedure of West (1995). Decoding We compute the rule score of each tree fragment from a single grammar sample as follows: nc,e(z) + αcP0(e|c) θc,e = (5) nc,·(z) + αc To make the grammar more robust, we also include all CFG rules in Po with zero counts in n. Scores for these r</context>
</contexts>
<marker>Bansal, Klein, 2010</marker>
<rawString>M. Bansal and D. Klein. 2010. Simple, accurate parsing with an all-fragments grammar. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bod</author>
</authors>
<title>A computation model of language performance: Data-Oriented Parsing.</title>
<date>1992</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="19156" citStr="Bod, 1992" startWordPosition="3178" endWordPosition="3179">diomatic usage. For example, consider the two utterances: (5) a. He [MWV kicked the bucket] . b. He [VP kicked [NP the pail]]. The examples in (5) may be equally probable and receive the same analysis under a PCFG; words are generated independently. However, recall that in our representation, (5a) should receive a flat analysis as MWV, whereas (5b) should have a conventional analysis of the verb kicked and its two arguments. An alternate view of parsing is one in which new utterances are built from previously observed fragments. This is the original motivation for data oriented parsing (DOP) (Bod, 1992), in which “idiomaticity is the rule rather than the exception” (Scha, 1990). If we have seen the collocation kicked the bucket several times before, we should store that whole fragment for later use. We consider a variant of the non-parametric PTSG model of Cohn et al. (2009) in which tree fragments are drawn from a Dirichlet process (DP) prior.4 The DP-TSG can be viewed as a DOP model with Bayesian parameter estimation. A PTSG is a 5-tuple (V, Σ, R, Q, θ) where c E V are non-terminals; 4Similar models were developed independently by O’Donnell et al. (2009) and Post and Gildea (2009). 729 αc </context>
</contexts>
<marker>Bod, 1992</marker>
<rawString>R. Bod. 1992. A computation model of language performance: Data-Oriented Parsing. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Candito</author>
<author>B Crabbé</author>
</authors>
<title>Improving generative statistical parsing with semi-supervised word clustering.</title>
<date>2009</date>
<booktitle>In IWPT.</booktitle>
<contexts>
<context position="9964" citStr="Candito and Crabbé, 2009" startWordPosition="1628" endWordPosition="1631">s (Schluter and Genabith, 2007). MFT (Schluter and Genabith, 2007): Manual revision to 3,800 sentences. Major changes included coordination raising, an expanded POS tag set, and the 273 of the unlabeled word types did not appear elsewhere in the treebank. All but 11 of these were nouns. We manually assigned the correct tags, but we would not expect a negative effect by deterministically labeling all of them as nouns. 3We automate tree manipulation with Tregex/Tsurgeon (Levy and Andrew, 2006). Our pre-processing package is available at http://nlp.stanford.edu/software/lex-parser.shtml. FTB-UC (Candito and Crabbé, 2009): An instantiation of the functionally annotated section that makes a distinction between MWEs that are “syntactically regular” and those that are not. Syntactically regular MWEs were given internal structure, while all other MWEs were concatenated into single tokens. For example, nouns followed by adjectives, such as loi agraire ‘land law’ or Union monétaire et économique ‘monetary and economic Union’ were considered syntactically regular. They are MWEs because the choice of adjective is arbitrary (loi agraire and not *loi agricole, similarly to ‘coal black’ but not *‘crow black’ for example)</context>
</contexts>
<marker>Candito, Crabbé, 2009</marker>
<rawString>M. Candito and B. Crabbé. 2009. Improving generative statistical parsing with semi-supervised word clustering. In IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Candito</author>
<author>B Crabbé</author>
<author>P Denis</author>
</authors>
<title>Statistical French dependency parsing: treebank conversion and first results.</title>
<date>2010</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="11213" citStr="Candito et al. (2010)" startWordPosition="1830" endWordPosition="1833">ure is not intrinsic to MWEs. In such cases, FTB-UC gives the MWE a conventional analysis of an NP with internal structure. Such analysis is indeed sufficient to recover the meaning of these semantically compositional MWEs that are extremely productive. On the other hand, the FTB-UC loses information about MWEs with noncompositional semantics. Almost all work on the FTB has followed ARUNCONT and used gold MWE pre-grouping. As a result, most results for French parsing are analogous to early results for Chinese, which used gold word segmentation, and Arabic, which used gold clitic segmentation. Candito et al. (2010) were the first to acknowledge and address this issue, but they still used FTBUC (with some pre-grouped MWEs). Since the syntax and definition of MWEs is a contentious issue, we take a more agnostic view—which is consistent with that of the FTB annotators—and leave them tokenized. This permits a data-oriented approach to MWE identification that is more robust to changes to the status of specific MWE instances. To set a baseline prior to grammar development, we trained the Stanford parser (Klein and Manning, 2003) with no grammar features, achieving 74.2% labeled F1 on the development set (sent</context>
</contexts>
<marker>Candito, Crabbé, Denis, 2010</marker>
<rawString>M. Candito, B. Crabbé, and P. Denis. 2010. Statistical French dependency parsing: treebank conversion and first results. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Carpuat</author>
<author>M Diab</author>
</authors>
<title>Task-based evaluation of multiword expressions: a pilot study in statistical machine translation.</title>
<date>2010</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="2664" citStr="Carpuat and Diab, 2010" startWordPosition="416" endWordPosition="419">y short term à moyen terme à long terme à tr8s long terme in the very long term Table 1: Semi-fixed MWEs in French and English. The French adverb à terme ‘in the end’ can be modified by a small set of adjectives, and in turn some of these adjectives can be modified by an adverb such as très ‘very’. Similar restrictions appear in English. Merging known MWEs into single tokens has been shown to improve accuracy for a variety of NLP tasks: dependency parsing (Nivre and Nilsson, 2004), constituency parsing (Arun and Keller, 2005), sentence generation (Hogan et al., 2007), and machine translation (Carpuat and Diab, 2010). Most experiments use gold MWE pre-grouping or languagespecific resources like WordNet. For unlabeled text, the best MWE identification methods, which are based on surface statistics (Pecina, 2010), suffer from sparsity induced by longer n-grams (Ramisch et al., 2010). A dilemma thus exists: MWE knowledge is useful, but MWEs are hard to identify. In this paper, we show the effectiveness of statistical parsers for MWE identification. Specifically, Tree Substitution Grammars (TSG) can achieve a 36.4% F1 absolute improvement over a state-of-theart surface statistics method. We choose French, whi</context>
</contexts>
<marker>Carpuat, Diab, 2010</marker>
<rawString>M. Carpuat and M. Diab. 2010. Task-based evaluation of multiword expressions: a pilot study in statistical machine translation. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cohn</author>
<author>S Goldwater</author>
<author>P Blunsom</author>
</authors>
<title>Inducing compact but accurate tree-substitution grammars.</title>
<date>2009</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="19433" citStr="Cohn et al. (2009)" startWordPosition="3224" endWordPosition="3227">t in our representation, (5a) should receive a flat analysis as MWV, whereas (5b) should have a conventional analysis of the verb kicked and its two arguments. An alternate view of parsing is one in which new utterances are built from previously observed fragments. This is the original motivation for data oriented parsing (DOP) (Bod, 1992), in which “idiomaticity is the rule rather than the exception” (Scha, 1990). If we have seen the collocation kicked the bucket several times before, we should store that whole fragment for later use. We consider a variant of the non-parametric PTSG model of Cohn et al. (2009) in which tree fragments are drawn from a Dirichlet process (DP) prior.4 The DP-TSG can be viewed as a DOP model with Bayesian parameter estimation. A PTSG is a 5-tuple (V, Σ, R, Q, θ) where c E V are non-terminals; 4Similar models were developed independently by O’Donnell et al. (2009) and Post and Gildea (2009). 729 αc DP concentration parameter for each c E V P0(e|c) CFG base distribution x Set of non-terminal nodes in the treebank S Set of sampling sites (one for each x E x) S A block of sampling sites, where S ⊆ S b = {bs}s∈S Binary variables to be sampled (bs = 1 → frontier node) z Laten</context>
</contexts>
<marker>Cohn, Goldwater, Blunsom, 2009</marker>
<rawString>T. Cohn, S. Goldwater, and P. Blunsom. 2009. Inducing compact but accurate tree-substitution grammars. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cohn</author>
<author>P Blunsom</author>
<author>S Goldwater</author>
</authors>
<title>Inducing treesubstitution grammars.</title>
<date>2010</date>
<journal>JMLR,</journal>
<pages>11--3053</pages>
<contexts>
<context position="26766" citStr="Cohn et al., 2010" startWordPosition="4537" endWordPosition="4540">orming guess trees to the reference (Sampson and Babarczy, 2003). It was developed in response to the nonterminal/terminal ratio bias of Evalb, which penalizes flat treebanks like the FTB. The range of the score is between 0 and 1 (higher is better). We report micro-averaged (whole corpus) and macro-averaged (per sentence) scores. In terms of parsing accuracy, the Berkeley parser exceeds both Stanford and DP-TSG. This is consistent with previous experiments for French by Seddah et al. (2009), who show that the Berkeley parser outperforms other models. It also matches the ordering for English (Cohn et al., 2010; Liang et al., 2010). However, the standard baseline for TSG models is a simple parent-annotated PCFG (PA-PCFG). For English, Liang et al. (2010) showed that a similar DP-TSG improved over PA-PCFG by 4.2% F1. For French, our gain is a more substantial 8.2% F1. 5.2 MWE Identification Experiments Table 7 lists overall and per-category MWE identification results for the parsing models. Although DPTSG is less accurate as a general parsing model, it is more effective at identifying MWEs. The predominant approach to MWE identification is the combination of lexical association measures (surface stat</context>
<context position="35200" citStr="Cohn et al., 2010" startWordPosition="5971" endWordPosition="5974">% F1 absolute improvement for MWE identification over a state-ofthe-art n-gram surface statistics package. Parsers also provide syntactic subcategorization, and do not require pre-filtering of the training data. We have also demonstrated that TSGs can capture idiomatic usage better than a PCFG. While the DP-TSG, which is a relatively new parsing model, still lags state-ofthe-art parsers in terms of overall labeling accuracy, we have shown that it is already very effective for other tasks like MWE identification. We plan to improve the DP-TSG by experimenting with alternate parsing objectives (Cohn et al., 2010), lexical representations, and parameterizations of the base distribution. A particularly promising base distribution is the latent variable PCFG learned by the Berkeley parser. However, initial experiments with this distribution were negative, so we leave further development to future work. We chose French for these experiments due to the pervasiveness of MWEs and the availability of an annotated corpus. However, MWE lists and syntactic treebanks exist for many of the world’s major languages. We will investigate automatic conversion of these treebanks (by flattening MWE bracketings) for MWE i</context>
</contexts>
<marker>Cohn, Blunsom, Goldwater, 2010</marker>
<rawString>T. Cohn, P. Blunsom, and S. Goldwater. 2010. Inducing treesubstitution grammars. JMLR, 11:3053–3096, Nov.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Crabbé</author>
<author>M Candito</author>
</authors>
<title>Expériences d’analyse syntaxique statistique du français.</title>
<date>2008</date>
<booktitle>In TALN.</booktitle>
<contexts>
<context position="7564" citStr="Crabbé and Candito (2008)" startWordPosition="1246" endWordPosition="1250">ways unique to MWEs: they appear in abundance elsewhere in the corpus. However, some MWEs contain normally ungrammatical POS sequences (e.g., adverb à la va vite ‘in a hurry’: P D V ADV [at the goes quick]), and some words appear only as part of an MWE, such as insu in à l’insu de ‘to the ignorance of’. Labels We augmented the basic FTB label set— which contains 14 POS tags and 19 phrasal tags—in two ways. First, we added 16 finer-grained POS tags for punctuation.1 Second, we added the 11 MWE 1Punctuation tag clusters—as used in the WSJ—did not improve accuracy. Enriched tag sets like that of Crabbé and Candito (2008) could also be investigated and compared to our results since Evalb is insensitive to POS tags. 726 correction of annotation errors. Like ARUN-CONT, MFT contains concatenated MWEs. labels shown in Table 3, resulting in 24 total phrasal categories. Corrections Historically, the FTB suffered from annotation errors such as missing POS and phrasal tags (Arun and Keller, 2005). We found that this problem has been largely resolved in the current release. However, 1,949 tokens and 36 MWE spans still lacked tags. We restored the labels by first assigning each token its most frequent POS tag elsewhere </context>
</contexts>
<marker>Crabbé, Candito, 2008</marker>
<rawString>B. Crabbé and M. Candito. 2008. Expériences d’analyse syntaxique statistique du français. In TALN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dybro-Johansen</author>
</authors>
<title>Extraction automatique de grammaires à partir d’un corpus français.</title>
<date>2004</date>
<tech>Master’s thesis,</tech>
<institution>Université Paris</institution>
<contexts>
<context position="16767" citStr="Dybro-Johansen (2004)" startWordPosition="2777" endWordPosition="2778">F1 for each of the manual state splits (development set, sentences ≤ 40 words). markMWE decreases overall accuracy, but increases both the number of correctly parsed trees (by 0.30%) and per category MWE accuracy. fragments larger than basic CFG rules. PTSG rules may also be lexicalized. This means that commonly observed collocations—some of which are MWEs— can be stored in the grammar. 4.1 Stanford Parser We configure the Stanford parser with settings that are effective for other languages: selective parent annotation, lexicon smoothing, and factored parsing. We use the head-finding rules of Dybro-Johansen (2004), which we find to yield an approximately 1.0% F1 development set improvement over those of Arun (2004). Finally, we include a simple unknown word model consisting entirely of surface features: - Nominal, adjectival, verbal, adverbial, and plural suffixes - Contains a digit or punctuation - Is capitalized (except the first word in a sentence) - Consists entirely of capital letters - If none of the above, add a one- or two-character suffix Combined with the grammar features, this unknown word model yields 97.3% tagging accuracy on the development set. 4.1.1 Grammar Development Table 4 lists the</context>
</contexts>
<marker>Dybro-Johansen, 2004</marker>
<rawString>A. Dybro-Johansen. 2004. Extraction automatique de grammaires à partir d’un corpus français. Master’s thesis, Université Paris 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Dyer</author>
<author>A Lopez</author>
<author>J Ganitkevitch</author>
<author>J Weese</author>
<author>F Ture</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models.</title>
<date>2010</date>
<booktitle>In ACL System Demonstrations.</booktitle>
<contexts>
<context position="25075" citStr="Dyer et al., 2010" startWordPosition="4267" endWordPosition="4270"> terminals and non-terminals. We encode this fragment as an SCFG rule of the form [X -+ &apos;y , X -+ i, Yl, ... , Yn] (6) where Yl, ... , Yn is the sequence of non-terminal nodes in &apos;y.8 During decoding, the input is rewritten as a sequence of tree fragment (rule) indices {i, j, k,... }. Because the TSG substitution operator always applies to the leftmost frontier node, we can deterministically recover the monolingual parse with top-down re-writes of Q. The SCFG formulation has a practical benefit: we can take advantage of the heavily-optimized SCFG decoders for machine translation. We use cdec (Dyer et al., 2010) to recover the Viterbi derivation under a DP-TSG grammar sample. 5 Experiments 5.1 Standard Parsing Experiments We evaluate parsing accuracy of the Stanford and DP-TSG models (Table 6). For comparison, we also include the Berkeley parser (Petrov et al., 2006).9 For the DP-TSG, we initialized all bs with fair coin tosses and ran for 400 iterations, after which likelihood stopped improving. 8This formulation is due to Chris Dyer. 9Training settings: right binarization, no parent annotation, six split-merge cycles, and random initialization. Leaf Ancestor LP Evalb EX% Corpus Sent LR F1 PA-PCFG 0</context>
</contexts>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Ture, 2010</marker>
<rawString>C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture, et al. 2010. cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models. In ACL System Demonstrations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gross</author>
</authors>
<title>Lexicon-Grammar: the representation of compound words. In</title>
<date>1986</date>
<journal>SIGKDD Explorations Newsletter,</journal>
<pages>11--10</pages>
<contexts>
<context position="5503" citStr="Gross (1986)" startWordPosition="890" endWordPosition="891">the bracketings and 13.0% of the tokens in the treebank. Only 21% of the MWEs occur once (“single”). We first introduce a new instantiation of the French Treebank that, unlike previous work, does not use gold MWE pre-grouping. Consequently, our experimental results also provide a better baseline for parsing raw French text. 2 French Treebank Setup The corpus used in our experiments is the French Treebank (Abeillé et al. (2003), version from June 2010, hereafter FTB). In French, there is a linguistic tradition of lexicography which compiles lists of MWEs occurring in the language. For example, Gross (1986) shows that dictionaries contain about 1,500 single-word adverbs but that French contains over 5,000 multiword adverbs. MWEs occur in every part-of-speech (POS) category (e.g., noun trousse de secours ‘first-aid kit’; verb faire mainbasse [do hand-low] ‘seize’; adverb comme dans du beurre [as in butter] ‘easily’; adjective ‘à part entière’ ‘wholly’). The FTB explicitly annotates MWEs (also called compounds in prior work). We used the subset of the corpus with functional annotations, not for those annotations but because this subset is known to be more consistently annotated. POS tags for MWEs </context>
<context position="12283" citStr="Gross, 1986" startWordPosition="2009" endWordPosition="2010">nt, we trained the Stanford parser (Klein and Manning, 2003) with no grammar features, achieving 74.2% labeled F1 on the development set (sentences ≤ 40 words). This is lower than the most recent results obtained by Seddah (2010). However, the results are not comparable: the data split was different, they made use of morphological information, and more importantly they concatenated MWEs. The focus of 727 our work is on models and data representations that enable MWE identification. 3 MWEs in Lexicon-Grammar The MWE representation in the FTB is close to the one proposed in the Lexicon-Grammar (Gross, 1986). In the Lexicon-Grammar, MWEs are classified according to their global POS tags (noun, verb, adverb, adjective), and described in terms of the sequence of the POS tags of the words that constitute the MWE (e.g., “N de N” garde d’enfant [guard of child] ‘daycare’, pied de guerre [foot of war] ‘at the ready’). In other words, MWEs are represented by a flat structure. The Lexicon-Grammar distinguishes between units that are fixed and have to appear as is (en tout et pour tout [in all and for all] ‘in total’) and units that accept some syntactic variation such as admitting the insertion of an adv</context>
</contexts>
<marker>Gross, 1986</marker>
<rawString>M. Gross. 1986. Lexicon-Grammar: the representation of compound words. In COLING. M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H. Witten. 2009. The WEKA data mining software: an update. SIGKDD Explorations Newsletter, 11:10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hogan</author>
<author>C Cafferkey</author>
<author>A Cahill</author>
<author>J van Genabith</author>
</authors>
<title>Exploiting multi-word units in history-based probabilistic generation.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL.</booktitle>
<marker>Hogan, Cafferkey, Cahill, van Genabith, 2007</marker>
<rawString>D. Hogan, C. Cafferkey, A. Cahill, and J. van Genabith. 2007. Exploiting multi-word units in history-based probabilistic generation. In EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="11731" citStr="Klein and Manning, 2003" startWordPosition="1918" endWordPosition="1921"> which used gold word segmentation, and Arabic, which used gold clitic segmentation. Candito et al. (2010) were the first to acknowledge and address this issue, but they still used FTBUC (with some pre-grouped MWEs). Since the syntax and definition of MWEs is a contentious issue, we take a more agnostic view—which is consistent with that of the FTB annotators—and leave them tokenized. This permits a data-oriented approach to MWE identification that is more robust to changes to the status of specific MWE instances. To set a baseline prior to grammar development, we trained the Stanford parser (Klein and Manning, 2003) with no grammar features, achieving 74.2% labeled F1 on the development set (sentences ≤ 40 words). This is lower than the most recent results obtained by Seddah (2010). However, the results are not comparable: the data split was different, they made use of morphological information, and more importantly they concatenated MWEs. The focus of 727 our work is on models and data representations that enable MWE identification. 3 MWEs in Lexicon-Grammar The MWE representation in the FTB is close to the one proposed in the Lexicon-Grammar (Gross, 1986). In the Lexicon-Grammar, MWEs are classified ac</context>
<context position="17850" citStr="Klein and Manning (2003)" startWordPosition="2953" endWordPosition="2956">e grammar features, this unknown word model yields 97.3% tagging accuracy on the development set. 4.1.1 Grammar Development Table 4 lists the symbol refinements used in our grammar. Most of the features are POS splits as many phrasal tag splits did not lead to any improvement. Parent annotation of POS tags (tagPA) captures information about the external context. markInf and markPart accomplish a finite/nonfinite distinction: they respectively specify whether the verb is an infinitive or a participle based on the type of the grandparent node. markVN captures the notion of verbal distance as in Klein and Manning (2003). We opted to keep the COORD phrasal tag, and to capture parallelism in coordination, we mark COORD with the type of its child (NP, AP, VPinf, etc.). markDe identifies the preposition de and its variants (du, des, d’) which is very frequent and appears in several different contexts. markP identifies prepositions which introduce PPs modifying a noun. Marking other kinds of prepositional modifiers (e.g., verb) did not help. markMWE adds an annotation to several MWE categories for frequently occuring POS sequences. For example, we mark MWNs that occur more than 600 times (e.g., “N P N” and “N N”)</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. D. Manning. 2003. Accurate unlexicalized parsing. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Knuth</author>
</authors>
<title>Two notes on notation.</title>
<date>1992</date>
<journal>American Mathematical Monthly,</journal>
<pages>99--403</pages>
<contexts>
<context position="36336" citStr="Knuth (1992)" startWordPosition="6170" endWordPosition="6171">atic conversion of these treebanks (by flattening MWE bracketings) for MWE identification. A Appendix A.1 Notes on the Rising Factorial The rising factorial—also known as the ascending factorial or Pochhammer symbol—arises in the context of samples from a Dirichlet process (see Prop. 3 of Antoniak (1974) for details). For a positive integer n and a complex number x, the rising factorial xn is defined14 by xn = x(x + 1)...(x + n − 1) n = H (x + j − 1) (7) j=1 The rising factorial can be generalized to a complex number α with the gamma function: Γ(x + α) xα = (8) Γ(x) 14We adopt the notation of Knuth (1992). where x0 ≡ 1. In our type-based sampler, we computed (7) directly in a dynamic program. We found that (8) was prohibitively slow for sampling. A.2 mwetoolkit Configuration We configured mwetoolkit15 with the four standard lexical features: the maximum likelihood estimator, Dice’s coefficient, pointwise mutual information (PMI), and Student’s t-score. We added the POS sequence for each n-gram as a single feature. We removed the web counts features to make the experiments comparable. To compensate for the absence of web counts, we computed the lexical features using the gold lemmas from the FT</context>
</contexts>
<marker>Knuth, 1992</marker>
<rawString>D. E. Knuth. 1992. Two notes on notation. American Mathematical Monthly, 99:403–422, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Levy</author>
<author>G Andrew</author>
</authors>
<title>Tregex and Tsurgeon: tools for querying and manipulating tree data structures. In</title>
<date>2006</date>
<note>Type-based MCMC.</note>
<contexts>
<context position="9835" citStr="Levy and Andrew, 2006" startWordPosition="1615" endWordPosition="1618">they mistakenly removed half of the corpus, believing that the multi-terminal (per POS tag) annotations of MWEs were XML errors (Schluter and Genabith, 2007). MFT (Schluter and Genabith, 2007): Manual revision to 3,800 sentences. Major changes included coordination raising, an expanded POS tag set, and the 273 of the unlabeled word types did not appear elsewhere in the treebank. All but 11 of these were nouns. We manually assigned the correct tags, but we would not expect a negative effect by deterministically labeling all of them as nouns. 3We automate tree manipulation with Tregex/Tsurgeon (Levy and Andrew, 2006). Our pre-processing package is available at http://nlp.stanford.edu/software/lex-parser.shtml. FTB-UC (Candito and Crabbé, 2009): An instantiation of the functionally annotated section that makes a distinction between MWEs that are “syntactically regular” and those that are not. Syntactically regular MWEs were given internal structure, while all other MWEs were concatenated into single tokens. For example, nouns followed by adjectives, such as loi agraire ‘land law’ or Union monétaire et économique ‘monetary and economic Union’ were considered syntactically regular. They are MWEs because the </context>
</contexts>
<marker>Levy, Andrew, 2006</marker>
<rawString>R. Levy and G. Andrew. 2006. Tregex and Tsurgeon: tools for querying and manipulating tree data structures. In LREC. P. Liang, M. I. Jordan, and D. Klein. 2010. Type-based MCMC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic identification of non-compositional phrases.</title>
<date>1999</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="33008" citStr="Lin (1999)" startWordPosition="5611" endWordPosition="5612">ench was identified by Abeillé (1988) and Abeillé and Schabes (1989). They manually developed a small Tree Adjoining Grammar (TAG) of 1,200 elementary trees and 4,000 lexical items that included MWEs. The classic statistical approach to MWE identification, Xtract (Smadja, 1993), used an in12See Seretan (2011) for a comprehensive survey of syntaxbased methods for MWE identification. For an overview of ngram methods like mwetoolkit, see Pecina (2010). Figure 4: Correct analyses by DP-TSG. (dev set) cremental parser in the third stage of its pipeline to identify predicate-argument relationships. Lin (1999) applied information-theoretic measures to automatically-extracted dependency relations to find MWEs. To our knowledge, Wehrli (2000) was the first to use syntactically annotated corpora to improve a parser for MWE identification. He proposed to rank analyses of a symbolic parser based on the presence of collocations, although details of the ranking function were not provided. The most similar work to ours is that of Nivre and Nilsson (2004), who converted a Swedish corpus into two versions: one in which MWEs were left as tokens, and one in which they were merged. On the first version, they sh</context>
</contexts>
<marker>Lin, 1999</marker>
<rawString>In HLT-NAACL. D. Lin. 1999. Automatic identification of non-compositional phrases. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Nilsson</author>
</authors>
<title>Multiword units in syntactic parsing.</title>
<date>2004</date>
<booktitle>In Methodologies and Evaluation of Multiword Units in Real-World Applications (MEMURA).</booktitle>
<contexts>
<context position="2526" citStr="Nivre and Nilsson, 2004" startWordPosition="396" endWordPosition="399"> so it is not possible to enumerate all variants (Table 1). French English in the near term in the short term à tr8s court terme in the very short term à moyen terme à long terme à tr8s long terme in the very long term Table 1: Semi-fixed MWEs in French and English. The French adverb à terme ‘in the end’ can be modified by a small set of adjectives, and in turn some of these adjectives can be modified by an adverb such as très ‘very’. Similar restrictions appear in English. Merging known MWEs into single tokens has been shown to improve accuracy for a variety of NLP tasks: dependency parsing (Nivre and Nilsson, 2004), constituency parsing (Arun and Keller, 2005), sentence generation (Hogan et al., 2007), and machine translation (Carpuat and Diab, 2010). Most experiments use gold MWE pre-grouping or languagespecific resources like WordNet. For unlabeled text, the best MWE identification methods, which are based on surface statistics (Pecina, 2010), suffer from sparsity induced by longer n-grams (Ramisch et al., 2010). A dilemma thus exists: MWE knowledge is useful, but MWEs are hard to identify. In this paper, we show the effectiveness of statistical parsers for MWE identification. Specifically, Tree Subst</context>
<context position="33453" citStr="Nivre and Nilsson (2004)" startWordPosition="5678" endWordPosition="5681">it, see Pecina (2010). Figure 4: Correct analyses by DP-TSG. (dev set) cremental parser in the third stage of its pipeline to identify predicate-argument relationships. Lin (1999) applied information-theoretic measures to automatically-extracted dependency relations to find MWEs. To our knowledge, Wehrli (2000) was the first to use syntactically annotated corpora to improve a parser for MWE identification. He proposed to rank analyses of a symbolic parser based on the presence of collocations, although details of the ranking function were not provided. The most similar work to ours is that of Nivre and Nilsson (2004), who converted a Swedish corpus into two versions: one in which MWEs were left as tokens, and one in which they were merged. On the first version, they showed that a deterministic dependency parser could identify MWEs at 71.1% F1, albeit without subcategory information. On the second version—which simulated perfect MWE identification—they showed that labeled attachment improved by about 1%. Recent statistical parsing work on French has included Stochastic Tree Insertion Grammars (STIGs), which are related to TAGs, but with a restricted adjunction operation.13 Seddah et al. (2009) and Seddah (</context>
</contexts>
<marker>Nivre, Nilsson, 2004</marker>
<rawString>J. Nivre and J. Nilsson. 2004. Multiword units in syntactic parsing. In Methodologies and Evaluation of Multiword Units in Real-World Applications (MEMURA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T J O’Donnell</author>
<author>J B Tenenbaum</author>
<author>N D Goodman</author>
</authors>
<title>Fragment grammars: Exploring computation and reuse in language.</title>
<date>2009</date>
<tech>Technical report,</tech>
<pages>2009--013</pages>
<institution>MIT Computer Science and Artificial Intelligence Laboratory</institution>
<marker>O’Donnell, Tenenbaum, Goodman, 2009</marker>
<rawString>T. J. O’Donnell, J. B. Tenenbaum, and N. D. Goodman. 2009. Fragment grammars: Exploring computation and reuse in language. Technical report, MIT Computer Science and Artificial Intelligence Laboratory Technical Report Series, MITCSAIL-TR-2009-013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pecina</author>
</authors>
<title>Lexical association measures and collocation extraction. Language Resources and Evaluation,</title>
<date>2010</date>
<pages>44--137</pages>
<contexts>
<context position="2862" citStr="Pecina, 2010" startWordPosition="447" endWordPosition="448">ves, and in turn some of these adjectives can be modified by an adverb such as très ‘very’. Similar restrictions appear in English. Merging known MWEs into single tokens has been shown to improve accuracy for a variety of NLP tasks: dependency parsing (Nivre and Nilsson, 2004), constituency parsing (Arun and Keller, 2005), sentence generation (Hogan et al., 2007), and machine translation (Carpuat and Diab, 2010). Most experiments use gold MWE pre-grouping or languagespecific resources like WordNet. For unlabeled text, the best MWE identification methods, which are based on surface statistics (Pecina, 2010), suffer from sparsity induced by longer n-grams (Ramisch et al., 2010). A dilemma thus exists: MWE knowledge is useful, but MWEs are hard to identify. In this paper, we show the effectiveness of statistical parsers for MWE identification. Specifically, Tree Substitution Grammars (TSG) can achieve a 36.4% F1 absolute improvement over a state-of-theart surface statistics method. We choose French, which has pervasive MWEs, for our experiments. Parsing models naturally accommodate discontinuous MWEs like phrasal verbs, and provide syntactic subcategorization. By contrast, surface statistics metho</context>
<context position="27413" citStr="Pecina, 2010" startWordPosition="4641" endWordPosition="4642">e standard baseline for TSG models is a simple parent-annotated PCFG (PA-PCFG). For English, Liang et al. (2010) showed that a similar DP-TSG improved over PA-PCFG by 4.2% F1. For French, our gain is a more substantial 8.2% F1. 5.2 MWE Identification Experiments Table 7 lists overall and per-category MWE identification results for the parsing models. Although DPTSG is less accurate as a general parsing model, it is more effective at identifying MWEs. The predominant approach to MWE identification is the combination of lexical association measures (surface statistics) with a binary classifier (Pecina, 2010). A state-of-the-art, language independent package that implements this approach for higher order n-grams is mwetoolkit (Ramisch et al., 2010).11 In Table 8 we compare DP-TSG to both 10Available at http://nlp.cs.nyu.edu/evalb/ (v.20080701). 11Available at http://multiword.sourceforge.net/. See §A.2 for 731 #gold Stanford DP-TSG Berkeley MWET 3 0.0 0.0 0.0 MWV 26 64.0 57.7 50.7 MWA 8 26.1 32.2 29.8 MWN 456 64.1 67.6 67.1 MWD 15 70.3 65.5 70.1 MWPRO 17 73.7 78.0 76.2 MWADV 220 74.6 72.7 70.4 MWP 162 81.3 80.5 77.7 MWC 47 83.5 83.5 80.8 954 70.1 71.1 69.6 Table 7: MWE identification per category </context>
<context position="32850" citStr="Pecina (2010)" startWordPosition="5588" endWordPosition="5589">iew closely related syntaxbased methods.12 The linguistic and computational attractiveness of lexicalized grammars for modeling idiosyncratic constructions in French was identified by Abeillé (1988) and Abeillé and Schabes (1989). They manually developed a small Tree Adjoining Grammar (TAG) of 1,200 elementary trees and 4,000 lexical items that included MWEs. The classic statistical approach to MWE identification, Xtract (Smadja, 1993), used an in12See Seretan (2011) for a comprehensive survey of syntaxbased methods for MWE identification. For an overview of ngram methods like mwetoolkit, see Pecina (2010). Figure 4: Correct analyses by DP-TSG. (dev set) cremental parser in the third stage of its pipeline to identify predicate-argument relationships. Lin (1999) applied information-theoretic measures to automatically-extracted dependency relations to find MWEs. To our knowledge, Wehrli (2000) was the first to use syntactically annotated corpora to improve a parser for MWE identification. He proposed to rank analyses of a symbolic parser based on the presence of collocations, although details of the ranking function were not provided. The most similar work to ours is that of Nivre and Nilsson (20</context>
</contexts>
<marker>Pecina, 2010</marker>
<rawString>P. Pecina. 2010. Lexical association measures and collocation extraction. Language Resources and Evaluation, 44:137– 158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>L Barrett</author>
<author>R Thibaux</author>
<author>D Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation. In</title>
<date>2006</date>
<booktitle>In ACL-IJCNLP, Short Papers.</booktitle>
<contexts>
<context position="22252" citStr="Petrov et al., 2006" startWordPosition="3754" endWordPosition="3757">onflicting sites of the same type. Define the type of a site t(z, s) def = (Ans:0, Ans:1). Sites (1) and (2) above have the same type since t(z, s1) = t(z, s2). However, the two sites conflict since the probabilities of setting bs1 and bs2 both depend on counts for the tree fragment rooted at NP. Consequently, sites (1) and (2) are not exchangeable: the probabilities of their assignments depend on the order in which they are sampled. ford parser.6,7 After applying the manual state splits, we perform simple right binarization, collapse unary rules, and replace rare words with their signatures (Petrov et al., 2006). For each non-terminal type c, we learn a stop probability sc - Beta(1,1). Under P0, the probability of generating a rule A+ -+ B− C+ composed of nonterminals is P0(A+ -+ B− C+) = pMLE(A -+ B C)sB(1−sC) (3) For lexical insertion rules, we add a penalty proportional to the frequency of the lexical item: P0(c -+ t) = pMLE(c -+ t)p(t) (4) where p(t) is equal to the MLE unigram probability of t in the treebank. Lexicalizing a rule makes it very specific, so we generally want to avoid lexicalization with rare words. Empirically, we found that this penalty reduces overfitting. Type-based Inference </context>
<context position="25335" citStr="Petrov et al., 2006" startWordPosition="4307" endWordPosition="4310">le) indices {i, j, k,... }. Because the TSG substitution operator always applies to the leftmost frontier node, we can deterministically recover the monolingual parse with top-down re-writes of Q. The SCFG formulation has a practical benefit: we can take advantage of the heavily-optimized SCFG decoders for machine translation. We use cdec (Dyer et al., 2010) to recover the Viterbi derivation under a DP-TSG grammar sample. 5 Experiments 5.1 Standard Parsing Experiments We evaluate parsing accuracy of the Stanford and DP-TSG models (Table 6). For comparison, we also include the Berkeley parser (Petrov et al., 2006).9 For the DP-TSG, we initialized all bs with fair coin tosses and ran for 400 iterations, after which likelihood stopped improving. 8This formulation is due to Chris Dyer. 9Training settings: right binarization, no parent annotation, six split-merge cycles, and random initialization. Leaf Ancestor LP Evalb EX% Corpus Sent LR F1 PA-PCFG 0.793 0.812 68.1 67.0 67.6 10.5 DP-TSG 0.823 0.842 75.6 76.0 75.8 15.1 Stanford 0.843 0.861 77.8 79.0 78.4 17.5 Berkeley 0.880 0.891 82.4 82.0 82.2 21.4 Table 6: Standard parsing experiments (test set, sentences &lt; 40 words). All parsers exceed 96% tagging accur</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In ACL. M. Post and D. Gildea. 2009. Bayesian learning of a tree substitution grammar. In ACL-IJCNLP, Short Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Ramisch</author>
<author>A Villavicencio</author>
<author>C Boitet</author>
</authors>
<title>mwetoolkit: a framework for multiword expression identification.</title>
<date>2010</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="2933" citStr="Ramisch et al., 2010" startWordPosition="456" endWordPosition="459"> adverb such as très ‘very’. Similar restrictions appear in English. Merging known MWEs into single tokens has been shown to improve accuracy for a variety of NLP tasks: dependency parsing (Nivre and Nilsson, 2004), constituency parsing (Arun and Keller, 2005), sentence generation (Hogan et al., 2007), and machine translation (Carpuat and Diab, 2010). Most experiments use gold MWE pre-grouping or languagespecific resources like WordNet. For unlabeled text, the best MWE identification methods, which are based on surface statistics (Pecina, 2010), suffer from sparsity induced by longer n-grams (Ramisch et al., 2010). A dilemma thus exists: MWE knowledge is useful, but MWEs are hard to identify. In this paper, we show the effectiveness of statistical parsers for MWE identification. Specifically, Tree Substitution Grammars (TSG) can achieve a 36.4% F1 absolute improvement over a state-of-theart surface statistics method. We choose French, which has pervasive MWEs, for our experiments. Parsing models naturally accommodate discontinuous MWEs like phrasal verbs, and provide syntactic subcategorization. By contrast, surface statistics methods are usually limited to binary judgements for contiguous n-grams or d</context>
<context position="27555" citStr="Ramisch et al., 2010" startWordPosition="4658" endWordPosition="4661"> DP-TSG improved over PA-PCFG by 4.2% F1. For French, our gain is a more substantial 8.2% F1. 5.2 MWE Identification Experiments Table 7 lists overall and per-category MWE identification results for the parsing models. Although DPTSG is less accurate as a general parsing model, it is more effective at identifying MWEs. The predominant approach to MWE identification is the combination of lexical association measures (surface statistics) with a binary classifier (Pecina, 2010). A state-of-the-art, language independent package that implements this approach for higher order n-grams is mwetoolkit (Ramisch et al., 2010).11 In Table 8 we compare DP-TSG to both 10Available at http://nlp.cs.nyu.edu/evalb/ (v.20080701). 11Available at http://multiword.sourceforge.net/. See §A.2 for 731 #gold Stanford DP-TSG Berkeley MWET 3 0.0 0.0 0.0 MWV 26 64.0 57.7 50.7 MWA 8 26.1 32.2 29.8 MWN 456 64.1 67.6 67.1 MWD 15 70.3 65.5 70.1 MWPRO 17 73.7 78.0 76.2 MWADV 220 74.6 72.7 70.4 MWP 162 81.3 80.5 77.7 MWC 47 83.5 83.5 80.8 954 70.1 71.1 69.6 Table 7: MWE identification per category and overall results (test set, sentences &lt; 40 words). MWI and MWCL do not occur in the test set. tour de passe - passe tour P NP de MWN (a) Re</context>
</contexts>
<marker>Ramisch, Villavicencio, Boitet, 2010</marker>
<rawString>C. Ramisch, A. Villavicencio, and C. Boitet. 2010. mwetoolkit: a framework for multiword expression identification. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Rayson</author>
<author>S Piao</author>
<author>S Sharoff</author>
<author>S Evert</author>
<author>B Moirón</author>
</authors>
<title>Multiword expressions: hard going or plain sailing? Language Resources and Evaluation,</title>
<date>2010</date>
<pages>44--1</pages>
<contexts>
<context position="1518" citStr="Rayson et al., 2010" startWordPosition="215" endWordPosition="218">tistics baseline, currently the predominant method for MWE identification. Our models are useful for several NLP tasks in which MWE pre-grouping has improved accuracy. 1 Introduction Multiword expressions (MWE) have long been a challenge for linguistic theory and NLP. There is no universally accepted definition of the term, but MWEs can be characterized as “idiosyncratic interpretations that cross word boundaries (or spaces)” (Sag et al., 2002) such as traffic light, or as “frequently occurring phrasal units which are subject to a certain level of semantic opaqueness, or noncompositionality” (Rayson et al., 2010). MWEs are often opaque fixed expressions, although the degree to which they are fixed can vary. Some MWEs do not allow morphosyntactic variation or internal modification (e.g., in short, but *in shorter or *in very short). Other MWEs are “semifixed,” meaning that they can be inflected or undergo internal modification. The type of modification is often limited, but not predictable, so it is not possible to enumerate all variants (Table 1). French English in the near term in the short term à tr8s court terme in the very short term à moyen terme à long terme à tr8s long terme in the very long te</context>
</contexts>
<marker>Rayson, Piao, Sharoff, Evert, Moirón, 2010</marker>
<rawString>P. Rayson, S. Piao, S. Sharoff, S. Evert, and B. Moirón. 2010. Multiword expressions: hard going or plain sailing? Language Resources and Evaluation, 44:1–5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I A Sag</author>
<author>T Baldwin</author>
<author>F Bond</author>
<author>A Copestake</author>
<author>D Flickinger</author>
</authors>
<title>Multiword expressions: A pain in the neck for NLP. In CICLing.</title>
<date>2002</date>
<contexts>
<context position="1346" citStr="Sag et al., 2002" startWordPosition="187" endWordPosition="190">rbitrary length, and that Tree Substitution Grammars achieve the best results. Our experiments show a 36.4% F1 absolute improvement for French over an n-gram surface statistics baseline, currently the predominant method for MWE identification. Our models are useful for several NLP tasks in which MWE pre-grouping has improved accuracy. 1 Introduction Multiword expressions (MWE) have long been a challenge for linguistic theory and NLP. There is no universally accepted definition of the term, but MWEs can be characterized as “idiosyncratic interpretations that cross word boundaries (or spaces)” (Sag et al., 2002) such as traffic light, or as “frequently occurring phrasal units which are subject to a certain level of semantic opaqueness, or noncompositionality” (Rayson et al., 2010). MWEs are often opaque fixed expressions, although the degree to which they are fixed can vary. Some MWEs do not allow morphosyntactic variation or internal modification (e.g., in short, but *in shorter or *in very short). Other MWEs are “semifixed,” meaning that they can be inflected or undergo internal modification. The type of modification is often limited, but not predictable, so it is not possible to enumerate all vari</context>
</contexts>
<marker>Sag, Baldwin, Bond, Copestake, Flickinger, 2002</marker>
<rawString>I. A. Sag, T. Baldwin, F. Bond, A. Copestake, and D. Flickinger. 2002. Multiword expressions: A pain in the neck for NLP. In CICLing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Sampson</author>
<author>A Babarczy</author>
</authors>
<title>A test of the leaf-ancestor metric for parse accuracy.</title>
<date>2003</date>
<journal>Natural Language Engineering,</journal>
<pages>9--365</pages>
<contexts>
<context position="26213" citStr="Sampson and Babarczy, 2003" startWordPosition="4443" endWordPosition="4447">cycles, and random initialization. Leaf Ancestor LP Evalb EX% Corpus Sent LR F1 PA-PCFG 0.793 0.812 68.1 67.0 67.6 10.5 DP-TSG 0.823 0.842 75.6 76.0 75.8 15.1 Stanford 0.843 0.861 77.8 79.0 78.4 17.5 Berkeley 0.880 0.891 82.4 82.0 82.2 21.4 Table 6: Standard parsing experiments (test set, sentences &lt; 40 words). All parsers exceed 96% tagging accuracy. Berkeley and DP-TSG results are the average of three independentruns. We report two different parsing metrics. Evalb is the standard labeled precision/recall metric.10 Leaf Ancestor measures the cost of transforming guess trees to the reference (Sampson and Babarczy, 2003). It was developed in response to the nonterminal/terminal ratio bias of Evalb, which penalizes flat treebanks like the FTB. The range of the score is between 0 and 1 (higher is better). We report micro-averaged (whole corpus) and macro-averaged (per sentence) scores. In terms of parsing accuracy, the Berkeley parser exceeds both Stanford and DP-TSG. This is consistent with previous experiments for French by Seddah et al. (2009), who show that the Berkeley parser outperforms other models. It also matches the ordering for English (Cohn et al., 2010; Liang et al., 2010). However, the standard ba</context>
</contexts>
<marker>Sampson, Babarczy, 2003</marker>
<rawString>G. Sampson and A. Babarczy. 2003. A test of the leaf-ancestor metric for parse accuracy. Natural Language Engineering, 9:365–380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Scha</author>
</authors>
<title>Taaltheorie en taaltechnologie: competence en performance,</title>
<date>1990</date>
<pages>7--22</pages>
<note>Landelijke Vereniging van Neerlandici (LVVNjaarboek).</note>
<contexts>
<context position="19232" citStr="Scha, 1990" startWordPosition="3191" endWordPosition="3192">cked the bucket] . b. He [VP kicked [NP the pail]]. The examples in (5) may be equally probable and receive the same analysis under a PCFG; words are generated independently. However, recall that in our representation, (5a) should receive a flat analysis as MWV, whereas (5b) should have a conventional analysis of the verb kicked and its two arguments. An alternate view of parsing is one in which new utterances are built from previously observed fragments. This is the original motivation for data oriented parsing (DOP) (Bod, 1992), in which “idiomaticity is the rule rather than the exception” (Scha, 1990). If we have seen the collocation kicked the bucket several times before, we should store that whole fragment for later use. We consider a variant of the non-parametric PTSG model of Cohn et al. (2009) in which tree fragments are drawn from a Dirichlet process (DP) prior.4 The DP-TSG can be viewed as a DOP model with Bayesian parameter estimation. A PTSG is a 5-tuple (V, Σ, R, Q, θ) where c E V are non-terminals; 4Similar models were developed independently by O’Donnell et al. (2009) and Post and Gildea (2009). 729 αc DP concentration parameter for each c E V P0(e|c) CFG base distribution x Se</context>
</contexts>
<marker>Scha, 1990</marker>
<rawString>R. Scha, 1990. Taaltheorie en taaltechnologie: competence en performance, pages 7–22. Landelijke Vereniging van Neerlandici (LVVNjaarboek).</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Schluter</author>
<author>J Genabith</author>
</authors>
<title>Preparing, restructuring, and augmenting a French treebank: Lexicalised parsers or coherent treebanks? In Pacling.</title>
<date>2007</date>
<contexts>
<context position="9370" citStr="Schluter and Genabith, 2007" startWordPosition="1539" endWordPosition="1542">revious work: ARUN-CONT and ARUN-EXP (Arun and Keller, 2005): Two instantiations of the full 20,000 sentence treebank that differed principally in their treatment of MWEs: (1) CONT, in which the tokens of each MWE were concatenated into a single token (en moyenne → en_moyenne); (2) Exp, in which they were marked with a flat structure. For both representations, they also gave results in which coordinated phrase structures were flattened. In the published experiments, they mistakenly removed half of the corpus, believing that the multi-terminal (per POS tag) annotations of MWEs were XML errors (Schluter and Genabith, 2007). MFT (Schluter and Genabith, 2007): Manual revision to 3,800 sentences. Major changes included coordination raising, an expanded POS tag set, and the 273 of the unlabeled word types did not appear elsewhere in the treebank. All but 11 of these were nouns. We manually assigned the correct tags, but we would not expect a negative effect by deterministically labeling all of them as nouns. 3We automate tree manipulation with Tregex/Tsurgeon (Levy and Andrew, 2006). Our pre-processing package is available at http://nlp.stanford.edu/software/lex-parser.shtml. FTB-UC (Candito and Crabbé, 2009): An i</context>
</contexts>
<marker>Schluter, Genabith, 2007</marker>
<rawString>N. Schluter and J. Genabith. 2007. Preparing, restructuring, and augmenting a French treebank: Lexicalised parsers or coherent treebanks? In Pacling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Seddah</author>
<author>M Candito</author>
<author>B Crabbé</author>
</authors>
<title>Cross parser evaluation and tagset variation: a French treebank study.</title>
<date>2009</date>
<booktitle>In IWPT.</booktitle>
<contexts>
<context position="26645" citStr="Seddah et al. (2009)" startWordPosition="4515" endWordPosition="4519">ifferent parsing metrics. Evalb is the standard labeled precision/recall metric.10 Leaf Ancestor measures the cost of transforming guess trees to the reference (Sampson and Babarczy, 2003). It was developed in response to the nonterminal/terminal ratio bias of Evalb, which penalizes flat treebanks like the FTB. The range of the score is between 0 and 1 (higher is better). We report micro-averaged (whole corpus) and macro-averaged (per sentence) scores. In terms of parsing accuracy, the Berkeley parser exceeds both Stanford and DP-TSG. This is consistent with previous experiments for French by Seddah et al. (2009), who show that the Berkeley parser outperforms other models. It also matches the ordering for English (Cohn et al., 2010; Liang et al., 2010). However, the standard baseline for TSG models is a simple parent-annotated PCFG (PA-PCFG). For English, Liang et al. (2010) showed that a similar DP-TSG improved over PA-PCFG by 4.2% F1. For French, our gain is a more substantial 8.2% F1. 5.2 MWE Identification Experiments Table 7 lists overall and per-category MWE identification results for the parsing models. Although DPTSG is less accurate as a general parsing model, it is more effective at identify</context>
<context position="34040" citStr="Seddah et al. (2009)" startWordPosition="5769" endWordPosition="5772"> that of Nivre and Nilsson (2004), who converted a Swedish corpus into two versions: one in which MWEs were left as tokens, and one in which they were merged. On the first version, they showed that a deterministic dependency parser could identify MWEs at 71.1% F1, albeit without subcategory information. On the second version—which simulated perfect MWE identification—they showed that labeled attachment improved by about 1%. Recent statistical parsing work on French has included Stochastic Tree Insertion Grammars (STIGs), which are related to TAGs, but with a restricted adjunction operation.13 Seddah et al. (2009) and Seddah (2010) showed that STIGs underperform CFGbased parsers on the FTB. In their experiments, MWEs were concatenated. 13TSGs differ from TAGs and STIGs in that they do not include an adjunction operator. MWN NP NP NP (c) DP-TSG (d) Stanford N PP de promotion campagne (a) DP-TSG P N N NP N promotion (b) Stanford campagne P de N MWADV P emplois N à domicile PP N NP P emplois N à domicile MWC C P N sous prétexte que (a) Reference 733 8 Conclusion The main result of this paper is that an existing statistical parser can achieve a 36.4% F1 absolute improvement for MWE identification over a st</context>
</contexts>
<marker>Seddah, Candito, Crabbé, 2009</marker>
<rawString>D. Seddah, M. Candito, and B. Crabbé. 2009. Cross parser evaluation and tagset variation: a French treebank study. In IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Seddah</author>
</authors>
<title>Exploring the Spinal-STIG model for parsing French.</title>
<date>2010</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="11900" citStr="Seddah (2010)" startWordPosition="1949" endWordPosition="1950"> FTBUC (with some pre-grouped MWEs). Since the syntax and definition of MWEs is a contentious issue, we take a more agnostic view—which is consistent with that of the FTB annotators—and leave them tokenized. This permits a data-oriented approach to MWE identification that is more robust to changes to the status of specific MWE instances. To set a baseline prior to grammar development, we trained the Stanford parser (Klein and Manning, 2003) with no grammar features, achieving 74.2% labeled F1 on the development set (sentences ≤ 40 words). This is lower than the most recent results obtained by Seddah (2010). However, the results are not comparable: the data split was different, they made use of morphological information, and more importantly they concatenated MWEs. The focus of 727 our work is on models and data representations that enable MWE identification. 3 MWEs in Lexicon-Grammar The MWE representation in the FTB is close to the one proposed in the Lexicon-Grammar (Gross, 1986). In the Lexicon-Grammar, MWEs are classified according to their global POS tags (noun, verb, adverb, adjective), and described in terms of the sequence of the POS tags of the words that constitute the MWE (e.g., “N d</context>
<context position="34058" citStr="Seddah (2010)" startWordPosition="5774" endWordPosition="5776">n (2004), who converted a Swedish corpus into two versions: one in which MWEs were left as tokens, and one in which they were merged. On the first version, they showed that a deterministic dependency parser could identify MWEs at 71.1% F1, albeit without subcategory information. On the second version—which simulated perfect MWE identification—they showed that labeled attachment improved by about 1%. Recent statistical parsing work on French has included Stochastic Tree Insertion Grammars (STIGs), which are related to TAGs, but with a restricted adjunction operation.13 Seddah et al. (2009) and Seddah (2010) showed that STIGs underperform CFGbased parsers on the FTB. In their experiments, MWEs were concatenated. 13TSGs differ from TAGs and STIGs in that they do not include an adjunction operator. MWN NP NP NP (c) DP-TSG (d) Stanford N PP de promotion campagne (a) DP-TSG P N N NP N promotion (b) Stanford campagne P de N MWADV P emplois N à domicile PP N NP P emplois N à domicile MWC C P N sous prétexte que (a) Reference 733 8 Conclusion The main result of this paper is that an existing statistical parser can achieve a 36.4% F1 absolute improvement for MWE identification over a state-ofthe-art n-gr</context>
</contexts>
<marker>Seddah, 2010</marker>
<rawString>D. Seddah. 2010. Exploring the Spinal-STIG model for parsing French. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Seretan</author>
</authors>
<title>Syntax-Based Collocation Extraction,</title>
<date>2011</date>
<journal>Text, Speech, and Language</journal>
<volume>44</volume>
<publisher>Technology. Springer.</publisher>
<contexts>
<context position="32708" citStr="Seretan (2011)" startWordPosition="5565" endWordPosition="5566">he MWADV in emplois à domicile [jobs at home] ‘homeworking’. 7 Related Work There is a voluminous literature on MWE identification. Here we review closely related syntaxbased methods.12 The linguistic and computational attractiveness of lexicalized grammars for modeling idiosyncratic constructions in French was identified by Abeillé (1988) and Abeillé and Schabes (1989). They manually developed a small Tree Adjoining Grammar (TAG) of 1,200 elementary trees and 4,000 lexical items that included MWEs. The classic statistical approach to MWE identification, Xtract (Smadja, 1993), used an in12See Seretan (2011) for a comprehensive survey of syntaxbased methods for MWE identification. For an overview of ngram methods like mwetoolkit, see Pecina (2010). Figure 4: Correct analyses by DP-TSG. (dev set) cremental parser in the third stage of its pipeline to identify predicate-argument relationships. Lin (1999) applied information-theoretic measures to automatically-extracted dependency relations to find MWEs. To our knowledge, Wehrli (2000) was the first to use syntactically annotated corpora to improve a parser for MWE identification. He proposed to rank analyses of a symbolic parser based on the presen</context>
</contexts>
<marker>Seretan, 2011</marker>
<rawString>V. Seretan. 2011. Syntax-Based Collocation Extraction, volume 44 of Text, Speech, and Language Technology. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Smadja</author>
</authors>
<title>Retrieving collocations from text: Xtract. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="32676" citStr="Smadja, 1993" startWordPosition="5559" endWordPosition="5560"> the TSG correctly identifies the MWADV in emplois à domicile [jobs at home] ‘homeworking’. 7 Related Work There is a voluminous literature on MWE identification. Here we review closely related syntaxbased methods.12 The linguistic and computational attractiveness of lexicalized grammars for modeling idiosyncratic constructions in French was identified by Abeillé (1988) and Abeillé and Schabes (1989). They manually developed a small Tree Adjoining Grammar (TAG) of 1,200 elementary trees and 4,000 lexical items that included MWEs. The classic statistical approach to MWE identification, Xtract (Smadja, 1993), used an in12See Seretan (2011) for a comprehensive survey of syntaxbased methods for MWE identification. For an overview of ngram methods like mwetoolkit, see Pecina (2010). Figure 4: Correct analyses by DP-TSG. (dev set) cremental parser in the third stage of its pipeline to identify predicate-argument relationships. Lin (1999) applied information-theoretic measures to automatically-extracted dependency relations to find MWEs. To our knowledge, Wehrli (2000) was the first to use syntactically annotated corpora to improve a parser for MWE identification. He proposed to rank analyses of a sym</context>
</contexts>
<marker>Smadja, 1993</marker>
<rawString>F. Smadja. 1993. Retrieving collocations from text: Xtract. Computational Linguistics, 19:143–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>D J Weir</author>
</authors>
<title>The use of shared forests in tree adjoining grammar parsing.</title>
<date>1993</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="24257" citStr="Vijay-Shanker and Weir, 1993" startWordPosition="4122" endWordPosition="4125">ues NChirac PUNC+(2) 11 � p(z) = cEV (2) 730 After each Gibbs iteration, we sample each sc directly using binomial-Beta conjugacy. We re-sample the DP concentration parameters αc with the auxiliary variable procedure of West (1995). Decoding We compute the rule score of each tree fragment from a single grammar sample as follows: nc,e(z) + αcP0(e|c) θc,e = (5) nc,·(z) + αc To make the grammar more robust, we also include all CFG rules in Po with zero counts in n. Scores for these rules follow from (5) with nc,e(z) = 0. For decoding, we note that the derivations of a TSG are a CFG parse forest (Vijay-Shanker and Weir, 1993). As such, we can use a Synchronous Context Free Grammar (SCFG) to translate the 1-best parse to its derivation. Consider a unique tree fragment ez rooted at X with frontier &apos;y, which is a sequence of terminals and non-terminals. We encode this fragment as an SCFG rule of the form [X -+ &apos;y , X -+ i, Yl, ... , Yn] (6) where Yl, ... , Yn is the sequence of non-terminal nodes in &apos;y.8 During decoding, the input is rewritten as a sequence of tree fragment (rule) indices {i, j, k,... }. Because the TSG substitution operator always applies to the leftmost frontier node, we can deterministically recov</context>
</contexts>
<marker>Vijay-Shanker, Weir, 1993</marker>
<rawString>K. Vijay-Shanker and D. J. Weir. 1993. The use of shared forests in tree adjoining grammar parsing. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Wehrli</author>
</authors>
<title>Parsing and collocations.</title>
<date>2000</date>
<booktitle>In Natural Language Processing—NLP</booktitle>
<volume>1835</volume>
<pages>272--282</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="33141" citStr="Wehrli (2000)" startWordPosition="5626" endWordPosition="5627"> of 1,200 elementary trees and 4,000 lexical items that included MWEs. The classic statistical approach to MWE identification, Xtract (Smadja, 1993), used an in12See Seretan (2011) for a comprehensive survey of syntaxbased methods for MWE identification. For an overview of ngram methods like mwetoolkit, see Pecina (2010). Figure 4: Correct analyses by DP-TSG. (dev set) cremental parser in the third stage of its pipeline to identify predicate-argument relationships. Lin (1999) applied information-theoretic measures to automatically-extracted dependency relations to find MWEs. To our knowledge, Wehrli (2000) was the first to use syntactically annotated corpora to improve a parser for MWE identification. He proposed to rank analyses of a symbolic parser based on the presence of collocations, although details of the ranking function were not provided. The most similar work to ours is that of Nivre and Nilsson (2004), who converted a Swedish corpus into two versions: one in which MWEs were left as tokens, and one in which they were merged. On the first version, they showed that a deterministic dependency parser could identify MWEs at 71.1% F1, albeit without subcategory information. On the second ve</context>
</contexts>
<marker>Wehrli, 2000</marker>
<rawString>E. Wehrli. 2000. Parsing and collocations. In Natural Language Processing—NLP 2000, volume 1835 of Lecture Notes in Computer Science, pages 272–282. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M West</author>
</authors>
<title>Hyperparameter estimation in Dirichlet process mixture models.</title>
<date>1995</date>
<tech>Technical report,</tech>
<institution>Duke University.</institution>
<contexts>
<context position="23859" citStr="West (1995)" startWordPosition="4049" endWordPosition="4050">of sites with bs = 1. Because this algorithm is a not a contribution of this paper, we refer the reader to Liang et al. (2010). 6The Stanford parser is a product model, so the results in §5.1 include the contribution of a dependency parser. 7Bansal and Klein (2010) also experimented with symbol refinement in an all-fragments (parametric) TSG for English. NP+ PUNC-(1) 11 N+ Jacques NChirac PUNC+(2) 11 � p(z) = cEV (2) 730 After each Gibbs iteration, we sample each sc directly using binomial-Beta conjugacy. We re-sample the DP concentration parameters αc with the auxiliary variable procedure of West (1995). Decoding We compute the rule score of each tree fragment from a single grammar sample as follows: nc,e(z) + αcP0(e|c) θc,e = (5) nc,·(z) + αc To make the grammar more robust, we also include all CFG rules in Po with zero counts in n. Scores for these rules follow from (5) with nc,e(z) = 0. For decoding, we note that the derivations of a TSG are a CFG parse forest (Vijay-Shanker and Weir, 1993). As such, we can use a Synchronous Context Free Grammar (SCFG) to translate the 1-best parse to its derivation. Consider a unique tree fragment ez rooted at X with frontier &apos;y, which is a sequence of t</context>
</contexts>
<marker>West, 1995</marker>
<rawString>M. West. 1995. Hyperparameter estimation in Dirichlet process mixture models. Technical report, Duke University.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>