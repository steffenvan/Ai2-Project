<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.069275">
<title confidence="0.98946">
Improving Citation Polarity Classification with Product Reviews
</title>
<author confidence="0.985441">
Charles Jochim∗ Hinrich Sch¨utze
</author>
<affiliation confidence="0.964647">
IBM Research – Ireland Center for Information &amp; Language Processing
charlesj@ie.ibm.com University of Munich
</affiliation>
<sectionHeader confidence="0.97913" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999915407407407">
Recent work classifying citations in scien-
tific literature has shown that it is possi-
ble to improve classification results with
extensive feature engineering. While this
result confirms that citation classification
is feasible, there are two drawbacks to
this approach: (i) it requires a large anno-
tated corpus for supervised classification,
which in the case of scientific literature
is quite expensive; and (ii) feature engi-
neering that is too specific to one area of
scientific literature may not be portable to
other domains, even within scientific liter-
ature. In this paper we address these two
drawbacks. First, we frame citation clas-
sification as a domain adaptation task and
leverage the abundant labeled data avail-
able in other domains. Then, to avoid
over-engineering specific citation features
for a particular scientific domain, we ex-
plore a deep learning neural network ap-
proach that has shown to generalize well
across domains using unigram and bigram
features. We achieve better citation clas-
sification results with this cross-domain
approach than using in-domain classifica-
tion.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.989620055555556">
Citations have been categorized and studied for
a half-century (Garfield, 1955) to better under-
stand when and how citations are used, and
to record and measure how information is ex-
changed (e.g., networks of co-cited papers or au-
thors (Small and Griffith, 1974)). Recently, the
value of this information has been shown in practi-
cal applications such as information retrieval (IR)
∗ This work was primarily conducted at the IMS – Uni-
versity of Stuttgart.
(Ritchie et al., 2008), summarization (Qazvinian
and Radev, 2008), and even identifying scientific
breakthroughs (Small and Klavans, 2011). We ex-
pect that by identifying and labeling the function
of citations we can improve the effectiveness of
these applications.
There has been no consensus on what aspects
or functions of a citation should be annotated and
how. Early citation classification focused more on
citation motivation (Garfield, 1964), while later
classification considered more the citation func-
tion (Chubin and Moitra, 1975). Recent stud-
ies using automatic classification have continued
this tradition of introducing a new classification
scheme with each new investigation into the use
of citations (Nanba and Okumura, 1999; Teufel
et al., 2006a; Dong and Sch¨afer, 2011; Abu-Jbara
et al., 2013). One distinction that has been more
consistently annotated across recent citation clas-
sification studies is between positive and negative
citations (Athar, 2011; Athar and Teufel, 2012;
Abu-Jbara et al., 2013).1 The popularity of this
distinction likely owes to the prominence of sen-
timent analysis in NLP (Liu, 2010). We follow
much of the recent work on citation classification
and concentrate on citation polarity.
</bodyText>
<sectionHeader confidence="0.989683" genericHeader="method">
2 Domain Adaptation
</sectionHeader>
<bodyText confidence="0.9998713">
By concentrating on citation polarity we are able
to compare our classification to previous citation
polarity work. This choice also allows us to access
the wealth of existing data containing polarity an-
notation and then frame the task as a domain adap-
tation problem. Of course the risk in approaching
the problem as domain adaptation is that the do-
mains are so different that the representation of
a positive instance of a movie or product review,
for example, will not coincide with that of a posi-
</bodyText>
<footnote confidence="0.995710333333333">
1Dong and Sch¨afer (2011) also annotate polarity, which
can be found in their dataset (described later), but this is not
discussed in their paper.
</footnote>
<page confidence="0.989232">
42
</page>
<bodyText confidence="0.968193902439025">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 42–48,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
tive scientific citation. On the other hand, because
there is a limited amount of annotated citation data
available, by leveraging large amounts of anno-
tated polarity data we could potentially even im-
prove citation classification.
We treat citation polarity classification as a sen-
timent analysis domain adaptation task and there-
fore must be careful not to define features that are
too domain specific. Previous work in citation po-
larity classification focuses on finding new cita-
tion features to improve classification, borrowing
a few from text classification in general (e.g., n-
grams), and perhaps others from sentiment analy-
sis problems (e.g., the polarity lexicon from Wil-
son et al. (2005)). We would like to do as little
feature engineering as possible to ensure that the
features we use are meaningful across domains.
However, we do still want features that somehow
capture the inherent positivity or negativity of our
labeled instances, i.e., citations or Amazon prod-
uct reviews. Currently a popular approach for ac-
complishing this is to use deep learning neural net-
works (Bengio, 2009), which have been shown
to perform well on a variety of NLP tasks us-
ing only bag-of-word features (Collobert et al.,
2011). More specifically related to our work, deep
learning neural networks have been successfully
employed for sentiment analysis (Socher et al.,
2011) and for sentiment domain adaptation (Glo-
rot et al., 2011). In this paper we examine one
of these approaches, marginalized stacked denois-
ing autoencoders (mSDA) from Chen et al. (2012),
which has been successful in classifying the po-
larity of Amazon product reviews across product
domains. Since mSDA achieved state-of-the-art
performance in Amazon product domain adapta-
tion, we are hopeful it will also be effective when
switching to a more distant domain like scientific
citations.
</bodyText>
<sectionHeader confidence="0.994365" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.99216">
3.1 Corpora
</subsectionHeader>
<bodyText confidence="0.999992072727273">
We are interested in domain adaptation for citation
classification and therefore need a target dataset of
citations and a non-citation source dataset. There
are two corpora available that contain citation
function annotation, the DFKI Citation Corpus
(Dong and Sch¨afer, 2011) and the IMS Citation
Corpus (Jochim and Sch¨utze, 2012). Both corpora
have only about 2000 instances; unfortunately,
there are no larger corpora available with citation
annotation and this task would benefit from more
annotated data. Due to the infrequent use of neg-
ative citations, a substantial annotation effort (an-
notating over 5 times more data) would be nec-
essary to reach 1000 negative citation instances,
which is the number of negative instances in a sin-
gle domain in the multi-domain corpus described
below.
The DFKI Citation Corpus2 has been used for
classifying citation function (Dong and Sch¨afer,
2011), but the dataset also includes polarity an-
notation. The dataset has 1768 citation sentences
with polarity annotation: 190 are labeled as pos-
itive, 57 as negative, and the vast majority, 1521,
are left neutral. The second citation corpus, the
IMS Citation Corpus3 contains 2008 annotated ci-
tations: 1836 are labeled positive and 172 are la-
beled negative. Jochim and Sch¨utze (2012) use
annotation labels from Moravcsik and Murugesan
(1975) where positive instances are labeled confir-
mative, negative instances are labeled negational,
and there is no neutral class. Because each of
the citation corpora is of modest size we combine
them to form one citation dataset, which we will
refer to as CITD. The two citation corpora com-
prising CITD both come from the ACL Anthol-
ogy (Bird et al., 2008): the IMS corpus uses the
ACL proceedings from 2004 and the DFKI corpus
uses parts of the proceedings from 2007 and 2008.
Since mSDA also makes use of large amounts of
unlabeled data, we extend our CITD corpus with
citations from the proceedings of the remaining
years of the ACL, 1979–2003, 2005–2006, and
2009.
There are a number of non-citation corpora
available that contain polarity annotation. For
these experiments we use the Multi-Domain Senti-
ment Dataset4 (henceforth MDSD), introduced by
Blitzer et al. (2007). We use the version of the
MDSD that includes positive and negative labels
for product reviews taken from Amazon.com in
the following domains: books, dvd, electronics,
and kitchen. For each domain there are 1000 pos-
itive reviews and 1000 negative reviews that com-
prise the “labeled” data, and then roughly 4000
more reviews in the “unlabeled”5 data. Reviews
</bodyText>
<footnote confidence="0.983629428571429">
2https://aclbib.opendfki.de/repos/
trunk/citation_classification_dataset/
3http://www.ims.uni-stuttgart.de/
˜jochimcs/citation-classification/
4http://www.cs.jhu.edu/˜mdredze/
datasets/sentiment/
5It is usually treated as unlabeled data even though it ac-
</footnote>
<page confidence="0.99913">
43
</page>
<table confidence="0.99635175">
Corpus Instances Pos. Neg. Neut.
DFKI 1768 190 57 1521
IMS 2008 1836 172 –
MDSD 27,677 13,882 13,795 –
</table>
<tableCaption confidence="0.999893">
Table 1: Polarity corpora.
</tableCaption>
<bodyText confidence="0.996521933333333">
were preprocessed so that for each review you find
a list of unigrams and bigrams with their frequency
within the review. Unigrams from a stop list of 55
stop words are removed, but stop words in bigrams
remain.
Table 1 shows the distribution of polarity labels
in the corpora we use for our experiments. We
combine the DFKI and IMS corpora into the CITD
corpus. We omit the citations labeled neutral from
the DFKI corpus because the IMS corpus does not
contain neutral annotation nor does the MDSD. It
is the case in many sentiment analysis corpora that
only positive and negative instances are included,
e.g., (Pang et al., 2002).
The citation corpora presented above are both
unbalanced and both have a highly skewed distri-
bution. The MDSD on the other hand is evenly
balanced and an effort was even made to keep
the data treated as “unlabeled” rather balanced.
For this reason, in line with previous work us-
ing MDSD, we balance the labeled portion of the
CITD corpus. This is done by taking 179 unique
negative sentences in the DFKI and IMS corpora
and randomly selecting an equal number of posi-
tive sentences. The IMS corpus can have multiple
labeled citations per sentence: there are 122 sen-
tences containing the 172 negative citations from
Table 1. The final CITD corpus comprises this
balanced corpus of 358 labeled citation sentences
plus another 22,093 unlabeled citation sentences.
</bodyText>
<subsectionHeader confidence="0.960452">
3.2 Features
</subsectionHeader>
<bodyText confidence="0.996085783783784">
In our experiments, we restrict our features to un-
igrams and bigrams from the product review or
citation context (i.e., the sentence containing the
citation). This follows previous studies in do-
main adaptation (Blitzer et al., 2007; Glorot et al.,
2011). Chen et al. (2012) achieve state-of-the-art
results on MDSD by testing the 5000 and 30,000
most frequent unigram and bigram features.
Previous work in citation classification has
largely focused on identifying new features for
tually contains positive and negative labels, which have been
used, e.g., in (Chen et al., 2012).
improving classification accuracy. A significant
amount of effort goes into engineering new fea-
tures, in particular for identifying cue phrases,
e.g., (Teufel et al., 2006b; Dong and Sch¨afer,
2011). However, there seems to be little consen-
sus on which features help most for this task. For
example, Abu-Jbara et al. (2013) and Jochim and
Sch¨utze (2012) find the list of polar words from
Wilson et al. (2005) to be useful, and neither study
lists dependency relations as significant features.
Athar (2011) on the other hand reported significant
improvement using dependency relation features
and found that the same list of polar words slightly
hurt classification accuracy. The classifiers and
implementation of features varies between these
studies, but the problem remains that there seems
to be no clear set of features for citation polarity
classification.
The lack of consensus on the most useful cita-
tion polarity features coupled with the recent suc-
cess of deep learning neural networks (Collobert et
al., 2011) further motivate our choice to limit our
features to the n-grams available in the product re-
view or citation context and not rely on external
resources or tools for additional features.
</bodyText>
<subsectionHeader confidence="0.999125">
3.3 Classification with mSDA
</subsectionHeader>
<bodyText confidence="0.9999501875">
For classification we use marginalized stacked de-
noising autoencoders (mSDA) from Chen et al.
(2012)6 plus a linear SVM. mSDA takes the con-
cept of denoising – introducing noise to make the
autoencoder more robust – from Vincent et al.
(2008), but does the optimization in closed form,
thereby avoiding iterating over the input vector to
stochastically introduce noise. The result of this
is faster run times and currently state-of-the-art
performance on MDSD, which makes it a good
choice for our domain adaptation task. The mSDA
implementation comes with LIBSVM, which we
replace with LIBLINEAR (Fan et al., 2008) for
faster run times with no decrease in accuracy. LIB-
LINEAR, with default settings, also serves as our
baseline.
</bodyText>
<subsectionHeader confidence="0.931263">
3.4 Outline of Experiments
</subsectionHeader>
<bodyText confidence="0.99971775">
Our initial experiments simply extend those of
Chen et al. (2012) (and others who have used
MDSD) by adding another domain, citations. We
train on each of the domains from the MDSD –
</bodyText>
<footnote confidence="0.994543333333333">
6We use their MATLAB implementation available at
http://www.cse.wustl.edu/˜mchen/code/
mSDA.tar.
</footnote>
<page confidence="0.996764">
44
</page>
<figure confidence="0.996233">
0.4
0.3
0.2
0.1
0.0
books dvd electronics kitchen
</figure>
<figureCaption confidence="0.9916702">
Figure 1: Cross domain macro-F1 results train-
ing on Multi-Domain Sentiment Dataset and test-
ing on citation dataset (CITD). The horizontal line
indicates macro-F1 for in-domain citation classifi-
cation.
</figureCaption>
<bodyText confidence="0.998195764705882">
books, dvd, electronics, and kitchen – and test on
the citation data. We split the labeled data 80/20
following Blitzer et al. (2007) (cf. Chen et al.
(2012) train on all “labeled” data and test on the
“unlabeled” data). These experiments should help
answer two questions: does a larger amount of
training data, even if out of domain, improve ci-
tation classification; and how well do the differ-
ent product domains generalize to citations (i.e.,
which domains are most similar to citations)?
In contrast to previous work using MDSD, a lot
of the work in domain adaptation also leverages a
small amount of labeled target data. In our second
set of experiments, we follow the domain adap-
tation approaches described in (Daum´e III, 2007)
and train on product review and citation data be-
fore testing on citations.
</bodyText>
<sectionHeader confidence="0.999663" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<subsectionHeader confidence="0.98674">
4.1 Citation mSDA
</subsectionHeader>
<bodyText confidence="0.999991090909091">
Our initial results show that using mSDA for do-
main adaptation to citations actually outperforms
in-domain classification. In Figure 1 we com-
pare citation classification with mSDA to the SVM
baseline. Each pair of vertical bars represents
training on a domain from MDSD (e.g., books)
and testing on CITD. The dark gray bar indicates
the F1 scores for the SVM baseline using the
30,000 features and the lighter gray bar shows the
mSDA results. The black horizontal line indicates
the F1 score for in-domain citation classification,
which sometimes represents the goal for domain
adaptation. We can see that using a larger dataset,
even if out of domain, does improve citation clas-
sification. For books, dvd, and electronics, even
the SVM baseline improves on in-domain classifi-
cation. mSDA does better than the baseline for all
domains except dvd. Using a larger training set,
along with mSDA, which makes use of the un-
labeled data, leads to the best results for citation
classification.
In domain adaptation we would expect the do-
mains most similar to the target to lead to the
highest results. Like Dai et al. (2007), we mea-
sure the Kullback-Leibler divergence between the
source and target domains’ distributions. Accord-
ing to this measure, citations are most similar to
the books domain. Therefore, it is not surprising
that training on books performs well on citations,
and intuitively, among the domains in the Amazon
dataset, a book review is most similar to a scien-
tific citation. This makes the good mSDA results
for electronics a bit more surprising.
</bodyText>
<subsectionHeader confidence="0.977705">
4.2 Easy Domain Adaptation
</subsectionHeader>
<bodyText confidence="0.999972333333333">
The results in Section 4.1 are for semi-supervised
domain adaptation: the case where we have some
large annotated corpus (Amazon product reviews)
and a large unannotated corpus (citations). There
have been a number of other successful attempts at
fully supervised domain adaptation, where it is as-
sumed that some small amount of data is annotated
in the target domain (Chelba and Acero, 2004;
Daum´e III, 2007; Jiang and Zhai, 2007). To see
how mSDA compares to supervised domain adap-
tation we take the various approaches presented by
Daum´e III (2007). The results of this comparison
can be seen in Table 2. Briefly, “All” trains on
source and target data; “Weight” is the same as
“All” except that instances may be weighted dif-
ferently based on their domain (weights are chosen
on a development set); “Pred” trains on the source
data, makes predictions on the target data, and
then trains on the target data with the predictions;
“LinInt” linearly interpolates predictions using the
source-only and target-only models (the interpola-
tion parameter is chosen on a development set);
“Augment” uses a larger feature set with source-
specific and target-specific copies of features; see
</bodyText>
<figure confidence="0.9826742">
0.6
0.5
SVM
mSDA
In−domain F1
</figure>
<page confidence="0.993912">
45
</page>
<table confidence="0.999332166666667">
Domain Baseline All Weight Pred LinInt Augment mSDA
books 54.5 54.8 52.0 51.9 53.4 53.4 57.1
dvd 53.2 50.9 56.0 53.4 51.9 47.5 51.6
electronics 53.4 49.0 50.5 53.4 54.8 51.9 59.2
kitchen 47.9 48.8 50.7 53.4 52.6 49.2 50.1
citations 51.9 – – – – – 54.9
</table>
<tableCaption confidence="0.991996">
Table 2: Macro-F1 results on CITD using different domain adaptation approaches.
</tableCaption>
<bodyText confidence="0.9977686">
(Daum´e III, 2007) for further details.
We are only interested in citations as the tar-
get domain. Daum´e’s source-only baseline cor-
responds to the “Baseline” column for domains:
books, dvd, electronics, and kitchen; while his
target-only baseline can be seen for citations in the
last row of the “Baseline” column in Table 2.
The semi-supervised mSDA performs quite
well with respect to the fully supervised ap-
proaches, obtaining the best results for books and
electronics, which are also the highest scores over-
all. Weight and Pred have the highest F1 scores for
dvd and kitchen respectively. Daum´e III (2007)
noted that the “Augment” algorithm performed
best when the target-only results were better than
the source-only results. When this was not the
case in his experiments, i.e., for the treebank
chunking task, both Weight and Pred were among
the best approaches. In our experiments, training
on source-only outperforms target-only, with the
exception of the kitchen domain.
We have included the line for citations to see the
results training only on the target data (F1 = 51.9)
and to see the improvement when using all of the
unlabeled data with mSDA (F1 = 54.9).
</bodyText>
<subsectionHeader confidence="0.996482">
4.3 Discussion
</subsectionHeader>
<bodyText confidence="0.9972936">
These results are very promising. Although they
are not quite as high as other published results
for citation polarity (Abu-Jbara et al., 2013)7, we
have shown that you can improve citation polarity
classification by leveraging large amounts of an-
notated data from other domains and using a sim-
ple set of features.
mSDA and fully supervised approaches can also
be straightforwardly combined. We do not present
those results here due to space constraints. The
</bodyText>
<footnote confidence="0.6029938">
7Their work included a CRF model to identify the citation
context that gave them an increase of 9.2 percent Fl over a
single sentence citation context. Our approach achieves sim-
ilar macro-Fl on only the citation sentence, but using a dif-
ferent corpus.
</footnote>
<bodyText confidence="0.99648775">
combination led to mixed results: adding mSDA
to the supervised approaches tended to improve F1
over those approaches but results never exceeded
the top mSDA numbers in Table 2.
</bodyText>
<sectionHeader confidence="0.999887" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999974">
Teufel et al. (2006b) introduced automatic citation
function classification, with classes that could be
grouped as positive, negative, and neutral. They
relied in part on a manually compiled list of cue
phrases that cannot easily be transferred to other
classification schemes or other scientific domains.
Athar (2011) followed this and was the first to
specifically target polarity classification on scien-
tific citations. He found that dependency tuples
contributed the most significant improvement in
results. Abu-Jbara et al. (2013) also looks at both
citation function and citation polarity. A big con-
tribution of this work is that they also train a CRF
sequence tagger to find the citation context, which
significantly improves results over using only the
citing sentence. Their feature analysis indicates
that lexicons for negation, speculation, and po-
larity were most important for improving polarity
classification.
</bodyText>
<sectionHeader confidence="0.999249" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.998938571428571">
Robust citation classification has been hindered by
the relative lack of annotated data. In this pa-
per we successfully use a large, out-of-domain,
annotated corpus to improve the citation polarity
classification. Our approach uses a deep learning
neural network for domain adaptation with labeled
out-of-domain data and unlabeled in-domain data.
This semi-supervised domain adaptation approach
outperforms the in-domain citation polarity classi-
fication and other fully supervised domain adapta-
tion approaches.
Acknowledgments. We thank the DFG for
funding this work (SPP 1335 Scalable Visual An-
alytics).
</bodyText>
<page confidence="0.999144">
46
</page>
<sectionHeader confidence="0.979691" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999263247619048">
Amjad Abu-Jbara, Jefferson Ezra, and Dragomir
Radev. 2013. Purpose and polarity of citation: To-
wards NLP-based bibliometrics. In Proceedings of
NAACL-HLT, pages 596–606.
Awais Athar and Simone Teufel. 2012. Context-
enhanced citation sentiment detection. In Proceed-
ings of NAACL-HLT, pages 597–601.
Awais Athar. 2011. Sentiment analysis of citations us-
ing sentence structure-based features. In Proceed-
ings of ACL Student Session, pages 81–87.
Yoshua Bengio. 2009. Learning deep architectures for
AI. Foundations and Trends in Machine Learning,
2(1):1–127.
Steven Bird, Robert Dale, Bonnie Dorr, Bryan Gibson,
Mark Joseph, Min-Yen Kan, Dongwon Lee, Brett
Powley, Dragomir Radev, and Yee Fan Tan. 2008.
The ACL anthology reference corpus: A reference
dataset for bibliographic research in computational
linguistics. In Proceedings of LREC, pages 1755–
1759.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, Bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Proceedings of ACL, pages 440–447.
Ciprian Chelba and Alex Acero. 2004. Adaptation of
maximum entropy capitalizer: Little data can help a
lot. In Proceedings of EMNLP, pages 285–292.
Minmin Chen, Zhixiang Eddie Xu, Kilian Q. Wein-
berger, and Fei Sha. 2012. Marginalized denoising
autoencoders for domain adaptation. In Proceedings
of ICML, pages 767–774.
Daryl E. Chubin and Soumyo D. Moitra. 1975. Con-
tent analysis of references: Adjunct or alternative to
citation counting? Social Studies of Science, 5:423–
441.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493–2537.
Wenyuan Dai, Gui-Rong Xue, Qiang Yang, and Yong
Yu. 2007. Transferring naive bayes classifiers for
text classification. In AAAI, pages 540–545.
Hal Daum´e III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of ACL, pages 256–263.
Cailing Dong and Ulrich Sch¨afer. 2011. Ensemble-
style self-training on citation classification. In Pro-
ceedings of IJCNLP, pages 623–631.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, 9:1871–1874.
Eugene Garfield. 1955. Citation indexes to science:
A new dimension in documentation through associ-
ation of ideas. Science, 122:108–111.
Eugene Garfield. 1964. Can citation indexing be au-
tomated? In Statistical Association Methods for
Mechanized Documentation, Symposium Proceed-
ings, pages 189–192.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Pro-
ceedings of ICML, pages 513–520.
Jing Jiang and ChengXiang Zhai. 2007. Instance
weighting for domain adaptation in NLP. In ACL,
pages 264–271.
Charles Jochim and Hinrich Sch¨utze. 2012. Towards
a generic and flexible citation classifier based on
a faceted classification scheme. In Proceedings of
COLING, pages 1343–1358.
Bing Liu. 2010. Sentiment analysis and subjectivity.
In Nitin Indurkhya and Fred J. Damerau, editors,
Handbook of Natural Language Processing, Second
Edition. CRC Press, Taylor and Francis Group.
Michael J. Moravcsik and Poovanalingam Murugesan.
1975. Some results on the function and quality of
citations. Social Studies of Science, 5:86–92.
Hidetsugu Nanba and Manabu Okumura. 1999. To-
wards multi-paper summarization using reference
information. In Proceedings of IJCAI, pages 926–
931.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification us-
ing machine learning techniques. In Proceedings of
EMNLP, pages 79–86.
Vahed Qazvinian and Dragomir R. Radev. 2008. Sci-
entific paper summarization using citation summary
networks. In Proceedings of COLING, pages 689–
696.
Anna Ritchie, Stephen Robertson, and Simone Teufel.
2008. Comparing citation contexts for information
retrieval. In Proceedings of CIKM, pages 213–222.
Henry G. Small and Belver C. Griffith. 1974. The
structure of scientific literatures I: Identifying and
graphing specialties. Science Studies, 4(1):17–40.
Henry Small and Richard Klavans. 2011. Identifying
scientific breakthroughs by combining co-citation
analysis and citation context. In Proceedings of In-
ternational Society for Scientometrics and Informet-
rics.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of
EMNLP, pages 151–161.
</reference>
<page confidence="0.986676">
47
</page>
<reference confidence="0.9997296">
Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006a. An annotation scheme for citation function.
In Proceedings of SIGdial Workshop on Discourse
and Dialogue, pages 80–87.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006b. Automatic classification of citation function.
In Proceedings of EMNLP, pages 103–110.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and
Pierre-Antoine Manzagol. 2008. Extracting and
composing robust features with denoising autoen-
coders. In Proceedings of ICML, pages 1096–1103.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of HLT-
EMNLP, pages 347–354.
</reference>
<page confidence="0.999353">
48
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.702875">
<title confidence="0.999492">Improving Citation Polarity Classification with Product Reviews</title>
<author confidence="0.933329">Sch¨utze</author>
<affiliation confidence="0.920971">IBM Research – Ireland Center for Information &amp; Language Processing of Munich</affiliation>
<abstract confidence="0.994659785714286">Recent work classifying citations in scientific literature has shown that it is possible to improve classification results with extensive feature engineering. While this result confirms that citation classification is feasible, there are two drawbacks to this approach: (i) it requires a large annotated corpus for supervised classification, which in the case of scientific literature is quite expensive; and (ii) feature engineering that is too specific to one area of scientific literature may not be portable to other domains, even within scientific literature. In this paper we address these two drawbacks. First, we frame citation classification as a domain adaptation task and leverage the abundant labeled data available in other domains. Then, to avoid over-engineering specific citation features for a particular scientific domain, we explore a deep learning neural network approach that has shown to generalize well across domains using unigram and bigram features. We achieve better citation classification results with this cross-domain approach than using in-domain classification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amjad Abu-Jbara</author>
<author>Jefferson Ezra</author>
<author>Dragomir Radev</author>
</authors>
<title>Purpose and polarity of citation: Towards NLP-based bibliometrics.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>596--606</pages>
<contexts>
<context position="2589" citStr="Abu-Jbara et al., 2013" startWordPosition="387" endWordPosition="390">function of citations we can improve the effectiveness of these applications. There has been no consensus on what aspects or functions of a citation should be annotated and how. Early citation classification focused more on citation motivation (Garfield, 1964), while later classification considered more the citation function (Chubin and Moitra, 1975). Recent studies using automatic classification have continued this tradition of introducing a new classification scheme with each new investigation into the use of citations (Nanba and Okumura, 1999; Teufel et al., 2006a; Dong and Sch¨afer, 2011; Abu-Jbara et al., 2013). One distinction that has been more consistently annotated across recent citation classification studies is between positive and negative citations (Athar, 2011; Athar and Teufel, 2012; Abu-Jbara et al., 2013).1 The popularity of this distinction likely owes to the prominence of sentiment analysis in NLP (Liu, 2010). We follow much of the recent work on citation classification and concentrate on citation polarity. 2 Domain Adaptation By concentrating on citation polarity we are able to compare our classification to previous citation polarity work. This choice also allows us to access the weal</context>
<context position="11025" citStr="Abu-Jbara et al. (2013)" startWordPosition="1721" endWordPosition="1724">e state-of-the-art results on MDSD by testing the 5000 and 30,000 most frequent unigram and bigram features. Previous work in citation classification has largely focused on identifying new features for tually contains positive and negative labels, which have been used, e.g., in (Chen et al., 2012). improving classification accuracy. A significant amount of effort goes into engineering new features, in particular for identifying cue phrases, e.g., (Teufel et al., 2006b; Dong and Sch¨afer, 2011). However, there seems to be little consensus on which features help most for this task. For example, Abu-Jbara et al. (2013) and Jochim and Sch¨utze (2012) find the list of polar words from Wilson et al. (2005) to be useful, and neither study lists dependency relations as significant features. Athar (2011) on the other hand reported significant improvement using dependency relation features and found that the same list of polar words slightly hurt classification accuracy. The classifiers and implementation of features varies between these studies, but the problem remains that there seems to be no clear set of features for citation polarity classification. The lack of consensus on the most useful citation polarity f</context>
<context position="18545" citStr="Abu-Jbara et al., 2013" startWordPosition="2947" endWordPosition="2950">the source-only results. When this was not the case in his experiments, i.e., for the treebank chunking task, both Weight and Pred were among the best approaches. In our experiments, training on source-only outperforms target-only, with the exception of the kitchen domain. We have included the line for citations to see the results training only on the target data (F1 = 51.9) and to see the improvement when using all of the unlabeled data with mSDA (F1 = 54.9). 4.3 Discussion These results are very promising. Although they are not quite as high as other published results for citation polarity (Abu-Jbara et al., 2013)7, we have shown that you can improve citation polarity classification by leveraging large amounts of annotated data from other domains and using a simple set of features. mSDA and fully supervised approaches can also be straightforwardly combined. We do not present those results here due to space constraints. The 7Their work included a CRF model to identify the citation context that gave them an increase of 9.2 percent Fl over a single sentence citation context. Our approach achieves similar macro-Fl on only the citation sentence, but using a different corpus. combination led to mixed results</context>
<context position="19841" citStr="Abu-Jbara et al. (2013)" startWordPosition="3151" endWordPosition="3154">e approaches but results never exceeded the top mSDA numbers in Table 2. 5 Related Work Teufel et al. (2006b) introduced automatic citation function classification, with classes that could be grouped as positive, negative, and neutral. They relied in part on a manually compiled list of cue phrases that cannot easily be transferred to other classification schemes or other scientific domains. Athar (2011) followed this and was the first to specifically target polarity classification on scientific citations. He found that dependency tuples contributed the most significant improvement in results. Abu-Jbara et al. (2013) also looks at both citation function and citation polarity. A big contribution of this work is that they also train a CRF sequence tagger to find the citation context, which significantly improves results over using only the citing sentence. Their feature analysis indicates that lexicons for negation, speculation, and polarity were most important for improving polarity classification. 6 Conclusion Robust citation classification has been hindered by the relative lack of annotated data. In this paper we successfully use a large, out-of-domain, annotated corpus to improve the citation polarity c</context>
</contexts>
<marker>Abu-Jbara, Ezra, Radev, 2013</marker>
<rawString>Amjad Abu-Jbara, Jefferson Ezra, and Dragomir Radev. 2013. Purpose and polarity of citation: Towards NLP-based bibliometrics. In Proceedings of NAACL-HLT, pages 596–606.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Awais Athar</author>
<author>Simone Teufel</author>
</authors>
<title>Contextenhanced citation sentiment detection.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>597--601</pages>
<contexts>
<context position="2774" citStr="Athar and Teufel, 2012" startWordPosition="413" endWordPosition="416">tation classification focused more on citation motivation (Garfield, 1964), while later classification considered more the citation function (Chubin and Moitra, 1975). Recent studies using automatic classification have continued this tradition of introducing a new classification scheme with each new investigation into the use of citations (Nanba and Okumura, 1999; Teufel et al., 2006a; Dong and Sch¨afer, 2011; Abu-Jbara et al., 2013). One distinction that has been more consistently annotated across recent citation classification studies is between positive and negative citations (Athar, 2011; Athar and Teufel, 2012; Abu-Jbara et al., 2013).1 The popularity of this distinction likely owes to the prominence of sentiment analysis in NLP (Liu, 2010). We follow much of the recent work on citation classification and concentrate on citation polarity. 2 Domain Adaptation By concentrating on citation polarity we are able to compare our classification to previous citation polarity work. This choice also allows us to access the wealth of existing data containing polarity annotation and then frame the task as a domain adaptation problem. Of course the risk in approaching the problem as domain adaptation is that the</context>
</contexts>
<marker>Athar, Teufel, 2012</marker>
<rawString>Awais Athar and Simone Teufel. 2012. Contextenhanced citation sentiment detection. In Proceedings of NAACL-HLT, pages 597–601.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Awais Athar</author>
</authors>
<title>Sentiment analysis of citations using sentence structure-based features.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL Student Session,</booktitle>
<pages>81--87</pages>
<contexts>
<context position="2750" citStr="Athar, 2011" startWordPosition="411" endWordPosition="412">how. Early citation classification focused more on citation motivation (Garfield, 1964), while later classification considered more the citation function (Chubin and Moitra, 1975). Recent studies using automatic classification have continued this tradition of introducing a new classification scheme with each new investigation into the use of citations (Nanba and Okumura, 1999; Teufel et al., 2006a; Dong and Sch¨afer, 2011; Abu-Jbara et al., 2013). One distinction that has been more consistently annotated across recent citation classification studies is between positive and negative citations (Athar, 2011; Athar and Teufel, 2012; Abu-Jbara et al., 2013).1 The popularity of this distinction likely owes to the prominence of sentiment analysis in NLP (Liu, 2010). We follow much of the recent work on citation classification and concentrate on citation polarity. 2 Domain Adaptation By concentrating on citation polarity we are able to compare our classification to previous citation polarity work. This choice also allows us to access the wealth of existing data containing polarity annotation and then frame the task as a domain adaptation problem. Of course the risk in approaching the problem as domai</context>
<context position="11208" citStr="Athar (2011)" startWordPosition="1753" endWordPosition="1754">res for tually contains positive and negative labels, which have been used, e.g., in (Chen et al., 2012). improving classification accuracy. A significant amount of effort goes into engineering new features, in particular for identifying cue phrases, e.g., (Teufel et al., 2006b; Dong and Sch¨afer, 2011). However, there seems to be little consensus on which features help most for this task. For example, Abu-Jbara et al. (2013) and Jochim and Sch¨utze (2012) find the list of polar words from Wilson et al. (2005) to be useful, and neither study lists dependency relations as significant features. Athar (2011) on the other hand reported significant improvement using dependency relation features and found that the same list of polar words slightly hurt classification accuracy. The classifiers and implementation of features varies between these studies, but the problem remains that there seems to be no clear set of features for citation polarity classification. The lack of consensus on the most useful citation polarity features coupled with the recent success of deep learning neural networks (Collobert et al., 2011) further motivate our choice to limit our features to the n-grams available in the pro</context>
<context position="19624" citStr="Athar (2011)" startWordPosition="3122" endWordPosition="3123"> Our approach achieves similar macro-Fl on only the citation sentence, but using a different corpus. combination led to mixed results: adding mSDA to the supervised approaches tended to improve F1 over those approaches but results never exceeded the top mSDA numbers in Table 2. 5 Related Work Teufel et al. (2006b) introduced automatic citation function classification, with classes that could be grouped as positive, negative, and neutral. They relied in part on a manually compiled list of cue phrases that cannot easily be transferred to other classification schemes or other scientific domains. Athar (2011) followed this and was the first to specifically target polarity classification on scientific citations. He found that dependency tuples contributed the most significant improvement in results. Abu-Jbara et al. (2013) also looks at both citation function and citation polarity. A big contribution of this work is that they also train a CRF sequence tagger to find the citation context, which significantly improves results over using only the citing sentence. Their feature analysis indicates that lexicons for negation, speculation, and polarity were most important for improving polarity classifica</context>
</contexts>
<marker>Athar, 2011</marker>
<rawString>Awais Athar. 2011. Sentiment analysis of citations using sentence structure-based features. In Proceedings of ACL Student Session, pages 81–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
</authors>
<title>Learning deep architectures for AI. Foundations and Trends</title>
<date>2009</date>
<booktitle>in Machine Learning,</booktitle>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="4980" citStr="Bengio, 2009" startWordPosition="767" endWordPosition="768">ion features to improve classification, borrowing a few from text classification in general (e.g., ngrams), and perhaps others from sentiment analysis problems (e.g., the polarity lexicon from Wilson et al. (2005)). We would like to do as little feature engineering as possible to ensure that the features we use are meaningful across domains. However, we do still want features that somehow capture the inherent positivity or negativity of our labeled instances, i.e., citations or Amazon product reviews. Currently a popular approach for accomplishing this is to use deep learning neural networks (Bengio, 2009), which have been shown to perform well on a variety of NLP tasks using only bag-of-word features (Collobert et al., 2011). More specifically related to our work, deep learning neural networks have been successfully employed for sentiment analysis (Socher et al., 2011) and for sentiment domain adaptation (Glorot et al., 2011). In this paper we examine one of these approaches, marginalized stacked denoising autoencoders (mSDA) from Chen et al. (2012), which has been successful in classifying the polarity of Amazon product reviews across product domains. Since mSDA achieved state-of-the-art perf</context>
</contexts>
<marker>Bengio, 2009</marker>
<rawString>Yoshua Bengio. 2009. Learning deep architectures for AI. Foundations and Trends in Machine Learning, 2(1):1–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Robert Dale</author>
<author>Bonnie Dorr</author>
<author>Bryan Gibson</author>
<author>Mark Joseph</author>
<author>Min-Yen Kan</author>
<author>Dongwon Lee</author>
<author>Brett Powley</author>
<author>Dragomir Radev</author>
<author>Yee Fan Tan</author>
</authors>
<title>The ACL anthology reference corpus: A reference dataset for bibliographic research in computational linguistics.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>1755--1759</pages>
<contexts>
<context position="7451" citStr="Bird et al., 2008" startWordPosition="1156" endWordPosition="1159">ast majority, 1521, are left neutral. The second citation corpus, the IMS Citation Corpus3 contains 2008 annotated citations: 1836 are labeled positive and 172 are labeled negative. Jochim and Sch¨utze (2012) use annotation labels from Moravcsik and Murugesan (1975) where positive instances are labeled confirmative, negative instances are labeled negational, and there is no neutral class. Because each of the citation corpora is of modest size we combine them to form one citation dataset, which we will refer to as CITD. The two citation corpora comprising CITD both come from the ACL Anthology (Bird et al., 2008): the IMS corpus uses the ACL proceedings from 2004 and the DFKI corpus uses parts of the proceedings from 2007 and 2008. Since mSDA also makes use of large amounts of unlabeled data, we extend our CITD corpus with citations from the proceedings of the remaining years of the ACL, 1979–2003, 2005–2006, and 2009. There are a number of non-citation corpora available that contain polarity annotation. For these experiments we use the Multi-Domain Sentiment Dataset4 (henceforth MDSD), introduced by Blitzer et al. (2007). We use the version of the MDSD that includes positive and negative labels for p</context>
</contexts>
<marker>Bird, Dale, Dorr, Gibson, Joseph, Kan, Lee, Powley, Radev, Tan, 2008</marker>
<rawString>Steven Bird, Robert Dale, Bonnie Dorr, Bryan Gibson, Mark Joseph, Min-Yen Kan, Dongwon Lee, Brett Powley, Dragomir Radev, and Yee Fan Tan. 2008. The ACL anthology reference corpus: A reference dataset for bibliographic research in computational linguistics. In Proceedings of LREC, pages 1755– 1759.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, Bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>440--447</pages>
<contexts>
<context position="7970" citStr="Blitzer et al. (2007)" startWordPosition="1240" endWordPosition="1243">as CITD. The two citation corpora comprising CITD both come from the ACL Anthology (Bird et al., 2008): the IMS corpus uses the ACL proceedings from 2004 and the DFKI corpus uses parts of the proceedings from 2007 and 2008. Since mSDA also makes use of large amounts of unlabeled data, we extend our CITD corpus with citations from the proceedings of the remaining years of the ACL, 1979–2003, 2005–2006, and 2009. There are a number of non-citation corpora available that contain polarity annotation. For these experiments we use the Multi-Domain Sentiment Dataset4 (henceforth MDSD), introduced by Blitzer et al. (2007). We use the version of the MDSD that includes positive and negative labels for product reviews taken from Amazon.com in the following domains: books, dvd, electronics, and kitchen. For each domain there are 1000 positive reviews and 1000 negative reviews that comprise the “labeled” data, and then roughly 4000 more reviews in the “unlabeled”5 data. Reviews 2https://aclbib.opendfki.de/repos/ trunk/citation_classification_dataset/ 3http://www.ims.uni-stuttgart.de/ ˜jochimcs/citation-classification/ 4http://www.cs.jhu.edu/˜mdredze/ datasets/sentiment/ 5It is usually treated as unlabeled data even</context>
<context position="10353" citStr="Blitzer et al., 2007" startWordPosition="1616" endWordPosition="1619">ntences in the DFKI and IMS corpora and randomly selecting an equal number of positive sentences. The IMS corpus can have multiple labeled citations per sentence: there are 122 sentences containing the 172 negative citations from Table 1. The final CITD corpus comprises this balanced corpus of 358 labeled citation sentences plus another 22,093 unlabeled citation sentences. 3.2 Features In our experiments, we restrict our features to unigrams and bigrams from the product review or citation context (i.e., the sentence containing the citation). This follows previous studies in domain adaptation (Blitzer et al., 2007; Glorot et al., 2011). Chen et al. (2012) achieve state-of-the-art results on MDSD by testing the 5000 and 30,000 most frequent unigram and bigram features. Previous work in citation classification has largely focused on identifying new features for tually contains positive and negative labels, which have been used, e.g., in (Chen et al., 2012). improving classification accuracy. A significant amount of effort goes into engineering new features, in particular for identifying cue phrases, e.g., (Teufel et al., 2006b; Dong and Sch¨afer, 2011). However, there seems to be little consensus on whic</context>
<context position="13361" citStr="Blitzer et al. (2007)" startWordPosition="2091" endWordPosition="2094"> Chen et al. (2012) (and others who have used MDSD) by adding another domain, citations. We train on each of the domains from the MDSD – 6We use their MATLAB implementation available at http://www.cse.wustl.edu/˜mchen/code/ mSDA.tar. 44 0.4 0.3 0.2 0.1 0.0 books dvd electronics kitchen Figure 1: Cross domain macro-F1 results training on Multi-Domain Sentiment Dataset and testing on citation dataset (CITD). The horizontal line indicates macro-F1 for in-domain citation classification. books, dvd, electronics, and kitchen – and test on the citation data. We split the labeled data 80/20 following Blitzer et al. (2007) (cf. Chen et al. (2012) train on all “labeled” data and test on the “unlabeled” data). These experiments should help answer two questions: does a larger amount of training data, even if out of domain, improve citation classification; and how well do the different product domains generalize to citations (i.e., which domains are most similar to citations)? In contrast to previous work using MDSD, a lot of the work in domain adaptation also leverages a small amount of labeled target data. In our second set of experiments, we follow the domain adaptation approaches described in (Daum´e III, 2007)</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, Bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In Proceedings of ACL, pages 440–447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciprian Chelba</author>
<author>Alex Acero</author>
</authors>
<title>Adaptation of maximum entropy capitalizer: Little data can help a lot.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>285--292</pages>
<contexts>
<context position="16058" citStr="Chelba and Acero, 2004" startWordPosition="2536" endWordPosition="2539">ell on citations, and intuitively, among the domains in the Amazon dataset, a book review is most similar to a scientific citation. This makes the good mSDA results for electronics a bit more surprising. 4.2 Easy Domain Adaptation The results in Section 4.1 are for semi-supervised domain adaptation: the case where we have some large annotated corpus (Amazon product reviews) and a large unannotated corpus (citations). There have been a number of other successful attempts at fully supervised domain adaptation, where it is assumed that some small amount of data is annotated in the target domain (Chelba and Acero, 2004; Daum´e III, 2007; Jiang and Zhai, 2007). To see how mSDA compares to supervised domain adaptation we take the various approaches presented by Daum´e III (2007). The results of this comparison can be seen in Table 2. Briefly, “All” trains on source and target data; “Weight” is the same as “All” except that instances may be weighted differently based on their domain (weights are chosen on a development set); “Pred” trains on the source data, makes predictions on the target data, and then trains on the target data with the predictions; “LinInt” linearly interpolates predictions using the source</context>
</contexts>
<marker>Chelba, Acero, 2004</marker>
<rawString>Ciprian Chelba and Alex Acero. 2004. Adaptation of maximum entropy capitalizer: Little data can help a lot. In Proceedings of EMNLP, pages 285–292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minmin Chen</author>
<author>Zhixiang Eddie Xu</author>
<author>Kilian Q Weinberger</author>
<author>Fei Sha</author>
</authors>
<title>Marginalized denoising autoencoders for domain adaptation.</title>
<date>2012</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>767--774</pages>
<contexts>
<context position="5433" citStr="Chen et al. (2012)" startWordPosition="838" endWordPosition="841">eled instances, i.e., citations or Amazon product reviews. Currently a popular approach for accomplishing this is to use deep learning neural networks (Bengio, 2009), which have been shown to perform well on a variety of NLP tasks using only bag-of-word features (Collobert et al., 2011). More specifically related to our work, deep learning neural networks have been successfully employed for sentiment analysis (Socher et al., 2011) and for sentiment domain adaptation (Glorot et al., 2011). In this paper we examine one of these approaches, marginalized stacked denoising autoencoders (mSDA) from Chen et al. (2012), which has been successful in classifying the polarity of Amazon product reviews across product domains. Since mSDA achieved state-of-the-art performance in Amazon product domain adaptation, we are hopeful it will also be effective when switching to a more distant domain like scientific citations. 3 Experimental Setup 3.1 Corpora We are interested in domain adaptation for citation classification and therefore need a target dataset of citations and a non-citation source dataset. There are two corpora available that contain citation function annotation, the DFKI Citation Corpus (Dong and Sch¨af</context>
<context position="10395" citStr="Chen et al. (2012)" startWordPosition="1624" endWordPosition="1627">omly selecting an equal number of positive sentences. The IMS corpus can have multiple labeled citations per sentence: there are 122 sentences containing the 172 negative citations from Table 1. The final CITD corpus comprises this balanced corpus of 358 labeled citation sentences plus another 22,093 unlabeled citation sentences. 3.2 Features In our experiments, we restrict our features to unigrams and bigrams from the product review or citation context (i.e., the sentence containing the citation). This follows previous studies in domain adaptation (Blitzer et al., 2007; Glorot et al., 2011). Chen et al. (2012) achieve state-of-the-art results on MDSD by testing the 5000 and 30,000 most frequent unigram and bigram features. Previous work in citation classification has largely focused on identifying new features for tually contains positive and negative labels, which have been used, e.g., in (Chen et al., 2012). improving classification accuracy. A significant amount of effort goes into engineering new features, in particular for identifying cue phrases, e.g., (Teufel et al., 2006b; Dong and Sch¨afer, 2011). However, there seems to be little consensus on which features help most for this task. For ex</context>
<context position="12038" citStr="Chen et al. (2012)" startWordPosition="1880" endWordPosition="1883">ures varies between these studies, but the problem remains that there seems to be no clear set of features for citation polarity classification. The lack of consensus on the most useful citation polarity features coupled with the recent success of deep learning neural networks (Collobert et al., 2011) further motivate our choice to limit our features to the n-grams available in the product review or citation context and not rely on external resources or tools for additional features. 3.3 Classification with mSDA For classification we use marginalized stacked denoising autoencoders (mSDA) from Chen et al. (2012)6 plus a linear SVM. mSDA takes the concept of denoising – introducing noise to make the autoencoder more robust – from Vincent et al. (2008), but does the optimization in closed form, thereby avoiding iterating over the input vector to stochastically introduce noise. The result of this is faster run times and currently state-of-the-art performance on MDSD, which makes it a good choice for our domain adaptation task. The mSDA implementation comes with LIBSVM, which we replace with LIBLINEAR (Fan et al., 2008) for faster run times with no decrease in accuracy. LIBLINEAR, with default settings, </context>
<context position="13385" citStr="Chen et al. (2012)" startWordPosition="2096" endWordPosition="2099">hers who have used MDSD) by adding another domain, citations. We train on each of the domains from the MDSD – 6We use their MATLAB implementation available at http://www.cse.wustl.edu/˜mchen/code/ mSDA.tar. 44 0.4 0.3 0.2 0.1 0.0 books dvd electronics kitchen Figure 1: Cross domain macro-F1 results training on Multi-Domain Sentiment Dataset and testing on citation dataset (CITD). The horizontal line indicates macro-F1 for in-domain citation classification. books, dvd, electronics, and kitchen – and test on the citation data. We split the labeled data 80/20 following Blitzer et al. (2007) (cf. Chen et al. (2012) train on all “labeled” data and test on the “unlabeled” data). These experiments should help answer two questions: does a larger amount of training data, even if out of domain, improve citation classification; and how well do the different product domains generalize to citations (i.e., which domains are most similar to citations)? In contrast to previous work using MDSD, a lot of the work in domain adaptation also leverages a small amount of labeled target data. In our second set of experiments, we follow the domain adaptation approaches described in (Daum´e III, 2007) and train on product re</context>
</contexts>
<marker>Chen, Xu, Weinberger, Sha, 2012</marker>
<rawString>Minmin Chen, Zhixiang Eddie Xu, Kilian Q. Weinberger, and Fei Sha. 2012. Marginalized denoising autoencoders for domain adaptation. In Proceedings of ICML, pages 767–774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daryl E Chubin</author>
<author>Soumyo D Moitra</author>
</authors>
<title>Content analysis of references: Adjunct or alternative to citation counting?</title>
<date>1975</date>
<journal>Social Studies of Science,</journal>
<volume>5</volume>
<pages>441</pages>
<contexts>
<context position="2318" citStr="Chubin and Moitra, 1975" startWordPosition="346" endWordPosition="349">ieval (IR) ∗ This work was primarily conducted at the IMS – University of Stuttgart. (Ritchie et al., 2008), summarization (Qazvinian and Radev, 2008), and even identifying scientific breakthroughs (Small and Klavans, 2011). We expect that by identifying and labeling the function of citations we can improve the effectiveness of these applications. There has been no consensus on what aspects or functions of a citation should be annotated and how. Early citation classification focused more on citation motivation (Garfield, 1964), while later classification considered more the citation function (Chubin and Moitra, 1975). Recent studies using automatic classification have continued this tradition of introducing a new classification scheme with each new investigation into the use of citations (Nanba and Okumura, 1999; Teufel et al., 2006a; Dong and Sch¨afer, 2011; Abu-Jbara et al., 2013). One distinction that has been more consistently annotated across recent citation classification studies is between positive and negative citations (Athar, 2011; Athar and Teufel, 2012; Abu-Jbara et al., 2013).1 The popularity of this distinction likely owes to the prominence of sentiment analysis in NLP (Liu, 2010). We follow</context>
</contexts>
<marker>Chubin, Moitra, 1975</marker>
<rawString>Daryl E. Chubin and Soumyo D. Moitra. 1975. Content analysis of references: Adjunct or alternative to citation counting? Social Studies of Science, 5:423– 441.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel P Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="5102" citStr="Collobert et al., 2011" startWordPosition="787" endWordPosition="790">perhaps others from sentiment analysis problems (e.g., the polarity lexicon from Wilson et al. (2005)). We would like to do as little feature engineering as possible to ensure that the features we use are meaningful across domains. However, we do still want features that somehow capture the inherent positivity or negativity of our labeled instances, i.e., citations or Amazon product reviews. Currently a popular approach for accomplishing this is to use deep learning neural networks (Bengio, 2009), which have been shown to perform well on a variety of NLP tasks using only bag-of-word features (Collobert et al., 2011). More specifically related to our work, deep learning neural networks have been successfully employed for sentiment analysis (Socher et al., 2011) and for sentiment domain adaptation (Glorot et al., 2011). In this paper we examine one of these approaches, marginalized stacked denoising autoencoders (mSDA) from Chen et al. (2012), which has been successful in classifying the polarity of Amazon product reviews across product domains. Since mSDA achieved state-of-the-art performance in Amazon product domain adaptation, we are hopeful it will also be effective when switching to a more distant dom</context>
<context position="11722" citStr="Collobert et al., 2011" startWordPosition="1830" endWordPosition="1833">t al. (2005) to be useful, and neither study lists dependency relations as significant features. Athar (2011) on the other hand reported significant improvement using dependency relation features and found that the same list of polar words slightly hurt classification accuracy. The classifiers and implementation of features varies between these studies, but the problem remains that there seems to be no clear set of features for citation polarity classification. The lack of consensus on the most useful citation polarity features coupled with the recent success of deep learning neural networks (Collobert et al., 2011) further motivate our choice to limit our features to the n-grams available in the product review or citation context and not rely on external resources or tools for additional features. 3.3 Classification with mSDA For classification we use marginalized stacked denoising autoencoders (mSDA) from Chen et al. (2012)6 plus a linear SVM. mSDA takes the concept of denoising – introducing noise to make the autoencoder more robust – from Vincent et al. (2008), but does the optimization in closed form, thereby avoiding iterating over the input vector to stochastically introduce noise. The result of t</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel P. Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenyuan Dai</author>
<author>Gui-Rong Xue</author>
<author>Qiang Yang</author>
<author>Yong Yu</author>
</authors>
<title>Transferring naive bayes classifiers for text classification.</title>
<date>2007</date>
<booktitle>In AAAI,</booktitle>
<pages>540--545</pages>
<contexts>
<context position="15197" citStr="Dai et al. (2007)" startWordPosition="2398" endWordPosition="2401">omain citation classification, which sometimes represents the goal for domain adaptation. We can see that using a larger dataset, even if out of domain, does improve citation classification. For books, dvd, and electronics, even the SVM baseline improves on in-domain classification. mSDA does better than the baseline for all domains except dvd. Using a larger training set, along with mSDA, which makes use of the unlabeled data, leads to the best results for citation classification. In domain adaptation we would expect the domains most similar to the target to lead to the highest results. Like Dai et al. (2007), we measure the Kullback-Leibler divergence between the source and target domains’ distributions. According to this measure, citations are most similar to the books domain. Therefore, it is not surprising that training on books performs well on citations, and intuitively, among the domains in the Amazon dataset, a book review is most similar to a scientific citation. This makes the good mSDA results for electronics a bit more surprising. 4.2 Easy Domain Adaptation The results in Section 4.1 are for semi-supervised domain adaptation: the case where we have some large annotated corpus (Amazon p</context>
</contexts>
<marker>Dai, Xue, Yang, Yu, 2007</marker>
<rawString>Wenyuan Dai, Gui-Rong Xue, Qiang Yang, and Yong Yu. 2007. Transferring naive bayes classifiers for text classification. In AAAI, pages 540–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>256--263</pages>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e III. 2007. Frustratingly easy domain adaptation. In Proceedings of ACL, pages 256–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cailing Dong</author>
<author>Ulrich Sch¨afer</author>
</authors>
<title>Ensemblestyle self-training on citation classification.</title>
<date>2011</date>
<booktitle>In Proceedings of IJCNLP,</booktitle>
<pages>623--631</pages>
<marker>Dong, Sch¨afer, 2011</marker>
<rawString>Cailing Dong and Ulrich Sch¨afer. 2011. Ensemblestyle self-training on citation classification. In Proceedings of IJCNLP, pages 623–631.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="12552" citStr="Fan et al., 2008" startWordPosition="1964" endWordPosition="1967">DA For classification we use marginalized stacked denoising autoencoders (mSDA) from Chen et al. (2012)6 plus a linear SVM. mSDA takes the concept of denoising – introducing noise to make the autoencoder more robust – from Vincent et al. (2008), but does the optimization in closed form, thereby avoiding iterating over the input vector to stochastically introduce noise. The result of this is faster run times and currently state-of-the-art performance on MDSD, which makes it a good choice for our domain adaptation task. The mSDA implementation comes with LIBSVM, which we replace with LIBLINEAR (Fan et al., 2008) for faster run times with no decrease in accuracy. LIBLINEAR, with default settings, also serves as our baseline. 3.4 Outline of Experiments Our initial experiments simply extend those of Chen et al. (2012) (and others who have used MDSD) by adding another domain, citations. We train on each of the domains from the MDSD – 6We use their MATLAB implementation available at http://www.cse.wustl.edu/˜mchen/code/ mSDA.tar. 44 0.4 0.3 0.2 0.1 0.0 books dvd electronics kitchen Figure 1: Cross domain macro-F1 results training on Multi-Domain Sentiment Dataset and testing on citation dataset (CITD). Th</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Garfield</author>
</authors>
<title>Citation indexes to science: A new dimension in documentation through association of ideas.</title>
<date>1955</date>
<journal>Science,</journal>
<pages>122--108</pages>
<contexts>
<context position="1404" citStr="Garfield, 1955" startWordPosition="205" endWordPosition="206">aper we address these two drawbacks. First, we frame citation classification as a domain adaptation task and leverage the abundant labeled data available in other domains. Then, to avoid over-engineering specific citation features for a particular scientific domain, we explore a deep learning neural network approach that has shown to generalize well across domains using unigram and bigram features. We achieve better citation classification results with this cross-domain approach than using in-domain classification. 1 Introduction Citations have been categorized and studied for a half-century (Garfield, 1955) to better understand when and how citations are used, and to record and measure how information is exchanged (e.g., networks of co-cited papers or authors (Small and Griffith, 1974)). Recently, the value of this information has been shown in practical applications such as information retrieval (IR) ∗ This work was primarily conducted at the IMS – University of Stuttgart. (Ritchie et al., 2008), summarization (Qazvinian and Radev, 2008), and even identifying scientific breakthroughs (Small and Klavans, 2011). We expect that by identifying and labeling the function of citations we can improve t</context>
</contexts>
<marker>Garfield, 1955</marker>
<rawString>Eugene Garfield. 1955. Citation indexes to science: A new dimension in documentation through association of ideas. Science, 122:108–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Garfield</author>
</authors>
<title>Can citation indexing be automated?</title>
<date>1964</date>
<booktitle>In Statistical Association Methods for Mechanized Documentation, Symposium Proceedings,</booktitle>
<pages>189--192</pages>
<contexts>
<context position="2226" citStr="Garfield, 1964" startWordPosition="335" endWordPosition="336"> this information has been shown in practical applications such as information retrieval (IR) ∗ This work was primarily conducted at the IMS – University of Stuttgart. (Ritchie et al., 2008), summarization (Qazvinian and Radev, 2008), and even identifying scientific breakthroughs (Small and Klavans, 2011). We expect that by identifying and labeling the function of citations we can improve the effectiveness of these applications. There has been no consensus on what aspects or functions of a citation should be annotated and how. Early citation classification focused more on citation motivation (Garfield, 1964), while later classification considered more the citation function (Chubin and Moitra, 1975). Recent studies using automatic classification have continued this tradition of introducing a new classification scheme with each new investigation into the use of citations (Nanba and Okumura, 1999; Teufel et al., 2006a; Dong and Sch¨afer, 2011; Abu-Jbara et al., 2013). One distinction that has been more consistently annotated across recent citation classification studies is between positive and negative citations (Athar, 2011; Athar and Teufel, 2012; Abu-Jbara et al., 2013).1 The popularity of this d</context>
</contexts>
<marker>Garfield, 1964</marker>
<rawString>Eugene Garfield. 1964. Can citation indexing be automated? In Statistical Association Methods for Mechanized Documentation, Symposium Proceedings, pages 189–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Glorot</author>
<author>Antoine Bordes</author>
<author>Yoshua Bengio</author>
</authors>
<title>Domain adaptation for large-scale sentiment classification: A deep learning approach.</title>
<date>2011</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>513--520</pages>
<contexts>
<context position="5307" citStr="Glorot et al., 2011" startWordPosition="817" endWordPosition="821">ningful across domains. However, we do still want features that somehow capture the inherent positivity or negativity of our labeled instances, i.e., citations or Amazon product reviews. Currently a popular approach for accomplishing this is to use deep learning neural networks (Bengio, 2009), which have been shown to perform well on a variety of NLP tasks using only bag-of-word features (Collobert et al., 2011). More specifically related to our work, deep learning neural networks have been successfully employed for sentiment analysis (Socher et al., 2011) and for sentiment domain adaptation (Glorot et al., 2011). In this paper we examine one of these approaches, marginalized stacked denoising autoencoders (mSDA) from Chen et al. (2012), which has been successful in classifying the polarity of Amazon product reviews across product domains. Since mSDA achieved state-of-the-art performance in Amazon product domain adaptation, we are hopeful it will also be effective when switching to a more distant domain like scientific citations. 3 Experimental Setup 3.1 Corpora We are interested in domain adaptation for citation classification and therefore need a target dataset of citations and a non-citation source</context>
<context position="10375" citStr="Glorot et al., 2011" startWordPosition="1620" endWordPosition="1623">d IMS corpora and randomly selecting an equal number of positive sentences. The IMS corpus can have multiple labeled citations per sentence: there are 122 sentences containing the 172 negative citations from Table 1. The final CITD corpus comprises this balanced corpus of 358 labeled citation sentences plus another 22,093 unlabeled citation sentences. 3.2 Features In our experiments, we restrict our features to unigrams and bigrams from the product review or citation context (i.e., the sentence containing the citation). This follows previous studies in domain adaptation (Blitzer et al., 2007; Glorot et al., 2011). Chen et al. (2012) achieve state-of-the-art results on MDSD by testing the 5000 and 30,000 most frequent unigram and bigram features. Previous work in citation classification has largely focused on identifying new features for tually contains positive and negative labels, which have been used, e.g., in (Chen et al., 2012). improving classification accuracy. A significant amount of effort goes into engineering new features, in particular for identifying cue phrases, e.g., (Teufel et al., 2006b; Dong and Sch¨afer, 2011). However, there seems to be little consensus on which features help most f</context>
</contexts>
<marker>Glorot, Bordes, Bengio, 2011</marker>
<rawString>Xavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Domain adaptation for large-scale sentiment classification: A deep learning approach. In Proceedings of ICML, pages 513–520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Instance weighting for domain adaptation in NLP.</title>
<date>2007</date>
<booktitle>In ACL,</booktitle>
<pages>264--271</pages>
<contexts>
<context position="16099" citStr="Jiang and Zhai, 2007" startWordPosition="2543" endWordPosition="2546">he domains in the Amazon dataset, a book review is most similar to a scientific citation. This makes the good mSDA results for electronics a bit more surprising. 4.2 Easy Domain Adaptation The results in Section 4.1 are for semi-supervised domain adaptation: the case where we have some large annotated corpus (Amazon product reviews) and a large unannotated corpus (citations). There have been a number of other successful attempts at fully supervised domain adaptation, where it is assumed that some small amount of data is annotated in the target domain (Chelba and Acero, 2004; Daum´e III, 2007; Jiang and Zhai, 2007). To see how mSDA compares to supervised domain adaptation we take the various approaches presented by Daum´e III (2007). The results of this comparison can be seen in Table 2. Briefly, “All” trains on source and target data; “Weight” is the same as “All” except that instances may be weighted differently based on their domain (weights are chosen on a development set); “Pred” trains on the source data, makes predictions on the target data, and then trains on the target data with the predictions; “LinInt” linearly interpolates predictions using the source-only and target-only models (the interpo</context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in NLP. In ACL, pages 264–271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Jochim</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Towards a generic and flexible citation classifier based on a faceted classification scheme.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>1343--1358</pages>
<marker>Jochim, Sch¨utze, 2012</marker>
<rawString>Charles Jochim and Hinrich Sch¨utze. 2012. Towards a generic and flexible citation classifier based on a faceted classification scheme. In Proceedings of COLING, pages 1343–1358.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment analysis and subjectivity.</title>
<date>2010</date>
<booktitle>In Nitin Indurkhya</booktitle>
<editor>and Fred J. Damerau, editors,</editor>
<publisher>CRC Press, Taylor and Francis Group.</publisher>
<contexts>
<context position="2907" citStr="Liu, 2010" startWordPosition="437" endWordPosition="438">bin and Moitra, 1975). Recent studies using automatic classification have continued this tradition of introducing a new classification scheme with each new investigation into the use of citations (Nanba and Okumura, 1999; Teufel et al., 2006a; Dong and Sch¨afer, 2011; Abu-Jbara et al., 2013). One distinction that has been more consistently annotated across recent citation classification studies is between positive and negative citations (Athar, 2011; Athar and Teufel, 2012; Abu-Jbara et al., 2013).1 The popularity of this distinction likely owes to the prominence of sentiment analysis in NLP (Liu, 2010). We follow much of the recent work on citation classification and concentrate on citation polarity. 2 Domain Adaptation By concentrating on citation polarity we are able to compare our classification to previous citation polarity work. This choice also allows us to access the wealth of existing data containing polarity annotation and then frame the task as a domain adaptation problem. Of course the risk in approaching the problem as domain adaptation is that the domains are so different that the representation of a positive instance of a movie or product review, for example, will not coincide</context>
</contexts>
<marker>Liu, 2010</marker>
<rawString>Bing Liu. 2010. Sentiment analysis and subjectivity. In Nitin Indurkhya and Fred J. Damerau, editors, Handbook of Natural Language Processing, Second Edition. CRC Press, Taylor and Francis Group.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Moravcsik</author>
<author>Poovanalingam Murugesan</author>
</authors>
<title>Some results on the function and quality of citations.</title>
<date>1975</date>
<journal>Social Studies of Science,</journal>
<pages>5--86</pages>
<contexts>
<context position="7099" citStr="Moravcsik and Murugesan (1975)" startWordPosition="1095" endWordPosition="1098">number of negative instances in a single domain in the multi-domain corpus described below. The DFKI Citation Corpus2 has been used for classifying citation function (Dong and Sch¨afer, 2011), but the dataset also includes polarity annotation. The dataset has 1768 citation sentences with polarity annotation: 190 are labeled as positive, 57 as negative, and the vast majority, 1521, are left neutral. The second citation corpus, the IMS Citation Corpus3 contains 2008 annotated citations: 1836 are labeled positive and 172 are labeled negative. Jochim and Sch¨utze (2012) use annotation labels from Moravcsik and Murugesan (1975) where positive instances are labeled confirmative, negative instances are labeled negational, and there is no neutral class. Because each of the citation corpora is of modest size we combine them to form one citation dataset, which we will refer to as CITD. The two citation corpora comprising CITD both come from the ACL Anthology (Bird et al., 2008): the IMS corpus uses the ACL proceedings from 2004 and the DFKI corpus uses parts of the proceedings from 2007 and 2008. Since mSDA also makes use of large amounts of unlabeled data, we extend our CITD corpus with citations from the proceedings of</context>
</contexts>
<marker>Moravcsik, Murugesan, 1975</marker>
<rawString>Michael J. Moravcsik and Poovanalingam Murugesan. 1975. Some results on the function and quality of citations. Social Studies of Science, 5:86–92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hidetsugu Nanba</author>
<author>Manabu Okumura</author>
</authors>
<title>Towards multi-paper summarization using reference information.</title>
<date>1999</date>
<booktitle>In Proceedings of IJCAI,</booktitle>
<pages>926--931</pages>
<contexts>
<context position="2517" citStr="Nanba and Okumura, 1999" startWordPosition="375" endWordPosition="378">mall and Klavans, 2011). We expect that by identifying and labeling the function of citations we can improve the effectiveness of these applications. There has been no consensus on what aspects or functions of a citation should be annotated and how. Early citation classification focused more on citation motivation (Garfield, 1964), while later classification considered more the citation function (Chubin and Moitra, 1975). Recent studies using automatic classification have continued this tradition of introducing a new classification scheme with each new investigation into the use of citations (Nanba and Okumura, 1999; Teufel et al., 2006a; Dong and Sch¨afer, 2011; Abu-Jbara et al., 2013). One distinction that has been more consistently annotated across recent citation classification studies is between positive and negative citations (Athar, 2011; Athar and Teufel, 2012; Abu-Jbara et al., 2013).1 The popularity of this distinction likely owes to the prominence of sentiment analysis in NLP (Liu, 2010). We follow much of the recent work on citation classification and concentrate on citation polarity. 2 Domain Adaptation By concentrating on citation polarity we are able to compare our classification to previo</context>
</contexts>
<marker>Nanba, Okumura, 1999</marker>
<rawString>Hidetsugu Nanba and Manabu Okumura. 1999. Towards multi-paper summarization using reference information. In Proceedings of IJCAI, pages 926– 931.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="9347" citStr="Pang et al., 2002" startWordPosition="1450" endWordPosition="1453">ssed so that for each review you find a list of unigrams and bigrams with their frequency within the review. Unigrams from a stop list of 55 stop words are removed, but stop words in bigrams remain. Table 1 shows the distribution of polarity labels in the corpora we use for our experiments. We combine the DFKI and IMS corpora into the CITD corpus. We omit the citations labeled neutral from the DFKI corpus because the IMS corpus does not contain neutral annotation nor does the MDSD. It is the case in many sentiment analysis corpora that only positive and negative instances are included, e.g., (Pang et al., 2002). The citation corpora presented above are both unbalanced and both have a highly skewed distribution. The MDSD on the other hand is evenly balanced and an effort was even made to keep the data treated as “unlabeled” rather balanced. For this reason, in line with previous work using MDSD, we balance the labeled portion of the CITD corpus. This is done by taking 179 unique negative sentences in the DFKI and IMS corpora and randomly selecting an equal number of positive sentences. The IMS corpus can have multiple labeled citations per sentence: there are 122 sentences containing the 172 negative</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? sentiment classification using machine learning techniques. In Proceedings of EMNLP, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vahed Qazvinian</author>
<author>Dragomir R Radev</author>
</authors>
<title>Scientific paper summarization using citation summary networks.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>689--696</pages>
<contexts>
<context position="1844" citStr="Qazvinian and Radev, 2008" startWordPosition="276" endWordPosition="279">classification results with this cross-domain approach than using in-domain classification. 1 Introduction Citations have been categorized and studied for a half-century (Garfield, 1955) to better understand when and how citations are used, and to record and measure how information is exchanged (e.g., networks of co-cited papers or authors (Small and Griffith, 1974)). Recently, the value of this information has been shown in practical applications such as information retrieval (IR) ∗ This work was primarily conducted at the IMS – University of Stuttgart. (Ritchie et al., 2008), summarization (Qazvinian and Radev, 2008), and even identifying scientific breakthroughs (Small and Klavans, 2011). We expect that by identifying and labeling the function of citations we can improve the effectiveness of these applications. There has been no consensus on what aspects or functions of a citation should be annotated and how. Early citation classification focused more on citation motivation (Garfield, 1964), while later classification considered more the citation function (Chubin and Moitra, 1975). Recent studies using automatic classification have continued this tradition of introducing a new classification scheme with </context>
</contexts>
<marker>Qazvinian, Radev, 2008</marker>
<rawString>Vahed Qazvinian and Dragomir R. Radev. 2008. Scientific paper summarization using citation summary networks. In Proceedings of COLING, pages 689– 696.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Ritchie</author>
<author>Stephen Robertson</author>
<author>Simone Teufel</author>
</authors>
<title>Comparing citation contexts for information retrieval.</title>
<date>2008</date>
<booktitle>In Proceedings of CIKM,</booktitle>
<pages>213--222</pages>
<contexts>
<context position="1801" citStr="Ritchie et al., 2008" startWordPosition="271" endWordPosition="274"> features. We achieve better citation classification results with this cross-domain approach than using in-domain classification. 1 Introduction Citations have been categorized and studied for a half-century (Garfield, 1955) to better understand when and how citations are used, and to record and measure how information is exchanged (e.g., networks of co-cited papers or authors (Small and Griffith, 1974)). Recently, the value of this information has been shown in practical applications such as information retrieval (IR) ∗ This work was primarily conducted at the IMS – University of Stuttgart. (Ritchie et al., 2008), summarization (Qazvinian and Radev, 2008), and even identifying scientific breakthroughs (Small and Klavans, 2011). We expect that by identifying and labeling the function of citations we can improve the effectiveness of these applications. There has been no consensus on what aspects or functions of a citation should be annotated and how. Early citation classification focused more on citation motivation (Garfield, 1964), while later classification considered more the citation function (Chubin and Moitra, 1975). Recent studies using automatic classification have continued this tradition of in</context>
</contexts>
<marker>Ritchie, Robertson, Teufel, 2008</marker>
<rawString>Anna Ritchie, Stephen Robertson, and Simone Teufel. 2008. Comparing citation contexts for information retrieval. In Proceedings of CIKM, pages 213–222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henry G Small</author>
<author>Belver C Griffith</author>
</authors>
<title>The structure of scientific literatures I: Identifying and graphing specialties.</title>
<date>1974</date>
<journal>Science Studies,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="1586" citStr="Small and Griffith, 1974" startWordPosition="235" endWordPosition="238">Then, to avoid over-engineering specific citation features for a particular scientific domain, we explore a deep learning neural network approach that has shown to generalize well across domains using unigram and bigram features. We achieve better citation classification results with this cross-domain approach than using in-domain classification. 1 Introduction Citations have been categorized and studied for a half-century (Garfield, 1955) to better understand when and how citations are used, and to record and measure how information is exchanged (e.g., networks of co-cited papers or authors (Small and Griffith, 1974)). Recently, the value of this information has been shown in practical applications such as information retrieval (IR) ∗ This work was primarily conducted at the IMS – University of Stuttgart. (Ritchie et al., 2008), summarization (Qazvinian and Radev, 2008), and even identifying scientific breakthroughs (Small and Klavans, 2011). We expect that by identifying and labeling the function of citations we can improve the effectiveness of these applications. There has been no consensus on what aspects or functions of a citation should be annotated and how. Early citation classification focused more</context>
</contexts>
<marker>Small, Griffith, 1974</marker>
<rawString>Henry G. Small and Belver C. Griffith. 1974. The structure of scientific literatures I: Identifying and graphing specialties. Science Studies, 4(1):17–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henry Small</author>
<author>Richard Klavans</author>
</authors>
<title>Identifying scientific breakthroughs by combining co-citation analysis and citation context.</title>
<date>2011</date>
<booktitle>In Proceedings of International Society for Scientometrics and Informetrics.</booktitle>
<contexts>
<context position="1917" citStr="Small and Klavans, 2011" startWordPosition="285" endWordPosition="288"> classification. 1 Introduction Citations have been categorized and studied for a half-century (Garfield, 1955) to better understand when and how citations are used, and to record and measure how information is exchanged (e.g., networks of co-cited papers or authors (Small and Griffith, 1974)). Recently, the value of this information has been shown in practical applications such as information retrieval (IR) ∗ This work was primarily conducted at the IMS – University of Stuttgart. (Ritchie et al., 2008), summarization (Qazvinian and Radev, 2008), and even identifying scientific breakthroughs (Small and Klavans, 2011). We expect that by identifying and labeling the function of citations we can improve the effectiveness of these applications. There has been no consensus on what aspects or functions of a citation should be annotated and how. Early citation classification focused more on citation motivation (Garfield, 1964), while later classification considered more the citation function (Chubin and Moitra, 1975). Recent studies using automatic classification have continued this tradition of introducing a new classification scheme with each new investigation into the use of citations (Nanba and Okumura, 1999</context>
</contexts>
<marker>Small, Klavans, 2011</marker>
<rawString>Henry Small and Richard Klavans. 2011. Identifying scientific breakthroughs by combining co-citation analysis and citation context. In Proceedings of International Society for Scientometrics and Informetrics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>151--161</pages>
<contexts>
<context position="5249" citStr="Socher et al., 2011" startWordPosition="808" endWordPosition="811">ing as possible to ensure that the features we use are meaningful across domains. However, we do still want features that somehow capture the inherent positivity or negativity of our labeled instances, i.e., citations or Amazon product reviews. Currently a popular approach for accomplishing this is to use deep learning neural networks (Bengio, 2009), which have been shown to perform well on a variety of NLP tasks using only bag-of-word features (Collobert et al., 2011). More specifically related to our work, deep learning neural networks have been successfully employed for sentiment analysis (Socher et al., 2011) and for sentiment domain adaptation (Glorot et al., 2011). In this paper we examine one of these approaches, marginalized stacked denoising autoencoders (mSDA) from Chen et al. (2012), which has been successful in classifying the polarity of Amazon product reviews across product domains. Since mSDA achieved state-of-the-art performance in Amazon product domain adaptation, we are hopeful it will also be effective when switching to a more distant domain like scientific citations. 3 Experimental Setup 3.1 Corpora We are interested in domain adaptation for citation classification and therefore ne</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning. 2011. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of EMNLP, pages 151–161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Advaith Siddharthan</author>
<author>Dan Tidhar</author>
</authors>
<title>An annotation scheme for citation function.</title>
<date>2006</date>
<booktitle>In Proceedings of SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>80--87</pages>
<contexts>
<context position="2538" citStr="Teufel et al., 2006" startWordPosition="379" endWordPosition="382">We expect that by identifying and labeling the function of citations we can improve the effectiveness of these applications. There has been no consensus on what aspects or functions of a citation should be annotated and how. Early citation classification focused more on citation motivation (Garfield, 1964), while later classification considered more the citation function (Chubin and Moitra, 1975). Recent studies using automatic classification have continued this tradition of introducing a new classification scheme with each new investigation into the use of citations (Nanba and Okumura, 1999; Teufel et al., 2006a; Dong and Sch¨afer, 2011; Abu-Jbara et al., 2013). One distinction that has been more consistently annotated across recent citation classification studies is between positive and negative citations (Athar, 2011; Athar and Teufel, 2012; Abu-Jbara et al., 2013).1 The popularity of this distinction likely owes to the prominence of sentiment analysis in NLP (Liu, 2010). We follow much of the recent work on citation classification and concentrate on citation polarity. 2 Domain Adaptation By concentrating on citation polarity we are able to compare our classification to previous citation polarity </context>
<context position="10873" citStr="Teufel et al., 2006" startWordPosition="1695" endWordPosition="1698"> containing the citation). This follows previous studies in domain adaptation (Blitzer et al., 2007; Glorot et al., 2011). Chen et al. (2012) achieve state-of-the-art results on MDSD by testing the 5000 and 30,000 most frequent unigram and bigram features. Previous work in citation classification has largely focused on identifying new features for tually contains positive and negative labels, which have been used, e.g., in (Chen et al., 2012). improving classification accuracy. A significant amount of effort goes into engineering new features, in particular for identifying cue phrases, e.g., (Teufel et al., 2006b; Dong and Sch¨afer, 2011). However, there seems to be little consensus on which features help most for this task. For example, Abu-Jbara et al. (2013) and Jochim and Sch¨utze (2012) find the list of polar words from Wilson et al. (2005) to be useful, and neither study lists dependency relations as significant features. Athar (2011) on the other hand reported significant improvement using dependency relation features and found that the same list of polar words slightly hurt classification accuracy. The classifiers and implementation of features varies between these studies, but the problem re</context>
<context position="19325" citStr="Teufel et al. (2006" startWordPosition="3077" endWordPosition="3080"> features. mSDA and fully supervised approaches can also be straightforwardly combined. We do not present those results here due to space constraints. The 7Their work included a CRF model to identify the citation context that gave them an increase of 9.2 percent Fl over a single sentence citation context. Our approach achieves similar macro-Fl on only the citation sentence, but using a different corpus. combination led to mixed results: adding mSDA to the supervised approaches tended to improve F1 over those approaches but results never exceeded the top mSDA numbers in Table 2. 5 Related Work Teufel et al. (2006b) introduced automatic citation function classification, with classes that could be grouped as positive, negative, and neutral. They relied in part on a manually compiled list of cue phrases that cannot easily be transferred to other classification schemes or other scientific domains. Athar (2011) followed this and was the first to specifically target polarity classification on scientific citations. He found that dependency tuples contributed the most significant improvement in results. Abu-Jbara et al. (2013) also looks at both citation function and citation polarity. A big contribution of t</context>
</contexts>
<marker>Teufel, Siddharthan, Tidhar, 2006</marker>
<rawString>Simone Teufel, Advaith Siddharthan, and Dan Tidhar. 2006a. An annotation scheme for citation function. In Proceedings of SIGdial Workshop on Discourse and Dialogue, pages 80–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Advaith Siddharthan</author>
<author>Dan Tidhar</author>
</authors>
<title>Automatic classification of citation function.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>103--110</pages>
<contexts>
<context position="2538" citStr="Teufel et al., 2006" startWordPosition="379" endWordPosition="382">We expect that by identifying and labeling the function of citations we can improve the effectiveness of these applications. There has been no consensus on what aspects or functions of a citation should be annotated and how. Early citation classification focused more on citation motivation (Garfield, 1964), while later classification considered more the citation function (Chubin and Moitra, 1975). Recent studies using automatic classification have continued this tradition of introducing a new classification scheme with each new investigation into the use of citations (Nanba and Okumura, 1999; Teufel et al., 2006a; Dong and Sch¨afer, 2011; Abu-Jbara et al., 2013). One distinction that has been more consistently annotated across recent citation classification studies is between positive and negative citations (Athar, 2011; Athar and Teufel, 2012; Abu-Jbara et al., 2013).1 The popularity of this distinction likely owes to the prominence of sentiment analysis in NLP (Liu, 2010). We follow much of the recent work on citation classification and concentrate on citation polarity. 2 Domain Adaptation By concentrating on citation polarity we are able to compare our classification to previous citation polarity </context>
<context position="10873" citStr="Teufel et al., 2006" startWordPosition="1695" endWordPosition="1698"> containing the citation). This follows previous studies in domain adaptation (Blitzer et al., 2007; Glorot et al., 2011). Chen et al. (2012) achieve state-of-the-art results on MDSD by testing the 5000 and 30,000 most frequent unigram and bigram features. Previous work in citation classification has largely focused on identifying new features for tually contains positive and negative labels, which have been used, e.g., in (Chen et al., 2012). improving classification accuracy. A significant amount of effort goes into engineering new features, in particular for identifying cue phrases, e.g., (Teufel et al., 2006b; Dong and Sch¨afer, 2011). However, there seems to be little consensus on which features help most for this task. For example, Abu-Jbara et al. (2013) and Jochim and Sch¨utze (2012) find the list of polar words from Wilson et al. (2005) to be useful, and neither study lists dependency relations as significant features. Athar (2011) on the other hand reported significant improvement using dependency relation features and found that the same list of polar words slightly hurt classification accuracy. The classifiers and implementation of features varies between these studies, but the problem re</context>
<context position="19325" citStr="Teufel et al. (2006" startWordPosition="3077" endWordPosition="3080"> features. mSDA and fully supervised approaches can also be straightforwardly combined. We do not present those results here due to space constraints. The 7Their work included a CRF model to identify the citation context that gave them an increase of 9.2 percent Fl over a single sentence citation context. Our approach achieves similar macro-Fl on only the citation sentence, but using a different corpus. combination led to mixed results: adding mSDA to the supervised approaches tended to improve F1 over those approaches but results never exceeded the top mSDA numbers in Table 2. 5 Related Work Teufel et al. (2006b) introduced automatic citation function classification, with classes that could be grouped as positive, negative, and neutral. They relied in part on a manually compiled list of cue phrases that cannot easily be transferred to other classification schemes or other scientific domains. Athar (2011) followed this and was the first to specifically target polarity classification on scientific citations. He found that dependency tuples contributed the most significant improvement in results. Abu-Jbara et al. (2013) also looks at both citation function and citation polarity. A big contribution of t</context>
</contexts>
<marker>Teufel, Siddharthan, Tidhar, 2006</marker>
<rawString>Simone Teufel, Advaith Siddharthan, and Dan Tidhar. 2006b. Automatic classification of citation function. In Proceedings of EMNLP, pages 103–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Vincent</author>
<author>Hugo Larochelle</author>
<author>Yoshua Bengio</author>
<author>Pierre-Antoine Manzagol</author>
</authors>
<title>Extracting and composing robust features with denoising autoencoders.</title>
<date>2008</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>1096--1103</pages>
<contexts>
<context position="12179" citStr="Vincent et al. (2008)" startWordPosition="1906" endWordPosition="1909">ation. The lack of consensus on the most useful citation polarity features coupled with the recent success of deep learning neural networks (Collobert et al., 2011) further motivate our choice to limit our features to the n-grams available in the product review or citation context and not rely on external resources or tools for additional features. 3.3 Classification with mSDA For classification we use marginalized stacked denoising autoencoders (mSDA) from Chen et al. (2012)6 plus a linear SVM. mSDA takes the concept of denoising – introducing noise to make the autoencoder more robust – from Vincent et al. (2008), but does the optimization in closed form, thereby avoiding iterating over the input vector to stochastically introduce noise. The result of this is faster run times and currently state-of-the-art performance on MDSD, which makes it a good choice for our domain adaptation task. The mSDA implementation comes with LIBSVM, which we replace with LIBLINEAR (Fan et al., 2008) for faster run times with no decrease in accuracy. LIBLINEAR, with default settings, also serves as our baseline. 3.4 Outline of Experiments Our initial experiments simply extend those of Chen et al. (2012) (and others who hav</context>
</contexts>
<marker>Vincent, Larochelle, Bengio, Manzagol, 2008</marker>
<rawString>Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. 2008. Extracting and composing robust features with denoising autoencoders. In Proceedings of ICML, pages 1096–1103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of HLTEMNLP,</booktitle>
<pages>347--354</pages>
<contexts>
<context position="4580" citStr="Wilson et al. (2005)" startWordPosition="699" endWordPosition="703">ount of annotated citation data available, by leveraging large amounts of annotated polarity data we could potentially even improve citation classification. We treat citation polarity classification as a sentiment analysis domain adaptation task and therefore must be careful not to define features that are too domain specific. Previous work in citation polarity classification focuses on finding new citation features to improve classification, borrowing a few from text classification in general (e.g., ngrams), and perhaps others from sentiment analysis problems (e.g., the polarity lexicon from Wilson et al. (2005)). We would like to do as little feature engineering as possible to ensure that the features we use are meaningful across domains. However, we do still want features that somehow capture the inherent positivity or negativity of our labeled instances, i.e., citations or Amazon product reviews. Currently a popular approach for accomplishing this is to use deep learning neural networks (Bengio, 2009), which have been shown to perform well on a variety of NLP tasks using only bag-of-word features (Collobert et al., 2011). More specifically related to our work, deep learning neural networks have be</context>
<context position="11111" citStr="Wilson et al. (2005)" startWordPosition="1737" endWordPosition="1740">nd bigram features. Previous work in citation classification has largely focused on identifying new features for tually contains positive and negative labels, which have been used, e.g., in (Chen et al., 2012). improving classification accuracy. A significant amount of effort goes into engineering new features, in particular for identifying cue phrases, e.g., (Teufel et al., 2006b; Dong and Sch¨afer, 2011). However, there seems to be little consensus on which features help most for this task. For example, Abu-Jbara et al. (2013) and Jochim and Sch¨utze (2012) find the list of polar words from Wilson et al. (2005) to be useful, and neither study lists dependency relations as significant features. Athar (2011) on the other hand reported significant improvement using dependency relation features and found that the same list of polar words slightly hurt classification accuracy. The classifiers and implementation of features varies between these studies, but the problem remains that there seems to be no clear set of features for citation polarity classification. The lack of consensus on the most useful citation polarity features coupled with the recent success of deep learning neural networks (Collobert et</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of HLTEMNLP, pages 347–354.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>