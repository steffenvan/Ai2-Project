<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004538">
<title confidence="0.996571">
UIUC: A Knowledge-rich Approach to Identifying Semantic Relations
between Nominals
</title>
<author confidence="0.899471">
Brandon Beamer,1,4 Suma Bhat,2,4 Brant Chee,3,4 Andrew Fister,1,4 Alla Rozovskaya,1,4
</author>
<affiliation confidence="0.833075166666667">
Roxana Girju1,4
Department of Linguistics1,
Department of Electrical and Computer Engineering2,
Department of Library and Information Science3,
Beckman Institute4,
University of Illinois at Urbana-Champaign
</affiliation>
<email confidence="0.98263">
{bbeamer, spbhat2, chee, afister2, rozovska, girju}@uiuc.edu
</email>
<sectionHeader confidence="0.998528" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999862">
This paper describes a supervised,
knowledge-intensive approach to the auto-
matic identification of semantic relations
between nominals in English sentences.
The system employs different sets of new
and previously used lexical, syntactic, and
semantic features extracted from various
knowledge sources. At SemEval 2007 the
system achieved an F-measure of 72.4% and
an accuracy of 76.3%.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.970451684210527">
The SemEval 2007 task on Semantic Relations be-
tween Nominals is to identify the underlying se-
mantic relation between two nouns in the context
of a sentence. The dataset provided consists of a
definition file and 140 training and about 70 test
sentences for each of the seven relations consid-
ered: Cause-Effect, Instrument-Agency, Product-
Producer, Origin-Entity, Theme-Tool, Part-Whole,
and Content-Container. The task is defined as a
binary classification problem. Thus, given a pair
of nouns and their sentential context, the classifier
decides whether the nouns are linked by the target
semantic relation. In each training and test exam-
ple sentence, the nouns are identified and manu-
ally labeled with their corresponding WordNet 3.0
senses. Moreover, each example is accompanied by
the heuristic pattern (query) the annotators used to
extract the sentence from the web and the position
of the arguments in the relation.
</bodyText>
<equation confidence="0.992955">
(1) 041 ”He derives great joy and &lt;e1&gt;happiness&lt;/e1&gt;
from &lt;e2&gt;cycling&lt;/e2&gt;.” WordNet(e1) =
”happiness%1:12:00::”, WordNet(e2) = ”cy-
cling%1:04:00::”, Cause-Effect(e2,e1) = ”true”,
Query = ”happiness from *”
</equation>
<bodyText confidence="0.9997003">
Based on the information employed, systems can
be classified in four types of classes: (A) systems
that use neither the given WordNet synsets nor the
queries, (B) systems that use only WordNet senses,
(C) systems that use only the queries, and (D) sys-
tems that use both.
In this paper we present a type-B system that re-
lies on various sets of new and previously used lin-
guistic features employed in a supervised learning
model.
</bodyText>
<sectionHeader confidence="0.713517" genericHeader="method">
2 Classification of Semantic Relations
</sectionHeader>
<bodyText confidence="0.994260631578948">
Semantic relations between nominals can be en-
coded by different syntactic constructions. We
extend here over previous work that has focused
mainly on noun compounds and other noun phrases,
and noun–verb–noun constructions.
We selected a list of 18 lexico-syntactic and se-
mantic features split here into three sets: feature set
#1 (core features), feature set #2 (context features),
and the feature set #3 (special features). Table 1
shows all three sets of features along with their defi-
nitions; a detailed description is presented next. For
some features, we list previous works where they
proved useful. While features F1 – F4 were selected
from our previous experiments, all the other features
are entirely the contribution of this research.
Feature set #1: Core features
This set contains six features that were employed
in all seven relation classifiers. The features take
into consideration only lexico-semantic information
</bodyText>
<page confidence="0.98753">
386
</page>
<bodyText confidence="0.3016565">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 386–389,
Prague, June 2007. c�2007 Association for Computational Linguistics
</bodyText>
<table confidence="0.964523944444445">
No. Feature Definition
Feature Set #1: Core features
F1 Argument position indicates the position of the arguments in the semantic relation
(Girju et al., 2005; Girju et al., 2006) (e.g., Part-Whole(e1, e2), where e1 is the part and e2 is the whole).
F2 Semantic specialization this is the prediction returned by the automatic WordNet IS-A semantic
(Girju et al., 2005; Girju et al., 2006) specialization procedure.
F3, F4 Nominalization indicates whether the nouns e1 (F3) and e2 (F4) are nominalizations
(Girju et al., 2004) or not. Specifically, we distinguish here between agential nouns,
other nominalizations, and neither.
F5, F6 Spatio-Temporal features indicate if e1 (F5) or e2 (F6) encode time or location.
Feature Set #2: Context features
F7, F8 Grammatical role describes the grammatical role of e1 (F7) and e2 (F8). There are three
possible values: subject, direct object, or neither.
F9 PP Attachment applies to NP PP constructions and indicates if the prepositional phrase
containing e2 attaches to the NP containing e1.
F10, F11 Semantic Role is concerned with the semantic role of the phrase containing
either e1 (F10) or e2 (F11). In particular, we focused on three semantic
roles: Time, Location, Manner. The feature is set to 1 if the target noun
</table>
<bodyText confidence="0.810189230769231">
is part of a phrase of that type and to 0 otherwise.
F12, F13, Inter-noun context sequence is a set of three features. F12 captures the sequence of stemmed
F14 words between e1 and e2, while F13 lists the part of speech sequence in
between the target nouns. F14 is a scoring weight (with possible values
1, 0.5, 0.25, and 0.125) which measures the similarity of an unseen
sequence to the set of sequence patterns associated with a relation.
Feature Set #3: Special features
F15, F16 Psychological feature is used in the Theme-Tool classifier; indicates if e1 (F15) or e2 (F16)
belong or not to a predefined set of psychological features.
F17 Instrument semantic role is used for the Instrument-Agency relation and indicates whether
the phrase containing e1 is labeled as em Instrument or not.
F18 Syntactic attachment is used for the Instrument-Agent relation and indicates whether the phrase
containing the Instrument role attaches to a noun or a verb
</bodyText>
<tableCaption confidence="0.997061">
Table 1: The three sets of features used for the automatic semantic relation classification.
</tableCaption>
<bodyText confidence="0.999261615384615">
about the two target nouns.
Argument position (F1) indicates the position of
the semantic arguments in the relation. This infor-
mation is very valuable, since some relations have a
particular argument arrangement depending on the
lexico-syntactic construction in which they occur.
For example, most of the noun compounds encod-
ing Stuff-Object / Part-Whole relations have e1 as
the part and e2 as the whole (e.g., silk dress).
Semantic specialization (F2) is a binary feature
representing the prediction of a semantic specializa-
tion learning model. The method consists of a set
of iterative procedures of specialization of the train-
ing examples on the WordNet IS-A hierarchy. Thus,
after all the initial noun–noun pairs are mapped
through generalization to entity – entity pairs in
WordNet, a set of necessary specialization iterations
is applied until it finds a boundary that separates pos-
itive and negative examples. This boundary is tested
on new examples for relation prediction.
The nominalization features (F3, F4) indicate if
the target noun is a nominalization and, if yes, of
what type. We distinguish here between agential
nouns, other nominalizations, and neither. The
features were identified based on WordNet and
NomLex-Plus1 and were introduced to filter some
of negative examples, such as car owner/THEME.
Spatio–Temporal features (F5, F6) were also in-
troduced to recognize some near miss examples,
such as Temporal and Location relations. For in-
stance, activation by summer (near-miss for Cause-
Effect) and mouse in thefield (near-miss for Content-
Container). Similarly, for Theme-Tool, a word act-
ing as a Theme should not indicate a period of time,
as in &lt;e1&gt;the appointment&lt;/e1&gt; was for more
than one &lt;e2&gt;year&lt;/e2&gt;. For this we used the in-
formation provided by WordNet and special classes
generated from the works of (Herskovits, 1987),
(Linstromberg, 1997), and (Tyler and Evans, 2003).
</bodyText>
<footnote confidence="0.894935666666667">
1NomLex-Plus is a hand-coded database of 5,000 verb nom-
inalizations, de-adjectival, and de-adverbial nouns.
http://nlp.cs.nyu.edu/nomlex/index.html
</footnote>
<page confidence="0.994448">
387
</page>
<bodyText confidence="0.999266217391305">
Feature set #2: Context features
This set takes advantage of the sentence context to
identify features at different linguistic levels.
The grammatical role features (F7, F8) determine
if e1 or e2 is the subject, direct object, or neither.
This feature helps filter out some instances with poor
context, such as noun compounds and identify some
near-miss examples. For example, a restriction im-
posed by the definition of Theme-Tool indicates that
in constructions such as Y/Tool is used for V-ing
X/Theme, neither X nor Y can be the subject of
the sentence, and hence Theme-Tool(X, Y) would be
false. This restriction is also captured by the nomi-
nalization feature in case X or Y is an agential noun.
PP attachment (F9) is defined for NP PP construc-
tions, where the prepositional phrase containing the
noun e2 attaches or not to the NP (containing e1).
The rationale is to identify negative instances where
the PP attaches to any other word before NP in the
sentence. For example, eat &lt;e1&gt;pizza&lt;/e1&gt; with
&lt;e2&gt;a fork&lt;/e2&gt;, where with a fork attaches to
the verb to eat (cf. (Charniak, 2000)).
Furthermore, we implemented and used two se-
mantic role features which identify the semantic role
of the phrase in a verb–argument structure, phrase
containing either e1 (F10) or e2 (F11). In particular,
we focus on three semantic roles: Time, Location,
Manner. The feature is set to 1 if the target noun
is part of a semantic role phrase and to 0 otherwise.
The idea is to filter out near-miss examples, expe-
cially for the Instrument-Agency relation. For this,
we used ASSERT, a semantic role labeler developed
at the University of Colorado at Boulder2 which was
queried through a web interface.
Inter-noun context sequence features (F12, F13)
encode the sequence of lexical and part of speech
information between the two target nouns. Feature
F14 is a weight feature on the values of F12 and
F13 and indicates how similar a new sequence is to
the already observed inter-noun context associated
with the relation. If there is a direct match, then the
weight is set to 1. If the part-of-speech pattern of the
new substring matches that of an already seen sub-
string, then the weight is set to 0.5. Weights 0.25
and 0.125 are given to those sequences that overlap
entirely or partially with patterns encoding other se-
</bodyText>
<footnote confidence="0.825051">
2http://oak.colorado.edu/assert/
</footnote>
<bodyText confidence="0.997889419354839">
mantic relations in the same contingency set (e.g.,
semantic relations that share syntactic pattern se-
quences). The value of the feature is the summation
of the weights thus obtained. The rationale is that
the greater the weight, the more representative is the
context sequence for that relation.
Feature set #3: Special features
This set includes features that help identify specific
information about some semantic relations.
Psychological feature was defined for the Theme-
Tool relation and indicates if the target noun (F15,
F16) belongs to a list of special concepts. This fea-
ture was obtained from the restrictions listed in the
definition of Theme-Tool. In the example need for
money, the noun need is a psychological feature, and
thus the instance cannot encode a Theme-Tool rela-
tion. A list of synsets from WordNet subhierarchy
of motivation and cognition constituted the psycho-
logical factors. This was augmented with precondi-
tions such as foundation and requirement since they
would not be allowed as tools for the theme.
The Instrument semantic role is used for the
Instrument-Agency relation as a boolean feature
(F17) indicating whether the argument identified as
Instrument in the relation (e.g., e1 if Instrument-
Agency(e1, e2)) belongs to an instrument phrase as
identified by a semantic role tool, such as ASSERT.
The syntactic attachment feature (F18) is a fea-
ture that indicates whether the argument identified
as Instrument in the relation attaches to a verb or to
a noun in the syntactically parsed sentence.
</bodyText>
<sectionHeader confidence="0.7457905" genericHeader="method">
3 Learning Model and Experimental
Setting
</sectionHeader>
<bodyText confidence="0.980463666666667">
For our experiments we chose libSVM, an open
source SVM package3. Since some of our features
are nominal, we followed the standard practice of
representing a nominal feature with n discrete val-
ues as n binary features. We used the RBF kernel.
We built a binary classifier for each of the seven
relations. Since the size of the task training data per
relation is small, we expanded it with new examples
from various sources. We added a new corpus of
3,000 sentences of news articles from the TREC-9
text collection (Girju, 2003) encoding Cause-Effect
(1,320) and Product-Producer (721). Another col-
</bodyText>
<footnote confidence="0.953331">
3http://www.csie.ntu.edu.tw/—cjlin/libsvm/
</footnote>
<page confidence="0.988593">
388
</page>
<table confidence="0.998533111111111">
Relation P R F Acc Total Base-F Base-Acc Best features
Cause-Effect 69.5 100.0 82.0 77.5 80 67.8 51.2 F1, F2, F5, F6, F12–F14
Instrument-Agency 68.2 78.9 73.2 71.8 78 65.5 51.3 F7, F8, F10, F11, F15–F18
Product-Producer 84.5 79.0 81.7 76.3 93 80.0 66.7 F1–F4, F12–F14
Origin-Entity 86.4 52.8 65.5 75.3 81 61.5 55.6 F1, F2, F5, F6, F12–F14
Theme-Tool 85.7 41.4 55.8 73.2 71 58.0 59.2 F1–F6, F15, F16
Part-Whole 70.8 65.4 68.0 77.8 72 53.1 63.9 F1–F4
Content-Container 93.1 71.1 80.6 82.4 74 67.9 51.4 F1–F6, F12–F14
Average 79.7 69.8 72.4 76.3 78.4
</table>
<tableCaption confidence="0.992503">
Table 2: Performance obtained per relation. Precision, Recall, F-measure, Accuracy, and Total (number of examples) are macro-
averaged for system’s performance on all 7 relations. Base-F shows the baseline F measure (all true), while Base-Acc shows the
baseline accuracy score (majority).
</tableCaption>
<bodyText confidence="0.99970855">
lection of 3,129 sentences from Wall Street Journal
(Moldovan et al., 2004; Girju et al., 2004) was con-
sidered for Part-Whole (1,003), Origin-Entity (167),
Product-Producer (112), and Theme-Tool (91). We
also extracted 552 Product-Producer instances from
eXtended WordNet4 (noun entries and their gloss
definition). Moreover, for Theme-Tool and Content-
Container we used special lists of constraints5. Be-
sides the selectional restrictions imposed on the
nouns by special features such as F15 and F16 (psy-
chological feature), we created lists of containers
from various thesauri6 and identified selectional re-
strictions that differentiate between containers and
locations relying on taxonomies of spatial entities
discussed in detail in (Herskovits, 1987) and (Tyler
and Evans, 2003).
Each instance in this text collection had the tar-
get nouns identified and annotated with WordNet
senses. Since the annotations used different Word-
Net versions, senses were mapped to sense keys.
</bodyText>
<sectionHeader confidence="0.999159" genericHeader="evaluation">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.99997775">
Table 2 shows the performance of our system for
each semantic relation. Base-F indicates the base-
line F-measure (all true), while Base-Acc shows the
baseline accuracy score (majority). The Average
score of precision, recall, F-measure, and accuracy
is macroaveraged over all seven relations. Overall,
all features contributed to the performance, with a
different contribution per relation (cf. Table 2).
</bodyText>
<sectionHeader confidence="0.999688" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999603">
This paper describes a method for the automatic
identification of a set of seven semantic relations
</bodyText>
<footnote confidence="0.9968245">
4http://xwn.hlt.utdallas.edu/
5The Instrument-Agency classifier was trained only on the
task dataset.
6Thesauri such as TheFreeDictionary.com.
</footnote>
<bodyText confidence="0.9995978">
based on support vector machines (SVMs). The ap-
proach benefits from an extended dataset on which
binary classifiers were trained for each relation. The
feature sets fed into the SVMs produced very good
results.
</bodyText>
<sectionHeader confidence="0.998596" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999325">
We would like to thank Brian Drexler for his valu-
able suggestions on the set of semantic relations.
</bodyText>
<sectionHeader confidence="0.999452" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998680862068965">
E. Charniak. 2000. A Maximum-entropy-inspired Parser. In
the Proceedings of the 1st NAACL Conference.
R. Girju, A. Giuglea, M. Olteanu, O. Fortu, O. Bolohan, and
D. Moldovan. 2004. Support vector machines applied to
the classification of semantic relations in nominalized noun
phrases. In the Proceedings of the HLT/NAACL Workshop
on Computational Lexical Semantics.
R. Girju, D. Moldovan, M. Tatu, and D. Antohe. 2005. On
the semantics of noun compounds. Computer Speech and
Language, 19(4):479–496.
R. Girju, A. Badulescu, and D. Moldovan. 2006. Automatic
discovery of part-whole relations. Computational Linguis-
tics, 32(1).
R. Girju. 2003. Automatic detection of causal relations for
question answering. In the Proceedings of the ACL Work-
shop on ”Multilingual Summarization and Question Answer-
ing - Machine Learning and Beyond”.
A. Herskovits. 1987. Language and spatial cognition: An in-
terdisciplinary study of the prepositions in English. Cam-
bridge University Press.
S. Linstromberg. 1997. English Prepositions Explained. John
Benjamins Publishing Co., Amsterdam/Philaderphia.
D. Moldovan, A. Badulescu, M. Tatu, D. Antohe, and R. Girju.
2004. Models for the semantic classification of noun
phrases. In the Proceedings of the HLT/NAACL Workshop
on Computational Lexical Semantics.
A. Tyler and V. Evans. 2003. The Semantics of English Prepo-
sitions: Spatial Sciences, Embodied Meaning, and Cogni-
tion. Cambridge University Press.
</reference>
<page confidence="0.999162">
389
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.090199">
<title confidence="0.956001">UIUC: A Knowledge-rich Approach to Identifying Semantic Relations between Nominals</title>
<author confidence="0.879456">Suma Brant Andrew Alla</author>
<degree confidence="0.630799">of of Electrical and Computer</degree>
<affiliation confidence="0.7916375">of Library and Information University of Illinois at Urbana-Champaign</affiliation>
<email confidence="0.409779">spbhat2,chee,afister2,rozovska,</email>
<abstract confidence="0.986292636363636">This paper describes a supervised, knowledge-intensive approach to the automatic identification of semantic relations between nominals in English sentences. The system employs different sets of new and previously used lexical, syntactic, and semantic features extracted from various knowledge sources. At SemEval 2007 the system achieved an F-measure of 72.4% and an accuracy of 76.3%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A Maximum-entropy-inspired Parser.</title>
<date>2000</date>
<booktitle>In the Proceedings of the 1st NAACL Conference.</booktitle>
<contexts>
<context position="9000" citStr="Charniak, 2000" startWordPosition="1391" endWordPosition="1392">Tool is used for V-ing X/Theme, neither X nor Y can be the subject of the sentence, and hence Theme-Tool(X, Y) would be false. This restriction is also captured by the nominalization feature in case X or Y is an agential noun. PP attachment (F9) is defined for NP PP constructions, where the prepositional phrase containing the noun e2 attaches or not to the NP (containing e1). The rationale is to identify negative instances where the PP attaches to any other word before NP in the sentence. For example, eat &lt;e1&gt;pizza&lt;/e1&gt; with &lt;e2&gt;a fork&lt;/e2&gt;, where with a fork attaches to the verb to eat (cf. (Charniak, 2000)). Furthermore, we implemented and used two semantic role features which identify the semantic role of the phrase in a verb–argument structure, phrase containing either e1 (F10) or e2 (F11). In particular, we focus on three semantic roles: Time, Location, Manner. The feature is set to 1 if the target noun is part of a semantic role phrase and to 0 otherwise. The idea is to filter out near-miss examples, expecially for the Instrument-Agency relation. For this, we used ASSERT, a semantic role labeler developed at the University of Colorado at Boulder2 which was queried through a web interface. I</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>E. Charniak. 2000. A Maximum-entropy-inspired Parser. In the Proceedings of the 1st NAACL Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Girju</author>
<author>A Giuglea</author>
<author>M Olteanu</author>
<author>O Fortu</author>
<author>O Bolohan</author>
<author>D Moldovan</author>
</authors>
<title>Support vector machines applied to the classification of semantic relations in nominalized noun phrases.</title>
<date>2004</date>
<booktitle>In the Proceedings of the HLT/NAACL Workshop on Computational Lexical Semantics.</booktitle>
<contexts>
<context position="4070" citStr="Girju et al., 2004" startWordPosition="600" endWordPosition="603">val-2007), pages 386–389, Prague, June 2007. c�2007 Association for Computational Linguistics No. Feature Definition Feature Set #1: Core features F1 Argument position indicates the position of the arguments in the semantic relation (Girju et al., 2005; Girju et al., 2006) (e.g., Part-Whole(e1, e2), where e1 is the part and e2 is the whole). F2 Semantic specialization this is the prediction returned by the automatic WordNet IS-A semantic (Girju et al., 2005; Girju et al., 2006) specialization procedure. F3, F4 Nominalization indicates whether the nouns e1 (F3) and e2 (F4) are nominalizations (Girju et al., 2004) or not. Specifically, we distinguish here between agential nouns, other nominalizations, and neither. F5, F6 Spatio-Temporal features indicate if e1 (F5) or e2 (F6) encode time or location. Feature Set #2: Context features F7, F8 Grammatical role describes the grammatical role of e1 (F7) and e2 (F8). There are three possible values: subject, direct object, or neither. F9 PP Attachment applies to NP PP constructions and indicates if the prepositional phrase containing e2 attaches to the NP containing e1. F10, F11 Semantic Role is concerned with the semantic role of the phrase containing either</context>
<context position="13395" citStr="Girju et al., 2004" startWordPosition="2100" endWordPosition="2103">5.6 F1, F2, F5, F6, F12–F14 Theme-Tool 85.7 41.4 55.8 73.2 71 58.0 59.2 F1–F6, F15, F16 Part-Whole 70.8 65.4 68.0 77.8 72 53.1 63.9 F1–F4 Content-Container 93.1 71.1 80.6 82.4 74 67.9 51.4 F1–F6, F12–F14 Average 79.7 69.8 72.4 76.3 78.4 Table 2: Performance obtained per relation. Precision, Recall, F-measure, Accuracy, and Total (number of examples) are macroaveraged for system’s performance on all 7 relations. Base-F shows the baseline F measure (all true), while Base-Acc shows the baseline accuracy score (majority). lection of 3,129 sentences from Wall Street Journal (Moldovan et al., 2004; Girju et al., 2004) was considered for Part-Whole (1,003), Origin-Entity (167), Product-Producer (112), and Theme-Tool (91). We also extracted 552 Product-Producer instances from eXtended WordNet4 (noun entries and their gloss definition). Moreover, for Theme-Tool and ContentContainer we used special lists of constraints5. Besides the selectional restrictions imposed on the nouns by special features such as F15 and F16 (psychological feature), we created lists of containers from various thesauri6 and identified selectional restrictions that differentiate between containers and locations relying on taxonomies of </context>
</contexts>
<marker>Girju, Giuglea, Olteanu, Fortu, Bolohan, Moldovan, 2004</marker>
<rawString>R. Girju, A. Giuglea, M. Olteanu, O. Fortu, O. Bolohan, and D. Moldovan. 2004. Support vector machines applied to the classification of semantic relations in nominalized noun phrases. In the Proceedings of the HLT/NAACL Workshop on Computational Lexical Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Girju</author>
<author>D Moldovan</author>
<author>M Tatu</author>
<author>D Antohe</author>
</authors>
<title>On the semantics of noun compounds.</title>
<date>2005</date>
<journal>Computer Speech and Language,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="3703" citStr="Girju et al., 2005" startWordPosition="541" endWordPosition="544">from our previous experiments, all the other features are entirely the contribution of this research. Feature set #1: Core features This set contains six features that were employed in all seven relation classifiers. The features take into consideration only lexico-semantic information 386 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 386–389, Prague, June 2007. c�2007 Association for Computational Linguistics No. Feature Definition Feature Set #1: Core features F1 Argument position indicates the position of the arguments in the semantic relation (Girju et al., 2005; Girju et al., 2006) (e.g., Part-Whole(e1, e2), where e1 is the part and e2 is the whole). F2 Semantic specialization this is the prediction returned by the automatic WordNet IS-A semantic (Girju et al., 2005; Girju et al., 2006) specialization procedure. F3, F4 Nominalization indicates whether the nouns e1 (F3) and e2 (F4) are nominalizations (Girju et al., 2004) or not. Specifically, we distinguish here between agential nouns, other nominalizations, and neither. F5, F6 Spatio-Temporal features indicate if e1 (F5) or e2 (F6) encode time or location. Feature Set #2: Context features F7, F8 Gr</context>
</contexts>
<marker>Girju, Moldovan, Tatu, Antohe, 2005</marker>
<rawString>R. Girju, D. Moldovan, M. Tatu, and D. Antohe. 2005. On the semantics of noun compounds. Computer Speech and Language, 19(4):479–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Girju</author>
<author>A Badulescu</author>
<author>D Moldovan</author>
</authors>
<title>Automatic discovery of part-whole relations.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="3724" citStr="Girju et al., 2006" startWordPosition="545" endWordPosition="548">periments, all the other features are entirely the contribution of this research. Feature set #1: Core features This set contains six features that were employed in all seven relation classifiers. The features take into consideration only lexico-semantic information 386 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 386–389, Prague, June 2007. c�2007 Association for Computational Linguistics No. Feature Definition Feature Set #1: Core features F1 Argument position indicates the position of the arguments in the semantic relation (Girju et al., 2005; Girju et al., 2006) (e.g., Part-Whole(e1, e2), where e1 is the part and e2 is the whole). F2 Semantic specialization this is the prediction returned by the automatic WordNet IS-A semantic (Girju et al., 2005; Girju et al., 2006) specialization procedure. F3, F4 Nominalization indicates whether the nouns e1 (F3) and e2 (F4) are nominalizations (Girju et al., 2004) or not. Specifically, we distinguish here between agential nouns, other nominalizations, and neither. F5, F6 Spatio-Temporal features indicate if e1 (F5) or e2 (F6) encode time or location. Feature Set #2: Context features F7, F8 Grammatical role descri</context>
</contexts>
<marker>Girju, Badulescu, Moldovan, 2006</marker>
<rawString>R. Girju, A. Badulescu, and D. Moldovan. 2006. Automatic discovery of part-whole relations. Computational Linguistics, 32(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Girju</author>
</authors>
<title>Automatic detection of causal relations for question answering.</title>
<date>2003</date>
<booktitle>In the Proceedings of the ACL Workshop on ”Multilingual Summarization and Question Answering - Machine Learning and Beyond”.</booktitle>
<contexts>
<context position="12348" citStr="Girju, 2003" startWordPosition="1940" endWordPosition="1941">noun in the syntactically parsed sentence. 3 Learning Model and Experimental Setting For our experiments we chose libSVM, an open source SVM package3. Since some of our features are nominal, we followed the standard practice of representing a nominal feature with n discrete values as n binary features. We used the RBF kernel. We built a binary classifier for each of the seven relations. Since the size of the task training data per relation is small, we expanded it with new examples from various sources. We added a new corpus of 3,000 sentences of news articles from the TREC-9 text collection (Girju, 2003) encoding Cause-Effect (1,320) and Product-Producer (721). Another col3http://www.csie.ntu.edu.tw/—cjlin/libsvm/ 388 Relation P R F Acc Total Base-F Base-Acc Best features Cause-Effect 69.5 100.0 82.0 77.5 80 67.8 51.2 F1, F2, F5, F6, F12–F14 Instrument-Agency 68.2 78.9 73.2 71.8 78 65.5 51.3 F7, F8, F10, F11, F15–F18 Product-Producer 84.5 79.0 81.7 76.3 93 80.0 66.7 F1–F4, F12–F14 Origin-Entity 86.4 52.8 65.5 75.3 81 61.5 55.6 F1, F2, F5, F6, F12–F14 Theme-Tool 85.7 41.4 55.8 73.2 71 58.0 59.2 F1–F6, F15, F16 Part-Whole 70.8 65.4 68.0 77.8 72 53.1 63.9 F1–F4 Content-Container 93.1 71.1 80.6 8</context>
</contexts>
<marker>Girju, 2003</marker>
<rawString>R. Girju. 2003. Automatic detection of causal relations for question answering. In the Proceedings of the ACL Workshop on ”Multilingual Summarization and Question Answering - Machine Learning and Beyond”.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Herskovits</author>
</authors>
<title>Language and spatial cognition: An interdisciplinary study of the prepositions in English.</title>
<date>1987</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="7707" citStr="Herskovits, 1987" startWordPosition="1185" endWordPosition="1186">lus1 and were introduced to filter some of negative examples, such as car owner/THEME. Spatio–Temporal features (F5, F6) were also introduced to recognize some near miss examples, such as Temporal and Location relations. For instance, activation by summer (near-miss for CauseEffect) and mouse in thefield (near-miss for ContentContainer). Similarly, for Theme-Tool, a word acting as a Theme should not indicate a period of time, as in &lt;e1&gt;the appointment&lt;/e1&gt; was for more than one &lt;e2&gt;year&lt;/e2&gt;. For this we used the information provided by WordNet and special classes generated from the works of (Herskovits, 1987), (Linstromberg, 1997), and (Tyler and Evans, 2003). 1NomLex-Plus is a hand-coded database of 5,000 verb nominalizations, de-adjectival, and de-adverbial nouns. http://nlp.cs.nyu.edu/nomlex/index.html 387 Feature set #2: Context features This set takes advantage of the sentence context to identify features at different linguistic levels. The grammatical role features (F7, F8) determine if e1 or e2 is the subject, direct object, or neither. This feature helps filter out some instances with poor context, such as noun compounds and identify some near-miss examples. For example, a restriction impo</context>
<context position="14053" citStr="Herskovits, 1987" startWordPosition="2193" endWordPosition="2194">rigin-Entity (167), Product-Producer (112), and Theme-Tool (91). We also extracted 552 Product-Producer instances from eXtended WordNet4 (noun entries and their gloss definition). Moreover, for Theme-Tool and ContentContainer we used special lists of constraints5. Besides the selectional restrictions imposed on the nouns by special features such as F15 and F16 (psychological feature), we created lists of containers from various thesauri6 and identified selectional restrictions that differentiate between containers and locations relying on taxonomies of spatial entities discussed in detail in (Herskovits, 1987) and (Tyler and Evans, 2003). Each instance in this text collection had the target nouns identified and annotated with WordNet senses. Since the annotations used different WordNet versions, senses were mapped to sense keys. 4 Experimental Results Table 2 shows the performance of our system for each semantic relation. Base-F indicates the baseline F-measure (all true), while Base-Acc shows the baseline accuracy score (majority). The Average score of precision, recall, F-measure, and accuracy is macroaveraged over all seven relations. Overall, all features contributed to the performance, with a </context>
</contexts>
<marker>Herskovits, 1987</marker>
<rawString>A. Herskovits. 1987. Language and spatial cognition: An interdisciplinary study of the prepositions in English. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Linstromberg</author>
</authors>
<title>English Prepositions Explained.</title>
<date>1997</date>
<publisher>John Benjamins Publishing Co., Amsterdam/Philaderphia.</publisher>
<contexts>
<context position="7729" citStr="Linstromberg, 1997" startWordPosition="1187" endWordPosition="1188">uced to filter some of negative examples, such as car owner/THEME. Spatio–Temporal features (F5, F6) were also introduced to recognize some near miss examples, such as Temporal and Location relations. For instance, activation by summer (near-miss for CauseEffect) and mouse in thefield (near-miss for ContentContainer). Similarly, for Theme-Tool, a word acting as a Theme should not indicate a period of time, as in &lt;e1&gt;the appointment&lt;/e1&gt; was for more than one &lt;e2&gt;year&lt;/e2&gt;. For this we used the information provided by WordNet and special classes generated from the works of (Herskovits, 1987), (Linstromberg, 1997), and (Tyler and Evans, 2003). 1NomLex-Plus is a hand-coded database of 5,000 verb nominalizations, de-adjectival, and de-adverbial nouns. http://nlp.cs.nyu.edu/nomlex/index.html 387 Feature set #2: Context features This set takes advantage of the sentence context to identify features at different linguistic levels. The grammatical role features (F7, F8) determine if e1 or e2 is the subject, direct object, or neither. This feature helps filter out some instances with poor context, such as noun compounds and identify some near-miss examples. For example, a restriction imposed by the definition </context>
</contexts>
<marker>Linstromberg, 1997</marker>
<rawString>S. Linstromberg. 1997. English Prepositions Explained. John Benjamins Publishing Co., Amsterdam/Philaderphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>A Badulescu</author>
<author>M Tatu</author>
<author>D Antohe</author>
<author>R Girju</author>
</authors>
<title>Models for the semantic classification of noun phrases.</title>
<date>2004</date>
<booktitle>In the Proceedings of the HLT/NAACL Workshop on Computational Lexical Semantics.</booktitle>
<contexts>
<context position="13374" citStr="Moldovan et al., 2004" startWordPosition="2096" endWordPosition="2099">2.8 65.5 75.3 81 61.5 55.6 F1, F2, F5, F6, F12–F14 Theme-Tool 85.7 41.4 55.8 73.2 71 58.0 59.2 F1–F6, F15, F16 Part-Whole 70.8 65.4 68.0 77.8 72 53.1 63.9 F1–F4 Content-Container 93.1 71.1 80.6 82.4 74 67.9 51.4 F1–F6, F12–F14 Average 79.7 69.8 72.4 76.3 78.4 Table 2: Performance obtained per relation. Precision, Recall, F-measure, Accuracy, and Total (number of examples) are macroaveraged for system’s performance on all 7 relations. Base-F shows the baseline F measure (all true), while Base-Acc shows the baseline accuracy score (majority). lection of 3,129 sentences from Wall Street Journal (Moldovan et al., 2004; Girju et al., 2004) was considered for Part-Whole (1,003), Origin-Entity (167), Product-Producer (112), and Theme-Tool (91). We also extracted 552 Product-Producer instances from eXtended WordNet4 (noun entries and their gloss definition). Moreover, for Theme-Tool and ContentContainer we used special lists of constraints5. Besides the selectional restrictions imposed on the nouns by special features such as F15 and F16 (psychological feature), we created lists of containers from various thesauri6 and identified selectional restrictions that differentiate between containers and locations rely</context>
</contexts>
<marker>Moldovan, Badulescu, Tatu, Antohe, Girju, 2004</marker>
<rawString>D. Moldovan, A. Badulescu, M. Tatu, D. Antohe, and R. Girju. 2004. Models for the semantic classification of noun phrases. In the Proceedings of the HLT/NAACL Workshop on Computational Lexical Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Tyler</author>
<author>V Evans</author>
</authors>
<title>The Semantics of English Prepositions: Spatial Sciences, Embodied Meaning, and Cognition.</title>
<date>2003</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="7758" citStr="Tyler and Evans, 2003" startWordPosition="1190" endWordPosition="1193">ative examples, such as car owner/THEME. Spatio–Temporal features (F5, F6) were also introduced to recognize some near miss examples, such as Temporal and Location relations. For instance, activation by summer (near-miss for CauseEffect) and mouse in thefield (near-miss for ContentContainer). Similarly, for Theme-Tool, a word acting as a Theme should not indicate a period of time, as in &lt;e1&gt;the appointment&lt;/e1&gt; was for more than one &lt;e2&gt;year&lt;/e2&gt;. For this we used the information provided by WordNet and special classes generated from the works of (Herskovits, 1987), (Linstromberg, 1997), and (Tyler and Evans, 2003). 1NomLex-Plus is a hand-coded database of 5,000 verb nominalizations, de-adjectival, and de-adverbial nouns. http://nlp.cs.nyu.edu/nomlex/index.html 387 Feature set #2: Context features This set takes advantage of the sentence context to identify features at different linguistic levels. The grammatical role features (F7, F8) determine if e1 or e2 is the subject, direct object, or neither. This feature helps filter out some instances with poor context, such as noun compounds and identify some near-miss examples. For example, a restriction imposed by the definition of Theme-Tool indicates that </context>
<context position="14081" citStr="Tyler and Evans, 2003" startWordPosition="2196" endWordPosition="2199">duct-Producer (112), and Theme-Tool (91). We also extracted 552 Product-Producer instances from eXtended WordNet4 (noun entries and their gloss definition). Moreover, for Theme-Tool and ContentContainer we used special lists of constraints5. Besides the selectional restrictions imposed on the nouns by special features such as F15 and F16 (psychological feature), we created lists of containers from various thesauri6 and identified selectional restrictions that differentiate between containers and locations relying on taxonomies of spatial entities discussed in detail in (Herskovits, 1987) and (Tyler and Evans, 2003). Each instance in this text collection had the target nouns identified and annotated with WordNet senses. Since the annotations used different WordNet versions, senses were mapped to sense keys. 4 Experimental Results Table 2 shows the performance of our system for each semantic relation. Base-F indicates the baseline F-measure (all true), while Base-Acc shows the baseline accuracy score (majority). The Average score of precision, recall, F-measure, and accuracy is macroaveraged over all seven relations. Overall, all features contributed to the performance, with a different contribution per r</context>
</contexts>
<marker>Tyler, Evans, 2003</marker>
<rawString>A. Tyler and V. Evans. 2003. The Semantics of English Prepositions: Spatial Sciences, Embodied Meaning, and Cognition. Cambridge University Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>