<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.831932">
Statistical Properties of Probabilistic
Context-Free Grammars
</title>
<author confidence="0.998795">
Zhiyi Chi*
</author>
<affiliation confidence="0.994106">
University of Chicago
</affiliation>
<bodyText confidence="0.9895984">
We prove a number of useful results about probabilistic context-free grammars (PCFGs) and
their Gibbs representations. We present a method, called the relative weighted frequency method,
to assign production probabilities that impose proper PCFG distributions on finite parses. We
demonstrate that these distributions have finite entropies. In addition, under the distributions,
sizes of parses have finite moment of any order. We show that Gibbs distributions on CFG parses,
which generalize PCFG distributions and are more powerful, become PCFG distributions if their
features only include frequencies of production rules in parses. Under these circumstances, we
prove the equivalence of the maximum-likelihood (ML) estimation procedures for these two types
of probability distributions on parses. We introduce the renormalization of improper PCFGs
to proper ones. We also study PCFGs from the perspective of stochastic branching processes.
We prove that with their production probabilities assigned by the relative weighted frequency
method, PCFGs are subcritical, i.e., their branching rates are less than one. We also show that
by renormalization, connected supercritical PCFGs become subcritical ones. Finally, some minor
issues, including identifiability and approximation of production probabilities of PCFGs, are
discussed.
</bodyText>
<sectionHeader confidence="0.992345" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.997212">
This article proves a number of useful properties of probabilistic context-free grammars
(PCFGs). In this section, we give an introduction to the results and related topics.
</bodyText>
<subsectionHeader confidence="0.999968">
1.1 Assignment of Proper PCFG Distributions
</subsectionHeader>
<bodyText confidence="0.999939875">
Finite parse trees, or parses, generated by a context-free grammar (CFG) can be equipped
with a variety of probability distributions. The simplest way to do this is by production
probabilities. First, for each nonterminal symbol in the CFG, a probability distribution
is placed on the set of all productions from that symbol. Then each finite parse tree
is allocated a probability equal to the product of the probabilities of all productions
in the tree. More specifically, denote a finite parse tree by T. For any production rule
A a of the CFG, let f (A a; r) be the number of times it occurs in T. Let R be the
set of all production rules. Then
</bodyText>
<equation confidence="0.9435645">
per) = H p(A Oz)f (A—&apos;°;7)
(A—c)ER
</equation>
<bodyText confidence="0.9508">
A CFG with a probability distribution on its parses assigned in this way is called
a probabilistic context-free grammar (PCFG) (Booth and Thompson 1973; Grenander
</bodyText>
<footnote confidence="0.931769">
* Department of Statistics, University of Chicago, Chicago, IL 60637, USA. Email:
chi@galton.uchicago.edu. This work was supported by the Army Research Office (DAAL03-92-G-0115),
the National Science Foundation (DMS-9217655), and the Office of Naval Research (N00014-96-1-0647).
</footnote>
<note confidence="0.8298595">
© 1999 Association for Computational Linguistics
Computational Linguistics Volume 25, Number 1
</note>
<bodyText confidence="0.993668666666667">
1976)1 and the probability distribution is called a PCFG distribution. A PCFG may
be improper, i.e., the total probability of parses may be less than one. For instance,
consider the CFG in Chomsky normal form:
</bodyText>
<figure confidence="0.6603175">
S SS (1)
S a
</figure>
<bodyText confidence="0.976406684210526">
where S is the only nonterminal symbol, and a is the only terminal symbol. If p(S
SS) = p, then p(S —&gt; a) =1— p. Let xh be the total probability of all parses with height
no larger than h. Clearly, xh is increasing. It is not hard to see that xh±i = 1 — p +
Therefore, the limit of xh, which is the total probability of all parses, is a solution for
the equation x = 1 — p + px2. The equation has two solutions: 1 and 1/p — 1. It can
be shown that x is the smaller of the two: x = min(1, 1/p — 1). Therefore, if p &gt; 1/2,
x &lt; 1—an improper probability.
How to assign proper production probabilities is quite a subtle problem. A suffi-
cient condition for proper assignment is established by Chi and Geman (1998), who
prove that production probabilities estimated by the maximum-likelihood (ML) esti-
mation procedure (or relative frequency estimation procedure, as it is called in com-
putational linguistics) always impose proper PCFG distributions. Without much diffi-
culty, this result can be generalized to a simple procedure, called &amp;quot;the relative weighted
frequency&amp;quot; method, which assigns proper production probabilities of PCFGs. We will
give more details of the generalization in Section 3 and summarize the method in
Proposition 1.
1.2 Entropy and Moments of Parse Tree Sizes
As a probabilistic model for languages, the PCFG model has several important statis-
tical properties, among which is the entropy of PCFG distribution on parses. Entropy
is a measure of the uncertainty of a probability distribution. The larger its entropy,
the less one can learn about parses randomly sampled from the distribution. As an
example, suppose we have a set S of N parses—or any objects-7-1, ,TN, where N is
very large. We may ask how much one can learn from the sentence &amp;quot;T is a random
sample from S.&amp;quot; At one extreme, let the distribution on S be p(Ti) = 1, and p(r1) = 0,
for i 1. Then, because with probability one, T = there is no uncertainty about the
sample. In other words, we can get full information from the above sentence. At the
other extreme, suppose the distribution is p(n) = = p(TN) = 1/N. In this case, all
the elements of S are statistically equivalent. No specific information is given about T
that would make it possible to know it from S. Greater effort is required—for example,
enumerating all the elements in S—to find what T is. Since S is big, the uncertainty
about the sample is then much greater. Correspondingly, for the two cases, the entropy
is 0 and N log N&gt;&gt; 0, respectively.
Entropy plays a central role in the theory of information. For an excellent exposi-
tion of this theory, we refer the reader to Cover and Thomas (1991). The theory has been
applied in probabilistic language modeling (Mark, Miller, and Grenander 1996; Mark
et al. 1996; Johnson 1998), natural language processing (Berger, Della Pietra, and Della
Pietra 1996; Della Pietra, Della Pietra, and Lafferty 1997), as well as computational
vision (Zhu, Wu, and Mumford 1997). In addition, all the models proposed in these
</bodyText>
<note confidence="0.8584285">
articles are based on an important principle called the maximum entropy principle.
Chapter 11 of Cover and Thomas (1991) gives an introduction to this principle.
</note>
<page confidence="0.7397615">
1 A probabilistic context-free grammar is also called a stochastic context-free grammar (SCFG)
132
</page>
<note confidence="0.768435">
Chi Probabilistic Context-Free Grammars
</note>
<bodyText confidence="0.994734">
Briefly, the maximum entropy principle says that among all the distributions that
satisfy the same given conditions, the one that achieves the largest entropy should be
the model of choice. For a distribution p on parses, its entropy is
</bodyText>
<equation confidence="0.991892">
H(p) = E p(T) log p(iy).
</equation>
<bodyText confidence="0.99984">
In order that the maximum entropy principle makes sense, all the candidate distribu-
tions should have finite entropies, and this is usually implicitly assumed.
Take Mark, Miller, and Grenander&apos;s (1996) model, for example. First, a PCFG
distribution p is selected to serve as a &amp;quot;reference&amp;quot; distribution on parses. Then, by
invoking the minimum relative entropy principle, which is a variant of the maximum
entropy principle, the distribution that minimizes
</bodyText>
<equation confidence="0.841047333333333">
D(q11p) = q(T)log( q(T) g(r) log H(q)
1
PT) Per)
</equation>
<bodyText confidence="0.998944217391304">
subject to a set of constraints incorporating context-sensitive features is chosen to be
the distribution of the model. It is then easy to see that the assumption that H(q) is
finite is necessary.
Conceptually, having finite entropy is a basic requirement for a &amp;quot;good&amp;quot; proba-
bilistic model because a probability distribution with infinite entropy has too much
uncertainty to be informative.
Problems regarding entropies of PCFGs are relatively easy to tackle because they
can be studied analytically. Several authors have reported results on this subject, in-
cluding Miller and O&apos;Sullivan (1992), who gave analytical results on the rates of en-
tropies of improper PCFGs. It is worthwhile to add a few more results on entropies of
proper PCFGs. In this paper, we show that the entropies of PCFG distributions im-
posed by production probabilities assigned by the relative weighted frequency method
are finite (Section 4, Corollary 2).
In addition to entropy, we will also study the moment of sizes of parses. The mo-
ment is of statistical interest because it gives information on how sizes of parses are
distributed. For PCFG distributions, the first moment of sizes of parses, i.e., the mean
size of parses, is directly linked with the entropy: the mean size of parses is finite if
and only if the entropy is. The second moment of sizes is another familiar quantity.
The difference between the second moment and the mean squared is the variance of
sizes, which tells us how &amp;quot;scattered&amp;quot; sizes of parses are distributed around the mean.
Proposition 2 shows that, under distributions imposed by production probabilities as-
signed by the relative weighted frequency method, sizes of parses have finite moment
of any order.
</bodyText>
<subsectionHeader confidence="0.994008">
1.3 Gibbs Distributions on Parses and Renormalization of Improper PCFGs
</subsectionHeader>
<bodyText confidence="0.9999809">
Besides PCFG distributions, a CFG can be equipped with many other types of proba-
bility distributions. Among the most widely studied is the Gibbs distribution (Mark,
Miller, and Grenander 1996; Mark et al. 1996; Mark 1997; Abney 1997). Gibbs distribu-
tions arise naturally by invoking the maximum entropy principle. They are considered
to be more powerful than PCFG distributions because they incorporate more features,
especially context-sensitive features, of natural languages, whereas PCFG distributions
only consider frequencies of production rules. On the other hand, Gibbs distributions
are not always superior to PCFG distributions. A Gibbs distribution, with only fre-
quencies of production rules in parse as its features, turns into a PCFG. More specif-
ically, we will show in Proposition 4 in Section 5, that a CFG equipped with a Gibbs
</bodyText>
<page confidence="0.997707">
133
</page>
<note confidence="0.853346">
Computational Linguistics Volume 25, Number 1
</note>
<bodyText confidence="0.857281">
distribution of the form
</bodyText>
<equation confidence="0.932837666666667">
1
PA (T) = — eAA-..f(A ;T)
ZA fl
</equation>
<bodyText confidence="0.999748052631579">
is actually a PCFG, and we can get the production probabilities of the PCFG explicitly
from the Gibbs form.
The fact that a Gibbs distribution of the form in (2) is imposed by production
probabilities has a useful consequence. Suppose p is an improper PCFG distribution.
If we write the sum of p over all parses as Z, and assign to each parse tree a new
probability equal to p(r)/Z, then we renormalize p to a Gibbs distribution1.5 on parses.
What (2) implies is that /3 is also a PCFG distribution (Corollary 3). Moreover, in
Section 6 we will show that, under certain conditions, 13 is subcritical.
There is another issue about the relations between PCFG distributions and Gibbs
distributions of the form in (2), from a statistical point of view. Although PCFG dis-
tributions are special cases of Gibbs distributions in the sense that the former can be
written in the form of the latter, PCFG distributions cannot be put in the framework
of Gibbs distributions if they have different parameter estimation procedures. We will
compare the maximum-likelihood (ML) estimation procedures for these two distribu-
tions. As will be seen in Section 5, numerically these two estimation procedures are
different. However, Corollary 4 shows that they are equivalent in the sense that esti-
mates by the two procedures impose the same distributions on parses. For this reason,
a Gibbs distribution may be considered a generalization of PCFG, not only in form,
but also in a certain statistical sense.
</bodyText>
<subsectionHeader confidence="0.999599">
1.4 Branching Rates of PCFGs
</subsectionHeader>
<bodyText confidence="0.999987375">
Because of their context-free nature, PCFG distributions can also be studied from the
perspective of stochastic processes. A PCFG can be described by a random branch-
ing process (Harris 1963), and its asymptotic behavior can be characterized by its
branching rate. A branching process, or its corresponding PCFG, is called subcritical
(critical, supercritical), if its branching rate &lt; 1 (= 1,&gt; 1). A subcritical PCFG is always
proper, whereas a supercritical PCFG is always improper. Many asymptotic properties
of supercritical branching processes are established by Miller and O&apos;Sullivan (1992).
Chi and Geman (1998) proved the properness of PCFG distributions imposed by esti-
mated production probabilities, and around the same time Sanchez and Benedi (1997)
established the subcriticality of the corresponding branching processes, hence their
properness. In this paper we will explore properties of branching rate further. First,
in Proposition 5, we will show that if a PCFG distribution is imposed by production
probabilities assigned by the relative weighted frequency method, then the PCFG is
subcritical. The result generalizes that of Sanchez and Benedi (1997), and has a less
involved proof. Then in Proposition 7, we will demonstrate that a connected and
improper PCFG, after being renormalized, becomes a subcritical PCFG.
</bodyText>
<subsectionHeader confidence="0.999217">
1.5 Identifiability and Approximation of Production Probabilities
</subsectionHeader>
<bodyText confidence="0.999752428571429">
Returning to the statistical aspect of PCFGs, we will discuss the identifiability of pro-
duction probabilities of PCFGs as well as parameters of Gibbs distributions. Briefly
speaking, production probabilities of PCFGs are identifiable, which means that differ-
ent production probabilities always impose different distributions on parses (Proposi-
tion 8). In contrast, for the Gibbs distribution given by (2), the A parameters are not
identifiable; in fact, there are infinitely many different A that impose the same Gibbs
distribution.
</bodyText>
<figure confidence="0.6457925">
(2)
(A-,a)R
</figure>
<page confidence="0.961476">
134
</page>
<subsectionHeader confidence="0.321506">
Chi Probabilistic Context-Free Grammars
</subsectionHeader>
<bodyText confidence="0.999833666666667">
Finally, in Proposition 9, we propose a method to approximate production prob-
abilities. Perhaps the most interesting part about the result lies in its proof, which is
largely information theoretic. We apply the Kullback-Leibler divergence, which is the
information distance between two probability distributions, to prove the convergence
of the approximation. In information theory literature, the Kullback-Leibler divergence
is also called the relative entropy. We also use Lagrange multipliers to solve the con-
strained minimization problem involved. Both Kullback-Leibler divergence and La-
grange multipliers method are becoming increasingly useful in statistical modeling,
e.g., modeling based on the maximum entropy principle.
</bodyText>
<subsectionHeader confidence="0.998686">
1.6 Summary
</subsectionHeader>
<bodyText confidence="0.999479166666667">
As a simple probabilistic model, the PCFG model is applied to problems in linguistics
and pattern recognition that do not involve much context sensitivity. To design sensi-
ble PCFG distributions for such problems, it is necessary to understand some of the
statistical properties of the distributions. On the other hand, the PCFG model serves as
a basis for more expressive linguistic models. For example, many Gibbs distributions
are built upon PCFG distributions by defining
</bodyText>
<equation confidence="0.9989905">
p(y)e(T)
P(r) = z
</equation>
<bodyText confidence="0.999946266666667">
where p is a PCFG distribution. Therefore, in order for the Gibbs distribution P to
have certain desired statistical properties, it is necessary for p to have those properties
first. This paper concerns some of the fundamental properties of PCFGs. However, the
methods used in the proofs are also useful for the study of statistical issues on other
probabilistic models.
This paper proceeds as follows: In Section 2, we gather the notations for PCFGs
that will be used in the remaining part of the paper. Section 3 establishes the rel-
ative weighted frequency method. Section 4 proves the finiteness of the entropies
of PCFG distributions when production probabilities are assigned using the relative
weighted frequency method. In addition, finiteness of the moment of sizes of parses
are proved. Section 5 discusses the connections between PCFG distributions and Gibbs
distributions on parses. Renormalization of improper PCFGs is also discussed here.
In Section 6, PCFGs are studied from the random branching process point of view.
Finally, in Section 7, identifiability of production probabilities and their approximation
are addressed.
</bodyText>
<sectionHeader confidence="0.864282" genericHeader="categories and subject descriptors">
2. Notations and Definitions
</sectionHeader>
<bodyText confidence="0.9991605">
In this section, we collect the notations and definitions we will use for the remaining
part of the paper.
</bodyText>
<subsectionHeader confidence="0.876971">
Definition 1
</subsectionHeader>
<bodyText confidence="0.996641">
A context-free grammar (CFG) G is a quadruple (N,T,R,S), where N is the set of
variables, T the set of terminals, R the set of production rules, and S E N is the start
symbol.2 Elements of N are also called nontermirtal symbols. N, T, and R are always
</bodyText>
<footnote confidence="0.5590525">
2 Some of our discussion requires that each sentential form have only finitely many parses. For this
reason, we shall assume that in G, there are no null or unit productions.
</footnote>
<page confidence="0.989806">
135
</page>
<note confidence="0.43419">
Computational Linguistics Volume 25, Number 1
</note>
<bodyText confidence="0.833877">
assumed to be finite. Let Q denote the set of finite parse trees of G, an element of
which is always denoted as T. For each 7 E Q and each production rule (A —&gt; a) E R,
define f(A -+ a; 7) to be the number of occurrences, or frequency, of the rule in T, and
f (A; 7) to be the number of occurrences of A in T. f (A; r) and f (A -4 a; 7) are related
by
</bodyText>
<equation confidence="0.995341666666667">
f (A; 7) = E f (A a; r).
cte (NUT)*
s.t. (A—a)ER
</equation>
<bodyText confidence="0.9545382">
Define h(r) as the height of T, which is the number of nonterminal nodes on the
longest route from T&apos;s root to its terminal nodes. Define 17-1 as the size of 7, which is
the total number of nonterminal nodes in T. For any A E N and any sentential form
E (N U T)*, define n(A; 7) as the number of instances of A in 7. Define 171 as the
length of the sentential form.
</bodyText>
<sectionHeader confidence="0.447333" genericHeader="method">
Definition 2
</sectionHeader>
<bodyText confidence="0.9994096">
Let A E r denote that the symbol A occurs in the parse T. If A E r, let TA be the
left-most maximum subtree of T rooted in A, which is the subtree of T rooted in A
satisfying the condition that if T1 0 TA is also a subtree of 7 rooted in A, then TI is
either a subtree of TA, or a right sibling of TA, or a subtree of a right sibling of TA. Let
AT be the root of TA, which is the left-most &amp;quot;shallowest&amp;quot; instance of A in T.
</bodyText>
<subsectionHeader confidence="0.378662">
Definition 3
</subsectionHeader>
<bodyText confidence="0.9958968">
For any two symbols A E N and B E NU T, not necessarily different, B is said to be
reachable from A in G, if there is a sequence of symbols Ao, A1,. , An with Ao = A
and An = B, and a sequence of sentential forms ap, , an_i, such that each A, a,
is a production in R and each a, contains the next symbol A,+1. G is called connected
if all elements in N U T can be reached from all nonterminal symbols.
We now define the probabilistic version of reachability in CFG. Suppose p is a
distribution on Q. For any two symbols A E N and B E NUT, not necessarily different,
B is said to be reachable from A in G under p, if there is aTEQ with p(r) &gt; 0 and
there is a subtree Tf of T, such that Tf is rooted in A and B E TI G is called connected
under p if all symbols in NU T can be reached from all nonterminal symbols under p.
</bodyText>
<subsectionHeader confidence="0.487924">
Definition 4
</subsectionHeader>
<bodyText confidence="0.965312">
A system of production probabilities of G is a function p : R —&gt; [0,1] such that for any
AEN,
</bodyText>
<equation confidence="0.983415666666667">
E p(A —&gt; a) = 1. (3)
ceE(NUT)*
s.t. (A—,a)ER
</equation>
<bodyText confidence="0.7951175">
We will also use p to represent the PCFG probability distribution on parses im-
posed by p, via the formula
</bodyText>
<equation confidence="0.9865375">
p(r) =- H p(A a)fT) (4)
(A—,•ce)ER
</equation>
<page confidence="0.98745">
136
</page>
<subsectionHeader confidence="0.327879">
Chi Probabilistic Context-Free Grammars
</subsectionHeader>
<bodyText confidence="0.997183333333333">
Similarly, for any estimated system of production probabilities P, we will also use p to
represent the probability distribution on parses imposed by fr. We will write p(ft) as
the total probability of all finite parse trees in a
</bodyText>
<subsectionHeader confidence="0.910948">
Definition 5
</subsectionHeader>
<bodyText confidence="0.996397666666667">
Now we introduce a notation in statistics. Let p be an arbitrary distribution on C2 and
g(r) a function of T E a The expected value of g under the distribution p, denoted
Epg(r), is defined as
</bodyText>
<equation confidence="0.900936333333333">
Epg(T) = E per)ger).
rei-2
Definition 6
</equation>
<bodyText confidence="0.9639275">
All the parse trees we have so far seen are rooted in S. It is often useful to investigate
subtrees of parses, therefore it is necessary to consider trees rooted in symbols other
than S. We call a tree rooted in A ENa parse (tree) rooted in A if it is generated from
A by the production rules in R. Let fill be the set of all finite parse trees with root A.
Define PA as the probability distribution on 12,4 imposed by a system of production
probabilities p, via (4). Also extend the notions of height and size of parses to trees
A.
When we write PA (r), we always assume that T is a parse tree rooted in A. When
p = PA, Epg(T) equals Epg(r) = OA pAer)g(r). We will use p(QA) instead of pA(S-2/1) to
denote the total probability of finite parse trees in A. With no subscripts, C2 and p are
assumed to be Sts and ps, respectively.
For convenience, we also extend the notion of trees to terminals. For each terminal
t c T, define 12t as the set of the single &amp;quot;tree&amp;quot; {t}. Define pt(t) = 1, It = 0 and h(t) = 0.
For this paper we make the following assumptions:
</bodyText>
<listItem confidence="0.747783428571429">
1. For each symbol A S, there is at least one parse T with root S such that
A E T. This will guarantee that each A S can be reached from S;
2. When a system of production probabilities p is not explicitly assigned,
each production rule (A —&gt; a) e R is assumed to have positive
probability, i.e., p(A —&gt; a) &gt; 0. This guarantees that there are no useless
productions in the PCFG.
3. Relative Weighted Frequency
</listItem>
<bodyText confidence="0.996012571428571">
The relative weighted frequency method is motivated by the maximum-likelihood
(ML) estimation of production probabilities. We shall first give a brief review of ML
estimation.
We consider two cases of ML estimation. In the first case, we assume the data
are fully observed, which means that all the samples are fully observed finite parses
trees. Let , be the samples. Then the ML estimate of p(A --&gt; a) is the ratio
between the total number of occurrences of the production A —&gt; a in the samples and
</bodyText>
<page confidence="0.987148">
137
</page>
<note confidence="0.405362">
Computational Linguistics Volume 25, Number 1
</note>
<bodyText confidence="0.547092">
the total number of occurrences of the symbol A in the samples,
</bodyText>
<equation confidence="0.97956625">
&gt;2f (A —&gt; a; TO
—&gt; a) = (5)
f (A; Ti)
1=1
</equation>
<bodyText confidence="0.97449075">
Because of the form of the estimator in (5), ML estimation in the full observa-
tion case is also called relative frequency estimation in computational linguistics. This
simple estimator, as shown by Chi and Geman (1998), assigns proper production prob-
abilities for PCFGs.
In the second case, the parse trees are unobserved. Instead, the yields Y1 =
= Y(T), which are the left-to-right sequences of terminals of the un-
known parses rl I • • • form the data. It can be proved that the ML estimate is
given by
</bodyText>
<equation confidence="0.9996325">
EE,-,[f(A a; T)IT E C2Y1]
po -4 a) = 1=1 n (6)
EE,,[fo;7-)IT c fly]
i=1
</equation>
<bodyText confidence="0.95954325">
where C21( is the set of all parses with yield Y, i.e., Sly = E C2 : Y(T) = Y}.
Equation (6) cannot be solved in closed form. Usually, the solution is computed
by the EM algorithm with the following iteration (Baum 1972; Baker 1979; Dempster,
Laird, and Rubin 1977):
</bodyText>
<equation confidence="0.992095333333333">
E Efr,[ f (A —4 a; T)IT E
Pk-Fi (A a) _ i=1 71 (7)
/3k[f(A; T)IT E
</equation>
<bodyText confidence="0.99686725">
Like in (5), Pk for k&gt; 0 impose proper probability distributions on C-2 (Chi and Geman
1998).
To unify (6) and (7), expand Efrk [ f (A -4 a; 7-) IT e fly], by the definition of expecta-
tion, into
</bodyText>
<equation confidence="0.7835885">
[f; (A —&gt; a; T) I T E SZyj =E f (A —&gt; a; -,-)j)k (r E Qyi).
rEcty,
</equation>
<bodyText confidence="0.7914825">
Let A be the set of parses whose yields belong to the data, i.e., A = {T : Y(T) E
{Yi, • ..,1(n}}. For each T E A, let y = Y(T) and
</bodyText>
<equation confidence="0.997329">
W(r) = LPk(TIT E
i:Y i=y
</equation>
<bodyText confidence="0.721522">
Then we observe that, for any production rule A —&gt; a,
</bodyText>
<equation confidence="0.867388545454545">
E E f(A a; T)Pk(TIT E Q1(1) = f(A —+ a; r)W(T)
i=1
138
Chi Probabilistic Context-Free Grammars
E ENV (A a; 7)17 E Oyj = E f (A
r=1 TEA
Therefore, (7) is transformed into
Ef (A —›
TEA
E f (A; 7-) IN (r)
TEA
</equation>
<bodyText confidence="0.999486888888889">
The ML estimator in (6) can also be written in the above form, as can be readily
checked by letting A be the set {71, ,r,,} and W(7), for each 7- E A, be the number
of occurrences of T in the data. In addition, in both full observation cases and partial
observation cases, we can divide the weights of W(7) by a constant so that their sum
is 1.
The above discussion leads us to define a procedure to assign production proba-
bilities as follows. First, pick an arbitrary finite subset A of Q, with every production
rule appearing in the trees in A. Second, assign to each 7 EAa positive weight W(7)
such that ETEA w(T) = 1. Finally, define a system of production probabilities p by
</bodyText>
<equation confidence="0.72894325">
Ef (A —› (8)
p (A —4 a) = TEA
E f (A; r)W (T)
TEA
</equation>
<bodyText confidence="0.9994915">
Because of the similarity between (5) and (8), we call the procedure to assign produc-
tion probabilities by (8) the &amp;quot;relative weighted frequency&amp;quot; method.
</bodyText>
<subsectionHeader confidence="0.762934">
Proposition 1
</subsectionHeader>
<bodyText confidence="0.999758">
Suppose all the symbols of N occur in the parses of A, and all the parses have positive
weight. Then the production probabilities given by (8) impose proper distributions on
parses.
</bodyText>
<subsectionHeader confidence="0.935196">
Proof
</subsectionHeader>
<bodyText confidence="0.9999458">
The proof is almost identical to the one given by Chi and Geman (1998). Let q A = p
(derivation tree rooted in A fails to terminate). We will show that qs = 0 (i.e., derivation
trees rooted in S always terminate). For each A E V. let f (A; 7) be the number of non-
root instances of A in T. Given a E (V U T)*, let a, be the ith symbol of the sentential
form a. For any A E V
</bodyText>
<equation confidence="0.9782693">
1- 9 U U{derivation begins A a, and a, fails to terminate}
(A—oe)ER
- Ea )19 U{a= fails to terminate})
(A-4oz)ER
&lt; E p (A ) p( fa, fails to terminatel)
(A—a)ER
- E p(A a) E n(B; a)(//3
(A—c)ER BEV
13k±i
qA
</equation>
<page confidence="0.927969">
139
</page>
<note confidence="0.327226">
Computational Linguistics Volume 25, Number 1
</note>
<equation confidence="0.843600375">
qB
{E(A*a)ERn(B; a) ETEAf(A -+a; T)W(T)}
L
BEV E,Af(A; T)w(y)
qA E f (A; r)W(7) &lt; E qB E E n(B;a)f (A ---&gt; a; T)W(T)
TEA BEV TEA (A --4a)ER
Sum over A E V:
E qA E f (A; T)W (T) 5 E q, E E E n(B; ay (A —&gt; ct; T)1A1 (T)
/ley TEA BEV TEA AEV (A—&gt;a)ER
= Eq, Ei(B;T)w(y)
BEV TEA
E qA Ev(A; y) — f (A; r))W (T) &gt; 0
AEv TEA
Clearly, for every T E A, f(A;r) = f(A;T) whenever A S and -PS; T) = f(S;T) — 1.
Hence qs = 0, completing the proof. 0
Corollary 1
</equation>
<bodyText confidence="0.796313">
Under the same assumption of Proposition 1, for each symbol A E N, p(C2A) = 1.
</bodyText>
<subsectionHeader confidence="0.809313">
Proof
</subsectionHeader>
<bodyText confidence="0.9798319375">
For any A E N, there is a TE A such that A E T. Since p(T) &gt; 0, this implies A is
reachable from S under p. Using the notation given in Definition 2, we have
qs &gt; p({A E T and TA fails to terminate})
= p({TA fails to terminate} IA E T)p(A c T)
p({TA fails to terminate} IA E 7-) = 0,
since qs = 0 and p(A E T) &gt; 0. By the nature of PCFGs, the form of TA is distributed
according to PA, independent of its location in T or of the choice of subtrees elsewhere
in T. Therefore the conditional probability of TA failing to terminate, given that A
occurs in T, equals qA, proving that qA = 0. 0
4., Entropy and Moments of Parse Tree Sizes
In this section, we will first show that if production probabilities are assigned by
the relative weighted frequency method, then they impose PCFG distributions under
which parse tree sizes have finite moment of any order. Based on this result, we
will then demonstrate that such PCFG distributions have finite entropy and give the
explicit form of the entropy.
The mth moment of sizes of parses is given by
</bodyText>
<equation confidence="0.806892">
Epirrn = peolyr
TEQ
</equation>
<page confidence="0.963435">
140
</page>
<subsectionHeader confidence="0.771596">
Chi Probabilistic Context-Free Grammars
</subsectionHeader>
<bodyText confidence="0.986045">
and the entropy of a P6FG distribution p is given by
</bodyText>
<equation confidence="0.966275">
H(p) = E p(r) log p(1r),
Tes2
</equation>
<bodyText confidence="0.894418">
To make the proofs more readable, we define, for any given A = {71, ..., Tn},
</bodyText>
<equation confidence="0.984971">
F (A —&gt; a) = Ef (A —&gt; a; &apos;OW (T),
TEA
for any (A a) E R, and
F (A) = F (A —&gt; a) =
aE(NUT)* TEA
s.t. (A—,a)ER
</equation>
<bodyText confidence="0.9024855">
for any A e N; that is, F (A —&gt; a) is the weighted sum of the number of occurrences
of the production rule A —4 a in A and F (A) is the weighted sum of the number of
occurrences of A in A.
The relative weighted frequency method given by (8) can be written as
</bodyText>
<equation confidence="0.998686666666667">
F (A —&gt;
p(A —&gt; a) = (9)
F (A)
</equation>
<bodyText confidence="0.997726">
We have the following simple lemma:
</bodyText>
<figure confidence="0.960137571428571">
Lemma 1
For any A E N,
E F (A) = E 17-iw(y) (10)
AEN TEA
and
E E F(
BEN 7 s.t. F (B —&gt; ry)n(A; 7) = S)— 1 if A = S F(A) if A S
(B—,y)ER
(If ETEA W(T) 1, F(S) — 1 should be changed to F(S) ETGA WM.)
Proof
For the first equation,
E F (A) = E Ef(A;r)W(r) = E Ef(A;7-)W(r) = E
AEN AEN EA TEA AEN TEA
For the second equation,
</figure>
<table confidence="0.86187525">
E E F(B —&gt; 7)n(A; = v
BEN 7 s.t. f (B -y; T)W (T)n(A; -y)
(B—,.y)ER BEN 7 s.t. rEA
(B—&gt;7)ER
=
W(T) E E f (B -y; T)n(A; 7) (12)
TEA BEN sot.
(B—y)ER
</table>
<page confidence="0.865198">
141
</page>
<figure confidence="0.8560296">
Computational Linguistics Volume 25, Number 1
For each A,
E E f(B —&gt; r)n(A; 7)
BEN -y s.t.
(B—■7)ER
</figure>
<figureCaption confidence="0.725096">
is the number of nonroot instances of A in T. When A 0 S, the number of nonroot
instances of A in 7- is equal to f (A; 7). Substitute this into (12) to prove (11) for the case
</figureCaption>
<bodyText confidence="0.99604">
A 0 S. The case A = S is similarly proved. 0
</bodyText>
<subsectionHeader confidence="0.87735">
Proposition 2
</subsectionHeader>
<bodyText confidence="0.999909">
Suppose all the symbols in N occur in the parses of A, and all parses have posi-
tive weights. If the production probabilities p are assigned by the relative weighted
frequency method in (8), then for each mENU {0}, EIr I &lt;00.
</bodyText>
<subsectionHeader confidence="0.836641">
Proof
</subsectionHeader>
<bodyText confidence="0.993106">
We shall show that for any A E N, if p = PA, then EpIrl. &lt; 00. When m = 0, this is
clearly true. Now suppose the claim is true for 0, m — 1. For each A E N and k E N,
define
</bodyText>
<equation confidence="0.961630875">
MkA = E pANITini•
TE0A
h(r)&lt;k
It is easy to check
= E E (1+ E irirp(A a)pai(ri) .. • p,,,,(71), (13)
aE (NUT)* 71,-ir i=1
(A--,a)ER TiE
h(1-)&lt;k
</equation>
<bodyText confidence="0.993292">
where for ease of typing, we write L for la l. For fixed a, write
</bodyText>
<equation confidence="0.908342">
(1+ EiTir = parii, • • „imp + &gt;2 IIm.
,=, i=1
P is a polynomial in HI, , Ird, each term of which is of the form
1711&amp;quot; I T2 I2 ITL 1st 0 &lt; si &lt; m, s1 + B2 + &lt; M. (14)
</equation>
<bodyText confidence="0.5920225">
By induction hypothesis, there is a constant C &gt; 1, such that for all 0 &lt; s &lt; m and
A GNU T,
</bodyText>
<equation confidence="0.761272166666667">
E PANITis = EpAiTis &lt; C.
TE0A
Then for each term with the form given in (14),
EHI&amp;quot; • • • 17-LisLp., (T1) p (TO = E ITirp.,(T) CL.
, .....TL
TjEnc,
</equation>
<page confidence="0.908309">
142
</page>
<figure confidence="0.675854958333333">
Chi Probabilistic Context-Free Grammars
There are less than Lm = la rn terms in F(17-11,• • • ,1711)• Hence
EPari 1, ..., ITLI)po a)Paieri) larded .
,...,TLTi
E i
h(Ti)_&lt;k
So we get
Mk+1,A larcialpo a)
aE(NUT)*
s.t. (A—&gt;a)ER
la I
E E E ITirp(A a)Poq (TO • • • Pal., (T1.1)
a E (NUT)* T1 .....T11 i=1
s.t. (A —■01)ER riEQ.i
h(T)&lt;k
1.1
larcHp(A — a) + E Emk,aip(A a).
ctE (NUT)* E (NUT)* i=1
s.t. (A—cs)ER s.t. (A—a)ER
Because the set of production rules is finite, the length of a sentential form that occurs
on the right-hand side of a production rule is upper bounded, i.e.,
sup{ jal : for some A E N, (A —&gt; a) E RI &lt;00.
Therefore we can bound (la&apos; + 1)7nClal by a constant, say, K. Then we get
1.1
Mk+1,A &lt; K + mkp(A —&gt; a). (15)
ctE (NUT)* i=1
s.t. (A—&gt;a)ER
Replace p(A —&gt; a) by F(A a)/F(A), then multiply both sides of (15) by F(A)
and sum over all A E N with F(A) &gt; 0. By (10) and (11), we then get
II
Emk+LAF(A) &lt; KE iyiw(T) + E
AEN TEA AGN E (NUT)* i=1
s.t. (A—&gt;a)ER
HW(r) + E E n(B; a)Mk,BF(A —&gt; a)
TEA AEN aE(NUT)* BEN
s.t. (A—cs)ER
= KE 1.7-1w(T) + Emk,B E E
TEA BEN AEN aE(NUT)*
s.t. (A—,a)ER
Iw(r) + Emk,BF(B) +mk,s(F(s) —1). (by (11))
TEA Bos
Because for each A E N, Mk+1A &gt; MkA, we get
E mk,AF(A) K E 17-iw(y) + MkAF(A) + Mk,s(F(S) — 1)
AEN TEA AOS
&gt; Mk,S K E mw(T) &lt;00.
TEA
mk,.,F(A a) (by (10))
n(B;a)F(A —&gt; a)
</figure>
<page confidence="0.856711">
143
</page>
<table confidence="0.6840012">
Computational Linguistics Volume 25, Number 1
Letting k ---+ 00, by Mks I Eirim, we get E5irIm&lt; K ET EA IT IW (T) &lt;00. To complete
the induction, we shall show for every A E NUT other than S, EP, 17-1m oo.
By conditional expectation, there is (see Definition 2 for the notations A E 7 and
TA)
Eps(17-Im) = Eps(Irlm IA E T)Ps(A E 7-) + Eps(17-Im IAcZ 7-)ps(A r) (16)
E5 intm
Eps (ITIm IA E &lt;
P S(A E r) cc&apos;
since ps(A E T) &gt; 0. Because 1 TAI &lt; III,Eps (irAlm IA E T) &lt;00.
</table>
<bodyText confidence="0.9878455">
As in the proof of Corollary 1, TA is independent of its location and other part of
7-, and is distributed by PA. Therefore
</bodyText>
<equation confidence="0.933688">
PS(7A IA E 7) = PA(711),
</equation>
<bodyText confidence="0.934865333333333">
which leads to EpA 17-1m = Eps(ITAim IA E n) &lt; 00. 0
From Proposition 2 it follows that the mean size of parses is finite under p. Since
f (A a; T) &lt; 17-1 for each production A —&gt; a, it follows that the mean frequency of
f (A a; T) is finite. The next proposition gives the explicit form of the mean frequency
in terms of the production probabilities assigned by the relative weighted frequency
method.
</bodyText>
<subsectionHeader confidence="0.684927">
Proposition 3
</subsectionHeader>
<bodyText confidence="0.992162666666667">
Under the same conditions of Proposition 2, the mean frequency of the production
rule (A —&gt; a) E R is the weighted sum of the numbers of its occurrences in parses of
A, with weights W(T), i.e.,
</bodyText>
<equation confidence="0.5350485">
Epf (A a; T f (A —4 a; 7-)W(7-) (17)
Proof
</equation>
<bodyText confidence="0.99348125">
Fix (A a) E R. For each C E N, write E(C) for Ep, f (A a; 7-). We shall find the
linear relations between E(C). To this end, for each T E C2c, let C --+ be the production
rule applied to T&apos;S root. Suppose -y is composed of m symbols, . ,rym, and 7-m
are the daughter subtrees of T rooted in 77, • • • ,77n, respectively. Then
</bodyText>
<equation confidence="0.9142225">
f (A —&gt; a; T) = x (C 7) f (A —4 a; 7-k),
k=1
</equation>
<bodyText confidence="0.678411">
where
</bodyText>
<equation confidence="0.9738675">
f0 if C----&gt;71-4a,
X(C — 1 1 otherwise.
</equation>
<bodyText confidence="0.9917295">
Multiply both sides by p(r) and sum over all T E C2c which have C —&gt; -y as
the production rule applied at the root. By the definition of PCFG, p(r) = p(C —&gt;
</bodyText>
<page confidence="0.984791">
144
</page>
<note confidence="0.469885">
Chi Probabilistic Context-Free Grammars
</note>
<equation confidence="0.9993824">
r)p(7-i) • • • p(rm), and rk can be any parse in Ilk. Therefore, by factorization, we get
E x(c &apos;OP(T) = E x(c 7)/9(c — 7)/9(70 • • • p(-ni)
p(C 7)X(C 7) H P(Q-yk)
k=1
p(C —&gt; 7)x(C —&gt; 7), (All p(12-yk) = 1 by Proposition 1),
</equation>
<bodyText confidence="0.9995945">
where 12c-1, stands for the set of trees in which C 7 is the rule applied at the root.
Similarly, for each k,
</bodyText>
<equation confidence="0.944426357142857">
E f (c —&gt; Tk)p(T) = p(C — 7) E f (c 7; TOP (m) p(7-,)
TEOC rES2c_., i=1
i0k
= P( 7)E(7k).
Therefore we get
E p(r)f(A —&gt; a;T) = P( 7)(x(c 7) + E E(7k))
k=1
= n(B; 7)E(7k))-
BEN
s.t. BEy
Sum over all production rules for C. The left-hand side totals E(C) and
E(C) = E p(C —&gt; -y)(x(C --&gt; 7) + E n(B; -y)E(B)).
&apos;YE (NUT)* BEN
s.t. (C—,7)ER s.t. BE&apos;Y
Replace p(C —&gt; -y) by F(C —4 7)/F(C), according to (9). Then multiply both sides by
F(C) and sum both sides over all C E N. We get
E F(C)E(C) E E F(c -y)x(c — 7)
CEN CEN E(NUT)*
s.t. (C—,y)ER
+ E E F(C -&apos;y) E n(B; -y)E(B)
CEN E (NUT)* BEN
s.t. (C—y)ER s.t. Bey
= F(Aa)+E(B) E E F(C --&gt; -y)n(B; -y)
BEN CEN 7 s.t.
(C--■-y)ER
F(A —* a) + &gt; E(B)F(B) — E(S) (By (11))
BEN
E(S) = F(A a),
</equation>
<bodyText confidence="0.93857">
completing the proof of (17). 0
Now we can calculate the entropy of p in terms of production probabilities.
</bodyText>
<page confidence="0.979339">
145
</page>
<figure confidence="0.497660857142857">
Computational Linguistics Volume 25, Number 1
Corollary 2
Under the conditions in Proposition 2,
1
H(p) =- E F (A —&gt; a) log 1 F (A a F (A) log F(A)&apos;
(A--■a)ER AEN
which is clearly finite.
</figure>
<subsectionHeader confidence="0.348225">
Proof
</subsectionHeader>
<bodyText confidence="0.611385">
The calculation goes as follows,
</bodyText>
<equation confidence="0.972499">
1
H(p) = E p(T) log
TEst
- E p(r) log
ct H p(A —&gt; a)fT)
TE
(A—.c)ER
- E p(r) E f (A —&gt; a; 7) log 1
p(A a)
TES2 (A--■a)ER
=_ E E p(r)f (A —&gt; a; r) log 1
p(A &amp;quot;a)
(Exchange the order of summation)
(A—&gt;a)ER rEft
- E Epf (A --&gt; a; r) log a)
(A—&gt;a)ER
F (A)
(A--..a)ER TEA
- E E f (A —&gt; a; r)W (r) logF(A)
(A—,a)ER TEA
- E E f (A —&gt; a; T)W(T) logF(A —&gt; a)
(A—a)ER TEA
- E F (A) logF(A) — E F (A a) logF(A a). 0
AEN (A—&gt;a)ER
</equation>
<sectionHeader confidence="0.362302" genericHeader="method">
5. Gibbs Distributions on Parses and Renormalization of Improper PCFGs
</sectionHeader>
<bodyText confidence="0.627644">
A Gibbs distribution on parses has the form
</bodyText>
<equation confidence="0.6883195">
PA (r) = eA•U(T)
ZA
</equation>
<bodyText confidence="0.999880428571429">
where ZA = E eA-u(T), and A = {Ai} and LI(r) -= {LIi(&apos;)} are constants and functions
on 9, respectively, both indexed by elements in a finite set I. The inner product A • LI =
E AiU, is called the potential function of the Gibbs distribution and Z is called the
partition number for the exponential
The functions LI are usually considered features of parses and the constants Ai
are weights of these features. The index set I and the functions U(r) can take var-
ious forms. Among the simplest choices for I is R, the set of production rules, and
</bodyText>
<page confidence="0.529075">
1
146
</page>
<figure confidence="0.973615125">
Chi Probabilistic Context-Free Grammars
correspondingly,
U(r) -=- f (T) = {f (A —&gt; a;7-)}(A)ER. (18)
Given constants A, if ZA &lt; oo, then we get a Gibbs distribution on parses given by
eAf(T) (19)
PAN LA
•
A proper PCFG distribution is a Gibbs distribution of the form in (19). To see this,
</figure>
<equation confidence="0.944659666666667">
let AA0, = log p(A a) for each (A —&gt; a) E R. Then
z, EeAf(T) = E p(r) =1
TEsz Tesz
p(T) =fi
p _y a)f(A--4a; 7-) eAf(T) 1 eA•11(7)
(Aa)€R Z A
</equation>
<bodyText confidence="0.972333777777778">
which is a Gibbs form.
A Gibbs distribution usually is not a PCFG distribution, because its potential
function in general includes features other than frequencies of production rules. What
if its potential function only has frequencies as features? More specifically, is the Gibbs
distribution in (19) a PCFG distribution? The next proposition gives a positive answer
to this question.
Proposition 4
The Gibbs distribution PA given by (19) is a PCFG distribution. That is, there are
production probabilities p, such that for every T E Q,
</bodyText>
<equation confidence="0.96146">
P A (7) ----- fi p(A --&gt; a)f.
(A–ce)ER
Proof
</equation>
<bodyText confidence="0.999104666666667">
The Gibbs distributions we have seen so far are only defined for parses rooted in S. By
obvious generalization, we can define for each nonterminal symbols A the partition
number
</bodyText>
<equation confidence="0.9948195">
z,(A)= E eAf (7)
TE 0A
</equation>
<bodyText confidence="0.9583345">
and the Gibbs distribution P(r) on parses rooted in A. For simplicity, also define
ZA (t) = 1 and Pt(t) =- 1 for each t E T.
We first show ZA (A) &lt; no for all A. Suppose (S a) E R with la I = n. The
sum of eA f (T) over all T E C2s with S a being applied at the root is equal to
</bodyText>
<equation confidence="0.837365333333333">
eAs—ZA(ai) ZA (an), while less than the sum of eA f (T) over all T E Qs, which is
ZA (S). Therefore,
ZA (S) &gt; eAs—ZA(cei) • • • ZA (an).
</equation>
<bodyText confidence="0.9848215">
Since ZA &lt; 00 and ZA (A) &gt; 0, for all A, it follows that Z A (ai) is finite. For any variable
A, there are variables Ao = S, Ai, ,An = A E N and sentential forms aM, , a(n-1) E
</bodyText>
<page confidence="0.987379">
147
</page>
<note confidence="0.503572">
Computational Linguistics Volume 25, Number 1
</note>
<equation confidence="0.92063168">
(N U T)*, such that (Ai —&gt; a(1)) E R and /11.4.1 E a(l). By the same argument as above,
we get
la(i)1
ZA(Ai) &gt; eAAi—a(i) H ZA(a/(ci)),
k=1
where ai(:) is the kth element in a(t). By induction, Z( A) &lt;o0.
Now for (A —&gt; a) E R, with I al = n, define
p(A —&gt; a) = zAlweAA—ZA(ai)...ZA(an), (20)
Since Z(A) and ZA (a,) are finite, p(A —&gt; a) is well defined.
The p&apos;s form a system of production probabilities, because for each A E N,
10,1
E p(A --&gt; a) = eAA-- HZA(ak) =
Z(A) eA-f(T) =1
1
(A-,a)ER (A—&gt;a)ER k=1 TEQA
We shall prove, by induction on h(r), that
P(i-) =H p(A
(A—.o)ER
When h(r) = 0, T is just a terminal, and the equation is obviously true. Suppose the
equation is true for all T E SZA with h(r) &lt; h, and all A E N. For any T E S2A with
h(r) = h, let A —&gt; /3 = • • • Om be the production rule applied at the root. Then
\ 1 f(T) 1 H
onA_.0 -Af(Tk)
Pjft T ZA(A)e ZA(A)- ,
k=1
</equation>
<bodyText confidence="0.7852935">
where rk is the daughter subtree of T rooted in /3k. Each rk has height &lt;h. Hence, by
induction assumption,
</bodyText>
<equation confidence="0.99471995">
1
CAl(Tk) = P() = H p(B —&gt; a)f(B—&gt;&amp;quot;;&amp;quot;-k)
ZA(0k)
(B—&gt;a)ER
eAf(m) = ZA (13k) p(B a)f(Tk)
(B—a)ER
P
1
He)l(Tk)
Z( A)
k=1
1
= ZA (A)eAA-0 Hz),(00 H p(B—
k=1 (B—&gt;a)ER
cof(B—&gt;ce;rk)
= P H fi p(B —&gt;
k=1 (B—&gt;a)ER
= fi p(B cv)fT),
(B—&gt;a)ER
proving P), is imposed by p. 0
</equation>
<page confidence="0.986956">
148
</page>
<note confidence="0.490437">
CM Probabilistic Context-Free Grammars
</note>
<bodyText confidence="0.979210666666667">
Proposition 4 has a useful application to the renormalization of improper PCFGs.
Suppose a PCFG distribution p on Q = Qs is improper. We define a new, proper
distribution /5 on Q, by
</bodyText>
<equation confidence="0.9101665">
rEft
P(Q)
</equation>
<bodyText confidence="0.6065075">
We call /5 the renormalized distribution of p on Q. We can also define the renormalized
distribution of PA on A. for each A E N, by
</bodyText>
<equation confidence="0.9995795">
(T) = p(1A)&apos;
PAN T E (21)
</equation>
<bodyText confidence="0.972078">
Comparing p with (19), we see that p is a Gibbs distribution with frequencies of
production rules as features. Therefore, by Proposition 4, p is a PCFG distribution, and
from the proof of Proposition 4, we get Corollary 3.
Corollary 3
Suppose the production probabilities of the improper distribution p are positive for
all the production rules. Then the renormalized distributions P are induced by the
production probabilities
</bodyText>
<equation confidence="0.8465156">
PO -4 a) = a)
P(CIA) BEN
H 0-2,13)n(B; a). (22)
Therefore, p on Q is a PCFG distribution.
Proof
</equation>
<bodyText confidence="0.99986725">
The only thing we have not mentioned is that AA_,(2, = log p(A -4 a) are all bounded,
since p are all positive.
We have seen that PCFG distributions can be expressed in the form of Gibbs distri-
butions. However, from the statistical point of view, this is not enough for regarding
PCFG distributions as special cases of Gibbs distributions. An important statistical
issue about a distribution is the estimation of its parameters. To equate PCFG distri-
butions with special cases of Gibbs distributions, we need to show that estimators for
production probabilities of PCFGs and parameters of Gibbs distributions produce the
same results.
Among many estimation procedures, the maximum-likelihood (ML) estimation
procedure is commonly used. In the full observation case, if the data is composed of
, 7,2, then the estimator for the system of production probabilities is
</bodyText>
<equation confidence="0.9845208">
= {/5(A —&gt; a)} = arg max H fi p(A (23)
i=1 (A—&gt;a)ER
subject to
E p(A a) = 1,
(A—&gt;a)ER
</equation>
<page confidence="0.995873">
149
</page>
<note confidence="0.604715">
Computational Linguistics Volume 25, Number 1
</note>
<bodyText confidence="0.937035">
for any A E N and the estimator for parameters of Gibbs distributions with A of the
form in (19) is
</bodyText>
<equation confidence="0.649519333333333">
n eA.11(r.) (24)
= arg max H -
A i_1
</equation>
<bodyText confidence="0.99955875">
In addition, the ML estimate p in (23) can be analytically solved and the solution is
given by Equation (5).
In the partial observation case, if • • • , Yn are the observed yields, then the esti-
mators for the two distributions are
</bodyText>
<equation confidence="0.9740312">
= {NA ail = arg max E H p(A
i=1 Y(r)=Y; (A—&gt;a)ER
subject to
E
(A—&gt;a)ER
for any A E N, and
,)U1(T)
A = arg max H E
ZA
A 1=1 rEfty,
</equation>
<bodyText confidence="0.996491">
respectively.
We want to compare the ML estimators for the two distributions and see if they
produce the same results in some sense. Since the parameters p serve as base numbers
in PCFG distributions, whereas A are exponents in Gibbs distributions, to make the
comparison sensible, we take the logarithms of 13 and ask whether or not log p and
A are the same. Since the ML estimation procedure for PCFGs involves constrained
optimization, whereas the estimation procedure for Gibbs distributions only involves
unconstrained optimization, it is reasonable to suspect log p A. Indeed, numerically
log fr and A are different. For example, the estimator (23) only gives one estimate of
the system of production probabilities, whereas the estimator (24) may yield infinitely
many solutions. Such uniqueness and nonuniqueness of estimates is related to the
identifiability of parameters. We will discuss this in more detail in Section 7.
Despite their numerical differences, the ML estimators for PCFG distributions and
Gibbs distributions with the form (19) are equivalent, in the sense that the estimates
produced by the estimators impose the same distributions on parses. Because of this,
in the context of ML estimation of parameters, we can regard PCFG distributions as
special cases of Gibbs distributions.
</bodyText>
<subsectionHeader confidence="0.533746">
Corollary 4
</subsectionHeader>
<bodyText confidence="0.98460475">
If is the solution of (23), then log p is a solution of ML estimation (24). Similarly,
if p is a solution of (25), then log p is a solution of ML estimation (26). Hence, the
estimates of production probabilities of PCFG distributions and parameters of Gibbs
distributions with the form (19) impose the same distributions on parses.
</bodyText>
<page confidence="0.992618">
150
</page>
<table confidence="0.1559195">
Chi Probabilistic Context-Free Grammars
Proof
</table>
<bodyText confidence="0.9663792">
Suppose S■ is a solution for (24). By Proposition 4, the Gibbs distribution Ps, is imposed
by a system of production probabilities /3. Then /3 is the solution of (23). Let A = log 13,
i.e., A(A a) = log (A —&gt; a). Then A impose the same distribution on parses as
A. Therefore A are also a solution to (24). This proves the first half of the result. The
second half is similarly proved.
</bodyText>
<subsectionHeader confidence="0.71833">
6. Branching Rates of PCFGs
</subsectionHeader>
<bodyText confidence="0.9994365">
In this section, we study PCFGs from the perspective of stochastic branching processes.
Adopting the set-up given by Miller and O&apos;Sullivan (1992), we define the mean matrix
M of p as a INI x IN1 square matrix, with its (A, B)th entry being the expected number
of variables B resulting from rewriting A:
</bodyText>
<equation confidence="0.99723075">
M(A,B) = E p(A ce)n(B; a). (27)
a E (Nu T)*
sot. (A—cs)ER
Clearly, M is a nonnegative matrix.
</equation>
<bodyText confidence="0.998440166666667">
We say B E N can be reached from A E N, if for some n &gt; 0, M(n) (A, B) &gt; 0, where
M(&apos;) (A, B) is the (A, B)th element of Mn. M is irreducible if for any pair A,&apos;B E N, B
can be reached from A. The corresponding branching process is called connected if M
is irreducible (Walters 1982). It is easy to check that these definitions are equivalent to
Definition 3.
We need the result below for the study of branching processes.
</bodyText>
<subsectionHeader confidence="0.655465">
Theorem 1: (Perron-Frobenius)
</subsectionHeader>
<bodyText confidence="0.979836">
Let M = [mg] be a nonnegative k x k matrix.
</bodyText>
<listItem confidence="0.98989925">
1. There is a nonnegative eigenvalue p such that no eigenvalue of A has
absolute value greater than p.
2. Corresponding to the eigenvalue p there is a nonnegative left (row)
eigenvector v = (v1, , uk) and a nonnegative right (column) eigenvector
</listItem>
<equation confidence="0.5489435">
itl
= ;:k) •
</equation>
<listItem confidence="0.974948333333333">
3. If M is irreducible then p is a simple eigenvalue (i.e., the multiplicity of p
is 1), and the corresponding eigenvectors are strictly positive (i.e. ui &gt; 0,
v, &gt; 0 all i).
</listItem>
<bodyText confidence="0.998899333333333">
The eigenvalue p is called the branching rate of the process. A branching process
is called subcritical (critical, supercritical), if p &lt; 1 (p -= 1, p &gt; 1). We also say a PCFG
is subcritical (critical, supercritical), if its corresponding branching process is. When a
PCFG is subcritical, it is proper. When a PCFG is supercritical, it is improper.
The next result demonstrates that production probabilities assigned by the relative
weighted frequency method impose subcritical PCFG distributions.
</bodyText>
<page confidence="0.991299">
151
</page>
<note confidence="0.445837">
Computational Linguistics Volume 25, Number 1
</note>
<subsectionHeader confidence="0.406107">
Proposition 5
</subsectionHeader>
<bodyText confidence="0.629878">
For p assigned by the relative weighted frequency method (8) and M by (27),
p &lt; 1. (28)
</bodyText>
<subsectionHeader confidence="0.762023">
Proof
</subsectionHeader>
<bodyText confidence="0.8378785">
Let p be the right eigenvector of p, as described in item (2) of Theorem 1. We have
Mp = pp,. For each variable A,
</bodyText>
<equation confidence="0.718021615384615">
= p(A).
BEN
Therefore
E E p(A a)n(B; ce)p(B) = pp(A).
BEN aE (NUT)*
s.t. (A—,00ER
Replacing p(A a) by F (A --+ a) IF (A), according to (9),
E E F (AF TA)a) n(13; a)11(13) = PIL(A).
BEN aE(NuT)
s.t. (A--■a)ER
Multiply both sides by F (A) and sum over A E N. By (11),
E F (A) p,(A) — it(S) = p E F (A) p,(A). (29)
AEN AEN
</equation>
<bodyText confidence="0.9933605">
We need to show that ,u(S) &gt; 0. Assume p(S) = 0. Then for any n &gt; 0, since Mnp = pt,
we have
</bodyText>
<equation confidence="0.65599775">
E m(n)(S,A)p,(A) = pn p(S) = 0.
AEN
For each A E N, M(n) (S, A),u(A) = 0. Because each A E N is reachable from S under
p, there is n &gt; 0 such that M(n)(S, A) &gt; 0. So we get p(A) = 0. Hence p, = 0. This
contradicts the fact that it is a nonnegative eigenvector of M. Therefore p(S) &gt; 0. By
(29),
E F (A) p,(A) &gt; p E F(A)p(A) &gt;0
AEN AEN
</equation>
<bodyText confidence="0.934503">
This proves p &lt; 1. 0
We will apply the above result to give another proof of Proposition 2. Before doing
this, we need to introduce a spectral theorem, which is well-known in matrix analysis.
</bodyText>
<subsectionHeader confidence="0.626652">
Theorem 2
</subsectionHeader>
<footnote confidence="0.630189">
Suppose M is an n x n real matrix. Let a(M) be the largest absolute value of M&apos;s
eigenvalues. Then
Cr (M) = lim 11Vin 111 n
n—&gt; oo
</footnote>
<page confidence="0.991528">
152
</page>
<bodyText confidence="0.8226865">
CM Probabilistic Context-Free Grammars
where M is the norm of M defined by
</bodyText>
<equation confidence="0.6413455">
iiMii = sup
161=1
</equation>
<bodyText confidence="0.913761">
Now we can prove the following result.
</bodyText>
<subsectionHeader confidence="0.826443">
Proposition 6
</subsectionHeader>
<bodyText confidence="0.888695">
If M given by (27) has branching rate p &lt; 1, then for each in e N U {0},
</bodyText>
<equation confidence="0.701889">
EpIT I m &lt; 00. (30)
Proof
</equation>
<bodyText confidence="0.9902025">
We repeat the proof of Proposition 2 from Section 4 up to (15). Then, instead of sum-
ming over A, we observe that (15) can be written as
</bodyText>
<equation confidence="0.6469275">
Mk-I-1, A &lt; K+ M(A, 13)Mk, B-
BEN
Write {Mk, /}AEN as Mk, which is a vector indexed by A E N. We then have
&lt; + MIC1x,
</equation>
<bodyText confidence="0.8510384">
where 1 is defined as {1, ...,1}, and for two-column vectors ri and 17, Z &lt; i; means
each component of i is &lt; the corresponding component of v. Since the components
in K1, M and Mk are positive, the above relation implies
MA-4k+i &lt; KM1+ M2A-4.k.
Hence, we get
</bodyText>
<equation confidence="0.692852555555556">
&lt;K1 + MA-4.k+i &lt; K1+ KM1 M21k•
By induction, we get
k-2
&lt;KMk mji
j=0 (31)
k-2
lMl
KE IlMk-110111.
j=0
</equation>
<bodyText confidence="0.9197215">
By Theorem 2, for any p &lt; p&apos; &lt; 1, IMII = o(p&apos;n). Then (31) implies that IA I is
bounded. Since Mk are positive and increasing, it follows that A-/Ik converge.
Next we investigate how branching rates of improper PCFGs change after renor-
malization. First, let us look at a simple example. Consider the CFG given by (1).
Assign probability p to the first production (S SS), and 1 — p to the second one
(S a). It was proved that the total probability of parses is min(1, 1/p — 1). If p&gt; 1/2,
</bodyText>
<page confidence="0.996516">
153
</page>
<note confidence="0.704458">
Computational Linguistics Volume 25, Number 1
</note>
<equation confidence="0.828437444444444">
then min(1, 1/p - 1) = 1/p - 1 &lt; 1, implying the PCFG is improper. To get the renor-
malized distribution, take a parse T with yield am. Since f (S -&gt; SS; 1-) = m - 1 and
f (S -&gt; a; r) = m, p(r) = pm-1(1 - p)m . Then the renormalized probability of T equals
19(T) Pm-1(1 --
= m
n n)m
NT) 1/p - 1 1/p - 1
Therefore, /3 is assigned by a system of production probabilities 13 with p(S -4 SS) =
1 - p &lt; 1/2, and p(s -&gt; a) = p. So the renormalized PCFG is subcritical.
</equation>
<bodyText confidence="0.9944745">
More generally, we have the following result, which says a connected, improper
PCFG, after being renormalized, becomes a subcritical PCFG.
</bodyText>
<subsectionHeader confidence="0.836995">
Proposition 7
</subsectionHeader>
<bodyText confidence="0.8962855">
If p is a connected, improper PCFG distribution on parses, then its renormalized ver-
sion fa is subcritical.
</bodyText>
<subsectionHeader confidence="0.858936">
Proof
</subsectionHeader>
<bodyText confidence="0.999388833333333">
We have 0 &lt; p(s) &lt; 1, and we shall first show, based on the fact that the PCFG is
connected, that all 0 &lt; p(12A) &lt; 1. Recall the proof of Corollary 1. There we got the
relation qs &gt; qAps (A E T), where qA is the probability that trees rooted in A fail to
terminate. Because the PCFG is connected, S is reachable from A, too. By the same
argument, we also have qA &gt; qspA(S E T). Since both qs and pA(S E T) &gt; 0, qA &gt; 0,
then p(SZA) = 1 - qA &lt;1. Similarly, we can prove p(QA) &gt; P(Ils)PA(s E T) &gt; 0.
</bodyText>
<equation confidence="0.91087025">
For each A, define generating functions fgAl as in Harris (1963, Section 2.2),
gA(s) =E p(A a) H szo,co, (32)
E (NUT)* BEN
s.t. (A—cs)ER
</equation>
<bodyText confidence="0.967577">
where s {sA}AEN. Write g = fgAlAEN and define g(n) fg.ln.)} recursively as
</bodyText>
<equation confidence="0.975689333333333">
(s) = gA (g(11-1)(s))
It is easy to see that gA(0) is the total probability of parses with root A and height
1. By induction, e) (0) is the total probability of parses with root A and height &lt; n.
Therefore, e) (0) 1 p(A) &lt;1.
Write r = {P(QA)}AGN. Then
g(r) g(ipncia g@1)(0)) = g(g(n) (0)) = g(n+1) (0) = r.
</equation>
<bodyText confidence="0.905664">
Therefore, r is a nonnegative solution of g(s) = s. It is also the smallest among such
solutions. That is, if there is another nonnegative solution r&apos; r, then r &lt; r&apos;. This is
because 0 &lt; r&apos; implies g@&apos;)(0) &lt; g(n)(r&apos;) = r&apos; for all n &gt; 0, and by letting n oo, r &lt; r&apos;.
Clearly, 1 is also a solution of g(s) = s.
We now renormalize p to get p by (22). Define generating functions f = {fA} of
and f() in the same way as (32) and (33). Then
</bodyText>
<equation confidence="0.9574256">
fA (s) = Ea) H sn(B*
SE (NUT)* BEN
s.t. (A—&gt;a)ER
()(s) (s)= g(s)
(33)g,
</equation>
<page confidence="0.997841">
154
</page>
<figure confidence="0.93313575">
Chi Probabilistic Context-Free Grammars
= (34)
E 1 po _&gt; a) TT ,n( n
B;cy) TT
1 1 &apos; B
a E(NUT)&apos; TA BEN BEN
s.t. (A—&gt;a)ER
For two vectors r = {TA} and {SA}, write rs for {rAsA}, and r/s for {TA/SA}. Then
(34) can be written
f(s) g(rs)
r
Since all TA -= p(A) are positive, f(s) are well defined by the fractions.
</figure>
<figureCaption confidence="0.221019333333333">
Because r is the smallest nonnegative solution of g(s) = s, by the above equation,
1 is the only solution of f(s) = s in the unit cube. Since g(s) = s also has a solution 1,
f(s) = s has a solution 1/r, which is strictly larger than 1.
</figureCaption>
<bodyText confidence="0.848643">
We want to know how f changes on the line segment connecting 1 and 1/r. Let
u = 1/r -1. Then u is strictly positive. Elements on the line segment between 1 and
1/r can be represented by 1 + tu, with t e [0,1]. Define h(t) = f (1 + tu) -1- u. Then
</bodyText>
<equation confidence="0.865165666666667">
hA(t) = E(A - a) 11 (1 + tuB)n(B* - 1 - tuA. (35)
aE(NUT)* BEN
s.t. (A—s)ER
</equation>
<bodyText confidence="0.979579615384615">
Differentiate h at t = 0. Then h&apos;(0) = Mu-u, where M is the mean matrix corresponding
to P. Every hA(t) is a convex function. Then, because hA (0) = hA(1) = 0, WA(0) &lt; 0,
which leads to Mu &lt;u.
We now show that for at least one A, (Mu)A &lt;u4. First of all, note that h(0) = 0
only if hA(t) is linear. Assume Mu = u, which leads to h/(0) = 0 and the linearity of
h(t). Together with h(0) = 0, this implies h(t) 0. Choose t &lt; 0 such that 1 + tuA &gt; 0
for all A. Then f (1+ tu) -1- tu = h(t) = 0. Therefore 1 + tu is a nonnegative solution
of f(s) = s and is strictly less than 1. This contradicts the fact that 1 is the smallest
nonnegative solution of f (s) = s.
Now we have Mu &lt; u, and 3A, s.t. (Mu)A &lt; uA. Because p is connected, M is
irreducible. By item (3) of Theorem 1, u is strictly positive, and there is a strictly
positive left eigenvector v such that vM = pv. Therefore vMu &lt; vu, or pvu &lt; vu.
Hence p &lt; 1. This completes the proof. 0
</bodyText>
<sectionHeader confidence="0.462443" genericHeader="conclusions">
7. Identifiability and Approximation of Production Probabilities of PCFGs
</sectionHeader>
<bodyText confidence="0.999224538461538">
Identifiability of parameters is related to the consistency of estimates, both being im-
portant statistical issues. Proving the consistency of the ML estimate of a system of
production probabilities given in (5) is relatively straightforward. Consistency in this
case means that, if p imposes a proper distribution, then as the size of the data com-
posed of independent and identically distributed (i.i.d.) samples goes to infinity, with
probability one, the estimate converges to p. To see this, think of the sample parses
as taken independently from a branching process governed by p. By the context-free
nature of the branching process, for A E N, each instance of A selects a production
A -&gt; a by probability p (A -&gt; a) independently of the other instances of A. As the
size of the data set goes to infinity, the number of occurrences of A goes to infinity.
Therefore, by the law of large numbers, the ratio between the number of occurrences
of A -4 a and the number of occurrences of A, which is p(A -&gt; a), converges to
p(A -&gt; a), with probability one.
</bodyText>
<page confidence="0.995436">
155
</page>
<note confidence="0.706711">
Computational Linguistics Volume 25, Number 1
</note>
<bodyText confidence="0.999498">
By the consistency of the ML estimate of a system of production probabilities, we
can prove that production probabilities are identifiable parameters of PCFGs. In other
words, different systems of production probabilities impose different PCFG distribu-
tions.
</bodyText>
<subsectionHeader confidence="0.859364">
Proposition 8
</subsectionHeader>
<bodyText confidence="0.988066">
If pi, p2 impose distributions P1. P2, respectively, and pi 0 192, then P1 P2.
</bodyText>
<subsectionHeader confidence="0.879587">
Proof
</subsectionHeader>
<bodyText confidence="0.998964636363636">
Assume P1 = P2. Then draw n i.i.d. samples from P1. Because the ML estimator is
consistent, as n oo, p pi, with probability 1. Because the n i.i.d. samples can also
be regarded as drawn from P2, with the same argument, p2, with probability 1.
Hence pi = p2, a contradiction. 0
We mentioned in Section 5 that the ML estimators (24) and (26) may produce
infinitely many estimates if the Gibbs distributions on parses have the form (19). This
phenomenon of multiple solutions results from the nonidentifiability of parameters
of the Gibbs distributions (19), which means that different parameters may yield the
same distributions.
To see why parameters of Gibbs distribution (19) are nonidentifiable, we note that
the frequencies of production rules are linearly dependent,
</bodyText>
<figure confidence="0.9211752">
E f (A —› a; 7-) =
(A-.ce)ER (B—ce)ER
if A 0 S,
E f (S —&gt; a; 7-) =
(S—&gt;a)ER (B—a)ER
</figure>
<bodyText confidence="0.987873">
Therefore, there exists Ao 0 0, such that for any r, Ao •f(T) = 0. If A is a solution for
(24), then for any number t,
</bodyText>
<equation confidence="0.995732">
(5■ + t AO • f (r) = 5■ • f (r)
e(5,+tA0)1(,) = eA1(,), z A+0,0 = ZA
PA-FtA0 (7) = PA (7)
</equation>
<bodyText confidence="0.9936654">
Thus for any t, A + tA0 is also a solution for (24). This shows that the parameters
of Gibbs distribution (19) are nonidentifiable.
Finally, we consider how to approximate production probabilities by mean fre-
quencies of productions. Given i.i.d. samples of parses • • • , r from the distribution
imposed by p, by the consistency of the ML estimate of p given by (5),
</bodyText>
<page confidence="0.8431014">
Ef (A —&gt; --+ p(A a),
i=1
&gt;f (A; TO/ n
i=1
156
</page>
<note confidence="0.291013">
Chi Probabilistic Context-Free Grammars
</note>
<bodyText confidence="0.999106">
with probability 1, as n —&gt; oo. If the entropy of the distribution p is finite, then for
every production rule (A 0) e R,
</bodyText>
<equation confidence="0.984256571428571">
f (A —&gt; 13; T) —&gt; Ep(f (A 44 /3;r)) with probability 1,
i=1
Ep(f (A —&gt; a; 7))
/3 (A —&gt; a) --&gt; with probability 1,
Ep(f (A; T.))
Ep(f (A -4 a; 7))
19(A a ) Ep(f (A; T))
</equation>
<bodyText confidence="0.9803565">
If the entropy is infinite, the above argument does not work, because both the
numerator and the denominator of the fraction are infinity Can we change the fraction
a little bit so that it still makes sense, and at the same time yields good approximation
to p(A —&gt; a)?
One way to do this is to pick a large finite subset ST of 12 and replace the fraction
by
</bodyText>
<equation confidence="0.5972205">
Ep(f (A —&gt; a; 7) IT E 0&apos;)
Ep(f (A; T)IT E SY) •
</equation>
<bodyText confidence="0.971046">
where Ep(f (A —&gt; a;7-)17- E 119 is the conditional expectation of PA -4 a; r) given IT,
which is defined as
</bodyText>
<equation confidence="0.6902535">
E f (A -4 a; r)p(r)
E p (f(A—&gt; a;7-)17- E = TEI2&apos;
</equation>
<bodyText confidence="0.99791075">
Because C•2&apos; is finite, the top of the fraction on the right-hand side is finite. Also the
bottom of the fraction is positive. Therefore the conditional expectation off is finite.
The conditional expectation Ep(f (A; T)IT E S2&apos;) is similarly defined.
The following result shows that as C2&apos; expands, the approximation gets better.
</bodyText>
<subsectionHeader confidence="0.882158">
Proposition 9
</subsectionHeader>
<bodyText confidence="0.996392333333333">
Suppose a system of production probabilities p imposes a proper distribution. Then
for any increasing sequence of finite subsets of cl with S•27, I 12, i.e., C21 C . . . C
finite and UCin =
</bodyText>
<equation confidence="0.9937475">
Ep(f (A —&gt; ct;r) jr E Qn)
) E Qn) •
</equation>
<bodyText confidence="0.999898666666667">
To prove the proposition, we introduce the Kullback-Leibler divergence. For any
two probability distributions p and q on St, the Kullback-Leibler divergence between
p and q is defined as
</bodyText>
<equation confidence="0.7327895">
D(pliq) = E p(r) log P(T)
TEQ gerr
</equation>
<bodyText confidence="0.999923">
where 0 log ? is defined as 0 for any q(r) &gt; 0. D(pliq) is nonnegative and equal
to 0 if and only if p = q. One thing to note is that q need not be proper in order
</bodyText>
<figure confidence="0.51371625">
E p(T)
rEsr
157
Computational Linguistics Volume 25, Number 1
</figure>
<bodyText confidence="0.9960022">
to make D(pllq) nonnegative. Even when E q(T) &lt; 1, it is still true that D(pliq) &gt; 0.
For more about the Kullback-Leibler divergence, we refer the readers to Cover and
Thomas (1991).
The Kullback-Leibler divergence has the simple property described below, which
will be used in the proof of Proposition 9.
</bodyText>
<equation confidence="0.7206472">
Lemma 2
If St&apos; is an arbitrary subset of St, then
P(1 — p(ST)
D(pllq) &gt;p q(S2(S2&apos;) log CY) + (1 p(1&apos;)) log 1
Proof
</equation>
<bodyText confidence="0.9817955">
Consider the Kullback-Leibler divergence between the conditional distributions per IQ&apos;)
and q(TIS21), which equals
</bodyText>
<equation confidence="0.9860152">
E p(7-152&apos;) log P(TICY) E p(T) log q(T) log &gt; 0
1 p(T)
9(719&apos;) PO) —
TEW TEW
per) P(,)
p(r) log &gt; p(12&apos;) log q(12&apos;) .
q(T) —
Similarly,
E p(r) log p(T) (1 p(12&apos;)) log 1 — p(1•2&apos;)
q(T) q(12\ SY) 1 — q(SY) •
</equation>
<bodyText confidence="0.782782666666667">
TEsz\sr
The second &amp;quot;&gt;&amp;quot; is because q(12) &lt; 1. These two inequalities together prove the lemma.
0
</bodyText>
<subsectionHeader confidence="0.499398">
Proof of Proposition 9
</subsectionHeader>
<bodyText confidence="0.999299">
Given n, for production probabilities q, let Kn(q) be the Kullback-Leibler divergence
between the conditional distribution p(r) and the distribution imposed by q,
</bodyText>
<equation confidence="0.9888928">
TC2 (36)
K(q) = E p(ric2n) log
P(In)
reQ„ H a)f(A--,a;T).
(A—■a)ER
</equation>
<bodyText confidence="0.99991">
We want to find q that minimizes Kn(q). This can be achieved by applying the
Lagrange multiplier method. The condition that q is subject to is,
</bodyText>
<equation confidence="0.992172">
E q(A a) = 1, (37)
</equation>
<bodyText confidence="0.994605">
for every A E N. There are WI such constraints. To incorporate them into the mini-
mization, we consider the function
</bodyText>
<equation confidence="0.738224333333333">
L(q) = K(q) + E AA E q(A —&gt; a) — 1) ,
AEN (A—&gt;a)ER
TECr
</equation>
<page confidence="0.946304">
158
</page>
<bodyText confidence="0.784103666666667">
Chi Probabilistic Context-Free Grammars
where the unknown coefficients {AA}AEN are called Lagrange multipliers.
The q that minimizes Kn(q) subject to (37) satisfies
</bodyText>
<figure confidence="0.867505823529412">
OL(q) _ 0
aq(A —&gt; a)
for all (A a) E R. By simple computation, this is equivalent to
E f (A —&gt; a; T)P (TIR) = AAq(A —&gt; a).
TEE2n
Sum both sides over all a E (N U T)* such that (A —&gt; a) E R. By the constraints (37),
AA =
TECin
Therefore we prove that if there is a minimizer, it has to be j)„, where
E f (A —&gt; a; T)p(r)
Pn(A a) = rEQ&amp;quot; f (A; T)p(T) •
Tec2„
To see that there is a minimizer of Kn(q) subject to (37), consider the boundary
points of the region
{el = {q(A --&gt; a)} : q(A a) &gt; 0, q(A a) = 1}
a s.t.
(A—&gt;a)ER
</figure>
<bodyText confidence="0.991381454545455">
Any boundary point of the region has a component equal to zero, hence for some
T E On, q(T) = 0, implying K(q) = oo. Because K(q) is a continuous function, Kn
must attain its minimum inside the above region, and this minimizer, as has been
shown, is Pt,.
We need to show Pn p. Let Cr = ft?, and apply Lemma 2 to p(7-112n) and pn(r).
Since p(QnI1ln) = 1, we get 0 &lt; — logN(On) &lt; Kn(Pn). On the other hand, because Pn
is the minimizer of Kn, Kn(N) K(p) = — log p(12n).
Because lin —&gt; St and p is proper, p(I) —&gt; 1. Therefore 0 &lt; —100n(1n) &lt;
— logp(12n) 0. Hence Pn(f2n) --&gt; 1.
Choose an arbitrary T E O. For all n large enough, T E Apply Lemma 2 to {r}
and get
</bodyText>
<figure confidence="0.934986263157895">
0&lt; )log 13(TIQn) + (1
Pn(rPn)
p(Tin)log (TiQn) + (1
Pn(TIC2n)
lim 13&amp;quot;211) =
1.
1)11(TIQn)
Together with p(TIC2n) p(T) &gt; 0 and f( l) —&gt; 1, this implies
limpn(T) = lim Pn(Tiltn) = lim Pn(TIC2n) = p(T).
n-400 n—,c&gt;o n—&gt;oo
Per I Qn ) &lt; Kn (pn ) --&gt;
i3n(TIQn)
1 /90-Pn) —40
nn)
p(TI1n))log 1 — 1)(TIQ
—
p (T I ft n )) log —
159
Computational Linguistics Volume 25, Number 1
</figure>
<bodyText confidence="0.9992241">
This nearly completes the proof. By the
Pn should converge to p. To make the argum
every subsequence of Pn has a limit point.
For any T, since Pn,(T) p(r), p&apos;(r) =
probabilities, p&apos; = p. Therefore p is the only
identifiability of production probabilities,
ent more rigorous, by compactness of fin,
Let p&apos; be a limit point of a subsequence
p(r). By the identifiability of production
limit point of Pn. This proves Pn —&gt; p.
</bodyText>
<sectionHeader confidence="0.990301" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999399285714286">
I am indebted to Stuart Geman and Mark
Johnson for encouraging me to look at
problems in this paper in the first place,
and for various discussions and suggestions
along the way. I am also grateful to the
referees&apos; careful reading of the original
manuscript.
</bodyText>
<sectionHeader confidence="0.996693" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999929302325581">
Abney, Steven P. 1997. Stochastic
attribute-value grammars. Computational
Linguistics, 23(4):597-618.
Baker, James K. 1979. Trainable grammars
for speech recognition. In Speech
Communications Papers of the 97th Meeting of
the Acoustical Society of America,
pages 547-550, Cambridge, MA.
Baum, Leonard E. 1972. An inequality and
associated maximization technique in
statistical estimation of probabilistic
functions of Markov processes.
Inequalities, 3:1-8.
Berger, Adam L., Stephen Della Pietra, and
Vincent Della Pietra. 1996. A maximum
entropy approach to natural language
processing. Computational Linguistics,
22(1):39-71.
Booth, Taylor L. and Richard A. Thompson.
1973. Applying probability measures to
abstract languages. IEEE Transactions on
Computers, C-22:442-450.
Chi, Zhiyi and Stuart Geman. 1998.
Estimation of probabilistic context-free
grammars. Computational Linguistics,
24(2):299-305.
Cover, Thomas M. and Joy A. Thomas.
1991. Elements of Information Theory. John
Wiley &amp; Sons, Inc.
Della Pietra, Stephen, Vincent Della Pietra,
and John Lafferty. 1997. Inducing features
of random fields. IEEE Transactions on
Pattern Analysis and Machine Intelligence,
19(4):1-13, April.
Dempster, Arthur Pentland, N. M. Laird,
and Donald B. Rubin. 1977. Maximum
likelihood from incomplete data via the
EM algorithm. Journal of Royal Statistical
Society, Series B, 39:1-38.
Grenander, Ulf. 1976. Lectures in Pattern
Theory Volume 1, Pattern Synthesis.
Springer-Verlag, New York.
Harris, Theodore Edward. 1963. The Theory
of Branching Processes. Springer-Verlag,
Berlin.
Johnson, Mark. 1998. PCFG models of
linguistic tree representations.
Computational Linguistics. To appear.
Mark, Kevin E. 1997. Markov Random Field
Models for Natural Language. Ph.D. thesis,
Department of Electrical Engineering,
Washington University, May.
Mark, Kevin E., Michael I. Miller, and Ulf
Grenander. 1996. Constrained stochastic
language models. In S. E. Levinson and L.
Shepp, editors, Image Models (and Their
Speech Model Cousins). Springer-Verlag,
pages 131-140.
Mark, Kevin E., Michael I. Miller, Ulf
Grenander, and Steven P. Abney. 1996.
Parameter estimation for constrained
context-free language models. In
Proceedings of the DARPA Speech and
Natural Language Workshop, Image Models
(and Their Speech Model Cousins),
pages 146-149, Harriman, NY, February.
Morgan Kaufmann.
Miller, Michael I. and Joseph A. O&apos;Sullivan.
1992. Entropies and combinatorics of
random branching processes and
context-free languages. IEEE Transactions
on Information Theory, 38(4), July.
Sdnchez, Joan-Artdreu and José-Miguel
Beneclii. 1997. Consistency of stochastic
context-free grammars from probabilistic
estimation based on growth
transformations. IEEE Transactions on
Pattern Analysis and Machine Intelligence,
19(9):1052-1055, September.
Walters, Peter. 1982. An Introduction to
Ergodic Theory. Springer-Verlag, NY.
Zhu, Song Chun, Ying Nian Wu, and David
B. Mumford. 1997. Minimax entropy
principle and its application to texture
modeling. Neural Computation,
9(8):1627-1660.
</reference>
<page confidence="0.997479">
160
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.882453">
<title confidence="0.9939975">Statistical Properties of Probabilistic Context-Free Grammars</title>
<author confidence="0.998589">Zhiyi Chi</author>
<affiliation confidence="0.996904">University of Chicago</affiliation>
<abstract confidence="0.992605133333333">We prove a number of useful results about probabilistic context-free grammars (PCFGs) and their Gibbs representations. We present a method, called the relative weighted frequency method, to assign production probabilities that impose proper PCFG distributions on finite parses. We demonstrate that these distributions have finite entropies. In addition, under the distributions, sizes of parses have finite moment of any order. We show that Gibbs distributions on CFG parses, which generalize PCFG distributions and are more powerful, become PCFG distributions if their features only include frequencies of production rules in parses. Under these circumstances, we prove the equivalence of the maximum-likelihood (ML) estimation procedures for these two types of probability distributions on parses. We introduce the renormalization of improper PCFGs to proper ones. We also study PCFGs from the perspective of stochastic branching processes. We prove that with their production probabilities assigned by the relative weighted frequency method, PCFGs are subcritical, i.e., their branching rates are less than one. We also show that by renormalization, connected supercritical PCFGs become subcritical ones. Finally, some minor issues, including identifiability and approximation of production probabilities of PCFGs, are discussed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven P Abney</author>
</authors>
<title>Stochastic attribute-value grammars.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--4</pages>
<contexts>
<context position="9194" citStr="Abney 1997" startWordPosition="1488" endWordPosition="1489">the mean squared is the variance of sizes, which tells us how &amp;quot;scattered&amp;quot; sizes of parses are distributed around the mean. Proposition 2 shows that, under distributions imposed by production probabilities assigned by the relative weighted frequency method, sizes of parses have finite moment of any order. 1.3 Gibbs Distributions on Parses and Renormalization of Improper PCFGs Besides PCFG distributions, a CFG can be equipped with many other types of probability distributions. Among the most widely studied is the Gibbs distribution (Mark, Miller, and Grenander 1996; Mark et al. 1996; Mark 1997; Abney 1997). Gibbs distributions arise naturally by invoking the maximum entropy principle. They are considered to be more powerful than PCFG distributions because they incorporate more features, especially context-sensitive features, of natural languages, whereas PCFG distributions only consider frequencies of production rules. On the other hand, Gibbs distributions are not always superior to PCFG distributions. A Gibbs distribution, with only frequencies of production rules in parse as its features, turns into a PCFG. More specifically, we will show in Proposition 4 in Section 5, that a CFG equipped wi</context>
</contexts>
<marker>Abney, 1997</marker>
<rawString>Abney, Steven P. 1997. Stochastic attribute-value grammars. Computational Linguistics, 23(4):597-618.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James K Baker</author>
</authors>
<title>Trainable grammars for speech recognition.</title>
<date>1979</date>
<booktitle>In Speech Communications Papers of the 97th Meeting of the Acoustical Society of America,</booktitle>
<pages>547--550</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="22198" citStr="Baker 1979" startWordPosition="3773" endWordPosition="3774">Chi and Geman (1998), assigns proper production probabilities for PCFGs. In the second case, the parse trees are unobserved. Instead, the yields Y1 = = Y(T), which are the left-to-right sequences of terminals of the unknown parses rl I • • • form the data. It can be proved that the ML estimate is given by EE,-,[f(A a; T)IT E C2Y1] po -4 a) = 1=1 n (6) EE,,[fo;7-)IT c fly] i=1 where C21( is the set of all parses with yield Y, i.e., Sly = E C2 : Y(T) = Y}. Equation (6) cannot be solved in closed form. Usually, the solution is computed by the EM algorithm with the following iteration (Baum 1972; Baker 1979; Dempster, Laird, and Rubin 1977): E Efr,[ f (A —4 a; T)IT E Pk-Fi (A a) _ i=1 71 (7) /3k[f(A; T)IT E Like in (5), Pk for k&gt; 0 impose proper probability distributions on C-2 (Chi and Geman 1998). To unify (6) and (7), expand Efrk [ f (A -4 a; 7-) IT e fly], by the definition of expectation, into [f; (A —&gt; a; T) I T E SZyj =E f (A —&gt; a; -,-)j)k (r E Qyi). rEcty, Let A be the set of parses whose yields belong to the data, i.e., A = {T : Y(T) E {Yi, • ..,1(n}}. For each T E A, let y = Y(T) and W(r) = LPk(TIT E i:Y i=y Then we observe that, for any production rule A —&gt; a, E E f(A a; T)Pk(TIT E Q1</context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>Baker, James K. 1979. Trainable grammars for speech recognition. In Speech Communications Papers of the 97th Meeting of the Acoustical Society of America, pages 547-550, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonard E Baum</author>
</authors>
<title>An inequality and associated maximization technique in statistical estimation of probabilistic functions of Markov processes.</title>
<date>1972</date>
<journal>Inequalities,</journal>
<pages>3--1</pages>
<contexts>
<context position="22186" citStr="Baum 1972" startWordPosition="3771" endWordPosition="3772">s shown by Chi and Geman (1998), assigns proper production probabilities for PCFGs. In the second case, the parse trees are unobserved. Instead, the yields Y1 = = Y(T), which are the left-to-right sequences of terminals of the unknown parses rl I • • • form the data. It can be proved that the ML estimate is given by EE,-,[f(A a; T)IT E C2Y1] po -4 a) = 1=1 n (6) EE,,[fo;7-)IT c fly] i=1 where C21( is the set of all parses with yield Y, i.e., Sly = E C2 : Y(T) = Y}. Equation (6) cannot be solved in closed form. Usually, the solution is computed by the EM algorithm with the following iteration (Baum 1972; Baker 1979; Dempster, Laird, and Rubin 1977): E Efr,[ f (A —4 a; T)IT E Pk-Fi (A a) _ i=1 71 (7) /3k[f(A; T)IT E Like in (5), Pk for k&gt; 0 impose proper probability distributions on C-2 (Chi and Geman 1998). To unify (6) and (7), expand Efrk [ f (A -4 a; 7-) IT e fly], by the definition of expectation, into [f; (A —&gt; a; T) I T E SZyj =E f (A —&gt; a; -,-)j)k (r E Qyi). rEcty, Let A be the set of parses whose yields belong to the data, i.e., A = {T : Y(T) E {Yi, • ..,1(n}}. For each T E A, let y = Y(T) and W(r) = LPk(TIT E i:Y i=y Then we observe that, for any production rule A —&gt; a, E E f(A a; T</context>
</contexts>
<marker>Baum, 1972</marker>
<rawString>Baum, Leonard E. 1972. An inequality and associated maximization technique in statistical estimation of probabilistic functions of Markov processes. Inequalities, 3:1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--1</pages>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Berger, Adam L., Stephen Della Pietra, and Vincent Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taylor L Booth</author>
<author>Richard A Thompson</author>
</authors>
<title>Applying probability measures to abstract languages.</title>
<date>1973</date>
<journal>IEEE Transactions on Computers,</journal>
<pages>22--442</pages>
<contexts>
<context position="2496" citStr="Booth and Thompson 1973" startWordPosition="373" endWordPosition="376">onterminal symbol in the CFG, a probability distribution is placed on the set of all productions from that symbol. Then each finite parse tree is allocated a probability equal to the product of the probabilities of all productions in the tree. More specifically, denote a finite parse tree by T. For any production rule A a of the CFG, let f (A a; r) be the number of times it occurs in T. Let R be the set of all production rules. Then per) = H p(A Oz)f (A—&apos;°;7) (A—c)ER A CFG with a probability distribution on its parses assigned in this way is called a probabilistic context-free grammar (PCFG) (Booth and Thompson 1973; Grenander * Department of Statistics, University of Chicago, Chicago, IL 60637, USA. Email: chi@galton.uchicago.edu. This work was supported by the Army Research Office (DAAL03-92-G-0115), the National Science Foundation (DMS-9217655), and the Office of Naval Research (N00014-96-1-0647). © 1999 Association for Computational Linguistics Computational Linguistics Volume 25, Number 1 1976)1 and the probability distribution is called a PCFG distribution. A PCFG may be improper, i.e., the total probability of parses may be less than one. For instance, consider the CFG in Chomsky normal form: S SS</context>
</contexts>
<marker>Booth, Thompson, 1973</marker>
<rawString>Booth, Taylor L. and Richard A. Thompson. 1973. Applying probability measures to abstract languages. IEEE Transactions on Computers, C-22:442-450.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiyi Chi</author>
<author>Stuart Geman</author>
</authors>
<title>Estimation of probabilistic context-free grammars.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<pages>24--2</pages>
<contexts>
<context position="3813" citStr="Chi and Geman (1998)" startWordPosition="607" endWordPosition="610">p, then p(S —&gt; a) =1— p. Let xh be the total probability of all parses with height no larger than h. Clearly, xh is increasing. It is not hard to see that xh±i = 1 — p + Therefore, the limit of xh, which is the total probability of all parses, is a solution for the equation x = 1 — p + px2. The equation has two solutions: 1 and 1/p — 1. It can be shown that x is the smaller of the two: x = min(1, 1/p — 1). Therefore, if p &gt; 1/2, x &lt; 1—an improper probability. How to assign proper production probabilities is quite a subtle problem. A sufficient condition for proper assignment is established by Chi and Geman (1998), who prove that production probabilities estimated by the maximum-likelihood (ML) estimation procedure (or relative frequency estimation procedure, as it is called in computational linguistics) always impose proper PCFG distributions. Without much difficulty, this result can be generalized to a simple procedure, called &amp;quot;the relative weighted frequency&amp;quot; method, which assigns proper production probabilities of PCFGs. We will give more details of the generalization in Section 3 and summarize the method in Proposition 1. 1.2 Entropy and Moments of Parse Tree Sizes As a probabilistic model for lan</context>
<context position="12045" citStr="Chi and Geman (1998)" startWordPosition="1945" endWordPosition="1948">Gs Because of their context-free nature, PCFG distributions can also be studied from the perspective of stochastic processes. A PCFG can be described by a random branching process (Harris 1963), and its asymptotic behavior can be characterized by its branching rate. A branching process, or its corresponding PCFG, is called subcritical (critical, supercritical), if its branching rate &lt; 1 (= 1,&gt; 1). A subcritical PCFG is always proper, whereas a supercritical PCFG is always improper. Many asymptotic properties of supercritical branching processes are established by Miller and O&apos;Sullivan (1992). Chi and Geman (1998) proved the properness of PCFG distributions imposed by estimated production probabilities, and around the same time Sanchez and Benedi (1997) established the subcriticality of the corresponding branching processes, hence their properness. In this paper we will explore properties of branching rate further. First, in Proposition 5, we will show that if a PCFG distribution is imposed by production probabilities assigned by the relative weighted frequency method, then the PCFG is subcritical. The result generalizes that of Sanchez and Benedi (1997), and has a less involved proof. Then in Proposit</context>
<context position="21608" citStr="Chi and Geman (1998)" startWordPosition="3656" endWordPosition="3659">re fully observed, which means that all the samples are fully observed finite parses trees. Let , be the samples. Then the ML estimate of p(A --&gt; a) is the ratio between the total number of occurrences of the production A —&gt; a in the samples and 137 Computational Linguistics Volume 25, Number 1 the total number of occurrences of the symbol A in the samples, &gt;2f (A —&gt; a; TO —&gt; a) = (5) f (A; Ti) 1=1 Because of the form of the estimator in (5), ML estimation in the full observation case is also called relative frequency estimation in computational linguistics. This simple estimator, as shown by Chi and Geman (1998), assigns proper production probabilities for PCFGs. In the second case, the parse trees are unobserved. Instead, the yields Y1 = = Y(T), which are the left-to-right sequences of terminals of the unknown parses rl I • • • form the data. It can be proved that the ML estimate is given by EE,-,[f(A a; T)IT E C2Y1] po -4 a) = 1=1 n (6) EE,,[fo;7-)IT c fly] i=1 where C21( is the set of all parses with yield Y, i.e., Sly = E C2 : Y(T) = Y}. Equation (6) cannot be solved in closed form. Usually, the solution is computed by the EM algorithm with the following iteration (Baum 1972; Baker 1979; Dempster</context>
<context position="24135" citStr="Chi and Geman (1998)" startWordPosition="4167" endWordPosition="4170">trees in A. Second, assign to each 7 EAa positive weight W(7) such that ETEA w(T) = 1. Finally, define a system of production probabilities p by Ef (A —› (8) p (A —4 a) = TEA E f (A; r)W (T) TEA Because of the similarity between (5) and (8), we call the procedure to assign production probabilities by (8) the &amp;quot;relative weighted frequency&amp;quot; method. Proposition 1 Suppose all the symbols of N occur in the parses of A, and all the parses have positive weight. Then the production probabilities given by (8) impose proper distributions on parses. Proof The proof is almost identical to the one given by Chi and Geman (1998). Let q A = p (derivation tree rooted in A fails to terminate). We will show that qs = 0 (i.e., derivation trees rooted in S always terminate). For each A E V. let f (A; 7) be the number of nonroot instances of A in T. Given a E (V U T)*, let a, be the ith symbol of the sentential form a. For any A E V 1- 9 U U{derivation begins A a, and a, fails to terminate} (A—oe)ER - Ea )19 U{a= fails to terminate}) (A-4oz)ER &lt; E p (A ) p( fa, fails to terminatel) (A—a)ER - E p(A a) E n(B; a)(//3 (A—c)ER BEV 13k±i qA 139 Computational Linguistics Volume 25, Number 1 qB {E(A*a)ERn(B; a) ETEAf(A -+a; T)W(T)}</context>
</contexts>
<marker>Chi, Geman, 1998</marker>
<rawString>Chi, Zhiyi and Stuart Geman. 1998. Estimation of probabilistic context-free grammars. Computational Linguistics, 24(2):299-305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas M Cover</author>
<author>Joy A Thomas</author>
</authors>
<title>Elements of Information Theory.</title>
<date>1991</date>
<publisher>John Wiley &amp; Sons, Inc.</publisher>
<contexts>
<context position="5748" citStr="Cover and Thomas (1991)" startWordPosition="937" endWordPosition="940">At the other extreme, suppose the distribution is p(n) = = p(TN) = 1/N. In this case, all the elements of S are statistically equivalent. No specific information is given about T that would make it possible to know it from S. Greater effort is required—for example, enumerating all the elements in S—to find what T is. Since S is big, the uncertainty about the sample is then much greater. Correspondingly, for the two cases, the entropy is 0 and N log N&gt;&gt; 0, respectively. Entropy plays a central role in the theory of information. For an excellent exposition of this theory, we refer the reader to Cover and Thomas (1991). The theory has been applied in probabilistic language modeling (Mark, Miller, and Grenander 1996; Mark et al. 1996; Johnson 1998), natural language processing (Berger, Della Pietra, and Della Pietra 1996; Della Pietra, Della Pietra, and Lafferty 1997), as well as computational vision (Zhu, Wu, and Mumford 1997). In addition, all the models proposed in these articles are based on an important principle called the maximum entropy principle. Chapter 11 of Cover and Thomas (1991) gives an introduction to this principle. 1 A probabilistic context-free grammar is also called a stochastic context-f</context>
<context position="55884" citStr="Cover and Thomas (1991)" startWordPosition="10520" endWordPosition="10523">ve the proposition, we introduce the Kullback-Leibler divergence. For any two probability distributions p and q on St, the Kullback-Leibler divergence between p and q is defined as D(pliq) = E p(r) log P(T) TEQ gerr where 0 log ? is defined as 0 for any q(r) &gt; 0. D(pliq) is nonnegative and equal to 0 if and only if p = q. One thing to note is that q need not be proper in order E p(T) rEsr 157 Computational Linguistics Volume 25, Number 1 to make D(pllq) nonnegative. Even when E q(T) &lt; 1, it is still true that D(pliq) &gt; 0. For more about the Kullback-Leibler divergence, we refer the readers to Cover and Thomas (1991). The Kullback-Leibler divergence has the simple property described below, which will be used in the proof of Proposition 9. Lemma 2 If St&apos; is an arbitrary subset of St, then P(1 — p(ST) D(pllq) &gt;p q(S2(S2&apos;) log CY) + (1 p(1&apos;)) log 1 Proof Consider the Kullback-Leibler divergence between the conditional distributions per IQ&apos;) and q(TIS21), which equals E p(7-152&apos;) log P(TICY) E p(T) log q(T) log &gt; 0 1 p(T) 9(719&apos;) PO) — TEW TEW per) P(,) p(r) log &gt; p(12&apos;) log q(12&apos;) . q(T) — Similarly, E p(r) log p(T) (1 p(12&apos;)) log 1 — p(1•2&apos;) q(T) q(12\ SY) 1 — q(SY) • TEsz\sr The second &amp;quot;&gt;&amp;quot; is because q(12)</context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>Cover, Thomas M. and Joy A. Thomas. 1991. Elements of Information Theory. John Wiley &amp; Sons, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Della Pietra</author>
<author>Vincent Della Pietra Stephen</author>
<author>John Lafferty</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1997</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>19--4</pages>
<marker>Pietra, Stephen, Lafferty, 1997</marker>
<rawString>Della Pietra, Stephen, Vincent Della Pietra, and John Lafferty. 1997. Inducing features of random fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4):1-13, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur Pentland Dempster</author>
<author>N M Laird</author>
<author>Donald B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of Royal Statistical Society, Series B,</journal>
<pages>39--1</pages>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Dempster, Arthur Pentland, N. M. Laird, and Donald B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of Royal Statistical Society, Series B, 39:1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulf Grenander</author>
</authors>
<date>1976</date>
<booktitle>Lectures in Pattern Theory Volume 1, Pattern Synthesis.</booktitle>
<publisher>Springer-Verlag,</publisher>
<location>New York.</location>
<marker>Grenander, 1976</marker>
<rawString>Grenander, Ulf. 1976. Lectures in Pattern Theory Volume 1, Pattern Synthesis. Springer-Verlag, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theodore Edward Harris</author>
</authors>
<title>The Theory of Branching Processes.</title>
<date>1963</date>
<publisher>Springer-Verlag,</publisher>
<location>Berlin.</location>
<contexts>
<context position="11618" citStr="Harris 1963" startWordPosition="1884" endWordPosition="1885"> two distributions. As will be seen in Section 5, numerically these two estimation procedures are different. However, Corollary 4 shows that they are equivalent in the sense that estimates by the two procedures impose the same distributions on parses. For this reason, a Gibbs distribution may be considered a generalization of PCFG, not only in form, but also in a certain statistical sense. 1.4 Branching Rates of PCFGs Because of their context-free nature, PCFG distributions can also be studied from the perspective of stochastic processes. A PCFG can be described by a random branching process (Harris 1963), and its asymptotic behavior can be characterized by its branching rate. A branching process, or its corresponding PCFG, is called subcritical (critical, supercritical), if its branching rate &lt; 1 (= 1,&gt; 1). A subcritical PCFG is always proper, whereas a supercritical PCFG is always improper. Many asymptotic properties of supercritical branching processes are established by Miller and O&apos;Sullivan (1992). Chi and Geman (1998) proved the properness of PCFG distributions imposed by estimated production probabilities, and around the same time Sanchez and Benedi (1997) established the subcriticality</context>
<context position="47998" citStr="Harris (1963" startWordPosition="8984" endWordPosition="8985">n its renormalized version fa is subcritical. Proof We have 0 &lt; p(s) &lt; 1, and we shall first show, based on the fact that the PCFG is connected, that all 0 &lt; p(12A) &lt; 1. Recall the proof of Corollary 1. There we got the relation qs &gt; qAps (A E T), where qA is the probability that trees rooted in A fail to terminate. Because the PCFG is connected, S is reachable from A, too. By the same argument, we also have qA &gt; qspA(S E T). Since both qs and pA(S E T) &gt; 0, qA &gt; 0, then p(SZA) = 1 - qA &lt;1. Similarly, we can prove p(QA) &gt; P(Ils)PA(s E T) &gt; 0. For each A, define generating functions fgAl as in Harris (1963, Section 2.2), gA(s) =E p(A a) H szo,co, (32) E (NUT)* BEN s.t. (A—cs)ER where s {sA}AEN. Write g = fgAlAEN and define g(n) fg.ln.)} recursively as (s) = gA (g(11-1)(s)) It is easy to see that gA(0) is the total probability of parses with root A and height 1. By induction, e) (0) is the total probability of parses with root A and height &lt; n. Therefore, e) (0) 1 p(A) &lt;1. Write r = {P(QA)}AGN. Then g(r) g(ipncia g@1)(0)) = g(g(n) (0)) = g(n+1) (0) = r. Therefore, r is a nonnegative solution of g(s) = s. It is also the smallest among such solutions. That is, if there is another nonnegative solut</context>
</contexts>
<marker>Harris, 1963</marker>
<rawString>Harris, Theodore Edward. 1963. The Theory of Branching Processes. Springer-Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>PCFG models of linguistic tree representations. Computational Linguistics.</title>
<date>1998</date>
<note>To appear.</note>
<contexts>
<context position="5879" citStr="Johnson 1998" startWordPosition="959" endWordPosition="960">specific information is given about T that would make it possible to know it from S. Greater effort is required—for example, enumerating all the elements in S—to find what T is. Since S is big, the uncertainty about the sample is then much greater. Correspondingly, for the two cases, the entropy is 0 and N log N&gt;&gt; 0, respectively. Entropy plays a central role in the theory of information. For an excellent exposition of this theory, we refer the reader to Cover and Thomas (1991). The theory has been applied in probabilistic language modeling (Mark, Miller, and Grenander 1996; Mark et al. 1996; Johnson 1998), natural language processing (Berger, Della Pietra, and Della Pietra 1996; Della Pietra, Della Pietra, and Lafferty 1997), as well as computational vision (Zhu, Wu, and Mumford 1997). In addition, all the models proposed in these articles are based on an important principle called the maximum entropy principle. Chapter 11 of Cover and Thomas (1991) gives an introduction to this principle. 1 A probabilistic context-free grammar is also called a stochastic context-free grammar (SCFG) 132 Chi Probabilistic Context-Free Grammars Briefly, the maximum entropy principle says that among all the distr</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Johnson, Mark. 1998. PCFG models of linguistic tree representations. Computational Linguistics. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin E Mark</author>
</authors>
<title>Markov Random Field Models for Natural Language.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Electrical Engineering, Washington University,</institution>
<contexts>
<context position="9181" citStr="Mark 1997" startWordPosition="1486" endWordPosition="1487">moment and the mean squared is the variance of sizes, which tells us how &amp;quot;scattered&amp;quot; sizes of parses are distributed around the mean. Proposition 2 shows that, under distributions imposed by production probabilities assigned by the relative weighted frequency method, sizes of parses have finite moment of any order. 1.3 Gibbs Distributions on Parses and Renormalization of Improper PCFGs Besides PCFG distributions, a CFG can be equipped with many other types of probability distributions. Among the most widely studied is the Gibbs distribution (Mark, Miller, and Grenander 1996; Mark et al. 1996; Mark 1997; Abney 1997). Gibbs distributions arise naturally by invoking the maximum entropy principle. They are considered to be more powerful than PCFG distributions because they incorporate more features, especially context-sensitive features, of natural languages, whereas PCFG distributions only consider frequencies of production rules. On the other hand, Gibbs distributions are not always superior to PCFG distributions. A Gibbs distribution, with only frequencies of production rules in parse as its features, turns into a PCFG. More specifically, we will show in Proposition 4 in Section 5, that a CF</context>
</contexts>
<marker>Mark, 1997</marker>
<rawString>Mark, Kevin E. 1997. Markov Random Field Models for Natural Language. Ph.D. thesis, Department of Electrical Engineering, Washington University, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin E Mark</author>
<author>Michael I Miller</author>
<author>Ulf Grenander</author>
</authors>
<title>Constrained stochastic language models.</title>
<date>1996</date>
<pages>131--140</pages>
<editor>In S. E. Levinson and L. Shepp, editors,</editor>
<publisher>Springer-Verlag,</publisher>
<contexts>
<context position="5864" citStr="Mark et al. 1996" startWordPosition="955" endWordPosition="958">ly equivalent. No specific information is given about T that would make it possible to know it from S. Greater effort is required—for example, enumerating all the elements in S—to find what T is. Since S is big, the uncertainty about the sample is then much greater. Correspondingly, for the two cases, the entropy is 0 and N log N&gt;&gt; 0, respectively. Entropy plays a central role in the theory of information. For an excellent exposition of this theory, we refer the reader to Cover and Thomas (1991). The theory has been applied in probabilistic language modeling (Mark, Miller, and Grenander 1996; Mark et al. 1996; Johnson 1998), natural language processing (Berger, Della Pietra, and Della Pietra 1996; Della Pietra, Della Pietra, and Lafferty 1997), as well as computational vision (Zhu, Wu, and Mumford 1997). In addition, all the models proposed in these articles are based on an important principle called the maximum entropy principle. Chapter 11 of Cover and Thomas (1991) gives an introduction to this principle. 1 A probabilistic context-free grammar is also called a stochastic context-free grammar (SCFG) 132 Chi Probabilistic Context-Free Grammars Briefly, the maximum entropy principle says that amon</context>
<context position="9170" citStr="Mark et al. 1996" startWordPosition="1482" endWordPosition="1485">etween the second moment and the mean squared is the variance of sizes, which tells us how &amp;quot;scattered&amp;quot; sizes of parses are distributed around the mean. Proposition 2 shows that, under distributions imposed by production probabilities assigned by the relative weighted frequency method, sizes of parses have finite moment of any order. 1.3 Gibbs Distributions on Parses and Renormalization of Improper PCFGs Besides PCFG distributions, a CFG can be equipped with many other types of probability distributions. Among the most widely studied is the Gibbs distribution (Mark, Miller, and Grenander 1996; Mark et al. 1996; Mark 1997; Abney 1997). Gibbs distributions arise naturally by invoking the maximum entropy principle. They are considered to be more powerful than PCFG distributions because they incorporate more features, especially context-sensitive features, of natural languages, whereas PCFG distributions only consider frequencies of production rules. On the other hand, Gibbs distributions are not always superior to PCFG distributions. A Gibbs distribution, with only frequencies of production rules in parse as its features, turns into a PCFG. More specifically, we will show in Proposition 4 in Section 5</context>
</contexts>
<marker>Mark, Miller, Grenander, 1996</marker>
<rawString>Mark, Kevin E., Michael I. Miller, and Ulf Grenander. 1996. Constrained stochastic language models. In S. E. Levinson and L. Shepp, editors, Image Models (and Their Speech Model Cousins). Springer-Verlag, pages 131-140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin E Mark</author>
<author>Michael I Miller</author>
<author>Ulf Grenander</author>
<author>Steven P Abney</author>
</authors>
<title>Parameter estimation for constrained context-free language models.</title>
<date>1996</date>
<booktitle>In Proceedings of the DARPA Speech and Natural Language Workshop, Image Models (and Their Speech Model Cousins),</booktitle>
<pages>146--149</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>Harriman, NY,</location>
<contexts>
<context position="5864" citStr="Mark et al. 1996" startWordPosition="955" endWordPosition="958">ly equivalent. No specific information is given about T that would make it possible to know it from S. Greater effort is required—for example, enumerating all the elements in S—to find what T is. Since S is big, the uncertainty about the sample is then much greater. Correspondingly, for the two cases, the entropy is 0 and N log N&gt;&gt; 0, respectively. Entropy plays a central role in the theory of information. For an excellent exposition of this theory, we refer the reader to Cover and Thomas (1991). The theory has been applied in probabilistic language modeling (Mark, Miller, and Grenander 1996; Mark et al. 1996; Johnson 1998), natural language processing (Berger, Della Pietra, and Della Pietra 1996; Della Pietra, Della Pietra, and Lafferty 1997), as well as computational vision (Zhu, Wu, and Mumford 1997). In addition, all the models proposed in these articles are based on an important principle called the maximum entropy principle. Chapter 11 of Cover and Thomas (1991) gives an introduction to this principle. 1 A probabilistic context-free grammar is also called a stochastic context-free grammar (SCFG) 132 Chi Probabilistic Context-Free Grammars Briefly, the maximum entropy principle says that amon</context>
<context position="9170" citStr="Mark et al. 1996" startWordPosition="1482" endWordPosition="1485">etween the second moment and the mean squared is the variance of sizes, which tells us how &amp;quot;scattered&amp;quot; sizes of parses are distributed around the mean. Proposition 2 shows that, under distributions imposed by production probabilities assigned by the relative weighted frequency method, sizes of parses have finite moment of any order. 1.3 Gibbs Distributions on Parses and Renormalization of Improper PCFGs Besides PCFG distributions, a CFG can be equipped with many other types of probability distributions. Among the most widely studied is the Gibbs distribution (Mark, Miller, and Grenander 1996; Mark et al. 1996; Mark 1997; Abney 1997). Gibbs distributions arise naturally by invoking the maximum entropy principle. They are considered to be more powerful than PCFG distributions because they incorporate more features, especially context-sensitive features, of natural languages, whereas PCFG distributions only consider frequencies of production rules. On the other hand, Gibbs distributions are not always superior to PCFG distributions. A Gibbs distribution, with only frequencies of production rules in parse as its features, turns into a PCFG. More specifically, we will show in Proposition 4 in Section 5</context>
</contexts>
<marker>Mark, Miller, Grenander, Abney, 1996</marker>
<rawString>Mark, Kevin E., Michael I. Miller, Ulf Grenander, and Steven P. Abney. 1996. Parameter estimation for constrained context-free language models. In Proceedings of the DARPA Speech and Natural Language Workshop, Image Models (and Their Speech Model Cousins), pages 146-149, Harriman, NY, February. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael I Miller</author>
<author>Joseph A O&apos;Sullivan</author>
</authors>
<title>Entropies and combinatorics of random branching processes and context-free languages.</title>
<date>1992</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>38</volume>
<issue>4</issue>
<contexts>
<context position="7770" citStr="Miller and O&apos;Sullivan (1992)" startWordPosition="1250" endWordPosition="1253">log( q(T) g(r) log H(q) 1 PT) Per) subject to a set of constraints incorporating context-sensitive features is chosen to be the distribution of the model. It is then easy to see that the assumption that H(q) is finite is necessary. Conceptually, having finite entropy is a basic requirement for a &amp;quot;good&amp;quot; probabilistic model because a probability distribution with infinite entropy has too much uncertainty to be informative. Problems regarding entropies of PCFGs are relatively easy to tackle because they can be studied analytically. Several authors have reported results on this subject, including Miller and O&apos;Sullivan (1992), who gave analytical results on the rates of entropies of improper PCFGs. It is worthwhile to add a few more results on entropies of proper PCFGs. In this paper, we show that the entropies of PCFG distributions imposed by production probabilities assigned by the relative weighted frequency method are finite (Section 4, Corollary 2). In addition to entropy, we will also study the moment of sizes of parses. The moment is of statistical interest because it gives information on how sizes of parses are distributed. For PCFG distributions, the first moment of sizes of parses, i.e., the mean size of</context>
<context position="12023" citStr="Miller and O&apos;Sullivan (1992)" startWordPosition="1941" endWordPosition="1944">se. 1.4 Branching Rates of PCFGs Because of their context-free nature, PCFG distributions can also be studied from the perspective of stochastic processes. A PCFG can be described by a random branching process (Harris 1963), and its asymptotic behavior can be characterized by its branching rate. A branching process, or its corresponding PCFG, is called subcritical (critical, supercritical), if its branching rate &lt; 1 (= 1,&gt; 1). A subcritical PCFG is always proper, whereas a supercritical PCFG is always improper. Many asymptotic properties of supercritical branching processes are established by Miller and O&apos;Sullivan (1992). Chi and Geman (1998) proved the properness of PCFG distributions imposed by estimated production probabilities, and around the same time Sanchez and Benedi (1997) established the subcriticality of the corresponding branching processes, hence their properness. In this paper we will explore properties of branching rate further. First, in Proposition 5, we will show that if a PCFG distribution is imposed by production probabilities assigned by the relative weighted frequency method, then the PCFG is subcritical. The result generalizes that of Sanchez and Benedi (1997), and has a less involved p</context>
<context position="42273" citStr="Miller and O&apos;Sullivan (1992)" startWordPosition="7827" endWordPosition="7830">ibutions on parses. 150 Chi Probabilistic Context-Free Grammars Proof Suppose S■ is a solution for (24). By Proposition 4, the Gibbs distribution Ps, is imposed by a system of production probabilities /3. Then /3 is the solution of (23). Let A = log 13, i.e., A(A a) = log (A —&gt; a). Then A impose the same distribution on parses as A. Therefore A are also a solution to (24). This proves the first half of the result. The second half is similarly proved. 6. Branching Rates of PCFGs In this section, we study PCFGs from the perspective of stochastic branching processes. Adopting the set-up given by Miller and O&apos;Sullivan (1992), we define the mean matrix M of p as a INI x IN1 square matrix, with its (A, B)th entry being the expected number of variables B resulting from rewriting A: M(A,B) = E p(A ce)n(B; a). (27) a E (Nu T)* sot. (A—cs)ER Clearly, M is a nonnegative matrix. We say B E N can be reached from A E N, if for some n &gt; 0, M(n) (A, B) &gt; 0, where M(&apos;) (A, B) is the (A, B)th element of Mn. M is irreducible if for any pair A,&apos;B E N, B can be reached from A. The corresponding branching process is called connected if M is irreducible (Walters 1982). It is easy to check that these definitions are equivalent to De</context>
</contexts>
<marker>Miller, O&apos;Sullivan, 1992</marker>
<rawString>Miller, Michael I. and Joseph A. O&apos;Sullivan. 1992. Entropies and combinatorics of random branching processes and context-free languages. IEEE Transactions on Information Theory, 38(4), July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joan-Artdreu Sdnchez</author>
<author>José-Miguel Beneclii</author>
</authors>
<title>Consistency of stochastic context-free grammars from probabilistic estimation based on growth transformations.</title>
<date>1997</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>19--9</pages>
<marker>Sdnchez, Beneclii, 1997</marker>
<rawString>Sdnchez, Joan-Artdreu and José-Miguel Beneclii. 1997. Consistency of stochastic context-free grammars from probabilistic estimation based on growth transformations. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(9):1052-1055, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Walters</author>
</authors>
<title>An Introduction to Ergodic Theory.</title>
<date>1982</date>
<publisher>Springer-Verlag, NY.</publisher>
<contexts>
<context position="42808" citStr="Walters 1982" startWordPosition="7942" endWordPosition="7943">branching processes. Adopting the set-up given by Miller and O&apos;Sullivan (1992), we define the mean matrix M of p as a INI x IN1 square matrix, with its (A, B)th entry being the expected number of variables B resulting from rewriting A: M(A,B) = E p(A ce)n(B; a). (27) a E (Nu T)* sot. (A—cs)ER Clearly, M is a nonnegative matrix. We say B E N can be reached from A E N, if for some n &gt; 0, M(n) (A, B) &gt; 0, where M(&apos;) (A, B) is the (A, B)th element of Mn. M is irreducible if for any pair A,&apos;B E N, B can be reached from A. The corresponding branching process is called connected if M is irreducible (Walters 1982). It is easy to check that these definitions are equivalent to Definition 3. We need the result below for the study of branching processes. Theorem 1: (Perron-Frobenius) Let M = [mg] be a nonnegative k x k matrix. 1. There is a nonnegative eigenvalue p such that no eigenvalue of A has absolute value greater than p. 2. Corresponding to the eigenvalue p there is a nonnegative left (row) eigenvector v = (v1, , uk) and a nonnegative right (column) eigenvector itl = ;:k) • 3. If M is irreducible then p is a simple eigenvalue (i.e., the multiplicity of p is 1), and the corresponding eigenvectors are</context>
</contexts>
<marker>Walters, 1982</marker>
<rawString>Walters, Peter. 1982. An Introduction to Ergodic Theory. Springer-Verlag, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Song Chun Zhu</author>
<author>Ying Nian Wu</author>
<author>David B Mumford</author>
</authors>
<title>Minimax entropy principle and its application to texture modeling.</title>
<date>1997</date>
<journal>Neural Computation,</journal>
<pages>9--8</pages>
<marker>Zhu, Wu, Mumford, 1997</marker>
<rawString>Zhu, Song Chun, Ying Nian Wu, and David B. Mumford. 1997. Minimax entropy principle and its application to texture modeling. Neural Computation, 9(8):1627-1660.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>