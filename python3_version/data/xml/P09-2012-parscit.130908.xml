<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004318">
<title confidence="0.984245">
Bayesian Learning of a Tree Substitution Grammar
</title>
<author confidence="0.998566">
Matt Post and Daniel Gildea
</author>
<affiliation confidence="0.9985">
Department of Computer Science
University of Rochester
</affiliation>
<address confidence="0.33792">
Rochester, NY 14627
</address>
<sectionHeader confidence="0.976683" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998611">
Tree substitution grammars (TSGs) of-
fer many advantages over context-free
grammars (CFGs), but are hard to learn.
Past approaches have resorted to heuris-
tics. In this paper, we learn a TSG us-
ing Gibbs sampling with a nonparamet-
ric prior to control subtree size. The
learned grammars perform significantly
better than heuristically extracted ones on
parsing accuracy.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965708333333">
Tree substition grammars (TSGs) have potential
advantages over regular context-free grammars
(CFGs), but there is no obvious way to learn these
grammars. In particular, learning procedures are
not able to take direct advantage of manually an-
notated corpora like the Penn Treebank, which are
not marked for derivations and thus assume a stan-
dard CFG. Since different TSG derivations can
produce the same parse tree, learning procedures
must guess the derivations, the number of which is
exponential in the tree size. This compels heuristic
methods of subtree extraction, or maximum like-
lihood estimators which tend to extract large sub-
trees that overfit the training data.
These problems are common in natural lan-
guage processing tasks that search for a hid-
den segmentation. Recently, many groups have
had success using Gibbs sampling to address the
complexity issue and nonparametric priors to ad-
dress the overfitting problem (DeNero et al., 2008;
Goldwater et al., 2009). In this paper we apply
these techniques to learn a tree substitution gram-
mar, evaluate it on the Wall Street Journal parsing
task, and compare it to previous work.
</bodyText>
<sectionHeader confidence="0.998981" genericHeader="introduction">
2 Model
</sectionHeader>
<subsectionHeader confidence="0.989724">
2.1 Tree substitution grammars
</subsectionHeader>
<bodyText confidence="0.999975588235294">
TSGs extend CFGs (and their probabilistic coun-
terparts, which concern us here) by allowing non-
terminals to be rewritten as subtrees of arbitrary
size. Although nonterminal rewrites are still
context-free, in practice TSGs can loosen the in-
dependence assumptions of CFGs because larger
rules capture more context. This is simpler than
the complex independence and backoff decisions
of Markovized grammars. Furthermore, subtrees
with terminal symbols can be viewed as learn-
ing dependencies among the words in the subtree,
obviating the need for the manual specification
(Magerman, 1995) or automatic inference (Chiang
and Bikel, 2002) of lexical dependencies.
Following standard notation for PCFGs, the
probability of a derivation d in the grammar is
given as
</bodyText>
<equation confidence="0.968027">
Pr(d) = � Pr(r)
r∈d
</equation>
<bodyText confidence="0.995646">
where each r is a rule used in the derivation. Un-
der a regular CFG, each parse tree uniquely idenfi-
fies a derivation. In contrast, multiple derivations
in a TSG can produce the same parse; obtaining
the parse probability requires a summation over
all derivations that could have produced it. This
disconnect between parses and derivations com-
plicates both inference and learning. The infer-
ence (parsing) task for TSGs is NP-hard (Sima’an,
1996), and in practice the most probable parse is
approximated (1) by sampling from the derivation
forest or (2) from the top k derivations.
Grammar learning is more difficult as well.
CFGs are usually trained on treebanks, especially
the Wall Street Journal (WSJ) portion of the Penn
Treebank. Once the model is defined, relevant
</bodyText>
<page confidence="0.994018">
45
</page>
<note confidence="0.9674655">
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 45–48,
Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP
</note>
<figure confidence="0.971160090909091">
400
350
300
250
200
150
100
50
0
0 2 4 6 8 10 12 14
subtree height
</figure>
<figureCaption confidence="0.757202">
Figure 1: Subtree count (thousands) across heights
for the “all subtrees” grammar (❑) and the supe-
rior “minimal subset” (0) from Bod (2001).
</figureCaption>
<bodyText confidence="0.99992">
events can simply be counted in the training data.
In contrast, there are no treebanks annotated with
TSG derivations, and a treebank parse tree of n
nodes is ambiguous among 2n possible deriva-
tions. One solution would be to manually annotate
a treebank with TSG derivations, but in addition
to being expensive, this task requires one to know
what the grammar actually is. Part of the thinking
motivating TSGs is to let the data determine the
best set of subtrees.
One approach to grammar-learning is Data-
Oriented Parsing (DOP), whose strategy is to sim-
ply take all subtrees in the training data as the
grammar (Bod, 1993). Bod (2001) did this, ap-
proximating “all subtrees” by extracting from the
Treebank 400K random subtrees for each subtree
height ranging from two to fourteen, and com-
pared the performance of that grammar to that
of a heuristically pruned “minimal subset” of it.
The latter’s performance was quite good, achiev-
ing 90.8% F1 score1 on section 23 of the WSJ.
This approach is unsatisfying in some ways,
however. Instead of heuristic extraction we would
prefer a model that explained the subtrees found
in the grammar. Furthermore, it seems unlikely
that subtrees with ten or so lexical items will be
useful on average at test time (Bod did not report
how often larger trees are used, but did report that
including subtrees with up to twelve lexical items
improved parser performance). We expect there to
be fewer large subtrees than small ones. Repeat-
ing Bod’s grammar extraction experiment, this is
indeed what we find when comparing these two
grammars (Figure 1).
In summary, we would like a principled (model-
based) means of determining from the data which
</bodyText>
<footnote confidence="0.795567">
1The harmonic mean of precision and recall: Fl = 2P R
</footnote>
<equation confidence="0.782549">
P +R .
</equation>
<bodyText confidence="0.999922428571429">
set of subtrees should be added to our grammar,
and we would like to do so in a manner that prefers
smaller subtrees but permits larger ones if the data
warrants it. This type of requirement is common in
NLP tasks that require searching for a hidden seg-
mentation, and in the following sections we apply
it to learning a TSG from the Penn Treebank.
</bodyText>
<subsectionHeader confidence="0.979735">
2.2 Collapsed Gibbs sampling with a DP
</subsectionHeader>
<bodyText confidence="0.996074777777778">
prior2
For an excellent introduction to collapsed Gibbs
sampling with a DP prior, we refer the reader to
Appendix A of Goldwater et al. (2009), which we
follow closely here. Our training data is a set of
parse trees T that we assume was produced by an
unknown TSG g with probability Pr(T |g). Using
Bayes’ rule, we can compute the probability of a
particular hypothesized grammar as
</bodyText>
<equation confidence="0.999518">
Pr(T  |g) Pr(g)
Pr(g  |T) =
Pr(T)
</equation>
<bodyText confidence="0.993007777777778">
Pr(g) is a distribution over grammars that ex-
presses our a priori preference for g. We use a set
of Dirichlet Process (DP) priors (Ferguson, 1973),
one for each nonterminal X E N, the set of non-
terminals in the grammar. A sample from a DP
is a distribution over events in an infinite sample
space (in our case, potential subtrees in a TSG)
which takes two parameters, a base measure and a
concentration parameter:
</bodyText>
<equation confidence="0.96694475">
gX ∼ DP(GX, α)
�
GX(t) = Pr$(|t|;p$)
r∈t
</equation>
<bodyText confidence="0.999869071428572">
The base measure GX defines the probability of a
subtree t as the product of the PCFG rules r E t
that constitute it and a geometric distribution Pr$
over the number of those rules, thus encoding a
preference for smaller subtrees.3 The parameter α
contributes to the probability that previously un-
seen subtrees will be sampled. All DPs share pa-
rameters p$ and α. An entire grammar is then
given as g = {gX : X E N}. We emphasize that
no head information is used by the sampler.
Rather than explicitly consider each segmen-
tation of the parse trees (which would define a
TSG and its associated parameters), we use a col-
lapsed Gibbs sampler to integrate over all possi-
</bodyText>
<footnote confidence="0.8832105">
2Cohn et al. (2009) and O’Donnell et al. (2009) indepen-
dently developed similar models.
</footnote>
<equation confidence="0.971828">
3GX(t) = 0 unless root(t) = X.
PrMLE(r)
</equation>
<page confidence="0.995898">
46
</page>
<figureCaption confidence="0.866618">
Figure 2: Depiction of sub(52) and sub(52).
Highlighted subtrees correspond with our spinal
extraction heuristic (§3). Circles denote nodes
whose flag=1.
</figureCaption>
<bodyText confidence="0.999616476190476">
ble grammars and sample directly from the poste-
rior. This is based on the Chinese Restaurant Pro-
cess (CRP) representation of the DP. The Gibbs
sampler is an iterative procedure. At initialization,
each parse tree in the corpus is annotated with a
specific derivation by marking each node in the
tree with a binary flag. This flag indicates whether
the subtree rooted at that node (a height one CFG
rule, at minimum) is part of the subtree contain-
ing its parent. The Gibbs sampler considers ev-
ery non-terminal, non-root node c of each parse
tree in turn, freezing the rest of the training data
and randomly choosing whether to join the sub-
trees above c and rooted at c (outcome h1) or to
split them (outcome h2) according to the probabil-
ity ratio φ(h1)/(φ(h1) + φ(h2)), where φ assigns
a probability to each of the outcomes (Figure 2).
Let sub(n) denote the subtree above and includ-
ing node n and sub(n) the subtree rooted at n; o is
a binary operator that forms a single subtree from
two adjacent ones. The outcome probabilities are:
</bodyText>
<equation confidence="0.999936">
φ(h1) = θ(t)
φ(h2) = θ(sub(c)) · θ(sub(c))
</equation>
<bodyText confidence="0.9991968">
where t = sub(c) o sub(c). Under the CRP, the
subtree probability θ(t) is a function of the current
state of the rest of the training corpus, the appro-
priate base measure Groot(t), and the concentra-
tion parameter α:
</bodyText>
<equation confidence="0.9984655">
countzt(t) + αGroot(t)(t)
JztJ + α
</equation>
<bodyText confidence="0.99977125">
where zt is the multiset of subtrees in the frozen
portion of the training corpus sharing the same
root as t, and countzt(t) is the count of subtree
t among them.
</bodyText>
<sectionHeader confidence="0.999269" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.98909">
3.1 Setup
</subsectionHeader>
<bodyText confidence="0.998793">
We used the standard split for the Wall Street Jour-
nal portion of the Treebank, training on sections 2
to 21, and reporting results on sentences with no
more than forty words from section 23.
We compare with three other grammars.
</bodyText>
<listItem confidence="0.986455545454545">
• A standard Treebank PCFG.
• A “spinal” TSG, produced by extracting n
lexicalized subtrees from each length n sen-
tence in the training data. Each subtree is de-
fined as the sequence of CFG rules from leaf
upward all sharing a head, according to the
Magerman head-selection rules. We detach
the top-level unary rule, and add in counts
from the Treebank CFG rules.
• An in-house version of the heuristic “mini-
mal subset” grammar of Bod (2001).4
</listItem>
<bodyText confidence="0.999798916666667">
We note two differences in our work that ex-
plain the large difference in scores for the minimal
grammar from those reported by Bod: (1) we did
not implement the smoothed “mismatch parsing”,
which permits lexical leaves of subtrees to act as
wildcards, and (2) we approximate the most prob-
able parse with the top single derivation instead of
the top 1,000.
Rule probabilities for all grammars were set
with relative frequency. The Gibbs sampler was
initialized with the spinal grammar derivations.
We construct sampled grammars in two ways: by
summing all subtree counts from the derivation
states of the first i sampling iterations together
with counts from the Treebank CFG rules (de-
noted (α, p$,≤i)), and by taking the counts only
from iteration i (denoted (α, p$, i)).
Our standard CKY parser and Gibbs sampler
were both written in Perl. TSG subtrees were flat-
tened to CFG rules and reconstructed afterward,
with identical mappings favoring the most proba-
ble rule. For pruning, we binned nonterminals ac-
cording to input span and degree of binarization,
keeping the ten highest scoring items in each bin.
</bodyText>
<subsectionHeader confidence="0.75908">
3.2 Results
</subsectionHeader>
<tableCaption confidence="0.584819">
Table 1 contains parser scores. The spinal TSG
outperforms a standard unlexicalized PCFG and
4All rules of height one, plus 400K subtrees sampled at
each height h, 2 &lt; h &lt; 14, minus unlexicalized subtrees of
h &gt; 6 and lexicalized subtrees with more than twelve words.
</tableCaption>
<figure confidence="0.971110642857143">
S1
NP
ADVP
VP
RB VBZ S2
Someone always makes
NP
VP
PRP
you
VB
quit
NN
θ(t) =
</figure>
<page confidence="0.996941">
47
</page>
<table confidence="0.999894842105263">
grammar size LP LR F1
PCFG 46K 75.37 70.05 72.61
spinal 190K 80.30 78.10 79.18
minimal subset 2.56M 76.40 78.29 77.33
(10, 0.7,100) 62K 81.48 81.03 81.25
(10, 0.8,100) 61K 81.23 80.79 81.00
(10, 0.9, 100) 61K 82.07 81.17 81.61
(100, 0.7,100) 64K 81.23 80.98 81.10
(100, 0.8,100) 63K 82.13 81.36 81.74
(100, 0.9,100) 62K 82.11 81.20 81.65
(100, 0.7, &lt;100) 798K 82.38 82.27 82.32
(100, 0.8, &lt;100) 506K 82.27 81.95 82.10
(100, 0.9, &lt;100) 290K 82.64 82.09 82.36
(100, 0.7, 500) 61K 81.95 81.76 81.85
(100, 0.8, 500) 60K 82.73 82.21 82.46
(100, 0.9, 500) 59K 82.57 81.53 82.04
(100, 0.7, &lt;500) 2.05M 82.81 82.01 82.40
(100, 0.8, &lt;500) 1.13M 83.06 82.10 82.57
(100, 0.9, &lt;500) 528K 83.17 81.91 82.53
</table>
<tableCaption confidence="0.943774">
Table 1: Labeled precision, recall, and F1 on
WSJ§23.
</tableCaption>
<bodyText confidence="0.999902583333333">
the significantly larger “minimal subset” grammar.
The sampled grammars outperform all of them.
Nearly all of the rules of the best single iteration
sampled grammar (100, 0.8, 500) are lexicalized
(50,820 of 60,633), and almost half of them have
a height greater than one (27,328). Constructing
sampled grammars by summing across iterations
improved over this in all cases, but at the expense
of a much larger grammar.
Figure 3 shows a histogram of subtree size taken
from the counts of the subtrees (by token, not type)
actually used in parsing WSJ§23. Parsing with
the “minimal subset” grammar uses highly lexi-
calized subtrees, but they do not improve accuracy.
We examined sentence-level F1 scores and found
that the use of larger subtrees did correlate with
accuracy; however, the low overall accuracy (and
the fact that there are so many of these large sub-
trees available in the grammar) suggests that such
rules are overfit. In contrast, the histogram of sub-
tree sizes used in parsing with the sampled gram-
mar matches the shape of the histogram from the
grammar itself. Gibbs sampling with a DP prior
chooses smaller but more general rules.
</bodyText>
<sectionHeader confidence="0.999526" genericHeader="method">
4 Summary
</sectionHeader>
<bodyText confidence="0.999701">
Collapsed Gibbs sampling with a DP prior fits
nicely with the task of learning a TSG. The sam-
pled grammars are model-based, are simple to
specify and extract, and take the expected shape
</bodyText>
<figure confidence="0.910719">
0 2 4 6 8 10 12
number of words in subtree’s frontier
</figure>
<figureCaption confidence="0.998140666666667">
Figure 3: Histogram of subtrees sizes used in pars-
ing WSJ§23 (filled points), as well as from the
grammars themselves (outlined points).
</figureCaption>
<bodyText confidence="0.996436166666667">
over subtree size. They substantially outperform
heuristically extracted grammars from previous
work as well as our novel spinal grammar, and can
do so with many fewer rules.
Acknowledgments This work was supported by
NSF grants IIS-0546554 and ITR-0428020.
</bodyText>
<sectionHeader confidence="0.999447" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999307296296296">
Rens Bod. 1993. Using an annotated corpus as a
stochastic grammar. In Proc. ACL.
Rens Bod. 2001. What is the minimal set of fragments
that achieves maximal parse accuracy. In Proc. ACL.
David Chiang and Daniel M. Bikel. 2002. Recovering
latent information in treebanks. In COLING.
Trevor Cohn, Sharon Goldwater, and Phil Blun-
som. 2009. Inducing compact but accurate tree-
substitution grammars. In Proc. NAACL.
John DeNero, Alexandre Bouchard-Cˆot´e, and Dan
Klein. 2008. Sampling alignment structure under
a Bayesian translation model. In EMNLP.
Thomas S. Ferguson. 1973. A Bayesian analysis of
some nonparametric problems. Annals of Mathe-
matical Statistics, 1(2):209–230.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2009. A Bayesian framework for word
segmentation: Exploring the effects of context.
Cognition.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proc. ACL.
T.J. O’Donnell, N.D. Goodman, J. Snedeker, and J.B.
Tenenbaum. 2009. Computation and reuse in lan-
guage. In Proc. Cognitive Science Society.
Khalil Sima’an. 1996. Computational complexity of
probabilistic disambiguation by means of tree gram-
mars. In COLING.
</reference>
<bodyText confidence="0.896401">
(100,0.8,500), actual grammar
(100,0.8,500), used parsing WSJ23
minimal, actual grammar
minimal, used parsing WSJ23
</bodyText>
<page confidence="0.77273825">
106
105
104
103
102
101
100
48
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.973188">
<title confidence="0.998859">Bayesian Learning of a Tree Substitution Grammar</title>
<author confidence="0.999712">Post Gildea</author>
<affiliation confidence="0.99995">Department of Computer Science University of Rochester</affiliation>
<address confidence="0.999815">Rochester, NY 14627</address>
<abstract confidence="0.997666363636364">Tree substitution grammars (TSGs) offer many advantages over context-free grammars (CFGs), but are hard to learn. Past approaches have resorted to heuristics. In this paper, we learn a TSG using Gibbs sampling with a nonparametric prior to control subtree size. The learned grammars perform significantly better than heuristically extracted ones on parsing accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>Using an annotated corpus as a stochastic grammar.</title>
<date>1993</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="4228" citStr="Bod, 1993" startWordPosition="682" endWordPosition="683">imply be counted in the training data. In contrast, there are no treebanks annotated with TSG derivations, and a treebank parse tree of n nodes is ambiguous among 2n possible derivations. One solution would be to manually annotate a treebank with TSG derivations, but in addition to being expensive, this task requires one to know what the grammar actually is. Part of the thinking motivating TSGs is to let the data determine the best set of subtrees. One approach to grammar-learning is DataOriented Parsing (DOP), whose strategy is to simply take all subtrees in the training data as the grammar (Bod, 1993). Bod (2001) did this, approximating “all subtrees” by extracting from the Treebank 400K random subtrees for each subtree height ranging from two to fourteen, and compared the performance of that grammar to that of a heuristically pruned “minimal subset” of it. The latter’s performance was quite good, achieving 90.8% F1 score1 on section 23 of the WSJ. This approach is unsatisfying in some ways, however. Instead of heuristic extraction we would prefer a model that explained the subtrees found in the grammar. Furthermore, it seems unlikely that subtrees with ten or so lexical items will be usef</context>
</contexts>
<marker>Bod, 1993</marker>
<rawString>Rens Bod. 1993. Using an annotated corpus as a stochastic grammar. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>What is the minimal set of fragments that achieves maximal parse accuracy.</title>
<date>2001</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="3604" citStr="Bod (2001)" startWordPosition="575" endWordPosition="576">ated (1) by sampling from the derivation forest or (2) from the top k derivations. Grammar learning is more difficult as well. CFGs are usually trained on treebanks, especially the Wall Street Journal (WSJ) portion of the Penn Treebank. Once the model is defined, relevant 45 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 45–48, Suntec, Singapore, 4 August 2009. c�2009 ACL and AFNLP 400 350 300 250 200 150 100 50 0 0 2 4 6 8 10 12 14 subtree height Figure 1: Subtree count (thousands) across heights for the “all subtrees” grammar (❑) and the superior “minimal subset” (0) from Bod (2001). events can simply be counted in the training data. In contrast, there are no treebanks annotated with TSG derivations, and a treebank parse tree of n nodes is ambiguous among 2n possible derivations. One solution would be to manually annotate a treebank with TSG derivations, but in addition to being expensive, this task requires one to know what the grammar actually is. Part of the thinking motivating TSGs is to let the data determine the best set of subtrees. One approach to grammar-learning is DataOriented Parsing (DOP), whose strategy is to simply take all subtrees in the training data as</context>
<context position="9736" citStr="Bod (2001)" startWordPosition="1660" endWordPosition="1661">on of the Treebank, training on sections 2 to 21, and reporting results on sentences with no more than forty words from section 23. We compare with three other grammars. • A standard Treebank PCFG. • A “spinal” TSG, produced by extracting n lexicalized subtrees from each length n sentence in the training data. Each subtree is defined as the sequence of CFG rules from leaf upward all sharing a head, according to the Magerman head-selection rules. We detach the top-level unary rule, and add in counts from the Treebank CFG rules. • An in-house version of the heuristic “minimal subset” grammar of Bod (2001).4 We note two differences in our work that explain the large difference in scores for the minimal grammar from those reported by Bod: (1) we did not implement the smoothed “mismatch parsing”, which permits lexical leaves of subtrees to act as wildcards, and (2) we approximate the most probable parse with the top single derivation instead of the top 1,000. Rule probabilities for all grammars were set with relative frequency. The Gibbs sampler was initialized with the spinal grammar derivations. We construct sampled grammars in two ways: by summing all subtree counts from the derivation states </context>
</contexts>
<marker>Bod, 2001</marker>
<rawString>Rens Bod. 2001. What is the minimal set of fragments that achieves maximal parse accuracy. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Daniel M Bikel</author>
</authors>
<title>Recovering latent information in treebanks.</title>
<date>2002</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="2351" citStr="Chiang and Bikel, 2002" startWordPosition="360" endWordPosition="363">nd CFGs (and their probabilistic counterparts, which concern us here) by allowing nonterminals to be rewritten as subtrees of arbitrary size. Although nonterminal rewrites are still context-free, in practice TSGs can loosen the independence assumptions of CFGs because larger rules capture more context. This is simpler than the complex independence and backoff decisions of Markovized grammars. Furthermore, subtrees with terminal symbols can be viewed as learning dependencies among the words in the subtree, obviating the need for the manual specification (Magerman, 1995) or automatic inference (Chiang and Bikel, 2002) of lexical dependencies. Following standard notation for PCFGs, the probability of a derivation d in the grammar is given as Pr(d) = � Pr(r) r∈d where each r is a rule used in the derivation. Under a regular CFG, each parse tree uniquely idenfifies a derivation. In contrast, multiple derivations in a TSG can produce the same parse; obtaining the parse probability requires a summation over all derivations that could have produced it. This disconnect between parses and derivations complicates both inference and learning. The inference (parsing) task for TSGs is NP-hard (Sima’an, 1996), and in p</context>
</contexts>
<marker>Chiang, Bikel, 2002</marker>
<rawString>David Chiang and Daniel M. Bikel. 2002. Recovering latent information in treebanks. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Sharon Goldwater</author>
<author>Phil Blunsom</author>
</authors>
<title>Inducing compact but accurate treesubstitution grammars.</title>
<date>2009</date>
<booktitle>In Proc. NAACL.</booktitle>
<contexts>
<context position="7285" citStr="Cohn et al. (2009)" startWordPosition="1226" endWordPosition="1230">product of the PCFG rules r E t that constitute it and a geometric distribution Pr$ over the number of those rules, thus encoding a preference for smaller subtrees.3 The parameter α contributes to the probability that previously unseen subtrees will be sampled. All DPs share parameters p$ and α. An entire grammar is then given as g = {gX : X E N}. We emphasize that no head information is used by the sampler. Rather than explicitly consider each segmentation of the parse trees (which would define a TSG and its associated parameters), we use a collapsed Gibbs sampler to integrate over all possi2Cohn et al. (2009) and O’Donnell et al. (2009) independently developed similar models. 3GX(t) = 0 unless root(t) = X. PrMLE(r) 46 Figure 2: Depiction of sub(52) and sub(52). Highlighted subtrees correspond with our spinal extraction heuristic (§3). Circles denote nodes whose flag=1. ble grammars and sample directly from the posterior. This is based on the Chinese Restaurant Process (CRP) representation of the DP. The Gibbs sampler is an iterative procedure. At initialization, each parse tree in the corpus is annotated with a specific derivation by marking each node in the tree with a binary flag. This flag indi</context>
</contexts>
<marker>Cohn, Goldwater, Blunsom, 2009</marker>
<rawString>Trevor Cohn, Sharon Goldwater, and Phil Blunsom. 2009. Inducing compact but accurate treesubstitution grammars. In Proc. NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Dan Klein</author>
</authors>
<title>Sampling alignment structure under a Bayesian translation model.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<marker>DeNero, Bouchard-Cˆot´e, Klein, 2008</marker>
<rawString>John DeNero, Alexandre Bouchard-Cˆot´e, and Dan Klein. 2008. Sampling alignment structure under a Bayesian translation model. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas S Ferguson</author>
</authors>
<title>A Bayesian analysis of some nonparametric problems.</title>
<date>1973</date>
<journal>Annals of Mathematical Statistics,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="6292" citStr="Ferguson, 1973" startWordPosition="1045" endWordPosition="1046">Penn Treebank. 2.2 Collapsed Gibbs sampling with a DP prior2 For an excellent introduction to collapsed Gibbs sampling with a DP prior, we refer the reader to Appendix A of Goldwater et al. (2009), which we follow closely here. Our training data is a set of parse trees T that we assume was produced by an unknown TSG g with probability Pr(T |g). Using Bayes’ rule, we can compute the probability of a particular hypothesized grammar as Pr(T |g) Pr(g) Pr(g |T) = Pr(T) Pr(g) is a distribution over grammars that expresses our a priori preference for g. We use a set of Dirichlet Process (DP) priors (Ferguson, 1973), one for each nonterminal X E N, the set of nonterminals in the grammar. A sample from a DP is a distribution over events in an infinite sample space (in our case, potential subtrees in a TSG) which takes two parameters, a base measure and a concentration parameter: gX ∼ DP(GX, α) � GX(t) = Pr$(|t|;p$) r∈t The base measure GX defines the probability of a subtree t as the product of the PCFG rules r E t that constitute it and a geometric distribution Pr$ over the number of those rules, thus encoding a preference for smaller subtrees.3 The parameter α contributes to the probability that previou</context>
</contexts>
<marker>Ferguson, 1973</marker>
<rawString>Thomas S. Ferguson. 1973. A Bayesian analysis of some nonparametric problems. Annals of Mathematical Statistics, 1(2):209–230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>A Bayesian framework for word segmentation: Exploring the effects of context.</title>
<date>2009</date>
<journal>Cognition.</journal>
<contexts>
<context position="1514" citStr="Goldwater et al., 2009" startWordPosition="231" endWordPosition="234">. Since different TSG derivations can produce the same parse tree, learning procedures must guess the derivations, the number of which is exponential in the tree size. This compels heuristic methods of subtree extraction, or maximum likelihood estimators which tend to extract large subtrees that overfit the training data. These problems are common in natural language processing tasks that search for a hidden segmentation. Recently, many groups have had success using Gibbs sampling to address the complexity issue and nonparametric priors to address the overfitting problem (DeNero et al., 2008; Goldwater et al., 2009). In this paper we apply these techniques to learn a tree substitution grammar, evaluate it on the Wall Street Journal parsing task, and compare it to previous work. 2 Model 2.1 Tree substitution grammars TSGs extend CFGs (and their probabilistic counterparts, which concern us here) by allowing nonterminals to be rewritten as subtrees of arbitrary size. Although nonterminal rewrites are still context-free, in practice TSGs can loosen the independence assumptions of CFGs because larger rules capture more context. This is simpler than the complex independence and backoff decisions of Markovized </context>
<context position="5873" citStr="Goldwater et al. (2009)" startWordPosition="967" endWordPosition="970"> determining from the data which 1The harmonic mean of precision and recall: Fl = 2P R P +R . set of subtrees should be added to our grammar, and we would like to do so in a manner that prefers smaller subtrees but permits larger ones if the data warrants it. This type of requirement is common in NLP tasks that require searching for a hidden segmentation, and in the following sections we apply it to learning a TSG from the Penn Treebank. 2.2 Collapsed Gibbs sampling with a DP prior2 For an excellent introduction to collapsed Gibbs sampling with a DP prior, we refer the reader to Appendix A of Goldwater et al. (2009), which we follow closely here. Our training data is a set of parse trees T that we assume was produced by an unknown TSG g with probability Pr(T |g). Using Bayes’ rule, we can compute the probability of a particular hypothesized grammar as Pr(T |g) Pr(g) Pr(g |T) = Pr(T) Pr(g) is a distribution over grammars that expresses our a priori preference for g. We use a set of Dirichlet Process (DP) priors (Ferguson, 1973), one for each nonterminal X E N, the set of nonterminals in the grammar. A sample from a DP is a distribution over events in an infinite sample space (in our case, potential subtre</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2009</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2009. A Bayesian framework for word segmentation: Exploring the effects of context. Cognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Magerman</author>
</authors>
<title>Statistical decision-tree models for parsing.</title>
<date>1995</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="2303" citStr="Magerman, 1995" startWordPosition="355" endWordPosition="356">2.1 Tree substitution grammars TSGs extend CFGs (and their probabilistic counterparts, which concern us here) by allowing nonterminals to be rewritten as subtrees of arbitrary size. Although nonterminal rewrites are still context-free, in practice TSGs can loosen the independence assumptions of CFGs because larger rules capture more context. This is simpler than the complex independence and backoff decisions of Markovized grammars. Furthermore, subtrees with terminal symbols can be viewed as learning dependencies among the words in the subtree, obviating the need for the manual specification (Magerman, 1995) or automatic inference (Chiang and Bikel, 2002) of lexical dependencies. Following standard notation for PCFGs, the probability of a derivation d in the grammar is given as Pr(d) = � Pr(r) r∈d where each r is a rule used in the derivation. Under a regular CFG, each parse tree uniquely idenfifies a derivation. In contrast, multiple derivations in a TSG can produce the same parse; obtaining the parse probability requires a summation over all derivations that could have produced it. This disconnect between parses and derivations complicates both inference and learning. The inference (parsing) ta</context>
</contexts>
<marker>Magerman, 1995</marker>
<rawString>David M. Magerman. 1995. Statistical decision-tree models for parsing. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T J O’Donnell</author>
<author>N D Goodman</author>
<author>J Snedeker</author>
<author>J B Tenenbaum</author>
</authors>
<title>Computation and reuse in language.</title>
<date>2009</date>
<booktitle>In Proc. Cognitive Science Society.</booktitle>
<marker>O’Donnell, Goodman, Snedeker, Tenenbaum, 2009</marker>
<rawString>T.J. O’Donnell, N.D. Goodman, J. Snedeker, and J.B. Tenenbaum. 2009. Computation and reuse in language. In Proc. Cognitive Science Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khalil Sima’an</author>
</authors>
<title>Computational complexity of probabilistic disambiguation by means of tree grammars.</title>
<date>1996</date>
<booktitle>In COLING.</booktitle>
<marker>Sima’an, 1996</marker>
<rawString>Khalil Sima’an. 1996. Computational complexity of probabilistic disambiguation by means of tree grammars. In COLING.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>