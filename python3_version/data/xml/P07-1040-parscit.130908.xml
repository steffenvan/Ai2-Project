<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000567">
<title confidence="0.6718815">
Improved Word-Level System Combination for Machine Translation
Antti-Veikko I. Rosti and Spyros Matsoukas and Richard Schwartz
</title>
<address confidence="0.6091825">
BBN Technologies, 10 Moulton Street
Cambridge, MA 02138
</address>
<email confidence="0.991628">
arosti,smatsouk,schwartz @bbn.com
</email>
<sectionHeader confidence="0.995532" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999955227272727">
Recently, confusion network decoding has
been applied in machine translation system
combination. Due to errors in the hypoth-
esis alignment, decoding may result in un-
grammatical combination outputs. This pa-
per describes an improved confusion net-
work based method to combine outputs from
multiple MT systems. In this approach, ar-
bitrary features may be added log-linearly
into the objective function, thus allowing
language model expansion and re-scoring.
Also, a novel method to automatically se-
lect the hypothesis which other hypotheses
are aligned against is proposed. A generic
weight tuning algorithm may be used to op-
timize various automatic evaluation metrics
including TER, BLEU and METEOR. The
experiments using the 2005 Arabic to En-
glish and Chinese to English NIST MT eval-
uation tasks show significant improvements
in BLEU scores compared to earlier confu-
sion network decoding based methods.
</bodyText>
<sectionHeader confidence="0.999335" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99950219047619">
System combination has been shown to improve
classification performance in various tasks. There
are several approaches for combining classifiers. In
ensemble learning, a collection of simple classifiers
is used to yield better performance than any single
classifier; for example boosting (Schapire, 1990).
Another approach is to combine outputs from a few
highly specialized classifiers. The classifiers may
be based on the same basic modeling techniques
but differ by, for example, alternative feature repre-
sentations. Combination of speech recognition out-
puts is an example of this approach (Fiscus, 1997).
In speech recognition, confusion network decoding
(Mangu et al., 2000) has become widely used in sys-
tem combination.
Unlike speech recognition, current statistical ma-
chine translation (MT) systems are based on various
different paradigms; for example phrasal, hierarchi-
cal and syntax-based systems. The idea of combin-
ing outputs from different MT systems to produce
consensus translations in the hope of generating bet-
ter translations has been around for a while (Fred-
erking and Nirenburg, 1994). Recently, confusion
network decoding for MT system combination has
been proposed (Bangalore et al., 2001). To generate
confusion networks, hypotheses have to be aligned
against each other. In (Bangalore et al., 2001), Lev-
enshtein alignment was used to generate the net-
work. As opposed to speech recognition, the word
order between two correct MT outputs may be dif-
ferent and the Levenshtein alignment may not be
able to align shifted words in the hypotheses. In
(Matusov et al., 2006), different word orderings are
taken into account by training alignment models by
considering all hypothesis pairs as a parallel corpus
using GIZA++ (Och and Ney, 2003). The size of
the test set may influence the quality of these align-
ments. Thus, system outputs from development sets
may have to be added to improve the GIZA++ align-
ments. A modified Levenshtein alignment allowing
shifts as in computation of the translation edit rate
(TER) (Snover et al., 2006) was used to align hy-
</bodyText>
<page confidence="0.97123">
312
</page>
<note confidence="0.932837">
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 312–319,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.994000438596491">
potheses in (Sim et al., 2007). The alignments from
TER are consistent as they do not depend on the test
set size. Also, a more heuristic alignment method
has been proposed in a different system combina-
tion approach (Jayaraman and Lavie, 2005). A full
comparison of different alignment methods would
be difficult as many approaches require a significant
amount of engineering.
Confusion networks are generated by choosing
one hypothesis as the “skeleton”, and other hypothe-
ses are aligned against it. The skeleton defines the
word order of the combination output. Minimum
Bayes risk (MBR) was used to choose the skeleton
in (Sim et al., 2007). The average TER score was
computed between each system’s-best hypothesis
and all other hypotheses. The MBR hypothesis is
the one with the minimum average TER and thus,
may be viewed as the closest to all other hypothe-
ses in terms of TER. This work was extended in
(Rosti et al., 2007) by introducing system weights
for word confidences. However, the system weights
did not influence the skeleton selection, so a hypoth-
esis from a system with zero weight might have been
chosen as the skeleton. In this work, confusion net-
works are generated by using the-best output from
each system as the skeleton, and prior probabili-
ties for each network are estimated from the average
TER scores between the skeleton and other hypothe-
ses. All resulting confusion networks are connected
in parallel into a joint lattice where the prior proba-
bilities are also multiplied by the system weights.
The combination outputs from confusion network
decoding may be ungrammatical due to alignment
errors. Also the word-level decoding may break
coherent phrases produced by the individual sys-
tems. In this work, log-posterior probabilities are
estimated for each confusion network arc instead of
using votes or simple word confidences. This allows
a log-linear addition of arbitrary features such as
language model (LM) scores. The LM scores should
increase the total log-posterior of more grammatical
hypotheses. Powell’s method (Brent, 1973) is used
to tune the system and feature weights simultane-
ously so as to optimize various automatic evaluation
metrics on a development set. Tuning is fully auto-
matic, as opposed to (Matusov et al., 2006) where
global system weights were set manually.
This paper is organized as follows. Three evalu-
ation metrics used in weights tuning and reporting
the test set results are reviewed in Section 2. Sec-
tion 3 describes confusion network decoding for MT
system combination. The extensions to add features
log-linearly and improve the skeleton selection are
presented in Sections 4 and 5, respectively. Section
6 details the weights optimization algorithm and the
experimental results are reported in Section 7. Con-
clusions and future work are discussed in Section 8.
</bodyText>
<sectionHeader confidence="0.986273" genericHeader="introduction">
2 Evaluation Metrics
</sectionHeader>
<bodyText confidence="0.995006535714286">
Currently, the most widely used automatic MT eval-
uation metric is the NIST BLEU-4 (Papineni et al.,
2002). It is computed as the geometric mean of -
gram precisions up to-grams between the hypoth-
esis and reference as follows
where is the brevity penalty and
are the -gram precisions. When mul-
tiple references are provided, the -gram counts
against all references are accumulated to compute
the precisions. Similarly, full test set scores are ob-
tained by accumulating counts over all hypothesis
and reference pairs. The BLEU scores are between
and, higher being better. Often BLEU scores are
reported as percentages and “one BLEU point gain”
usually means a BLEU increase of .
Other evaluation metrics have been proposed to
replace BLEU. It has been argued that METEOR
correlates better with human judgment due to higher
weight on recall than precision (Banerjee and Lavie,
2005). METEOR is based on the weighted harmonic
mean of the precision and recall measured on uni-
gram matches as follows
where is the total number of unigram matches,
is the hypothesis length, is the reference length
and is the minimum number of -gram matches
that covers the alignment. The second term is a
fragmentation penalty which penalizes the harmonic
mean by a factor of up to when ; i.e.,
</bodyText>
<page confidence="0.999024">
313
</page>
<bodyText confidence="0.999531375">
there are no matching -grams higher than .
By default, METEOR script counts the words that
match exactly, and words that match after a simple
Porter stemmer. Additional matching modules in-
cluding WordNet stemming and synonymy may also
be used. When multiple references are provided, the
lowest score is reported. Full test set scores are ob-
tained by accumulating statistics over all test sen-
tences. The METEOR scores are also between and
, higher being better. The scores in the results sec-
tion are reported as percentages.
Translation edit rate (TER) (Snover et al., 2006)
has been proposed as more intuitive evaluation met-
ric since it is based on the rate of edits required to
transform the hypothesis into the reference. The
TER score is computed as follows
</bodyText>
<equation confidence="0.589712">
(3)
</equation>
<bodyText confidence="0.999979846153846">
where is the reference length. The only differ-
ence to word error rate is that the TER allows shifts.
A shift of a sequence of words is counted as a sin-
gle edit. The minimum translation edit alignment is
usually found through a beam search. When multi-
ple references are provided, the edits from the clos-
est reference are divided by the average reference
length. Full test set scores are obtained by accumu-
lating the edits and the average reference lengths.
The perfect TER score is 0, and otherwise higher
than zero. The TER score may also be higher than 1
due to insertions. Also TER is reported as a percent-
age in the results section.
</bodyText>
<sectionHeader confidence="0.997691" genericHeader="method">
3 Confusion Network Decoding
</sectionHeader>
<bodyText confidence="0.997037708333333">
Confusion network decoding in MT has to pick
one hypothesis as the skeleton which determines the
word order of the combination. The other hypothe-
ses are aligned against the skeleton. Either votes or
some form of confidences are assigned to each word
in the network. For example using “cat sat the mat”
as the skeleton, aligning “cat sitting on the mat” and
“hat on a mat” against it might yield the following
alignments:
where represents a NULL word. In graphical form,
the resulting confusion network is shown in Figure
1. Each arc represents an alternative word at that
position in the sentence and the number of votes for
each word is marked in parentheses. Confusion net-
work decoding usually requires finding the path with
the highest confidence in the network. Based on vote
counts, there are three alternatives in the example:
“cat sat on the mat”, “cat on the mat” and “cat sit-
ting on the mat”, each having accumulated 10 votes.
The alignment procedure plays an important role, as
by switching the position of the word ‘sat’ and the
following NULL in the skeleton, there would be a
single highest scoring path through the network; that
is, “cat on the mat”.
</bodyText>
<figureCaption confidence="0.99288">
Figure 1: Example consensus network with votes on
word arcs.
</figureCaption>
<bodyText confidence="0.974866066666667">
Different alignment methods yield different con-
fusion networks. The modified Levenshtein align-
ment as used in TER is more natural than simple edit
distance such as word error rate since machine trans-
lation hypotheses may have different word orders
while having the same meaning. As the skeleton
determines the word order, the quality of the com-
bination output also depends on which hypothesis is
chosen as the skeleton. Since the modified Leven-
shtein alignment produces TER scores between the
skeleton and the other hypotheses, a natural choice
for selecting the skeleton is the minimum average
TER score. The hypothesis resulting in the lowest
average TER score when aligned against all other
hypotheses is chosen as the skeleton as follows
</bodyText>
<equation confidence="0.573773">
(4)
</equation>
<bodyText confidence="0.9991">
where is the number of systems. This is equiv-
alent to minimum Bayes risk decoding with uni-
form posterior probabilities (Sim et al., 2007). Other
evaluation metrics may also be used as the MBR
loss function. For BLEU and METEOR, the loss
function would be and
.
It has been found that multiple hypotheses from
each system may be used to improve the quality of
cat sat the mat
cat sitting on the mat
hat on a mat
</bodyText>
<equation confidence="0.9978165">
cat (2) sat (1) on (2) the (2)
hat (1) sitting (1) ε (1) a (1)
ε (1) mat (3)
1 2 3 4 5 6
</equation>
<page confidence="0.988856">
314
</page>
<bodyText confidence="0.98700115625">
the combination output (Sim et al., 2007). When
using -best lists from each system, the words may
be assigned a different score based on the rank of the
hypothesis. In (Rosti et al., 2007), simple
score was assigned to the word coming from the th-
best hypothesis. Due to the computational burden
of the TER alignment, only-best hypotheses were
considered as possible skeletons, and hy-
potheses per system were aligned. Similar approach
to estimate word posteriors is adopted in this work.
System weights may be used to assign a system
specific confidence on each word in the network.
The weights may be based on the systems’ relative
performance on a separate development set or they
may be automatically tuned to optimize some evalu-
ation metric on the development set. In (Rosti et al.,
2007), the total confidence of the th best confusion
network hypothesis , including NULL words,
given the th source sentence was given by
(5)
word-level decoding. For example, two synony-
mous words may be aligned to other words not al-
ready aligned, which may result in repetitive output.
Second, the additive confidence scores in Equation
5 have no probabilistic meaning and cannot there-
fore be combined with language model scores. Lan-
guage model expansion and re-scoring may help by
increasing the probability of more grammatical hy-
potheses in decoding. Third, the system weights are
independent of the skeleton selection. Therefore, a
hypothesis from a system with a low or zero weight
may be chosen as the skeleton.
</bodyText>
<sectionHeader confidence="0.985524" genericHeader="method">
4 Log-Linear Combination with Arbitrary
Features
</sectionHeader>
<bodyText confidence="0.999876666666667">
To address the issue with ungrammatical hypotheses
and allow language model expansion and re-scoring,
the hypothesis confidence computation is modified.
Instead of summing arbitrary confidence scores as in
Equation 5, word posterior probabilities are used as
follows
</bodyText>
<equation confidence="0.664161">
(6)
</equation>
<bodyText confidence="0.996239833333333">
where is the number of nodes in the confu-
sion network for the source sentence ,is the
number of translation systems, is the th system
weight, is the accumulated confidence for word
produced by system between nodes and ,
and is a weight for the number of NULL links
along the hypothesis . The word con-
fidences were increased by if the
word aligns between nodes and in the net-
work. If no word aligns between nodes and ,
the NULL word confidence at that position was in-
creased by . The last term controls the
number of NULL words generated in the output and
may be viewed as an insertion penalty. Each arc in
the confusion network carries the word label and
scores . The decoder outputs the hypothesis
with the highest given the current set of
weights.
</bodyText>
<subsectionHeader confidence="0.938637">
3.1 Discussion
</subsectionHeader>
<bodyText confidence="0.99921788">
There are several problems with the previous con-
fusion network decoding approaches. First, the
decoding can generate ungrammatical hypotheses
due to alignment errors and phrases broken by the
where is the language model weight,
is the LM log-probability and is the
number of words in the hypothesis . The word
posteriors are estimated by scaling the con-
fidences to sum to one for each system over all
words in between nodes and . The system
weights are also constrained to sum to one. Equation
6 may be viewed as a log-linear sum of sentence-
level features. The first feature is the sum of word
log-posteriors, the second is the LM log-probability,
the third is the log-NULL score and the last is the
log-length score. The last two terms are not com-
pletely independent but seem to help based on ex-
perimental results.
The number of paths through a confusion net-
work grows exponentially with the number of nodes.
Therefore expanding a network with an -gram lan-
guage model may result in huge lattices if is high.
Instead of high order -grams with heavy pruning, a
bi-gram may first be used to expand the lattice. Af-
ter optimizing one set of weights for the expanded
</bodyText>
<page confidence="0.996777">
315
</page>
<bodyText confidence="0.99972825">
confusion network, a second set of weights for -
best list re-scoring with a higher order -gram model
may be optimized. On a test set, the first set of
weights is used to generate an -best list from the
bi-gram expanded lattice. This -best list is then
re-scored with the higher order -gram. The second
set of weights is used to find the final-best from
the re-scored -best list.
</bodyText>
<sectionHeader confidence="0.995404" genericHeader="method">
5 Multiple Confusion Network Decoding
</sectionHeader>
<bodyText confidence="0.999901">
As discussed in Section 3, there is a disconnect be-
tween the skeleton selection and confidence estima-
tion. To prevent the-best from a system with a low
or zero weight being selected as the skeleton, confu-
sion networks are generated for each system and the
average TER score in Equation 4 is used to estimate
a prior probability for the corresponding network.
All confusion networks are connected to a single
start node with NULL arcs which contain the prior
probability from the system used as the skeleton for
that network. All confusion network are connected
to a common end node with NULL arcs. The final
arcs have a probability of one. The prior probabil-
ities in the arcs leaving the first node will be mul-
tiplied by the corresponding system weights which
guarantees that a path through a network generated
around a-best from a system with a zero weight
will not be chosen.
The prior probabilities are estimated by viewing
the negative average TER scores between the skele-
ton and other hypotheses as log-probabilities. These
log-probabilities are scaled so that the priors sum to
one. There is a concern that the prior probabilities
estimated this way may be inaccurate. Therefore,
the priors may have to be smoothed by a tunable
exponent. However, the optimization experiments
showed that the best performance was obtained by
having a smoothing factor of 1 which is equivalent
to the original priors. Thus, no smoothing was used
in the experiments presented later in this paper.
An example joint network with the priors is
shown in Figure 2. This example has three confu-
sion networks with priors ,and . The to-
tal number of nodes in the network is represented
by . Similar combination of multiple confusion
networks was presented in (Matusov et al., 2006).
However, this approach did not include sentence
</bodyText>
<figureCaption confidence="0.918034">
Figure 2: Three confusion networks with prior prob-
abilities.
</figureCaption>
<bodyText confidence="0.956669666666667">
specific prior estimates, word posterior estimates,
and did not allow joint optimization of the system
and feature weights.
</bodyText>
<sectionHeader confidence="0.996004" genericHeader="method">
6 Weights Optimization
</sectionHeader>
<bodyText confidence="0.992334178571428">
The optimization of the system and feature weights
may be carried out using -best lists as in (Osten-
dorf et al., 1991). A confusion network may be rep-
resented by a word lattice and standard tools may be
used to generate -best hypothesis lists including
word confidence scores, language model scores and
other features. The -best list may be re-ordered
using the sentence-level posteriors from
Equation 6 for the th source sentence and the
corresponding th hypothesis . The current
-best hypothesis given a set of weights
may be represented as fol-
lows
The objective is to optimize the-best score on
a development set given a set of reference transla-
tions. For example, estimating weights which mini-
mize TER between a set of-best hypothesis and
reference translations can be written as
This objective function is very complicated, so
gradient-based optimization methods may not be
used. In this work, modified Powell’s method as
proposed by (Brent, 1973) is used. The algorithm
explores better weights iteratively starting from a
set of initial weights. First, each dimension is op-
timized using a grid-based line minimization algo-
rithm. Then, a new direction based on the changes
in the objective function is estimated to speed up
the search. To improve the chances of finding a
</bodyText>
<equation confidence="0.9676908">
ε (0.5)
ε (1)
ε (1)
ε (0.3)
1ε (1) N
ε
0
2
.
)
</equation>
<page confidence="0.995226">
316
</page>
<bodyText confidence="0.999951733333333">
global optimum, 19 random perturbations of the ini-
tial weights are used in parallel optimization runs.
Since the -best list represents only a small portion
of all hypotheses in the confusion network, the op-
timized weights from one iteration may be used to
generate a new -best list from the lattice for the
next iteration. Similarly, weights which maximize
BLEU or METEOR may be optimized.
The same Powell’s method has been used to es-
timate feature weights of a standard feature-based
phrasal MT decoder in (Och, 2003). A more effi-
cient algorithm for log-linear models was also pro-
posed. In this work, both the system and feature
weights are jointly optimized, so the efficient algo-
rithm for the log-linear models cannot be used.
</bodyText>
<sectionHeader confidence="0.999835" genericHeader="evaluation">
7 Results
</sectionHeader>
<bodyText confidence="0.998914966666667">
The improved system combination method was
compared to a simple confusion network decoding
without system weights and the method proposed
in (Rosti et al., 2007) on the Arabic to English and
Chinese to English NIST MT05 tasks. Six MT sys-
tems were combined: three (A,C,E) were phrase-
based similar to (Koehn, 2004), two (B,D) were
hierarchical similar to (Chiang, 2005) and one (F)
was syntax-based similar to (Galley et al., 2006).
All systems were trained on the same data and the
outputs used the same tokenization. The decoder
weights for systems A and B were tuned to optimize
TER, and others were tuned to optimize BLEU. All
decoder weight tuning was done on the NIST MT02
task.
The joint confusion network was expanded with
a bi-gram language model and a -best list was
generated from the lattice for each tuning iteration.
The system and feature weights were tuned on the
union of NIST MT03 and MT04 tasks. All four ref-
erence translations available for the tuning and test
sets were used. A first set of weights with the bi-
gram LM was optimized with three iterations. A
second set of weights was tuned for 5-gram -best
list re-scoring. The bi-gram and 5-gram English lan-
guage models were trained on about 7 billion words.
The final combination outputs were detokenized and
cased before scoring.
The tuning set results on the Arabic to English
NIST MT03+MT04 task are shown in Table 1. The
</bodyText>
<table confidence="0.999463083333333">
Arabic tuning TER BLEU MTR
system A 44.93 45.71 66.09
system B 46.41 43.07 64.79
system C 46.10 46.41 65.33
system D 44.36 46.83 66.91
system E 45.35 45.44 65.69
system F 47.10 44.52 65.28
no weights 42.35 48.91 67.76
baseline 42.19 49.86 68.34
TER tuned 41.88 51.45 68.62
BLEU tuned 42.12 51.72 68.59
MTR tuned 54.08 38.93 71.42
</table>
<tableCaption confidence="0.974517">
Table 1: Mixed-case TER and BLEU, and
lower-case METEOR scores on Arabic NIST
MT03+MT04.
</tableCaption>
<table confidence="0.999888833333333">
Arabic test TER BLEU MTR
system A 42.98 49.58 69.86
system B 43.79 47.06 68.62
system C 43.92 47.87 66.97
system D 40.75 52.09 71.23
system E 42.19 50.86 70.02
system F 44.30 50.15 69.75
no weights 39.33 53.66 71.61
baseline 39.29 54.51 72.20
TER tuned 39.10 55.30 72.53
BLEU tuned 39.13 55.48 72.81
MTR tuned 51.56 41.73 74.79
</table>
<tableCaption confidence="0.798258">
Table 2: Mixed-case TER and BLEU, and lower-
case METEOR scores on Arabic NIST MT05.
</tableCaption>
<bodyText confidence="0.9989266">
best score on each metric is shown in bold face fonts.
The row labeled as no weights corresponds to
Equation 5 with uniform system weights and
zero NULL weight. The baseline corresponds
to Equation 5 with TER tuned weights. The follow-
ing three rows correspond to the improved confusion
network decoding with different optimization met-
rics. As expected, the scores on the metric used in
tuning are the best on that metric. Also, the combi-
nation results are better than any single system on all
metrics in the case of TER and BLEU tuning. How-
ever, the METEOR tuning yields extremely high
TER and low BLEU scores. This must be due to the
higher weight on the recall compared to precision in
the harmonic mean used to compute the METEOR
</bodyText>
<page confidence="0.994851">
317
</page>
<table confidence="0.999411583333333">
Chinese tuning TER BLEU MTR
system A 56.56 29.39 54.54
system B 55.88 30.45 54.36
system C 58.35 32.88 56.72
system D 57.09 36.18 57.11
system E 57.69 33.85 58.28
system F 56.11 36.64 58.90
no weights 53.11 37.77 59.19
baseline 53.40 38.52 59.56
TER tuned 52.13 36.87 57.30
BLEU tuned 53.03 39.99 58.97
MTR tuned 70.27 28.60 63.10
</table>
<tableCaption confidence="0.669865666666667">
Table 3: Mixed-case TER and BLEU, and
lower-case METEOR scores on Chinese NIST
MT03+MT04.
</tableCaption>
<bodyText confidence="0.999139933333334">
score. Even though METEOR has been shown to be
a good metric on a given MT output, tuning to op-
timize METEOR results in a high insertion rate and
low precision. The Arabic test set results are shown
in Table 2. The TER and BLEU optimized com-
bination results beat all single system scores on all
metrics. The best results on a given metric are again
obtained by the combination optimized for the corre-
sponding metric. It should be noted that the TER op-
timized combination has significantly higher BLEU
score than the TER optimized baseline. Compared
to the baseline system which is also optimized for
TER, the BLEU score is improved by 0.97 points.
Also, the METEOR score using the METEOR op-
timized weights is very high. However, the other
scores are worse in common with the tuning set re-
sults.
The tuning set results on the Chinese to English
NIST MT03+MT04 task are shown in Table 3. The
baseline combination weights were tuned to opti-
mize BLEU. Again, the best scores on each met-
ric are obtained by the combination tuned for that
metric. Only the METEOR score of the TER tuned
combination is worse than the METEOR scores of
systems E and F - other combinations are better than
any single system on all metrics apart from the ME-
TEOR tuned combinations. The test set results fol-
low clearly the tuning results again - the TER tuned
combination is the best in terms of TER, the BLEU
tuned in terms of BLEU, and the METEOR tuned in
</bodyText>
<table confidence="0.99872775">
Chinese test TER BLEU MTR
system A 56.57 29.63 56.63
system B 56.30 29.62 55.61
system C 59.48 31.32 57.71
system D 58.32 33.77 57.92
system E 58.46 32.40 59.75
system F 56.79 35.30 60.82
no weights 53.80 36.17 60.75
baseline 54.34 36.44 61.05
TER tuned 52.90 35.76 58.60
BLEU tuned 54.05 37.91 60.31
MTR tuned 72.59 26.96 64.35
</table>
<tableCaption confidence="0.8188455">
Table 4: Mixed-case TER and BLEU, and lower-
case METEOR scores on Chinese NIST MT05.
</tableCaption>
<bodyText confidence="0.98911825">
terms of METEOR. Compared to the baseline, the
BLEU score of the BLEU tuned combination is im-
proved by 1.47 points. Again, the METEOR tuned
weights hurt the other metrics significantly.
</bodyText>
<sectionHeader confidence="0.998935" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.99997396">
An improved confusion network decoding method
combining the word posteriors with arbitrary fea-
tures was presented. This allows the addition of
language model scores by expanding the lattices or
re-scoring -best lists. The LM integration should
result in more grammatical combination outputs.
Also, confusion networks generated by using the
-best hypothesis from all systems as the skeleton
were used with prior probabilities derived from the
average TER scores. This guarantees that the best
path will not be found from a network generated for
a system with zero weight. Compared to the earlier
system combination approaches, this method is fully
automatic and requires very little additional infor-
mation on top of the development set outputs from
the individual systems to tune the weights.
The new method was evaluated on the Arabic to
English and Chinese to English NIST MT05 tasks.
Compared to the baseline from (Rosti et al., 2007),
the new method improves the BLEU scores signif-
icantly. The combination weights were tuned to
optimize three automatic evaluation metrics: TER,
BLEU and METEOR. The TER tuning seems to
yield very good results on Arabic - the BLEU tun-
ing seems to be better on Chinese. It also seems like
</bodyText>
<page confidence="0.996799">
318
</page>
<bodyText confidence="0.999825733333333">
METEOR should not be used in tuning due to high
insertion rate and low precision. It would be interest-
ing to know which tuning metric results in the best
translations in terms of human judgment. However,
this would require time consuming evaluations such
as human mediated TER post-editing (Snover et al.,
2006).
The improved confusion network decoding ap-
proach allows arbitrary features to be used in the
combination. New features may be added in the fu-
ture. Hypothesis alignment is also very important
in confusion network generation. Better alignment
methods which take synonymy into account should
be investigated. This method could also benefit from
more sophisticated word posterior estimation.
</bodyText>
<sectionHeader confidence="0.998929" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999682">
This work was supported by DARPA/IPTO Contract
No. HR0011-06-C-0022 under the GALE program
(approved for public release, distribution unlimited).
The authors would like to thank ISI and University
of Edinburgh for sharing their MT system outputs.
</bodyText>
<sectionHeader confidence="0.999087" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999718698630137">
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with improved
correlation with human judgments. In Proc. ACL
Workshop on Intrinsic and Extrinsic Evaluation Mea-
sures for Machine Translation and/or Summarization,
pages 65–72.
Srinivas Bangalore, German Bordel, and Giuseppe Ric-
cardi. 2001. Computing consensus translation from
multiple machine translation systems. In Proc. ASRU,
pages 351–354.
Richard P. Brent. 1973. Algorithms for Minimization
Without Derivatives. Prentice-Hall.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. ACL, pages
263–270.
Jonathan G. Fiscus. 1997. A post-processing system to
yield reduced word error rates: Recognizer output vot-
ing error reduction (ROVER). In Proc. ASRU, pages
347–354.
Robert Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In Proc. ANLP, pages 95–
100.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inferences and training of
context-rich syntax translation models. In Proc. COL-
ING/ACL, pages 961–968.
Shyamsundar Jayaraman and Alon Lavie. 2005. Multi-
engine machine translation guided by explicit word
matching. In Proc. EAMT, pages 143–152.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proc. AMTA, pages 115–124.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: Word error
minimization and other applications of confusion net-
works. Computer Speech and Language, 14(4):373–
400.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multiple
machine translation systems using enhanced hypothe-
ses alignment. In Proc. EACL, pages 33–40.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19–51.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. ACL, pages 160–
167.
Mari Ostendorf, Ashvin Kannan, Steve Austin, Owen
Kimball, Richard Schwartz, and Jan Robin Rohlicek.
1991. Integration of diverse recognition methodolo-
gies through reevaluation of N-best sentence hypothe-
ses. In Proc. HLT, pages 83–87.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. ACL, pages
311–318.
Antti-Veikko I. Rosti, Bing Xiang, Spyros Matsoukas,
Richard Schwartz, Necip Fazil Ayan, and Bonnie J.
Dorr. 2007. Combining outputs from multiple ma-
chine translation systems. In Proc. NAACL-HLT 2007,
pages 228–235.
Robert E. Schapire. 1990. The strength of weak learn-
ability. Machine Learning, 5(2):197–227.
Khe Chai Sim, William J. Byrne, Mark J.F. Gales,
Hichem Sahbi, and Phil C. Woodland. 2007. Consen-
sus network decoding for statistical machine transla-
tion system combination. In Proc. ICASSP, volume 4,
pages 105–108.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciula, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proc. AMTA, pages 223–231.
</reference>
<page confidence="0.999353">
319
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.629419">
<title confidence="0.99968">Improved Word-Level System Combination for Machine Translation</title>
<author confidence="0.992325">I Rosti Matsoukas Schwartz</author>
<affiliation confidence="0.67222">BBN Technologies, 10 Moulton Street</affiliation>
<address confidence="0.999992">Cambridge, MA 02138</address>
<email confidence="0.99496">arosti,smatsouk,schwartz@bbn.com</email>
<abstract confidence="0.997325913043478">Recently, confusion network decoding has been applied in machine translation system combination. Due to errors in the hypothesis alignment, decoding may result in ungrammatical combination outputs. This paper describes an improved confusion network based method to combine outputs from multiple MT systems. In this approach, arbitrary features may be added log-linearly into the objective function, thus allowing language model expansion and re-scoring. Also, a novel method to automatically select the hypothesis which other hypotheses are aligned against is proposed. A generic weight tuning algorithm may be used to optimize various automatic evaluation metrics including TER, BLEU and METEOR. The experiments using the 2005 Arabic to English and Chinese to English NIST MT evaluation tasks show significant improvements in BLEU scores compared to earlier confusion network decoding based methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Proc. ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>65--72</pages>
<contexts>
<context position="7132" citStr="Banerjee and Lavie, 2005" startWordPosition="1112" endWordPosition="1115"> and are the -gram precisions. When multiple references are provided, the -gram counts against all references are accumulated to compute the precisions. Similarly, full test set scores are obtained by accumulating counts over all hypothesis and reference pairs. The BLEU scores are between and, higher being better. Often BLEU scores are reported as percentages and “one BLEU point gain” usually means a BLEU increase of . Other evaluation metrics have been proposed to replace BLEU. It has been argued that METEOR correlates better with human judgment due to higher weight on recall than precision (Banerjee and Lavie, 2005). METEOR is based on the weighted harmonic mean of the precision and recall measured on unigram matches as follows where is the total number of unigram matches, is the hypothesis length, is the reference length and is the minimum number of -gram matches that covers the alignment. The second term is a fragmentation penalty which penalizes the harmonic mean by a factor of up to when ; i.e., 313 there are no matching -grams higher than . By default, METEOR script counts the words that match exactly, and words that match after a simple Porter stemmer. Additional matching modules including WordNet </context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proc. ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>German Bordel</author>
<author>Giuseppe Riccardi</author>
</authors>
<title>Computing consensus translation from multiple machine translation systems.</title>
<date>2001</date>
<booktitle>In Proc. ASRU,</booktitle>
<pages>351--354</pages>
<contexts>
<context position="2353" citStr="Bangalore et al., 2001" startWordPosition="341" endWordPosition="344">oach (Fiscus, 1997). In speech recognition, confusion network decoding (Mangu et al., 2000) has become widely used in system combination. Unlike speech recognition, current statistical machine translation (MT) systems are based on various different paradigms; for example phrasal, hierarchical and syntax-based systems. The idea of combining outputs from different MT systems to produce consensus translations in the hope of generating better translations has been around for a while (Frederking and Nirenburg, 1994). Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001). To generate confusion networks, hypotheses have to be aligned against each other. In (Bangalore et al., 2001), Levenshtein alignment was used to generate the network. As opposed to speech recognition, the word order between two correct MT outputs may be different and the Levenshtein alignment may not be able to align shifted words in the hypotheses. In (Matusov et al., 2006), different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus using GIZA++ (Och and Ney, 2003). The size of the test set may influence the quality </context>
</contexts>
<marker>Bangalore, Bordel, Riccardi, 2001</marker>
<rawString>Srinivas Bangalore, German Bordel, and Giuseppe Riccardi. 2001. Computing consensus translation from multiple machine translation systems. In Proc. ASRU, pages 351–354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard P Brent</author>
</authors>
<title>Algorithms for Minimization Without Derivatives.</title>
<date>1973</date>
<publisher>Prentice-Hall.</publisher>
<contexts>
<context position="5469" citStr="Brent, 1973" startWordPosition="844" endWordPosition="845">where the prior probabilities are also multiplied by the system weights. The combination outputs from confusion network decoding may be ungrammatical due to alignment errors. Also the word-level decoding may break coherent phrases produced by the individual systems. In this work, log-posterior probabilities are estimated for each confusion network arc instead of using votes or simple word confidences. This allows a log-linear addition of arbitrary features such as language model (LM) scores. The LM scores should increase the total log-posterior of more grammatical hypotheses. Powell’s method (Brent, 1973) is used to tune the system and feature weights simultaneously so as to optimize various automatic evaluation metrics on a development set. Tuning is fully automatic, as opposed to (Matusov et al., 2006) where global system weights were set manually. This paper is organized as follows. Three evaluation metrics used in weights tuning and reporting the test set results are reviewed in Section 2. Section 3 describes confusion network decoding for MT system combination. The extensions to add features log-linearly and improve the skeleton selection are presented in Sections 4 and 5, respectively. S</context>
<context position="18576" citStr="Brent, 1973" startWordPosition="3075" endWordPosition="3076"> re-ordered using the sentence-level posteriors from Equation 6 for the th source sentence and the corresponding th hypothesis . The current -best hypothesis given a set of weights may be represented as follows The objective is to optimize the-best score on a development set given a set of reference translations. For example, estimating weights which minimize TER between a set of-best hypothesis and reference translations can be written as This objective function is very complicated, so gradient-based optimization methods may not be used. In this work, modified Powell’s method as proposed by (Brent, 1973) is used. The algorithm explores better weights iteratively starting from a set of initial weights. First, each dimension is optimized using a grid-based line minimization algorithm. Then, a new direction based on the changes in the objective function is estimated to speed up the search. To improve the chances of finding a ε (0.5) ε (1) ε (1) ε (0.3) 1ε (1) N ε 0 2 . ) 316 global optimum, 19 random perturbations of the initial weights are used in parallel optimization runs. Since the -best list represents only a small portion of all hypotheses in the confusion network, the optimized weights fr</context>
</contexts>
<marker>Brent, 1973</marker>
<rawString>Richard P. Brent. 1973. Algorithms for Minimization Without Derivatives. Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="20059" citStr="Chiang, 2005" startWordPosition="3331" endWordPosition="3332">T decoder in (Och, 2003). A more efficient algorithm for log-linear models was also proposed. In this work, both the system and feature weights are jointly optimized, so the efficient algorithm for the log-linear models cannot be used. 7 Results The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks. Six MT systems were combined: three (A,C,E) were phrasebased similar to (Koehn, 2004), two (B,D) were hierarchical similar to (Chiang, 2005) and one (F) was syntax-based similar to (Galley et al., 2006). All systems were trained on the same data and the outputs used the same tokenization. The decoder weights for systems A and B were tuned to optimize TER, and others were tuned to optimize BLEU. All decoder weight tuning was done on the NIST MT02 task. The joint confusion network was expanded with a bi-gram language model and a -best list was generated from the lattice for each tuning iteration. The system and feature weights were tuned on the union of NIST MT03 and MT04 tasks. All four reference translations available for the tuni</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proc. ACL, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan G Fiscus</author>
</authors>
<title>A post-processing system to yield reduced word error rates: Recognizer output voting error reduction (ROVER).</title>
<date>1997</date>
<booktitle>In Proc. ASRU,</booktitle>
<pages>347--354</pages>
<contexts>
<context position="1749" citStr="Fiscus, 1997" startWordPosition="253" endWordPosition="254">ction System combination has been shown to improve classification performance in various tasks. There are several approaches for combining classifiers. In ensemble learning, a collection of simple classifiers is used to yield better performance than any single classifier; for example boosting (Schapire, 1990). Another approach is to combine outputs from a few highly specialized classifiers. The classifiers may be based on the same basic modeling techniques but differ by, for example, alternative feature representations. Combination of speech recognition outputs is an example of this approach (Fiscus, 1997). In speech recognition, confusion network decoding (Mangu et al., 2000) has become widely used in system combination. Unlike speech recognition, current statistical machine translation (MT) systems are based on various different paradigms; for example phrasal, hierarchical and syntax-based systems. The idea of combining outputs from different MT systems to produce consensus translations in the hope of generating better translations has been around for a while (Frederking and Nirenburg, 1994). Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2</context>
</contexts>
<marker>Fiscus, 1997</marker>
<rawString>Jonathan G. Fiscus. 1997. A post-processing system to yield reduced word error rates: Recognizer output voting error reduction (ROVER). In Proc. ASRU, pages 347–354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Frederking</author>
<author>Sergei Nirenburg</author>
</authors>
<title>Three heads are better than one.</title>
<date>1994</date>
<booktitle>In Proc. ANLP,</booktitle>
<pages>95--100</pages>
<contexts>
<context position="2246" citStr="Frederking and Nirenburg, 1994" startWordPosition="325" endWordPosition="329"> example, alternative feature representations. Combination of speech recognition outputs is an example of this approach (Fiscus, 1997). In speech recognition, confusion network decoding (Mangu et al., 2000) has become widely used in system combination. Unlike speech recognition, current statistical machine translation (MT) systems are based on various different paradigms; for example phrasal, hierarchical and syntax-based systems. The idea of combining outputs from different MT systems to produce consensus translations in the hope of generating better translations has been around for a while (Frederking and Nirenburg, 1994). Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001). To generate confusion networks, hypotheses have to be aligned against each other. In (Bangalore et al., 2001), Levenshtein alignment was used to generate the network. As opposed to speech recognition, the word order between two correct MT outputs may be different and the Levenshtein alignment may not be able to align shifted words in the hypotheses. In (Matusov et al., 2006), different word orderings are taken into account by training alignment models by considering all hypothesis pairs</context>
</contexts>
<marker>Frederking, Nirenburg, 1994</marker>
<rawString>Robert Frederking and Sergei Nirenburg. 1994. Three heads are better than one. In Proc. ANLP, pages 95– 100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inferences and training of context-rich syntax translation models.</title>
<date>2006</date>
<booktitle>In Proc. COLING/ACL,</booktitle>
<pages>961--968</pages>
<contexts>
<context position="20121" citStr="Galley et al., 2006" startWordPosition="3340" endWordPosition="3343">r log-linear models was also proposed. In this work, both the system and feature weights are jointly optimized, so the efficient algorithm for the log-linear models cannot be used. 7 Results The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks. Six MT systems were combined: three (A,C,E) were phrasebased similar to (Koehn, 2004), two (B,D) were hierarchical similar to (Chiang, 2005) and one (F) was syntax-based similar to (Galley et al., 2006). All systems were trained on the same data and the outputs used the same tokenization. The decoder weights for systems A and B were tuned to optimize TER, and others were tuned to optimize BLEU. All decoder weight tuning was done on the NIST MT02 task. The joint confusion network was expanded with a bi-gram language model and a -best list was generated from the lattice for each tuning iteration. The system and feature weights were tuned on the union of NIST MT03 and MT04 tasks. All four reference translations available for the tuning and test sets were used. A first set of weights with the bi</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inferences and training of context-rich syntax translation models. In Proc. COLING/ACL, pages 961–968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shyamsundar Jayaraman</author>
<author>Alon Lavie</author>
</authors>
<title>Multiengine machine translation guided by explicit word matching.</title>
<date>2005</date>
<booktitle>In Proc. EAMT,</booktitle>
<pages>143--152</pages>
<contexts>
<context position="3650" citStr="Jayaraman and Lavie, 2005" startWordPosition="553" endWordPosition="556">e to be added to improve the GIZA++ alignments. A modified Levenshtein alignment allowing shifts as in computation of the translation edit rate (TER) (Snover et al., 2006) was used to align hy312 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 312–319, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics potheses in (Sim et al., 2007). The alignments from TER are consistent as they do not depend on the test set size. Also, a more heuristic alignment method has been proposed in a different system combination approach (Jayaraman and Lavie, 2005). A full comparison of different alignment methods would be difficult as many approaches require a significant amount of engineering. Confusion networks are generated by choosing one hypothesis as the “skeleton”, and other hypotheses are aligned against it. The skeleton defines the word order of the combination output. Minimum Bayes risk (MBR) was used to choose the skeleton in (Sim et al., 2007). The average TER score was computed between each system’s-best hypothesis and all other hypotheses. The MBR hypothesis is the one with the minimum average TER and thus, may be viewed as the closest to</context>
</contexts>
<marker>Jayaraman, Lavie, 2005</marker>
<rawString>Shyamsundar Jayaraman and Alon Lavie. 2005. Multiengine machine translation guided by explicit word matching. In Proc. EAMT, pages 143–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In Proc. AMTA,</booktitle>
<pages>115--124</pages>
<contexts>
<context position="20004" citStr="Koehn, 2004" startWordPosition="3323" endWordPosition="3324"> feature weights of a standard feature-based phrasal MT decoder in (Och, 2003). A more efficient algorithm for log-linear models was also proposed. In this work, both the system and feature weights are jointly optimized, so the efficient algorithm for the log-linear models cannot be used. 7 Results The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks. Six MT systems were combined: three (A,C,E) were phrasebased similar to (Koehn, 2004), two (B,D) were hierarchical similar to (Chiang, 2005) and one (F) was syntax-based similar to (Galley et al., 2006). All systems were trained on the same data and the outputs used the same tokenization. The decoder weights for systems A and B were tuned to optimize TER, and others were tuned to optimize BLEU. All decoder weight tuning was done on the NIST MT02 task. The joint confusion network was expanded with a bi-gram language model and a -best list was generated from the lattice for each tuning iteration. The system and feature weights were tuned on the union of NIST MT03 and MT04 tasks.</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. In Proc. AMTA, pages 115–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lidia Mangu</author>
<author>Eric Brill</author>
<author>Andreas Stolcke</author>
</authors>
<title>Finding consensus in speech recognition: Word error minimization and other applications of confusion networks.</title>
<date>2000</date>
<journal>Computer Speech and Language,</journal>
<volume>14</volume>
<issue>4</issue>
<pages>400</pages>
<contexts>
<context position="1821" citStr="Mangu et al., 2000" startWordPosition="261" endWordPosition="264">performance in various tasks. There are several approaches for combining classifiers. In ensemble learning, a collection of simple classifiers is used to yield better performance than any single classifier; for example boosting (Schapire, 1990). Another approach is to combine outputs from a few highly specialized classifiers. The classifiers may be based on the same basic modeling techniques but differ by, for example, alternative feature representations. Combination of speech recognition outputs is an example of this approach (Fiscus, 1997). In speech recognition, confusion network decoding (Mangu et al., 2000) has become widely used in system combination. Unlike speech recognition, current statistical machine translation (MT) systems are based on various different paradigms; for example phrasal, hierarchical and syntax-based systems. The idea of combining outputs from different MT systems to produce consensus translations in the hope of generating better translations has been around for a while (Frederking and Nirenburg, 1994). Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001). To generate confusion networks, hypotheses have to be aligned agai</context>
</contexts>
<marker>Mangu, Brill, Stolcke, 2000</marker>
<rawString>Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000. Finding consensus in speech recognition: Word error minimization and other applications of confusion networks. Computer Speech and Language, 14(4):373– 400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeny Matusov</author>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment.</title>
<date>2006</date>
<booktitle>In Proc. EACL,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="2732" citStr="Matusov et al., 2006" startWordPosition="405" endWordPosition="408">oduce consensus translations in the hope of generating better translations has been around for a while (Frederking and Nirenburg, 1994). Recently, confusion network decoding for MT system combination has been proposed (Bangalore et al., 2001). To generate confusion networks, hypotheses have to be aligned against each other. In (Bangalore et al., 2001), Levenshtein alignment was used to generate the network. As opposed to speech recognition, the word order between two correct MT outputs may be different and the Levenshtein alignment may not be able to align shifted words in the hypotheses. In (Matusov et al., 2006), different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus using GIZA++ (Och and Ney, 2003). The size of the test set may influence the quality of these alignments. Thus, system outputs from development sets may have to be added to improve the GIZA++ alignments. A modified Levenshtein alignment allowing shifts as in computation of the translation edit rate (TER) (Snover et al., 2006) was used to align hy312 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 312–319, Prague, C</context>
<context position="5672" citStr="Matusov et al., 2006" startWordPosition="877" endWordPosition="880"> decoding may break coherent phrases produced by the individual systems. In this work, log-posterior probabilities are estimated for each confusion network arc instead of using votes or simple word confidences. This allows a log-linear addition of arbitrary features such as language model (LM) scores. The LM scores should increase the total log-posterior of more grammatical hypotheses. Powell’s method (Brent, 1973) is used to tune the system and feature weights simultaneously so as to optimize various automatic evaluation metrics on a development set. Tuning is fully automatic, as opposed to (Matusov et al., 2006) where global system weights were set manually. This paper is organized as follows. Three evaluation metrics used in weights tuning and reporting the test set results are reviewed in Section 2. Section 3 describes confusion network decoding for MT system combination. The extensions to add features log-linearly and improve the skeleton selection are presented in Sections 4 and 5, respectively. Section 6 details the weights optimization algorithm and the experimental results are reported in Section 7. Conclusions and future work are discussed in Section 8. 2 Evaluation Metrics Currently, the mos</context>
<context position="17366" citStr="Matusov et al., 2006" startWordPosition="2880" endWordPosition="2883">stimated this way may be inaccurate. Therefore, the priors may have to be smoothed by a tunable exponent. However, the optimization experiments showed that the best performance was obtained by having a smoothing factor of 1 which is equivalent to the original priors. Thus, no smoothing was used in the experiments presented later in this paper. An example joint network with the priors is shown in Figure 2. This example has three confusion networks with priors ,and . The total number of nodes in the network is represented by . Similar combination of multiple confusion networks was presented in (Matusov et al., 2006). However, this approach did not include sentence Figure 2: Three confusion networks with prior probabilities. specific prior estimates, word posterior estimates, and did not allow joint optimization of the system and feature weights. 6 Weights Optimization The optimization of the system and feature weights may be carried out using -best lists as in (Ostendorf et al., 1991). A confusion network may be represented by a word lattice and standard tools may be used to generate -best hypothesis lists including word confidence scores, language model scores and other features. The -best list may be r</context>
</contexts>
<marker>Matusov, Ueffing, Ney, 2006</marker>
<rawString>Evgeny Matusov, Nicola Ueffing, and Hermann Ney. 2006. Computing consensus translation from multiple machine translation systems using enhanced hypotheses alignment. In Proc. EACL, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="2900" citStr="Och and Ney, 2003" startWordPosition="431" endWordPosition="434">ng for MT system combination has been proposed (Bangalore et al., 2001). To generate confusion networks, hypotheses have to be aligned against each other. In (Bangalore et al., 2001), Levenshtein alignment was used to generate the network. As opposed to speech recognition, the word order between two correct MT outputs may be different and the Levenshtein alignment may not be able to align shifted words in the hypotheses. In (Matusov et al., 2006), different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus using GIZA++ (Och and Ney, 2003). The size of the test set may influence the quality of these alignments. Thus, system outputs from development sets may have to be added to improve the GIZA++ alignments. A modified Levenshtein alignment allowing shifts as in computation of the translation edit rate (TER) (Snover et al., 2006) was used to align hy312 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 312–319, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics potheses in (Sim et al., 2007). The alignments from TER are consistent as they do not depend </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz J. Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="19470" citStr="Och, 2003" startWordPosition="3233" endWordPosition="3234">rch. To improve the chances of finding a ε (0.5) ε (1) ε (1) ε (0.3) 1ε (1) N ε 0 2 . ) 316 global optimum, 19 random perturbations of the initial weights are used in parallel optimization runs. Since the -best list represents only a small portion of all hypotheses in the confusion network, the optimized weights from one iteration may be used to generate a new -best list from the lattice for the next iteration. Similarly, weights which maximize BLEU or METEOR may be optimized. The same Powell’s method has been used to estimate feature weights of a standard feature-based phrasal MT decoder in (Och, 2003). A more efficient algorithm for log-linear models was also proposed. In this work, both the system and feature weights are jointly optimized, so the efficient algorithm for the log-linear models cannot be used. 7 Results The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks. Six MT systems were combined: three (A,C,E) were phrasebased similar to (Koehn, 2004), two (B,D) were hierarchical similar to (Chiang, 2005) and one (F</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz J. Och. 2003. Minimum error rate training in statistical machine translation. In Proc. ACL, pages 160– 167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mari Ostendorf</author>
<author>Ashvin Kannan</author>
<author>Steve Austin</author>
<author>Owen Kimball</author>
<author>Richard Schwartz</author>
<author>Jan Robin Rohlicek</author>
</authors>
<title>Integration of diverse recognition methodologies through reevaluation of N-best sentence hypotheses.</title>
<date>1991</date>
<booktitle>In Proc. HLT,</booktitle>
<pages>83--87</pages>
<contexts>
<context position="17742" citStr="Ostendorf et al., 1991" startWordPosition="2938" endWordPosition="2942">h the priors is shown in Figure 2. This example has three confusion networks with priors ,and . The total number of nodes in the network is represented by . Similar combination of multiple confusion networks was presented in (Matusov et al., 2006). However, this approach did not include sentence Figure 2: Three confusion networks with prior probabilities. specific prior estimates, word posterior estimates, and did not allow joint optimization of the system and feature weights. 6 Weights Optimization The optimization of the system and feature weights may be carried out using -best lists as in (Ostendorf et al., 1991). A confusion network may be represented by a word lattice and standard tools may be used to generate -best hypothesis lists including word confidence scores, language model scores and other features. The -best list may be re-ordered using the sentence-level posteriors from Equation 6 for the th source sentence and the corresponding th hypothesis . The current -best hypothesis given a set of weights may be represented as follows The objective is to optimize the-best score on a development set given a set of reference translations. For example, estimating weights which minimize TER between a se</context>
</contexts>
<marker>Ostendorf, Kannan, Austin, Kimball, Schwartz, Rohlicek, 1991</marker>
<rawString>Mari Ostendorf, Ashvin Kannan, Steve Austin, Owen Kimball, Richard Schwartz, and Jan Robin Rohlicek. 1991. Integration of diverse recognition methodologies through reevaluation of N-best sentence hypotheses. In Proc. HLT, pages 83–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="6359" citStr="Papineni et al., 2002" startWordPosition="986" endWordPosition="989">anized as follows. Three evaluation metrics used in weights tuning and reporting the test set results are reviewed in Section 2. Section 3 describes confusion network decoding for MT system combination. The extensions to add features log-linearly and improve the skeleton selection are presented in Sections 4 and 5, respectively. Section 6 details the weights optimization algorithm and the experimental results are reported in Section 7. Conclusions and future work are discussed in Section 8. 2 Evaluation Metrics Currently, the most widely used automatic MT evaluation metric is the NIST BLEU-4 (Papineni et al., 2002). It is computed as the geometric mean of - gram precisions up to-grams between the hypothesis and reference as follows where is the brevity penalty and are the -gram precisions. When multiple references are provided, the -gram counts against all references are accumulated to compute the precisions. Similarly, full test set scores are obtained by accumulating counts over all hypothesis and reference pairs. The BLEU scores are between and, higher being better. Often BLEU scores are reported as percentages and “one BLEU point gain” usually means a BLEU increase of . Other evaluation metrics have</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko I Rosti</author>
<author>Bing Xiang</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
<author>Necip Fazil Ayan</author>
<author>Bonnie J Dorr</author>
</authors>
<title>Combining outputs from multiple machine translation systems.</title>
<date>2007</date>
<booktitle>In Proc. NAACL-HLT</booktitle>
<pages>228--235</pages>
<contexts>
<context position="4335" citStr="Rosti et al., 2007" startWordPosition="667" endWordPosition="670">ult as many approaches require a significant amount of engineering. Confusion networks are generated by choosing one hypothesis as the “skeleton”, and other hypotheses are aligned against it. The skeleton defines the word order of the combination output. Minimum Bayes risk (MBR) was used to choose the skeleton in (Sim et al., 2007). The average TER score was computed between each system’s-best hypothesis and all other hypotheses. The MBR hypothesis is the one with the minimum average TER and thus, may be viewed as the closest to all other hypotheses in terms of TER. This work was extended in (Rosti et al., 2007) by introducing system weights for word confidences. However, the system weights did not influence the skeleton selection, so a hypothesis from a system with zero weight might have been chosen as the skeleton. In this work, confusion networks are generated by using the-best output from each system as the skeleton, and prior probabilities for each network are estimated from the average TER scores between the skeleton and other hypotheses. All resulting confusion networks are connected in parallel into a joint lattice where the prior probabilities are also multiplied by the system weights. The c</context>
<context position="11622" citStr="Rosti et al., 2007" startWordPosition="1902" endWordPosition="1905">decoding with uniform posterior probabilities (Sim et al., 2007). Other evaluation metrics may also be used as the MBR loss function. For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of cat sat the mat cat sitting on the mat hat on a mat cat (2) sat (1) on (2) the (2) hat (1) sitting (1) ε (1) a (1) ε (1) mat (3) 1 2 3 4 5 6 314 the combination output (Sim et al., 2007). When using -best lists from each system, the words may be assigned a different score based on the rank of the hypothesis. In (Rosti et al., 2007), simple score was assigned to the word coming from the thbest hypothesis. Due to the computational burden of the TER alignment, only-best hypotheses were considered as possible skeletons, and hypotheses per system were aligned. Similar approach to estimate word posteriors is adopted in this work. System weights may be used to assign a system specific confidence on each word in the network. The weights may be based on the systems’ relative performance on a separate development set or they may be automatically tuned to optimize some evaluation metric on the development set. In (Rosti et al., 20</context>
<context position="19853" citStr="Rosti et al., 2007" startWordPosition="3294" endWordPosition="3297">from the lattice for the next iteration. Similarly, weights which maximize BLEU or METEOR may be optimized. The same Powell’s method has been used to estimate feature weights of a standard feature-based phrasal MT decoder in (Och, 2003). A more efficient algorithm for log-linear models was also proposed. In this work, both the system and feature weights are jointly optimized, so the efficient algorithm for the log-linear models cannot be used. 7 Results The improved system combination method was compared to a simple confusion network decoding without system weights and the method proposed in (Rosti et al., 2007) on the Arabic to English and Chinese to English NIST MT05 tasks. Six MT systems were combined: three (A,C,E) were phrasebased similar to (Koehn, 2004), two (B,D) were hierarchical similar to (Chiang, 2005) and one (F) was syntax-based similar to (Galley et al., 2006). All systems were trained on the same data and the outputs used the same tokenization. The decoder weights for systems A and B were tuned to optimize TER, and others were tuned to optimize BLEU. All decoder weight tuning was done on the NIST MT02 task. The joint confusion network was expanded with a bi-gram language model and a -</context>
<context position="26054" citStr="Rosti et al., 2007" startWordPosition="4376" endWordPosition="4379">generated by using the -best hypothesis from all systems as the skeleton were used with prior probabilities derived from the average TER scores. This guarantees that the best path will not be found from a network generated for a system with zero weight. Compared to the earlier system combination approaches, this method is fully automatic and requires very little additional information on top of the development set outputs from the individual systems to tune the weights. The new method was evaluated on the Arabic to English and Chinese to English NIST MT05 tasks. Compared to the baseline from (Rosti et al., 2007), the new method improves the BLEU scores significantly. The combination weights were tuned to optimize three automatic evaluation metrics: TER, BLEU and METEOR. The TER tuning seems to yield very good results on Arabic - the BLEU tuning seems to be better on Chinese. It also seems like 318 METEOR should not be used in tuning due to high insertion rate and low precision. It would be interesting to know which tuning metric results in the best translations in terms of human judgment. However, this would require time consuming evaluations such as human mediated TER post-editing (Snover et al., 20</context>
</contexts>
<marker>Rosti, Xiang, Matsoukas, Schwartz, Ayan, Dorr, 2007</marker>
<rawString>Antti-Veikko I. Rosti, Bing Xiang, Spyros Matsoukas, Richard Schwartz, Necip Fazil Ayan, and Bonnie J. Dorr. 2007. Combining outputs from multiple machine translation systems. In Proc. NAACL-HLT 2007, pages 228–235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Schapire</author>
</authors>
<title>The strength of weak learnability.</title>
<date>1990</date>
<booktitle>Machine Learning,</booktitle>
<volume>5</volume>
<issue>2</issue>
<contexts>
<context position="1446" citStr="Schapire, 1990" startWordPosition="207" endWordPosition="208">ay be used to optimize various automatic evaluation metrics including TER, BLEU and METEOR. The experiments using the 2005 Arabic to English and Chinese to English NIST MT evaluation tasks show significant improvements in BLEU scores compared to earlier confusion network decoding based methods. 1 Introduction System combination has been shown to improve classification performance in various tasks. There are several approaches for combining classifiers. In ensemble learning, a collection of simple classifiers is used to yield better performance than any single classifier; for example boosting (Schapire, 1990). Another approach is to combine outputs from a few highly specialized classifiers. The classifiers may be based on the same basic modeling techniques but differ by, for example, alternative feature representations. Combination of speech recognition outputs is an example of this approach (Fiscus, 1997). In speech recognition, confusion network decoding (Mangu et al., 2000) has become widely used in system combination. Unlike speech recognition, current statistical machine translation (MT) systems are based on various different paradigms; for example phrasal, hierarchical and syntax-based syste</context>
</contexts>
<marker>Schapire, 1990</marker>
<rawString>Robert E. Schapire. 1990. The strength of weak learnability. Machine Learning, 5(2):197–227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khe Chai Sim</author>
<author>William J Byrne</author>
<author>Mark J F Gales</author>
<author>Hichem Sahbi</author>
<author>Phil C Woodland</author>
</authors>
<title>Consensus network decoding for statistical machine translation system combination.</title>
<date>2007</date>
<booktitle>In Proc. ICASSP,</booktitle>
<volume>4</volume>
<pages>105--108</pages>
<contexts>
<context position="3437" citStr="Sim et al., 2007" startWordPosition="517" endWordPosition="520">ering all hypothesis pairs as a parallel corpus using GIZA++ (Och and Ney, 2003). The size of the test set may influence the quality of these alignments. Thus, system outputs from development sets may have to be added to improve the GIZA++ alignments. A modified Levenshtein alignment allowing shifts as in computation of the translation edit rate (TER) (Snover et al., 2006) was used to align hy312 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 312–319, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics potheses in (Sim et al., 2007). The alignments from TER are consistent as they do not depend on the test set size. Also, a more heuristic alignment method has been proposed in a different system combination approach (Jayaraman and Lavie, 2005). A full comparison of different alignment methods would be difficult as many approaches require a significant amount of engineering. Confusion networks are generated by choosing one hypothesis as the “skeleton”, and other hypotheses are aligned against it. The skeleton defines the word order of the combination output. Minimum Bayes risk (MBR) was used to choose the skeleton in (Sim e</context>
<context position="11067" citStr="Sim et al., 2007" startWordPosition="1787" endWordPosition="1790">ame meaning. As the skeleton determines the word order, the quality of the combination output also depends on which hypothesis is chosen as the skeleton. Since the modified Levenshtein alignment produces TER scores between the skeleton and the other hypotheses, a natural choice for selecting the skeleton is the minimum average TER score. The hypothesis resulting in the lowest average TER score when aligned against all other hypotheses is chosen as the skeleton as follows (4) where is the number of systems. This is equivalent to minimum Bayes risk decoding with uniform posterior probabilities (Sim et al., 2007). Other evaluation metrics may also be used as the MBR loss function. For BLEU and METEOR, the loss function would be and . It has been found that multiple hypotheses from each system may be used to improve the quality of cat sat the mat cat sitting on the mat hat on a mat cat (2) sat (1) on (2) the (2) hat (1) sitting (1) ε (1) a (1) ε (1) mat (3) 1 2 3 4 5 6 314 the combination output (Sim et al., 2007). When using -best lists from each system, the words may be assigned a different score based on the rank of the hypothesis. In (Rosti et al., 2007), simple score was assigned to the word comin</context>
</contexts>
<marker>Sim, Byrne, Gales, Sahbi, Woodland, 2007</marker>
<rawString>Khe Chai Sim, William J. Byrne, Mark J.F. Gales, Hichem Sahbi, and Phil C. Woodland. 2007. Consensus network decoding for statistical machine translation system combination. In Proc. ICASSP, volume 4, pages 105–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciula</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proc. AMTA,</booktitle>
<pages>223--231</pages>
<contexts>
<context position="3195" citStr="Snover et al., 2006" startWordPosition="481" endWordPosition="484">een two correct MT outputs may be different and the Levenshtein alignment may not be able to align shifted words in the hypotheses. In (Matusov et al., 2006), different word orderings are taken into account by training alignment models by considering all hypothesis pairs as a parallel corpus using GIZA++ (Och and Ney, 2003). The size of the test set may influence the quality of these alignments. Thus, system outputs from development sets may have to be added to improve the GIZA++ alignments. A modified Levenshtein alignment allowing shifts as in computation of the translation edit rate (TER) (Snover et al., 2006) was used to align hy312 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 312–319, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics potheses in (Sim et al., 2007). The alignments from TER are consistent as they do not depend on the test set size. Also, a more heuristic alignment method has been proposed in a different system combination approach (Jayaraman and Lavie, 2005). A full comparison of different alignment methods would be difficult as many approaches require a significant amount of engineering. Confusion n</context>
<context position="8101" citStr="Snover et al., 2006" startWordPosition="1277" endWordPosition="1280">nic mean by a factor of up to when ; i.e., 313 there are no matching -grams higher than . By default, METEOR script counts the words that match exactly, and words that match after a simple Porter stemmer. Additional matching modules including WordNet stemming and synonymy may also be used. When multiple references are provided, the lowest score is reported. Full test set scores are obtained by accumulating statistics over all test sentences. The METEOR scores are also between and , higher being better. The scores in the results section are reported as percentages. Translation edit rate (TER) (Snover et al., 2006) has been proposed as more intuitive evaluation metric since it is based on the rate of edits required to transform the hypothesis into the reference. The TER score is computed as follows (3) where is the reference length. The only difference to word error rate is that the TER allows shifts. A shift of a sequence of words is counted as a single edit. The minimum translation edit alignment is usually found through a beam search. When multiple references are provided, the edits from the closest reference are divided by the average reference length. Full test set scores are obtained by accumulati</context>
<context position="26657" citStr="Snover et al., 2006" startWordPosition="4479" endWordPosition="4482">sti et al., 2007), the new method improves the BLEU scores significantly. The combination weights were tuned to optimize three automatic evaluation metrics: TER, BLEU and METEOR. The TER tuning seems to yield very good results on Arabic - the BLEU tuning seems to be better on Chinese. It also seems like 318 METEOR should not be used in tuning due to high insertion rate and low precision. It would be interesting to know which tuning metric results in the best translations in terms of human judgment. However, this would require time consuming evaluations such as human mediated TER post-editing (Snover et al., 2006). The improved confusion network decoding approach allows arbitrary features to be used in the combination. New features may be added in the future. Hypothesis alignment is also very important in confusion network generation. Better alignment methods which take synonymy into account should be investigated. This method could also benefit from more sophisticated word posterior estimation. Acknowledgments This work was supported by DARPA/IPTO Contract No. HR0011-06-C-0022 under the GALE program (approved for public release, distribution unlimited). The authors would like to thank ISI and Universi</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciula, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciula, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proc. AMTA, pages 223–231.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>