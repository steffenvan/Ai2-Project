<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003678">
<title confidence="0.9969845">
Bridging Lexical Gaps between Queries and Questions on
Large Online Q&amp;A Collections with Compact Translation Models
</title>
<author confidence="0.9372">
Jung-Tae Lee† and Sang-Bum Kim§ and Young-In Song$ and Hae-Chang Rim††Dept. of Computer &amp; Radio Communications Engineering, Korea University, Seoul, Korea
</author>
<affiliation confidence="0.826991">
§Search Business Team, SK Telecom, Seoul, Korea
$Dept. of Computer Science &amp; Engineering, Korea University, Seoul, Korea
</affiliation>
<email confidence="0.996771">
{jtlee,sbkim,song,rim}@nlp.korea.ac.kr
</email>
<sectionHeader confidence="0.995608" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9994193">
Lexical gaps between queries and questions
(documents) have been a major issue in ques-
tion retrieval on large online question and
answer (Q&amp;A) collections. Previous stud-
ies address the issue by implicitly expanding
queries with the help of translation models
pre-constructed using statistical techniques.
However, since it is possible for unimpor-
tant words (e.g., non-topical words, common
words) to be included in the translation mod-
els, a lack of noise control on the models can
cause degradation of retrieval performance.
This paper investigates a number of empirical
methods for eliminating unimportant words in
order to construct compact translation mod-
els for retrieval purposes. Experiments con-
ducted on a real world Q&amp;A collection show
that substantial improvements in retrieval per-
formance can be achieved by using compact
translation models.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9996893">
Community-driven question answering services,
such as Yahoo! Answers1 and Live Search QnA2,
have been rapidly gaining popularity among Web
users interested in sharing information online. By
inducing users to collaboratively submit questions
and answer questions posed by other users, large
amounts of information have been collected in the
form of question and answer (Q&amp;A) pairs in recent
years. This user-generated information is a valu-
able resource for many information seekers, because
</bodyText>
<footnote confidence="0.999775">
1http://answers.yahoo.com/
2http://qna.live.com/
</footnote>
<bodyText confidence="0.993727777777778">
users can acquire information straightforwardly by
searching through answered questions that satisfy
their information need.
Retrieval models for such Q&amp;A collections
should manage to handle the lexical gaps or word
mismatches between user questions (queries) and
answered questions in the collection. Consider the
two following examples of questions that are seman-
tically similar to each other:
</bodyText>
<listItem confidence="0.9997215">
• “Where can I get cheap airplane tickets?”
• “Any travel website for low airfares?”
</listItem>
<bodyText confidence="0.999889666666667">
Conventional word-based retrieval models would
fail to capture the similarity between the two, be-
cause they have no words in common. To bridge the
query-question gap, prior work on Q&amp;A retrieval by
Jeon et al. (2005) implicitly expands queries with the
use of pre-constructed translation models, which lets
you generate query words not in a question by trans-
lation to alternate words that are related. In prac-
tice, these translation models are often constructed
using statistical machine translation techniques that
primarily rely on word co-occurrence statistics ob-
tained from parallel strings (e.g., question-answer
pairs).
A critical issue of the translation-based ap-
proaches is the quality of translation models con-
structed in advance. If no noise control is conducted
during the construction, it is possible for translation
models to contain “unnecessary” translations (i.e.,
translating a word into an unimportant word, such as
a non-topical or common word). In the query expan-
sion viewpoint, an attempt to identify and decrease
</bodyText>
<page confidence="0.962969">
410
</page>
<note confidence="0.9620565">
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 410–418,
Honolulu, October 2008.c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.999934205882353">
the proportion of unnecessary translations in a trans-
lation model may produce an effect of “selective”
implicit query expansion and result in improved re-
trieval. However, prior work on translation-based
Q&amp;A retrieval does not recognize this issue and uses
the translation model as it is; essentially no attention
seems to have been paid to improving the perfor-
mance of the translation-based approach by enhanc-
ing the quality of translation models.
In this paper, we explore a number of empiri-
cal methods for selecting and eliminating unimpor-
tant words from parallel strings to avoid unnecessary
translations from being learned in translation models
built for retrieval purposes. We use the term compact
translation models to refer to the resulting models,
since the total number of parameters for modeling
translations would be minimized naturally. We also
present experiments in which compact translation
models are used in Q&amp;A retrieval. The main goal of
our study is to investigate if and how compact trans-
lation models can improve the performance of Q&amp;A
retrieval.
The rest of this paper is organized as follows.
The next section introduces a translation-based re-
trieval model and accompanying techniques used to
retrieve query-relevant questions. Section 3 presents
a number of empirical ways to select and eliminate
unimportant words from parallel strings for training
compact translation models. Section 4 summarizes
the compact translation models we built for retrieval
experiments. Section 5 presents and discusses the
results of retrieval experiments. Section 6 presents
related works. Finally, the last section concludes the
paper and discusses future directions.
</bodyText>
<sectionHeader confidence="0.975973" genericHeader="method">
2 Translation-based Retrieval Model
</sectionHeader>
<bodyText confidence="0.999844666666667">
This section introduces the translation-based lan-
guage modeling approach to retrieval that has been
used to bridge the lexical gap between queries and
already-answered questions in this paper.
In the basic language modeling framework for re-
trieval (Ponte and Croft, 1998), the similarity be-
tween a query Q and a document D for ranking may
be modeled as the probability of the document lan-
guage model MD built from D generating Q:
</bodyText>
<equation confidence="0.991304">
sim(Q,D) ≈ P(Q|MD) (1)
</equation>
<bodyText confidence="0.676311">
Assuming that query words occur independently
given a particular document language model, the
query-likelihood P(Q|MD) is calculated as:
</bodyText>
<equation confidence="0.9972605">
P(Q|MD) = rl P(q|MD) (2)
qEQ
</equation>
<bodyText confidence="0.999542833333333">
where q represents a query word.
To avoid zero probabilities in document language
models, a mixture between a document-specific
multinomial distribution and a multinomial distribu-
tion estimated from the entire document collection
is widely used in practice:
</bodyText>
<equation confidence="0.9999165">
P(Q|MD) = rl [ (1 − A) · P(q|MD)
qEQ
]
+A · P(q|MC) (3)
</equation>
<bodyText confidence="0.999850714285714">
where 0 &lt; A &lt; 1 and MC represents a language
model built from the entire collection. The probabil-
ities P(w|MD) and P(w|MC) are calculated using
maximum likelihood estimation.
The basic language modeling framework does not
address the issue of lexical gaps between queries
and question. Berger and Lafferty (1999) viewed
information retrieval as statistical document-query
translation and introduced translation models to map
query words to document words. Assuming that
a translation model can be represented by a condi-
tional probability distribution of translation T(·|·)
between words, we can model P(q|MD) in Equa-
tion 3 as:
</bodyText>
<equation confidence="0.997419">
P(q|MD) = 1] T(q|w)P(w|MD) (4)
wED
</equation>
<bodyText confidence="0.999861923076923">
where w represents a document word.3
The translation probability T (q|w) virtually rep-
resents the degree of relationship between query
word q and document word w captured in a differ-
ent, machine translation setting. Then, in the tra-
ditional information retrieval viewpoint, the use of
translation models produce an implicit query expan-
sion effect, since query words not in a document are
mapped to related words in the document. This im-
plies that translation-based retrieval models would
make positive contributions to retrieval performance
only when the pre-constructed translation models
have reliable translation probability distributions.
</bodyText>
<footnote confidence="0.898273">
3The formulation of our retrieval model is basically equiva-
lent to the approach of Jeon et al. (2005).
</footnote>
<page confidence="0.994991">
411
</page>
<subsectionHeader confidence="0.587012">
2.1 IBM Translation Model 1
</subsectionHeader>
<bodyText confidence="0.999969125">
Obviously, we need to build a translation model in
advance. Usually the IBM Model 1, developed in
the statistical machine translation field (Brown et al.,
1993), is used to construct translation models for
retrieval purposes in practice. Specifically, given a
number of parallel strings, the IBM Model 1 learns
the translation probability from a source word s to a
target word t as:
</bodyText>
<equation confidence="0.90799">
c(t|s; Ji) (5)
</equation>
<bodyText confidence="0.99992475">
where As is a normalization factor to make the sum
of translation probabilities for the word s equal to 1,
N is the number of parallel string pairs, and Ji is the
ith parallel string pair. c(t|s; Ji) is calculated as:
</bodyText>
<equation confidence="0.999691">
c(t|s; Ji) =(P(t|s1) P(t|s) 1
+ ··· + P(t|sn)J
xfreqt,Ji x freqs,Ji (6)
</equation>
<bodyText confidence="0.999944777777778">
where {s1,... , sn} are words in the source text in
Ji. freqt,Ji and freqs,Ji are the number of times
that t and s occur in Ji, respectively.
Given the initial values of T (t|s), Equations (5)
and (6) are used to update T (t|s) repeatedly until
the probabilities converge, in an EM-based manner.
Note that the IBM Model 1 solely relies on
word co-occurrence statistics obtained from paral-
lel strings in order to learn translation probabilities.
This implies that if parallel strings have unimportant
words, a resulted translation model based on IBM
Model 1 may contain unimportant words with non-
zero translation probabilities.
We alleviate this drawback by eliminating unim-
portant words from parallel strings, avoiding them
from being included in the conditional translation
probability distribution. This naturally induces the
construction of compact translation models.
</bodyText>
<subsectionHeader confidence="0.9900405">
2.2 Gathering Parallel Strings from Q&amp;A
Collections
</subsectionHeader>
<bodyText confidence="0.999509133333333">
The construction of statistical translation models
previously discussed requires a corpus consisting of
parallel strings. Since monolingual parallel texts are
generally not available in real world, one must arti-
ficially generate a “synthetic” parallel corpus.
Question and answer as parallel pairs: The
simplest approach is to directly employ questions
and their answers in the collections by setting ei-
ther as source strings and the other as target strings,
with the assumption that a question and its cor-
responding answer are naturally parallel to each
other. Formally, if we have a Q&amp;A collection as
C = {D1, D2, ... , Dn}, where Di refers to an ith
Q&amp;A data consisting of a question qi and its an-
swer ai, we can construct a parallel corpus C&apos; as
{(q1, a1), ... , (qn, an)}U{(a1, q1), ... , (an, qn)} =
C&apos; where each element (s, t) refers to a parallel pair
consisting of source string s and target string t. The
number of parallel string samples would eventually
be twice the size of the collections.
Similar questions as parallel pairs: Jeon et
al. (2005) proposed an alternative way of auto-
matically collecting a relatively larger set of par-
allel strings from Q&amp;A collections. Motivated
by the observation that many semantically identi-
cal questions can be found in typical Q&amp;A collec-
tions, they used similarities between answers cal-
culated by conventional word-based retrieval mod-
els to automatically group questions in a Q&amp;A col-
lection as pairs. Formally, two question strings qi
and qj would be included in a parallel corpus C&apos;
as {(qi, qj), (qj, qi)} C C&apos; only if their answer
strings ai and aj have a similarity higher than a
pre-defined threshold value. The similarity is cal-
culated as the reverse of the harmonic mean of ranks
as sim(ai, aj) = 12( rj 1 + ri 1 ), where rj and ri refer to
the rank of the aj and ai when ai and aj are given as
queries, respectively. This approach may artificially
produce much more parallel string pairs for training
the IBM Model 1 than the former approach, depend-
ing on the threshold value.4
To our knowledge, there has not been any study
comparing the effectiveness of the two approaches
yet. In this paper, we try both approaches and com-
pare the effectiveness in retrieval performance.
</bodyText>
<sectionHeader confidence="0.991619" genericHeader="method">
3 Eliminating Unimportant Words
</sectionHeader>
<bodyText confidence="0.999956666666667">
We adopt a term weight ranking approach to iden-
tify and eliminate unimportant words from parallel
strings, assuming that a word in a string is unim-
</bodyText>
<footnote confidence="0.7403105">
4We have empirically set the threshold (0.05) for our exper-
iments.
</footnote>
<equation confidence="0.95556325">
T(t|s) = A�1
s
N
i
</equation>
<page confidence="0.989259">
412
</page>
<figureCaption confidence="0.999137">
Figure 1: Term weighting results of tf-idf and TextRank (window=3). Weighting is done on underlined words only.
</figureCaption>
<bodyText confidence="0.499096333333333">
portant if it holds a relatively low significance in the
document (Q&amp;A pair) of which the string is origi-
nally taken from. Some issues may arise:
</bodyText>
<listItem confidence="0.99028675">
• How to assign a weight to each word in a doc-
ument for term ranking?
• How much to remove as unimportant words
from the ranked list?
</listItem>
<bodyText confidence="0.9981165">
The following subsections discuss strategies we use
to handle each of the issues above.
</bodyText>
<subsectionHeader confidence="0.999886">
3.1 Assigning Term Weights
</subsectionHeader>
<bodyText confidence="0.999899142857143">
In this section, the two different term weighting
strategies are introduced.
tf-idf: The use of tf-idf weighting on evaluating
how unimportant a word is to a document seems to
be a good idea to begin with. We have used the fol-
lowing formulas to calculate the weight of word w
in document D:
</bodyText>
<equation confidence="0.98261575">
tf-idfw,D = tfw,D X idfw (7)
freqw,D
tfw,D = |D |, idfw = log |C|
dfw
</equation>
<bodyText confidence="0.999959384615385">
where freqw,D refers to the number of times w oc-
curs in D, |D |refers to the size of D (in words), |C|
refers to the size of the document collection, and dfw
refers to the number of documents where w appears.
Eventually, words with low tf-idf weights may be
considered as unimportant.
TextRank: The task of term weighting, in fact,
has been often applied to the keyword extraction
task in natural language processing studies. As
an alternative term weighting approach, we have
used a variant of Mihalcea and Tarau (2004)’s Tex-
tRank, a graph-based ranking model for keyword
extraction which achieves state-of-the-art accuracy
without the need of deep linguistic knowledge or
domain-specific corpora.
Specifically, the ranking algorithm proceeds as
follows. First, words in a given document are added
as vertices in a graph G. Then, edges are added be-
tween words (vertices) if the words co-occur in a
fixed-sized window. The number of co-occurrences
becomes the weight of an edge. When the graph is
constructed, the score of each vertex is initialized
as 1, and the PageRank-based ranking algorithm is
run on the graph iteratively until convergence. The
TextRank score of a word w in document D at kth
iteration is defined as follows:
</bodyText>
<equation confidence="0.9751615">
ei,j Rk−1
Ebl:(j,l)EG ej,l w,D
</equation>
<bodyText confidence="0.909810642857143">
(8)
where d is a damping factor usually set to 0.85, and
ei,j is an edge weight between i and j.
The assumption behind the use of the variant of
TextRank is that a word is likely to be an important
word in a document if it co-occurs frequently with
other important words in the document. Eventually,
words with low TextRank scores may be considered
as unimportant. The main differences of TextRank
compared to tf-idf is that it utilizes the context infor-
mation of words to assign term weights.
Figure 1 demonstrates that term weighting results
of TextRank and tf-idf are greatly different. Notice
that TextRank assigns low scores to words that co-
</bodyText>
<equation confidence="0.908797666666667">
�
Rkw,D = (1−d)+d·
bj:(i,j)EG
</equation>
<page confidence="0.991663">
413
</page>
<table confidence="0.999556142857143">
Corpus: (QkA) Vocabulary Size (%chg) Average Translations (%chg)
tf-idf TextRank tf-idf TextRank
Initial 90,441 73
25%Removal 90,326 (∇0.1%) 73,021 (∇19.3 %) 73 (∇0.0%) 44 (∇39.7 %)
50%Removal 90,230 (∇0.2%) 72,225 (∇20.1 %) 72 (∇1.4%) 43 (∇41.1 %)
75%Removal 88,763 (∇1.9%) 65,268 (∇27.8 %) 53 (∇27.4%) 38 (∇47.9 %)
Avg.Score 66,412 (∇26.6%) 31,849 (∇64.8 %) 14 (∇80.8%) 18 (∇75.3 %)
</table>
<tableCaption confidence="0.994817">
Table 1: Impact of various word elimination strategies on translation model construction using (QkA) corpus.
</tableCaption>
<table confidence="0.999935285714286">
Corpus: (QkQ) Vocabulary Size (%chg) Average Translations (%chg)
tf-idf TextRank tf-idf TextRank
Initial 34,485 442
25%Removal 34,374 (∇0.3%) 26,900 (∇22.0 %) 437 (∇1.1%) 282 (∇36.2 %)
50%Removal 34,262 (∇0.6%) 26,421 (∇23.4 %) 423 (∇4.3%) 274 (∇38.0 %)
75%Removal 32,813 (∇4.8%) 23,354 (∇32.3 %) 288 (∇34.8%) 213 (∇51.8 %)
Avg.Score 28,613 (∇17.0%) 16,492 (∇52.2 %) 163 (∇63.1%) 164 (∇62.9 %)
</table>
<tableCaption confidence="0.999367">
Table 2: Impact of various word elimination strategies on translation model construction using (QkQ) corpus.
</tableCaption>
<bodyText confidence="0.999124333333333">
occur only with stopwords. This implies that Tex-
tRank weighs terms more “strictly” than the tf-idf
approach, with use of contexts of words.
</bodyText>
<subsectionHeader confidence="0.997091">
3.2 Deciding the Quantity to be Removed from
Ranked List
</subsectionHeader>
<bodyText confidence="0.999984095238095">
Once a final score (either tf-idf or TextRank score)
is obtained for each word, we create a list of words
ranked in decreasing order of their scores and elim-
inate the ones at lower ranks as unimportant words.
The question here is how to decide the proportion or
quantity to be removed from the ranked list.
Removing a fixed proportion: The first ap-
proach we have used is to decide the number of
unimportant words based on the size of the original
string. For our experiments, we manually vary the
proportion to be removed as 25%, 50%, and 75%.
For instance, if the proportion is set to 50% and an
original string consists of ten words, at most five
words would be remained as important words.
Using average score as threshold: We also have
used an alternate approach to deciding the quantity.
Instead of eliminating a fixed proportion, words are
removed if their score is lower than the average score
of all words in a document. This approach decides
the proportion to be removed more flexibly than the
former approach.
</bodyText>
<sectionHeader confidence="0.991858" genericHeader="method">
4 Building Compact Translation Models
</sectionHeader>
<bodyText confidence="0.999991913043478">
We have initially built two parallel corpora from
a Q&amp;A collection5, denoted as (QkA) corpus and
(QkQ) corpus henceforth, by varying the methods
in which parallel strings are gathered (described in
Section 2.2). The (QkA) corpus consists of 85,938
parallel string pairs, and the (QkQ) corpus contains
575,649 parallel string pairs.
In order to build compact translation models, we
have preprocessed the parallel corpus using differ-
ent word elimination strategies so that unimpor-
tant words would be removed from parallel strings.
We have also used a stoplist6 consisting of 429
words to remove stopwords. The out-of-the-box
GIZA++7 (Och and Ney, 2004) has been used to
learn translation models using the pre-processed par-
allel corpus for our retrieval experiments. We have
also trained initial translation models, using a par-
allel corpus from which only the stopwords are re-
moved, to compare with the compact translation
models.
Eventually, the number of parameters needed
for modeling translations would be minimized if
unimportant words are eliminated with different ap-
</bodyText>
<footnote confidence="0.999927333333333">
5Details on this data will be introduced in the next section.
6http://truereader.com/manuals/onix/stopwords1.html
7http://www.fjoch.com/GIZA++.html
</footnote>
<page confidence="0.997803">
414
</page>
<bodyText confidence="0.9999922">
proaches. Table 1 and 2 shows the impact of various
word elimination strategies on the construction of
compact translation models using the (QkA) corpus
and the (QkQ) corpus, respectively. The two tables
report the size of the vocabulary contained and the
average number of translations per word in the re-
sulting compact translation models, along with per-
centage decreases with respect to the initial transla-
tion models in which only stopwords are removed.
We make these observations:
</bodyText>
<listItem confidence="0.963983904761905">
• The translation models learned from the (QkQ)
corpus have less vocabularies but more aver-
age translations per word than the ones learned
from the (QkA) corpus. This result implies that
a large amount of noise may have been cre-
ated inevitably when a large number of parallel
strings (pairs of similar questions) were artifi-
cially gathered from the Q&amp;A collection.
• The TextRank strategy tends to eliminate larger
sets of words as unimportant words than the
tf-idf strategy when a fixed proportion is re-
moved, regardless of the corpus type. Recall
that the TextRank approach assigns weights to
words more strictly by using contexts of words.
• The approach to remove words according to
the average weight of a document (denoted as
Avg.Score) tends to eliminate relatively larger
portions of words as unimportant words than
any of the fixed-proportion strategies, regard-
less of either the corpus type or the ranking
strategy.
</listItem>
<sectionHeader confidence="0.993015" genericHeader="method">
5 Retrieval Experiments
</sectionHeader>
<bodyText confidence="0.999687666666667">
Experiments have been conducted on a real world
Q&amp;A collection to demonstrate the effectiveness of
compact translation models on Q&amp;A retrieval.
</bodyText>
<subsectionHeader confidence="0.969255">
5.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.998739469387755">
In this section, four experimental settings for the
Q&amp;A retrieval experiments are described in detail.
Data: For the experiments, Q&amp;A data have been
collected from the Science domain of Yahoo! An-
swers, one of the most popular community-based
question answering service on the Web. We have
obtained a total of 43,001 questions with a best an-
swer (selected either by the questioner or by votes of
other users) by recursively traversing subcategories
of the Science domain, with up to 1,000 question
pages retrieved.8
Among the obtained Q&amp;A pairs, 32 Q&amp;A pairs
have been randomly selected as the test set, and the
remaining 42,969 questions have been the reference
set to be retrieved. Each Q&amp;A pair has three text
fields: question title, question content, and answer.9
The fields of each Q&amp;A pair in the test set are con-
sidered as various test queries; the question title,
the question content, and the answer are regarded
as a short query, a long query, and a supplementary
query, respectively. We have used long queries and
supplementary queries only in the relevance judg-
ment procedure. All retrieval experiments have been
conducted using short queries only.
Relevance judgments: To find relevant Q&amp;A
pairs given a short query, we have employed a pool-
ing technique used in the TREC conference series.
We have pooled the top 40 Q&amp;A pairs from each
retrieval results generated by varying the retrieval
algorithms, the search field, and the query type.
Popular word-based models, including the Okapi
BM25, query-likelihood language model, and pre-
vious translation-based models (Jeon et al., 2005),
have been used.10
Relevance judgments have been done by two stu-
dent volunteers (both fluent in English). Since
many community-based question answering ser-
vices present their search results in a hierarchical
fashion (i.e. a list of relevant questions is shown
first, and then the user chooses a specific question
from the list to see its answers), a Q&amp;A pair has been
judged as relevant if its question is semantically sim-
ilar to the query; neither quality nor rightness of the
answer has not been considered. When a disagree-
ment has been made between two volunteers, one of
the authors has made the final judgment. As a result,
177 relevant Q&amp;A pairs have been found in total for
the 32 short queries.
Baseline retrieval models: The proposed ap-
</bodyText>
<footnote confidence="0.993350142857143">
8Yahoo! Answers did not expose additional question pages
to external requests at the time of collecting the data.
9When collecting parallel strings from the Q&amp;A collection,
we have put together the question title and the question content
as one question string.
10The retrieval model using compact translation models has
not been used in the pooling procedure.
</footnote>
<page confidence="0.997535">
415
</page>
<bodyText confidence="0.991486363636364">
proach to Q&amp;A retrieval using compact translation
models (denoted as CTLM henceforth) is compared
to three baselines:
QLM: Query-likelihood language model for re-
trieval (equivalent to Equation 3, without use of
translation models). This model represents word-
based retrieval models widely used in practice.
TLM(Q IQ): Translation-based language model
for question retrieval (Jeon et al., 2005). This model
uses IBM Model 1 learned from the (Q IQ) corpus
of which stopwords are removed.
</bodyText>
<listItem confidence="0.559793272727273">
TLM(Q IA): A variant of the translation-based ap-
proach. This model uses IBM model 1 learned from
the (Q IA) corpus.
Evaluation metrics: We have reported the re-
trieval performance in terms of Mean Average Pre-
cision (MAP) and Mean R-Precision (R-Prec).
Average Precision can be computed based on the
precision at each relevant document in the ranking.
Mean Average Precision is defined as the mean of
the Average Precision values across the set of all
queries:
</listItem>
<table confidence="0.9996795">
Retrieval unit MAP R-Prec
Question title 0.1031 0.2396
Question content 0.0422 0.0999
Answer 0.0566 0.1062
</table>
<tableCaption confidence="0.942092">
Table 3: Preliminary retrieval results.
</tableCaption>
<table confidence="0.999881909090909">
Model MAP R-Prec
(%chg) (%chg)
QLM 0.1031 0.2396
TLM(Q IQ)* 0.1121 0.2251
(A9%) (V6%)
CTLM(Q IQ) 0.1415 0.2425
(A37%) (A1%)
TLM(Q IA) 0.1935 0.3135
(A88%) (A31%)
CTLM(Q IA) 0.2095 0.3585
(A103%) (A50%)
</table>
<tableCaption confidence="0.9788535">
Table 4: Comparisons with three baseline retrieval mod-
els. * indicates that it is equivalent to Jeon et al. (2005)’s
approach. MAP improvements of CTLMs have been
tested to be statistically significant using paired t-test.
</tableCaption>
<equation confidence="0.966626666666667">
mq
MAP(Q) = 1 1
|
|
Q
QEQ mQ k=1 Precision(Rk) (9)
</equation>
<bodyText confidence="0.99922825">
where Q is the set of test queries, mQ is the number
of relevant documents for a query q, Rk is the set of
ranked retrieval results from the top until rank posi-
tion k, and Precision(Rk) is the fraction of relevant
documents in Rk (Manning et al., 2008).
R-Precision is defined as the precision after
R documents have been retrieved where R is
the number of relevant documents for the current
query (Buckley and Voorhees, 2000). Mean R-
Precision is the mean of the R-Precisions across the
set of all queries.
We take MAP as our primary evaluation metric.
</bodyText>
<subsectionHeader confidence="0.998628">
5.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.9999472">
Preliminary retrieval experiments have been con-
ducted using the baseline QLM and different fields
of Q&amp;A data as retrieval unit. Table 3 shows the
effectiveness of each field.
The results imply that the question title field is the
most important field in our Yahoo! Answers collec-
tion; this also supports the observation presented by
Jeon et al. (2005). Based on the preliminary obser-
vations, all retrieval models tested in this paper have
ranked Q&amp;A pairs according to the similarity scores
between queries and question titles.
Table 4 presents the comparison results of three
baseline retrieval models and the proposed CTLMs.
For each method, the best performance after empir-
ical A parameter tuning according to MAP is pre-
sented.
Notice that both the TLMs and CTLMs have out-
performed the word-based QLM. This implies that
word-based models that do not address the issue of
lexical gaps between queries and questions often fail
to retrieve relevant Q&amp;A data that have little word
overlap with queries, as noted by Jeon et al. (2005).
Moreover, notice that the proposed CTLMs have
achieved significantly better performances in all
evaluation metrics than both QLM and TLMs, regard-
less of the parallel corpus in which the incorporated
translation models are trained from. This is a clear
indication that the use of compact translation models
built with appropriate word elimination strategies is
effective in closing the query-question lexical gaps
</bodyText>
<page confidence="0.996631">
416
</page>
<table confidence="0.999289142857143">
(QIIQ) MAP (%chg)
tf-idf TextRank
Initial 0.1121
25%Rmv 0.1141 (A1.8) 0.1308 (A16.7)
50%Rmv 0.1261 (A12.5) 0.1334 (A19.00)
75%Rmv 0.1115 (V0.5) 0.1160 (A3.5)
Avg.Score 0.1056 (V5.8) 0.1415 (A26.2)
</table>
<tableCaption confidence="0.9923585">
Table 5: Contributions of various word elimination strate-
gies on MAP performance of CTLM(QIIQ).
</tableCaption>
<table confidence="0.999595857142857">
(QIIA) MAP (%chg)
tf-idf TextRank
Initial 0.1935
25%Rmv 0.2095 (A8.3) 0.1733 (V10.4)
50%Rmv 0.2085 (A7.8) 0.1623 (V16.1)
75%Rmv 0.1449 (V25.1) 0.1515 (V21.7)
Avg.Score 0.1168 (V39.6) 0.1124 (V41.9)
</table>
<tableCaption confidence="0.9968005">
Table 6: Contributions of various word elimination strate-
gies on MAP performance of CTLM(QIIA).
</tableCaption>
<bodyText confidence="0.9999856875">
for improving the performance of question retrieval
in the context of language modeling framework.
Note that the retrieval performance varies by the
type of training corpus; CTLM(QIIA) has outper-
formed CTLM(QIIQ) significantly. This proves the
statement we made earlier that the (QIIQ) corpus
would contain much noise since the translation mod-
els learned from the (QIIQ) corpus tend to have
smaller vocabulary sizes but significantly more aver-
age translations per word than the ones learned from
the (QIIA) corpus.
Table 5 and 6 show the effect of various word
elimination strategies on the retrieval performance
of CTLMs in which the incorporated compact trans-
lation models are trained from the (QIIQ) corpus and
the (QIIA) corpus, respectively. It is interesting to
note that the importance of modifications in word
elimination strategies also varies by the type of train-
ing corpus.
The retrieval results indicate that when the trans-
lation model is trained from the “less noisy” (QIIA)
corpus, eliminating a relatively large proportions of
words may hurt the retrieval performance of CTLM.
In the case when the translation model is trained
from the “noisy” (QIIQ) corpus, a better retrieval
performance may be achieved if words are elimi-
nated appropriately to a certain extent.
In terms of weighting scheme, the TextRank ap-
proach, which is more “strict” than tf-idf in elim-
inating unimportant words, has led comparatively
higher retrieval performances on all levels of re-
moval quantity when the translation model has been
trained from the “noisy” (QIIQ) corpus. On the con-
trary, the “less strict” tf-idf approach has led better
performances when the translation model has been
trained from the “less noisy” (QIIA) corpus.
In summary, the results imply that the perfor-
mance of translation-based retrieval models can be
significantly improved when strategies for building
of compact translation models are chosen properly,
regarding the expected noise level of the parallel cor-
pus for training the translation models. In a case
where a noisy parallel corpus is given for training
of translation models, it is better to get rid of noise
as much as possible by using “strict” term weight-
ing algorithms; when a less noisy parallel corpus is
given for building the translation models, a tolerant
approach would yield better retrieval performance.
</bodyText>
<sectionHeader confidence="0.999918" genericHeader="method">
6 Related Works
</sectionHeader>
<bodyText confidence="0.999834272727273">
Our work is most closely related to Jeon et
al. (2005)’s work, which addresses the issue of
word mismatch between queries and questions in
large online Q&amp;A collections by using translation-
based methods. Apart from their work, there have
been some related works on applying translation-
based methods for retrieving FAQ data. Berger et
al. (2000) report some of the earliest work on FAQ
retrieval using statistical retrieval models, includ-
ing translation-based approaches, with a small set
of FAQ data. Soricut and Brill (2004) present an an-
swer passage retrieval system that is trained from 1
million FAQs collected from the Web using trans-
lation methods. Riezler et al. (2007) demonstrate
the advantages of translation-based approach to an-
swer retrieval by utilizing a more complex trans-
lation model also trained from a large amount of
data extracted from FAQs on the Web. Although all
of these translation-based approaches are based on
the statistical translation models, including the IBM
Model 1, none of them focus on addressing the noise
issues in translation models.
</bodyText>
<page confidence="0.997396">
417
</page>
<sectionHeader confidence="0.987413" genericHeader="conclusions">
7 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999982333333333">
Bridging the query-question gap has been a major is-
sue in retrieval models for large online Q&amp;A collec-
tions. In this paper, we have shown that the perfor-
mance of translation-based retrieval on real online
Q&amp;A collections can be significantly improved by
using compact translation models of which the noise
(unimportant word translations) is properly reduced.
We have also observed that the performance en-
hancement may be achieved by choosing the appro-
priate strategies regarding the strictness of various
term weighting algorithms and the expected noise
level of the parallel data for learning such transla-
tion models.
Future work will focus on testing the effective-
ness of the proposed method on a larger set of Q&amp;A
collections with broader domains. Since the pro-
posed approach cannot handle many-to-one or one-
to-many word transformations, we also plan to in-
vestigate the effectiveness of phrase-based transla-
tion models in closing gaps between queries and
questions for further enhancement of Q&amp;A retrieval.
</bodyText>
<sectionHeader confidence="0.9982" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999474">
This work was supported by Microsoft Research
Asia. Any opinions, findings, and conclusions or
recommendations expressed above are those of the
authors and do not necessarily reflect the views of
the sponsor.
</bodyText>
<sectionHeader confidence="0.999256" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99928805882353">
Adam Berger, Rich Caruana, David Cohn, Dayne Fre-
itag, and Vibhu Mittal. 2000. Bridging the Lexical
Chasm: Statistical Approaches to Answer-Finding. In
Proceedings of the 23rd Annual International ACM SI-
GIR Conference on Research and Development in In-
formation Retrieval, pages 192–199.
Adam Berger and John Lafferty. 1999. Information Re-
trieval as Statistical Translation. In Proceedings of the
22nd Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 222–229.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The Mathematics
of Statistical Machine Translation: Parameter Estima-
tion. Computational Linguistics, 19(2):263–311.
Chris Buckley and Ellen M. Voorhees. 2000. Evaluating
Evaluation Measure Stability. In Proceedings of the
23rd Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 33–40.
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding Similar Questions in Large Question and An-
swer Archives. In Proceedings of the 14th ACM Inter-
national Conference on Information and Knowledge
Management, pages 84–90.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Sch¨utze. 2008. Introduction to Information Re-
trieval. Cambridge University Press.
Rada Mihalcea and Paul Tarau. 2004. TextRank: Bring-
ing Order into Text. In Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Language
Processing, pages 404–411.
Franz J. Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19–51.
Jay M. Ponte and W. Bruce Croft. 1998. A Language
Modeling Approach to Information Retrieval. In Pro-
ceedings of the 21st Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, pages 275–281.
Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-
taridis, Vibhu Mittal, and Yi Liu. 2007. Statistical
Machine Translation for Query Expansion in Answer
Retrieval. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics,
pages 464–471.
Radu Soricut and Eric Brill. 2004. Automatic Question
Answering: Beyond the Factoid. In Proceedings of
the 2004 Human Language Technology and Confer-
ence of the North American Chapter of the Association
for Computational Linguistics, pages 57–64.
</reference>
<page confidence="0.992999">
418
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.587816">
<title confidence="0.873198333333333">Bridging Lexical Gaps between Queries and Questions Large Online Q&amp;A Collections with Compact Translation Models and and and of Computer &amp; Radio Communications Engineering, Korea University, Seoul,</title>
<author confidence="0.730458">Business Team</author>
<author confidence="0.730458">SK Telecom</author>
<author confidence="0.730458">Seoul</author>
<affiliation confidence="0.795122">of Computer Science &amp; Engineering, Korea University, Seoul,</affiliation>
<abstract confidence="0.999567285714286">Lexical gaps between queries and questions (documents) have been a major issue in question retrieval on large online question and answer (Q&amp;A) collections. Previous studaddress the issue by queries with the help of translation models pre-constructed using statistical techniques. since it is possible for unimpor- (e.g., non-topical words, common words) to be included in the translation models, a lack of noise control on the models can cause degradation of retrieval performance. This paper investigates a number of empirical methods for eliminating unimportant words in to construct translation modretrieval purposes. Experiments conducted on a real world Q&amp;A collection show that substantial improvements in retrieval performance can be achieved by using compact translation models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam Berger</author>
<author>Rich Caruana</author>
<author>David Cohn</author>
<author>Dayne Freitag</author>
<author>Vibhu Mittal</author>
</authors>
<title>Bridging the Lexical Chasm: Statistical Approaches to Answer-Finding.</title>
<date>2000</date>
<booktitle>In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>192--199</pages>
<contexts>
<context position="29222" citStr="Berger et al. (2000)" startWordPosition="4661" endWordPosition="4664"> translation models, it is better to get rid of noise as much as possible by using “strict” term weighting algorithms; when a less noisy parallel corpus is given for building the translation models, a tolerant approach would yield better retrieval performance. 6 Related Works Our work is most closely related to Jeon et al. (2005)’s work, which addresses the issue of word mismatch between queries and questions in large online Q&amp;A collections by using translationbased methods. Apart from their work, there have been some related works on applying translationbased methods for retrieving FAQ data. Berger et al. (2000) report some of the earliest work on FAQ retrieval using statistical retrieval models, including translation-based approaches, with a small set of FAQ data. Soricut and Brill (2004) present an answer passage retrieval system that is trained from 1 million FAQs collected from the Web using translation methods. Riezler et al. (2007) demonstrate the advantages of translation-based approach to answer retrieval by utilizing a more complex translation model also trained from a large amount of data extracted from FAQs on the Web. Although all of these translation-based approaches are based on the sta</context>
</contexts>
<marker>Berger, Caruana, Cohn, Freitag, Mittal, 2000</marker>
<rawString>Adam Berger, Rich Caruana, David Cohn, Dayne Freitag, and Vibhu Mittal. 2000. Bridging the Lexical Chasm: Statistical Approaches to Answer-Finding. In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 192–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Berger</author>
<author>John Lafferty</author>
</authors>
<title>Information Retrieval as Statistical Translation.</title>
<date>1999</date>
<booktitle>In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>222--229</pages>
<contexts>
<context position="6497" citStr="Berger and Lafferty (1999)" startWordPosition="966" endWordPosition="969">where q represents a query word. To avoid zero probabilities in document language models, a mixture between a document-specific multinomial distribution and a multinomial distribution estimated from the entire document collection is widely used in practice: P(Q|MD) = rl [ (1 − A) · P(q|MD) qEQ ] +A · P(q|MC) (3) where 0 &lt; A &lt; 1 and MC represents a language model built from the entire collection. The probabilities P(w|MD) and P(w|MC) are calculated using maximum likelihood estimation. The basic language modeling framework does not address the issue of lexical gaps between queries and question. Berger and Lafferty (1999) viewed information retrieval as statistical document-query translation and introduced translation models to map query words to document words. Assuming that a translation model can be represented by a conditional probability distribution of translation T(·|·) between words, we can model P(q|MD) in Equation 3 as: P(q|MD) = 1] T(q|w)P(w|MD) (4) wED where w represents a document word.3 The translation probability T (q|w) virtually represents the degree of relationship between query word q and document word w captured in a different, machine translation setting. Then, in the traditional informati</context>
</contexts>
<marker>Berger, Lafferty, 1999</marker>
<rawString>Adam Berger and John Lafferty. 1999. Information Retrieval as Statistical Translation. In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 222–229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter Estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="7785" citStr="Brown et al., 1993" startWordPosition="1162" endWordPosition="1165">icit query expansion effect, since query words not in a document are mapped to related words in the document. This implies that translation-based retrieval models would make positive contributions to retrieval performance only when the pre-constructed translation models have reliable translation probability distributions. 3The formulation of our retrieval model is basically equivalent to the approach of Jeon et al. (2005). 411 2.1 IBM Translation Model 1 Obviously, we need to build a translation model in advance. Usually the IBM Model 1, developed in the statistical machine translation field (Brown et al., 1993), is used to construct translation models for retrieval purposes in practice. Specifically, given a number of parallel strings, the IBM Model 1 learns the translation probability from a source word s to a target word t as: c(t|s; Ji) (5) where As is a normalization factor to make the sum of translation probabilities for the word s equal to 1, N is the number of parallel string pairs, and Ji is the ith parallel string pair. c(t|s; Ji) is calculated as: c(t|s; Ji) =(P(t|s1) P(t|s) 1 + ··· + P(t|sn)J xfreqt,Ji x freqs,Ji (6) where {s1,... , sn} are words in the source text in Ji. freqt,Ji and fre</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Buckley</author>
<author>Ellen M Voorhees</author>
</authors>
<title>Evaluating Evaluation Measure Stability.</title>
<date>2000</date>
<booktitle>In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="24329" citStr="Buckley and Voorhees, 2000" startWordPosition="3891" endWordPosition="3894">at it is equivalent to Jeon et al. (2005)’s approach. MAP improvements of CTLMs have been tested to be statistically significant using paired t-test. mq MAP(Q) = 1 1 | | Q QEQ mQ k=1 Precision(Rk) (9) where Q is the set of test queries, mQ is the number of relevant documents for a query q, Rk is the set of ranked retrieval results from the top until rank position k, and Precision(Rk) is the fraction of relevant documents in Rk (Manning et al., 2008). R-Precision is defined as the precision after R documents have been retrieved where R is the number of relevant documents for the current query (Buckley and Voorhees, 2000). Mean RPrecision is the mean of the R-Precisions across the set of all queries. We take MAP as our primary evaluation metric. 5.2 Experimental Results Preliminary retrieval experiments have been conducted using the baseline QLM and different fields of Q&amp;A data as retrieval unit. Table 3 shows the effectiveness of each field. The results imply that the question title field is the most important field in our Yahoo! Answers collection; this also supports the observation presented by Jeon et al. (2005). Based on the preliminary observations, all retrieval models tested in this paper have ranked Q</context>
</contexts>
<marker>Buckley, Voorhees, 2000</marker>
<rawString>Chris Buckley and Ellen M. Voorhees. 2000. Evaluating Evaluation Measure Stability. In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwoon Jeon</author>
<author>W Bruce Croft</author>
<author>Joon Ho Lee</author>
</authors>
<title>Finding Similar Questions in Large Question and Answer Archives.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th ACM International Conference on Information and Knowledge Management,</booktitle>
<pages>84--90</pages>
<contexts>
<context position="2543" citStr="Jeon et al. (2005)" startWordPosition="362" endWordPosition="365">uestions that satisfy their information need. Retrieval models for such Q&amp;A collections should manage to handle the lexical gaps or word mismatches between user questions (queries) and answered questions in the collection. Consider the two following examples of questions that are semantically similar to each other: • “Where can I get cheap airplane tickets?” • “Any travel website for low airfares?” Conventional word-based retrieval models would fail to capture the similarity between the two, because they have no words in common. To bridge the query-question gap, prior work on Q&amp;A retrieval by Jeon et al. (2005) implicitly expands queries with the use of pre-constructed translation models, which lets you generate query words not in a question by translation to alternate words that are related. In practice, these translation models are often constructed using statistical machine translation techniques that primarily rely on word co-occurrence statistics obtained from parallel strings (e.g., question-answer pairs). A critical issue of the translation-based approaches is the quality of translation models constructed in advance. If no noise control is conducted during the construction, it is possible for</context>
<context position="7591" citStr="Jeon et al. (2005)" startWordPosition="1130" endWordPosition="1133">en query word q and document word w captured in a different, machine translation setting. Then, in the traditional information retrieval viewpoint, the use of translation models produce an implicit query expansion effect, since query words not in a document are mapped to related words in the document. This implies that translation-based retrieval models would make positive contributions to retrieval performance only when the pre-constructed translation models have reliable translation probability distributions. 3The formulation of our retrieval model is basically equivalent to the approach of Jeon et al. (2005). 411 2.1 IBM Translation Model 1 Obviously, we need to build a translation model in advance. Usually the IBM Model 1, developed in the statistical machine translation field (Brown et al., 1993), is used to construct translation models for retrieval purposes in practice. Specifically, given a number of parallel strings, the IBM Model 1 learns the translation probability from a source word s to a target word t as: c(t|s; Ji) (5) where As is a normalization factor to make the sum of translation probabilities for the word s equal to 1, N is the number of parallel string pairs, and Ji is the ith p</context>
<context position="10297" citStr="Jeon et al. (2005)" startWordPosition="1581" endWordPosition="1584"> target strings, with the assumption that a question and its corresponding answer are naturally parallel to each other. Formally, if we have a Q&amp;A collection as C = {D1, D2, ... , Dn}, where Di refers to an ith Q&amp;A data consisting of a question qi and its answer ai, we can construct a parallel corpus C&apos; as {(q1, a1), ... , (qn, an)}U{(a1, q1), ... , (an, qn)} = C&apos; where each element (s, t) refers to a parallel pair consisting of source string s and target string t. The number of parallel string samples would eventually be twice the size of the collections. Similar questions as parallel pairs: Jeon et al. (2005) proposed an alternative way of automatically collecting a relatively larger set of parallel strings from Q&amp;A collections. Motivated by the observation that many semantically identical questions can be found in typical Q&amp;A collections, they used similarities between answers calculated by conventional word-based retrieval models to automatically group questions in a Q&amp;A collection as pairs. Formally, two question strings qi and qj would be included in a parallel corpus C&apos; as {(qi, qj), (qj, qi)} C C&apos; only if their answer strings ai and aj have a similarity higher than a pre-defined threshold va</context>
<context position="21225" citStr="Jeon et al., 2005" startWordPosition="3380" endWordPosition="3383">ry, respectively. We have used long queries and supplementary queries only in the relevance judgment procedure. All retrieval experiments have been conducted using short queries only. Relevance judgments: To find relevant Q&amp;A pairs given a short query, we have employed a pooling technique used in the TREC conference series. We have pooled the top 40 Q&amp;A pairs from each retrieval results generated by varying the retrieval algorithms, the search field, and the query type. Popular word-based models, including the Okapi BM25, query-likelihood language model, and previous translation-based models (Jeon et al., 2005), have been used.10 Relevance judgments have been done by two student volunteers (both fluent in English). Since many community-based question answering services present their search results in a hierarchical fashion (i.e. a list of relevant questions is shown first, and then the user chooses a specific question from the list to see its answers), a Q&amp;A pair has been judged as relevant if its question is semantically similar to the query; neither quality nor rightness of the answer has not been considered. When a disagreement has been made between two volunteers, one of the authors has made the</context>
<context position="22728" citStr="Jeon et al., 2005" startWordPosition="3620" endWordPosition="3623"> strings from the Q&amp;A collection, we have put together the question title and the question content as one question string. 10The retrieval model using compact translation models has not been used in the pooling procedure. 415 proach to Q&amp;A retrieval using compact translation models (denoted as CTLM henceforth) is compared to three baselines: QLM: Query-likelihood language model for retrieval (equivalent to Equation 3, without use of translation models). This model represents wordbased retrieval models widely used in practice. TLM(Q IQ): Translation-based language model for question retrieval (Jeon et al., 2005). This model uses IBM Model 1 learned from the (Q IQ) corpus of which stopwords are removed. TLM(Q IA): A variant of the translation-based approach. This model uses IBM model 1 learned from the (Q IA) corpus. Evaluation metrics: We have reported the retrieval performance in terms of Mean Average Precision (MAP) and Mean R-Precision (R-Prec). Average Precision can be computed based on the precision at each relevant document in the ranking. Mean Average Precision is defined as the mean of the Average Precision values across the set of all queries: Retrieval unit MAP R-Prec Question title 0.1031 </context>
<context position="24833" citStr="Jeon et al. (2005)" startWordPosition="3975" endWordPosition="3978">have been retrieved where R is the number of relevant documents for the current query (Buckley and Voorhees, 2000). Mean RPrecision is the mean of the R-Precisions across the set of all queries. We take MAP as our primary evaluation metric. 5.2 Experimental Results Preliminary retrieval experiments have been conducted using the baseline QLM and different fields of Q&amp;A data as retrieval unit. Table 3 shows the effectiveness of each field. The results imply that the question title field is the most important field in our Yahoo! Answers collection; this also supports the observation presented by Jeon et al. (2005). Based on the preliminary observations, all retrieval models tested in this paper have ranked Q&amp;A pairs according to the similarity scores between queries and question titles. Table 4 presents the comparison results of three baseline retrieval models and the proposed CTLMs. For each method, the best performance after empirical A parameter tuning according to MAP is presented. Notice that both the TLMs and CTLMs have outperformed the word-based QLM. This implies that word-based models that do not address the issue of lexical gaps between queries and questions often fail to retrieve relevant Q&amp;</context>
<context position="28933" citStr="Jeon et al. (2005)" startWordPosition="4616" endWordPosition="4619"> retrieval models can be significantly improved when strategies for building of compact translation models are chosen properly, regarding the expected noise level of the parallel corpus for training the translation models. In a case where a noisy parallel corpus is given for training of translation models, it is better to get rid of noise as much as possible by using “strict” term weighting algorithms; when a less noisy parallel corpus is given for building the translation models, a tolerant approach would yield better retrieval performance. 6 Related Works Our work is most closely related to Jeon et al. (2005)’s work, which addresses the issue of word mismatch between queries and questions in large online Q&amp;A collections by using translationbased methods. Apart from their work, there have been some related works on applying translationbased methods for retrieving FAQ data. Berger et al. (2000) report some of the earliest work on FAQ retrieval using statistical retrieval models, including translation-based approaches, with a small set of FAQ data. Soricut and Brill (2004) present an answer passage retrieval system that is trained from 1 million FAQs collected from the Web using translation methods. </context>
</contexts>
<marker>Jeon, Croft, Lee, 2005</marker>
<rawString>Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005. Finding Similar Questions in Large Question and Answer Archives. In Proceedings of the 14th ACM International Conference on Information and Knowledge Management, pages 84–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press.</publisher>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. Introduction to Information Retrieval. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Paul Tarau</author>
</authors>
<title>TextRank: Bringing Order into Text.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>404--411</pages>
<contexts>
<context position="13128" citStr="Mihalcea and Tarau (2004)" startWordPosition="2083" endWordPosition="2086">he weight of word w in document D: tf-idfw,D = tfw,D X idfw (7) freqw,D tfw,D = |D |, idfw = log |C| dfw where freqw,D refers to the number of times w occurs in D, |D |refers to the size of D (in words), |C| refers to the size of the document collection, and dfw refers to the number of documents where w appears. Eventually, words with low tf-idf weights may be considered as unimportant. TextRank: The task of term weighting, in fact, has been often applied to the keyword extraction task in natural language processing studies. As an alternative term weighting approach, we have used a variant of Mihalcea and Tarau (2004)’s TextRank, a graph-based ranking model for keyword extraction which achieves state-of-the-art accuracy without the need of deep linguistic knowledge or domain-specific corpora. Specifically, the ranking algorithm proceeds as follows. First, words in a given document are added as vertices in a graph G. Then, edges are added between words (vertices) if the words co-occur in a fixed-sized window. The number of co-occurrences becomes the weight of an edge. When the graph is constructed, the score of each vertex is initialized as 1, and the PageRank-based ranking algorithm is run on the graph ite</context>
</contexts>
<marker>Mihalcea, Tarau, 2004</marker>
<rawString>Rada Mihalcea and Paul Tarau. 2004. TextRank: Bringing Order into Text. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 404–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<marker>Och, Ney, 2003</marker>
<rawString>Franz J. Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay M Ponte</author>
<author>W Bruce Croft</author>
</authors>
<title>A Language Modeling Approach to Information Retrieval.</title>
<date>1998</date>
<booktitle>In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>275--281</pages>
<contexts>
<context position="5523" citStr="Ponte and Croft, 1998" startWordPosition="805" endWordPosition="808">s for training compact translation models. Section 4 summarizes the compact translation models we built for retrieval experiments. Section 5 presents and discusses the results of retrieval experiments. Section 6 presents related works. Finally, the last section concludes the paper and discusses future directions. 2 Translation-based Retrieval Model This section introduces the translation-based language modeling approach to retrieval that has been used to bridge the lexical gap between queries and already-answered questions in this paper. In the basic language modeling framework for retrieval (Ponte and Croft, 1998), the similarity between a query Q and a document D for ranking may be modeled as the probability of the document language model MD built from D generating Q: sim(Q,D) ≈ P(Q|MD) (1) Assuming that query words occur independently given a particular document language model, the query-likelihood P(Q|MD) is calculated as: P(Q|MD) = rl P(q|MD) (2) qEQ where q represents a query word. To avoid zero probabilities in document language models, a mixture between a document-specific multinomial distribution and a multinomial distribution estimated from the entire document collection is widely used in prac</context>
</contexts>
<marker>Ponte, Croft, 1998</marker>
<rawString>Jay M. Ponte and W. Bruce Croft. 1998. A Language Modeling Approach to Information Retrieval. In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 275–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Alexander Vasserman</author>
<author>Ioannis Tsochantaridis</author>
<author>Vibhu Mittal</author>
<author>Yi Liu</author>
</authors>
<title>Statistical Machine Translation for Query Expansion in Answer Retrieval.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>464--471</pages>
<contexts>
<context position="29554" citStr="Riezler et al. (2007)" startWordPosition="4715" endWordPosition="4718">’s work, which addresses the issue of word mismatch between queries and questions in large online Q&amp;A collections by using translationbased methods. Apart from their work, there have been some related works on applying translationbased methods for retrieving FAQ data. Berger et al. (2000) report some of the earliest work on FAQ retrieval using statistical retrieval models, including translation-based approaches, with a small set of FAQ data. Soricut and Brill (2004) present an answer passage retrieval system that is trained from 1 million FAQs collected from the Web using translation methods. Riezler et al. (2007) demonstrate the advantages of translation-based approach to answer retrieval by utilizing a more complex translation model also trained from a large amount of data extracted from FAQs on the Web. Although all of these translation-based approaches are based on the statistical translation models, including the IBM Model 1, none of them focus on addressing the noise issues in translation models. 417 7 Conclusion and Future Work Bridging the query-question gap has been a major issue in retrieval models for large online Q&amp;A collections. In this paper, we have shown that the performance of translat</context>
</contexts>
<marker>Riezler, Vasserman, Tsochantaridis, Mittal, Liu, 2007</marker>
<rawString>Stefan Riezler, Alexander Vasserman, Ioannis Tsochantaridis, Vibhu Mittal, and Yi Liu. 2007. Statistical Machine Translation for Query Expansion in Answer Retrieval. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, pages 464–471.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Eric Brill</author>
</authors>
<title>Automatic Question Answering: Beyond the Factoid.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Human Language Technology and Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>57--64</pages>
<contexts>
<context position="29403" citStr="Soricut and Brill (2004)" startWordPosition="4689" endWordPosition="4692">e translation models, a tolerant approach would yield better retrieval performance. 6 Related Works Our work is most closely related to Jeon et al. (2005)’s work, which addresses the issue of word mismatch between queries and questions in large online Q&amp;A collections by using translationbased methods. Apart from their work, there have been some related works on applying translationbased methods for retrieving FAQ data. Berger et al. (2000) report some of the earliest work on FAQ retrieval using statistical retrieval models, including translation-based approaches, with a small set of FAQ data. Soricut and Brill (2004) present an answer passage retrieval system that is trained from 1 million FAQs collected from the Web using translation methods. Riezler et al. (2007) demonstrate the advantages of translation-based approach to answer retrieval by utilizing a more complex translation model also trained from a large amount of data extracted from FAQs on the Web. Although all of these translation-based approaches are based on the statistical translation models, including the IBM Model 1, none of them focus on addressing the noise issues in translation models. 417 7 Conclusion and Future Work Bridging the query-</context>
</contexts>
<marker>Soricut, Brill, 2004</marker>
<rawString>Radu Soricut and Eric Brill. 2004. Automatic Question Answering: Beyond the Factoid. In Proceedings of the 2004 Human Language Technology and Conference of the North American Chapter of the Association for Computational Linguistics, pages 57–64.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>