<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.996051">
A Joint Sequence Translation Model with Integrated Reordering
</title>
<author confidence="0.994615">
Nadir Durrani Helmut Schmid Alexander Fraser
</author>
<affiliation confidence="0.995148">
Institute for Natural Language Processing
University of Stuttgart
</affiliation>
<email confidence="0.995211">
{durrani,schmid,fraser}@ims.uni-stuttgart.de
</email>
<sectionHeader confidence="0.995569" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99987605882353">
We present a novel machine translation model
which models translation by a linear sequence
of operations. In contrast to the “N-gram”
model, this sequence includes not only trans-
lation but also reordering operations. Key
ideas of our model are (i) a new reordering
approach which better restricts the position to
which a word or phrase can be moved, and
is able to handle short and long distance re-
orderings in a unified way, and (ii) a joint
sequence model for the translation and re-
ordering probabilities which is more flexi-
ble than standard phrase-based MT. We ob-
serve statistically significant improvements in
BLEU over Moses for German-to-English and
Spanish-to-English tasks, and comparable re-
sults for a French-to-English task.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999984">
We present a novel generative model that explains
the translation process as a linear sequence of oper-
ations which generate a source and target sentence
in parallel. Possible operations are (i) generation of
a sequence of source and target words (ii) insertion
of gaps as explicit target positions for reordering op-
erations, and (iii) forward and backward jump oper-
ations which do the actual reordering. The probabil-
ity of a sequence of operations is defined according
to an N-gram model, i.e., the probability of an op-
eration depends on the n − 1 preceding operations.
Since the translation (generation) and reordering op-
erations are coupled in a single generative story,
the reordering decisions may depend on preceding
translation decisions and translation decisions may
depend on preceding reordering decisions. This pro-
vides a natural reordering mechanism which is able
to deal with local and long-distance reorderings in a
consistent way. Our approach can be viewed as an
extension of the N-gram SMT approach (Mari˜no et
al., 2006) but our model does reordering as an inte-
gral part of a generative model.
The paper is organized as follows. Section 2 dis-
cusses the relation of our work to phrase-based and
the N-gram SMT. Section 3 describes our genera-
tive story. Section 4 defines the probability model,
which is first presented as a generative model, and
then shifted to a discriminative framework. Section
5 provides details on the search strategy. Section 6
explains the training process. Section 7 describes
the experimental setup and results. Section 8 gives
a few examples illustrating different aspects of our
model and Section 9 concludes the paper.
</bodyText>
<sectionHeader confidence="0.91844" genericHeader="introduction">
2 Motivation and Previous Work
</sectionHeader>
<subsectionHeader confidence="0.992238">
2.1 Relation of our work to PBSMT
</subsectionHeader>
<bodyText confidence="0.999899153846154">
Phrase-based SMT provides a powerful translation
mechanism which learns local reorderings, transla-
tion of short idioms, and the insertion and deletion of
words sensitive to local context. However, PBSMT
also has some drawbacks. (i) Dependencies across
phrases are not directly represented in the translation
model. (ii) Discontinuous phrases cannot be used.
(iii) The presence of many different equivalent seg-
mentations increases the search space.
Phrase-based SMT models dependencies between
words and their translations inside of a phrase well.
However, dependencies across phrase boundaries
are largely ignored due to the strong phrasal inde-
</bodyText>
<page confidence="0.939658">
1045
</page>
<note confidence="0.986533733333333">
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1045–1054,
Portland, Oregon, June 19-24, 2011. c�2011 Association for Computational Linguistics
German English
hat er ein buch gelesen he read a book
hat eine pizza gegessen has eaten a pizza
er he
hat has
ein a
eine a
menge lot of
butterkekse butter cookies
gegessen eaten
buch book
zeitung newspaper
dann then
</note>
<tableCaption confidence="0.996724">
Table 1: Sample Phrase Table
</tableCaption>
<bodyText confidence="0.999652875">
pendence assumption. A phrase-based system us-
ing the phrase table1 shown in Table 1, for exam-
ple, correctly translates the German sentence “er
hat eine pizza gegessen” to “he has eaten a pizza”,
but fails while translating “er hat eine menge but-
terkekse gegessen” (see Table 1 for a gloss) which
is translated as “he has a lot of butter cookies eaten”
unless the language model provides strong enough
evidence for a different ordering. The generation of
this sentence in our model starts with generating “er
– he”, “hat – has”. Then a gap is inserted on the Ger-
man side, followed by the generation of “gegessen –
eaten”. At this point, the (partial) German and En-
glish sentences look as follows:
gegessen
he has eaten
We jump back to the gap on the German side
and fill it by generating “eine – a” and “pizza –
pizza”, for the first example and generating “eine –
a”, “menge – lot of”, “butterkekse – butter cookies”
for the second example, thus handling both short
and long distance reordering in a unified manner.
Learning the pattern “hat
helps us to generalize to the second example with
unseen context. Notice how the reordering deci-
sion is triggered by the translation decision in our
model. The probability of a gap insertion operation
after the generation of the auxiliaries “hat – has” will
be high because reordering is necessary in order to
move the second part of the German verb complex
(“gegessen”) to its correct position at the end of the
clause. This mechanism better restricts reordering
</bodyText>
<footnote confidence="0.9973805">
1The examples given in this section are not taken from the
real data/system, but made-up for the sake of argument.
</footnote>
<figureCaption confidence="0.999262">
Figure 1: (a) Known Context (b) Unknown Context
</figureCaption>
<bodyText confidence="0.99649735">
than traditional PBSMT and is able to deal with local
and long-distance reorderings in a consistent way.
Another weakness of the traditional phrase-based
system is that it can only capitalize on continuous
phrases. Given the phrase inventory in Table 1,
phrasal MT is able to generate example in Figure
1(a). The information “hat...gelesen – read” is inter-
nal to the phrase pair “hat er ein buch gelesen – he
read a book”, and is therefore handled conveniently.
On the other hand, the phrase table does not have
the entry “hat er eine zeitung gelesen – he read a
newspaper” (Figure 1(b)). Hence, there is no option
but to translate “hat...gelesen” separately, translat-
ing “hat” to “has” which is a common translation for
“hat” but wrong in the given context. Context-free
hierarchical models (Chiang, 2007; Melamed, 2004)
have rules like “hat er X gelesen – he read X” to han-
dle such cases. Galley and Manning (2010) recently
solved this problem for phrasal MT by extracting
phrase pairs with source and target-side gaps. Our
model can also use tuples with source-side discon-
tinuities. The above sentence would be generated
by the following sequence of operations: (i) gener-
ate “dann – then” (ii) insert a gap (iii) generate “er
– he” (iv) backward jump to the gap (v) generate
“hat...[gelesen] – read” (only “hat” and “read” are
added to the sentences yet) (vi) jump forward to the
right-most source word so far generated (vii) insert
a gap (viii) continue the source cept (“gelesen” is in-
serted now) (ix) backward jump to the gap (x) gen-
erate “ein – a” (xi) generate “buch – book”.
From this operation se-
quence, the model learns a
pattern (Figure 2) which al-
lows it to generalize to the
example in Figure 1(b). The open gap represented
serves a similar purpose as the non-terminal
categories in a hierarchical phrase-based system
such as Hiero. Thus it generalizes to translate “eine
zeitung” in exactly the same way as “ein buch”.
</bodyText>
<figure confidence="0.39301725">
er hat
gegessen – has eaten”
Figure 2: Pattern
by
</figure>
<page confidence="0.970679">
1046
</page>
<bodyText confidence="0.999955428571428">
Another problem of phrasal MT is spurious
phrasal segmentation. Given a sentence pair and
a corresponding word alignment, phrasal MT can
learn an arbitrary number of source segmentations.
This is problematic during decoding because differ-
ent compositions of the same minimal phrasal units
are allowed to compete with each other.
</bodyText>
<subsectionHeader confidence="0.99976">
2.2 Relation of our work to N-gram SMT
</subsectionHeader>
<bodyText confidence="0.99995992">
N-gram based SMT is an alternative to hierarchi-
cal and non-hierarchical phrase-based systems. The
main difference between phrase-based and N-gram
SMT is the extraction procedure of translation units
and the statistical modeling of translation context
(Crego et al., 2005a). The tuples used in N-gram
systems are much smaller translation units than
phrases and are extracted in such a way that a unique
segmentation of each bilingual sentence pair is pro-
duced. This helps N-gram systems to avoid the
spurious phrasal segmentation problem. Reorder-
ing works by linearization of the source side and tu-
ple unfolding (Crego et al., 2005b). The decoder
uses word lattices which are built with linguistically
motivated re-write rules. This mechanism is further
enhanced with an N-gram model of bilingual units
built using POS tags (Crego and Yvon, 2010). A
drawback of their reordering approach is that search
is only performed on a small number of reorderings
that are pre-calculated on the source side indepen-
dently of the target side. Often, the evidence for
the correct ordering is provided by the target-side
language model (LM). In the N-gram approach, the
LM only plays a role in selecting between the pre-
calculated orderings.
Our model is based on the N-gram SMT model,
but differs from previous N-gram systems in some
important aspects. It uses operation n-grams rather
than tuple n-grams. The reordering approach is en-
tirely different and considers all possible orderings
instead of a small set of pre-calculated orderings.
The standard N-gram model heavily relies on POS
tags for reordering and is unable to use lexical trig-
gers whereas our model exclusively uses lexical trig-
gers and no POS information. Linearization and un-
folding of the source sentence according to the target
sentence enables N-gram systems to handle source-
side gaps. We deal with this phenomenon more di-
rectly by means of tuples with source-side discon-
tinuities. The most notable feature of our work is
that it has a complete generative story of transla-
tion which combines translation and reordering op-
erations into a single operation sequence model.
Like the N-gram model2, our model cannot deal
with target-side discontinuities. These are elimi-
nated from the training data by a post-editing pro-
cess on the alignments (see Section 6). Galley and
Manning (2010) found that target-side gaps were not
useful in their system and not useful in the hierarchi-
cal phrase-based system Joshua (Li et al., 2009).
</bodyText>
<sectionHeader confidence="0.99641" genericHeader="method">
3 Generative Story
</sectionHeader>
<bodyText confidence="0.994958103448276">
Our generative story is motivated by the complex re-
orderings in the German-to-English translation task.
The German and English sentences are jointly gen-
erated through a sequence of operations. The En-
glish words are generated in linear order3 while
the German words are generated in parallel with
their English translations. Occasionally the trans-
lator jumps back on the German side to insert some
material at an earlier position. After this is done, it
jumps forward again and continues the translation.
The backward jumps always end at designated land-
ing sites (gaps) which were explicitly inserted be-
fore. We use 4 translation and 3 reordering opera-
tions. Each is briefly discussed below.
Generate (X,Y): X and Y are German and English
cepts4 respectively, each with one or more words.
Words in X (German) may be consecutive or discon-
tinuous, but the words in Y (English) must be con-
secutive. This operation causes the words in Y and
the first word in X to be added to the English and
German strings respectively, that were generated so
far. Subsequent words in X are added to a queue to
be generated later. All the English words in Y are
generated immediately because English is generated
in linear order. The generation of the second (and
subsequent) German word in a multi-word cept can
be delayed by gaps, jumps and the Generate Source
Only operation defined below.
Continue Source Cept: The German words added
</bodyText>
<footnote confidence="0.790698857142857">
2However, Crego and Yvon (2009), in their N-gram system,
use split rules to handle target-side gaps and show a slight im-
provement on a Chinese-English translation task.
3Generating the English words in order is also what the de-
coder does when translating from German to English.
4A cept is a group of words in one language translated as a
minimal unit in one specific context (Brown et al., 1993).
</footnote>
<page confidence="0.9745">
1047
</page>
<bodyText confidence="0.997890188405797">
to the queue by the Generate (X,Y) operation are
generated by the Continue Source Cept operation.
Each Continue Source Cept operation removes one
German word from the queue and copies it to the
German string. If X contains more than one German
word, say n many, then it requires n translation op-
erations, an initial Generate (X1...Xn, Y ) operation
and n − 1 Continue Source Cept operations. For
example “hat...gelesen – read” is generated by the
operation Generate (hat gelesen, read), which adds
“hat” and “read” to the German and English strings
and “gelesen” to a queue. A Continue Source Cept
operation later removes “gelesen” from the queue
and adds it to the German string.
Generate Source Only (X): The string X is added
at the current position in the German string. This op-
eration is used to generate a German word X with no
corresponding English word. It is performed imme-
diately after its preceding German word is covered.
This is because there is no evidence on the English-
side which indicates when to generate X. Generate
Source Only (X) helps us learn a source word dele-
tion model. It is used during decoding, where a Ger-
man word (X) is either translated to some English
word(s) by a Generate (X,Y) operation or deleted
with a Generate Source Only (X) operation.
Generate Identical: The same word is added at
the current position in both the German and En-
glish strings. The Generate Identical operation is
used during decoding for the translation of unknown
words. The probability of this operation is estimated
from singleton German words that are translated to
an identical string. For example, for a tuple “Port-
land – Portland”, where German “Portland” was ob-
served exactly once during training, we use a Gen-
erate Identical operation rather than Generate (Port-
land, Portland).
We now discuss the set of reordering operations
used by the generative story. Reordering has to be
performed whenever the German word to be gen-
erated next does not immediately follow the previ-
ously generated German word. During the genera-
tion process, the translator maintains an index which
specifies the position after the previously covered
German word (j), an index (Z) which specifies the
index after the right-most German word covered so
far, and an index of the next German word to be cov-
ered (j�). The set of reordering operations used in
1048
Table 2: Step-wise Generation of Example 1(a). The ar-
row indicates position j.
generation depends upon these indexes.
Insert Gap: This operation inserts a gap which acts
as a place-holder for the skipped words. There can
be more than one open gap at a time.
Jump Back (W): This operation lets the translator
jump back to an open gap. It takes a parameter W
specifying which gap to jump to. Jump Back (1)
jumps to the closest gap to Z, Jump Back (2) jumps
to the second closest gap to Z, etc. After the back-
ward jump the target gap is closed.
Jump Forward: This operation makes the transla-
tor jump to Z. It is performed if some already gen-
erated German word is between the previously gen-
erated word and the word to be generated next. A
Jump Back (W) operation is only allowed at position
Z. Therefore, if j =� Z, a Jump Forward operation
has to be performed prior to a Jump Back operation.
Table 2 shows step by step the generation of a
</bodyText>
<table confidence="0.876091923076923">
German/English sentence pair, the corresponding
translation operations, and the respective values of
the index variables. A formal algorithm for convert-
ing a word-aligned bilingual corpus into an opera-
tion sequence is presented in Algorithm 1.
4 Model
Our translation model p(F, E) is based on opera-
tion N-gram model which integrates translation and
reordering operations. Given a source string F, a
sequence of tuples T = (t1,...,tn) as hypothe-
sized by the decoder to generate a target string E,
the translation model estimates the probability of a
Algorithm 1 Corpus Conversion Algorithm
</table>
<tableCaption confidence="0.711164">
i Position of current English cept
j Position of current German word
j0 Position of next German word
</tableCaption>
<table confidence="0.969115333333333">
N Total number of English cepts
fj German word at position j
Ei English cept at position i
</table>
<tableCaption confidence="0.97805925">
Fi Sequence of German words linked to Ei
Li Number of German words linked with Ei
k Number of already generated German words for Ei
aik Position of kth German translation of Ei
</tableCaption>
<equation confidence="0.868885444444444">
Z Position after right-most generated German word
S Position of the first word of a target gap
i := 0; j := 0; k := 0
while fj is an unaligned word do
Generate Source Only (fj)
j := j + 1
Z := j
while i &lt; N do
j0 := aik
</equation>
<construct confidence="0.655981">
if j &lt; j0 then
if fj was not generated yet then
</construct>
<figure confidence="0.8723515">
Insert Gap
if j = Z then
j := j0
else
Jump Forward
if j0 &lt; j then
if j &lt; Z and fj was not generated yet then
Insert Gap
W := relative position of target gap
Jump Back (W)
j := S
if j &lt; j0 then
Insert Gap
j :=j0
ifk=0then
Generate (Fi, Ei) for Generate Identical}
else
Continue Source Cept
</figure>
<equation confidence="0.932577333333333">
j := j + 1; k := k + 1
while fj is an unaligned word do
Generate Source Only (fj)
j := j + 1
if Z &lt; j then
Z := j
if k = Li then
i := i + 1; k := 0
Remarks:
</equation>
<bodyText confidence="0.980862714285714">
We use cept positions for English (not word positions) because
English cepts are composed of consecutive words. German po-
sitions are word-based.
The relative position of the target gap is 1 if it is closest to Z, 2
if it is the second closest gap etc.
The operation Generate Identical is chosen if Fi = Ei and the
overall frequency of the German cept Fi is 1.
</bodyText>
<equation confidence="0.88424125">
generated operation sequence O = (o1, ... , oJ) as:
J
p(F, E) Pz H p(oj|oj−m+1...oj−1)
j=1
</equation>
<bodyText confidence="0.999309428571429">
where m indicates the amount of context used. Our
translation model is implemented as an N-gram
model of operations using SRILM-Toolkit (Stolcke,
2002) with Kneser-Ney smoothing. We use a 9-gram
model (m = 8).
Integrating the language model the search is de-
fined as:
</bodyText>
<equation confidence="0.998668">
Eˆ = arg maxpLM(E)p(F,E)
E
</equation>
<bodyText confidence="0.99982875">
where pLM(E) is the monolingual language model
and p(F, E) is the translation model. But our trans-
lation model is a joint probability model, because of
which E is generated twice in the numerator. We
add a factor, prior probability pp,(E), in the denom-
inator, to negate this effect. It is used to marginalize
the joint-probability model p(F, E). The search is
then redefined as:
</bodyText>
<equation confidence="0.9982865">
Eˆ = arg max pLM(E)p(F, E)
E pp,(E)
</equation>
<bodyText confidence="0.997151666666667">
Both, the monolingual language and the prior
probability model are implemented as standard
word-based n-gram models:
</bodyText>
<equation confidence="0.971591666666667">
J
p�(E) Pz H p(wj|wj−m+1, ... , wj−1)
j=1
</equation>
<bodyText confidence="0.9993868">
where m = 4 (5-gram model) for the standard
monolingual model (x = LM) and m = 8 (same
as the operation model5) for the prior probability
model (x = pr).
In order to improve end-to-end accuracy, we in-
troduce new features for our model and shift from
the generative6 model to the standard log-linear ap-
proach (Och and Ney, 2004) to tune7 them. We
search for a target string E which maximizes a linear
combination of feature functions:
</bodyText>
<footnote confidence="0.798746666666667">
5In decoding, the amount of context used for the prior prob-
ability is synchronized with the position of back-off in the op-
eration model.
6Our generative model is about 3 BLEU points worse than
the best discriminative results.
7We tune the operation, monolingual and prior probability
models as separate features. We expect the prior probability
model to get a negative weight but we do not force MERT to
assign a negative weight to this feature.
</footnote>
<page confidence="0.920837">
1049
</page>
<equation confidence="0.9871585">
E� = arg max
E
</equation>
<bodyText confidence="0.981458169811321">
where λj is the weight associated with the feature
hj(F, E). Other than the 3 features discussed above
(log probabilities of the operation model, monolin-
gual language model and prior probability model),
we train 8 additional features discussed below:
Length Bonus The length bonus feature counts the
length of the target sentence in words.
Deletion Penalty Another feature for avoiding too
short translations is the deletion penalty. Deleting a
source word (Generate Source Only (X)) is a com-
mon operation in the generative story. Because there
is no corresponding target-side word, the monolin-
gual language model score tends to favor this op-
eration. The deletion penalty counts the number of
deleted source words.
Gap Bonus and Open Gap Penalty These features
are introduced to guide the reordering decisions. We
observe a large amount of reordering in the automat-
ically word aligned training text. However, given
only the source sentence (and little world knowl-
edge), it is not realistic to try to model the reasons
for all of this reordering. Therefore we can use a
more robust model that reorders less than humans.
The gap bonus feature sums to the total number of
gaps inserted to produce a target sentence. The open
gap penalty feature is a penalty (paid once for each
translation operation performed) whose value is the
number of open gaps. This penalty controls how
quickly gaps are closed.
Distortion and Gap Distance Penalty We have
two additional features to control the reordering de-
cisions. One of them is similar8 to the distance-
based reordering model used by phrasal MT. The
other feature is the gap distance penalty which calcu-
lates the distance between the first word of a source
cept X and the start of the left-most gap. This cost is
paid once for each Generate, Generate Identical and
Generate Source Only. For a source cept coverd by
indexes X1, ... , Xn, we get the feature value gj =
X1 − S, where S is the index of the left-most source
word where a gap starts.
8Let X1, ... , Xn and Y1, ... , Ym represent indexes of the
source words covered by the tuples tj and tj_1 respectively.
The distance between tj and tj_1 is given as dj = min(|Xk −
Yl |− 1) ∀ Xk ∈ {X1,..., Xn} and ∀ Yl ∈ {Y1,..., Ym}
Lexical Features We also use source-to-target
p(e|f) and target-to-source p(f|e) lexical transla-
tion probabilities. Our lexical features are standard
(Koehn et al., 2003). The estimation is motivated by
IBM Model-1. Given a tuple ti with source words
f = f1, f2, . . . , fn, target words e = e1, e2, ... , em
and an alignment a between the source word posi-
tions x = 1, ... , n and the target word positions
</bodyText>
<equation confidence="0.9806904">
y = 1, ... , m, the lexical feature pw(f|e) is com-
puted as follows:
1
{y e (x,y) E a} |E w(fx|ey)
pw(e|f, a) is computed in the same way.
</equation>
<sectionHeader confidence="0.99497" genericHeader="method">
5 Decoding
</sectionHeader>
<bodyText confidence="0.995690542857143">
Our decoder for the new model performs a stack-
based search with a beam-search algorithm similar
to that used in Pharoah (Koehn, 2004a). Given an
input sentence F, it first extracts a set of match-
ing source-side cepts along with their n-best trans-
lations to form a tuple inventory. During hypoth-
esis expansion, the decoder picks a tuple from the
inventory and generates the sequence of operations
required for the translation with this tuple in light
of the previous hypothesis.9 The sequence of op-
erations may include translation (generate, continue
source cept etc.) and reordering (gap insertions,
jumps) operations. The decoder also calculates the
overall cost of the new hypothesis. Recombination
is performed on hypotheses having the same cov-
erage vector, monolingual language model context,
and operation model context. We do histogram-
based pruning, maintaining the 500 best hypotheses
for each stack.10
9A hypothesis maintains the index of the last source word
covered (j), the position of the right-most source word covered
so far (Z), the number of open gaps, the number of gaps so
far inserted, the previously generated operations, the generated
target string, and the accumulated values of all the features dis-
cussed in Section 4.
10We need a higher beam size to produce translation units
similar to the phrase-based systems. For example, the phrase-
based system can learn the phrase pair “zum Beispiel – for ex-
ample” and generate it in a single step placing it directly into the
stack two words to the right. Our system generates this example
with two separate tuple translations “zum – for” and “Beispiel
– example” in two adjacent stacks. Because “zum – for” is not
a frequent translation unit, it will be ranked quite low in the first
stack until the tuple “Beispiel – example” appears in the second
stack. Koehn and his colleagues have repeatedly shown that in-
</bodyText>
<equation confidence="0.962461666666667">
{ EJ λjhj(F,E) }
j=1
n
pw(f|e, a) = H
x=1
V(x,y)Ea
</equation>
<page confidence="0.956426">
1050
</page>
<sectionHeader confidence="0.7353785" genericHeader="method">
7 Experimental Setup
7.1 Data
</sectionHeader>
<bodyText confidence="0.968333142857143">
We evaluated the system on three data sets with
German-to-English, Spanish-to-English and French-
to-English news translations, respectively. We used
data from the 4th version of the Europarl Corpus
and the News Commentary which was made avail-
able for the translation task of the Fourth Workshop
on Statistical Machine Translation.11 We use 200K
bilingual sentences, composed by concatenating the
entire news commentary (Pz� 74K sentences) and Eu-
roparl (Pz� 126K sentence), for the estimation of the
translation model. Word alignments were generated
with GIZA++ (Och and Ney, 2003), using the grow-
diag-final-and heuristic (Koehn et al., 2005). In or-
der to obtain the best alignment quality, the align-
ment task is performed on the entire parallel data and
not just on the training data we use. All data is low-
ercased, and we use the Moses tokenizer and recap-
italizer. Our monolingual language model is trained
on 500K sentences. These comprise 300K sentences
from the monolingual corpus (news commentary)
and 200K sentences from the target-side part of the
bilingual corpus. The latter part is also used to train
the prior probability model. The dev and test sets
are news-dev2009a and news-dev2009b which con-
tain 1025 and 1026 parallel sentences. The feature
weights are tuned with Z-MERT (Zaidan, 2009).
7.2 Results
Baseline: We compare our model to a recent ver-
sion of Moses (Koehn et al., 2007) using Koehn’s
training scripts and evaluate with BLEU (Papineni
et al., 2002). We provide Moses with the same ini-
tial alignments as we are using to train our system.12
We use the default parameters for Moses, and a 5-
gram English language model (the same as in our
system).
We compare two variants of our system. The first
system (Twno−rl) applies no hard reordering limit
and uses the distortion and gap distance penalty fea-
tures as soft constraints, allowing all possible re-
orderings. The second system (Twrl−6) uses no dis-
tortion and gap distance features, but applies a hard
constraint which limits reordering to no more than 6
</bodyText>
<figureCaption confidence="0.781867">
Figure 3: Post-editing of Alignments (a) Initial (b) No
Target-Discontinuities (c) Final Alignments
6 Training
Training includes: (i) post-editing of the alignments,
(ii) generation of the operation sequence (iii) estima-
tion of the n-gram language models.
Our generative story does not handle target-side
discontinuities and unaligned target words. There-
</figureCaption>
<bodyText confidence="0.993252818181818">
fore we eliminate them from the training corpus in a
3-step process: If a source word is aligned with mul-
tiple target words which are not consecutive, first
the link to the least frequent target word is iden-
tified, and the group of links containing this word
is retained while the others are deleted. The in-
tuition here is to keep the alignments containing
content words (which are less frequent than func-
tional words). The new alignment has no target-
side discontinuities anymore, but might still contain
unaligned target words. For each unaligned target
word, we determine the (left or right) neighbour that
it appears more frequently with and align it with the
same source word as the neighbour. The result is
an alignment without target-side discontinuities and
unaligned target words. Figure 3 shows an illustra-
tive example of the process. The tuples in Figure 3c
are “A – U V”, “B – W X Y”, “C – NULL”, “D – Z”.
We apply Algorithm 1 to convert the preprocessed
aligned corpus into a sequence of translation opera-
tions. The resulting operation corpus contains one
sequence of operations per sentence pair.
In the final training step, the three language mod-
els are trained using the SRILM Toolkit. The oper-
ation model is estimated from the operation corpus.
The prior probability model is estimated from the
target side part of the bilingual corpus. The mono-
lingual language model is estimated from the target
side of the bilingual corpus and additional monolin-
gual data.
creasing the Moses stack size from 200 to 1000 does not have 11http://www.statmt.org/wmt09/translation-task.html
a significant effect on translation into English, see (Koehn and 12We tried applying our post-processing to the alignments
Haddow, 2009) and other shared task papers. provided to Moses and found that this made little difference.
</bodyText>
<table confidence="0.973499">
1051
Source German Spanish French
Blno−rl 17.41 19.85 19.39
Blrl−6 18.57 21.67 20.84
Twno−rl 18.97 22.17 20.94
Twrl−6 19.03 21.88 20.72
</table>
<tableCaption confidence="0.972997">
Table 3: This Work(Tw) vs Moses (Bl), no-rl = No Re-
ordering Limit, rl-6 = Reordering limit 6
</tableCaption>
<bodyText confidence="0.999768138888889">
positions. Specifically, we do not extend hypotheses
that are more than 6 words apart from the first word
of the left-most gap during decoding. In this exper-
iment, we disallowed tuples which were discontin-
uous on the source side. We compare our systems
with two Moses systems as baseline, one using no
reordering limit (Blno−rl) and one using the default
distortion limit of 6 (Blrl−6).
Both of our systems (see Table 3) outperform
Moses on the German-to-English and Spanish-to-
English tasks and get comparable results for French-
to-English. Our best system (Twno−rl), which uses
no hard reordering limit, gives statistically signifi-
cant (p &lt; 0.05)13 improvements over Moses (both
baselines) for the German-to-English and Spanish-
to-English translation task. The results for Moses
drop by more than a BLEU point without the re-
ordering limit (see Blno−rl in Table 3). All our
results are statistically significant over the baseline
Blno−rl for all the language pairs.
In another experiment, we tested our system also
with tuples which were discontinuous on the source
side. These gappy translation units neither improved
the performance of the system with hard reordering
limit (Twrl−6−asg) nor that of the system without
reordering limit (Twno−rl−asg) as Table 4 shows.
In an analysis of the output we found two reasons
for this result: (i) Using tuples with source gaps in-
creases the list of extracted n-best translation tuples
exponentially which makes the search problem even
more difficult. Table 5 shows the number of tuples
(with and without gaps) extracted when decoding
the test file with 10-best translations. (ii) The fu-
ture cost14 is poorly estimated in case of tuples with
gappy source cepts, causing search errors.
In an experiment, we deleted gappy tuples with
</bodyText>
<footnote confidence="0.9789612">
13We used Kevin Gimpel’s implementation of pairwise boot-
strap resampling (Koehn, 2004b), 1000 samples.
14The dynamic programming approach of calculating future
cost for bigger spans gives erroneous results when gappy cepts
can interleave. Details omitted due to space limitations.
</footnote>
<table confidence="0.9771788">
Source German Spanish French
Twno−rl−asg 18.61 21.60 20.59
Twrl−6−asg 18.65 21.40 20.47
Twno−rl−hsg 18.91 21.93 20.87
Twrl−6−hsg 19.23 21.79 20.85
</table>
<tableCaption confidence="0.8779955">
Table 4: Our Systems with Gappy Units, asg = All Gappy
Units, hsg = Heuristic for pruning Gappy Units
</tableCaption>
<table confidence="0.9995535">
Source German Spanish French
Gaps 965515 1705156 1473798
No-Gaps 256992 313690 343220
Heuristic (hsg) 281618 346993 385869
</table>
<tableCaption confidence="0.995413">
Table 5: 10-best Translation Options With &amp; Without
Gaps and using our Heuristic
</tableCaption>
<bodyText confidence="0.888357375">
a score (future cost estimate) lower than the sum of
the best scores of the parts. This heuristic removes
many useless discontinuous tuples. We found that
results improved (Twno−rl−hsg and Twrl−6−hsg in
Table 4) compared to the version using all gaps
(Twno−rl−asg, Twrl−6−asg), and are closer to the
results without discontinuous tuples (Twno−rl and
Twrl−6 in Table 3).
</bodyText>
<sectionHeader confidence="0.98015" genericHeader="method">
8 Sample Output
</sectionHeader>
<bodyText confidence="0.999967">
In this section we compare the output of our sys-
tems and Moses. Example 1 in Figure 4 shows
the powerful reordering mechanism of our model
which moves the English verb phrase “do not want
to negotiate” to its correct position between the sub-
ject “they” and the prepositional phrase “about con-
crete figures”. Moses failed to produce the correct
word order in this example. Notice that although
our model is using smaller translation units “nicht
– do not”, “verhandlen – negotiate” and “wollen –
want to”, it is able to memorize the phrase transla-
tion “nicht verhandlen wollen – do not want to ne-
gotiate” as a sequence of translation and reordering
operations. It learns the reordering of “verhandlen –
negotiate” and “wollen – want to” and also captures
dependencies across phrase boundaries.
Example 2 shows how our system without a re-
ordering limit moves the English translation “vote”
of the German clause-final verb “stimmen” across
about 20 English tokens to its correct position be-
hind the auxiliary “would”.
Example 3 shows how the system with gappy tu-
ples translates a German sentence with the particle
verb “kehrten...zur¨uck” using a single tuple (dashed
lines). Handling phenomena like particle verbs
</bodyText>
<page confidence="0.996676">
1052
</page>
<figureCaption confidence="0.999665">
Figure 4: Sample Output Sentences
</figureCaption>
<bodyText confidence="0.999969066666667">
strongly motivates our treatment of source side gaps.
The system without gappy units happens to pro-
duce the same translation by translating “kehrten” to
“returned” and deleting the particle “zur¨uck” (solid
lines). This is surprising because the operation for
translating “kehrten” to “returned” and for deleting
the particle are too far apart to influence each other
in an n-gram model. Moses run on the same exam-
ple deletes the main verb (“kehrten”), an error that
we frequently observed in the output of Moses.
Our last example (Figure 5) shows that our model
learns idioms like “meiner Meinung nach – In my
opinion ,” and short phrases like “gibt es – there
are” showing its ability to memorize these “phrasal”
translations, just like Moses.
</bodyText>
<sectionHeader confidence="0.996143" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999780818181818">
We have presented a new model for statistical MT
which can be used as an alternative to phrase-
based translation. Similar to N-gram based MT,
it addresses three drawbacks of traditional phrasal
MT by better handling dependencies across phrase
boundaries, using source-side gaps, and solving the
phrasal segmentation problem. In contrast to N-
gram based MT, our model has a generative story
which tightly couples translation and reordering.
Furthermore it considers all possible reorderings un-
like N-gram systems that perform search only on
</bodyText>
<figureCaption confidence="0.997842">
Figure 5: Learning Idioms
</figureCaption>
<bodyText confidence="0.998741">
a limited number of pre-calculated orderings. Our
model is able to correctly reorder words across
large distances, and it memorizes frequent phrasal
translations including their reordering as probable
operations sequences. Our system outperformed
Moses on standard Spanish-to-English and German-
to-English tasks and achieved comparable results for
French-to-English. A binary version of the corpus
conversion algorithm and the decoder is available.15
</bodyText>
<sectionHeader confidence="0.997495" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.982290125">
The authors thank Fabienne Braune and the re-
viewers for their comments. Nadir Durrani was
funded by the Higher Education Commission (HEC)
of Pakistan. Alexander Fraser was funded by
Deutsche Forschungsgemeinschaft grant Models
of Morphosyntax for Statistical Machine Transla-
tion. Helmut Schmid was supported by Deutsche
Forschungsgemeinschaft grant SFB 732.
</bodyText>
<footnote confidence="0.761397">
15http://www.ims.uni-stuttgart.de/—durrani/resources.html
</footnote>
<page confidence="0.990082">
1053
</page>
<sectionHeader confidence="0.995652" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99996299">
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and R. L. Mercer. 1993. The mathematics of
statistical machine translation: parameter estimation.
Computational Linguistics, 19(2):263–311.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201–228.
Josep Maria Crego and Franois Yvon. 2009. Gappy
translation units under left-to-right smt decoding. In
Proceedings of the meeting of the European Associa-
tion for Machine Translation (EAMT), pages 66–73,
Barcelona, Spain.
Josep Maria Crego and Franc¸ois Yvon. 2010. Improv-
ing reordering with linguistically informed bilingual
n-grams. In Coling 2010: Posters, pages 197–205,
Beijing, China, August. Coling 2010 Organizing Com-
mittee.
Josep M. Crego, Marta R. Costa-juss, Jos B. Mario,
and Jos A. R. Fonollosa. 2005a. Ngram-based ver-
sus phrasebased statistical machine translation. In In
Proceedings of the International Workshop on Spoken
Language Technology (IWSLT05, pages 177–184.
Josep M. Crego, Jos´e B. Mariˆno, and Adri`a de Gispert.
2005b. Reordered search and unfolding tuples for
ngram-based SMT. In Proceedings of the 10th Ma-
chine Translation Summit (MT Summit X), pages 283–
289, Phuket, Thailand.
Michel Galley and Christopher D. Manning. 2010. Ac-
curate non-hierarchical phrase-based translation. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 966–
974, Los Angeles, California, June. Association for
Computational Linguistics.
Philipp Koehn and Barry Haddow. 2009. Edinburgh’s
submission to all tracks of the WMT 2009 shared task
with reordering and speed improvements to Moses.
In Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 160–164, Athens, Greece,
March. Association for Computational Linguistics.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
the Human Language Technology and North Ameri-
can Association for Computational Linguistics Con-
ference, pages 127–133, Edmonton, Canada.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description for
the 2005 iwslt speech translation evaluation. In Inter-
national Workshop on Spoken Language Translation
2005.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics, Demonstration Program,
Prague, Czech Republic.
Philipp Koehn. 2004a. Pharaoh: A beam search decoder
for phrase-based statistical machine translation mod-
els. In AMTA, pages 115–124.
Philipp Koehn. 2004b. Statistical significance tests
for machine translation evaluation. In Dekang Lin
and Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388–395, Barcelona, Spain, July. Association
for Computational Linguistics.
Zhifei Li, Chris Callison-burch, Chris Dyer, Juri Ganitke-
vitch, Sanjeev Khudanpur, Lane Schwartz, Wren N. G.
Thornton, Jonathan Weese, and Omar F. Zaidan. 2009.
Joshua: An open source toolkit for parsing-based ma-
chine translation.
J.B. Mari˜no, R.E. Banchs, J.M. Crego, A. de Gispert,
P. Lambert, J.A.R. Fonollosa, and M.R. Costa-juss`a.
2006. N-gram-based machine translation. Computa-
tional Linguistics, 32(4):527–549.
I. Dan Melamed. 2004. Statistical machine translation
by parsing. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics,
Barcelona, Spain.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19–51.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(1):417–449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ’02, pages 311–318, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Intl. Conf. Spoken Language Pro-
cessing, Denver, Colorado.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79–88.
</reference>
<page confidence="0.995204">
1054
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.934415">
<title confidence="0.999839">A Joint Sequence Translation Model with Integrated Reordering</title>
<author confidence="0.996071">Nadir Durrani Helmut Schmid Alexander Fraser</author>
<affiliation confidence="0.999165">Institute for Natural Language University of Stuttgart</affiliation>
<abstract confidence="0.996543888888889">We present a novel machine translation model which models translation by a linear sequence In contrast to the “N-gram” model, this sequence includes not only translation but also reordering operations. Key ideas of our model are (i) a new reordering approach which better restricts the position to which a word or phrase can be moved, and is able to handle short and long distance reorderings in a unified way, and (ii) a joint sequence model for the translation and reordering probabilities which is more flexible than standard phrase-based MT. We observe statistically significant improvements in BLEU over Moses for German-to-English and Spanish-to-English tasks, and comparable results for a French-to-English task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="12127" citStr="Brown et al., 1993" startWordPosition="1984" endWordPosition="1987"> linear order. The generation of the second (and subsequent) German word in a multi-word cept can be delayed by gaps, jumps and the Generate Source Only operation defined below. Continue Source Cept: The German words added 2However, Crego and Yvon (2009), in their N-gram system, use split rules to handle target-side gaps and show a slight improvement on a Chinese-English translation task. 3Generating the English words in order is also what the decoder does when translating from German to English. 4A cept is a group of words in one language translated as a minimal unit in one specific context (Brown et al., 1993). 1047 to the queue by the Generate (X,Y) operation are generated by the Continue Source Cept operation. Each Continue Source Cept operation removes one German word from the queue and copies it to the German string. If X contains more than one German word, say n many, then it requires n translation operations, an initial Generate (X1...Xn, Y ) operation and n − 1 Continue Source Cept operations. For example “hat...gelesen – read” is generated by the operation Generate (hat gelesen, read), which adds “hat” and “read” to the German and English strings and “gelesen” to a queue. A Continue Source </context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="6265" citStr="Chiang, 2007" startWordPosition="1010" endWordPosition="1011">n continuous phrases. Given the phrase inventory in Table 1, phrasal MT is able to generate example in Figure 1(a). The information “hat...gelesen – read” is internal to the phrase pair “hat er ein buch gelesen – he read a book”, and is therefore handled conveniently. On the other hand, the phrase table does not have the entry “hat er eine zeitung gelesen – he read a newspaper” (Figure 1(b)). Hence, there is no option but to translate “hat...gelesen” separately, translating “hat” to “has” which is a common translation for “hat” but wrong in the given context. Context-free hierarchical models (Chiang, 2007; Melamed, 2004) have rules like “hat er X gelesen – he read X” to handle such cases. Galley and Manning (2010) recently solved this problem for phrasal MT by extracting phrase pairs with source and target-side gaps. Our model can also use tuples with source-side discontinuities. The above sentence would be generated by the following sequence of operations: (i) generate “dann – then” (ii) insert a gap (iii) generate “er – he” (iv) backward jump to the gap (v) generate “hat...[gelesen] – read” (only “hat” and “read” are added to the sentences yet) (vi) jump forward to the right-most source word</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josep Maria Crego</author>
<author>Franois Yvon</author>
</authors>
<title>Gappy translation units under left-to-right smt decoding.</title>
<date>2009</date>
<booktitle>In Proceedings of the meeting of the European Association for Machine Translation (EAMT),</booktitle>
<pages>66--73</pages>
<location>Barcelona,</location>
<contexts>
<context position="11762" citStr="Crego and Yvon (2009)" startWordPosition="1920" endWordPosition="1923">or discontinuous, but the words in Y (English) must be consecutive. This operation causes the words in Y and the first word in X to be added to the English and German strings respectively, that were generated so far. Subsequent words in X are added to a queue to be generated later. All the English words in Y are generated immediately because English is generated in linear order. The generation of the second (and subsequent) German word in a multi-word cept can be delayed by gaps, jumps and the Generate Source Only operation defined below. Continue Source Cept: The German words added 2However, Crego and Yvon (2009), in their N-gram system, use split rules to handle target-side gaps and show a slight improvement on a Chinese-English translation task. 3Generating the English words in order is also what the decoder does when translating from German to English. 4A cept is a group of words in one language translated as a minimal unit in one specific context (Brown et al., 1993). 1047 to the queue by the Generate (X,Y) operation are generated by the Continue Source Cept operation. Each Continue Source Cept operation removes one German word from the queue and copies it to the German string. If X contains more </context>
</contexts>
<marker>Crego, Yvon, 2009</marker>
<rawString>Josep Maria Crego and Franois Yvon. 2009. Gappy translation units under left-to-right smt decoding. In Proceedings of the meeting of the European Association for Machine Translation (EAMT), pages 66–73, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josep Maria Crego</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Improving reordering with linguistically informed bilingual n-grams.</title>
<date>2010</date>
<journal>Organizing Committee.</journal>
<booktitle>In Coling 2010: Posters,</booktitle>
<pages>197--205</pages>
<location>Beijing, China,</location>
<contexts>
<context position="8662" citStr="Crego and Yvon, 2010" startWordPosition="1406" endWordPosition="1409">ing of translation context (Crego et al., 2005a). The tuples used in N-gram systems are much smaller translation units than phrases and are extracted in such a way that a unique segmentation of each bilingual sentence pair is produced. This helps N-gram systems to avoid the spurious phrasal segmentation problem. Reordering works by linearization of the source side and tuple unfolding (Crego et al., 2005b). The decoder uses word lattices which are built with linguistically motivated re-write rules. This mechanism is further enhanced with an N-gram model of bilingual units built using POS tags (Crego and Yvon, 2010). A drawback of their reordering approach is that search is only performed on a small number of reorderings that are pre-calculated on the source side independently of the target side. Often, the evidence for the correct ordering is provided by the target-side language model (LM). In the N-gram approach, the LM only plays a role in selecting between the precalculated orderings. Our model is based on the N-gram SMT model, but differs from previous N-gram systems in some important aspects. It uses operation n-grams rather than tuple n-grams. The reordering approach is entirely different and cons</context>
</contexts>
<marker>Crego, Yvon, 2010</marker>
<rawString>Josep Maria Crego and Franc¸ois Yvon. 2010. Improving reordering with linguistically informed bilingual n-grams. In Coling 2010: Posters, pages 197–205, Beijing, China, August. Coling 2010 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josep M Crego</author>
<author>Marta R Costa-juss</author>
<author>Jos B Mario</author>
<author>Jos A R Fonollosa</author>
</authors>
<title>Ngram-based versus phrasebased statistical machine translation. In</title>
<date>2005</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Technology (IWSLT05,</booktitle>
<pages>177--184</pages>
<contexts>
<context position="8087" citStr="Crego et al., 2005" startWordPosition="1313" endWordPosition="1316">hrasal MT is spurious phrasal segmentation. Given a sentence pair and a corresponding word alignment, phrasal MT can learn an arbitrary number of source segmentations. This is problematic during decoding because different compositions of the same minimal phrasal units are allowed to compete with each other. 2.2 Relation of our work to N-gram SMT N-gram based SMT is an alternative to hierarchical and non-hierarchical phrase-based systems. The main difference between phrase-based and N-gram SMT is the extraction procedure of translation units and the statistical modeling of translation context (Crego et al., 2005a). The tuples used in N-gram systems are much smaller translation units than phrases and are extracted in such a way that a unique segmentation of each bilingual sentence pair is produced. This helps N-gram systems to avoid the spurious phrasal segmentation problem. Reordering works by linearization of the source side and tuple unfolding (Crego et al., 2005b). The decoder uses word lattices which are built with linguistically motivated re-write rules. This mechanism is further enhanced with an N-gram model of bilingual units built using POS tags (Crego and Yvon, 2010). A drawback of their reo</context>
</contexts>
<marker>Crego, Costa-juss, Mario, Fonollosa, 2005</marker>
<rawString>Josep M. Crego, Marta R. Costa-juss, Jos B. Mario, and Jos A. R. Fonollosa. 2005a. Ngram-based versus phrasebased statistical machine translation. In In Proceedings of the International Workshop on Spoken Language Technology (IWSLT05, pages 177–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josep M Crego</author>
<author>Jos´e B Mariˆno</author>
<author>Adri`a de Gispert</author>
</authors>
<title>Reordered search and unfolding tuples for ngram-based SMT.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th Machine Translation Summit (MT Summit X),</booktitle>
<pages>283--289</pages>
<location>Phuket, Thailand.</location>
<marker>Crego, Mariˆno, de Gispert, 2005</marker>
<rawString>Josep M. Crego, Jos´e B. Mariˆno, and Adri`a de Gispert. 2005b. Reordered search and unfolding tuples for ngram-based SMT. In Proceedings of the 10th Machine Translation Summit (MT Summit X), pages 283– 289, Phuket, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate non-hierarchical phrase-based translation.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>966--974</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="6376" citStr="Galley and Manning (2010)" startWordPosition="1030" endWordPosition="1033">e in Figure 1(a). The information “hat...gelesen – read” is internal to the phrase pair “hat er ein buch gelesen – he read a book”, and is therefore handled conveniently. On the other hand, the phrase table does not have the entry “hat er eine zeitung gelesen – he read a newspaper” (Figure 1(b)). Hence, there is no option but to translate “hat...gelesen” separately, translating “hat” to “has” which is a common translation for “hat” but wrong in the given context. Context-free hierarchical models (Chiang, 2007; Melamed, 2004) have rules like “hat er X gelesen – he read X” to handle such cases. Galley and Manning (2010) recently solved this problem for phrasal MT by extracting phrase pairs with source and target-side gaps. Our model can also use tuples with source-side discontinuities. The above sentence would be generated by the following sequence of operations: (i) generate “dann – then” (ii) insert a gap (iii) generate “er – he” (iv) backward jump to the gap (v) generate “hat...[gelesen] – read” (only “hat” and “read” are added to the sentences yet) (vi) jump forward to the right-most source word so far generated (vii) insert a gap (viii) continue the source cept (“gelesen” is inserted now) (ix) backward </context>
<context position="10153" citStr="Galley and Manning (2010)" startWordPosition="1649" endWordPosition="1652">earization and unfolding of the source sentence according to the target sentence enables N-gram systems to handle sourceside gaps. We deal with this phenomenon more directly by means of tuples with source-side discontinuities. The most notable feature of our work is that it has a complete generative story of translation which combines translation and reordering operations into a single operation sequence model. Like the N-gram model2, our model cannot deal with target-side discontinuities. These are eliminated from the training data by a post-editing process on the alignments (see Section 6). Galley and Manning (2010) found that target-side gaps were not useful in their system and not useful in the hierarchical phrase-based system Joshua (Li et al., 2009). 3 Generative Story Our generative story is motivated by the complex reorderings in the German-to-English translation task. The German and English sentences are jointly generated through a sequence of operations. The English words are generated in linear order3 while the German words are generated in parallel with their English translations. Occasionally the translator jumps back on the German side to insert some material at an earlier position. After thi</context>
</contexts>
<marker>Galley, Manning, 2010</marker>
<rawString>Michel Galley and Christopher D. Manning. 2010. Accurate non-hierarchical phrase-based translation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 966– 974, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Barry Haddow</author>
</authors>
<title>Edinburgh’s submission to all tracks of the WMT</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>160--164</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<marker>Koehn, Haddow, 2009</marker>
<rawString>Philipp Koehn and Barry Haddow. 2009. Edinburgh’s submission to all tracks of the WMT 2009 shared task with reordering and speed improvements to Moses. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 160–164, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology and North American Association for Computational Linguistics Conference,</booktitle>
<pages>127--133</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="21697" citStr="Koehn et al., 2003" startWordPosition="3704" endWordPosition="3707">h Generate, Generate Identical and Generate Source Only. For a source cept coverd by indexes X1, ... , Xn, we get the feature value gj = X1 − S, where S is the index of the left-most source word where a gap starts. 8Let X1, ... , Xn and Y1, ... , Ym represent indexes of the source words covered by the tuples tj and tj_1 respectively. The distance between tj and tj_1 is given as dj = min(|Xk − Yl |− 1) ∀ Xk ∈ {X1,..., Xn} and ∀ Yl ∈ {Y1,..., Ym} Lexical Features We also use source-to-target p(e|f) and target-to-source p(f|e) lexical translation probabilities. Our lexical features are standard (Koehn et al., 2003). The estimation is motivated by IBM Model-1. Given a tuple ti with source words f = f1, f2, . . . , fn, target words e = e1, e2, ... , em and an alignment a between the source word positions x = 1, ... , n and the target word positions y = 1, ... , m, the lexical feature pw(f|e) is computed as follows: 1 {y e (x,y) E a} |E w(fx|ey) pw(e|f, a) is computed in the same way. 5 Decoding Our decoder for the new model performs a stackbased search with a beam-search algorithm similar to that used in Pharoah (Koehn, 2004a). Given an input sentence F, it first extracts a set of matching source-side cep</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the Human Language Technology and North American Association for Computational Linguistics Conference, pages 127–133, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra Birch Mayne</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
</authors>
<title>Edinburgh system description for the 2005 iwslt speech translation evaluation.</title>
<date>2005</date>
<booktitle>In International Workshop on Spoken Language Translation</booktitle>
<contexts>
<context position="24684" citStr="Koehn et al., 2005" startWordPosition="4213" endWordPosition="4216">em on three data sets with German-to-English, Spanish-to-English and Frenchto-English news translations, respectively. We used data from the 4th version of the Europarl Corpus and the News Commentary which was made available for the translation task of the Fourth Workshop on Statistical Machine Translation.11 We use 200K bilingual sentences, composed by concatenating the entire news commentary (Pz� 74K sentences) and Europarl (Pz� 126K sentence), for the estimation of the translation model. Word alignments were generated with GIZA++ (Och and Ney, 2003), using the growdiag-final-and heuristic (Koehn et al., 2005). In order to obtain the best alignment quality, the alignment task is performed on the entire parallel data and not just on the training data we use. All data is lowercased, and we use the Moses tokenizer and recapitalizer. Our monolingual language model is trained on 500K sentences. These comprise 300K sentences from the monolingual corpus (news commentary) and 200K sentences from the target-side part of the bilingual corpus. The latter part is also used to train the prior probability model. The dev and test sets are news-dev2009a and news-dev2009b which contain 1025 and 1026 parallel senten</context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh system description for the 2005 iwslt speech translation evaluation. In International Workshop on Spoken Language Translation 2005.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Hieu Hoang,</title>
<location>Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi,</location>
<marker>Koehn, </marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, Demonstration Program,</booktitle>
<location>Prague, Czech Republic.</location>
<marker>Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, Demonstration Program, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: A beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In AMTA,</booktitle>
<pages>115--124</pages>
<contexts>
<context position="22215" citStr="Koehn, 2004" startWordPosition="3815" endWordPosition="3816">|e) lexical translation probabilities. Our lexical features are standard (Koehn et al., 2003). The estimation is motivated by IBM Model-1. Given a tuple ti with source words f = f1, f2, . . . , fn, target words e = e1, e2, ... , em and an alignment a between the source word positions x = 1, ... , n and the target word positions y = 1, ... , m, the lexical feature pw(f|e) is computed as follows: 1 {y e (x,y) E a} |E w(fx|ey) pw(e|f, a) is computed in the same way. 5 Decoding Our decoder for the new model performs a stackbased search with a beam-search algorithm similar to that used in Pharoah (Koehn, 2004a). Given an input sentence F, it first extracts a set of matching source-side cepts along with their n-best translations to form a tuple inventory. During hypothesis expansion, the decoder picks a tuple from the inventory and generates the sequence of operations required for the translation with this tuple in light of the previous hypothesis.9 The sequence of operations may include translation (generate, continue source cept etc.) and reordering (gap insertions, jumps) operations. The decoder also calculates the overall cost of the new hypothesis. Recombination is performed on hypotheses havi</context>
<context position="30330" citStr="Koehn, 2004" startWordPosition="5136" endWordPosition="5137">g) as Table 4 shows. In an analysis of the output we found two reasons for this result: (i) Using tuples with source gaps increases the list of extracted n-best translation tuples exponentially which makes the search problem even more difficult. Table 5 shows the number of tuples (with and without gaps) extracted when decoding the test file with 10-best translations. (ii) The future cost14 is poorly estimated in case of tuples with gappy source cepts, causing search errors. In an experiment, we deleted gappy tuples with 13We used Kevin Gimpel’s implementation of pairwise bootstrap resampling (Koehn, 2004b), 1000 samples. 14The dynamic programming approach of calculating future cost for bigger spans gives erroneous results when gappy cepts can interleave. Details omitted due to space limitations. Source German Spanish French Twno−rl−asg 18.61 21.60 20.59 Twrl−6−asg 18.65 21.40 20.47 Twno−rl−hsg 18.91 21.93 20.87 Twrl−6−hsg 19.23 21.79 20.85 Table 4: Our Systems with Gappy Units, asg = All Gappy Units, hsg = Heuristic for pruning Gappy Units Source German Spanish French Gaps 965515 1705156 1473798 No-Gaps 256992 313690 343220 Heuristic (hsg) 281618 346993 385869 Table 5: 10-best Translation Opt</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004a. Pharaoh: A beam search decoder for phrase-based statistical machine translation models. In AMTA, pages 115–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004,</booktitle>
<pages>388--395</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="22215" citStr="Koehn, 2004" startWordPosition="3815" endWordPosition="3816">|e) lexical translation probabilities. Our lexical features are standard (Koehn et al., 2003). The estimation is motivated by IBM Model-1. Given a tuple ti with source words f = f1, f2, . . . , fn, target words e = e1, e2, ... , em and an alignment a between the source word positions x = 1, ... , n and the target word positions y = 1, ... , m, the lexical feature pw(f|e) is computed as follows: 1 {y e (x,y) E a} |E w(fx|ey) pw(e|f, a) is computed in the same way. 5 Decoding Our decoder for the new model performs a stackbased search with a beam-search algorithm similar to that used in Pharoah (Koehn, 2004a). Given an input sentence F, it first extracts a set of matching source-side cepts along with their n-best translations to form a tuple inventory. During hypothesis expansion, the decoder picks a tuple from the inventory and generates the sequence of operations required for the translation with this tuple in light of the previous hypothesis.9 The sequence of operations may include translation (generate, continue source cept etc.) and reordering (gap insertions, jumps) operations. The decoder also calculates the overall cost of the new hypothesis. Recombination is performed on hypotheses havi</context>
<context position="30330" citStr="Koehn, 2004" startWordPosition="5136" endWordPosition="5137">g) as Table 4 shows. In an analysis of the output we found two reasons for this result: (i) Using tuples with source gaps increases the list of extracted n-best translation tuples exponentially which makes the search problem even more difficult. Table 5 shows the number of tuples (with and without gaps) extracted when decoding the test file with 10-best translations. (ii) The future cost14 is poorly estimated in case of tuples with gappy source cepts, causing search errors. In an experiment, we deleted gappy tuples with 13We used Kevin Gimpel’s implementation of pairwise bootstrap resampling (Koehn, 2004b), 1000 samples. 14The dynamic programming approach of calculating future cost for bigger spans gives erroneous results when gappy cepts can interleave. Details omitted due to space limitations. Source German Spanish French Twno−rl−asg 18.61 21.60 20.59 Twrl−6−asg 18.65 21.40 20.47 Twno−rl−hsg 18.91 21.93 20.87 Twrl−6−hsg 19.23 21.79 20.85 Table 4: Our Systems with Gappy Units, asg = All Gappy Units, hsg = Heuristic for pruning Gappy Units Source German Spanish French Gaps 965515 1705156 1473798 No-Gaps 256992 313690 343220 Heuristic (hsg) 281618 346993 385869 Table 5: 10-best Translation Opt</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004b. Statistical significance tests for machine translation evaluation. In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 388–395, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhifei Li</author>
<author>Chris Callison-burch</author>
<author>Chris Dyer</author>
<author>Juri Ganitkevitch</author>
<author>Sanjeev Khudanpur</author>
<author>Lane Schwartz</author>
<author>Wren N G Thornton</author>
<author>Jonathan Weese</author>
<author>Omar F Zaidan</author>
</authors>
<title>Joshua: An open source toolkit for parsing-based machine translation.</title>
<date>2009</date>
<contexts>
<context position="10293" citStr="Li et al., 2009" startWordPosition="1673" endWordPosition="1676">phenomenon more directly by means of tuples with source-side discontinuities. The most notable feature of our work is that it has a complete generative story of translation which combines translation and reordering operations into a single operation sequence model. Like the N-gram model2, our model cannot deal with target-side discontinuities. These are eliminated from the training data by a post-editing process on the alignments (see Section 6). Galley and Manning (2010) found that target-side gaps were not useful in their system and not useful in the hierarchical phrase-based system Joshua (Li et al., 2009). 3 Generative Story Our generative story is motivated by the complex reorderings in the German-to-English translation task. The German and English sentences are jointly generated through a sequence of operations. The English words are generated in linear order3 while the German words are generated in parallel with their English translations. Occasionally the translator jumps back on the German side to insert some material at an earlier position. After this is done, it jumps forward again and continues the translation. The backward jumps always end at designated landing sites (gaps) which were</context>
</contexts>
<marker>Li, Callison-burch, Dyer, Ganitkevitch, Khudanpur, Schwartz, Thornton, Weese, Zaidan, 2009</marker>
<rawString>Zhifei Li, Chris Callison-burch, Chris Dyer, Juri Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren N. G. Thornton, Jonathan Weese, and Omar F. Zaidan. 2009. Joshua: An open source toolkit for parsing-based machine translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B Mari˜no</author>
<author>R E Banchs</author>
<author>J M Crego</author>
<author>A de Gispert</author>
<author>P Lambert</author>
<author>J A R Fonollosa</author>
<author>M R Costa-juss`a</author>
</authors>
<title>N-gram-based machine translation.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>4</issue>
<marker>Mari˜no, Banchs, Crego, de Gispert, Lambert, Fonollosa, Costa-juss`a, 2006</marker>
<rawString>J.B. Mari˜no, R.E. Banchs, J.M. Crego, A. de Gispert, P. Lambert, J.A.R. Fonollosa, and M.R. Costa-juss`a. 2006. N-gram-based machine translation. Computational Linguistics, 32(4):527–549.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Statistical machine translation by parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="6281" citStr="Melamed, 2004" startWordPosition="1012" endWordPosition="1013">hrases. Given the phrase inventory in Table 1, phrasal MT is able to generate example in Figure 1(a). The information “hat...gelesen – read” is internal to the phrase pair “hat er ein buch gelesen – he read a book”, and is therefore handled conveniently. On the other hand, the phrase table does not have the entry “hat er eine zeitung gelesen – he read a newspaper” (Figure 1(b)). Hence, there is no option but to translate “hat...gelesen” separately, translating “hat” to “has” which is a common translation for “hat” but wrong in the given context. Context-free hierarchical models (Chiang, 2007; Melamed, 2004) have rules like “hat er X gelesen – he read X” to handle such cases. Galley and Manning (2010) recently solved this problem for phrasal MT by extracting phrase pairs with source and target-side gaps. Our model can also use tuples with source-side discontinuities. The above sentence would be generated by the following sequence of operations: (i) generate “dann – then” (ii) insert a gap (iii) generate “er – he” (iv) backward jump to the gap (v) generate “hat...[gelesen] – read” (only “hat” and “read” are added to the sentences yet) (vi) jump forward to the right-most source word so far generate</context>
</contexts>
<marker>Melamed, 2004</marker>
<rawString>I. Dan Melamed. 2004. Statistical machine translation by parsing. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="24623" citStr="Och and Ney, 2003" startWordPosition="4204" endWordPosition="4207">)Ea 1050 7 Experimental Setup 7.1 Data We evaluated the system on three data sets with German-to-English, Spanish-to-English and Frenchto-English news translations, respectively. We used data from the 4th version of the Europarl Corpus and the News Commentary which was made available for the translation task of the Fourth Workshop on Statistical Machine Translation.11 We use 200K bilingual sentences, composed by concatenating the entire news commentary (Pz� 74K sentences) and Europarl (Pz� 126K sentence), for the estimation of the translation model. Word alignments were generated with GIZA++ (Och and Ney, 2003), using the growdiag-final-and heuristic (Koehn et al., 2005). In order to obtain the best alignment quality, the alignment task is performed on the entire parallel data and not just on the training data we use. All data is lowercased, and we use the Moses tokenizer and recapitalizer. Our monolingual language model is trained on 500K sentences. These comprise 300K sentences from the monolingual corpus (news commentary) and 200K sentences from the target-side part of the bilingual corpus. The latter part is also used to train the prior probability model. The dev and test sets are news-dev2009a </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz J. Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="18735" citStr="Och and Ney, 2004" startWordPosition="3192" endWordPosition="3195">It is used to marginalize the joint-probability model p(F, E). The search is then redefined as: Eˆ = arg max pLM(E)p(F, E) E pp,(E) Both, the monolingual language and the prior probability model are implemented as standard word-based n-gram models: J p�(E) Pz H p(wj|wj−m+1, ... , wj−1) j=1 where m = 4 (5-gram model) for the standard monolingual model (x = LM) and m = 8 (same as the operation model5) for the prior probability model (x = pr). In order to improve end-to-end accuracy, we introduce new features for our model and shift from the generative6 model to the standard log-linear approach (Och and Ney, 2004) to tune7 them. We search for a target string E which maximizes a linear combination of feature functions: 5In decoding, the amount of context used for the prior probability is synchronized with the position of back-off in the operation model. 6Our generative model is about 3 BLEU points worse than the best discriminative results. 7We tune the operation, monolingual and prior probability models as separate features. We expect the prior probability model to get a negative weight but we do not force MERT to assign a negative weight to this feature. 1049 E� = arg max E where λj is the weight asso</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Franz J. Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(1):417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="25517" citStr="Papineni et al., 2002" startWordPosition="4354" endWordPosition="4357">ecapitalizer. Our monolingual language model is trained on 500K sentences. These comprise 300K sentences from the monolingual corpus (news commentary) and 200K sentences from the target-side part of the bilingual corpus. The latter part is also used to train the prior probability model. The dev and test sets are news-dev2009a and news-dev2009b which contain 1025 and 1026 parallel sentences. The feature weights are tuned with Z-MERT (Zaidan, 2009). 7.2 Results Baseline: We compare our model to a recent version of Moses (Koehn et al., 2007) using Koehn’s training scripts and evaluate with BLEU (Papineni et al., 2002). We provide Moses with the same initial alignments as we are using to train our system.12 We use the default parameters for Moses, and a 5- gram English language model (the same as in our system). We compare two variants of our system. The first system (Twno−rl) applies no hard reordering limit and uses the distortion and gap distance penalty features as soft constraints, allowing all possible reorderings. The second system (Twrl−6) uses no distortion and gap distance features, but applies a hard constraint which limits reordering to no more than 6 Figure 3: Post-editing of Alignments (a) Ini</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 311–318, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Intl. Conf. Spoken Language Processing,</booktitle>
<location>Denver, Colorado.</location>
<contexts>
<context position="17691" citStr="Stolcke, 2002" startWordPosition="3009" endWordPosition="3010"> 0 Remarks: We use cept positions for English (not word positions) because English cepts are composed of consecutive words. German positions are word-based. The relative position of the target gap is 1 if it is closest to Z, 2 if it is the second closest gap etc. The operation Generate Identical is chosen if Fi = Ei and the overall frequency of the German cept Fi is 1. generated operation sequence O = (o1, ... , oJ) as: J p(F, E) Pz H p(oj|oj−m+1...oj−1) j=1 where m indicates the amount of context used. Our translation model is implemented as an N-gram model of operations using SRILM-Toolkit (Stolcke, 2002) with Kneser-Ney smoothing. We use a 9-gram model (m = 8). Integrating the language model the search is defined as: Eˆ = arg maxpLM(E)p(F,E) E where pLM(E) is the monolingual language model and p(F, E) is the translation model. But our translation model is a joint probability model, because of which E is generated twice in the numerator. We add a factor, prior probability pp,(E), in the denominator, to negate this effect. It is used to marginalize the joint-probability model p(F, E). The search is then redefined as: Eˆ = arg max pLM(E)p(F, E) E pp,(E) Both, the monolingual language and the pri</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Intl. Conf. Spoken Language Processing, Denver, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
</authors>
<title>Z-MERT: A fully configurable open source tool for minimum error rate training of machine translation systems.</title>
<date>2009</date>
<booktitle>The Prague Bulletin of Mathematical Linguistics,</booktitle>
<pages>91--79</pages>
<contexts>
<context position="25345" citStr="Zaidan, 2009" startWordPosition="4326" endWordPosition="4327"> the alignment task is performed on the entire parallel data and not just on the training data we use. All data is lowercased, and we use the Moses tokenizer and recapitalizer. Our monolingual language model is trained on 500K sentences. These comprise 300K sentences from the monolingual corpus (news commentary) and 200K sentences from the target-side part of the bilingual corpus. The latter part is also used to train the prior probability model. The dev and test sets are news-dev2009a and news-dev2009b which contain 1025 and 1026 parallel sentences. The feature weights are tuned with Z-MERT (Zaidan, 2009). 7.2 Results Baseline: We compare our model to a recent version of Moses (Koehn et al., 2007) using Koehn’s training scripts and evaluate with BLEU (Papineni et al., 2002). We provide Moses with the same initial alignments as we are using to train our system.12 We use the default parameters for Moses, and a 5- gram English language model (the same as in our system). We compare two variants of our system. The first system (Twno−rl) applies no hard reordering limit and uses the distortion and gap distance penalty features as soft constraints, allowing all possible reorderings. The second system</context>
</contexts>
<marker>Zaidan, 2009</marker>
<rawString>Omar F. Zaidan. 2009. Z-MERT: A fully configurable open source tool for minimum error rate training of machine translation systems. The Prague Bulletin of Mathematical Linguistics, 91:79–88.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>