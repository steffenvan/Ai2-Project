<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003635">
<title confidence="0.9982445">
Representing Uncertainty about Complex User Goals in Statistical
Dialogue Systems
</title>
<author confidence="0.511159">
Paul A. Crook
</author>
<affiliation confidence="0.765701666666667">
Interaction Lab
Heriot-Watt University
Edinburgh, United Kingdom
</affiliation>
<email confidence="0.995658">
p.a.crook@hw.ac.uk
</email>
<sectionHeader confidence="0.993841" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999502636363636">
We point out several problems in scaling-
up statistical approaches to spoken dia-
logue systems to enable them to deal with
complex but natural user goals, such as
disjunctive and negated goals and prefer-
ences. In particular, we explore restric-
tions imposed by current independence as-
sumptions in POMDP dialogue models.
This position paper proposes the use of
Automatic Belief Compression methods to
remedy these problems.
</bodyText>
<sectionHeader confidence="0.998802" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.976015326530612">
One of the main problems for a spoken dia-
logue system is to determine the user’s goal (e.g.
plan suitable meeting times or find a good Indian
restaurant nearby) under uncertainty, and thereby
to compute the optimal next system dialogue ac-
tion (e.g. offer a restaurant, ask for clarification).
Recent research in statistical spoken dialogue sys-
tems (SSDS) has successfully addressed aspects of
these problems through the application of Partially
Observable Markov Decision Process (POMDP)
approaches (Thomson and Young, 2010; Young et
al., 2010). However POMDP SSDS are currently
limited by an impoverished representation of user
goals adopted to enable tractable learning.
Current POMDP SSDS state approximations
make it impossible to represent some plausible
user goals, e.g. someone who wants to know about
nearby cheap restaurants and high-quality ones
further away, or wants to schedule a meeting any-
time this week except monday afternoon (also see
Examples in Tables 1–3). This renders dialogue
management sub-optimal and makes it impossi-
ble to deal adequately with the following types of
user utterance: “I’m looking for French or Ital-
ian food”, or “Not Italian, unless it’s expensive”.
User utterances with negations and disjunctions of
Oliver Lemon
Interaction Lab
Heriot-Watt University
Edinburgh, United Kingdom
o.lemon@hw.ac.uk
various sorts are very natural, and exploit the full
power of natural language input. Moreover, work
in dialogue system evaluation, e.g. (Walker et al.,
2004; Lemon et al., 2006), shows that real user
goals are generally sets of items with different fea-
tures, rather than a single item. People like to ex-
plore possible trade offs between features of items.
A central challenge for the field of spoken di-
alogue systems is therefore to: develop realistic
large-scale statistical approaches with an accurate,
extended representation of user goals.
In this paper we propose that the independence
assumptions that have guided POMDP SSDS de-
sign to date should be relaxed, user goal sets
should be introduced and that the subsequent ex-
plosion in the size of the state space should be
dealt with by employing Automatic Belief Com-
pression (ABC) techniques.
</bodyText>
<sectionHeader confidence="0.995663" genericHeader="method">
2 POMDP SSDS
</sectionHeader>
<bodyText confidence="0.990194380952381">
Partially Observable Markov Decision Processes
(POMDPs) are Markov Decision Processes where
the system’s state is only partially observable, i.e.
there is uncertainty as to what the true state is.
The ability to account for uncertainty is crucial for
spoken dialogue systems because their knowledge
about the state is uncertain due to speech recogni-
tion errors and the fact that the user’s goals are not
directly observable. In POMDP models of spo-
ken dialogue (Williams and Young, 2005; Thom-
son and Young, 2010; Young et al., 2010) the dia-
logue policy (what the system should say next) is
based not on a single view of the current state of
the conversation, but on a probability distribution
over all possible states of the conversation. The
optimal POMDP SSDS dialogue act thus automat-
ically takes account of the uncertainty about the
user’s utterances and goals.
All work to date on POMDP SSDS has assumed
(i) that a user has a singular, fully constrained,
fixed goal and (ii) that there is one source of un-
</bodyText>
<subsubsectionHeader confidence="0.672854">
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 209–212,
</subsubsectionHeader>
<affiliation confidence="0.892122">
The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics
</affiliation>
<page confidence="0.998341">
209
</page>
<table confidence="0.9989326">
User goal: good Italian but not Thai food User Goal: good Italian but not Thai food
POMDP SSDS (Example 1) ABC-SSDS (Example 2)
U1: What restaurants are nearby? U’1: What restaurants are nearby?
S1: There are 3 nearby. What type of food do you S’1: There are 3 nearby. What type of food do
want? you want?
U2: Not Italian, unless it’s really good U’2: Not Italian, unless it’s really good
S2: There are 2 Thai restaurants. S’2: Ok. Al Capone’s is highly rated. There are
(The compressed state (i.e. marginal likelihoods in Figure 1) also 2 Thai restaurants nearby.
indicates that Italian food is dis-preferred.) U’3: Great. I’ll try Al Capone’s.
U3: No Thanks. Bye.
</table>
<tableCaption confidence="0.99993">
Table 1: State-of-the-art POMDP SSDS vs ABC-SSDS: Example dialogues 1 and 2. U=User, S=System
</tableCaption>
<bodyText confidence="0.9993955">
certainty; errors in speech recognition. The tradi-
tional role of the Dialogue Manager (DM) is there-
fore one of reducing uncertainty until its belief is
strongly focused on a single goal which strictly fits
the user’s stated constraints. Both the policies that
DMs currently follow and the design decisions im-
plemented to make SDSS tasks learnable use these
assumptions (Thomson and Young, 2010; Young
et al., 2010). The resulting systems force users
into selecting a single domain object as their goal
in order to progress the dialogue. The example
POMDP SSDS dialogues, Examples 1 and 3 (Ta-
bles 1 and 2), show the problems inherent in this
approach.
</bodyText>
<listItem confidence="0.9341636">
User goal: good Italian or good French food
S1: How can I help you?
U1: What restaurants are nearby?
S2: There are 6, what type of food do you
want?
U2: Err... Italian or French
S3: Do you want Italian or French food?
U3: Uh. either.
S4: Do you want Italian or French food?
U4: Ok , how about French then.
S5: There are 2 good French restaurants
nearby. What price range do you want?
U5: What about Italian?
S6: Did you say Italian?
U6: QUIT
</listItem>
<tableCaption confidence="0.946571">
Table 2: POMDP SSDS: Example 3
</tableCaption>
<sectionHeader confidence="0.99285" genericHeader="method">
3 Independence Assumptions
</sectionHeader>
<bodyText confidence="0.999980931034483">
Even considering limited domains, POMDP state
spaces grow very quickly. For example, consider
finding a user’s restaurant preference, which in-
volves getting 4 pieces of information, i.e. food-
type, city-area, price range, quality rating. Given 8
food types, 8 city areas, 3 price ranges and 3 qual-
ity ratings, coupled with 7 user actions and a 34 di-
alogue progress indicator1 then the dialogue state
space contains 8 x 8 x 3 x 3 x 7 x 34 = 326, 592
states. A POMDP belief space is a probability
distribution over all these dialogue states, i.e. a
326, 592 dimensional real valued (R) space.
In order to render such large belief spaces
tractable, the current state of the art in POMDP
SSDS uses a variety of handcrafted compression
techniques, such as making several types of in-
dependence assumption. For example, by assum-
ing that users are only ever interested in one type
of food or one location, and that their interests
in food type, price range, quality, etc. are inde-
pendent, the 326, 592 real valued state space can
be reduced to a much smaller “summary space”
(Williams and Young, 2005) consisting of, say,
4 x R values2. See Figure 1 for a graphical de-
piction of such assumptions3.
As illustrated by Figure 1 the information lost
due to the independence assumptions mean that
these approaches are unable to support conversa-
tions such as that shown in Example 2 (Table 1).
</bodyText>
<sectionHeader confidence="0.936378" genericHeader="method">
4 Sets of User Goals
</sectionHeader>
<bodyText confidence="0.9999429">
Getting rid of independence assumptions allows
the DM to reason and ask questions about the
user’s requirements in a more rational way. It can,
for example distinguish between the user want-
ing “excellent Italian” or “any Thai” versus only
“excellent” restaurants – see Figure 1. However,
the resulting high dimensional real valued state
space can still only represent uncertainly over sin-
gular user goals (limited to single points in the
feature space, e.g. an excellent Italian restaurant).
</bodyText>
<footnote confidence="0.9992642">
1Whether each piece of information is obtained, con-
firmed or unknown.
2By considering only the maximum marginal likelihood
for each of the features.
3These apply after utterance U2/U’2 of Example 1.
</footnote>
<page confidence="0.996136">
210
</page>
<figure confidence="0.9971944">
Independence between
slots assumed
italian
thai
reasonable goodexcellent
� � � �
marginals for food type
user’s
goal
marginals for quality
</figure>
<figureCaption confidence="0.6745808">
Figure 1: Assuming independence of features is
equivalent to marginalising across features. Here,
marginalisation incorrectly suppresses belief in
Italian. Thai retains a uniform belief (which ex-
ists across all restaurant types not yet mentioned).
</figureCaption>
<bodyText confidence="0.999824666666667">
To achieve a substantial gain in the flexibility of
SSDS we need to allow user’s goals that are sets
of points. Maintaining beliefs over “sets of goals”
allows a POMDP DM to refine its belief in the
user’s requirements (managing speech recognition
errors) without forcing the user to specify a sin-
gular tightly constrained goal. The disadvantage
of this approach is a further expansion of the state
space.
</bodyText>
<sectionHeader confidence="0.987215" genericHeader="method">
5 Automatic Belief Compression
</sectionHeader>
<bodyText confidence="0.989958125">
To allow for expansion of the state space, whilst
keeping its size tractable for policy learning, we
suggest replacing handcraft approaches with Au-
tomatic Belief Compression (ABC) techniques.
We propose to use proven, principled statisti-
cal learning methods for automatically reducing
the dimensionality of belief spaces, but which pre-
serve the useful distributions within the full space.
Two complementary methods that we are cur-
rently investigating are VDC (Poupart, 2005) and
E-PCA (Roy and Gordon, 2002; Roy et al., 2005).
These methods have been applied successfully in a
real-time daily living assistant with over 106 states
(St-Aubin et al., 2000; Hoey and Poupart, 2005;
Poupart et al., 2006) and to robotic navigation by
(Roy and Gordon, 2002; Roy et al., 2005). They:
</bodyText>
<listItem confidence="0.744415666666667">
• reduce the dimensionality of state spaces that
were previously intractable for POMDP solu-
tion methods, and
• automatically compress the representation of
belief space distributions to take advantage of
sparsity between likely distributions.
</listItem>
<bodyText confidence="0.999277653846154">
The tight coupling between some dia-
logue states and actions (e.g. a user’s goal
state travel-from-London and system act
confirm-from-London) has led some researchers
to conclude that compression techniques, such as
state aggregation, are not useful in the dialogue
domain (Williams and Young, 2007). However,
such tight coupling may not exist for all states,
indeed VDC has already been applied to a small
spoken dialogue system problem (Poupart, 2005)
where it was shown that compressions could be
found without losing any information4. Further,
for POMDP approaches the state is not the
dialogue state but the belief distribution over
dialogue states. Incompressibility at the dialogue
state level does not rule out compressibility of
belief distributions. Finally, our introduction
of sets for user goals should provide additional
possibilities for compression.
Our aim in applying ABC methods is to allow
POMDP SSDS to handle the much larger state
spaces that are required to achieve the expressive-
ness which we believe will be a real benefit to
users. We plan to do this for real world tasks, e.g. a
city search over 1000s of entities with an uncom-
pressed belief space of the order of 108 x R.
</bodyText>
<sectionHeader confidence="0.988889" genericHeader="method">
6 Target Dialogues
</sectionHeader>
<bodyText confidence="0.999830833333333">
In general, when a user starts a dialogue they
rarely have a singular goal in mind (Walker et al.,
2004; Lemon et al., 2006). Their goal is not a fixed
point in the domain but instead can be thought
of as a (possibly disconnected) set of points, for
example either a nearby cheap restaurant or high-
quality one further away. The set represents trade
offs that the particular user is interested in. People
rarely communicate their goals in terms of such
distributions or trade offs, preferring to provide in-
formation in a piecemeal manner and thus incre-
mentally explore the domain.
In Examples 1–4 (Tables 1–3) we contrast the
operation of a current state-of-the-art POMDP
SSDS with our proposed ABC-SSDS system. The
user’s goal in Examples 3 and 4 (Tables 2 and 3)
is to explore what restaurants are nearby, with a
preference for French or Italian. Current POMDP
SSDS approaches assume that any spread of prob-
ability mass in the belief space represents uncer-
tainty which needs to be resolved. This gener-
ates problems for the POMDP SSDS in Example 3
since the user is forced into specifying one food
type at a time, resulting in an unwieldy confirma-
</bodyText>
<footnote confidence="0.959872">
4Compressing a test problem of 433 states to 31 basis
functions, i.e. a summary space of 31 states.
</footnote>
<page confidence="0.99298">
211
</page>
<listItem confidence="0.916095909090909">
User goal: good Italian or good French food
S’1: How can I help you?
U’1: What restaurants are nearby?
S’2: There are 6, what type of food do you
want?
U2’: Err... Italian or French
S’3: Ok, there are 2 good French restaurants
nearby, and one good Italian.
U’4: OK. Which is best quality?
S’3: Mamma Mia’s has the best rating.
U’5: Great. I’ll go there!
</listItem>
<tableCaption confidence="0.998667">
Table 3: Proposed ABC-SSDS: Example 4
</tableCaption>
<bodyText confidence="0.999978541666667">
tion step (S6 of Example 3) where the user is as-
sumed to have changed their mind. In contrast, the
proposed ABC-SSDS system can believe that the
user has requested information on the combined
set of French and Italian restaurants.
In Examples 1 and 2 (both shown in Table 1)
the user’s goal is to explore restaurants nearby, in-
cluding only well-rated Italians. Here the standard
POMDP SSDS is forced by its “summary space”
(see marginals in Figure 1) to incorrectly represent
the user’s goal after U2 “Not Italian, unless it’s
really good” by ruling out all Italian restaurants5.
The ABC-SSDS user is able to find the restaurant
of their choice, whereas the POMDP SSDS user’s
choice is artificially restricted, and they quit hav-
ing failed to find a suitable item.
The ABC-SSDS style of dialogue is clearly more
efficient than that of current POMDP SSDS. It
seems likely that users of such a system may also
find the style of the conversation more natural, and
may be more confident that their eventual choices
really meet their goals (Walker et al., 2004).
All of these hypotheses remain to be explored
in our future empirical work.
</bodyText>
<sectionHeader confidence="0.998485" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999301">
We present several problems for current POMDP
approaches to spoken dialogue systems, concern-
ing the representation of complex, but natural, user
goals. We propose the development of princi-
pled automatic methods for dimensionality reduc-
tion, in place of the ad-hoc assumptions and hand-
crafted compressions currently used.
In parallel we are also exploring: (i) what ap-
proaches are required for updating beliefs over
sets in real time – in principle a method similar
</bodyText>
<footnote confidence="0.8037305">
5There are several ways to try to remedy this, but all have
problems.
</footnote>
<bodyText confidence="0.999630333333333">
to user goal state partitioning (Young et al., 2010)
would appear to be sufficient, (ii) what exploitable
bounds exist on the sets of goals that are commu-
nicable and (iii) to what extent the complexity of
user goal sets can be traded off against the overall
user experience.
</bodyText>
<sectionHeader confidence="0.997409" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.988778">
Thanks to Dr. Jesse Hoey, the SIGdial reviewers
and the Engineering and Physical Sciences Re-
search Council (EPSRC) project EP/G069840/1.
</bodyText>
<sectionHeader confidence="0.998554" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999902976744186">
J. Hoey and P. Poupart. 2005. Solving POMDPs with
Continuous or Large Discrete Observation Spaces.
In IJCAI.
O. Lemon, K. Georgila, and J. Henderson. 2006. Eval-
uating Effectiveness and Portability of Reinforce-
ment Learned Dialogue Strategies with real users:
the TALK TownInfo Evaluation. In IEEE/ACL Spo-
ken Language Technology.
P. Poupart, N. Vlassis, and J. Hoey. 2006. An An-
alytic Solution to Discrete Bayesian Reinforcement
Learning. In ICML.
P. Poupart. 2005. Exploiting Structure to Efficiently
Solve Large Scale Partially Observable Markov De-
cision Processes. Ph.D. thesis, Dept. Computer Sci-
ence, University of Toronto.
N. Roy and G. Gordon. 2002. Exponential Family
PCA for Belief Compression in POMDPs. In NIPS.
N. Roy, G. Gordon, and S. Thrun. 2005. Finding Ap-
proximate POMDP Solutions Through Belief Com-
pression. Artificial Intelligence Research, 22(1-40).
R. St-Aubin, J. Hoey, and C. Boutilier. 2000. Approx-
imate policy construction using decision diagrams.
In NIPS.
B. Thomson and S. Young. 2010. Bayesian update
of dialogue state: A POMDP framework for spoken
dialogue systems. Computer Speech and Language,
24(4):562–588.
M. Walker, S. Whittaker, A. Stent, P. Maloor, J. Moore,
M. Johnston, and G. Vasireddy. 2004. User tai-
lored generation in the match multimodal dialogue
system. Cognitive Science, 28:811–840.
J. Williams and S. Young. 2005. Scaling Up POMDPs
for Dialog Management: The ”Summary POMDP”
Method. In Proc. ASRU.
J. Williams and S. Young. 2007. Scaling POMDPs
for spoken dialog management. IEEE Transac-
tions on Audio, Speech, and Language Processing,
15(7):2116 –2129, Sept.
S. Young, M. Gaˇsi´c, S. Keizer, F. Mairesse, B. Thom-
son, and K. Yu. 2010. The Hidden Information
State model: a practical framework for POMDP
based spoken dialogue management. Computer
Speech and Language, 24(2):150–174.
</reference>
<page confidence="0.998494">
212
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.124071">
<title confidence="0.997757">Representing Uncertainty about Complex User Goals in Dialogue Systems</title>
<author confidence="0.890229">A Paul</author>
<affiliation confidence="0.312508666666667">Interaction Heriot-Watt Edinburgh, United</affiliation>
<email confidence="0.92664">p.a.crook@hw.ac.uk</email>
<abstract confidence="0.9952105">We point out several problems in scalingup statistical approaches to spoken dialogue systems to enable them to deal with complex but natural user goals, such as disjunctive and negated goals and preferences. In particular, we explore restrictions imposed by current independence assumptions in POMDP dialogue models. This position paper proposes the use of Automatic Belief Compression methods to remedy these problems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Hoey</author>
<author>P Poupart</author>
</authors>
<title>Solving POMDPs with Continuous or Large Discrete Observation Spaces.</title>
<date>2005</date>
<booktitle>In IJCAI.</booktitle>
<contexts>
<context position="9566" citStr="Hoey and Poupart, 2005" startWordPosition="1551" endWordPosition="1554">ng its size tractable for policy learning, we suggest replacing handcraft approaches with Automatic Belief Compression (ABC) techniques. We propose to use proven, principled statistical learning methods for automatically reducing the dimensionality of belief spaces, but which preserve the useful distributions within the full space. Two complementary methods that we are currently investigating are VDC (Poupart, 2005) and E-PCA (Roy and Gordon, 2002; Roy et al., 2005). These methods have been applied successfully in a real-time daily living assistant with over 106 states (St-Aubin et al., 2000; Hoey and Poupart, 2005; Poupart et al., 2006) and to robotic navigation by (Roy and Gordon, 2002; Roy et al., 2005). They: • reduce the dimensionality of state spaces that were previously intractable for POMDP solution methods, and • automatically compress the representation of belief space distributions to take advantage of sparsity between likely distributions. The tight coupling between some dialogue states and actions (e.g. a user’s goal state travel-from-London and system act confirm-from-London) has led some researchers to conclude that compression techniques, such as state aggregation, are not useful in the </context>
</contexts>
<marker>Hoey, Poupart, 2005</marker>
<rawString>J. Hoey and P. Poupart. 2005. Solving POMDPs with Continuous or Large Discrete Observation Spaces. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Lemon</author>
<author>K Georgila</author>
<author>J Henderson</author>
</authors>
<title>Evaluating Effectiveness and Portability of Reinforcement Learned Dialogue Strategies with real users: the TALK TownInfo Evaluation.</title>
<date>2006</date>
<booktitle>In IEEE/ACL Spoken Language Technology.</booktitle>
<contexts>
<context position="2141" citStr="Lemon et al., 2006" startWordPosition="316" endWordPosition="319">ng anytime this week except monday afternoon (also see Examples in Tables 1–3). This renders dialogue management sub-optimal and makes it impossible to deal adequately with the following types of user utterance: “I’m looking for French or Italian food”, or “Not Italian, unless it’s expensive”. User utterances with negations and disjunctions of Oliver Lemon Interaction Lab Heriot-Watt University Edinburgh, United Kingdom o.lemon@hw.ac.uk various sorts are very natural, and exploit the full power of natural language input. Moreover, work in dialogue system evaluation, e.g. (Walker et al., 2004; Lemon et al., 2006), shows that real user goals are generally sets of items with different features, rather than a single item. People like to explore possible trade offs between features of items. A central challenge for the field of spoken dialogue systems is therefore to: develop realistic large-scale statistical approaches with an accurate, extended representation of user goals. In this paper we propose that the independence assumptions that have guided POMDP SSDS design to date should be relaxed, user goal sets should be introduced and that the subsequent explosion in the size of the state space should be d</context>
<context position="11252" citStr="Lemon et al., 2006" startWordPosition="1824" endWordPosition="1827">essibility of belief distributions. Finally, our introduction of sets for user goals should provide additional possibilities for compression. Our aim in applying ABC methods is to allow POMDP SSDS to handle the much larger state spaces that are required to achieve the expressiveness which we believe will be a real benefit to users. We plan to do this for real world tasks, e.g. a city search over 1000s of entities with an uncompressed belief space of the order of 108 x R. 6 Target Dialogues In general, when a user starts a dialogue they rarely have a singular goal in mind (Walker et al., 2004; Lemon et al., 2006). Their goal is not a fixed point in the domain but instead can be thought of as a (possibly disconnected) set of points, for example either a nearby cheap restaurant or highquality one further away. The set represents trade offs that the particular user is interested in. People rarely communicate their goals in terms of such distributions or trade offs, preferring to provide information in a piecemeal manner and thus incrementally explore the domain. In Examples 1–4 (Tables 1–3) we contrast the operation of a current state-of-the-art POMDP SSDS with our proposed ABC-SSDS system. The user’s go</context>
</contexts>
<marker>Lemon, Georgila, Henderson, 2006</marker>
<rawString>O. Lemon, K. Georgila, and J. Henderson. 2006. Evaluating Effectiveness and Portability of Reinforcement Learned Dialogue Strategies with real users: the TALK TownInfo Evaluation. In IEEE/ACL Spoken Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Poupart</author>
<author>N Vlassis</author>
<author>J Hoey</author>
</authors>
<title>An Analytic Solution to Discrete Bayesian Reinforcement Learning.</title>
<date>2006</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="9589" citStr="Poupart et al., 2006" startWordPosition="1555" endWordPosition="1558">r policy learning, we suggest replacing handcraft approaches with Automatic Belief Compression (ABC) techniques. We propose to use proven, principled statistical learning methods for automatically reducing the dimensionality of belief spaces, but which preserve the useful distributions within the full space. Two complementary methods that we are currently investigating are VDC (Poupart, 2005) and E-PCA (Roy and Gordon, 2002; Roy et al., 2005). These methods have been applied successfully in a real-time daily living assistant with over 106 states (St-Aubin et al., 2000; Hoey and Poupart, 2005; Poupart et al., 2006) and to robotic navigation by (Roy and Gordon, 2002; Roy et al., 2005). They: • reduce the dimensionality of state spaces that were previously intractable for POMDP solution methods, and • automatically compress the representation of belief space distributions to take advantage of sparsity between likely distributions. The tight coupling between some dialogue states and actions (e.g. a user’s goal state travel-from-London and system act confirm-from-London) has led some researchers to conclude that compression techniques, such as state aggregation, are not useful in the dialogue domain (Willia</context>
</contexts>
<marker>Poupart, Vlassis, Hoey, 2006</marker>
<rawString>P. Poupart, N. Vlassis, and J. Hoey. 2006. An Analytic Solution to Discrete Bayesian Reinforcement Learning. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Poupart</author>
</authors>
<title>Exploiting Structure to Efficiently Solve Large Scale Partially Observable Markov Decision Processes.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Dept. Computer Science, University of Toronto.</institution>
<contexts>
<context position="9363" citStr="Poupart, 2005" startWordPosition="1519" endWordPosition="1520">gular tightly constrained goal. The disadvantage of this approach is a further expansion of the state space. 5 Automatic Belief Compression To allow for expansion of the state space, whilst keeping its size tractable for policy learning, we suggest replacing handcraft approaches with Automatic Belief Compression (ABC) techniques. We propose to use proven, principled statistical learning methods for automatically reducing the dimensionality of belief spaces, but which preserve the useful distributions within the full space. Two complementary methods that we are currently investigating are VDC (Poupart, 2005) and E-PCA (Roy and Gordon, 2002; Roy et al., 2005). These methods have been applied successfully in a real-time daily living assistant with over 106 states (St-Aubin et al., 2000; Hoey and Poupart, 2005; Poupart et al., 2006) and to robotic navigation by (Roy and Gordon, 2002; Roy et al., 2005). They: • reduce the dimensionality of state spaces that were previously intractable for POMDP solution methods, and • automatically compress the representation of belief space distributions to take advantage of sparsity between likely distributions. The tight coupling between some dialogue states and a</context>
</contexts>
<marker>Poupart, 2005</marker>
<rawString>P. Poupart. 2005. Exploiting Structure to Efficiently Solve Large Scale Partially Observable Markov Decision Processes. Ph.D. thesis, Dept. Computer Science, University of Toronto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Roy</author>
<author>G Gordon</author>
</authors>
<title>Exponential Family PCA for Belief Compression in POMDPs.</title>
<date>2002</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="9395" citStr="Roy and Gordon, 2002" startWordPosition="1523" endWordPosition="1526">goal. The disadvantage of this approach is a further expansion of the state space. 5 Automatic Belief Compression To allow for expansion of the state space, whilst keeping its size tractable for policy learning, we suggest replacing handcraft approaches with Automatic Belief Compression (ABC) techniques. We propose to use proven, principled statistical learning methods for automatically reducing the dimensionality of belief spaces, but which preserve the useful distributions within the full space. Two complementary methods that we are currently investigating are VDC (Poupart, 2005) and E-PCA (Roy and Gordon, 2002; Roy et al., 2005). These methods have been applied successfully in a real-time daily living assistant with over 106 states (St-Aubin et al., 2000; Hoey and Poupart, 2005; Poupart et al., 2006) and to robotic navigation by (Roy and Gordon, 2002; Roy et al., 2005). They: • reduce the dimensionality of state spaces that were previously intractable for POMDP solution methods, and • automatically compress the representation of belief space distributions to take advantage of sparsity between likely distributions. The tight coupling between some dialogue states and actions (e.g. a user’s goal state</context>
</contexts>
<marker>Roy, Gordon, 2002</marker>
<rawString>N. Roy and G. Gordon. 2002. Exponential Family PCA for Belief Compression in POMDPs. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Roy</author>
<author>G Gordon</author>
<author>S Thrun</author>
</authors>
<date>2005</date>
<journal>Finding Approximate POMDP Solutions Through Belief Compression. Artificial Intelligence Research,</journal>
<pages>22--1</pages>
<contexts>
<context position="9414" citStr="Roy et al., 2005" startWordPosition="1527" endWordPosition="1530"> of this approach is a further expansion of the state space. 5 Automatic Belief Compression To allow for expansion of the state space, whilst keeping its size tractable for policy learning, we suggest replacing handcraft approaches with Automatic Belief Compression (ABC) techniques. We propose to use proven, principled statistical learning methods for automatically reducing the dimensionality of belief spaces, but which preserve the useful distributions within the full space. Two complementary methods that we are currently investigating are VDC (Poupart, 2005) and E-PCA (Roy and Gordon, 2002; Roy et al., 2005). These methods have been applied successfully in a real-time daily living assistant with over 106 states (St-Aubin et al., 2000; Hoey and Poupart, 2005; Poupart et al., 2006) and to robotic navigation by (Roy and Gordon, 2002; Roy et al., 2005). They: • reduce the dimensionality of state spaces that were previously intractable for POMDP solution methods, and • automatically compress the representation of belief space distributions to take advantage of sparsity between likely distributions. The tight coupling between some dialogue states and actions (e.g. a user’s goal state travel-from-London</context>
</contexts>
<marker>Roy, Gordon, Thrun, 2005</marker>
<rawString>N. Roy, G. Gordon, and S. Thrun. 2005. Finding Approximate POMDP Solutions Through Belief Compression. Artificial Intelligence Research, 22(1-40).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R St-Aubin</author>
<author>J Hoey</author>
<author>C Boutilier</author>
</authors>
<title>Approximate policy construction using decision diagrams.</title>
<date>2000</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="9542" citStr="St-Aubin et al., 2000" startWordPosition="1547" endWordPosition="1550">ate space, whilst keeping its size tractable for policy learning, we suggest replacing handcraft approaches with Automatic Belief Compression (ABC) techniques. We propose to use proven, principled statistical learning methods for automatically reducing the dimensionality of belief spaces, but which preserve the useful distributions within the full space. Two complementary methods that we are currently investigating are VDC (Poupart, 2005) and E-PCA (Roy and Gordon, 2002; Roy et al., 2005). These methods have been applied successfully in a real-time daily living assistant with over 106 states (St-Aubin et al., 2000; Hoey and Poupart, 2005; Poupart et al., 2006) and to robotic navigation by (Roy and Gordon, 2002; Roy et al., 2005). They: • reduce the dimensionality of state spaces that were previously intractable for POMDP solution methods, and • automatically compress the representation of belief space distributions to take advantage of sparsity between likely distributions. The tight coupling between some dialogue states and actions (e.g. a user’s goal state travel-from-London and system act confirm-from-London) has led some researchers to conclude that compression techniques, such as state aggregation</context>
</contexts>
<marker>St-Aubin, Hoey, Boutilier, 2000</marker>
<rawString>R. St-Aubin, J. Hoey, and C. Boutilier. 2000. Approximate policy construction using decision diagrams. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Thomson</author>
<author>S Young</author>
</authors>
<title>Bayesian update of dialogue state: A POMDP framework for spoken dialogue systems.</title>
<date>2010</date>
<journal>Computer Speech and Language,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="1147" citStr="Thomson and Young, 2010" startWordPosition="166" endWordPosition="169">r proposes the use of Automatic Belief Compression methods to remedy these problems. 1 Introduction One of the main problems for a spoken dialogue system is to determine the user’s goal (e.g. plan suitable meeting times or find a good Indian restaurant nearby) under uncertainty, and thereby to compute the optimal next system dialogue action (e.g. offer a restaurant, ask for clarification). Recent research in statistical spoken dialogue systems (SSDS) has successfully addressed aspects of these problems through the application of Partially Observable Markov Decision Process (POMDP) approaches (Thomson and Young, 2010; Young et al., 2010). However POMDP SSDS are currently limited by an impoverished representation of user goals adopted to enable tractable learning. Current POMDP SSDS state approximations make it impossible to represent some plausible user goals, e.g. someone who wants to know about nearby cheap restaurants and high-quality ones further away, or wants to schedule a meeting anytime this week except monday afternoon (also see Examples in Tables 1–3). This renders dialogue management sub-optimal and makes it impossible to deal adequately with the following types of user utterance: “I’m looking </context>
<context position="3334" citStr="Thomson and Young, 2010" startWordPosition="510" endWordPosition="514">of the state space should be dealt with by employing Automatic Belief Compression (ABC) techniques. 2 POMDP SSDS Partially Observable Markov Decision Processes (POMDPs) are Markov Decision Processes where the system’s state is only partially observable, i.e. there is uncertainty as to what the true state is. The ability to account for uncertainty is crucial for spoken dialogue systems because their knowledge about the state is uncertain due to speech recognition errors and the fact that the user’s goals are not directly observable. In POMDP models of spoken dialogue (Williams and Young, 2005; Thomson and Young, 2010; Young et al., 2010) the dialogue policy (what the system should say next) is based not on a single view of the current state of the conversation, but on a probability distribution over all possible states of the conversation. The optimal POMDP SSDS dialogue act thus automatically takes account of the uncertainty about the user’s utterances and goals. All work to date on POMDP SSDS has assumed (i) that a user has a singular, fully constrained, fixed goal and (ii) that there is one source of unProceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and D</context>
<context position="5212" citStr="Thomson and Young, 2010" startWordPosition="825" endWordPosition="828">1) also 2 Thai restaurants nearby. indicates that Italian food is dis-preferred.) U’3: Great. I’ll try Al Capone’s. U3: No Thanks. Bye. Table 1: State-of-the-art POMDP SSDS vs ABC-SSDS: Example dialogues 1 and 2. U=User, S=System certainty; errors in speech recognition. The traditional role of the Dialogue Manager (DM) is therefore one of reducing uncertainty until its belief is strongly focused on a single goal which strictly fits the user’s stated constraints. Both the policies that DMs currently follow and the design decisions implemented to make SDSS tasks learnable use these assumptions (Thomson and Young, 2010; Young et al., 2010). The resulting systems force users into selecting a single domain object as their goal in order to progress the dialogue. The example POMDP SSDS dialogues, Examples 1 and 3 (Tables 1 and 2), show the problems inherent in this approach. User goal: good Italian or good French food S1: How can I help you? U1: What restaurants are nearby? S2: There are 6, what type of food do you want? U2: Err... Italian or French S3: Do you want Italian or French food? U3: Uh. either. S4: Do you want Italian or French food? U4: Ok , how about French then. S5: There are 2 good French restaura</context>
</contexts>
<marker>Thomson, Young, 2010</marker>
<rawString>B. Thomson and S. Young. 2010. Bayesian update of dialogue state: A POMDP framework for spoken dialogue systems. Computer Speech and Language, 24(4):562–588.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Walker</author>
<author>S Whittaker</author>
<author>A Stent</author>
<author>P Maloor</author>
<author>J Moore</author>
<author>M Johnston</author>
<author>G Vasireddy</author>
</authors>
<title>User tailored generation in the match multimodal dialogue system.</title>
<date>2004</date>
<journal>Cognitive Science,</journal>
<pages>28--811</pages>
<contexts>
<context position="2120" citStr="Walker et al., 2004" startWordPosition="312" endWordPosition="315">s to schedule a meeting anytime this week except monday afternoon (also see Examples in Tables 1–3). This renders dialogue management sub-optimal and makes it impossible to deal adequately with the following types of user utterance: “I’m looking for French or Italian food”, or “Not Italian, unless it’s expensive”. User utterances with negations and disjunctions of Oliver Lemon Interaction Lab Heriot-Watt University Edinburgh, United Kingdom o.lemon@hw.ac.uk various sorts are very natural, and exploit the full power of natural language input. Moreover, work in dialogue system evaluation, e.g. (Walker et al., 2004; Lemon et al., 2006), shows that real user goals are generally sets of items with different features, rather than a single item. People like to explore possible trade offs between features of items. A central challenge for the field of spoken dialogue systems is therefore to: develop realistic large-scale statistical approaches with an accurate, extended representation of user goals. In this paper we propose that the independence assumptions that have guided POMDP SSDS design to date should be relaxed, user goal sets should be introduced and that the subsequent explosion in the size of the st</context>
<context position="11231" citStr="Walker et al., 2004" startWordPosition="1820" endWordPosition="1823">es not rule out compressibility of belief distributions. Finally, our introduction of sets for user goals should provide additional possibilities for compression. Our aim in applying ABC methods is to allow POMDP SSDS to handle the much larger state spaces that are required to achieve the expressiveness which we believe will be a real benefit to users. We plan to do this for real world tasks, e.g. a city search over 1000s of entities with an uncompressed belief space of the order of 108 x R. 6 Target Dialogues In general, when a user starts a dialogue they rarely have a singular goal in mind (Walker et al., 2004; Lemon et al., 2006). Their goal is not a fixed point in the domain but instead can be thought of as a (possibly disconnected) set of points, for example either a nearby cheap restaurant or highquality one further away. The set represents trade offs that the particular user is interested in. People rarely communicate their goals in terms of such distributions or trade offs, preferring to provide information in a piecemeal manner and thus incrementally explore the domain. In Examples 1–4 (Tables 1–3) we contrast the operation of a current state-of-the-art POMDP SSDS with our proposed ABC-SSDS </context>
<context position="13826" citStr="Walker et al., 2004" startWordPosition="2273" endWordPosition="2276">ls in Figure 1) to incorrectly represent the user’s goal after U2 “Not Italian, unless it’s really good” by ruling out all Italian restaurants5. The ABC-SSDS user is able to find the restaurant of their choice, whereas the POMDP SSDS user’s choice is artificially restricted, and they quit having failed to find a suitable item. The ABC-SSDS style of dialogue is clearly more efficient than that of current POMDP SSDS. It seems likely that users of such a system may also find the style of the conversation more natural, and may be more confident that their eventual choices really meet their goals (Walker et al., 2004). All of these hypotheses remain to be explored in our future empirical work. 7 Conclusion We present several problems for current POMDP approaches to spoken dialogue systems, concerning the representation of complex, but natural, user goals. We propose the development of principled automatic methods for dimensionality reduction, in place of the ad-hoc assumptions and handcrafted compressions currently used. In parallel we are also exploring: (i) what approaches are required for updating beliefs over sets in real time – in principle a method similar 5There are several ways to try to remedy thi</context>
</contexts>
<marker>Walker, Whittaker, Stent, Maloor, Moore, Johnston, Vasireddy, 2004</marker>
<rawString>M. Walker, S. Whittaker, A. Stent, P. Maloor, J. Moore, M. Johnston, and G. Vasireddy. 2004. User tailored generation in the match multimodal dialogue system. Cognitive Science, 28:811–840.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Williams</author>
<author>S Young</author>
</authors>
<title>Scaling Up POMDPs for Dialog Management: The ”Summary POMDP” Method. In</title>
<date>2005</date>
<booktitle>Proc. ASRU.</booktitle>
<contexts>
<context position="3309" citStr="Williams and Young, 2005" startWordPosition="506" endWordPosition="509">ent explosion in the size of the state space should be dealt with by employing Automatic Belief Compression (ABC) techniques. 2 POMDP SSDS Partially Observable Markov Decision Processes (POMDPs) are Markov Decision Processes where the system’s state is only partially observable, i.e. there is uncertainty as to what the true state is. The ability to account for uncertainty is crucial for spoken dialogue systems because their knowledge about the state is uncertain due to speech recognition errors and the fact that the user’s goals are not directly observable. In POMDP models of spoken dialogue (Williams and Young, 2005; Thomson and Young, 2010; Young et al., 2010) the dialogue policy (what the system should say next) is based not on a single view of the current state of the conversation, but on a probability distribution over all possible states of the conversation. The optimal POMDP SSDS dialogue act thus automatically takes account of the uncertainty about the user’s utterances and goals. All work to date on POMDP SSDS has assumed (i) that a user has a singular, fully constrained, fixed goal and (ii) that there is one source of unProceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest</context>
<context position="7062" citStr="Williams and Young, 2005" startWordPosition="1156" endWordPosition="1159">ief space is a probability distribution over all these dialogue states, i.e. a 326, 592 dimensional real valued (R) space. In order to render such large belief spaces tractable, the current state of the art in POMDP SSDS uses a variety of handcrafted compression techniques, such as making several types of independence assumption. For example, by assuming that users are only ever interested in one type of food or one location, and that their interests in food type, price range, quality, etc. are independent, the 326, 592 real valued state space can be reduced to a much smaller “summary space” (Williams and Young, 2005) consisting of, say, 4 x R values2. See Figure 1 for a graphical depiction of such assumptions3. As illustrated by Figure 1 the information lost due to the independence assumptions mean that these approaches are unable to support conversations such as that shown in Example 2 (Table 1). 4 Sets of User Goals Getting rid of independence assumptions allows the DM to reason and ask questions about the user’s requirements in a more rational way. It can, for example distinguish between the user wanting “excellent Italian” or “any Thai” versus only “excellent” restaurants – see Figure 1. However, the </context>
</contexts>
<marker>Williams, Young, 2005</marker>
<rawString>J. Williams and S. Young. 2005. Scaling Up POMDPs for Dialog Management: The ”Summary POMDP” Method. In Proc. ASRU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Williams</author>
<author>S Young</author>
</authors>
<title>Scaling POMDPs for spoken dialog management.</title>
<date>2007</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing,</journal>
<volume>15</volume>
<issue>7</issue>
<pages>2129</pages>
<contexts>
<context position="10208" citStr="Williams and Young, 2007" startWordPosition="1647" endWordPosition="1650"> 2006) and to robotic navigation by (Roy and Gordon, 2002; Roy et al., 2005). They: • reduce the dimensionality of state spaces that were previously intractable for POMDP solution methods, and • automatically compress the representation of belief space distributions to take advantage of sparsity between likely distributions. The tight coupling between some dialogue states and actions (e.g. a user’s goal state travel-from-London and system act confirm-from-London) has led some researchers to conclude that compression techniques, such as state aggregation, are not useful in the dialogue domain (Williams and Young, 2007). However, such tight coupling may not exist for all states, indeed VDC has already been applied to a small spoken dialogue system problem (Poupart, 2005) where it was shown that compressions could be found without losing any information4. Further, for POMDP approaches the state is not the dialogue state but the belief distribution over dialogue states. Incompressibility at the dialogue state level does not rule out compressibility of belief distributions. Finally, our introduction of sets for user goals should provide additional possibilities for compression. Our aim in applying ABC methods i</context>
</contexts>
<marker>Williams, Young, 2007</marker>
<rawString>J. Williams and S. Young. 2007. Scaling POMDPs for spoken dialog management. IEEE Transactions on Audio, Speech, and Language Processing, 15(7):2116 –2129, Sept.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Young</author>
<author>M Gaˇsi´c</author>
<author>S Keizer</author>
<author>F Mairesse</author>
<author>B Thomson</author>
<author>K Yu</author>
</authors>
<title>The Hidden Information State model: a practical framework for POMDP based spoken dialogue management.</title>
<date>2010</date>
<journal>Computer Speech and Language,</journal>
<volume>24</volume>
<issue>2</issue>
<marker>Young, Gaˇsi´c, Keizer, Mairesse, Thomson, Yu, 2010</marker>
<rawString>S. Young, M. Gaˇsi´c, S. Keizer, F. Mairesse, B. Thomson, and K. Yu. 2010. The Hidden Information State model: a practical framework for POMDP based spoken dialogue management. Computer Speech and Language, 24(2):150–174.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>