<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.021081">
<title confidence="0.998161">
Extending the METEOR Machine Translation Evaluation Metric to the
Phrase Level
</title>
<author confidence="0.987484">
Michael Denkowski and Alon Lavie
</author>
<affiliation confidence="0.91094575">
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15232, USA
</affiliation>
<email confidence="0.999206">
{mdenkows,alavie}@cs.cmu.edu
</email>
<sectionHeader confidence="0.998509" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.968982428571429">
This paper presents METEOR-NEXT, an ex-
tended version of the METEOR metric de-
signed to have high correlation with post-
editing measures of machine translation qual-
ity. We describe changes made to the met-
ric’s sentence aligner and scoring scheme as
well as a method for tuning the metric’s pa-
rameters to optimize correlation with human-
targeted Translation Edit Rate (HTER). We
then show that METEOR-NEXT improves cor-
relation with HTER over baseline metrics, in-
cluding earlier versions of METEOR, and ap-
proaches the correlation level of a state-of-the-
art metric, TER-plus (TERp).
</bodyText>
<sectionHeader confidence="0.99948" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999969695652174">
Recent focus on the need for accurate automatic
metrics for evaluating the quality of machine trans-
lation output has spurred much development in the
field of MT. Workshops such as WMT09 (Callison-
Burch et al., 2009) and the MetricsMATR08 chal-
lenge (Przybocki et al., 2008) encourage the devel-
opment of new MT metrics and reliable human judg-
ment tasks.
This paper describes our work extending the ME-
TEOR metric to improve correlation with human-
targeted Translation Edit Rate (HTER) (Snover et
al., 2006), a semi-automatic post-editing based met-
ric which measures the distance between MT out-
put and a targeted reference. We identify several
limitations of the original METEOR metric and de-
scribe our modifications to improve performance on
this task. Our extended metric, METEOR-NEXT, is
then tuned to maximize segment-level correlation
with HTER scores and tested against several base-
line metrics. We show that METEOR-NEXT outper-
forms earlier versions of METEOR when tuned to the
same HTER data and approaches the performance of
a state-of-the-art TER-based metric, TER-plus.
</bodyText>
<sectionHeader confidence="0.974564" genericHeader="method">
2 The METEOR-NEXT Metric
</sectionHeader>
<subsectionHeader confidence="0.835358">
2.1 Traditional METEOR Scoring
</subsectionHeader>
<bodyText confidence="0.999670916666667">
Given a machine translation hypothesis and a refer-
ence translation, the traditional METEOR metric cal-
culates a lexical similarity score based on a word-
to-word alignment between the two strings (Baner-
jee and Lavie, 2005). When multiple references are
available, the hypothesis is scored against each and
the reference producing the highest score is used.
Alignments are built incrementally in a series of
stages using the following METEOR matchers:
Exact: Words are matched if and only if their sur-
face forms are identical.
Stem: Words are stemmed using a language-
appropriate Snowball Stemmer (Porter, 2001) and
matched if the stems are identical.
Synonym: Words are matched if they are both
members of a synonym set according to the Word-
Net (Miller and Fellbaum, 2007) database. This
matcher is limited to translations into English.
At each stage, one of the above matchers iden-
tifies all possible word matches between the two
translations using words not aligned in previous
stages. An alignment is then identified as the largest
subset of these matches in which every word in each
sentence aligns to zero or one words in the other sen-
</bodyText>
<page confidence="0.910525">
250
</page>
<subsubsectionHeader confidence="0.582894">
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 250–253,
</subsubsectionHeader>
<subsectionHeader confidence="0.276584">
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.999148571428572">
tence. If multiple such alignments exist, the align-
ment is chosen that best preserves word order by
having the fewest crossing alignment links. At the
end of each stage, matched words are fixed so that
they are not considered in future stages. The final
alignment is defined as the union of all stage align-
ments.
Once an alignment has been constructed, the to-
tal number of unigram matches (m), the number of
words in the hypothesis (t), and the number of words
in the reference (r) are used to calculate precision
(P = m/t) and recall (R = m/r). The parame-
terized harmonic mean of P and R (van Rijsbergen,
1979) is then calculated:
</bodyText>
<equation confidence="0.610515">
Fmean = a · P + (1 − a) · R
</equation>
<bodyText confidence="0.999736333333333">
To account for differences in word order, the min-
imum number of “chunks” (ch) is calculated where a
chunk is defined as a series of matched unigrams that
is contiguous and identically ordered in both sen-
tences. The fragmentation (frag = ch/m) is then
used to calculate a fragmentation penalty:
</bodyText>
<equation confidence="0.820516333333333">
Pen = -y · fraga
The final METEOR score is then calculated:
Score = (1 − Pen) · Fmean
</equation>
<bodyText confidence="0.999817666666667">
The free parameters a, Q, and -y can be tuned to
maximize correlation with various types of human
judgments (Lavie and Agarwal, 2007).
</bodyText>
<subsectionHeader confidence="0.999524">
2.2 Extending the METEOR Aligner
</subsectionHeader>
<bodyText confidence="0.999182769230769">
Traditional METEOR is limited to unigram matches,
making it strictly a word-level metric. By focus-
ing on only one match type per stage, the aligner
misses a significant part of the possible alignment
space. Further, selecting partial alignments based
only on the fewest number of per-stage crossing
alignment links can in practice lead to missing full
alignments with the same number of matches in
fewer chunks. Our extended aligner addresses these
limitations by introducing support for multiple-word
phrase matches and considering all possible matches
in a single alignment stage.
We introduce an additional paraphrase matcher
which matches phrases (one or more successive
words) if one phrase is considered a paraphrase of
the other by a paraphrase database. For English, we
use the paraphrase database developed by Snover et
al. (2009), using techniques presented by Bannard
and Callison-Burch (2005).
The extended aligner first constructs a search
space by applying all matchers in sequence to iden-
tify all possible matches between the hypothesis and
reference. To reduce redundant matches, stem and
synonym matches between pairs of words which
have already been identified as exact matches are not
considered. Matches have start positions and lengths
in both sentences; a word occurring less than length
positions after a match start is said to be covered by
the match. As exact, stem, and synonym matches
will always have length one in both sentences, they
can be considered phrase matches of length one.
Since other matches can cover phrases of different
lengths in the two sentences, matches are now said
to be one-to-one at the phrase level rather than the
word level.
Once all possible matches have been identified,
the aligner identifies the final alignment as the
largest subset of these matches meeting the follow-
ing criteria in order of importance:
</bodyText>
<listItem confidence="0.998199333333333">
1. Each word in each sentence is covered by zero
or one matches
2. Largest number of covered words across both
sentences
3. Smallest number of chunks, where a chunk is
now defined as a series of matched phrases that
is contiguous and identically ordered in both
sentences
4. Smallest sum of absolute distances between
match start positions in the two sentences (pre-
fer to align words and phrases that occur at sim-
ilar positions in both sentences)
</listItem>
<bodyText confidence="0.9985815">
The resulting alignment is selected from the full
space of possible alignments and directly optimizes
the statistics on which the the final score will be cal-
culated.
</bodyText>
<subsectionHeader confidence="0.874226">
2.3 Extended METEOR Scoring
</subsectionHeader>
<bodyText confidence="0.963913">
Once an alignment has been chosen, the METEOR-
NEXT score is calculated using extended versions of
</bodyText>
<equation confidence="0.920032">
P · R
</equation>
<page confidence="0.980491">
251
</page>
<bodyText confidence="0.989311545454546">
the traditional METEOR statistics. We also introduce
a tunable weight vector used to dictate the relative
contribution of each match type. The extended ME-
TEOR score is calculated as follows.
The number of words in the hypothesis (t) and
reference (r) are counted. For each of the match-
ers (mi), count the number of words covered by
matches of this type in the hypothesis (mi(t)) and
reference (mi(r)) and apply the appropriate module
weight (wi). The weighted Precision and Recall are
then calculated:
</bodyText>
<equation confidence="0.9935055">
_ �i wi - mi(t)
P t
</equation>
<bodyText confidence="0.9991965">
The minimum number of chunks (ch) is then cal-
culated using the new chunk definition. Once P, R,
and ch are calculated, the remaining statistics and
final score can be calculated as in Section 2.1.
</bodyText>
<sectionHeader confidence="0.974903" genericHeader="method">
3 Tuning for Post-Editing Measures of
Quality
</sectionHeader>
<bodyText confidence="0.999023384615385">
Human-targeted Translation Edit Rate (HTER)
(Snover et al., 2006), is a semi-automatic assessment
of machine translation quality based on the number
of edits required to correct translation hypotheses. A
human annotator edits each MT hypothesis so that it
is meaning-equivalent with a reference translation,
with an emphasis on making the minimum possible
number of edits. The Translation Edit Rate (TER)
is then calculated using the human-edited transla-
tion as a targeted reference for the MT hypothe-
sis. The resulting scores are shown to correlate well
with other types of human judgments (Snover et al.,
2006).
</bodyText>
<subsectionHeader confidence="0.99974">
3.1 Tuning Toward HTER
</subsectionHeader>
<bodyText confidence="0.999883">
The GALE (Olive, 2005) Phase 2 unsequestered
data includes HTER scores for multiple Arabic-to-
English and Chinese-to-English MT systems. We
used HTER scores for 10838 segments from 1045
documents from this data set to tune both the orig-
inal METEOR and METEOR-NEXT. Both were ex-
haustively tuned to maximize the length-weighted
segment-level Pearson’s correlation with the HTER
scores. This produced globally optimal α, Q, and -y
values for METEOR and optimal α, Q, -y values plus
stem, synonym, and paraphrase match weights for
</bodyText>
<table confidence="0.993374857142857">
Task α Q -y
Adequacy &amp; Fluency 0.81 0.83 0.28
Ranking 0.95 0.50 0.50
HTER 0.70 1.95 0.50
HTER (extended) 0.65 1.95 0.45
Stem Syn Par
0 0.4 0.9
</table>
<tableCaption confidence="0.9938265">
Table 1: Parameter values for various METEOR tasks for
translations into English.
</tableCaption>
<bodyText confidence="0.999282">
METEOR-NEXT (with the weight of exact matches
fixed at 1). Table 1 compares the new HTER pa-
rameters to those tuned for other tasks including ad-
equacy and fluency (Lavie and Agarwal, 2007) and
ranking (Agarwal and Lavie, 2008).
As observed by Snover et al. (2009), HTER
prefers metrics which are more balanced between
precision and recall: this results in the lowest values
of α for any task. Additionally, non-exact matches
receive lower weights, with stem matches receiving
zero weight. This reflects a weakness in HTER scor-
ing where words with matching stems are treated as
completely dissimilar, requiring full word substitu-
tions (Snover et al., 2006).
</bodyText>
<sectionHeader confidence="0.999383" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9999455">
The GALE (Olive, 2005) Phase 3 unsequestered
data includes HTER scores for Arabic-to-English
MT output. We created a test set from HTER scores
of 2245 segments from 195 documents in this data
set. Our evaluation metric (METEOR-NEXT-hter)
was tested against the following established metrics:
BLEU (Papineni et al., 2002) with a maximum N-
gram length of 4, TER (Snover et al., 2006), versions
of METEOR based on release 0.7 tuned for adequacy
and fluency (METEOR-0.7-af) (Lavie and Agarwal,
2007), ranking (METEOR-0.7-rank) (Agarwal and
Lavie, 2008), and HTER (METEOR-0.7-hter). Also
included is the HTER-tuned version of TER-plus
(TERp-hter), a metric with state-of-the-art perfor-
mance in recent evaluations (Snover et al., 2009).
Length-weighted Pearson’s and Spearman’s correla-
tion are shown for all metrics at both the segment
(Table 2) and document level (Table 3). System level
correlations are not shown as the Phase 3 data only
contained the output of 2 systems.
</bodyText>
<equation confidence="0.992929">
R = �i wi - mi (r)
r
</equation>
<page confidence="0.993212">
252
</page>
<table confidence="0.999715">
Metric Pearson’s r Spearman’s p
BLEU-4 -0.496 -0.510
TER 0.539 0.510
METEOR-0.7-af -0.573 -0.561
METEOR-0.7-rank -0.561 -0.556
METEOR-0.7-hter -0.574 -0.562
METEOR-NEXT-hter -0.600 -0.581
TERp-hter 0.627 0.610
</table>
<tableCaption confidence="0.979695">
Table 2: Segment level correlation with HTER.
</tableCaption>
<table confidence="0.99986175">
Metric Pearson’s r Spearman’s p
BLEU-4 -0.689 -0.686
TER 0.675 0.679
METEOR-0.7-af -0.696 -0.699
METEOR-0.7-rank -0.691 -0.693
METEOR-0.7-hter -0.704 -0.705
METEOR-NEXT-hter -0.719 -0.713
TERp-hter 0.738 0.747
</table>
<tableCaption confidence="0.999757">
Table 3: Document level correlation with HTER.
</tableCaption>
<bodyText confidence="0.9991994">
METEOR-NEXT-hter outperforms all baseline
metrics at both the segment and document level.
Bootstrap sampling indicates that the segment-level
correlation improvements of 0.026 in Pearson’s r
and 0.019 in Spearman’s p over METEOR-0.7-hter
are statistically significant at the 95% level. TERp’s
correlation with HTER is still significantly higher
across all categories. Our metric does run signifi-
cantly faster than TERp, scoring approximately 120
segments per second to TERp’s 3.8.
</bodyText>
<sectionHeader confidence="0.999512" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.99998575">
We have presented an extended METEOR metric
which shows higher correlation with HTER than
baseline metrics, including traditional METEOR
tuned on the same data. Our extensions are not
specific to HTER tasks; improved alignments and
additional features should improve performance on
any task having sufficient tuning data. Although our
metric does not outperform TERp, it should be noted
that HTER incorporates TER alignments, providing
TER-based metrics a natural advantage. Our metric
also scores segments relatively quickly, making it a
viable choice for tuning MT systems.
</bodyText>
<sectionHeader confidence="0.99557" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9967225">
This work was funded in part by NSF grants IIS-
0534932 and IIS-0915327.
</bodyText>
<sectionHeader confidence="0.995161" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998439212765958">
Abhaya Agarwal and Alon Lavie. 2008. Meteor, m-bleu
and m-ter: Evaluation Metrics for High-Correlation
with Human Rankings of Machine Translation Output.
In Proc. of WMT08, pages 115–118.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Proc.
of the ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization, pages 65–72.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proc. of
ACL05, pages 597–604.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In Proc.
of WMT09, pages 1–28.
Alon Lavie and Abhaya Agarwal. 2007. METEOR: An
Automatic Metric for MT Evaluation with High Lev-
els of Correlation with Human Judgments. In Proc. of
WMT07, pages 228–231.
George Miller and Christiane Fellbaum. 2007. WordNet.
http://wordnet.princeton.edu/.
Joseph Olive. 2005. Global Autonomous Language Ex-
ploitation (GALE). DARPA/IPTO Proposer Informa-
tion Pamphlet.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proc. of ACL02,
pages 311–318.
Martin Porter. 2001. Snowball: A language for stem-
ming algorithms. http://snowball.tartarus.org/texts/.
M. Przybocki, K. Peterson, and S Bronsart. 2008.
Official results of the NIST 2008 &amp;quot;Metrics for
MAchine TRanslation&amp;quot; Challenge (MetricsMATR08).
http://nist.gov/speech/tests/metricsmatr/2008/results/.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Proc. of AMTA-2006, pages 223–231.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, Adequacy, or
HTER? Exploring Different Human Judgments with a
Tunable MT Metric. In Proc. of WMT09, pages 259–
268.
C. van Rijsbergen, 1979. Information Retrieval, chap-
ter 7. 2nd edition.
</reference>
<page confidence="0.998936">
253
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.527929">
<title confidence="0.790727">the Translation Evaluation Metric to the Phrase Level Denkowski Language Technologies</title>
<affiliation confidence="0.9968145">School of Computer Carnegie Mellon</affiliation>
<address confidence="0.955061">Pittsburgh, PA 15232,</address>
<email confidence="0.998992">mdenkows@cs.cmu.edu</email>
<email confidence="0.998992">alavie@cs.cmu.edu</email>
<abstract confidence="0.9958906">paper presents an exversion of the designed to have high correlation with postediting measures of machine translation quality. We describe changes made to the metric’s sentence aligner and scoring scheme as well as a method for tuning the metric’s parameters to optimize correlation with humantargeted Translation Edit Rate (HTER). We show that correlation with HTER over baseline metrics, inearlier versions of and approaches the correlation level of a state-of-theart metric, TER-plus (TERp).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Abhaya Agarwal</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor, m-bleu and m-ter: Evaluation Metrics for High-Correlation with Human Rankings of Machine Translation Output.</title>
<date>2008</date>
<booktitle>In Proc. of WMT08,</booktitle>
<pages>115--118</pages>
<contexts>
<context position="9519" citStr="Agarwal and Lavie, 2008" startWordPosition="1555" endWordPosition="1558">relation with the HTER scores. This produced globally optimal α, Q, and -y values for METEOR and optimal α, Q, -y values plus stem, synonym, and paraphrase match weights for Task α Q -y Adequacy &amp; Fluency 0.81 0.83 0.28 Ranking 0.95 0.50 0.50 HTER 0.70 1.95 0.50 HTER (extended) 0.65 1.95 0.45 Stem Syn Par 0 0.4 0.9 Table 1: Parameter values for various METEOR tasks for translations into English. METEOR-NEXT (with the weight of exact matches fixed at 1). Table 1 compares the new HTER parameters to those tuned for other tasks including adequacy and fluency (Lavie and Agarwal, 2007) and ranking (Agarwal and Lavie, 2008). As observed by Snover et al. (2009), HTER prefers metrics which are more balanced between precision and recall: this results in the lowest values of α for any task. Additionally, non-exact matches receive lower weights, with stem matches receiving zero weight. This reflects a weakness in HTER scoring where words with matching stems are treated as completely dissimilar, requiring full word substitutions (Snover et al., 2006). 4 Experiments The GALE (Olive, 2005) Phase 3 unsequestered data includes HTER scores for Arabic-to-English MT output. We created a test set from HTER scores of 2245 segm</context>
</contexts>
<marker>Agarwal, Lavie, 2008</marker>
<rawString>Abhaya Agarwal and Alon Lavie. 2008. Meteor, m-bleu and m-ter: Evaluation Metrics for High-Correlation with Human Rankings of Machine Translation Output. In Proc. of WMT08, pages 115–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments.</title>
<date>2005</date>
<booktitle>In Proc. of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>65--72</pages>
<contexts>
<context position="2202" citStr="Banerjee and Lavie, 2005" startWordPosition="334" endWordPosition="338">rformance on this task. Our extended metric, METEOR-NEXT, is then tuned to maximize segment-level correlation with HTER scores and tested against several baseline metrics. We show that METEOR-NEXT outperforms earlier versions of METEOR when tuned to the same HTER data and approaches the performance of a state-of-the-art TER-based metric, TER-plus. 2 The METEOR-NEXT Metric 2.1 Traditional METEOR Scoring Given a machine translation hypothesis and a reference translation, the traditional METEOR metric calculates a lexical similarity score based on a wordto-word alignment between the two strings (Banerjee and Lavie, 2005). When multiple references are available, the hypothesis is scored against each and the reference producing the highest score is used. Alignments are built incrementally in a series of stages using the following METEOR matchers: Exact: Words are matched if and only if their surface forms are identical. Stem: Words are stemmed using a languageappropriate Snowball Stemmer (Porter, 2001) and matched if the stems are identical. Synonym: Words are matched if they are both members of a synonym set according to the WordNet (Miller and Fellbaum, 2007) database. This matcher is limited to translations </context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In Proc. of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>In Proc. of ACL05,</booktitle>
<pages>597--604</pages>
<contexts>
<context position="5433" citStr="Bannard and Callison-Burch (2005)" startWordPosition="875" endWordPosition="878">umber of per-stage crossing alignment links can in practice lead to missing full alignments with the same number of matches in fewer chunks. Our extended aligner addresses these limitations by introducing support for multiple-word phrase matches and considering all possible matches in a single alignment stage. We introduce an additional paraphrase matcher which matches phrases (one or more successive words) if one phrase is considered a paraphrase of the other by a paraphrase database. For English, we use the paraphrase database developed by Snover et al. (2009), using techniques presented by Bannard and Callison-Burch (2005). The extended aligner first constructs a search space by applying all matchers in sequence to identify all possible matches between the hypothesis and reference. To reduce redundant matches, stem and synonym matches between pairs of words which have already been identified as exact matches are not considered. Matches have start positions and lengths in both sentences; a word occurring less than length positions after a match start is said to be covered by the match. As exact, stem, and synonym matches will always have length one in both sentences, they can be considered phrase matches of leng</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In Proc. of ACL05, pages 597–604.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<date>2009</date>
<booktitle>Findings of the 2009 Workshop on Statistical Machine Translation. In Proc. of WMT09,</booktitle>
<pages>1--28</pages>
<marker>Callison-Burch, Koehn, Monz, Schroeder, 2009</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Josh Schroeder. 2009. Findings of the 2009 Workshop on Statistical Machine Translation. In Proc. of WMT09, pages 1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Abhaya Agarwal</author>
</authors>
<title>METEOR: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments.</title>
<date>2007</date>
<booktitle>In Proc. of WMT07,</booktitle>
<pages>228--231</pages>
<contexts>
<context position="4494" citStr="Lavie and Agarwal, 2007" startWordPosition="732" endWordPosition="735"> parameterized harmonic mean of P and R (van Rijsbergen, 1979) is then calculated: Fmean = a · P + (1 − a) · R To account for differences in word order, the minimum number of “chunks” (ch) is calculated where a chunk is defined as a series of matched unigrams that is contiguous and identically ordered in both sentences. The fragmentation (frag = ch/m) is then used to calculate a fragmentation penalty: Pen = -y · fraga The final METEOR score is then calculated: Score = (1 − Pen) · Fmean The free parameters a, Q, and -y can be tuned to maximize correlation with various types of human judgments (Lavie and Agarwal, 2007). 2.2 Extending the METEOR Aligner Traditional METEOR is limited to unigram matches, making it strictly a word-level metric. By focusing on only one match type per stage, the aligner misses a significant part of the possible alignment space. Further, selecting partial alignments based only on the fewest number of per-stage crossing alignment links can in practice lead to missing full alignments with the same number of matches in fewer chunks. Our extended aligner addresses these limitations by introducing support for multiple-word phrase matches and considering all possible matches in a single</context>
<context position="9481" citStr="Lavie and Agarwal, 2007" startWordPosition="1549" endWordPosition="1552">h-weighted segment-level Pearson’s correlation with the HTER scores. This produced globally optimal α, Q, and -y values for METEOR and optimal α, Q, -y values plus stem, synonym, and paraphrase match weights for Task α Q -y Adequacy &amp; Fluency 0.81 0.83 0.28 Ranking 0.95 0.50 0.50 HTER 0.70 1.95 0.50 HTER (extended) 0.65 1.95 0.45 Stem Syn Par 0 0.4 0.9 Table 1: Parameter values for various METEOR tasks for translations into English. METEOR-NEXT (with the weight of exact matches fixed at 1). Table 1 compares the new HTER parameters to those tuned for other tasks including adequacy and fluency (Lavie and Agarwal, 2007) and ranking (Agarwal and Lavie, 2008). As observed by Snover et al. (2009), HTER prefers metrics which are more balanced between precision and recall: this results in the lowest values of α for any task. Additionally, non-exact matches receive lower weights, with stem matches receiving zero weight. This reflects a weakness in HTER scoring where words with matching stems are treated as completely dissimilar, requiring full word substitutions (Snover et al., 2006). 4 Experiments The GALE (Olive, 2005) Phase 3 unsequestered data includes HTER scores for Arabic-to-English MT output. We created a </context>
</contexts>
<marker>Lavie, Agarwal, 2007</marker>
<rawString>Alon Lavie and Abhaya Agarwal. 2007. METEOR: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments. In Proc. of WMT07, pages 228–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
<author>Christiane Fellbaum</author>
</authors>
<date>2007</date>
<note>WordNet. http://wordnet.princeton.edu/.</note>
<contexts>
<context position="2751" citStr="Miller and Fellbaum, 2007" startWordPosition="424" endWordPosition="427">on a wordto-word alignment between the two strings (Banerjee and Lavie, 2005). When multiple references are available, the hypothesis is scored against each and the reference producing the highest score is used. Alignments are built incrementally in a series of stages using the following METEOR matchers: Exact: Words are matched if and only if their surface forms are identical. Stem: Words are stemmed using a languageappropriate Snowball Stemmer (Porter, 2001) and matched if the stems are identical. Synonym: Words are matched if they are both members of a synonym set according to the WordNet (Miller and Fellbaum, 2007) database. This matcher is limited to translations into English. At each stage, one of the above matchers identifies all possible word matches between the two translations using words not aligned in previous stages. An alignment is then identified as the largest subset of these matches in which every word in each sentence aligns to zero or one words in the other sen250 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 250–253, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics tence. If multiple such alignm</context>
</contexts>
<marker>Miller, Fellbaum, 2007</marker>
<rawString>George Miller and Christiane Fellbaum. 2007. WordNet. http://wordnet.princeton.edu/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Olive</author>
</authors>
<title>Global Autonomous Language Exploitation (GALE). DARPA/IPTO Proposer Information Pamphlet.</title>
<date>2005</date>
<contexts>
<context position="8565" citStr="Olive, 2005" startWordPosition="1398" endWordPosition="1399">over et al., 2006), is a semi-automatic assessment of machine translation quality based on the number of edits required to correct translation hypotheses. A human annotator edits each MT hypothesis so that it is meaning-equivalent with a reference translation, with an emphasis on making the minimum possible number of edits. The Translation Edit Rate (TER) is then calculated using the human-edited translation as a targeted reference for the MT hypothesis. The resulting scores are shown to correlate well with other types of human judgments (Snover et al., 2006). 3.1 Tuning Toward HTER The GALE (Olive, 2005) Phase 2 unsequestered data includes HTER scores for multiple Arabic-toEnglish and Chinese-to-English MT systems. We used HTER scores for 10838 segments from 1045 documents from this data set to tune both the original METEOR and METEOR-NEXT. Both were exhaustively tuned to maximize the length-weighted segment-level Pearson’s correlation with the HTER scores. This produced globally optimal α, Q, and -y values for METEOR and optimal α, Q, -y values plus stem, synonym, and paraphrase match weights for Task α Q -y Adequacy &amp; Fluency 0.81 0.83 0.28 Ranking 0.95 0.50 0.50 HTER 0.70 1.95 0.50 HTER (e</context>
<context position="9986" citStr="Olive, 2005" startWordPosition="1631" endWordPosition="1632">new HTER parameters to those tuned for other tasks including adequacy and fluency (Lavie and Agarwal, 2007) and ranking (Agarwal and Lavie, 2008). As observed by Snover et al. (2009), HTER prefers metrics which are more balanced between precision and recall: this results in the lowest values of α for any task. Additionally, non-exact matches receive lower weights, with stem matches receiving zero weight. This reflects a weakness in HTER scoring where words with matching stems are treated as completely dissimilar, requiring full word substitutions (Snover et al., 2006). 4 Experiments The GALE (Olive, 2005) Phase 3 unsequestered data includes HTER scores for Arabic-to-English MT output. We created a test set from HTER scores of 2245 segments from 195 documents in this data set. Our evaluation metric (METEOR-NEXT-hter) was tested against the following established metrics: BLEU (Papineni et al., 2002) with a maximum Ngram length of 4, TER (Snover et al., 2006), versions of METEOR based on release 0.7 tuned for adequacy and fluency (METEOR-0.7-af) (Lavie and Agarwal, 2007), ranking (METEOR-0.7-rank) (Agarwal and Lavie, 2008), and HTER (METEOR-0.7-hter). Also included is the HTER-tuned version of TE</context>
</contexts>
<marker>Olive, 2005</marker>
<rawString>Joseph Olive. 2005. Global Autonomous Language Exploitation (GALE). DARPA/IPTO Proposer Information Pamphlet.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL02,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="10284" citStr="Papineni et al., 2002" startWordPosition="1674" endWordPosition="1677">lues of α for any task. Additionally, non-exact matches receive lower weights, with stem matches receiving zero weight. This reflects a weakness in HTER scoring where words with matching stems are treated as completely dissimilar, requiring full word substitutions (Snover et al., 2006). 4 Experiments The GALE (Olive, 2005) Phase 3 unsequestered data includes HTER scores for Arabic-to-English MT output. We created a test set from HTER scores of 2245 segments from 195 documents in this data set. Our evaluation metric (METEOR-NEXT-hter) was tested against the following established metrics: BLEU (Papineni et al., 2002) with a maximum Ngram length of 4, TER (Snover et al., 2006), versions of METEOR based on release 0.7 tuned for adequacy and fluency (METEOR-0.7-af) (Lavie and Agarwal, 2007), ranking (METEOR-0.7-rank) (Agarwal and Lavie, 2008), and HTER (METEOR-0.7-hter). Also included is the HTER-tuned version of TER-plus (TERp-hter), a metric with state-of-the-art performance in recent evaluations (Snover et al., 2009). Length-weighted Pearson’s and Spearman’s correlation are shown for all metrics at both the segment (Table 2) and document level (Table 3). System level correlations are not shown as the Phas</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proc. of ACL02, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Porter</author>
</authors>
<title>Snowball: A language for stemming algorithms.</title>
<date>2001</date>
<note>http://snowball.tartarus.org/texts/.</note>
<contexts>
<context position="2589" citStr="Porter, 2001" startWordPosition="397" endWordPosition="398">coring Given a machine translation hypothesis and a reference translation, the traditional METEOR metric calculates a lexical similarity score based on a wordto-word alignment between the two strings (Banerjee and Lavie, 2005). When multiple references are available, the hypothesis is scored against each and the reference producing the highest score is used. Alignments are built incrementally in a series of stages using the following METEOR matchers: Exact: Words are matched if and only if their surface forms are identical. Stem: Words are stemmed using a languageappropriate Snowball Stemmer (Porter, 2001) and matched if the stems are identical. Synonym: Words are matched if they are both members of a synonym set according to the WordNet (Miller and Fellbaum, 2007) database. This matcher is limited to translations into English. At each stage, one of the above matchers identifies all possible word matches between the two translations using words not aligned in previous stages. An alignment is then identified as the largest subset of these matches in which every word in each sentence aligns to zero or one words in the other sen250 Human Language Technologies: The 2010 Annual Conference of the Nor</context>
</contexts>
<marker>Porter, 2001</marker>
<rawString>Martin Porter. 2001. Snowball: A language for stemming algorithms. http://snowball.tartarus.org/texts/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Przybocki</author>
<author>K Peterson</author>
<author>S Bronsart</author>
</authors>
<title>Metrics for MAchine TRanslation&amp;quot; Challenge (MetricsMATR08).</title>
<date>2008</date>
<booktitle>Official results of the NIST</booktitle>
<note>http://nist.gov/speech/tests/metricsmatr/2008/results/.</note>
<contexts>
<context position="1124" citStr="Przybocki et al., 2008" startWordPosition="168" endWordPosition="171">eme as well as a method for tuning the metric’s parameters to optimize correlation with humantargeted Translation Edit Rate (HTER). We then show that METEOR-NEXT improves correlation with HTER over baseline metrics, including earlier versions of METEOR, and approaches the correlation level of a state-of-theart metric, TER-plus (TERp). 1 Introduction Recent focus on the need for accurate automatic metrics for evaluating the quality of machine translation output has spurred much development in the field of MT. Workshops such as WMT09 (CallisonBurch et al., 2009) and the MetricsMATR08 challenge (Przybocki et al., 2008) encourage the development of new MT metrics and reliable human judgment tasks. This paper describes our work extending the METEOR metric to improve correlation with humantargeted Translation Edit Rate (HTER) (Snover et al., 2006), a semi-automatic post-editing based metric which measures the distance between MT output and a targeted reference. We identify several limitations of the original METEOR metric and describe our modifications to improve performance on this task. Our extended metric, METEOR-NEXT, is then tuned to maximize segment-level correlation with HTER scores and tested against s</context>
</contexts>
<marker>Przybocki, Peterson, Bronsart, 2008</marker>
<rawString>M. Przybocki, K. Peterson, and S Bronsart. 2008. Official results of the NIST 2008 &amp;quot;Metrics for MAchine TRanslation&amp;quot; Challenge (MetricsMATR08). http://nist.gov/speech/tests/metricsmatr/2008/results/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>In Proc. of AMTA-2006,</booktitle>
<pages>223--231</pages>
<contexts>
<context position="1354" citStr="Snover et al., 2006" startWordPosition="206" endWordPosition="209"> versions of METEOR, and approaches the correlation level of a state-of-theart metric, TER-plus (TERp). 1 Introduction Recent focus on the need for accurate automatic metrics for evaluating the quality of machine translation output has spurred much development in the field of MT. Workshops such as WMT09 (CallisonBurch et al., 2009) and the MetricsMATR08 challenge (Przybocki et al., 2008) encourage the development of new MT metrics and reliable human judgment tasks. This paper describes our work extending the METEOR metric to improve correlation with humantargeted Translation Edit Rate (HTER) (Snover et al., 2006), a semi-automatic post-editing based metric which measures the distance between MT output and a targeted reference. We identify several limitations of the original METEOR metric and describe our modifications to improve performance on this task. Our extended metric, METEOR-NEXT, is then tuned to maximize segment-level correlation with HTER scores and tested against several baseline metrics. We show that METEOR-NEXT outperforms earlier versions of METEOR when tuned to the same HTER data and approaches the performance of a state-of-the-art TER-based metric, TER-plus. 2 The METEOR-NEXT Metric 2.</context>
<context position="7971" citStr="Snover et al., 2006" startWordPosition="1302" endWordPosition="1305"> in the hypothesis (t) and reference (r) are counted. For each of the matchers (mi), count the number of words covered by matches of this type in the hypothesis (mi(t)) and reference (mi(r)) and apply the appropriate module weight (wi). The weighted Precision and Recall are then calculated: _ �i wi - mi(t) P t The minimum number of chunks (ch) is then calculated using the new chunk definition. Once P, R, and ch are calculated, the remaining statistics and final score can be calculated as in Section 2.1. 3 Tuning for Post-Editing Measures of Quality Human-targeted Translation Edit Rate (HTER) (Snover et al., 2006), is a semi-automatic assessment of machine translation quality based on the number of edits required to correct translation hypotheses. A human annotator edits each MT hypothesis so that it is meaning-equivalent with a reference translation, with an emphasis on making the minimum possible number of edits. The Translation Edit Rate (TER) is then calculated using the human-edited translation as a targeted reference for the MT hypothesis. The resulting scores are shown to correlate well with other types of human judgments (Snover et al., 2006). 3.1 Tuning Toward HTER The GALE (Olive, 2005) Phase</context>
<context position="9948" citStr="Snover et al., 2006" startWordPosition="1623" endWordPosition="1626">act matches fixed at 1). Table 1 compares the new HTER parameters to those tuned for other tasks including adequacy and fluency (Lavie and Agarwal, 2007) and ranking (Agarwal and Lavie, 2008). As observed by Snover et al. (2009), HTER prefers metrics which are more balanced between precision and recall: this results in the lowest values of α for any task. Additionally, non-exact matches receive lower weights, with stem matches receiving zero weight. This reflects a weakness in HTER scoring where words with matching stems are treated as completely dissimilar, requiring full word substitutions (Snover et al., 2006). 4 Experiments The GALE (Olive, 2005) Phase 3 unsequestered data includes HTER scores for Arabic-to-English MT output. We created a test set from HTER scores of 2245 segments from 195 documents in this data set. Our evaluation metric (METEOR-NEXT-hter) was tested against the following established metrics: BLEU (Papineni et al., 2002) with a maximum Ngram length of 4, TER (Snover et al., 2006), versions of METEOR based on release 0.7 tuned for adequacy and fluency (METEOR-0.7-af) (Lavie and Agarwal, 2007), ranking (METEOR-0.7-rank) (Agarwal and Lavie, 2008), and HTER (METEOR-0.7-hter). Also in</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. In Proc. of AMTA-2006, pages 223–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Nitin Madnani</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>Fluency, Adequacy, or HTER? Exploring Different Human Judgments with a Tunable MT Metric.</title>
<date>2009</date>
<booktitle>In Proc. of WMT09,</booktitle>
<pages>259--268</pages>
<contexts>
<context position="5368" citStr="Snover et al. (2009)" startWordPosition="867" endWordPosition="870">ecting partial alignments based only on the fewest number of per-stage crossing alignment links can in practice lead to missing full alignments with the same number of matches in fewer chunks. Our extended aligner addresses these limitations by introducing support for multiple-word phrase matches and considering all possible matches in a single alignment stage. We introduce an additional paraphrase matcher which matches phrases (one or more successive words) if one phrase is considered a paraphrase of the other by a paraphrase database. For English, we use the paraphrase database developed by Snover et al. (2009), using techniques presented by Bannard and Callison-Burch (2005). The extended aligner first constructs a search space by applying all matchers in sequence to identify all possible matches between the hypothesis and reference. To reduce redundant matches, stem and synonym matches between pairs of words which have already been identified as exact matches are not considered. Matches have start positions and lengths in both sentences; a word occurring less than length positions after a match start is said to be covered by the match. As exact, stem, and synonym matches will always have length one</context>
<context position="9556" citStr="Snover et al. (2009)" startWordPosition="1562" endWordPosition="1565">ced globally optimal α, Q, and -y values for METEOR and optimal α, Q, -y values plus stem, synonym, and paraphrase match weights for Task α Q -y Adequacy &amp; Fluency 0.81 0.83 0.28 Ranking 0.95 0.50 0.50 HTER 0.70 1.95 0.50 HTER (extended) 0.65 1.95 0.45 Stem Syn Par 0 0.4 0.9 Table 1: Parameter values for various METEOR tasks for translations into English. METEOR-NEXT (with the weight of exact matches fixed at 1). Table 1 compares the new HTER parameters to those tuned for other tasks including adequacy and fluency (Lavie and Agarwal, 2007) and ranking (Agarwal and Lavie, 2008). As observed by Snover et al. (2009), HTER prefers metrics which are more balanced between precision and recall: this results in the lowest values of α for any task. Additionally, non-exact matches receive lower weights, with stem matches receiving zero weight. This reflects a weakness in HTER scoring where words with matching stems are treated as completely dissimilar, requiring full word substitutions (Snover et al., 2006). 4 Experiments The GALE (Olive, 2005) Phase 3 unsequestered data includes HTER scores for Arabic-to-English MT output. We created a test set from HTER scores of 2245 segments from 195 documents in this data </context>
</contexts>
<marker>Snover, Madnani, Dorr, Schwartz, 2009</marker>
<rawString>Matthew Snover, Nitin Madnani, Bonnie Dorr, and Richard Schwartz. 2009. Fluency, Adequacy, or HTER? Exploring Different Human Judgments with a Tunable MT Metric. In Proc. of WMT09, pages 259– 268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C van Rijsbergen</author>
</authors>
<date>1979</date>
<note>Information Retrieval, chapter 7. 2nd edition.</note>
<marker>van Rijsbergen, 1979</marker>
<rawString>C. van Rijsbergen, 1979. Information Retrieval, chapter 7. 2nd edition.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>