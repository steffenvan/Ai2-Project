<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004824">
<title confidence="0.540751">
Parsing
</title>
<author confidence="0.607687">
W. A. Martin
</author>
<affiliation confidence="0.754692">
Laboratory for Computer Science
Massachusetts Institute of Technology
Cambridge, Massachusetts 02139
</affiliation>
<bodyText confidence="0.999812117647059">
Looking at the Proceedings of last year&apos;s Annual Meeting, one secs that the
session most closely paralleling this one was entitled Language Structure and
Parsing. In a very nice presentation, Martin Kay was able to unite the papers of
that session under a single theme. As he stated it,
There has been a shift of emphasis away from highly
structured systems of complex rules as the principal
repository of information about the syntax of a
language towards a view in which the responsibility
is distributed among the lexicon, semantic parts of
the linguistic description, and a cognitive or strategic
component. Concomitantly, interest has shifted
from algorithms for syntactic analysis and
generation. in which the central structure and the
exact sequence of events are paramount, to systems
in which a heavier burden is carried by the data
suucturc and in which the order of events is a meter
of strategy.
&apos;this year, the papers of the session represent a greater diversity of research
directions. The paper by Hayes. and the paper by Wilensky and Arens are both
examples of what Kay had in mind, but the paper by Church, with regard to the
question of algorithms, is quite the opposite. lie holds that once the full range
of constraints describing people&apos;s processing behavior has been captured, the
best parsing strategics will be rather straightforward, and easily explained as
algorithms.
Perhaps the seven papers in this year&apos;s session can best he introduced by briefly
citing some of the achievements and problems reported in the works they
reference.
In the late 1960&apos;s Woods (Woods701 capped an effort by several people to
develop NUN parsing. &apos;Ibis well known technique applies a straightforward top
down, left to right, depth first parsing algorithm to a syntactic grammar.
Especially in the compiled form produced by Burton Illurton764 the parser
was able to produce the first parse in good time, but without semantic
constraints, numerous syntactic analyses could be and sometimes were found,
especially in sentences with conjunctions. A strength of the system was the
A-IN grammar, which can be described as a set of context free production rules
whose right hand sides are finite state machines and whose transition arcs have
been augmented with functions able to read and set registers, and also able to
block a transition on their arc. Many people have found this a convenient
formulism in which to develop grammars of English.
The Woods ATN parser was a great success, and attempts were made to exploit
it (a) as a model of human processing and (b) as a tool for writing grammars. At
the same time it was recognized to have limitations. It wasn&apos;t tolerant of errors,
and it couldn&apos;t handle unknown words or constructions (there were many
syntactic constructions which it didn&apos;t know). In addition, the question
answering system fed by the parser had a weak notion of word and phrase
semantics and it was not always able to handle quantifiers properly. It is not
clear these components could have supported a stronger interaction with
syntactic parsing, had Woods chosen to attempt iL
On the success side. Kaplan [Kaplan72] was inspired to claim that the ATN
parser provided a good model for some aspects of human processing. Some
aspects which might be modeled are:
</bodyText>
<figure confidence="0.622667">
Lineuistic Phenomenon ATN Comnutationa( Mechanism
Preferred readings of Ordered Trying of
Ambiguous Sentences Alternative Arcs
Garden Path Sentences Back-tracking
Perceived Complexity Hold List Costing
Differences Counting Total Transitions
Center Embedding Bounds None
</figure>
<bodyText confidence="0.981293795454545">
In one study, most people got the a) reading of 1). One can try to explain this
1) They told the girl that Bill liked the story.
Ta) They told the girl [that (Bill liked the storyls[.
Tb) They told [the girl that Bill likedINp the
story.
by ordering the arcs leaving the state where the head noun of an NP has been
accepted: a pop are (terminating the NP) is tried before an arc accepting a
modifying relative clause. However, Rich [Rich751 points out that this arc
ordering solution would seem to have difficulties with 2). This sentence is often
not perceived
2) They told the girl that Bill liked that he
would be at the football game.
as requiring backup, yet if the arcs arc ordered as for 1), it does require backup.
There is no doubt that whatever is going on. the awareness of backup in 3) is so
much stronger than in 2) that it seems like a different phenomenon. To resolve
this,
3) The horse raced past the barn fell.
one could claim that perceived backup is some function of the length of the
actual backup, or maybe of the degree of commitment to the original path
(although it isn t clear what this would mean in ATN terms).
In this session, Ferrari and Stock will turn the arc ordering game around and
describe, for actual texts, the probability that a given are is the correct exit are
from a node, given the arc by which the parser arrived at the node. it will be
interesting to look at their distributions. In the speech project at IBM Watson
Laboratories [Baker751 it was discovered some time ago that, for a given text, the
syntactic class of a word could be predicted correctly over 90% of the time given
only the syntactic class of the preceding word. Interestingly, the correctness of
predictions fell off less than 10% when only the current word was used. One
wonders if this same level of skewness holds across texts, or (what we will hear)
for the continuation of phrases. These results should be helpful in discussing
the whole issue of arc ordering.
Implicit in any arc ordering strategy is the assumption that not all parses of a
sentence will be fund. Having the &amp;quot;best&amp;quot; path, the parser will stop when it gets
an acceptable analysis. Arc ordering helps find that &amp;quot;best&amp;quot; path. Marcus
[Mareus78j, agreed with the idea of following only a best path, but he claimed
that the reason there is no perceived backup in 2) is that the human parser is
able to look ahead a few constituents instead of just one state and one
constituent in making a transition. He claims this makes a more accurate model
of human garden path behavior, but it doesn&apos;t address the issue of unlimited
stack depth. Here, Church will describe a parser similar in design to Marcus&apos;,
except that it conserves incmory. This allows Church to address psychological
facts not addressed by either Marcus or the ATN models. Church claims that
exploiting stack size constraints will increase the chances of building a good best
path parser.
</bodyText>
<page confidence="0.994738">
91
</page>
<bodyText confidence="0.999863857142857">
Besides psychological modeling, there is also an interest in using the ATN
formalism for writing and teaching grammars. Paramount here is eJtplanation,
both of the grammar and its application to a particular sentence. The paper by
Kehler and Woods reports on this. Weischedel picks a particular problem,
responding to an input which the ATN can&apos;t handle. He aszociates a list of
diagnostic conditions and actions with each state. When no parse is found, the
parser finds the last state on the path which progressed the farthest through the
input string and executes its diagnostic conditions and actions. When a primer
uses only syntactic constraints, one expects it to find a lot of parses. Usually the
number of parses grows more than linearly with sentence length. Thus, for a
fairly complete grammar and moderate to long sentences, one would expect
that the case of no parses (handled by Weischedel) would be rare in comparison
with the other two cases (not handled) where the set of parses doesn&apos;t include
the correct one, or where the grammar has been mistakenly written to allow
undesired parses. Success of the above efforts to follow only the best path
would clearly be relevant here. No doubt Weischeders procedure can help find
a lot of bugs if the test examples are chosen with a little care. But there is still
interesting work to be done on grammar and parser explanation, and
Weischedel is one of those who intends to explore it.
The remaining three papers stem from three separate traditions which reject the
strict syntactic ATN fonnalism, each for its own reasons. They are:
</bodyText>
<listItem confidence="0.925629166666667">
i) Semantic Grammars -- the Davidson and
Kaplan paper
ii) Semantic Structure Driven Parsing â€”
Wilensky and Arens paper
iii) Multiple knowledge Source Parsing -- Hayes
paper
</listItem>
<bodyText confidence="0.994060934782609">
Each of these systems claims some advantage over the more widely known and
accepted ATN.
The semantic grammar parser can be viewed as a variation of the AIN which
attempts to cope with the ATN&apos;s lack of semantics. Kaplan&apos;s work builds on
work started by Burton filurton761,1 and picked up by Hendrix et al
[I lendrix.781. The semantic grammar parser uses semantic in.acad of syntactic
arc categories. &apos;This collapses syntax and semantics into a single structure.
When an ATN parsing strategy is used the result is actually Igs_s flexible than a
syntactic ATN, but it is faster because syntactic possibilities are eliminated by
the semantics of the domain. &apos;The strategy is justified in terms of the
performance of actual running systems. Kaplan also calls on a speed criteria in
suggesting that when an unknown word is encountered the system assume all
possibilities which will let parsing proceed. Then if more than one possibility
leads to a successful parse, the system should attempt to resolve the word further
by file search or user query.
As Kaplan points out, this trick is not limited to semantic grammars, but only to
systems haying enough constraints. It would be interesting to know how we&amp;quot; it
would work for systems using Osherson&apos;s 10sherson78] predicability criterion.
instead of truth for their semantics. Osherson distinguishes between &amp;quot;green
idea&amp;quot;, which he says is silly and &amp;quot;married bachelor&amp;quot; which he says is just false.
Ile notes that &amp;quot;idea is not green&amp;quot; is no better, but &amp;quot;bachelor is not married&amp;quot; is
fine. Predicahility is a looser constraint than Kaplan uses, and if it would still be
enough to limit database search this we,. be interesi:ng, because predicability
is easier to implement across a broad domain,
lliilensky is a former suigent of Schanles and thus comes from a tradition which
emphasizes semantics over syntax. He is right in emphasizing the importance
of phrase semantics. The gthininarians Quirk and Greenbaum [Quirk731 point
out the syntactic .ind semantic importance of verb phrases over vcrbs.- In
linguistics. Bresnan Il3resnan801 is developing a theory of lexical phrases which
accounts, by lexical relations between constituents of a phrase, for many of the
phenomena explained by the old transformational grammar. For example,
given
4) There were reported to have been lions
sighted,
a typical ATN parser would attempt by register manipulations to make &amp;quot;lions&amp;quot;
the subject. Using a phrase approach, &amp;quot;there be lions sighted&amp;quot; can be taken as
meaning &amp;quot;exist lions sighted.&amp;quot; where &amp;quot;lions&amp;quot; is an object and &amp;quot;sighted&amp;quot; an object
complement &amp;quot;There is related to the &amp;quot;be&amp;quot; in &amp;quot;been&amp;quot; by a series of
relationships between the arguments of semantic structures. Wilensky appears
to have suppressed syntax into his semantic component, and so it will be
inteiesting to see how he handles the traditional syntactic phenomena of 4), like
passive and verb forms.
Finally, the paper by Hayes shows the influence of the speech recognition
projects where bad input gave the Woods ATN great difficulty. Text input is
much better than speech input. However, examination of actual input
(Malhotra75I docs show sentences like:
</bodyText>
<listItem confidence="0.768839">
5) What would have profits have been?
</listItem>
<bodyText confidence="0.991746583333334">
Fortunately, these cases are rare. Much more likely is clipsis and the omission
of syntax when the semantics are clear. For example, the missing commas in
6) Give ratios of manufacturing costs to sales
for plants I 2 3 and 4 for 72 and 73.
Examples like these show that errors and omissions arc not random phenomena
and that there can be something to the study of errors and how to deal with
them.
In summary, it can be seen that while much progress has been made in
constructing usable parsers, the basic issues, such as the division of syntax,
semantics, and pragmatics both in representation and in order of processing, are
still up for grabs. The problem has plenty of structure, so there is good fun to
be had.
</bodyText>
<table confidence="0.9798765">
References Baker, J.K. &amp;quot;Stochastic Modeling for
(Baker751 Automatic Speech Understanding,&amp;quot; anggch
RecognititLE Invited Papers of Mg IEEE
5vinnosium, Reddy, D.R. (Ed.), 1975.
</table>
<reference confidence="0.844840235294118">
Il1resnan80) Bresnan, Joan. &amp;quot;Polyadicity: Part I of a
Theory of lexical Rules and
Representations,&amp;quot; MIT Department of
Linguistics (January 1980).
[Burton76a1 Burton, Richard R. and Woods, William A.
&amp;quot;A Compiling System for Augmented
Transition Networks,&amp;quot; COLING 76.
fBunon76b) Burton, Richard R. &amp;quot;Semantic Grammar: An
Engineering Technique for Constructing
Natural Language Understanding Systems,&amp;quot;
BBN Report 3453, Bolt, Beranek, and
Newman, Boston, Ma. (December 1976).
[Hendrix781 Hendrix, Gary G., Sacercioti, E.D.,
Sagalowicz. D., and Slocum. J. &amp;quot;Developing a
Natural Language interface to Complex
Data.&amp;quot; ACM Trans, 02 Database Systems vol.
3. no. 2 (June 1978), pp. 105-147.
</reference>
<page confidence="0.972297">
92
</page>
<reference confidence="0.975338">
[Kaplan721 Kaplan, Ronald M. &amp;quot;Augmented Transition
Networks as Psychological Models of
Sentence Comprehension,&amp;quot; Artificial
Intelligence, 3 (October 1972). pp. 77-100.
[Malhotra751 Malhotra, Ashok. &amp;quot;Design Criteria for a
Knowledge-Based English Language System
for Management: An Experimental
Analysis,&amp;quot; M IT/LCS/TR- 146, MIT,
Laboratory for Computer Science,
Cambridge. Ma. (February 1975).
[Marcus781 Marcus, Mitchell. &amp;quot;A Theory of Syntactic
Recognition for Natural Languages,&amp;quot; Ph.D.
thesis. mrr Dept. of Electrical Engineering
and Computer Science, Cambridge, Ma. (to
be published by MIT Press).
[Osherson7131 Osherson, Daniel N. &amp;quot;Three Conditions on
[Quirk731 Conceptual Naturalness,&amp;quot; Cognition 6 (1978),
fltich751 pp. 263-289.
Quirk, R. and Greenbaum. S. p cgardsg
G rammar d Contemporary English, Harcourt
13race Jovanovich, New York (1973).
Rich. Charles. &amp;quot;On the Psychological Reality
of Augmented Transition Network Models of
Sentence Comprehension,&amp;quot; unpublished
paper, MIT Artificial Intelligence Laboratory,
Cambridge, Ma. (July 1975).
(Woods701 Woods, William A. &amp;quot;Transition Network
Grammars for Natural Language Analysis&amp;quot;
CACM 13 10 (October 1970), pp. 591-602.
</reference>
<page confidence="0.998986">
93
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.088547">
<title confidence="0.999886">Parsing</title>
<author confidence="0.999689">W A Martin</author>
<affiliation confidence="0.9996315">Laboratory for Computer Science Massachusetts Institute of Technology</affiliation>
<address confidence="0.999924">Cambridge, Massachusetts 02139</address>
<abstract confidence="0.997956666666667">Looking at the Proceedings of last year&apos;s Annual Meeting, one secs that the session most closely paralleling this one was entitled Language Structure and Parsing. In a very nice presentation, Martin Kay was able to unite the papers of that session under a single theme. As he stated it, There has been a shift of emphasis away from highly structured systems of complex rules as the principal repository of information about the syntax of a language towards a view in which the responsibility is distributed among the lexicon, semantic parts of the linguistic description, and a cognitive or strategic component. Concomitantly, interest has shifted from algorithms for syntactic analysis and generation. in which the central structure and the exact sequence of events are paramount, to systems in which a heavier burden is carried by the data suucturc and in which the order of events is a meter of strategy. &apos;this year, the papers of the session represent a greater diversity of research directions. The paper by Hayes. and the paper by Wilensky and Arens are both examples of what Kay had in mind, but the paper by Church, with regard to the question of algorithms, is quite the opposite. lie holds that once the full range of constraints describing people&apos;s processing behavior has been captured, the best parsing strategics will be rather straightforward, and easily explained as algorithms. Perhaps the seven papers in this year&apos;s session can best he introduced by briefly citing some of the achievements and problems reported in the works they reference. In the late 1960&apos;s Woods (Woods701 capped an effort by several people to develop NUN parsing. &apos;Ibis well known technique applies a straightforward top down, left to right, depth first parsing algorithm to a syntactic grammar. Especially in the compiled form produced by Burton Illurton764 the parser was able to produce the first parse in good time, but without semantic constraints, numerous syntactic analyses could be and sometimes were found, especially in sentences with conjunctions. A strength of the system was the grammar, which can be described as a set of context free production rules whose right hand sides are finite state machines and whose transition arcs have been augmented with functions able to read and set registers, and also able to block a transition on their arc. Many people have found this a convenient formulism in which to develop grammars of English. The Woods ATN parser was a great success, and attempts were made to exploit it (a) as a model of human processing and (b) as a tool for writing grammars. At the same time it was recognized to have limitations. It wasn&apos;t tolerant of errors, and it couldn&apos;t handle unknown words or constructions (there were many syntactic constructions which it didn&apos;t know). In addition, the question answering system fed by the parser had a weak notion of word and phrase semantics and it was not always able to handle quantifiers properly. It is not clear these components could have supported a stronger interaction with syntactic parsing, had Woods chosen to attempt iL On the success side. Kaplan [Kaplan72] was inspired to claim that the ATN parser provided a good model for some aspects of human processing. Some aspects which might be modeled are:</abstract>
<title confidence="0.994823">Lineuistic Phenomenon ATN Comnutationa( Mechanism Preferred readings of Ordered Trying of Ambiguous Sentences Alternative Arcs Garden Path Sentences Back-tracking Perceived Complexity Hold List Costing Differences Counting Total Transitions</title>
<author confidence="0.815966">Center Embedding Bounds None</author>
<abstract confidence="0.999526076335878">In one study, most people got the a) reading of 1). One can try to explain this 1) They told the girl that Bill liked the story. They told the girl [that (Bill liked the They told [the girl that Bill the story. by ordering the arcs leaving the state where the head noun of an NP has been accepted: a pop are (terminating the NP) is tried before an arc accepting a modifying relative clause. However, Rich [Rich751 points out that this arc ordering solution would seem to have difficulties with 2). This sentence is often not perceived 2) They told the girl that Bill liked that he would be at the football game. as requiring backup, yet if the arcs arc ordered as for 1), it does require backup. There is no doubt that whatever is going on. the awareness of backup in 3) is so much stronger than in 2) that it seems like a different phenomenon. To resolve this, 3) The horse raced past the barn fell. one could claim that perceived backup is some function of the length of the actual backup, or maybe of the degree of commitment to the original path (although it isn t clear what this would mean in ATN terms). In this session, Ferrari and Stock will turn the arc ordering game around and describe, for actual texts, the probability that a given are is the correct exit are from a node, given the arc by which the parser arrived at the node. it will be interesting to look at their distributions. In the speech project at IBM Watson Laboratories [Baker751 it was discovered some time ago that, for a given text, the syntactic class of a word could be predicted correctly over 90% of the time given only the syntactic class of the preceding word. Interestingly, the correctness of predictions fell off less than 10% when only the current word was used. One wonders if this same level of skewness holds across texts, or (what we will hear) for the continuation of phrases. These results should be helpful in discussing the whole issue of arc ordering. Implicit in any arc ordering strategy is the assumption that not all parses of a sentence will be fund. Having the &amp;quot;best&amp;quot; path, the parser will stop when it gets an acceptable analysis. Arc ordering helps find that &amp;quot;best&amp;quot; path. Marcus [Mareus78j, agreed with the idea of following only a best path, but he claimed that the reason there is no perceived backup in 2) is that the human parser is able to look ahead a few constituents instead of just one state and one constituent in making a transition. He claims this makes a more accurate model of human garden path behavior, but it doesn&apos;t address the issue of unlimited stack depth. Here, Church will describe a parser similar in design to Marcus&apos;, except that it conserves incmory. This allows Church to address psychological facts not addressed by either Marcus or the ATN models. Church claims that exploiting stack size constraints will increase the chances of building a good best path parser. 91 Besides psychological modeling, there is also an interest in using the ATN formalism for writing and teaching grammars. Paramount here is eJtplanation, both of the grammar and its application to a particular sentence. The paper by Kehler and Woods reports on this. Weischedel picks a particular problem, input which the ATN can&apos;t handle. He aszociates a list of diagnostic conditions and actions with each state. When no parse is found, the parser finds the last state on the path which progressed the farthest through the input string and executes its diagnostic conditions and actions. When a primer uses only syntactic constraints, one expects it to find a lot of parses. Usually the number of parses grows more than linearly with sentence length. Thus, for a fairly complete grammar and moderate to long sentences, one would expect that the case of no parses (handled by Weischedel) would be rare in comparison with the other two cases (not handled) where the set of parses doesn&apos;t include the correct one, or where the grammar has been mistakenly written to allow undesired parses. Success of the above efforts to follow only the best path would clearly be relevant here. No doubt Weischeders procedure can help find a lot of bugs if the test examples are chosen with a little care. But there is still interesting work to be done on grammar and parser explanation, and Weischedel is one of those who intends to explore it. The remaining three papers stem from three separate traditions which reject the strict syntactic ATN fonnalism, each for its own reasons. They are: i) Semantic Grammars -the Davidson and Kaplan paper ii) Semantic Structure Driven Parsing â€” Wilensky and Arens paper iii) Multiple knowledge Source Parsing -- Hayes paper Each of these systems claims some advantage over the more widely known and accepted ATN. The semantic grammar parser can be viewed as a variation of the AIN which attempts to cope with the ATN&apos;s lack of semantics. Kaplan&apos;s work builds on work started by Burton filurton761,1 and picked up by Hendrix et al [I lendrix.781. The semantic grammar parser uses semantic in.acad of syntactic arc categories. &apos;This collapses syntax and semantics into a single structure. When an ATN parsing strategy is used the result is actually Igs_s flexible than a syntactic ATN, but it is faster because syntactic possibilities are eliminated by the semantics of the domain. &apos;The strategy is justified in terms of the performance of actual running systems. Kaplan also calls on a speed criteria in suggesting that when an unknown word is encountered the system assume all possibilities which will let parsing proceed. Then if more than one possibility leads to a successful parse, the system should attempt to resolve the word further by file search or user query. As Kaplan points out, this trick is not limited to semantic grammars, but only to haying enough constraints. It would interesting to know how we&amp;quot; it would work for systems using Osherson&apos;s 10sherson78] predicability criterion. instead of truth for their semantics. Osherson distinguishes between &amp;quot;green which he says is silly bachelor&amp;quot; which he says is just false. Ile notes that &amp;quot;idea is not green&amp;quot; is no better, but &amp;quot;bachelor is not married&amp;quot; is fine. Predicahility is a looser constraint than Kaplan uses, and if it would still be to search we,. be interesi:ng, because predicability is easier to implement across a broad domain, is former suigent of and thus comes from tradition which over syntax. He right in emphasizing the of phrase semantics. The gthininarians Quirk and Greenbaum [Quirk731 point the syntactic .ind semantic importance of phrases In linguistics. Bresnan Il3resnan801 is developing a theory of lexical phrases which accounts, by lexical relations between constituents of a phrase, for many of the phenomena explained by the old transformational grammar. For example, given There were reported been lions sighted, a typical ATN parser would attempt by register manipulations to make &amp;quot;lions&amp;quot; the subject. Using a phrase approach, &amp;quot;there be lions sighted&amp;quot; can be taken as meaning &amp;quot;exist lions sighted.&amp;quot; where &amp;quot;lions&amp;quot; is an object and &amp;quot;sighted&amp;quot; an object complement &amp;quot;There is related to the &amp;quot;be&amp;quot; in &amp;quot;been&amp;quot; by a series of relationships between the arguments of semantic structures. Wilensky appears to have suppressed syntax into his semantic component, and so it will be inteiesting to see how he handles the traditional syntactic phenomena of 4), like passive and verb forms. Finally, the paper by Hayes shows the influence of the speech recognition projects where bad input gave the Woods ATN great difficulty. Text input is much better than speech input. However, examination of actual input (Malhotra75I docs show sentences like: 5) What would have profits have been? Fortunately, these cases are rare. Much more likely is clipsis and the omission of syntax when the semantics are clear. For example, the missing commas in 6) Give ratios of manufacturing costs to sales for plants I 2 3 and 4 for 72 and 73. Examples like these show that errors and omissions arc not random phenomena and that there can be something to the study of errors and how to deal with them. In summary, it can be seen that while much progress has been made in constructing usable parsers, the basic issues, such as the division of syntax, semantics, and pragmatics both in representation and in order of processing, are still up for grabs. The problem has plenty of structure, so there is good fun to be had.</abstract>
<note confidence="0.980808">References Baker, J.K. &amp;quot;Stochastic Modeling for Automatic Speech Understanding,&amp;quot; anggch Invited Papersof Mg 5vinnosium,Reddy, D.R. (Ed.), 1975. (Baker751 Il1resnan80) Bresnan, Joan. &amp;quot;Polyadicity: Part I of a Theory of lexical Rules and Representations,&amp;quot; MIT Department of Linguistics (January 1980). [Burton76a1 Burton, Richard R. and Woods, William A. &amp;quot;A Compiling System for Augmented Transition Networks,&amp;quot; COLING 76. fBunon76b) Burton, Richard R. &amp;quot;Semantic Grammar: An</note>
<title confidence="0.963275">Engineering Technique for Constructing Natural Language Understanding Systems,&amp;quot;</title>
<note confidence="0.912829517241379">BBN Report 3453, Bolt, Beranek, and Newman, Boston, Ma. (December 1976). [Hendrix781 Hendrix, Gary G., Sacercioti, E.D., Sagalowicz. D., and Slocum. J. &amp;quot;Developing a Natural Language interface to Complex Trans,02 DatabaseSystemsvol. 3. no. 2 (June 1978), pp. 105-147. 92 [Kaplan721 Kaplan, Ronald M. &amp;quot;Augmented Transition Networks as Psychological Models of Comprehension,&amp;quot; Artificial Intelligence,3 (October 1972). pp. 77-100. [Malhotra751 Malhotra, Ashok. &amp;quot;Design Criteria for a Knowledge-Based English Language System for Management: An Experimental Analysis,&amp;quot; M IT/LCS/TR- 146, MIT, Laboratory for Computer Science, Cambridge. Ma. (February 1975). [Marcus781 Marcus, Mitchell. &amp;quot;A Theory of Syntactic Recognition for Natural Languages,&amp;quot; Ph.D. of Electrical Engineering and Computer Science, Cambridge, Ma. (to be published by MIT Press). [Osherson7131 Osherson, Daniel N. &amp;quot;Three Conditions on Naturalness,&amp;quot; Cognition6 (1978), pp. 263-289. [Quirk731 R. and Greenbaum. S. rammard English,Harcourt 13race Jovanovich, New York (1973). fltich751 Rich. Charles. &amp;quot;On the Psychological Reality of Augmented Transition Network Models of Sentence Comprehension,&amp;quot; unpublished paper, MIT Artificial Intelligence Laboratory, Cambridge, Ma. (July 1975). (Woods701 Woods, William A. &amp;quot;Transition Network Grammars for Natural Language Analysis&amp;quot; CACM13 10 (October 1970), pp. 591-602.</note>
<intro confidence="0.582847">93</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Joan Bresnan</author>
</authors>
<title>Polyadicity: Part I of a Theory of lexical Rules and Representations,&amp;quot;</title>
<date>1980</date>
<institution>MIT Department of Linguistics</institution>
<marker>Bresnan, 1980</marker>
<rawString>Il1resnan80) Bresnan, Joan. &amp;quot;Polyadicity: Part I of a Theory of lexical Rules and Representations,&amp;quot; MIT Department of Linguistics (January 1980).</rawString>
</citation>
<citation valid="false">
<title>A Compiling System for Augmented Transition Networks,&amp;quot;</title>
<journal>COLING</journal>
<volume>76</volume>
<marker></marker>
<rawString>[Burton76a1 Burton, Richard R. and Woods, William A. &amp;quot;A Compiling System for Augmented Transition Networks,&amp;quot; COLING 76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard R Burton</author>
</authors>
<title>Semantic Grammar: An Engineering Technique for Constructing Natural Language Understanding Systems,&amp;quot;</title>
<date>1976</date>
<tech>BBN Report 3453,</tech>
<location>Bolt, Beranek, and Newman, Boston, Ma.</location>
<marker>Burton, 1976</marker>
<rawString>fBunon76b) Burton, Richard R. &amp;quot;Semantic Grammar: An Engineering Technique for Constructing Natural Language Understanding Systems,&amp;quot; BBN Report 3453, Bolt, Beranek, and Newman, Boston, Ma. (December 1976).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D</author>
<author>J Slocum</author>
</authors>
<title>Developing a Natural Language interface to Complex Data.&amp;quot;</title>
<date>1978</date>
<journal>ACM Trans, 02 Database Systems</journal>
<volume>3</volume>
<pages>105--147</pages>
<marker>D, Slocum, 1978</marker>
<rawString>[Hendrix781 Hendrix, Gary G., Sacercioti, E.D., Sagalowicz. D., and Slocum. J. &amp;quot;Developing a Natural Language interface to Complex Data.&amp;quot; ACM Trans, 02 Database Systems vol. 3. no. 2 (June 1978), pp. 105-147.</rawString>
</citation>
<citation valid="true">
<title>Augmented Transition Networks as Psychological Models of Sentence Comprehension,&amp;quot;</title>
<date>1972</date>
<journal>Artificial Intelligence,</journal>
<volume>3</volume>
<pages>77--100</pages>
<marker>1972</marker>
<rawString>[Kaplan721 Kaplan, Ronald M. &amp;quot;Augmented Transition Networks as Psychological Models of Sentence Comprehension,&amp;quot; Artificial Intelligence, 3 (October 1972). pp. 77-100.</rawString>
</citation>
<citation valid="true">
<title>Design Criteria for a Knowledge-Based English Language System for Management: An Experimental Analysis,&amp;quot;</title>
<date>1975</date>
<journal>M IT/LCS/TR-</journal>
<volume>146</volume>
<institution>MIT, Laboratory for Computer Science,</institution>
<location>Cambridge. Ma.</location>
<marker>1975</marker>
<rawString>[Malhotra751 Malhotra, Ashok. &amp;quot;Design Criteria for a Knowledge-Based English Language System for Management: An Experimental Analysis,&amp;quot; M IT/LCS/TR- 146, MIT, Laboratory for Computer Science, Cambridge. Ma. (February 1975).</rawString>
</citation>
<citation valid="false">
<title>A Theory of Syntactic Recognition for Natural Languages,&amp;quot;</title>
<tech>Ph.D. thesis. mrr</tech>
<publisher>MIT Press).</publisher>
<institution>Dept. of Electrical Engineering and Computer Science,</institution>
<location>Cambridge, Ma.</location>
<note>to be published by</note>
<marker></marker>
<rawString>[Marcus781 Marcus, Mitchell. &amp;quot;A Theory of Syntactic Recognition for Natural Languages,&amp;quot; Ph.D. thesis. mrr Dept. of Electrical Engineering and Computer Science, Cambridge, Ma. (to be published by MIT Press).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Daniel N Osherson</author>
</authors>
<title>Three Conditions on Conceptual Naturalness,&amp;quot;</title>
<date>1978</date>
<journal>Cognition</journal>
<tech>Osherson7131 [Quirk731 fltich751</tech>
<volume>6</volume>
<pages>263--289</pages>
<location>New York</location>
<marker>Osherson, 1978</marker>
<rawString>[Osherson7131 [Quirk731 fltich751 Osherson, Daniel N. &amp;quot;Three Conditions on Conceptual Naturalness,&amp;quot; Cognition 6 (1978), pp. 263-289. Quirk, R. and Greenbaum. S. p cgardsg G rammar d Contemporary English, Harcourt 13race Jovanovich, New York (1973). Rich. Charles. &amp;quot;On the Psychological Reality of Augmented Transition Network Models of Sentence Comprehension,&amp;quot; unpublished paper, MIT Artificial Intelligence Laboratory, Cambridge, Ma. (July 1975).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>