<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000123">
<title confidence="0.975539">
Frame Semantic Tree Kernels for Social Network Extraction from Text
</title>
<author confidence="0.969916">
Apoorv Agarwal Sriramkumar Balasubramanian
</author>
<affiliation confidence="0.9997">
Dept. of Computer Science Dept. of Computer Science
Columbia University Columbia University
</affiliation>
<address confidence="0.48924475">
New York, NY, USA New York, NY, USA
Anup Kotalwar
Microsoft, Inc.
Redmonad, WA, USA
</address>
<author confidence="0.587339">
Jiehan Zheng
</author>
<affiliation confidence="0.409257">
The Peddie School
Hightstown, NJ, USA
</affiliation>
<author confidence="0.915915">
Owen Rambow
</author>
<affiliation confidence="0.871394">
CCLS
Columbia University
</affiliation>
<address confidence="0.911952">
New York, NY, USA
</address>
<email confidence="0.999718">
apoorv@cs.columbia.edu
</email>
<sectionHeader confidence="0.993921" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999798764705882">
In this paper, we present work on ex-
tracting social networks from unstructured
text. We introduce novel features de-
rived from semantic annotations based on
FrameNet. We also introduce novel se-
mantic tree kernels that help us improve
the performance of the best reported sys-
tem on social event detection and classi-
fication by a statistically significant mar-
gin. We show results for combining the
models for the two aforementioned sub-
tasks into the overall task of social net-
work extraction. We show that a combina-
tion of features from all three levels of ab-
stractions (lexical, syntactic and semantic)
are required to achieve the best performing
system.
</bodyText>
<sectionHeader confidence="0.998995" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9995484">
Social network extraction from text has recently
been gaining a considerable amount of attention
(Agarwal and Rambow, 2010; Elson et al., 2010;
Agarwal et al., 2013a; Agarwal et al., 2013b; He
et al., 2013). One of the reason for this attention,
we believe, is that being able to extract social net-
works from unstructured text may provide a pow-
erful new tool for historians, political scientists,
scholars of literature, and journalists to analyze
large collections of texts around entities and their
interactions. The tool would allow researchers to
quickly extract networks and assess their size, na-
ture, and cohesiveness, a task that would otherwise
be impossible with corpora numbering millions of
documents. It would also make it possible to make
falsifiable claims about these networks, bringing
the experimental method to disciplines like his-
tory, where it is still relatively rare.
In our previous work (Agarwal et al., 2010),
we proposed a definition of a network based on
interactions: nodes are entities and links are so-
cial events. We defined two broad types of links:
one-directional links (one person thinking about
or talking about another person) and bi-directional
links (two people having a conversation, a meet-
ing, etc.). For example, in the following sen-
tence, we would add two links to the network: a
one-directional link between Toujan Faisal and
the committee, triggered by the word said (be-
cause Toujan is talking about the committee) and
a bi-directional link between the same entities trig-
gered by the word informed (a mutual interaction).
(1) [Toujan Faisal], 54, said [she] was informed
of the refusal by an [Interior Ministry com-
mittee] overseeing election preparations.
In this paper, we extract networks using the
aforementioned definition of social networks. We
introduce and add tree kernel representations and
features derived from frame-semantic parses to
our previously proposed system. Our results show
that hand-crafted frame semantic features, which
are linguistically motivated, add less value to
the overall performance in comparison with the
frame-semantic tree kernels. We believe this is due
to the fact that hand-crafted features require frame
parses to be highly accurate and complete. In con-
trast, tree kernels are able to find and leverage less
strict patterns without requiring the semantic parse
to be entirely accurate or complete.
Apart from introducing semantic features and
tree structures, we evaluate on the task of social
network extraction, which is a combination of two
sub-tasks: social event detection and social event
classification. In our previous work (Agarwal and
Rambow, 2010), we presented results for the two
</bodyText>
<page confidence="0.981651">
211
</page>
<note confidence="0.9929315">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 211–219,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.94444375">
sub-tasks, but no evaluation was presented for the
task of social network extraction. We experiment
with two different designs of combining models
for the two sub-tasks: 1) One-versus-All and 2)
Hierarchical. We find that the hierarchical de-
sign outperforms the more commonly used One-
versus-All by a statistically significant margin.
Following are the contributions of this paper:
1. We design and propose novel frame semantic
features and tree-based representations and
show that tree kernels are well suited to work
with noisy semantic parses.
</bodyText>
<listItem confidence="0.937406583333333">
2. We show that in order to achieve the best
performing system, we need to include fea-
tures and tree structures from all levels of
abstractions, lexical, syntactic, and semantic,
and that the convolution kernel framework is
well-suited for creating such a combination.
3. We combine the previously proposed sub-
tasks (social event detection and classifica-
tion) into a single task, social network ex-
traction, and show that combining the mod-
els using a hierarchical design is significantly
better than the one-versus-all design.
</listItem>
<bodyText confidence="0.999935769230769">
The rest of the paper is structured as follows:
In Section 2, we give a precise definition of the
task and describe the data. In Section 3, we give
a brief overview of frame semantics and motivate
the need to use frame semantics for the tasks ad-
dressed in this paper. In Section 4, we present
semantic features and tree kernel representations
designed for the tasks. In Section 5, we briefly
review tree kernels and support vector machines
(SVM). In Section 6 we present experiments and
discuss the results. In Section 7 we discuss related
work. We conclude and give future directions of
work in Section 8.
</bodyText>
<sectionHeader confidence="0.903339" genericHeader="introduction">
2 Data and Task Definition
</sectionHeader>
<bodyText confidence="0.99832575">
In Agarwal et al. (2010), we presented the annota-
tion details of social events on a well-known cor-
pus – Automated Content Extraction1 (ACE2005).
We defined a social event to be a happening be-
tween two entities (of type person) E1 and E2
(E1 =� E2), in which at least one entity is cog-
nitively aware of the other and of the happen-
ing taking place. We defined two broad cate-
</bodyText>
<footnote confidence="0.816205">
1Version: 6.0, Catalog number: LDC2005E18
</footnote>
<table confidence="0.997767">
No-Event INR OBS
# of Examples 1,609 199 199
</table>
<tableCaption confidence="0.9811745">
Table 1: Data distribution; INR are interaction so-
cial events. OBS are observation social events.
</tableCaption>
<bodyText confidence="0.997002342105263">
gories of social events: Interaction (INR) and Ob-
servation (OBS). In a social event of type INR,
the two participating entities are mutually aware
of each other, i.e., INR is a bi-directional social
event. For example, meetings and dinners are so-
cial events of type interaction. In a social event of
type OBS, only one of the two participating enti-
ties is aware of the other and therefore, OBS is a
one-directional social event, directed from the en-
tity that is aware of the other to the other entity.
For example, thinking about someone, or missing
someone are social events of type OBS. Table 1
shows the distribution of the data. There are 199
INR type of social events, 199 OBS events, and
1,609 pairs of entity mentions have no event be-
tween them.
Task definition : The task is, given a pair of en-
tity mentions in a sentence, to predict if the en-
tities are participating in a social event or not
(social event detection, SED), and if they are, to
further predict the type of social event (INR or
OBS, social event classification, SEC). In this pa-
per, we evaluate our system on the above tasks as
well as a combined task: social network extraction
(SNE): given a sentence and a pair of entity men-
tions, predict the class of the example from one of
the following three categories: {No-Event, INR,
OBS}.
For the purposes of this paper, we use gold
named entity mentions to avoid errors caused due
to named entity recognition systems. This is a
common practice used in the literature for re-
porting relation extraction systems (Zelenko et
al., 2003; Kambhatla, 2004; Zhao and Grishman,
2005; GuoDong et al., 2005; Harabagiu et al.,
2005; Nguyen et al., 2009). We use standard ter-
minology from the literature to refer to the pair of
entities mentions as target entities T1 and T2.
</bodyText>
<sectionHeader confidence="0.795219" genericHeader="method">
3 Frame Semantics and FrameNet
</sectionHeader>
<bodyText confidence="0.9965528">
FrameNet (Baker et al., 1998) is a resource which
associates words of English with their meaning.
Word meanings are based on the notion of “se-
mantic frame”. A frame is a conceptual descrip-
tion of a type of event, relation, or entity, and it
</bodyText>
<page confidence="0.99697">
212
</page>
<bodyText confidence="0.995610333333333">
includes a list of possible participants in terms of
the roles they play; these participants are called
“frame elements”. Through the following exam-
ple, we present the terminology and acronyms that
will be used throughout the paper.
Example (2) shows the frame annotations for
the sentence Toujan Faisal said she was informed
of the refusal by an Interior Ministry committee.
One of the semantic frames in the sentence is
Statement. The frame evoking element (FEE) for
this frame is said. It has two frame elements (FE):
one of type Speaker (Toujan Faisal) and the other
of type Message (she was informed ... by an Inte-
rior Ministry committee).
(2) [FE−Speaker Toujan Faisal] [FEE−Statement
said] [FE−Message she was informed of the
refusal by an Interior Ministry committee]
In example (2), the speaker of the message (Toujan
Faisal) is mentioning another group of people (the
Interior Ministry committee) in her message. By
definition, this is a social event of type OBS. In
general, there is an OBS social event between any
Speaker and any person mentioned in the frame
element Message of the frame Statement. This
close relation between frames and social events is
the reason for our investigation and use of frame
semantics for the tasks addressed in this paper.
</bodyText>
<sectionHeader confidence="0.772993" genericHeader="method">
4 Feature space and data representation
</sectionHeader>
<bodyText confidence="0.999980071428571">
We convert examples2 into two kinds of structured
representations: feature vectors and tree struc-
tures. Each of these structural representations may
broadly be categorized into one or more of the fol-
lowing levels of abstraction: {Lexical, Syntactic,
Semantic}. Table 2 presents this distribution. Our
final results show that all of our top performing
models use a data representation that is a combi-
nation of features and structures from all levels of
abstraction. We review previously proposed fea-
tures and tree structures in subsections 4.1, 4.2,
and 4.3. To the best of our knowledge, the re-
maining features and structures presented in this
section are novel.
</bodyText>
<subsectionHeader confidence="0.999853">
4.1 Bag of words (BOW)
</subsectionHeader>
<bodyText confidence="0.944915733333333">
We create a vocabulary from our training data
by using the Stanford tokenizer (Klein and Man-
ning, 2003) followed by removal of stop words
2An input example is a sentence with a pair of entity men-
tions between whom we predict and classify social events.
and Porter Stemming. We convert each example
(~x) to a set of three boolean vectors: {~b1, ~b2, ~b3}.
~b1 is the occurrence of words before the first tar-
get, ~b2 between the two targets and ~b3 after the sec-
ond target. Here the first target and second target
are defined in terms of the surface order of words.
Though these features have been previously pro-
posed for relation extraction on ACE (GuoDong
et al., 2005), they have not been utilized for the
task we address in this paper.
</bodyText>
<subsectionHeader confidence="0.997167">
4.2 Syntactic structures (AR2010)
</subsectionHeader>
<bodyText confidence="0.999993333333333">
In Agarwal and Rambow (2010), we explored
a wide range of syntactic structures for the two
tasks of social event detection (SED) and classi-
fication (SEC). All our previous structures were
derived from a variation of two underlying tree
structures: phrase structure trees and depen-
dency trees. The best structure we proposed was
PET_GR_SqGRW, which was a linear combina-
tion of two tree kernels and one word kernel: 1)
a structure derived from a phrase structure tree
(PET); 2) a grammatical role tree (GR), which is
a dependency tree in which words are replaced
with their grammatical roles; and 3) a path from
one entity to the other in a dependency tree, in
which grammatical roles of words are inserted as
additional nodes between the dependent and par-
ent (SqGRW). We refer the reader to Agarwal
and Rambow (2010) for details of these structures.
For the rest of the paper, we refer to this struc-
ture, PET_GR_SqGRW, as “AR2010”. We use
AR2010 as one of our baselines.
</bodyText>
<subsectionHeader confidence="0.999864">
4.3 Bag of frames (BOF)
</subsectionHeader>
<bodyText confidence="0.999980428571429">
We use Semafor (Chen et al., 2010) for obtaining
the semantic parse of a sentence. Semafor found
instances of 1,174 different FrameNet frames in
our corpus. Each example (~x) is converted to a
vector of dimension 1,174, in which xi (the ith
component of vector ~x) is 1 if the frame number
i appears in the example, and 0 otherwise.
</bodyText>
<subsectionHeader confidence="0.992415">
4.4 Hand-crafted semantic features (RULES)
</subsectionHeader>
<bodyText confidence="0.999580714285714">
We use the manual of the FrameNet resource to
hand-craft 199 rules that are intended to detect the
presence and determine the type of social events
between two entities mentioned in a sentence. An
example of one such rule is given in section 3,
which we reformulate here. We also present an-
other example:
</bodyText>
<page confidence="0.997268">
213
</page>
<table confidence="0.9992878">
Feature Vectors Tree Structures
BOW BOF RULES AR2010 FrameForest FrameTree FrameTreeProp
Lexical ✓ ✓ ✓
Syntactic ✓ ✓
Semantic (novel) ✓ ✓ ✓ ✓ ✓
</table>
<tableCaption confidence="0.948349">
Table 2: Features and tree structures and the level of abstraction they fall into.
</tableCaption>
<listItem confidence="0.971841285714286">
(3) If the frame is Statement, and the first tar-
get entity mention is contained in the FE
Speaker, and the second is contained in the
FE Message, then there is an OBS social
event from the first entity to the second.
(4) If the frame is Commerce_buy, and one tar-
get entity mention is contained in the FE
</listItem>
<bodyText confidence="0.923248857142857">
Buyer, and the other is contained in the FE
Seller, then there is an INR social event be-
tween the two entities.
Each rule corresponds to a binary feature: it
takes a value 1 if the rule fires for an input ex-
ample, and 0 otherwise. Consider the following
sentence:
</bodyText>
<listItem confidence="0.628955666666667">
(5) [Coleman]T1−Ind {claimed}
[he]T1&apos;−Ind {bought} drugs from the
[defendants]T2−Grp.
</listItem>
<bodyText confidence="0.996357375">
In this sentence, there are two social events:
1) an OBS event triggered by the word claimed
between Coleman and defendants and 2) an INR
event triggered by the word bought between he
(co-referential with Coleman) and the defendants.
Semafor correctly detects two frames in this
sentence: 1) the frame Statement, with Coleman
as Speaker, and he bought ... defendants as Mes-
sage, and 2) the frame Commerce_buy, with he as
the Buyer, drugs as the Goods and the defendants
as the Seller. Both hand-crafted rules (3 and 4)
fire and the corresponding feature values for these
rules is set to 1. Firing of these rules (and thus
the effectiveness these features) is of course highly
dependent on the fact that Semafor provides an ac-
curate frame parse for the sentence.
</bodyText>
<subsectionHeader confidence="0.667475">
4.5 Semantic trees (FrameForest,
FrameTree, FrameTreeProp)
</subsectionHeader>
<bodyText confidence="0.9954724">
Semafor labels text spans in sentences as frame
evoking elements (FEE) or frame elements (FE).
A sentence usually has multiple frames and the
frame annotations may overlap. There may be two
ways in which spans overlap (Figure 1): (a) one
</bodyText>
<figureCaption confidence="0.941346">
Figure 1: Two overlapping scenarios for frame an-
notations of a sentence, where F1, F2 are frames.
</figureCaption>
<bodyText confidence="0.998064">
frame annotation is completely embedded in the
other frame annotation and (b) some of the frame
elements overlap (in terms of text spans). We now
present the three frame semantic tree kernel rep-
resentations that handle these overlapping issues,
along with providing a meaningful semantic ker-
nel representation for the tasks addressed in this
paper.
For each of the following representations,
we assume that for each sentence s, we have
the set of semantic frames, Fs = {F =
hFEE, [FE1, FE2, ... , FEnJi} with each frame
F having an FEE and a list of FEs.. We illustrate
the structures using sentence (5).
</bodyText>
<subsectionHeader confidence="0.813515">
4.5.1 FrameForest Tree Representation
</subsectionHeader>
<bodyText confidence="0.97670975">
We first create a tree for each frame annota-
tion F in the sentence. Consider a frame,
F = hFEE, [FE1, FE2, ... , FEnJi. For the
purposes of tree construction, we treat FEE as
another FE (call it FE0) of type Target. For
each FEi, we choose the subtree from the de-
pendency parse tree that is the smallest subtree
containing all words annotated as FEi by Se-
</bodyText>
<listItem confidence="0.908844333333333">
mafor. Call this subtree extracted from the de-
pendency parse DepTree_FEi. We then cre-
ate a larger tree by adding DepTree_FEi as
a child of a new node labeled with frame el-
ement FEi: (FEi DepTree_FEi). Call this
resulting tree SubTree_FEi. We then connect
all the SubTree_FEi (i ∈ {0,1, 2, ... , n}) to
a new root node labeled with the frame F:
(F SubTree_FE0 ... SubTree_FEn). This
</listItem>
<bodyText confidence="0.980171333333333">
is the tree for a frame F. Since the sentence
could have multiple frames, we connect the for-
est of frame trees to a new node called ROOT.
</bodyText>
<page confidence="0.99442">
214
</page>
<figure confidence="0.999623259259259">
ROOT Statement
Commerce_buy
Statement
Seller
from
T2-Grp
Target
A
Buyer
T1-Ind
Buyer
T1’-Ind
Seller
T2-Grp
he defendants
Target
claimed
A
Speaker
T1’-Ind
Message
A
Speaker
T1-Ind
Coleman
Message
Commerce_buy
</figure>
<figureCaption confidence="0.998314">
Figure 2: Semantic trees for the sentence “Coleman claimed [he]T1−Ind bought drugs from the
</figureCaption>
<bodyText confidence="0.97363203125">
[defendants]T2−Grp.”. The tree on the left is FrameForest and the tree on the right is FrameTree. A
in FrameForest refers to the subtree (bought (T1-Ind) (from T2-Grp)). Ind refers to individual and Grp
refers to group.
We prune away all subtrees that do not contain
the target entities. We refer to the resulting tree
as FrameForest.
For example, in Figure 2, the left tree is the
FrameForest tree for sentence (5). There are two
frames in this sentence that appear in the final tree
because both these frames contain the target enti-
ties and thus are not pruned away. The two frames
are Commerce_buy and Statement. We first cre-
ate trees for each of the frames. For the Com-
merce_buy frame, there are three frame elements:
Target (the frame evoking element), Buyer and
Seller. For each frame element, we get the sub-
tree from the dependency tree that contains all the
words belonging to that frame element. The sub-
tree for FEE Target is (bought T1-Ind (from T2-
Grp)). The subtree for FE Buyer is (T1-Ind) and
the subtree for FE Seller is (from T2-Grp). We
connect these subtrees to their respective frame el-
ements and connect the resulting subtrees to the
frame (Commerce_buy). Similarly, we create a
tree for the frame Statement. Finally, we connect
all frame trees to the ROOT.
In this representation, we have avoided the
frame overlapping issues by repeating the com-
mon subtrees: the subtree (bought T1-Ind (from
T2-Grp)) is repeated under the FEE Target of the
Statement frame as well as under the FE Message
of the Statement frame.
</bodyText>
<subsectionHeader confidence="0.899605">
4.5.2 FrameTree Tree Representation
</subsectionHeader>
<bodyText confidence="0.999389263157895">
For the design of this tree, we deal with the two
overlapping conditions shown in Figure 1 differ-
ently. If one frame is fully embedded in another
frame, we add the former as a child of the latter
frame. In Figure 2, the frame Commerce_buy is
fully embedded in the frame element Message of
the frame Statement. Therefore, the frame sub-
tree for Commerce_buy appears as a subtree of
Message.
If the frames overlap partially, we copy over the
overlapping portions of the structures to each of
the frame sub-trees.
For the design of this representation, we remove
all lexical nodes (struck out nodes in Figure 2) and
trees that do not span any of the target entities (not
shown in the figure). As a result, this structure
is the smallest semantic structure that contains the
two target entities. The right tree in Figure 2 is the
FrameTree tree for sentence (5).
</bodyText>
<subsectionHeader confidence="0.803298">
4.5.3 FrameTreeProp Tree Representation
</subsectionHeader>
<bodyText confidence="0.9998663125">
We are using a partial tree kernel (PTK) for calcu-
lating the similarity of two trees (as detailed in sec-
tion 5). The PTK does not skip over nodes of the
tree that lie on the same path. For establishing an
OBS social event between Coleman and the defen-
dants, all the structure needs to encode is the fact
that one target appears as a Speaker and the other
appears in the Message (of the speaker). In Frame-
Tree, this information is encoded but in an unclear
manner – there are two nodes (Commerce_buy
and Seller) that come in between the node Mes-
sage and T2-Grp.
For this reason, we copy the nodes labeled with
the target annotations (T1−*, T2−*) to all nodes
(that are frame elements of a frame) on the path
from them to the root in FrameTree. We call this
</bodyText>
<page confidence="0.997675">
215
</page>
<bodyText confidence="0.9999172">
variation of FrameTree, in which we propagate
T1 − *, T2 − * nodes to the root, FrameTreeP-
rop. For the running example, FrameTreeProp
will be: (Statement (Speaker T1-Ind) (Message
(Commerce_buy ...) (T2-Grp))). Using this tree
representation, one of the sub-trees in the implicit
feature space will be (Statement (Speaker T1-Ind)
(Message (T2-Grp)), which encodes the relation
between the two targets in a more direct manner
as compared to FrameTree.
</bodyText>
<sectionHeader confidence="0.995349" genericHeader="method">
5 Machine Learning
</sectionHeader>
<bodyText confidence="0.999888">
We represent our data in form of feature vectors
and tree structures. We use convolution kernels
(Haussler, 1999) that make use of the dual form
of Support Vector Machines (SVMs). In the dual
form, the optimization problem that SVM solves
is the following (Burges, 1998):
</bodyText>
<equation confidence="0.981662333333333">
max Eiµi − Ei,jµiµjyiyjK(xi, xj)
s.t. Eiµiyi = 0
µi &gt; 0 Vi = 1, 2,...,l
</equation>
<bodyText confidence="0.999611807692308">
Here, xi is the input example, yi is the class of
the example xi, µi is the Lagrange multiplier as-
sociated with example xi, l is the number of train-
ing examples, and K is the kernel function that
returns a similarity between two examples. More
formally, K is the function, K : X x X —* R,
that maps a pair of objects belonging to the set X
to a real number. For example, if we represent our
input examples as feature vectors, the set X would
be the set of feature vectors. For feature vectors,
we use a linear kernel, i.e. K(xi, xj) = xi · xj
(dot product of the two vectors). For our tree rep-
resentations, we use a Partial Tree Kernel (PTK),
first proposed by Moschitti (2006). PTK is a re-
laxed version of the Subset Tree (SST) kernel pro-
posed by Collins and Duffy (2002). A subset
tree kernel measures the similarity between two
trees by counting all subtrees common to the two
trees. However, there is one constraint: all daugh-
ter nodes of a parent node must be included (in
the sub-trees). In PTK, this constraint is removed.
Therefore, in contrast to SST, PT kernels compare
many more substructures. For a combination of
feature vectors and tree representations, we sim-
ply use the linear combination of their respective
kernels.
</bodyText>
<sectionHeader confidence="0.993252" genericHeader="method">
6 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999971444444445">
We present 5-fold cross-validation results on the
ACE2005 corpus annotated for social events.
Since the number of types of features and struc-
tures is not large (Table 2), we run an exhaustive
set of 27 − 1 = 127 experiments for each of three
tasks: Social Event Detection (SED), Social Event
Classification (SEC) and Social Network Extrac-
tion (SNE). To avoid over-fitting to a particular
partition into folds, we run each 5-fold experi-
ment 50 times, for 50 randomly generated parti-
tions. The results reported in the following tables
are all averaged over these 50 partitions. The ab-
solute standard deviation on an average is less than
0.004. This means that the performance of our
models across 50 random folds does not fluctuate
and hence the system is robust. We use McNe-
mar’s significance test and refer to statistical sig-
nificance as p &lt; 0.05.
</bodyText>
<subsectionHeader confidence="0.9770015">
6.1 Social event detection (SED) and
classification (SEC)
</subsectionHeader>
<bodyText confidence="0.999662807692308">
We report precision (P), recall (R) and F1 measure
for the detection task, and % accuracy for the clas-
sification task. For both these tasks, our previous
best performing system was PET_GR_SqGRW
(which we refer to as AR2010). We use this as
a baseline, and introduce two new baselines: the
bag-of-words (BOW) baseline and a linear com-
bination of BOW and AR2010, referred to as
BOW_AR2010.
Table 3 presents the results for these two tasks
for various features and structures. The results
show that our purely semantic models (RULES,
BOF, FrameTree, FrameTreeProp) do not perform
well alone. FrameForest, which encodes some
lexical and syntactic level features (but is primar-
ily semantic), also performs worse than the base-
lines when used alone. However, a combination
of lexical, syntactic and semantic structures im-
proves the performance by an absolute of 1.1% in
F1-measure for SED (from 0.574 to 0.585). This
gain is statistically significant. For SEC, the abso-
lute gain from our best baseline (BOW_AR2010)
is 0.8% in F1-measure (from 82.3 to 83.1), which
is not statistically significant. However, the gain
of 2% from our previously proposed best system
(AR2010) is statistically significant.
</bodyText>
<page confidence="0.996564">
216
</page>
<table confidence="0.999894153846154">
SED SEC SNE Hierarchical
Model P R F1 %Acc P R F1
BOW 0.343 0.391 0.365 70.9 0.247 0.277 0.261
AR2010 0.464 0.751 0.574 81.1 0.375 0.611 0.465
BOW_AR2010 0.488 0.645 0.555 82.3 0.399 0.532 0.456
RULES 0.508 0.097 0.164 60.2 0.301 0.059 0.099
BOF 0.296 0.416 0.346 64.4 0.183 0.266 0.217
FrameForest 0.331 0.594 0.425 74.5 0.247 0.442 0.317
FrameTree 0.295 0.594 0.395 68.3 0.206 0.405 0.273
FrameTreeProp 0.308 0.554 0.396 70.7 0.217 0.390 0.279
All 0.494 0.641 0.558 82.5 0.405 0.531 0.460
BOW_AR2010_FrameForest_FrameTreeProp 0.490 0.633 0.552 83.1 0.405 0.528 0.459
AR2010_FrameTreeProp 0.484 0.740 0.585 82.0 0.397 0.608 0.480
</table>
<tableCaption confidence="0.989787">
Table 3: Results for three tasks: “SED” is Social Event Detection, “SEC” is Social Event Classification,
</tableCaption>
<bodyText confidence="0.9586612">
“SNE” is Social Network Extraction. The first three models are the baseline models. The next five
models are the novel semantic features and structures we propose in this paper. “All” refers to the
model that uses all the listed structures together. “BOW_AR2010_FrameForest_FrameTreeProp” refers
to the model that uses a linear combination of mentioned structures. AR2010_FrameTreeProp is a linear
combination of AR2010 and FrameTreeProp.
</bodyText>
<subsectionHeader confidence="0.999134">
6.2 Social network extraction (SNE)
</subsectionHeader>
<bodyText confidence="0.999702787878788">
Social network extraction is a multi-way classifi-
cation task, in which, given an example, we clas-
sify it into one of three categories: {No-Event,
INR, OBS}. A popular technique of performing
multi-way classification using a binary classifier
like SVM, is one-versus-all (OVA). We try this
along with a less commonly used technique, in
which we stack two binary classifiers in a hier-
archy. For the hierarchical design, we train two
models: (1) the SED model ({INR + OBS} ver-
sus No-Event) and (2) the SEC model (INR versus
OBS). Given a test example, it is first classified us-
ing the SED model. If the prediction is less than
zero, we label it as No-Event. Otherwise, the test
example is passed onto SEC and finally classified
into either INR or OBS.
We see that none of the semantic features and
structures alone outperform the baseline. How-
ever, a combination of structures from different
levels of abstraction achieve the best performance:
an absolute gain of 1.5% in F1 (statistically sig-
nificant) when we use a hierarchical design (from
0.465 to 0.480).
Comparing hierarchical verus OVA approaches,
we observe that the hierarchical approach
outperforms the OVA approach for all our
models by a statistically significant margin.
The performance for our best reported model
(AR2010_FrameTreeProp) for OVA in terms
precision, recall, and F1-measure is 0.375, 0.592,
0.459 respectively. This is statistically signifi-
cantly worse than hierarchical approach (0.397,
0.608, 0.480).
</bodyText>
<subsectionHeader confidence="0.999866">
6.3 Discussion of results
</subsectionHeader>
<bodyText confidence="0.999868590909091">
Performing well on SED is more important than
SEC, because if a social event is not detected in
the first place, the goodness of the SEC model is
irrelevant. Therefore, the best feature and struc-
ture combination we report in this paper is a com-
bination of AR2010 and FrameTreeProp.
To gain insight into the how each type of se-
mantic feature and structure contribute to our
previously proposed lexical and syntactic model
(AR2010), we perform experiments in which we
add one semantic feature/structure at a time to
AR2010. Table 4 presents the results for this
study. We see that the hand-crafted RULES do
not help in the overall task. We investigated the
reason for RULES not being as helpful as we had
expected. We found that when there is no social
event, the rules fire in 7% of the cases. When
there is a social event, they fire in 17% of cases.
So while they fire more often when there is a so-
cial event, the percentage of cases in which they
fire is small. We hypothesize that this is due the
dependence of RULES on the correctness of se-
</bodyText>
<page confidence="0.991203">
217
</page>
<bodyText confidence="0.999957052631579">
mantic parses. For example, Rule (4) correctly
detects the social event in sentence (5), since Se-
mafor correctly parses the input. In contrast, Se-
mafor does not correctly parse the input sentence
(1): it correctly identifies the Statement frame and
its Message frame element, but it fails to find the
Speaker. As a result, Rule (3) does not fire, even
though the semantic structure is partially identi-
fied. This, we believe, highlights the main strength
of tree kernels – they are able to learn seman-
tic patterns, without requiring correctness or com-
pleteness of the semantic parse.
Out of the semantic structures we propose,
FrameTreeProp adds the most value to the base-
line system as compared to other semantic features
and structures. This supports our intuition that we
need to reduce unbounded semantic dependencies
between the target entities by propagating the tar-
get entity tags to the top of the semantic tree.
</bodyText>
<table confidence="0.999216125">
Model SED SEC SNE Hier.
(F1) (%A) (F1)
AR2010 0.574 81.1 0.465
+ RULES 0.576 80.8 0.465
+ BOF 0.569 80.7 0.459
+ FrameForest 0.571 82.6 0.472
+ FrameTree 0.579 81.5 0.473
+ FrameTreeProp 0.585 82.0 0.480
</table>
<tableCaption confidence="0.996386">
Table 4: A study to show which semantic features
</tableCaption>
<bodyText confidence="0.9984136">
and structures add the most value to the baseline.
The top row gives the performance of the base-
line. Each consecutive row shows the result of
the baseline plus the feature/structure mentioned
in that row.
</bodyText>
<sectionHeader confidence="0.999959" genericHeader="method">
7 Related Work
</sectionHeader>
<bodyText confidence="0.998544447368421">
There have been recent efforts to extract net-
works from text (Elson et al., 2010; He et al.,
2013). However, these efforts extract a different
type of network: a network of only bi-directional
links, where the links are triggered by quotation
marks. For example, Elson et al. (2010) and He
et al. (2013) will extract an interaction link be-
tween Emma and Harriet in the following sen-
tence. However, their system will not detect any
interaction links in the other examples mentioned
in this paper.
(6) “Take it,” said Emma, smiling, and pushing
the paper towards Harriet “it is for you. Take
your own.”
Our approach to extract and classify social
events builds on our previous work (Agarwal and
Rambow, 2010), which in turn builds on work
from the relation extraction community (Nguyen
et al., 2009). Therefore, the task of relation extrac-
tion is most closely related to the tasks addressed
in this paper. Researchers have used other notions
of semantics in the literature such as latent se-
mantic analysis (Plank and Moschitti, 2013) and
relation-specific semantics (Zelenko et al., 2003;
Culotta and Sorensen, 2004). To the best of our
knowledge, there is only one work that uses frame
semantics for relation extraction (Harabagiu et al.,
2005). Harabagiu et al. (2005) propose a novel se-
mantic kernel that incorporates frame parse infor-
mation in the kernel computation that calculates
similarity between two dependency trees. They,
however, do not propose data representations that
are based on frame parses and the resulting ar-
borescent structures, instead adding features to
syntactic trees. We believe the implicit feature
space of kernels based on our data representation
encode a richer and larger feature space than the
one proposed by Harabagiu et al. (2005).
</bodyText>
<sectionHeader confidence="0.976548" genericHeader="conclusions">
8 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9998906">
This work has only scratched the surface of possi-
bilities for using frame semantic features and tree
structures for the task of social event extraction.
We have shown that tree kernels are well suited to
work with possibly inaccurate semantic parses in
contrast to hand-crafted features that require the
semantic parses to be completely accurate. We
have also extended our previous work by design-
ing and evaluating a full system for social network
extraction.
A more natural data representation for seman-
tic parses is a graph structure. We are actively
exploring the design of semantic graph structures
that may be brought to bear with the use of graph
kernels (Vishwanathan et al., 2010).
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999527428571429">
We would like to thank CCLS’s IT heads, Hatim
Diab and Manoj Pooleery, for providing the infras-
tructure support. This paper is based upon work
supported in part by the DARPA DEFT Program.
The views expressed are those of the authors and
do not reflect the official policy or position of the
Department of Defense or the U.S. Government.
</bodyText>
<page confidence="0.997846">
218
</page>
<sectionHeader confidence="0.990026" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999590295238095">
Apoorv Agarwal and Owen Rambow. 2010. Auto-
matic detection and classification of social events.
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1024–1034, Cambridge, MA, October. Association
for Computational Linguistics.
Apoorv Agarwal, Owen C. Rambow, and Rebecca J.
Passonneau. 2010. Annotation scheme for social
network extraction from text. In Proceedings of the
Fourth Linguistic Annotation Workshop.
Apoorv Agarwal, Anup Kotalwar, and Owen Ram-
bow. 2013a. Automatic extraction of social net-
works from literary text: A case study on alice in
wonderland. In the Proceedings of the 6th Interna-
tional Joint Conference on Natural Language Pro-
cessing (IJCNLP 2013).
Apoorv Agarwal, Anup Kotalwar, Jiehan Zheng, and
Owen Rambow. 2013b. Sinnet: Social interaction
network extractor from text. In Sixth International
Joint Conference on Natural Language Processing,
page 33.
Collin F. Baker, J. Fillmore, and John B. Lowe. 1998.
The Berkeley FrameNet project. In 36th Meet-
ing of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics (COLING-ACL’98), pages 86–90,
Montréal.
Chris Burges. 1998. A tutorial on support vector ma-
chines for pattern recognition. Data mining and
knowledge discovery.
Desai Chen, Nathan Schneider, Dipanjan Das, and
Noah A. Smith. 2010. Semafor: Frame argument
resolution with log-linear models. In Proceedings of
the 5th International Workshop on Semantic Evalu-
ation, pages 264–267, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels over
discrete structures, and the voted perceptron. In Pro-
ceedings of the 40th annual meeting on association
for computational linguistics, pages 263–270. Asso-
ciation for Computational Linguistics.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings
of the 42nd Meeting of the Association for Compu-
tational Linguistics (ACL’04), Main Volume, pages
423–429, Barcelona, Spain, July.
David K. Elson, Nicholas Dames, and Kathleen R.
McKeown. 2010. Extracting social networks from
literary fiction. Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 138–147.
Zhou GuoDong, Su Jian, Zhang Jie, and Zhang Min.
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of 43th Annual Meeting of
the Association for Computational Linguistics.
Sanda Harabagiu, Cosmin Adrian Bejan, and Paul
Morarescu. 2005. Shallow semantics for relation
extraction. In International Joint Conference On Ar-
tificial Intelligence.
David Haussler. 1999. Convolution kernels on discrete
structures. Technical report, University of Califor-
nia at Santa Cruz.
Hua He, Denilson Barbosa, and Grzegorz Kondrak.
2013. Identification of speakers in novels. The
51st Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2013).
Nanda Kambhatla. 2004. Combining lexical, syntac-
tic, and semantic features with maximum entropy
models for extracting relations. In Proceedings of
the ACL 2004 on Interactive poster and demonstra-
tion sessions, page 22. Association for Computa-
tional Linguistics.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. Proceedings of the 41st
Meeting of the Association for Computational Lin-
guistics, pages 423–430.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Proceedings of the 17th European Conference on
Machine Learning.
Truc-Vien T. Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009. Convolution kernels on
constituent, dependency and sequential structures
for relation extraction. Conference on Empirical
Methods in Natural Language Processing.
Barbara Plank and Alessandro Moschitti. 2013. Em-
bedding semantic similarity in tree kernels for do-
main adaptation of relation extraction. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 1498–1507, Sofia, Bulgaria, August.
Association for Computational Linguistics.
SVN Vishwanathan, Nicol N Schraudolph, Risi Kon-
dor, and Karsten M Borgwardt. 2010. Graph ker-
nels. The Journal of Machine Learning Research,
11:1201–1242.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. The Journal of Machine Learning Re-
search, 3:1083–1106.
Shubin Zhao and Ralph Grishman. 2005. Extract-
ing relations with integrated information using ker-
nel methods. In Proceedings of the 43rd Meeting of
the ACL.
</reference>
<page confidence="0.9992">
219
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.127865">
<title confidence="0.998877">Frame Semantic Tree Kernels for Social Network Extraction from Text</title>
<author confidence="0.985177">Apoorv Agarwal Sriramkumar Balasubramanian</author>
<affiliation confidence="0.999878">Dept. of Computer Science Dept. of Computer Science Columbia University Columbia University</affiliation>
<address confidence="0.999715">New York, NY, USA New York, NY, USA</address>
<email confidence="0.583546">Anup</email>
<affiliation confidence="0.927415">Microsoft,</affiliation>
<address confidence="0.998206">Redmonad, WA, USA</address>
<author confidence="0.550475">Jiehan The Peddie</author>
<address confidence="0.950777">Hightstown, NJ, USA</address>
<author confidence="0.842022">Owen</author>
<affiliation confidence="0.773744">Columbia</affiliation>
<address confidence="0.998364">New York, NY, USA</address>
<email confidence="0.999634">apoorv@cs.columbia.edu</email>
<abstract confidence="0.999379555555555">In this paper, we present work on extracting social networks from unstructured text. We introduce novel features derived from semantic annotations based on FrameNet. We also introduce novel semantic tree kernels that help us improve the performance of the best reported syson event detection classia statistically significant margin. We show results for combining the models for the two aforementioned subinto the overall task of net- We show that a combination of features from all three levels of abstractions (lexical, syntactic and semantic) are required to achieve the best performing system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Apoorv Agarwal</author>
<author>Owen Rambow</author>
</authors>
<title>Automatic detection and classification of social events.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1024--1034</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="1215" citStr="Agarwal and Rambow, 2010" startWordPosition="184" endWordPosition="187">We also introduce novel semantic tree kernels that help us improve the performance of the best reported system on social event detection and classification by a statistically significant margin. We show results for combining the models for the two aforementioned subtasks into the overall task of social network extraction. We show that a combination of features from all three levels of abstractions (lexical, syntactic and semantic) are required to achieve the best performing system. 1 Introduction Social network extraction from text has recently been gaining a considerable amount of attention (Agarwal and Rambow, 2010; Elson et al., 2010; Agarwal et al., 2013a; Agarwal et al., 2013b; He et al., 2013). One of the reason for this attention, we believe, is that being able to extract social networks from unstructured text may provide a powerful new tool for historians, political scientists, scholars of literature, and journalists to analyze large collections of texts around entities and their interactions. The tool would allow researchers to quickly extract networks and assess their size, nature, and cohesiveness, a task that would otherwise be impossible with corpora numbering millions of documents. It would </context>
<context position="3736" citStr="Agarwal and Rambow, 2010" startWordPosition="578" endWordPosition="581"> add less value to the overall performance in comparison with the frame-semantic tree kernels. We believe this is due to the fact that hand-crafted features require frame parses to be highly accurate and complete. In contrast, tree kernels are able to find and leverage less strict patterns without requiring the semantic parse to be entirely accurate or complete. Apart from introducing semantic features and tree structures, we evaluate on the task of social network extraction, which is a combination of two sub-tasks: social event detection and social event classification. In our previous work (Agarwal and Rambow, 2010), we presented results for the two 211 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 211–219, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics sub-tasks, but no evaluation was presented for the task of social network extraction. We experiment with two different designs of combining models for the two sub-tasks: 1) One-versus-All and 2) Hierarchical. We find that the hierarchical design outperforms the more commonly used Oneversus-All by a statistically significant margin. Following are th</context>
<context position="11099" citStr="Agarwal and Rambow (2010)" startWordPosition="1823" endWordPosition="1826">a pair of entity mentions between whom we predict and classify social events. and Porter Stemming. We convert each example (~x) to a set of three boolean vectors: {~b1, ~b2, ~b3}. ~b1 is the occurrence of words before the first target, ~b2 between the two targets and ~b3 after the second target. Here the first target and second target are defined in terms of the surface order of words. Though these features have been previously proposed for relation extraction on ACE (GuoDong et al., 2005), they have not been utilized for the task we address in this paper. 4.2 Syntactic structures (AR2010) In Agarwal and Rambow (2010), we explored a wide range of syntactic structures for the two tasks of social event detection (SED) and classification (SEC). All our previous structures were derived from a variation of two underlying tree structures: phrase structure trees and dependency trees. The best structure we proposed was PET_GR_SqGRW, which was a linear combination of two tree kernels and one word kernel: 1) a structure derived from a phrase structure tree (PET); 2) a grammatical role tree (GR), which is a dependency tree in which words are replaced with their grammatical roles; and 3) a path from one entity to the </context>
<context position="29881" citStr="Agarwal and Rambow, 2010" startWordPosition="5030" endWordPosition="5033"> He et al., 2013). However, these efforts extract a different type of network: a network of only bi-directional links, where the links are triggered by quotation marks. For example, Elson et al. (2010) and He et al. (2013) will extract an interaction link between Emma and Harriet in the following sentence. However, their system will not detect any interaction links in the other examples mentioned in this paper. (6) “Take it,” said Emma, smiling, and pushing the paper towards Harriet “it is for you. Take your own.” Our approach to extract and classify social events builds on our previous work (Agarwal and Rambow, 2010), which in turn builds on work from the relation extraction community (Nguyen et al., 2009). Therefore, the task of relation extraction is most closely related to the tasks addressed in this paper. Researchers have used other notions of semantics in the literature such as latent semantic analysis (Plank and Moschitti, 2013) and relation-specific semantics (Zelenko et al., 2003; Culotta and Sorensen, 2004). To the best of our knowledge, there is only one work that uses frame semantics for relation extraction (Harabagiu et al., 2005). Harabagiu et al. (2005) propose a novel semantic kernel that </context>
</contexts>
<marker>Agarwal, Rambow, 2010</marker>
<rawString>Apoorv Agarwal and Owen Rambow. 2010. Automatic detection and classification of social events. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1024–1034, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Apoorv Agarwal</author>
<author>Owen C Rambow</author>
<author>Rebecca J Passonneau</author>
</authors>
<title>Annotation scheme for social network extraction from text.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourth Linguistic Annotation Workshop.</booktitle>
<contexts>
<context position="2026" citStr="Agarwal et al., 2010" startWordPosition="314" endWordPosition="317">tructured text may provide a powerful new tool for historians, political scientists, scholars of literature, and journalists to analyze large collections of texts around entities and their interactions. The tool would allow researchers to quickly extract networks and assess their size, nature, and cohesiveness, a task that would otherwise be impossible with corpora numbering millions of documents. It would also make it possible to make falsifiable claims about these networks, bringing the experimental method to disciplines like history, where it is still relatively rare. In our previous work (Agarwal et al., 2010), we proposed a definition of a network based on interactions: nodes are entities and links are social events. We defined two broad types of links: one-directional links (one person thinking about or talking about another person) and bi-directional links (two people having a conversation, a meeting, etc.). For example, in the following sentence, we would add two links to the network: a one-directional link between Toujan Faisal and the committee, triggered by the word said (because Toujan is talking about the committee) and a bi-directional link between the same entities triggered by the word </context>
<context position="5716" citStr="Agarwal et al. (2010)" startWordPosition="895" endWordPosition="898">s follows: In Section 2, we give a precise definition of the task and describe the data. In Section 3, we give a brief overview of frame semantics and motivate the need to use frame semantics for the tasks addressed in this paper. In Section 4, we present semantic features and tree kernel representations designed for the tasks. In Section 5, we briefly review tree kernels and support vector machines (SVM). In Section 6 we present experiments and discuss the results. In Section 7 we discuss related work. We conclude and give future directions of work in Section 8. 2 Data and Task Definition In Agarwal et al. (2010), we presented the annotation details of social events on a well-known corpus – Automated Content Extraction1 (ACE2005). We defined a social event to be a happening between two entities (of type person) E1 and E2 (E1 =� E2), in which at least one entity is cognitively aware of the other and of the happening taking place. We defined two broad cate1Version: 6.0, Catalog number: LDC2005E18 No-Event INR OBS # of Examples 1,609 199 199 Table 1: Data distribution; INR are interaction social events. OBS are observation social events. gories of social events: Interaction (INR) and Observation (OBS). I</context>
</contexts>
<marker>Agarwal, Rambow, Passonneau, 2010</marker>
<rawString>Apoorv Agarwal, Owen C. Rambow, and Rebecca J. Passonneau. 2010. Annotation scheme for social network extraction from text. In Proceedings of the Fourth Linguistic Annotation Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Apoorv Agarwal</author>
<author>Anup Kotalwar</author>
<author>Owen Rambow</author>
</authors>
<title>Automatic extraction of social networks from literary text: A case study on alice in wonderland.</title>
<date>2013</date>
<booktitle>In the Proceedings of the 6th International Joint Conference on Natural Language Processing (IJCNLP</booktitle>
<contexts>
<context position="1257" citStr="Agarwal et al., 2013" startWordPosition="192" endWordPosition="195">that help us improve the performance of the best reported system on social event detection and classification by a statistically significant margin. We show results for combining the models for the two aforementioned subtasks into the overall task of social network extraction. We show that a combination of features from all three levels of abstractions (lexical, syntactic and semantic) are required to achieve the best performing system. 1 Introduction Social network extraction from text has recently been gaining a considerable amount of attention (Agarwal and Rambow, 2010; Elson et al., 2010; Agarwal et al., 2013a; Agarwal et al., 2013b; He et al., 2013). One of the reason for this attention, we believe, is that being able to extract social networks from unstructured text may provide a powerful new tool for historians, political scientists, scholars of literature, and journalists to analyze large collections of texts around entities and their interactions. The tool would allow researchers to quickly extract networks and assess their size, nature, and cohesiveness, a task that would otherwise be impossible with corpora numbering millions of documents. It would also make it possible to make falsifiable </context>
</contexts>
<marker>Agarwal, Kotalwar, Rambow, 2013</marker>
<rawString>Apoorv Agarwal, Anup Kotalwar, and Owen Rambow. 2013a. Automatic extraction of social networks from literary text: A case study on alice in wonderland. In the Proceedings of the 6th International Joint Conference on Natural Language Processing (IJCNLP 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Apoorv Agarwal</author>
<author>Anup Kotalwar</author>
<author>Jiehan Zheng</author>
<author>Owen Rambow</author>
</authors>
<title>Sinnet: Social interaction network extractor from text.</title>
<date>2013</date>
<booktitle>In Sixth International Joint Conference on Natural Language Processing,</booktitle>
<pages>33</pages>
<contexts>
<context position="1257" citStr="Agarwal et al., 2013" startWordPosition="192" endWordPosition="195">that help us improve the performance of the best reported system on social event detection and classification by a statistically significant margin. We show results for combining the models for the two aforementioned subtasks into the overall task of social network extraction. We show that a combination of features from all three levels of abstractions (lexical, syntactic and semantic) are required to achieve the best performing system. 1 Introduction Social network extraction from text has recently been gaining a considerable amount of attention (Agarwal and Rambow, 2010; Elson et al., 2010; Agarwal et al., 2013a; Agarwal et al., 2013b; He et al., 2013). One of the reason for this attention, we believe, is that being able to extract social networks from unstructured text may provide a powerful new tool for historians, political scientists, scholars of literature, and journalists to analyze large collections of texts around entities and their interactions. The tool would allow researchers to quickly extract networks and assess their size, nature, and cohesiveness, a task that would otherwise be impossible with corpora numbering millions of documents. It would also make it possible to make falsifiable </context>
</contexts>
<marker>Agarwal, Kotalwar, Zheng, Rambow, 2013</marker>
<rawString>Apoorv Agarwal, Anup Kotalwar, Jiehan Zheng, and Owen Rambow. 2013b. Sinnet: Social interaction network extractor from text. In Sixth International Joint Conference on Natural Language Processing, page 33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The Berkeley FrameNet project.</title>
<date>1998</date>
<booktitle>In 36th Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics (COLING-ACL’98),</booktitle>
<pages>86--90</pages>
<location>Montréal.</location>
<contexts>
<context position="8089" citStr="Baker et al., 1998" startWordPosition="1316" endWordPosition="1319">ass of the example from one of the following three categories: {No-Event, INR, OBS}. For the purposes of this paper, we use gold named entity mentions to avoid errors caused due to named entity recognition systems. This is a common practice used in the literature for reporting relation extraction systems (Zelenko et al., 2003; Kambhatla, 2004; Zhao and Grishman, 2005; GuoDong et al., 2005; Harabagiu et al., 2005; Nguyen et al., 2009). We use standard terminology from the literature to refer to the pair of entities mentions as target entities T1 and T2. 3 Frame Semantics and FrameNet FrameNet (Baker et al., 1998) is a resource which associates words of English with their meaning. Word meanings are based on the notion of “semantic frame”. A frame is a conceptual description of a type of event, relation, or entity, and it 212 includes a list of possible participants in terms of the roles they play; these participants are called “frame elements”. Through the following example, we present the terminology and acronyms that will be used throughout the paper. Example (2) shows the frame annotations for the sentence Toujan Faisal said she was informed of the refusal by an Interior Ministry committee. One of t</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet project. In 36th Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics (COLING-ACL’98), pages 86–90, Montréal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Burges</author>
</authors>
<title>A tutorial on support vector machines for pattern recognition. Data mining and knowledge discovery.</title>
<date>1998</date>
<contexts>
<context position="20580" citStr="Burges, 1998" startWordPosition="3468" endWordPosition="3469">eeProp will be: (Statement (Speaker T1-Ind) (Message (Commerce_buy ...) (T2-Grp))). Using this tree representation, one of the sub-trees in the implicit feature space will be (Statement (Speaker T1-Ind) (Message (T2-Grp)), which encodes the relation between the two targets in a more direct manner as compared to FrameTree. 5 Machine Learning We represent our data in form of feature vectors and tree structures. We use convolution kernels (Haussler, 1999) that make use of the dual form of Support Vector Machines (SVMs). In the dual form, the optimization problem that SVM solves is the following (Burges, 1998): max Eiµi − Ei,jµiµjyiyjK(xi, xj) s.t. Eiµiyi = 0 µi &gt; 0 Vi = 1, 2,...,l Here, xi is the input example, yi is the class of the example xi, µi is the Lagrange multiplier associated with example xi, l is the number of training examples, and K is the kernel function that returns a similarity between two examples. More formally, K is the function, K : X x X —* R, that maps a pair of objects belonging to the set X to a real number. For example, if we represent our input examples as feature vectors, the set X would be the set of feature vectors. For feature vectors, we use a linear kernel, i.e. K(x</context>
</contexts>
<marker>Burges, 1998</marker>
<rawString>Chris Burges. 1998. A tutorial on support vector machines for pattern recognition. Data mining and knowledge discovery.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Desai Chen</author>
<author>Nathan Schneider</author>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>Semafor: Frame argument resolution with log-linear models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>264--267</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="12099" citStr="Chen et al., 2010" startWordPosition="1997" endWordPosition="2000">el: 1) a structure derived from a phrase structure tree (PET); 2) a grammatical role tree (GR), which is a dependency tree in which words are replaced with their grammatical roles; and 3) a path from one entity to the other in a dependency tree, in which grammatical roles of words are inserted as additional nodes between the dependent and parent (SqGRW). We refer the reader to Agarwal and Rambow (2010) for details of these structures. For the rest of the paper, we refer to this structure, PET_GR_SqGRW, as “AR2010”. We use AR2010 as one of our baselines. 4.3 Bag of frames (BOF) We use Semafor (Chen et al., 2010) for obtaining the semantic parse of a sentence. Semafor found instances of 1,174 different FrameNet frames in our corpus. Each example (~x) is converted to a vector of dimension 1,174, in which xi (the ith component of vector ~x) is 1 if the frame number i appears in the example, and 0 otherwise. 4.4 Hand-crafted semantic features (RULES) We use the manual of the FrameNet resource to hand-craft 199 rules that are intended to detect the presence and determine the type of social events between two entities mentioned in a sentence. An example of one such rule is given in section 3, which we refo</context>
</contexts>
<marker>Chen, Schneider, Das, Smith, 2010</marker>
<rawString>Desai Chen, Nathan Schneider, Dipanjan Das, and Noah A. Smith. 2010. Semafor: Frame argument resolution with log-linear models. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 264–267, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th annual meeting on association for computational linguistics,</booktitle>
<pages>263--270</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="21426" citStr="Collins and Duffy (2002)" startWordPosition="3633" endWordPosition="3636">ing examples, and K is the kernel function that returns a similarity between two examples. More formally, K is the function, K : X x X —* R, that maps a pair of objects belonging to the set X to a real number. For example, if we represent our input examples as feature vectors, the set X would be the set of feature vectors. For feature vectors, we use a linear kernel, i.e. K(xi, xj) = xi · xj (dot product of the two vectors). For our tree representations, we use a Partial Tree Kernel (PTK), first proposed by Moschitti (2006). PTK is a relaxed version of the Subset Tree (SST) kernel proposed by Collins and Duffy (2002). A subset tree kernel measures the similarity between two trees by counting all subtrees common to the two trees. However, there is one constraint: all daughter nodes of a parent node must be included (in the sub-trees). In PTK, this constraint is removed. Therefore, in contrast to SST, PT kernels compare many more substructures. For a combination of feature vectors and tree representations, we simply use the linear combination of their respective kernels. 6 Experiments and Results We present 5-fold cross-validation results on the ACE2005 corpus annotated for social events. Since the number o</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Michael Collins and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 263–270. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Jeffrey Sorensen</author>
</authors>
<title>Dependency tree kernels for relation extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>423--429</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="30289" citStr="Culotta and Sorensen, 2004" startWordPosition="5094" endWordPosition="5097">s paper. (6) “Take it,” said Emma, smiling, and pushing the paper towards Harriet “it is for you. Take your own.” Our approach to extract and classify social events builds on our previous work (Agarwal and Rambow, 2010), which in turn builds on work from the relation extraction community (Nguyen et al., 2009). Therefore, the task of relation extraction is most closely related to the tasks addressed in this paper. Researchers have used other notions of semantics in the literature such as latent semantic analysis (Plank and Moschitti, 2013) and relation-specific semantics (Zelenko et al., 2003; Culotta and Sorensen, 2004). To the best of our knowledge, there is only one work that uses frame semantics for relation extraction (Harabagiu et al., 2005). Harabagiu et al. (2005) propose a novel semantic kernel that incorporates frame parse information in the kernel computation that calculates similarity between two dependency trees. They, however, do not propose data representations that are based on frame parses and the resulting arborescent structures, instead adding features to syntactic trees. We believe the implicit feature space of kernels based on our data representation encode a richer and larger feature spa</context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree kernels for relation extraction. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume, pages 423–429, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David K Elson</author>
<author>Nicholas Dames</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Extracting social networks from literary fiction.</title>
<date>2010</date>
<booktitle>Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>138--147</pages>
<contexts>
<context position="1235" citStr="Elson et al., 2010" startWordPosition="188" endWordPosition="191">mantic tree kernels that help us improve the performance of the best reported system on social event detection and classification by a statistically significant margin. We show results for combining the models for the two aforementioned subtasks into the overall task of social network extraction. We show that a combination of features from all three levels of abstractions (lexical, syntactic and semantic) are required to achieve the best performing system. 1 Introduction Social network extraction from text has recently been gaining a considerable amount of attention (Agarwal and Rambow, 2010; Elson et al., 2010; Agarwal et al., 2013a; Agarwal et al., 2013b; He et al., 2013). One of the reason for this attention, we believe, is that being able to extract social networks from unstructured text may provide a powerful new tool for historians, political scientists, scholars of literature, and journalists to analyze large collections of texts around entities and their interactions. The tool would allow researchers to quickly extract networks and assess their size, nature, and cohesiveness, a task that would otherwise be impossible with corpora numbering millions of documents. It would also make it possibl</context>
<context position="29255" citStr="Elson et al., 2010" startWordPosition="4924" endWordPosition="4927">agating the target entity tags to the top of the semantic tree. Model SED SEC SNE Hier. (F1) (%A) (F1) AR2010 0.574 81.1 0.465 + RULES 0.576 80.8 0.465 + BOF 0.569 80.7 0.459 + FrameForest 0.571 82.6 0.472 + FrameTree 0.579 81.5 0.473 + FrameTreeProp 0.585 82.0 0.480 Table 4: A study to show which semantic features and structures add the most value to the baseline. The top row gives the performance of the baseline. Each consecutive row shows the result of the baseline plus the feature/structure mentioned in that row. 7 Related Work There have been recent efforts to extract networks from text (Elson et al., 2010; He et al., 2013). However, these efforts extract a different type of network: a network of only bi-directional links, where the links are triggered by quotation marks. For example, Elson et al. (2010) and He et al. (2013) will extract an interaction link between Emma and Harriet in the following sentence. However, their system will not detect any interaction links in the other examples mentioned in this paper. (6) “Take it,” said Emma, smiling, and pushing the paper towards Harriet “it is for you. Take your own.” Our approach to extract and classify social events builds on our previous work </context>
</contexts>
<marker>Elson, Dames, McKeown, 2010</marker>
<rawString>David K. Elson, Nicholas Dames, and Kathleen R. McKeown. 2010. Extracting social networks from literary fiction. Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 138–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhou GuoDong</author>
<author>Su Jian</author>
<author>Zhang Jie</author>
<author>Zhang Min</author>
</authors>
<title>Exploring various knowledge in relation extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of 43th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7861" citStr="GuoDong et al., 2005" startWordPosition="1276" endWordPosition="1279"> (INR or OBS, social event classification, SEC). In this paper, we evaluate our system on the above tasks as well as a combined task: social network extraction (SNE): given a sentence and a pair of entity mentions, predict the class of the example from one of the following three categories: {No-Event, INR, OBS}. For the purposes of this paper, we use gold named entity mentions to avoid errors caused due to named entity recognition systems. This is a common practice used in the literature for reporting relation extraction systems (Zelenko et al., 2003; Kambhatla, 2004; Zhao and Grishman, 2005; GuoDong et al., 2005; Harabagiu et al., 2005; Nguyen et al., 2009). We use standard terminology from the literature to refer to the pair of entities mentions as target entities T1 and T2. 3 Frame Semantics and FrameNet FrameNet (Baker et al., 1998) is a resource which associates words of English with their meaning. Word meanings are based on the notion of “semantic frame”. A frame is a conceptual description of a type of event, relation, or entity, and it 212 includes a list of possible participants in terms of the roles they play; these participants are called “frame elements”. Through the following example, we </context>
<context position="10968" citStr="GuoDong et al., 2005" startWordPosition="1801" endWordPosition="1804"> using the Stanford tokenizer (Klein and Manning, 2003) followed by removal of stop words 2An input example is a sentence with a pair of entity mentions between whom we predict and classify social events. and Porter Stemming. We convert each example (~x) to a set of three boolean vectors: {~b1, ~b2, ~b3}. ~b1 is the occurrence of words before the first target, ~b2 between the two targets and ~b3 after the second target. Here the first target and second target are defined in terms of the surface order of words. Though these features have been previously proposed for relation extraction on ACE (GuoDong et al., 2005), they have not been utilized for the task we address in this paper. 4.2 Syntactic structures (AR2010) In Agarwal and Rambow (2010), we explored a wide range of syntactic structures for the two tasks of social event detection (SED) and classification (SEC). All our previous structures were derived from a variation of two underlying tree structures: phrase structure trees and dependency trees. The best structure we proposed was PET_GR_SqGRW, which was a linear combination of two tree kernels and one word kernel: 1) a structure derived from a phrase structure tree (PET); 2) a grammatical role tr</context>
</contexts>
<marker>GuoDong, Jian, Jie, Min, 2005</marker>
<rawString>Zhou GuoDong, Su Jian, Zhang Jie, and Zhang Min. 2005. Exploring various knowledge in relation extraction. In Proceedings of 43th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Cosmin Adrian Bejan</author>
<author>Paul Morarescu</author>
</authors>
<title>Shallow semantics for relation extraction.</title>
<date>2005</date>
<booktitle>In International Joint Conference On Artificial Intelligence.</booktitle>
<contexts>
<context position="7885" citStr="Harabagiu et al., 2005" startWordPosition="1280" endWordPosition="1283">vent classification, SEC). In this paper, we evaluate our system on the above tasks as well as a combined task: social network extraction (SNE): given a sentence and a pair of entity mentions, predict the class of the example from one of the following three categories: {No-Event, INR, OBS}. For the purposes of this paper, we use gold named entity mentions to avoid errors caused due to named entity recognition systems. This is a common practice used in the literature for reporting relation extraction systems (Zelenko et al., 2003; Kambhatla, 2004; Zhao and Grishman, 2005; GuoDong et al., 2005; Harabagiu et al., 2005; Nguyen et al., 2009). We use standard terminology from the literature to refer to the pair of entities mentions as target entities T1 and T2. 3 Frame Semantics and FrameNet FrameNet (Baker et al., 1998) is a resource which associates words of English with their meaning. Word meanings are based on the notion of “semantic frame”. A frame is a conceptual description of a type of event, relation, or entity, and it 212 includes a list of possible participants in terms of the roles they play; these participants are called “frame elements”. Through the following example, we present the terminology </context>
<context position="30418" citStr="Harabagiu et al., 2005" startWordPosition="5116" endWordPosition="5119">ract and classify social events builds on our previous work (Agarwal and Rambow, 2010), which in turn builds on work from the relation extraction community (Nguyen et al., 2009). Therefore, the task of relation extraction is most closely related to the tasks addressed in this paper. Researchers have used other notions of semantics in the literature such as latent semantic analysis (Plank and Moschitti, 2013) and relation-specific semantics (Zelenko et al., 2003; Culotta and Sorensen, 2004). To the best of our knowledge, there is only one work that uses frame semantics for relation extraction (Harabagiu et al., 2005). Harabagiu et al. (2005) propose a novel semantic kernel that incorporates frame parse information in the kernel computation that calculates similarity between two dependency trees. They, however, do not propose data representations that are based on frame parses and the resulting arborescent structures, instead adding features to syntactic trees. We believe the implicit feature space of kernels based on our data representation encode a richer and larger feature space than the one proposed by Harabagiu et al. (2005). 8 Conclusion and Future Work This work has only scratched the surface of pos</context>
</contexts>
<marker>Harabagiu, Bejan, Morarescu, 2005</marker>
<rawString>Sanda Harabagiu, Cosmin Adrian Bejan, and Paul Morarescu. 2005. Shallow semantics for relation extraction. In International Joint Conference On Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Haussler</author>
</authors>
<title>Convolution kernels on discrete structures.</title>
<date>1999</date>
<tech>Technical report,</tech>
<institution>University of California at Santa Cruz.</institution>
<contexts>
<context position="20423" citStr="Haussler, 1999" startWordPosition="3441" endWordPosition="3442"> in FrameTree. We call this 215 variation of FrameTree, in which we propagate T1 − *, T2 − * nodes to the root, FrameTreeProp. For the running example, FrameTreeProp will be: (Statement (Speaker T1-Ind) (Message (Commerce_buy ...) (T2-Grp))). Using this tree representation, one of the sub-trees in the implicit feature space will be (Statement (Speaker T1-Ind) (Message (T2-Grp)), which encodes the relation between the two targets in a more direct manner as compared to FrameTree. 5 Machine Learning We represent our data in form of feature vectors and tree structures. We use convolution kernels (Haussler, 1999) that make use of the dual form of Support Vector Machines (SVMs). In the dual form, the optimization problem that SVM solves is the following (Burges, 1998): max Eiµi − Ei,jµiµjyiyjK(xi, xj) s.t. Eiµiyi = 0 µi &gt; 0 Vi = 1, 2,...,l Here, xi is the input example, yi is the class of the example xi, µi is the Lagrange multiplier associated with example xi, l is the number of training examples, and K is the kernel function that returns a similarity between two examples. More formally, K is the function, K : X x X —* R, that maps a pair of objects belonging to the set X to a real number. For example</context>
</contexts>
<marker>Haussler, 1999</marker>
<rawString>David Haussler. 1999. Convolution kernels on discrete structures. Technical report, University of California at Santa Cruz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua He</author>
<author>Denilson Barbosa</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Identification of speakers in novels.</title>
<date>2013</date>
<booktitle>The 51st Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="1299" citStr="He et al., 2013" startWordPosition="200" endWordPosition="203">st reported system on social event detection and classification by a statistically significant margin. We show results for combining the models for the two aforementioned subtasks into the overall task of social network extraction. We show that a combination of features from all three levels of abstractions (lexical, syntactic and semantic) are required to achieve the best performing system. 1 Introduction Social network extraction from text has recently been gaining a considerable amount of attention (Agarwal and Rambow, 2010; Elson et al., 2010; Agarwal et al., 2013a; Agarwal et al., 2013b; He et al., 2013). One of the reason for this attention, we believe, is that being able to extract social networks from unstructured text may provide a powerful new tool for historians, political scientists, scholars of literature, and journalists to analyze large collections of texts around entities and their interactions. The tool would allow researchers to quickly extract networks and assess their size, nature, and cohesiveness, a task that would otherwise be impossible with corpora numbering millions of documents. It would also make it possible to make falsifiable claims about these networks, bringing the </context>
<context position="29273" citStr="He et al., 2013" startWordPosition="4928" endWordPosition="4931">ntity tags to the top of the semantic tree. Model SED SEC SNE Hier. (F1) (%A) (F1) AR2010 0.574 81.1 0.465 + RULES 0.576 80.8 0.465 + BOF 0.569 80.7 0.459 + FrameForest 0.571 82.6 0.472 + FrameTree 0.579 81.5 0.473 + FrameTreeProp 0.585 82.0 0.480 Table 4: A study to show which semantic features and structures add the most value to the baseline. The top row gives the performance of the baseline. Each consecutive row shows the result of the baseline plus the feature/structure mentioned in that row. 7 Related Work There have been recent efforts to extract networks from text (Elson et al., 2010; He et al., 2013). However, these efforts extract a different type of network: a network of only bi-directional links, where the links are triggered by quotation marks. For example, Elson et al. (2010) and He et al. (2013) will extract an interaction link between Emma and Harriet in the following sentence. However, their system will not detect any interaction links in the other examples mentioned in this paper. (6) “Take it,” said Emma, smiling, and pushing the paper towards Harriet “it is for you. Take your own.” Our approach to extract and classify social events builds on our previous work (Agarwal and Rambo</context>
</contexts>
<marker>He, Barbosa, Kondrak, 2013</marker>
<rawString>Hua He, Denilson Barbosa, and Grzegorz Kondrak. 2013. Identification of speakers in novels. The 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nanda Kambhatla</author>
</authors>
<title>Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL</booktitle>
<contexts>
<context position="7814" citStr="Kambhatla, 2004" startWordPosition="1270" endWordPosition="1271">o further predict the type of social event (INR or OBS, social event classification, SEC). In this paper, we evaluate our system on the above tasks as well as a combined task: social network extraction (SNE): given a sentence and a pair of entity mentions, predict the class of the example from one of the following three categories: {No-Event, INR, OBS}. For the purposes of this paper, we use gold named entity mentions to avoid errors caused due to named entity recognition systems. This is a common practice used in the literature for reporting relation extraction systems (Zelenko et al., 2003; Kambhatla, 2004; Zhao and Grishman, 2005; GuoDong et al., 2005; Harabagiu et al., 2005; Nguyen et al., 2009). We use standard terminology from the literature to refer to the pair of entities mentions as target entities T1 and T2. 3 Frame Semantics and FrameNet FrameNet (Baker et al., 1998) is a resource which associates words of English with their meaning. Word meanings are based on the notion of “semantic frame”. A frame is a conceptual description of a type of event, relation, or entity, and it 212 includes a list of possible participants in terms of the roles they play; these participants are called “fram</context>
</contexts>
<marker>Kambhatla, 2004</marker>
<rawString>Nanda Kambhatla. 2004. Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations. In Proceedings of the ACL 2004 on Interactive poster and demonstration sessions, page 22. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>Proceedings of the 41st Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="10402" citStr="Klein and Manning, 2003" startWordPosition="1698" endWordPosition="1702">orized into one or more of the following levels of abstraction: {Lexical, Syntactic, Semantic}. Table 2 presents this distribution. Our final results show that all of our top performing models use a data representation that is a combination of features and structures from all levels of abstraction. We review previously proposed features and tree structures in subsections 4.1, 4.2, and 4.3. To the best of our knowledge, the remaining features and structures presented in this section are novel. 4.1 Bag of words (BOW) We create a vocabulary from our training data by using the Stanford tokenizer (Klein and Manning, 2003) followed by removal of stop words 2An input example is a sentence with a pair of entity mentions between whom we predict and classify social events. and Porter Stemming. We convert each example (~x) to a set of three boolean vectors: {~b1, ~b2, ~b3}. ~b1 is the occurrence of words before the first target, ~b2 between the two targets and ~b3 after the second target. Here the first target and second target are defined in terms of the surface order of words. Though these features have been previously proposed for relation extraction on ACE (GuoDong et al., 2005), they have not been utilized for </context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. Proceedings of the 41st Meeting of the Association for Computational Linguistics, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Efficient convolution kernels for dependency and constituent syntactic trees.</title>
<date>2006</date>
<booktitle>In Proceedings of the 17th European Conference on Machine Learning.</booktitle>
<contexts>
<context position="21331" citStr="Moschitti (2006)" startWordPosition="3616" endWordPosition="3617"> xi, µi is the Lagrange multiplier associated with example xi, l is the number of training examples, and K is the kernel function that returns a similarity between two examples. More formally, K is the function, K : X x X —* R, that maps a pair of objects belonging to the set X to a real number. For example, if we represent our input examples as feature vectors, the set X would be the set of feature vectors. For feature vectors, we use a linear kernel, i.e. K(xi, xj) = xi · xj (dot product of the two vectors). For our tree representations, we use a Partial Tree Kernel (PTK), first proposed by Moschitti (2006). PTK is a relaxed version of the Subset Tree (SST) kernel proposed by Collins and Duffy (2002). A subset tree kernel measures the similarity between two trees by counting all subtrees common to the two trees. However, there is one constraint: all daughter nodes of a parent node must be included (in the sub-trees). In PTK, this constraint is removed. Therefore, in contrast to SST, PT kernels compare many more substructures. For a combination of feature vectors and tree representations, we simply use the linear combination of their respective kernels. 6 Experiments and Results We present 5-fold</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Efficient convolution kernels for dependency and constituent syntactic trees. In Proceedings of the 17th European Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Truc-Vien T Nguyen</author>
<author>Alessandro Moschitti</author>
<author>Giuseppe Riccardi</author>
</authors>
<title>Convolution kernels on constituent, dependency and sequential structures for relation extraction.</title>
<date>2009</date>
<booktitle>Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="7907" citStr="Nguyen et al., 2009" startWordPosition="1284" endWordPosition="1287">). In this paper, we evaluate our system on the above tasks as well as a combined task: social network extraction (SNE): given a sentence and a pair of entity mentions, predict the class of the example from one of the following three categories: {No-Event, INR, OBS}. For the purposes of this paper, we use gold named entity mentions to avoid errors caused due to named entity recognition systems. This is a common practice used in the literature for reporting relation extraction systems (Zelenko et al., 2003; Kambhatla, 2004; Zhao and Grishman, 2005; GuoDong et al., 2005; Harabagiu et al., 2005; Nguyen et al., 2009). We use standard terminology from the literature to refer to the pair of entities mentions as target entities T1 and T2. 3 Frame Semantics and FrameNet FrameNet (Baker et al., 1998) is a resource which associates words of English with their meaning. Word meanings are based on the notion of “semantic frame”. A frame is a conceptual description of a type of event, relation, or entity, and it 212 includes a list of possible participants in terms of the roles they play; these participants are called “frame elements”. Through the following example, we present the terminology and acronyms that will</context>
<context position="29972" citStr="Nguyen et al., 2009" startWordPosition="5045" endWordPosition="5048"> bi-directional links, where the links are triggered by quotation marks. For example, Elson et al. (2010) and He et al. (2013) will extract an interaction link between Emma and Harriet in the following sentence. However, their system will not detect any interaction links in the other examples mentioned in this paper. (6) “Take it,” said Emma, smiling, and pushing the paper towards Harriet “it is for you. Take your own.” Our approach to extract and classify social events builds on our previous work (Agarwal and Rambow, 2010), which in turn builds on work from the relation extraction community (Nguyen et al., 2009). Therefore, the task of relation extraction is most closely related to the tasks addressed in this paper. Researchers have used other notions of semantics in the literature such as latent semantic analysis (Plank and Moschitti, 2013) and relation-specific semantics (Zelenko et al., 2003; Culotta and Sorensen, 2004). To the best of our knowledge, there is only one work that uses frame semantics for relation extraction (Harabagiu et al., 2005). Harabagiu et al. (2005) propose a novel semantic kernel that incorporates frame parse information in the kernel computation that calculates similarity b</context>
</contexts>
<marker>Nguyen, Moschitti, Riccardi, 2009</marker>
<rawString>Truc-Vien T. Nguyen, Alessandro Moschitti, and Giuseppe Riccardi. 2009. Convolution kernels on constituent, dependency and sequential structures for relation extraction. Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Plank</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Embedding semantic similarity in tree kernels for domain adaptation of relation extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1498--1507</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="30206" citStr="Plank and Moschitti, 2013" startWordPosition="5083" endWordPosition="5086">ystem will not detect any interaction links in the other examples mentioned in this paper. (6) “Take it,” said Emma, smiling, and pushing the paper towards Harriet “it is for you. Take your own.” Our approach to extract and classify social events builds on our previous work (Agarwal and Rambow, 2010), which in turn builds on work from the relation extraction community (Nguyen et al., 2009). Therefore, the task of relation extraction is most closely related to the tasks addressed in this paper. Researchers have used other notions of semantics in the literature such as latent semantic analysis (Plank and Moschitti, 2013) and relation-specific semantics (Zelenko et al., 2003; Culotta and Sorensen, 2004). To the best of our knowledge, there is only one work that uses frame semantics for relation extraction (Harabagiu et al., 2005). Harabagiu et al. (2005) propose a novel semantic kernel that incorporates frame parse information in the kernel computation that calculates similarity between two dependency trees. They, however, do not propose data representations that are based on frame parses and the resulting arborescent structures, instead adding features to syntactic trees. We believe the implicit feature space</context>
</contexts>
<marker>Plank, Moschitti, 2013</marker>
<rawString>Barbara Plank and Alessandro Moschitti. 2013. Embedding semantic similarity in tree kernels for domain adaptation of relation extraction. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1498–1507, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>SVN Vishwanathan</author>
<author>Nicol N Schraudolph</author>
<author>Risi Kondor</author>
<author>Karsten M Borgwardt</author>
</authors>
<title>Graph kernels.</title>
<date>2010</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>11--1201</pages>
<marker>Vishwanathan, Schraudolph, Kondor, Borgwardt, 2010</marker>
<rawString>SVN Vishwanathan, Nicol N Schraudolph, Risi Kondor, and Karsten M Borgwardt. 2010. Graph kernels. The Journal of Machine Learning Research, 11:1201–1242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Zelenko</author>
<author>Chinatsu Aone</author>
<author>Anthony Richardella</author>
</authors>
<title>Kernel methods for relation extraction.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--1083</pages>
<contexts>
<context position="7797" citStr="Zelenko et al., 2003" startWordPosition="1266" endWordPosition="1269">D), and if they are, to further predict the type of social event (INR or OBS, social event classification, SEC). In this paper, we evaluate our system on the above tasks as well as a combined task: social network extraction (SNE): given a sentence and a pair of entity mentions, predict the class of the example from one of the following three categories: {No-Event, INR, OBS}. For the purposes of this paper, we use gold named entity mentions to avoid errors caused due to named entity recognition systems. This is a common practice used in the literature for reporting relation extraction systems (Zelenko et al., 2003; Kambhatla, 2004; Zhao and Grishman, 2005; GuoDong et al., 2005; Harabagiu et al., 2005; Nguyen et al., 2009). We use standard terminology from the literature to refer to the pair of entities mentions as target entities T1 and T2. 3 Frame Semantics and FrameNet FrameNet (Baker et al., 1998) is a resource which associates words of English with their meaning. Word meanings are based on the notion of “semantic frame”. A frame is a conceptual description of a type of event, relation, or entity, and it 212 includes a list of possible participants in terms of the roles they play; these participants</context>
<context position="30260" citStr="Zelenko et al., 2003" startWordPosition="5090" endWordPosition="5093">mples mentioned in this paper. (6) “Take it,” said Emma, smiling, and pushing the paper towards Harriet “it is for you. Take your own.” Our approach to extract and classify social events builds on our previous work (Agarwal and Rambow, 2010), which in turn builds on work from the relation extraction community (Nguyen et al., 2009). Therefore, the task of relation extraction is most closely related to the tasks addressed in this paper. Researchers have used other notions of semantics in the literature such as latent semantic analysis (Plank and Moschitti, 2013) and relation-specific semantics (Zelenko et al., 2003; Culotta and Sorensen, 2004). To the best of our knowledge, there is only one work that uses frame semantics for relation extraction (Harabagiu et al., 2005). Harabagiu et al. (2005) propose a novel semantic kernel that incorporates frame parse information in the kernel computation that calculates similarity between two dependency trees. They, however, do not propose data representations that are based on frame parses and the resulting arborescent structures, instead adding features to syntactic trees. We believe the implicit feature space of kernels based on our data representation encode a </context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2003</marker>
<rawString>Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2003. Kernel methods for relation extraction. The Journal of Machine Learning Research, 3:1083–1106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shubin Zhao</author>
<author>Ralph Grishman</author>
</authors>
<title>Extracting relations with integrated information using kernel methods.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Meeting of the ACL.</booktitle>
<contexts>
<context position="7839" citStr="Zhao and Grishman, 2005" startWordPosition="1272" endWordPosition="1275"> the type of social event (INR or OBS, social event classification, SEC). In this paper, we evaluate our system on the above tasks as well as a combined task: social network extraction (SNE): given a sentence and a pair of entity mentions, predict the class of the example from one of the following three categories: {No-Event, INR, OBS}. For the purposes of this paper, we use gold named entity mentions to avoid errors caused due to named entity recognition systems. This is a common practice used in the literature for reporting relation extraction systems (Zelenko et al., 2003; Kambhatla, 2004; Zhao and Grishman, 2005; GuoDong et al., 2005; Harabagiu et al., 2005; Nguyen et al., 2009). We use standard terminology from the literature to refer to the pair of entities mentions as target entities T1 and T2. 3 Frame Semantics and FrameNet FrameNet (Baker et al., 1998) is a resource which associates words of English with their meaning. Word meanings are based on the notion of “semantic frame”. A frame is a conceptual description of a type of event, relation, or entity, and it 212 includes a list of possible participants in terms of the roles they play; these participants are called “frame elements”. Through the </context>
</contexts>
<marker>Zhao, Grishman, 2005</marker>
<rawString>Shubin Zhao and Ralph Grishman. 2005. Extracting relations with integrated information using kernel methods. In Proceedings of the 43rd Meeting of the ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>