<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000942">
<note confidence="0.74937025">
Learning to Exploit Structured Resources for Lexical Inference
Vered Shwartz† Omer Levy† Ido Dagan† Jacob Goldberger§
† Computer Science Department, Bar-Ilan University
§ Faculty of Engineering, Bar-Ilan University
</note>
<email confidence="0.971263">
vered1986@gmail.com {omerlevy,dagan}@cs.biu.ac.il
jacob.goldberger@biu.ac.il
</email>
<sectionHeader confidence="0.993799" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999390888888889">
Massive knowledge resources, such as
Wikidata, can provide valuable informa-
tion for lexical inference, especially for
proper-names. Prior resource-based ap-
proaches typically select the subset of each
resource’s relations which are relevant for
a particular given task. The selection
process is done manually, limiting these
approaches to smaller resources such as
WordNet, which lacks coverage of proper-
names and recent terminology. This paper
presents a supervised framework for auto-
matically selecting an optimized subset of
resource relations for a given target infer-
ence task. Our approach enables the use
of large-scale knowledge resources, thus
providing a rich source of high-precision
inferences over proper-names.1
</bodyText>
<sectionHeader confidence="0.999136" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999694153846154">
Recognizing lexical inference is an important
component in semantic tasks. Various lexical-
semantic relations, such as synonomy, class-
membership, part-of, and causality may be used
to infer the meaning of one word from another,
in order to address lexical variability. For in-
stance, a question answering system asked “which
artist’s net worth is $450 million?” might re-
trieve the candidates Beyonc´e Knowles and
Lloyd Blankfein, who are both worth $450 mil-
lion. To correctly answer the question, the appli-
cation needs to know that Beyonc´e is an artist, and
that Lloyd Blankfein is not.
</bodyText>
<footnote confidence="0.795845">
1Our code and data are available at:
https://github.com/vered1986/LinKeR
</footnote>
<bodyText confidence="0.999657789473684">
Corpus-based methods are often employed to
recognize lexical inferences, based on either co-
occurrence patterns (Hearst, 1992; Turney, 2006)
or distributional representations (Weeds and Weir,
2003; Kotlerman et al., 2010). While earlier meth-
ods were mostly unsupervised, recent trends intro-
duced supervised methods for the task (Baroni et
al., 2012; Turney and Mohammad, 2015; Roller
et al., 2014). In these settings, a targeted lexical
inference relation is implicitly defined by a train-
ing set of term-pairs, which are annotated as posi-
tive or negative examples of this relation. Several
such datasets have been created, each representing
a somewhat different flavor of lexical inference.
While corpus-based methods usually enjoy high
recall, their precision is often limited, hinder-
ing their applicability. An alternative common
practice is to mine high-precision lexical in-
ferences from structured resources, particularly
WordNet (Fellbaum, 1998). Nevertheless, Word-
Net is an ontology of the English language,
which, by definition, does not cover many proper-
names (Beyonc´e → artist) and recent termi-
nology (Facebook → social network). A po-
tential solution may lie in rich and up-to-date
structured knowledge resources such as Wikidata
(Vrandeˇci´c, 2012), DBPedia (Auer et al., 2007),
and Yago (Suchanek et al., 2007). In this paper, we
investigate how these resources can be exploited
for lexical inference over proper-names.
We begin by examining whether the common
usage of WordNet for lexical inference can be ex-
tended to larger resources. Typically, a subset of
WordNet relations is manually selected (e.g. all
synonyms and hypernyms). By nature, each ap-
plication captures a different aspect of lexical in-
ference, and thus defines different relations as in-
dicative of its particular flavor of lexical infer-
</bodyText>
<page confidence="0.982176">
175
</page>
<note confidence="0.9903885">
Proceedings of the 19th Conference on Computational Language Learning, pages 175–184,
Beijing, China, July 30-31, 2015. c�2015 Association for Computational Linguistics
</note>
<tableCaption confidence="0.9925865">
Table 1: Examples of Wikidata relations that are indicative of
lexical inference.
</tableCaption>
<bodyText confidence="0.999971666666667">
ence. For instance, the hypernym relation is
indicative of the is a flavor of lexical inference
(e.g. musician —* artist), but does not indicate
causality.
Since WordNet has a relatively simple schema,
manually finding such an optimal subset is fea-
sible. However, structured knowledge resources’
schemas contain thousands of relations, dozens of
which may be indicative. Many of these are not
trivial to identify by hand, as shown in Table 1.
A manual effort to construct a distinct subset for
each task is thus quite challenging, and an auto-
mated method is required.
We present a principled supervised framework,
which automates the selection process of resource
relations, and optimizes this subset for a given
target inference relation. This automation al-
lows us to leverage large-scale resources, and ex-
tract many high-precision inferences over proper-
names, which are absent from WordNet. Finally,
we show that our framework complements state-
of-the-art corpus-based methods. Combining the
two approaches can particularly benefit real-world
tasks in which proper-names are prominent.
</bodyText>
<sectionHeader confidence="0.994921" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.995903">
2.1 Common Use of WordNet for Inference
</subsectionHeader>
<bodyText confidence="0.997819333333334">
WordNet (Fellbaum, 1998) is widely used for
identifying lexical inference. It is usually used in
an unsupervised setting where the relations rele-
vant for each specific inference task are manually
selected a priori.
One approach looks for chains of these prede-
fined relations (Harabagiu and Moldovan, 1998),
e.g. dog —* mammal using a chain of hy-
pernyms: dog —* canine —* carnivore —*
placental mammal —* mammal. Another ap-
proach is via WordNet Similarity (Pedersen et al.,
2004), which takes two synsets and returns a nu-
meric value that represents their similarity based
on WordNet’s hierarchical hypernymy structure.
While there is a broad consensus that synonyms
entail each other (elevator H lift) and hy-
ponyms entail their hypernyms (cat —* animal),
other relations, such as meronymy, are not agreed
</bodyText>
<table confidence="0.999168">
Resource #Entities #Properties Version
DBPedia 4,500,000 1,367 July 2014
Wikidata 6,000,000 1,200 July 2014
Yago 10,000,000 70 December 2014
WordNet 150,000 13 3.0
</table>
<tableCaption confidence="0.998882">
Table 2: Structured resources explored in this work.
</tableCaption>
<bodyText confidence="0.9961935">
upon, and may vary depending on task and context
(e.g. living in London —* living in England,
but leaving London —*4 leaving England).
Overall, there is no principled way to select the
subset of relevant relations, and a suitable subset
is usually tailored to each dataset and task. This
work addresses this issue by automatically learn-
ing the subset of relations relevant to the task.
</bodyText>
<subsectionHeader confidence="0.998809">
2.2 Structured Knowledge Resources
</subsectionHeader>
<bodyText confidence="0.9998796875">
While WordNet is quite extensive, it is hand-
crafted by expert lexicographers, and thus cannot
compete in terms of scale with community-built
knowledge bases such as Wikidata (Vrandeˇci´c,
2012), which connect millions of entities through
a rich variety of structured relations (properties).
Using these resources for various NLP tasks has
become exceedingly popular (Wu and Weld, 2010;
Rahman and Ng, 2011; Unger et al., 2012; Be-
rant et al., 2013). Little attention, however, was
given to leveraging them for identifying lexical in-
ference; the exception being Shnarch et al. (2009),
who used structured data from Wikipedia for this
purpose.
In this paper, we experimented with such re-
sources, in addition to WordNet. DBPedia (Auer
et al., 2007) contains structured information from
Wikipedia: info boxes, redirections, disambigua-
tion links, etc. Wikidata (Vrandeˇci´c, 2012) con-
tains facts edited by humans to support Wikipedia
and other Wikimedia projects. Yago (Suchanek et
al., 2007) is a semantic knowledge base derived
from Wikipedia, WordNet, and GeoNames.2
Table 2 compares the scale of the resources we
used. The massive scale of the more recent re-
sources and their rich schemas can potentially in-
crease the coverage of current WordNet-based ap-
proaches, yet make it difficult to manually select
an optimized subset of relations for a task. Our
method automatically learns such a subset, and
provides lexical inferences on entities that are ab-
sent from WordNet, particularly proper-names.
</bodyText>
<footnote confidence="0.973619">
2We also considered Freebase, but it required significantly
larger computational resources to work in our framework,
which, at the time of writing, exceeded our capacity. §4.1
discusses complexity.
</footnote>
<figure confidence="0.979576315789474">
Resource Relation
Example
position held
performer
David Petraeus → Director of CIA
Sheldon Cooper → Jim Parsons
operating system
iPhone → iOS
176
term to concept
“Beyonc´e”
subclass of
concept to term
“artist”
Beyonc´e
Knowles
musician
artist
occupation
</figure>
<figureCaption confidence="0.9843995">
Figure 1: An excerpt of a resource graph (Wikidata) connecting “Beyonc´e” to “artist”. Resource graphs contain two types of
nodes: terms (ellipses) and concepts (rectangles).
</figureCaption>
<sectionHeader confidence="0.858547" genericHeader="method">
3 Task Definition and Representation
</sectionHeader>
<bodyText confidence="0.99996921875">
We wish to leverage the information in structured
resources to identify whether a certain lexical-
inference relation R holds between a pair of terms.
Formally, we wish to classify whether a term-pair
(x, y) satisfies the relation R. R is implicitly de-
fined by a training set of (x, y) pairs, annotated as
positive or negative examples. We are also given
a set of structured resources, which we will utilize
to classify (x, y).
Each resource can be naturally viewed as a di-
rected graph G (Figure 1). There are two types
of nodes in G: term (lemma) nodes and con-
cept (synset) nodes. The edges in G are each la-
beled with a property (edge type), defining a wide
range of semantic relations between concepts (e.g.
occupation, subclass of). In addition, terms are
mapped to the concepts they represent via term-
concept edge types.
When using multiple resources, G is a dis-
connected graph composed of a subgraph per re-
source, without edges connecting nodes from dif-
ferent resources. One may consider connect-
ing multiple resource graphs at the term nodes.
However, this may cause sense-shifts, i.e. con-
nect two distinct concepts (in different resources)
through the same term. For example, the concept
January 1st in Wikidata is connected to the con-
cept fruit in WordNet through the polysemous
term date. The alternative, aligning resources in
the concept space, is not trivial. Some partial map-
pings exist (e.g. Yago-WordNet), which can be ex-
plored in future work.
</bodyText>
<sectionHeader confidence="0.996595" genericHeader="method">
4 Algorithmic Framework
</sectionHeader>
<bodyText confidence="0.999992">
We present an algorithmic framework for learning
whether a term-pair (x, y) satisfies a relation R,
given an annotated set of term-pairs and a resource
graph G. We first represent (x, y) as the set of
paths connecting x and y in G (§4.1). We then
classify each such path as indicative or not of R,
and decide accordingly whether xRy (§4.2).
</bodyText>
<subsectionHeader confidence="0.997984">
4.1 Representing Term-Pairs as Path-Sets
</subsectionHeader>
<bodyText confidence="0.999972375">
We represent each (x, y) pair as the set of paths
that link x and y within each resource. We retain
only the shortest paths (all paths x ❀ y of minimal
length) as they yielded better performance.
Resource graphs are densely connected, and
thus have a huge branching factor b. We thus lim-
ited the maximum path length to E = 8 and em-
ployed bidirectional search (Russell and Norvig,
2009, Ch.3) to find the shortest paths. This algo-
rithm runs two simultaneous instances of breadth-
first search (BFS), one from x and another from y,
halting when they meet in the middle. It is much
more efficient, having a complexity of O(bP/2) =
O(b4) instead of BFS’s O(bP) = O(b8).
To further reduce complexity, we split the
search to two phases: we first find all nodes along
the shortest paths between x and y, and then re-
construct the actual paths. Searching for rele-
vant nodes ignores edge types, inducing a sim-
pler resource graph, which can be represented as
a sparse adjacency matrix and manipulated effi-
ciently with matrix operations (elaborated in ap-
pendix A). Once the search space is limited to rel-
evant nodes alone, path-finding becomes trivial.
</bodyText>
<subsectionHeader confidence="0.980899">
4.2 Classification Framework
</subsectionHeader>
<bodyText confidence="0.999242260869565">
We consider edge types that typically connect be-
tween concepts in R to be “indicative”; for exam-
ple, the occupation edge type is indicative of the
is a relation, as in “Beyonc´e is a musician”. Our
framework’s goal is to learn which edge types are
indicative of a given relation R, and use that infor-
mation to classify new (x, y) term-pairs.
Figure 2 presents the dependencies between
edge types, paths, and term-pairs. As discussed in
the previous section, we represent each term-pair
as a set of paths. In turn, we represent each path as
a “bag of edges”, a vector with an entry for each
edge type.3 To model the edges’ “indicativeness”,
we assign a parameter to each edge type, and learn
these parameters from the term-pair level supervi-
sion provided by the training data.
In this work, we are not only interested in opti-
mizing accuracy or F1, but in exploring the entire
recall-precision trade-off. Therefore, we optimize
3We add special markers to the first and last edges within
each path. This allows the algorithm to learn that applying
term-to-concept and concept-to-term edge types in the middle
of a path causes sense-shifts.
</bodyText>
<page confidence="0.993816">
177
</page>
<figureCaption confidence="0.9990965">
Figure 2: The dependencies between term-pairs (x → y),
paths (pj), and edge types (ei).
</figureCaption>
<bodyText confidence="0.999501">
the Fβ objective, where β2 balances the recall-
precision trade-off.4 In particular, we expect struc-
tured resources to facilitate high-precision infer-
ences, and are thus more interested in lower values
of β2, which emphasize precision over recall.
</bodyText>
<sectionHeader confidence="0.383877" genericHeader="method">
4.2.1 Weighted Edge Model
</sectionHeader>
<bodyText confidence="0.99998324">
A typical neural network approach is to assign a
weight wi to each edge type ei, where more in-
dicative edge types should have higher values of
wi. The indicativeness of a path (ˆp) is modeled
using logistic regression: pˆ°= σ(w · 0—), where 0�
is the path’s “bag of edges” representation, i.e. a
feature vector of each edge type’s frequency in the
path.
The probability of a term-pair being positive
can be determined using either the sum of all
path scores or the score of its most indicative
path (max-pooling). We trained both variants with
back-propagation (Rumelhart et al., 1986) and
gradient ascent. In particular, we optimized Fβ
using a variant of Jansche’s (2005) derivation of
Fβ-optimized logistic regression (see suplemen-
tary material5 for full derivation).
This model can theoretically quantify how in-
dicative each edge type is of R. Specifically,
it can differentiate weakly indicative edges (e.g.
meronyms) from those that contradict R (e.g.
antonyms). However, on our datasets, this model
yielded sub-optimal results (see §6.1), and there-
fore serves as a baseline to the binary model pre-
sented in the following section.
</bodyText>
<subsectionHeader confidence="0.705455">
4.2.2 Binary Edge Model
</subsectionHeader>
<bodyText confidence="0.999729">
Preliminary experiments suggested that in most
datasets, each edge type is either indicative or
non-indicative of the target relation R. We there-
fore developed a binary model, which defines a
</bodyText>
<footnote confidence="0.596464">
5http://u.cs.biu.ac.il/%7enlp/wp-content/uploads/LinKeR-sup.pdf
</footnote>
<bodyText confidence="0.999498625">
global set of edge types that are indicative of R:
a whitelist.
Classification We represent each path p as a
binary “bag of edges” 0, i.e. the set of edge
types that were applied in p. Given a term-pair
(x, y) represented as a path-set paths(x, y), and a
whitelist w, the model classifies (x, y) as positive
if:
</bodyText>
<equation confidence="0.945839">
∃0 E paths(x, y) : 0 C w (1)
</equation>
<bodyText confidence="0.90096">
In other words:
</bodyText>
<listItem confidence="0.98673925">
1. A path is classified as indicative if all its edge
types are whitelisted.
2. A term-pair is classified as positive if at least
one of its paths is indicative.6
</listItem>
<bodyText confidence="0.999880625">
The first design choice essentially assumes that R
is a transitive relation. This is usually the case in
most inference relations (e.g. hypernymy, causal-
ity). In addition, notice that the second modeling
assumption is unidirectional; in some cases xRy,
yet an indicative path between them does not ex-
ist. This can happen, for example, if the relation
between them is not covered by the resource, e.g.
causality in WordNet.
Training Learning the optimal whitelist over a
training set can be cast as a subset selection prob-
lem: given a set of possible edge types E =
{e1,..., en} and a utility function u : 2E → R,
find the subset (whitelist) w C E that maximizes
the utility, i.e. w∗ = arg maxw u(w). In our case,
the utility u is the Fβ score over the training set.
Structured knowledge resources contain hun-
dreds of different edge types, making E very large,
and an exhaustive search over its powerset infea-
sible. The standard approach to this class of sub-
set selection problems is to apply local search al-
gorithms, which find an approximation of the op-
timal subset. We tried several local search algo-
rithms, and found that genetic search (Russell and
Norvig, 2009, Ch.4) performed well. In general,
genetic search is claimed to be a preferred strategy
for subset selection (Yang and Honavar, 1998).
In our application of genetic search, each in-
dividual (candidate solution) is a whitelist, repre-
sented by a bit vector with a bit for each edge type.
We defined the fitness function of a whitelist w ac-
cording to the Fβ score of w over the training set.
</bodyText>
<footnote confidence="0.858771">
6As a corollary, if x✚Ry, then every path between them is
non-indicative.
</footnote>
<figure confidence="0.853040363636364">
supervision
p1
e1
x → y
p2
parameters
e2
p3
e3
4F = (1+02) ·precision ·recall
0 02·precision+recall
</figure>
<page confidence="0.951702">
178
</page>
<table confidence="0.9985532">
Dataset #Instances #Positive #Negative
kotlerman2010 2,940 880 2,060
turney2014 1,692 920 772
levy2014 12,602 945 11,657
proper2015 1,500 750 750
</table>
<tableCaption confidence="0.999877">
Table 3: Datasets evaluated in this work.
</tableCaption>
<bodyText confidence="0.998888615384615">
We also applied L2 regularization to reduce the fit-
ness of large whitelists.
The binary edge model works well in practice,
successfully replicating the common practice of
manually selected relations from WordNet (see
§6.1). In addition, the model outputs a human-
interpretable set of indicative edges.
Although the weighted model’s hypothesis
space subsumes the binary model’s, the binary
model performed better on our datasets. We con-
jecture that this stems from the limited amount of
training instances, which prevents a more general
model from converging into an optimal solution.
</bodyText>
<sectionHeader confidence="0.999342" genericHeader="method">
5 Datasets
</sectionHeader>
<bodyText confidence="0.999941333333333">
We used 3 existing common-noun datasets and
one new proper-name dataset. Each dataset con-
sists of annotated (x, y) term-pairs, where both x
and y are noun phrases. Since each dataset was
created in a slightly different manner, the underly-
ing semantic relation R varies as well.
</bodyText>
<subsectionHeader confidence="0.98747">
5.1 Existing Datasets
</subsectionHeader>
<bodyText confidence="0.999915533333333">
kotlerman2010 (Kotlerman et al., 2010) is
a manually annotated lexical entailment dataset
of distributionally similar nouns. turney2014
(Turney and Mohammad, 2015) is based on a
crowdsourced dataset of semantic relations, from
which we removed non-nouns and lemmatized
plurals. levy2014 (Levy et al., 2014) was gen-
erated from manually annotated entailment graphs
of subject-verb-object tuples. Table 3 provides
metadata on each dataset.
Two additional datasets were created using
WordNet (Baroni and Lenci, 2011; Baroni et al.,
2012), whose definition of R can be trivially cap-
tured by a resource-based approach using Word-
Net. Hence, they are omitted from our evaluation.
</bodyText>
<subsectionHeader confidence="0.992599">
5.2 A New Proper-Name Dataset
</subsectionHeader>
<bodyText confidence="0.999240103448276">
An important linguistic component that is miss-
ing from these lexical-inference datasets is proper-
names. We conjecture that much of the added
value in utilizing structured resources is the abil-
ity to cover terms such as celebrities (Lady Gaga)
and recent terminology (social networks) that do
not appear in WordNet. We thus created a new
dataset of (x, y) pairs in which x is a proper-name,
y is a common noun, and R is the is a relation.
For instance, (Lady Gaga, singer) is true, but
(Lady Gaga, film) is false.
To construct the dataset, we sampled 70 articles
in 9 different topics from a corpus of recent events
(online magazines). As candidate (x, y) pairs, we
extracted 24,000 pairs of noun phrases x and y
that belonged to the same paragraph in the orig-
inal text, selecting those in which x is a proper-
name. These pairs were manually annotated by
graduate students, who were instructed to use their
world knowledge and the original text for disam-
biguation (e.g. England → team in the context
of football). The agreement on a subset of 4,500
pairs was κ = 0.954.
After annotation, we had roughly 800 positive
and 23,000 negative pairs. To balance the dataset,
we sampled negative examples according to the
frequency of y in positive pairs, creating “harder”
negative examples, such as (Sherlock, lady) and
(Kylie Minogue, vice president).
</bodyText>
<sectionHeader confidence="0.99993" genericHeader="method">
6 Results
</sectionHeader>
<bodyText confidence="0.999982166666667">
We first validate our framework by checking
whether it can automatically replicate the com-
mon manual usage of WordNet. We then evaluate
it on the proper-name dataset using additional re-
sources. Finally, we compare our method to state-
of-the-art distributional methods.
Experimental Setup While F1 is a standard
measure of performance, it captures only one point
on the recall-precision curve. Instead, we present
the entire curve, while expecting the contribution
of structured resources to be in the high-precision
region. To create these curves, we optimized our
method and the baselines using Fβ with 40 values
of β2 E (0, 2).
We randomly split each dataset into 70% train,
25% test and 5% validation.7 We applied L2 regu-
larization to our method and the baselines, tuning
the regularization parameter on the validation set.
</bodyText>
<subsectionHeader confidence="0.997756">
6.1 Performance on WordNet
</subsectionHeader>
<bodyText confidence="0.999722">
We examine whether our algorithm can replicate
the common use of WordNet (§2.1), by manually
constructing 4 whitelists based on the literature
</bodyText>
<footnote confidence="0.990862">
7Since our methods do not use lexical features, we did not
use lexical splits as in (Levy et al., 2015).
</footnote>
<page confidence="0.99725">
179
</page>
<figureCaption confidence="0.992896">
Figure 3: Recall-precision curve of each dataset with Word-
</figureCaption>
<bodyText confidence="0.979476272727273">
Net as the only resource. Each point in the graph stands for
the performance on a certain value of β. Notice that in some
of the graphs, different β values yield the same performance,
causing less points to be displayed.
(see Table 4), and evaluating their performance us-
ing the classification methods in §4.2. In addition,
we compare our method to Resnik’s (1995) Word-
Net similarity, which scores each pair of terms
based on their lowest common hypernym. This
score was used as a single feature in Fβ-optimized
logistic regression to create a classifier.
</bodyText>
<figureCaption confidence="0.997431">
Figure 4: Recall-precision curve for proper2015.
</figureCaption>
<table confidence="0.8967842">
Name Edge Types
basic {synonym, hypernym, instance hypernym}
+holo basic U {holonym}
+mero basic U {meronym}
+hypo basic U {hyponym}
</table>
<tableCaption confidence="0.999077">
Table 4: The manual whitelists commonly used in WordNet.
</tableCaption>
<bodyText confidence="0.998838705882353">
Figure 3 compares our algorithm to Word-
Net’s baselines, showing that our binary model
always replicates the best-performing manually-
constructed whitelists, for certain values of β&apos;.
Synonyms and hypernyms are often selected,
and additional edges are added to match the
semantic flavor of each particular dataset. In
turney2014, for example, where meronyms are
common, our binary model learns that they are in-
dicative by including meronymy in its whitelist. In
levy2014, however, where meronyms are less
indicative, the model does not select them.
We also observe that, in most cases, our algo-
rithm outperforms Resnik’s similarity. In addition,
the weighted model does not perform as well as
the binary model, as discussed in §4.2. We there-
fore focus our presentation on the binary model.
</bodyText>
<subsectionHeader confidence="0.999152">
6.2 Lexical Inference over Proper-Names
</subsectionHeader>
<bodyText confidence="0.999952666666667">
We evaluated our model on the new proper-name
dataset proper2015 described in §5.2. This
time, we incorporated all the resources described
in §2.2 (including WordNet) into our framework,
and compared the performance to that of using
WordNet alone. Indeed, our algorithm is able to
exploit the information in the additional resources
and greatly increase performance, particularly re-
call, on this dataset (Figure 4).8
</bodyText>
<footnote confidence="0.958831666666667">
8We also evaluated our algorithm on the common-nouns
datasets with all resources, but apparently, adding resources
did not significantly improve performance.
</footnote>
<page confidence="0.995182">
180
</page>
<figureCaption confidence="0.999863">
Figure 5: Recall-precision curve of each dataset using: (1) Supervised word2vec (2) Our binary model.
</figureCaption>
<bodyText confidence="0.999490388888889">
The binary model yields 97% precision at 29%
recall, at the top of the “precision cliff”. The
whitelist learnt at this point contains 44 edge
types, mainly from Wikidata and Yago. Even
though the is a relation implicitly defined in
proper2015 is described using many different
edge types, our binary model still manages to learn
which of the over 2,500 edge types are indicative.
Table 5 shows some of the learnt edge types (see
the supplementary material for the complete list).
The performance boost in proper2015
demonstrates that community-built resources have
much added value when considering proper-
names. As expected, many proper-names do not
appear in WordNet (Doctor Who). That said,
even when both terms appear in WordNet, they
often lack important properties covered by other
resources (Louisa May Alcott is a woman).
</bodyText>
<subsectionHeader confidence="0.99917">
6.3 Comparison to Corpus-based Methods
</subsectionHeader>
<bodyText confidence="0.9996465">
Lexical inference has been thoroughly explored
in distributional semantics, with recent supervised
methods (Baroni et al., 2012; Turney and Mo-
hammad, 2015) showing promising results. While
</bodyText>
<tableCaption confidence="0.969938333333333">
Table 5: An excerpt of the whitelist learnt for proper2015
by the binary model with accompanying true-positives that
do not have an indicative path in WordNet.
</tableCaption>
<bodyText confidence="0.999621615384615">
these methods leverage huge corpora to increase
coverage, they often introduce noise that affects
their precision. Structured resources, on the other
hand, are precision-oriented. We therefore expect
our approach to complement distributional meth-
ods in high-precision scenarios.
To represent term-pairs with distributional fea-
tures, we downloaded the pre-trained word2vec
embeddings.9 These vectors were trained over
a huge corpus (100 billion words) using a state-
of-the-art embedding algorithm (Mikolov et al.,
2013). Since each vector represents a single term
(either x or y), we used 3 state-of-the-art meth-
</bodyText>
<footnote confidence="0.688184">
9http://code.google.com/p/word2vec/
</footnote>
<figure confidence="0.883667214285714">
Edge Type
occupation
sex or gender
instance of
acted in
genre
Example
Daniel Radcliffe → actor
Louisa May Alcott → woman
Doctor Who → series
Michael Keaton → Beetlejuice
Touch → drama
position played on team
Jason Collins → center
</figure>
<page confidence="0.995156">
181
</page>
<bodyText confidence="0.999814483870968">
ods to construct a feature vector for each term-
pair: concatenation x� ® y� (Baroni et al., 2012),
difference y� − x� (Roller et al., 2014; Fu et al.,
2014; Weeds et al., 2014), and similarity x� · y.
We then used Fβ-optimized logistic regression to
train a classifier. Figure 5 compares our methods
to concatenation, which was the best-performing
corpus-based method.10
In turney2014 and proper2015, the
embeddings retain over 80% precision while
boasting higher recall than our method’s. In
turney2014, it is often a result of the more
associative relations prominent in the dataset
(football —* playbook), which seldom are ex-
pressed in structured resources. In proper2015,
the difference in recall seems to be from miss-
ing terminology (Twitter —* social network).
However, the corpus-based method’s precision
does not exceed the low 80’s, while our bi-
nary algorithm yields 93% @ 27% precision-at-
recall on turney2014 and 97% @ 29% on
proper2015.
In levy2014, there is an overwhelming ad-
vantage to our resource-based method over the
corpus-based method. This dataset contains
healthcare terms and might require a domain-
specific corpus to train the embeddings. Having
said that, many of its examples are of an ontologi-
cal nature (drug x treats disease y), which may be
more suited to our resource-based approach, re-
gardless of domain.
</bodyText>
<sectionHeader confidence="0.997075" genericHeader="method">
7 Error Analysis
</sectionHeader>
<bodyText confidence="0.999401785714286">
Since resource-based methods are precision-
oriented, we analyzed our binary model by select-
ing the setting with the highest attainable recall
that maintains high precision. This point is often
at the top of a “precision cliff” in Figures 3 and 4.
These settings are presented in Table 6.
The high-precision settings we chose resulted
in few false positives, most of which are caused
by annotation errors or resource errors. Naturally,
regions of higher recall and lower precision will
yield more false positives and less false negatives.
We thus focus the rest of our discussion on false
negatives (Table 7).
While structured resources cover most terms,
</bodyText>
<footnote confidence="0.9211386">
10Note that the corpus-based method benefits from lexical
memorization (Levy et al., 2015), overfitting for the lexical
terms in the training set, while our resource-based method
does not. This means that Figure 5 paints a relatively opti-
mistic picture of the embeddings’ actual performance.
</footnote>
<table confidence="0.999710714285714">
Dataset β Whitelist Prec. Rec.
kotlerman2010 0.05 basic 83% 9%
turney2014 0.05 +mero 93% 27%
levy2014 10−5 basic 87% 37%
proper2015 0.3 44 edge types 97% 29%
from all resources
(see supplementary material)
</table>
<tableCaption confidence="0.988392">
Table 6: The error analysis setting of each dataset.
</tableCaption>
<table confidence="0.99967525">
Error Type kotlerman levy turney proper
2010 2014 2014 2015
Not Covered 2% 12% 4% 13%
No Indicative Paths 35% 48% 73% 75%
Whitelist Error 6% 3% 5% 8%
Resource Error 15% 11% 7% 0%
Annotation Error 40% 23% 7% 1%
Other 2% 3% 4% 3%
</table>
<tableCaption confidence="0.987921">
Table 7: Analysis of false negatives in each dataset. We ob-
served the following errors: (1) One of the terms is out-of-
vocabulary (2) All paths are not indicative (3) An indicative
path exists, but discarded by the whitelist (4) The resource
describes an inaccurate relation between the terms (5) The
term-pair was incorrectly annotated as positive.
</tableCaption>
<bodyText confidence="0.999909833333333">
the majority of false negatives stem from the
lack of indicative paths between them. Many
important relations are not explicitly covered by
the resources, such as noun-quality (saint —*
holiness), which are abundant in turney2014,
or causality (germ —* infection), which appear
in levy2014. These examples are occasionally
captured by other (more specific) relations, and
tend to be domain-specific.
In kotlerman2010, we found that many
false negatives are caused by annotation errors in
this dataset. Pairs are often annotated as positive
based on associative similarity (e.g. transport —*
environment, financing —* management),
making it difficult to even manually construct a co-
herent whitelist for this dataset. This may explain
the poor performance of our method and other
baselines on this dataset.
</bodyText>
<sectionHeader confidence="0.936864" genericHeader="conclusions">
8 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999736846153846">
In this paper, we presented a supervised frame-
work for utilizing structured resources to rec-
ognize lexical inference. We demonstrated that
our framework replicates the common practice of
WordNet and can increase the coverage of proper-
names by exploiting larger structured resources.
Compared to the prior practice of manually identi-
fying useful relations in structured resources, our
contribution offers a principled learning approach
for automating and optimizing this common need.
While our method enjoys high-precision, its re-
call is limited by the resources’ coverage. In fu-
ture work, combining our method with high-recall
</bodyText>
<page confidence="0.994021">
182
</page>
<bodyText confidence="0.999903727272727">
corpus-based methods may have synergistic re-
sults. Another direction for increasing recall is
to use cross-resource mappings to allow cross-
resource paths (connected at the concept-level).
Finally, our method can be extended to become
context-sensitive, that is, deciding whether the lex-
ical inference holds in a given context. This may
be done by applying a resource-based WSD ap-
proach similar to (Brody et al., 2006; Agirre et al.,
2014), detecting the concept node that matches the
term’s sense in the given context.
</bodyText>
<sectionHeader confidence="0.997387" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99524">
This work was supported by an Intel ICRI-
CI grant, the Google Research Award Program
and the German Research Foundation via the
German-Israeli Project Cooperation (DIP, grant
DA 1600/1-1).
</bodyText>
<sectionHeader confidence="0.998836" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999603056179775">
Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa.
2014. Random walks for knowledge-based word
sense disambiguation. Computational Linguistics,
40(1):57–84.
S¨oren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives.
2007. Dbpedia: A nucleus for a web of open data.
Springer.
Marco Baroni and Alessandro Lenci. 2011. How we
blessed distributional semantic evaluation. In Pro-
ceedings of the GEMS 2011 Workshop on GEomet-
rical Models of Natural Language Semantics, pages
1–10, Edinburgh, UK, July. Association for Compu-
tational Linguistics.
Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do,
and Chung-chieh Shan. 2012. Entailment above the
word level in distributional semantics. In Proceed-
ings of the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 23–32, Avignon, France, April. Association
for Computational Linguistics.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1533–1544, Seattle, Wash-
ington, USA, October. Association for Computa-
tional Linguistics.
Samuel Brody, Roberto Navigli, and Mirella Lapata.
2006. Ensemble methods for unsupervised wsd.
In Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 97–104. Association for Computa-
tional Linguistics.
Christiane Fellbaum. 1998. WordNet. Wiley Online
Library.
Ruiji Fu, Jiang Guo, Bing Qin, Wanxiang Che, Haifeng
Wang, and Ting Liu. 2014. Learning semantic hier-
archies via word embeddings. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1199–1209, Baltimore, Maryland, June. Association
for Computational Linguistics.
Sanda Harabagiu and Dan Moldovan. 1998. Knowl-
edge processing on an extended wordNet. WordNet:
An electronic lexical database, 305:381–405.
Marti A Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In COLING 1992
Volume 2: The 15th International Conference on
Computational Linguistics, pages 529–545, Nantes,
France.
Martin Jansche. 2005. Maximum expected f-measure
training of logistic regression models. In Proceed-
ings of Human Language Technology Conference
and Conference on Empirical Methods in Natural
Language Processing, pages 692–699, Vancouver,
British Columbia, Canada, October. Association for
Computational Linguistics.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(04):359–389.
Omer Levy, Ido Dagan, and Jacob Goldberger. 2014.
Focused entailment graphs for open ie propositions.
In Proceedings of the Eighteenth Conference on
Computational Natural Language Learning, pages
87–97, Ann Arbor, Michigan, June. Association for
Computational Linguistics.
Omer Levy, Steffen Remus, Chris Biemann, and Ido
Dagan. 2015. Do supervised distributional meth-
ods really learn lexical inference relations? In Pro-
ceedings of the 2015 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
970–976, Denver, Colorado, May–June. Association
for Computational Linguistics.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S
Corrado, and Jeffrey Dean. 2013. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In Advances in Neural Information Pro-
cessing Systems, pages 3111–3119.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity -measuring the re-
latedness of concepts. In Daniel Marcu Susan Du-
mais and Salim Roukos, editors, HLT-NAACL 2004:
Demonstration Papers, pages 38–41, Boston, Mas-
sachusetts, USA, May 2 - May 7. Association for
Computational Linguistics.
</reference>
<page confidence="0.989984">
183
</page>
<reference confidence="0.999463794520548">
Altaf Rahman and Vincent Ng. 2011. Coreference res-
olution with world knowledge. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 814–824, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In
Proceedings of the 14th international joint confer-
ence on Artificial intelligence - Volume 1, IJCAI’95,
pages 448–453. Morgan Kaufmann Publishers Inc.
Stephen Roller, Katrin Erk, and Gemma Boleda. 2014.
Inclusive yet selective: Supervised distributional hy-
pernymy detection. In Proceedings of COLING
2014, the 25th International Conference on Compu-
tational Linguistics: Technical Papers, pages 1025–
1036, Dublin, Ireland, August. Dublin City Univer-
sity and Association for Computational Linguistics.
D E Rumelhart, G E Hinton, and R J Williams. 1986.
Learning representations by back-propagating er-
rors. Nature, pages 533–536.
Stuart Russell and Peter Norvig. 2009. Artificial Intel-
ligence: A Modern Approach. Prentice Hall.
Eyal Shnarch, Libby Barak, and Ido Dagan. 2009. Ex-
tracting lexical reference rules from wikipedia. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 450–458, Suntec, Singapore,
August. Association for Computational Linguistics.
Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In Proceedings of the 16th international con-
ference on World Wide Web, pages 697–706. ACM.
Peter D Turney and Saif M Mohammad. 2015. Ex-
periments with three approaches to recognizing lex-
ical entailment. Natural Language Engineering,
21(03):437–476.
Peter D Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379–416.
Christina Unger, Lorenz B¨uhmann, Jens Lehmann,
Axel-Cyrille Ngonga Ngomo, Daniel Gerber, and
Philipp Cimiano. 2012. Template-based question
answering over rdf data. In Proceedings of the 21st
international conference on World Wide Web, pages
639–648. ACM.
Denny Vrandeˇci´c. 2012. Wikidata: A new platform
for collaborative data collection. In Proceedings
of the 21st international conference companion on
World Wide Web, pages 1063–1064. ACM.
Julie Weeds and David Weir. 2003. A general
framework for distributional similarity. In Michael
Collins and Mark Steedman, editors, Proceedings of
the 2003 Conference on Empirical Methods in Nat-
ural Language Processing, pages 81–88.
Julie Weeds, Daoud Clarke, Jeremy Reffin, David Weir,
and Bill Keller. 2014. Learning to distinguish hy-
pernyms and co-hyponyms. In Proceedings of COL-
ING 2014, the 25th International Conference on
Computational Linguistics: Technical Papers, pages
2249–2259, Dublin, Ireland, August. Dublin City
University and Association for Computational Lin-
guistics.
Fei Wu and Daniel S. Weld. 2010. Open information
extraction using wikipedia. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 118–127, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.
Jihoon Yang and Vasant Honavar. 1998. Feature subset
selection using a genetic algorithm. In Feature ex-
traction, construction and selection, pages 117–136.
Springer.
</reference>
<sectionHeader confidence="0.975705" genericHeader="references">
Appendix A Efficient Path-Finding
</sectionHeader>
<bodyText confidence="0.998464">
We split the search to two phases: we first find all
nodes along the shortest paths between x and y,
and then reconstruct the actual paths. The first
phase ignores edge types, inducing a simpler re-
source graph, which we represent as a sparse adja-
cency matrix and manipulate efficiently with ma-
trix operations (Algorithm 1). Once the search
space is limited to relevant nodes only, the second
phase becomes trivial.
</bodyText>
<reference confidence="0.927231866666667">
Algorithm 1 Find Relevant Nodes
1: function NODESINPATH(nx, ny, len)
2: if len == 1 then
3: return nx U ny
4: for 0 &lt; k &lt; len do
5: if k is odd then
6: nx = nx A
7: else
8: ny = ny AT
9: if nx ny &gt; 0 then
10: nxy = nx n ny
11: nforward = nodesInPath(nx,nxy, [k])
12: nbackward = nodesInPath(nxy, ny, Lk2 j)
13: return 7iforward U nbackward
14: return 0�
</reference>
<bodyText confidence="0.966309333333333">
The algorithm finds all nodes in the paths between x and y
subject to the maximum length (len). A is the resource adja-
cency matrix and nx, ny are one-hot vectors of x, y.
At each iteration, we either make a forward (line 6) or a back-
ward (8) step. If the forward and backward search meet (9),
we recursively call the algorithm for each side (11-12), and
merge their results (13). The stop conditions are len = 0, re-
turning an empty set when no path was found, and len = 1,
merging both sides when they are connected by single edges.
</bodyText>
<page confidence="0.997997">
184
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.372709">
<title confidence="0.982191">Learning to Exploit Structured Resources for Lexical Inference</title>
<author confidence="0.7664535">Science Department</author>
<author confidence="0.7664535">Bar-Ilan of Engineering</author>
<author confidence="0.7664535">Bar-Ilan</author>
<email confidence="0.990527">jacob.goldberger@biu.ac.il</email>
<abstract confidence="0.999834722222222">Massive knowledge resources, such as Wikidata, can provide valuable information for lexical inference, especially for proper-names. Prior resource-based approaches typically select the subset of each resource’s relations which are relevant for a particular given task. The selection process is done manually, limiting these approaches to smaller resources such as WordNet, which lacks coverage of propernames and recent terminology. This paper presents a supervised framework for automatically selecting an optimized subset of resource relations for a given target inference task. Our approach enables the use of large-scale knowledge resources, thus providing a rich source of high-precision</abstract>
<intro confidence="0.636838">over</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
</authors>
<title>Oier Lopez de Lacalle, and Aitor Soroa.</title>
<date>2014</date>
<journal>Computational Linguistics,</journal>
<volume>40</volume>
<issue>1</issue>
<marker>Agirre, 2014</marker>
<rawString>Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa. 2014. Random walks for knowledge-based word sense disambiguation. Computational Linguistics, 40(1):57–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S¨oren Auer</author>
<author>Christian Bizer</author>
<author>Georgi Kobilarov</author>
<author>Jens Lehmann</author>
<author>Richard Cyganiak</author>
<author>Zachary Ives</author>
</authors>
<title>Dbpedia: A nucleus for a web of open data.</title>
<date>2007</date>
<publisher>Springer.</publisher>
<contexts>
<context position="2986" citStr="Auer et al., 2007" startWordPosition="425" endWordPosition="428">f lexical inference. While corpus-based methods usually enjoy high recall, their precision is often limited, hindering their applicability. An alternative common practice is to mine high-precision lexical inferences from structured resources, particularly WordNet (Fellbaum, 1998). Nevertheless, WordNet is an ontology of the English language, which, by definition, does not cover many propernames (Beyonc´e → artist) and recent terminology (Facebook → social network). A potential solution may lie in rich and up-to-date structured knowledge resources such as Wikidata (Vrandeˇci´c, 2012), DBPedia (Auer et al., 2007), and Yago (Suchanek et al., 2007). In this paper, we investigate how these resources can be exploited for lexical inference over proper-names. We begin by examining whether the common usage of WordNet for lexical inference can be extended to larger resources. Typically, a subset of WordNet relations is manually selected (e.g. all synonyms and hypernyms). By nature, each application captures a different aspect of lexical inference, and thus defines different relations as indicative of its particular flavor of lexical infer175 Proceedings of the 19th Conference on Computational Language Learnin</context>
<context position="7095" citStr="Auer et al., 2007" startWordPosition="1063" endWordPosition="1066">ommunity-built knowledge bases such as Wikidata (Vrandeˇci´c, 2012), which connect millions of entities through a rich variety of structured relations (properties). Using these resources for various NLP tasks has become exceedingly popular (Wu and Weld, 2010; Rahman and Ng, 2011; Unger et al., 2012; Berant et al., 2013). Little attention, however, was given to leveraging them for identifying lexical inference; the exception being Shnarch et al. (2009), who used structured data from Wikipedia for this purpose. In this paper, we experimented with such resources, in addition to WordNet. DBPedia (Auer et al., 2007) contains structured information from Wikipedia: info boxes, redirections, disambiguation links, etc. Wikidata (Vrandeˇci´c, 2012) contains facts edited by humans to support Wikipedia and other Wikimedia projects. Yago (Suchanek et al., 2007) is a semantic knowledge base derived from Wikipedia, WordNet, and GeoNames.2 Table 2 compares the scale of the resources we used. The massive scale of the more recent resources and their rich schemas can potentially increase the coverage of current WordNet-based approaches, yet make it difficult to manually select an optimized subset of relations for a ta</context>
</contexts>
<marker>Auer, Bizer, Kobilarov, Lehmann, Cyganiak, Ives, 2007</marker>
<rawString>S¨oren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. 2007. Dbpedia: A nucleus for a web of open data. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Alessandro Lenci</author>
</authors>
<title>How we blessed distributional semantic evaluation.</title>
<date>2011</date>
<booktitle>In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics,</booktitle>
<pages>1--10</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, UK,</location>
<contexts>
<context position="18304" citStr="Baroni and Lenci, 2011" startWordPosition="2912" endWordPosition="2915">eated in a slightly different manner, the underlying semantic relation R varies as well. 5.1 Existing Datasets kotlerman2010 (Kotlerman et al., 2010) is a manually annotated lexical entailment dataset of distributionally similar nouns. turney2014 (Turney and Mohammad, 2015) is based on a crowdsourced dataset of semantic relations, from which we removed non-nouns and lemmatized plurals. levy2014 (Levy et al., 2014) was generated from manually annotated entailment graphs of subject-verb-object tuples. Table 3 provides metadata on each dataset. Two additional datasets were created using WordNet (Baroni and Lenci, 2011; Baroni et al., 2012), whose definition of R can be trivially captured by a resource-based approach using WordNet. Hence, they are omitted from our evaluation. 5.2 A New Proper-Name Dataset An important linguistic component that is missing from these lexical-inference datasets is propernames. We conjecture that much of the added value in utilizing structured resources is the ability to cover terms such as celebrities (Lady Gaga) and recent terminology (social networks) that do not appear in WordNet. We thus created a new dataset of (x, y) pairs in which x is a proper-name, y is a common noun,</context>
</contexts>
<marker>Baroni, Lenci, 2011</marker>
<rawString>Marco Baroni and Alessandro Lenci. 2011. How we blessed distributional semantic evaluation. In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics, pages 1–10, Edinburgh, UK, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Raffaella Bernardi</author>
<author>Ngoc-Quynh Do</author>
<author>Chung-chieh Shan</author>
</authors>
<title>Entailment above the word level in distributional semantics.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>23--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Avignon, France,</location>
<contexts>
<context position="2046" citStr="Baroni et al., 2012" startWordPosition="285" endWordPosition="288">es Beyonc´e Knowles and Lloyd Blankfein, who are both worth $450 million. To correctly answer the question, the application needs to know that Beyonc´e is an artist, and that Lloyd Blankfein is not. 1Our code and data are available at: https://github.com/vered1986/LinKeR Corpus-based methods are often employed to recognize lexical inferences, based on either cooccurrence patterns (Hearst, 1992; Turney, 2006) or distributional representations (Weeds and Weir, 2003; Kotlerman et al., 2010). While earlier methods were mostly unsupervised, recent trends introduced supervised methods for the task (Baroni et al., 2012; Turney and Mohammad, 2015; Roller et al., 2014). In these settings, a targeted lexical inference relation is implicitly defined by a training set of term-pairs, which are annotated as positive or negative examples of this relation. Several such datasets have been created, each representing a somewhat different flavor of lexical inference. While corpus-based methods usually enjoy high recall, their precision is often limited, hindering their applicability. An alternative common practice is to mine high-precision lexical inferences from structured resources, particularly WordNet (Fellbaum, 199</context>
<context position="18326" citStr="Baroni et al., 2012" startWordPosition="2916" endWordPosition="2919">erent manner, the underlying semantic relation R varies as well. 5.1 Existing Datasets kotlerman2010 (Kotlerman et al., 2010) is a manually annotated lexical entailment dataset of distributionally similar nouns. turney2014 (Turney and Mohammad, 2015) is based on a crowdsourced dataset of semantic relations, from which we removed non-nouns and lemmatized plurals. levy2014 (Levy et al., 2014) was generated from manually annotated entailment graphs of subject-verb-object tuples. Table 3 provides metadata on each dataset. Two additional datasets were created using WordNet (Baroni and Lenci, 2011; Baroni et al., 2012), whose definition of R can be trivially captured by a resource-based approach using WordNet. Hence, they are omitted from our evaluation. 5.2 A New Proper-Name Dataset An important linguistic component that is missing from these lexical-inference datasets is propernames. We conjecture that much of the added value in utilizing structured resources is the ability to cover terms such as celebrities (Lady Gaga) and recent terminology (social networks) that do not appear in WordNet. We thus created a new dataset of (x, y) pairs in which x is a proper-name, y is a common noun, and R is the is a rel</context>
<context position="24311" citStr="Baroni et al., 2012" startWordPosition="3878" endWordPosition="3881">ve. Table 5 shows some of the learnt edge types (see the supplementary material for the complete list). The performance boost in proper2015 demonstrates that community-built resources have much added value when considering propernames. As expected, many proper-names do not appear in WordNet (Doctor Who). That said, even when both terms appear in WordNet, they often lack important properties covered by other resources (Louisa May Alcott is a woman). 6.3 Comparison to Corpus-based Methods Lexical inference has been thoroughly explored in distributional semantics, with recent supervised methods (Baroni et al., 2012; Turney and Mohammad, 2015) showing promising results. While Table 5: An excerpt of the whitelist learnt for proper2015 by the binary model with accompanying true-positives that do not have an indicative path in WordNet. these methods leverage huge corpora to increase coverage, they often introduce noise that affects their precision. Structured resources, on the other hand, are precision-oriented. We therefore expect our approach to complement distributional methods in high-precision scenarios. To represent term-pairs with distributional features, we downloaded the pre-trained word2vec embedd</context>
</contexts>
<marker>Baroni, Bernardi, Do, Shan, 2012</marker>
<rawString>Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do, and Chung-chieh Shan. 2012. Entailment above the word level in distributional semantics. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 23–32, Avignon, France, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Andrew Chou</author>
<author>Roy Frostig</author>
<author>Percy Liang</author>
</authors>
<title>Semantic parsing on Freebase from question-answer pairs.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1533--1544</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="6798" citStr="Berant et al., 2013" startWordPosition="1015" endWordPosition="1019">tailored to each dataset and task. This work addresses this issue by automatically learning the subset of relations relevant to the task. 2.2 Structured Knowledge Resources While WordNet is quite extensive, it is handcrafted by expert lexicographers, and thus cannot compete in terms of scale with community-built knowledge bases such as Wikidata (Vrandeˇci´c, 2012), which connect millions of entities through a rich variety of structured relations (properties). Using these resources for various NLP tasks has become exceedingly popular (Wu and Weld, 2010; Rahman and Ng, 2011; Unger et al., 2012; Berant et al., 2013). Little attention, however, was given to leveraging them for identifying lexical inference; the exception being Shnarch et al. (2009), who used structured data from Wikipedia for this purpose. In this paper, we experimented with such resources, in addition to WordNet. DBPedia (Auer et al., 2007) contains structured information from Wikipedia: info boxes, redirections, disambiguation links, etc. Wikidata (Vrandeˇci´c, 2012) contains facts edited by humans to support Wikipedia and other Wikimedia projects. Yago (Suchanek et al., 2007) is a semantic knowledge base derived from Wikipedia, WordNet</context>
</contexts>
<marker>Berant, Chou, Frostig, Liang, 2013</marker>
<rawString>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533–1544, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Brody</author>
<author>Roberto Navigli</author>
<author>Mirella Lapata</author>
</authors>
<title>Ensemble methods for unsupervised wsd.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>97--104</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Brody, Navigli, Lapata, 2006</marker>
<rawString>Samuel Brody, Roberto Navigli, and Mirella Lapata. 2006. Ensemble methods for unsupervised wsd. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 97–104. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<date>1998</date>
<publisher>WordNet. Wiley Online Library.</publisher>
<contexts>
<context position="2648" citStr="Fellbaum, 1998" startWordPosition="374" endWordPosition="375"> et al., 2012; Turney and Mohammad, 2015; Roller et al., 2014). In these settings, a targeted lexical inference relation is implicitly defined by a training set of term-pairs, which are annotated as positive or negative examples of this relation. Several such datasets have been created, each representing a somewhat different flavor of lexical inference. While corpus-based methods usually enjoy high recall, their precision is often limited, hindering their applicability. An alternative common practice is to mine high-precision lexical inferences from structured resources, particularly WordNet (Fellbaum, 1998). Nevertheless, WordNet is an ontology of the English language, which, by definition, does not cover many propernames (Beyonc´e → artist) and recent terminology (Facebook → social network). A potential solution may lie in rich and up-to-date structured knowledge resources such as Wikidata (Vrandeˇci´c, 2012), DBPedia (Auer et al., 2007), and Yago (Suchanek et al., 2007). In this paper, we investigate how these resources can be exploited for lexical inference over proper-names. We begin by examining whether the common usage of WordNet for lexical inference can be extended to larger resources. T</context>
<context position="4933" citStr="Fellbaum, 1998" startWordPosition="722" endWordPosition="723">ethod is required. We present a principled supervised framework, which automates the selection process of resource relations, and optimizes this subset for a given target inference relation. This automation allows us to leverage large-scale resources, and extract many high-precision inferences over propernames, which are absent from WordNet. Finally, we show that our framework complements stateof-the-art corpus-based methods. Combining the two approaches can particularly benefit real-world tasks in which proper-names are prominent. 2 Background 2.1 Common Use of WordNet for Inference WordNet (Fellbaum, 1998) is widely used for identifying lexical inference. It is usually used in an unsupervised setting where the relations relevant for each specific inference task are manually selected a priori. One approach looks for chains of these predefined relations (Harabagiu and Moldovan, 1998), e.g. dog —* mammal using a chain of hypernyms: dog —* canine —* carnivore —* placental mammal —* mammal. Another approach is via WordNet Similarity (Pedersen et al., 2004), which takes two synsets and returns a numeric value that represents their similarity based on WordNet’s hierarchical hypernymy structure. While </context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet. Wiley Online Library.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruiji Fu</author>
<author>Jiang Guo</author>
<author>Bing Qin</author>
<author>Wanxiang Che</author>
<author>Haifeng Wang</author>
<author>Ting Liu</author>
</authors>
<title>Learning semantic hierarchies via word embeddings.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1199--1209</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="25568" citStr="Fu et al., 2014" startWordPosition="4072" endWordPosition="4075">uge corpus (100 billion words) using a stateof-the-art embedding algorithm (Mikolov et al., 2013). Since each vector represents a single term (either x or y), we used 3 state-of-the-art meth9http://code.google.com/p/word2vec/ Edge Type occupation sex or gender instance of acted in genre Example Daniel Radcliffe → actor Louisa May Alcott → woman Doctor Who → series Michael Keaton → Beetlejuice Touch → drama position played on team Jason Collins → center 181 ods to construct a feature vector for each termpair: concatenation x� ® y� (Baroni et al., 2012), difference y� − x� (Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014), and similarity x� · y. We then used Fβ-optimized logistic regression to train a classifier. Figure 5 compares our methods to concatenation, which was the best-performing corpus-based method.10 In turney2014 and proper2015, the embeddings retain over 80% precision while boasting higher recall than our method’s. In turney2014, it is often a result of the more associative relations prominent in the dataset (football —* playbook), which seldom are expressed in structured resources. In proper2015, the difference in recall seems to be from missing terminology (Twitter —* socia</context>
</contexts>
<marker>Fu, Guo, Qin, Che, Wang, Liu, 2014</marker>
<rawString>Ruiji Fu, Jiang Guo, Bing Qin, Wanxiang Che, Haifeng Wang, and Ting Liu. 2014. Learning semantic hierarchies via word embeddings. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1199–1209, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Dan Moldovan</author>
</authors>
<title>Knowledge processing on an extended wordNet. WordNet: An electronic lexical database,</title>
<date>1998</date>
<pages>305--381</pages>
<contexts>
<context position="5214" citStr="Harabagiu and Moldovan, 1998" startWordPosition="764" endWordPosition="767">y high-precision inferences over propernames, which are absent from WordNet. Finally, we show that our framework complements stateof-the-art corpus-based methods. Combining the two approaches can particularly benefit real-world tasks in which proper-names are prominent. 2 Background 2.1 Common Use of WordNet for Inference WordNet (Fellbaum, 1998) is widely used for identifying lexical inference. It is usually used in an unsupervised setting where the relations relevant for each specific inference task are manually selected a priori. One approach looks for chains of these predefined relations (Harabagiu and Moldovan, 1998), e.g. dog —* mammal using a chain of hypernyms: dog —* canine —* carnivore —* placental mammal —* mammal. Another approach is via WordNet Similarity (Pedersen et al., 2004), which takes two synsets and returns a numeric value that represents their similarity based on WordNet’s hierarchical hypernymy structure. While there is a broad consensus that synonyms entail each other (elevator H lift) and hyponyms entail their hypernyms (cat —* animal), other relations, such as meronymy, are not agreed Resource #Entities #Properties Version DBPedia 4,500,000 1,367 July 2014 Wikidata 6,000,000 1,200 Jul</context>
</contexts>
<marker>Harabagiu, Moldovan, 1998</marker>
<rawString>Sanda Harabagiu and Dan Moldovan. 1998. Knowledge processing on an extended wordNet. WordNet: An electronic lexical database, 305:381–405.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In COLING 1992 Volume 2: The 15th International Conference on Computational Linguistics,</booktitle>
<pages>529--545</pages>
<location>Nantes, France.</location>
<contexts>
<context position="1823" citStr="Hearst, 1992" startWordPosition="254" endWordPosition="255"> be used to infer the meaning of one word from another, in order to address lexical variability. For instance, a question answering system asked “which artist’s net worth is $450 million?” might retrieve the candidates Beyonc´e Knowles and Lloyd Blankfein, who are both worth $450 million. To correctly answer the question, the application needs to know that Beyonc´e is an artist, and that Lloyd Blankfein is not. 1Our code and data are available at: https://github.com/vered1986/LinKeR Corpus-based methods are often employed to recognize lexical inferences, based on either cooccurrence patterns (Hearst, 1992; Turney, 2006) or distributional representations (Weeds and Weir, 2003; Kotlerman et al., 2010). While earlier methods were mostly unsupervised, recent trends introduced supervised methods for the task (Baroni et al., 2012; Turney and Mohammad, 2015; Roller et al., 2014). In these settings, a targeted lexical inference relation is implicitly defined by a training set of term-pairs, which are annotated as positive or negative examples of this relation. Several such datasets have been created, each representing a somewhat different flavor of lexical inference. While corpus-based methods usually</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In COLING 1992 Volume 2: The 15th International Conference on Computational Linguistics, pages 529–545, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Jansche</author>
</authors>
<title>Maximum expected f-measure training of logistic regression models.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>692--699</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Vancouver, British Columbia, Canada,</location>
<marker>Jansche, 2005</marker>
<rawString>Martin Jansche. 2005. Maximum expected f-measure training of logistic regression models. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 692–699, Vancouver, British Columbia, Canada, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lili Kotlerman</author>
<author>Ido Dagan</author>
<author>Idan Szpektor</author>
<author>Maayan Zhitomirsky-Geffet</author>
</authors>
<title>Directional distributional similarity for lexical inference.</title>
<date>2010</date>
<journal>Natural Language Engineering,</journal>
<volume>16</volume>
<issue>04</issue>
<contexts>
<context position="1919" citStr="Kotlerman et al., 2010" startWordPosition="265" endWordPosition="268">ariability. For instance, a question answering system asked “which artist’s net worth is $450 million?” might retrieve the candidates Beyonc´e Knowles and Lloyd Blankfein, who are both worth $450 million. To correctly answer the question, the application needs to know that Beyonc´e is an artist, and that Lloyd Blankfein is not. 1Our code and data are available at: https://github.com/vered1986/LinKeR Corpus-based methods are often employed to recognize lexical inferences, based on either cooccurrence patterns (Hearst, 1992; Turney, 2006) or distributional representations (Weeds and Weir, 2003; Kotlerman et al., 2010). While earlier methods were mostly unsupervised, recent trends introduced supervised methods for the task (Baroni et al., 2012; Turney and Mohammad, 2015; Roller et al., 2014). In these settings, a targeted lexical inference relation is implicitly defined by a training set of term-pairs, which are annotated as positive or negative examples of this relation. Several such datasets have been created, each representing a somewhat different flavor of lexical inference. While corpus-based methods usually enjoy high recall, their precision is often limited, hindering their applicability. An alternat</context>
<context position="17831" citStr="Kotlerman et al., 2010" startWordPosition="2845" endWordPosition="2848"> the weighted model’s hypothesis space subsumes the binary model’s, the binary model performed better on our datasets. We conjecture that this stems from the limited amount of training instances, which prevents a more general model from converging into an optimal solution. 5 Datasets We used 3 existing common-noun datasets and one new proper-name dataset. Each dataset consists of annotated (x, y) term-pairs, where both x and y are noun phrases. Since each dataset was created in a slightly different manner, the underlying semantic relation R varies as well. 5.1 Existing Datasets kotlerman2010 (Kotlerman et al., 2010) is a manually annotated lexical entailment dataset of distributionally similar nouns. turney2014 (Turney and Mohammad, 2015) is based on a crowdsourced dataset of semantic relations, from which we removed non-nouns and lemmatized plurals. levy2014 (Levy et al., 2014) was generated from manually annotated entailment graphs of subject-verb-object tuples. Table 3 provides metadata on each dataset. Two additional datasets were created using WordNet (Baroni and Lenci, 2011; Baroni et al., 2012), whose definition of R can be trivially captured by a resource-based approach using WordNet. Hence, they</context>
</contexts>
<marker>Kotlerman, Dagan, Szpektor, Zhitomirsky-Geffet, 2010</marker>
<rawString>Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan Zhitomirsky-Geffet. 2010. Directional distributional similarity for lexical inference. Natural Language Engineering, 16(04):359–389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Ido Dagan</author>
<author>Jacob Goldberger</author>
</authors>
<title>Focused entailment graphs for open ie propositions.</title>
<date>2014</date>
<booktitle>In Proceedings of the Eighteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>87--97</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="18099" citStr="Levy et al., 2014" startWordPosition="2883" endWordPosition="2886"> 5 Datasets We used 3 existing common-noun datasets and one new proper-name dataset. Each dataset consists of annotated (x, y) term-pairs, where both x and y are noun phrases. Since each dataset was created in a slightly different manner, the underlying semantic relation R varies as well. 5.1 Existing Datasets kotlerman2010 (Kotlerman et al., 2010) is a manually annotated lexical entailment dataset of distributionally similar nouns. turney2014 (Turney and Mohammad, 2015) is based on a crowdsourced dataset of semantic relations, from which we removed non-nouns and lemmatized plurals. levy2014 (Levy et al., 2014) was generated from manually annotated entailment graphs of subject-verb-object tuples. Table 3 provides metadata on each dataset. Two additional datasets were created using WordNet (Baroni and Lenci, 2011; Baroni et al., 2012), whose definition of R can be trivially captured by a resource-based approach using WordNet. Hence, they are omitted from our evaluation. 5.2 A New Proper-Name Dataset An important linguistic component that is missing from these lexical-inference datasets is propernames. We conjecture that much of the added value in utilizing structured resources is the ability to cover</context>
</contexts>
<marker>Levy, Dagan, Goldberger, 2014</marker>
<rawString>Omer Levy, Ido Dagan, and Jacob Goldberger. 2014. Focused entailment graphs for open ie propositions. In Proceedings of the Eighteenth Conference on Computational Natural Language Learning, pages 87–97, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Steffen Remus</author>
<author>Chris Biemann</author>
<author>Ido Dagan</author>
</authors>
<title>Do supervised distributional methods really learn lexical inference relations?</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>970--976</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Denver, Colorado, May–June.</location>
<contexts>
<context position="20948" citStr="Levy et al., 2015" startWordPosition="3355" endWordPosition="3358">urces to be in the high-precision region. To create these curves, we optimized our method and the baselines using Fβ with 40 values of β2 E (0, 2). We randomly split each dataset into 70% train, 25% test and 5% validation.7 We applied L2 regularization to our method and the baselines, tuning the regularization parameter on the validation set. 6.1 Performance on WordNet We examine whether our algorithm can replicate the common use of WordNet (§2.1), by manually constructing 4 whitelists based on the literature 7Since our methods do not use lexical features, we did not use lexical splits as in (Levy et al., 2015). 179 Figure 3: Recall-precision curve of each dataset with WordNet as the only resource. Each point in the graph stands for the performance on a certain value of β. Notice that in some of the graphs, different β values yield the same performance, causing less points to be displayed. (see Table 4), and evaluating their performance using the classification methods in §4.2. In addition, we compare our method to Resnik’s (1995) WordNet similarity, which scores each pair of terms based on their lowest common hypernym. This score was used as a single feature in Fβ-optimized logistic regression to c</context>
<context position="27508" citStr="Levy et al., 2015" startWordPosition="4379" endWordPosition="4382">tainable recall that maintains high precision. This point is often at the top of a “precision cliff” in Figures 3 and 4. These settings are presented in Table 6. The high-precision settings we chose resulted in few false positives, most of which are caused by annotation errors or resource errors. Naturally, regions of higher recall and lower precision will yield more false positives and less false negatives. We thus focus the rest of our discussion on false negatives (Table 7). While structured resources cover most terms, 10Note that the corpus-based method benefits from lexical memorization (Levy et al., 2015), overfitting for the lexical terms in the training set, while our resource-based method does not. This means that Figure 5 paints a relatively optimistic picture of the embeddings’ actual performance. Dataset β Whitelist Prec. Rec. kotlerman2010 0.05 basic 83% 9% turney2014 0.05 +mero 93% 27% levy2014 10−5 basic 87% 37% proper2015 0.3 44 edge types 97% 29% from all resources (see supplementary material) Table 6: The error analysis setting of each dataset. Error Type kotlerman levy turney proper 2010 2014 2014 2015 Not Covered 2% 12% 4% 13% No Indicative Paths 35% 48% 73% 75% Whitelist Error 6</context>
</contexts>
<marker>Levy, Remus, Biemann, Dagan, 2015</marker>
<rawString>Omer Levy, Steffen Remus, Chris Biemann, and Ido Dagan. 2015. Do supervised distributional methods really learn lexical inference relations? In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 970–976, Denver, Colorado, May–June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Gregory S Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="25050" citStr="Mikolov et al., 2013" startWordPosition="3984" endWordPosition="3987">15 by the binary model with accompanying true-positives that do not have an indicative path in WordNet. these methods leverage huge corpora to increase coverage, they often introduce noise that affects their precision. Structured resources, on the other hand, are precision-oriented. We therefore expect our approach to complement distributional methods in high-precision scenarios. To represent term-pairs with distributional features, we downloaded the pre-trained word2vec embeddings.9 These vectors were trained over a huge corpus (100 billion words) using a stateof-the-art embedding algorithm (Mikolov et al., 2013). Since each vector represents a single term (either x or y), we used 3 state-of-the-art meth9http://code.google.com/p/word2vec/ Edge Type occupation sex or gender instance of acted in genre Example Daniel Radcliffe → actor Louisa May Alcott → woman Doctor Who → series Michael Keaton → Beetlejuice Touch → drama position played on team Jason Collins → center 181 ods to construct a feature vector for each termpair: concatenation x� ® y� (Baroni et al., 2012), difference y� − x� (Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014), and similarity x� · y. We then used Fβ-optimized logistic r</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>Wordnet::similarity -measuring the relatedness of concepts.</title>
<date>2004</date>
<booktitle>In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Demonstration Papers,</booktitle>
<volume>2</volume>
<pages>38--41</pages>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="5387" citStr="Pedersen et al., 2004" startWordPosition="795" endWordPosition="798"> approaches can particularly benefit real-world tasks in which proper-names are prominent. 2 Background 2.1 Common Use of WordNet for Inference WordNet (Fellbaum, 1998) is widely used for identifying lexical inference. It is usually used in an unsupervised setting where the relations relevant for each specific inference task are manually selected a priori. One approach looks for chains of these predefined relations (Harabagiu and Moldovan, 1998), e.g. dog —* mammal using a chain of hypernyms: dog —* canine —* carnivore —* placental mammal —* mammal. Another approach is via WordNet Similarity (Pedersen et al., 2004), which takes two synsets and returns a numeric value that represents their similarity based on WordNet’s hierarchical hypernymy structure. While there is a broad consensus that synonyms entail each other (elevator H lift) and hyponyms entail their hypernyms (cat —* animal), other relations, such as meronymy, are not agreed Resource #Entities #Properties Version DBPedia 4,500,000 1,367 July 2014 Wikidata 6,000,000 1,200 July 2014 Yago 10,000,000 70 December 2014 WordNet 150,000 13 3.0 Table 2: Structured resources explored in this work. upon, and may vary depending on task and context (e.g. li</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. Wordnet::similarity -measuring the relatedness of concepts. In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Demonstration Papers, pages 38–41, Boston, Massachusetts, USA, May 2 - May 7. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Altaf Rahman</author>
<author>Vincent Ng</author>
</authors>
<title>Coreference resolution with world knowledge.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>814--824</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="6756" citStr="Rahman and Ng, 2011" startWordPosition="1007" endWordPosition="1010">ations, and a suitable subset is usually tailored to each dataset and task. This work addresses this issue by automatically learning the subset of relations relevant to the task. 2.2 Structured Knowledge Resources While WordNet is quite extensive, it is handcrafted by expert lexicographers, and thus cannot compete in terms of scale with community-built knowledge bases such as Wikidata (Vrandeˇci´c, 2012), which connect millions of entities through a rich variety of structured relations (properties). Using these resources for various NLP tasks has become exceedingly popular (Wu and Weld, 2010; Rahman and Ng, 2011; Unger et al., 2012; Berant et al., 2013). Little attention, however, was given to leveraging them for identifying lexical inference; the exception being Shnarch et al. (2009), who used structured data from Wikipedia for this purpose. In this paper, we experimented with such resources, in addition to WordNet. DBPedia (Auer et al., 2007) contains structured information from Wikipedia: info boxes, redirections, disambiguation links, etc. Wikidata (Vrandeˇci´c, 2012) contains facts edited by humans to support Wikipedia and other Wikimedia projects. Yago (Suchanek et al., 2007) is a semantic know</context>
</contexts>
<marker>Rahman, Ng, 2011</marker>
<rawString>Altaf Rahman and Vincent Ng. 2011. Coreference resolution with world knowledge. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 814–824, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th international joint conference on Artificial intelligence - Volume 1, IJCAI’95,</booktitle>
<pages>448--453</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In Proceedings of the 14th international joint conference on Artificial intelligence - Volume 1, IJCAI’95, pages 448–453. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Roller</author>
<author>Katrin Erk</author>
<author>Gemma Boleda</author>
</authors>
<title>Inclusive yet selective: Supervised distributional hypernymy detection.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,</booktitle>
<pages>1025--1036</pages>
<institution>Dublin City University and Association for Computational Linguistics.</institution>
<location>Dublin, Ireland,</location>
<contexts>
<context position="2095" citStr="Roller et al., 2014" startWordPosition="293" endWordPosition="296"> both worth $450 million. To correctly answer the question, the application needs to know that Beyonc´e is an artist, and that Lloyd Blankfein is not. 1Our code and data are available at: https://github.com/vered1986/LinKeR Corpus-based methods are often employed to recognize lexical inferences, based on either cooccurrence patterns (Hearst, 1992; Turney, 2006) or distributional representations (Weeds and Weir, 2003; Kotlerman et al., 2010). While earlier methods were mostly unsupervised, recent trends introduced supervised methods for the task (Baroni et al., 2012; Turney and Mohammad, 2015; Roller et al., 2014). In these settings, a targeted lexical inference relation is implicitly defined by a training set of term-pairs, which are annotated as positive or negative examples of this relation. Several such datasets have been created, each representing a somewhat different flavor of lexical inference. While corpus-based methods usually enjoy high recall, their precision is often limited, hindering their applicability. An alternative common practice is to mine high-precision lexical inferences from structured resources, particularly WordNet (Fellbaum, 1998). Nevertheless, WordNet is an ontology of the E</context>
<context position="25551" citStr="Roller et al., 2014" startWordPosition="4068" endWordPosition="4071">were trained over a huge corpus (100 billion words) using a stateof-the-art embedding algorithm (Mikolov et al., 2013). Since each vector represents a single term (either x or y), we used 3 state-of-the-art meth9http://code.google.com/p/word2vec/ Edge Type occupation sex or gender instance of acted in genre Example Daniel Radcliffe → actor Louisa May Alcott → woman Doctor Who → series Michael Keaton → Beetlejuice Touch → drama position played on team Jason Collins → center 181 ods to construct a feature vector for each termpair: concatenation x� ® y� (Baroni et al., 2012), difference y� − x� (Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014), and similarity x� · y. We then used Fβ-optimized logistic regression to train a classifier. Figure 5 compares our methods to concatenation, which was the best-performing corpus-based method.10 In turney2014 and proper2015, the embeddings retain over 80% precision while boasting higher recall than our method’s. In turney2014, it is often a result of the more associative relations prominent in the dataset (football —* playbook), which seldom are expressed in structured resources. In proper2015, the difference in recall seems to be from missing terminology </context>
</contexts>
<marker>Roller, Erk, Boleda, 2014</marker>
<rawString>Stephen Roller, Katrin Erk, and Gemma Boleda. 2014. Inclusive yet selective: Supervised distributional hypernymy detection. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1025– 1036, Dublin, Ireland, August. Dublin City University and Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Rumelhart</author>
<author>G E Hinton</author>
<author>R J Williams</author>
</authors>
<title>Learning representations by back-propagating errors.</title>
<date>1986</date>
<journal>Nature,</journal>
<pages>533--536</pages>
<contexts>
<context position="13627" citStr="Rumelhart et al., 1986" startWordPosition="2148" endWordPosition="2151">ver recall. 4.2.1 Weighted Edge Model A typical neural network approach is to assign a weight wi to each edge type ei, where more indicative edge types should have higher values of wi. The indicativeness of a path (ˆp) is modeled using logistic regression: pˆ°= σ(w · 0—), where 0� is the path’s “bag of edges” representation, i.e. a feature vector of each edge type’s frequency in the path. The probability of a term-pair being positive can be determined using either the sum of all path scores or the score of its most indicative path (max-pooling). We trained both variants with back-propagation (Rumelhart et al., 1986) and gradient ascent. In particular, we optimized Fβ using a variant of Jansche’s (2005) derivation of Fβ-optimized logistic regression (see suplementary material5 for full derivation). This model can theoretically quantify how indicative each edge type is of R. Specifically, it can differentiate weakly indicative edges (e.g. meronyms) from those that contradict R (e.g. antonyms). However, on our datasets, this model yielded sub-optimal results (see §6.1), and therefore serves as a baseline to the binary model presented in the following section. 4.2.2 Binary Edge Model Preliminary experiments </context>
</contexts>
<marker>Rumelhart, Hinton, Williams, 1986</marker>
<rawString>D E Rumelhart, G E Hinton, and R J Williams. 1986. Learning representations by back-propagating errors. Nature, pages 533–536.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Russell</author>
<author>Peter Norvig</author>
</authors>
<title>Artificial Intelligence: A Modern Approach.</title>
<date>2009</date>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="10757" citStr="Russell and Norvig, 2009" startWordPosition="1663" endWordPosition="1666"> resource graph G. We first represent (x, y) as the set of paths connecting x and y in G (§4.1). We then classify each such path as indicative or not of R, and decide accordingly whether xRy (§4.2). 4.1 Representing Term-Pairs as Path-Sets We represent each (x, y) pair as the set of paths that link x and y within each resource. We retain only the shortest paths (all paths x ❀ y of minimal length) as they yielded better performance. Resource graphs are densely connected, and thus have a huge branching factor b. We thus limited the maximum path length to E = 8 and employed bidirectional search (Russell and Norvig, 2009, Ch.3) to find the shortest paths. This algorithm runs two simultaneous instances of breadthfirst search (BFS), one from x and another from y, halting when they meet in the middle. It is much more efficient, having a complexity of O(bP/2) = O(b4) instead of BFS’s O(bP) = O(b8). To further reduce complexity, we split the search to two phases: we first find all nodes along the shortest paths between x and y, and then reconstruct the actual paths. Searching for relevant nodes ignores edge types, inducing a simpler resource graph, which can be represented as a sparse adjacency matrix and manipula</context>
<context position="16144" citStr="Russell and Norvig, 2009" startWordPosition="2572" endWordPosition="2575">possible edge types E = {e1,..., en} and a utility function u : 2E → R, find the subset (whitelist) w C E that maximizes the utility, i.e. w∗ = arg maxw u(w). In our case, the utility u is the Fβ score over the training set. Structured knowledge resources contain hundreds of different edge types, making E very large, and an exhaustive search over its powerset infeasible. The standard approach to this class of subset selection problems is to apply local search algorithms, which find an approximation of the optimal subset. We tried several local search algorithms, and found that genetic search (Russell and Norvig, 2009, Ch.4) performed well. In general, genetic search is claimed to be a preferred strategy for subset selection (Yang and Honavar, 1998). In our application of genetic search, each individual (candidate solution) is a whitelist, represented by a bit vector with a bit for each edge type. We defined the fitness function of a whitelist w according to the Fβ score of w over the training set. 6As a corollary, if x✚Ry, then every path between them is non-indicative. supervision p1 e1 x → y p2 parameters e2 p3 e3 4F = (1+02) ·precision ·recall 0 02·precision+recall 178 Dataset #Instances #Positive #Neg</context>
</contexts>
<marker>Russell, Norvig, 2009</marker>
<rawString>Stuart Russell and Peter Norvig. 2009. Artificial Intelligence: A Modern Approach. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eyal Shnarch</author>
<author>Libby Barak</author>
<author>Ido Dagan</author>
</authors>
<title>Extracting lexical reference rules from wikipedia.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>450--458</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="6932" citStr="Shnarch et al. (2009)" startWordPosition="1036" endWordPosition="1039">ask. 2.2 Structured Knowledge Resources While WordNet is quite extensive, it is handcrafted by expert lexicographers, and thus cannot compete in terms of scale with community-built knowledge bases such as Wikidata (Vrandeˇci´c, 2012), which connect millions of entities through a rich variety of structured relations (properties). Using these resources for various NLP tasks has become exceedingly popular (Wu and Weld, 2010; Rahman and Ng, 2011; Unger et al., 2012; Berant et al., 2013). Little attention, however, was given to leveraging them for identifying lexical inference; the exception being Shnarch et al. (2009), who used structured data from Wikipedia for this purpose. In this paper, we experimented with such resources, in addition to WordNet. DBPedia (Auer et al., 2007) contains structured information from Wikipedia: info boxes, redirections, disambiguation links, etc. Wikidata (Vrandeˇci´c, 2012) contains facts edited by humans to support Wikipedia and other Wikimedia projects. Yago (Suchanek et al., 2007) is a semantic knowledge base derived from Wikipedia, WordNet, and GeoNames.2 Table 2 compares the scale of the resources we used. The massive scale of the more recent resources and their rich sc</context>
</contexts>
<marker>Shnarch, Barak, Dagan, 2009</marker>
<rawString>Eyal Shnarch, Libby Barak, and Ido Dagan. 2009. Extracting lexical reference rules from wikipedia. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 450–458, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago: a core of semantic knowledge.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th international conference on World Wide Web,</booktitle>
<pages>697--706</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="3020" citStr="Suchanek et al., 2007" startWordPosition="431" endWordPosition="434">pus-based methods usually enjoy high recall, their precision is often limited, hindering their applicability. An alternative common practice is to mine high-precision lexical inferences from structured resources, particularly WordNet (Fellbaum, 1998). Nevertheless, WordNet is an ontology of the English language, which, by definition, does not cover many propernames (Beyonc´e → artist) and recent terminology (Facebook → social network). A potential solution may lie in rich and up-to-date structured knowledge resources such as Wikidata (Vrandeˇci´c, 2012), DBPedia (Auer et al., 2007), and Yago (Suchanek et al., 2007). In this paper, we investigate how these resources can be exploited for lexical inference over proper-names. We begin by examining whether the common usage of WordNet for lexical inference can be extended to larger resources. Typically, a subset of WordNet relations is manually selected (e.g. all synonyms and hypernyms). By nature, each application captures a different aspect of lexical inference, and thus defines different relations as indicative of its particular flavor of lexical infer175 Proceedings of the 19th Conference on Computational Language Learning, pages 175–184, Beijing, China, </context>
<context position="7337" citStr="Suchanek et al., 2007" startWordPosition="1096" endWordPosition="1099">r (Wu and Weld, 2010; Rahman and Ng, 2011; Unger et al., 2012; Berant et al., 2013). Little attention, however, was given to leveraging them for identifying lexical inference; the exception being Shnarch et al. (2009), who used structured data from Wikipedia for this purpose. In this paper, we experimented with such resources, in addition to WordNet. DBPedia (Auer et al., 2007) contains structured information from Wikipedia: info boxes, redirections, disambiguation links, etc. Wikidata (Vrandeˇci´c, 2012) contains facts edited by humans to support Wikipedia and other Wikimedia projects. Yago (Suchanek et al., 2007) is a semantic knowledge base derived from Wikipedia, WordNet, and GeoNames.2 Table 2 compares the scale of the resources we used. The massive scale of the more recent resources and their rich schemas can potentially increase the coverage of current WordNet-based approaches, yet make it difficult to manually select an optimized subset of relations for a task. Our method automatically learns such a subset, and provides lexical inferences on entities that are absent from WordNet, particularly proper-names. 2We also considered Freebase, but it required significantly larger computational resources</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2007</marker>
<rawString>Fabian M Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: a core of semantic knowledge. In Proceedings of the 16th international conference on World Wide Web, pages 697–706. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Saif M Mohammad</author>
</authors>
<title>Experiments with three approaches to recognizing lexical entailment.</title>
<date>2015</date>
<journal>Natural Language Engineering,</journal>
<volume>21</volume>
<issue>03</issue>
<contexts>
<context position="2073" citStr="Turney and Mohammad, 2015" startWordPosition="289" endWordPosition="292">nd Lloyd Blankfein, who are both worth $450 million. To correctly answer the question, the application needs to know that Beyonc´e is an artist, and that Lloyd Blankfein is not. 1Our code and data are available at: https://github.com/vered1986/LinKeR Corpus-based methods are often employed to recognize lexical inferences, based on either cooccurrence patterns (Hearst, 1992; Turney, 2006) or distributional representations (Weeds and Weir, 2003; Kotlerman et al., 2010). While earlier methods were mostly unsupervised, recent trends introduced supervised methods for the task (Baroni et al., 2012; Turney and Mohammad, 2015; Roller et al., 2014). In these settings, a targeted lexical inference relation is implicitly defined by a training set of term-pairs, which are annotated as positive or negative examples of this relation. Several such datasets have been created, each representing a somewhat different flavor of lexical inference. While corpus-based methods usually enjoy high recall, their precision is often limited, hindering their applicability. An alternative common practice is to mine high-precision lexical inferences from structured resources, particularly WordNet (Fellbaum, 1998). Nevertheless, WordNet i</context>
<context position="17956" citStr="Turney and Mohammad, 2015" startWordPosition="2861" endWordPosition="2864">conjecture that this stems from the limited amount of training instances, which prevents a more general model from converging into an optimal solution. 5 Datasets We used 3 existing common-noun datasets and one new proper-name dataset. Each dataset consists of annotated (x, y) term-pairs, where both x and y are noun phrases. Since each dataset was created in a slightly different manner, the underlying semantic relation R varies as well. 5.1 Existing Datasets kotlerman2010 (Kotlerman et al., 2010) is a manually annotated lexical entailment dataset of distributionally similar nouns. turney2014 (Turney and Mohammad, 2015) is based on a crowdsourced dataset of semantic relations, from which we removed non-nouns and lemmatized plurals. levy2014 (Levy et al., 2014) was generated from manually annotated entailment graphs of subject-verb-object tuples. Table 3 provides metadata on each dataset. Two additional datasets were created using WordNet (Baroni and Lenci, 2011; Baroni et al., 2012), whose definition of R can be trivially captured by a resource-based approach using WordNet. Hence, they are omitted from our evaluation. 5.2 A New Proper-Name Dataset An important linguistic component that is missing from these </context>
<context position="24339" citStr="Turney and Mohammad, 2015" startWordPosition="3882" endWordPosition="3886">e of the learnt edge types (see the supplementary material for the complete list). The performance boost in proper2015 demonstrates that community-built resources have much added value when considering propernames. As expected, many proper-names do not appear in WordNet (Doctor Who). That said, even when both terms appear in WordNet, they often lack important properties covered by other resources (Louisa May Alcott is a woman). 6.3 Comparison to Corpus-based Methods Lexical inference has been thoroughly explored in distributional semantics, with recent supervised methods (Baroni et al., 2012; Turney and Mohammad, 2015) showing promising results. While Table 5: An excerpt of the whitelist learnt for proper2015 by the binary model with accompanying true-positives that do not have an indicative path in WordNet. these methods leverage huge corpora to increase coverage, they often introduce noise that affects their precision. Structured resources, on the other hand, are precision-oriented. We therefore expect our approach to complement distributional methods in high-precision scenarios. To represent term-pairs with distributional features, we downloaded the pre-trained word2vec embeddings.9 These vectors were tr</context>
</contexts>
<marker>Turney, Mohammad, 2015</marker>
<rawString>Peter D Turney and Saif M Mohammad. 2015. Experiments with three approaches to recognizing lexical entailment. Natural Language Engineering, 21(03):437–476.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Similarity of semantic relations.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>3</issue>
<contexts>
<context position="1838" citStr="Turney, 2006" startWordPosition="256" endWordPosition="257">fer the meaning of one word from another, in order to address lexical variability. For instance, a question answering system asked “which artist’s net worth is $450 million?” might retrieve the candidates Beyonc´e Knowles and Lloyd Blankfein, who are both worth $450 million. To correctly answer the question, the application needs to know that Beyonc´e is an artist, and that Lloyd Blankfein is not. 1Our code and data are available at: https://github.com/vered1986/LinKeR Corpus-based methods are often employed to recognize lexical inferences, based on either cooccurrence patterns (Hearst, 1992; Turney, 2006) or distributional representations (Weeds and Weir, 2003; Kotlerman et al., 2010). While earlier methods were mostly unsupervised, recent trends introduced supervised methods for the task (Baroni et al., 2012; Turney and Mohammad, 2015; Roller et al., 2014). In these settings, a targeted lexical inference relation is implicitly defined by a training set of term-pairs, which are annotated as positive or negative examples of this relation. Several such datasets have been created, each representing a somewhat different flavor of lexical inference. While corpus-based methods usually enjoy high rec</context>
</contexts>
<marker>Turney, 2006</marker>
<rawString>Peter D Turney. 2006. Similarity of semantic relations. Computational Linguistics, 32(3):379–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christina Unger</author>
<author>Lorenz B¨uhmann</author>
<author>Jens Lehmann</author>
<author>Axel-Cyrille Ngonga Ngomo</author>
<author>Daniel Gerber</author>
<author>Philipp Cimiano</author>
</authors>
<title>Template-based question answering over rdf data.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st international conference on World Wide Web,</booktitle>
<pages>639--648</pages>
<publisher>ACM.</publisher>
<marker>Unger, B¨uhmann, Lehmann, Ngomo, Gerber, Cimiano, 2012</marker>
<rawString>Christina Unger, Lorenz B¨uhmann, Jens Lehmann, Axel-Cyrille Ngonga Ngomo, Daniel Gerber, and Philipp Cimiano. 2012. Template-based question answering over rdf data. In Proceedings of the 21st international conference on World Wide Web, pages 639–648. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Denny Vrandeˇci´c</author>
</authors>
<title>Wikidata: A new platform for collaborative data collection.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st international conference companion on World Wide Web,</booktitle>
<pages>1063--1064</pages>
<publisher>ACM.</publisher>
<marker>Vrandeˇci´c, 2012</marker>
<rawString>Denny Vrandeˇci´c. 2012. Wikidata: A new platform for collaborative data collection. In Proceedings of the 21st international conference companion on World Wide Web, pages 1063–1064. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
</authors>
<title>A general framework for distributional similarity.</title>
<date>2003</date>
<booktitle>Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>81--88</pages>
<editor>In Michael Collins and Mark Steedman, editors,</editor>
<contexts>
<context position="1894" citStr="Weeds and Weir, 2003" startWordPosition="261" endWordPosition="264">r to address lexical variability. For instance, a question answering system asked “which artist’s net worth is $450 million?” might retrieve the candidates Beyonc´e Knowles and Lloyd Blankfein, who are both worth $450 million. To correctly answer the question, the application needs to know that Beyonc´e is an artist, and that Lloyd Blankfein is not. 1Our code and data are available at: https://github.com/vered1986/LinKeR Corpus-based methods are often employed to recognize lexical inferences, based on either cooccurrence patterns (Hearst, 1992; Turney, 2006) or distributional representations (Weeds and Weir, 2003; Kotlerman et al., 2010). While earlier methods were mostly unsupervised, recent trends introduced supervised methods for the task (Baroni et al., 2012; Turney and Mohammad, 2015; Roller et al., 2014). In these settings, a targeted lexical inference relation is implicitly defined by a training set of term-pairs, which are annotated as positive or negative examples of this relation. Several such datasets have been created, each representing a somewhat different flavor of lexical inference. While corpus-based methods usually enjoy high recall, their precision is often limited, hindering their a</context>
</contexts>
<marker>Weeds, Weir, 2003</marker>
<rawString>Julie Weeds and David Weir. 2003. A general framework for distributional similarity. In Michael Collins and Mark Steedman, editors, Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>Daoud Clarke</author>
<author>Jeremy Reffin</author>
<author>David Weir</author>
<author>Bill Keller</author>
</authors>
<title>Learning to distinguish hypernyms and co-hyponyms.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,</booktitle>
<pages>2249--2259</pages>
<institution>Dublin City University and Association for Computational Linguistics.</institution>
<location>Dublin, Ireland,</location>
<contexts>
<context position="25589" citStr="Weeds et al., 2014" startWordPosition="4076" endWordPosition="4079">illion words) using a stateof-the-art embedding algorithm (Mikolov et al., 2013). Since each vector represents a single term (either x or y), we used 3 state-of-the-art meth9http://code.google.com/p/word2vec/ Edge Type occupation sex or gender instance of acted in genre Example Daniel Radcliffe → actor Louisa May Alcott → woman Doctor Who → series Michael Keaton → Beetlejuice Touch → drama position played on team Jason Collins → center 181 ods to construct a feature vector for each termpair: concatenation x� ® y� (Baroni et al., 2012), difference y� − x� (Roller et al., 2014; Fu et al., 2014; Weeds et al., 2014), and similarity x� · y. We then used Fβ-optimized logistic regression to train a classifier. Figure 5 compares our methods to concatenation, which was the best-performing corpus-based method.10 In turney2014 and proper2015, the embeddings retain over 80% precision while boasting higher recall than our method’s. In turney2014, it is often a result of the more associative relations prominent in the dataset (football —* playbook), which seldom are expressed in structured resources. In proper2015, the difference in recall seems to be from missing terminology (Twitter —* social network). However, </context>
</contexts>
<marker>Weeds, Clarke, Reffin, Weir, Keller, 2014</marker>
<rawString>Julie Weeds, Daoud Clarke, Jeremy Reffin, David Weir, and Bill Keller. 2014. Learning to distinguish hypernyms and co-hyponyms. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 2249–2259, Dublin, Ireland, August. Dublin City University and Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Daniel S Weld</author>
</authors>
<title>Open information extraction using wikipedia.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>118--127</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="6735" citStr="Wu and Weld, 2010" startWordPosition="1003" endWordPosition="1006">set of relevant relations, and a suitable subset is usually tailored to each dataset and task. This work addresses this issue by automatically learning the subset of relations relevant to the task. 2.2 Structured Knowledge Resources While WordNet is quite extensive, it is handcrafted by expert lexicographers, and thus cannot compete in terms of scale with community-built knowledge bases such as Wikidata (Vrandeˇci´c, 2012), which connect millions of entities through a rich variety of structured relations (properties). Using these resources for various NLP tasks has become exceedingly popular (Wu and Weld, 2010; Rahman and Ng, 2011; Unger et al., 2012; Berant et al., 2013). Little attention, however, was given to leveraging them for identifying lexical inference; the exception being Shnarch et al. (2009), who used structured data from Wikipedia for this purpose. In this paper, we experimented with such resources, in addition to WordNet. DBPedia (Auer et al., 2007) contains structured information from Wikipedia: info boxes, redirections, disambiguation links, etc. Wikidata (Vrandeˇci´c, 2012) contains facts edited by humans to support Wikipedia and other Wikimedia projects. Yago (Suchanek et al., 200</context>
</contexts>
<marker>Wu, Weld, 2010</marker>
<rawString>Fei Wu and Daniel S. Weld. 2010. Open information extraction using wikipedia. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 118–127, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jihoon Yang</author>
<author>Vasant Honavar</author>
</authors>
<title>Feature subset selection using a genetic algorithm. In Feature extraction, construction and selection,</title>
<date>1998</date>
<pages>117--136</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="16278" citStr="Yang and Honavar, 1998" startWordPosition="2593" endWordPosition="2596">.e. w∗ = arg maxw u(w). In our case, the utility u is the Fβ score over the training set. Structured knowledge resources contain hundreds of different edge types, making E very large, and an exhaustive search over its powerset infeasible. The standard approach to this class of subset selection problems is to apply local search algorithms, which find an approximation of the optimal subset. We tried several local search algorithms, and found that genetic search (Russell and Norvig, 2009, Ch.4) performed well. In general, genetic search is claimed to be a preferred strategy for subset selection (Yang and Honavar, 1998). In our application of genetic search, each individual (candidate solution) is a whitelist, represented by a bit vector with a bit for each edge type. We defined the fitness function of a whitelist w according to the Fβ score of w over the training set. 6As a corollary, if x✚Ry, then every path between them is non-indicative. supervision p1 e1 x → y p2 parameters e2 p3 e3 4F = (1+02) ·precision ·recall 0 02·precision+recall 178 Dataset #Instances #Positive #Negative kotlerman2010 2,940 880 2,060 turney2014 1,692 920 772 levy2014 12,602 945 11,657 proper2015 1,500 750 750 Table 3: Datasets eva</context>
</contexts>
<marker>Yang, Honavar, 1998</marker>
<rawString>Jihoon Yang and Vasant Honavar. 1998. Feature subset selection using a genetic algorithm. In Feature extraction, construction and selection, pages 117–136. Springer.</rawString>
</citation>
<citation valid="false">
<title>Algorithm 1 Find Relevant Nodes 1: function NODESINPATH(nx, ny, len) 2: if len == 1 then 3: return nx U ny 4: for 0 &lt; k &lt; len do 5: if k is odd then 6: nx = nx A 7: else 8: ny = ny AT 9: if nx ny &gt; 0 then 10: nxy = nx n ny 11: nforward = nodesInPath(nx,nxy, [k]) 12: nbackward = nodesInPath(nxy, ny, Lk2 j) 13: return 7iforward U nbackward 14: return 0�</title>
<marker></marker>
<rawString>Algorithm 1 Find Relevant Nodes 1: function NODESINPATH(nx, ny, len) 2: if len == 1 then 3: return nx U ny 4: for 0 &lt; k &lt; len do 5: if k is odd then 6: nx = nx A 7: else 8: ny = ny AT 9: if nx ny &gt; 0 then 10: nxy = nx n ny 11: nforward = nodesInPath(nx,nxy, [k]) 12: nbackward = nodesInPath(nxy, ny, Lk2 j) 13: return 7iforward U nbackward 14: return 0�</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>