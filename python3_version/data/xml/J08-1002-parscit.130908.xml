<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9974635">
Feature Forest Models for Probabilistic
HPSG Parsing
</title>
<author confidence="0.999165">
Yusuke Miyao*
</author>
<affiliation confidence="0.996986">
University of Tokyo
</affiliation>
<author confidence="0.98305">
Jun’ichi Tsujii**
</author>
<affiliation confidence="0.999315">
University of Tokyo
University of Manchester
</affiliation>
<bodyText confidence="0.955518619047619">
Probabilistic modeling of lexicalized grammars is difficult because these grammars exploit com-
plicated data structures, such as typed feature structures. This prevents us from applying
common methods of probabilistic modeling in which a complete structure is divided into sub-
structures under the assumption of statistical independence among sub-structures. For example,
part-of-speech tagging of a sentence is decomposed into tagging of each word, and CFG parsing
is split into applications of CFG rules. These methods have relied on the structure of the target
problem, namely lattices or trees, and cannot be applied to graph structures including typed fea-
ture structures.
This article proposes the feature forest model as a solution to the problem of probabilistic
modeling of complex data structures including typed feature structures. The feature forest model
provides a method for probabilistic modeling without the independence assumption when prob-
abilistic events are represented with feature forests. Feature forests are generic data structures
that represent ambiguous trees in a packed forest structure. Feature forest models are maximum
entropy models defined over feature forests. A dynamic programming algorithm is proposed for
maximum entropy estimation without unpacking feature forests. Thus probabilistic modeling of
any data structures is possible when they are represented by feature forests.
This article also describes methods for representing HPSG syntactic structures and
predicate–argument structures with feature forests. Hence, we describe a complete strategy for
developing probabilistic models for HPSG parsing. The effectiveness of the proposed methods is
empirically evaluated through parsing experiments on the Penn Treebank, and the promise of
applicability to parsing of real-world sentences is discussed.
</bodyText>
<sectionHeader confidence="0.996732" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.9961205">
Following the successful development of wide-coverage lexicalized grammars (Riezler
et al. 2000; Hockenmaier and Steedman 2002; Burke et al. 2004; Miyao, Ninomiya, and
</bodyText>
<affiliation confidence="0.64944">
* Department of Computer Science, University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Japan.
</affiliation>
<email confidence="0.896707">
E-mail: yusuke@is.s.u-tokyo.ac.jp.
</email>
<affiliation confidence="0.738679">
** Department of Computer Science, University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Japan.
</affiliation>
<email confidence="0.970901">
E-mail: tsujii@is.s.u-tokyo.ac.jp.
</email>
<note confidence="0.93570675">
Submission received: 11 June 2006; revised submission received: 2 March 2007; accepted for publication:
5 May 2007.
© 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 1
</note>
<bodyText confidence="0.999810933333334">
Tsujii 2005), statistical modeling of these grammars is attracting considerable attention.
This is because natural language processing applications usually require disambiguated
or ranked parse results, and statistical modeling of syntactic/semantic preference is one
of the most promising methods for disambiguation.
The focus of this article is the problem of probabilistic modeling of wide-coverage
HPSG parsing. Although previous studies have proposed maximum entropy mod-
els (Berger, Della Pietra, and Della Pietra 1996) of HPSG-style parse trees (Oepen,
Toutanova, et al. 2002b; Toutanova and Manning 2002; Baldridge and Osborne 2003;
Malouf and van Noord 2004), the straightforward application of maximum entropy
models to wide-coverage HPSG parsing is infeasible because estimation of maximum
entropy models is computationally expensive, especially when targeting wide-coverage
parsing. In general, complete structures, such as transition sequences in Markov models
and parse trees, have an exponential number of ambiguities. This causes an exponential
explosion when estimating the parameters of maximum entropy models. We therefore
require solutions to make model estimation tractable.
This article first proposes feature forest models, which are a general solution to
the problem of maximum entropy modeling of tree structures (Miyao and Tsujii 2002).
Our algorithm avoids exponential explosion by representing probabilistic events with
feature forests, which are packed representations of tree structures. When complete
structures are represented with feature forests of a tractable size, the parameters of
maximum entropy models are efficiently estimated without unpacking the feature
forests. This is due to dynamic programming similar to the algorithm for computing
inside/outside probabilities in PCFG parsing.
The latter half of this article (Section 4) is on the application of feature forest
models to disambiguation in wide-coverage HPSG parsing. We describe methods for
representing HPSG parse trees and predicate–argument structures using feature forests
(Miyao, Ninomiya, and Tsujii 2003; Miyao and Tsujii 2003, 2005). Together with the
parameter estimation algorithm for feature forest models, these methods constitute a
complete procedure for the probabilistic modeling of wide-coverage HPSG parsing.
The methods we propose here were applied to an English HPSG parser, Enju (Tsujii
Laboratory 2004). We report on an extensive evaluation of the parser through parsing
experiments on the Wall Street Journal portion of the Penn Treebank (Marcus et al. 1994).
The content of this article is an extended version of our earlier work reported
in Miyao and Tsujii (2002, 2003, 2005) and Miyao, Ninomiya, and Tsujii (2003). The
major contribution of this article is a strict mathematical definition of the feature forest
model and the parameter estimation algorithm, which are substantially refined and
extended from Miyao and Tsujii (2002). Another contribution is that this article thor-
oughly discusses the relationships between the feature forest model and its application
to HPSG parsing. We also provide an extensive empirical evaluation of the resulting
HPSG parsing approach using real-world text.
Section 2 discusses a problem of conventional probabilistic models for lexicalized
grammars. Section 3 proposes feature forest models for solving this problem. Section 4
describes the application of feature forest models to probabilistic HPSG parsing. Sec-
tion 5 presents an empirical evaluation of probabilistic HPSG parsing, and Section 6
introduces research related to our proposals. Section 7 concludes.
</bodyText>
<sectionHeader confidence="0.984381" genericHeader="keywords">
2. Problem
</sectionHeader>
<bodyText confidence="0.981787">
Maximum entropy models (Berger, Della Pietra, and Della Pietra 1996) are now be-
coming the de facto standard approach for disambiguation models for lexicalized or
</bodyText>
<page confidence="0.995371">
36
</page>
<note confidence="0.940924">
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
</note>
<bodyText confidence="0.9631184375">
feature structure grammars (Johnson et al. 1999; Riezler et al. 2000, 2002; Geman and
Johnson 2002; Clark and Curran 2003, 2004b; Kaplan et al. 2004; Carroll and Oepen
2005). Previous studies on probabilistic models for HPSG (Oepen, Toutanova et al. 2002;
Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van Noord
2004) have also adopted log-linear models. This is because these grammar formalisms
exploit feature structures to represent linguistic constraints. Such constraints are known
to introduce inconsistencies in probabilistic models estimated using simple relative
frequency, as discussed in Abney (1997). The maximum entropy model is a reasonable
choice for credible probabilistic models. It also allows various overlapping features to
be incorporated, and we can expect higher accuracy in disambiguation.
A maximum entropy model gives a probabilistic distribution that maximizes the
likelihood of training data under given feature functions. Given training data E =
{(x, y)}, a maximum entropy model gives conditional probability p(y|x) as follows.
Definition 1(Maximum entropy model)
A maximum entropy model is defined as the solution of the following optimization
problem.
</bodyText>
<equation confidence="0.99692425">
pM(y|x) = argmax { E� ˜p(x,y) logp(y|x) }
p (x,y)EE
where:
p(y |x) = Z1x) exp E λifi(x, y)
i
E �� �
Z(x) = exp λifi(x, y)
yEY(x) i
</equation>
<bodyText confidence="0.999951947368421">
In this definition, ˜p(x, y) is the relative frequency of (x, y) in the training data. fi is a
feature function, which represents a characteristic of probabilistic events by mapping
an event into a real value. λi is the model parameter of a corresponding feature function
fi, and is determined so as to maximize the likelihood of the training data (i.e., the
optimization in this definition). Y(x) is a set of y for given x; for example, in parsing, x is
a given sentence and Y(x) is a parse forest for x. An advantage of maximum entropy
models is that feature functions can represent any characteristics of events. That is,
independence assumptions are unnecessary for the design of feature functions. Hence,
this method provides a principled solution for the estimation of consistent probabilistic
distributions over feature structure grammars.
The remaining issue is how to estimate parameters. Several numerical algorithms,
such as Generalized Iterative Scaling (GIS) (Darroch and Ratcliff 1972), Improved
Iterative Scaling (IIS) (Della Pietra, Della Pietra, and Lafferty 1997), and the Limited-
memory Broyden-Fletcher-Goldfarb-Shanno method (L-BFGS) (Nocedal and Wright
1999), have been proposed for parameter estimation. Although the algorithm proposed
in the present article is applicable to all of the above algorithms, we used L-BFGS for
experiments.
However, a computational problem arises in these parameter estimation algo-
rithms. The size of Y(x) (i.e., the number of parse trees for a sentence) is generally
</bodyText>
<page confidence="0.998274">
37
</page>
<note confidence="0.800597">
Computational Linguistics Volume 34, Number 1
</note>
<bodyText confidence="0.99966275">
very large. This is because local ambiguities in parse trees potentially cause exponential
growth in the number of structures assigned to sub-sequences of words, resulting in
billions of structures for whole sentences. For example, when we apply rewriting rule
S → NP VP, and the left NP and the right VP, respectively, have n and m ambiguous
subtrees, the result of the rule application generates n × m trees.
This is problematic because the complexity of parameter estimation is proportional
to the size of Y(x). The cost of the parameter estimation algorithms is bound by the
computation of model expectation, µi, given as (Malouf 2002):
</bodyText>
<equation confidence="0.992264333333333">
Eµi = E fi(x, y)p(y|x)
xEX ˜p(x)
yEY(x)
XE E � �
˜p(x) fi(x,y)Z1 exp Eλjfj(x,y) (1)
yEY(x) j
</equation>
<bodyText confidence="0.999898363636364">
As shown in this definition, the computation of model expectation requires the summa-
tion over Y(x) for every x in the training data. The complexity of the overall estimation
algorithm is O( ˜|Y |˜|F||E|), where ˜|Y |and ˜|F |are the average numbers of y and activated
features for an event, respectively, and |E |is the number of events. When Y(x) grows
exponentially, the parameter estimation becomes intractable.
In PCFGs, the problem of computing probabilities of parse trees is avoided by using
a dynamic programming algorithm for computing inside/outside probabilities (Baker
1979). With the algorithm, the computation becomes tractable. We can expect that the
same approach would be effective for maximum entropy models as well.
This notion yields a novel algorithm for parameter estimation for maximum en-
tropy models, as described in the next section.
</bodyText>
<sectionHeader confidence="0.997971" genericHeader="introduction">
3. Feature Forest Model
</sectionHeader>
<bodyText confidence="0.996772526315789">
Our solution to the problem is a dynamic programming algorithm for computing
inside/outside α-products. Inside/outside α-products roughly correspond to inside/
outside probabilities in PCFGs. In maximum entropy models, a probability is defined
as a normalized product of αfj j(= exp(λjfj)). Hence, similar to the algorithm of computing
(� )
inside/outside probabilities, we can compute exp j λjfj , which we define as the
α-product, for each node in a tree structure. If we can compute α-products at a tractable
cost, the model expectation µi is also computed at a tractable cost.
We first define the notion of a feature forest, a packed representation of a set
of an exponential number of tree structures. Feature forests correspond to packed
charts in CFG parsing. Because feature forests are generalized representations of forest
structures, the notion is not only applicable to syntactic parsing but also to sequence
tagging, such as POS tagging and named entity recognition (which will be discussed in
Section 6). We then define inside/outside α-products that represent the α-products of
partial structures of a feature forest. Inside α-products correspond to inside probabilities
in PCFG, and represent the summation of α-products of the daughter sub-trees. Outside
α-products correspond to outside probabilities in PCFG, and represent the summation
of α-products in the upper part of the feature forest. Both can be computed incre-
mentally by a dynamic programming algorithm similar to the algorithm for computing
</bodyText>
<page confidence="0.996113">
38
</page>
<note confidence="0.941393">
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
</note>
<bodyText confidence="0.997185333333333">
inside/outside probabilities in PCFG. Given inside/outside o -products of all nodes in
a feature forest, the model expectation µi is easily computed by multiplying them for
each node.
</bodyText>
<subsectionHeader confidence="0.984552">
3.1 Feature Forest
</subsectionHeader>
<bodyText confidence="0.999786666666667">
To describe the algorithm, we first define the notion of a feature forest, the generalized
representation of features in a packed forest structure. Feature forests are used for
enumerating possible structures of events, that is, they correspond to Y(x) in Equation 1.
</bodyText>
<subsectionHeader confidence="0.419743">
Definition 2 (Feature forest)
</subsectionHeader>
<bodyText confidence="0.926839">
A feature forest Φ is a tuple (C, D, r, -y, b), where:
</bodyText>
<listItem confidence="0.9997982">
• C is a set of conjunctive nodes,
• D is a set of disjunctive nodes,
• r is the root node: r E C,
• -y : D H 2C is a conjunctive daughter function,
• b : C H 2D is a disjunctive daughter function.
</listItem>
<bodyText confidence="0.9995215">
We denote a feature forest for x as Φ(x). For example, Φ(x) can represent the set of all
possible tag sequences of a given sentence x, or the set of all parse trees of x. A feature
forest is an acyclic graph, and unpacked structures extracted from a feature forest are
trees. We also assume that terminal nodes of feature forests are conjunctive nodes. That
is, disjunctive nodes must have daughters (i.e., -y(d) =� 0 for all d E D).
A feature forest represents a set of trees of conjunctive nodes in a packed structure.
Conjunctive nodes correspond to entities such as states in Markov chains and nodes
in CFG trees. Feature functions are assigned to conjunctive nodes and express their
characteristics. Disjunctive nodes are for enumerating alternative choices. Conjunctive/
disjunctive daughter functions represent immediate relations of conjunctive and dis-
junctive nodes. By selecting a conjunctive node as a child of each disjunctive node, we
can extract a tree consisting of conjunctive nodes from a feature forest.
</bodyText>
<figureCaption confidence="0.724274333333333">
Figure 1 shows an example of a feature forest. Each disjunctive node enumerates
alternative nodes, which are conjunctive nodes. Each conjunctive node has disjunctive
Figure 1
</figureCaption>
<bodyText confidence="0.49468">
A feature forest.
</bodyText>
<page confidence="0.986251">
39
</page>
<note confidence="0.473617">
Computational Linguistics Volume 34, Number 1
</note>
<figureCaption confidence="0.741655">
Figure 2
Unpacked trees.
</figureCaption>
<bodyText confidence="0.9982782">
nodes as its daughters. The feature forest in Figure 1 represents a set of 2 x 2 x 2 = 8
unpacked trees shown in Figure 2. For example, by selecting the left-most conjunctive
node at each disjunctive node, we extract an unpacked tree (c1, c2, c4, c6). An unpacked
tree is represented as a set of conjunctive nodes. Generally, a feature forest represents
an exponential number of trees with a polynomial number of nodes. Thus, complete
structures, such as tag sequences and parse trees with ambiguities, can be represented
in a tractable form.
Feature functions are defined over conjunctive nodes.1
Definition 3 (Feature function for feature forests)
A feature function for a feature forest is:
</bodyText>
<equation confidence="0.795428">
fi : C H R
</equation>
<bodyText confidence="0.999869421052632">
Hence, together with feature functions, a feature forest represents a set of trees of
features.
Feature forests may be regarded as a packed chart in CFG parsing. Although feature
forests have the same structure as PCFG parse forests, nodes in feature forests do not
necessarily correspond to nodes in PCFG parse forests. In fact, in Sections 4.2 and 4.3, we
will demonstrate that syntactic structures and predicate–argument structures in HPSG
can be represented with tractable-size feature forests. The actual interpretation of a node
in a feature forest may thus be ignored in the following discussion. Our algorithm is
applicable whenever feature forests are of a tractable size. The descriptive power of
feature forests will be discussed again in Section 6.
As mentioned, a feature forest is a packed representation of trees of features. We first
define model expectations, µi, on a set of unpacked trees, and then show that they can
be computed without unpacking feature forests. We denote an unpacked tree as a set,
c ⊆ C, of conjunctive nodes. Our concern is only the set of features associated with each
conjunctive node, and the shape of the tree structure is irrelevant to the computation of
probabilities of unpacked trees. Hence, we do not distinguish an unpacked tree from a
set of conjunctive nodes.
The collection of unpacked trees represented by a feature forest is defined as a multi-
set of unpacked trees because we allow multiple occurrences of equivalent unpacked
</bodyText>
<footnote confidence="0.7083045">
1 Feature functions may also be conditioned on x. In this case, feature functions can be written as fi(c, x).
For simplicity, we omit x in the following discussion.
</footnote>
<page confidence="0.993723">
40
</page>
<note confidence="0.938974">
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
</note>
<bodyText confidence="0.953344">
trees in a feature forest.2 Given multisets of unpacked trees, A, B, we define the union
and the product as follows.
</bodyText>
<equation confidence="0.3614055">
A®B-AUB
A ® B - {a U b|a E A,b E B}
</equation>
<bodyText confidence="0.832464">
Intuitively, the first operation is a collection of trees, and the second lists all combina-
tions of trees in A and B. It is trivial that they satisfy commutative, associative, and
distributive laws.
</bodyText>
<figure confidence="0.8313846">
A®B=B®A
A®B=B®A
A®(B®C)=(A®B)®C
A®(B®C)=(A®B)®C
A®(B®C)=(A®B)®(A®C)
</figure>
<bodyText confidence="0.9997394">
We denote a set of unpacked trees rooted at node n E C U D as Ω(n). Ω(n) is de-
fined recursively. For a terminal node c E C, obviously Ω(c) = {{c}}. For an internal
conjunctive node c E C, an unpacked tree is a combination of trees, each of which is
selected from a disjunctive daughter. Hence, a set of all unpacked trees is represented
as a product of trees from disjunctive daughters.
</bodyText>
<equation confidence="0.9941155">
Ω(c) = {{c}} ® � Ω(d)
d∈δ(c)
</equation>
<bodyText confidence="0.982346">
A disjunctive node d E D represents alternatives of packed trees, and obviously a set
of its unpacked trees is represented as a union of the daughter trees, that is, Ω(d) =
</bodyText>
<equation confidence="0.8858705">
®
c∈γ(d) Ω(c).
</equation>
<bodyText confidence="0.64231">
To summarize, a set of unpacked trees is defined formally as follows.
</bodyText>
<subsectionHeader confidence="0.415487">
Definition 4 (Unpacked tree)
</subsectionHeader>
<bodyText confidence="0.920066">
Given a feature forest Φ = (C, D, r,γ, 6), a set Ω(n) of unpacked trees rooted at node
n E C U D is defined recursively as follows.
</bodyText>
<listItem confidence="0.999919666666667">
• If n E C is a terminal, that is, 6(n) = 0,
Ω(n) - {{n}}
• IfnEC,
</listItem>
<equation confidence="0.9774405">
Ω(n) - {{n}} ® � Ω(d)
d∈δ(n)
</equation>
<footnote confidence="0.996591666666667">
2 In fact, no feature forests include equivalent unpacked trees if no disjunctive nodes have identical
daughter nodes. Thus we may define a set of unpacked trees as an ordinary set, although the details
are omitted here for simplicity.
</footnote>
<page confidence="0.994099">
41
</page>
<figure confidence="0.329255">
Computational Linguistics Volume 34, Number 1
</figure>
<listItem confidence="0.933123">
• If n E D,
</listItem>
<equation confidence="0.9963175">
Ω(n) ≡ � Ω(c)
c∈γ(n)
</equation>
<bodyText confidence="0.967634636363636">
Feature forests are directed acyclic graphs and, as such, this definition does not include
a loop. Hence, Ω(n) is properly defined.
A set of all unpacked trees is then represented by Ω(r); henceforth, we denote Ω(r)
as Ω(Φ), or just Ω when it is not confusing in context. Figure 3 shows Ω(Φ) of the feature
forest in Figure 1. Following Definition 4, the first element of each set is the root node,
c1, and the rest are elements of the product of {c2, c3}, {c4, c5}, and {c6, c7}. Each set in
Figure 3 corresponds to a tree in Figure 2.
Given this formalization, the feature function for an unpacked tree is defined as
follows.
Definition 5 (Feature function for unpacked tree)
The feature function fi for an unpacked tree, c E Ω(Φ) is defined as:
</bodyText>
<equation confidence="0.987796333333333">
�fi(c) =
fi(c)
c∈c
</equation>
<bodyText confidence="0.910365">
Because c E Ω(Φ) corresponds to y of the conventional maximum entropy model, this
function substitutes for fi(x,y) in the conventional model. Once a feature function for
an unpacked tree is given, a model expectation is defined as in the traditional model.
Definition 6 (Model expectation of feature forests)
The model expectation µi for a set of feature forests {Φ(x)} is defined as:
</bodyText>
<equation confidence="0.9584419">
�µi = � fi(c)p(c|x)
x∈X ˜p(x)
c∈Ω(Φ(x))
�= �  
x∈X ˜p(x) fi (c)Z1x) exp �λjfj(c)
c∈Ω(Φ(x)) j

� �
where Z(x) = exp λjfj(c)
c∈Ω(Φ(x)) j
</equation>
<bodyText confidence="0.815758166666667">
It is evident that the naive computation of model expectations requires exponential
time complexity because the number of unpacked trees (i.e., |Ω(Φ)|) is exponentially
related to the number of nodes in the feature forest Φ. We therefore need an algorithm
for computing model expectations without unpacking a feature forest.
Figure 3
Unpacked trees represented as sets of conjunctive nodes.
</bodyText>
<page confidence="0.996718">
42
</page>
<note confidence="0.678595">
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
</note>
<figureCaption confidence="0.969196">
Figure 4
</figureCaption>
<bodyText confidence="0.75789">
Inside/outside at node c2 in a feature forest.
</bodyText>
<subsectionHeader confidence="0.996483">
3.2 Dynamic Programming
</subsectionHeader>
<bodyText confidence="0.99989525">
To efficiently compute model expectations, we incorporate an approach similar to the
dynamic programming algorithm for computing inside/outside probabilities in PCFGs.
We first define the notion of inside/outside of a feature forest. Figure 4 illustrates this
concept, which is similar to the analogous concept in PCFGs.3 Inside denotes a set of
partial trees (sets of conjunctive nodes) derived from node c2. Outside denotes a set of
partial trees that derive node c2. That is, outside trees are partial trees of complements
of inside trees.
We denote a set of inside trees at node n as ι(n), and that of outside trees as o(n).
</bodyText>
<subsectionHeader confidence="0.662344">
Definition 7 (Inside trees)
</subsectionHeader>
<bodyText confidence="0.9992735">
We define a set ι(n) of inside trees rooted at node n ∈ C ∪ D as a set of unpacked trees
rooted at n.
</bodyText>
<equation confidence="0.782397">
ι(n) ≡ Ω(n)
Definition 8 (Outside trees)
</equation>
<bodyText confidence="0.918487">
We define a set o(n) of outside trees rooted at node n ∈ C ∪ D as follows.
</bodyText>
<equation confidence="0.992137833333333">
o(r) ≡ {∅}
�o(c) ≡ o(d)
dEγ−1(c)
�
o(d) ≡
cEδ−1(d)
�
{{c}} ⊗ o(c) ⊗
d1Eδ(c),d&apos;#d
{
}
ι(d�)
</equation>
<footnote confidence="0.705091">
3 A node may have multiple outside trees in general as in the case of CFGs, although Figure 4 shows only
one outside tree of c2 for simplicity.
</footnote>
<page confidence="0.99917">
43
</page>
<note confidence="0.293529">
Computational Linguistics Volume 34, Number 1
</note>
<bodyText confidence="0.9338765">
In the definition, γ−1 and δ−1 denote mothers of conjunctive and disjunctive nodes,
respectively. Formally,
</bodyText>
<equation confidence="0.988931115384615">
γ−1(c) - {dJc E γ(d)}
δ−1(d) - {cJd E δ(c)}
Next, inside/outside α-products are defined for conjunctive and disjunctive nodes.
�� �
The inside (or outside) α-products are the summation of exp j λj fj(c) of all inside (or
outside) trees c.
Definition 9 (Inside/outside α-product)
An inside α-product at conjunctive node c E C is
�ϕc = � �
cEι(c) ��
exp λjfj(c) �
j
An outside α-product is
�ψc = � �
cEo(c) ��
exp λjfj(c) �
j
Similarly, inside/outside α-products at disjunctive node d E D are defined as follows:
� � �
ϕd = ��
cEι(d) exp λjfj(c) �
j
� � �
ψd = ��
cEo(d) exp λjfj(c) �
j
</equation>
<bodyText confidence="0.9145408">
We can derive that the model expectations of a feature forest are computed as the
product of the inside and outside α-products.
Theorem 1(Model expectation of feature forests)
The model expectation µi of a feature forest Φ(x) = (Cx, Dx, rx, γx, δx) is computed as the
product of inside and outside α-products as follows:
</bodyText>
<equation confidence="0.814664666666667">
�µi = � fi(c)ϕcψc
xEX ˜p(x) 1
Z(x)cECx
</equation>
<bodyText confidence="0.875211">
where Z(x) = ϕrx
</bodyText>
<page confidence="0.998154">
44
</page>
<note confidence="0.592498">
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
</note>
<figureCaption confidence="0.703616666666667">
Figure 5
Incremental computation of inside α-products at conjunctive node c2.
Figure 6
</figureCaption>
<subsectionHeader confidence="0.660058">
Incremental computation of inside α-products at disjunctive node d4.
</subsectionHeader>
<bodyText confidence="0.9997436">
This equation shows a method for efficiently computing model expectations by
traversing conjunctive nodes without unpacking the forest, if the inside/outside
α-products are given. The remaining issue is how to efficiently compute inside/outside
α-products.
Fortunately, inside/outside α-products can be incrementally computed by dynamic
programming without unpacking feature forests. Figure 5 shows the process of com-
puting the inside α-product at a conjunctive node from the inside α-products of its
daughter nodes. Because the inside of a conjunctive node is a set of the combinations of
all of its descendants, the α-product is computed by multiplying the α-products of the
daughter trees. The following equation is derived.
</bodyText>
<equation confidence="0.9670105">
ϕc = ri ϕd�exp E λjfj (c)
(d∈δ(c) j
</equation>
<bodyText confidence="0.9949855">
The inside of a disjunctive node is the collection of the inside trees of its daughter nodes.
Hence, the inside α-product at disjunctive node d ∈ D is computed as follows (Figure 6).
</bodyText>
<equation confidence="0.9804775">
ϕd = E ϕc
c∈γ(d)
</equation>
<page confidence="0.99029">
45
</page>
<figure confidence="0.443651">
Computational Linguistics Volume 34, Number 1
Theorem 2 (Inside α-product)
</figure>
<bodyText confidence="0.6742325">
The inside α-product ϕc at a conjunctive node c is computed by the following equation
if ϕd is given for all daughter disjunctive nodes d ∈ δ(c).
</bodyText>
<equation confidence="0.921959333333333">
ϕc = ri ϕdexp E λjfj (c)
(dEδ(c) j
The inside α-product ϕd at a disjunctive node d is computed by the following equation
if ϕc is given for all daughter conjunctive nodes c ∈ γ(d).
ϕd = � ϕc
cEγ(d)
</equation>
<bodyText confidence="0.999770666666667">
The outside of a disjunctive node is equivalent to the outside of its daughter nodes.
Hence, the outside α-product of a disjunctive node is propagated to its daughter con-
junctive nodes (Figure 7).
</bodyText>
<equation confidence="0.9836225">
�ψc = ψd
{dJcEγ(d)}
</equation>
<bodyText confidence="0.988792">
The computation of the outside α-product of a disjunctive node is somewhat com-
plicated. As shown in Figure 8, the outside trees of a disjunctive node are all com-
binations of
</bodyText>
<listItem confidence="0.999704">
• the outside trees of the mother nodes, and
• the inside trees of the sister nodes.
</listItem>
<figureCaption confidence="0.991089">
Figure 7
</figureCaption>
<bodyText confidence="0.567037">
Incremental computation of outside α-products at conjunctive node c2.
</bodyText>
<page confidence="0.997443">
46
</page>
<note confidence="0.558103">
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
</note>
<figureCaption confidence="0.782405">
Figure 8
</figureCaption>
<figure confidence="0.864520428571429">
Incremental computation of outside o -products at disjunctive node d4.
From this, we find:
�     yd1 
*d = �  � 
{c|d∈b(c)} *c exp Ajfj(c)  
 j dl∈b(c)
d&apos;#d
</figure>
<bodyText confidence="0.8036535">
We finally find the following theorem for the computation of outside o -products.
Theorem 3 (Outside o -product)
The outside o -product *c at conjunctive node c is computed by the following equation
if *d is given for all mother disjunctive nodes, that is, all d such that c ∈ -y(d).
</bodyText>
<equation confidence="0.9082545">
�*c = *d
{d|c∈y(d)}
</equation>
<bodyText confidence="0.971625666666667">
The outside o -product *d at disjunctive node d is computed by the following equation
if *c is given for all mother conjunctive nodes, that is, all c such that d ∈ b(c), and yds
for all sibling disjunctive nodes d&apos;.
</bodyText>
<figure confidence="0.9577244">
�     yd1 
*d = �  � 
{c|d∈b(c)} *c exp Ajfj(c)  
 j dl∈b(c)
d&apos;#d
</figure>
<figureCaption confidence="0.5505804">
Figure 9 shows the overall algorithm for estimating the parameters, given a set
of feature forests. The key point of the algorithm is to compute inside o -products y
and outside o -products * for each node in C, and not for all unpacked trees. The func-
tions inside product and outside product compute y and * efficiently by dynamic
programming.
</figureCaption>
<bodyText confidence="0.993406">
Note that the order in which nodes are traversed is important for incremental com-
putation, although it is not shown in Figure 9. The computation for the daughter
nodes and mother nodes must be completed before computing the inside and outside
</bodyText>
<page confidence="0.997486">
47
</page>
<figure confidence="0.862455">
Computational Linguistics Volume 34, Number 1
</figure>
<figureCaption confidence="0.939873">
Figure 9
</figureCaption>
<subsectionHeader confidence="0.425044">
Algorithm for computing model expectations of feature forests.
</subsectionHeader>
<bodyText confidence="0.817445857142857">
α-products, respectively. This constraint is easily solved using any topological sort
algorithm. A topological sort is applied once at the beginning. The result of the sorting
does not affect the cost and the result of estimation. In our implementation, we assume
that conjunctive/disjunctive nodes are already ordered from the root node in input data.
The complexity of this algorithm is O(( ˜|C |+˜|D|) ˜|F||E|), where ˜|C |and ˜|D |are the
average numbers of conjunctive and disjunctive nodes, respectively. This is tractable
when ˜|C |and ˜|D |are of a reasonable size. As noted in this section, the number of
</bodyText>
<page confidence="0.996728">
48
</page>
<note confidence="0.616486">
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
</note>
<bodyText confidence="0.870208666666667">
nodes in a feature forest is usually polynomial even when that of the unpacked trees
is exponential. Thus we can efficiently compute model expectations with polynomial
computational complexity.
</bodyText>
<sectionHeader confidence="0.873341" genericHeader="method">
4. Probabilistic HPSG Parsing
</sectionHeader>
<bodyText confidence="0.9983375">
Following previous studies on probabilistic models for HPSG (Oepen, Toutanova, et al.
2002; Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van
Noord 2004), we apply a maximum entropy model to HPSG parse disambiguation. The
probability, p(t|w), of producing parse result t of a given sentence w is defined as
</bodyText>
<equation confidence="0.998569333333333">
p(t|w) =2 p0(t|w)exp Eλifi(t,w)
w
i
</equation>
<bodyText confidence="0.503987">
where
</bodyText>
<equation confidence="0.9961632">
�E �
p0(t�|w) exp λi fi(t�, w)
EZw =
t�∈T(w)
i
</equation>
<bodyText confidence="0.999914235294118">
where p0(t|w) is a reference distribution (usually assumed to be a uniform distribution)
and T(w) is a set of parse candidates assigned to w. The feature function fi(t,w) rep-
resents the characteristics of t and w, and the corresponding model parameter λi is its
weight. Model parameters that maximize the log-likelihood of the training data are
computed using a numerical optimization method (Malouf 2002).
Estimation of the model requires a set of pairs (tw, T(w)), where tw is the correct
parse for a sentence w. Whereas tw is provided by a treebank, T(w) has to be computed
by parsing each w in the treebank. Previous studies assumed T(w) could be enumer-
ated; however, this assumption is impractical because the size of T(w) is exponentially
related to the length of w.
Our solution here is to apply the feature forest model of Section 3 to the probabilistic
modeling of HPSG parsing. Section 4.1 briefly introduces HPSG. Section 4.2 and 4.3
describe how to represent HPSG parse trees and predicate–argument structures by
feature forests. Together with the parameter estimation algorithm in Section 3, these
methods constitute a complete method for probabilistic disambiguation. We also ad-
dress a method for accelerating the construction of feature forests for all treebank
sentences in Section 4.4. The design of feature functions will be given in Section 4.5.
</bodyText>
<subsectionHeader confidence="0.92093">
4.1 HPSG
</subsectionHeader>
<bodyText confidence="0.999930666666667">
HPSG (Pollard and Sag 1994; Sag, Wasow, and Bender 2003) is a syntactic theory that fol-
lows the lexicalist framework. In HPSG, linguistic entities, such as words and phrases,
are denoted by signs, which are represented by typed feature structures (Carpenter
1992). Signs are a formal representation of combinations of phonological forms and
syntactic/semantic structures, and express which phonological form signifies which
syntactic/semantic structure. Figure 10 shows the lexical sign for loves. The geometry
of signs follows Pollard and Sag: HEAD represents the part-of-speech of the head word,
MOD denotes modifiee constraints, and SPR, SUBJ, and COMPS describe constraints
of a specifier, a syntactic subject, and complements, respectively. CONT denotes the
</bodyText>
<page confidence="0.994171">
49
</page>
<figure confidence="0.89702">
Computational Linguistics Volume 34, Number 1
</figure>
<figureCaption confidence="0.806582">
Figure 10
</figureCaption>
<bodyText confidence="0.493539">
Lexical entry for the transitive verb loves.
</bodyText>
<subsectionHeader confidence="0.236042">
Figure 11
</subsectionHeader>
<bodyText confidence="0.974293555555556">
Simplified representation of the lexical entry in Figure 10.
predicate–argument structure of a phrase/sentence. The notation of CONT in this article
is borrowed from that of Minimal Recursion Semantics (Copestake et al. 2006): HOOK
represents a structure accessed by other phrases, and RELS describes the remaining
structure of the semantics. In what follows, we represent signs in a reduced form as
shown in Figure 11, because of the large size of typical HPSG signs, which often include
information not immediately relevant to the point being discussed. We will only show
attributes that are relevant to an explanation, expecting that readers can fill in the values
of suppressed attributes.
</bodyText>
<page confidence="0.928252">
50
</page>
<note confidence="0.571833">
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
</note>
<bodyText confidence="0.9999852">
In our actual implementation of the HPSG grammar, lexical/phrasal signs contain
additional attributes that are not defined in the standard HPSG theory but are used
by a disambiguation model. Examples include the surface form of lexical heads, and
the type of lexical entry assigned to lexical heads, which are respectively used for
computing the features WORD and LE introduced in Section 4.5. By incorporating ad-
ditional attributes into signs, we can straightforwardly compute feature functions for
each sign. This allows for a simple mapping between a parsing chart and a feature forest
as described subsequently. However, this might increase the size of parse forests and
therefore decrease parsing efficiency, because differences between additional attributes
interfere with equivalence relations for ambiguity packing.
</bodyText>
<subsectionHeader confidence="0.982926">
4.2 Packed Representation of HPSG Parse Trees
</subsectionHeader>
<bodyText confidence="0.996930136363636">
We represent an HPSG parse tree with a set of tuples (m, l, r), where m,l, and r are the
signs of the mother, left daughter, and right daughter, respectively.4 In chart parsing,
partial parse candidates are stored in a chart, in which phrasal signs are identified and
packed into equivalence classes if they are judged to be equivalent and dominate the
same word sequences. A set of parse trees is then represented as a set of relations among
equivalence classes.5
Figure 12 shows a chart for parsing he saw a girl with a telescope, where the modifiee
of with is ambiguous (saw or girl). Each feature structure expresses an equivalence class,
and the arrows represent immediate-dominance relations. The phrase, saw a girl with
a telescope, has two trees (A in the figure). Because the signs of the top-most nodes are
equivalent, they are packed into an equivalence class. The ambiguity is represented as
the two pairs of arrows leaving the node A.
A set of HPSG parse trees is represented in a chart as a tuple (E, Er, o ), where E is a
set of equivalence classes, Er C E is a set of root nodes, and o : E -4 2E×E is a function
to represent immediate-dominance relations.
Our representation of a chart can be interpreted as an instance of a feature forest.
We map the tuple (em, el, er), which corresponds to (m, l, r), into a conjunctive node.
Figure 13 shows (a part of) the HPSG parse trees in Figure 12 represented as a feature
forest. Square boxes (ci) are conjunctive nodes, and di disjunctive nodes. A solid arrow
represents a disjunctive daughter function, and a dotted line expresses a conjunctive
daughter function.
Formally, a chart (E, Er, o ) is mapped into a feature forest (C, D, R,-y, b) as follows.6
</bodyText>
<listItem confidence="0.9737625">
• C = {(em, el, er) em E E ∧ (el, er) E o (em)} U {w w E w}
• D = E
• R = {(em, el, er) em E Er ∧ (em, el, er) E C}
4 For simplicity, only binary trees are considered. Extension to unary and n-ary (n &gt; 2) trees is trivial.
</listItem>
<bodyText confidence="0.949450777777778">
5 We assume that CONT and DTRS (a feature used to represent daughter signs) are restricted (Shieber 1985),
and we will discuss a method for encoding CONT in a feature forest in Section 4.3. We also assume that
parse trees are packed according to equivalence relations rather than subsumption relations (Oepen and
Carroll 2000). We cannot simply map parse forests packed under subsumption into feature forests,
because they over-generate possible unpacked trees.
6 For ease of explanation, the definition of the root node is different from the original definition given
in Section 3. In this section, we define R as a set of conjunctive nodes rather than a single node r. The
definition here is translated into the original definition by introducing a dummy root node r&apos; that has
no features and only one disjunctive daughter whose daughters are R.
</bodyText>
<page confidence="0.982694">
51
</page>
<figure confidence="0.826389333333333">
Computational Linguistics Volume 34, Number 1
Figure 12
Chart for parsing he saw a girl with a telescope.
</figure>
<figureCaption confidence="0.961018">
Figure 13
</figureCaption>
<bodyText confidence="0.775214">
Feature forest representation of HPSG parse trees in Figure 12.
</bodyText>
<page confidence="0.986376">
52
</page>
<listItem confidence="0.898573666666667">
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
• γ(em) =f {(em, el, er)|(el,er) ∈ α(em)} if α(em) =� ∅
{w|em is a lexical entry for w} otherwise
• b(c) = {el,er} if c = ~em, el, er�
∅
if c ∈ w
</listItem>
<bodyText confidence="0.760201333333333">
changing the model. Actually, we successfully developed a probabili
stic model includ-
ing features on nonlocalpredicate–argument dependencies, as described subsequently.
</bodyText>
<subsectionHeader confidence="0.961082">
4.3 Packed Representation of Predicate–Argument Structures
</subsectionHeader>
<bodyText confidence="0.990840939393939">
Locality: In each step of composition of
structure, only a
limited depth of the
structures are referred to.
That is, local structures in the deep descendent phrases maybe ignored to
construct larger phrases. This assumption mean
apredicate–argument
daughters’predicate–argument
s that predicate–argument
structures can be packed into conjunctive nodes by ignoring local structures.
One may claim that restricting the domain of feature functions to (em, el, er) limits the
flexibility of feature design. Although this is true to some extent, it does not necessarily
mean the impossibility of incorporating features on nonlocal dependencies into the
model. This is because a feature forest model does not assume probabilistic indepen-
dence of conjunctive nodes. This means that we can unpack a part of the forest without
With the method previously described, we can represent an HPSG parsing chart with
a feature forest. However, equivalence classes in a chart might increase exponentially
because predicate–argument structures in HPSG signs represent the semantic relations
of all words that the phrase dominates. For example, Figure 14 shows phrasal signs with
predicate–argument structures for saw a girl with a telescope. In the chart in Figure 12,
these signs are packed into an equivalence class. However, Figure 14 shows that the
values of CONT, that is, predicate–argument structures, have different values, and the
signs as they are cannot be equivalent. As seen in this example, predicate–argument
structures prevent us from packing signs into equivalence classes.
In this section, we apply the feature forest model to predicate–argument structures,
which may include reentrant structures and non-local dependencies. It is theoretically
difficult to apply the feature forest model to predicate–argument structures; a feature
forest cannot represent graph structures that include reentrant structures in a straight-
forward manner. However, if predicate–argument structures are constructed as in the
manner described subsequently, they can be represented by feature forests of a tracta-
ble size.
Feature forests can represent predicate–argument structures if we assume some
locality and monotonicity in the composition of predicate–argument structures.
</bodyText>
<figureCaption confidence="0.770359">
Figure 14
</figureCaption>
<bodyText confidence="0.439257">
Signs with predicate–argument structures.
</bodyText>
<page confidence="0.939168">
53
</page>
<bodyText confidence="0.976292166666667">
Computational Linguistics Volume 34, Number 1
Monotonicity: All relations in the daughters’ predicate–argument structures
are percolated to the mother. That is, none of the predicate–argument
relations in the daughter phrases disappear in the mother. Thus
predicate–argument structures of descendent phrases can be located at
lower nodes in a feature forest.
Predicate–argument structures usually satisfy the above conditions, even when they
include non-local dependencies. For example, Figure 15 shows HPSG lexical entries
for the wh-extraction of the object of love (left) and for the control construction of try
(right). The first condition is satisfied because both lexical entries refer to CONT|HOOK
of argument signs in SUBJ, COMPS, and SLASH. None of the lexical entries directly
access ARGX of the arguments. The second condition is also satisfied because the values
of CONT|HOOK of all of the argument signs are percolated to ARGX of the mother. In
addition, the elements in CONT|RELS are percolated to the mother by the Semantic Prin-
ciple. Compositional semantics usually satisfies the above conditions, including MRS
(Copestake et al. 1995, 2006). The composition of MRS refers to HOOK, and no internal
structures of daughters. The Semantic Principle of MRS also assures that all semantic
relations in RELS are percolated to the mother. When these conditions are satisfied,
semantics may include any constraints, such as selectional restrictions, although the
grammar we used in the experiments does not include semantic restrictions to constrain
parse forests.
Under these conditions, local structures of predicate–argument structures are en-
coded into a conjunctive node when the values of all of its arguments have been
instantiated. We introduce the notion of inactives to denote such local structures.
</bodyText>
<subsectionHeader confidence="0.551664">
Definition 10 (Inactives)
</subsectionHeader>
<bodyText confidence="0.999820833333333">
An inactive is a subset of predicate–argument structures in which all arguments have
been instantiated.
Because inactive parts will not change during the rest of the parsing process, they can
be placed in a conjunctive node. By placing newly generated inactives into correspond-
ing conjunctive nodes, a set of predicate–argument structures can be represented in a
feature forest by packing local ambiguities, and non-local dependencies are preserved.
</bodyText>
<figureCaption confidence="0.570649">
Figure 16 illustrates a process of parsing the sentence She ignored the fact that I wanted
to dispute, where dispute has an ambiguity (dispute1, intransitive, and dispute2, transitive)
Figure 15
</figureCaption>
<bodyText confidence="0.404122">
Lexical entries including non-local relations.
</bodyText>
<page confidence="0.848962">
54
</page>
<figure confidence="0.540612666666667">
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
Figure 16
Process of composing predicate–argument structures.
</figure>
<figureCaption confidence="0.942253">
Figure 17
</figureCaption>
<subsectionHeader confidence="0.555726">
Predicate–argument structures of dispute.
</subsectionHeader>
<bodyText confidence="0.999948125">
and fact may optionally take a complementizer phrase.7 The predicate–argument struc-
tures for dispute1 and dispute2 are shown in Figure 17. Curly braces express the am-
biguities of partially constructed predicate–argument structures. The resulting feature
forest is shown in Figure 18. The boxes denote conjunctive nodes and dx represent
disjunctive nodes.
The clause I wanted to dispute has two possible predicate–argument structures: one
corresponding to dispute1 (α in Figure 16) and the other corresponding to dispute2 (β
in Figure 16). The nodes of the predicate–argument structure α are all instantiated, that
is, it contains only inactives. The corresponding conjunctive node (α&apos; in Figure 18) has
two inactives, for want and dispute1. The other structure β has an unfilled object in the
argument (ARG28) of dispute2, which will be filled by the non-local dependency. Hence,
the corresponding conjunctive node β&apos; has only one inactive corresponding to want,
and the remaining part that corresponds to dispute2 is passed on for further processing.
When we process the phrase the fact that I wanted to dispute, the object of dispute2 is filled
by fact (γ in Figure 16), and the predicate–argument structure of dispute2 is then placed
into a conjunctive node (γ&apos; in Figure 18).
</bodyText>
<footnote confidence="0.5357536">
7 In Figure 16, feature structures of different nodes of parse trees are assigned distinct variables, even when
they are from the same lexical entries. This is because feature structures are copied during chart parsing.
Although these variables are from the same lexical entry, it is copied to several chart items, and hence
there are no structure sharings among them.
8 ⊥ (bottom) represents an uninstantiated value (Carpenter 1992).
</footnote>
<page confidence="0.967518">
55
</page>
<figure confidence="0.89367">
Computational Linguistics Volume 34, Number 1
</figure>
<figureCaption confidence="0.858181">
Figure 18
</figureCaption>
<bodyText confidence="0.966480833333333">
A feature forest representation of predicate–argument structures.
One of the beneficial characteristics of this packed representation is that the rep-
resentation is isomorphic to the parsing process, that is, a chart. Hence, we can assign
features of HPSG parse trees to a conjunctive node, together with features of predicate–
argument structures. In Section 5, we will investigate the contribution of features on
parse trees and predicate–argument structures to the disambiguation of HPSG parsing.
</bodyText>
<subsectionHeader confidence="0.99915">
4.4 Filtering by Preliminary Distribution
</subsectionHeader>
<bodyText confidence="0.999991875">
The method just described is the essence of our solution for the tractable estimation
of maximum entropy models on exponentially many HPSG parse trees. However,
the problem of computational cost remains. Construction of feature forests requires
parsing of all of the sentences in a treebank. Despite the development of methods to
improve HPSG parsing efficiency (Oepen, Flickinger, et al. 2002), exhaustive parsing of
all sentences is still expensive.
We assume that computation of parse trees with low probabilities can be omitted
in the estimation stage because T(w) can be approximated by parse trees with high
probabilities. To achieve this, we first prepared a preliminary probabilistic model whose
estimation did not require the parsing of a treebank. The preliminary model was used
to reduce the search space for parsing a training treebank.
The preliminary model in this study is a unigram model, ¯p(t|w) _ fJw∈w p(l|w),
where w ∈ w is a word in the sentence w, and l is a lexical entry assigned to w. This
model is estimated by counting the relative frequencies of lexical entries used for w in
the training data. Hence, the estimation does not require parsing of a treebank. Actually,
we use a maximum entropy model to compute this probability as described in Section 5.
</bodyText>
<page confidence="0.943011">
56
</page>
<note confidence="0.580522">
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
</note>
<bodyText confidence="0.989325291666667">
The preliminary model is used for filtering lexical entries when we parse a treebank.
Given this model, we restrict the number of lexical entries used to parse a treebank. With
a threshold n for the number of lexical entries and a threshold c for the probability,
lexical entries are assigned to a word in descending order of probability, until the
number of assigned entries exceeds n, or the accumulated probability exceeds c. If this
procedure does not assign a lexical entry necessary to produce a correct parse (i.e., an
oracle lexical entry), it is added to the list of lexical entries. It should be noted that oracle
lexical entries are given by the HPSG treebank. This assures that the filtering method
does not exclude correct parse trees from parse forests.
Figure 19 shows an example of filtering the lexical entries assigned to saw. With c =
0.95, four lexical entries are assigned. Although the lexicon includes other lexical entries,
such as a verbal entry taking a sentential complement (p = 0.01 in the figure), they are
filtered out. Although this method reduces the time required for parsing a treebank, this
approximation causes bias in the training data and results in lower accuracy. The trade-
off between parsing cost and accuracy will be examined experimentally in Section 5.4.
We have several ways to integrate p¯ with the estimated model p(tjT(w)). In the
experiments, we will empirically compare the following methods in terms of accuracy
and estimation time.
Filtering only: The unigram probability p¯ is used only for filtering in training.
Product: The probability is defined as the product of p¯ and the estimated model p.
Reference distribution: p¯ is used as a reference distribution of p.
Feature function: log p¯ is used as a feature function of p. This method has been
shown to be a generalization of the reference distribution method (Johnson
and Riezler 2000).
</bodyText>
<subsectionHeader confidence="0.829245">
4.5 Features
</subsectionHeader>
<bodyText confidence="0.99917">
Feature functions in maximum entropy models are designed to capture the characteris-
tics of (em, el, er). In this article, we investigate combinations of the atomic features listed
</bodyText>
<figureCaption confidence="0.629215">
Figure 19
</figureCaption>
<bodyText confidence="0.873791">
Filtering of lexical entries for saw.
</bodyText>
<page confidence="0.988972">
57
</page>
<table confidence="0.438299">
Computational Linguistics Volume 34, Number 1
</table>
<tableCaption confidence="0.595712666666667">
Table 1
Templates for atomic features.
RULE name of the applied schema
DIST distance between the head words of the daughters
COMMA whether a comma exists between daughters and/or inside of daughter phrases
SPAN number of words dominated by the phrase
</tableCaption>
<bodyText confidence="0.709038428571429">
SYM symbol of the phrasal category (e.g., NP, VP)
WORD surface form of the head word
POS part-of-speech of the head word
LE lexical entry assigned to the head word
ARG argument label of a predicate
in Table 1. The following combinations are used for representing the characteristics of
binary/unary schema applications.
� RULE,DIST,COMMA,
fbinary = SPANl, SYMl, WORDl, POSl, LEl,
SPANr, SYMr, WORDr, POSr, LEr
funary = (RULE,SYM,WORD,POS,LE)
where subscripts l and r denote left and right daughters.
In addition, the following is used for expressing the condition of the root node of
the parse tree.
</bodyText>
<equation confidence="0.557445">
froot = (SYM,WORD,POS,LE)
</equation>
<bodyText confidence="0.9907642">
Feature functions to capture predicate–argument dependencies are represented as
follows:
fpa = ARG, DIST, WORDp, POSp, LEp, WORDa, POSa, LEa)
where subscripts p and a represent predicate and argument, respectively.
Figure 20 shows examples: froot is for the root node, in which the phrase symbol
is S and the surface form, part-of-speech, and lexical entry of the lexical head are saw,
VBD, and a transitive verb, respectively. fbinary is for the binary rule application to saw a
girl and with a telescope, in which the applied schema is the Head-Modifier Schema, the
left daughter is VP headed by saw, and the right daughter is PP headed by with, whose
part-of-speech is IN and whose lexical entry is a VP-modifying preposition.
Figure 21 shows example features for predicate–argument structures. The figure
shows features assigned to the conjunctive node denoted as α&apos; in Figure 18. Because
inactive structures in the node have three predicate–argument relations, three features
are activated. The first one is for the relation of want and I, where the label of the relation
is ARG1, the distance between the head words is 1, the surface string and the POS of
</bodyText>
<page confidence="0.991616">
58
</page>
<figure confidence="0.627977666666667">
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
Figure 20
Example features for binary schema application and root condition.
</figure>
<figureCaption confidence="0.862655">
Figure 21
</figureCaption>
<bodyText confidence="0.950382">
Example features for predicate–argument structures.
the predicate are want and VBD, and those of the argument are I and PRP. The second
and the third features are for the other two relations. We may include features on more
than two relations, such as the dependencies among want, I, and dispute, although such
features are not incorporated currently.
In our implementation, some of the atomic features are abstracted (i.e., ignored) for
smoothing. Tables 2, 3, and 4 show the full set of templates of combined features used in
the experiments. Each row represents the template for a feature function. A check indi-
cates the atomic feature is incorporated, and a hyphen indicates the feature is ignored.
</bodyText>
<page confidence="0.991776">
59
</page>
<table confidence="0.581303">
Computational Linguistics Volume 34, Number 1
</table>
<tableCaption confidence="0.990634">
Table 2
</tableCaption>
<table confidence="0.989246711538462">
Feature templates for binary schema (left) and unary schema (right).
RULE DIST COMMA SPAN SYM WORD POS LE
√ √ √ √ √ √
– –
√ √ √ √ √ –
– –
√ √ √ √ – √
– √ √ –
– –
√ √ √
–
√ √ √ – √ √ √
–
√ √ √ – √ √ –
–
√ √ √ – √ – √
–
√ √ √ √ √ –
–√ √ – – – √ √
√ √ √ – √ –
– –
√ √ √ – √
– – √ –
– –
√ √ √
– –
√√ √ – – √ √
–
√ √ √ – – √ –
–
√ √ √ – – √
– –
√ √ √ √ –
– – –
RULE SYM WORD POS LE
√ √ √ √
–
√ √ √ –
–
√ √ – √
√ √ –
–
√
– √ √ –
√
–
√ – √ –
–
√ – √
√ –
– –
√– –
</table>
<tableCaption confidence="0.999022">
Table 3
</tableCaption>
<bodyText confidence="0.863048">
Feature templates for root condition.
</bodyText>
<equation confidence="0.945878428571429">
SYM WORD POS LE
Y Y Y
Y Y
– Y – Y
Y Y
– –
– –
– – – Y
Y – – –
–
– –
Y Y
Y
–
</equation>
<tableCaption confidence="0.940851">
Table 4
</tableCaption>
<bodyText confidence="0.70064">
Feature templates for predicate–argument dependencies.
</bodyText>
<equation confidence="0.957506357142857">
ARG DIST WORD POS LE
Y Y Y Y Y
Y Y Y – Y
Y Y – Y Y
Y – Y Y Y
Y – Y – Y
Y – – Y Y
Y Y Y Y–
Y/-
Y/–
Y – Y Y –
Y – Y –
– Y – –
Y –
</equation>
<page confidence="0.985343">
60
</page>
<note confidence="0.909983">
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
</note>
<sectionHeader confidence="0.993539" genericHeader="method">
5. Experiments
</sectionHeader>
<bodyText confidence="0.999929">
This section presents experimental results on the parsing accuracy attained by the
feature forest models. In all of the following experiments, we use the HPSG grammar
developed by the method of Miyao, Ninomiya, and Tsujii (2005). Section 5.1 describes
how this grammar was developed. Section 5.2 explains other aspects of the experimental
settings. In Sections 5.3 to 5.7, we report results of the experiments on HPSG parsing.
</bodyText>
<subsectionHeader confidence="0.962603">
5.1 The HPSG Grammar
</subsectionHeader>
<bodyText confidence="0.999914666666667">
In the following experiments, we use Enju 2.1 (Tsujii Laboratory 2004), which is a wide-
coverage HPSG grammar extracted from the Penn Treebank by the method of Miyao,
Ninomiya, and Tsujii (2005). In this method, we convert the Penn Treebank into an
HPSG treebank, and collect HPSG lexical entries from terminal nodes of the HPSG
treebank. Figure 22 illustrates the process of treebank conversion and lexicon collection.
We first convert and fertilize parse trees of the Penn Treebank. This step identifies
syntactic constructions that require special treatment in HPSG, such as raising/control
and long-distance dependencies. These constructions are then annotated with typed
feature structures so that they conform to the HPSG analysis. Next, we apply HPSG
schemas and principles, and obtain fully specified HPSG parse trees. This step solves
feature structure constraints given in the previous step, and fills unspecified constraints.
Failures of schema/principle applications indicate that the annotated constraints do not
</bodyText>
<figureCaption confidence="0.594134">
Figure 22
</figureCaption>
<bodyText confidence="0.567365">
Extracting HPSG lexical entries from the Penn Treebank.
</bodyText>
<page confidence="0.96649">
61
</page>
<note confidence="0.473407">
Computational Linguistics Volume 34, Number 1
</note>
<bodyText confidence="0.999865166666667">
conform to the HPSG analysis, and require revisions. Finally, we obtain lexical entries
from the HPSG parse trees. The terminal nodes of HPSG parse trees are collected, and
they are generalized by removing word-specific or context-specific constraints.
An advantage of this method is that a wide-coverage HPSG lexicon is obtained
because lexical entries are extracted from real-world sentences. Obtained lexical entries
are guaranteed to construct well-formed HPSG parse trees because HPSG schemas
and principles are successfully applied during the development of the HPSG treebank.
Another notable feature is that we can additionally obtain an HPSG treebank, which
can be used as training data for disambiguation models. In the following experiments,
this HPSG treebank is used for the training of maximum entropy models.
The lexicon used in the following experiments was extracted from Sections 02–21
of the Wall Street Journal portion of the Penn Treebank. This lexicon can assign correct
lexical entries to 99.09% of words in the HPSG treebank converted from Penn Treebank
Section 23. This number expresses “lexical coverage” in the strong sense defined by
Hockenmaier and Steedman (2002). In this notion of “coverage,” this lexicon has 84.1%
sentential coverage, where this means that the lexicon can assign correct lexical entries
to all of the words in a sentence. Although the parser might produce parse results for
uncovered sentences, these parse results cannot be completely correct.
</bodyText>
<subsectionHeader confidence="0.993127">
5.2 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.999985625">
The data for the training of the disambiguation models was the HPSG treebank derived
from Sections 02–21 of the Wall Street Journal portion of the Penn Treebank, that is, the
same set used for lexicon extraction. For training of the disambiguation models, we
eliminated sentences of 40 words or more and sentences for which the parser could not
produce the correct parses. The resulting training set consists of 33,604 sentences (when
n = 10 and c = 0.95; see Section 5.4 for details). The treebanks derived from Sections
22 and 23 were used as the development and final test sets, respectively. Following
previous studies on parsing with PCFG-based models (Collins 1997; Charniak 2000),
accuracy is measured for sentences of less than 40 words and for those with less than
100 words. Table 5 shows the specifications of the test data.
The measure for evaluating parsing accuracy is precision/recall of predicate–
argument dependencies output by the parser. A predicate–argument dependency is
defined as a tuple (wh,wn,7t, p), where wh is the head word of the predicate, wn is the
head word of the argument, 7t is the type of the predicate (e.g., adjective, intransitive
verb), and p is an argument label (MODARG, ARG1, ..., ARG4). For example, He tried
running has three dependencies as follows:
</bodyText>
<listItem confidence="0.995209">
• (tried, he, transitive verb, ARG1)
</listItem>
<tableCaption confidence="0.99944">
Table 5
</tableCaption>
<table confidence="0.7986531">
Specification of test data for the evaluation of parsing accuracy.
Test set (Section 23, &lt; 40 words)
Test set (Section 23, &lt; 100 words)
Development set (Section 22, &lt; 40 words)
Development set (Section 22, &lt; 100 words)
No. of Sentences Avg. Length
2,144 20.52
2,299 22.23
1,525 20.69
1,641 22.43
</table>
<page confidence="0.898727">
62
</page>
<note confidence="0.545792">
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
</note>
<listItem confidence="0.999363">
• (tried, running, transitive verb, ARG2)
• (running, he, intransitive verb, ARG1)
</listItem>
<bodyText confidence="0.999661066666667">
Labeled precision/recall (LP/LR) is the ratio of tuples correctly identified by the parser,
and unlabeled precision/recall (UP/UR) is the ratio of wh and wn correctly identified
regardless of π and p. F-score is the harmonic mean of LP and LR. Sentence accuracy
is the exact match accuracy of complete predicate–argument relations in a sentence.
These measures correspond to those used in other studies measuring the accuracy of
predicate–argument dependencies in CCG parsing (Clark, Hockenmaier, and Steedman
2002; Hockenmaier 2003; Clark and Curran 2004b) and LFG parsing (Burke et al. 2004),
although exact figures cannot be compared directly because the definitions of depen-
dencies are different. All predicate–argument dependencies in a sentence are the target
of evaluation except quotation marks and periods. The accuracy is measured by parsing
test sentences with gold-standard part-of-speech tags from the Penn Treebank unless
otherwise noted.
The Gaussian prior was used for smoothing (Chen and Rosenfeld 1999a), and its
hyper-parameter was tuned for each model to maximize F-score for the development
set. The algorithm for parameter estimation was the limited-memory BFGS method
(Nocedal 1980; Nocedal and Wright 1999). The parser was implemented in C++ with
the LiLFeS library (Makino et al. 2002), and various speed-up techniques for HPSG
parsing were used such as quick check and iterative beam search (Tsuruoka, Miyao, and
Tsujii 2004; Ninomiya et al. 2005). Other efficient parsing techniques, including global
thresholding, hybrid parsing with a chunk parser, and large constituent inhibition, were
not used. The results obtained using these techniques are given in Ninomiya et al. A
limit on the number of constituents was set for time-out; the parser stopped parsing
when the number of constituents created during parsing exceeded 50,000. In such a
case, the parser output nothing, and the recall was computed as zero.
Features occurring more than twice were included in the probabilistic models. A
method of filtering lexical entries was applied to the parsing of training data (Sec-
tion 4.4). Unless otherwise noted, parameters for filtering were n = 10 and c = 0.95, and
a reference distribution method was applied. The unigram model, p0(t|s), for filtering is
a maximum entropy model with two feature templates, (WORD, POS, LE) and (POS, LE).
The model includes 24,847 features.
</bodyText>
<subsectionHeader confidence="0.998135">
5.3 Efficacy of Feature Forest Models
</subsectionHeader>
<bodyText confidence="0.9776215">
Tables 6 and 7 show parsing accuracy for the test set. In the tables, “Syntactic features”
denotes a model with syntactic features, that is, fbinary, funary, and froot introduced
</bodyText>
<tableCaption confidence="0.983557">
Table 6
</tableCaption>
<table confidence="0.942734">
Accuracy of predicate–argument relations (test set, &lt;40 words).
LP LR UP UR F-score Sentence acc.
Baseline 78.10 77.39 82.83 82.08 77.74 18.3
Syntactic features 86.92 86.28 90.53 89.87 86.60 36.3
Semantic features 84.29 83.74 88.32 87.75 84.01 30.9
All 86.54 86.02 90.32 89.78 86.28 36.0
63
Computational Linguistics Volume 34, Number 1
</table>
<tableCaption confidence="0.981546">
Table 7
</tableCaption>
<table confidence="0.997175">
Accuracy of predicate–argument relations (test set, &lt;100 words).
LP LR UP UR F-score Sentence acc.
Baseline 77.58 76.84 82.22 81.43 77.21 17.1
Syntactic features 86.47 85.83 90.06 89.40 86.15 34.1
Semantic features 83.81 83.26 87.75 87.16 83.53 28.9
All 86.13 85.59 89.85 89.29 85.86 33.8
</table>
<bodyText confidence="0.999418052631579">
in Section 4.5. “Semantic features” represents a model with features on predicate–
argument structures, that is, fpa given in Table 4. “All” is a model with both syntactic
and semantic features. The “Baseline” row shows the results for the reference model,
p0(t|s), used for lexical entry filtering in the estimation of the other models. This model
is considered as a simple application of a traditional PCFG-style model; that is, p(r) = 1
for any rule r in the construction rules of the HPSG grammar.
The results demonstrate that feature forest models have significantly higher ac-
curacy than a baseline model. Comparing “Syntactic features” with “Semantic fea-
tures,” we see that the former model attained significantly higher accuracy than the
latter. This indicates that syntactic features are more important for overall accuracy.
We will examine the contributions of each atomic feature of the syntactic features in
Section 5.5.
Features on predicate–argument relations were generally considered as important
for the accurate disambiguation of syntactic structures. For example, PP-attachment
ambiguity cannot be resolved with only syntactic preferences. However, the results
show that a model with only semantic features performs significantly worse than one
with syntactic features. Even when combined with syntactic features, semantic features
do not improve accuracy. Obviously, semantic preferences are necessary for accurate
parsing, but the features used in this work were not sufficient to capture semantic pref-
erences. A possible reason is that, as reported in Gildea (2001), bilexical dependencies
may be too sparse to capture semantic preferences.
For reference, our results are competitive with the best corresponding results re-
ported in CCG parsing (LP/LR = 86.6/86.3) (Clark and Curran 2004b), although our
results cannot be compared directly with other grammar formalisms because each
formalism represents predicate–argument dependencies differently. In contrast with the
results of CCG and PCFG (Collins 1997, 1999, 2003; Charniak 2000), the recall is clearly
lower than precision. This may have resulted from the HPSG grammar having stricter
feature constraints and the parser not being able to produce parse results for around
1% of the sentences. To improve recall, we need techniques to deal with these 1% of
sentences.
Table 8 gives the computation/space costs of model estimation. “Estimation time”
indicates user times required for running the parameter estimation algorithm. “No. of
feature occurrences” denotes the total number of occurrences of features in the training
data, and “Data size” gives the sizes of the compressed files of training data. We can
conclude that feature forest models are estimated at a tractable computational cost and
a reasonable data size, even when a model includes semantic features including non-
local dependencies. The results reveal that feature forest models essentially solve the
problem of the estimation of probabilistic models of sentence structures.
</bodyText>
<page confidence="0.999327">
64
</page>
<note confidence="0.910736">
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
</note>
<tableCaption confidence="0.816978">
Table 8
Computation/space costs of model estimation.
</tableCaption>
<table confidence="0.999940666666667">
No. of features Estimation No. of feature Data size
time (sec.) occurrences (MB)
Baseline 24,847 499 6,948,364 21
Syntactic features 599,104 511 127,497,615 727
Semantic features 334,821 278 176,534,753 375
All 933,925 716 304,032,368 1,093
</table>
<tableCaption confidence="0.996311">
Table 9
</tableCaption>
<table confidence="0.932556">
Estimation method vs. accuracy and estimation time.
LP LR F-score Estimation time (sec.)
Filtering only 51.70 49.89 50.78 449
Product 86.50 85.94 86.22 1,568
Reference distribution 86.92 86.28 86.60 511
Feature function 84.81 84.09 84.45 945
</table>
<subsectionHeader confidence="0.995611">
5.4 Comparison of Filtering Methods
</subsectionHeader>
<bodyText confidence="0.999966066666667">
Table 9 compares the estimation methods introduced in Section 4.4. In all of the follow-
ing experiments, we show the accuracy for the test set (&lt;40 words) only. Table 9 reveals
that our method achieves significantly lower accuracy when it is used only for filtering
in the training phrase. One reason is that the feature forest model prefers lexical entries
that are filtered out in the training phase, because they are always oracle lexical entries
in the training. This means that we must incorporate the preference of filtering into the
final parse selection. As shown in Table 9, the models combined with a preliminary
model achieved sufficient accuracy. The reference distribution method achieved higher
accuracy and lower cost. The feature function method achieved lower accuracy in our
experiments. A possible reason for this is that a hyper-parameter of the prior was set to
the same value for all the features including the feature of the log-probability given by
the preliminary distribution.
Tables 10 and 11 show the results of changing the filtering threshold. We can
determine the correlation between the estimation/parsing cost and accuracy. In our
experiment, n &gt; 10 and c &gt; 0.90 seem necessary to preserve the F-score over 86.0.
</bodyText>
<subsectionHeader confidence="0.998854">
5.5 Contribution of Features
</subsectionHeader>
<bodyText confidence="0.999957666666667">
Table 12 shows the accuracy with different feature sets. Accuracy was measured for 15
models with some atomic features removed from the final model. The last row denotes
the accuracy attained by the unigram model (i.e., the reference distribution). The num-
bers in bold type represent a significant difference from the final model according to
stratified shuffling tests with the Bonferroni correction (Cohen 1995) with p-value &lt; .05
for 32 pairwise comparisons. The results indicate that DIST, COMMA, SPAN, WORD, and
</bodyText>
<page confidence="0.998978">
65
</page>
<table confidence="0.425801">
Computational Linguistics Volume 34, Number 1
</table>
<tableCaption confidence="0.993277">
Table 10
</tableCaption>
<table confidence="0.995834928571428">
Filtering threshold vs. accuracy.
n, c LP LR F-score Sentence acc.
5, 0.80 85.09 84.30 84.69 32.4
5, 0.90 85.44 84.61 85.02 32.5
5, 0.95 85.52 84.66 85.09 32.7
5, 0.98 85.50 84.63 85.06 32.6
10, 0.80 85.60 84.65 85.12 32.5
10, 0.90 86.49 85.92 86.20 34.7
10, 0.95 86.92 86.28 86.60 36.3
10, 0.98 87.18 86.66 86.92 37.7
15, 0.80 85.59 84.63 85.11 32.4
15, 0.90 86.48 85.80 86.14 35.7
15, 0.95 87.21 86.68 86.94 37.0
15, 0.98 87.69 87.16 87.42 39.2
</table>
<tableCaption confidence="0.998405">
Table 11
</tableCaption>
<bodyText confidence="0.979051166666667">
Filtering threshold vs. estimation cost.
n, c Estimation time (sec.) Parsing time (sec.) Data size (MB)
5, 0.80 108 5,103 341
5, 0.90 150 6,242 407
5, 0.95 190 7,724 469
5, 0.98 259 9,604 549
10, 0.80 130 6,003 370
10, 0.90 268 8,855 511
10, 0.95 511 15,393 727
10, 0.98 1,395 36,009 1,230
15, 0.80 123 6,298 372
15, 0.90 259 9,543 526
15, 0.95 735 20,508 854
15, 0.98 3,777 86,844 2,031
POS features contributed to the final accuracy, although the differences were slight. In
contrast, RULE, SYM, and LE features did not affect accuracy. However, when each was
removed together with another feature, the accuracy decreased drastically. This implies
that such features carry overlapping information.
</bodyText>
<subsectionHeader confidence="0.742938">
5.6 Factors for Parsing Accuracy
</subsectionHeader>
<bodyText confidence="0.999557125">
Table 13 shows parsing accuracy for covered and uncovered sentences. As defined in
Section 5.1, “covered” indicates that the HPSG lexicon has all correct lexical entries for a
sentence. In other words, for covered sentences, exactly correct parse trees are obtained
if the disambiguation model worked perfectly. The result reveals clear differences in
accuracy between covered and uncovered sentences. The F-score for covered sentences
is around 2.5 points higher than the overall F-score, whereas the F-score is more than
10 points lower for uncovered sentences. This result indicates improvement of lexicon
quality is an important factor for higher accuracy.
</bodyText>
<page confidence="0.985215">
66
</page>
<note confidence="0.90247">
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
</note>
<tableCaption confidence="0.997816">
Table 12
</tableCaption>
<table confidence="0.997966789473684">
Accuracy with different feature sets.
Features LP LR F-score Sentence acc. No. of features
All 86.92 86.28 86.60 36.3 599,104
–RULE 86.83 86.19 86.51 36.3 596,446
–DIST 86.52 85.96 86.24 35.7 579,666
–COMMA 86.31 85.81 86.06 34.4 584,040
–SPAN 86.32 85.75 86.03 35.5 559,490
–SYM 86.74 86.16 86.45 35.4 406,545
–WORD 86.39 85.77 86.08 35.3 91,004
–POS 86.18 85.61 85.89 34.1 406,545
–LE 86.91 86.32 86.61 36.8 387,938
–DIST,SPAN 85.39 84.82 85.10 33.1 270,467
–DIST,SPAN,COMMA 83.75 83.25 83.50 28.9 261,968
–RULE,DIST,SPAN,COMMA 83.44 82.93 83.18 27.6 259,372
–WORD,LE 86.40 85.81 86.10 34.7 25,429
–WORD,POS 85.44 84.87 85.15 32.7 40,102
–WORD,POS,LE 84.68 84.12 84.40 31.1 8,899
–SYM,WORD,POS,LE 82.77 82.14 82.45 24.9 1,914
None 78.10 77.39 77.74 18.3 0
</table>
<tableCaption confidence="0.992191">
Table 13
</tableCaption>
<table confidence="0.609407">
Accuracy for covered/uncovered sentences.
LP LR F-score Sentence acc. No. of sentences
covered sentences 89.36 88.96 89.16 42.2 1,825
uncovered sentences 75.57 74.04 74.80 2.5 319
</table>
<bodyText confidence="0.997450842105263">
Figure 23 shows the learning curve. A feature set was fixed, and the parameter of
the Gaussian prior was optimized for each model. High accuracy is attained even with a
small training set, and the accuracy seems to be saturated. This indicates that we cannot
further improve the accuracy simply by increasing the size of the training data set. The
exploration of new types of features is necessary for higher accuracy. It should also be
noted that the upper bound of the accuracy is not 100%, because the grammar cannot
produce completely correct parse results for uncovered sentences.
Figure 24 shows the accuracy for each sentence length. It is apparent from this
figure that the accuracy is significantly higher for sentences with less than 10 words.
This implies that experiments with only short sentences overestimate the performance
of parsers. Sentences with at least 10 words are necessary to properly evaluate the
performance of parsing real-world texts. The accuracies for the sentences with more
than 10 words are not very different, although data points for sentences with more than
50 words are not reliable.
Table 14 shows the accuracies for predicate–argument relations when parts-
of-speech tags are assigned automatically by a maximum-entropy-based parts-of-
speech tagger (Tsuruoka and Tsujii 2005). The results indicate a drop of about three
points in labeled precision/recall (a two-point drop in unlabeled precision/recall).
A reason why we observed larger accuracy drops in labeled precision/recall is that
</bodyText>
<page confidence="0.992624">
67
</page>
<figure confidence="0.765823">
Computational Linguistics Volume 34, Number 1
Figure 23
Corpus size vs. accuracy.
</figure>
<figureCaption confidence="0.972687">
Figure 24
</figureCaption>
<bodyText confidence="0.959642666666667">
Sentence length vs. accuracy.
predicate–argument relations are fragile with respect to parts-of-speech errors because
predicate types (e.g., adjective, intransitive verb) are determined depending on the
parts-of-speech of predicate words. Although our current parsing strategy assumes that
parts-of-speech are given beforehand, for higher accuracy in real application contexts,
we will need a method for determining parts-of-speech and parse trees jointly.
</bodyText>
<tableCaption confidence="0.737291666666667">
Table 14
Accuracy with automatic parts-of-speech tags (test set).
LP LR UP UR F-score Sentence acc.
</tableCaption>
<table confidence="0.7260655">
&lt;40 words 83.88 82.84 88.83 87.73 83.36 30.1
&lt;100 words 83.45 82.40 88.37 87.26 82.92 28.2
</table>
<page confidence="0.991645">
68
</page>
<note confidence="0.852196">
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
</note>
<subsectionHeader confidence="0.993844">
5.7 Analysis of Disambiguation Errors
</subsectionHeader>
<bodyText confidence="0.999966166666667">
Table 15 shows a manual classification of the causes of disambiguation errors in 100 sen-
tences randomly chosen from Section 00. In our evaluation, one error source may cause
multiple dependency errors. For example, if an incorrect lexical entry is assigned to a
verb, all of the argument dependencies of the verb are counted as errors. The numbers
in the table include such double-counting. Figure 25 shows examples of disambiguation
errors. The figure shows output from the parser.
Major causes are classified into three types: attachment ambiguity, argument/
modifier distinction, and lexical ambiguity. As attachment ambiguities are well-known
error sources, PP-attachment is the largest source of errors in our evaluation. Our
disambiguation model cannot accurately resolve PP-attachment ambiguities because it
does not include dependencies among a modifiee and the argument of the preposition.
Because previous studies revealed that such dependencies are effective features for
PP-attachment resolution, we should incorporate them into our model. Some of the
attachment ambiguities, including adjective and adverb, should also be resolved
with an extension of features. However, we cannot identify any effective features
for the disambiguation of attachment of verbal phrases, including relative clauses,
verb phrases, subordinate clauses, and to-infinitives. For example, Figure 25 shows
an example error of the attachment of a relative clause. The correct answer is that the
</bodyText>
<tableCaption confidence="0.984766">
Table 15
</tableCaption>
<table confidence="0.580543478260869">
Classification of disambiguation errors.
Error cause No. of errors
Attachment ambiguity prepositional phrase 32
relative clause 14
adjective 7
adverb 6
verb phrase 5
subordinate clause 3
to-infinitive 3
others 6
Argument/modifier distinction to-infinitive 19
noun phrase 7
verb phrase 7
subordinate clause 7
others 9
Lexical ambiguity preposition/modifier 13
verb subcategorization frame 13
participle/adjective 12
others 6
Test set errors errors of treebank conversion 18
errors of Penn Treebank 4
Comma 32
Noun phrase identification 15
</table>
<tableCaption confidence="0.263334333333333">
Coordination/insertion 15
Zero-pronoun resolution 9
Others 4
</tableCaption>
<page confidence="0.817275">
69
</page>
<figure confidence="0.86166">
Computational Linguistics Volume 34, Number 1
</figure>
<figureCaption confidence="0.912297">
Figure 25
</figureCaption>
<bodyText confidence="0.976761333333333">
Examples of disambiguation errors.
subject of yielded is acre, but this cannot be determined only by the relation among yield,
grapes, and acre. The resolution of these errors requires a novel type of feature function.
Errors of argument/modifier distinction are prominent in deep syntactic analysis,
because arguments and modifiers are not explicitly distinguished in the evaluation of
CFG parsers. Figure 25 shows an example of the argument/modifier distinction of a
to-infinitive clause. In this case, the to-infinitive clause is a complement of tempts. The
subcategorization frame of tempts seems responsible for this problem. However, the
disambiguation model wrongly assigned a lexical entry for a transitive verb because
of the sparseness of the training data (tempts occurred only once in the training data).
The resolution of this sort of ambiguity requires the refinement of a probabilistic model
of lexical entries. Errors of verb phrases and subordinate clauses are similar to this
example. Errors of argument/modifier distinction of noun phrases are mainly caused by
temporal nouns and cardinal numbers. The resolution of these errors seems to require
the identification of temporal expressions and usage of cardinal numbers.
Errors of lexical ambiguities were mainly caused by idioms. For example, in Fig-
ure 25, compared with is a compound preposition, but the parser recognized it as a
verb phrase. This indicates that the grammar or the disambiguation model requires
the special treatment of idioms. Errors of verb subcategorization frames were mainly
caused by difficult constructions such as insertions. Figure 25 shows that the parser
could not identify the inserted clause (says John Siegel...) and a lexical entry for a
declarative transitive verb was chosen.
Attachment errors of commas are also significant. It should be noted that commas
were ignored in the evaluation of CFG parsers. We did not eliminate punctuation
from the evaluation because punctuation sometimes contributes to semantics, as
in coordination and insertion. In this error analysis, errors of commas representing
coordination/insertion are classified into “coordination/insertion,” and “comma” in-
dicates errors that do not contribute to the computation of semantics.
Errors of noun phrase identification mean that a noun phrase was split into two
phrases. These errors were mainly caused by the indirect effects of other errors.
</bodyText>
<page confidence="0.992954">
70
</page>
<note confidence="0.804154">
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
</note>
<bodyText confidence="0.99704075">
Errors of identifying coordination/insertion structures sometimes resulted in
catastrophic analyses. While accurate analysis of such constructions is indispensable,
it is also known to be difficult because disambiguation of coordination/insertion
requires the computation of preferences over global structures, such as the similarity
of syntactic/semantic structure of coordinates. Incorporating features for representing
the similarity of global structures is difficult for feature forest models.
Zero-pronoun resolution is also a difficult problem. However, we found that
most were indirectly caused by errors of argument/modifier distinction in to-infinitive
clauses.
A significant portion of the errors discussed above cannot be resolved by the fea-
tures we investigated in this study, and the design of other features will be necessary
for improving parsing accuracy.
</bodyText>
<sectionHeader confidence="0.9993" genericHeader="method">
6. Discussion
</sectionHeader>
<subsectionHeader confidence="0.982575">
6.1 Probabilistic Modeling of Complete Structures
</subsectionHeader>
<bodyText confidence="0.9999751875">
The model described in this article was first published in Miyao and Tsujii (2002), and
has been applied to probabilistic models for parsing with lexicalized grammars. Appli-
cations to CCG parsing (Clark and Curran 2003, 2004b) and LFG parsing (Kaplan et al.
2004; Riezler and Vasserman 2004) demonstrated that feature forest models attained
higher accuracy than other models. These researchers applied feature forests to repre-
sentations of the packed parse results of LFG and the dependency/derivation structures
of CCG. Their work demonstrated the applicability and effectiveness of feature forest
models in parsing with wide-coverage lexicalized grammars. Feature forest models
were also shown to be effective for wide-coverage sentence realization (Nakanishi,
Miyao, and Tsujii 2005). This work demonstrated that feature forest models are generic
enough to be applied to natural language processing tasks other than parsing.
The work of Geman and Johnson (2002) independently developed a dynamic pro-
gramming algorithm for maximum entropy models. The solution was similar to our
approach, although their method was designed to traverse LFG parse results repre-
sented with disjunctive feature structures as proposed by Maxwell and Kaplan (1995).
The difference between the two approaches is that feature forests use a simpler generic
data structure to represent packed forest structures. Therefore, without assuming what
feature forests represent, our algorithm can be applied to various tasks, including
theirs.
Another approach to the probabilistic modeling of complete structures is a method
of approximation. The work on whole sentence maximum entropy models (Rosenfeld
1997; Chen and Rosenfeld 1999b) proposed an approximation algorithm to estimate
parameters of maximum entropy models on whole sentence structures. However, the
algorithm suffered from slow convergence, and the model was basically a sequence
model. It could not produce a solution for complex structures as our model can.
We should also mention Conditional Random Fields (CRFs) (Lafferty, McCallum,
and Pereira 2001) for solving a similar problem in the context of maximum entropy
Markov models. Their solution was an algorithm similar to the computation of
forward/backward probabilities of hidden Markov models (HMMs). Their algorithm is
a special case of our algorithm in which each conjunctive node has only one daughter.
This is obvious because feature forests can represent Markov chains. In an analogy,
CRFs correspond to HMMs, whereas feature forest models correspond to PCFGs.
</bodyText>
<page confidence="0.994828">
71
</page>
<note confidence="0.592542">
Computational Linguistics Volume 34, Number 1
</note>
<bodyText confidence="0.99989416">
Extensions of CRFs, such as semi-Markov CRFs (Sarawagi and Cohen 2004), are also
regarded as instances of feature forest models. This fact implies that our algorithm is
applicable to not only parsing but also to other tasks. CRFs are now widely used for
sequence-based tasks, such as parts-of-speech tagging and named entity recognition,
and have been shown to achieve the best performance in various tasks (McCallum and
Li 2003; McCallum, Rohanimanesh, and Sutton 2003; Pinto et al. 2003; Sha and Pereira
2003; Peng and McCallum 2004; Roark et al. 2004; Settles 2004; Sutton, Rohanimanesh,
and McCallum 2004). These results suggest that the method proposed in the present
article will achieve high accuracy when applied to various statistical models with
tree structures. Dynamic CRFs (McCallum, Rohanimanesh, and Sutton 2003; Sutton,
Rohanimanesh, and McCallum 2004) provide us with an interesting inspiration for
extending feature forest models. The purpose of dynamic CRFs is to incorporate feature
functions that are not represented locally, and the solution is to apply a variational
method, which is an algorithm of numerical computation, to obtain approximate so-
lutions. A similar method may be developed to overcome a bottleneck of feature forest
models, that is, the fact that feature functions are localized to conjunctive nodes.
The structure of feature forests is common in natural language processing and
computational linguistics. As is easily seen, lattices, Markov chains, and CFG parse
trees are represented by feature forests. Furthermore, because conjunctive nodes do
not necessarily represent CFG nodes or rules and terminals of feature forests need
not be words, feature forests can express any forest structures in which ambiguities
are packed in local structures. Examples include the derivation trees of LTAG and
CCG. Chiang (2003) proved that feature forests could be considered as the derivation
forests of linear context-free rewriting systems (LCFRSs) (Vijay-Shanker, Weir, and Joshi
1987; Weir 1988). LCFRSs define a wide variety of grammars, including LTAG and
CCG, while preserving polynomial-time complexity of parsing. This demonstrates that
feature forest models are applicable to probabilistic models far beyond PCFGs. Feature
forests are also isomorphic to support graphs (or explanation graphs) used in the graphical
EM algorithm (Kameya and Sato 2000). In their framework, a program in a logic pro-
gramming language, PRISM (Sato and Kameya 1997), is converted into support graphs,
and parameters of probabilistic models are automatically learned by an EM algorithm.
Support graphs have been proved to represent various statistical structural models, in-
cluding HMMs, PCFGs, Bayesian networks, and many other graphical structures (Sato
and Kameya 2001; Sato 2005). Taken together, these results imply the high applicability
of feature forest models to various real tasks.
Because feature forests have a structure isomorphic to parse forests of PCFG, it
might seem that they can represent only immediate dominance relations of CFG rules
as in PCFG, resulting in only a slight, trivial extension of PCFG. As described herein,
however, feature forests can represent structures beyond CFG parse trees. Furthermore,
because feature forests are a generalized representation of ambiguous structures, each
node in a feature forest need not correspond to a node in a PCFG parse forest. That is,
a node in a feature forest may represent any linguistic entity, including a fragment of a
syntactic structure, a semantic relation, or other sentence-level information.
The idea of feature forest models could be applied to non-probabilistic machine
learning methods. Taskar et al. (2004) proposed a dynamic programming algorithm
for the learning of large-margin classifiers including support vector machines (Vapnik
1995), and presented its application to disambiguation in CFG parsing. Their algorithm
resembles feature forest models; an optimization function is computed by a dynamic
programing algorithm without unpacking packed forest structures. From the discussion
in this article, it is evident that if the main part of an update formula is represented
</bodyText>
<page confidence="0.994638">
72
</page>
<note confidence="0.86637">
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
</note>
<bodyText confidence="0.9928795">
with (the exponential of) linear combinations, a method similar to feature forest models
should be applicable.
</bodyText>
<subsectionHeader confidence="0.997009">
6.2 Probabilistic Parsing with Lexicalized Grammars
</subsectionHeader>
<bodyText confidence="0.999959755555556">
Before the advent of feature forest models, studies on probabilistic models of HPSG
adopted conventional maximum entropy models to select the most probable parse from
parse candidates given by HPSG grammars (Oepen, Toutanova, et al. 2002; Toutanova
and Manning 2002; Baldridge and Osborne 2003). The difference between these studies
and our work is that we used feature forests to avoid the exponential increase in the
number of structures that results from unpacked parse results. These studies ignored
the problem of exponential explosion; in fact, training sets in these studies were very
small and consisted only of short sentences. A possible approach to avoid this problem
is to develop a fully restrictive grammar that never causes an exponential explosion, al-
though the development of such a grammar requires considerable effort and it cannot be
acquired from treebanks using existing approaches. We think that exponential explosion
is inevitable, particularly with the large-scale wide-coverage grammars required to an-
alyze real-world texts. In such cases, these methods of model estimation are intractable.
Another approach to estimating log-linear models for HPSG was to extract a small
informative sample from the original set T(w) (Osborne 2000). The method was suc-
cessfully applied to Dutch HPSG parsing (Malouf and van Noord 2004). A possible
problem with this method is in the approximation of exponentially many parse trees by
a polynomial-size sample. However, their method has an advantage in that any features
on parse results can be incorporated into a model, whereas our method forces feature
functions to be defined locally on conjunctive nodes. We will discuss the trade-off
between the approximation solution and the locality of feature functions in Section 6.3.
Non-probabilistic statistical classifiers have also been applied to disambiguation in
HPSG parsing: voted perceptrons (Baldridge and Osborne 2003) and support vector
machines (Toutanova, Markova, and Manning 2004). However, the problem of expo-
nential explosion is also inevitable using their methods. As described in Section 6.1, an
approach similar to ours may be applied, following the study of Taskar et al. (2004).
A series of studies on parsing with LFG (Johnson et al. 1999; Riezler et al. 2000,
2002) also proposed a maximum entropy model for probabilistic modeling of LFG pars-
ing. However, similarly to the previous studies on HPSG parsing, these groups had
no solution to the problem of exponential explosion of unpacked parse results. As dis-
cussed in Section 6.1, Geman and Johnson (2002) proposed an algorithm for maximum
entropy estimation for packed representations of LFG parses.
Recent studies on CCG have proposed probabilistic models of dependency struc-
tures or predicate–argument dependencies, which are essentially the same as the
predicate–argument structures described in the present article. Clark, Hockenmaier, and
Steedman (2002) attempted the modeling of dependency structures, but the model was
inconsistent because of the violation of the independence assumption. Hockenmaier
(2003) proposed a consistent generative model of predicate–argument structures. The
probability of a non-local dependency was conditioned on multiple words to preserve
the consistency of the probability model; that is, probability p(Ilwant, dispute) in Sec-
tion 4.3 was directly estimated. The problem was that such probabilities could not be
estimated directly from the data due to data sparseness, and a heuristic method had
to be employed. Probabilities were therefore estimated as the average of individual
probabilities conditioned on a single word. Another problem is that the model is no
longer consistent when unification constraints such as those in HPSG are introduced.
</bodyText>
<page confidence="0.995785">
73
</page>
<note confidence="0.594098">
Computational Linguistics Volume 34, Number 1
</note>
<bodyText confidence="0.999835958333333">
Our solution is free of these problems, and is applicable to various grammars, not only
HPSG and CCG.
Most of the state-of-the-art studies on parsing with lexicalized grammars have
adopted feature forest models (Clark and Curran 2003, 2004b; Kaplan et al. 2004; Riezler
and Vasserman 2004). Their methods of translating parse results into feature forests are
basically the same as our method described in Section 4, and details differ because
different grammar theories represent syntactic structures differently. They reported
higher accuracy in parsing the Penn Treebank than the previous methods introduced
herein, and these results attest the effectiveness of feature forest models in practical
deep parsing. A remaining problem is that no studies could provide empirical compar-
isons across grammar theories. The above studies and our research evaluated parsing
accuracy on their own test sets. The construction of theory-independent standard test
sets requires enormous effort because we must establish theory-independent criteria
such as agreed definitions of phrases and headedness. Although this issue is beyond
the scope of the present article, it is a fundamental obstacle to the transparency of these
studies on parsing.
Clark and Curran (2004a) described a method for reducing the cost of parsing a
training treebank without sacrificing accuracy in the context of CCG parsing. They first
assigned each word a small number of supertags, corresponding to lexical entries in
our case, and parsed supertagged sentences. Because they did not use the probabilities of
supertags in a parsing stage, their method corresponds to our “filtering only” method.
The difference from our approach is that they also applied the supertagger in a parsing
stage. We suppose that this was crucial for high accuracy in their approach, although
empirical investigation is necessary.
</bodyText>
<subsectionHeader confidence="0.996408">
6.3 Trade-Off between Dynamic Programming and Feature Locality
</subsectionHeader>
<bodyText confidence="0.999993304347826">
The proposed algorithm is an essential solution to the problem of estimating probabilis-
tic models on exponentially many complete structures. However, the applicability of
this algorithm relies on the constraint that features are defined locally in conjunctive
nodes. As discussed in Section 6.1, this does not necessarily mean that features in our
model can represent only the immediate-dominance relations of CFG rules, because
conjunctive nodes may encode any fragments of complete structures. In fact, we demon-
strated in Section 4.3 that certain assumptions allowed us to encode non-local predicate–
argument dependencies in tractable-size feature forests. In addition, although in the
experiments we used only features on bilexical dependencies, the method described in
Section 4.3 allows us to define any features on a predicate and all of its arguments, such
as a ternary relation among a subject, a verb, and a complement (e.g., the relation among
I, want, and dispute1 in Figure 21), and a generalized relation among semantic classes
of a predicate and its arguments. This is because a predicate and all of its arguments
are included in a conjunctive node, and feature functions can represent any relations
expressed within a conjunctive node.
When we define more global features, such as co-occurrences of structures at distant
places in a sentence, conjunctive nodes must be expanded so that they include all
structures that are necessary to define these features. However, this obviously increases
the number of conjunctive nodes, and consequently, the cost of parameter estimation
increases. In an extreme case, for example, if we define features on any co-occurrences
of partial parse trees, the full unpacking of parse forests would be necessary, and pa-
rameter estimation would be intractable. This indicates that there is a trade-off between
the locality of features and the cost of estimation. That is, larger context features might
</bodyText>
<page confidence="0.995987">
74
</page>
<note confidence="0.866918">
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
</note>
<bodyText confidence="0.999824722222222">
contribute to higher accuracy, while they inflate the size of feature forests and increase
the cost of parameter estimation.
Sampling techniques (Rosenfeld 1997; Chen and Rosenfeld 1999b; Osborne 2000;
Malouf and van Noord 2004) allow us to define any features on complete structures
without any constraints. However, they force us to employ approximation methods
for tractable computation. The effectiveness of those techniques therefore relies on
convergence speed and approximation errors, which may vary depending on the char-
acteristics of target problems and features.
It is an open research question whether dynamic programming or sampling can
deliver a better balance of estimation efficiency and accuracy. The answer will differ in
different problems. When most effective features can be represented locally in tractable-
size feature forests, dynamic programming methods including ours are suitable.
However, when global context features are indispensable for high accuracy, sampling
methods might be better. We should also investigate compromise solutions such as
dynamic CRFs (McCallum, Rohanimanesh, and Sutton 2003; Sutton, Rohanimanesh,
and McCallum 2004) and reranking techniques (Collins 2000; Charniak and Johnson
2005). There is no analytical way of predicting the best solution, and it must be
investigated experimentally for each target task.
</bodyText>
<sectionHeader confidence="0.819271" genericHeader="conclusions">
7. Conclusion
</sectionHeader>
<bodyText confidence="0.9992954">
A dynamic programming algorithm was presented for maximum entropy modeling
and shown to provide a solution to the parameter estimation of probabilistic models of
complete structures without the independence assumption. We first defined the notion
of a feature forest, which is a packed representation of an exponential number of trees of
features. When training data is represented with feature forests, model parameters are
estimated at a tractable cost without unpacking the forests. The method provides a more
flexible modeling scheme than previous methods of application of maximum entropy
models to natural language processing. Furthermore, it is applicable to complex data
structures where an event is difficult to decompose into independent sub-events.
We also demonstrated that feature forest models are applicable to probabilistic mod-
eling of linguistic structures such as the syntactic structures of HPSG and predicate–
argument structures including non-local dependencies. The presented approach can
be regarded as a general solution to the probabilistic modeling of syntactic analysis
with lexicalized grammars. Table 16 summarizes the best performance of the HPSG
parser described in this article. The parser demonstrated impressively high coverage
and accuracy for real-world texts. We therefore conclude that the HPSG parser for
English is moving toward a practical level of use in real-world applications. Recently,
the applicability of the HPSG parser to practical applications, such as information
extraction and retrieval, has also been demonstrated (Miyao et al. 2006; Yakushiji et al.
2006; Chun 2007).
</bodyText>
<tableCaption confidence="0.838885">
Table 16
Final results.
</tableCaption>
<footnote confidence="0.86169825">
Parsing accuracy for Section 23 (&lt;40 words)
# parsed sentences 2,137/2,144 (99.7%)
Precision/recall 87.69%/87.16%
Sentential accuracy 39.2%
</footnote>
<page confidence="0.991569">
75
</page>
<note confidence="0.592602">
Computational Linguistics Volume 34, Number 1
</note>
<bodyText confidence="0.999959111111111">
From our extensive investigation of HPSG parsing, we observed that exploration
of new types of features is indispensable to further improvement of parsing accuracy.
A possible research direction is to encode larger contexts of parse trees, which has
been shown to improve accuracy (Toutanova and Manning 2002; Toutanova, Markova,
and Manning 2004). Future work includes not only the investigation of these features
but also the abstraction of predicate–argument dependencies using semantic classes.
Experimental results also suggest that an improvement in grammar coverage is crucial
for higher accuracy. This indicates that an improvement in the quality of the grammar
is a key factor for the improvement of parsing accuracy.
The feature forest model provides new insight into the relationship between a
linguistic structure and a unit of probability. Traditionally, a unit of probability was
implicitly assumed to correspond to a meaningful linguistic structure; a tagging of a
word or an application of a rewriting rule. One reason for the assumption is to enable
dynamic programming algorithms, such as the Viterbi algorithm. The probability of a
complete structure must be decomposed into atomic structures in which ambiguities
are limited to a tractable size. Another reason is to estimate plausible probabilities.
Because a probability is defined over atomic structures, they should also be meaning-
ful so as to be assigned a probability. In feature forest models, however, conjunctive
nodes are responsible for the former, whereas feature functions are responsible for the
latter. Although feature functions must be defined locally in conjunctive nodes, they
are not necessarily equivalent. Conjunctive nodes may represent any fragments of a
complete structure, which are not necessarily linguistically meaningful. They should
be designed to pack ambiguities and enable us to define useful features. Meanwhile,
feature functions indicate an atomic unit of probability, and are designed to capture
statistical regularity of the target problem. We expect the separation of a unit of prob-
ability from linguistic structures to open up a new framework for flexible probabilistic
modeling.
</bodyText>
<sectionHeader confidence="0.987906" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9995234">
The authors wish to thank the anonymous
reviewers of Computational Linguistics for
their helpful comments and discussions. We
would also like to thank Takashi Ninomiya
and Kenji Sagae for their precious support.
</bodyText>
<sectionHeader confidence="0.998872" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997891166666666">
Abney, Steven P. 1997. Stochastic
attribute-value grammars. Computational
Linguistics, 23(4):597–618.
Baker, James K. 1979. Trainable grammars
for speech recognition. In Jared J. Wolf
and Dennis H. Klatt, editors, Speech
Communication Papers Presented at the 97th
Meeting of the Acoustical Society of America.
MIT Press, Cambridge, MA, pages 547–550.
Baldridge, Jason and Miles Osborne. 2003.
Active learning for HPSG parse selection.
In Proceedings of the Seventh Conference on
Natural Language Learning at HLT-NAACL
2003, pages 17–24, Edmonton, Canada.
Berger, Adam L., Stephen A. Della Pietra, and
Vincent J. Della Pietra. 1996. A maximum
entropy approach to natural language
processing. Computational Linguistics,
22(1):39–71.
Burke, Michael, Aoife Cahill, Ruth
O’Donovan, Josef van Genabith,
and Andy Way. 2004. Treebank-based
acquisition of wide-coverage, probabilistic
LFG resources: Project overview,
results and evaluation. In Proceedings
of the IJCNLP-04 Workshop “Beyond Shallow
Analyses”, Hainan Island. Available
at www-tsujii.is.s.u-tokyo.ac.jp/bsa.
Carpenter, Bob. 1992. The Logic of Typed
Feature Structures. Cambridge University
Press, Cambridge, England.
Carroll, John and Stephan Oepen. 2005.
High efficiency realization for a
wide-coverage unification grammar.
In Proceedings of the 2nd International
Joint Conference on Natural Language
Processing (IJCNLP-05), pages 165–176,
Jeju Island.
Charniak, Eugene. 2000. A maximum-
entropy-inspired parser. In Proceedings
of the First Conference on North American
Chapter of the Association for Computational
</reference>
<page confidence="0.983131">
76
</page>
<note confidence="0.906114">
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
</note>
<reference confidence="0.995256974576271">
Linguistics (NAACL 2000), pages 132–139,
Seattle, WA.
Charniak, Eugene and Mark Johnson.
2005. Coarse-to-fine n-best parsing
and MaxEnt discriminative reranking.
In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics
(ACL 2005), pages 173–180, Ann Arbor, MI.
Chen, Stanley and Ronald Rosenfeld.
1999a. A Gaussian prior for smoothing
maximum entropy models. Technical
Report CMUCS-99-108, Carnegie
Mellon University.
Chen, Stanley F. and Ronald Rosenfeld.
1999b. Efficient sampling and feature
selection in whole sentence maximum
entropy language models. In Proceedings
of the 1999 IEEE International Conference on
Acoustics, Speech, and Signal Processing,
pages 549–552, Phoenix, AZ.
Chiang, David. 2003. Mildly context sensitive
grammars for estimating maximum
entropy parsing models. In Proceedings of
the 8th Conference on Formal Grammar,
pages 19–31, Vienna.
Chun, Hong-Woo. 2007. Mining Literature for
Disease-Gene Relations. Ph.D. thesis,
University of Tokyo.
Clark, Stephen and James R. Curran. 2003.
Log-linear models for wide-coverage
CCG parsing. In Proceedings of the 2003
Conference on Empirical Methods in
Natural Language Processing (EMNLP 2003),
pages 97–104, Sapporo.
Clark, Stephen and James R. Curran.
2004a. The importance of supertagging
for wide-coverage CCG parsing.
In Proceedings of the 20th International
Conference on Computational Linguistics
(COLING 2004), pages 282–288, Geneva.
Clark, Stephen and James R. Curran. 2004b.
Parsing the WSJ using CCG and log-linear
models. In Proceedings of the 42nd Annual
Meeting of the Association for Computational
Linguistics (ACL 2004), pages 104–111,
Barcelona.
Clark, Stephen, Julia Hockenmaier, and
Mark Steedman. 2002. Building deep
dependency structures with a wide-
coverage CCG parser. In Proceedings of the
40th Annual Meeting of the Association for
Computational Linguistics (ACL 2002),
pages 327–334, Philadephia.
Cohen, Paul R. 1995. Empirical Methods for
Artificial Intelligence. The MIT Press,
Cambridge, MA.
Collins, Michael. 1997. Three generative,
lexicalised models for statistical parsing.
In Proceedings of the 35th Annual Meeting
of the Association for Computational
Linguistics (ACL’97), pages 16–23,
Madrid.
Collins, Michael. 1999. Head-Driven Statistical
Models for Natural Language Parsing.
Ph.D. thesis, University of Pennsylvania.
Collins, Michael. 2000. Discriminative
reranking for natural language
parsing. In Proceedings of the Seventeenth
International Conference on Machine
Learning, pages 175–182, Palo Alto, CA.
Collins, Michael. 2003. Head-driven statistical
models for natural language parsing.
Computational Linguistics, 29(4):589–637.
Copestake, Ann, Dan Flickinger,
Rob Malouf, Susanne Riehemann, and
Ivan Sag. 1995. Translation using minimal
recursion semantics. In Proceedings of the
Sixth International Conference on Theoretical
and Methodological Issues in Machine
Translation (TMI95), pages 15–32, Leuven.
Copestake, Ann, Dan Flickinger, Ivan A. Sag,
and Carl Pollard. 2006. Minimal recursion
semantics: An introduction. Research
on Language and Computation, 3(4):281–332.
Darroch, J. N. and D. Ratcliff. 1972.
Generalized iterative scaling for log-linear
models. The Annals of Mathematical
Statistics, 43(5):1470–1480.
Della Pietra, Stephen, Vincent Della Pietra,
and John Lafferty. 1997. Inducing features
of random fields. IEEE Transactions on
Pattern Analysis and Machine Intelligence,
19(4):380–393.
Geman, Stuart and Mark Johnson. 2002.
Dynamic programming for parsing and
estimation of stochastic unification-based
grammars. In Proceedings of the 40th Annual
Meeting of the Association for Computational
Linguistics (ACL 2002), pages 279–286,
Philadelphia, PA.
Gildea, Daniel. 2001. Corpus variation and
parser performance. In Proceedings of the
2001 Conference on Empirical Methods in
Natural Language Processing (EMNLP 2001),
pages 167–202, Pittsburgh, PA.
Hockenmaier, Julia. 2003. Parsing with
generative models of predicate-argument
structure. In Proceedings of the 41st Annual
Meeting of the Association for Computational
Linguistics (ACL 2003), pages 359–366,
Sapporo.
Hockenmaier, Julia and Mark Steedman.
2002. Acquiring compact lexicalized
grammars from a cleaner treebank.
In Proceedings of the Third International
Conference on Language Resources and
Evaluation (LREC-2002), pages 1974–1981,
Las Palmas.
</reference>
<page confidence="0.992001">
77
</page>
<note confidence="0.565431">
Computational Linguistics Volume 34, Number 1
</note>
<reference confidence="0.998407864406779">
Johnson, Mark, Stuart Geman, Stephen
Canon, Zhiyi Chi, and Stefan Riezler.
1999. Estimators for stochastic “unification-
based” grammars. In Proceedings
of the 37th Annual Meeting of the Association
for Computational Linguistics (ACL’99),
pages 535–541, College Park, Maryland.
Johnson, Mark and Stefan Riezler. 2000.
Exploiting auxiliary distributions in
stochastic unification-based grammars.
In Proceedings of the First Conference
on North American Chapter of the
Association for Computational Linguistics,
pages 154–161, Seattle, WA.
Kameya, Yoshitaka and Taisuke Sato.
2000. Efficient EM learning with tabulation
for parameterized logic programs.
In Proceedings of the 1st International
Conference on Computational Logic
(CL2000), volume 1861 of Lecture Notes
in Artificial Intelligence (LNAI),
pages 269–294, Imperial College, London.
Kaplan, Ronald M., Stefan Riezler, Tracy H.
King, John T. Maxwell, III, Alexander
Vasserman, and Richard Crouch.
2004. Speed and accuracy in shallow
and deep stochastic parsing. In Proceedings
of the Human Language Technology
Conference and the North American Chapter
of the Association for Computational
Linguistics (HLT-NAACL 2004),
pages 97–104, Boston, MA.
Lafferty, John, Andrew McCallum, and
Fernando Pereira. 2001. Conditional
random fields: Probabilistic models for
segmenting and labeling sequence data.
In Proceedings of the International Conference
on Machine Learning 2001, pages 282–289,
Williams College, Williamstown, MA.
Makino, Takaki, Yusuke Miyao, Kentaro
Torisawa, and Jun’ichi Tsujii. 2002.
Native-code compilation of feature
structures. In Stephen Oepen, Dan
Flickinger, Jun’ichi Tsujii, and Hans
Uszkoreit, editors, Collaborative
Language Engineering: A Case Study
in Efficient Grammar-based Parsing. CSLI
Publications, Palo Alto, CA, pages 49–80.
Malouf, Robert. 2002. A comparison
of algorithms for maximum entropy
parameter estimation. In Proceedings
of the Sixth Conference on Natural
Language Learning (CoNLL-2002),
pages 1–7, Taipei.
Malouf, Robert and Gertjan van Noord.
2004. Wide coverage parsing with
stochastic attribute value grammars.
In Proceedings of the IJCNLP-04 Workshop
“Beyond Shallow Analyses”, Hainan Island.
Available at www.tsujii.is.s.u-tokyo.
ac.jp/bsa.
Marcus, Mitchell, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre,
Ann Bies, Mark Ferguson, Karen Katz,
and Britta Schasberger. 1994. The Penn
Treebank: Annotating predicate
argument structure. In Proceedings
of the Workshop on Human Language
Technology, pages 114–119, Plainsboro, NJ.
Maxwell John T., III and Ronald M. Kaplan.
1995. A method for disjunctive constraint
satisfaction. In Mary Dalrymple, Ronald
M. Kaplan, John T. Maxwell, III, and
Annie Zaenen, editors, Formal Issues in
Lexical-Functional Grammar, number 47
in CSLI Lecture Notes Series. CSLI
Publications, Palo Alto, CA, chapter 14,
pages 381–481.
McCallum, Andrew and Wei Li. 2003.
Early results for named entity recognition
with conditional random fields, feature
induction and web-enhanced lexicons.
In Proceedings of the 7th Conference
on Natural Language Learning (CoNLL),
pages 188–191, Edmonton.
McCallum, Andrew, Khashayar
Rohanimanesh, and Charles Sutton.
2003. Dynamic conditional random fields
for jointly labeling multiple sequences. In
Proceedings of the Workshop on Syntax,
Semantics, Statistics at the 16th Annual
Conference on Neural Information Processing
Systems, Vancouver. Available at
www.cs.umasse.du/∼mccallum/papers/
derf-nips03.pdf.
Miyao, Yusuke, Takashi Ninomiya, and
Jun’ichi Tsujii. 2003. Probabilistic modeling
of argument structures including
non-local dependencies. In Proceedings
of the International Conference on Recent
Advances in Natural Language Processing
(RANLP 2003), pages 285–291, Borovets.
Miyao, Yusuke, Takashi Ninomiya,
and Jun’ichi Tsujii. 2005. Corpus-oriented
grammar development for acquiring a
head-driven phrase structure grammar
from the Penn Treebank. In Natural
Language Processing - IJCNLP 2004,
pages 684–693, Hainan Island.
Miyao, Yusuke, Tomoko Ohta,
Katsuya Masuda, Yoshimasa Tsuruoka,
Kazuhiro Yoshida, Takashi Ninomiya,
and Jun’ichi Tsujii. 2006. Semantic retrieval
for the accurate identification of relational
concepts in massive textbases. In
Proceedings of the Joint Conference of the 21st
International Conference on Computational
Linguistics and the 44th Annual Meeting of the
</reference>
<page confidence="0.980026">
78
</page>
<note confidence="0.720509">
Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing
</note>
<reference confidence="0.999406093220339">
Association for Computational Linguistics
(COLING-ACL 2006), pages 1017–1024,
Sydney.
Miyao, Yusuke and Jun’ichi Tsujii. 2002.
Maximum entropy estimation for feature
forests. In Proceedings of the Human
Language Technology Conference (HLT-2002),
pages 292–297, San Diego, CA.
Miyao, Yusuke and Jun’ichi Tsujii. 2003.
A model of syntactic disambiguation
based on lexicalized grammars. In
Proceedings of the Seventh Conference on
Computational Natural Language Learning
(CoNLL-2003), pages 1–8, Edmonton.
Miyao, Yusuke and Jun’ichi Tsujii. 2005.
Probabilistic disambiguation models
for wide-coverage HPSG parsing. In
Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics
(ACL 2005), pages 83–90, Ann Arbor, MI.
Nakanishi, Hiroko, Yusuke Miyao, and
Jun’ichi Tsujii. 2005. Probabilistic models
for disambiguation of an HPSG-based
chart generator. In Proceedings of the
9th International Workshop on Parsing
Technologies (IWPT 2005), pages 93–102,
Vancouver.
Ninomiya, Takashi, Yoshimasa Tsuruoka,
Yusuke Miyao, and Jun’ichi Tsujii. 2005.
Efficacy of beam thresholding, unification
filtering and hybrid parsing in probabilistic
HPSG parsing. In Proceedings of the 9th
International Workshop on Parsing
Technologies, pages 103–114, Vancouver.
Nocedal, Jorge. 1980. Updating
quasi-Newton matrices with limited
storage. Mathematics of Computation,
35:773–782.
Nocedal, Jorge and Stephen J. Wright.
1999. Numerical Optimization. Springer,
New York.
Oepen, Stephan and John Carroll. 2000.
Ambiguity packing in constraint-based
parsing: practical results. In Proceedings
of the First Conference of the North American
Chapter of the Association for Computational
Linguistics (NAACL 2000), pages 162–169,
Seattle, WA.
Oepen, Stephan, Dan Flickinger, Jun’ichi
Tsujii, and Hans Uszkoreit, editors. 2002.
Collaborative Language Engineering: A Case
Study in Efficient Grammar-Based Processing.
CSLI Publications, Palo Alto, CA.
Oepen, Stephan, Kristina Toutanova,
Stuart Shieber, Christopher Manning, Dan
Flickinger, and Thorsten Brants. 2002. The
LinGO Redwoods treebank motivation and
preliminary applications. In Proceedings
of the 19th International Conference on
Computational Linguistics (COLING 2002),
volume 2, pages 1–5, Taipei.
Osborne, Miles. 2000. Estimation of
stochastic attribute-value grammar using
an informative sample. In Proceedings
of the 18th International Conference on
Computational Linguistics (COLING 2000),
volume 1, pages 586–592, Saarbr¨ucken.
Peng, Fuchun and Andrew McCallum. 2004.
Accurate information extraction from
research papers using conditional
random fields. In Proceedings of Human
Language Technology Conference and
North American Chapter of the Association
for Computational Linguistics (HLT/
NAACL-04), pages 329–336, Boston, MA.
Pinto, David, Andrew McCallum, Xen Lee,
and W. Bruce Croft. 2003. Table extraction
using conditional random fields. In
Proceedings of the 26th Annual International
ACM SIGIR Conference on Research
and Development in Information Retrieval
(SIGIR 2003), pages 235–242, Toronto.
Pollard, Carl and Ivan A. Sag. 1994.
Head-Driven Phrase Structure Grammar.
University of Chicago Press, Chicago, IL.
Riezler, Stefan, Tracy H. King, Ronald M.
Kaplan, Richard Crouch, John T.
Maxwell, III, and Mark Johnson. 2002.
Parsing the Wall Street Journal using
a lexical-functional grammar and
discriminative estimation techniques.
In Proceedings of the 40th Annual Meeting
of the Association for Computational
Linguistics (ACL 2002), pages 271–278,
Philadephia, PA.
Riezler, Stefan, Detlef Prescher, Jonas Kuhn,
and Mark Johnson. 2000. Lexicalized
stochastic modeling of constraint-based
grammars using log-linear measures
and EM training. In Proceedings of the
38th Annual Meeting of the Association
for Computational Linguistics (ACL 2000),
pages 480–487, Hong Kong.
Riezler, Stefan and Alexander Vasserman.
2004. Incremental feature selection and
l1 regularization for relaxed maximum-
entropy modeling. In Proceedings
of the 2004 Conference on Empirical
Methods in Natural Language Processing
(EMNLP 2004), pages 174–181, Barcelona.
Roark, Brian, Murat Saraclar, Michael Collins,
and Mark Johnson. 2004. Discriminative
language modeling with conditional
random fields and the perceptron
algorithm. In Proceedings of the 42nd Annual
Meeting of the Association for Computational
Linguistics (ACL 2004), pages 47–54,
Barcelona.
</reference>
<page confidence="0.945968">
79
</page>
<reference confidence="0.995036075630252">
Computational Linguistics Volume 34, Number 1
Rosenfeld, Ronald. 1997. A whole sentence
maximum entropy language model. In
Proceedings of the IEEE Workshop on Automatic
Speech Recognition and Understanding,
pages 230–237, Santa Barbara, CA.
Sag, Ivan A., Thomas Wasow, and Emily M.
Bender. 2003. Syntactic Theory: A Formal
Introduction. Number 152 in CSLI Lecture
Notes. CSLI Publications, Standford, CA.
Sarawagi, Sunita and William W. Cohen.
2004. Semi-Markov conditional random
fields for information extraction. In
Proceedings of the 18th Annual Conference
on Neural Information Processing
Systems, pages 1185–1192, Vancouver.
Sato, Taisuke. 2005. A generic approach to em
learning for symbolic-statistical models. In
Proceedings of the 4th Learning Language in
Logic Workshop (LLL05), pages 21–28, Bonn.
Sato, Taisuke and Yoshitaka Kameya.1997.
PRISM: a language for symbolic-statistical
modeling. In Proceedings of the 15th
International Joint Conference on Artificial
Intelligence (IJCAI ’97), pages 1330–1335,
Nagoya.
Sato, Taisuke and Yoshitaka Kameya. 2001.
Parameter learning of logic programs for
symbolic-statistical modeling. Journal of
Artificial Intelligence Research, 15:391–454.
Settles, Burr. 2004. Biomedical named entity
recognition using conditional random fields
and rich feature sets. In Proceedings of the
International Joint Workshop on Natural
Language Processing in Biomedicine and its
Applications (NLPBA), pages 104–107,
Geneva.
Sha, Fei and Fernando Pereira. 2003.
Shallow parsing with conditional
random fields. In Proceedings of the 2003
Human Language Technology Conference and
North American Chapter of the Association
for Computational Linguistics (HLT-NAACL
2003), pages 213–220, Edmonton.
Shieber, Stuart M. 1985. Using restriction
to extend parsing algorithms for
complex-feature-based formalisms. In
Proceedings of the 23rd Annual Meeting
on Association for Computational Linguistics,
pages 145–152, Chicago, IL.
Sutton, Charles, Khashayar Rohanimanesh,
and Andrew McCallum. 2004. Dynamic
conditional random fields: Factorized
probabilistic models for labeling and
segmenting sequence data. In Proceedings
of the 21st International Conference
on Machine Learning (ICML 2004),
pages 783–790, Alberta.
Taskar, Ben, Dan Klein, Michael Collins,
Daphne Koller, and Chris Manning.
2004. Max-margin parsing. In Proceedings
of the 2004 Conference on Empirical
Methods in Natural Language Processing
(EMNLP 2004), pages 1–8, Barcelona.
Toutanova, Kristina and Christopher
Manning. 2002. Feature selection for a
rich HPSG grammar using decision trees.
In Proceedings of the Sixth Conference on
Natural Language Lerning (CoNLL-2002),
pages 77–83, Taipei.
Toutanova, Kristina, Penka Markova,
and Christopher Manning. 2004. The
leaf projection path view of parse trees:
Exploring string kernels for HPSG parse
selection. In Proceedings of the 2004
Conference on Empirical Methods in
Natural Language Processing (EMNLP
2004), pages 166–173, Barcelona.
Tsujii Laboratory. 2004. Enju—A practical
HPSG parser. Available at http://www.
tsujii.is.s.u-tokyo.ac.jp/enju/.
Tsuruoka, Yoshimasa, Yusuke Miyao, and
Jun’ichi Tsujii. 2004. Towards efficient
probabilistic HPSG parsing: Integrating
semantic and syntatic preference
to guide the parsing. In Proceedings
of the IJCNLP-04 Workshop “Beyond Shallow
Analyses”, Hainan Island. Available
at www.tsujii.is.s.u-tokyo.ac.jp/bsa.
Tsuruoka, Yoshimasa and Jun’ichi Tsujii.
2005. Bidirectional inference with the
easiest-first strategy for tagging sequence
data. In Proceedings of Human Language
Technology Conference and Conference on
Empirical Methods in Natural Language
Processing (HLT/EMNLP 2005),
pages 467–474, Vancouver.
Vapnik, Vladimir N. 1995. The Nature of
Statistical Learning Theory. Springer-Verlag,
New York.
Vijay-Shanker, K., David J. Weir, and
Aravind K. Joshi.1987. Characterizing
structural descriptions produced by
various grammatical formalisms. In
Proceedings of the 25th Annual Meeting
of the Association for Computational
Linguistics, pages 104–111, Palo Alto, CA.
Weir, David J. 1988. Characterizing Mildly
Context-Sensitive Grammar Formalisms.
Ph.D. thesis, University of Pennsylvania.
Yakushiji, Akane, Yusuke Miyao,
Tomoko Ohta, Yuka Tateisi, and Jun’ichi
Tsujii. 2006. Automatic construction of
predicate-argument structure patterns for
biomedical information extraction.
In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language
Processing (EMNLP 2006), pages 284–292,
Sydney.
</reference>
<page confidence="0.998243">
80
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.598792">
<title confidence="0.99932">Feature Forest Models for Probabilistic</title>
<author confidence="0.643696">HPSG Parsing</author>
<affiliation confidence="0.999309333333333">University of Tokyo University of Tokyo University of Manchester</affiliation>
<abstract confidence="0.996548095238095">Probabilistic modeling of lexicalized grammars is difficult because these grammars exploit complicated data structures, such as typed feature structures. This prevents us from applying common methods of probabilistic modeling in which a complete structure is divided into substructures under the assumption of statistical independence among sub-structures. For example, part-of-speech tagging of a sentence is decomposed into tagging of each word, and CFG parsing is split into applications of CFG rules. These methods have relied on the structure of the target problem, namely lattices or trees, and cannot be applied to graph structures including typed feature structures. This article proposes the feature forest model as a solution to the problem of probabilistic modeling of complex data structures including typed feature structures. The feature forest model provides a method for probabilistic modeling without the independence assumption when probabilistic events are represented with feature forests. Feature forests are generic data structures that represent ambiguous trees in a packed forest structure. Feature forest models are maximum entropy models defined over feature forests. A dynamic programming algorithm is proposed for maximum entropy estimation without unpacking feature forests. Thus probabilistic modeling of any data structures is possible when they are represented by feature forests. This article also describes methods for representing HPSG syntactic structures and predicate–argument structures with feature forests. Hence, we describe a complete strategy for developing probabilistic models for HPSG parsing. The effectiveness of the proposed methods is empirically evaluated through parsing experiments on the Penn Treebank, and the promise of applicability to parsing of real-world sentences is discussed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven P Abney</author>
</authors>
<title>Stochastic attribute-value grammars.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>4</issue>
<contexts>
<context position="7164" citStr="Abney (1997)" startWordPosition="1010" endWordPosition="1011">ammars (Johnson et al. 1999; Riezler et al. 2000, 2002; Geman and Johnson 2002; Clark and Curran 2003, 2004b; Kaplan et al. 2004; Carroll and Oepen 2005). Previous studies on probabilistic models for HPSG (Oepen, Toutanova et al. 2002; Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van Noord 2004) have also adopted log-linear models. This is because these grammar formalisms exploit feature structures to represent linguistic constraints. Such constraints are known to introduce inconsistencies in probabilistic models estimated using simple relative frequency, as discussed in Abney (1997). The maximum entropy model is a reasonable choice for credible probabilistic models. It also allows various overlapping features to be incorporated, and we can expect higher accuracy in disambiguation. A maximum entropy model gives a probabilistic distribution that maximizes the likelihood of training data under given feature functions. Given training data E = {(x, y)}, a maximum entropy model gives conditional probability p(y|x) as follows. Definition 1(Maximum entropy model) A maximum entropy model is defined as the solution of the following optimization problem. pM(y|x) = argmax { E� ˜p(x,</context>
</contexts>
<marker>Abney, 1997</marker>
<rawString>Abney, Steven P. 1997. Stochastic attribute-value grammars. Computational Linguistics, 23(4):597–618.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James K Baker</author>
</authors>
<title>Trainable grammars for speech recognition. In</title>
<date>1979</date>
<booktitle>Speech Communication Papers Presented at the 97th Meeting of the Acoustical Society of America.</booktitle>
<pages>547--550</pages>
<editor>Jared J. Wolf and Dennis H. Klatt, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA,</location>
<contexts>
<context position="10757" citStr="Baker 1979" startWordPosition="1579" endWordPosition="1580"> Eλjfj(x,y) (1) yEY(x) j As shown in this definition, the computation of model expectation requires the summation over Y(x) for every x in the training data. The complexity of the overall estimation algorithm is O( ˜|Y |˜|F||E|), where ˜|Y |and ˜|F |are the average numbers of y and activated features for an event, respectively, and |E |is the number of events. When Y(x) grows exponentially, the parameter estimation becomes intractable. In PCFGs, the problem of computing probabilities of parse trees is avoided by using a dynamic programming algorithm for computing inside/outside probabilities (Baker 1979). With the algorithm, the computation becomes tractable. We can expect that the same approach would be effective for maximum entropy models as well. This notion yields a novel algorithm for parameter estimation for maximum entropy models, as described in the next section. 3. Feature Forest Model Our solution to the problem is a dynamic programming algorithm for computing inside/outside α-products. Inside/outside α-products roughly correspond to inside/ outside probabilities in PCFGs. In maximum entropy models, a probability is defined as a normalized product of αfj j(= exp(λjfj)). Hence, simil</context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>Baker, James K. 1979. Trainable grammars for speech recognition. In Jared J. Wolf and Dennis H. Klatt, editors, Speech Communication Papers Presented at the 97th Meeting of the Acoustical Society of America. MIT Press, Cambridge, MA, pages 547–550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Baldridge</author>
<author>Miles Osborne</author>
</authors>
<title>Active learning for HPSG parse selection.</title>
<date>2003</date>
<booktitle>In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL</booktitle>
<pages>17--24</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="3296" citStr="Baldridge and Osborne 2003" startWordPosition="449" endWordPosition="452">, statistical modeling of these grammars is attracting considerable attention. This is because natural language processing applications usually require disambiguated or ranked parse results, and statistical modeling of syntactic/semantic preference is one of the most promising methods for disambiguation. The focus of this article is the problem of probabilistic modeling of wide-coverage HPSG parsing. Although previous studies have proposed maximum entropy models (Berger, Della Pietra, and Della Pietra 1996) of HPSG-style parse trees (Oepen, Toutanova, et al. 2002b; Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van Noord 2004), the straightforward application of maximum entropy models to wide-coverage HPSG parsing is infeasible because estimation of maximum entropy models is computationally expensive, especially when targeting wide-coverage parsing. In general, complete structures, such as transition sequences in Markov models and parse trees, have an exponential number of ambiguities. This causes an exponential explosion when estimating the parameters of maximum entropy models. We therefore require solutions to make model estimation tractable. This article first proposes feature forest </context>
<context position="6842" citStr="Baldridge and Osborne 2003" startWordPosition="965" endWordPosition="968"> 6 introduces research related to our proposals. Section 7 concludes. 2. Problem Maximum entropy models (Berger, Della Pietra, and Della Pietra 1996) are now becoming the de facto standard approach for disambiguation models for lexicalized or 36 Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing feature structure grammars (Johnson et al. 1999; Riezler et al. 2000, 2002; Geman and Johnson 2002; Clark and Curran 2003, 2004b; Kaplan et al. 2004; Carroll and Oepen 2005). Previous studies on probabilistic models for HPSG (Oepen, Toutanova et al. 2002; Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van Noord 2004) have also adopted log-linear models. This is because these grammar formalisms exploit feature structures to represent linguistic constraints. Such constraints are known to introduce inconsistencies in probabilistic models estimated using simple relative frequency, as discussed in Abney (1997). The maximum entropy model is a reasonable choice for credible probabilistic models. It also allows various overlapping features to be incorporated, and we can expect higher accuracy in disambiguation. A maximum entropy model gives a probabilistic distribution that maximizes t</context>
<context position="27646" citStr="Baldridge and Osborne 2003" startWordPosition="4464" endWordPosition="4467">|D |are the average numbers of conjunctive and disjunctive nodes, respectively. This is tractable when ˜|C |and ˜|D |are of a reasonable size. As noted in this section, the number of 48 Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing nodes in a feature forest is usually polynomial even when that of the unpacked trees is exponential. Thus we can efficiently compute model expectations with polynomial computational complexity. 4. Probabilistic HPSG Parsing Following previous studies on probabilistic models for HPSG (Oepen, Toutanova, et al. 2002; Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van Noord 2004), we apply a maximum entropy model to HPSG parse disambiguation. The probability, p(t|w), of producing parse result t of a given sentence w is defined as p(t|w) =2 p0(t|w)exp Eλifi(t,w) w i where �E � p0(t�|w) exp λi fi(t�, w) EZw = t�∈T(w) i where p0(t|w) is a reference distribution (usually assumed to be a uniform distribution) and T(w) is a set of parse candidates assigned to w. The feature function fi(t,w) represents the characteristics of t and w, and the corresponding model parameter λi is its weight. Model parameters that maximize the log-likelihood of the tr</context>
<context position="81770" citStr="Baldridge and Osborne 2003" startWordPosition="13018" endWordPosition="13021">cussion in this article, it is evident that if the main part of an update formula is represented 72 Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing with (the exponential of) linear combinations, a method similar to feature forest models should be applicable. 6.2 Probabilistic Parsing with Lexicalized Grammars Before the advent of feature forest models, studies on probabilistic models of HPSG adopted conventional maximum entropy models to select the most probable parse from parse candidates given by HPSG grammars (Oepen, Toutanova, et al. 2002; Toutanova and Manning 2002; Baldridge and Osborne 2003). The difference between these studies and our work is that we used feature forests to avoid the exponential increase in the number of structures that results from unpacked parse results. These studies ignored the problem of exponential explosion; in fact, training sets in these studies were very small and consisted only of short sentences. A possible approach to avoid this problem is to develop a fully restrictive grammar that never causes an exponential explosion, although the development of such a grammar requires considerable effort and it cannot be acquired from treebanks using existing a</context>
<context position="83410" citStr="Baldridge and Osborne 2003" startWordPosition="13266" endWordPosition="13269">SG parsing (Malouf and van Noord 2004). A possible problem with this method is in the approximation of exponentially many parse trees by a polynomial-size sample. However, their method has an advantage in that any features on parse results can be incorporated into a model, whereas our method forces feature functions to be defined locally on conjunctive nodes. We will discuss the trade-off between the approximation solution and the locality of feature functions in Section 6.3. Non-probabilistic statistical classifiers have also been applied to disambiguation in HPSG parsing: voted perceptrons (Baldridge and Osborne 2003) and support vector machines (Toutanova, Markova, and Manning 2004). However, the problem of exponential explosion is also inevitable using their methods. As described in Section 6.1, an approach similar to ours may be applied, following the study of Taskar et al. (2004). A series of studies on parsing with LFG (Johnson et al. 1999; Riezler et al. 2000, 2002) also proposed a maximum entropy model for probabilistic modeling of LFG parsing. However, similarly to the previous studies on HPSG parsing, these groups had no solution to the problem of exponential explosion of unpacked parse results. A</context>
</contexts>
<marker>Baldridge, Osborne, 2003</marker>
<rawString>Baldridge, Jason and Miles Osborne. 2003. Active learning for HPSG parse selection. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages 17–24, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Berger, Adam L., Stephen A. Della Pietra, and Vincent J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Burke</author>
<author>Aoife Cahill</author>
<author>Ruth O’Donovan</author>
<author>Josef van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Treebank-based acquisition of wide-coverage, probabilistic LFG resources: Project overview, results and evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the IJCNLP-04 Workshop “Beyond Shallow Analyses”,</booktitle>
<location>Hainan</location>
<marker>Burke, Cahill, O’Donovan, van Genabith, Way, 2004</marker>
<rawString>Burke, Michael, Aoife Cahill, Ruth O’Donovan, Josef van Genabith, and Andy Way. 2004. Treebank-based acquisition of wide-coverage, probabilistic LFG resources: Project overview, results and evaluation. In Proceedings of the IJCNLP-04 Workshop “Beyond Shallow Analyses”, Hainan Island. Available at www-tsujii.is.s.u-tokyo.ac.jp/bsa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Carpenter</author>
</authors>
<title>The Logic of Typed Feature Structures.</title>
<date>1992</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, England.</location>
<contexts>
<context position="29554" citStr="Carpenter 1992" startWordPosition="4780" endWordPosition="4781">by feature forests. Together with the parameter estimation algorithm in Section 3, these methods constitute a complete method for probabilistic disambiguation. We also address a method for accelerating the construction of feature forests for all treebank sentences in Section 4.4. The design of feature functions will be given in Section 4.5. 4.1 HPSG HPSG (Pollard and Sag 1994; Sag, Wasow, and Bender 2003) is a syntactic theory that follows the lexicalist framework. In HPSG, linguistic entities, such as words and phrases, are denoted by signs, which are represented by typed feature structures (Carpenter 1992). Signs are a formal representation of combinations of phonological forms and syntactic/semantic structures, and express which phonological form signifies which syntactic/semantic structure. Figure 10 shows the lexical sign for loves. The geometry of signs follows Pollard and Sag: HEAD represents the part-of-speech of the head word, MOD denotes modifiee constraints, and SPR, SUBJ, and COMPS describe constraints of a specifier, a syntactic subject, and complements, respectively. CONT denotes the 49 Computational Linguistics Volume 34, Number 1 Figure 10 Lexical entry for the transitive verb lov</context>
<context position="41969" citStr="Carpenter 1992" startWordPosition="6728" endWordPosition="6729">ted to dispute, the object of dispute2 is filled by fact (γ in Figure 16), and the predicate–argument structure of dispute2 is then placed into a conjunctive node (γ&apos; in Figure 18). 7 In Figure 16, feature structures of different nodes of parse trees are assigned distinct variables, even when they are from the same lexical entries. This is because feature structures are copied during chart parsing. Although these variables are from the same lexical entry, it is copied to several chart items, and hence there are no structure sharings among them. 8 ⊥ (bottom) represents an uninstantiated value (Carpenter 1992). 55 Computational Linguistics Volume 34, Number 1 Figure 18 A feature forest representation of predicate–argument structures. One of the beneficial characteristics of this packed representation is that the representation is isomorphic to the parsing process, that is, a chart. Hence, we can assign features of HPSG parse trees to a conjunctive node, together with features of predicate– argument structures. In Section 5, we will investigate the contribution of features on parse trees and predicate–argument structures to the disambiguation of HPSG parsing. 4.4 Filtering by Preliminary Distributio</context>
</contexts>
<marker>Carpenter, 1992</marker>
<rawString>Carpenter, Bob. 1992. The Logic of Typed Feature Structures. Cambridge University Press, Cambridge, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Stephan Oepen</author>
</authors>
<title>High efficiency realization for a wide-coverage unification grammar.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2nd International Joint Conference on Natural Language Processing (IJCNLP-05),</booktitle>
<pages>165--176</pages>
<location>Jeju Island.</location>
<contexts>
<context position="6705" citStr="Carroll and Oepen 2005" startWordPosition="945" endWordPosition="948">ure forest models to probabilistic HPSG parsing. Section 5 presents an empirical evaluation of probabilistic HPSG parsing, and Section 6 introduces research related to our proposals. Section 7 concludes. 2. Problem Maximum entropy models (Berger, Della Pietra, and Della Pietra 1996) are now becoming the de facto standard approach for disambiguation models for lexicalized or 36 Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing feature structure grammars (Johnson et al. 1999; Riezler et al. 2000, 2002; Geman and Johnson 2002; Clark and Curran 2003, 2004b; Kaplan et al. 2004; Carroll and Oepen 2005). Previous studies on probabilistic models for HPSG (Oepen, Toutanova et al. 2002; Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van Noord 2004) have also adopted log-linear models. This is because these grammar formalisms exploit feature structures to represent linguistic constraints. Such constraints are known to introduce inconsistencies in probabilistic models estimated using simple relative frequency, as discussed in Abney (1997). The maximum entropy model is a reasonable choice for credible probabilistic models. It also allows various overlapping features to be incor</context>
</contexts>
<marker>Carroll, Oepen, 2005</marker>
<rawString>Carroll, John and Stephan Oepen. 2005. High efficiency realization for a wide-coverage unification grammar. In Proceedings of the 2nd International Joint Conference on Natural Language Processing (IJCNLP-05), pages 165–176, Jeju Island.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximumentropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the First Conference on North American Chapter of the Association for Computational Linguistics (NAACL</booktitle>
<pages>132--139</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="53723" citStr="Charniak 2000" startWordPosition="8762" endWordPosition="8763">bank derived from Sections 02–21 of the Wall Street Journal portion of the Penn Treebank, that is, the same set used for lexicon extraction. For training of the disambiguation models, we eliminated sentences of 40 words or more and sentences for which the parser could not produce the correct parses. The resulting training set consists of 33,604 sentences (when n = 10 and c = 0.95; see Section 5.4 for details). The treebanks derived from Sections 22 and 23 were used as the development and final test sets, respectively. Following previous studies on parsing with PCFG-based models (Collins 1997; Charniak 2000), accuracy is measured for sentences of less than 40 words and for those with less than 100 words. Table 5 shows the specifications of the test data. The measure for evaluating parsing accuracy is precision/recall of predicate– argument dependencies output by the parser. A predicate–argument dependency is defined as a tuple (wh,wn,7t, p), where wh is the head word of the predicate, wn is the head word of the argument, 7t is the type of the predicate (e.g., adjective, intransitive verb), and p is an argument label (MODARG, ARG1, ..., ARG4). For example, He tried running has three dependencies a</context>
<context position="60147" citStr="Charniak 2000" startWordPosition="9754" endWordPosition="9755">, but the features used in this work were not sufficient to capture semantic preferences. A possible reason is that, as reported in Gildea (2001), bilexical dependencies may be too sparse to capture semantic preferences. For reference, our results are competitive with the best corresponding results reported in CCG parsing (LP/LR = 86.6/86.3) (Clark and Curran 2004b), although our results cannot be compared directly with other grammar formalisms because each formalism represents predicate–argument dependencies differently. In contrast with the results of CCG and PCFG (Collins 1997, 1999, 2003; Charniak 2000), the recall is clearly lower than precision. This may have resulted from the HPSG grammar having stricter feature constraints and the parser not being able to produce parse results for around 1% of the sentences. To improve recall, we need techniques to deal with these 1% of sentences. Table 8 gives the computation/space costs of model estimation. “Estimation time” indicates user times required for running the parameter estimation algorithm. “No. of feature occurrences” denotes the total number of occurrences of features in the training data, and “Data size” gives the sizes of the compressed </context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Charniak, Eugene. 2000. A maximumentropy-inspired parser. In Proceedings of the First Conference on North American Chapter of the Association for Computational Linguistics (NAACL 2000), pages 132–139, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-to-fine n-best parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>173--180</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="90487" citStr="Charniak and Johnson 2005" startWordPosition="14326" endWordPosition="14329">ether dynamic programming or sampling can deliver a better balance of estimation efficiency and accuracy. The answer will differ in different problems. When most effective features can be represented locally in tractablesize feature forests, dynamic programming methods including ours are suitable. However, when global context features are indispensable for high accuracy, sampling methods might be better. We should also investigate compromise solutions such as dynamic CRFs (McCallum, Rohanimanesh, and Sutton 2003; Sutton, Rohanimanesh, and McCallum 2004) and reranking techniques (Collins 2000; Charniak and Johnson 2005). There is no analytical way of predicting the best solution, and it must be investigated experimentally for each target task. 7. Conclusion A dynamic programming algorithm was presented for maximum entropy modeling and shown to provide a solution to the parameter estimation of probabilistic models of complete structures without the independence assumption. We first defined the notion of a feature forest, which is a packed representation of an exponential number of trees of features. When training data is represented with feature forests, model parameters are estimated at a tractable cost with</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Charniak, Eugene and Mark Johnson. 2005. Coarse-to-fine n-best parsing and MaxEnt discriminative reranking. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL 2005), pages 173–180, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Chen</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>A Gaussian prior for smoothing maximum entropy models.</title>
<date>1999</date>
<tech>Technical Report CMUCS-99-108,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="55850" citStr="Chen and Rosenfeld 1999" startWordPosition="9094" endWordPosition="9097">tudies measuring the accuracy of predicate–argument dependencies in CCG parsing (Clark, Hockenmaier, and Steedman 2002; Hockenmaier 2003; Clark and Curran 2004b) and LFG parsing (Burke et al. 2004), although exact figures cannot be compared directly because the definitions of dependencies are different. All predicate–argument dependencies in a sentence are the target of evaluation except quotation marks and periods. The accuracy is measured by parsing test sentences with gold-standard part-of-speech tags from the Penn Treebank unless otherwise noted. The Gaussian prior was used for smoothing (Chen and Rosenfeld 1999a), and its hyper-parameter was tuned for each model to maximize F-score for the development set. The algorithm for parameter estimation was the limited-memory BFGS method (Nocedal 1980; Nocedal and Wright 1999). The parser was implemented in C++ with the LiLFeS library (Makino et al. 2002), and various speed-up techniques for HPSG parsing were used such as quick check and iterative beam search (Tsuruoka, Miyao, and Tsujii 2004; Ninomiya et al. 2005). Other efficient parsing techniques, including global thresholding, hybrid parsing with a chunk parser, and large constituent inhibition, were no</context>
<context position="76153" citStr="Chen and Rosenfeld 1999" startWordPosition="12179" endWordPosition="12182"> approach, although their method was designed to traverse LFG parse results represented with disjunctive feature structures as proposed by Maxwell and Kaplan (1995). The difference between the two approaches is that feature forests use a simpler generic data structure to represent packed forest structures. Therefore, without assuming what feature forests represent, our algorithm can be applied to various tasks, including theirs. Another approach to the probabilistic modeling of complete structures is a method of approximation. The work on whole sentence maximum entropy models (Rosenfeld 1997; Chen and Rosenfeld 1999b) proposed an approximation algorithm to estimate parameters of maximum entropy models on whole sentence structures. However, the algorithm suffered from slow convergence, and the model was basically a sequence model. It could not produce a solution for complex structures as our model can. We should also mention Conditional Random Fields (CRFs) (Lafferty, McCallum, and Pereira 2001) for solving a similar problem in the context of maximum entropy Markov models. Their solution was an algorithm similar to the computation of forward/backward probabilities of hidden Markov models (HMMs). Their alg</context>
<context position="89438" citStr="Chen and Rosenfeld 1999" startWordPosition="14178" endWordPosition="14181">timation increases. In an extreme case, for example, if we define features on any co-occurrences of partial parse trees, the full unpacking of parse forests would be necessary, and parameter estimation would be intractable. This indicates that there is a trade-off between the locality of features and the cost of estimation. That is, larger context features might 74 Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing contribute to higher accuracy, while they inflate the size of feature forests and increase the cost of parameter estimation. Sampling techniques (Rosenfeld 1997; Chen and Rosenfeld 1999b; Osborne 2000; Malouf and van Noord 2004) allow us to define any features on complete structures without any constraints. However, they force us to employ approximation methods for tractable computation. The effectiveness of those techniques therefore relies on convergence speed and approximation errors, which may vary depending on the characteristics of target problems and features. It is an open research question whether dynamic programming or sampling can deliver a better balance of estimation efficiency and accuracy. The answer will differ in different problems. When most effective featu</context>
</contexts>
<marker>Chen, Rosenfeld, 1999</marker>
<rawString>Chen, Stanley and Ronald Rosenfeld. 1999a. A Gaussian prior for smoothing maximum entropy models. Technical Report CMUCS-99-108, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>Efficient sampling and feature selection in whole sentence maximum entropy language models.</title>
<date>1999</date>
<booktitle>In Proceedings of the 1999 IEEE International Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>549--552</pages>
<location>Phoenix, AZ.</location>
<contexts>
<context position="55850" citStr="Chen and Rosenfeld 1999" startWordPosition="9094" endWordPosition="9097">tudies measuring the accuracy of predicate–argument dependencies in CCG parsing (Clark, Hockenmaier, and Steedman 2002; Hockenmaier 2003; Clark and Curran 2004b) and LFG parsing (Burke et al. 2004), although exact figures cannot be compared directly because the definitions of dependencies are different. All predicate–argument dependencies in a sentence are the target of evaluation except quotation marks and periods. The accuracy is measured by parsing test sentences with gold-standard part-of-speech tags from the Penn Treebank unless otherwise noted. The Gaussian prior was used for smoothing (Chen and Rosenfeld 1999a), and its hyper-parameter was tuned for each model to maximize F-score for the development set. The algorithm for parameter estimation was the limited-memory BFGS method (Nocedal 1980; Nocedal and Wright 1999). The parser was implemented in C++ with the LiLFeS library (Makino et al. 2002), and various speed-up techniques for HPSG parsing were used such as quick check and iterative beam search (Tsuruoka, Miyao, and Tsujii 2004; Ninomiya et al. 2005). Other efficient parsing techniques, including global thresholding, hybrid parsing with a chunk parser, and large constituent inhibition, were no</context>
<context position="76153" citStr="Chen and Rosenfeld 1999" startWordPosition="12179" endWordPosition="12182"> approach, although their method was designed to traverse LFG parse results represented with disjunctive feature structures as proposed by Maxwell and Kaplan (1995). The difference between the two approaches is that feature forests use a simpler generic data structure to represent packed forest structures. Therefore, without assuming what feature forests represent, our algorithm can be applied to various tasks, including theirs. Another approach to the probabilistic modeling of complete structures is a method of approximation. The work on whole sentence maximum entropy models (Rosenfeld 1997; Chen and Rosenfeld 1999b) proposed an approximation algorithm to estimate parameters of maximum entropy models on whole sentence structures. However, the algorithm suffered from slow convergence, and the model was basically a sequence model. It could not produce a solution for complex structures as our model can. We should also mention Conditional Random Fields (CRFs) (Lafferty, McCallum, and Pereira 2001) for solving a similar problem in the context of maximum entropy Markov models. Their solution was an algorithm similar to the computation of forward/backward probabilities of hidden Markov models (HMMs). Their alg</context>
<context position="89438" citStr="Chen and Rosenfeld 1999" startWordPosition="14178" endWordPosition="14181">timation increases. In an extreme case, for example, if we define features on any co-occurrences of partial parse trees, the full unpacking of parse forests would be necessary, and parameter estimation would be intractable. This indicates that there is a trade-off between the locality of features and the cost of estimation. That is, larger context features might 74 Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing contribute to higher accuracy, while they inflate the size of feature forests and increase the cost of parameter estimation. Sampling techniques (Rosenfeld 1997; Chen and Rosenfeld 1999b; Osborne 2000; Malouf and van Noord 2004) allow us to define any features on complete structures without any constraints. However, they force us to employ approximation methods for tractable computation. The effectiveness of those techniques therefore relies on convergence speed and approximation errors, which may vary depending on the characteristics of target problems and features. It is an open research question whether dynamic programming or sampling can deliver a better balance of estimation efficiency and accuracy. The answer will differ in different problems. When most effective featu</context>
</contexts>
<marker>Chen, Rosenfeld, 1999</marker>
<rawString>Chen, Stanley F. and Ronald Rosenfeld. 1999b. Efficient sampling and feature selection in whole sentence maximum entropy language models. In Proceedings of the 1999 IEEE International Conference on Acoustics, Speech, and Signal Processing, pages 549–552, Phoenix, AZ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Mildly context sensitive grammars for estimating maximum entropy parsing models.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th Conference on Formal Grammar,</booktitle>
<pages>19--31</pages>
<location>Vienna.</location>
<contexts>
<context position="78914" citStr="Chiang (2003)" startWordPosition="12599" endWordPosition="12600">ature forest models, that is, the fact that feature functions are localized to conjunctive nodes. The structure of feature forests is common in natural language processing and computational linguistics. As is easily seen, lattices, Markov chains, and CFG parse trees are represented by feature forests. Furthermore, because conjunctive nodes do not necessarily represent CFG nodes or rules and terminals of feature forests need not be words, feature forests can express any forest structures in which ambiguities are packed in local structures. Examples include the derivation trees of LTAG and CCG. Chiang (2003) proved that feature forests could be considered as the derivation forests of linear context-free rewriting systems (LCFRSs) (Vijay-Shanker, Weir, and Joshi 1987; Weir 1988). LCFRSs define a wide variety of grammars, including LTAG and CCG, while preserving polynomial-time complexity of parsing. This demonstrates that feature forest models are applicable to probabilistic models far beyond PCFGs. Feature forests are also isomorphic to support graphs (or explanation graphs) used in the graphical EM algorithm (Kameya and Sato 2000). In their framework, a program in a logic programming language, P</context>
</contexts>
<marker>Chiang, 2003</marker>
<rawString>Chiang, David. 2003. Mildly context sensitive grammars for estimating maximum entropy parsing models. In Proceedings of the 8th Conference on Formal Grammar, pages 19–31, Vienna.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong-Woo Chun</author>
</authors>
<title>Mining Literature for Disease-Gene Relations.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Tokyo.</institution>
<contexts>
<context position="92250" citStr="Chun 2007" startWordPosition="14586" endWordPosition="14587">e regarded as a general solution to the probabilistic modeling of syntactic analysis with lexicalized grammars. Table 16 summarizes the best performance of the HPSG parser described in this article. The parser demonstrated impressively high coverage and accuracy for real-world texts. We therefore conclude that the HPSG parser for English is moving toward a practical level of use in real-world applications. Recently, the applicability of the HPSG parser to practical applications, such as information extraction and retrieval, has also been demonstrated (Miyao et al. 2006; Yakushiji et al. 2006; Chun 2007). Table 16 Final results. Parsing accuracy for Section 23 (&lt;40 words) # parsed sentences 2,137/2,144 (99.7%) Precision/recall 87.69%/87.16% Sentential accuracy 39.2% 75 Computational Linguistics Volume 34, Number 1 From our extensive investigation of HPSG parsing, we observed that exploration of new types of features is indispensable to further improvement of parsing accuracy. A possible research direction is to encode larger contexts of parse trees, which has been shown to improve accuracy (Toutanova and Manning 2002; Toutanova, Markova, and Manning 2004). Future work includes not only the in</context>
</contexts>
<marker>Chun, 2007</marker>
<rawString>Chun, Hong-Woo. 2007. Mining Literature for Disease-Gene Relations. Ph.D. thesis, University of Tokyo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Log-linear models for wide-coverage CCG parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>97--104</pages>
<location>Sapporo.</location>
<contexts>
<context position="6653" citStr="Clark and Curran 2003" startWordPosition="936" endWordPosition="939">oblem. Section 4 describes the application of feature forest models to probabilistic HPSG parsing. Section 5 presents an empirical evaluation of probabilistic HPSG parsing, and Section 6 introduces research related to our proposals. Section 7 concludes. 2. Problem Maximum entropy models (Berger, Della Pietra, and Della Pietra 1996) are now becoming the de facto standard approach for disambiguation models for lexicalized or 36 Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing feature structure grammars (Johnson et al. 1999; Riezler et al. 2000, 2002; Geman and Johnson 2002; Clark and Curran 2003, 2004b; Kaplan et al. 2004; Carroll and Oepen 2005). Previous studies on probabilistic models for HPSG (Oepen, Toutanova et al. 2002; Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van Noord 2004) have also adopted log-linear models. This is because these grammar formalisms exploit feature structures to represent linguistic constraints. Such constraints are known to introduce inconsistencies in probabilistic models estimated using simple relative frequency, as discussed in Abney (1997). The maximum entropy model is a reasonable choice for credible probabilistic models. It </context>
<context position="74668" citStr="Clark and Curran 2003" startWordPosition="11965" endWordPosition="11968">ifficult problem. However, we found that most were indirectly caused by errors of argument/modifier distinction in to-infinitive clauses. A significant portion of the errors discussed above cannot be resolved by the features we investigated in this study, and the design of other features will be necessary for improving parsing accuracy. 6. Discussion 6.1 Probabilistic Modeling of Complete Structures The model described in this article was first published in Miyao and Tsujii (2002), and has been applied to probabilistic models for parsing with lexicalized grammars. Applications to CCG parsing (Clark and Curran 2003, 2004b) and LFG parsing (Kaplan et al. 2004; Riezler and Vasserman 2004) demonstrated that feature forest models attained higher accuracy than other models. These researchers applied feature forests to representations of the packed parse results of LFG and the dependency/derivation structures of CCG. Their work demonstrated the applicability and effectiveness of feature forest models in parsing with wide-coverage lexicalized grammars. Feature forest models were also shown to be effective for wide-coverage sentence realization (Nakanishi, Miyao, and Tsujii 2005). This work demonstrated that fe</context>
<context position="85528" citStr="Clark and Curran 2003" startWordPosition="13585" endWordPosition="13588">e estimated directly from the data due to data sparseness, and a heuristic method had to be employed. Probabilities were therefore estimated as the average of individual probabilities conditioned on a single word. Another problem is that the model is no longer consistent when unification constraints such as those in HPSG are introduced. 73 Computational Linguistics Volume 34, Number 1 Our solution is free of these problems, and is applicable to various grammars, not only HPSG and CCG. Most of the state-of-the-art studies on parsing with lexicalized grammars have adopted feature forest models (Clark and Curran 2003, 2004b; Kaplan et al. 2004; Riezler and Vasserman 2004). Their methods of translating parse results into feature forests are basically the same as our method described in Section 4, and details differ because different grammar theories represent syntactic structures differently. They reported higher accuracy in parsing the Penn Treebank than the previous methods introduced herein, and these results attest the effectiveness of feature forest models in practical deep parsing. A remaining problem is that no studies could provide empirical comparisons across grammar theories. The above studies an</context>
</contexts>
<marker>Clark, Curran, 2003</marker>
<rawString>Clark, Stephen and James R. Curran. 2003. Log-linear models for wide-coverage CCG parsing. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing (EMNLP 2003), pages 97–104, Sapporo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>The importance of supertagging for wide-coverage CCG parsing.</title>
<date>2004</date>
<contexts>
<context position="55386" citStr="Clark and Curran 2004" startWordPosition="9026" endWordPosition="9029">, transitive verb, ARG2) • (running, he, intransitive verb, ARG1) Labeled precision/recall (LP/LR) is the ratio of tuples correctly identified by the parser, and unlabeled precision/recall (UP/UR) is the ratio of wh and wn correctly identified regardless of π and p. F-score is the harmonic mean of LP and LR. Sentence accuracy is the exact match accuracy of complete predicate–argument relations in a sentence. These measures correspond to those used in other studies measuring the accuracy of predicate–argument dependencies in CCG parsing (Clark, Hockenmaier, and Steedman 2002; Hockenmaier 2003; Clark and Curran 2004b) and LFG parsing (Burke et al. 2004), although exact figures cannot be compared directly because the definitions of dependencies are different. All predicate–argument dependencies in a sentence are the target of evaluation except quotation marks and periods. The accuracy is measured by parsing test sentences with gold-standard part-of-speech tags from the Penn Treebank unless otherwise noted. The Gaussian prior was used for smoothing (Chen and Rosenfeld 1999a), and its hyper-parameter was tuned for each model to maximize F-score for the development set. The algorithm for parameter estimation</context>
<context position="59899" citStr="Clark and Curran 2004" startWordPosition="9719" endWordPosition="9722">at a model with only semantic features performs significantly worse than one with syntactic features. Even when combined with syntactic features, semantic features do not improve accuracy. Obviously, semantic preferences are necessary for accurate parsing, but the features used in this work were not sufficient to capture semantic preferences. A possible reason is that, as reported in Gildea (2001), bilexical dependencies may be too sparse to capture semantic preferences. For reference, our results are competitive with the best corresponding results reported in CCG parsing (LP/LR = 86.6/86.3) (Clark and Curran 2004b), although our results cannot be compared directly with other grammar formalisms because each formalism represents predicate–argument dependencies differently. In contrast with the results of CCG and PCFG (Collins 1997, 1999, 2003; Charniak 2000), the recall is clearly lower than precision. This may have resulted from the HPSG grammar having stricter feature constraints and the parser not being able to produce parse results for around 1% of the sentences. To improve recall, we need techniques to deal with these 1% of sentences. Table 8 gives the computation/space costs of model estimation. “</context>
<context position="86549" citStr="Clark and Curran (2004" startWordPosition="13735" endWordPosition="13738">lts attest the effectiveness of feature forest models in practical deep parsing. A remaining problem is that no studies could provide empirical comparisons across grammar theories. The above studies and our research evaluated parsing accuracy on their own test sets. The construction of theory-independent standard test sets requires enormous effort because we must establish theory-independent criteria such as agreed definitions of phrases and headedness. Although this issue is beyond the scope of the present article, it is a fundamental obstacle to the transparency of these studies on parsing. Clark and Curran (2004a) described a method for reducing the cost of parsing a training treebank without sacrificing accuracy in the context of CCG parsing. They first assigned each word a small number of supertags, corresponding to lexical entries in our case, and parsed supertagged sentences. Because they did not use the probabilities of supertags in a parsing stage, their method corresponds to our “filtering only” method. The difference from our approach is that they also applied the supertagger in a parsing stage. We suppose that this was crucial for high accuracy in their approach, although empirical investiga</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Clark, Stephen and James R. Curran. 2004a. The importance of supertagging for wide-coverage CCG parsing.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (COLING 2004),</booktitle>
<pages>282--288</pages>
<location>Geneva.</location>
<marker></marker>
<rawString>In Proceedings of the 20th International Conference on Computational Linguistics (COLING 2004), pages 282–288, Geneva.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Parsing the WSJ using CCG and log-linear models.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL 2004),</booktitle>
<pages>104--111</pages>
<location>Barcelona.</location>
<contexts>
<context position="55386" citStr="Clark and Curran 2004" startWordPosition="9026" endWordPosition="9029">, transitive verb, ARG2) • (running, he, intransitive verb, ARG1) Labeled precision/recall (LP/LR) is the ratio of tuples correctly identified by the parser, and unlabeled precision/recall (UP/UR) is the ratio of wh and wn correctly identified regardless of π and p. F-score is the harmonic mean of LP and LR. Sentence accuracy is the exact match accuracy of complete predicate–argument relations in a sentence. These measures correspond to those used in other studies measuring the accuracy of predicate–argument dependencies in CCG parsing (Clark, Hockenmaier, and Steedman 2002; Hockenmaier 2003; Clark and Curran 2004b) and LFG parsing (Burke et al. 2004), although exact figures cannot be compared directly because the definitions of dependencies are different. All predicate–argument dependencies in a sentence are the target of evaluation except quotation marks and periods. The accuracy is measured by parsing test sentences with gold-standard part-of-speech tags from the Penn Treebank unless otherwise noted. The Gaussian prior was used for smoothing (Chen and Rosenfeld 1999a), and its hyper-parameter was tuned for each model to maximize F-score for the development set. The algorithm for parameter estimation</context>
<context position="59899" citStr="Clark and Curran 2004" startWordPosition="9719" endWordPosition="9722">at a model with only semantic features performs significantly worse than one with syntactic features. Even when combined with syntactic features, semantic features do not improve accuracy. Obviously, semantic preferences are necessary for accurate parsing, but the features used in this work were not sufficient to capture semantic preferences. A possible reason is that, as reported in Gildea (2001), bilexical dependencies may be too sparse to capture semantic preferences. For reference, our results are competitive with the best corresponding results reported in CCG parsing (LP/LR = 86.6/86.3) (Clark and Curran 2004b), although our results cannot be compared directly with other grammar formalisms because each formalism represents predicate–argument dependencies differently. In contrast with the results of CCG and PCFG (Collins 1997, 1999, 2003; Charniak 2000), the recall is clearly lower than precision. This may have resulted from the HPSG grammar having stricter feature constraints and the parser not being able to produce parse results for around 1% of the sentences. To improve recall, we need techniques to deal with these 1% of sentences. Table 8 gives the computation/space costs of model estimation. “</context>
<context position="86549" citStr="Clark and Curran (2004" startWordPosition="13735" endWordPosition="13738">lts attest the effectiveness of feature forest models in practical deep parsing. A remaining problem is that no studies could provide empirical comparisons across grammar theories. The above studies and our research evaluated parsing accuracy on their own test sets. The construction of theory-independent standard test sets requires enormous effort because we must establish theory-independent criteria such as agreed definitions of phrases and headedness. Although this issue is beyond the scope of the present article, it is a fundamental obstacle to the transparency of these studies on parsing. Clark and Curran (2004a) described a method for reducing the cost of parsing a training treebank without sacrificing accuracy in the context of CCG parsing. They first assigned each word a small number of supertags, corresponding to lexical entries in our case, and parsed supertagged sentences. Because they did not use the probabilities of supertags in a parsing stage, their method corresponds to our “filtering only” method. The difference from our approach is that they also applied the supertagger in a parsing stage. We suppose that this was crucial for high accuracy in their approach, although empirical investiga</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Clark, Stephen and James R. Curran. 2004b. Parsing the WSJ using CCG and log-linear models. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL 2004), pages 104–111, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Building deep dependency structures with a widecoverage CCG parser.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>327--334</pages>
<marker>Clark, Hockenmaier, Steedman, 2002</marker>
<rawString>Clark, Stephen, Julia Hockenmaier, and Mark Steedman. 2002. Building deep dependency structures with a widecoverage CCG parser. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL 2002), pages 327–334, Philadephia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul R Cohen</author>
</authors>
<title>Empirical Methods for Artificial Intelligence.</title>
<date>1995</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="63450" citStr="Cohen 1995" startWordPosition="10268" endWordPosition="10269">can determine the correlation between the estimation/parsing cost and accuracy. In our experiment, n &gt; 10 and c &gt; 0.90 seem necessary to preserve the F-score over 86.0. 5.5 Contribution of Features Table 12 shows the accuracy with different feature sets. Accuracy was measured for 15 models with some atomic features removed from the final model. The last row denotes the accuracy attained by the unigram model (i.e., the reference distribution). The numbers in bold type represent a significant difference from the final model according to stratified shuffling tests with the Bonferroni correction (Cohen 1995) with p-value &lt; .05 for 32 pairwise comparisons. The results indicate that DIST, COMMA, SPAN, WORD, and 65 Computational Linguistics Volume 34, Number 1 Table 10 Filtering threshold vs. accuracy. n, c LP LR F-score Sentence acc. 5, 0.80 85.09 84.30 84.69 32.4 5, 0.90 85.44 84.61 85.02 32.5 5, 0.95 85.52 84.66 85.09 32.7 5, 0.98 85.50 84.63 85.06 32.6 10, 0.80 85.60 84.65 85.12 32.5 10, 0.90 86.49 85.92 86.20 34.7 10, 0.95 86.92 86.28 86.60 36.3 10, 0.98 87.18 86.66 86.92 37.7 15, 0.80 85.59 84.63 85.11 32.4 15, 0.90 86.48 85.80 86.14 35.7 15, 0.95 87.21 86.68 86.94 37.0 15, 0.98 87.69 87.16 87</context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>Cohen, Paul R. 1995. Empirical Methods for Artificial Intelligence. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL’97),</booktitle>
<pages>16--23</pages>
<location>Madrid.</location>
<contexts>
<context position="53707" citStr="Collins 1997" startWordPosition="8760" endWordPosition="8761"> the HPSG treebank derived from Sections 02–21 of the Wall Street Journal portion of the Penn Treebank, that is, the same set used for lexicon extraction. For training of the disambiguation models, we eliminated sentences of 40 words or more and sentences for which the parser could not produce the correct parses. The resulting training set consists of 33,604 sentences (when n = 10 and c = 0.95; see Section 5.4 for details). The treebanks derived from Sections 22 and 23 were used as the development and final test sets, respectively. Following previous studies on parsing with PCFG-based models (Collins 1997; Charniak 2000), accuracy is measured for sentences of less than 40 words and for those with less than 100 words. Table 5 shows the specifications of the test data. The measure for evaluating parsing accuracy is precision/recall of predicate– argument dependencies output by the parser. A predicate–argument dependency is defined as a tuple (wh,wn,7t, p), where wh is the head word of the predicate, wn is the head word of the argument, 7t is the type of the predicate (e.g., adjective, intransitive verb), and p is an argument label (MODARG, ARG1, ..., ARG4). For example, He tried running has thre</context>
<context position="60119" citStr="Collins 1997" startWordPosition="9750" endWordPosition="9751">ssary for accurate parsing, but the features used in this work were not sufficient to capture semantic preferences. A possible reason is that, as reported in Gildea (2001), bilexical dependencies may be too sparse to capture semantic preferences. For reference, our results are competitive with the best corresponding results reported in CCG parsing (LP/LR = 86.6/86.3) (Clark and Curran 2004b), although our results cannot be compared directly with other grammar formalisms because each formalism represents predicate–argument dependencies differently. In contrast with the results of CCG and PCFG (Collins 1997, 1999, 2003; Charniak 2000), the recall is clearly lower than precision. This may have resulted from the HPSG grammar having stricter feature constraints and the parser not being able to produce parse results for around 1% of the sentences. To improve recall, we need techniques to deal with these 1% of sentences. Table 8 gives the computation/space costs of model estimation. “Estimation time” indicates user times required for running the parameter estimation algorithm. “No. of feature occurrences” denotes the total number of occurrences of features in the training data, and “Data size” gives </context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Collins, Michael. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (ACL’97), pages 16–23, Madrid.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<marker>Collins, 1999</marker>
<rawString>Collins, Michael. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In Proceedings of the Seventeenth International Conference on Machine Learning,</booktitle>
<pages>175--182</pages>
<location>Palo Alto, CA.</location>
<contexts>
<context position="90459" citStr="Collins 2000" startWordPosition="14324" endWordPosition="14325">ch question whether dynamic programming or sampling can deliver a better balance of estimation efficiency and accuracy. The answer will differ in different problems. When most effective features can be represented locally in tractablesize feature forests, dynamic programming methods including ours are suitable. However, when global context features are indispensable for high accuracy, sampling methods might be better. We should also investigate compromise solutions such as dynamic CRFs (McCallum, Rohanimanesh, and Sutton 2003; Sutton, Rohanimanesh, and McCallum 2004) and reranking techniques (Collins 2000; Charniak and Johnson 2005). There is no analytical way of predicting the best solution, and it must be investigated experimentally for each target task. 7. Conclusion A dynamic programming algorithm was presented for maximum entropy modeling and shown to provide a solution to the parameter estimation of probabilistic models of complete structures without the independence assumption. We first defined the notion of a feature forest, which is a packed representation of an exponential number of trees of features. When training data is represented with feature forests, model parameters are estima</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Collins, Michael. 2000. Discriminative reranking for natural language parsing. In Proceedings of the Seventeenth International Conference on Machine Learning, pages 175–182, Palo Alto, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<marker>Collins, 2003</marker>
<rawString>Collins, Michael. 2003. Head-driven statistical models for natural language parsing. Computational Linguistics, 29(4):589–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
<author>Rob Malouf</author>
<author>Susanne Riehemann</author>
<author>Ivan Sag</author>
</authors>
<title>Translation using minimal recursion semantics.</title>
<date>1995</date>
<booktitle>In Proceedings of the Sixth International Conference on Theoretical and Methodological Issues in Machine Translation (TMI95),</booktitle>
<pages>15--32</pages>
<location>Leuven.</location>
<contexts>
<context position="38684" citStr="Copestake et al. 1995" startWordPosition="6227" endWordPosition="6230">he wh-extraction of the object of love (left) and for the control construction of try (right). The first condition is satisfied because both lexical entries refer to CONT|HOOK of argument signs in SUBJ, COMPS, and SLASH. None of the lexical entries directly access ARGX of the arguments. The second condition is also satisfied because the values of CONT|HOOK of all of the argument signs are percolated to ARGX of the mother. In addition, the elements in CONT|RELS are percolated to the mother by the Semantic Principle. Compositional semantics usually satisfies the above conditions, including MRS (Copestake et al. 1995, 2006). The composition of MRS refers to HOOK, and no internal structures of daughters. The Semantic Principle of MRS also assures that all semantic relations in RELS are percolated to the mother. When these conditions are satisfied, semantics may include any constraints, such as selectional restrictions, although the grammar we used in the experiments does not include semantic restrictions to constrain parse forests. Under these conditions, local structures of predicate–argument structures are encoded into a conjunctive node when the values of all of its arguments have been instantiated. We </context>
</contexts>
<marker>Copestake, Flickinger, Malouf, Riehemann, Sag, 1995</marker>
<rawString>Copestake, Ann, Dan Flickinger, Rob Malouf, Susanne Riehemann, and Ivan Sag. 1995. Translation using minimal recursion semantics. In Proceedings of the Sixth International Conference on Theoretical and Methodological Issues in Machine Translation (TMI95), pages 15–32, Leuven.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
<author>Ivan A Sag</author>
<author>Carl Pollard</author>
</authors>
<title>Minimal recursion semantics: An introduction.</title>
<date>2006</date>
<journal>Research on Language and Computation,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="30393" citStr="Copestake et al. 2006" startWordPosition="4898" endWordPosition="4901">l sign for loves. The geometry of signs follows Pollard and Sag: HEAD represents the part-of-speech of the head word, MOD denotes modifiee constraints, and SPR, SUBJ, and COMPS describe constraints of a specifier, a syntactic subject, and complements, respectively. CONT denotes the 49 Computational Linguistics Volume 34, Number 1 Figure 10 Lexical entry for the transitive verb loves. Figure 11 Simplified representation of the lexical entry in Figure 10. predicate–argument structure of a phrase/sentence. The notation of CONT in this article is borrowed from that of Minimal Recursion Semantics (Copestake et al. 2006): HOOK represents a structure accessed by other phrases, and RELS describes the remaining structure of the semantics. In what follows, we represent signs in a reduced form as shown in Figure 11, because of the large size of typical HPSG signs, which often include information not immediately relevant to the point being discussed. We will only show attributes that are relevant to an explanation, expecting that readers can fill in the values of suppressed attributes. 50 Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing In our actual implementation of the HPSG grammar, lexical/</context>
</contexts>
<marker>Copestake, Flickinger, Sag, Pollard, 2006</marker>
<rawString>Copestake, Ann, Dan Flickinger, Ivan A. Sag, and Carl Pollard. 2006. Minimal recursion semantics: An introduction. Research on Language and Computation, 3(4):281–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J N Darroch</author>
<author>D Ratcliff</author>
</authors>
<title>Generalized iterative scaling for log-linear models.</title>
<date>1972</date>
<journal>The Annals of Mathematical Statistics,</journal>
<volume>43</volume>
<issue>5</issue>
<contexts>
<context position="8867" citStr="Darroch and Ratcliff 1972" startWordPosition="1280" endWordPosition="1283">n this definition). Y(x) is a set of y for given x; for example, in parsing, x is a given sentence and Y(x) is a parse forest for x. An advantage of maximum entropy models is that feature functions can represent any characteristics of events. That is, independence assumptions are unnecessary for the design of feature functions. Hence, this method provides a principled solution for the estimation of consistent probabilistic distributions over feature structure grammars. The remaining issue is how to estimate parameters. Several numerical algorithms, such as Generalized Iterative Scaling (GIS) (Darroch and Ratcliff 1972), Improved Iterative Scaling (IIS) (Della Pietra, Della Pietra, and Lafferty 1997), and the Limitedmemory Broyden-Fletcher-Goldfarb-Shanno method (L-BFGS) (Nocedal and Wright 1999), have been proposed for parameter estimation. Although the algorithm proposed in the present article is applicable to all of the above algorithms, we used L-BFGS for experiments. However, a computational problem arises in these parameter estimation algorithms. The size of Y(x) (i.e., the number of parse trees for a sentence) is generally 37 Computational Linguistics Volume 34, Number 1 very large. This is because lo</context>
</contexts>
<marker>Darroch, Ratcliff, 1972</marker>
<rawString>Darroch, J. N. and D. Ratcliff. 1972. Generalized iterative scaling for log-linear models. The Annals of Mathematical Statistics, 43(5):1470–1480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Della Pietra</author>
<author>Vincent Della Pietra Stephen</author>
<author>John Lafferty</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1997</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>19</volume>
<issue>4</issue>
<marker>Pietra, Stephen, Lafferty, 1997</marker>
<rawString>Della Pietra, Stephen, Vincent Della Pietra, and John Lafferty. 1997. Inducing features of random fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4):380–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Geman</author>
<author>Mark Johnson</author>
</authors>
<title>Dynamic programming for parsing and estimation of stochastic unification-based grammars.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>279--286</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="6630" citStr="Geman and Johnson 2002" startWordPosition="932" endWordPosition="935">dels for solving this problem. Section 4 describes the application of feature forest models to probabilistic HPSG parsing. Section 5 presents an empirical evaluation of probabilistic HPSG parsing, and Section 6 introduces research related to our proposals. Section 7 concludes. 2. Problem Maximum entropy models (Berger, Della Pietra, and Della Pietra 1996) are now becoming the de facto standard approach for disambiguation models for lexicalized or 36 Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing feature structure grammars (Johnson et al. 1999; Riezler et al. 2000, 2002; Geman and Johnson 2002; Clark and Curran 2003, 2004b; Kaplan et al. 2004; Carroll and Oepen 2005). Previous studies on probabilistic models for HPSG (Oepen, Toutanova et al. 2002; Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van Noord 2004) have also adopted log-linear models. This is because these grammar formalisms exploit feature structures to represent linguistic constraints. Such constraints are known to introduce inconsistencies in probabilistic models estimated using simple relative frequency, as discussed in Abney (1997). The maximum entropy model is a reasonable choice for credible pr</context>
<context position="75414" citStr="Geman and Johnson (2002)" startWordPosition="12072" endWordPosition="12075"> higher accuracy than other models. These researchers applied feature forests to representations of the packed parse results of LFG and the dependency/derivation structures of CCG. Their work demonstrated the applicability and effectiveness of feature forest models in parsing with wide-coverage lexicalized grammars. Feature forest models were also shown to be effective for wide-coverage sentence realization (Nakanishi, Miyao, and Tsujii 2005). This work demonstrated that feature forest models are generic enough to be applied to natural language processing tasks other than parsing. The work of Geman and Johnson (2002) independently developed a dynamic programming algorithm for maximum entropy models. The solution was similar to our approach, although their method was designed to traverse LFG parse results represented with disjunctive feature structures as proposed by Maxwell and Kaplan (1995). The difference between the two approaches is that feature forests use a simpler generic data structure to represent packed forest structures. Therefore, without assuming what feature forests represent, our algorithm can be applied to various tasks, including theirs. Another approach to the probabilistic modeling of c</context>
<context position="84062" citStr="Geman and Johnson (2002)" startWordPosition="13373" endWordPosition="13376"> (Toutanova, Markova, and Manning 2004). However, the problem of exponential explosion is also inevitable using their methods. As described in Section 6.1, an approach similar to ours may be applied, following the study of Taskar et al. (2004). A series of studies on parsing with LFG (Johnson et al. 1999; Riezler et al. 2000, 2002) also proposed a maximum entropy model for probabilistic modeling of LFG parsing. However, similarly to the previous studies on HPSG parsing, these groups had no solution to the problem of exponential explosion of unpacked parse results. As discussed in Section 6.1, Geman and Johnson (2002) proposed an algorithm for maximum entropy estimation for packed representations of LFG parses. Recent studies on CCG have proposed probabilistic models of dependency structures or predicate–argument dependencies, which are essentially the same as the predicate–argument structures described in the present article. Clark, Hockenmaier, and Steedman (2002) attempted the modeling of dependency structures, but the model was inconsistent because of the violation of the independence assumption. Hockenmaier (2003) proposed a consistent generative model of predicate–argument structures. The probability</context>
</contexts>
<marker>Geman, Johnson, 2002</marker>
<rawString>Geman, Stuart and Mark Johnson. 2002. Dynamic programming for parsing and estimation of stochastic unification-based grammars. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL 2002), pages 279–286, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Corpus variation and parser performance.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>167--202</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="59678" citStr="Gildea (2001)" startWordPosition="9688" endWordPosition="9689">were generally considered as important for the accurate disambiguation of syntactic structures. For example, PP-attachment ambiguity cannot be resolved with only syntactic preferences. However, the results show that a model with only semantic features performs significantly worse than one with syntactic features. Even when combined with syntactic features, semantic features do not improve accuracy. Obviously, semantic preferences are necessary for accurate parsing, but the features used in this work were not sufficient to capture semantic preferences. A possible reason is that, as reported in Gildea (2001), bilexical dependencies may be too sparse to capture semantic preferences. For reference, our results are competitive with the best corresponding results reported in CCG parsing (LP/LR = 86.6/86.3) (Clark and Curran 2004b), although our results cannot be compared directly with other grammar formalisms because each formalism represents predicate–argument dependencies differently. In contrast with the results of CCG and PCFG (Collins 1997, 1999, 2003; Charniak 2000), the recall is clearly lower than precision. This may have resulted from the HPSG grammar having stricter feature constraints and </context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>Gildea, Daniel. 2001. Corpus variation and parser performance. In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing (EMNLP 2001), pages 167–202, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
</authors>
<title>Parsing with generative models of predicate-argument structure.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>359--366</pages>
<location>Sapporo.</location>
<contexts>
<context position="55363" citStr="Hockenmaier 2003" startWordPosition="9024" endWordPosition="9025"> • (tried, running, transitive verb, ARG2) • (running, he, intransitive verb, ARG1) Labeled precision/recall (LP/LR) is the ratio of tuples correctly identified by the parser, and unlabeled precision/recall (UP/UR) is the ratio of wh and wn correctly identified regardless of π and p. F-score is the harmonic mean of LP and LR. Sentence accuracy is the exact match accuracy of complete predicate–argument relations in a sentence. These measures correspond to those used in other studies measuring the accuracy of predicate–argument dependencies in CCG parsing (Clark, Hockenmaier, and Steedman 2002; Hockenmaier 2003; Clark and Curran 2004b) and LFG parsing (Burke et al. 2004), although exact figures cannot be compared directly because the definitions of dependencies are different. All predicate–argument dependencies in a sentence are the target of evaluation except quotation marks and periods. The accuracy is measured by parsing test sentences with gold-standard part-of-speech tags from the Penn Treebank unless otherwise noted. The Gaussian prior was used for smoothing (Chen and Rosenfeld 1999a), and its hyper-parameter was tuned for each model to maximize F-score for the development set. The algorithm f</context>
<context position="84573" citStr="Hockenmaier (2003)" startWordPosition="13443" endWordPosition="13444">m of exponential explosion of unpacked parse results. As discussed in Section 6.1, Geman and Johnson (2002) proposed an algorithm for maximum entropy estimation for packed representations of LFG parses. Recent studies on CCG have proposed probabilistic models of dependency structures or predicate–argument dependencies, which are essentially the same as the predicate–argument structures described in the present article. Clark, Hockenmaier, and Steedman (2002) attempted the modeling of dependency structures, but the model was inconsistent because of the violation of the independence assumption. Hockenmaier (2003) proposed a consistent generative model of predicate–argument structures. The probability of a non-local dependency was conditioned on multiple words to preserve the consistency of the probability model; that is, probability p(Ilwant, dispute) in Section 4.3 was directly estimated. The problem was that such probabilities could not be estimated directly from the data due to data sparseness, and a heuristic method had to be employed. Probabilities were therefore estimated as the average of individual probabilities conditioned on a single word. Another problem is that the model is no longer consi</context>
</contexts>
<marker>Hockenmaier, 2003</marker>
<rawString>Hockenmaier, Julia. 2003. Parsing with generative models of predicate-argument structure. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL 2003), pages 359–366, Sapporo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Acquiring compact lexicalized grammars from a cleaner treebank.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC-2002),</booktitle>
<pages>1974--1981</pages>
<location>Las Palmas.</location>
<contexts>
<context position="2132" citStr="Hockenmaier and Steedman 2002" startWordPosition="293" endWordPosition="296">s is possible when they are represented by feature forests. This article also describes methods for representing HPSG syntactic structures and predicate–argument structures with feature forests. Hence, we describe a complete strategy for developing probabilistic models for HPSG parsing. The effectiveness of the proposed methods is empirically evaluated through parsing experiments on the Penn Treebank, and the promise of applicability to parsing of real-world sentences is discussed. 1. Introduction Following the successful development of wide-coverage lexicalized grammars (Riezler et al. 2000; Hockenmaier and Steedman 2002; Burke et al. 2004; Miyao, Ninomiya, and * Department of Computer Science, University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Japan. E-mail: yusuke@is.s.u-tokyo.ac.jp. ** Department of Computer Science, University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Japan. E-mail: tsujii@is.s.u-tokyo.ac.jp. Submission received: 11 June 2006; revised submission received: 2 March 2007; accepted for publication: 5 May 2007. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 1 Tsujii 2005), statistical modeling of these grammars is attracting conside</context>
<context position="52708" citStr="Hockenmaier and Steedman (2002)" startWordPosition="8597" endWordPosition="8600">of the HPSG treebank. Another notable feature is that we can additionally obtain an HPSG treebank, which can be used as training data for disambiguation models. In the following experiments, this HPSG treebank is used for the training of maximum entropy models. The lexicon used in the following experiments was extracted from Sections 02–21 of the Wall Street Journal portion of the Penn Treebank. This lexicon can assign correct lexical entries to 99.09% of words in the HPSG treebank converted from Penn Treebank Section 23. This number expresses “lexical coverage” in the strong sense defined by Hockenmaier and Steedman (2002). In this notion of “coverage,” this lexicon has 84.1% sentential coverage, where this means that the lexicon can assign correct lexical entries to all of the words in a sentence. Although the parser might produce parse results for uncovered sentences, these parse results cannot be completely correct. 5.2 Experimental Settings The data for the training of the disambiguation models was the HPSG treebank derived from Sections 02–21 of the Wall Street Journal portion of the Penn Treebank, that is, the same set used for lexicon extraction. For training of the disambiguation models, we eliminated s</context>
</contexts>
<marker>Hockenmaier, Steedman, 2002</marker>
<rawString>Hockenmaier, Julia and Mark Steedman. 2002. Acquiring compact lexicalized grammars from a cleaner treebank. In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC-2002), pages 1974–1981, Las Palmas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Stuart Geman</author>
<author>Stephen Canon</author>
<author>Zhiyi Chi</author>
<author>Stefan Riezler</author>
</authors>
<title>Estimators for stochastic “unificationbased” grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL’99),</booktitle>
<pages>535--541</pages>
<location>College Park, Maryland.</location>
<contexts>
<context position="6579" citStr="Johnson et al. 1999" startWordPosition="923" endWordPosition="926">d grammars. Section 3 proposes feature forest models for solving this problem. Section 4 describes the application of feature forest models to probabilistic HPSG parsing. Section 5 presents an empirical evaluation of probabilistic HPSG parsing, and Section 6 introduces research related to our proposals. Section 7 concludes. 2. Problem Maximum entropy models (Berger, Della Pietra, and Della Pietra 1996) are now becoming the de facto standard approach for disambiguation models for lexicalized or 36 Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing feature structure grammars (Johnson et al. 1999; Riezler et al. 2000, 2002; Geman and Johnson 2002; Clark and Curran 2003, 2004b; Kaplan et al. 2004; Carroll and Oepen 2005). Previous studies on probabilistic models for HPSG (Oepen, Toutanova et al. 2002; Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van Noord 2004) have also adopted log-linear models. This is because these grammar formalisms exploit feature structures to represent linguistic constraints. Such constraints are known to introduce inconsistencies in probabilistic models estimated using simple relative frequency, as discussed in Abney (1997). The maximum e</context>
<context position="83743" citStr="Johnson et al. 1999" startWordPosition="13321" endWordPosition="13324">n conjunctive nodes. We will discuss the trade-off between the approximation solution and the locality of feature functions in Section 6.3. Non-probabilistic statistical classifiers have also been applied to disambiguation in HPSG parsing: voted perceptrons (Baldridge and Osborne 2003) and support vector machines (Toutanova, Markova, and Manning 2004). However, the problem of exponential explosion is also inevitable using their methods. As described in Section 6.1, an approach similar to ours may be applied, following the study of Taskar et al. (2004). A series of studies on parsing with LFG (Johnson et al. 1999; Riezler et al. 2000, 2002) also proposed a maximum entropy model for probabilistic modeling of LFG parsing. However, similarly to the previous studies on HPSG parsing, these groups had no solution to the problem of exponential explosion of unpacked parse results. As discussed in Section 6.1, Geman and Johnson (2002) proposed an algorithm for maximum entropy estimation for packed representations of LFG parses. Recent studies on CCG have proposed probabilistic models of dependency structures or predicate–argument dependencies, which are essentially the same as the predicate–argument structures</context>
</contexts>
<marker>Johnson, Geman, Canon, Chi, Riezler, 1999</marker>
<rawString>Johnson, Mark, Stuart Geman, Stephen Canon, Zhiyi Chi, and Stefan Riezler. 1999. Estimators for stochastic “unificationbased” grammars. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL’99), pages 535–541, College Park, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Stefan Riezler</author>
</authors>
<title>Exploiting auxiliary distributions in stochastic unification-based grammars.</title>
<date>2000</date>
<booktitle>In Proceedings of the First Conference on North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>154--161</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="45820" citStr="Johnson and Riezler 2000" startWordPosition="7349" endWordPosition="7352">xperimentally in Section 5.4. We have several ways to integrate p¯ with the estimated model p(tjT(w)). In the experiments, we will empirically compare the following methods in terms of accuracy and estimation time. Filtering only: The unigram probability p¯ is used only for filtering in training. Product: The probability is defined as the product of p¯ and the estimated model p. Reference distribution: p¯ is used as a reference distribution of p. Feature function: log p¯ is used as a feature function of p. This method has been shown to be a generalization of the reference distribution method (Johnson and Riezler 2000). 4.5 Features Feature functions in maximum entropy models are designed to capture the characteristics of (em, el, er). In this article, we investigate combinations of the atomic features listed Figure 19 Filtering of lexical entries for saw. 57 Computational Linguistics Volume 34, Number 1 Table 1 Templates for atomic features. RULE name of the applied schema DIST distance between the head words of the daughters COMMA whether a comma exists between daughters and/or inside of daughter phrases SPAN number of words dominated by the phrase SYM symbol of the phrasal category (e.g., NP, VP) WORD su</context>
</contexts>
<marker>Johnson, Riezler, 2000</marker>
<rawString>Johnson, Mark and Stefan Riezler. 2000. Exploiting auxiliary distributions in stochastic unification-based grammars. In Proceedings of the First Conference on North American Chapter of the Association for Computational Linguistics, pages 154–161, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshitaka Kameya</author>
<author>Taisuke Sato</author>
</authors>
<title>Efficient EM learning with tabulation for parameterized logic programs.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st International Conference on Computational Logic (CL2000),</booktitle>
<volume>1861</volume>
<pages>269--294</pages>
<location>Imperial College, London.</location>
<contexts>
<context position="79448" citStr="Kameya and Sato 2000" startWordPosition="12673" endWordPosition="12676"> local structures. Examples include the derivation trees of LTAG and CCG. Chiang (2003) proved that feature forests could be considered as the derivation forests of linear context-free rewriting systems (LCFRSs) (Vijay-Shanker, Weir, and Joshi 1987; Weir 1988). LCFRSs define a wide variety of grammars, including LTAG and CCG, while preserving polynomial-time complexity of parsing. This demonstrates that feature forest models are applicable to probabilistic models far beyond PCFGs. Feature forests are also isomorphic to support graphs (or explanation graphs) used in the graphical EM algorithm (Kameya and Sato 2000). In their framework, a program in a logic programming language, PRISM (Sato and Kameya 1997), is converted into support graphs, and parameters of probabilistic models are automatically learned by an EM algorithm. Support graphs have been proved to represent various statistical structural models, including HMMs, PCFGs, Bayesian networks, and many other graphical structures (Sato and Kameya 2001; Sato 2005). Taken together, these results imply the high applicability of feature forest models to various real tasks. Because feature forests have a structure isomorphic to parse forests of PCFG, it m</context>
</contexts>
<marker>Kameya, Sato, 2000</marker>
<rawString>Kameya, Yoshitaka and Taisuke Sato. 2000. Efficient EM learning with tabulation for parameterized logic programs. In Proceedings of the 1st International Conference on Computational Logic (CL2000), volume 1861 of Lecture Notes in Artificial Intelligence (LNAI), pages 269–294, Imperial College, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald M Kaplan</author>
<author>Stefan Riezler</author>
<author>Tracy H King</author>
<author>John T Maxwell</author>
<author>Alexander Vasserman</author>
<author>Richard Crouch</author>
</authors>
<title>Speed and accuracy in shallow and deep stochastic parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics (HLT-NAACL 2004),</booktitle>
<pages>97--104</pages>
<location>Boston, MA.</location>
<contexts>
<context position="6680" citStr="Kaplan et al. 2004" startWordPosition="941" endWordPosition="944"> application of feature forest models to probabilistic HPSG parsing. Section 5 presents an empirical evaluation of probabilistic HPSG parsing, and Section 6 introduces research related to our proposals. Section 7 concludes. 2. Problem Maximum entropy models (Berger, Della Pietra, and Della Pietra 1996) are now becoming the de facto standard approach for disambiguation models for lexicalized or 36 Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing feature structure grammars (Johnson et al. 1999; Riezler et al. 2000, 2002; Geman and Johnson 2002; Clark and Curran 2003, 2004b; Kaplan et al. 2004; Carroll and Oepen 2005). Previous studies on probabilistic models for HPSG (Oepen, Toutanova et al. 2002; Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van Noord 2004) have also adopted log-linear models. This is because these grammar formalisms exploit feature structures to represent linguistic constraints. Such constraints are known to introduce inconsistencies in probabilistic models estimated using simple relative frequency, as discussed in Abney (1997). The maximum entropy model is a reasonable choice for credible probabilistic models. It also allows various overlap</context>
<context position="74712" citStr="Kaplan et al. 2004" startWordPosition="11973" endWordPosition="11976">ere indirectly caused by errors of argument/modifier distinction in to-infinitive clauses. A significant portion of the errors discussed above cannot be resolved by the features we investigated in this study, and the design of other features will be necessary for improving parsing accuracy. 6. Discussion 6.1 Probabilistic Modeling of Complete Structures The model described in this article was first published in Miyao and Tsujii (2002), and has been applied to probabilistic models for parsing with lexicalized grammars. Applications to CCG parsing (Clark and Curran 2003, 2004b) and LFG parsing (Kaplan et al. 2004; Riezler and Vasserman 2004) demonstrated that feature forest models attained higher accuracy than other models. These researchers applied feature forests to representations of the packed parse results of LFG and the dependency/derivation structures of CCG. Their work demonstrated the applicability and effectiveness of feature forest models in parsing with wide-coverage lexicalized grammars. Feature forest models were also shown to be effective for wide-coverage sentence realization (Nakanishi, Miyao, and Tsujii 2005). This work demonstrated that feature forest models are generic enough to be</context>
<context position="85555" citStr="Kaplan et al. 2004" startWordPosition="13590" endWordPosition="13593">data due to data sparseness, and a heuristic method had to be employed. Probabilities were therefore estimated as the average of individual probabilities conditioned on a single word. Another problem is that the model is no longer consistent when unification constraints such as those in HPSG are introduced. 73 Computational Linguistics Volume 34, Number 1 Our solution is free of these problems, and is applicable to various grammars, not only HPSG and CCG. Most of the state-of-the-art studies on parsing with lexicalized grammars have adopted feature forest models (Clark and Curran 2003, 2004b; Kaplan et al. 2004; Riezler and Vasserman 2004). Their methods of translating parse results into feature forests are basically the same as our method described in Section 4, and details differ because different grammar theories represent syntactic structures differently. They reported higher accuracy in parsing the Penn Treebank than the previous methods introduced herein, and these results attest the effectiveness of feature forest models in practical deep parsing. A remaining problem is that no studies could provide empirical comparisons across grammar theories. The above studies and our research evaluated pa</context>
</contexts>
<marker>Kaplan, Riezler, King, Maxwell, Vasserman, Crouch, 2004</marker>
<rawString>Kaplan, Ronald M., Stefan Riezler, Tracy H. King, John T. Maxwell, III, Alexander Vasserman, and Richard Crouch. 2004. Speed and accuracy in shallow and deep stochastic parsing. In Proceedings of the Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics (HLT-NAACL 2004), pages 97–104, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the International Conference on Machine Learning</booktitle>
<pages>282--289</pages>
<location>Williams College, Williamstown, MA.</location>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>Lafferty, John, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the International Conference on Machine Learning 2001, pages 282–289, Williams College, Williamstown, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takaki Makino</author>
<author>Yusuke Miyao</author>
<author>Kentaro Torisawa</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Native-code compilation of feature structures.</title>
<date>2002</date>
<booktitle>Collaborative Language Engineering: A Case Study in Efficient Grammar-based Parsing. CSLI Publications,</booktitle>
<pages>49--80</pages>
<editor>In Stephen Oepen, Dan Flickinger, Jun’ichi Tsujii, and Hans Uszkoreit, editors,</editor>
<location>Palo Alto, CA,</location>
<contexts>
<context position="56141" citStr="Makino et al. 2002" startWordPosition="9139" endWordPosition="9142">ifferent. All predicate–argument dependencies in a sentence are the target of evaluation except quotation marks and periods. The accuracy is measured by parsing test sentences with gold-standard part-of-speech tags from the Penn Treebank unless otherwise noted. The Gaussian prior was used for smoothing (Chen and Rosenfeld 1999a), and its hyper-parameter was tuned for each model to maximize F-score for the development set. The algorithm for parameter estimation was the limited-memory BFGS method (Nocedal 1980; Nocedal and Wright 1999). The parser was implemented in C++ with the LiLFeS library (Makino et al. 2002), and various speed-up techniques for HPSG parsing were used such as quick check and iterative beam search (Tsuruoka, Miyao, and Tsujii 2004; Ninomiya et al. 2005). Other efficient parsing techniques, including global thresholding, hybrid parsing with a chunk parser, and large constituent inhibition, were not used. The results obtained using these techniques are given in Ninomiya et al. A limit on the number of constituents was set for time-out; the parser stopped parsing when the number of constituents created during parsing exceeded 50,000. In such a case, the parser output nothing, and the </context>
</contexts>
<marker>Makino, Miyao, Torisawa, Tsujii, 2002</marker>
<rawString>Makino, Takaki, Yusuke Miyao, Kentaro Torisawa, and Jun’ichi Tsujii. 2002. Native-code compilation of feature structures. In Stephen Oepen, Dan Flickinger, Jun’ichi Tsujii, and Hans Uszkoreit, editors, Collaborative Language Engineering: A Case Study in Efficient Grammar-based Parsing. CSLI Publications, Palo Alto, CA, pages 49–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
</authors>
<title>A comparison of algorithms for maximum entropy parameter estimation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Sixth Conference on Natural Language Learning (CoNLL-2002),</booktitle>
<pages>1--7</pages>
<location>Taipei.</location>
<contexts>
<context position="10076" citStr="Malouf 2002" startWordPosition="1469" endWordPosition="1470"> local ambiguities in parse trees potentially cause exponential growth in the number of structures assigned to sub-sequences of words, resulting in billions of structures for whole sentences. For example, when we apply rewriting rule S → NP VP, and the left NP and the right VP, respectively, have n and m ambiguous subtrees, the result of the rule application generates n × m trees. This is problematic because the complexity of parameter estimation is proportional to the size of Y(x). The cost of the parameter estimation algorithms is bound by the computation of model expectation, µi, given as (Malouf 2002): Eµi = E fi(x, y)p(y|x) xEX ˜p(x) yEY(x) XE E � � ˜p(x) fi(x,y)Z1 exp Eλjfj(x,y) (1) yEY(x) j As shown in this definition, the computation of model expectation requires the summation over Y(x) for every x in the training data. The complexity of the overall estimation algorithm is O( ˜|Y |˜|F||E|), where ˜|Y |and ˜|F |are the average numbers of y and activated features for an event, respectively, and |E |is the number of events. When Y(x) grows exponentially, the parameter estimation becomes intractable. In PCFGs, the problem of computing probabilities of parse trees is avoided by using a dyna</context>
<context position="28322" citStr="Malouf 2002" startWordPosition="4579" endWordPosition="4580">o HPSG parse disambiguation. The probability, p(t|w), of producing parse result t of a given sentence w is defined as p(t|w) =2 p0(t|w)exp Eλifi(t,w) w i where �E � p0(t�|w) exp λi fi(t�, w) EZw = t�∈T(w) i where p0(t|w) is a reference distribution (usually assumed to be a uniform distribution) and T(w) is a set of parse candidates assigned to w. The feature function fi(t,w) represents the characteristics of t and w, and the corresponding model parameter λi is its weight. Model parameters that maximize the log-likelihood of the training data are computed using a numerical optimization method (Malouf 2002). Estimation of the model requires a set of pairs (tw, T(w)), where tw is the correct parse for a sentence w. Whereas tw is provided by a treebank, T(w) has to be computed by parsing each w in the treebank. Previous studies assumed T(w) could be enumerated; however, this assumption is impractical because the size of T(w) is exponentially related to the length of w. Our solution here is to apply the feature forest model of Section 3 to the probabilistic modeling of HPSG parsing. Section 4.1 briefly introduces HPSG. Section 4.2 and 4.3 describe how to represent HPSG parse trees and predicate–arg</context>
</contexts>
<marker>Malouf, 2002</marker>
<rawString>Malouf, Robert. 2002. A comparison of algorithms for maximum entropy parameter estimation. In Proceedings of the Sixth Conference on Natural Language Learning (CoNLL-2002), pages 1–7, Taipei.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
<author>Gertjan van Noord</author>
</authors>
<title>Wide coverage parsing with stochastic attribute value grammars.</title>
<date>2004</date>
<booktitle>In Proceedings of the IJCNLP-04 Workshop “Beyond Shallow Analyses”,</booktitle>
<location>Hainan</location>
<note>ac.jp/bsa.</note>
<marker>Malouf, van Noord, 2004</marker>
<rawString>Malouf, Robert and Gertjan van Noord. 2004. Wide coverage parsing with stochastic attribute value grammars. In Proceedings of the IJCNLP-04 Workshop “Beyond Shallow Analyses”, Hainan Island. Available at www.tsujii.is.s.u-tokyo. ac.jp/bsa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Grace Kim</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Robert MacIntyre</author>
<author>Ann Bies</author>
<author>Mark Ferguson</author>
<author>Karen Katz</author>
<author>Britta Schasberger</author>
</authors>
<title>The Penn Treebank: Annotating predicate argument structure.</title>
<date>1994</date>
<booktitle>In Proceedings of the Workshop on Human Language Technology,</booktitle>
<pages>114--119</pages>
<location>Plainsboro, NJ.</location>
<contexts>
<context position="5237" citStr="Marcus et al. 1994" startWordPosition="723" endWordPosition="726">sing. We describe methods for representing HPSG parse trees and predicate–argument structures using feature forests (Miyao, Ninomiya, and Tsujii 2003; Miyao and Tsujii 2003, 2005). Together with the parameter estimation algorithm for feature forest models, these methods constitute a complete procedure for the probabilistic modeling of wide-coverage HPSG parsing. The methods we propose here were applied to an English HPSG parser, Enju (Tsujii Laboratory 2004). We report on an extensive evaluation of the parser through parsing experiments on the Wall Street Journal portion of the Penn Treebank (Marcus et al. 1994). The content of this article is an extended version of our earlier work reported in Miyao and Tsujii (2002, 2003, 2005) and Miyao, Ninomiya, and Tsujii (2003). The major contribution of this article is a strict mathematical definition of the feature forest model and the parameter estimation algorithm, which are substantially refined and extended from Miyao and Tsujii (2002). Another contribution is that this article thoroughly discusses the relationships between the feature forest model and its application to HPSG parsing. We also provide an extensive empirical evaluation of the resulting HPS</context>
</contexts>
<marker>Marcus, Kim, Marcinkiewicz, MacIntyre, Bies, Ferguson, Katz, Schasberger, 1994</marker>
<rawString>Marcus, Mitchell, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The Penn Treebank: Annotating predicate argument structure. In Proceedings of the Workshop on Human Language Technology, pages 114–119, Plainsboro, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maxwell John T</author>
<author>Ronald M Kaplan</author>
</authors>
<title>A method for disjunctive constraint satisfaction.</title>
<date>1995</date>
<booktitle>Formal Issues in Lexical-Functional Grammar, number 47 in CSLI Lecture Notes Series. CSLI Publications, Palo Alto, CA, chapter 14,</booktitle>
<pages>381--481</pages>
<editor>In Mary Dalrymple, Ronald M. Kaplan, John T. Maxwell, III, and Annie Zaenen, editors,</editor>
<marker>T, Kaplan, 1995</marker>
<rawString>Maxwell John T., III and Ronald M. Kaplan. 1995. A method for disjunctive constraint satisfaction. In Mary Dalrymple, Ronald M. Kaplan, John T. Maxwell, III, and Annie Zaenen, editors, Formal Issues in Lexical-Functional Grammar, number 47 in CSLI Lecture Notes Series. CSLI Publications, Palo Alto, CA, chapter 14, pages 381–481.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Wei Li</author>
</authors>
<title>Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons.</title>
<date>2003</date>
<booktitle>In Proceedings of the 7th Conference on Natural Language Learning (CoNLL),</booktitle>
<pages>188--191</pages>
<location>Edmonton.</location>
<contexts>
<context position="77486" citStr="McCallum and Li 2003" startWordPosition="12383" endWordPosition="12386">ecause feature forests can represent Markov chains. In an analogy, CRFs correspond to HMMs, whereas feature forest models correspond to PCFGs. 71 Computational Linguistics Volume 34, Number 1 Extensions of CRFs, such as semi-Markov CRFs (Sarawagi and Cohen 2004), are also regarded as instances of feature forest models. This fact implies that our algorithm is applicable to not only parsing but also to other tasks. CRFs are now widely used for sequence-based tasks, such as parts-of-speech tagging and named entity recognition, and have been shown to achieve the best performance in various tasks (McCallum and Li 2003; McCallum, Rohanimanesh, and Sutton 2003; Pinto et al. 2003; Sha and Pereira 2003; Peng and McCallum 2004; Roark et al. 2004; Settles 2004; Sutton, Rohanimanesh, and McCallum 2004). These results suggest that the method proposed in the present article will achieve high accuracy when applied to various statistical models with tree structures. Dynamic CRFs (McCallum, Rohanimanesh, and Sutton 2003; Sutton, Rohanimanesh, and McCallum 2004) provide us with an interesting inspiration for extending feature forest models. The purpose of dynamic CRFs is to incorporate feature functions that are not re</context>
</contexts>
<marker>McCallum, Li, 2003</marker>
<rawString>McCallum, Andrew and Wei Li. 2003. Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons. In Proceedings of the 7th Conference on Natural Language Learning (CoNLL), pages 188–191, Edmonton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Khashayar Rohanimanesh</author>
<author>Charles Sutton</author>
</authors>
<title>Dynamic conditional random fields for jointly labeling multiple sequences.</title>
<date>2003</date>
<booktitle>In Proceedings of the Workshop on Syntax, Semantics, Statistics at the 16th Annual Conference on Neural Information Processing Systems,</booktitle>
<location>Vancouver.</location>
<note>Available at www.cs.umasse.du/∼mccallum/papers/ derf-nips03.pdf.</note>
<marker>McCallum, Rohanimanesh, Sutton, 2003</marker>
<rawString>McCallum, Andrew, Khashayar Rohanimanesh, and Charles Sutton. 2003. Dynamic conditional random fields for jointly labeling multiple sequences. In Proceedings of the Workshop on Syntax, Semantics, Statistics at the 16th Annual Conference on Neural Information Processing Systems, Vancouver. Available at www.cs.umasse.du/∼mccallum/papers/ derf-nips03.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Takashi Ninomiya</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic modeling of argument structures including non-local dependencies.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP</booktitle>
<pages>285--291</pages>
<location>Borovets.</location>
<marker>Miyao, Ninomiya, Tsujii, 2003</marker>
<rawString>Miyao, Yusuke, Takashi Ninomiya, and Jun’ichi Tsujii. 2003. Probabilistic modeling of argument structures including non-local dependencies. In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2003), pages 285–291, Borovets.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Takashi Ninomiya</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Corpus-oriented grammar development for acquiring a head-driven phrase structure grammar from the Penn Treebank.</title>
<date>2005</date>
<booktitle>In Natural Language Processing - IJCNLP</booktitle>
<pages>684--693</pages>
<location>Hainan Island.</location>
<marker>Miyao, Ninomiya, Tsujii, 2005</marker>
<rawString>Miyao, Yusuke, Takashi Ninomiya, and Jun’ichi Tsujii. 2005. Corpus-oriented grammar development for acquiring a head-driven phrase structure grammar from the Penn Treebank. In Natural Language Processing - IJCNLP 2004, pages 684–693, Hainan Island.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Yusuke Miyao</author>
<author>Tomoko Ohta</author>
<author>Katsuya Masuda</author>
<author>Yoshimasa Tsuruoka</author>
<author>Kazuhiro Yoshida</author>
<author>Takashi Ninomiya</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Semantic retrieval for the accurate identification of relational concepts in massive textbases.</title>
<date>2006</date>
<booktitle>In Proceedings of the Joint Conference of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL</booktitle>
<pages>1017--1024</pages>
<location>Sydney.</location>
<contexts>
<context position="92215" citStr="Miyao et al. 2006" startWordPosition="14578" endWordPosition="14581">dependencies. The presented approach can be regarded as a general solution to the probabilistic modeling of syntactic analysis with lexicalized grammars. Table 16 summarizes the best performance of the HPSG parser described in this article. The parser demonstrated impressively high coverage and accuracy for real-world texts. We therefore conclude that the HPSG parser for English is moving toward a practical level of use in real-world applications. Recently, the applicability of the HPSG parser to practical applications, such as information extraction and retrieval, has also been demonstrated (Miyao et al. 2006; Yakushiji et al. 2006; Chun 2007). Table 16 Final results. Parsing accuracy for Section 23 (&lt;40 words) # parsed sentences 2,137/2,144 (99.7%) Precision/recall 87.69%/87.16% Sentential accuracy 39.2% 75 Computational Linguistics Volume 34, Number 1 From our extensive investigation of HPSG parsing, we observed that exploration of new types of features is indispensable to further improvement of parsing accuracy. A possible research direction is to encode larger contexts of parse trees, which has been shown to improve accuracy (Toutanova and Manning 2002; Toutanova, Markova, and Manning 2004). F</context>
</contexts>
<marker>Miyao, Ohta, Masuda, Tsuruoka, Yoshida, Ninomiya, Tsujii, 2006</marker>
<rawString>Miyao, Yusuke, Tomoko Ohta, Katsuya Masuda, Yoshimasa Tsuruoka, Kazuhiro Yoshida, Takashi Ninomiya, and Jun’ichi Tsujii. 2006. Semantic retrieval for the accurate identification of relational concepts in massive textbases. In Proceedings of the Joint Conference of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL 2006), pages 1017–1024, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Maximum entropy estimation for feature forests.</title>
<date>2002</date>
<booktitle>In Proceedings of the Human Language Technology Conference (HLT-2002),</booktitle>
<pages>292--297</pages>
<location>San Diego, CA.</location>
<contexts>
<context position="4018" citStr="Miyao and Tsujii 2002" startWordPosition="549" endWordPosition="552">rage HPSG parsing is infeasible because estimation of maximum entropy models is computationally expensive, especially when targeting wide-coverage parsing. In general, complete structures, such as transition sequences in Markov models and parse trees, have an exponential number of ambiguities. This causes an exponential explosion when estimating the parameters of maximum entropy models. We therefore require solutions to make model estimation tractable. This article first proposes feature forest models, which are a general solution to the problem of maximum entropy modeling of tree structures (Miyao and Tsujii 2002). Our algorithm avoids exponential explosion by representing probabilistic events with feature forests, which are packed representations of tree structures. When complete structures are represented with feature forests of a tractable size, the parameters of maximum entropy models are efficiently estimated without unpacking the feature forests. This is due to dynamic programming similar to the algorithm for computing inside/outside probabilities in PCFG parsing. The latter half of this article (Section 4) is on the application of feature forest models to disambiguation in wide-coverage HPSG par</context>
<context position="5344" citStr="Miyao and Tsujii (2002" startWordPosition="742" endWordPosition="745">ure forests (Miyao, Ninomiya, and Tsujii 2003; Miyao and Tsujii 2003, 2005). Together with the parameter estimation algorithm for feature forest models, these methods constitute a complete procedure for the probabilistic modeling of wide-coverage HPSG parsing. The methods we propose here were applied to an English HPSG parser, Enju (Tsujii Laboratory 2004). We report on an extensive evaluation of the parser through parsing experiments on the Wall Street Journal portion of the Penn Treebank (Marcus et al. 1994). The content of this article is an extended version of our earlier work reported in Miyao and Tsujii (2002, 2003, 2005) and Miyao, Ninomiya, and Tsujii (2003). The major contribution of this article is a strict mathematical definition of the feature forest model and the parameter estimation algorithm, which are substantially refined and extended from Miyao and Tsujii (2002). Another contribution is that this article thoroughly discusses the relationships between the feature forest model and its application to HPSG parsing. We also provide an extensive empirical evaluation of the resulting HPSG parsing approach using real-world text. Section 2 discusses a problem of conventional probabilistic model</context>
<context position="74532" citStr="Miyao and Tsujii (2002)" startWordPosition="11944" endWordPosition="11947"> features for representing the similarity of global structures is difficult for feature forest models. Zero-pronoun resolution is also a difficult problem. However, we found that most were indirectly caused by errors of argument/modifier distinction in to-infinitive clauses. A significant portion of the errors discussed above cannot be resolved by the features we investigated in this study, and the design of other features will be necessary for improving parsing accuracy. 6. Discussion 6.1 Probabilistic Modeling of Complete Structures The model described in this article was first published in Miyao and Tsujii (2002), and has been applied to probabilistic models for parsing with lexicalized grammars. Applications to CCG parsing (Clark and Curran 2003, 2004b) and LFG parsing (Kaplan et al. 2004; Riezler and Vasserman 2004) demonstrated that feature forest models attained higher accuracy than other models. These researchers applied feature forests to representations of the packed parse results of LFG and the dependency/derivation structures of CCG. Their work demonstrated the applicability and effectiveness of feature forest models in parsing with wide-coverage lexicalized grammars. Feature forest models we</context>
</contexts>
<marker>Miyao, Tsujii, 2002</marker>
<rawString>Miyao, Yusuke and Jun’ichi Tsujii. 2002. Maximum entropy estimation for feature forests. In Proceedings of the Human Language Technology Conference (HLT-2002), pages 292–297, San Diego, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>A model of syntactic disambiguation based on lexicalized grammars.</title>
<date>2003</date>
<booktitle>In Proceedings of the Seventh Conference on Computational Natural Language Learning (CoNLL-2003),</booktitle>
<pages>1--8</pages>
<location>Edmonton.</location>
<contexts>
<context position="4790" citStr="Miyao and Tsujii 2003" startWordPosition="656" endWordPosition="659">ures. When complete structures are represented with feature forests of a tractable size, the parameters of maximum entropy models are efficiently estimated without unpacking the feature forests. This is due to dynamic programming similar to the algorithm for computing inside/outside probabilities in PCFG parsing. The latter half of this article (Section 4) is on the application of feature forest models to disambiguation in wide-coverage HPSG parsing. We describe methods for representing HPSG parse trees and predicate–argument structures using feature forests (Miyao, Ninomiya, and Tsujii 2003; Miyao and Tsujii 2003, 2005). Together with the parameter estimation algorithm for feature forest models, these methods constitute a complete procedure for the probabilistic modeling of wide-coverage HPSG parsing. The methods we propose here were applied to an English HPSG parser, Enju (Tsujii Laboratory 2004). We report on an extensive evaluation of the parser through parsing experiments on the Wall Street Journal portion of the Penn Treebank (Marcus et al. 1994). The content of this article is an extended version of our earlier work reported in Miyao and Tsujii (2002, 2003, 2005) and Miyao, Ninomiya, and Tsujii </context>
</contexts>
<marker>Miyao, Tsujii, 2003</marker>
<rawString>Miyao, Yusuke and Jun’ichi Tsujii. 2003. A model of syntactic disambiguation based on lexicalized grammars. In Proceedings of the Seventh Conference on Computational Natural Language Learning (CoNLL-2003), pages 1–8, Edmonton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic disambiguation models for wide-coverage HPSG parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>83--90</pages>
<location>Ann Arbor, MI.</location>
<marker>Miyao, Tsujii, 2005</marker>
<rawString>Miyao, Yusuke and Jun’ichi Tsujii. 2005. Probabilistic disambiguation models for wide-coverage HPSG parsing. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL 2005), pages 83–90, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroko Nakanishi</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Probabilistic models for disambiguation of an HPSG-based chart generator.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th International Workshop on Parsing Technologies (IWPT</booktitle>
<pages>93--102</pages>
<location>Vancouver.</location>
<marker>Nakanishi, Miyao, Tsujii, 2005</marker>
<rawString>Nakanishi, Hiroko, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Probabilistic models for disambiguation of an HPSG-based chart generator. In Proceedings of the 9th International Workshop on Parsing Technologies (IWPT 2005), pages 93–102, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takashi Ninomiya</author>
<author>Yoshimasa Tsuruoka</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Efficacy of beam thresholding, unification filtering and hybrid parsing in probabilistic HPSG parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th International Workshop on Parsing Technologies,</booktitle>
<pages>103--114</pages>
<location>Vancouver.</location>
<contexts>
<context position="56304" citStr="Ninomiya et al. 2005" startWordPosition="9165" endWordPosition="9168">test sentences with gold-standard part-of-speech tags from the Penn Treebank unless otherwise noted. The Gaussian prior was used for smoothing (Chen and Rosenfeld 1999a), and its hyper-parameter was tuned for each model to maximize F-score for the development set. The algorithm for parameter estimation was the limited-memory BFGS method (Nocedal 1980; Nocedal and Wright 1999). The parser was implemented in C++ with the LiLFeS library (Makino et al. 2002), and various speed-up techniques for HPSG parsing were used such as quick check and iterative beam search (Tsuruoka, Miyao, and Tsujii 2004; Ninomiya et al. 2005). Other efficient parsing techniques, including global thresholding, hybrid parsing with a chunk parser, and large constituent inhibition, were not used. The results obtained using these techniques are given in Ninomiya et al. A limit on the number of constituents was set for time-out; the parser stopped parsing when the number of constituents created during parsing exceeded 50,000. In such a case, the parser output nothing, and the recall was computed as zero. Features occurring more than twice were included in the probabilistic models. A method of filtering lexical entries was applied to the</context>
</contexts>
<marker>Ninomiya, Tsuruoka, Miyao, Tsujii, 2005</marker>
<rawString>Ninomiya, Takashi, Yoshimasa Tsuruoka, Yusuke Miyao, and Jun’ichi Tsujii. 2005. Efficacy of beam thresholding, unification filtering and hybrid parsing in probabilistic HPSG parsing. In Proceedings of the 9th International Workshop on Parsing Technologies, pages 103–114, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorge Nocedal</author>
</authors>
<title>Updating quasi-Newton matrices with limited storage.</title>
<date>1980</date>
<journal>Mathematics of Computation,</journal>
<pages>35--773</pages>
<contexts>
<context position="56035" citStr="Nocedal 1980" startWordPosition="9123" endWordPosition="9124">), although exact figures cannot be compared directly because the definitions of dependencies are different. All predicate–argument dependencies in a sentence are the target of evaluation except quotation marks and periods. The accuracy is measured by parsing test sentences with gold-standard part-of-speech tags from the Penn Treebank unless otherwise noted. The Gaussian prior was used for smoothing (Chen and Rosenfeld 1999a), and its hyper-parameter was tuned for each model to maximize F-score for the development set. The algorithm for parameter estimation was the limited-memory BFGS method (Nocedal 1980; Nocedal and Wright 1999). The parser was implemented in C++ with the LiLFeS library (Makino et al. 2002), and various speed-up techniques for HPSG parsing were used such as quick check and iterative beam search (Tsuruoka, Miyao, and Tsujii 2004; Ninomiya et al. 2005). Other efficient parsing techniques, including global thresholding, hybrid parsing with a chunk parser, and large constituent inhibition, were not used. The results obtained using these techniques are given in Ninomiya et al. A limit on the number of constituents was set for time-out; the parser stopped parsing when the number o</context>
</contexts>
<marker>Nocedal, 1980</marker>
<rawString>Nocedal, Jorge. 1980. Updating quasi-Newton matrices with limited storage. Mathematics of Computation, 35:773–782.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorge Nocedal</author>
<author>Stephen J Wright</author>
</authors>
<title>Numerical Optimization.</title>
<date>1999</date>
<publisher>Springer,</publisher>
<location>New York.</location>
<contexts>
<context position="9047" citStr="Nocedal and Wright 1999" startWordPosition="1302" endWordPosition="1305">ature functions can represent any characteristics of events. That is, independence assumptions are unnecessary for the design of feature functions. Hence, this method provides a principled solution for the estimation of consistent probabilistic distributions over feature structure grammars. The remaining issue is how to estimate parameters. Several numerical algorithms, such as Generalized Iterative Scaling (GIS) (Darroch and Ratcliff 1972), Improved Iterative Scaling (IIS) (Della Pietra, Della Pietra, and Lafferty 1997), and the Limitedmemory Broyden-Fletcher-Goldfarb-Shanno method (L-BFGS) (Nocedal and Wright 1999), have been proposed for parameter estimation. Although the algorithm proposed in the present article is applicable to all of the above algorithms, we used L-BFGS for experiments. However, a computational problem arises in these parameter estimation algorithms. The size of Y(x) (i.e., the number of parse trees for a sentence) is generally 37 Computational Linguistics Volume 34, Number 1 very large. This is because local ambiguities in parse trees potentially cause exponential growth in the number of structures assigned to sub-sequences of words, resulting in billions of structures for whole se</context>
<context position="56061" citStr="Nocedal and Wright 1999" startWordPosition="9125" endWordPosition="9128">act figures cannot be compared directly because the definitions of dependencies are different. All predicate–argument dependencies in a sentence are the target of evaluation except quotation marks and periods. The accuracy is measured by parsing test sentences with gold-standard part-of-speech tags from the Penn Treebank unless otherwise noted. The Gaussian prior was used for smoothing (Chen and Rosenfeld 1999a), and its hyper-parameter was tuned for each model to maximize F-score for the development set. The algorithm for parameter estimation was the limited-memory BFGS method (Nocedal 1980; Nocedal and Wright 1999). The parser was implemented in C++ with the LiLFeS library (Makino et al. 2002), and various speed-up techniques for HPSG parsing were used such as quick check and iterative beam search (Tsuruoka, Miyao, and Tsujii 2004; Ninomiya et al. 2005). Other efficient parsing techniques, including global thresholding, hybrid parsing with a chunk parser, and large constituent inhibition, were not used. The results obtained using these techniques are given in Ninomiya et al. A limit on the number of constituents was set for time-out; the parser stopped parsing when the number of constituents created dur</context>
</contexts>
<marker>Nocedal, Wright, 1999</marker>
<rawString>Nocedal, Jorge and Stephen J. Wright. 1999. Numerical Optimization. Springer, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Oepen</author>
<author>John Carroll</author>
</authors>
<title>Ambiguity packing in constraint-based parsing: practical results.</title>
<date>2000</date>
<booktitle>In Proceedings of the First Conference of the North American Chapter of the Association for Computational Linguistics (NAACL</booktitle>
<pages>162--169</pages>
<location>Seattle, WA.</location>
<contexts>
<context position="34071" citStr="Oepen and Carroll 2000" startWordPosition="5531" endWordPosition="5534"> (E, Er, o ) is mapped into a feature forest (C, D, R,-y, b) as follows.6 • C = {(em, el, er) em E E ∧ (el, er) E o (em)} U {w w E w} • D = E • R = {(em, el, er) em E Er ∧ (em, el, er) E C} 4 For simplicity, only binary trees are considered. Extension to unary and n-ary (n &gt; 2) trees is trivial. 5 We assume that CONT and DTRS (a feature used to represent daughter signs) are restricted (Shieber 1985), and we will discuss a method for encoding CONT in a feature forest in Section 4.3. We also assume that parse trees are packed according to equivalence relations rather than subsumption relations (Oepen and Carroll 2000). We cannot simply map parse forests packed under subsumption into feature forests, because they over-generate possible unpacked trees. 6 For ease of explanation, the definition of the root node is different from the original definition given in Section 3. In this section, we define R as a set of conjunctive nodes rather than a single node r. The definition here is translated into the original definition by introducing a dummy root node r&apos; that has no features and only one disjunctive daughter whose daughters are R. 51 Computational Linguistics Volume 34, Number 1 Figure 12 Chart for parsing h</context>
</contexts>
<marker>Oepen, Carroll, 2000</marker>
<rawString>Oepen, Stephan and John Carroll. 2000. Ambiguity packing in constraint-based parsing: practical results. In Proceedings of the First Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2000), pages 162–169, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<date>2002</date>
<booktitle>Collaborative Language Engineering: A Case Study in Efficient Grammar-Based Processing.</booktitle>
<editor>Oepen, Stephan, Dan Flickinger, Jun’ichi Tsujii, and Hans Uszkoreit, editors.</editor>
<publisher>CSLI Publications,</publisher>
<location>Palo Alto, CA.</location>
<contexts>
<context position="5357" citStr="(2002, 2003, 2005)" startWordPosition="745" endWordPosition="747">o, Ninomiya, and Tsujii 2003; Miyao and Tsujii 2003, 2005). Together with the parameter estimation algorithm for feature forest models, these methods constitute a complete procedure for the probabilistic modeling of wide-coverage HPSG parsing. The methods we propose here were applied to an English HPSG parser, Enju (Tsujii Laboratory 2004). We report on an extensive evaluation of the parser through parsing experiments on the Wall Street Journal portion of the Penn Treebank (Marcus et al. 1994). The content of this article is an extended version of our earlier work reported in Miyao and Tsujii (2002, 2003, 2005) and Miyao, Ninomiya, and Tsujii (2003). The major contribution of this article is a strict mathematical definition of the feature forest model and the parameter estimation algorithm, which are substantially refined and extended from Miyao and Tsujii (2002). Another contribution is that this article thoroughly discusses the relationships between the feature forest model and its application to HPSG parsing. We also provide an extensive empirical evaluation of the resulting HPSG parsing approach using real-world text. Section 2 discusses a problem of conventional probabilistic models for lexical</context>
<context position="52708" citStr="(2002)" startWordPosition="8600" endWordPosition="8600">ther notable feature is that we can additionally obtain an HPSG treebank, which can be used as training data for disambiguation models. In the following experiments, this HPSG treebank is used for the training of maximum entropy models. The lexicon used in the following experiments was extracted from Sections 02–21 of the Wall Street Journal portion of the Penn Treebank. This lexicon can assign correct lexical entries to 99.09% of words in the HPSG treebank converted from Penn Treebank Section 23. This number expresses “lexical coverage” in the strong sense defined by Hockenmaier and Steedman (2002). In this notion of “coverage,” this lexicon has 84.1% sentential coverage, where this means that the lexicon can assign correct lexical entries to all of the words in a sentence. Although the parser might produce parse results for uncovered sentences, these parse results cannot be completely correct. 5.2 Experimental Settings The data for the training of the disambiguation models was the HPSG treebank derived from Sections 02–21 of the Wall Street Journal portion of the Penn Treebank, that is, the same set used for lexicon extraction. For training of the disambiguation models, we eliminated s</context>
<context position="74532" citStr="(2002)" startWordPosition="11947" endWordPosition="11947">resenting the similarity of global structures is difficult for feature forest models. Zero-pronoun resolution is also a difficult problem. However, we found that most were indirectly caused by errors of argument/modifier distinction in to-infinitive clauses. A significant portion of the errors discussed above cannot be resolved by the features we investigated in this study, and the design of other features will be necessary for improving parsing accuracy. 6. Discussion 6.1 Probabilistic Modeling of Complete Structures The model described in this article was first published in Miyao and Tsujii (2002), and has been applied to probabilistic models for parsing with lexicalized grammars. Applications to CCG parsing (Clark and Curran 2003, 2004b) and LFG parsing (Kaplan et al. 2004; Riezler and Vasserman 2004) demonstrated that feature forest models attained higher accuracy than other models. These researchers applied feature forests to representations of the packed parse results of LFG and the dependency/derivation structures of CCG. Their work demonstrated the applicability and effectiveness of feature forest models in parsing with wide-coverage lexicalized grammars. Feature forest models we</context>
<context position="84062" citStr="(2002)" startWordPosition="13376" endWordPosition="13376">va, and Manning 2004). However, the problem of exponential explosion is also inevitable using their methods. As described in Section 6.1, an approach similar to ours may be applied, following the study of Taskar et al. (2004). A series of studies on parsing with LFG (Johnson et al. 1999; Riezler et al. 2000, 2002) also proposed a maximum entropy model for probabilistic modeling of LFG parsing. However, similarly to the previous studies on HPSG parsing, these groups had no solution to the problem of exponential explosion of unpacked parse results. As discussed in Section 6.1, Geman and Johnson (2002) proposed an algorithm for maximum entropy estimation for packed representations of LFG parses. Recent studies on CCG have proposed probabilistic models of dependency structures or predicate–argument dependencies, which are essentially the same as the predicate–argument structures described in the present article. Clark, Hockenmaier, and Steedman (2002) attempted the modeling of dependency structures, but the model was inconsistent because of the violation of the independence assumption. Hockenmaier (2003) proposed a consistent generative model of predicate–argument structures. The probability</context>
</contexts>
<marker>2002</marker>
<rawString>Oepen, Stephan, Dan Flickinger, Jun’ichi Tsujii, and Hans Uszkoreit, editors. 2002. Collaborative Language Engineering: A Case Study in Efficient Grammar-Based Processing. CSLI Publications, Palo Alto, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Oepen</author>
<author>Kristina Toutanova</author>
<author>Stuart Shieber</author>
<author>Christopher Manning</author>
<author>Dan Flickinger</author>
<author>Thorsten Brants</author>
</authors>
<title>The LinGO Redwoods treebank motivation and preliminary applications.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics (COLING</booktitle>
<volume>2</volume>
<pages>1--5</pages>
<location>Taipei.</location>
<marker>Oepen, Toutanova, Shieber, Manning, Flickinger, Brants, 2002</marker>
<rawString>Oepen, Stephan, Kristina Toutanova, Stuart Shieber, Christopher Manning, Dan Flickinger, and Thorsten Brants. 2002. The LinGO Redwoods treebank motivation and preliminary applications. In Proceedings of the 19th International Conference on Computational Linguistics (COLING 2002), volume 2, pages 1–5, Taipei.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miles Osborne</author>
</authors>
<title>Estimation of stochastic attribute-value grammar using an informative sample.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics (COLING</booktitle>
<volume>1</volume>
<pages>586--592</pages>
<contexts>
<context position="82734" citStr="Osborne 2000" startWordPosition="13166" endWordPosition="13167">le approach to avoid this problem is to develop a fully restrictive grammar that never causes an exponential explosion, although the development of such a grammar requires considerable effort and it cannot be acquired from treebanks using existing approaches. We think that exponential explosion is inevitable, particularly with the large-scale wide-coverage grammars required to analyze real-world texts. In such cases, these methods of model estimation are intractable. Another approach to estimating log-linear models for HPSG was to extract a small informative sample from the original set T(w) (Osborne 2000). The method was successfully applied to Dutch HPSG parsing (Malouf and van Noord 2004). A possible problem with this method is in the approximation of exponentially many parse trees by a polynomial-size sample. However, their method has an advantage in that any features on parse results can be incorporated into a model, whereas our method forces feature functions to be defined locally on conjunctive nodes. We will discuss the trade-off between the approximation solution and the locality of feature functions in Section 6.3. Non-probabilistic statistical classifiers have also been applied to di</context>
<context position="89453" citStr="Osborne 2000" startWordPosition="14182" endWordPosition="14183">extreme case, for example, if we define features on any co-occurrences of partial parse trees, the full unpacking of parse forests would be necessary, and parameter estimation would be intractable. This indicates that there is a trade-off between the locality of features and the cost of estimation. That is, larger context features might 74 Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing contribute to higher accuracy, while they inflate the size of feature forests and increase the cost of parameter estimation. Sampling techniques (Rosenfeld 1997; Chen and Rosenfeld 1999b; Osborne 2000; Malouf and van Noord 2004) allow us to define any features on complete structures without any constraints. However, they force us to employ approximation methods for tractable computation. The effectiveness of those techniques therefore relies on convergence speed and approximation errors, which may vary depending on the characteristics of target problems and features. It is an open research question whether dynamic programming or sampling can deliver a better balance of estimation efficiency and accuracy. The answer will differ in different problems. When most effective features can be repr</context>
</contexts>
<marker>Osborne, 2000</marker>
<rawString>Osborne, Miles. 2000. Estimation of stochastic attribute-value grammar using an informative sample. In Proceedings of the 18th International Conference on Computational Linguistics (COLING 2000), volume 1, pages 586–592, Saarbr¨ucken.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Andrew McCallum</author>
</authors>
<title>Accurate information extraction from research papers using conditional random fields.</title>
<date>2004</date>
<booktitle>In Proceedings of Human Language Technology Conference and North American Chapter of the Association for Computational Linguistics (HLT/ NAACL-04),</booktitle>
<pages>329--336</pages>
<location>Boston, MA.</location>
<contexts>
<context position="77592" citStr="Peng and McCallum 2004" startWordPosition="12400" endWordPosition="12403">ure forest models correspond to PCFGs. 71 Computational Linguistics Volume 34, Number 1 Extensions of CRFs, such as semi-Markov CRFs (Sarawagi and Cohen 2004), are also regarded as instances of feature forest models. This fact implies that our algorithm is applicable to not only parsing but also to other tasks. CRFs are now widely used for sequence-based tasks, such as parts-of-speech tagging and named entity recognition, and have been shown to achieve the best performance in various tasks (McCallum and Li 2003; McCallum, Rohanimanesh, and Sutton 2003; Pinto et al. 2003; Sha and Pereira 2003; Peng and McCallum 2004; Roark et al. 2004; Settles 2004; Sutton, Rohanimanesh, and McCallum 2004). These results suggest that the method proposed in the present article will achieve high accuracy when applied to various statistical models with tree structures. Dynamic CRFs (McCallum, Rohanimanesh, and Sutton 2003; Sutton, Rohanimanesh, and McCallum 2004) provide us with an interesting inspiration for extending feature forest models. The purpose of dynamic CRFs is to incorporate feature functions that are not represented locally, and the solution is to apply a variational method, which is an algorithm of numerical c</context>
</contexts>
<marker>Peng, McCallum, 2004</marker>
<rawString>Peng, Fuchun and Andrew McCallum. 2004. Accurate information extraction from research papers using conditional random fields. In Proceedings of Human Language Technology Conference and North American Chapter of the Association for Computational Linguistics (HLT/ NAACL-04), pages 329–336, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Pinto</author>
<author>Andrew McCallum</author>
<author>Xen Lee</author>
<author>W Bruce Croft</author>
</authors>
<title>Table extraction using conditional random fields.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR</booktitle>
<pages>235--242</pages>
<location>Toronto.</location>
<contexts>
<context position="77546" citStr="Pinto et al. 2003" startWordPosition="12392" endWordPosition="12395">gy, CRFs correspond to HMMs, whereas feature forest models correspond to PCFGs. 71 Computational Linguistics Volume 34, Number 1 Extensions of CRFs, such as semi-Markov CRFs (Sarawagi and Cohen 2004), are also regarded as instances of feature forest models. This fact implies that our algorithm is applicable to not only parsing but also to other tasks. CRFs are now widely used for sequence-based tasks, such as parts-of-speech tagging and named entity recognition, and have been shown to achieve the best performance in various tasks (McCallum and Li 2003; McCallum, Rohanimanesh, and Sutton 2003; Pinto et al. 2003; Sha and Pereira 2003; Peng and McCallum 2004; Roark et al. 2004; Settles 2004; Sutton, Rohanimanesh, and McCallum 2004). These results suggest that the method proposed in the present article will achieve high accuracy when applied to various statistical models with tree structures. Dynamic CRFs (McCallum, Rohanimanesh, and Sutton 2003; Sutton, Rohanimanesh, and McCallum 2004) provide us with an interesting inspiration for extending feature forest models. The purpose of dynamic CRFs is to incorporate feature functions that are not represented locally, and the solution is to apply a variationa</context>
</contexts>
<marker>Pinto, McCallum, Lee, Croft, 2003</marker>
<rawString>Pinto, David, Andrew McCallum, Xen Lee, and W. Bruce Croft. 2003. Table extraction using conditional random fields. In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2003), pages 235–242, Toronto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago, IL.</location>
<contexts>
<context position="29317" citStr="Pollard and Sag 1994" startWordPosition="4741" endWordPosition="4744">ution here is to apply the feature forest model of Section 3 to the probabilistic modeling of HPSG parsing. Section 4.1 briefly introduces HPSG. Section 4.2 and 4.3 describe how to represent HPSG parse trees and predicate–argument structures by feature forests. Together with the parameter estimation algorithm in Section 3, these methods constitute a complete method for probabilistic disambiguation. We also address a method for accelerating the construction of feature forests for all treebank sentences in Section 4.4. The design of feature functions will be given in Section 4.5. 4.1 HPSG HPSG (Pollard and Sag 1994; Sag, Wasow, and Bender 2003) is a syntactic theory that follows the lexicalist framework. In HPSG, linguistic entities, such as words and phrases, are denoted by signs, which are represented by typed feature structures (Carpenter 1992). Signs are a formal representation of combinations of phonological forms and syntactic/semantic structures, and express which phonological form signifies which syntactic/semantic structure. Figure 10 shows the lexical sign for loves. The geometry of signs follows Pollard and Sag: HEAD represents the part-of-speech of the head word, MOD denotes modifiee constra</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Pollard, Carl and Ivan A. Sag. 1994. Head-Driven Phrase Structure Grammar. University of Chicago Press, Chicago, IL.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Stefan Riezler</author>
<author>Tracy H King</author>
<author>Ronald M Kaplan</author>
<author>Richard Crouch</author>
<author>T John</author>
</authors>
<marker>Riezler, King, Kaplan, Crouch, John, </marker>
<rawString>Riezler, Stefan, Tracy H. King, Ronald M. Kaplan, Richard Crouch, John T.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maxwell</author>
<author>Mark Johnson</author>
</authors>
<title>Parsing the Wall Street Journal using a lexical-functional grammar and discriminative estimation techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>271--278</pages>
<location>Philadephia, PA.</location>
<marker>Maxwell, Johnson, 2002</marker>
<rawString>Maxwell, III, and Mark Johnson. 2002. Parsing the Wall Street Journal using a lexical-functional grammar and discriminative estimation techniques. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL 2002), pages 271–278, Philadephia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Detlef Prescher</author>
<author>Jonas Kuhn</author>
<author>Mark Johnson</author>
</authors>
<title>Lexicalized stochastic modeling of constraint-based grammars using log-linear measures and EM training.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>480--487</pages>
<location>Hong Kong.</location>
<contexts>
<context position="2101" citStr="Riezler et al. 2000" startWordPosition="289" endWordPosition="292">of any data structures is possible when they are represented by feature forests. This article also describes methods for representing HPSG syntactic structures and predicate–argument structures with feature forests. Hence, we describe a complete strategy for developing probabilistic models for HPSG parsing. The effectiveness of the proposed methods is empirically evaluated through parsing experiments on the Penn Treebank, and the promise of applicability to parsing of real-world sentences is discussed. 1. Introduction Following the successful development of wide-coverage lexicalized grammars (Riezler et al. 2000; Hockenmaier and Steedman 2002; Burke et al. 2004; Miyao, Ninomiya, and * Department of Computer Science, University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Japan. E-mail: yusuke@is.s.u-tokyo.ac.jp. ** Department of Computer Science, University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Japan. E-mail: tsujii@is.s.u-tokyo.ac.jp. Submission received: 11 June 2006; revised submission received: 2 March 2007; accepted for publication: 5 May 2007. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 1 Tsujii 2005), statistical modeling of these</context>
<context position="6600" citStr="Riezler et al. 2000" startWordPosition="927" endWordPosition="930"> proposes feature forest models for solving this problem. Section 4 describes the application of feature forest models to probabilistic HPSG parsing. Section 5 presents an empirical evaluation of probabilistic HPSG parsing, and Section 6 introduces research related to our proposals. Section 7 concludes. 2. Problem Maximum entropy models (Berger, Della Pietra, and Della Pietra 1996) are now becoming the de facto standard approach for disambiguation models for lexicalized or 36 Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing feature structure grammars (Johnson et al. 1999; Riezler et al. 2000, 2002; Geman and Johnson 2002; Clark and Curran 2003, 2004b; Kaplan et al. 2004; Carroll and Oepen 2005). Previous studies on probabilistic models for HPSG (Oepen, Toutanova et al. 2002; Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van Noord 2004) have also adopted log-linear models. This is because these grammar formalisms exploit feature structures to represent linguistic constraints. Such constraints are known to introduce inconsistencies in probabilistic models estimated using simple relative frequency, as discussed in Abney (1997). The maximum entropy model is a rea</context>
<context position="83764" citStr="Riezler et al. 2000" startWordPosition="13325" endWordPosition="13328">We will discuss the trade-off between the approximation solution and the locality of feature functions in Section 6.3. Non-probabilistic statistical classifiers have also been applied to disambiguation in HPSG parsing: voted perceptrons (Baldridge and Osborne 2003) and support vector machines (Toutanova, Markova, and Manning 2004). However, the problem of exponential explosion is also inevitable using their methods. As described in Section 6.1, an approach similar to ours may be applied, following the study of Taskar et al. (2004). A series of studies on parsing with LFG (Johnson et al. 1999; Riezler et al. 2000, 2002) also proposed a maximum entropy model for probabilistic modeling of LFG parsing. However, similarly to the previous studies on HPSG parsing, these groups had no solution to the problem of exponential explosion of unpacked parse results. As discussed in Section 6.1, Geman and Johnson (2002) proposed an algorithm for maximum entropy estimation for packed representations of LFG parses. Recent studies on CCG have proposed probabilistic models of dependency structures or predicate–argument dependencies, which are essentially the same as the predicate–argument structures described in the pre</context>
</contexts>
<marker>Riezler, Prescher, Kuhn, Johnson, 2000</marker>
<rawString>Riezler, Stefan, Detlef Prescher, Jonas Kuhn, and Mark Johnson. 2000. Lexicalized stochastic modeling of constraint-based grammars using log-linear measures and EM training. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics (ACL 2000), pages 480–487, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>Alexander Vasserman</author>
</authors>
<title>Incremental feature selection and l1 regularization for relaxed maximumentropy modeling.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP 2004),</booktitle>
<pages>174--181</pages>
<location>Barcelona.</location>
<contexts>
<context position="74741" citStr="Riezler and Vasserman 2004" startWordPosition="11977" endWordPosition="11980">d by errors of argument/modifier distinction in to-infinitive clauses. A significant portion of the errors discussed above cannot be resolved by the features we investigated in this study, and the design of other features will be necessary for improving parsing accuracy. 6. Discussion 6.1 Probabilistic Modeling of Complete Structures The model described in this article was first published in Miyao and Tsujii (2002), and has been applied to probabilistic models for parsing with lexicalized grammars. Applications to CCG parsing (Clark and Curran 2003, 2004b) and LFG parsing (Kaplan et al. 2004; Riezler and Vasserman 2004) demonstrated that feature forest models attained higher accuracy than other models. These researchers applied feature forests to representations of the packed parse results of LFG and the dependency/derivation structures of CCG. Their work demonstrated the applicability and effectiveness of feature forest models in parsing with wide-coverage lexicalized grammars. Feature forest models were also shown to be effective for wide-coverage sentence realization (Nakanishi, Miyao, and Tsujii 2005). This work demonstrated that feature forest models are generic enough to be applied to natural language </context>
<context position="85584" citStr="Riezler and Vasserman 2004" startWordPosition="13594" endWordPosition="13597">rseness, and a heuristic method had to be employed. Probabilities were therefore estimated as the average of individual probabilities conditioned on a single word. Another problem is that the model is no longer consistent when unification constraints such as those in HPSG are introduced. 73 Computational Linguistics Volume 34, Number 1 Our solution is free of these problems, and is applicable to various grammars, not only HPSG and CCG. Most of the state-of-the-art studies on parsing with lexicalized grammars have adopted feature forest models (Clark and Curran 2003, 2004b; Kaplan et al. 2004; Riezler and Vasserman 2004). Their methods of translating parse results into feature forests are basically the same as our method described in Section 4, and details differ because different grammar theories represent syntactic structures differently. They reported higher accuracy in parsing the Penn Treebank than the previous methods introduced herein, and these results attest the effectiveness of feature forest models in practical deep parsing. A remaining problem is that no studies could provide empirical comparisons across grammar theories. The above studies and our research evaluated parsing accuracy on their own t</context>
</contexts>
<marker>Riezler, Vasserman, 2004</marker>
<rawString>Riezler, Stefan and Alexander Vasserman. 2004. Incremental feature selection and l1 regularization for relaxed maximumentropy modeling. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP 2004), pages 174–181, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Murat Saraclar</author>
<author>Michael Collins</author>
<author>Mark Johnson</author>
</authors>
<title>Discriminative language modeling with conditional random fields and the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL 2004),</booktitle>
<pages>47--54</pages>
<location>Barcelona.</location>
<contexts>
<context position="77611" citStr="Roark et al. 2004" startWordPosition="12404" endWordPosition="12407">pond to PCFGs. 71 Computational Linguistics Volume 34, Number 1 Extensions of CRFs, such as semi-Markov CRFs (Sarawagi and Cohen 2004), are also regarded as instances of feature forest models. This fact implies that our algorithm is applicable to not only parsing but also to other tasks. CRFs are now widely used for sequence-based tasks, such as parts-of-speech tagging and named entity recognition, and have been shown to achieve the best performance in various tasks (McCallum and Li 2003; McCallum, Rohanimanesh, and Sutton 2003; Pinto et al. 2003; Sha and Pereira 2003; Peng and McCallum 2004; Roark et al. 2004; Settles 2004; Sutton, Rohanimanesh, and McCallum 2004). These results suggest that the method proposed in the present article will achieve high accuracy when applied to various statistical models with tree structures. Dynamic CRFs (McCallum, Rohanimanesh, and Sutton 2003; Sutton, Rohanimanesh, and McCallum 2004) provide us with an interesting inspiration for extending feature forest models. The purpose of dynamic CRFs is to incorporate feature functions that are not represented locally, and the solution is to apply a variational method, which is an algorithm of numerical computation, to obta</context>
</contexts>
<marker>Roark, Saraclar, Collins, Johnson, 2004</marker>
<rawString>Roark, Brian, Murat Saraclar, Michael Collins, and Mark Johnson. 2004. Discriminative language modeling with conditional random fields and the perceptron algorithm. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL 2004), pages 47–54, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Rosenfeld</author>
</authors>
<title>A whole sentence maximum entropy language model.</title>
<date>1997</date>
<booktitle>In Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding,</booktitle>
<pages>230--237</pages>
<location>Santa Barbara, CA.</location>
<contexts>
<context position="76128" citStr="Rosenfeld 1997" startWordPosition="12177" endWordPosition="12178">s similar to our approach, although their method was designed to traverse LFG parse results represented with disjunctive feature structures as proposed by Maxwell and Kaplan (1995). The difference between the two approaches is that feature forests use a simpler generic data structure to represent packed forest structures. Therefore, without assuming what feature forests represent, our algorithm can be applied to various tasks, including theirs. Another approach to the probabilistic modeling of complete structures is a method of approximation. The work on whole sentence maximum entropy models (Rosenfeld 1997; Chen and Rosenfeld 1999b) proposed an approximation algorithm to estimate parameters of maximum entropy models on whole sentence structures. However, the algorithm suffered from slow convergence, and the model was basically a sequence model. It could not produce a solution for complex structures as our model can. We should also mention Conditional Random Fields (CRFs) (Lafferty, McCallum, and Pereira 2001) for solving a similar problem in the context of maximum entropy Markov models. Their solution was an algorithm similar to the computation of forward/backward probabilities of hidden Markov</context>
<context position="89413" citStr="Rosenfeld 1997" startWordPosition="14176" endWordPosition="14177"> of parameter estimation increases. In an extreme case, for example, if we define features on any co-occurrences of partial parse trees, the full unpacking of parse forests would be necessary, and parameter estimation would be intractable. This indicates that there is a trade-off between the locality of features and the cost of estimation. That is, larger context features might 74 Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing contribute to higher accuracy, while they inflate the size of feature forests and increase the cost of parameter estimation. Sampling techniques (Rosenfeld 1997; Chen and Rosenfeld 1999b; Osborne 2000; Malouf and van Noord 2004) allow us to define any features on complete structures without any constraints. However, they force us to employ approximation methods for tractable computation. The effectiveness of those techniques therefore relies on convergence speed and approximation errors, which may vary depending on the characteristics of target problems and features. It is an open research question whether dynamic programming or sampling can deliver a better balance of estimation efficiency and accuracy. The answer will differ in different problems. </context>
</contexts>
<marker>Rosenfeld, 1997</marker>
<rawString>Rosenfeld, Ronald. 1997. A whole sentence maximum entropy language model. In Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding, pages 230–237, Santa Barbara, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan A Sag</author>
<author>Thomas Wasow</author>
<author>Emily M Bender</author>
</authors>
<title>Syntactic Theory: A Formal Introduction.</title>
<date>2003</date>
<journal>Number</journal>
<booktitle>in CSLI Lecture Notes.</booktitle>
<volume>152</volume>
<publisher>CSLI Publications,</publisher>
<location>Standford, CA.</location>
<marker>Sag, Wasow, Bender, 2003</marker>
<rawString>Sag, Ivan A., Thomas Wasow, and Emily M. Bender. 2003. Syntactic Theory: A Formal Introduction. Number 152 in CSLI Lecture Notes. CSLI Publications, Standford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sunita Sarawagi</author>
<author>William W Cohen</author>
</authors>
<title>Semi-Markov conditional random fields for information extraction.</title>
<date>2004</date>
<booktitle>In Proceedings of the 18th Annual Conference on Neural Information Processing Systems,</booktitle>
<pages>1185--1192</pages>
<location>Vancouver.</location>
<contexts>
<context position="77128" citStr="Sarawagi and Cohen 2004" startWordPosition="12325" endWordPosition="12328">fferty, McCallum, and Pereira 2001) for solving a similar problem in the context of maximum entropy Markov models. Their solution was an algorithm similar to the computation of forward/backward probabilities of hidden Markov models (HMMs). Their algorithm is a special case of our algorithm in which each conjunctive node has only one daughter. This is obvious because feature forests can represent Markov chains. In an analogy, CRFs correspond to HMMs, whereas feature forest models correspond to PCFGs. 71 Computational Linguistics Volume 34, Number 1 Extensions of CRFs, such as semi-Markov CRFs (Sarawagi and Cohen 2004), are also regarded as instances of feature forest models. This fact implies that our algorithm is applicable to not only parsing but also to other tasks. CRFs are now widely used for sequence-based tasks, such as parts-of-speech tagging and named entity recognition, and have been shown to achieve the best performance in various tasks (McCallum and Li 2003; McCallum, Rohanimanesh, and Sutton 2003; Pinto et al. 2003; Sha and Pereira 2003; Peng and McCallum 2004; Roark et al. 2004; Settles 2004; Sutton, Rohanimanesh, and McCallum 2004). These results suggest that the method proposed in the prese</context>
</contexts>
<marker>Sarawagi, Cohen, 2004</marker>
<rawString>Sarawagi, Sunita and William W. Cohen. 2004. Semi-Markov conditional random fields for information extraction. In Proceedings of the 18th Annual Conference on Neural Information Processing Systems, pages 1185–1192, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taisuke Sato</author>
</authors>
<title>A generic approach to em learning for symbolic-statistical models.</title>
<date>2005</date>
<booktitle>In Proceedings of the 4th Learning Language in Logic Workshop (LLL05),</booktitle>
<pages>21--28</pages>
<location>Bonn.</location>
<contexts>
<context position="79857" citStr="Sato 2005" startWordPosition="12736" endWordPosition="12737">t models are applicable to probabilistic models far beyond PCFGs. Feature forests are also isomorphic to support graphs (or explanation graphs) used in the graphical EM algorithm (Kameya and Sato 2000). In their framework, a program in a logic programming language, PRISM (Sato and Kameya 1997), is converted into support graphs, and parameters of probabilistic models are automatically learned by an EM algorithm. Support graphs have been proved to represent various statistical structural models, including HMMs, PCFGs, Bayesian networks, and many other graphical structures (Sato and Kameya 2001; Sato 2005). Taken together, these results imply the high applicability of feature forest models to various real tasks. Because feature forests have a structure isomorphic to parse forests of PCFG, it might seem that they can represent only immediate dominance relations of CFG rules as in PCFG, resulting in only a slight, trivial extension of PCFG. As described herein, however, feature forests can represent structures beyond CFG parse trees. Furthermore, because feature forests are a generalized representation of ambiguous structures, each node in a feature forest need not correspond to a node in a PCFG </context>
</contexts>
<marker>Sato, 2005</marker>
<rawString>Sato, Taisuke. 2005. A generic approach to em learning for symbolic-statistical models. In Proceedings of the 4th Learning Language in Logic Workshop (LLL05), pages 21–28, Bonn.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Sato</author>
</authors>
<title>Taisuke and Yoshitaka Kameya.1997. PRISM: a language for symbolic-statistical modeling.</title>
<booktitle>In Proceedings of the 15th International Joint Conference on Artificial Intelligence (IJCAI ’97),</booktitle>
<pages>1330--1335</pages>
<location>Nagoya.</location>
<marker>Sato, </marker>
<rawString>Sato, Taisuke and Yoshitaka Kameya.1997. PRISM: a language for symbolic-statistical modeling. In Proceedings of the 15th International Joint Conference on Artificial Intelligence (IJCAI ’97), pages 1330–1335, Nagoya.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taisuke Sato</author>
<author>Yoshitaka Kameya</author>
</authors>
<title>Parameter learning of logic programs for symbolic-statistical modeling.</title>
<date>2001</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>15--391</pages>
<contexts>
<context position="79845" citStr="Sato and Kameya 2001" startWordPosition="12732" endWordPosition="12735">tes that feature forest models are applicable to probabilistic models far beyond PCFGs. Feature forests are also isomorphic to support graphs (or explanation graphs) used in the graphical EM algorithm (Kameya and Sato 2000). In their framework, a program in a logic programming language, PRISM (Sato and Kameya 1997), is converted into support graphs, and parameters of probabilistic models are automatically learned by an EM algorithm. Support graphs have been proved to represent various statistical structural models, including HMMs, PCFGs, Bayesian networks, and many other graphical structures (Sato and Kameya 2001; Sato 2005). Taken together, these results imply the high applicability of feature forest models to various real tasks. Because feature forests have a structure isomorphic to parse forests of PCFG, it might seem that they can represent only immediate dominance relations of CFG rules as in PCFG, resulting in only a slight, trivial extension of PCFG. As described herein, however, feature forests can represent structures beyond CFG parse trees. Furthermore, because feature forests are a generalized representation of ambiguous structures, each node in a feature forest need not correspond to a nod</context>
</contexts>
<marker>Sato, Kameya, 2001</marker>
<rawString>Sato, Taisuke and Yoshitaka Kameya. 2001. Parameter learning of logic programs for symbolic-statistical modeling. Journal of Artificial Intelligence Research, 15:391–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burr Settles</author>
</authors>
<title>Biomedical named entity recognition using conditional random fields and rich feature sets.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA),</booktitle>
<pages>104--107</pages>
<location>Geneva.</location>
<contexts>
<context position="77625" citStr="Settles 2004" startWordPosition="12408" endWordPosition="12409">omputational Linguistics Volume 34, Number 1 Extensions of CRFs, such as semi-Markov CRFs (Sarawagi and Cohen 2004), are also regarded as instances of feature forest models. This fact implies that our algorithm is applicable to not only parsing but also to other tasks. CRFs are now widely used for sequence-based tasks, such as parts-of-speech tagging and named entity recognition, and have been shown to achieve the best performance in various tasks (McCallum and Li 2003; McCallum, Rohanimanesh, and Sutton 2003; Pinto et al. 2003; Sha and Pereira 2003; Peng and McCallum 2004; Roark et al. 2004; Settles 2004; Sutton, Rohanimanesh, and McCallum 2004). These results suggest that the method proposed in the present article will achieve high accuracy when applied to various statistical models with tree structures. Dynamic CRFs (McCallum, Rohanimanesh, and Sutton 2003; Sutton, Rohanimanesh, and McCallum 2004) provide us with an interesting inspiration for extending feature forest models. The purpose of dynamic CRFs is to incorporate feature functions that are not represented locally, and the solution is to apply a variational method, which is an algorithm of numerical computation, to obtain approximate</context>
</contexts>
<marker>Settles, 2004</marker>
<rawString>Settles, Burr. 2004. Biomedical named entity recognition using conditional random fields and rich feature sets. In Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA), pages 104–107, Geneva.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Sha</author>
<author>Fernando Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Human Language Technology Conference and North American Chapter of the Association for Computational Linguistics (HLT-NAACL</booktitle>
<pages>213--220</pages>
<location>Edmonton.</location>
<contexts>
<context position="77568" citStr="Sha and Pereira 2003" startWordPosition="12396" endWordPosition="12399"> to HMMs, whereas feature forest models correspond to PCFGs. 71 Computational Linguistics Volume 34, Number 1 Extensions of CRFs, such as semi-Markov CRFs (Sarawagi and Cohen 2004), are also regarded as instances of feature forest models. This fact implies that our algorithm is applicable to not only parsing but also to other tasks. CRFs are now widely used for sequence-based tasks, such as parts-of-speech tagging and named entity recognition, and have been shown to achieve the best performance in various tasks (McCallum and Li 2003; McCallum, Rohanimanesh, and Sutton 2003; Pinto et al. 2003; Sha and Pereira 2003; Peng and McCallum 2004; Roark et al. 2004; Settles 2004; Sutton, Rohanimanesh, and McCallum 2004). These results suggest that the method proposed in the present article will achieve high accuracy when applied to various statistical models with tree structures. Dynamic CRFs (McCallum, Rohanimanesh, and Sutton 2003; Sutton, Rohanimanesh, and McCallum 2004) provide us with an interesting inspiration for extending feature forest models. The purpose of dynamic CRFs is to incorporate feature functions that are not represented locally, and the solution is to apply a variational method, which is an </context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Sha, Fei and Fernando Pereira. 2003. Shallow parsing with conditional random fields. In Proceedings of the 2003 Human Language Technology Conference and North American Chapter of the Association for Computational Linguistics (HLT-NAACL 2003), pages 213–220, Edmonton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>Using restriction to extend parsing algorithms for complex-feature-based formalisms.</title>
<date>1985</date>
<booktitle>In Proceedings of the 23rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>145--152</pages>
<location>Chicago, IL.</location>
<contexts>
<context position="33850" citStr="Shieber 1985" startWordPosition="5497" endWordPosition="5498">forest. Square boxes (ci) are conjunctive nodes, and di disjunctive nodes. A solid arrow represents a disjunctive daughter function, and a dotted line expresses a conjunctive daughter function. Formally, a chart (E, Er, o ) is mapped into a feature forest (C, D, R,-y, b) as follows.6 • C = {(em, el, er) em E E ∧ (el, er) E o (em)} U {w w E w} • D = E • R = {(em, el, er) em E Er ∧ (em, el, er) E C} 4 For simplicity, only binary trees are considered. Extension to unary and n-ary (n &gt; 2) trees is trivial. 5 We assume that CONT and DTRS (a feature used to represent daughter signs) are restricted (Shieber 1985), and we will discuss a method for encoding CONT in a feature forest in Section 4.3. We also assume that parse trees are packed according to equivalence relations rather than subsumption relations (Oepen and Carroll 2000). We cannot simply map parse forests packed under subsumption into feature forests, because they over-generate possible unpacked trees. 6 For ease of explanation, the definition of the root node is different from the original definition given in Section 3. In this section, we define R as a set of conjunctive nodes rather than a single node r. The definition here is translated </context>
</contexts>
<marker>Shieber, 1985</marker>
<rawString>Shieber, Stuart M. 1985. Using restriction to extend parsing algorithms for complex-feature-based formalisms. In Proceedings of the 23rd Annual Meeting on Association for Computational Linguistics, pages 145–152, Chicago, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Khashayar Rohanimanesh</author>
<author>Andrew McCallum</author>
</authors>
<title>Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data.</title>
<date>2004</date>
<booktitle>In Proceedings of the 21st International Conference on Machine Learning (ICML 2004),</booktitle>
<pages>783--790</pages>
<location>Alberta.</location>
<marker>Sutton, Rohanimanesh, McCallum, 2004</marker>
<rawString>Sutton, Charles, Khashayar Rohanimanesh, and Andrew McCallum. 2004. Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data. In Proceedings of the 21st International Conference on Machine Learning (ICML 2004), pages 783–790, Alberta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Dan Klein</author>
<author>Michael Collins</author>
<author>Daphne Koller</author>
<author>Chris Manning</author>
</authors>
<title>Max-margin parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP 2004),</booktitle>
<pages>1--8</pages>
<location>Barcelona.</location>
<contexts>
<context position="80767" citStr="Taskar et al. (2004)" startWordPosition="12874" endWordPosition="12877">ting in only a slight, trivial extension of PCFG. As described herein, however, feature forests can represent structures beyond CFG parse trees. Furthermore, because feature forests are a generalized representation of ambiguous structures, each node in a feature forest need not correspond to a node in a PCFG parse forest. That is, a node in a feature forest may represent any linguistic entity, including a fragment of a syntactic structure, a semantic relation, or other sentence-level information. The idea of feature forest models could be applied to non-probabilistic machine learning methods. Taskar et al. (2004) proposed a dynamic programming algorithm for the learning of large-margin classifiers including support vector machines (Vapnik 1995), and presented its application to disambiguation in CFG parsing. Their algorithm resembles feature forest models; an optimization function is computed by a dynamic programing algorithm without unpacking packed forest structures. From the discussion in this article, it is evident that if the main part of an update formula is represented 72 Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing with (the exponential of) linear combinations, a metho</context>
<context position="83681" citStr="Taskar et al. (2004)" startWordPosition="13309" endWordPosition="13312">eas our method forces feature functions to be defined locally on conjunctive nodes. We will discuss the trade-off between the approximation solution and the locality of feature functions in Section 6.3. Non-probabilistic statistical classifiers have also been applied to disambiguation in HPSG parsing: voted perceptrons (Baldridge and Osborne 2003) and support vector machines (Toutanova, Markova, and Manning 2004). However, the problem of exponential explosion is also inevitable using their methods. As described in Section 6.1, an approach similar to ours may be applied, following the study of Taskar et al. (2004). A series of studies on parsing with LFG (Johnson et al. 1999; Riezler et al. 2000, 2002) also proposed a maximum entropy model for probabilistic modeling of LFG parsing. However, similarly to the previous studies on HPSG parsing, these groups had no solution to the problem of exponential explosion of unpacked parse results. As discussed in Section 6.1, Geman and Johnson (2002) proposed an algorithm for maximum entropy estimation for packed representations of LFG parses. Recent studies on CCG have proposed probabilistic models of dependency structures or predicate–argument dependencies, which</context>
</contexts>
<marker>Taskar, Klein, Collins, Koller, Manning, 2004</marker>
<rawString>Taskar, Ben, Dan Klein, Michael Collins, Daphne Koller, and Chris Manning. 2004. Max-margin parsing. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP 2004), pages 1–8, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher Manning</author>
</authors>
<title>Feature selection for a rich HPSG grammar using decision trees.</title>
<date>2002</date>
<booktitle>In Proceedings of the Sixth Conference on Natural Language Lerning (CoNLL-2002),</booktitle>
<pages>77--83</pages>
<location>Taipei.</location>
<contexts>
<context position="3268" citStr="Toutanova and Manning 2002" startWordPosition="445" endWordPosition="448">me 34, Number 1 Tsujii 2005), statistical modeling of these grammars is attracting considerable attention. This is because natural language processing applications usually require disambiguated or ranked parse results, and statistical modeling of syntactic/semantic preference is one of the most promising methods for disambiguation. The focus of this article is the problem of probabilistic modeling of wide-coverage HPSG parsing. Although previous studies have proposed maximum entropy models (Berger, Della Pietra, and Della Pietra 1996) of HPSG-style parse trees (Oepen, Toutanova, et al. 2002b; Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van Noord 2004), the straightforward application of maximum entropy models to wide-coverage HPSG parsing is infeasible because estimation of maximum entropy models is computationally expensive, especially when targeting wide-coverage parsing. In general, complete structures, such as transition sequences in Markov models and parse trees, have an exponential number of ambiguities. This causes an exponential explosion when estimating the parameters of maximum entropy models. We therefore require solutions to make model estimation tractable. This article fi</context>
<context position="6814" citStr="Toutanova and Manning 2002" startWordPosition="961" endWordPosition="964">ic HPSG parsing, and Section 6 introduces research related to our proposals. Section 7 concludes. 2. Problem Maximum entropy models (Berger, Della Pietra, and Della Pietra 1996) are now becoming the de facto standard approach for disambiguation models for lexicalized or 36 Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing feature structure grammars (Johnson et al. 1999; Riezler et al. 2000, 2002; Geman and Johnson 2002; Clark and Curran 2003, 2004b; Kaplan et al. 2004; Carroll and Oepen 2005). Previous studies on probabilistic models for HPSG (Oepen, Toutanova et al. 2002; Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van Noord 2004) have also adopted log-linear models. This is because these grammar formalisms exploit feature structures to represent linguistic constraints. Such constraints are known to introduce inconsistencies in probabilistic models estimated using simple relative frequency, as discussed in Abney (1997). The maximum entropy model is a reasonable choice for credible probabilistic models. It also allows various overlapping features to be incorporated, and we can expect higher accuracy in disambiguation. A maximum entropy model gives a probabilistic d</context>
<context position="27618" citStr="Toutanova and Manning 2002" startWordPosition="4460" endWordPosition="4463">) ˜|F||E|), where ˜|C |and ˜|D |are the average numbers of conjunctive and disjunctive nodes, respectively. This is tractable when ˜|C |and ˜|D |are of a reasonable size. As noted in this section, the number of 48 Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing nodes in a feature forest is usually polynomial even when that of the unpacked trees is exponential. Thus we can efficiently compute model expectations with polynomial computational complexity. 4. Probabilistic HPSG Parsing Following previous studies on probabilistic models for HPSG (Oepen, Toutanova, et al. 2002; Toutanova and Manning 2002; Baldridge and Osborne 2003; Malouf and van Noord 2004), we apply a maximum entropy model to HPSG parse disambiguation. The probability, p(t|w), of producing parse result t of a given sentence w is defined as p(t|w) =2 p0(t|w)exp Eλifi(t,w) w i where �E � p0(t�|w) exp λi fi(t�, w) EZw = t�∈T(w) i where p0(t|w) is a reference distribution (usually assumed to be a uniform distribution) and T(w) is a set of parse candidates assigned to w. The feature function fi(t,w) represents the characteristics of t and w, and the corresponding model parameter λi is its weight. Model parameters that maximize </context>
<context position="81741" citStr="Toutanova and Manning 2002" startWordPosition="13014" endWordPosition="13017">est structures. From the discussion in this article, it is evident that if the main part of an update formula is represented 72 Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing with (the exponential of) linear combinations, a method similar to feature forest models should be applicable. 6.2 Probabilistic Parsing with Lexicalized Grammars Before the advent of feature forest models, studies on probabilistic models of HPSG adopted conventional maximum entropy models to select the most probable parse from parse candidates given by HPSG grammars (Oepen, Toutanova, et al. 2002; Toutanova and Manning 2002; Baldridge and Osborne 2003). The difference between these studies and our work is that we used feature forests to avoid the exponential increase in the number of structures that results from unpacked parse results. These studies ignored the problem of exponential explosion; in fact, training sets in these studies were very small and consisted only of short sentences. A possible approach to avoid this problem is to develop a fully restrictive grammar that never causes an exponential explosion, although the development of such a grammar requires considerable effort and it cannot be acquired fr</context>
<context position="92773" citStr="Toutanova and Manning 2002" startWordPosition="14659" endWordPosition="14662">raction and retrieval, has also been demonstrated (Miyao et al. 2006; Yakushiji et al. 2006; Chun 2007). Table 16 Final results. Parsing accuracy for Section 23 (&lt;40 words) # parsed sentences 2,137/2,144 (99.7%) Precision/recall 87.69%/87.16% Sentential accuracy 39.2% 75 Computational Linguistics Volume 34, Number 1 From our extensive investigation of HPSG parsing, we observed that exploration of new types of features is indispensable to further improvement of parsing accuracy. A possible research direction is to encode larger contexts of parse trees, which has been shown to improve accuracy (Toutanova and Manning 2002; Toutanova, Markova, and Manning 2004). Future work includes not only the investigation of these features but also the abstraction of predicate–argument dependencies using semantic classes. Experimental results also suggest that an improvement in grammar coverage is crucial for higher accuracy. This indicates that an improvement in the quality of the grammar is a key factor for the improvement of parsing accuracy. The feature forest model provides new insight into the relationship between a linguistic structure and a unit of probability. Traditionally, a unit of probability was implicitly ass</context>
</contexts>
<marker>Toutanova, Manning, 2002</marker>
<rawString>Toutanova, Kristina and Christopher Manning. 2002. Feature selection for a rich HPSG grammar using decision trees. In Proceedings of the Sixth Conference on Natural Language Lerning (CoNLL-2002), pages 77–83, Taipei.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Penka Markova</author>
<author>Christopher Manning</author>
</authors>
<title>The leaf projection path view of parse trees: Exploring string kernels for HPSG parse selection.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP 2004),</booktitle>
<pages>166--173</pages>
<location>Barcelona.</location>
<marker>Toutanova, Markova, Manning, 2004</marker>
<rawString>Toutanova, Kristina, Penka Markova, and Christopher Manning. 2004. The leaf projection path view of parse trees: Exploring string kernels for HPSG parse selection. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing (EMNLP 2004), pages 166–173, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tsujii Laboratory</author>
</authors>
<date>2004</date>
<note>Enju—A practical HPSG parser. Available at http://www. tsujii.is.s.u-tokyo.ac.jp/enju/.</note>
<contexts>
<context position="5080" citStr="Laboratory 2004" startWordPosition="699" endWordPosition="700">es in PCFG parsing. The latter half of this article (Section 4) is on the application of feature forest models to disambiguation in wide-coverage HPSG parsing. We describe methods for representing HPSG parse trees and predicate–argument structures using feature forests (Miyao, Ninomiya, and Tsujii 2003; Miyao and Tsujii 2003, 2005). Together with the parameter estimation algorithm for feature forest models, these methods constitute a complete procedure for the probabilistic modeling of wide-coverage HPSG parsing. The methods we propose here were applied to an English HPSG parser, Enju (Tsujii Laboratory 2004). We report on an extensive evaluation of the parser through parsing experiments on the Wall Street Journal portion of the Penn Treebank (Marcus et al. 1994). The content of this article is an extended version of our earlier work reported in Miyao and Tsujii (2002, 2003, 2005) and Miyao, Ninomiya, and Tsujii (2003). The major contribution of this article is a strict mathematical definition of the feature forest model and the parameter estimation algorithm, which are substantially refined and extended from Miyao and Tsujii (2002). Another contribution is that this article thoroughly discusses t</context>
<context position="50446" citStr="Laboratory 2004" startWordPosition="8261" endWordPosition="8262">Y – – Y – 60 Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing 5. Experiments This section presents experimental results on the parsing accuracy attained by the feature forest models. In all of the following experiments, we use the HPSG grammar developed by the method of Miyao, Ninomiya, and Tsujii (2005). Section 5.1 describes how this grammar was developed. Section 5.2 explains other aspects of the experimental settings. In Sections 5.3 to 5.7, we report results of the experiments on HPSG parsing. 5.1 The HPSG Grammar In the following experiments, we use Enju 2.1 (Tsujii Laboratory 2004), which is a widecoverage HPSG grammar extracted from the Penn Treebank by the method of Miyao, Ninomiya, and Tsujii (2005). In this method, we convert the Penn Treebank into an HPSG treebank, and collect HPSG lexical entries from terminal nodes of the HPSG treebank. Figure 22 illustrates the process of treebank conversion and lexicon collection. We first convert and fertilize parse trees of the Penn Treebank. This step identifies syntactic constructions that require special treatment in HPSG, such as raising/control and long-distance dependencies. These constructions are then annotated with t</context>
</contexts>
<marker>Laboratory, 2004</marker>
<rawString>Tsujii Laboratory. 2004. Enju—A practical HPSG parser. Available at http://www. tsujii.is.s.u-tokyo.ac.jp/enju/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Towards efficient probabilistic HPSG parsing: Integrating semantic and syntatic preference to guide the parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the IJCNLP-04 Workshop “Beyond Shallow Analyses”,</booktitle>
<location>Hainan</location>
<marker>Tsuruoka, Miyao, Tsujii, 2004</marker>
<rawString>Tsuruoka, Yoshimasa, Yusuke Miyao, and Jun’ichi Tsujii. 2004. Towards efficient probabilistic HPSG parsing: Integrating semantic and syntatic preference to guide the parsing. In Proceedings of the IJCNLP-04 Workshop “Beyond Shallow Analyses”, Hainan Island. Available at www.tsujii.is.s.u-tokyo.ac.jp/bsa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Bidirectional inference with the easiest-first strategy for tagging sequence data.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP</booktitle>
<pages>467--474</pages>
<location>Vancouver.</location>
<contexts>
<context position="67803" citStr="Tsuruoka and Tsujii 2005" startWordPosition="10966" endWordPosition="10969">acy is significantly higher for sentences with less than 10 words. This implies that experiments with only short sentences overestimate the performance of parsers. Sentences with at least 10 words are necessary to properly evaluate the performance of parsing real-world texts. The accuracies for the sentences with more than 10 words are not very different, although data points for sentences with more than 50 words are not reliable. Table 14 shows the accuracies for predicate–argument relations when partsof-speech tags are assigned automatically by a maximum-entropy-based parts-ofspeech tagger (Tsuruoka and Tsujii 2005). The results indicate a drop of about three points in labeled precision/recall (a two-point drop in unlabeled precision/recall). A reason why we observed larger accuracy drops in labeled precision/recall is that 67 Computational Linguistics Volume 34, Number 1 Figure 23 Corpus size vs. accuracy. Figure 24 Sentence length vs. accuracy. predicate–argument relations are fragile with respect to parts-of-speech errors because predicate types (e.g., adjective, intransitive verb) are determined depending on the parts-of-speech of predicate words. Although our current parsing strategy assumes that pa</context>
</contexts>
<marker>Tsuruoka, Tsujii, 2005</marker>
<rawString>Tsuruoka, Yoshimasa and Jun’ichi Tsujii. 2005. Bidirectional inference with the easiest-first strategy for tagging sequence data. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP 2005), pages 467–474, Vancouver.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer-Verlag,</publisher>
<location>New York.</location>
<contexts>
<context position="80901" citStr="Vapnik 1995" startWordPosition="12893" endWordPosition="12894">s. Furthermore, because feature forests are a generalized representation of ambiguous structures, each node in a feature forest need not correspond to a node in a PCFG parse forest. That is, a node in a feature forest may represent any linguistic entity, including a fragment of a syntactic structure, a semantic relation, or other sentence-level information. The idea of feature forest models could be applied to non-probabilistic machine learning methods. Taskar et al. (2004) proposed a dynamic programming algorithm for the learning of large-margin classifiers including support vector machines (Vapnik 1995), and presented its application to disambiguation in CFG parsing. Their algorithm resembles feature forest models; an optimization function is computed by a dynamic programing algorithm without unpacking packed forest structures. From the discussion in this article, it is evident that if the main part of an update formula is represented 72 Miyao and Tsujii Feature Forest Models for Probabilistic HPSG Parsing with (the exponential of) linear combinations, a method similar to feature forest models should be applicable. 6.2 Probabilistic Parsing with Lexicalized Grammars Before the advent of feat</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vapnik, Vladimir N. 1995. The Nature of Statistical Learning Theory. Springer-Verlag, New York.</rawString>
</citation>
<citation valid="false">
<authors>
<author>K Vijay-Shanker</author>
<author>David J Weir</author>
<author>Aravind K Joshi 1987</author>
</authors>
<title>Characterizing structural descriptions produced by various grammatical formalisms.</title>
<booktitle>In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>104--111</pages>
<location>Palo Alto, CA.</location>
<marker>Vijay-Shanker, Weir, 1987, </marker>
<rawString>Vijay-Shanker, K., David J. Weir, and Aravind K. Joshi.1987. Characterizing structural descriptions produced by various grammatical formalisms. In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics, pages 104–111, Palo Alto, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David J Weir</author>
</authors>
<title>Characterizing Mildly Context-Sensitive Grammar Formalisms.</title>
<date>1988</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="79087" citStr="Weir 1988" startWordPosition="12623" endWordPosition="12624">computational linguistics. As is easily seen, lattices, Markov chains, and CFG parse trees are represented by feature forests. Furthermore, because conjunctive nodes do not necessarily represent CFG nodes or rules and terminals of feature forests need not be words, feature forests can express any forest structures in which ambiguities are packed in local structures. Examples include the derivation trees of LTAG and CCG. Chiang (2003) proved that feature forests could be considered as the derivation forests of linear context-free rewriting systems (LCFRSs) (Vijay-Shanker, Weir, and Joshi 1987; Weir 1988). LCFRSs define a wide variety of grammars, including LTAG and CCG, while preserving polynomial-time complexity of parsing. This demonstrates that feature forest models are applicable to probabilistic models far beyond PCFGs. Feature forests are also isomorphic to support graphs (or explanation graphs) used in the graphical EM algorithm (Kameya and Sato 2000). In their framework, a program in a logic programming language, PRISM (Sato and Kameya 1997), is converted into support graphs, and parameters of probabilistic models are automatically learned by an EM algorithm. Support graphs have been </context>
</contexts>
<marker>Weir, 1988</marker>
<rawString>Weir, David J. 1988. Characterizing Mildly Context-Sensitive Grammar Formalisms. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akane Yakushiji</author>
<author>Yusuke Miyao</author>
<author>Tomoko Ohta</author>
<author>Yuka Tateisi</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Automatic construction of predicate-argument structure patterns for biomedical information extraction.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>284--292</pages>
<location>Sydney.</location>
<contexts>
<context position="92238" citStr="Yakushiji et al. 2006" startWordPosition="14582" endWordPosition="14585">resented approach can be regarded as a general solution to the probabilistic modeling of syntactic analysis with lexicalized grammars. Table 16 summarizes the best performance of the HPSG parser described in this article. The parser demonstrated impressively high coverage and accuracy for real-world texts. We therefore conclude that the HPSG parser for English is moving toward a practical level of use in real-world applications. Recently, the applicability of the HPSG parser to practical applications, such as information extraction and retrieval, has also been demonstrated (Miyao et al. 2006; Yakushiji et al. 2006; Chun 2007). Table 16 Final results. Parsing accuracy for Section 23 (&lt;40 words) # parsed sentences 2,137/2,144 (99.7%) Precision/recall 87.69%/87.16% Sentential accuracy 39.2% 75 Computational Linguistics Volume 34, Number 1 From our extensive investigation of HPSG parsing, we observed that exploration of new types of features is indispensable to further improvement of parsing accuracy. A possible research direction is to encode larger contexts of parse trees, which has been shown to improve accuracy (Toutanova and Manning 2002; Toutanova, Markova, and Manning 2004). Future work includes not</context>
</contexts>
<marker>Yakushiji, Miyao, Ohta, Tateisi, Tsujii, 2006</marker>
<rawString>Yakushiji, Akane, Yusuke Miyao, Tomoko Ohta, Yuka Tateisi, and Jun’ichi Tsujii. 2006. Automatic construction of predicate-argument structure patterns for biomedical information extraction. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 284–292, Sydney.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>