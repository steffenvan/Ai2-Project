<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.996003">
Managing Uncertainty in Semantic Tagging
</title>
<author confidence="0.982878">
Silvie Cinkov´a and Martin Holub and Vincent Kr´ıˇz
</author>
<affiliation confidence="0.985845">
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
</affiliation>
<email confidence="0.9883565">
{cinkova|holub}@ufal.mff.cuni.cz
vincent.kriz@gmail.com
</email>
<sectionHeader confidence="0.997303" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99947905">
Low interannotator agreement (IAA) is a
well-known issue in manual semantic tag-
ging (sense tagging). IAA correlates with
the granularity of word senses and they
both correlate with the amount of informa-
tion they give as well as with its reliability.
We compare different approaches to seman-
tic tagging in WordNet, FrameNet, Prop-
Bank and OntoNotes with a small tagged
data sample based on the Corpus Pattern
Analysis to present the reliable information
gain (RG), a measure used to optimize the
semantic granularity of a sense inventory
with respect to its reliability indicated by
the IAA in the given data set. RG can also
be used as feedback for lexicographers, and
as a supporting component of automatic se-
mantic classifiers, especially when dealing
with a very fine-grained set of semantic cat-
egories.
</bodyText>
<sectionHeader confidence="0.999453" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.943393307692308">
The term semantic tagging is used in two diver-
gent areas:
1) recognizing objects of semantic importance,
such as entities, events and polarity, often tailored
to a restricted domain, or
2) relating occurrences of words in a corpus to a
lexicon and selecting the most appropriate seman-
tic categories (such as synsets, semantic frames,
wordsenses, semantic patterns or framesets).
We are concerned with the second case, which
seeks to make lexical semantics tractable for com-
puters. Lexical semantics, as opposed to proposi-
tional semantics, focuses the meaning of lexical
items. The disciplines that focus lexical seman-
tics are lexicology and lexicography rather than
logic. By semantic tagging we mean a process of
assigning semantic categories to target words in
given contexts. This process can be either manual
or automatic.
Traditionally, semantic tagging relies on the
tacit assumption that various uses of polysemous
words can be sorted into discrete senses; under-
standing or using an unfamiliar word be then like
looking it up in a dictionary. When building a dic-
tionary entry for a given word, the lexicographer
sorts a number of its occurrences into discrete
senses present (or emerging) in his/her mental lex-
icon, which is supposed to be shared by all speak-
ers of the same language. The assumed common
mental representation of a words meaning should
make it easy for other humans to assign random
occurrences of the word to one of the pre-defined
senses (Fellbaum et al., 1997).
This assumption seems to be falsified by the
interannotator agreement (IAA, sometimes ITA)
constantly reported much lower in semantic than
in morphological or syntactic annotation, as well
as by the general divergence of opinion on which
value of which IAA measure indicates a reliable
annotation. In some projects (e.g. OntoNotes
(Hovy et al., 2006)), the percentage of agreements
between two annotators is used, but a number
of more complex measures are available (for a
comprehensive survey see (Artstein and Poesio,
2008)). Consequently, using different measures
for IAA makes the reported IAA values incompa-
rable across different projects.
Even skilled lexicographers have trouble se-
lecting one discrete sense for a concordance (Kr-
ishnamurthy and Nicholls, 2000), and, more to
say, when the tagging performance of lexicog-
raphers and ordinary annotators (students) was
</bodyText>
<page confidence="0.954697">
840
</page>
<note confidence="0.9756585">
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 840–850,
Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999973880952381">
compared, the experiment showed that the men-
tal representations of a word’s semantics differ for
each group (Fellbaum et al., 1997), and cf. (Jor-
gensen, 1990). Lexicographers are trained in con-
sidering subtle differences among various uses of
a word, which ordinary language users do not re-
flect. Identifying a semantic difference between
uses of a word and deciding whether a difference
is important enough to constitute a separate sense
means presenting a word with a certain degree
of semantic granularity. Intuitively, the finer the
granularity of a word entry is, the more oppor-
tunities for interannotator disagreement there are
and the lower IAA can be expected. Brown et al.
proved this hypothesis experimentally (Brown et
al., 2010). Also, the annotators are less confident
in their decisions, when they have many options
to choose from (Fellbaum et al. (1998) reported a
drop in subjective annotators confidence in words
with 8+ senses).
Despite all the known issues in semantic tag-
ging, the major lexical resources (WordNet (Fell-
baum, 1998), FrameNet (Ruppenhofer et al.,
2010), PropBank (Palmer et al., 2005) and the
word-sense part of OntoNotes (Weischedel et al.,
2011)) are still maintained and their annotation
schemes are adopted for creating new manually
annotated data (e.g. MASC, the Manually An-
notated Subcorpus (Ide et al., 2008)). More to
say, these resources are not only used in WSD and
semantic labeling, but also in research directions
that in their turn do not rely on the idea of an in-
ventory of discrete senses any more, e.g. in dis-
tributional semantics (Erk, 2010) and recognizing
textual entailment (e.g. (Zanzotto et al., 2009) and
(Aharon et al., 2010)).
It is a remarkable fact that, to the best of our
knowledge, there is no measure that would relate
granularity, reliability of the annotation (derived
from IAA) and the resulting information gain.
Therefore it is impossible to say where the opti-
mum for granularity and IAA lies.
</bodyText>
<sectionHeader confidence="0.99566" genericHeader="method">
2 Approaches to semantic tagging
</sectionHeader>
<subsectionHeader confidence="0.8999735">
2.1 Semantic tagging vs. morphological or
syntactic analysis
</subsectionHeader>
<bodyText confidence="0.999970054545455">
Manual semantic tagging is in many respects sim-
ilar to morphological tagging and syntactic anal-
ysis: human annotators are trained to sort cer-
tain elements occurring in a running text ac-
cording to a reference source. There is, never-
theless, a substantial difference: whereas mor-
phologically or syntactically annotated data ex-
ist separately from the reference (tagset, anno-
tation guide, annotation scheme), a semantically
tagged resource can be regarded both as a cor-
pus of texts disambiguated according to an at-
tached inventory of semantic categories and as
a lexicon with links to example concordances
for each semantic category. So, in semanti-
cally tagged resources, the data and the reference
are intertwined. Such double-faced semantic re-
sources have also been called semantic concor-
dances (Miller et al., 1993a). For instance, one of
the earlier versions of WordNet, the largest lexi-
cal resource for English, was used in the seman-
tic concordance SemCor (Miller et al., 1993b).
More recent lexical resources have been built as
semantic concordances from the very beginning
(PropBank (Palmer et al., 2005), OntoNotes word
senses (Weischedel et al., 2011)).
In morphological or syntactic annotation, the
tagset or inventory of constituents are given be-
forehand and are supposed to hold for all to-
kens/sentences contained in the corpus. Prob-
lematic and theory-dependent issues are few and
mostly well-known in advance. Therefore they
can be reflected by a few additional conventions in
the annotation manual (e.g. where to draw the line
between particles and prepositions or between ad-
jectives and verbs in past participles (Santorini,
1990) or where to attach a prepositional phrase
following a noun phrase and how to treat specific
“financialspeak” structures (Bies et al., 1995)).
Even in difficult cases, there are hardly more than
two options of interpretation. Data manually an-
notated for morphology or surface syntax are reli-
able enough to train syntactic parsers with an ac-
curacy above 80 % (e.g. (Zhang and Clark, 2011;
McDonald et al., 2006)).
On the other hand, semantic tagging actually
employs a different tagset for each word lemma.
Even within the same part of speech, individual
words require individual descriptions. Possible
similarities among them come into relief ex post
rather than that they could be imposed on the lex-
icographers from the beginning. When assign-
ing senses to concordances, the annotator often
has to select among more than two relevant op-
tions. These two aspects make achieving good
IAA much harder than in morphology and syn-
</bodyText>
<page confidence="0.995632">
841
</page>
<bodyText confidence="0.999725363636364">
tax tasks. In addition, while a linguistically edu-
cated annotator can have roughly the same idea of
parts of speech as the author of the tagset, there
is no chance that two humans (not even two pro-
fessional lexicographers) would create identical
entries for e.g. a polysemous verb. Any human
evaluation of complete entries would be subjec-
tive. The maximum to be achieved is that the en-
try reflects the corpus data in a reasonable gran-
ular way on which annotators still can reach rea-
sonable IAA.
</bodyText>
<subsectionHeader confidence="0.999677">
2.2 Major existing semantic resources
</subsectionHeader>
<bodyText confidence="0.99997046969697">
The granularity vs. IAA equilibrium is of great
concern in creating lexical resources as well as in
applications dealing with semantic tasks. When
WordNet (Fellbaum, 1998) was created, both IAA
and subjective confidence measurements served
as an informal feedback to lexicographers (Fell-
baum et al., (1998), p. 200). In general, WordNet
has been considered a resource too fine-grained
for most annotations (and applications). Nav-
igli (2006) developed a method of reducing the
granularity of WordNet by mapping the synsets
to senses in a more coarse-grained dictionary. A
manual, more coarse-grained grouping of Word-
Net senses has been performed in OntoNotes
(Weischedel et al., 2011). The OntoNotes 90%
solution (Hovy et al., 2006) actually means such
a degree of granularity that enables a 90-%-IAA.
OntoNotes is a reaction to the traditionally poor
IAA in WordNet annotated corpora, caused by the
high granularity of senses. The quality of seman-
tic concordances is maintained by numerous itera-
tions between lexicographers and annotators. The
categories ‘right’–‘wrong’ have been, for the pur-
pose of the annotated linguistic resource, defined
by the IAA score, which is—in OntoNotes—
calculated as the percentage of agreements be-
tween two annotators.
Two other, somewhat different, lexical re-
sources have to be mentioned to complete the pic-
ture: FrameNet (Ruppenhofer et al., 2010) and
PropBank (Palmer et al., 2005). While Word-
Net and OntoNotes pair words and word senses in
a way comparable to printed lexicons, FrameNet
is primarily an inventory of semantic frames and
PropBank focuses the argument structure of verbs
and nouns (NomBank (Meyers et al., 2008), a re-
lated project capturing the argument structure of
nouns, was later integrated in OntoNotes).
In FrameNet corpora, content words are associ-
ated to particular semantic frames that they evoke
(e.g. charm would relate to the Aesthetics frame)
and their collocates in relevant syntactic positions
(arguments of verbs, head nouns of adjectives,
etc.) would be assigned the corresponding frame-
element labels (e.g. in their dazzling charm, their
would be The Entity for which a particular grad-
able Attribute is appropriate and under considera-
tion and dazzling would be Degree). Neither IAA
nor granularity seem to be an issue in FrameNet.
We have not succeeded in finding a report on IAA
in the original FrameNet annotation, except one
measurement in progress in the annotation of the
Manually Annotated Subcorpus of English (Ide et
al., 2008).1
PropBank is a valency (argument structure) lex-
icon. The current resource lists and labels ar-
guments and obligatory modifiers typical of each
(very coarse) word sense (called frameset). Two
core criteria for distinguishing among framesets
are the semantic roles of the arguments along
with the syntactic alternations that the verb can
undergo with that particular argument set. To
keep low granularity, this lexicon—among other
things—does usually not make special framesets
for metaphoric uses. The overall IAA measured
on verbs was 94 % (Palmer et al., 2005).
</bodyText>
<subsectionHeader confidence="0.998776">
2.3 Semantic Pattern Recognition
</subsectionHeader>
<bodyText confidence="0.999540388888889">
From corpus-based lexicography to semantic
patterns
The modern, corpus-based lexicology of 1990s
(Sinclair, 1991; Fillmore and Atkins, 1994) has
had a great impact on lexicography. There is a
general consensus that dictionary definitions need
to be supported by corpus examples. Cf. Fell-
baum (2001):
“For polysemous words, dictionaries [... ] do
not say enough about the range of possible con-
texts that differentiate the senses. [... ] On the
other hand, texts or corpora [... ] are not ex-
plicit about the word’s meaning. When we first
encounter a new word in a text, we can usually
form only a vague idea of its meaning; checking a
dictionary will clarify the meaning. But the more
contexts we encounter for a word, the harder it is
to match them against only one dictionary sense.”
</bodyText>
<footnote confidence="0.9763275">
1Checked on the project web www.anc.org/MASC/Home
2011-10-29.
</footnote>
<page confidence="0.995814">
842
</page>
<bodyText confidence="0.9999826">
The lexical description in modern English
monolingual dictionaries (Sinclair et al., 1987;
Rundell, 2002) explicitly emphasizes contextual
clues, such as typical collocates and the syntac-
tic surroundings of the given lexical item, rather
than relying on very detailed definitions. In
other words, the sense definitions are obtained
as syntactico-semantic abstractions of manually
clustered corpus concordances in the modern
corpus-based lexicography: in classical dictionar-
ies as well as in semantic concordances.
Nevertheless, the word senses, even when ob-
tained by a collective mind of lexicographers and
annotators, are naturally hard-wired and tailored
to the annotated corpus. They may be too fine-
grained or too coarse-grained for automatic pro-
cessing of different corpora (e.g. a restricted-
domain corpus). Kilgarriff (1997, p. 115) shows
(the handbag example) that there is no reason to
expect the same set of word senses to be relevant
for different tasks and that the corpus dictates the
word senses and therefore ‘word sense’ was not
found to be sufficiently well-defined to be a work-
able basic unit of meaning (p. 116). On the other
hand, even non-experts seem to agree reasonably
well when judging the similarity of use of a word
in different contexts (Rumshisky et al., 2009). Erk
et al. (2009) showed promising annotation results
with a scheme that allowed the annotators graded
judgments of similarity between two words or be-
tween a word and its definition.
Verbs are the most challenging part of speech.
We see two major causes: vagueness and coer-
cion. We neglect ambiguity, since it has proved to
be rare in our experience.
</bodyText>
<sectionHeader confidence="0.855992" genericHeader="method">
CPA and PDEV
</sectionHeader>
<bodyText confidence="0.99993603125">
Our current work focuses on English verbs.
It has been inspired by the manual Corpus Pat-
tern Analysis method (CPA) (Hanks, forthcom-
ing) and its implementation, the Pattern Dictio-
nary of English Verbs (PDEV) (Hanks and Puste-
jovsky, 2005). PDEV is a semantic concordance
built on yet a different principle than FrameNet,
WordNet, PropBank or OntoNotes. The man-
ually extracted patterns of frequent and normal
verb uses are, roughly speaking, intuitively sim-
ilar uses of a verb that express—in a syntacti-
cally similar form—a similar event in which sim-
ilar participants (e.g. humans, artifacts, institu-
tions, other events) are involved. Two patterns
can be semantically so tightly related that they
could appear together under one sense in a tradi-
tional dictionary. The patterns are not senses but
syntactico-semantically characterized prototypes
(see the example verb submit in Table 1). Con-
cordances that match these prototypes well are
called norms in Hanks (forthcoming). Concor-
dances that match with a reservation (metaphor-
ical uses, argument mismatch, etc.) are called ex-
ploitations. The PDEV corpus annotation indi-
cates the norm-exploitation status for each con-
cordance.
Compared to other semantic concordances, the
granularity of PDEV is high and thus discourag-
ing in terms of expected IAA. However, select-
ing among patterns does not really mean disam-
biguating a concordance but rather determining to
which pattern it is most similar—a task easier for
humans than WSD is. This principle seems par-
ticularly promising for verbs as words expressing
events, which resist the traditional word sense dis-
ambiguation the most.
A novel approach to semantic tagging
We present the semantic pattern recognition as
a novel approach to semantic tagging, which is
different from the traditional word-sense assign-
ment tasks. We adopt the central idea of CPA that
words do not have fixed senses but that regular
patterns can be identified in the corpus that ac-
tivate different conversational implicatures from
the meaning potential of the given verb. Our
method draws on a hard-wired, fine-grained in-
ventory of semantic categories manually extracted
from corpus data. This inventory represents the
maximum semantic granularity that humans are
able to recognize in normal and frequent uses of a
verb in a balanced corpus. We thoroughly analyze
the interannotator agreement to find out which of
the highly semantic categories are useful in the
sense of information gain. Our goal is a dynamic
optimization of semantic granularity with respect
to given data and target application.
Like Passonneau et al. (2010), we are con-
vinced that IAA is specific to each respective
word and reflects its inherent semantic properties
as well as the specificity of contexts the given
word occurs in, even within the same balanced
corpus. We accept as a matter of fact that inter-
annotator confusion is inevitable in semantic tag-
ging. However, the amount of uncertainty of the
</bodyText>
<page confidence="0.996882">
843
</page>
<table confidence="0.999906857142857">
No. Pattern / Implicature
1 [[Human 1  |Institution 1] &amp;quot; [Human 1  |Institution 1 = Competitor]] submit [[Plan  |Document
 |Speech Act  |Proposition  |{complaint  |demand  |request  |claim  |application  |proposal
 |report  |resignation  |information  |plea  |petition  |memorandum  |budget  |amendment |
programme  |... }] &amp;quot; [Artifact  |Artwork  |Service  |Activity  |{design  |tender  |bid  |entry
 |dance  |... }]] (({to} Human 2  |Institution 2 = authority)&amp;quot;({to} Human 2  |Institution 2 =
referee)) ({for} {approval  |discussion  |arbitration  |inspection  |designation  |assessment |
funding  |taxation  |... })
[[Human 1  |Institution 1]] presents [[Plan  |Document]] to [[Human 2  |Institution 2]] for {approval
 |discussion  |arbitration  |inspection  |designation  |assessment  |taxation  |... }
2 [Human  |Institution] submit [THAT-CL|QUOTE]
[[Human  |Institution]] respectfully expresses {that [CLAUSE]} and invites listeners or readers to
accept that {that [CLAUSE]} is true}
4 [Human 1  |Institution 1] submit (Self) ({to} Human 2  |Institution 2)
[[Human 1  |Institution 1]] acknowledges the superior force of [[Human 2  |Institution 2]] and puts
[[Self]] in the power of [[Human 2  |Institution 2]]
5 [Human 1] submit (Self) [[{to} Eventuality = Unpleasant] &amp;quot; [{to} Rule]]
[[Human 1]] accepts [[Rule |Eventuality = Unpleasant]] without complaining
6 [passive]
[Human |Institution] submit [Anything] [{to} Eventuality]
[[Human 1|Institution 1]] exposes [[Anything]] to [[Eventuality]]
</table>
<tableCaption confidence="0.999929">
Table 1: Example of patterns defined for the verb submit.
</tableCaption>
<bodyText confidence="0.999893333333333">
“right” tag differs a lot, and should be quantified.
For that purpose we developed the reliable infor-
mation gain measure presented in Section 3.2.
</bodyText>
<sectionHeader confidence="0.686795" genericHeader="method">
CPA Verb Validation Sample
</sectionHeader>
<bodyText confidence="0.974435388888889">
The original PDEV had never been tested with
respect to IAA. Each entry had been based on
concordances annotated solely by the author of
that particular entry. The annotation instructions
had been transmitted only orally. The data had
been evolving along with the method, which im-
plied inconsistencies. We put down an annotation
manual (a momentary snapshot of the theory) and
trained three annotators accordingly. For practical
annotation we use the infrastructure developed at
Masaryk University in Brno (Hor´ak et al., 2008),
which was also used for the original PDEV de-
velopment. After initial IAA experiments with
the original PDEV, we decided to select 30 verb
entries from PDEV along with the annotated con-
cordances. We made a new semantic concordance
sample (Cinkov´a et al., 2012) for the validation of
the annotation scheme. We refer to this new col-
lection2 as VPS-30-En (Verb Pattern Sample, 30
English verbs).
We slightly revised some entries and updated
the reference samples (usually 250 concordances
2This new lexical resource, including the complete docu-
mentation, is publicly available at http://ufal.mff.cuni.cz/spr.
per verb). The annotators were given the en-
tries as well as the reference sample annotated
by the lexicographer and a test sample of 50 con-
cordances for annotation. We measured IAA, us-
ing Fleiss’s kappa,3 and analyzed the interannota-
tor confusion manually. IAA varied from verb to
verb, mostly reaching safely above 0.6. When the
IAA was low and the type of confusion indicated a
problem in the entry, the entry was revised. Then
the lexicographer revised the original reference
sample along with the first 50-concordance sam-
ple. The annotators got back the revised entry, the
newly revised reference sample and an entirely
new 50-concordance annotation batch. The fi-
nal multiple 50-concordance sample went through
one more additional procedure, the adjudication:
first, the lexicographer compared the three anno-
tations and eliminated evident errors. Then the
lexicographer selected one value for each concor-
dance to remain in the resulting one-value-per-
concordance gold standard data and recorded it
into the gold standard set. The adjudication pro-
3Fleiss’s kappa (Fleiss, 1971) is a generalization of
Scott’s it statistic (Scott, 1955). In contrast to Cohen’s kappa
(Cohen, 1960), Fleiss’s kappa evaluates agreement between
multiple raters. However, Fleiss’s kappa is not a generaliza-
tion of Cohen’s kappa, which is a different, yet related, sta-
tistical measure. Sometimes, the terminology about kappas
is confusing in the literature. For a detailed explanation refer
e.g. to (Artstein and Poesio, 2008).
</bodyText>
<page confidence="0.990683">
844
</page>
<bodyText confidence="0.99966975">
tocol has been kept for further experiments. All
values except the marked errors are regarded as
equally acceptable for this type of experiments.
In the end, we get for each verb:
</bodyText>
<listItem confidence="0.998467428571429">
• an entry, which is an inventory of semantic
categories (patterns)
• 300+ manually annotated concordances (sin-
gle values)
• out of which 50 are manually annotated and
adjudicated concordances (multiple values
without evident errors).
</listItem>
<sectionHeader confidence="0.987172" genericHeader="method">
3 Tagging confusion analysis
</sectionHeader>
<subsectionHeader confidence="0.981238">
3.1 Formal model of tagging confusion
</subsectionHeader>
<bodyText confidence="0.999137260869565">
To formally describe the semantic tagging task,
we assume a target word and a (randomly se-
lected) corpus sample of its occurrences. The
tagged sample is S = {s1,... , sr}, where each
instance si is an occurrence of the target word
with its context, and r is the sample size.
For multiple annotation we need a set of m an-
notators A = {A1,..., Am} who choose from
a given set of semantic categories represented
by a set of n semantic tags T = {t1, ... , tn}.
Generally, if we admitted assigning more tags to
one word occurrence, annotators could assign any
subset of T to an instance. In our experiments,
however, annotators were allowed to assign just
one tag to each tagged instance. Therefore each
annotator is described as a function that assigns a
single member set to each instance Ai(s) = {t},
where s ∈ S, t ∈ T . When a pair of annotators
tag an instance s, they produce a set of one or two
different tags {t, t&apos;} = Ai(s) ∪ Aj(s).
Detailed information about interannotator
(dis)agreement on a given sample S is rep-
resented by a set of (m ) symmetric matrices
</bodyText>
<equation confidence="0.977332333333333">
2
CAkAl
ij = |{s ∈ S  |Ak(s) ∪ Al(s) = {ti,tj}}|,
</equation>
<bodyText confidence="0.947173285714286">
for 1 ≤ k &lt; l ≤ m, and i, j ∈ {1, . . . , n}.
Note that each of those matrices can be easily
computed as CAkAl = C + CT − InC, where
C is a conventional confusion matrix representing
the agreement between annotators Ak and Al,
and In is a unit matrix.
Definition: Aggregated Confusion Matrix (ACM)
</bodyText>
<equation confidence="0.8477935">
�C? = CAkAl.
1&lt;k&lt;l&lt;m
</equation>
<bodyText confidence="0.946472">
Properties: ACM is symmetric and for any i =6 j
the number C?ij says how many times a pair of
annotators disagreed on two tags ti and tj, while
C?ii is the frequency of agreements on ti; the sum
in the i-th row E j C?ij is the total frequency of
assigned sets {t, t&apos;} that contain ti.
</bodyText>
<tableCaption confidence="0.696608333333333">
An example of ACM is given in Table 2. The
corresponding confusion matrices are shown in
Table 3.
</tableCaption>
<table confidence="0.999880833333333">
1 1.a 2 4 5
1 85 8 2 0 0
1.a 8 1 2 0 0
2 2 2 34 0 0
4 0 0 0 4 8
5 0 0 0 8 6
</table>
<tableCaption confidence="0.99577">
Table 2: Aggregated Confusion Matrix.
</tableCaption>
<bodyText confidence="0.999909363636364">
Our approach to exact tagging confusion analy-
sis is based on probability and information theory.
Assigning semantic tags by annotators is viewed
as a random process. We define (categorical) ran-
dom variable T1 as the outcome of one annota-
tor; its values are single member sets {t}, and we
have mr observations to compute their probabil-
ities. The probability that an annotator will use
ti is denoted by p1(ti) = Pr(T1 = {ti}) and is
practically computed as the relative frequency of
ti among all mr assigned tags. Formally,
</bodyText>
<equation confidence="0.858006">
|Ak(sj) ∩ {ti}|.
</equation>
<bodyText confidence="0.9999035">
The outcome of two annotators (they both tag
the same instance) is described by random vari-
able T2; its values are single or double member
sets {t, t&apos;}, and we have (m )r observations to
</bodyText>
<page confidence="0.406161">
2
</page>
<bodyText confidence="0.953341833333333">
compute their probabilities. In contrast to p1, the
probability that ti will be used by a pair of anno-
tators is denoted by p2(ti) = Pr(T2 ⊇ {ti}), and
is computed as the relative frequency of assigned
sets {t, t&apos;} containing ti among all (2) r observa-
tions:
</bodyText>
<equation confidence="0.990851">
1
p2(ti)C _ (m2 )r �ik.
k
</equation>
<bodyText confidence="0.94330675">
We also need the conditional probability that an
annotator will use ti given that another annotator
has used tj. For convenience, we use the nota-
tion p2(ti  |tj) = Pr(T2 ⊇ {ti}  |T2 ⊇ {tj}).
</bodyText>
<equation confidence="0.993456285714286">
1
p1(ti) =
mr
r
j=1
�m
k=1
</equation>
<page confidence="0.997367">
845
</page>
<table confidence="0.997523857142857">
A1 vs. A2 A1 vs. A3 A2 vs. A3
1 1.a 2 4 5 1 1.a 2 4 5 1 1.a 2 4 5
1 29 1 1 0 0 1 29 2 0 0 0 1 27 2 0 0 0
1.a 0 1 0 0 0 1.a 1 0 0 0 0 1.a 2 0 1 0 0
2 0 1 11 0 0 2 0 0 12 0 0 2 1 0 11 0 0
4 0 0 0 2 0 4 0 0 0 1 1 4 0 0 0 1 4
5 0 0 0 3 1 5 0 0 0 0 4 5 0 0 0 0 1
</table>
<tableCaption confidence="0.999894">
Table 3: Example of all confusion matrices for the target word submit and three annotators.
</tableCaption>
<bodyText confidence="0.585112">
Obviously, it can be computed as
</bodyText>
<equation confidence="0.9971056">
Pr(T2 = {ti, tj})
p2(ti  |tj) = Pr(T2 ⊇ {tj})
C?ij
(m )r · p2(tj)
2
</equation>
<bodyText confidence="0.9992637">
Properties: The sum in any row is 1. The j-th
row of CPM contains probabilities of assigning ti
given that another annotator has chosen tj for the
same instance. Thus, the j-th row of CPM de-
scribes expected tagging confusion related to the
tag tj.
An example is given in Table 3 (all confusion
matrices for three annotators), in Table 2 (the
corresponding ACM), and in Table 4 (the corre-
sponding CPM).
</bodyText>
<table confidence="0.999362833333333">
1 1.a 2 4 5
1 0.895 0.084 0.021 0.000 0.000
1.a 0.727 0.091 0.182 0.000 0.000
2 0.053 0.053 0.895 0.000 0.000
4 0.000 0.000 0.000 0.333 0.667
5 0.000 0.000 0.000 0.571 0.429
</table>
<tableCaption confidence="0.999744">
Table 4: Example of Confusion Probability Matrix.
</tableCaption>
<subsectionHeader confidence="0.998491">
3.2 Semantic granularity optimization
</subsectionHeader>
<bodyText confidence="0.987054303030303">
Now, having a detailed analysis of expected tag-
ging confusion described in CPM, we are able to
compare usefulness of different semantic tags us-
ing a measure of the information content associ-
ated with them (in the information theory sense).
Traditionally, the amount of self-information con-
tained in a tag (as a probabilistic event) depends
only on the probability of that tag, and would be
defined as I(tj) = − log p1(tj). However, intu-
itively one can say that a good measure of use-
fulness of a particular tag should also take into
consideration the expected tagging confusion re-
lated to the tag. Therefore, to exactly measure
usefulness of the tag tj we propose to compare
and measure similarity of the distribution p1(ti)
and the distribution p2(ti  |tj), i = 1, ... , n.
How much information do we gain when an an-
notator assigns the tag tj to an instance? When
the tag tj has once been assigned to an instance
by an annotator, one would naturally expect that
another annotator will probably tend to assign the
same tag tj to the same instance. Formally, things
make good sense if p2(tj  |tj) &gt; p1(tj) and if
p2(ti  |tj) &lt; p1(ti) for any i different from j.
If p2(tj  |tj) = 100 %, then there is full con-
sensus about assigning tj among annotators; then
and only then the measure of usefulness of the tag
tj should be maximal and should have the value
of − log p1(tj). Otherwise, the value of useful-
ness should be smaller. This is our motivation to
define a quantity of reliable information gain ob-
tained from semantic tags as follows:
Definition: Reliable Gain (RG) from the tag tj is
</bodyText>
<equation confidence="0.999114">
−(−1)δkjp2(tk|tj) log p2(tk|tj)
p1(tk) .
</equation>
<bodyText confidence="0.979485636363636">
Properties: RG is similar to the well known
Kullback-Leibler divergence (or information
gain). If p2(ti  |tj) = p1(ti) for all i = 1, ... , n,
then RG(tj) = 0. If p2(tj  |tj) = 100 %, then
and only then RG(tj) = −log p1(tj), which
is the maximum. If p2(ti  |tj) &lt; p1(ti) for
all i different from j, the greater difference in
probabilities, the bigger (and positive) RG(tj).
And vice versa, the inequality p2(ti  |tj) &gt; p1(ti)
for all i different from j implies a negative value
of RG(tj).
</bodyText>
<figure confidence="0.537953272727273">
Definition: Confusion Probability Matrix (CPM)
C?ij
Cpji = p2(ti |tj) =E.
k C?jk
=
C?ij
E.
k C?jk
�
RG(tj) =
k
</figure>
<page confidence="0.974842">
846
</page>
<bodyText confidence="0.323098666666667">
Definition: Average Reliable Gain (ARG) from
the tagset {t1, ... , tn} is computed as an expected
value of RG(tj):
</bodyText>
<equation confidence="0.580157">
ARG =� p1(tj)RG(tj)
j
</equation>
<bodyText confidence="0.9512524">
Properties: ARG has its maximum value if the
CPM is a unit matrix, which is the case of the
absolute agreement among all annotators. Then
ARG has the value of the entropy of the p1 distri-
bution: ARG,,,,,, = H(p1(t1), ... , p1(tn)).
</bodyText>
<subsectionHeader confidence="0.858571">
Merging tags with poor RG
</subsectionHeader>
<bodyText confidence="0.999826363636364">
The main motivation for developing the ARG
value was the optimization of the tagset granular-
ity. We use a semi-greedy algorithm that searches
for an “optimal” tagset. The optimization process
starts with the fine-grained list of CPA semantic
categories and then the algorithm merges some
tags in order to maximize the ARG value. An ex-
ample is given in Table 5. Tables 6 and 7 show
the ACM and the CPM after merging. The ex-
amples relate to the verb submit already shown in
Tables 1, 2, 3 and 4.
</bodyText>
<subsectionHeader confidence="0.787312">
3.3 Classifier evaluation with respect to
expected tagging confusion
</subsectionHeader>
<bodyText confidence="0.926189590909091">
An automatic classifier is considered to be a func-
tion c that—the same way as annotators— assigns
tags to instances s E S, so that c(s) = {t},
t E T . The traditional way to evaluate the ac-
curacy of an automatic classifier means to com-
pare its output with the correct semantic tags on
a Gold Standard (GS) dataset. Within our formal
framework, we can imagine that we have a “gold”
annotator Ag, so that the GS dataset is represented
by Ag(s1), ... , Ag(sr). Then the classic accuracy
score can be computed as 1 Er i=1 |Ag(si)nc(si)|.
r
However, that approach does not take into con-
sideration the fact that some semantic tags are
quite confusing even for human annotators. In our
opinion, automatic classifier should not be penal-
ized for mistakes that would be made even by hu-
mans. So we propose a more complex evaluation
score using the knowledge of the expected tagging
confusion stored in CPM.
Definition: Classifier evaluation Score with re-
spect to tagging confusion is defined as the pro-
</bodyText>
<equation confidence="0.973623222222222">
portion Score(c) = S(c)/Smax, where
r
i=1
α
S(c) =
r
|Ag(si) n c(si) |+
r
i=1
1 − α
+
r
p2(c(si)  |Ag(si))
r
i=1
p2(Ag(si)  |Ag(si)).
1 − α
Smax = α + r
</equation>
<table confidence="0.982959714285714">
Original tagset Optimal merge
Tag f RG Tag f RG
1 90 +0.300 1 + 1.a 96 +0.425
1.a 6 −0.001
2 36 +0.447 2 36 +0.473
4 8 −0.071 4 + 5 18 +0.367
5 10 −0.054
</table>
<tableCaption confidence="0.98752">
Table 5: Frequency and Reliable Gain of tags.
</tableCaption>
<table confidence="0.9998165">
1 2 4
1 94 4 0
2 4 34 0
4 0 0 18
</table>
<tableCaption confidence="0.895941">
Table 6: Aggregated Confusion Matrix after merging.
</tableCaption>
<table confidence="0.999663">
1 2 4
1 0.959 0.041 0.000
2 0.105 0.895 0.000
4 0.000 0.000 1.000
</table>
<tableCaption confidence="0.995434">
Table 7: Confusion Probability Matrix after merging.
</tableCaption>
<table confidence="0.932458">
α = 1 α = 0.5 α = 0
Verb Score Score Score
halt 1 0.84 2 0.90 4 0.81
submit 2 0.83 1 0.90 1 0.84
ally 3 0.82 3 0.89 5 0.76
cry 4 0.79 4 0.88 2 0.82
arrive 5 0.74 5 0.85 3 0.81
plough 6 0.70 6 0.81 6 0.72
deny 7 0.62 7 0.74 7 0.66
cool 8 0.58 8 0.69 8 0.53
yield 9 0.55 9 0.67 9 0.52
</table>
<tableCaption confidence="0.961056333333333">
Table 8: Evaluation with different α values.
Table 8 gives an illustration of the fact that us-
ing different α values one can get different re-
</tableCaption>
<page confidence="0.993142">
847
</page>
<sectionHeader confidence="0.820859" genericHeader="method">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.998585">
The usefulness of a semantic resource depends on
two aspects:
sults when comparing tagging accuracy for dif-
ferent words (a classifier based on bag-of-words
approach was used). The same holds true for com-
parison of different classifiers.
</bodyText>
<sectionHeader confidence="0.877198" genericHeader="discussions">
3.4 Related work
</sectionHeader>
<bodyText confidence="0.999931487179487">
In their extensive survey article Artstein and Poe-
sio (2008) state that word sense tagging is one
of the hardest annotation tasks. They assume
that making distinctions between semantic cate-
gories must rely on a dictionary. The problem
is that annotators often cannot consistently make
the fine-grained distinctions proposed by trained
lexicographers, which is particularly serious for
verbs, because verbs generally tend to be polyse-
mous rather than homonymous.
A few approaches have been suggested in
the literature that address the problem of the
fine-grained semantic distinctions by (automatic)
measuring sense distinguishability. Diab (2004)
computes sense perplexity using the entropy func-
tion as a characteristic of training data. She also
compares the sense distributions to obtain sense
distributional correlation, which can serve as a
“very good direct indicator of performance ra-
tio”, especially together with sense context con-
fusability (another indicator observed in the train-
ing data). Resnik and Yarowsky (1999) intro-
duced the communicative/semantic distance be-
tween the predicted sense and the “correct” sense.
Then they use it for evaluation metric that pro-
vides partial credit for incorrectly classified in-
stances. Cohn (2003) introduces the concept of
(non-uniform) misclassification costs. He makes
use of the communicative/semantic distance and
proposes a metric for evaluating word sense dis-
ambiguation performance using the Receiver Op-
erating Characteristics curve that takes the mis-
classification costs into account. Bruce and
Wiebe (1998) analyze the agreement among hu-
man judges for the purpose of formulating a re-
fined and more reliable set of sense tags. Their
method is based on statistical analysis of inter-
annotator confusion matrices. An extended study
is given in (Bruce and Wiebe, 1999).
</bodyText>
<listItem confidence="0.998829">
• reliability of the annotation
• information gain from the annotation.
</listItem>
<bodyText confidence="0.999918117647059">
In practice, each semantic resource emphasizes
one aspect: OntoNotes, e.g., guarantees reliabil-
ity, whereas the WordNet-annotated corpora seek
to convey as much semantic nuance as possible.
To the best of our knowledge, there has been no
exact measure for the optimization, and the use-
fulness of a given resource can only be assessed
when it is finished and used in applications. We
propose the reliable information gain, a measure
based on information theory and on the analysis of
interannotator confusion matrices for each word
entry, that can be continually applied during the
creation of a semantic resource, and that provides
automatic feedback about the granularity of the
used tagset. Moreover, the computed information
about the amount of expected tagging confusion
is also used in evaluation of automatic classifiers.
</bodyText>
<sectionHeader confidence="0.998385" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999921785714286">
This work has been supported by the Czech Sci-
ence Foundation projects GK103/12/G084 and
P406/2010/0875 and partly by the project Euro-
MatrixPlus (FP7-ICT-2007-3-231720 of the EU
and 7E09003+7E11051 of the Ministry of Edu-
cation, Youth and Sports of the Czech Republic).
We thank our friends from Masaryk University
in Brno for providing the annotation infrastruc-
ture and for their permanent technical support.
We thank Patrick Hanks for his CPA method, for
the original PDEV development, and for numer-
ous discussions about the semantics of English
verbs. We also thank three anonymous reviewers
for their valuable comments.
</bodyText>
<page confidence="0.996473">
848
</page>
<sectionHeader confidence="0.996349" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999768460176991">
Roni Ben Aharon, Idan Szpektor, and Ido Dagan.
2010. Generating entailment rules from FrameNet.
In Proceedings of the ACL 2010 Conference Short
Papers., pages 241–246, Uppsala, Sweden.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555–596, December.
Ann Bies, Mark Ferguson, Karen Katz, Robert Mac-
Intyre, Victoria Tredinnick, Grace Kim, Mary Ann
Marcinkiewicz, and Britta Schasberger. 1995.
Bracketing guidelines for treebank II style. Tech-
nical report, University of Pennsylvania.
Susan Windisch Brown, Travis Rood, and Martha
Palmer. 2010. Number or nuance: Which factors
restrict reliable word sense annotation? In LREC,
pages 3237–3243. European Language Resources
Association (ELRA).
Rebecca F. Bruce and Janyce M. Wiebe. 1998. Word-
sense distinguishability and inter-coder agreement.
In Proceedings of the Third Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP ’98), pages 53–60. Granada, Spain, June.
Rebecca F. Bruce and Janyce M. Wiebe. 1999. Recog-
nizing subjectivity: A case study of manual tagging.
Natural Language Engineering, 5(2):187–205.
Silvie Cinkov´a, Martin Holub, Adam Rambousek, and
Lenka Smejkalov´a. 2012. A database of seman-
tic clusters of verb usages. In Proceedings of the
LREC ’2012 International Conference on Language
Resources and Evaluation. To appear.
Jacob Cohen. 1960. A coefficient of agreement for
nominal scales. Educational and Psychological
Measurement, 20(1):37–46.
Trevor Cohn. 2003. Performance metrics for word
sense disambiguation. In Proceedings of the Aus-
tralasian Language Technology Workshop 2003,
pages 86–93, Melbourne, Australia, December.
Mona T. Diab. 2004. Relieving the data acquisition
bottleneck in word sense disambiguation. In Pro-
ceedings of the 42nd Annual Meeting of the ACL,
pages 303–310. Barcelona, Spain. Association for
Computational Linguistics.
Katrin Erk, Diana McCarthy, and Nicholas Gaylord.
2009. Investigations on word senses and word us-
ages. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP, pages 10–18, Suntec,
Singapore, August. Association for Computational
Linguistics.
Katrin Erk. 2010. What is word meaning, really?
(And how can distributional models help us de-
scribe it?). In Proceedings of the 2010 Workshop
on GEometrical Models of Natural Language Se-
mantics, pages 17–26, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
Christiane Fellbaum, Joachim Grabowski, and Shari
Landes. 1997. Analysis of a hand-tagging task. In
Proceedings of the ACL/Siglex Workshop, Somer-
set, NJ.
Christiane Fellbaum, J. Grabowski, and S. Landes.
1998. Performance and confidence in a semantic
annotation task. In WordNet: An Electronic Lexical
Database, pages 217–238. Cambridge (Mass.): The
MIT Press., Cambridge (Mass.).
Christiane Fellbaum, Martha Palmer, Hoa Trang Dang,
Lauren Delfs, and Susanne Wolf. 2001. Manual
and automatic semantic annotation with WordNet.
Christiane Fellbaum. 1998. WordNet. An Electronic
Lexical Database. MIT Press, Cambridge, MA.
Charles J. Fillmore and B. T. S. Atkins. 1994. Start-
ing where the dictionaries stop: The challenge for
computational lexicography. In Computational Ap-
proaches to the Lexicon, pages 349–393. Oxford
University Press.
Joseph L. Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological Bul-
letin, 76:378–382.
Patrick Hanks and James Pustejovsky. 2005. A pat-
tern dictionary for natural language processing. Re-
vue Francaise de linguistique applique, 10(2).
Patrick Hanks. forthcoming. Lexical Analysis: Norms
and Exploitations. MIT Press.
Aleˇs Hor´ak, Adam Rambousek, and Piek Vossen.
2008. A distributed database system for develop-
ing ontological and lexical resources in harmony.
In 9th International Conference on Intelligent Text
Processing and Computational Linguistics, pages
1–15. Berlin: Springer.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: the 90% solution. In Proceedings
of the Human Language Technology Conference
of the NAACL, Companion Volume: Short Papers,
NAACL-Short ’06, pages 57–60, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Nancy Ide, Collin Baker, Christiane Fellbaum, Charles
Fillmore, and Rebecca Passoneau. 2008. MASC:
The Manually Annotated Sub-Corpus of American
English. In Proceedings of the Sixth International
Conference on Language Resources and Evaluation
(LREC’08), pages 28–30. European Language Re-
sources Association (ELRA).
Julia Jorgensen. 1990. The psycholinguistic reality of
word senses. Journal of Psycholinguistic Research,
(19):167–190.
Adam Kilgarriff. 1997. “I don’t believe in word
senses”. Computers and the Humanities, 31(2):91–
113.
Ramesh Krishnamurthy and Diane Nicholls. 2000.
Peeling an onion: The lexicographer’s experience
of manual sense tagging. Computers and the Hu-
manities, 34:85–97.
</reference>
<page confidence="0.989341">
849
</page>
<reference confidence="0.999751875">
Ryan McDonald, Kevin Lerman, and Fernando
Pereira. 2006. Multilingual dependency analysis
with a two-stage discriminative parser. In Proceed-
ings of the Tenth Conference on Computational Nat-
ural Language Learning CoNLLX 06, pages 216–
220. Association for Computational Linguistics.
Adam Meyers, Ruth Reeves, and Catherine Macleod.
2008. NomBank v 1.0.
G. A. Miller, C. Leacock, R. Tengi, and R. T. Bunker.
1993a. A semantic concordance. In Proceedings of
ARPA Workshop on Human Language Technology.
G. A. Miller, C. Leacock, R. Tengi, and R. T. Bunker.
1993b. A semantic concordance. In Proceedings of
ARPA Workshop on Human Language Technology.
Roberto Navigli. 2006. Meaningful clustering of
senses helps boost word sense disambiguation per-
formance. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the ACL, pages 105–112, Syd-
ney, Australia.
Martha Palmer, Dan Gildea, and Paul Kingsbury.
2005. The proposition bank: A corpus annotated
with semantic roles. Computational Linguistics
Journal, 31(1).
Rebecca J. Passonneau, Ansaf Salleb-Aoussi, Vikas
Bhardwaj, and Nancy Ide. 2010. Word sense anno-
tation of PolysemousWords by multiple annotators.
In LREC Proceedings, pages 3244–3249, Valetta,
Malta.
Philip Resnik and David Yarowsky. 1999. Distin-
guishing systems and distinguishing senses: New
evaluation methods for word sense disambiguation.
Natural Language Engineering, 5(2):113–133.
Anna Rumshisky, M. Verhagen, and J. Moszkowicz.
2009. The holy grail of sense definition: Creating
a Sense-Disambiguated corpus from scratch. Pisa,
Italy.
Michael Rundell. 2002. Macmillan English Dictio-
nary for advanced learners. Macmillan Education.
Josef Ruppenhofer, Michael Ellsworth, Miriam R. L.
Petruck, Christopher R. Johnson, and Jan Schef-
fczyk. 2010. FrameNet II: Extended Theory and
Practice. ICSI, University of Berkeley, September.
Beatrice Santorini. 1990. Part-of-Speech tagging
guidelines for the penn treebank project. University
of Pennsylvania 3rd Revision 2nd Printing, (MS-
CIS-90-47):33.
William A. Scott. 1955. Reliability of content analy-
sis: The case of nominal scale coding. Public Opin-
ion Quarterly, 19(3):321–325.
John Sinclair, Patrick Hanks, and et al. 1987. Collins
Cobuild English Dictionary for Advanced Learn-
ers 4th edition published in 2003. HarperCollins
Publishers 1987, 1995, 2001, 2003 and Collins
A–Z Thesaurus 1st edition first published in 1995.
HarperCollins Publishers 1995.
John Sinclair. 1991. Corpus, Concordance, Colloca-
tion. Describing English Language. Oxford Univer-
sity Press.
Ralph Weischedel, Martha Palmer, Mitchell Marcus,
Eduard Hovy, Sameer Pradhan, Lance Ramshaw,
Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle
Franchini, Mohammed El-Bachouti, Robert Belvin,
and Ann Houston. 2011. OntoNotes release 4.0.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Alessandro Moschitti. 2009. A machine learning
approach to textual entailment recognition. Natural
Language Engineering, 15(4):551–582.
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(November
2009):105–151.
</reference>
<page confidence="0.998161">
850
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.791236">
<title confidence="0.999917">Managing Uncertainty in Semantic Tagging</title>
<author confidence="0.989827">Cinkov´a Holub</author>
<affiliation confidence="0.9931255">Charles University in Prague, Faculty of Mathematics and Institute of Formal and Applied</affiliation>
<email confidence="0.998057">vincent.kriz@gmail.com</email>
<abstract confidence="0.990905952380952">Low interannotator agreement (IAA) is a well-known issue in manual semantic tagging (sense tagging). IAA correlates with the granularity of word senses and they both correlate with the amount of information they give as well as with its reliability. We compare different approaches to semantic tagging in WordNet, FrameNet, Prop- Bank and OntoNotes with a small tagged data sample based on the Corpus Pattern to present the information a measure used to optimize the semantic granularity of a sense inventory with respect to its reliability indicated by the IAA in the given data set. RG can also be used as feedback for lexicographers, and as a supporting component of automatic semantic classifiers, especially when dealing with a very fine-grained set of semantic categories.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Roni Ben Aharon</author>
<author>Idan Szpektor</author>
<author>Ido Dagan</author>
</authors>
<title>Generating entailment rules from FrameNet.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers.,</booktitle>
<pages>241--246</pages>
<location>Uppsala,</location>
<contexts>
<context position="5348" citStr="Aharon et al., 2010" startWordPosition="836" endWordPosition="839">penhofer et al., 2010), PropBank (Palmer et al., 2005) and the word-sense part of OntoNotes (Weischedel et al., 2011)) are still maintained and their annotation schemes are adopted for creating new manually annotated data (e.g. MASC, the Manually Annotated Subcorpus (Ide et al., 2008)). More to say, these resources are not only used in WSD and semantic labeling, but also in research directions that in their turn do not rely on the idea of an inventory of discrete senses any more, e.g. in distributional semantics (Erk, 2010) and recognizing textual entailment (e.g. (Zanzotto et al., 2009) and (Aharon et al., 2010)). It is a remarkable fact that, to the best of our knowledge, there is no measure that would relate granularity, reliability of the annotation (derived from IAA) and the resulting information gain. Therefore it is impossible to say where the optimum for granularity and IAA lies. 2 Approaches to semantic tagging 2.1 Semantic tagging vs. morphological or syntactic analysis Manual semantic tagging is in many respects similar to morphological tagging and syntactic analysis: human annotators are trained to sort certain elements occurring in a running text according to a reference source. There is,</context>
</contexts>
<marker>Aharon, Szpektor, Dagan, 2010</marker>
<rawString>Roni Ben Aharon, Idan Szpektor, and Ido Dagan. 2010. Generating entailment rules from FrameNet. In Proceedings of the ACL 2010 Conference Short Papers., pages 241–246, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ron Artstein</author>
<author>Massimo Poesio</author>
</authors>
<title>Inter-coder agreement for computational linguistics.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="3101" citStr="Artstein and Poesio, 2008" startWordPosition="483" endWordPosition="486">s to assign random occurrences of the word to one of the pre-defined senses (Fellbaum et al., 1997). This assumption seems to be falsified by the interannotator agreement (IAA, sometimes ITA) constantly reported much lower in semantic than in morphological or syntactic annotation, as well as by the general divergence of opinion on which value of which IAA measure indicates a reliable annotation. In some projects (e.g. OntoNotes (Hovy et al., 2006)), the percentage of agreements between two annotators is used, but a number of more complex measures are available (for a comprehensive survey see (Artstein and Poesio, 2008)). Consequently, using different measures for IAA makes the reported IAA values incomparable across different projects. Even skilled lexicographers have trouble selecting one discrete sense for a concordance (Krishnamurthy and Nicholls, 2000), and, more to say, when the tagging performance of lexicographers and ordinary annotators (students) was 840 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 840–850, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics compared, the experiment showed that t</context>
<context position="21693" citStr="Artstein and Poesio, 2008" startWordPosition="3391" endWordPosition="3394">ted one value for each concordance to remain in the resulting one-value-perconcordance gold standard data and recorded it into the gold standard set. The adjudication pro3Fleiss’s kappa (Fleiss, 1971) is a generalization of Scott’s it statistic (Scott, 1955). In contrast to Cohen’s kappa (Cohen, 1960), Fleiss’s kappa evaluates agreement between multiple raters. However, Fleiss’s kappa is not a generalization of Cohen’s kappa, which is a different, yet related, statistical measure. Sometimes, the terminology about kappas is confusing in the literature. For a detailed explanation refer e.g. to (Artstein and Poesio, 2008). 844 tocol has been kept for further experiments. All values except the marked errors are regarded as equally acceptable for this type of experiments. In the end, we get for each verb: • an entry, which is an inventory of semantic categories (patterns) • 300+ manually annotated concordances (single values) • out of which 50 are manually annotated and adjudicated concordances (multiple values without evident errors). 3 Tagging confusion analysis 3.1 Formal model of tagging confusion To formally describe the semantic tagging task, we assume a target word and a (randomly selected) corpus sample </context>
<context position="31941" citStr="Artstein and Poesio (2008)" startWordPosition="5389" endWordPosition="5393">0.79 4 0.88 2 0.82 arrive 5 0.74 5 0.85 3 0.81 plough 6 0.70 6 0.81 6 0.72 deny 7 0.62 7 0.74 7 0.66 cool 8 0.58 8 0.69 8 0.53 yield 9 0.55 9 0.67 9 0.52 Table 8: Evaluation with different α values. Table 8 gives an illustration of the fact that using different α values one can get different re847 4 Conclusion The usefulness of a semantic resource depends on two aspects: sults when comparing tagging accuracy for different words (a classifier based on bag-of-words approach was used). The same holds true for comparison of different classifiers. 3.4 Related work In their extensive survey article Artstein and Poesio (2008) state that word sense tagging is one of the hardest annotation tasks. They assume that making distinctions between semantic categories must rely on a dictionary. The problem is that annotators often cannot consistently make the fine-grained distinctions proposed by trained lexicographers, which is particularly serious for verbs, because verbs generally tend to be polysemous rather than homonymous. A few approaches have been suggested in the literature that address the problem of the fine-grained semantic distinctions by (automatic) measuring sense distinguishability. Diab (2004) computes sens</context>
</contexts>
<marker>Artstein, Poesio, 2008</marker>
<rawString>Ron Artstein and Massimo Poesio. 2008. Inter-coder agreement for computational linguistics. Computational Linguistics, 34(4):555–596, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Bies</author>
<author>Mark Ferguson</author>
<author>Karen Katz</author>
<author>Robert MacIntyre</author>
<author>Victoria Tredinnick</author>
<author>Grace Kim</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Britta Schasberger</author>
</authors>
<title>Bracketing guidelines for treebank II style.</title>
<date>1995</date>
<tech>Technical report,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="7504" citStr="Bies et al., 1995" startWordPosition="1175" endWordPosition="1178">orphological or syntactic annotation, the tagset or inventory of constituents are given beforehand and are supposed to hold for all tokens/sentences contained in the corpus. Problematic and theory-dependent issues are few and mostly well-known in advance. Therefore they can be reflected by a few additional conventions in the annotation manual (e.g. where to draw the line between particles and prepositions or between adjectives and verbs in past participles (Santorini, 1990) or where to attach a prepositional phrase following a noun phrase and how to treat specific “financialspeak” structures (Bies et al., 1995)). Even in difficult cases, there are hardly more than two options of interpretation. Data manually annotated for morphology or surface syntax are reliable enough to train syntactic parsers with an accuracy above 80 % (e.g. (Zhang and Clark, 2011; McDonald et al., 2006)). On the other hand, semantic tagging actually employs a different tagset for each word lemma. Even within the same part of speech, individual words require individual descriptions. Possible similarities among them come into relief ex post rather than that they could be imposed on the lexicographers from the beginning. When ass</context>
</contexts>
<marker>Bies, Ferguson, Katz, MacIntyre, Tredinnick, Kim, Marcinkiewicz, Schasberger, 1995</marker>
<rawString>Ann Bies, Mark Ferguson, Karen Katz, Robert MacIntyre, Victoria Tredinnick, Grace Kim, Mary Ann Marcinkiewicz, and Britta Schasberger. 1995. Bracketing guidelines for treebank II style. Technical report, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Windisch Brown</author>
<author>Travis Rood</author>
<author>Martha Palmer</author>
</authors>
<title>Number or nuance: Which factors restrict reliable word sense annotation? In</title>
<date>2010</date>
<booktitle>LREC,</booktitle>
<pages>3237--3243</pages>
<contexts>
<context position="4403" citStr="Brown et al., 2010" startWordPosition="681" endWordPosition="684">al., 1997), and cf. (Jorgensen, 1990). Lexicographers are trained in considering subtle differences among various uses of a word, which ordinary language users do not reflect. Identifying a semantic difference between uses of a word and deciding whether a difference is important enough to constitute a separate sense means presenting a word with a certain degree of semantic granularity. Intuitively, the finer the granularity of a word entry is, the more opportunities for interannotator disagreement there are and the lower IAA can be expected. Brown et al. proved this hypothesis experimentally (Brown et al., 2010). Also, the annotators are less confident in their decisions, when they have many options to choose from (Fellbaum et al. (1998) reported a drop in subjective annotators confidence in words with 8+ senses). Despite all the known issues in semantic tagging, the major lexical resources (WordNet (Fellbaum, 1998), FrameNet (Ruppenhofer et al., 2010), PropBank (Palmer et al., 2005) and the word-sense part of OntoNotes (Weischedel et al., 2011)) are still maintained and their annotation schemes are adopted for creating new manually annotated data (e.g. MASC, the Manually Annotated Subcorpus (Ide et </context>
</contexts>
<marker>Brown, Rood, Palmer, 2010</marker>
<rawString>Susan Windisch Brown, Travis Rood, and Martha Palmer. 2010. Number or nuance: Which factors restrict reliable word sense annotation? In LREC, pages 3237–3243. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca F Bruce</author>
<author>Janyce M Wiebe</author>
</authors>
<title>Wordsense distinguishability and inter-coder agreement.</title>
<date>1998</date>
<booktitle>In Proceedings of the Third Conference on Empirical Methods in Natural Language Processing (EMNLP ’98),</booktitle>
<pages>53--60</pages>
<location>Granada, Spain,</location>
<contexts>
<context position="33444" citStr="Bruce and Wiebe (1998)" startWordPosition="5608" endWordPosition="5611">ability (another indicator observed in the training data). Resnik and Yarowsky (1999) introduced the communicative/semantic distance between the predicted sense and the “correct” sense. Then they use it for evaluation metric that provides partial credit for incorrectly classified instances. Cohn (2003) introduces the concept of (non-uniform) misclassification costs. He makes use of the communicative/semantic distance and proposes a metric for evaluating word sense disambiguation performance using the Receiver Operating Characteristics curve that takes the misclassification costs into account. Bruce and Wiebe (1998) analyze the agreement among human judges for the purpose of formulating a refined and more reliable set of sense tags. Their method is based on statistical analysis of interannotator confusion matrices. An extended study is given in (Bruce and Wiebe, 1999). • reliability of the annotation • information gain from the annotation. In practice, each semantic resource emphasizes one aspect: OntoNotes, e.g., guarantees reliability, whereas the WordNet-annotated corpora seek to convey as much semantic nuance as possible. To the best of our knowledge, there has been no exact measure for the optimizat</context>
</contexts>
<marker>Bruce, Wiebe, 1998</marker>
<rawString>Rebecca F. Bruce and Janyce M. Wiebe. 1998. Wordsense distinguishability and inter-coder agreement. In Proceedings of the Third Conference on Empirical Methods in Natural Language Processing (EMNLP ’98), pages 53–60. Granada, Spain, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca F Bruce</author>
<author>Janyce M Wiebe</author>
</authors>
<title>Recognizing subjectivity: A case study of manual tagging.</title>
<date>1999</date>
<journal>Natural Language Engineering,</journal>
<volume>5</volume>
<issue>2</issue>
<contexts>
<context position="33701" citStr="Bruce and Wiebe, 1999" startWordPosition="5652" endWordPosition="5655"> incorrectly classified instances. Cohn (2003) introduces the concept of (non-uniform) misclassification costs. He makes use of the communicative/semantic distance and proposes a metric for evaluating word sense disambiguation performance using the Receiver Operating Characteristics curve that takes the misclassification costs into account. Bruce and Wiebe (1998) analyze the agreement among human judges for the purpose of formulating a refined and more reliable set of sense tags. Their method is based on statistical analysis of interannotator confusion matrices. An extended study is given in (Bruce and Wiebe, 1999). • reliability of the annotation • information gain from the annotation. In practice, each semantic resource emphasizes one aspect: OntoNotes, e.g., guarantees reliability, whereas the WordNet-annotated corpora seek to convey as much semantic nuance as possible. To the best of our knowledge, there has been no exact measure for the optimization, and the usefulness of a given resource can only be assessed when it is finished and used in applications. We propose the reliable information gain, a measure based on information theory and on the analysis of interannotator confusion matrices for each </context>
</contexts>
<marker>Bruce, Wiebe, 1999</marker>
<rawString>Rebecca F. Bruce and Janyce M. Wiebe. 1999. Recognizing subjectivity: A case study of manual tagging. Natural Language Engineering, 5(2):187–205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silvie Cinkov´a</author>
<author>Martin Holub</author>
<author>Adam Rambousek</author>
<author>Lenka Smejkalov´a</author>
</authors>
<title>A database of semantic clusters of verb usages.</title>
<date>2012</date>
<booktitle>In Proceedings of the LREC ’2012 International Conference on Language Resources and Evaluation.</booktitle>
<note>To appear.</note>
<marker>Cinkov´a, Holub, Rambousek, Smejkalov´a, 2012</marker>
<rawString>Silvie Cinkov´a, Martin Holub, Adam Rambousek, and Lenka Smejkalov´a. 2012. A database of semantic clusters of verb usages. In Proceedings of the LREC ’2012 International Conference on Language Resources and Evaluation. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measurement,</booktitle>
<pages>20--1</pages>
<contexts>
<context position="21369" citStr="Cohen, 1960" startWordPosition="3345" endWordPosition="3346">ry, the newly revised reference sample and an entirely new 50-concordance annotation batch. The final multiple 50-concordance sample went through one more additional procedure, the adjudication: first, the lexicographer compared the three annotations and eliminated evident errors. Then the lexicographer selected one value for each concordance to remain in the resulting one-value-perconcordance gold standard data and recorded it into the gold standard set. The adjudication pro3Fleiss’s kappa (Fleiss, 1971) is a generalization of Scott’s it statistic (Scott, 1955). In contrast to Cohen’s kappa (Cohen, 1960), Fleiss’s kappa evaluates agreement between multiple raters. However, Fleiss’s kappa is not a generalization of Cohen’s kappa, which is a different, yet related, statistical measure. Sometimes, the terminology about kappas is confusing in the literature. For a detailed explanation refer e.g. to (Artstein and Poesio, 2008). 844 tocol has been kept for further experiments. All values except the marked errors are regarded as equally acceptable for this type of experiments. In the end, we get for each verb: • an entry, which is an inventory of semantic categories (patterns) • 300+ manually annota</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1):37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
</authors>
<title>Performance metrics for word sense disambiguation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Australasian Language Technology Workshop</booktitle>
<pages>86--93</pages>
<location>Melbourne, Australia,</location>
<contexts>
<context position="33125" citStr="Cohn (2003)" startWordPosition="5566" endWordPosition="5567">. Diab (2004) computes sense perplexity using the entropy function as a characteristic of training data. She also compares the sense distributions to obtain sense distributional correlation, which can serve as a “very good direct indicator of performance ratio”, especially together with sense context confusability (another indicator observed in the training data). Resnik and Yarowsky (1999) introduced the communicative/semantic distance between the predicted sense and the “correct” sense. Then they use it for evaluation metric that provides partial credit for incorrectly classified instances. Cohn (2003) introduces the concept of (non-uniform) misclassification costs. He makes use of the communicative/semantic distance and proposes a metric for evaluating word sense disambiguation performance using the Receiver Operating Characteristics curve that takes the misclassification costs into account. Bruce and Wiebe (1998) analyze the agreement among human judges for the purpose of formulating a refined and more reliable set of sense tags. Their method is based on statistical analysis of interannotator confusion matrices. An extended study is given in (Bruce and Wiebe, 1999). • reliability of the a</context>
</contexts>
<marker>Cohn, 2003</marker>
<rawString>Trevor Cohn. 2003. Performance metrics for word sense disambiguation. In Proceedings of the Australasian Language Technology Workshop 2003, pages 86–93, Melbourne, Australia, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mona T Diab</author>
</authors>
<title>Relieving the data acquisition bottleneck in word sense disambiguation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the ACL,</booktitle>
<pages>303--310</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Barcelona,</location>
<contexts>
<context position="32527" citStr="Diab (2004)" startWordPosition="5476" endWordPosition="5477">rtstein and Poesio (2008) state that word sense tagging is one of the hardest annotation tasks. They assume that making distinctions between semantic categories must rely on a dictionary. The problem is that annotators often cannot consistently make the fine-grained distinctions proposed by trained lexicographers, which is particularly serious for verbs, because verbs generally tend to be polysemous rather than homonymous. A few approaches have been suggested in the literature that address the problem of the fine-grained semantic distinctions by (automatic) measuring sense distinguishability. Diab (2004) computes sense perplexity using the entropy function as a characteristic of training data. She also compares the sense distributions to obtain sense distributional correlation, which can serve as a “very good direct indicator of performance ratio”, especially together with sense context confusability (another indicator observed in the training data). Resnik and Yarowsky (1999) introduced the communicative/semantic distance between the predicted sense and the “correct” sense. Then they use it for evaluation metric that provides partial credit for incorrectly classified instances. Cohn (2003) i</context>
</contexts>
<marker>Diab, 2004</marker>
<rawString>Mona T. Diab. 2004. Relieving the data acquisition bottleneck in word sense disambiguation. In Proceedings of the 42nd Annual Meeting of the ACL, pages 303–310. Barcelona, Spain. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Diana McCarthy</author>
<author>Nicholas Gaylord</author>
</authors>
<title>Investigations on word senses and word usages.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>10--18</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="14080" citStr="Erk et al. (2009)" startWordPosition="2224" endWordPosition="2227">y be too finegrained or too coarse-grained for automatic processing of different corpora (e.g. a restricteddomain corpus). Kilgarriff (1997, p. 115) shows (the handbag example) that there is no reason to expect the same set of word senses to be relevant for different tasks and that the corpus dictates the word senses and therefore ‘word sense’ was not found to be sufficiently well-defined to be a workable basic unit of meaning (p. 116). On the other hand, even non-experts seem to agree reasonably well when judging the similarity of use of a word in different contexts (Rumshisky et al., 2009). Erk et al. (2009) showed promising annotation results with a scheme that allowed the annotators graded judgments of similarity between two words or between a word and its definition. Verbs are the most challenging part of speech. We see two major causes: vagueness and coercion. We neglect ambiguity, since it has proved to be rare in our experience. CPA and PDEV Our current work focuses on English verbs. It has been inspired by the manual Corpus Pattern Analysis method (CPA) (Hanks, forthcoming) and its implementation, the Pattern Dictionary of English Verbs (PDEV) (Hanks and Pustejovsky, 2005). PDEV is a seman</context>
</contexts>
<marker>Erk, McCarthy, Gaylord, 2009</marker>
<rawString>Katrin Erk, Diana McCarthy, and Nicholas Gaylord. 2009. Investigations on word senses and word usages. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 10–18, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>What is word meaning, really? (And how can distributional models help us describe it?).</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics,</booktitle>
<pages>17--26</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="5257" citStr="Erk, 2010" startWordPosition="824" endWordPosition="825">tic tagging, the major lexical resources (WordNet (Fellbaum, 1998), FrameNet (Ruppenhofer et al., 2010), PropBank (Palmer et al., 2005) and the word-sense part of OntoNotes (Weischedel et al., 2011)) are still maintained and their annotation schemes are adopted for creating new manually annotated data (e.g. MASC, the Manually Annotated Subcorpus (Ide et al., 2008)). More to say, these resources are not only used in WSD and semantic labeling, but also in research directions that in their turn do not rely on the idea of an inventory of discrete senses any more, e.g. in distributional semantics (Erk, 2010) and recognizing textual entailment (e.g. (Zanzotto et al., 2009) and (Aharon et al., 2010)). It is a remarkable fact that, to the best of our knowledge, there is no measure that would relate granularity, reliability of the annotation (derived from IAA) and the resulting information gain. Therefore it is impossible to say where the optimum for granularity and IAA lies. 2 Approaches to semantic tagging 2.1 Semantic tagging vs. morphological or syntactic analysis Manual semantic tagging is in many respects similar to morphological tagging and syntactic analysis: human annotators are trained to s</context>
</contexts>
<marker>Erk, 2010</marker>
<rawString>Katrin Erk. 2010. What is word meaning, really? (And how can distributional models help us describe it?). In Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, pages 17–26, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
<author>Joachim Grabowski</author>
<author>Shari Landes</author>
</authors>
<title>Analysis of a hand-tagging task.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL/Siglex Workshop,</booktitle>
<location>Somerset, NJ.</location>
<contexts>
<context position="2574" citStr="Fellbaum et al., 1997" startWordPosition="402" endWordPosition="405">s on the tacit assumption that various uses of polysemous words can be sorted into discrete senses; understanding or using an unfamiliar word be then like looking it up in a dictionary. When building a dictionary entry for a given word, the lexicographer sorts a number of its occurrences into discrete senses present (or emerging) in his/her mental lexicon, which is supposed to be shared by all speakers of the same language. The assumed common mental representation of a words meaning should make it easy for other humans to assign random occurrences of the word to one of the pre-defined senses (Fellbaum et al., 1997). This assumption seems to be falsified by the interannotator agreement (IAA, sometimes ITA) constantly reported much lower in semantic than in morphological or syntactic annotation, as well as by the general divergence of opinion on which value of which IAA measure indicates a reliable annotation. In some projects (e.g. OntoNotes (Hovy et al., 2006)), the percentage of agreements between two annotators is used, but a number of more complex measures are available (for a comprehensive survey see (Artstein and Poesio, 2008)). Consequently, using different measures for IAA makes the reported IAA </context>
</contexts>
<marker>Fellbaum, Grabowski, Landes, 1997</marker>
<rawString>Christiane Fellbaum, Joachim Grabowski, and Shari Landes. 1997. Analysis of a hand-tagging task. In Proceedings of the ACL/Siglex Workshop, Somerset, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
<author>J Grabowski</author>
<author>S Landes</author>
</authors>
<title>Performance and confidence in a semantic annotation task.</title>
<date>1998</date>
<booktitle>In WordNet: An Electronic Lexical Database,</booktitle>
<pages>217--238</pages>
<publisher>The MIT Press.,</publisher>
<location>Cambridge (Mass.):</location>
<contexts>
<context position="4531" citStr="Fellbaum et al. (1998)" startWordPosition="702" endWordPosition="705">ord, which ordinary language users do not reflect. Identifying a semantic difference between uses of a word and deciding whether a difference is important enough to constitute a separate sense means presenting a word with a certain degree of semantic granularity. Intuitively, the finer the granularity of a word entry is, the more opportunities for interannotator disagreement there are and the lower IAA can be expected. Brown et al. proved this hypothesis experimentally (Brown et al., 2010). Also, the annotators are less confident in their decisions, when they have many options to choose from (Fellbaum et al. (1998) reported a drop in subjective annotators confidence in words with 8+ senses). Despite all the known issues in semantic tagging, the major lexical resources (WordNet (Fellbaum, 1998), FrameNet (Ruppenhofer et al., 2010), PropBank (Palmer et al., 2005) and the word-sense part of OntoNotes (Weischedel et al., 2011)) are still maintained and their annotation schemes are adopted for creating new manually annotated data (e.g. MASC, the Manually Annotated Subcorpus (Ide et al., 2008)). More to say, these resources are not only used in WSD and semantic labeling, but also in research directions that i</context>
<context position="9130" citStr="Fellbaum et al., (1998)" startWordPosition="1442" endWordPosition="1446">phers) would create identical entries for e.g. a polysemous verb. Any human evaluation of complete entries would be subjective. The maximum to be achieved is that the entry reflects the corpus data in a reasonable granular way on which annotators still can reach reasonable IAA. 2.2 Major existing semantic resources The granularity vs. IAA equilibrium is of great concern in creating lexical resources as well as in applications dealing with semantic tasks. When WordNet (Fellbaum, 1998) was created, both IAA and subjective confidence measurements served as an informal feedback to lexicographers (Fellbaum et al., (1998), p. 200). In general, WordNet has been considered a resource too fine-grained for most annotations (and applications). Navigli (2006) developed a method of reducing the granularity of WordNet by mapping the synsets to senses in a more coarse-grained dictionary. A manual, more coarse-grained grouping of WordNet senses has been performed in OntoNotes (Weischedel et al., 2011). The OntoNotes 90% solution (Hovy et al., 2006) actually means such a degree of granularity that enables a 90-%-IAA. OntoNotes is a reaction to the traditionally poor IAA in WordNet annotated corpora, caused by the high gr</context>
</contexts>
<marker>Fellbaum, Grabowski, Landes, 1998</marker>
<rawString>Christiane Fellbaum, J. Grabowski, and S. Landes. 1998. Performance and confidence in a semantic annotation task. In WordNet: An Electronic Lexical Database, pages 217–238. Cambridge (Mass.): The MIT Press., Cambridge (Mass.).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
<author>Martha Palmer</author>
<author>Hoa Trang Dang</author>
<author>Lauren Delfs</author>
<author>Susanne Wolf</author>
</authors>
<title>Manual and automatic semantic annotation with WordNet.</title>
<date>2001</date>
<marker>Fellbaum, Palmer, Dang, Delfs, Wolf, 2001</marker>
<rawString>Christiane Fellbaum, Martha Palmer, Hoa Trang Dang, Lauren Delfs, and Susanne Wolf. 2001. Manual and automatic semantic annotation with WordNet.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet. An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="4713" citStr="Fellbaum, 1998" startWordPosition="732" endWordPosition="734">nse means presenting a word with a certain degree of semantic granularity. Intuitively, the finer the granularity of a word entry is, the more opportunities for interannotator disagreement there are and the lower IAA can be expected. Brown et al. proved this hypothesis experimentally (Brown et al., 2010). Also, the annotators are less confident in their decisions, when they have many options to choose from (Fellbaum et al. (1998) reported a drop in subjective annotators confidence in words with 8+ senses). Despite all the known issues in semantic tagging, the major lexical resources (WordNet (Fellbaum, 1998), FrameNet (Ruppenhofer et al., 2010), PropBank (Palmer et al., 2005) and the word-sense part of OntoNotes (Weischedel et al., 2011)) are still maintained and their annotation schemes are adopted for creating new manually annotated data (e.g. MASC, the Manually Annotated Subcorpus (Ide et al., 2008)). More to say, these resources are not only used in WSD and semantic labeling, but also in research directions that in their turn do not rely on the idea of an inventory of discrete senses any more, e.g. in distributional semantics (Erk, 2010) and recognizing textual entailment (e.g. (Zanzotto et a</context>
<context position="8995" citStr="Fellbaum, 1998" startWordPosition="1425" endWordPosition="1426">me idea of parts of speech as the author of the tagset, there is no chance that two humans (not even two professional lexicographers) would create identical entries for e.g. a polysemous verb. Any human evaluation of complete entries would be subjective. The maximum to be achieved is that the entry reflects the corpus data in a reasonable granular way on which annotators still can reach reasonable IAA. 2.2 Major existing semantic resources The granularity vs. IAA equilibrium is of great concern in creating lexical resources as well as in applications dealing with semantic tasks. When WordNet (Fellbaum, 1998) was created, both IAA and subjective confidence measurements served as an informal feedback to lexicographers (Fellbaum et al., (1998), p. 200). In general, WordNet has been considered a resource too fine-grained for most annotations (and applications). Navigli (2006) developed a method of reducing the granularity of WordNet by mapping the synsets to senses in a more coarse-grained dictionary. A manual, more coarse-grained grouping of WordNet senses has been performed in OntoNotes (Weischedel et al., 2011). The OntoNotes 90% solution (Hovy et al., 2006) actually means such a degree of granula</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet. An Electronic Lexical Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Fillmore</author>
<author>B T S Atkins</author>
</authors>
<title>Starting where the dictionaries stop: The challenge for computational lexicography.</title>
<date>1994</date>
<booktitle>In Computational Approaches to the Lexicon,</booktitle>
<pages>349--393</pages>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="12064" citStr="Fillmore and Atkins, 1994" startWordPosition="1897" endWordPosition="1900">gatory modifiers typical of each (very coarse) word sense (called frameset). Two core criteria for distinguishing among framesets are the semantic roles of the arguments along with the syntactic alternations that the verb can undergo with that particular argument set. To keep low granularity, this lexicon—among other things—does usually not make special framesets for metaphoric uses. The overall IAA measured on verbs was 94 % (Palmer et al., 2005). 2.3 Semantic Pattern Recognition From corpus-based lexicography to semantic patterns The modern, corpus-based lexicology of 1990s (Sinclair, 1991; Fillmore and Atkins, 1994) has had a great impact on lexicography. There is a general consensus that dictionary definitions need to be supported by corpus examples. Cf. Fellbaum (2001): “For polysemous words, dictionaries [... ] do not say enough about the range of possible contexts that differentiate the senses. [... ] On the other hand, texts or corpora [... ] are not explicit about the word’s meaning. When we first encounter a new word in a text, we can usually form only a vague idea of its meaning; checking a dictionary will clarify the meaning. But the more contexts we encounter for a word, the harder it is to mat</context>
</contexts>
<marker>Fillmore, Atkins, 1994</marker>
<rawString>Charles J. Fillmore and B. T. S. Atkins. 1994. Starting where the dictionaries stop: The challenge for computational lexicography. In Computational Approaches to the Lexicon, pages 349–393. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph L Fleiss</author>
</authors>
<title>Measuring nominal scale agreement among many raters.</title>
<date>1971</date>
<journal>Psychological Bulletin,</journal>
<pages>76--378</pages>
<contexts>
<context position="21267" citStr="Fleiss, 1971" startWordPosition="3329" endWordPosition="3330">al reference sample along with the first 50-concordance sample. The annotators got back the revised entry, the newly revised reference sample and an entirely new 50-concordance annotation batch. The final multiple 50-concordance sample went through one more additional procedure, the adjudication: first, the lexicographer compared the three annotations and eliminated evident errors. Then the lexicographer selected one value for each concordance to remain in the resulting one-value-perconcordance gold standard data and recorded it into the gold standard set. The adjudication pro3Fleiss’s kappa (Fleiss, 1971) is a generalization of Scott’s it statistic (Scott, 1955). In contrast to Cohen’s kappa (Cohen, 1960), Fleiss’s kappa evaluates agreement between multiple raters. However, Fleiss’s kappa is not a generalization of Cohen’s kappa, which is a different, yet related, statistical measure. Sometimes, the terminology about kappas is confusing in the literature. For a detailed explanation refer e.g. to (Artstein and Poesio, 2008). 844 tocol has been kept for further experiments. All values except the marked errors are regarded as equally acceptable for this type of experiments. In the end, we get for</context>
</contexts>
<marker>Fleiss, 1971</marker>
<rawString>Joseph L. Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological Bulletin, 76:378–382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Hanks</author>
<author>James Pustejovsky</author>
</authors>
<title>A pattern dictionary for natural language processing. Revue Francaise de linguistique applique,</title>
<date>2005</date>
<pages>10--2</pages>
<contexts>
<context position="14663" citStr="Hanks and Pustejovsky, 2005" startWordPosition="2321" endWordPosition="2325">s (Rumshisky et al., 2009). Erk et al. (2009) showed promising annotation results with a scheme that allowed the annotators graded judgments of similarity between two words or between a word and its definition. Verbs are the most challenging part of speech. We see two major causes: vagueness and coercion. We neglect ambiguity, since it has proved to be rare in our experience. CPA and PDEV Our current work focuses on English verbs. It has been inspired by the manual Corpus Pattern Analysis method (CPA) (Hanks, forthcoming) and its implementation, the Pattern Dictionary of English Verbs (PDEV) (Hanks and Pustejovsky, 2005). PDEV is a semantic concordance built on yet a different principle than FrameNet, WordNet, PropBank or OntoNotes. The manually extracted patterns of frequent and normal verb uses are, roughly speaking, intuitively similar uses of a verb that express—in a syntactically similar form—a similar event in which similar participants (e.g. humans, artifacts, institutions, other events) are involved. Two patterns can be semantically so tightly related that they could appear together under one sense in a traditional dictionary. The patterns are not senses but syntactico-semantically characterized proto</context>
</contexts>
<marker>Hanks, Pustejovsky, 2005</marker>
<rawString>Patrick Hanks and James Pustejovsky. 2005. A pattern dictionary for natural language processing. Revue Francaise de linguistique applique, 10(2).</rawString>
</citation>
<citation valid="false">
<authors>
<author>forthcoming</author>
</authors>
<title>Lexical Analysis: Norms and Exploitations.</title>
<publisher>MIT Press.</publisher>
<marker>forthcoming, </marker>
<rawString>Patrick Hanks. forthcoming. Lexical Analysis: Norms and Exploitations. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aleˇs Hor´ak</author>
<author>Adam Rambousek</author>
<author>Piek Vossen</author>
</authors>
<title>A distributed database system for developing ontological and lexical resources in harmony.</title>
<date>2008</date>
<booktitle>In 9th International Conference on Intelligent Text Processing and Computational Linguistics,</booktitle>
<pages>1--15</pages>
<publisher>Springer.</publisher>
<location>Berlin:</location>
<marker>Hor´ak, Rambousek, Vossen, 2008</marker>
<rawString>Aleˇs Hor´ak, Adam Rambousek, and Piek Vossen. 2008. A distributed database system for developing ontological and lexical resources in harmony. In 9th International Conference on Intelligent Text Processing and Computational Linguistics, pages 1–15. Berlin: Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>OntoNotes: the 90% solution.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, NAACL-Short ’06,</booktitle>
<pages>57--60</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2926" citStr="Hovy et al., 2006" startWordPosition="456" endWordPosition="459">n, which is supposed to be shared by all speakers of the same language. The assumed common mental representation of a words meaning should make it easy for other humans to assign random occurrences of the word to one of the pre-defined senses (Fellbaum et al., 1997). This assumption seems to be falsified by the interannotator agreement (IAA, sometimes ITA) constantly reported much lower in semantic than in morphological or syntactic annotation, as well as by the general divergence of opinion on which value of which IAA measure indicates a reliable annotation. In some projects (e.g. OntoNotes (Hovy et al., 2006)), the percentage of agreements between two annotators is used, but a number of more complex measures are available (for a comprehensive survey see (Artstein and Poesio, 2008)). Consequently, using different measures for IAA makes the reported IAA values incomparable across different projects. Even skilled lexicographers have trouble selecting one discrete sense for a concordance (Krishnamurthy and Nicholls, 2000), and, more to say, when the tagging performance of lexicographers and ordinary annotators (students) was 840 Proceedings of the 13th Conference of the European Chapter of the Associa</context>
<context position="9555" citStr="Hovy et al., 2006" startWordPosition="1509" endWordPosition="1512">dealing with semantic tasks. When WordNet (Fellbaum, 1998) was created, both IAA and subjective confidence measurements served as an informal feedback to lexicographers (Fellbaum et al., (1998), p. 200). In general, WordNet has been considered a resource too fine-grained for most annotations (and applications). Navigli (2006) developed a method of reducing the granularity of WordNet by mapping the synsets to senses in a more coarse-grained dictionary. A manual, more coarse-grained grouping of WordNet senses has been performed in OntoNotes (Weischedel et al., 2011). The OntoNotes 90% solution (Hovy et al., 2006) actually means such a degree of granularity that enables a 90-%-IAA. OntoNotes is a reaction to the traditionally poor IAA in WordNet annotated corpora, caused by the high granularity of senses. The quality of semantic concordances is maintained by numerous iterations between lexicographers and annotators. The categories ‘right’–‘wrong’ have been, for the purpose of the annotated linguistic resource, defined by the IAA score, which is—in OntoNotes— calculated as the percentage of agreements between two annotators. Two other, somewhat different, lexical resources have to be mentioned to comple</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. OntoNotes: the 90% solution. In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, NAACL-Short ’06, pages 57–60, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
<author>Collin Baker</author>
<author>Christiane Fellbaum</author>
<author>Charles Fillmore</author>
<author>Rebecca Passoneau</author>
</authors>
<title>MASC: The Manually Annotated Sub-Corpus of American English.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08),</booktitle>
<pages>28--30</pages>
<contexts>
<context position="5013" citStr="Ide et al., 2008" startWordPosition="777" endWordPosition="780">, 2010). Also, the annotators are less confident in their decisions, when they have many options to choose from (Fellbaum et al. (1998) reported a drop in subjective annotators confidence in words with 8+ senses). Despite all the known issues in semantic tagging, the major lexical resources (WordNet (Fellbaum, 1998), FrameNet (Ruppenhofer et al., 2010), PropBank (Palmer et al., 2005) and the word-sense part of OntoNotes (Weischedel et al., 2011)) are still maintained and their annotation schemes are adopted for creating new manually annotated data (e.g. MASC, the Manually Annotated Subcorpus (Ide et al., 2008)). More to say, these resources are not only used in WSD and semantic labeling, but also in research directions that in their turn do not rely on the idea of an inventory of discrete senses any more, e.g. in distributional semantics (Erk, 2010) and recognizing textual entailment (e.g. (Zanzotto et al., 2009) and (Aharon et al., 2010)). It is a remarkable fact that, to the best of our knowledge, there is no measure that would relate granularity, reliability of the annotation (derived from IAA) and the resulting information gain. Therefore it is impossible to say where the optimum for granularit</context>
<context position="11327" citStr="Ide et al., 2008" startWordPosition="1789" endWordPosition="1792">he Aesthetics frame) and their collocates in relevant syntactic positions (arguments of verbs, head nouns of adjectives, etc.) would be assigned the corresponding frameelement labels (e.g. in their dazzling charm, their would be The Entity for which a particular gradable Attribute is appropriate and under consideration and dazzling would be Degree). Neither IAA nor granularity seem to be an issue in FrameNet. We have not succeeded in finding a report on IAA in the original FrameNet annotation, except one measurement in progress in the annotation of the Manually Annotated Subcorpus of English (Ide et al., 2008).1 PropBank is a valency (argument structure) lexicon. The current resource lists and labels arguments and obligatory modifiers typical of each (very coarse) word sense (called frameset). Two core criteria for distinguishing among framesets are the semantic roles of the arguments along with the syntactic alternations that the verb can undergo with that particular argument set. To keep low granularity, this lexicon—among other things—does usually not make special framesets for metaphoric uses. The overall IAA measured on verbs was 94 % (Palmer et al., 2005). 2.3 Semantic Pattern Recognition Fro</context>
</contexts>
<marker>Ide, Baker, Fellbaum, Fillmore, Passoneau, 2008</marker>
<rawString>Nancy Ide, Collin Baker, Christiane Fellbaum, Charles Fillmore, and Rebecca Passoneau. 2008. MASC: The Manually Annotated Sub-Corpus of American English. In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC’08), pages 28–30. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Jorgensen</author>
</authors>
<title>The psycholinguistic reality of word senses.</title>
<date>1990</date>
<journal>Journal of Psycholinguistic Research,</journal>
<contexts>
<context position="3821" citStr="Jorgensen, 1990" startWordPosition="590" endWordPosition="592">erent projects. Even skilled lexicographers have trouble selecting one discrete sense for a concordance (Krishnamurthy and Nicholls, 2000), and, more to say, when the tagging performance of lexicographers and ordinary annotators (students) was 840 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 840–850, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics compared, the experiment showed that the mental representations of a word’s semantics differ for each group (Fellbaum et al., 1997), and cf. (Jorgensen, 1990). Lexicographers are trained in considering subtle differences among various uses of a word, which ordinary language users do not reflect. Identifying a semantic difference between uses of a word and deciding whether a difference is important enough to constitute a separate sense means presenting a word with a certain degree of semantic granularity. Intuitively, the finer the granularity of a word entry is, the more opportunities for interannotator disagreement there are and the lower IAA can be expected. Brown et al. proved this hypothesis experimentally (Brown et al., 2010). Also, the annota</context>
</contexts>
<marker>Jorgensen, 1990</marker>
<rawString>Julia Jorgensen. 1990. The psycholinguistic reality of word senses. Journal of Psycholinguistic Research, (19):167–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
</authors>
<title>I don’t believe in word senses”.</title>
<date>1997</date>
<journal>Computers and the Humanities,</journal>
<volume>31</volume>
<issue>2</issue>
<pages>113</pages>
<contexts>
<context position="13602" citStr="Kilgarriff (1997" startWordPosition="2140" endWordPosition="2141">iven lexical item, rather than relying on very detailed definitions. In other words, the sense definitions are obtained as syntactico-semantic abstractions of manually clustered corpus concordances in the modern corpus-based lexicography: in classical dictionaries as well as in semantic concordances. Nevertheless, the word senses, even when obtained by a collective mind of lexicographers and annotators, are naturally hard-wired and tailored to the annotated corpus. They may be too finegrained or too coarse-grained for automatic processing of different corpora (e.g. a restricteddomain corpus). Kilgarriff (1997, p. 115) shows (the handbag example) that there is no reason to expect the same set of word senses to be relevant for different tasks and that the corpus dictates the word senses and therefore ‘word sense’ was not found to be sufficiently well-defined to be a workable basic unit of meaning (p. 116). On the other hand, even non-experts seem to agree reasonably well when judging the similarity of use of a word in different contexts (Rumshisky et al., 2009). Erk et al. (2009) showed promising annotation results with a scheme that allowed the annotators graded judgments of similarity between two </context>
</contexts>
<marker>Kilgarriff, 1997</marker>
<rawString>Adam Kilgarriff. 1997. “I don’t believe in word senses”. Computers and the Humanities, 31(2):91– 113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ramesh Krishnamurthy</author>
<author>Diane Nicholls</author>
</authors>
<title>Peeling an onion: The lexicographer’s experience of manual sense tagging. Computers and the Humanities,</title>
<date>2000</date>
<pages>34--85</pages>
<contexts>
<context position="3343" citStr="Krishnamurthy and Nicholls, 2000" startWordPosition="516" endWordPosition="520">than in morphological or syntactic annotation, as well as by the general divergence of opinion on which value of which IAA measure indicates a reliable annotation. In some projects (e.g. OntoNotes (Hovy et al., 2006)), the percentage of agreements between two annotators is used, but a number of more complex measures are available (for a comprehensive survey see (Artstein and Poesio, 2008)). Consequently, using different measures for IAA makes the reported IAA values incomparable across different projects. Even skilled lexicographers have trouble selecting one discrete sense for a concordance (Krishnamurthy and Nicholls, 2000), and, more to say, when the tagging performance of lexicographers and ordinary annotators (students) was 840 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 840–850, Avignon, France, April 23 - 27 2012. c�2012 Association for Computational Linguistics compared, the experiment showed that the mental representations of a word’s semantics differ for each group (Fellbaum et al., 1997), and cf. (Jorgensen, 1990). Lexicographers are trained in considering subtle differences among various uses of a word, which ordinary language users</context>
</contexts>
<marker>Krishnamurthy, Nicholls, 2000</marker>
<rawString>Ramesh Krishnamurthy and Diane Nicholls. 2000. Peeling an onion: The lexicographer’s experience of manual sense tagging. Computers and the Humanities, 34:85–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Kevin Lerman</author>
<author>Fernando Pereira</author>
</authors>
<title>Multilingual dependency analysis with a two-stage discriminative parser.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning CoNLLX 06,</booktitle>
<pages>216--220</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7774" citStr="McDonald et al., 2006" startWordPosition="1221" endWordPosition="1224">they can be reflected by a few additional conventions in the annotation manual (e.g. where to draw the line between particles and prepositions or between adjectives and verbs in past participles (Santorini, 1990) or where to attach a prepositional phrase following a noun phrase and how to treat specific “financialspeak” structures (Bies et al., 1995)). Even in difficult cases, there are hardly more than two options of interpretation. Data manually annotated for morphology or surface syntax are reliable enough to train syntactic parsers with an accuracy above 80 % (e.g. (Zhang and Clark, 2011; McDonald et al., 2006)). On the other hand, semantic tagging actually employs a different tagset for each word lemma. Even within the same part of speech, individual words require individual descriptions. Possible similarities among them come into relief ex post rather than that they could be imposed on the lexicographers from the beginning. When assigning senses to concordances, the annotator often has to select among more than two relevant options. These two aspects make achieving good IAA much harder than in morphology and syn841 tax tasks. In addition, while a linguistically educated annotator can have roughly </context>
</contexts>
<marker>McDonald, Lerman, Pereira, 2006</marker>
<rawString>Ryan McDonald, Kevin Lerman, and Fernando Pereira. 2006. Multilingual dependency analysis with a two-stage discriminative parser. In Proceedings of the Tenth Conference on Computational Natural Language Learning CoNLLX 06, pages 216– 220. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Meyers</author>
<author>Ruth Reeves</author>
<author>Catherine Macleod</author>
</authors>
<date>2008</date>
<note>NomBank v 1.0.</note>
<contexts>
<context position="10486" citStr="Meyers et al., 2008" startWordPosition="1655" endWordPosition="1658">e categories ‘right’–‘wrong’ have been, for the purpose of the annotated linguistic resource, defined by the IAA score, which is—in OntoNotes— calculated as the percentage of agreements between two annotators. Two other, somewhat different, lexical resources have to be mentioned to complete the picture: FrameNet (Ruppenhofer et al., 2010) and PropBank (Palmer et al., 2005). While WordNet and OntoNotes pair words and word senses in a way comparable to printed lexicons, FrameNet is primarily an inventory of semantic frames and PropBank focuses the argument structure of verbs and nouns (NomBank (Meyers et al., 2008), a related project capturing the argument structure of nouns, was later integrated in OntoNotes). In FrameNet corpora, content words are associated to particular semantic frames that they evoke (e.g. charm would relate to the Aesthetics frame) and their collocates in relevant syntactic positions (arguments of verbs, head nouns of adjectives, etc.) would be assigned the corresponding frameelement labels (e.g. in their dazzling charm, their would be The Entity for which a particular gradable Attribute is appropriate and under consideration and dazzling would be Degree). Neither IAA nor granular</context>
</contexts>
<marker>Meyers, Reeves, Macleod, 2008</marker>
<rawString>Adam Meyers, Ruth Reeves, and Catherine Macleod. 2008. NomBank v 1.0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
<author>C Leacock</author>
<author>R Tengi</author>
<author>R T Bunker</author>
</authors>
<title>A semantic concordance.</title>
<date>1993</date>
<booktitle>In Proceedings of ARPA Workshop on Human Language Technology.</booktitle>
<contexts>
<context position="6536" citStr="Miller et al., 1993" startWordPosition="1024" endWordPosition="1027">o a reference source. There is, nevertheless, a substantial difference: whereas morphologically or syntactically annotated data exist separately from the reference (tagset, annotation guide, annotation scheme), a semantically tagged resource can be regarded both as a corpus of texts disambiguated according to an attached inventory of semantic categories and as a lexicon with links to example concordances for each semantic category. So, in semantically tagged resources, the data and the reference are intertwined. Such double-faced semantic resources have also been called semantic concordances (Miller et al., 1993a). For instance, one of the earlier versions of WordNet, the largest lexical resource for English, was used in the semantic concordance SemCor (Miller et al., 1993b). More recent lexical resources have been built as semantic concordances from the very beginning (PropBank (Palmer et al., 2005), OntoNotes word senses (Weischedel et al., 2011)). In morphological or syntactic annotation, the tagset or inventory of constituents are given beforehand and are supposed to hold for all tokens/sentences contained in the corpus. Problematic and theory-dependent issues are few and mostly well-known in adv</context>
</contexts>
<marker>Miller, Leacock, Tengi, Bunker, 1993</marker>
<rawString>G. A. Miller, C. Leacock, R. Tengi, and R. T. Bunker. 1993a. A semantic concordance. In Proceedings of ARPA Workshop on Human Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
<author>C Leacock</author>
<author>R Tengi</author>
<author>R T Bunker</author>
</authors>
<title>A semantic concordance.</title>
<date>1993</date>
<booktitle>In Proceedings of ARPA Workshop on Human Language Technology.</booktitle>
<contexts>
<context position="6536" citStr="Miller et al., 1993" startWordPosition="1024" endWordPosition="1027">o a reference source. There is, nevertheless, a substantial difference: whereas morphologically or syntactically annotated data exist separately from the reference (tagset, annotation guide, annotation scheme), a semantically tagged resource can be regarded both as a corpus of texts disambiguated according to an attached inventory of semantic categories and as a lexicon with links to example concordances for each semantic category. So, in semantically tagged resources, the data and the reference are intertwined. Such double-faced semantic resources have also been called semantic concordances (Miller et al., 1993a). For instance, one of the earlier versions of WordNet, the largest lexical resource for English, was used in the semantic concordance SemCor (Miller et al., 1993b). More recent lexical resources have been built as semantic concordances from the very beginning (PropBank (Palmer et al., 2005), OntoNotes word senses (Weischedel et al., 2011)). In morphological or syntactic annotation, the tagset or inventory of constituents are given beforehand and are supposed to hold for all tokens/sentences contained in the corpus. Problematic and theory-dependent issues are few and mostly well-known in adv</context>
</contexts>
<marker>Miller, Leacock, Tengi, Bunker, 1993</marker>
<rawString>G. A. Miller, C. Leacock, R. Tengi, and R. T. Bunker. 1993b. A semantic concordance. In Proceedings of ARPA Workshop on Human Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Meaningful clustering of senses helps boost word sense disambiguation performance.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL,</booktitle>
<pages>105--112</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="9264" citStr="Navigli (2006)" startWordPosition="1464" endWordPosition="1466">be achieved is that the entry reflects the corpus data in a reasonable granular way on which annotators still can reach reasonable IAA. 2.2 Major existing semantic resources The granularity vs. IAA equilibrium is of great concern in creating lexical resources as well as in applications dealing with semantic tasks. When WordNet (Fellbaum, 1998) was created, both IAA and subjective confidence measurements served as an informal feedback to lexicographers (Fellbaum et al., (1998), p. 200). In general, WordNet has been considered a resource too fine-grained for most annotations (and applications). Navigli (2006) developed a method of reducing the granularity of WordNet by mapping the synsets to senses in a more coarse-grained dictionary. A manual, more coarse-grained grouping of WordNet senses has been performed in OntoNotes (Weischedel et al., 2011). The OntoNotes 90% solution (Hovy et al., 2006) actually means such a degree of granularity that enables a 90-%-IAA. OntoNotes is a reaction to the traditionally poor IAA in WordNet annotated corpora, caused by the high granularity of senses. The quality of semantic concordances is maintained by numerous iterations between lexicographers and annotators. </context>
</contexts>
<marker>Navigli, 2006</marker>
<rawString>Roberto Navigli. 2006. Meaningful clustering of senses helps boost word sense disambiguation performance. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 105–112, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Dan Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: A corpus annotated with semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics Journal,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="4782" citStr="Palmer et al., 2005" startWordPosition="741" endWordPosition="744">anularity. Intuitively, the finer the granularity of a word entry is, the more opportunities for interannotator disagreement there are and the lower IAA can be expected. Brown et al. proved this hypothesis experimentally (Brown et al., 2010). Also, the annotators are less confident in their decisions, when they have many options to choose from (Fellbaum et al. (1998) reported a drop in subjective annotators confidence in words with 8+ senses). Despite all the known issues in semantic tagging, the major lexical resources (WordNet (Fellbaum, 1998), FrameNet (Ruppenhofer et al., 2010), PropBank (Palmer et al., 2005) and the word-sense part of OntoNotes (Weischedel et al., 2011)) are still maintained and their annotation schemes are adopted for creating new manually annotated data (e.g. MASC, the Manually Annotated Subcorpus (Ide et al., 2008)). More to say, these resources are not only used in WSD and semantic labeling, but also in research directions that in their turn do not rely on the idea of an inventory of discrete senses any more, e.g. in distributional semantics (Erk, 2010) and recognizing textual entailment (e.g. (Zanzotto et al., 2009) and (Aharon et al., 2010)). It is a remarkable fact that, t</context>
<context position="6830" citStr="Palmer et al., 2005" startWordPosition="1071" endWordPosition="1074">ated according to an attached inventory of semantic categories and as a lexicon with links to example concordances for each semantic category. So, in semantically tagged resources, the data and the reference are intertwined. Such double-faced semantic resources have also been called semantic concordances (Miller et al., 1993a). For instance, one of the earlier versions of WordNet, the largest lexical resource for English, was used in the semantic concordance SemCor (Miller et al., 1993b). More recent lexical resources have been built as semantic concordances from the very beginning (PropBank (Palmer et al., 2005), OntoNotes word senses (Weischedel et al., 2011)). In morphological or syntactic annotation, the tagset or inventory of constituents are given beforehand and are supposed to hold for all tokens/sentences contained in the corpus. Problematic and theory-dependent issues are few and mostly well-known in advance. Therefore they can be reflected by a few additional conventions in the annotation manual (e.g. where to draw the line between particles and prepositions or between adjectives and verbs in past participles (Santorini, 1990) or where to attach a prepositional phrase following a noun phrase</context>
<context position="10241" citStr="Palmer et al., 2005" startWordPosition="1615" endWordPosition="1618">AA. OntoNotes is a reaction to the traditionally poor IAA in WordNet annotated corpora, caused by the high granularity of senses. The quality of semantic concordances is maintained by numerous iterations between lexicographers and annotators. The categories ‘right’–‘wrong’ have been, for the purpose of the annotated linguistic resource, defined by the IAA score, which is—in OntoNotes— calculated as the percentage of agreements between two annotators. Two other, somewhat different, lexical resources have to be mentioned to complete the picture: FrameNet (Ruppenhofer et al., 2010) and PropBank (Palmer et al., 2005). While WordNet and OntoNotes pair words and word senses in a way comparable to printed lexicons, FrameNet is primarily an inventory of semantic frames and PropBank focuses the argument structure of verbs and nouns (NomBank (Meyers et al., 2008), a related project capturing the argument structure of nouns, was later integrated in OntoNotes). In FrameNet corpora, content words are associated to particular semantic frames that they evoke (e.g. charm would relate to the Aesthetics frame) and their collocates in relevant syntactic positions (arguments of verbs, head nouns of adjectives, etc.) woul</context>
<context position="11889" citStr="Palmer et al., 2005" startWordPosition="1875" endWordPosition="1878">Manually Annotated Subcorpus of English (Ide et al., 2008).1 PropBank is a valency (argument structure) lexicon. The current resource lists and labels arguments and obligatory modifiers typical of each (very coarse) word sense (called frameset). Two core criteria for distinguishing among framesets are the semantic roles of the arguments along with the syntactic alternations that the verb can undergo with that particular argument set. To keep low granularity, this lexicon—among other things—does usually not make special framesets for metaphoric uses. The overall IAA measured on verbs was 94 % (Palmer et al., 2005). 2.3 Semantic Pattern Recognition From corpus-based lexicography to semantic patterns The modern, corpus-based lexicology of 1990s (Sinclair, 1991; Fillmore and Atkins, 1994) has had a great impact on lexicography. There is a general consensus that dictionary definitions need to be supported by corpus examples. Cf. Fellbaum (2001): “For polysemous words, dictionaries [... ] do not say enough about the range of possible contexts that differentiate the senses. [... ] On the other hand, texts or corpora [... ] are not explicit about the word’s meaning. When we first encounter a new word in a tex</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005. The proposition bank: A corpus annotated with semantic roles. Computational Linguistics Journal, 31(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
<author>Ansaf Salleb-Aoussi</author>
<author>Vikas Bhardwaj</author>
<author>Nancy Ide</author>
</authors>
<title>Word sense annotation of PolysemousWords by multiple annotators.</title>
<date>2010</date>
<booktitle>In LREC Proceedings,</booktitle>
<pages>3244--3249</pages>
<location>Valetta,</location>
<contexts>
<context position="17022" citStr="Passonneau et al. (2010)" startWordPosition="2690" endWordPosition="2693">ational implicatures from the meaning potential of the given verb. Our method draws on a hard-wired, fine-grained inventory of semantic categories manually extracted from corpus data. This inventory represents the maximum semantic granularity that humans are able to recognize in normal and frequent uses of a verb in a balanced corpus. We thoroughly analyze the interannotator agreement to find out which of the highly semantic categories are useful in the sense of information gain. Our goal is a dynamic optimization of semantic granularity with respect to given data and target application. Like Passonneau et al. (2010), we are convinced that IAA is specific to each respective word and reflects its inherent semantic properties as well as the specificity of contexts the given word occurs in, even within the same balanced corpus. We accept as a matter of fact that interannotator confusion is inevitable in semantic tagging. However, the amount of uncertainty of the 843 No. Pattern / Implicature 1 [[Human 1 |Institution 1] &amp;quot; [Human 1 |Institution 1 = Competitor]] submit [[Plan |Document |Speech Act |Proposition |{complaint |demand |request |claim |application |proposal |report |resignation |information |plea |pe</context>
</contexts>
<marker>Passonneau, Salleb-Aoussi, Bhardwaj, Ide, 2010</marker>
<rawString>Rebecca J. Passonneau, Ansaf Salleb-Aoussi, Vikas Bhardwaj, and Nancy Ide. 2010. Word sense annotation of PolysemousWords by multiple annotators. In LREC Proceedings, pages 3244–3249, Valetta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
<author>David Yarowsky</author>
</authors>
<title>Distinguishing systems and distinguishing senses: New evaluation methods for word sense disambiguation.</title>
<date>1999</date>
<journal>Natural Language Engineering,</journal>
<volume>5</volume>
<issue>2</issue>
<contexts>
<context position="32907" citStr="Resnik and Yarowsky (1999)" startWordPosition="5531" endWordPosition="5534">verbs generally tend to be polysemous rather than homonymous. A few approaches have been suggested in the literature that address the problem of the fine-grained semantic distinctions by (automatic) measuring sense distinguishability. Diab (2004) computes sense perplexity using the entropy function as a characteristic of training data. She also compares the sense distributions to obtain sense distributional correlation, which can serve as a “very good direct indicator of performance ratio”, especially together with sense context confusability (another indicator observed in the training data). Resnik and Yarowsky (1999) introduced the communicative/semantic distance between the predicted sense and the “correct” sense. Then they use it for evaluation metric that provides partial credit for incorrectly classified instances. Cohn (2003) introduces the concept of (non-uniform) misclassification costs. He makes use of the communicative/semantic distance and proposes a metric for evaluating word sense disambiguation performance using the Receiver Operating Characteristics curve that takes the misclassification costs into account. Bruce and Wiebe (1998) analyze the agreement among human judges for the purpose of fo</context>
</contexts>
<marker>Resnik, Yarowsky, 1999</marker>
<rawString>Philip Resnik and David Yarowsky. 1999. Distinguishing systems and distinguishing senses: New evaluation methods for word sense disambiguation. Natural Language Engineering, 5(2):113–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Rumshisky</author>
<author>M Verhagen</author>
<author>J Moszkowicz</author>
</authors>
<title>The holy grail of sense definition: Creating a Sense-Disambiguated corpus from scratch.</title>
<date>2009</date>
<location>Pisa, Italy.</location>
<contexts>
<context position="14061" citStr="Rumshisky et al., 2009" startWordPosition="2220" endWordPosition="2223">annotated corpus. They may be too finegrained or too coarse-grained for automatic processing of different corpora (e.g. a restricteddomain corpus). Kilgarriff (1997, p. 115) shows (the handbag example) that there is no reason to expect the same set of word senses to be relevant for different tasks and that the corpus dictates the word senses and therefore ‘word sense’ was not found to be sufficiently well-defined to be a workable basic unit of meaning (p. 116). On the other hand, even non-experts seem to agree reasonably well when judging the similarity of use of a word in different contexts (Rumshisky et al., 2009). Erk et al. (2009) showed promising annotation results with a scheme that allowed the annotators graded judgments of similarity between two words or between a word and its definition. Verbs are the most challenging part of speech. We see two major causes: vagueness and coercion. We neglect ambiguity, since it has proved to be rare in our experience. CPA and PDEV Our current work focuses on English verbs. It has been inspired by the manual Corpus Pattern Analysis method (CPA) (Hanks, forthcoming) and its implementation, the Pattern Dictionary of English Verbs (PDEV) (Hanks and Pustejovsky, 200</context>
</contexts>
<marker>Rumshisky, Verhagen, Moszkowicz, 2009</marker>
<rawString>Anna Rumshisky, M. Verhagen, and J. Moszkowicz. 2009. The holy grail of sense definition: Creating a Sense-Disambiguated corpus from scratch. Pisa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Rundell</author>
</authors>
<title>Macmillan English Dictionary for advanced learners.</title>
<date>2002</date>
<publisher>Macmillan Education.</publisher>
<contexts>
<context position="12879" citStr="Rundell, 2002" startWordPosition="2036" endWordPosition="2037">] do not say enough about the range of possible contexts that differentiate the senses. [... ] On the other hand, texts or corpora [... ] are not explicit about the word’s meaning. When we first encounter a new word in a text, we can usually form only a vague idea of its meaning; checking a dictionary will clarify the meaning. But the more contexts we encounter for a word, the harder it is to match them against only one dictionary sense.” 1Checked on the project web www.anc.org/MASC/Home 2011-10-29. 842 The lexical description in modern English monolingual dictionaries (Sinclair et al., 1987; Rundell, 2002) explicitly emphasizes contextual clues, such as typical collocates and the syntactic surroundings of the given lexical item, rather than relying on very detailed definitions. In other words, the sense definitions are obtained as syntactico-semantic abstractions of manually clustered corpus concordances in the modern corpus-based lexicography: in classical dictionaries as well as in semantic concordances. Nevertheless, the word senses, even when obtained by a collective mind of lexicographers and annotators, are naturally hard-wired and tailored to the annotated corpus. They may be too finegra</context>
</contexts>
<marker>Rundell, 2002</marker>
<rawString>Michael Rundell. 2002. Macmillan English Dictionary for advanced learners. Macmillan Education.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Ruppenhofer</author>
<author>Michael Ellsworth</author>
<author>Miriam R L Petruck</author>
<author>Christopher R Johnson</author>
<author>Jan Scheffczyk</author>
</authors>
<title>FrameNet II: Extended Theory and Practice. ICSI,</title>
<date>2010</date>
<institution>University of Berkeley,</institution>
<contexts>
<context position="4750" citStr="Ruppenhofer et al., 2010" startWordPosition="736" endWordPosition="739"> with a certain degree of semantic granularity. Intuitively, the finer the granularity of a word entry is, the more opportunities for interannotator disagreement there are and the lower IAA can be expected. Brown et al. proved this hypothesis experimentally (Brown et al., 2010). Also, the annotators are less confident in their decisions, when they have many options to choose from (Fellbaum et al. (1998) reported a drop in subjective annotators confidence in words with 8+ senses). Despite all the known issues in semantic tagging, the major lexical resources (WordNet (Fellbaum, 1998), FrameNet (Ruppenhofer et al., 2010), PropBank (Palmer et al., 2005) and the word-sense part of OntoNotes (Weischedel et al., 2011)) are still maintained and their annotation schemes are adopted for creating new manually annotated data (e.g. MASC, the Manually Annotated Subcorpus (Ide et al., 2008)). More to say, these resources are not only used in WSD and semantic labeling, but also in research directions that in their turn do not rely on the idea of an inventory of discrete senses any more, e.g. in distributional semantics (Erk, 2010) and recognizing textual entailment (e.g. (Zanzotto et al., 2009) and (Aharon et al., 2010)).</context>
<context position="10206" citStr="Ruppenhofer et al., 2010" startWordPosition="1609" endWordPosition="1612">ree of granularity that enables a 90-%-IAA. OntoNotes is a reaction to the traditionally poor IAA in WordNet annotated corpora, caused by the high granularity of senses. The quality of semantic concordances is maintained by numerous iterations between lexicographers and annotators. The categories ‘right’–‘wrong’ have been, for the purpose of the annotated linguistic resource, defined by the IAA score, which is—in OntoNotes— calculated as the percentage of agreements between two annotators. Two other, somewhat different, lexical resources have to be mentioned to complete the picture: FrameNet (Ruppenhofer et al., 2010) and PropBank (Palmer et al., 2005). While WordNet and OntoNotes pair words and word senses in a way comparable to printed lexicons, FrameNet is primarily an inventory of semantic frames and PropBank focuses the argument structure of verbs and nouns (NomBank (Meyers et al., 2008), a related project capturing the argument structure of nouns, was later integrated in OntoNotes). In FrameNet corpora, content words are associated to particular semantic frames that they evoke (e.g. charm would relate to the Aesthetics frame) and their collocates in relevant syntactic positions (arguments of verbs, h</context>
</contexts>
<marker>Ruppenhofer, Ellsworth, Petruck, Johnson, Scheffczyk, 2010</marker>
<rawString>Josef Ruppenhofer, Michael Ellsworth, Miriam R. L. Petruck, Christopher R. Johnson, and Jan Scheffczyk. 2010. FrameNet II: Extended Theory and Practice. ICSI, University of Berkeley, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beatrice Santorini</author>
</authors>
<title>Part-of-Speech tagging guidelines for the penn treebank project.</title>
<date>1990</date>
<booktitle>University of Pennsylvania 3rd Revision 2nd Printing,</booktitle>
<pages>90--47</pages>
<contexts>
<context position="7364" citStr="Santorini, 1990" startWordPosition="1155" endWordPosition="1156">t as semantic concordances from the very beginning (PropBank (Palmer et al., 2005), OntoNotes word senses (Weischedel et al., 2011)). In morphological or syntactic annotation, the tagset or inventory of constituents are given beforehand and are supposed to hold for all tokens/sentences contained in the corpus. Problematic and theory-dependent issues are few and mostly well-known in advance. Therefore they can be reflected by a few additional conventions in the annotation manual (e.g. where to draw the line between particles and prepositions or between adjectives and verbs in past participles (Santorini, 1990) or where to attach a prepositional phrase following a noun phrase and how to treat specific “financialspeak” structures (Bies et al., 1995)). Even in difficult cases, there are hardly more than two options of interpretation. Data manually annotated for morphology or surface syntax are reliable enough to train syntactic parsers with an accuracy above 80 % (e.g. (Zhang and Clark, 2011; McDonald et al., 2006)). On the other hand, semantic tagging actually employs a different tagset for each word lemma. Even within the same part of speech, individual words require individual descriptions. Possibl</context>
</contexts>
<marker>Santorini, 1990</marker>
<rawString>Beatrice Santorini. 1990. Part-of-Speech tagging guidelines for the penn treebank project. University of Pennsylvania 3rd Revision 2nd Printing, (MSCIS-90-47):33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Scott</author>
</authors>
<title>Reliability of content analysis: The case of nominal scale coding. Public Opinion Quarterly,</title>
<date>1955</date>
<contexts>
<context position="21325" citStr="Scott, 1955" startWordPosition="3338" endWordPosition="3339">ple. The annotators got back the revised entry, the newly revised reference sample and an entirely new 50-concordance annotation batch. The final multiple 50-concordance sample went through one more additional procedure, the adjudication: first, the lexicographer compared the three annotations and eliminated evident errors. Then the lexicographer selected one value for each concordance to remain in the resulting one-value-perconcordance gold standard data and recorded it into the gold standard set. The adjudication pro3Fleiss’s kappa (Fleiss, 1971) is a generalization of Scott’s it statistic (Scott, 1955). In contrast to Cohen’s kappa (Cohen, 1960), Fleiss’s kappa evaluates agreement between multiple raters. However, Fleiss’s kappa is not a generalization of Cohen’s kappa, which is a different, yet related, statistical measure. Sometimes, the terminology about kappas is confusing in the literature. For a detailed explanation refer e.g. to (Artstein and Poesio, 2008). 844 tocol has been kept for further experiments. All values except the marked errors are regarded as equally acceptable for this type of experiments. In the end, we get for each verb: • an entry, which is an inventory of semantic </context>
</contexts>
<marker>Scott, 1955</marker>
<rawString>William A. Scott. 1955. Reliability of content analysis: The case of nominal scale coding. Public Opinion Quarterly, 19(3):321–325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Sinclair</author>
<author>Patrick Hanks</author>
</authors>
<title>Collins Cobuild English Dictionary for Advanced Learners 4th edition published in</title>
<date>1987</date>
<publisher>HarperCollins Publishers</publisher>
<marker>Sinclair, Hanks, 1987</marker>
<rawString>John Sinclair, Patrick Hanks, and et al. 1987. Collins Cobuild English Dictionary for Advanced Learners 4th edition published in 2003. HarperCollins Publishers 1987, 1995, 2001, 2003 and Collins A–Z Thesaurus 1st edition first published in 1995. HarperCollins Publishers 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Sinclair</author>
</authors>
<title>Corpus, Concordance, Collocation. Describing English Language.</title>
<date>1991</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="12036" citStr="Sinclair, 1991" startWordPosition="1895" endWordPosition="1896">guments and obligatory modifiers typical of each (very coarse) word sense (called frameset). Two core criteria for distinguishing among framesets are the semantic roles of the arguments along with the syntactic alternations that the verb can undergo with that particular argument set. To keep low granularity, this lexicon—among other things—does usually not make special framesets for metaphoric uses. The overall IAA measured on verbs was 94 % (Palmer et al., 2005). 2.3 Semantic Pattern Recognition From corpus-based lexicography to semantic patterns The modern, corpus-based lexicology of 1990s (Sinclair, 1991; Fillmore and Atkins, 1994) has had a great impact on lexicography. There is a general consensus that dictionary definitions need to be supported by corpus examples. Cf. Fellbaum (2001): “For polysemous words, dictionaries [... ] do not say enough about the range of possible contexts that differentiate the senses. [... ] On the other hand, texts or corpora [... ] are not explicit about the word’s meaning. When we first encounter a new word in a text, we can usually form only a vague idea of its meaning; checking a dictionary will clarify the meaning. But the more contexts we encounter for a w</context>
</contexts>
<marker>Sinclair, 1991</marker>
<rawString>John Sinclair. 1991. Corpus, Concordance, Collocation. Describing English Language. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
<author>Martha Palmer</author>
<author>Mitchell Marcus</author>
</authors>
<title>Eduard Hovy, Sameer Pradhan, Lance Ramshaw,</title>
<date>2011</date>
<location>Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle Franchini, Mohammed</location>
<contexts>
<context position="4845" citStr="Weischedel et al., 2011" startWordPosition="751" endWordPosition="754"> entry is, the more opportunities for interannotator disagreement there are and the lower IAA can be expected. Brown et al. proved this hypothesis experimentally (Brown et al., 2010). Also, the annotators are less confident in their decisions, when they have many options to choose from (Fellbaum et al. (1998) reported a drop in subjective annotators confidence in words with 8+ senses). Despite all the known issues in semantic tagging, the major lexical resources (WordNet (Fellbaum, 1998), FrameNet (Ruppenhofer et al., 2010), PropBank (Palmer et al., 2005) and the word-sense part of OntoNotes (Weischedel et al., 2011)) are still maintained and their annotation schemes are adopted for creating new manually annotated data (e.g. MASC, the Manually Annotated Subcorpus (Ide et al., 2008)). More to say, these resources are not only used in WSD and semantic labeling, but also in research directions that in their turn do not rely on the idea of an inventory of discrete senses any more, e.g. in distributional semantics (Erk, 2010) and recognizing textual entailment (e.g. (Zanzotto et al., 2009) and (Aharon et al., 2010)). It is a remarkable fact that, to the best of our knowledge, there is no measure that would rel</context>
<context position="6879" citStr="Weischedel et al., 2011" startWordPosition="1078" endWordPosition="1081">mantic categories and as a lexicon with links to example concordances for each semantic category. So, in semantically tagged resources, the data and the reference are intertwined. Such double-faced semantic resources have also been called semantic concordances (Miller et al., 1993a). For instance, one of the earlier versions of WordNet, the largest lexical resource for English, was used in the semantic concordance SemCor (Miller et al., 1993b). More recent lexical resources have been built as semantic concordances from the very beginning (PropBank (Palmer et al., 2005), OntoNotes word senses (Weischedel et al., 2011)). In morphological or syntactic annotation, the tagset or inventory of constituents are given beforehand and are supposed to hold for all tokens/sentences contained in the corpus. Problematic and theory-dependent issues are few and mostly well-known in advance. Therefore they can be reflected by a few additional conventions in the annotation manual (e.g. where to draw the line between particles and prepositions or between adjectives and verbs in past participles (Santorini, 1990) or where to attach a prepositional phrase following a noun phrase and how to treat specific “financialspeak” struc</context>
<context position="9507" citStr="Weischedel et al., 2011" startWordPosition="1501" endWordPosition="1504">creating lexical resources as well as in applications dealing with semantic tasks. When WordNet (Fellbaum, 1998) was created, both IAA and subjective confidence measurements served as an informal feedback to lexicographers (Fellbaum et al., (1998), p. 200). In general, WordNet has been considered a resource too fine-grained for most annotations (and applications). Navigli (2006) developed a method of reducing the granularity of WordNet by mapping the synsets to senses in a more coarse-grained dictionary. A manual, more coarse-grained grouping of WordNet senses has been performed in OntoNotes (Weischedel et al., 2011). The OntoNotes 90% solution (Hovy et al., 2006) actually means such a degree of granularity that enables a 90-%-IAA. OntoNotes is a reaction to the traditionally poor IAA in WordNet annotated corpora, caused by the high granularity of senses. The quality of semantic concordances is maintained by numerous iterations between lexicographers and annotators. The categories ‘right’–‘wrong’ have been, for the purpose of the annotated linguistic resource, defined by the IAA score, which is—in OntoNotes— calculated as the percentage of agreements between two annotators. Two other, somewhat different, </context>
</contexts>
<marker>Weischedel, Palmer, Marcus, 2011</marker>
<rawString>Ralph Weischedel, Martha Palmer, Mitchell Marcus, Eduard Hovy, Sameer Pradhan, Lance Ramshaw, Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle Franchini, Mohammed El-Bachouti, Robert Belvin, and Ann Houston. 2011. OntoNotes release 4.0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Marco Pennacchiotti</author>
<author>Alessandro Moschitti</author>
</authors>
<title>A machine learning approach to textual entailment recognition.</title>
<date>2009</date>
<journal>Natural Language Engineering,</journal>
<volume>15</volume>
<issue>4</issue>
<contexts>
<context position="5322" citStr="Zanzotto et al., 2009" startWordPosition="831" endWordPosition="834">llbaum, 1998), FrameNet (Ruppenhofer et al., 2010), PropBank (Palmer et al., 2005) and the word-sense part of OntoNotes (Weischedel et al., 2011)) are still maintained and their annotation schemes are adopted for creating new manually annotated data (e.g. MASC, the Manually Annotated Subcorpus (Ide et al., 2008)). More to say, these resources are not only used in WSD and semantic labeling, but also in research directions that in their turn do not rely on the idea of an inventory of discrete senses any more, e.g. in distributional semantics (Erk, 2010) and recognizing textual entailment (e.g. (Zanzotto et al., 2009) and (Aharon et al., 2010)). It is a remarkable fact that, to the best of our knowledge, there is no measure that would relate granularity, reliability of the annotation (derived from IAA) and the resulting information gain. Therefore it is impossible to say where the optimum for granularity and IAA lies. 2 Approaches to semantic tagging 2.1 Semantic tagging vs. morphological or syntactic analysis Manual semantic tagging is in many respects similar to morphological tagging and syntactic analysis: human annotators are trained to sort certain elements occurring in a running text according to a r</context>
</contexts>
<marker>Zanzotto, Pennacchiotti, Moschitti, 2009</marker>
<rawString>Fabio Massimo Zanzotto, Marco Pennacchiotti, and Alessandro Moschitti. 2009. A machine learning approach to textual entailment recognition. Natural Language Engineering, 15(4):551–582.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Syntactic processing using the generalized perceptron and beam search. Computational Linguistics,</title>
<date>2011</date>
<contexts>
<context position="7750" citStr="Zhang and Clark, 2011" startWordPosition="1217" endWordPosition="1220"> in advance. Therefore they can be reflected by a few additional conventions in the annotation manual (e.g. where to draw the line between particles and prepositions or between adjectives and verbs in past participles (Santorini, 1990) or where to attach a prepositional phrase following a noun phrase and how to treat specific “financialspeak” structures (Bies et al., 1995)). Even in difficult cases, there are hardly more than two options of interpretation. Data manually annotated for morphology or surface syntax are reliable enough to train syntactic parsers with an accuracy above 80 % (e.g. (Zhang and Clark, 2011; McDonald et al., 2006)). On the other hand, semantic tagging actually employs a different tagset for each word lemma. Even within the same part of speech, individual words require individual descriptions. Possible similarities among them come into relief ex post rather than that they could be imposed on the lexicographers from the beginning. When assigning senses to concordances, the annotator often has to select among more than two relevant options. These two aspects make achieving good IAA much harder than in morphology and syn841 tax tasks. In addition, while a linguistically educated ann</context>
</contexts>
<marker>Zhang, Clark, 2011</marker>
<rawString>Yue Zhang and Stephen Clark. 2011. Syntactic processing using the generalized perceptron and beam search. Computational Linguistics, 37(November 2009):105–151.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>