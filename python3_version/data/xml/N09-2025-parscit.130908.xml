<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001804">
<title confidence="0.986883">
Learning Combination Features with L1 Regularization
</title>
<author confidence="0.97172">
Daisuke Okanohara† Jun’ichi Tsujii†‡§
</author>
<affiliation confidence="0.969809333333333">
†Department of Computer Science, University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan
‡School of Informatics, University of Manchester
</affiliation>
<address confidence="0.586237">
§NaCTeM (National Center for Text Mining)
</address>
<email confidence="0.999488">
{hillbig,tsujii}@is.s.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.998603" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999839052631579">
When linear classifiers cannot successfully
classify data, we often add combination fea-
tures, which are products of several original
features. The searching for effective combi-
nation features, namely feature engineering,
requires domain-specific knowledge and hard
work. We present herein an efficient algorithm
for learning an L1 regularized logistic regres-
sion model with combination features. We
propose to use the grafting algorithm with ef-
ficient computation of gradients. This enables
us to find optimal weights efficiently without
enumerating all combination features. By us-
ing L1 regularization, the result we obtain is
very compact and achieves very efficient in-
ference. In experiments with NLP tasks, we
show that the proposed method can extract ef-
fective combination features, and achieve high
performance with very few features.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999630625">
A linear classifier is a fundamental tool for many
NLP applications, including logistic regression
models (LR), in that its score is based on a lin-
ear combination of features and their weights,. Al-
though a linear classifier is very simple, it can
achieve high performance on many NLP tasks,
partly because many problems are described with
very high-dimensional data, and high dimensional
weight vectors are effective in discriminating among
examples.
However, when an original problem cannot be
handled linearly, combination features are often
added to the feature set, where combination features
are products of several original features. Examples
of combination features are, word pairs in docu-
ment classification, or part-of-speech pairs of head
</bodyText>
<page confidence="0.993601">
97
</page>
<bodyText confidence="0.99997845945946">
and modifier words in a dependency analysis task.
However, the task of determining effective combina-
tion features, namely feature engineering, requires
domain-specific knowledge and hard work.
Such a non-linear phenomenon can be implic-
itly captured by using the kernel trick. However,
its computational cost is very high, not only during
training but also at inference time. Moreover, the
model is not interpretable, in that effective features
are not represented explicitly. Many kernels meth-
ods assume an L2 regularizer, in that many features
are equally relevant to the tasks (Ng, 2004).
There have been several studies to find efficient
ways to obtain (combination) features. In the con-
text of boosting, Kudo (2004) have proposed a
method to extract complex features that is similar
to the item set mining algorithm. In the context of
L1 regularization. Dudik (2007), Gao (2006), and
Tsuda (2007) have also proposed methods by which
effective features are extracted from huge sets of fea-
ture candidates. However, their methods are still
very computationally expensive, and we cannot di-
rectly apply this kind of method to a large-scale NLP
problem.
In the present paper, we propose a novel algorithm
for learning of an L1 regularized LR with combina-
tion features. In our algorithm, we can exclusively
extract effective combination features without enu-
merating all of the candidate features. Our method
relies on a grafting algorithm (Perkins and Theeiler,
2003), which incrementally adds features like boost-
ing, but it can converge to the global optimum.
We use L1 regularization because we can obtain
a sparse parameter vector, for which many of the
parameter values are exactly zero. In other words,
learning with L1 regularization naturally has an in-
trinsic effect of feature selection, which results in an
</bodyText>
<subsubsectionHeader confidence="0.834336">
Proceedings of NAACL HLT 2009: Short Papers, pages 97–100,
</subsubsectionHeader>
<bodyText confidence="0.9743278">
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
efficient and interpretable inference with almost the
same performance as L2 regularization (Gao et al.,
2007).
The heart of our algorithm is a way to find a
feature that has the largest gradient value of likeli-
hood from among the huge set of candidates. To
solve this problem, we propose an example-wise al-
gorithm with filtering. This algorithm is very simple
and easy to implement, but effective in practice.
We applied the proposed methods to NLP tasks,
and found that our methods can achieve the same
high performance as kernel methods, whereas the
number of active combination features is relatively
small, such as several thousands.
</bodyText>
<sectionHeader confidence="0.998274" genericHeader="introduction">
2 Preliminaries
</sectionHeader>
<subsectionHeader confidence="0.9836">
2.1 Logistic Regression Model
</subsectionHeader>
<bodyText confidence="0.964441666666667">
In this paper, we consider a multi-class logistic re-
gression model (LR). For an input x, and an output
label y E 9 , we define a feature vector φ(x, y) E
Rm.
Then in LR, the probability for a label y, given an
input x, is defined as follows:
</bodyText>
<equation confidence="0.9970245">
p(y|x; w) = Z(x, w) exp (wT φ(x, y)) , (1)
1
</equation>
<bodyText confidence="0.999322">
where w E Rm is a weight vector1 correspond-
ing to each input dimension, and Z(x, w) =
Ey exp(wTφ(x, y)) is the partition function.
We estimate the parameter w by a maximum like-
lihood estimation (MLE) with L1 regularization us-
ing training examples {(x1, y1), . . . , (xn, yn)1:
</bodyText>
<equation confidence="0.9957252">
w* = arg min
W − L(w) + C � |wi |(2)
i
L(w) = � log p(yi|xi; w)
i=1...n
</equation>
<bodyText confidence="0.999781333333333">
where C &gt; 0 is the trade-off parameter between the
likelihood term and the regularization term. This es-
timation is a convex optimization problem.
</bodyText>
<subsectionHeader confidence="0.998093">
2.2 Grafting
</subsectionHeader>
<bodyText confidence="0.9999254">
To maximize the effect of L1 regularization, we use
the grafting algorithm (Perkins and Theeiler, 2003);
namely, we begin with the empty feature set, and
incrementally add effective features to the current
problem. Note that although this is similar to the
</bodyText>
<footnote confidence="0.7986285">
1A bias term b is often considered by adding an additional
dimension to O(x, y)
</footnote>
<bodyText confidence="0.9794568">
boosting algorithm for learning, the obtained result
is always optimal. We explain the grafting algorithm
here again for the sake of clarity.
The grafting algorithm is summarized in Algo-
rithm 1.
In this algorithm we retain two variables; w stores
the current weight vector, and H stores the set of
features with a non-zero weight. Initially, we set
w = 0, and H = 11. At each iteration, the fea-
ture is selected that has the largest absolute value of
the gradient of the likelihood. Let vk = ∂L(W)
∂wk be
the gradient value of the likelihood of a feature k.
By following the definition, the value vk can be cal-
culated as follows,
</bodyText>
<equation confidence="0.9685735">
�vk = αi,yφk(xi, y), (3)
i,y
</equation>
<bodyText confidence="0.999297818181818">
where αi,y = I(yi = y) − p(yi|xi; w) and I(a) is 1
if a is true and 0 otherwise.
Then, we add k* = arg maxk |vk |to H and opti-
mize (2) with regard to H only. The solution w that
is obtained is used in the next search. The iteration
is continued until |v*k |&lt; C.
We briefly explain why we can find the optimal
weight by this algorithm. Suppose that we optimize
(2) with all features, and initialize the weights us-
ing the results obtained from the grafting algorithm.
Since all gradients of likelihoods satisfy |vk |&lt; C,
and the regularization term pushes the weight toward
0 by C, any changes of the weight vector cannot in-
crease the objective value in (2). Since (2) is the
convex optimization problem, the local optimum is
always the global optimum, and therefore this is the
global optimum for (2)
The point is that, given an efficient method to esti-
mate v*k without the enumeration of all features, we
can solve the optimization in time proportional to the
active feature, regardless of the number of candidate
features. We will discuss this in the next section.
</bodyText>
<sectionHeader confidence="0.94236" genericHeader="method">
3 Extraction of Combination Features
</sectionHeader>
<bodyText confidence="0.999971181818182">
This section presents an algorithm to compute, for
combination features, the feature v*k that has the
largest absolute value of the gradient.
We propose an element-wise extraction method,
where we make use of the sparseness of the training
data.
In this paper, we assume that the values of the
combination features are less than or equal to the
original ones. This assumption is typical; for exam-
ple, it is made in the case where we use binary values
for original and combination features.
</bodyText>
<page confidence="0.989282">
98
</page>
<figure confidence="0.938274909090909">
Algorithm 1 Grafting
Input: training data (xi, yi) (i = 1, , n) and
parameter C
H = {},w = 0
loop
k
if |vk∗ |&lt; C then break
H=HUk*
Optimize w with regards to H
end loop
Output w and H
</figure>
<bodyText confidence="0.91413725">
First, we sort the examples in the order of their
Ey |αi,y |values. Then, we look at the examples one
by one. Let us assume that r examples have been
examined so far. Let us define
</bodyText>
<equation confidence="0.972790666666667">
αi,y0(xi, y) (4)
�α−i,y0(xi, y) t+ = α+i,y0(xi, y)
i&gt;r,y
</equation>
<bodyText confidence="0.9847028">
where α−i,y = min(αi,y, 0) and α+i,y = max(αi,y, 0).
Then, simple calculus shows that the gradient
value for a combination feature k, vk, for which
the original features are k1 and k2, is bounded be-
low/above thus;
</bodyText>
<equation confidence="0.998332666666667">
tk + t−k &lt; vk &lt; tk + t+ (5)
k
tk + max(t−k1, t−k2) &lt; vk &lt; tk + min(t+k1, t+k2).
</equation>
<bodyText confidence="0.993827904761905">
Intuitively, the upper bound of (5) is the case where
the combination feature fires only for the examples
with αi,y &gt; 0, and the lower bound of (5is the case
where the combination feature fires only for the ex-
amples with αi,y &lt; 0. The second inequality arises
from the fact that the value of a combination feature
is equal to or less than the values of its original fea-
tures. Therefore, we examine (5) and check whether
or not |vk |will be larger than C. If not, we can re-
move the feature safely.
Since the examples are sorted in the order of their
Ey |αi,y|, the bound will become tighter quickly.
Therefore, many combination features are filtered
out in the early steps. In experiments, the weights
for the original features are optimized first, and then
the weights for combination features are optimized.
This significantly reduces the number of candidates
for combination features.
Algorithm 2 Algorithm to return the feature that has
the largest gradient value.
Input: training data (xi, yi) and its αi,y value
</bodyText>
<equation confidence="0.94075425">
(i = 1, ... , n, y = 1, ... , |9 |), and the param-
eter C. Examples are sorted with respect to their
Ey |αi,y |values.
t+ = En
i=1 Ey max(αi,y, 0)0(x, y)
t− = EZ 1 Ey min(αi,y, 0)0(x, y)
t = 0, H = {} // Active Combination Feature
for i = 1 to n and y E 9 do
</equation>
<bodyText confidence="0.6515905">
for all combination features k in xi do
if |vk |&gt; C (Check by using Eq.(5) ) then
</bodyText>
<equation confidence="0.95856675">
vk := vk + αi,y0k(xi, y)
H=HUk
end if
end for
t+ := t+ − max(αi,y, 0)0(xi, y)
t− := t− − min(αi,y, 0)0(xi, y)
end for
Output: arg maxkEH vk
</equation>
<bodyText confidence="0.99932725">
Algorithm 2 presents the details of the overall al-
gorithm for the extraction of effective combination
features. Note that many candidate features will be
removed just before adding.
</bodyText>
<sectionHeader confidence="0.999385" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9999033">
To measure the effectiveness of the proposed
method (called L1-Comb), we conducted experi-
ments on the dependency analysis task, and the doc-
ument classification task. In all experiments, the pa-
rameter C was tuned using the development data set.
In the first experiment, we performed Japanese
dependency analysis. We used the Kyoto Text Cor-
pus (Version 3.0), Jan. 1, 3-8 as the training data,
Jan. 10 as the development data, and Jan. 9 as the
test data so that the result could be compared to those
from previous studies (Sassano, 2004)2. We used the
shift-reduce dependency algorithm (Sassano, 2004).
The number of training events was 11, 3332, each of
which consisted of two word positions as inputs, and
y = {0, 1} as an output indicating the dependency
relation. For the training data, the number of orig-
inal features was 78570, and the number of combi-
nation features of degrees 2 and 3 was 5787361, and
169430335, respectively. Note that we need not see
all of them using our algorithm.
</bodyText>
<footnote confidence="0.9834045">
2The data set is different from that in the CoNLL shared
task. This data set is more difficult.
</footnote>
<equation confidence="0.987119222222222">
∂L(w)
v = (L(w) is the log likelihood term)
∂w
k* = arg max
|vk |(The result of Algorithm 2)
�t =
i&lt;r,y
t− = �
i&gt;r,y
</equation>
<page confidence="0.998326">
99
</page>
<tableCaption confidence="0.839661833333333">
Table 2: Document classification results for the Tech-TC-
300 data set. The column F2 shows the average of F2
scores for each method of classification.
Table 1: The performance of the Japanese dependency
task on the Test set. The active features column shows
the number of nonzero weight features.
</tableCaption>
<table confidence="0.992451571428571">
DEP. TRAIN ACTIVE
ACC. (%) TIME (S) FEAT.
L1-COMB 89.03 605 78002
L1-ORIG 88.50 35 29166
SVM 3-POLY 88.72 35720 (KERNEL)
L2-COMB3 89.52 22197 91477782
AVE. PERCE. 87.23 5 45089
</table>
<bodyText confidence="0.999498081081081">
In all experiments, combination features of de-
grees 2 and 3 (the products of two or three original
features) were used.
We compared our methods using LR with L1
regularization using original features (L1-Original),
SVM with a 3rd-polynomial Kernel, LR with L2
regularization using combination features with up to
3 combinations (L2-Comb3), and an averaged per-
ceptron with original features (Ave. Perceptron).
Table 1 shows the result of the Japanese depen-
dency task. The accuracy result indicates that the
accuracy was improved with automatically extracted
combination features. In the column of active fea-
tures, the number of active features is listed. This
indicates that L1 regularization automatically selects
very few effective features. Note that, in training,
L1-Comb used around 100 MB, while L2-Comb3
used more than 30 GB. The most time consuming
part for L1-Comb was the optimization of the L1-
LR problem.
Examples of extracted combination features in-
clude POS pairs of head and modifiers, such as
Head/Noun-Modifier/Noun, and combinations of
distance features with the POS of head.
For the second experiment, we performed the
document classification task using the Tech-TC-300
data set (Davidov et al., 2004)3. We used the tf-idf
scores as feature values. We did not filter out any
words beforehand. The Tech-TC-300 data set con-
sists of 295 binary classification tasks. We divided
each document set into a training and a test set. The
ratio of the test set to the training set was 1 : 4. The
average number of features for tasks was 25, 389.
Table 2 shows the results for L1-LR with combi-
nation features and SVM with linear kernel4. The
results indicate that the combination features are ef-
fective.
</bodyText>
<footnote confidence="0.995855">
3http://techtc.cs.technion.ac.il/techtc300/techtc300.html
4SVM with polynomial kernel did not achieve significant
improvement
</footnote>
<note confidence="0.57971525">
F2
L1-COMB 0.949
L1-ORIG 0.917
SVM (LINEAR KERNEL) 0.896
</note>
<sectionHeader confidence="0.998283" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999955">
We have presented a method to extract effective
combination features for the L1 regularized logis-
tic regression model. We have shown that a simple
filtering technique is effective for enumerating effec-
tive combination features in the grafting algorithm,
even for large-scale problems. Experimental results
show that a L1 regularized logistic regression model
with combination features can achieve comparable
or better results than those from other methods, and
its result is very compact and easy to interpret. We
plan to extend our method to include more complex
features, and apply it to structured output learning.
</bodyText>
<sectionHeader confidence="0.999458" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999887074074074">
Davidov, D., E. Gabrilovich, and S. Markovitch. 2004.
Parameterized generation of labeled datasets for text
categorization based on a hierarchical directory. In
Proc. of SIGIR.
Dud´ık, Miroslav, Steven J. Phillips, and Robert E.
Schapire. 2007. Maximum entropy density estima-
tion with generalized regularization and an application
to species distribution modeling. JMLR, 8:1217–1260.
Gao, J., H. Suzuki, and B. Yu. 2006. Approximation
lasso methods for language modeling. In Proc. of
ACL/COLING.
Gao, J., G. Andrew, M. Johnson, and K. Toutanova.
2007. A comparative study of parameter estimation
methods for statistical natural language processing. In
Proc. of ACL, pages 824–831.
Kudo, T. and Y. Matsumoto. 2004. A boosting algorithm
for classification of semi-structured text. In Proc. of
EMNLP.
Ng, A. 2004. Feature selection, l1 vs. l2 regularization,
and rotational invariance. In NIPS.
Perkins, S. and J. Theeiler. 2003. Online feature selec-
tion using grafting. ICML.
Saigo, H., T. Uno, and K. Tsuda. 2007. Mining com-
plex genotypic features for predicting HIV-1 drug re-
sistance. Bioinformatics, 23:2455–2462.
Sassano, Manabu. 2004. Linear-time dependency analy-
sis for japanese. In Proc. of COLING.
</reference>
<page confidence="0.990767">
100
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.319842">
<title confidence="0.9259275">Combination Features with of Computer Science, University of</title>
<author confidence="0.4719">Tokyo Bunkyo-ku</author>
<affiliation confidence="0.4076505">of Informatics, University of (National Center for Text</affiliation>
<abstract confidence="0.9993266">When linear classifiers cannot successfully data, we often add feawhich are products of several original features. The searching for effective combifeatures, namely requires domain-specific knowledge and hard work. We present herein an efficient algorithm learning an logistic regression model with combination features. We propose to use the grafting algorithm with efficient computation of gradients. This enables us to find optimal weights efficiently without enumerating all combination features. By usthe result we obtain is very compact and achieves very efficient inference. In experiments with NLP tasks, we show that the proposed method can extract effective combination features, and achieve high performance with very few features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Davidov</author>
<author>E Gabrilovich</author>
<author>S Markovitch</author>
</authors>
<title>Parameterized generation of labeled datasets for text categorization based on a hierarchical directory.</title>
<date>2004</date>
<booktitle>In Proc. of SIGIR.</booktitle>
<contexts>
<context position="13256" citStr="Davidov et al., 2004" startWordPosition="2259" endWordPosition="2262">ive features, the number of active features is listed. This indicates that L1 regularization automatically selects very few effective features. Note that, in training, L1-Comb used around 100 MB, while L2-Comb3 used more than 30 GB. The most time consuming part for L1-Comb was the optimization of the L1- LR problem. Examples of extracted combination features include POS pairs of head and modifiers, such as Head/Noun-Modifier/Noun, and combinations of distance features with the POS of head. For the second experiment, we performed the document classification task using the Tech-TC-300 data set (Davidov et al., 2004)3. We used the tf-idf scores as feature values. We did not filter out any words beforehand. The Tech-TC-300 data set consists of 295 binary classification tasks. We divided each document set into a training and a test set. The ratio of the test set to the training set was 1 : 4. The average number of features for tasks was 25, 389. Table 2 shows the results for L1-LR with combination features and SVM with linear kernel4. The results indicate that the combination features are effective. 3http://techtc.cs.technion.ac.il/techtc300/techtc300.html 4SVM with polynomial kernel did not achieve signifi</context>
</contexts>
<marker>Davidov, Gabrilovich, Markovitch, 2004</marker>
<rawString>Davidov, D., E. Gabrilovich, and S. Markovitch. 2004. Parameterized generation of labeled datasets for text categorization based on a hierarchical directory. In Proc. of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miroslav Dud´ık</author>
<author>Steven J Phillips</author>
<author>Robert E Schapire</author>
</authors>
<title>Maximum entropy density estimation with generalized regularization and an application to species distribution modeling.</title>
<date>2007</date>
<journal>JMLR,</journal>
<pages>8--1217</pages>
<marker>Dud´ık, Phillips, Schapire, 2007</marker>
<rawString>Dud´ık, Miroslav, Steven J. Phillips, and Robert E. Schapire. 2007. Maximum entropy density estimation with generalized regularization and an application to species distribution modeling. JMLR, 8:1217–1260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gao</author>
<author>H Suzuki</author>
<author>B Yu</author>
</authors>
<title>Approximation lasso methods for language modeling.</title>
<date>2006</date>
<booktitle>In Proc. of ACL/COLING.</booktitle>
<marker>Gao, Suzuki, Yu, 2006</marker>
<rawString>Gao, J., H. Suzuki, and B. Yu. 2006. Approximation lasso methods for language modeling. In Proc. of ACL/COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gao</author>
<author>G Andrew</author>
<author>M Johnson</author>
<author>K Toutanova</author>
</authors>
<title>A comparative study of parameter estimation methods for statistical natural language processing.</title>
<date>2007</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>824--831</pages>
<contexts>
<context position="3986" citStr="Gao et al., 2007" startWordPosition="592" endWordPosition="595"> and Theeiler, 2003), which incrementally adds features like boosting, but it can converge to the global optimum. We use L1 regularization because we can obtain a sparse parameter vector, for which many of the parameter values are exactly zero. In other words, learning with L1 regularization naturally has an intrinsic effect of feature selection, which results in an Proceedings of NAACL HLT 2009: Short Papers, pages 97–100, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics efficient and interpretable inference with almost the same performance as L2 regularization (Gao et al., 2007). The heart of our algorithm is a way to find a feature that has the largest gradient value of likelihood from among the huge set of candidates. To solve this problem, we propose an example-wise algorithm with filtering. This algorithm is very simple and easy to implement, but effective in practice. We applied the proposed methods to NLP tasks, and found that our methods can achieve the same high performance as kernel methods, whereas the number of active combination features is relatively small, such as several thousands. 2 Preliminaries 2.1 Logistic Regression Model In this paper, we conside</context>
</contexts>
<marker>Gao, Andrew, Johnson, Toutanova, 2007</marker>
<rawString>Gao, J., G. Andrew, M. Johnson, and K. Toutanova. 2007. A comparative study of parameter estimation methods for statistical natural language processing. In Proc. of ACL, pages 824–831.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>A boosting algorithm for classification of semi-structured text.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<marker>Kudo, Matsumoto, 2004</marker>
<rawString>Kudo, T. and Y. Matsumoto. 2004. A boosting algorithm for classification of semi-structured text. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ng</author>
</authors>
<title>Feature selection, l1 vs. l2 regularization, and rotational invariance.</title>
<date>2004</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="2514" citStr="Ng, 2004" startWordPosition="363" endWordPosition="364">ch pairs of head 97 and modifier words in a dependency analysis task. However, the task of determining effective combination features, namely feature engineering, requires domain-specific knowledge and hard work. Such a non-linear phenomenon can be implicitly captured by using the kernel trick. However, its computational cost is very high, not only during training but also at inference time. Moreover, the model is not interpretable, in that effective features are not represented explicitly. Many kernels methods assume an L2 regularizer, in that many features are equally relevant to the tasks (Ng, 2004). There have been several studies to find efficient ways to obtain (combination) features. In the context of boosting, Kudo (2004) have proposed a method to extract complex features that is similar to the item set mining algorithm. In the context of L1 regularization. Dudik (2007), Gao (2006), and Tsuda (2007) have also proposed methods by which effective features are extracted from huge sets of feature candidates. However, their methods are still very computationally expensive, and we cannot directly apply this kind of method to a large-scale NLP problem. In the present paper, we propose a no</context>
</contexts>
<marker>Ng, 2004</marker>
<rawString>Ng, A. 2004. Feature selection, l1 vs. l2 regularization, and rotational invariance. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Perkins</author>
<author>J Theeiler</author>
</authors>
<title>Online feature selection using grafting.</title>
<date>2003</date>
<publisher>ICML.</publisher>
<contexts>
<context position="3389" citStr="Perkins and Theeiler, 2003" startWordPosition="501" endWordPosition="504">f L1 regularization. Dudik (2007), Gao (2006), and Tsuda (2007) have also proposed methods by which effective features are extracted from huge sets of feature candidates. However, their methods are still very computationally expensive, and we cannot directly apply this kind of method to a large-scale NLP problem. In the present paper, we propose a novel algorithm for learning of an L1 regularized LR with combination features. In our algorithm, we can exclusively extract effective combination features without enumerating all of the candidate features. Our method relies on a grafting algorithm (Perkins and Theeiler, 2003), which incrementally adds features like boosting, but it can converge to the global optimum. We use L1 regularization because we can obtain a sparse parameter vector, for which many of the parameter values are exactly zero. In other words, learning with L1 regularization naturally has an intrinsic effect of feature selection, which results in an Proceedings of NAACL HLT 2009: Short Papers, pages 97–100, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics efficient and interpretable inference with almost the same performance as L2 regularization (Gao et al., 2007). T</context>
<context position="5459" citStr="Perkins and Theeiler, 2003" startWordPosition="863" endWordPosition="866">(x, y)) , (1) 1 where w E Rm is a weight vector1 corresponding to each input dimension, and Z(x, w) = Ey exp(wTφ(x, y)) is the partition function. We estimate the parameter w by a maximum likelihood estimation (MLE) with L1 regularization using training examples {(x1, y1), . . . , (xn, yn)1: w* = arg min W − L(w) + C � |wi |(2) i L(w) = � log p(yi|xi; w) i=1...n where C &gt; 0 is the trade-off parameter between the likelihood term and the regularization term. This estimation is a convex optimization problem. 2.2 Grafting To maximize the effect of L1 regularization, we use the grafting algorithm (Perkins and Theeiler, 2003); namely, we begin with the empty feature set, and incrementally add effective features to the current problem. Note that although this is similar to the 1A bias term b is often considered by adding an additional dimension to O(x, y) boosting algorithm for learning, the obtained result is always optimal. We explain the grafting algorithm here again for the sake of clarity. The grafting algorithm is summarized in Algorithm 1. In this algorithm we retain two variables; w stores the current weight vector, and H stores the set of features with a non-zero weight. Initially, we set w = 0, and H = 11</context>
</contexts>
<marker>Perkins, Theeiler, 2003</marker>
<rawString>Perkins, S. and J. Theeiler. 2003. Online feature selection using grafting. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Saigo</author>
<author>T Uno</author>
<author>K Tsuda</author>
</authors>
<title>Mining complex genotypic features for predicting HIV-1 drug resistance.</title>
<date>2007</date>
<journal>Bioinformatics,</journal>
<pages>23--2455</pages>
<marker>Saigo, Uno, Tsuda, 2007</marker>
<rawString>Saigo, H., T. Uno, and K. Tsuda. 2007. Mining complex genotypic features for predicting HIV-1 drug resistance. Bioinformatics, 23:2455–2462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manabu Sassano</author>
</authors>
<title>Linear-time dependency analysis for japanese.</title>
<date>2004</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="10891" citStr="Sassano, 2004" startWordPosition="1869" endWordPosition="1870">that many candidate features will be removed just before adding. 4 Experiments To measure the effectiveness of the proposed method (called L1-Comb), we conducted experiments on the dependency analysis task, and the document classification task. In all experiments, the parameter C was tuned using the development data set. In the first experiment, we performed Japanese dependency analysis. We used the Kyoto Text Corpus (Version 3.0), Jan. 1, 3-8 as the training data, Jan. 10 as the development data, and Jan. 9 as the test data so that the result could be compared to those from previous studies (Sassano, 2004)2. We used the shift-reduce dependency algorithm (Sassano, 2004). The number of training events was 11, 3332, each of which consisted of two word positions as inputs, and y = {0, 1} as an output indicating the dependency relation. For the training data, the number of original features was 78570, and the number of combination features of degrees 2 and 3 was 5787361, and 169430335, respectively. Note that we need not see all of them using our algorithm. 2The data set is different from that in the CoNLL shared task. This data set is more difficult. ∂L(w) v = (L(w) is the log likelihood term) ∂w k</context>
</contexts>
<marker>Sassano, 2004</marker>
<rawString>Sassano, Manabu. 2004. Linear-time dependency analysis for japanese. In Proc. of COLING.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>