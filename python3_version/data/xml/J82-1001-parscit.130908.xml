<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.9072445">
Phrase Structure Trees Bear More Fruit
than You Would Have Thoughtl
</note>
<author confidence="0.45454">
Aravind K. Joshi
</author>
<affiliation confidence="0.3698916">
Department of Computer and Information Science
The Moore School/D2
University of Pennsylvania
Philadelphia, PA 19104
Leon S. Levy
</affiliation>
<sectionHeader confidence="0.834277" genericHeader="method">
Bell Telephone Laboratories
Whippany, NJ 07981
</sectionHeader>
<bodyText confidence="0.965394709677419">
In this paper we will present several results concerning phrase structure trees. These
results show that phrase structure trees, when viewed in certain ways, have much more
descriptive power than one would have thought. We have given a brief account of local
constraints on structural descriptions and an intuitive proof of a theorem about local
constraints. We have compared the local constraints approach to some aspects of Gazdar&apos;s
framework and that of Peters and Ritchie and of Karttunen. We have also presented some
results on skeletons (phrase structure trees without labels) which show that phrase structure
trees, even when deprived of the labels, retain in a certain sense all the structural informa-
tion. This result has implications for grammatical inference procedures.
1. Introduction pletely precise on this aspect. Berwick has shown that
the Kaplan-Bresnan system is nearly equivalent to the
There is renewed interest in examining the descrip-
tive as well as generative power of phrase structure
grammars. The primary motivation for this interest
has come from the recent investigations in alternatives
to transformational grammars (e.g., Bresnan 1978;
Kaplan and Bresnan 1979; Gazdar 1978, 1979a,
1979b; Peters 1980; Karttunen 1980).2 Some of
these approaches require amendments to phrase struc-
ture grammars (especially Gazdar 1978, 1979a,
1979b; Peters 1980; Karttunen 1980) that increase
their descriptive power without increasing their gener-
ative power. Gazdar wants to restrict the power to
that of context-free languages. Others are not corn-
1 This work was partially supported by NSF grant MCS 79-
08401 and MCS 81-07290.
This paper is a revised and expanded version of a paper pres-
ented at the 18th Annual Meeting of the Association for Computa-
tion Linguistics, University of Pennsylvania, Philadelphia, June
1980.
Thanks are extended to the two referees of this paper and to
Michael McCord for their valuable comments, which helped in the
improvement of both the content and the presentation of the mate-
rial herein.
2 Since this paper was submitted for publication, a number of
papers have appeared that should be of interest to its readers.
&amp;quot;Phrase Linking Grammars&amp;quot; by S. Peters and R.W. Ritchie de-
scribes their system (Technical Report, Department of Linguistics,
University of Texas at Austin, 1982). Strong adequacy of context-
free grammars has been discussed by J. Bresnan, R.M. Kaplan, S.
Peters, and A. Zaenan in &amp;quot;Cross-Serial Dependencies in Dutch&amp;quot; (to
appear in Linguistic Inquiry in 1982). This paper shows that
context-free grammars are not strongly adequate (i.e., they are
unable to provide the appropriate structural descriptions) to charac-
terize cross-serial dependencies in Dutch. In a recent paper (&amp;quot;How
much context-sensitivity is needed to provide reasonable structural
descriptions: A tree adjoining system for generating phrase struc-
ture trees,&amp;quot; presented at the Parsing Workshop, Ohio State Univer-
sity, May 1982; also Technical Report, Department of Computer
and Information Science, University of Pennsylvania, 1982), A.
Joshi discusses weak and strong adequacy of grammars and propos-
es a tree adjoining grammar (TAG) that appears to be strongly
adequate and has only slightly more power than context-free gram-
mars. Joshi has also given a rough characterization of a class of
context-sensitive language (MCSL) that appears to be suitable to
characterize natural languages. Languages of TAG belong to
MCSL, and languages of PLG&apos;s of Peters and Ritchie also belong to
this class. (TAG&apos;s use a linking device similar to that in the
PLG&apos;s.)
Copyright 1982 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted
provided that the copies are not made for direct commercial advantage and the Journal reference and this copyright notice are included on
the first page. To copy otherwise, or to republish, requires a fee and/or specific permission.
</bodyText>
<page confidence="0.417522">
0362-613X/82/010001-11$01.00
</page>
<note confidence="0.950601">
American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 1
Aravind K. Joshi and Leon S. Levy Phrase Structure Trees Bear More Fruit
</note>
<bodyText confidence="0.999141636363637">
so-called indexed grammars. The power of the phrase
linking grammars of Peters and Ritchie is not com-
pletely known at this time.
The notion of node admissibility plays an important
role in these formulations. The earliest reference to
node admissibility appears in Chomsky 1965 (p.
215); he suggests the possibility of constructing a rew-
riting system where the rewriting of a symbol is deter-
mined not only by the symbol being rewritten but also
by the dominating category symbol. In his analysis of
the base component of a transformation grammar,
McCawley 1968 suggested that the appropriate role of
context-sensitive rules in the base component of a
transformational grammar can be viewed as node ad-
missibility conditions on the base trees. The base
component is thus a set of labeled trees satisfying
certain conditions. Peters and Ritchie 1969 made this
notion precise and proved an important result, which
roughly states that the weak generative power of a
context-sensitive grammar is that of a context-free
grammar, if the rules are used as node admissibility
conditions. Later Joshi and Levy 1977 made a sub-
stantial extension of this result and showed that, if the
node admissibility conditions include Boolean combi-
nations of proper analysis predicates and domination
predicates, the weak generative capacity is still that of
a context-free grammar.
Besides the notion of node admissibility, Gazdar
introduces two other notions in his framework
(Generalized Phrase Structure Grammars, GPSG).
These are (1) categories with holes and an associated
set of derived rules and linking rules, and (2) meta-
rules for deriving rules from one another. The cate-
gories with holes and the associated rules do not in-
crease the weak generative power beyond that of
context-free grammars. The metarules, unless con-
strained in some fashion, will increase the generative
power, because, for example, a metarule can generate
an infinite set of context-free rules that can generate a
strictly context-sensitive language. (The language
anbncn/n&gt; 1] can be generated in this way.) The
metarules in the actual grammars written in the GPSG
framework so far are constrained enough so that they
do not increase the generative power.
Besides node admissibility conditions, Peters 1980
introduces a device for &amp;quot;linking&amp;quot; nodes (see also Kart-
tunen 1980). A lower node can be &amp;quot;linked&amp;quot; to a node
higher in the tree and becomes &amp;quot;visible&amp;quot; while the
semantic interpretation is carried out at the lower
node. The idea here is to let the context-free grammar
overgenerate and the semantic interpretation weed out
ill-formed structures. Karttunen 1980 has developed a
parser using this idea.
Kaplan and Bresnan 1979 have proposed an inter-
mediate level of representation called functional struc-
ture. This level serves to filter structures generated by
a phrase structure grammar. Categories with holes are
not used in their framework. In this paper we will not
be concerned with the Kaplan-Bresnan system.
In Section 2 we briefly review Gazdar&apos;s proposal,
especially his notion of categories with holes. We give
a short historical review of this notion.
In Section 3 we briefly describe our work on local
constraints on structural descriptions (Joshi and Levy
1977; Joshi, Levy, and Yueh 1980). We give an intui-
tive explanation of these results.
In Section 4 we propose some extensions of our
results and discuss them in the context of some long
distance rules. We also describe Peters&apos;s 1980 ap-
proach and present some suggestions for &amp;quot;context-
sensitive&amp;quot; compositional semantics.
In Section 5 we briefly present the framework of
Peters and Karttunen and compare it with that of Gaz-
dar and of ourselves.
In Section 6 we briefly discuss our results concern-
ing a characterization of structural descriptions entire-
ly in terms of trees without labels.
</bodyText>
<sectionHeader confidence="0.957579" genericHeader="method">
2. Gazdar&apos;s Formulation
</sectionHeader>
<bodyText confidence="0.999841">
Gazdar 1979 has introduced categories with holes
and some associated rules in order to allow for the
base generation of &amp;quot;unbounded&amp;quot; dependencies. Let
VN be the set of basic nonterminal symbols. Then we
define a set D(VN) of derived nonterminal symbols as
follows.
</bodyText>
<equation confidence="0.801239">
D(VN) = lam I a, E VN
</equation>
<bodyText confidence="0.937359263157895">
For example, if S and N P are the only two nonter-
minal symbols, then D(VN) would consist of S / S,
S / N P, N P/ N P, and N P / S. The intended interpreta-
tion of a derived category (slashed category or a cate-
gory with a hole) is as follows: A node labeled a/P
will dominate subtrees identical to those that can be
dominated by a, except that somewhere in every sub-
tree of the a/g type there will occur a node of the
form g/I3 dominating a resumptive pronoun, a trace, or
the empty string, and every node linking a//3 and 0/0
will be of the form a//3. Thus a/I3 labels a node of
type a that dominates material containing a hole of the
type 13 (i.e., an extraction site in a movement analysis).
For example, S / N P is a sentence that has an NP miss-
ing somewhere. The derived rules allow the propaga-
tion of a hole and the linking rules allow the introduc-
tion of a category with a hole. For example, given the
rule (1)
[ s NP VP] 3 ( 1 )
</bodyText>
<footnote confidence="0.329384">
3 This is the same as the rule
S NP VP
but written as a node admissibility condition.
2 American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982
</footnote>
<note confidence="0.484838">
Aravind K. Joshi and Leon S. Levy Phrase Structure Trees Bear More Fruit
</note>
<equation confidence="0.957786333333333">
we will get two derived rules (2) and (3)
[S/NPNP/NP VP] (2)
[siwNPVP/NP] (3)
</equation>
<bodyText confidence="0.977039333333333">
An example of a linking rule is a rule (rule schema)
that introduces a category with a hole as needed for
topicalization.
</bodyText>
<equation confidence="0.929649">
[s a S/a] (4)
For a = PP this becomes
[s PP S/PP (5)
</equation>
<bodyText confidence="0.999972555555556">
This rule will induce a structure like (6). The tech-
nique of categories with holes and the associated de-
rived and linking rules allows unbounded dependencies
to be accounted for in the phrase structure representa-
tion; however, this is accomplished at the expense of
proliferation of categories of the type a/p (see also
Karttunen 1980). Later, in Section 3, we will present
an alternate way of representing (6) by means of local
constraints and some of their generalizations.
</bodyText>
<sectionHeader confidence="0.633307" genericHeader="method">
PP S/PP
</sectionHeader>
<figure confidence="0.9950892">
NP NP VP/PP (6)
Bill Mari
to V NP PP/PP
I
gave a book
</figure>
<bodyText confidence="0.999732363636364">
The notion of categories with holes is not com-
pletely new. In his &apos;String Analysis of Language
Structure&apos;, Harris 1956, 1962 introduces categories
such as S — NP or 5—NP (like S/ N P of Gazdar) to
account for moved constituents. He does not however
seem to provide, at least not explicitly, machinery for
carrying the &amp;quot;hole&amp;quot; downwards. He also has rules in
his framework for introducing categories with holes.
Thus, in his framework, something like (6) would be
accomplished by allowing for a sentence form (a cen-
ter string) of the form (7) (not entirely his notation).
</bodyText>
<equation confidence="0.586773">
N P V O_N p (7)
= Object or Complement of V
</equation>
<bodyText confidence="0.999965642857143">
Sager, who has constructed a very substantial parser
starting from some of these ideas and extending them
significantly, has allowed for the propagation of the
&apos;hole&apos; resulting in structures very similar to those of
Gazdar. She has also used the notion of categories
with holes in order to carry out some coordinate struc-
ture computation. For example, Sager allows for the
coordination of S /a and S /a but not S and S /a. (See
Sager 1967 for an early reference to her work.)
Gazdar is the first, however, to incorporate the
notion of categories with holes and the associated
rules in a formal framework for his syntactical theory
and also to exploit it in a systematic manner for ex-
plaining coordinate structure phenomena.
</bodyText>
<sectionHeader confidence="0.747819" genericHeader="method">
3. Local Constraints
</sectionHeader>
<bodyText confidence="0.999929133333333">
In this section we briefly review our work on local
constraints. Although this work has already appeared
(Joshi and Levy 1977, Joshi, Levy, and Yueh 1980)
and attracted some attention recently, the demonstra-
tion of our results has remained somewhat inaccessible
to many due to the technicalities of the tree automata
theory. In this paper we present an intuitive account
of these results in terms of interacting finite state ma-
chines.
The method of local constraints is an attempt to
describe context-free languages in an apparently
context-sensitive form that helps to retain the intuitive
insights about the grammatical structure. This form of
description, while apparently context-sensitive, is in
fact context-free.
</bodyText>
<subsectionHeader confidence="0.999987">
3.1 Definition of Local Constraints
</subsectionHeader>
<bodyText confidence="0.83031">
Context-sensitive grammars, in general, are more
powerful (with respect to weak generative capacity)
than context-free grammars. A fascinating result of
Peters and Ritchie 1969 is that if a context-sensitive
grammar G is used for &amp;quot;analysis&amp;quot; then the language
&amp;quot;analyzed&amp;quot; by G is context-free. First, we describe
what we mean by the use of a context-sensitive gram-
mar G for &amp;quot;analysis&amp;quot;. Given a tree t, we define the
set of proper analyses of t. Roughly speaking, a proper
analysis of a tree is a slice across the tree. More pre-
cisely, the following recursive definition applies:
American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 3
Aravind K. Joshi and Leon S. Levy Phrase Structure Trees Bear More Fruit
Definition 3.1. The set of proper analyses of a tree t,
denoted Pt, is defined as follows.
(i) If t = 4) (the empty tree), then
Pt = 0.
(ii) lit =
then Pt = {A} u P(to).P(ti).... .P(tn)
where to, t1, tn are trees, and `.&apos; denotes conca-
tenation (of sets).
</bodyText>
<equation confidence="0.660560666666667">
Example 3.1
A B
t= /\
C d E
Pt = IS, AB, AE, Ae, CdB, CdE, Cde, cdB, cdE,
cdel.
</equation>
<bodyText confidence="0.995533682926829">
Let G be a context-sensitive grammar; i.e., its rules
are of the form
A co/v.
where A EV—E (V is the alphabet and E is the set of
terminal symbols), co E V+ (set of non-null strings on
V) and r, 4, e V* (set of all strings on V). If Tr and 4,
are both null, then the rule is a context-free rule. A
tree t is said to be &amp;quot;analyzable&amp;quot; with respect to G if
for each node of t some rule of G &amp;quot;holds&amp;quot;. It is obvi-
ous how to check whether a context-free rule holds of
a node or not. A context-sensitive rule A -3. co/Tr 0
holds of a node labeled A if the string corresponding
to the immediate descendants of that node is co and
there is a proper analysis of t of the form pi vA0p2
that &amp;quot;passes through&amp;quot; the node, (pi ,p2 E V*). We call
the contextual condition 1T4 a proper analysis
predicate.
Similar to these context-sensitive rules, which allow
us to specify context on the &amp;quot;right&amp;quot; and &amp;quot;left&amp;quot;, we
often need rules to specify context on the &amp;quot;top&amp;quot; or
&amp;quot;bottom&amp;quot;. Given a node labeled A in a tree t, we say
that DOM(7r 0), IT, 4, E V*, holds of a node labeled
A if there is a path from the root of the tree to the
frontier, which passes through the node labeled A, and
is of the form
pi irAci5p2 (pi ,p2€ V*).
The contextual condition associated with such a
&amp;quot;vertical&amp;quot; proper analysi§ is called a domination
predicate.
The general form of a local constraint combines the
proper analysis and domination predicates as follows:
Definition 3.2. A local constraint rule is a rule of the
form
A -a. co/CA
where CA is a Boolean combination of proper analysis
and domination predicates.
In transformational linguistics the context-sensitive
and domination predicates are used to describe condi-
tions on transformations; hence we have referred to
these local constraints elsewhere as local transforma-
tions.
</bodyText>
<subsectionHeader confidence="0.99901">
3.2 Results on Local Constraints
</subsectionHeader>
<bodyText confidence="0.9992094">
Theorem 3.1 (Joshi and Levy 1977) Let G be a
finite set of local constraint rules and r(G) the set of
trees analyzable by G. (It is assumed here that the
trees in T(G) are sentential trees; i.e., the root node of
a tree in T(G) is labeled by the start symbol, S, and
the terminal nodes are labeled by terminal symbols.)
Then the string language L(r(G)) = fx I x is the yield
oft and t E T(G)1 is context-free.
Example 3.2 Let V = {S,T,a,b,c,e} and E = {a,b,c,e},
and G be a finite set of local constraint rules:
</bodyText>
<listItem confidence="0.9962414">
1. S e
2. S aT
3. T aS
4. S bTc / (a ) A DOM (T )
5. T bSc / (a_) A DOM (S_)
</listItem>
<bodyText confidence="0.9820906">
In rules 1, 2, and 3, the context is null, and these rules
are context-free. In rule 4 (and in rule 5), the const-
raint requires an &apos;a&apos; on the left, and the node dominat-
ed (immediately) by a T (and by an S in rule 5).
The language generated by G can be derived by G1:
</bodyText>
<table confidence="0.95817725">
S e S -aT1
S aT T aS
T aS T1 bSc
S1 bTc
</table>
<bodyText confidence="0.9987704">
In G1 there are additional nonterminals S1 and T1 that
enable the context checking of the local constraints
grammar, G, in the generation process.
It is easy to see that, under the homomorphism that
removes subscripts on the nonterminals T1 and S1,
each tree generable in G1 is analyzable in G. Also,
each tree analyzable in G has a homomorphic pre-
image in G1.
The methods used in the proof of the theorem use
tree automata to check the local constraint predicates,
</bodyText>
<figure confidence="0.56855">
• • •
</figure>
<page confidence="0.680379">
4 American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982
</page>
<note confidence="0.792024">
Aravind K. Joshi and Leon S. Levy Phrase Structure Trees Bear More Fruit
</note>
<bodyText confidence="0.999760520833334">
since tree automata used as recognizers accept only
tree sets whose yield languages are context-free.
We now give an informal introduction to the ideas
of (bottom-up) tree automata. Tree automata process
labeled trees, where there is a left-to-right ordering on
the successors of a node in the tree. When all the
successors of a node v have been assigned states, then
a state is assigned to v by a rule that depends on the
label of v and the states of the successors of v consid-
ering their left-to-right ordering. Note that the auto-
maton may immediately assign states to the nodes on
the frontier of the tree since these nodes have no suc-
cessors. If the set of states is partitioned into final
and non-final states, then a tree is accepted by the
automaton if the state assigned to the root is a final
state. A set of trees accepted by a tree automaton is
called a recognizable set. Note that the automaton may
operate non-deterministically, in which case, as usual,
a tree is accepted if there is some set of state assign-
ments leading to its acceptance.
The importance of tree automata is that they are
related to the sets of derivation trees of context-free
grammars. Specifically, if T is the set of derivation
trees of a context-free grammar, G, then there is a tree
automaton that recognizes T. Conversely, if T is the
set of trees recognized by a tree automaton, A, then T
may be systematically relabeled as the set of deriva-
tion trees of a context-free grammar.
The basic idea presented in detail in Joshi and Levy
1977 is that, because tree automata have nice closure
properties (closure under union, intersection, and con-
catenation), they can do the computations required to
check the local constraints.
Another way of looking at the checking of a la-
beled tree by a tree automaton is as follows. We im-
agine a finite state machine sitting at each node of a
tree. The role of the finite state machine is to check
that a correct rule application is made at the node it is
checking. Initially, the nodes on the frontier are
turned on and signal their parent nodes. At any other
node in the tree, the machine at that node is turned on
as soon as all its direct descendants are active. Assum-
ing that at each node the machine for that node has
checked that the rule applied there was one of the
rules of the context-free grammar we are looking for,
then when the root node of the tree signals that it has
correctly checked the root we know that the tree is a
proper tree for the given context-free grammar.
When checking for local constraints, a machine at a
given node not only passes information to its parent,
as described above, but also passes information about
those parts of the local constraints, corresponding to
the given node as well as all its descendants, that have
not yet been satisfied. The point is that this informa-
tion is always bounded and hence a finite number of
states are adequate to code this information.
The fact that the closure properties hold can be
seen as follows. Consider a slightly more general situ-
ation. We consider an A machine and a B machine at
each node. Depending on the connections between
these A and B machines, we obtain additional results.
For example, as each A machine passes information to
its parent, it may also pass information to the B ma-
chine, but the B machine will not pass information
back to the A machine. The tree is accepted if the B
machine at the root node of the tree ends up in a final
state. Although this seems to be a more complicated
model, it can in fact be subsumed in our first model
and is the basis of an informal proof that the recogni-
tion rules are closed under intersection, since the A
machine and the B machine can check different rules.
An important point is that the local constraint on a
rule applied at a given node may only be verified by
the checking automata at some distant ancestor of that
node. In particular, in the case of a proper analysis
constraint, it can only be verified at a node sufficiently
high in the tree to dominate the entire string specified
in the constraint.
The perceptive reader may now be wondering what
replaces all these hypothetical finite state machines
when the set of trees corresponds to a context-free
grammar. Well, if we were to convert our local const-
raints grammar into a standard form context-free
grammar, we would require a larger nonterminal set.
In effect this larger nonterminal set is an encoding of
the finite state information stored at the nodes.
The intuitive explanation presented in this section
is, in fact, a complete characterization of recognizabili-
ty. Given a context-free grammar, one can specify the
finite state machine to be posted at each node of a
tree to check the tree. And conversely, given the fin-
ite state machine description, one can derive the
equivalent context-free grammar
The essence of the local constraints formulation is
to paraphrase the finite state checking at the nodes of
the tree in terms of patterns or predicates.
</bodyText>
<sectionHeader confidence="0.732257" genericHeader="method">
4. Some Generalizations
</sectionHeader>
<bodyText confidence="0.995424090909091">
The result of Theorem 3.1 can be generalized in
various ways. Generalizations in (i) and (ii) below are
immediate.
(i) Variables can be included in the constraint.
Thus, for example, a local constraint rule can be
of the form
A -* w I BCDXE FYG
where A,B,C,D,E,F,G are nonterminals, w is a
string of terminals and/or nonterminals, and X
and Y are variables that range over arbitrary
strings of terminals and nonterminals.
</bodyText>
<note confidence="0.679444">
American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 5
Aravind K. Joshi and Leon S. Levy Phrase Structure Trees Bear More Fruit
</note>
<bodyText confidence="0.876741666666667">
(ii) Finite tree fragments can be included in the
constraint. Thus, for example, a local constraint
rule can be of the form
</bodyText>
<figure confidence="0.861919666666667">
A-6-w BG PP
/\ /\
C D P NP
to
Example 4.1
Let us consider (6) in Section 2 (topicalization).
</figure>
<bodyText confidence="0.776952333333333">
Consider the following fragment of a local const-
raints grammar, G. (Only some of the relevant rules
are shown.)
</bodyText>
<equation confidence="0.59479">
S N P VP
VP V NP PP
S&apos; PP S
V give I N P PP
</equation>
<bodyText confidence="0.968484833333333">
Another useful generalization has the following
essential character.
(iii) Predicates that relate nodes mentioned in the prop-
er analysis predicates and domination predicates
(associated with a rule), as well as nodes in fin-
ite tree fragments dominated by these nodes,
can be included in the constraint. Unfortunate-
ly, at this time we are unable to give a precise
characterization of this generalization. The fol-
lowing two predicates are special cases of this
generalization, and Theorem 3.1 holds for these
two cases.
</bodyText>
<figure confidence="0.9678055">
(a) COMMAND
COM (A B C)
B immediately dominates A and B dominates C,
not necessarily immediately. Usually B is an S
node.
(b) LEFTMOSTSISTER
LMS(A B)
A is the leftmost sister of B
PP E I PPi X V2 NP3
A DOM (S&apos;4 S5 Y VP6 )
A COM (PPi S&apos;4 VP6)
A LMS (PPi S5)
</figure>
<bodyText confidence="0.9877705">
The last rule has a proper analysis predicate, a domi-
nation predicate, and COMMAND and LEFTMOST-
SISTER predicates whose arguments satisfy the re-
quirements mentioned in (iii) above (i.e., they relate
nodes mentioned in proper analysis predicates and
domination predicates). The indexing makes this
clear.
Structure (7) will be well-formed. Compare (7)
with (6) in Section 1 (Gazdar&apos;s framework) and with
(8) in Section 5 (Peters&apos;s and Karttunen&apos;s framework).
</bodyText>
<equation confidence="0.970006666666667">
S14 (7)
PPi s5
/\ /
</equation>
<figure confidence="0.8804732">
V2
gave
NP3 PP
a book £
NP
Bill
to
Mary
NP VP6
/
</figure>
<page confidence="0.938518">
6 American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982
</page>
<note confidence="0.913711">
Aravind K. Joshi and Leon S. Levy Phrase Structure Trees Bear More Fruit
</note>
<subsectionHeader confidence="0.999403">
4.1 Local Constraints in Semantics
</subsectionHeader>
<bodyText confidence="0.983246315789474">
Since a local constraint rule has a context-free part
and contextual constraint part, it is possible to define
context-sensitive compositional semantics in the fol-
lowing manner.
For a context-free rule of the form
A BC
if a(A), a(B), a(C) are the &apos;semantic&apos; translations as-
sociated with A, B, and C, respectively, then a(A) is a
composition of a(B) and a(C).
For a local constraint rule of the form
A BC I P
where A BC is the context-free part and P is the
contextual constraint, we can have a(A) as a compos-
ition of a(B) and a(C), which depends on P. This idea
has been pursued in the context of programming lan-
guages (Joshi, Levy, and Yueh 1980). Whether such
an approach would be useful for natural language is an
open question. (An additional comment appears in
Section 5.)
</bodyText>
<sectionHeader confidence="0.572665" genericHeader="method">
5. Linked Nodes4 (Peters&apos;s and Kartunnen&apos;s frame-
work)
</sectionHeader>
<bodyText confidence="0.996073166666667">
Peters 1980 and Kartunnen 1980 have proposed a
device for linking nodes to handle unbounded depen-
dencies. Thus, for example, instead of (6) or (7), we
have (8).
The dotted line that loops from the VP node back to
the moved constituent is a way of indicating the loca-
tion of the gap in the object position under the VP.
The link also indicates that there is certain dependency
between the gap and the dislocated element. Both in
our approach and that of Peters and Karttunen, prolif-
eration of categories as in Gazdar&apos;s approach is avoid-
ed. Further, for Peters and Karttunen, while carrying
</bodyText>
<footnote confidence="0.57183125">
4 We give a very informal description of a linked tree. A
precise definition can be found in S. Peters and R.W. Ritchie,
Phrase Linking Grammars, Technical Report, Department of Lin-
guistics, University of Texas at Austin, 1982.
</footnote>
<bodyText confidence="0.998436833333333">
out bottom-up semantic translation, the moved constit-
uent is &amp;quot;visible&amp;quot; at the VP node. In our approach, this
&amp;quot;visibility&amp;quot; can be obtained if the translation is made
to depend on the contextual constraint which, of
course, has already been checked prior to the transla-
tion. This is the essence of our suggestion in Section
</bodyText>
<subsectionHeader confidence="0.858112">
4.1.
</subsectionHeader>
<bodyText confidence="0.999977962962963">
Karttunen 1980 has constructed a parser incorpo-
rating the device of linked nodes. Karttunen also dis-
cusses the problem of complex patterns of moved con-
stituents and their associated gaps or resumptive pro-
nouns. This is not easy to handle in Gazdar&apos;s frame-
work without multiplying the categories even further,
e.g., by providing categories such as S/NP NP, etc.5
Karttunen handles this problem by essentially incorpo-
rating the checking of the patterns of gaps and fillers
in the parser, i.e., in the control structure of the par-
ser.
Our approach can be regarded as somewhat inter-
mediate between Gazdar&apos;s and that of Peters and
Karttunen in the following sense. We avoid multipli-
cation of categories as do Peters and Karttunen. On
the other hand, the relationship between the moved
constituent and the gap is expressed in the grammar
itself (more in the spirit of Gazdar) instead of in the
parser (more precisely, in the data structure created by
the parser) as in the Peters and Karttunen approach.
We have not pursued the topic of multiple gaps and
fillers in our framework but, obviously, in it we would
opt for Karttunen&apos;s suggestion of checking the const-
raints on the patterns of gaps and fillers in the parser
itself. It could not be done by local constraints alone
because local constraints essentially do the work of the
links in the Peters and Karttunen framework.
</bodyText>
<sectionHeader confidence="0.710718" genericHeader="method">
6. Skeletal Structural Descriptions (Skeletons)
</sectionHeader>
<bodyText confidence="0.997966352941176">
In Section 4, we showed how local constraints al-
lowed us to prevent proliferation of categories. We
can dispense with the local constraints and construct
an equivalent context-free grammar that would have
potentially a very large number of categories. While
pursuing the relation between &apos;structure&apos; and the size
of the nonterminal vocabulary (i.e., the syntactic cate-
gories), we were led to the following surprising result:
the actual labels, in a sense, carry no information.
(This result was also used by us in developing some
heuristics for converting a context-free grammar into a
more compact but equivalent local constraints gram-
mar. We will not describe this use of our result in the
present paper. (For further information, see Joshi,
Levy, and Yueh 1980.)
First we need some definitions. A phrase structure
tree without labels will be called a skeletal structural
</bodyText>
<figure confidence="0.9570388125">
5 S/NP NP means an S tree with two NP type holes.
S&apos;
.----PP S
I/ VN /N
/ p
/ NP NP VP
1 I I /&apos;S-.,.
/ 1
/
1 to Bill Mary V NP N,
I — I I .
, ,
■
. gave a book \
- -
(8)
</figure>
<note confidence="0.5412995">
American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 7
Aravind K. Joshi and Leon S. Levy Phrase Structure Trees Bear More Fruit
</note>
<bodyText confidence="0.9995942">
description or a skeleton. A skeleton exhibits all of the
grouping structure without naming the syntactic cate-
gories. For example, (9) is a skeleton. The structural
description is characterized only by the shape of the
tree and not the associated labels. The only symbols
appearing in the structure are the terminal symbols
(more precisely, the preterminal symbols and the ter-
minal symbols, in the linguistic context, as in (10);
however, for the rest of the discussion, we will take
skeletons to mean trees with terminal symbols only).
Let G be a context-free grammar and let TG be the
set of sentential derivation trees (structural descrip-
tions) of G. Let SG be the skeletons of G, i.e., all
trees in TG with the labels removed.
It is possible to show that for every context-free
grammar G we can construct a skeletal generating
system (consisting of skeletons and skeletal rewriting
rules) that generates exactly SG; i.e., all category la-
bels can be eliminated while retaining the structural
information (Levy and Joshi 1979).
</bodyText>
<figure confidence="0.98001992">
gave /\ /
a book to Mary
•
•
Bill V •
I /\
•
DET N PN
a book to Mary
gave
8 American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982
Aravind K. Joshi and Leon S. Levy Phrase Structure Trees Bear More Fruit
/SI\
a •b
/I\
a • b
a b
(3)
• • •
• • •
Example 6.1 • /I\
G : S aSb /\ a b
S ab a b /\
SG: ab
(1) (2)
</figure>
<bodyText confidence="0.8435795">
A skeletal generating system can be constructed as
follows. We have a finite set of initial skeletons and a
finite set of skeletal rewriting rules whose left-hand
and right-hand sides are skeletons.
</bodyText>
<figure confidence="0.995446111111111">
Initial Skeletons
•
a b
Skeletal rewriting rules
•
/IN
a b a • b
/
a b
</figure>
<bodyText confidence="0.999711615384616">
In this system, generation proceeds from an initial
skeleton through a sequence of intermediate skeletons
to the desired skeleton. Clearly, because of the defini-
tion of a skeleton and the nature of the skeletal rewrit-
ing rules, the rules must always apply to one of the
lowermost configurations in a skeleton that matches
with the left-hand side of a rule. Thus the derivation
of the skeleton (3) in SG would be as in (11). The
configurations encircled by a dotted line are the ones
to which the skeletal rule is applied.
In the above example, there was only one nonter-
minal; hence the result is obvious. Following is a
somewhat more complicated example.
</bodyText>
<equation confidence="0.9567414">
Example 6.2
G: E
E (E)
E E + E/-.(+_)A
E E * E/-.(+ )
</equation>
<bodyText confidence="0.99700947368421">
G is a local constraints grammar. Clearly there is a
context-free grammar G&apos; that is equivalent to G.
Rather than taking a complicated context-free gram-
mar and then exhibiting the equivalent skeletal gram-
mar, we will take the local constraints grammar G and
exhibit a skeletal grammar equivalent to G. This will
allow us to present a complicated example without
making the resulting skeletal grammar too unwieldy.
Also, this example will give some idea about the rela-
tionship between local constraints grammars and skele-
tal grammars; in particular, the skeletal rewriting rules
indirectly encode the local constraints in the rules in
Example 6.2.
We have eliminated all labels by introducing struc-
tural rewriting rules and defining the derivation as
proceeding from skeleton to skeleton rather than from
string to string. This result clearly brings out the rela-
tionship between the grouping structure and the syn-
tactic categories labeling the nodes.
</bodyText>
<figure confidence="0.998468657894737">
S.
S.
&amp;quot;•••
/ •
I
b
(1
•
\
a _•,b
a- b ‘■
American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 9
Aravind K. Joshi and Leon S. Levy Phrase Structure Trees Bear More Fruit
Skeletal grammar equivalent to G:
Initial Skeletons:
Skeletal Rewriting Rules:
•
./1\. •/I
•
--IN. /1\
• * •
a
•
/1 /1\
• 4. •
/IN
• * •
Ia a
a a
• •
ZIN /1\
--*-
• * • • •
1 I /I\
Q a 1
• * •
! { a
Q Q
</figure>
<page confidence="0.786846">
10 American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982
</page>
<note confidence="0.712721">
Aravind K. Joshi and Leon S. Levy Phrase Structure Trees Bear More Fruit
</note>
<bodyText confidence="0.999926666666667">
Since skeletons pay attention to grouping only, this
result may be psycholinguistically important because
our first intuition about the structure of a sentence is
more likely to be in terms of the grouping structure
and not in terms of the corresponding syntactic cate-
gories, especially those beyond the preterminal cate-
gories.
The theory of skeletons may also provide some
insight into the problem of grammatical inference. For
a finite state string automaton, it is well know that if
the number of states is 2k then, if we are presented
with all acceptable stings of length &lt;2k, the finite
state automaton is completely determined. We have a
similar situation with the skeletons. First, it can be
shown that for each skeletal set SG (i.e., the set of
skeletons of a context-free grammar) we can construct
a bottom-up tree automaton that recognizes precisely
SG (Levy and Joshi 1978). Further, if the number of
states of this automaton is k, then the set of all ac-
ceptable sets of skeletons of depth &lt;2k completely
determines SG (Levy and Joshi 1979). Using skele-
tons (i.e., string with their grouping structure) rather
than just strings as input to a grammatical inference
machine is an idea worth pursuing further.
</bodyText>
<sectionHeader confidence="0.985789" genericHeader="conclusions">
6. Conclusion:
</sectionHeader>
<bodyText confidence="0.999973083333333">
We have presented several results concerning
phrase structure trees that show that phrase structure
trees when viewed in certain ways have much more
descriptive power than one would have thought. We
have given a brief account of our work on local const-
raints and presented an intuitive proof. We have also
compared it to some aspects of the framework of Gaz-
dar and that of Peters and Karttunen. We have also
shown that phrase structure trees, even when deprived
of the labels, retain in a certain sense all the structural
information. This result has implications for grammat-
ical inference procedures.
</bodyText>
<sectionHeader confidence="0.995027" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.966126980769231">
Bresnan, J.W. 1978 A Realistic transformational grammar. In
Halle, M., Bresnan, J. and Miller, G.A., Ed., Linguistic Theory
and Psychological Reality. The MIT Press, Cambridge, Mass.
Chomsky, N. 1965 Aspects of the Theory of Syntax, The MIT Press,
Cambridge, Mass.
Gazdar, G.J.M. 1978 English as a context-free language. unpubl-
ished manuscript.
Gazdar, G.J.M. 1981 Unbounded dependencies and coordinate
structure. Linguistic Inquiry 12, 2.
Gazdar. G.J.M. 1982 Phrase Structure Grammar. To appear in
Jacobson, P. and Pullam, G.K., Ed., The Nature of Syntactic
Representation. Reidel, Boston, Mass.
Harris, Z.S. 1956 String analysing of language structure. manu-
script. Also in manuscripts around 1958 and published in 1962
by Mouton and Co., The Hague.
Joshi, A.K. and Levy, L.S. 1977 Constraints on structural descrip-
tions. SIAM Journal of Computing.
Joshi, A.K., Levy, L.S., and Yueh, K. 1980 Local constraints in
the syntax and semantics of programming language. Journal of
Theoretical Computer Science.
Kaplan, R. and Bresnan, J.W. 1979 A formal system for grammat-
ical representation. To appear in Bresnan, J.W., Ed., The Men-
tal Representation of Grammatical Relations. The MIT Press,
Mass.
Karttunen, L. 1980 Unbounded dependencies in phrase structure
grammar: slash categories versus dotted lines. Paper presented
at the Third Amsterdam Colloquium: Formal Methods in the
Study of Language, Amsterdam.
Levy, L.S. and Joshi, A.K. 1978 Skeletal structural descriptions.
Information and Control.
McCawley, J.D. 1967 Concerning the base component of a trans-
formational grammar. Foundations of Language, Vol. 4. Reprint-
ed in McCawley, J.D., Grammar and Meaning, Academic Press,
New York, NY, 1968.
Peters, S. 1980 (Talk presented at the Workshop on Alternatives
to Transformation Grammars, Stanford University, January.)
Peters, S. and Ritchie, R.W. 1969 Context-sensitive immediate
constituent analysis. Proc. ACM Symposium on Theory of
Computing.
Sager, N. 1967 Syntactic analysis of natural languages. Advances
in Computers, Vol. 8 ed. M. Alt and M. Rubinoff, Academic
Press, New York, NY.
Aravind K. Joshi is a professor of computer and
information science, and that department&apos;s chairman, at
the University of Pennsylvania, Philadelphia. He re-
ceived the Ph.D. degree in electrical engineering from
the University of Pennsylvania in 1960.
Leon S. Levy is a member of the technical staff of
Bell Telephone Laboratories at Whippany, New Jersey.
He received the Ph.D. degree in Computer Science from
the University of Pennsylvania in 1970.
American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 11
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.654959">
<title confidence="0.93171">Phrase Structure Trees Bear More than You Would Have Thoughtl</title>
<author confidence="0.999969">Aravind K Joshi</author>
<affiliation confidence="0.937464333333333">Department of Computer and Information The Moore University of</affiliation>
<address confidence="0.998155">Philadelphia, PA 19104</address>
<author confidence="0.993063">Leon S Levy</author>
<affiliation confidence="0.991717">Bell Telephone</affiliation>
<address confidence="0.976304">Whippany, NJ 07981</address>
<abstract confidence="0.995617888888889">In this paper we will present several results concerning phrase structure trees. These results show that phrase structure trees, when viewed in certain ways, have much more descriptive power than one would have thought. We have given a brief account of local constraints on structural descriptions and an intuitive proof of a theorem about local constraints. We have compared the local constraints approach to some aspects of Gazdar&apos;s framework and that of Peters and Ritchie and of Karttunen. We have also presented some results on skeletons (phrase structure trees without labels) which show that phrase structure trees, even when deprived of the labels, retain in a certain sense all the structural information. This result has implications for grammatical inference procedures.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J W Bresnan</author>
</authors>
<title>A Realistic transformational grammar. In</title>
<date>1978</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context position="1427" citStr="Bresnan 1978" startWordPosition="214" endWordPosition="215">ructure trees without labels) which show that phrase structure trees, even when deprived of the labels, retain in a certain sense all the structural information. This result has implications for grammatical inference procedures. 1. Introduction pletely precise on this aspect. Berwick has shown that the Kaplan-Bresnan system is nearly equivalent to the There is renewed interest in examining the descriptive as well as generative power of phrase structure grammars. The primary motivation for this interest has come from the recent investigations in alternatives to transformational grammars (e.g., Bresnan 1978; Kaplan and Bresnan 1979; Gazdar 1978, 1979a, 1979b; Peters 1980; Karttunen 1980).2 Some of these approaches require amendments to phrase structure grammars (especially Gazdar 1978, 1979a, 1979b; Peters 1980; Karttunen 1980) that increase their descriptive power without increasing their generative power. Gazdar wants to restrict the power to that of context-free languages. Others are not corn1 This work was partially supported by NSF grant MCS 79- 08401 and MCS 81-07290. This paper is a revised and expanded version of a paper presented at the 18th Annual Meeting of the Association for Computa</context>
</contexts>
<marker>Bresnan, 1978</marker>
<rawString>Bresnan, J.W. 1978 A Realistic transformational grammar. In Halle, M., Bresnan, J. and Miller, G.A., Ed., Linguistic Theory and Psychological Reality. The MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<title>Aspects of the Theory of Syntax,</title>
<date>1965</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context position="4680" citStr="Chomsky 1965" startWordPosition="719" endWordPosition="720">ference and this copyright notice are included on the first page. To copy otherwise, or to republish, requires a fee and/or specific permission. 0362-613X/82/010001-11$01.00 American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 1 Aravind K. Joshi and Leon S. Levy Phrase Structure Trees Bear More Fruit so-called indexed grammars. The power of the phrase linking grammars of Peters and Ritchie is not completely known at this time. The notion of node admissibility plays an important role in these formulations. The earliest reference to node admissibility appears in Chomsky 1965 (p. 215); he suggests the possibility of constructing a rewriting system where the rewriting of a symbol is determined not only by the symbol being rewritten but also by the dominating category symbol. In his analysis of the base component of a transformation grammar, McCawley 1968 suggested that the appropriate role of context-sensitive rules in the base component of a transformational grammar can be viewed as node admissibility conditions on the base trees. The base component is thus a set of labeled trees satisfying certain conditions. Peters and Ritchie 1969 made this notion precise and p</context>
</contexts>
<marker>Chomsky, 1965</marker>
<rawString>Chomsky, N. 1965 Aspects of the Theory of Syntax, The MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G J M Gazdar</author>
</authors>
<title>English as a context-free language.</title>
<date>1978</date>
<note>unpublished manuscript.</note>
<contexts>
<context position="1465" citStr="Gazdar 1978" startWordPosition="220" endWordPosition="221">w that phrase structure trees, even when deprived of the labels, retain in a certain sense all the structural information. This result has implications for grammatical inference procedures. 1. Introduction pletely precise on this aspect. Berwick has shown that the Kaplan-Bresnan system is nearly equivalent to the There is renewed interest in examining the descriptive as well as generative power of phrase structure grammars. The primary motivation for this interest has come from the recent investigations in alternatives to transformational grammars (e.g., Bresnan 1978; Kaplan and Bresnan 1979; Gazdar 1978, 1979a, 1979b; Peters 1980; Karttunen 1980).2 Some of these approaches require amendments to phrase structure grammars (especially Gazdar 1978, 1979a, 1979b; Peters 1980; Karttunen 1980) that increase their descriptive power without increasing their generative power. Gazdar wants to restrict the power to that of context-free languages. Others are not corn1 This work was partially supported by NSF grant MCS 79- 08401 and MCS 81-07290. This paper is a revised and expanded version of a paper presented at the 18th Annual Meeting of the Association for Computation Linguistics, University of Pennsy</context>
</contexts>
<marker>Gazdar, 1978</marker>
<rawString>Gazdar, G.J.M. 1978 English as a context-free language. unpublished manuscript.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G J M Gazdar</author>
</authors>
<title>Unbounded dependencies and coordinate structure.</title>
<date>1981</date>
<journal>Linguistic Inquiry</journal>
<volume>12</volume>
<pages>2</pages>
<marker>Gazdar, 1981</marker>
<rawString>Gazdar, G.J.M. 1981 Unbounded dependencies and coordinate structure. Linguistic Inquiry 12, 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G J M</author>
</authors>
<title>Phrase Structure Grammar.</title>
<date>1982</date>
<location>Reidel, Boston, Mass.</location>
<note>To appear in</note>
<marker>M, 1982</marker>
<rawString>Gazdar. G.J.M. 1982 Phrase Structure Grammar. To appear in Jacobson, P. and Pullam, G.K., Ed., The Nature of Syntactic Representation. Reidel, Boston, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z S Harris</author>
</authors>
<title>String analysing of language structure.</title>
<date>1956</date>
<note>manuscript. Also in manuscripts around 1958 and published in</note>
<contexts>
<context position="10618" citStr="Harris 1956" startWordPosition="1739" endWordPosition="1740">he technique of categories with holes and the associated derived and linking rules allows unbounded dependencies to be accounted for in the phrase structure representation; however, this is accomplished at the expense of proliferation of categories of the type a/p (see also Karttunen 1980). Later, in Section 3, we will present an alternate way of representing (6) by means of local constraints and some of their generalizations. PP S/PP NP NP VP/PP (6) Bill Mari to V NP PP/PP I gave a book The notion of categories with holes is not completely new. In his &apos;String Analysis of Language Structure&apos;, Harris 1956, 1962 introduces categories such as S — NP or 5—NP (like S/ N P of Gazdar) to account for moved constituents. He does not however seem to provide, at least not explicitly, machinery for carrying the &amp;quot;hole&amp;quot; downwards. He also has rules in his framework for introducing categories with holes. Thus, in his framework, something like (6) would be accomplished by allowing for a sentence form (a center string) of the form (7) (not entirely his notation). N P V O_N p (7) = Object or Complement of V Sager, who has constructed a very substantial parser starting from some of these ideas and extending the</context>
</contexts>
<marker>Harris, 1956</marker>
<rawString>Harris, Z.S. 1956 String analysing of language structure. manuscript. Also in manuscripts around 1958 and published in 1962 by Mouton and Co., The Hague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
<author>L S Levy</author>
</authors>
<title>Constraints on structural descriptions.</title>
<date>1977</date>
<journal>SIAM Journal of Computing.</journal>
<contexts>
<context position="5506" citStr="Joshi and Levy 1977" startWordPosition="851" endWordPosition="854">is analysis of the base component of a transformation grammar, McCawley 1968 suggested that the appropriate role of context-sensitive rules in the base component of a transformational grammar can be viewed as node admissibility conditions on the base trees. The base component is thus a set of labeled trees satisfying certain conditions. Peters and Ritchie 1969 made this notion precise and proved an important result, which roughly states that the weak generative power of a context-sensitive grammar is that of a context-free grammar, if the rules are used as node admissibility conditions. Later Joshi and Levy 1977 made a substantial extension of this result and showed that, if the node admissibility conditions include Boolean combinations of proper analysis predicates and domination predicates, the weak generative capacity is still that of a context-free grammar. Besides the notion of node admissibility, Gazdar introduces two other notions in his framework (Generalized Phrase Structure Grammars, GPSG). These are (1) categories with holes and an associated set of derived rules and linking rules, and (2) metarules for deriving rules from one another. The categories with holes and the associated rules do </context>
<context position="7635" citStr="Joshi and Levy 1977" startWordPosition="1186" endWordPosition="1189">en 1980 has developed a parser using this idea. Kaplan and Bresnan 1979 have proposed an intermediate level of representation called functional structure. This level serves to filter structures generated by a phrase structure grammar. Categories with holes are not used in their framework. In this paper we will not be concerned with the Kaplan-Bresnan system. In Section 2 we briefly review Gazdar&apos;s proposal, especially his notion of categories with holes. We give a short historical review of this notion. In Section 3 we briefly describe our work on local constraints on structural descriptions (Joshi and Levy 1977; Joshi, Levy, and Yueh 1980). We give an intuitive explanation of these results. In Section 4 we propose some extensions of our results and discuss them in the context of some long distance rules. We also describe Peters&apos;s 1980 approach and present some suggestions for &amp;quot;contextsensitive&amp;quot; compositional semantics. In Section 5 we briefly present the framework of Peters and Karttunen and compare it with that of Gazdar and of ourselves. In Section 6 we briefly discuss our results concerning a characterization of structural descriptions entirely in terms of trees without labels. 2. Gazdar&apos;s Formul</context>
<context position="11984" citStr="Joshi and Levy 1977" startWordPosition="1975" endWordPosition="1978">n of categories with holes in order to carry out some coordinate structure computation. For example, Sager allows for the coordination of S /a and S /a but not S and S /a. (See Sager 1967 for an early reference to her work.) Gazdar is the first, however, to incorporate the notion of categories with holes and the associated rules in a formal framework for his syntactical theory and also to exploit it in a systematic manner for explaining coordinate structure phenomena. 3. Local Constraints In this section we briefly review our work on local constraints. Although this work has already appeared (Joshi and Levy 1977, Joshi, Levy, and Yueh 1980) and attracted some attention recently, the demonstration of our results has remained somewhat inaccessible to many due to the technicalities of the tree automata theory. In this paper we present an intuitive account of these results in terms of interacting finite state machines. The method of local constraints is an attempt to describe context-free languages in an apparently context-sensitive form that helps to retain the intuitive insights about the grammatical structure. This form of description, while apparently context-sensitive, is in fact context-free. 3.1 D</context>
<context position="15541" citStr="Joshi and Levy 1977" startWordPosition="2605" endWordPosition="2608">ed with such a &amp;quot;vertical&amp;quot; proper analysi§ is called a domination predicate. The general form of a local constraint combines the proper analysis and domination predicates as follows: Definition 3.2. A local constraint rule is a rule of the form A -a. co/CA where CA is a Boolean combination of proper analysis and domination predicates. In transformational linguistics the context-sensitive and domination predicates are used to describe conditions on transformations; hence we have referred to these local constraints elsewhere as local transformations. 3.2 Results on Local Constraints Theorem 3.1 (Joshi and Levy 1977) Let G be a finite set of local constraint rules and r(G) the set of trees analyzable by G. (It is assumed here that the trees in T(G) are sentential trees; i.e., the root node of a tree in T(G) is labeled by the start symbol, S, and the terminal nodes are labeled by terminal symbols.) Then the string language L(r(G)) = fx I x is the yield oft and t E T(G)1 is context-free. Example 3.2 Let V = {S,T,a,b,c,e} and E = {a,b,c,e}, and G be a finite set of local constraint rules: 1. S e 2. S aT 3. T aS 4. S bTc / (a ) A DOM (T ) 5. T bSc / (a_) A DOM (S_) In rules 1, 2, and 3, the context is null, a</context>
<context position="18563" citStr="Joshi and Levy 1977" startWordPosition="3181" endWordPosition="3184">may operate non-deterministically, in which case, as usual, a tree is accepted if there is some set of state assignments leading to its acceptance. The importance of tree automata is that they are related to the sets of derivation trees of context-free grammars. Specifically, if T is the set of derivation trees of a context-free grammar, G, then there is a tree automaton that recognizes T. Conversely, if T is the set of trees recognized by a tree automaton, A, then T may be systematically relabeled as the set of derivation trees of a context-free grammar. The basic idea presented in detail in Joshi and Levy 1977 is that, because tree automata have nice closure properties (closure under union, intersection, and concatenation), they can do the computations required to check the local constraints. Another way of looking at the checking of a labeled tree by a tree automaton is as follows. We imagine a finite state machine sitting at each node of a tree. The role of the finite state machine is to check that a correct rule application is made at the node it is checking. Initially, the nodes on the frontier are turned on and signal their parent nodes. At any other node in the tree, the machine at that node </context>
</contexts>
<marker>Joshi, Levy, 1977</marker>
<rawString>Joshi, A.K. and Levy, L.S. 1977 Constraints on structural descriptions. SIAM Journal of Computing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
<author>L S Levy</author>
<author>K Yueh</author>
</authors>
<title>Local constraints in the syntax and semantics of programming language.</title>
<date>1980</date>
<journal>Journal of Theoretical Computer Science.</journal>
<marker>Joshi, Levy, Yueh, 1980</marker>
<rawString>Joshi, A.K., Levy, L.S., and Yueh, K. 1980 Local constraints in the syntax and semantics of programming language. Journal of Theoretical Computer Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kaplan</author>
<author>J W Bresnan</author>
</authors>
<title>A formal system for grammatical representation.</title>
<date>1979</date>
<publisher>The MIT Press,</publisher>
<location>Mass.</location>
<note>To appear in</note>
<contexts>
<context position="1452" citStr="Kaplan and Bresnan 1979" startWordPosition="216" endWordPosition="219">without labels) which show that phrase structure trees, even when deprived of the labels, retain in a certain sense all the structural information. This result has implications for grammatical inference procedures. 1. Introduction pletely precise on this aspect. Berwick has shown that the Kaplan-Bresnan system is nearly equivalent to the There is renewed interest in examining the descriptive as well as generative power of phrase structure grammars. The primary motivation for this interest has come from the recent investigations in alternatives to transformational grammars (e.g., Bresnan 1978; Kaplan and Bresnan 1979; Gazdar 1978, 1979a, 1979b; Peters 1980; Karttunen 1980).2 Some of these approaches require amendments to phrase structure grammars (especially Gazdar 1978, 1979a, 1979b; Peters 1980; Karttunen 1980) that increase their descriptive power without increasing their generative power. Gazdar wants to restrict the power to that of context-free languages. Others are not corn1 This work was partially supported by NSF grant MCS 79- 08401 and MCS 81-07290. This paper is a revised and expanded version of a paper presented at the 18th Annual Meeting of the Association for Computation Linguistics, Univers</context>
<context position="7087" citStr="Kaplan and Bresnan 1979" startWordPosition="1099" endWordPosition="1102">ay.) The metarules in the actual grammars written in the GPSG framework so far are constrained enough so that they do not increase the generative power. Besides node admissibility conditions, Peters 1980 introduces a device for &amp;quot;linking&amp;quot; nodes (see also Karttunen 1980). A lower node can be &amp;quot;linked&amp;quot; to a node higher in the tree and becomes &amp;quot;visible&amp;quot; while the semantic interpretation is carried out at the lower node. The idea here is to let the context-free grammar overgenerate and the semantic interpretation weed out ill-formed structures. Karttunen 1980 has developed a parser using this idea. Kaplan and Bresnan 1979 have proposed an intermediate level of representation called functional structure. This level serves to filter structures generated by a phrase structure grammar. Categories with holes are not used in their framework. In this paper we will not be concerned with the Kaplan-Bresnan system. In Section 2 we briefly review Gazdar&apos;s proposal, especially his notion of categories with holes. We give a short historical review of this notion. In Section 3 we briefly describe our work on local constraints on structural descriptions (Joshi and Levy 1977; Joshi, Levy, and Yueh 1980). We give an intuitive </context>
</contexts>
<marker>Kaplan, Bresnan, 1979</marker>
<rawString>Kaplan, R. and Bresnan, J.W. 1979 A formal system for grammatical representation. To appear in Bresnan, J.W., Ed., The Mental Representation of Grammatical Relations. The MIT Press, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Karttunen</author>
</authors>
<title>Unbounded dependencies in phrase structure grammar: slash categories versus dotted lines. Paper presented at the Third Amsterdam Colloquium: Formal Methods in the Study of Language,</title>
<date>1980</date>
<location>Amsterdam.</location>
<contexts>
<context position="1509" citStr="Karttunen 1980" startWordPosition="226" endWordPosition="227"> deprived of the labels, retain in a certain sense all the structural information. This result has implications for grammatical inference procedures. 1. Introduction pletely precise on this aspect. Berwick has shown that the Kaplan-Bresnan system is nearly equivalent to the There is renewed interest in examining the descriptive as well as generative power of phrase structure grammars. The primary motivation for this interest has come from the recent investigations in alternatives to transformational grammars (e.g., Bresnan 1978; Kaplan and Bresnan 1979; Gazdar 1978, 1979a, 1979b; Peters 1980; Karttunen 1980).2 Some of these approaches require amendments to phrase structure grammars (especially Gazdar 1978, 1979a, 1979b; Peters 1980; Karttunen 1980) that increase their descriptive power without increasing their generative power. Gazdar wants to restrict the power to that of context-free languages. Others are not corn1 This work was partially supported by NSF grant MCS 79- 08401 and MCS 81-07290. This paper is a revised and expanded version of a paper presented at the 18th Annual Meeting of the Association for Computation Linguistics, University of Pennsylvania, Philadelphia, June 1980. Thanks are </context>
<context position="6733" citStr="Karttunen 1980" startWordPosition="1042" endWordPosition="1044"> the weak generative power beyond that of context-free grammars. The metarules, unless constrained in some fashion, will increase the generative power, because, for example, a metarule can generate an infinite set of context-free rules that can generate a strictly context-sensitive language. (The language anbncn/n&gt; 1] can be generated in this way.) The metarules in the actual grammars written in the GPSG framework so far are constrained enough so that they do not increase the generative power. Besides node admissibility conditions, Peters 1980 introduces a device for &amp;quot;linking&amp;quot; nodes (see also Karttunen 1980). A lower node can be &amp;quot;linked&amp;quot; to a node higher in the tree and becomes &amp;quot;visible&amp;quot; while the semantic interpretation is carried out at the lower node. The idea here is to let the context-free grammar overgenerate and the semantic interpretation weed out ill-formed structures. Karttunen 1980 has developed a parser using this idea. Kaplan and Bresnan 1979 have proposed an intermediate level of representation called functional structure. This level serves to filter structures generated by a phrase structure grammar. Categories with holes are not used in their framework. In this paper we will not b</context>
<context position="10297" citStr="Karttunen 1980" startWordPosition="1680" endWordPosition="1681">ture Trees Bear More Fruit we will get two derived rules (2) and (3) [S/NPNP/NP VP] (2) [siwNPVP/NP] (3) An example of a linking rule is a rule (rule schema) that introduces a category with a hole as needed for topicalization. [s a S/a] (4) For a = PP this becomes [s PP S/PP (5) This rule will induce a structure like (6). The technique of categories with holes and the associated derived and linking rules allows unbounded dependencies to be accounted for in the phrase structure representation; however, this is accomplished at the expense of proliferation of categories of the type a/p (see also Karttunen 1980). Later, in Section 3, we will present an alternate way of representing (6) by means of local constraints and some of their generalizations. PP S/PP NP NP VP/PP (6) Bill Mari to V NP PP/PP I gave a book The notion of categories with holes is not completely new. In his &apos;String Analysis of Language Structure&apos;, Harris 1956, 1962 introduces categories such as S — NP or 5—NP (like S/ N P of Gazdar) to account for moved constituents. He does not however seem to provide, at least not explicitly, machinery for carrying the &amp;quot;hole&amp;quot; downwards. He also has rules in his framework for introducing categories</context>
<context position="26490" citStr="Karttunen 1980" startWordPosition="4578" endWordPosition="4579">er, for Peters and Karttunen, while carrying 4 We give a very informal description of a linked tree. A precise definition can be found in S. Peters and R.W. Ritchie, Phrase Linking Grammars, Technical Report, Department of Linguistics, University of Texas at Austin, 1982. out bottom-up semantic translation, the moved constituent is &amp;quot;visible&amp;quot; at the VP node. In our approach, this &amp;quot;visibility&amp;quot; can be obtained if the translation is made to depend on the contextual constraint which, of course, has already been checked prior to the translation. This is the essence of our suggestion in Section 4.1. Karttunen 1980 has constructed a parser incorporating the device of linked nodes. Karttunen also discusses the problem of complex patterns of moved constituents and their associated gaps or resumptive pronouns. This is not easy to handle in Gazdar&apos;s framework without multiplying the categories even further, e.g., by providing categories such as S/NP NP, etc.5 Karttunen handles this problem by essentially incorporating the checking of the patterns of gaps and fillers in the parser, i.e., in the control structure of the parser. Our approach can be regarded as somewhat intermediate between Gazdar&apos;s and that of</context>
</contexts>
<marker>Karttunen, 1980</marker>
<rawString>Karttunen, L. 1980 Unbounded dependencies in phrase structure grammar: slash categories versus dotted lines. Paper presented at the Third Amsterdam Colloquium: Formal Methods in the Study of Language, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L S Levy</author>
<author>A K Joshi</author>
</authors>
<title>Skeletal structural descriptions. Information and Control.</title>
<date>1978</date>
<contexts>
<context position="33848" citStr="Levy and Joshi 1978" startWordPosition="5880" endWordPosition="5883">categories, especially those beyond the preterminal categories. The theory of skeletons may also provide some insight into the problem of grammatical inference. For a finite state string automaton, it is well know that if the number of states is 2k then, if we are presented with all acceptable stings of length &lt;2k, the finite state automaton is completely determined. We have a similar situation with the skeletons. First, it can be shown that for each skeletal set SG (i.e., the set of skeletons of a context-free grammar) we can construct a bottom-up tree automaton that recognizes precisely SG (Levy and Joshi 1978). Further, if the number of states of this automaton is k, then the set of all acceptable sets of skeletons of depth &lt;2k completely determines SG (Levy and Joshi 1979). Using skeletons (i.e., string with their grouping structure) rather than just strings as input to a grammatical inference machine is an idea worth pursuing further. 6. Conclusion: We have presented several results concerning phrase structure trees that show that phrase structure trees when viewed in certain ways have much more descriptive power than one would have thought. We have given a brief account of our work on local cons</context>
</contexts>
<marker>Levy, Joshi, 1978</marker>
<rawString>Levy, L.S. and Joshi, A.K. 1978 Skeletal structural descriptions. Information and Control.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D McCawley</author>
</authors>
<title>Concerning the base component of a transformational grammar.</title>
<date>1967</date>
<journal>Foundations of Language,</journal>
<volume>4</volume>
<publisher>Academic Press,</publisher>
<location>New York, NY,</location>
<note>Reprinted in McCawley, J.D., Grammar and Meaning,</note>
<marker>McCawley, 1967</marker>
<rawString>McCawley, J.D. 1967 Concerning the base component of a transformational grammar. Foundations of Language, Vol. 4. Reprinted in McCawley, J.D., Grammar and Meaning, Academic Press, New York, NY, 1968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Peters</author>
</authors>
<title>Talk presented at the Workshop on Alternatives to Transformation Grammars,</title>
<date>1980</date>
<institution>Stanford University,</institution>
<contexts>
<context position="1492" citStr="Peters 1980" startWordPosition="224" endWordPosition="225">es, even when deprived of the labels, retain in a certain sense all the structural information. This result has implications for grammatical inference procedures. 1. Introduction pletely precise on this aspect. Berwick has shown that the Kaplan-Bresnan system is nearly equivalent to the There is renewed interest in examining the descriptive as well as generative power of phrase structure grammars. The primary motivation for this interest has come from the recent investigations in alternatives to transformational grammars (e.g., Bresnan 1978; Kaplan and Bresnan 1979; Gazdar 1978, 1979a, 1979b; Peters 1980; Karttunen 1980).2 Some of these approaches require amendments to phrase structure grammars (especially Gazdar 1978, 1979a, 1979b; Peters 1980; Karttunen 1980) that increase their descriptive power without increasing their generative power. Gazdar wants to restrict the power to that of context-free languages. Others are not corn1 This work was partially supported by NSF grant MCS 79- 08401 and MCS 81-07290. This paper is a revised and expanded version of a paper presented at the 18th Annual Meeting of the Association for Computation Linguistics, University of Pennsylvania, Philadelphia, June </context>
<context position="6667" citStr="Peters 1980" startWordPosition="1032" endWordPosition="1033">categories with holes and the associated rules do not increase the weak generative power beyond that of context-free grammars. The metarules, unless constrained in some fashion, will increase the generative power, because, for example, a metarule can generate an infinite set of context-free rules that can generate a strictly context-sensitive language. (The language anbncn/n&gt; 1] can be generated in this way.) The metarules in the actual grammars written in the GPSG framework so far are constrained enough so that they do not increase the generative power. Besides node admissibility conditions, Peters 1980 introduces a device for &amp;quot;linking&amp;quot; nodes (see also Karttunen 1980). A lower node can be &amp;quot;linked&amp;quot; to a node higher in the tree and becomes &amp;quot;visible&amp;quot; while the semantic interpretation is carried out at the lower node. The idea here is to let the context-free grammar overgenerate and the semantic interpretation weed out ill-formed structures. Karttunen 1980 has developed a parser using this idea. Kaplan and Bresnan 1979 have proposed an intermediate level of representation called functional structure. This level serves to filter structures generated by a phrase structure grammar. Categories with </context>
<context position="25343" citStr="Peters 1980" startWordPosition="4381" endWordPosition="4382">, a(C) are the &apos;semantic&apos; translations associated with A, B, and C, respectively, then a(A) is a composition of a(B) and a(C). For a local constraint rule of the form A BC I P where A BC is the context-free part and P is the contextual constraint, we can have a(A) as a composition of a(B) and a(C), which depends on P. This idea has been pursued in the context of programming languages (Joshi, Levy, and Yueh 1980). Whether such an approach would be useful for natural language is an open question. (An additional comment appears in Section 5.) 5. Linked Nodes4 (Peters&apos;s and Kartunnen&apos;s framework) Peters 1980 and Kartunnen 1980 have proposed a device for linking nodes to handle unbounded dependencies. Thus, for example, instead of (6) or (7), we have (8). The dotted line that loops from the VP node back to the moved constituent is a way of indicating the location of the gap in the object position under the VP. The link also indicates that there is certain dependency between the gap and the dislocated element. Both in our approach and that of Peters and Karttunen, proliferation of categories as in Gazdar&apos;s approach is avoided. Further, for Peters and Karttunen, while carrying 4 We give a very infor</context>
</contexts>
<marker>Peters, 1980</marker>
<rawString>Peters, S. 1980 (Talk presented at the Workshop on Alternatives to Transformation Grammars, Stanford University, January.)</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Peters</author>
<author>R W Ritchie</author>
</authors>
<title>Context-sensitive immediate constituent analysis.</title>
<date>1969</date>
<booktitle>Proc. ACM Symposium on Theory of Computing.</booktitle>
<contexts>
<context position="5249" citStr="Peters and Ritchie 1969" startWordPosition="810" endWordPosition="813">eference to node admissibility appears in Chomsky 1965 (p. 215); he suggests the possibility of constructing a rewriting system where the rewriting of a symbol is determined not only by the symbol being rewritten but also by the dominating category symbol. In his analysis of the base component of a transformation grammar, McCawley 1968 suggested that the appropriate role of context-sensitive rules in the base component of a transformational grammar can be viewed as node admissibility conditions on the base trees. The base component is thus a set of labeled trees satisfying certain conditions. Peters and Ritchie 1969 made this notion precise and proved an important result, which roughly states that the weak generative power of a context-sensitive grammar is that of a context-free grammar, if the rules are used as node admissibility conditions. Later Joshi and Levy 1977 made a substantial extension of this result and showed that, if the node admissibility conditions include Boolean combinations of proper analysis predicates and domination predicates, the weak generative capacity is still that of a context-free grammar. Besides the notion of node admissibility, Gazdar introduces two other notions in his fra</context>
<context position="12791" citStr="Peters and Ritchie 1969" startWordPosition="2093" endWordPosition="2096">utomata theory. In this paper we present an intuitive account of these results in terms of interacting finite state machines. The method of local constraints is an attempt to describe context-free languages in an apparently context-sensitive form that helps to retain the intuitive insights about the grammatical structure. This form of description, while apparently context-sensitive, is in fact context-free. 3.1 Definition of Local Constraints Context-sensitive grammars, in general, are more powerful (with respect to weak generative capacity) than context-free grammars. A fascinating result of Peters and Ritchie 1969 is that if a context-sensitive grammar G is used for &amp;quot;analysis&amp;quot; then the language &amp;quot;analyzed&amp;quot; by G is context-free. First, we describe what we mean by the use of a context-sensitive grammar G for &amp;quot;analysis&amp;quot;. Given a tree t, we define the set of proper analyses of t. Roughly speaking, a proper analysis of a tree is a slice across the tree. More precisely, the following recursive definition applies: American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 3 Aravind K. Joshi and Leon S. Levy Phrase Structure Trees Bear More Fruit Definition 3.1. The set of proper anal</context>
</contexts>
<marker>Peters, Ritchie, 1969</marker>
<rawString>Peters, S. and Ritchie, R.W. 1969 Context-sensitive immediate constituent analysis. Proc. ACM Symposium on Theory of Computing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Sager</author>
</authors>
<title>Syntactic analysis of natural languages.</title>
<date>1967</date>
<journal>Advances in Computers,</journal>
<volume>8</volume>
<editor>ed. M. Alt and M. Rubinoff,</editor>
<publisher>Academic Press,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="11552" citStr="Sager 1967" startWordPosition="1906" endWordPosition="1907">g like (6) would be accomplished by allowing for a sentence form (a center string) of the form (7) (not entirely his notation). N P V O_N p (7) = Object or Complement of V Sager, who has constructed a very substantial parser starting from some of these ideas and extending them significantly, has allowed for the propagation of the &apos;hole&apos; resulting in structures very similar to those of Gazdar. She has also used the notion of categories with holes in order to carry out some coordinate structure computation. For example, Sager allows for the coordination of S /a and S /a but not S and S /a. (See Sager 1967 for an early reference to her work.) Gazdar is the first, however, to incorporate the notion of categories with holes and the associated rules in a formal framework for his syntactical theory and also to exploit it in a systematic manner for explaining coordinate structure phenomena. 3. Local Constraints In this section we briefly review our work on local constraints. Although this work has already appeared (Joshi and Levy 1977, Joshi, Levy, and Yueh 1980) and attracted some attention recently, the demonstration of our results has remained somewhat inaccessible to many due to the technicaliti</context>
</contexts>
<marker>Sager, 1967</marker>
<rawString>Sager, N. 1967 Syntactic analysis of natural languages. Advances in Computers, Vol. 8 ed. M. Alt and M. Rubinoff, Academic Press, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Aravind</author>
</authors>
<title>Joshi is a professor of computer and information science, and that department&apos;s chairman, at the</title>
<date>1960</date>
<institution>University of Pennsylvania, Philadelphia. He</institution>
<marker>Aravind, 1960</marker>
<rawString>Aravind K. Joshi is a professor of computer and information science, and that department&apos;s chairman, at the University of Pennsylvania, Philadelphia. He received the Ph.D. degree in electrical engineering from the University of Pennsylvania in 1960.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Leon</author>
</authors>
<title>Levy is a member of the technical staff of Bell Telephone Laboratories at Whippany,</title>
<date>1970</date>
<booktitle>He received the Ph.D. degree in Computer Science from the University of Pennsylvania in</booktitle>
<location>New Jersey.</location>
<marker>Leon, 1970</marker>
<rawString>Leon S. Levy is a member of the technical staff of Bell Telephone Laboratories at Whippany, New Jersey. He received the Ph.D. degree in Computer Science from the University of Pennsylvania in 1970.</rawString>
</citation>
<citation valid="true">
<date>1982</date>
<journal>American Journal of Computational Linguistics,</journal>
<volume>8</volume>
<pages>11</pages>
<marker>1982</marker>
<rawString>American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 11</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>