<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9951335">
The Haves and the Have-Nots: Leveraging Unlabelled Corpora for
Sentiment Analysis
</title>
<author confidence="0.987181">
Kashyap Popat2 Balamurali A R1,2,3 Pushpak Bhattacharyya2 Gholamreza Haffari3
</author>
<affiliation confidence="0.9947945">
1IITB-Monash Research Academy, IIT Bombay 3Monash University
2Dept. of Computer Science and Engineering, IIT Bombay Australia
</affiliation>
<email confidence="0.998868">
{kashyap,balamurali,pb}@cse.iitb.ac.in reza@monash.edu
</email>
<sectionHeader confidence="0.993886" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999686782608696">
Expensive feature engineering based on
WordNet senses has been shown to
be useful for document level sentiment
classification. A plausible reason for
such a performance improvement is the
reduction in data sparsity. However,
such a reduction could be achieved with
a lesser effort through the means of
syntagma based word clustering. In
this paper, the problem of data sparsity
in sentiment analysis, both monolingual
and cross-lingual, is addressed through
the means of clustering. Experiments
show that cluster based data sparsity
reduction leads to performance better than
sense based classification for sentiment
analysis at document level. Similar idea
is applied to Cross Lingual Sentiment
Analysis (CLSA), and it is shown that
reduction in data sparsity (after translation
or bilingual-mapping) produces accuracy
higher than Machine Translation based
CLSA and sense based CLSA.
</bodyText>
<sectionHeader confidence="0.999125" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99974592">
Data sparsity is the bane of Natural Language
Processing (NLP) (Xue et al., 2005; Minkov et al.,
2007). Language units encountered in the test data
but absent in the training data severely degrade the
performance of an NLP task. NLP applications
innovatively handle data sparsity through various
means. A special, but very common kind of
data sparsity viz., word sparsity, can be addressed
in one of the two obvious ways: 1) sparsity
reduction through paradigmatically related words
or 2) sparsity reduction through syntagmatically
related words.
Paradigmatic analysis of text is the analysis
of concepts embedded in the text (Cruse, 1986;
Chandler, 2012). WordNet is a byproduct of such
an analysis. In WordNet, paradigms are manually
generated based on the principles of lexical and
semantic relationship among words (Fellbaum,
1998). WordNets are primarily used to address the
problem of word sense disambiguation. However,
at present there are many NLP applications
which use WordNet. One such application is
Sentiment Analysis (SA) (Pang and Lee, 2002).
Recent research has shown that word sense based
semantic features can improve the performance of
SA systems (Rentoumi et al., 2009; Tamara et al.,
2010; Balamurali et al., 2011) compared to word
based features.
Syntagmatic analysis of text concentrates on
the surface properties of the text. Compared
to paradigmatic property extraction, syntagmatic
processing is relatively light weight. One of
the obvious syntagmas is words, and words are
grouped into equivalence classes or clusters, thus
reducing the model parameters of a statistical NLP
system (Brown et al., 1992). When used as
an additional feature with word based language
models, it has been shown to improve the system
performance viz., machine translation (Uszkoreit
and Brants, 2008; Stymne, 2012), speech
recognition (Martin et al., 1995; Samuelsson and
Reichl, 1999), dependency parsing (Koo et al.,
2008; Haffari et al., 2011; Zhang and Nivre, 2011;
Tratz and Hovy, 2011) and NER (Miller et al.,
2004; Faruqui and Pad´o, 2010; Turian et al., 2010;
T¨ackstr¨om et al., 2012).
In this paper, the focus is on alleviating the
data sparsity faced by supervised approaches
for SA through the means of cluster based
features. As WordNets are essentially word
</bodyText>
<page confidence="0.970312">
412
</page>
<note confidence="0.9142965">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 412–422,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.997150568627451">
clusters wherein words with the same meaning
are clubbed together, they address the problem of
data sparsity at word level. The abstraction and
dimensionality reduction thus achieved attributes
to the superior performance for SA systems that
employs WordNet senses as features. However,
WordNets are manually created. Automatic
creation of the same is challenging and not much
successful because of the linguistic complexity
involved. In case of SA, manually creating the
features based on WordNet senses is a tedious and
an expensive process. Moreover, WordNets are
not present for many languages. All these factors
make the paradigmatic property based cluster
features like WordNet senses a less promising
pursuit for SA.
The syntagmatic analysis essentially makes use
of distributional similarity and may in many
circumstances subsume the paradigmatic analysis.
In the current work, this particular insight is
used to solve the data sparsity problem in
the sentiment analysis by leveraging unlabelled
monolingual corpora. Specifically, experiments
are performed to investigate whether features
developed from manually crafted clusterings
(coming from WordNet) can be replaced by those
generated from clustering based on syntagmatic
properties.
Further, cluster based features are used to
address the problem of scarcity of sentiment
annotated data in a language. Popular
approaches for Cross-Lingual Sentiment Analysis
(CLSA) (Wan, 2009; Duh et al., 2011) depend
on Machine Translation (MT) for converting
the labeled data from one language to the
other (Hiroshi et al., 2004; Banea et al., 2008;
Wan, 2009). However, many languages which
are truly resource scarce, do not have an MT
system or existing MT systems are not ripe to
be used for CLSA (Balamurali et al., 2013). To
perform CLSA, this study leverages unlabelled
parallel corpus to generate the word alignments.
These word alignments are then used to link
cluster based features to obliterate the language
gap for performing SA. No MT systems or
bilingual dictionaries are used for this study.
Instead, language gap for performing CLSA is
bridged using linked cluster or cross-lingual
clusters (explained in section 4) with the
help of unlabelled monolingual corpora. The
contributions of this paper are two fold:
</bodyText>
<listItem confidence="0.996751">
1. Features created from manually built and
finer clusters can be replaced by inexpensive
cluster based features generated solely from
unlabelled corpora. Experiments performed
on four publicly available datasets in three
languages viz., English, Hindi and Marathi1
suggest that cluster based features can
considerably boost the performance of an SA
system. Moreover, state of the art result
is obtained for one of the publicly available
dataset.
2. An alternative and effective approach for
CLSA is demonstrated using clusters as
</listItem>
<bodyText confidence="0.8728811875">
features. Word clustering is a powerful
mechanism to “transfer” a sentiment
classifier from one language to another. Thus
can be used in truly resource scarce scenarios
like that of English-Marathi CLSA.
The rest of the paper is organized as follows:
section 2 presents related work. Section 3 explains
different word cluster based features employed
to reduce data sparsity for monolingual SA. In
section 4, alternative CLSA approaches based
on word clustering are elucidated. Experimental
details are explained in section 5. Results and
discussions are presented in section 6 and section
7 respectively. Finally, section 8 concludes
the paper pointing to some future research
possibilities.
</bodyText>
<sectionHeader confidence="0.999689" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.952353526315789">
The problem of SA at document level is defined
as the classification of document into different
polarity classes (positive and negative) (Turney,
2002). Both supervised (Benamara et al., 2007;
Martineau and Finin, 2009) and unsupervised
approaches (Mei et al., 2007; Lin and He, 2009)
exist for this task.
Supervised approaches are popular because
of their superior classification accuracy (Mullen
and Collier, 2004; Pang and Lee, 2008).
Feature engineering plays an important role
in these systems. Apart from the commonly
used bag-of-words features based on
unigrams/bigrams/ngrams (Dave et al., 2003;
Ng et al., 2006; Martineau and Finin, 2009),
1Hindi and Marathi belong to the Indo-Aryan subgroup
of the Indo-European language family and are two widely
spoken Indian languages with a speaker population of 450
million and 72 million respectively.
</bodyText>
<page confidence="0.998757">
413
</page>
<bodyText confidence="0.999966192307692">
syntax (Matsumoto et al., 2005; Nakagawa et
al., 2010), semantic (Balamurali et al., 2011)
and negation (Ikeda et al., 2008) have also been
explored for this task. There has been research
related to clustering and sentiment analysis. In
Rooney et al. (2011), documents are clustered
based on the context of each document and
sentiment labels are attached at the cluster level.
Zhai et al. (2011) attempts to cluster features of a
product to perform sentiment analysis on product
reviews. In this work, word clusters (syntagmatic
and paradigmatic) encoding a mixture of syntactic
and semantic information are used for feature
engineering.
In situations where labeled data is not present
in a language, approaches based on cross-lingual
sentiment analysis are used. Most often these
methods depend on an intermediary machine
translation system (Wan, 2009; Brooke et al.,
2009) or a bilingual dictionary (Ghorbel and
Jacot, 2011; Lu et al., 2011) to bridge the
language gap. Given the subtle and different
ways the sentiment can be expressed which itself
manifested as a result of cultural diversity amongst
different languages, an MT system has to be of a
superior quality to capture them.
</bodyText>
<sectionHeader confidence="0.998967" genericHeader="method">
3 Clustering for Sentiment Analysis
</sectionHeader>
<bodyText confidence="0.99985">
The goal of this paper, to remind the reader, is to
investigate whether superior word cluster features
based on manually crafted and fine grained lexical
resource like WordNet can be replaced with the
syntagmatic property based word clusters created
from unlabelled monolingual corpora.
In this section, different clustering approaches
are presented for feature engineering in a
monolingual setting.
</bodyText>
<subsectionHeader confidence="0.9997255">
3.1 Approach 1: Clustering based on
WordIet Sense
</subsectionHeader>
<bodyText confidence="0.99995925">
A synonymous set of words in a WordNet is called
a synset. Each synset can be considered as a word
cluster comprising of semantically similar words.
Balamurali et al. (2011) showed that WordNet
synsets can act as good features for document level
sentiment classification.
Motivation for their study stems from the fact
that different senses of a word can have different
polarities. To empirically prove the superiority
of sense based features, different variants of
a travel review domain corpus were generated
by using automatic/manual sense disambiguation
techniques. Thereafter, accuracies of classifiers
based on different sense-based and word-based
features were compared. The results suggested
that WordNet synset based features performed
better than word-based features.
In this study, synset identifiers are extracted
from manually/automatically sense annotated
corpora and used as features for creating sentiment
classifiers. The classifier thus build is used as
a baseline. Apart from this, another baseline
employing word based features are used for a
comprehensive comparison.
</bodyText>
<subsectionHeader confidence="0.9955105">
3.2 Approach 2: Syntagmatic Property based
Clustering
</subsectionHeader>
<bodyText confidence="0.997473357142857">
For this particular study, a co-occurrence based
algorithm is used to create word clusters. As
the algorithm is based on co-occurrence, one
can extract the classes that have the flavour of
syntagmatic grouping, depending on the nature
of underlying statistics. Agglomerative clustering
algorithm by Brown et al. (1992) is used for this
purpose. It is a hard clustering algorithm i.e., each
word belongs to one cluster only.
Formally, as mentioned in Brown et al. (1992),
let C be a hard clustering function which maps
vocabulary V to one of the K clusters. Then,
the likelihood (L()) of a sequence of word tokens,
w = [wj]mj=1, with wj ∈ V , can be factored as,
</bodyText>
<equation confidence="0.98250925">
m
L(w; C) = H p(wj|C(wj))p(C(wj)|C(wj−1)))
j=1
(1)
</equation>
<bodyText confidence="0.999839">
Words are assigned to clusters such that the
above quantity is maximized. For the purpose
of sentiment classification, cluster identifiers
representing words in the document are used as
features for training.
</bodyText>
<sectionHeader confidence="0.99027" genericHeader="method">
4 Clustering for Cross Lingual
</sectionHeader>
<subsectionHeader confidence="0.973332">
Sentiment Analysis
</subsectionHeader>
<bodyText confidence="0.999987777777778">
Existing approaches for CLSA depend on an
intermediary machine translation system to bridge
the language gap (Hiroshi et al., 2004; Banea et
al., 2008). Machine translation is very resource
intensive. If a language is truly resource scarce, it
is mostly unlikely to have an MT system. Given
that sentiment analysis is a less resource intensive
task compared to machine translation, the use of
an MT system is hard to justify for performing
</bodyText>
<page confidence="0.994631">
414
</page>
<bodyText confidence="0.999921">
CLSA. As a viable alternative, cluster linkages
could be learned from a bilingual parallel corpus
and these linkages can be used to bridge the
language gap for CLSA.
In this section, three approaches using clusters
as features for CLSA are compared. The language
whose annotated data is used for training is
called the source language (S), while the language
whose documents are to be sentiment classified is
referred to as the target language (T).
</bodyText>
<sectionHeader confidence="0.35574" genericHeader="method">
4.1 Approach1: Projection based on Sense
(PS)
</sectionHeader>
<bodyText confidence="0.8849323125">
In this approach, a Multidict is used to bridge the
language gap for SA. A Multidict is an instance
of WordNet where the same sense from different
languages are linked (Mohanty et al., 2008).
An entry in the multidict will have a WordNet
sense identifier from S and the corresponding
WordNet sense identifier from T. The approach
of projection based on sense is explained in
Algorithm 1. Note that after the Sense Mark
operation, each document will be represented as
a vector of WordNet sense identifiers.
Algorithm 1 Projection based on sense
Input: Polarity labeled data in source language
(S) and data in target language (T) to be
labeled
Output: Classified documents
</bodyText>
<listItem confidence="0.987863571428571">
1: Sense mark the polarity labeled data from S
2: Project the sense marked corpora from S to T
using a Multidict
3: Model the sentiment classifier using the data
obtained in step-2
4: Sense mark the unlabelled data from T
5: Test the sentiment classifier on data obtained
</listItem>
<bodyText confidence="0.936796">
in step-4 using model obtained in step-3
Sense identifiers are the features for the
classifier. For those sense identifiers which do not
have a corresponding entry in the Multidict, no
projection is performed.
</bodyText>
<sectionHeader confidence="0.474009" genericHeader="method">
4.2 Approach 2: Direct Cluster Linking
(DCL)
</sectionHeader>
<bodyText confidence="0.999498363636364">
Given a parallel bilingual corpus, word clusters in
S can be aligned to clusters in T. Word alignments
are created using parallel corpora. Given two
aligned word sequences wS = [wSj ]m j=1 and
wT = [wTk ]nk=1, let αT|S be a set of scored
alignments from the source language to the target
language. Here, an alignment from the akth source
word to the kth target word, with score sk,ak &gt; e
is represented as (wTk , wSak, sk,ak) E αT|S. To
simplify, k E αT|S is used to denote those target
words wTk that are aligned to some source word
</bodyText>
<equation confidence="0.865525">
wS
ak.
</equation>
<bodyText confidence="0.9854735">
The source and the target side clusters are linked
using the Equation (2).
</bodyText>
<equation confidence="0.826034">
sk,ak (2)
</equation>
<bodyText confidence="0.999943833333333">
Here, a target side cluster t E CT is linked to
a source side cluster l E CS such that the total
alignment score between words in l and words in
t is maximum. CS and CT stands for source and
target side cluster list respectively. LC(l) gives
the target side cluster t to which l is linked.
</bodyText>
<subsectionHeader confidence="0.784206">
4.3 Approach 3: Cross-Lingual Clustering
</subsectionHeader>
<bodyText confidence="0.977941863636364">
(XC)
Direct cluster linking approach suffers from the
size of alignment dataset in the form of parallel
corpora. The size of the alignment dataset is
typically smaller than the monolingual dataset.
To circumvent this problem, T¨ackstr¨om et al.
(2012) introduced cross-lingual clustering. In
cross-lingual clustering, the objective function
maximizes the joint likelihood of monolingual
and cross-lingual factors. Given a list of
words and clusters it belongs to, a clustering
algorithm tries to obtain word-cluster association
which maximizes the joint likelihood of words
and clusters. Whereas in case of cross-
lingual clustering, the same clustering can be
explained in terms of maximizing the likelihood
of monolingual word-cluster pairs of the source,
the target and alignments between them.
Formally, as stated in T¨ackstr¨om et al. (2012),
Using the model of Uszkoreit and Brants (2008),
the likelihood of a sequence of word tokens,
w = [wj]mj=1, with wj E V , can be factored as,
</bodyText>
<equation confidence="0.977031">
m
L(w; C) = H p(wj|C(wj))p(C(wj)|wj−1))
j=1
</equation>
<bodyText confidence="0.88083725">
(3)
Note this is different from the likelihood
estimation of Brown et al. (1992) (Equation (1)),
where C(wj) was conditioned on C(wj−1). This
</bodyText>
<figure confidence="0.3885975">
LC(l) = argmax E
t k∈αT|S ∪ αS|T
s.t.CT (wTk )=t
CS (wSak )=l
</figure>
<page confidence="0.995111">
415
</page>
<bodyText confidence="0.992592">
makes the computation easier as suggested in the
original paper. The Equation (3) in a cross lingual
setting will be transformed as given below:
</bodyText>
<equation confidence="0.9951165">
LS,T (wS, wT ; αT|S, αS|T ,CS, CT ) =
LS(...).LT (...).LT |S(...).LS|T (...) (4)
</equation>
<bodyText confidence="0.9908815">
Here, LT |S(...) and LS|T (...) are factors based on
word alignments, which can be represented as:
</bodyText>
<equation confidence="0.96785025">
LT |S(wT ; αT |S, CT , CS) =
H p(wTk |CT (wTk ))p(CT (wTk )|CS(wSak)))
k∈αT|S
(5)
</equation>
<bodyText confidence="0.8700434">
Based on the optimization objective in
Equation (4), a pseudo algorithm is defined in
Algorithm 2. For more information, readers are
requested to refer T¨ackstr¨om et al. (2012).
Algorithm 2 Cross-lingual Clustering (XC)
</bodyText>
<listItem confidence="0.964707111111111">
Input: Source and target language corpus
Output: Cross-lingual clusters
1: ## CS, CT randomly initialized
2: for i ← 1toNdo
3: Find CS∗ Pz argmaxCS LS(wS; CS)
4: Project CS∗ to CT
5: Find CT∗ Pz argmaxCT LT (wT; CT)
6: Project CT∗ to CS
7: end for
</listItem>
<bodyText confidence="0.9931662">
An MT based CLSA approach is used as the
baseline. Training data from S is translated to T
and classification model is learned using unigram
based features. Thereafter, the classifier is directly
tested on data from T.
</bodyText>
<sectionHeader confidence="0.998786" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<bodyText confidence="0.998537">
Analysis was performed on three languages, viz.,
English (En), Hindi (Hi) and Marathi (Mar).
CLSA was performed on two language
pairs, English-Hindi and English-Marathi.
For clustering the words, monolingual data of
Indian Languages Corpora Initiative (ILCI)2 was
used. It should also be noted that sentiment
annotated data was also included in the data used
for the word clusterings process. For Brown
clustering, an implementation by Liang (2005)
was used. Cross-lingual clustering for CLSA
</bodyText>
<footnote confidence="0.8782035">
2http://sanskrit.jnu.ac.in/ilci/index.
jsp
</footnote>
<bodyText confidence="0.992269822222222">
was implemented as directed in T¨ackstr¨om et al.
(2012).
Monolingual SA: For experiments in English,
two polarity datasets were used. The first
one (En-TD) by Ye et al. (2009) contains user-
written reviews on travel destinations. The
dataset consists of approximately 600 positive
and 591 negative reviews. Reviews were also
manually sense annotated using WordNet 2.1.
The sense annotation was performed by two
annotators with an inter-annotation agreement of
93%. The second dataset (En-PD)3 on product
reviews (music instruments) from Amazon by
Blitzer et al. (2007) contains 1000 positive and
1000 negative reviews. This dataset was sense
annotated using an automatic WSD engine which
was trained on tourism domain (Khapra et al.,
2010). Experiments using this dataset were
done to study the effect of domain on CLSA.
For experiments in Hindi and Marathi, polarity
datasets by Balamurali et al. (2012) were used.4
These are reviews collected from various Hindi
and Marathi blogs and Sunday editorials. Hindi
dataset consist of 98 positive and 100 negative
reviews. Whereas Marathi dataset contains 75
positive and 75 negative reviews. Apart from
being marked with polarity labels at document
level, they are also manually sense annotated using
Hindi and Marathi WordNet respectively.
CLSA: The same datasets used in SA are also
used for CLSA. Three approaches (as described
in section 4) were tested for English-Hindi
and English-Marathi language pairs. To create
alignments, English-Hindi and English-Marathi
parallel corpora from ILCI were used. English-
Hindi parallel corpus contains 45992 sentences
and English-Marathi parallel corpus contains
47881 sentences. To create alignments, GIZA++5
was used (Och and Ney, 2003).
As a preprocessing step, all stop words
were removed. Stemming was performed on
English and Hindi whereas for Marathi data,
Morphological Analyzer was used to reduce the
words to their respective lemmas.
All experiments were performed using C-SVM
</bodyText>
<footnote confidence="0.997031428571429">
3http://www.cs.jhu.edu/˜mdredze/
datasets/sentiment/
4http://www.cfilt.iitb.ac.
in/resources/senti/MPLC_tour_
downloaderInfo.php
5http://www-i6.informatik.rwth-aachen.
de/Colleagues/och/software/GIZA++.html
</footnote>
<page confidence="0.982145">
416
</page>
<table confidence="0.99936825">
Features En-TD En-PD Hi Mar
Words 87.02 77.60 77.36 92.28
WordNet Sense (Paradigmatic) 89.13 74.50 85.80 96.88
Clusters (Syntagmatic) 97.45 87.80 83.50 &apos;14 98.66
</table>
<tableCaption confidence="0.9755">
Table 1: Classification accuracy for monolingual sentiment analysis. For English, results are reported on
two publicly available datasets based on Travel Domain (TD) and Product Domain (PD).
</tableCaption>
<table confidence="0.999745666666667">
Features Words Clust-200 Clust-500 Clust-1000 Clust-1500 Clust-2000 Clust-2500 Clust-3000
En-TD 87.02 97.37 97.45 96.94 96.94 96.52 96.52 96.52
En-PD 77.60 73.20 82.30 84.30 86.35 86.45 87.80 87.40
</table>
<tableCaption confidence="0.999524">
Table 2: Classification accuracy (in %) versus cluster size (number of clusters to be used).
</tableCaption>
<bodyText confidence="0.999908">
(linear kernel with parameter optimized over
training set using 5 fold cross validation) available
as a part of LibSVM package6. SVM was used
since it is known to perform well for sentiment
classification (Pang et al., 2002). Results reported
are based on the average of ten-fold cross-
validation accuracies. Standard text metrics are
used for reporting the experimental results.
</bodyText>
<sectionHeader confidence="0.999936" genericHeader="method">
6 Results
</sectionHeader>
<bodyText confidence="0.999947869565217">
Monolingual classification results are shown in
Table71. Table shows accuracies of SA systems
developed on feature set based on words, senses
and clusters. It must be noted that accuracies
reported for cluster based features are with
respect to the best accuracy based on different
cluster sizes. The improvements in results of
cluster features based approach is found to be
statistically significant over the word features
based approach and sense features based approach
at 95% confidence level when tested using a paired
t-test (except for Hindi cluster features based
approach). But in general, their accuracies do not
significantly vary after cluster size crosses 1500.
Table 2 shows the classification accuracy
variation when cluster size is altered. For,
En-TD and En-PD experiments, the cluster size
was varied between 200-3000 with an interval
of 500 (after a size of 500). In the En-TD
experiment, the best accuracy is achieved for
cluster size 500, which is lesser than the number of
unique-words/unique-senses (6435/6004) present
in the data. Similarly, for the En-PD experiment,
</bodyText>
<footnote confidence="0.9887152">
6http://www.csie.ntu.edu.tw/˜cjlin/
libsvm
7All results reported here are based on 10-fold except for
Marathi (2-fold-5-repeats), as it had comparatively lesser data
samples.
</footnote>
<bodyText confidence="0.966777">
the optimal cluster size of 2500 is also lesser
than the number of unique-words/unique-senses
(30468/4735) present in the data.
To see the effect of training data size variation
for different SA approaches in the En-TD
experiment, the training data size is varied
between 50 to 500. For this, a test set consisting
of 100 positive and 100 negative documents is
fixed. The training data size is varied by selecting
different number of documents from rest of the
dataset (-500 negative and -500 positive) as a
training set. For each training data set 10 repeats
are performed, e.g., for training data size of 50, 50
negative and 50 positive documents are randomly
selected from the training data pool of -500
negative and -500 positive. This was repeated
10 times (with replacement). The results of this
experiment are presented in Figure 1.
</bodyText>
<figureCaption confidence="0.9772615">
Figure 1: Training data variation on En-TD
dataset.
</figureCaption>
<tableCaption confidence="0.932261">
Cross-lingual SA accuracies are presented in
Table 3. As in monolingual case, the reported
accuracies are for features based on the best
cluster size.
</tableCaption>
<figure confidence="0.997448692307692">
0 100 200 300 400 500
Training data size
Accuracy(%)
100
95
90
85
80
75
70
Words
Senses (Paradigmatic)
Clusters (Syntagmatic)
</figure>
<page confidence="0.98593">
417
</page>
<table confidence="0.98518">
Target Language MT PS DCL XC
T=Hi 63.13 53.80 51.51 66.16
T=Mar NA 54.00 56.00 60.30
</table>
<tableCaption confidence="0.959452">
Table 3: Cross-Lingual SA accuracy (%) on T=Hi and T=Mar with 5=En for different approaches
(MT=Machine Translation, PS=Projection based on Sense, DCL=Direct Cluster Linking , XC=Cross-
Lingual Clustering. There is no MT system available for (5=En, T=Mar).
</tableCaption>
<sectionHeader confidence="0.990545" genericHeader="method">
7 Discussions
</sectionHeader>
<bodyText confidence="0.989929468354431">
In this section, some important observations from
the results are discussed.
1. Syntagmatic analysis may be used in lieu
of paradigmatic analysis for SA: The results
suggest that word cluster based features using
syntagmatic analysis is comparatively better than
cluster (sense) based features using paradigmatic
analysis. For two datasets in English and for
the one in Marathi this holds true. For English,
the gap between classification accuracy based on
sense features and cluster features is around 10%.
A state-of-art accuracy is obtained for the public
dataset on travel domain (En-TD).
The difference in accuracy reduces as the
language gets morphologically rich. In a
morphologically rich language, morphology
encompasses syntactical information, limiting the
context it can provide for clustering. This can be
seen from the classification results on Marathi.
However for Hindi, classifier built on features
based on syntagmatic analysis trails the one based
on paradigmatic analysis.
Compared to Marathi, Hindi is a less
morphologically rich language, hence, a better
result was expected. However, a contrary result
was obtained. In Hindi, the subject and the
object of the sentence are linked using a case
marker. Upon error analysis, it was found that
there was a lot of irregular compounding based
on case markers. Case markers were compounded
with the succeeding word. This is a deviation
from the real scenario which would have resulted
in incorrect clustering leading to an unexpected
result. However, the same would not have
occurred for a classifier developed on sense based
features as it was manually sense tagged.
Clustering induces a reduction in the data
sparsity. For example, on En-PD, percentage of
features present in the test set and not present in
the training set to those present in the test set
are 34.17%, 11.24%, 0.31% for words, synsets
and cluster based features respectively. The
improvement in the performance of classifiers
may be attributed to this feature size reduction.
However, it must be noted that clustering based
on unlabelled corpora is less taxing than manually
creating paradigmatic property based clusters like
WordNet synsets.
Barring one instance, both cluster based
features outperform word based features. The
reason for the drop in the accuracy of approach
based on sense features for En-PD dataset
is the domain specific nature of sentiment
analysis (Blitzer et al., 2007), which is explained
in the next point.
2. Domain issues are resolved while using
cluster based features: For En-PD, the classifier
developed using sense features based on
paradigmatic analysis performs inferior to
word based features. Compared to other datasets
used for analysis, this dataset was sense annotated
using an automatic WSD engine. This engine was
trained on a travel domain corpus and as WSD
is also domain specific, the final classification
performance suffered. Additionally, as the target
domain was on products, the automatic WSD
engine employed had an in-domain accuracy
of 78%. The sense disambiguation accuracy of
the same would have lowered in a cross-domain
setting. This might have had a degrading effect on
the SA accuracy.
However, it was seen that classifier developed
on cluster features based on syntagmatic analysis
do not suffer from this. Such clusters
obliterate domain relates issues. In addition, as
more unlabelled data is included for clustering,
the classification accuracy improves.8 Thus,
clustering may be employed to tackle other
specific domain related issues in SA.
</bodyText>
<footnote confidence="0.993841">
8It was observed that adding 0.1 million unlabelled
documents, SA accuracy improved by 1%. This was observed
in the case of English for which there is abundant unlabelled
corpus.
</footnote>
<page confidence="0.981577">
418
</page>
<bodyText confidence="0.914767928571429">
3. Cluster based features using syntagmatic
analysis requires lesser training data: Cluster
based features drastically reduces the dimension
of the feature vector. For instance, the size
of sense based features for En-TD dataset was
1/6th of the size of word based features. This
reduces the perplexity of the classification model.
The reduction in the perplexity leads to the
reduction of training documents to attain the same
classification accuracy without any dimensionality
reduction. This is evident from Figure 1
where accuracy of the cluster features based on
unlabelled corpora are higher even with lesser
training data.
4. Effect of cluster size: The cluster size
(number of clusters employed) has an implication
on the purity of each cluster with respect to the
application. The system performance improved
upon increasing the cluster size and converged
after attaining a certain level of accuracy. In
general, it was found that the best classification
accuracy was obtained for a cluster size between
1000 and 2500. As evident from Table 2, once
the optimal accuracy is obtained, no significant
changes were observed by increasing the cluster
size.
5. Clustering based CLSA is effective:
For target language as Hindi, CLSA accuracy
based on cross-lingual clustering (syntagmatic)
outperforms the one based on MT (refer to
Table 3). This was true for the constraint
clustering approach based on cross-lingual
clustering. Whereas, sentiment classifier using
sense (PS) or direct cluster linking (DCL) is
not very effective. In case of PS approach, the
coverage of the multidict was a problem. The
number of a linkages between sense from English
to Hindi is only around 1/3rd the size of Princeton
WordNet (Fellbaum, 1998). Similarly in case
of DCL approach, monolingual likelihood is
different from the cross-lingual likelihood in
terms of the linkages.
</bodyText>
<listItem confidence="0.871789333333333">
6. A note on CLSA for truly resource scarce
languages: Note that there is no publicly available
MT system for English to Marathi. Moreover,
</listItem>
<bodyText confidence="0.950560714285714">
the digital content in Marathi language does not
have a standard encoding format. This impedes
the automatic crawling of the web for corpora
creation for SA. Much manual effort has to be put
to collect enough corpora for analysis. However,
even in these languages, unlabelled corpora is
easy to obtain. Marathi was chosen to depict
a truly resource scarce SA scenario. Cluster
features based classifier comparatively performed
well with 60% classification accuracy. An MT
based system would have suffered in this case as
Marathi, as stated earlier, is a morphologically
rich language and as compared to English, has a
different word ordering. This could degrade the
accuracy of the machine translation itself, limiting
the performance of an MT based CLSA system.
All this is obliterated by the use of a cluster based
CLSA approach. Moreover, as more monolingual
copora is added for clustering, the cross lingual
cluster linkages could be refined. This can further
boost the CLSA accuracy.
</bodyText>
<sectionHeader confidence="0.945401" genericHeader="conclusions">
8 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999722647058824">
This paper explored feasibility of using word
cluster based features in lieu of features based on
WordNet senses for sentiment analysis to alleviate
the problem of data sparsity. Abstractly, the
motivation was to see if highly effective features
based on paradigmatic property based clustering
could be replaced with the inexpensive ones based
on syntagmatic property for SA.
The study was performed for both monolingual
SA and cross-lingual SA. It was found that
cluster features based on syntagmatic analysis
are better than the WordNet sense features based
on paradigmatic analysis for SA. Invesitgation
revealed that a considerable decrease in the
training data could be achieved while using such
class based features. Moreover, as syntagma based
word clusters are homogenous, it was able to
address domain specific nature of SA as well.
For CLSA, clusters linked together using
unlabelled parallel corpora do away with the need
of translating labelled corpora from one language
to another using an intermediary MT system or
bilingual dictionary. Such a method outperforms
an MT based CLSA approach. Further, this
approach was found to be useful in cases where
there are no MT systems to perform CLSA and
the language of analysis is truly resource scarce.
Thus, wider implication of this study is that many
widely spoken yet resource scare languages like
Pashto, Sundanese, Hausa, Gujarati and Punjabi
which do not have an MT system could now be
analysed for sentiment. The approach presented
here for CLSA will still require a parallel corpora.
However, the size of the parallel corpora required
</bodyText>
<page confidence="0.997485">
419
</page>
<bodyText confidence="0.999877916666667">
for CLSA can considerably be much lesser than
the size of the parallel corpora required to train an
MT system.
A naive cluster linkage algorithm based on word
alignments was used to perform CLSA. As a
result, there were many erroneous linkages which
lowered the final SA accuracy. Better cluster-
linking approaches could be explored to alleviate
this problem. There are many applications which
use WordNet like IR, IE etc. It would be
interesting to see if these could be replaced by
clusters based on the syntagmatic property.
</bodyText>
<sectionHeader confidence="0.998501" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999578866666667">
A. R. Balamurali, Aditya Joshi, and Pushpak Bhat-
tacharyya. 2011. Harnessing wordnet senses for su-
pervised sentiment classification. In Proceedings of
EMNLP 2011, pages 1081–1091, Stroudsburg, PA,
USA.
A. R. Balamurali, Aditya Joshi, and Pushpak Bhat-
tacharyya. 2012. Cross-lingual sentiment analysis
for Indian languages using linked wordnets. In Pro-
ceedings of COLING 2012, pages 73–82, Mumbai,
India.
A. R. Balamurali, Mitesh M. Khapra, and Pushpak
Bhattacharyya. 2013. Lost in translation: viability
of machine translation for cross language sentiment
analysis. In Proceedings of CICLing 2013, pages
38–49, Berlin, Heidelberg.
Carmen Banea, Rada Mihalcea, Janyce Wiebe, and
Samer Hassan. 2008. Multilingual subjectivity
analysis using machine translation. In Proceedings
ofEMNLP 2008, pages 127–135, Honolulu, Hawaii.
Farah Benamara, Sabatier Irit, Carmine Cesarano,
Napoli Federico, and Diego Reforgiato. 2007. Sen-
timent analysis: Adjectives and adverbs are better
than adjectives alone. In Proceedings of the Inter-
national Conference on Weblogs and Social Media.
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classi-
fication. In Proceedings of ACL 2007, pages 440–
447, Prague, Czech Republic.
Julian Brooke, Milan Tofiloski, and Maite Taboada.
2009. Cross-linguistic sentiment analysis: From en-
glish to spanish. In Proceedings of the International
Conference RANLP-2009, pages 50–54, Borovets,
Bulgaria.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, pages 467–479,
December.
D. Chandler. 2012. Semiotics for begin-
ners. http://users.aber.ac.uk/dgc/
Documents/S4B/sem01.html. Online, ac-
cessed 20-February-2013.
D. A. Cruse. 1986. Lexical Semantics. Cambridge
University Press.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the peanut gallery: opinion extraction
and semantic classification of product reviews. In
Proceedings of WWW 2003, pages 519–528, New
York, NY, USA.
Kevin Duh, Akinori Fujino, and Masaaki Nagata.
2011. Is machine translation ripe for cross-lingual
sentiment classification? In Proceedings of ACL-
HLT 2011, pages 429–433, Stroudsburg, PA, USA.
Manaal Faruqui and Sebastian Pad´o. 2010. Training
and Evaluating a German Named Entity Recognizer
with Semantic Generalization. In Proceedings of
KONVENS 2010, Saarbr¨ucken, Germany.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Hatem Ghorbel and David Jacot. 2011. Further ex-
periments in sentiment analysis of french movie re-
views. In Proceedings ofAWIC 2011, pages 19–28,
Fribourg, Switzerland.
Gholamreza Haffari, Marzieh Razavi, and Anoop
Sarkar. 2011. An ensemble model that combines
syntactic and semantic clustering for discriminative
dependency parsing. In Proceedings of ACL-HLT
2011, pages 710–714, Stroudsburg, PA, USA.
Kanayama Hiroshi, Nasukawa Tetsuya, and Watanabe
Hideo. 2004. Deeper sentiment analysis using
machine translation technology. In Proceedings of
COLING 2004, Stroudsburg, PA, USA.
Daisuke Ikeda, Hiroya Takamura, Lev arie Ratinov, and
Manabu Okumura. 2008. Learning to shift the po-
larity of words for sentiment classification. In Pro-
ceedings of the Third International Joint Conference
on Natural Language Processing.
Mitesh Khapra, Sapan Shah, Piyush Kedia, and Push-
pak Bhattacharyya. 2010. Domain-specific word
sense disambiguation combining corpus based and
wordnet based parameters. In Proceedings of
Global Wordnet Conference.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL-HLT 2008, pages 595–603,
Columbus, Ohio.
Percy Liang. 2005. Semi-supervised learning for natu-
ral language. M. eng. thesis, Massachusetts Institute
of Technology.
</reference>
<page confidence="0.989737">
420
</page>
<reference confidence="0.997598132075472">
Chenghua Lin and Yulan He. 2009. Joint senti-
ment/topic model for sentiment analysis. In Pro-
ceedings of CIKM 2009, pages 375–384, New York,
NY, USA.
Bin Lu, Chenhao Tan, Claire Cardie, and Benjamin K.
Tsou. 2011. Joint bilingual sentiment classification
with unlabeled parallel corpora. In Proceedings of
ACL-HLT 2011, pages 320–330, Stroudsburg, PA,
USA.
Sven Martin, Jrg Liermann, and Hermann Ney. 1995.
Algorithms for bigram and trigram word clustering.
In Speech Communication, pages 1253–1256.
Justin Martineau and Tim Finin. 2009. Delta TFIDF:
An improved feature space for sentiment analysis.
In Proceedings ofICWSM.
Shotaro Matsumoto, Hiroya Takamura, and Manabu
Okumura. 2005. Sentiment classification using
word sub-sequences and dependency sub-trees. In
Advances in Knowledge Discovery and Data Min-
ing, Lecture Notes in Computer Science, pages 301–
311.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,
and ChengXiang Zhai. 2007. Topic sentiment mix-
ture: modeling facets and opinions in weblogs. In
Proceedings of WWW 2007, pages 171–180, New
York, NY, USA.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and dis-
criminative training. In Proceedings ofHLT-IAACL
2004: Main Proceedings, pages 337–342, Boston,
Massachusetts, USA.
Einat Minkov, Kristina Toutanova, and Hisami Suzuki.
2007. Generating complex morphology for machine
translation. In Proceedings of ACL 2007, pages
128–135, Prague, Czech Republic.
Rajat Mohanty, Pushpak Bhattacharyya, Prabhakar
Pande, Shraddha Kalele, Mitesh Khapra, and Aditya
Sharma. 2008. Synset based multilingual dictio-
nary: Insights, applications and challenges. In Pro-
ceedings of Global Wordnet Conference.
Tony Mullen and Nigel Collier. 2004. Sentiment anal-
ysis using support vector machines with diverse in-
formation sources. In Proceedings ofEMILP 2004,
pages 412–418, Barcelona, Spain.
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi.
2010. Dependency tree-based sentiment classifica-
tion using crfs with hidden variables. In Proceed-
ings ofHLT-IAACL 2010, pages 786–794, Strouds-
burg, PA, USA.
Vincent Ng, Sajib Dasgupta, and S. M. Niaz Arifin.
2006. Examining the role of linguistic knowledge
sources in the automatic identification and classifi-
cation of reviews. In Proceedings of the COLIIG
2006, pages 611–618, Stroudsburg, PA, USA.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51,
March.
Bo Pang and Lillian Lee. 2002. Thumbs up? sen-
timent classification using machine learning tech-
niques. In Proceedings of EMILP 2002, pages 79–
86, Stroudsburg, PA, USA.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1-2):1–135, January.
Vassiliki Rentoumi, George Giannakopoulos, Vangelis
Karkaletsis, and George A. Vouros. 2009. Senti-
ment analysis of figurative language using a word
sense disambiguation approach. In Proceedings of
RAILP 2009, pages 370–375, Borovets, Bulgaria,
September.
Niall Rooney, Hui Wang, Fiona Browne, Fergal Mon-
aghan, Jann Mller, Alan Sergeant, Zhiwei Lin,
Philip Taylor, and Vladimir Dobrynin. 2011. An ex-
ploration into the use of contextual document clus-
tering for cluster sentiment analysis. In Proceedings
ofRAILP 2011, pages 140–145, Hissar, Bulgaria.
C. Samuelsson and W. Reichl. 1999. A class-based
language model for large-vocabulary speech recog-
nition extracted from part-of-speech statistics. In
Proceedings ofICASSP 1999, pages 537–540.
Sara Stymne. 2012. Clustered word classes for pre-
ordering in statistical machine translation. In Pro-
ceedings ofthe Joint Workshop on Unsupervised and
Semi-Supervised Learning in ILP, pages 28–34.
Oscar T¨ackstr¨om, Ryan McDonald, and Jakob Uszkor-
eit. 2012. Cross-lingual Word Clusters for Direct
Transfer of Linguistic Structure. In Proceedings
of IAACL-HLT 2012, pages 477–487, Montr´eal,
Canada.
Martin Tamara, Balahur Alexandra, and Montoyo An-
dres. 2010. Word sense disambiguation in opinion
mining: Pros and cons. Journal Research in Com-
puting Science, 46:119–130.
Stephen Tratz and Eduard Hovy. 2011. A fast, ac-
curate, non-projective, semantically-enriched parser.
In Proceedings of EMILP 2011, pages 1257–1268,
Stroudsburg, PA, USA.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of
ACL 2010, pages 384–394, Stroudsburg, PA, USA.
Peter D. Turney. 2002. Thumbs up or thumbs down?:
semantic orientation applied to unsupervised classi-
fication of reviews. In Proceedings of ACL 2002,
pages 417–424, Stroudsburg, PA, USA.
</reference>
<page confidence="0.981832">
421
</page>
<reference confidence="0.998366461538462">
Jakob Uszkoreit and Thorsten Brants. 2008. Dis-
tributed word clustering for large scale class-based
language modeling in machine translation. In Pro-
ceedings ofACL-HLT 2008, pages 755–762, Colum-
bus, Ohio.
Xiaojun Wan. 2009. Co-training for cross-lingual sen-
timent classification. In Proceedings of ACL 2009,
pages 235–243, Stroudsburg, PA, USA.
Gui-Rong Xue, Chenxi Lin, Qiang Yang, WenSi Xi,
Hua-Jun Zeng, Yong Yu, and Zheng Chen. 2005.
Scalable collaborative filtering using cluster-based
smoothing. In Proceedings of SIGIR 2005, pages
114–121, New York, NY, USA.
Qiang Ye, Ziqiong Zhang, and Rob Law. 2009.
Sentiment classification of online reviews to travel
destinations by supervised machine learning ap-
proaches. Expert Systems with Applications, 36(3,
Part 2):6527–6535.
Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia. 2011.
Clustering product features for opinion mining. In
Proceedings of WSDM 2011, pages 347–354, New
York, NY, USA.
Yue Zhang and Joakim Nivre. 2011. Transition-
based dependency parsing with rich non-local fea-
tures. In Proceedings ofACL-HLT 2011, pages 188–
193, Stroudsburg, PA, USA.
</reference>
<page confidence="0.998528">
422
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.560943">
<title confidence="0.998561">Haves and the Leveraging Unlabelled Corpora Sentiment Analysis</title>
<author confidence="0.985303">Balamurali A Pushpak Gholamreza</author>
<affiliation confidence="0.998343">Research Academy, IIT Bombay University</affiliation>
<address confidence="0.723365">of Computer Science and Engineering, IIT Bombay Australia</address>
<email confidence="0.999911">reza@monash.edu</email>
<abstract confidence="0.991130041666667">Expensive feature engineering based on WordNet senses has been shown to be useful for document level sentiment classification. A plausible reason such a performance improvement is the reduction in data sparsity. such a reduction could be achieved with a lesser effort through the means of syntagma based word clustering. this paper, the problem of data sparsity in sentiment analysis, both monolingual and cross-lingual, is addressed through the means of clustering. Experiments show that cluster based data sparsity reduction leads to performance better than sense based classification for sentiment analysis at document level. Similar idea is applied to Cross Lingual Sentiment Analysis (CLSA), and it is shown that reduction in data sparsity (after translation or bilingual-mapping) produces accuracy higher than Machine Translation based CLSA and sense based CLSA.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A R Balamurali</author>
<author>Aditya Joshi</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Harnessing wordnet senses for supervised sentiment classification.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP 2011,</booktitle>
<pages>1081--1091</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2486" citStr="Balamurali et al., 2011" startWordPosition="359" endWordPosition="362">d in the text (Cruse, 1986; Chandler, 2012). WordNet is a byproduct of such an analysis. In WordNet, paradigms are manually generated based on the principles of lexical and semantic relationship among words (Fellbaum, 1998). WordNets are primarily used to address the problem of word sense disambiguation. However, at present there are many NLP applications which use WordNet. One such application is Sentiment Analysis (SA) (Pang and Lee, 2002). Recent research has shown that word sense based semantic features can improve the performance of SA systems (Rentoumi et al., 2009; Tamara et al., 2010; Balamurali et al., 2011) compared to word based features. Syntagmatic analysis of text concentrates on the surface properties of the text. Compared to paradigmatic property extraction, syntagmatic processing is relatively light weight. One of the obvious syntagmas is words, and words are grouped into equivalence classes or clusters, thus reducing the model parameters of a statistical NLP system (Brown et al., 1992). When used as an additional feature with word based language models, it has been shown to improve the system performance viz., machine translation (Uszkoreit and Brants, 2008; Stymne, 2012), speech recogni</context>
<context position="8180" citStr="Balamurali et al., 2011" startWordPosition="1219" endWordPosition="1222">aches are popular because of their superior classification accuracy (Mullen and Collier, 2004; Pang and Lee, 2008). Feature engineering plays an important role in these systems. Apart from the commonly used bag-of-words features based on unigrams/bigrams/ngrams (Dave et al., 2003; Ng et al., 2006; Martineau and Finin, 2009), 1Hindi and Marathi belong to the Indo-Aryan subgroup of the Indo-European language family and are two widely spoken Indian languages with a speaker population of 450 million and 72 million respectively. 413 syntax (Matsumoto et al., 2005; Nakagawa et al., 2010), semantic (Balamurali et al., 2011) and negation (Ikeda et al., 2008) have also been explored for this task. There has been research related to clustering and sentiment analysis. In Rooney et al. (2011), documents are clustered based on the context of each document and sentiment labels are attached at the cluster level. Zhai et al. (2011) attempts to cluster features of a product to perform sentiment analysis on product reviews. In this work, word clusters (syntagmatic and paradigmatic) encoding a mixture of syntactic and semantic information are used for feature engineering. In situations where labeled data is not present in a</context>
<context position="9937" citStr="Balamurali et al. (2011)" startWordPosition="1495" endWordPosition="1498"> The goal of this paper, to remind the reader, is to investigate whether superior word cluster features based on manually crafted and fine grained lexical resource like WordNet can be replaced with the syntagmatic property based word clusters created from unlabelled monolingual corpora. In this section, different clustering approaches are presented for feature engineering in a monolingual setting. 3.1 Approach 1: Clustering based on WordIet Sense A synonymous set of words in a WordNet is called a synset. Each synset can be considered as a word cluster comprising of semantically similar words. Balamurali et al. (2011) showed that WordNet synsets can act as good features for document level sentiment classification. Motivation for their study stems from the fact that different senses of a word can have different polarities. To empirically prove the superiority of sense based features, different variants of a travel review domain corpus were generated by using automatic/manual sense disambiguation techniques. Thereafter, accuracies of classifiers based on different sense-based and word-based features were compared. The results suggested that WordNet synset based features performed better than word-based featu</context>
</contexts>
<marker>Balamurali, Joshi, Bhattacharyya, 2011</marker>
<rawString>A. R. Balamurali, Aditya Joshi, and Pushpak Bhattacharyya. 2011. Harnessing wordnet senses for supervised sentiment classification. In Proceedings of EMNLP 2011, pages 1081–1091, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A R Balamurali</author>
<author>Aditya Joshi</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Cross-lingual sentiment analysis for Indian languages using linked wordnets.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012,</booktitle>
<pages>73--82</pages>
<location>Mumbai, India.</location>
<contexts>
<context position="18768" citStr="Balamurali et al. (2012)" startWordPosition="2932" endWordPosition="2935"> and 591 negative reviews. Reviews were also manually sense annotated using WordNet 2.1. The sense annotation was performed by two annotators with an inter-annotation agreement of 93%. The second dataset (En-PD)3 on product reviews (music instruments) from Amazon by Blitzer et al. (2007) contains 1000 positive and 1000 negative reviews. This dataset was sense annotated using an automatic WSD engine which was trained on tourism domain (Khapra et al., 2010). Experiments using this dataset were done to study the effect of domain on CLSA. For experiments in Hindi and Marathi, polarity datasets by Balamurali et al. (2012) were used.4 These are reviews collected from various Hindi and Marathi blogs and Sunday editorials. Hindi dataset consist of 98 positive and 100 negative reviews. Whereas Marathi dataset contains 75 positive and 75 negative reviews. Apart from being marked with polarity labels at document level, they are also manually sense annotated using Hindi and Marathi WordNet respectively. CLSA: The same datasets used in SA are also used for CLSA. Three approaches (as described in section 4) were tested for English-Hindi and English-Marathi language pairs. To create alignments, English-Hindi and English</context>
</contexts>
<marker>Balamurali, Joshi, Bhattacharyya, 2012</marker>
<rawString>A. R. Balamurali, Aditya Joshi, and Pushpak Bhattacharyya. 2012. Cross-lingual sentiment analysis for Indian languages using linked wordnets. In Proceedings of COLING 2012, pages 73–82, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A R Balamurali</author>
<author>Mitesh M Khapra</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Lost in translation: viability of machine translation for cross language sentiment analysis.</title>
<date>2013</date>
<booktitle>In Proceedings of CICLing 2013,</booktitle>
<pages>38--49</pages>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="5494" citStr="Balamurali et al., 2013" startWordPosition="813" endWordPosition="816">om WordNet) can be replaced by those generated from clustering based on syntagmatic properties. Further, cluster based features are used to address the problem of scarcity of sentiment annotated data in a language. Popular approaches for Cross-Lingual Sentiment Analysis (CLSA) (Wan, 2009; Duh et al., 2011) depend on Machine Translation (MT) for converting the labeled data from one language to the other (Hiroshi et al., 2004; Banea et al., 2008; Wan, 2009). However, many languages which are truly resource scarce, do not have an MT system or existing MT systems are not ripe to be used for CLSA (Balamurali et al., 2013). To perform CLSA, this study leverages unlabelled parallel corpus to generate the word alignments. These word alignments are then used to link cluster based features to obliterate the language gap for performing SA. No MT systems or bilingual dictionaries are used for this study. Instead, language gap for performing CLSA is bridged using linked cluster or cross-lingual clusters (explained in section 4) with the help of unlabelled monolingual corpora. The contributions of this paper are two fold: 1. Features created from manually built and finer clusters can be replaced by inexpensive cluster </context>
</contexts>
<marker>Balamurali, Khapra, Bhattacharyya, 2013</marker>
<rawString>A. R. Balamurali, Mitesh M. Khapra, and Pushpak Bhattacharyya. 2013. Lost in translation: viability of machine translation for cross language sentiment analysis. In Proceedings of CICLing 2013, pages 38–49, Berlin, Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carmen Banea</author>
<author>Rada Mihalcea</author>
<author>Janyce Wiebe</author>
<author>Samer Hassan</author>
</authors>
<title>Multilingual subjectivity analysis using machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings ofEMNLP 2008,</booktitle>
<pages>127--135</pages>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="5317" citStr="Banea et al., 2008" startWordPosition="781" endWordPosition="784">y leveraging unlabelled monolingual corpora. Specifically, experiments are performed to investigate whether features developed from manually crafted clusterings (coming from WordNet) can be replaced by those generated from clustering based on syntagmatic properties. Further, cluster based features are used to address the problem of scarcity of sentiment annotated data in a language. Popular approaches for Cross-Lingual Sentiment Analysis (CLSA) (Wan, 2009; Duh et al., 2011) depend on Machine Translation (MT) for converting the labeled data from one language to the other (Hiroshi et al., 2004; Banea et al., 2008; Wan, 2009). However, many languages which are truly resource scarce, do not have an MT system or existing MT systems are not ripe to be used for CLSA (Balamurali et al., 2013). To perform CLSA, this study leverages unlabelled parallel corpus to generate the word alignments. These word alignments are then used to link cluster based features to obliterate the language gap for performing SA. No MT systems or bilingual dictionaries are used for this study. Instead, language gap for performing CLSA is bridged using linked cluster or cross-lingual clusters (explained in section 4) with the help of</context>
<context position="12030" citStr="Banea et al., 2008" startWordPosition="1813" endWordPosition="1816">on which maps vocabulary V to one of the K clusters. Then, the likelihood (L()) of a sequence of word tokens, w = [wj]mj=1, with wj ∈ V , can be factored as, m L(w; C) = H p(wj|C(wj))p(C(wj)|C(wj−1))) j=1 (1) Words are assigned to clusters such that the above quantity is maximized. For the purpose of sentiment classification, cluster identifiers representing words in the document are used as features for training. 4 Clustering for Cross Lingual Sentiment Analysis Existing approaches for CLSA depend on an intermediary machine translation system to bridge the language gap (Hiroshi et al., 2004; Banea et al., 2008). Machine translation is very resource intensive. If a language is truly resource scarce, it is mostly unlikely to have an MT system. Given that sentiment analysis is a less resource intensive task compared to machine translation, the use of an MT system is hard to justify for performing 414 CLSA. As a viable alternative, cluster linkages could be learned from a bilingual parallel corpus and these linkages can be used to bridge the language gap for CLSA. In this section, three approaches using clusters as features for CLSA are compared. The language whose annotated data is used for training is</context>
</contexts>
<marker>Banea, Mihalcea, Wiebe, Hassan, 2008</marker>
<rawString>Carmen Banea, Rada Mihalcea, Janyce Wiebe, and Samer Hassan. 2008. Multilingual subjectivity analysis using machine translation. In Proceedings ofEMNLP 2008, pages 127–135, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Farah Benamara</author>
<author>Sabatier Irit</author>
<author>Carmine Cesarano</author>
<author>Napoli Federico</author>
<author>Diego Reforgiato</author>
</authors>
<title>Sentiment analysis: Adjectives and adverbs are better than adjectives alone.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on Weblogs and Social Media.</booktitle>
<contexts>
<context position="7425" citStr="Benamara et al., 2007" startWordPosition="1105" endWordPosition="1108"> work. Section 3 explains different word cluster based features employed to reduce data sparsity for monolingual SA. In section 4, alternative CLSA approaches based on word clustering are elucidated. Experimental details are explained in section 5. Results and discussions are presented in section 6 and section 7 respectively. Finally, section 8 concludes the paper pointing to some future research possibilities. 2 Related Work The problem of SA at document level is defined as the classification of document into different polarity classes (positive and negative) (Turney, 2002). Both supervised (Benamara et al., 2007; Martineau and Finin, 2009) and unsupervised approaches (Mei et al., 2007; Lin and He, 2009) exist for this task. Supervised approaches are popular because of their superior classification accuracy (Mullen and Collier, 2004; Pang and Lee, 2008). Feature engineering plays an important role in these systems. Apart from the commonly used bag-of-words features based on unigrams/bigrams/ngrams (Dave et al., 2003; Ng et al., 2006; Martineau and Finin, 2009), 1Hindi and Marathi belong to the Indo-Aryan subgroup of the Indo-European language family and are two widely spoken Indian languages with a sp</context>
</contexts>
<marker>Benamara, Irit, Cesarano, Federico, Reforgiato, 2007</marker>
<rawString>Farah Benamara, Sabatier Irit, Carmine Cesarano, Napoli Federico, and Diego Reforgiato. 2007. Sentiment analysis: Adjectives and adverbs are better than adjectives alone. In Proceedings of the International Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Mark Dredze</author>
<author>Fernando Pereira</author>
</authors>
<title>Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL 2007,</booktitle>
<pages>440--447</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="18432" citStr="Blitzer et al. (2007)" startWordPosition="2878" endWordPosition="2881"> for CLSA 2http://sanskrit.jnu.ac.in/ilci/index. jsp was implemented as directed in T¨ackstr¨om et al. (2012). Monolingual SA: For experiments in English, two polarity datasets were used. The first one (En-TD) by Ye et al. (2009) contains userwritten reviews on travel destinations. The dataset consists of approximately 600 positive and 591 negative reviews. Reviews were also manually sense annotated using WordNet 2.1. The sense annotation was performed by two annotators with an inter-annotation agreement of 93%. The second dataset (En-PD)3 on product reviews (music instruments) from Amazon by Blitzer et al. (2007) contains 1000 positive and 1000 negative reviews. This dataset was sense annotated using an automatic WSD engine which was trained on tourism domain (Khapra et al., 2010). Experiments using this dataset were done to study the effect of domain on CLSA. For experiments in Hindi and Marathi, polarity datasets by Balamurali et al. (2012) were used.4 These are reviews collected from various Hindi and Marathi blogs and Sunday editorials. Hindi dataset consist of 98 positive and 100 negative reviews. Whereas Marathi dataset contains 75 positive and 75 negative reviews. Apart from being marked with p</context>
<context position="26305" citStr="Blitzer et al., 2007" startWordPosition="4067" endWordPosition="4070">t in the test set are 34.17%, 11.24%, 0.31% for words, synsets and cluster based features respectively. The improvement in the performance of classifiers may be attributed to this feature size reduction. However, it must be noted that clustering based on unlabelled corpora is less taxing than manually creating paradigmatic property based clusters like WordNet synsets. Barring one instance, both cluster based features outperform word based features. The reason for the drop in the accuracy of approach based on sense features for En-PD dataset is the domain specific nature of sentiment analysis (Blitzer et al., 2007), which is explained in the next point. 2. Domain issues are resolved while using cluster based features: For En-PD, the classifier developed using sense features based on paradigmatic analysis performs inferior to word based features. Compared to other datasets used for analysis, this dataset was sense annotated using an automatic WSD engine. This engine was trained on a travel domain corpus and as WSD is also domain specific, the final classification performance suffered. Additionally, as the target domain was on products, the automatic WSD engine employed had an in-domain accuracy of 78%. T</context>
</contexts>
<marker>Blitzer, Dredze, Pereira, 2007</marker>
<rawString>John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In Proceedings of ACL 2007, pages 440– 447, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Brooke</author>
<author>Milan Tofiloski</author>
<author>Maite Taboada</author>
</authors>
<title>Cross-linguistic sentiment analysis: From english to spanish.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Conference RANLP-2009,</booktitle>
<pages>50--54</pages>
<location>Borovets, Bulgaria.</location>
<contexts>
<context position="8964" citStr="Brooke et al., 2009" startWordPosition="1341" endWordPosition="1344">11), documents are clustered based on the context of each document and sentiment labels are attached at the cluster level. Zhai et al. (2011) attempts to cluster features of a product to perform sentiment analysis on product reviews. In this work, word clusters (syntagmatic and paradigmatic) encoding a mixture of syntactic and semantic information are used for feature engineering. In situations where labeled data is not present in a language, approaches based on cross-lingual sentiment analysis are used. Most often these methods depend on an intermediary machine translation system (Wan, 2009; Brooke et al., 2009) or a bilingual dictionary (Ghorbel and Jacot, 2011; Lu et al., 2011) to bridge the language gap. Given the subtle and different ways the sentiment can be expressed which itself manifested as a result of cultural diversity amongst different languages, an MT system has to be of a superior quality to capture them. 3 Clustering for Sentiment Analysis The goal of this paper, to remind the reader, is to investigate whether superior word cluster features based on manually crafted and fine grained lexical resource like WordNet can be replaced with the syntagmatic property based word clusters created </context>
</contexts>
<marker>Brooke, Tofiloski, Taboada, 2009</marker>
<rawString>Julian Brooke, Milan Tofiloski, and Maite Taboada. 2009. Cross-linguistic sentiment analysis: From english to spanish. In Proceedings of the International Conference RANLP-2009, pages 50–54, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V deSouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>467--479</pages>
<contexts>
<context position="2880" citStr="Brown et al., 1992" startWordPosition="417" endWordPosition="420">s Sentiment Analysis (SA) (Pang and Lee, 2002). Recent research has shown that word sense based semantic features can improve the performance of SA systems (Rentoumi et al., 2009; Tamara et al., 2010; Balamurali et al., 2011) compared to word based features. Syntagmatic analysis of text concentrates on the surface properties of the text. Compared to paradigmatic property extraction, syntagmatic processing is relatively light weight. One of the obvious syntagmas is words, and words are grouped into equivalence classes or clusters, thus reducing the model parameters of a statistical NLP system (Brown et al., 1992). When used as an additional feature with word based language models, it has been shown to improve the system performance viz., machine translation (Uszkoreit and Brants, 2008; Stymne, 2012), speech recognition (Martin et al., 1995; Samuelsson and Reichl, 1999), dependency parsing (Koo et al., 2008; Haffari et al., 2011; Zhang and Nivre, 2011; Tratz and Hovy, 2011) and NER (Miller et al., 2004; Faruqui and Pad´o, 2010; Turian et al., 2010; T¨ackstr¨om et al., 2012). In this paper, the focus is on alleviating the data sparsity faced by supervised approaches for SA through the means of cluster b</context>
<context position="11225" citStr="Brown et al. (1992)" startWordPosition="1677" endWordPosition="1680">tomatically sense annotated corpora and used as features for creating sentiment classifiers. The classifier thus build is used as a baseline. Apart from this, another baseline employing word based features are used for a comprehensive comparison. 3.2 Approach 2: Syntagmatic Property based Clustering For this particular study, a co-occurrence based algorithm is used to create word clusters. As the algorithm is based on co-occurrence, one can extract the classes that have the flavour of syntagmatic grouping, depending on the nature of underlying statistics. Agglomerative clustering algorithm by Brown et al. (1992) is used for this purpose. It is a hard clustering algorithm i.e., each word belongs to one cluster only. Formally, as mentioned in Brown et al. (1992), let C be a hard clustering function which maps vocabulary V to one of the K clusters. Then, the likelihood (L()) of a sequence of word tokens, w = [wj]mj=1, with wj ∈ V , can be factored as, m L(w; C) = H p(wj|C(wj))p(C(wj)|C(wj−1))) j=1 (1) Words are assigned to clusters such that the above quantity is maximized. For the purpose of sentiment classification, cluster identifiers representing words in the document are used as features for traini</context>
<context position="16084" citStr="Brown et al. (1992)" startWordPosition="2498" endWordPosition="2501"> to obtain word-cluster association which maximizes the joint likelihood of words and clusters. Whereas in case of crosslingual clustering, the same clustering can be explained in terms of maximizing the likelihood of monolingual word-cluster pairs of the source, the target and alignments between them. Formally, as stated in T¨ackstr¨om et al. (2012), Using the model of Uszkoreit and Brants (2008), the likelihood of a sequence of word tokens, w = [wj]mj=1, with wj E V , can be factored as, m L(w; C) = H p(wj|C(wj))p(C(wj)|wj−1)) j=1 (3) Note this is different from the likelihood estimation of Brown et al. (1992) (Equation (1)), where C(wj) was conditioned on C(wj−1). This LC(l) = argmax E t k∈αT|S ∪ αS|T s.t.CT (wTk )=t CS (wSak )=l 415 makes the computation easier as suggested in the original paper. The Equation (3) in a cross lingual setting will be transformed as given below: LS,T (wS, wT ; αT|S, αS|T ,CS, CT ) = LS(...).LT (...).LT |S(...).LS|T (...) (4) Here, LT |S(...) and LS|T (...) are factors based on word alignments, which can be represented as: LT |S(wT ; αT |S, CT , CS) = H p(wTk |CT (wTk ))p(CT (wTk )|CS(wSak))) k∈αT|S (5) Based on the optimization objective in Equation (4), a pseudo alg</context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based n-gram models of natural language. Computational Linguistics, pages 467–479, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chandler</author>
</authors>
<title>Semiotics for beginners.</title>
<date>2012</date>
<note>http://users.aber.ac.uk/dgc/ Documents/S4B/sem01.html. Online, accessed 20-February-2013.</note>
<contexts>
<context position="1905" citStr="Chandler, 2012" startWordPosition="271" endWordPosition="272">guage Processing (NLP) (Xue et al., 2005; Minkov et al., 2007). Language units encountered in the test data but absent in the training data severely degrade the performance of an NLP task. NLP applications innovatively handle data sparsity through various means. A special, but very common kind of data sparsity viz., word sparsity, can be addressed in one of the two obvious ways: 1) sparsity reduction through paradigmatically related words or 2) sparsity reduction through syntagmatically related words. Paradigmatic analysis of text is the analysis of concepts embedded in the text (Cruse, 1986; Chandler, 2012). WordNet is a byproduct of such an analysis. In WordNet, paradigms are manually generated based on the principles of lexical and semantic relationship among words (Fellbaum, 1998). WordNets are primarily used to address the problem of word sense disambiguation. However, at present there are many NLP applications which use WordNet. One such application is Sentiment Analysis (SA) (Pang and Lee, 2002). Recent research has shown that word sense based semantic features can improve the performance of SA systems (Rentoumi et al., 2009; Tamara et al., 2010; Balamurali et al., 2011) compared to word b</context>
</contexts>
<marker>Chandler, 2012</marker>
<rawString>D. Chandler. 2012. Semiotics for beginners. http://users.aber.ac.uk/dgc/ Documents/S4B/sem01.html. Online, accessed 20-February-2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Cruse</author>
</authors>
<title>Lexical Semantics.</title>
<date>1986</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1888" citStr="Cruse, 1986" startWordPosition="269" endWordPosition="270">f Natural Language Processing (NLP) (Xue et al., 2005; Minkov et al., 2007). Language units encountered in the test data but absent in the training data severely degrade the performance of an NLP task. NLP applications innovatively handle data sparsity through various means. A special, but very common kind of data sparsity viz., word sparsity, can be addressed in one of the two obvious ways: 1) sparsity reduction through paradigmatically related words or 2) sparsity reduction through syntagmatically related words. Paradigmatic analysis of text is the analysis of concepts embedded in the text (Cruse, 1986; Chandler, 2012). WordNet is a byproduct of such an analysis. In WordNet, paradigms are manually generated based on the principles of lexical and semantic relationship among words (Fellbaum, 1998). WordNets are primarily used to address the problem of word sense disambiguation. However, at present there are many NLP applications which use WordNet. One such application is Sentiment Analysis (SA) (Pang and Lee, 2002). Recent research has shown that word sense based semantic features can improve the performance of SA systems (Rentoumi et al., 2009; Tamara et al., 2010; Balamurali et al., 2011) c</context>
</contexts>
<marker>Cruse, 1986</marker>
<rawString>D. A. Cruse. 1986. Lexical Semantics. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kushal Dave</author>
<author>Steve Lawrence</author>
<author>David M Pennock</author>
</authors>
<title>Mining the peanut gallery: opinion extraction and semantic classification of product reviews.</title>
<date>2003</date>
<booktitle>In Proceedings of WWW 2003,</booktitle>
<pages>519--528</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="7836" citStr="Dave et al., 2003" startWordPosition="1165" endWordPosition="1168">2 Related Work The problem of SA at document level is defined as the classification of document into different polarity classes (positive and negative) (Turney, 2002). Both supervised (Benamara et al., 2007; Martineau and Finin, 2009) and unsupervised approaches (Mei et al., 2007; Lin and He, 2009) exist for this task. Supervised approaches are popular because of their superior classification accuracy (Mullen and Collier, 2004; Pang and Lee, 2008). Feature engineering plays an important role in these systems. Apart from the commonly used bag-of-words features based on unigrams/bigrams/ngrams (Dave et al., 2003; Ng et al., 2006; Martineau and Finin, 2009), 1Hindi and Marathi belong to the Indo-Aryan subgroup of the Indo-European language family and are two widely spoken Indian languages with a speaker population of 450 million and 72 million respectively. 413 syntax (Matsumoto et al., 2005; Nakagawa et al., 2010), semantic (Balamurali et al., 2011) and negation (Ikeda et al., 2008) have also been explored for this task. There has been research related to clustering and sentiment analysis. In Rooney et al. (2011), documents are clustered based on the context of each document and sentiment labels are </context>
</contexts>
<marker>Dave, Lawrence, Pennock, 2003</marker>
<rawString>Kushal Dave, Steve Lawrence, and David M. Pennock. 2003. Mining the peanut gallery: opinion extraction and semantic classification of product reviews. In Proceedings of WWW 2003, pages 519–528, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Duh</author>
<author>Akinori Fujino</author>
<author>Masaaki Nagata</author>
</authors>
<title>Is machine translation ripe for cross-lingual sentiment classification?</title>
<date>2011</date>
<booktitle>In Proceedings of ACLHLT 2011,</booktitle>
<pages>429--433</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5177" citStr="Duh et al., 2011" startWordPosition="757" endWordPosition="760"> paradigmatic analysis. In the current work, this particular insight is used to solve the data sparsity problem in the sentiment analysis by leveraging unlabelled monolingual corpora. Specifically, experiments are performed to investigate whether features developed from manually crafted clusterings (coming from WordNet) can be replaced by those generated from clustering based on syntagmatic properties. Further, cluster based features are used to address the problem of scarcity of sentiment annotated data in a language. Popular approaches for Cross-Lingual Sentiment Analysis (CLSA) (Wan, 2009; Duh et al., 2011) depend on Machine Translation (MT) for converting the labeled data from one language to the other (Hiroshi et al., 2004; Banea et al., 2008; Wan, 2009). However, many languages which are truly resource scarce, do not have an MT system or existing MT systems are not ripe to be used for CLSA (Balamurali et al., 2013). To perform CLSA, this study leverages unlabelled parallel corpus to generate the word alignments. These word alignments are then used to link cluster based features to obliterate the language gap for performing SA. No MT systems or bilingual dictionaries are used for this study. I</context>
</contexts>
<marker>Duh, Fujino, Nagata, 2011</marker>
<rawString>Kevin Duh, Akinori Fujino, and Masaaki Nagata. 2011. Is machine translation ripe for cross-lingual sentiment classification? In Proceedings of ACLHLT 2011, pages 429–433, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manaal Faruqui</author>
<author>Sebastian Pad´o</author>
</authors>
<title>Training and Evaluating a German Named Entity Recognizer with Semantic Generalization.</title>
<date>2010</date>
<booktitle>In Proceedings of KONVENS</booktitle>
<location>Saarbr¨ucken, Germany.</location>
<marker>Faruqui, Pad´o, 2010</marker>
<rawString>Manaal Faruqui and Sebastian Pad´o. 2010. Training and Evaluating a German Named Entity Recognizer with Semantic Generalization. In Proceedings of KONVENS 2010, Saarbr¨ucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>Bradford Books.</publisher>
<contexts>
<context position="2085" citStr="Fellbaum, 1998" startWordPosition="298" endWordPosition="299">LP task. NLP applications innovatively handle data sparsity through various means. A special, but very common kind of data sparsity viz., word sparsity, can be addressed in one of the two obvious ways: 1) sparsity reduction through paradigmatically related words or 2) sparsity reduction through syntagmatically related words. Paradigmatic analysis of text is the analysis of concepts embedded in the text (Cruse, 1986; Chandler, 2012). WordNet is a byproduct of such an analysis. In WordNet, paradigms are manually generated based on the principles of lexical and semantic relationship among words (Fellbaum, 1998). WordNets are primarily used to address the problem of word sense disambiguation. However, at present there are many NLP applications which use WordNet. One such application is Sentiment Analysis (SA) (Pang and Lee, 2002). Recent research has shown that word sense based semantic features can improve the performance of SA systems (Rentoumi et al., 2009; Tamara et al., 2010; Balamurali et al., 2011) compared to word based features. Syntagmatic analysis of text concentrates on the surface properties of the text. Compared to paradigmatic property extraction, syntagmatic processing is relatively l</context>
<context position="29330" citStr="Fellbaum, 1998" startWordPosition="4539" endWordPosition="4540">ere observed by increasing the cluster size. 5. Clustering based CLSA is effective: For target language as Hindi, CLSA accuracy based on cross-lingual clustering (syntagmatic) outperforms the one based on MT (refer to Table 3). This was true for the constraint clustering approach based on cross-lingual clustering. Whereas, sentiment classifier using sense (PS) or direct cluster linking (DCL) is not very effective. In case of PS approach, the coverage of the multidict was a problem. The number of a linkages between sense from English to Hindi is only around 1/3rd the size of Princeton WordNet (Fellbaum, 1998). Similarly in case of DCL approach, monolingual likelihood is different from the cross-lingual likelihood in terms of the linkages. 6. A note on CLSA for truly resource scarce languages: Note that there is no publicly available MT system for English to Marathi. Moreover, the digital content in Marathi language does not have a standard encoding format. This impedes the automatic crawling of the web for corpora creation for SA. Much manual effort has to be put to collect enough corpora for analysis. However, even in these languages, unlabelled corpora is easy to obtain. Marathi was chosen to de</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. Bradford Books.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hatem Ghorbel</author>
<author>David Jacot</author>
</authors>
<title>Further experiments in sentiment analysis of french movie reviews.</title>
<date>2011</date>
<booktitle>In Proceedings ofAWIC 2011,</booktitle>
<pages>pages</pages>
<location>Fribourg, Switzerland.</location>
<contexts>
<context position="9015" citStr="Ghorbel and Jacot, 2011" startWordPosition="1349" endWordPosition="1352">t of each document and sentiment labels are attached at the cluster level. Zhai et al. (2011) attempts to cluster features of a product to perform sentiment analysis on product reviews. In this work, word clusters (syntagmatic and paradigmatic) encoding a mixture of syntactic and semantic information are used for feature engineering. In situations where labeled data is not present in a language, approaches based on cross-lingual sentiment analysis are used. Most often these methods depend on an intermediary machine translation system (Wan, 2009; Brooke et al., 2009) or a bilingual dictionary (Ghorbel and Jacot, 2011; Lu et al., 2011) to bridge the language gap. Given the subtle and different ways the sentiment can be expressed which itself manifested as a result of cultural diversity amongst different languages, an MT system has to be of a superior quality to capture them. 3 Clustering for Sentiment Analysis The goal of this paper, to remind the reader, is to investigate whether superior word cluster features based on manually crafted and fine grained lexical resource like WordNet can be replaced with the syntagmatic property based word clusters created from unlabelled monolingual corpora. In this sectio</context>
</contexts>
<marker>Ghorbel, Jacot, 2011</marker>
<rawString>Hatem Ghorbel and David Jacot. 2011. Further experiments in sentiment analysis of french movie reviews. In Proceedings ofAWIC 2011, pages 19–28, Fribourg, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gholamreza Haffari</author>
<author>Marzieh Razavi</author>
<author>Anoop Sarkar</author>
</authors>
<title>An ensemble model that combines syntactic and semantic clustering for discriminative dependency parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL-HLT 2011,</booktitle>
<pages>710--714</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3201" citStr="Haffari et al., 2011" startWordPosition="466" endWordPosition="469">erties of the text. Compared to paradigmatic property extraction, syntagmatic processing is relatively light weight. One of the obvious syntagmas is words, and words are grouped into equivalence classes or clusters, thus reducing the model parameters of a statistical NLP system (Brown et al., 1992). When used as an additional feature with word based language models, it has been shown to improve the system performance viz., machine translation (Uszkoreit and Brants, 2008; Stymne, 2012), speech recognition (Martin et al., 1995; Samuelsson and Reichl, 1999), dependency parsing (Koo et al., 2008; Haffari et al., 2011; Zhang and Nivre, 2011; Tratz and Hovy, 2011) and NER (Miller et al., 2004; Faruqui and Pad´o, 2010; Turian et al., 2010; T¨ackstr¨om et al., 2012). In this paper, the focus is on alleviating the data sparsity faced by supervised approaches for SA through the means of cluster based features. As WordNets are essentially word 412 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 412–422, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics clusters wherein words with the same meaning are clubbed together, they address th</context>
</contexts>
<marker>Haffari, Razavi, Sarkar, 2011</marker>
<rawString>Gholamreza Haffari, Marzieh Razavi, and Anoop Sarkar. 2011. An ensemble model that combines syntactic and semantic clustering for discriminative dependency parsing. In Proceedings of ACL-HLT 2011, pages 710–714, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kanayama Hiroshi</author>
<author>Nasukawa Tetsuya</author>
<author>Watanabe Hideo</author>
</authors>
<title>Deeper sentiment analysis using machine translation technology.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING 2004,</booktitle>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5297" citStr="Hiroshi et al., 2004" startWordPosition="777" endWordPosition="780">e sentiment analysis by leveraging unlabelled monolingual corpora. Specifically, experiments are performed to investigate whether features developed from manually crafted clusterings (coming from WordNet) can be replaced by those generated from clustering based on syntagmatic properties. Further, cluster based features are used to address the problem of scarcity of sentiment annotated data in a language. Popular approaches for Cross-Lingual Sentiment Analysis (CLSA) (Wan, 2009; Duh et al., 2011) depend on Machine Translation (MT) for converting the labeled data from one language to the other (Hiroshi et al., 2004; Banea et al., 2008; Wan, 2009). However, many languages which are truly resource scarce, do not have an MT system or existing MT systems are not ripe to be used for CLSA (Balamurali et al., 2013). To perform CLSA, this study leverages unlabelled parallel corpus to generate the word alignments. These word alignments are then used to link cluster based features to obliterate the language gap for performing SA. No MT systems or bilingual dictionaries are used for this study. Instead, language gap for performing CLSA is bridged using linked cluster or cross-lingual clusters (explained in section</context>
<context position="12009" citStr="Hiroshi et al., 2004" startWordPosition="1809" endWordPosition="1812">hard clustering function which maps vocabulary V to one of the K clusters. Then, the likelihood (L()) of a sequence of word tokens, w = [wj]mj=1, with wj ∈ V , can be factored as, m L(w; C) = H p(wj|C(wj))p(C(wj)|C(wj−1))) j=1 (1) Words are assigned to clusters such that the above quantity is maximized. For the purpose of sentiment classification, cluster identifiers representing words in the document are used as features for training. 4 Clustering for Cross Lingual Sentiment Analysis Existing approaches for CLSA depend on an intermediary machine translation system to bridge the language gap (Hiroshi et al., 2004; Banea et al., 2008). Machine translation is very resource intensive. If a language is truly resource scarce, it is mostly unlikely to have an MT system. Given that sentiment analysis is a less resource intensive task compared to machine translation, the use of an MT system is hard to justify for performing 414 CLSA. As a viable alternative, cluster linkages could be learned from a bilingual parallel corpus and these linkages can be used to bridge the language gap for CLSA. In this section, three approaches using clusters as features for CLSA are compared. The language whose annotated data is</context>
</contexts>
<marker>Hiroshi, Tetsuya, Hideo, 2004</marker>
<rawString>Kanayama Hiroshi, Nasukawa Tetsuya, and Watanabe Hideo. 2004. Deeper sentiment analysis using machine translation technology. In Proceedings of COLING 2004, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Ikeda</author>
<author>Hiroya Takamura</author>
<author>Lev arie Ratinov</author>
<author>Manabu Okumura</author>
</authors>
<title>Learning to shift the polarity of words for sentiment classification.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third International Joint Conference on Natural Language Processing.</booktitle>
<contexts>
<context position="8214" citStr="Ikeda et al., 2008" startWordPosition="1225" endWordPosition="1228">rior classification accuracy (Mullen and Collier, 2004; Pang and Lee, 2008). Feature engineering plays an important role in these systems. Apart from the commonly used bag-of-words features based on unigrams/bigrams/ngrams (Dave et al., 2003; Ng et al., 2006; Martineau and Finin, 2009), 1Hindi and Marathi belong to the Indo-Aryan subgroup of the Indo-European language family and are two widely spoken Indian languages with a speaker population of 450 million and 72 million respectively. 413 syntax (Matsumoto et al., 2005; Nakagawa et al., 2010), semantic (Balamurali et al., 2011) and negation (Ikeda et al., 2008) have also been explored for this task. There has been research related to clustering and sentiment analysis. In Rooney et al. (2011), documents are clustered based on the context of each document and sentiment labels are attached at the cluster level. Zhai et al. (2011) attempts to cluster features of a product to perform sentiment analysis on product reviews. In this work, word clusters (syntagmatic and paradigmatic) encoding a mixture of syntactic and semantic information are used for feature engineering. In situations where labeled data is not present in a language, approaches based on cro</context>
</contexts>
<marker>Ikeda, Takamura, Ratinov, Okumura, 2008</marker>
<rawString>Daisuke Ikeda, Hiroya Takamura, Lev arie Ratinov, and Manabu Okumura. 2008. Learning to shift the polarity of words for sentiment classification. In Proceedings of the Third International Joint Conference on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitesh Khapra</author>
<author>Sapan Shah</author>
<author>Piyush Kedia</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Domain-specific word sense disambiguation combining corpus based and wordnet based parameters.</title>
<date>2010</date>
<booktitle>In Proceedings of Global Wordnet Conference.</booktitle>
<contexts>
<context position="18603" citStr="Khapra et al., 2010" startWordPosition="2905" endWordPosition="2908">sets were used. The first one (En-TD) by Ye et al. (2009) contains userwritten reviews on travel destinations. The dataset consists of approximately 600 positive and 591 negative reviews. Reviews were also manually sense annotated using WordNet 2.1. The sense annotation was performed by two annotators with an inter-annotation agreement of 93%. The second dataset (En-PD)3 on product reviews (music instruments) from Amazon by Blitzer et al. (2007) contains 1000 positive and 1000 negative reviews. This dataset was sense annotated using an automatic WSD engine which was trained on tourism domain (Khapra et al., 2010). Experiments using this dataset were done to study the effect of domain on CLSA. For experiments in Hindi and Marathi, polarity datasets by Balamurali et al. (2012) were used.4 These are reviews collected from various Hindi and Marathi blogs and Sunday editorials. Hindi dataset consist of 98 positive and 100 negative reviews. Whereas Marathi dataset contains 75 positive and 75 negative reviews. Apart from being marked with polarity labels at document level, they are also manually sense annotated using Hindi and Marathi WordNet respectively. CLSA: The same datasets used in SA are also used for</context>
</contexts>
<marker>Khapra, Shah, Kedia, Bhattacharyya, 2010</marker>
<rawString>Mitesh Khapra, Sapan Shah, Piyush Kedia, and Pushpak Bhattacharyya. 2010. Domain-specific word sense disambiguation combining corpus based and wordnet based parameters. In Proceedings of Global Wordnet Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-HLT 2008,</booktitle>
<pages>595--603</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="3179" citStr="Koo et al., 2008" startWordPosition="462" endWordPosition="465">n the surface properties of the text. Compared to paradigmatic property extraction, syntagmatic processing is relatively light weight. One of the obvious syntagmas is words, and words are grouped into equivalence classes or clusters, thus reducing the model parameters of a statistical NLP system (Brown et al., 1992). When used as an additional feature with word based language models, it has been shown to improve the system performance viz., machine translation (Uszkoreit and Brants, 2008; Stymne, 2012), speech recognition (Martin et al., 1995; Samuelsson and Reichl, 1999), dependency parsing (Koo et al., 2008; Haffari et al., 2011; Zhang and Nivre, 2011; Tratz and Hovy, 2011) and NER (Miller et al., 2004; Faruqui and Pad´o, 2010; Turian et al., 2010; T¨ackstr¨om et al., 2012). In this paper, the focus is on alleviating the data sparsity faced by supervised approaches for SA through the means of cluster based features. As WordNets are essentially word 412 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 412–422, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics clusters wherein words with the same meaning are clubbed tog</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Proceedings of ACL-HLT 2008, pages 595–603, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
</authors>
<title>Semi-supervised learning for natural language.</title>
<date>2005</date>
<journal>M. eng. thesis,</journal>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="17776" citStr="Liang (2005)" startWordPosition="2786" endWordPosition="2787">ranslated to T and classification model is learned using unigram based features. Thereafter, the classifier is directly tested on data from T. 5 Experimental Setup Analysis was performed on three languages, viz., English (En), Hindi (Hi) and Marathi (Mar). CLSA was performed on two language pairs, English-Hindi and English-Marathi. For clustering the words, monolingual data of Indian Languages Corpora Initiative (ILCI)2 was used. It should also be noted that sentiment annotated data was also included in the data used for the word clusterings process. For Brown clustering, an implementation by Liang (2005) was used. Cross-lingual clustering for CLSA 2http://sanskrit.jnu.ac.in/ilci/index. jsp was implemented as directed in T¨ackstr¨om et al. (2012). Monolingual SA: For experiments in English, two polarity datasets were used. The first one (En-TD) by Ye et al. (2009) contains userwritten reviews on travel destinations. The dataset consists of approximately 600 positive and 591 negative reviews. Reviews were also manually sense annotated using WordNet 2.1. The sense annotation was performed by two annotators with an inter-annotation agreement of 93%. The second dataset (En-PD)3 on product reviews </context>
</contexts>
<marker>Liang, 2005</marker>
<rawString>Percy Liang. 2005. Semi-supervised learning for natural language. M. eng. thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chenghua Lin</author>
<author>Yulan He</author>
</authors>
<title>Joint sentiment/topic model for sentiment analysis.</title>
<date>2009</date>
<booktitle>In Proceedings of CIKM 2009,</booktitle>
<pages>375--384</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="7518" citStr="Lin and He, 2009" startWordPosition="1120" endWordPosition="1123">for monolingual SA. In section 4, alternative CLSA approaches based on word clustering are elucidated. Experimental details are explained in section 5. Results and discussions are presented in section 6 and section 7 respectively. Finally, section 8 concludes the paper pointing to some future research possibilities. 2 Related Work The problem of SA at document level is defined as the classification of document into different polarity classes (positive and negative) (Turney, 2002). Both supervised (Benamara et al., 2007; Martineau and Finin, 2009) and unsupervised approaches (Mei et al., 2007; Lin and He, 2009) exist for this task. Supervised approaches are popular because of their superior classification accuracy (Mullen and Collier, 2004; Pang and Lee, 2008). Feature engineering plays an important role in these systems. Apart from the commonly used bag-of-words features based on unigrams/bigrams/ngrams (Dave et al., 2003; Ng et al., 2006; Martineau and Finin, 2009), 1Hindi and Marathi belong to the Indo-Aryan subgroup of the Indo-European language family and are two widely spoken Indian languages with a speaker population of 450 million and 72 million respectively. 413 syntax (Matsumoto et al., 20</context>
</contexts>
<marker>Lin, He, 2009</marker>
<rawString>Chenghua Lin and Yulan He. 2009. Joint sentiment/topic model for sentiment analysis. In Proceedings of CIKM 2009, pages 375–384, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bin Lu</author>
<author>Chenhao Tan</author>
<author>Claire Cardie</author>
<author>Benjamin K Tsou</author>
</authors>
<title>Joint bilingual sentiment classification with unlabeled parallel corpora.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL-HLT 2011,</booktitle>
<pages>320--330</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="9033" citStr="Lu et al., 2011" startWordPosition="1353" endWordPosition="1356">ntiment labels are attached at the cluster level. Zhai et al. (2011) attempts to cluster features of a product to perform sentiment analysis on product reviews. In this work, word clusters (syntagmatic and paradigmatic) encoding a mixture of syntactic and semantic information are used for feature engineering. In situations where labeled data is not present in a language, approaches based on cross-lingual sentiment analysis are used. Most often these methods depend on an intermediary machine translation system (Wan, 2009; Brooke et al., 2009) or a bilingual dictionary (Ghorbel and Jacot, 2011; Lu et al., 2011) to bridge the language gap. Given the subtle and different ways the sentiment can be expressed which itself manifested as a result of cultural diversity amongst different languages, an MT system has to be of a superior quality to capture them. 3 Clustering for Sentiment Analysis The goal of this paper, to remind the reader, is to investigate whether superior word cluster features based on manually crafted and fine grained lexical resource like WordNet can be replaced with the syntagmatic property based word clusters created from unlabelled monolingual corpora. In this section, different clust</context>
</contexts>
<marker>Lu, Tan, Cardie, Tsou, 2011</marker>
<rawString>Bin Lu, Chenhao Tan, Claire Cardie, and Benjamin K. Tsou. 2011. Joint bilingual sentiment classification with unlabeled parallel corpora. In Proceedings of ACL-HLT 2011, pages 320–330, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sven Martin</author>
<author>Jrg Liermann</author>
<author>Hermann Ney</author>
</authors>
<title>Algorithms for bigram and trigram word clustering. In Speech Communication,</title>
<date>1995</date>
<pages>1253--1256</pages>
<contexts>
<context position="3111" citStr="Martin et al., 1995" startWordPosition="452" endWordPosition="455">red to word based features. Syntagmatic analysis of text concentrates on the surface properties of the text. Compared to paradigmatic property extraction, syntagmatic processing is relatively light weight. One of the obvious syntagmas is words, and words are grouped into equivalence classes or clusters, thus reducing the model parameters of a statistical NLP system (Brown et al., 1992). When used as an additional feature with word based language models, it has been shown to improve the system performance viz., machine translation (Uszkoreit and Brants, 2008; Stymne, 2012), speech recognition (Martin et al., 1995; Samuelsson and Reichl, 1999), dependency parsing (Koo et al., 2008; Haffari et al., 2011; Zhang and Nivre, 2011; Tratz and Hovy, 2011) and NER (Miller et al., 2004; Faruqui and Pad´o, 2010; Turian et al., 2010; T¨ackstr¨om et al., 2012). In this paper, the focus is on alleviating the data sparsity faced by supervised approaches for SA through the means of cluster based features. As WordNets are essentially word 412 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 412–422, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Ling</context>
</contexts>
<marker>Martin, Liermann, Ney, 1995</marker>
<rawString>Sven Martin, Jrg Liermann, and Hermann Ney. 1995. Algorithms for bigram and trigram word clustering. In Speech Communication, pages 1253–1256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justin Martineau</author>
<author>Tim Finin</author>
</authors>
<title>Delta TFIDF: An improved feature space for sentiment analysis.</title>
<date>2009</date>
<booktitle>In Proceedings ofICWSM.</booktitle>
<contexts>
<context position="7453" citStr="Martineau and Finin, 2009" startWordPosition="1109" endWordPosition="1112">ns different word cluster based features employed to reduce data sparsity for monolingual SA. In section 4, alternative CLSA approaches based on word clustering are elucidated. Experimental details are explained in section 5. Results and discussions are presented in section 6 and section 7 respectively. Finally, section 8 concludes the paper pointing to some future research possibilities. 2 Related Work The problem of SA at document level is defined as the classification of document into different polarity classes (positive and negative) (Turney, 2002). Both supervised (Benamara et al., 2007; Martineau and Finin, 2009) and unsupervised approaches (Mei et al., 2007; Lin and He, 2009) exist for this task. Supervised approaches are popular because of their superior classification accuracy (Mullen and Collier, 2004; Pang and Lee, 2008). Feature engineering plays an important role in these systems. Apart from the commonly used bag-of-words features based on unigrams/bigrams/ngrams (Dave et al., 2003; Ng et al., 2006; Martineau and Finin, 2009), 1Hindi and Marathi belong to the Indo-Aryan subgroup of the Indo-European language family and are two widely spoken Indian languages with a speaker population of 450 mill</context>
</contexts>
<marker>Martineau, Finin, 2009</marker>
<rawString>Justin Martineau and Tim Finin. 2009. Delta TFIDF: An improved feature space for sentiment analysis. In Proceedings ofICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shotaro Matsumoto</author>
<author>Hiroya Takamura</author>
<author>Manabu Okumura</author>
</authors>
<title>Sentiment classification using word sub-sequences and dependency sub-trees.</title>
<date>2005</date>
<booktitle>In Advances in Knowledge Discovery and Data Mining, Lecture Notes in Computer Science,</booktitle>
<pages>301--311</pages>
<contexts>
<context position="8120" citStr="Matsumoto et al., 2005" startWordPosition="1210" endWordPosition="1213">7; Lin and He, 2009) exist for this task. Supervised approaches are popular because of their superior classification accuracy (Mullen and Collier, 2004; Pang and Lee, 2008). Feature engineering plays an important role in these systems. Apart from the commonly used bag-of-words features based on unigrams/bigrams/ngrams (Dave et al., 2003; Ng et al., 2006; Martineau and Finin, 2009), 1Hindi and Marathi belong to the Indo-Aryan subgroup of the Indo-European language family and are two widely spoken Indian languages with a speaker population of 450 million and 72 million respectively. 413 syntax (Matsumoto et al., 2005; Nakagawa et al., 2010), semantic (Balamurali et al., 2011) and negation (Ikeda et al., 2008) have also been explored for this task. There has been research related to clustering and sentiment analysis. In Rooney et al. (2011), documents are clustered based on the context of each document and sentiment labels are attached at the cluster level. Zhai et al. (2011) attempts to cluster features of a product to perform sentiment analysis on product reviews. In this work, word clusters (syntagmatic and paradigmatic) encoding a mixture of syntactic and semantic information are used for feature engin</context>
</contexts>
<marker>Matsumoto, Takamura, Okumura, 2005</marker>
<rawString>Shotaro Matsumoto, Hiroya Takamura, and Manabu Okumura. 2005. Sentiment classification using word sub-sequences and dependency sub-trees. In Advances in Knowledge Discovery and Data Mining, Lecture Notes in Computer Science, pages 301– 311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiaozhu Mei</author>
<author>Xu Ling</author>
<author>Matthew Wondra</author>
<author>Hang Su</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Topic sentiment mixture: modeling facets and opinions in weblogs.</title>
<date>2007</date>
<booktitle>In Proceedings of WWW</booktitle>
<pages>171--180</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="7499" citStr="Mei et al., 2007" startWordPosition="1116" endWordPosition="1119">uce data sparsity for monolingual SA. In section 4, alternative CLSA approaches based on word clustering are elucidated. Experimental details are explained in section 5. Results and discussions are presented in section 6 and section 7 respectively. Finally, section 8 concludes the paper pointing to some future research possibilities. 2 Related Work The problem of SA at document level is defined as the classification of document into different polarity classes (positive and negative) (Turney, 2002). Both supervised (Benamara et al., 2007; Martineau and Finin, 2009) and unsupervised approaches (Mei et al., 2007; Lin and He, 2009) exist for this task. Supervised approaches are popular because of their superior classification accuracy (Mullen and Collier, 2004; Pang and Lee, 2008). Feature engineering plays an important role in these systems. Apart from the commonly used bag-of-words features based on unigrams/bigrams/ngrams (Dave et al., 2003; Ng et al., 2006; Martineau and Finin, 2009), 1Hindi and Marathi belong to the Indo-Aryan subgroup of the Indo-European language family and are two widely spoken Indian languages with a speaker population of 450 million and 72 million respectively. 413 syntax (M</context>
</contexts>
<marker>Mei, Ling, Wondra, Su, Zhai, 2007</marker>
<rawString>Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and ChengXiang Zhai. 2007. Topic sentiment mixture: modeling facets and opinions in weblogs. In Proceedings of WWW 2007, pages 171–180, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>Jethran Guinness</author>
<author>Alex Zamanian</author>
</authors>
<title>Name tagging with word clusters and discriminative training.</title>
<date>2004</date>
<booktitle>In Proceedings ofHLT-IAACL 2004: Main Proceedings,</booktitle>
<pages>337--342</pages>
<location>Boston, Massachusetts, USA.</location>
<contexts>
<context position="3276" citStr="Miller et al., 2004" startWordPosition="480" endWordPosition="483">c processing is relatively light weight. One of the obvious syntagmas is words, and words are grouped into equivalence classes or clusters, thus reducing the model parameters of a statistical NLP system (Brown et al., 1992). When used as an additional feature with word based language models, it has been shown to improve the system performance viz., machine translation (Uszkoreit and Brants, 2008; Stymne, 2012), speech recognition (Martin et al., 1995; Samuelsson and Reichl, 1999), dependency parsing (Koo et al., 2008; Haffari et al., 2011; Zhang and Nivre, 2011; Tratz and Hovy, 2011) and NER (Miller et al., 2004; Faruqui and Pad´o, 2010; Turian et al., 2010; T¨ackstr¨om et al., 2012). In this paper, the focus is on alleviating the data sparsity faced by supervised approaches for SA through the means of cluster based features. As WordNets are essentially word 412 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 412–422, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics clusters wherein words with the same meaning are clubbed together, they address the problem of data sparsity at word level. The abstraction and dimensionalit</context>
</contexts>
<marker>Miller, Guinness, Zamanian, 2004</marker>
<rawString>Scott Miller, Jethran Guinness, and Alex Zamanian. 2004. Name tagging with word clusters and discriminative training. In Proceedings ofHLT-IAACL 2004: Main Proceedings, pages 337–342, Boston, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Einat Minkov</author>
<author>Kristina Toutanova</author>
<author>Hisami Suzuki</author>
</authors>
<title>Generating complex morphology for machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL 2007,</booktitle>
<pages>128--135</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1352" citStr="Minkov et al., 2007" startWordPosition="185" endWordPosition="188">entiment analysis, both monolingual and cross-lingual, is addressed through the means of clustering. Experiments show that cluster based data sparsity reduction leads to performance better than sense based classification for sentiment analysis at document level. Similar idea is applied to Cross Lingual Sentiment Analysis (CLSA), and it is shown that reduction in data sparsity (after translation or bilingual-mapping) produces accuracy higher than Machine Translation based CLSA and sense based CLSA. 1 Introduction Data sparsity is the bane of Natural Language Processing (NLP) (Xue et al., 2005; Minkov et al., 2007). Language units encountered in the test data but absent in the training data severely degrade the performance of an NLP task. NLP applications innovatively handle data sparsity through various means. A special, but very common kind of data sparsity viz., word sparsity, can be addressed in one of the two obvious ways: 1) sparsity reduction through paradigmatically related words or 2) sparsity reduction through syntagmatically related words. Paradigmatic analysis of text is the analysis of concepts embedded in the text (Cruse, 1986; Chandler, 2012). WordNet is a byproduct of such an analysis. I</context>
</contexts>
<marker>Minkov, Toutanova, Suzuki, 2007</marker>
<rawString>Einat Minkov, Kristina Toutanova, and Hisami Suzuki. 2007. Generating complex morphology for machine translation. In Proceedings of ACL 2007, pages 128–135, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajat Mohanty</author>
<author>Pushpak Bhattacharyya</author>
<author>Prabhakar Pande</author>
<author>Shraddha Kalele</author>
<author>Mitesh Khapra</author>
<author>Aditya Sharma</author>
</authors>
<title>Synset based multilingual dictionary: Insights, applications and challenges.</title>
<date>2008</date>
<booktitle>In Proceedings of Global Wordnet Conference.</booktitle>
<contexts>
<context position="13008" citStr="Mohanty et al., 2008" startWordPosition="1977" endWordPosition="1980"> from a bilingual parallel corpus and these linkages can be used to bridge the language gap for CLSA. In this section, three approaches using clusters as features for CLSA are compared. The language whose annotated data is used for training is called the source language (S), while the language whose documents are to be sentiment classified is referred to as the target language (T). 4.1 Approach1: Projection based on Sense (PS) In this approach, a Multidict is used to bridge the language gap for SA. A Multidict is an instance of WordNet where the same sense from different languages are linked (Mohanty et al., 2008). An entry in the multidict will have a WordNet sense identifier from S and the corresponding WordNet sense identifier from T. The approach of projection based on sense is explained in Algorithm 1. Note that after the Sense Mark operation, each document will be represented as a vector of WordNet sense identifiers. Algorithm 1 Projection based on sense Input: Polarity labeled data in source language (S) and data in target language (T) to be labeled Output: Classified documents 1: Sense mark the polarity labeled data from S 2: Project the sense marked corpora from S to T using a Multidict 3: Mod</context>
</contexts>
<marker>Mohanty, Bhattacharyya, Pande, Kalele, Khapra, Sharma, 2008</marker>
<rawString>Rajat Mohanty, Pushpak Bhattacharyya, Prabhakar Pande, Shraddha Kalele, Mitesh Khapra, and Aditya Sharma. 2008. Synset based multilingual dictionary: Insights, applications and challenges. In Proceedings of Global Wordnet Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony Mullen</author>
<author>Nigel Collier</author>
</authors>
<title>Sentiment analysis using support vector machines with diverse information sources.</title>
<date>2004</date>
<booktitle>In Proceedings ofEMILP 2004,</booktitle>
<pages>412--418</pages>
<location>Barcelona,</location>
<contexts>
<context position="7649" citStr="Mullen and Collier, 2004" startWordPosition="1138" endWordPosition="1141">are explained in section 5. Results and discussions are presented in section 6 and section 7 respectively. Finally, section 8 concludes the paper pointing to some future research possibilities. 2 Related Work The problem of SA at document level is defined as the classification of document into different polarity classes (positive and negative) (Turney, 2002). Both supervised (Benamara et al., 2007; Martineau and Finin, 2009) and unsupervised approaches (Mei et al., 2007; Lin and He, 2009) exist for this task. Supervised approaches are popular because of their superior classification accuracy (Mullen and Collier, 2004; Pang and Lee, 2008). Feature engineering plays an important role in these systems. Apart from the commonly used bag-of-words features based on unigrams/bigrams/ngrams (Dave et al., 2003; Ng et al., 2006; Martineau and Finin, 2009), 1Hindi and Marathi belong to the Indo-Aryan subgroup of the Indo-European language family and are two widely spoken Indian languages with a speaker population of 450 million and 72 million respectively. 413 syntax (Matsumoto et al., 2005; Nakagawa et al., 2010), semantic (Balamurali et al., 2011) and negation (Ikeda et al., 2008) have also been explored for this t</context>
</contexts>
<marker>Mullen, Collier, 2004</marker>
<rawString>Tony Mullen and Nigel Collier. 2004. Sentiment analysis using support vector machines with diverse information sources. In Proceedings ofEMILP 2004, pages 412–418, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuji Nakagawa</author>
<author>Kentaro Inui</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Dependency tree-based sentiment classification using crfs with hidden variables.</title>
<date>2010</date>
<booktitle>In Proceedings ofHLT-IAACL 2010,</booktitle>
<pages>786--794</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="8144" citStr="Nakagawa et al., 2010" startWordPosition="1214" endWordPosition="1217">st for this task. Supervised approaches are popular because of their superior classification accuracy (Mullen and Collier, 2004; Pang and Lee, 2008). Feature engineering plays an important role in these systems. Apart from the commonly used bag-of-words features based on unigrams/bigrams/ngrams (Dave et al., 2003; Ng et al., 2006; Martineau and Finin, 2009), 1Hindi and Marathi belong to the Indo-Aryan subgroup of the Indo-European language family and are two widely spoken Indian languages with a speaker population of 450 million and 72 million respectively. 413 syntax (Matsumoto et al., 2005; Nakagawa et al., 2010), semantic (Balamurali et al., 2011) and negation (Ikeda et al., 2008) have also been explored for this task. There has been research related to clustering and sentiment analysis. In Rooney et al. (2011), documents are clustered based on the context of each document and sentiment labels are attached at the cluster level. Zhai et al. (2011) attempts to cluster features of a product to perform sentiment analysis on product reviews. In this work, word clusters (syntagmatic and paradigmatic) encoding a mixture of syntactic and semantic information are used for feature engineering. In situations wh</context>
</contexts>
<marker>Nakagawa, Inui, Kurohashi, 2010</marker>
<rawString>Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi. 2010. Dependency tree-based sentiment classification using crfs with hidden variables. In Proceedings ofHLT-IAACL 2010, pages 786–794, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Sajib Dasgupta</author>
<author>S M Niaz Arifin</author>
</authors>
<title>Examining the role of linguistic knowledge sources in the automatic identification and classification of reviews.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLIIG</booktitle>
<pages>611--618</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7853" citStr="Ng et al., 2006" startWordPosition="1169" endWordPosition="1172">problem of SA at document level is defined as the classification of document into different polarity classes (positive and negative) (Turney, 2002). Both supervised (Benamara et al., 2007; Martineau and Finin, 2009) and unsupervised approaches (Mei et al., 2007; Lin and He, 2009) exist for this task. Supervised approaches are popular because of their superior classification accuracy (Mullen and Collier, 2004; Pang and Lee, 2008). Feature engineering plays an important role in these systems. Apart from the commonly used bag-of-words features based on unigrams/bigrams/ngrams (Dave et al., 2003; Ng et al., 2006; Martineau and Finin, 2009), 1Hindi and Marathi belong to the Indo-Aryan subgroup of the Indo-European language family and are two widely spoken Indian languages with a speaker population of 450 million and 72 million respectively. 413 syntax (Matsumoto et al., 2005; Nakagawa et al., 2010), semantic (Balamurali et al., 2011) and negation (Ikeda et al., 2008) have also been explored for this task. There has been research related to clustering and sentiment analysis. In Rooney et al. (2011), documents are clustered based on the context of each document and sentiment labels are attached at the c</context>
</contexts>
<marker>Ng, Dasgupta, Arifin, 2006</marker>
<rawString>Vincent Ng, Sajib Dasgupta, and S. M. Niaz Arifin. 2006. Examining the role of linguistic knowledge sources in the automatic identification and classification of reviews. In Proceedings of the COLIIG 2006, pages 611–618, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="19589" citStr="Och and Ney, 2003" startWordPosition="3052" endWordPosition="3055">itive and 75 negative reviews. Apart from being marked with polarity labels at document level, they are also manually sense annotated using Hindi and Marathi WordNet respectively. CLSA: The same datasets used in SA are also used for CLSA. Three approaches (as described in section 4) were tested for English-Hindi and English-Marathi language pairs. To create alignments, English-Hindi and English-Marathi parallel corpora from ILCI were used. EnglishHindi parallel corpus contains 45992 sentences and English-Marathi parallel corpus contains 47881 sentences. To create alignments, GIZA++5 was used (Och and Ney, 2003). As a preprocessing step, all stop words were removed. Stemming was performed on English and Hindi whereas for Marathi data, Morphological Analyzer was used to reduce the words to their respective lemmas. All experiments were performed using C-SVM 3http://www.cs.jhu.edu/˜mdredze/ datasets/sentiment/ 4http://www.cfilt.iitb.ac. in/resources/senti/MPLC_tour_ downloaderInfo.php 5http://www-i6.informatik.rwth-aachen. de/Colleagues/och/software/GIZA++.html 416 Features En-TD En-PD Hi Mar Words 87.02 77.60 77.36 92.28 WordNet Sense (Paradigmatic) 89.13 74.50 85.80 96.88 Clusters (Syntagmatic) 97.45 </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Thumbs up? sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of EMILP 2002,</booktitle>
<pages>79--86</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2307" citStr="Pang and Lee, 2002" startWordPosition="330" endWordPosition="333">ction through paradigmatically related words or 2) sparsity reduction through syntagmatically related words. Paradigmatic analysis of text is the analysis of concepts embedded in the text (Cruse, 1986; Chandler, 2012). WordNet is a byproduct of such an analysis. In WordNet, paradigms are manually generated based on the principles of lexical and semantic relationship among words (Fellbaum, 1998). WordNets are primarily used to address the problem of word sense disambiguation. However, at present there are many NLP applications which use WordNet. One such application is Sentiment Analysis (SA) (Pang and Lee, 2002). Recent research has shown that word sense based semantic features can improve the performance of SA systems (Rentoumi et al., 2009; Tamara et al., 2010; Balamurali et al., 2011) compared to word based features. Syntagmatic analysis of text concentrates on the surface properties of the text. Compared to paradigmatic property extraction, syntagmatic processing is relatively light weight. One of the obvious syntagmas is words, and words are grouped into equivalence classes or clusters, thus reducing the model parameters of a statistical NLP system (Brown et al., 1992). When used as an additiona</context>
</contexts>
<marker>Pang, Lee, 2002</marker>
<rawString>Bo Pang and Lillian Lee. 2002. Thumbs up? sentiment classification using machine learning techniques. In Proceedings of EMILP 2002, pages 79– 86, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="7670" citStr="Pang and Lee, 2008" startWordPosition="1142" endWordPosition="1145">. Results and discussions are presented in section 6 and section 7 respectively. Finally, section 8 concludes the paper pointing to some future research possibilities. 2 Related Work The problem of SA at document level is defined as the classification of document into different polarity classes (positive and negative) (Turney, 2002). Both supervised (Benamara et al., 2007; Martineau and Finin, 2009) and unsupervised approaches (Mei et al., 2007; Lin and He, 2009) exist for this task. Supervised approaches are popular because of their superior classification accuracy (Mullen and Collier, 2004; Pang and Lee, 2008). Feature engineering plays an important role in these systems. Apart from the commonly used bag-of-words features based on unigrams/bigrams/ngrams (Dave et al., 2003; Ng et al., 2006; Martineau and Finin, 2009), 1Hindi and Marathi belong to the Indo-Aryan subgroup of the Indo-European language family and are two widely spoken Indian languages with a speaker population of 450 million and 72 million respectively. 413 syntax (Matsumoto et al., 2005; Nakagawa et al., 2010), semantic (Balamurali et al., 2011) and negation (Ikeda et al., 2008) have also been explored for this task. There has been r</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1-2):1–135, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vassiliki Rentoumi</author>
<author>George Giannakopoulos</author>
<author>Vangelis Karkaletsis</author>
<author>George A Vouros</author>
</authors>
<title>Sentiment analysis of figurative language using a word sense disambiguation approach.</title>
<date>2009</date>
<booktitle>In Proceedings of RAILP 2009,</booktitle>
<pages>370--375</pages>
<location>Borovets, Bulgaria,</location>
<contexts>
<context position="2439" citStr="Rentoumi et al., 2009" startWordPosition="351" endWordPosition="354"> of text is the analysis of concepts embedded in the text (Cruse, 1986; Chandler, 2012). WordNet is a byproduct of such an analysis. In WordNet, paradigms are manually generated based on the principles of lexical and semantic relationship among words (Fellbaum, 1998). WordNets are primarily used to address the problem of word sense disambiguation. However, at present there are many NLP applications which use WordNet. One such application is Sentiment Analysis (SA) (Pang and Lee, 2002). Recent research has shown that word sense based semantic features can improve the performance of SA systems (Rentoumi et al., 2009; Tamara et al., 2010; Balamurali et al., 2011) compared to word based features. Syntagmatic analysis of text concentrates on the surface properties of the text. Compared to paradigmatic property extraction, syntagmatic processing is relatively light weight. One of the obvious syntagmas is words, and words are grouped into equivalence classes or clusters, thus reducing the model parameters of a statistical NLP system (Brown et al., 1992). When used as an additional feature with word based language models, it has been shown to improve the system performance viz., machine translation (Uszkoreit </context>
</contexts>
<marker>Rentoumi, Giannakopoulos, Karkaletsis, Vouros, 2009</marker>
<rawString>Vassiliki Rentoumi, George Giannakopoulos, Vangelis Karkaletsis, and George A. Vouros. 2009. Sentiment analysis of figurative language using a word sense disambiguation approach. In Proceedings of RAILP 2009, pages 370–375, Borovets, Bulgaria, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Niall Rooney</author>
<author>Hui Wang</author>
<author>Fiona Browne</author>
<author>Fergal Monaghan</author>
<author>Jann Mller</author>
<author>Alan Sergeant</author>
<author>Zhiwei Lin</author>
<author>Philip Taylor</author>
<author>Vladimir Dobrynin</author>
</authors>
<title>An exploration into the use of contextual document clustering for cluster sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings ofRAILP 2011,</booktitle>
<pages>140--145</pages>
<location>Hissar, Bulgaria.</location>
<contexts>
<context position="8347" citStr="Rooney et al. (2011)" startWordPosition="1247" endWordPosition="1250">stems. Apart from the commonly used bag-of-words features based on unigrams/bigrams/ngrams (Dave et al., 2003; Ng et al., 2006; Martineau and Finin, 2009), 1Hindi and Marathi belong to the Indo-Aryan subgroup of the Indo-European language family and are two widely spoken Indian languages with a speaker population of 450 million and 72 million respectively. 413 syntax (Matsumoto et al., 2005; Nakagawa et al., 2010), semantic (Balamurali et al., 2011) and negation (Ikeda et al., 2008) have also been explored for this task. There has been research related to clustering and sentiment analysis. In Rooney et al. (2011), documents are clustered based on the context of each document and sentiment labels are attached at the cluster level. Zhai et al. (2011) attempts to cluster features of a product to perform sentiment analysis on product reviews. In this work, word clusters (syntagmatic and paradigmatic) encoding a mixture of syntactic and semantic information are used for feature engineering. In situations where labeled data is not present in a language, approaches based on cross-lingual sentiment analysis are used. Most often these methods depend on an intermediary machine translation system (Wan, 2009; Bro</context>
</contexts>
<marker>Rooney, Wang, Browne, Monaghan, Mller, Sergeant, Lin, Taylor, Dobrynin, 2011</marker>
<rawString>Niall Rooney, Hui Wang, Fiona Browne, Fergal Monaghan, Jann Mller, Alan Sergeant, Zhiwei Lin, Philip Taylor, and Vladimir Dobrynin. 2011. An exploration into the use of contextual document clustering for cluster sentiment analysis. In Proceedings ofRAILP 2011, pages 140–145, Hissar, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Samuelsson</author>
<author>W Reichl</author>
</authors>
<title>A class-based language model for large-vocabulary speech recognition extracted from part-of-speech statistics.</title>
<date>1999</date>
<booktitle>In Proceedings ofICASSP</booktitle>
<pages>537--540</pages>
<contexts>
<context position="3141" citStr="Samuelsson and Reichl, 1999" startWordPosition="456" endWordPosition="459">tures. Syntagmatic analysis of text concentrates on the surface properties of the text. Compared to paradigmatic property extraction, syntagmatic processing is relatively light weight. One of the obvious syntagmas is words, and words are grouped into equivalence classes or clusters, thus reducing the model parameters of a statistical NLP system (Brown et al., 1992). When used as an additional feature with word based language models, it has been shown to improve the system performance viz., machine translation (Uszkoreit and Brants, 2008; Stymne, 2012), speech recognition (Martin et al., 1995; Samuelsson and Reichl, 1999), dependency parsing (Koo et al., 2008; Haffari et al., 2011; Zhang and Nivre, 2011; Tratz and Hovy, 2011) and NER (Miller et al., 2004; Faruqui and Pad´o, 2010; Turian et al., 2010; T¨ackstr¨om et al., 2012). In this paper, the focus is on alleviating the data sparsity faced by supervised approaches for SA through the means of cluster based features. As WordNets are essentially word 412 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 412–422, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics clusters wherein words</context>
</contexts>
<marker>Samuelsson, Reichl, 1999</marker>
<rawString>C. Samuelsson and W. Reichl. 1999. A class-based language model for large-vocabulary speech recognition extracted from part-of-speech statistics. In Proceedings ofICASSP 1999, pages 537–540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Stymne</author>
</authors>
<title>Clustered word classes for preordering in statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings ofthe Joint Workshop on Unsupervised and Semi-Supervised Learning in ILP,</booktitle>
<pages>28--34</pages>
<contexts>
<context position="3070" citStr="Stymne, 2012" startWordPosition="448" endWordPosition="449">010; Balamurali et al., 2011) compared to word based features. Syntagmatic analysis of text concentrates on the surface properties of the text. Compared to paradigmatic property extraction, syntagmatic processing is relatively light weight. One of the obvious syntagmas is words, and words are grouped into equivalence classes or clusters, thus reducing the model parameters of a statistical NLP system (Brown et al., 1992). When used as an additional feature with word based language models, it has been shown to improve the system performance viz., machine translation (Uszkoreit and Brants, 2008; Stymne, 2012), speech recognition (Martin et al., 1995; Samuelsson and Reichl, 1999), dependency parsing (Koo et al., 2008; Haffari et al., 2011; Zhang and Nivre, 2011; Tratz and Hovy, 2011) and NER (Miller et al., 2004; Faruqui and Pad´o, 2010; Turian et al., 2010; T¨ackstr¨om et al., 2012). In this paper, the focus is on alleviating the data sparsity faced by supervised approaches for SA through the means of cluster based features. As WordNets are essentially word 412 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 412–422, Sofia, Bulgaria, August 4-9 2013. </context>
</contexts>
<marker>Stymne, 2012</marker>
<rawString>Sara Stymne. 2012. Clustered word classes for preordering in statistical machine translation. In Proceedings ofthe Joint Workshop on Unsupervised and Semi-Supervised Learning in ILP, pages 28–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Ryan McDonald</author>
<author>Jakob Uszkoreit</author>
</authors>
<title>Cross-lingual Word Clusters for Direct Transfer of Linguistic Structure.</title>
<date>2012</date>
<booktitle>In Proceedings of IAACL-HLT 2012,</booktitle>
<pages>477--487</pages>
<location>Montr´eal, Canada.</location>
<marker>T¨ackstr¨om, McDonald, Uszkoreit, 2012</marker>
<rawString>Oscar T¨ackstr¨om, Ryan McDonald, and Jakob Uszkoreit. 2012. Cross-lingual Word Clusters for Direct Transfer of Linguistic Structure. In Proceedings of IAACL-HLT 2012, pages 477–487, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Tamara</author>
<author>Balahur Alexandra</author>
<author>Montoyo Andres</author>
</authors>
<title>Word sense disambiguation in opinion mining: Pros and cons.</title>
<date>2010</date>
<journal>Journal Research in Computing Science,</journal>
<pages>46--119</pages>
<contexts>
<context position="2460" citStr="Tamara et al., 2010" startWordPosition="355" endWordPosition="358">s of concepts embedded in the text (Cruse, 1986; Chandler, 2012). WordNet is a byproduct of such an analysis. In WordNet, paradigms are manually generated based on the principles of lexical and semantic relationship among words (Fellbaum, 1998). WordNets are primarily used to address the problem of word sense disambiguation. However, at present there are many NLP applications which use WordNet. One such application is Sentiment Analysis (SA) (Pang and Lee, 2002). Recent research has shown that word sense based semantic features can improve the performance of SA systems (Rentoumi et al., 2009; Tamara et al., 2010; Balamurali et al., 2011) compared to word based features. Syntagmatic analysis of text concentrates on the surface properties of the text. Compared to paradigmatic property extraction, syntagmatic processing is relatively light weight. One of the obvious syntagmas is words, and words are grouped into equivalence classes or clusters, thus reducing the model parameters of a statistical NLP system (Brown et al., 1992). When used as an additional feature with word based language models, it has been shown to improve the system performance viz., machine translation (Uszkoreit and Brants, 2008; Sty</context>
</contexts>
<marker>Tamara, Alexandra, Andres, 2010</marker>
<rawString>Martin Tamara, Balahur Alexandra, and Montoyo Andres. 2010. Word sense disambiguation in opinion mining: Pros and cons. Journal Research in Computing Science, 46:119–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Tratz</author>
<author>Eduard Hovy</author>
</authors>
<title>A fast, accurate, non-projective, semantically-enriched parser.</title>
<date>2011</date>
<booktitle>In Proceedings of EMILP 2011,</booktitle>
<pages>1257--1268</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3247" citStr="Tratz and Hovy, 2011" startWordPosition="474" endWordPosition="477">property extraction, syntagmatic processing is relatively light weight. One of the obvious syntagmas is words, and words are grouped into equivalence classes or clusters, thus reducing the model parameters of a statistical NLP system (Brown et al., 1992). When used as an additional feature with word based language models, it has been shown to improve the system performance viz., machine translation (Uszkoreit and Brants, 2008; Stymne, 2012), speech recognition (Martin et al., 1995; Samuelsson and Reichl, 1999), dependency parsing (Koo et al., 2008; Haffari et al., 2011; Zhang and Nivre, 2011; Tratz and Hovy, 2011) and NER (Miller et al., 2004; Faruqui and Pad´o, 2010; Turian et al., 2010; T¨ackstr¨om et al., 2012). In this paper, the focus is on alleviating the data sparsity faced by supervised approaches for SA through the means of cluster based features. As WordNets are essentially word 412 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 412–422, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics clusters wherein words with the same meaning are clubbed together, they address the problem of data sparsity at word level. The </context>
</contexts>
<marker>Tratz, Hovy, 2011</marker>
<rawString>Stephen Tratz and Eduard Hovy. 2011. A fast, accurate, non-projective, semantically-enriched parser. In Proceedings of EMILP 2011, pages 1257–1268, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL 2010,</booktitle>
<pages>384--394</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3322" citStr="Turian et al., 2010" startWordPosition="488" endWordPosition="491">f the obvious syntagmas is words, and words are grouped into equivalence classes or clusters, thus reducing the model parameters of a statistical NLP system (Brown et al., 1992). When used as an additional feature with word based language models, it has been shown to improve the system performance viz., machine translation (Uszkoreit and Brants, 2008; Stymne, 2012), speech recognition (Martin et al., 1995; Samuelsson and Reichl, 1999), dependency parsing (Koo et al., 2008; Haffari et al., 2011; Zhang and Nivre, 2011; Tratz and Hovy, 2011) and NER (Miller et al., 2004; Faruqui and Pad´o, 2010; Turian et al., 2010; T¨ackstr¨om et al., 2012). In this paper, the focus is on alleviating the data sparsity faced by supervised approaches for SA through the means of cluster based features. As WordNets are essentially word 412 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 412–422, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics clusters wherein words with the same meaning are clubbed together, they address the problem of data sparsity at word level. The abstraction and dimensionality reduction thus achieved attributes to the su</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of ACL 2010, pages 384–394, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL 2002,</booktitle>
<pages>417--424</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7385" citStr="Turney, 2002" startWordPosition="1101" endWordPosition="1102">lows: section 2 presents related work. Section 3 explains different word cluster based features employed to reduce data sparsity for monolingual SA. In section 4, alternative CLSA approaches based on word clustering are elucidated. Experimental details are explained in section 5. Results and discussions are presented in section 6 and section 7 respectively. Finally, section 8 concludes the paper pointing to some future research possibilities. 2 Related Work The problem of SA at document level is defined as the classification of document into different polarity classes (positive and negative) (Turney, 2002). Both supervised (Benamara et al., 2007; Martineau and Finin, 2009) and unsupervised approaches (Mei et al., 2007; Lin and He, 2009) exist for this task. Supervised approaches are popular because of their superior classification accuracy (Mullen and Collier, 2004; Pang and Lee, 2008). Feature engineering plays an important role in these systems. Apart from the commonly used bag-of-words features based on unigrams/bigrams/ngrams (Dave et al., 2003; Ng et al., 2006; Martineau and Finin, 2009), 1Hindi and Marathi belong to the Indo-Aryan subgroup of the Indo-European language family and are two </context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter D. Turney. 2002. Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews. In Proceedings of ACL 2002, pages 417–424, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Uszkoreit</author>
<author>Thorsten Brants</author>
</authors>
<title>Distributed word clustering for large scale class-based language modeling in machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-HLT 2008,</booktitle>
<pages>755--762</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="3055" citStr="Uszkoreit and Brants, 2008" startWordPosition="444" endWordPosition="447"> al., 2009; Tamara et al., 2010; Balamurali et al., 2011) compared to word based features. Syntagmatic analysis of text concentrates on the surface properties of the text. Compared to paradigmatic property extraction, syntagmatic processing is relatively light weight. One of the obvious syntagmas is words, and words are grouped into equivalence classes or clusters, thus reducing the model parameters of a statistical NLP system (Brown et al., 1992). When used as an additional feature with word based language models, it has been shown to improve the system performance viz., machine translation (Uszkoreit and Brants, 2008; Stymne, 2012), speech recognition (Martin et al., 1995; Samuelsson and Reichl, 1999), dependency parsing (Koo et al., 2008; Haffari et al., 2011; Zhang and Nivre, 2011; Tratz and Hovy, 2011) and NER (Miller et al., 2004; Faruqui and Pad´o, 2010; Turian et al., 2010; T¨ackstr¨om et al., 2012). In this paper, the focus is on alleviating the data sparsity faced by supervised approaches for SA through the means of cluster based features. As WordNets are essentially word 412 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 412–422, Sofia, Bulgaria, Au</context>
<context position="15865" citStr="Uszkoreit and Brants (2008)" startWordPosition="2457" endWordPosition="2460">oss-lingual clustering. In cross-lingual clustering, the objective function maximizes the joint likelihood of monolingual and cross-lingual factors. Given a list of words and clusters it belongs to, a clustering algorithm tries to obtain word-cluster association which maximizes the joint likelihood of words and clusters. Whereas in case of crosslingual clustering, the same clustering can be explained in terms of maximizing the likelihood of monolingual word-cluster pairs of the source, the target and alignments between them. Formally, as stated in T¨ackstr¨om et al. (2012), Using the model of Uszkoreit and Brants (2008), the likelihood of a sequence of word tokens, w = [wj]mj=1, with wj E V , can be factored as, m L(w; C) = H p(wj|C(wj))p(C(wj)|wj−1)) j=1 (3) Note this is different from the likelihood estimation of Brown et al. (1992) (Equation (1)), where C(wj) was conditioned on C(wj−1). This LC(l) = argmax E t k∈αT|S ∪ αS|T s.t.CT (wTk )=t CS (wSak )=l 415 makes the computation easier as suggested in the original paper. The Equation (3) in a cross lingual setting will be transformed as given below: LS,T (wS, wT ; αT|S, αS|T ,CS, CT ) = LS(...).LT (...).LT |S(...).LS|T (...) (4) Here, LT |S(...) and LS|T (</context>
</contexts>
<marker>Uszkoreit, Brants, 2008</marker>
<rawString>Jakob Uszkoreit and Thorsten Brants. 2008. Distributed word clustering for large scale class-based language modeling in machine translation. In Proceedings ofACL-HLT 2008, pages 755–762, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
</authors>
<title>Co-training for cross-lingual sentiment classification.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL 2009,</booktitle>
<pages>235--243</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5158" citStr="Wan, 2009" startWordPosition="755" endWordPosition="756">subsume the paradigmatic analysis. In the current work, this particular insight is used to solve the data sparsity problem in the sentiment analysis by leveraging unlabelled monolingual corpora. Specifically, experiments are performed to investigate whether features developed from manually crafted clusterings (coming from WordNet) can be replaced by those generated from clustering based on syntagmatic properties. Further, cluster based features are used to address the problem of scarcity of sentiment annotated data in a language. Popular approaches for Cross-Lingual Sentiment Analysis (CLSA) (Wan, 2009; Duh et al., 2011) depend on Machine Translation (MT) for converting the labeled data from one language to the other (Hiroshi et al., 2004; Banea et al., 2008; Wan, 2009). However, many languages which are truly resource scarce, do not have an MT system or existing MT systems are not ripe to be used for CLSA (Balamurali et al., 2013). To perform CLSA, this study leverages unlabelled parallel corpus to generate the word alignments. These word alignments are then used to link cluster based features to obliterate the language gap for performing SA. No MT systems or bilingual dictionaries are use</context>
<context position="8942" citStr="Wan, 2009" startWordPosition="1339" endWordPosition="1340"> et al. (2011), documents are clustered based on the context of each document and sentiment labels are attached at the cluster level. Zhai et al. (2011) attempts to cluster features of a product to perform sentiment analysis on product reviews. In this work, word clusters (syntagmatic and paradigmatic) encoding a mixture of syntactic and semantic information are used for feature engineering. In situations where labeled data is not present in a language, approaches based on cross-lingual sentiment analysis are used. Most often these methods depend on an intermediary machine translation system (Wan, 2009; Brooke et al., 2009) or a bilingual dictionary (Ghorbel and Jacot, 2011; Lu et al., 2011) to bridge the language gap. Given the subtle and different ways the sentiment can be expressed which itself manifested as a result of cultural diversity amongst different languages, an MT system has to be of a superior quality to capture them. 3 Clustering for Sentiment Analysis The goal of this paper, to remind the reader, is to investigate whether superior word cluster features based on manually crafted and fine grained lexical resource like WordNet can be replaced with the syntagmatic property based </context>
</contexts>
<marker>Wan, 2009</marker>
<rawString>Xiaojun Wan. 2009. Co-training for cross-lingual sentiment classification. In Proceedings of ACL 2009, pages 235–243, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gui-Rong Xue</author>
<author>Chenxi Lin</author>
<author>Qiang Yang</author>
<author>WenSi Xi</author>
<author>Hua-Jun Zeng</author>
<author>Yong Yu</author>
<author>Zheng Chen</author>
</authors>
<title>Scalable collaborative filtering using cluster-based smoothing.</title>
<date>2005</date>
<booktitle>In Proceedings of SIGIR</booktitle>
<pages>114--121</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="1330" citStr="Xue et al., 2005" startWordPosition="181" endWordPosition="184">data sparsity in sentiment analysis, both monolingual and cross-lingual, is addressed through the means of clustering. Experiments show that cluster based data sparsity reduction leads to performance better than sense based classification for sentiment analysis at document level. Similar idea is applied to Cross Lingual Sentiment Analysis (CLSA), and it is shown that reduction in data sparsity (after translation or bilingual-mapping) produces accuracy higher than Machine Translation based CLSA and sense based CLSA. 1 Introduction Data sparsity is the bane of Natural Language Processing (NLP) (Xue et al., 2005; Minkov et al., 2007). Language units encountered in the test data but absent in the training data severely degrade the performance of an NLP task. NLP applications innovatively handle data sparsity through various means. A special, but very common kind of data sparsity viz., word sparsity, can be addressed in one of the two obvious ways: 1) sparsity reduction through paradigmatically related words or 2) sparsity reduction through syntagmatically related words. Paradigmatic analysis of text is the analysis of concepts embedded in the text (Cruse, 1986; Chandler, 2012). WordNet is a byproduct </context>
</contexts>
<marker>Xue, Lin, Yang, Xi, Zeng, Yu, Chen, 2005</marker>
<rawString>Gui-Rong Xue, Chenxi Lin, Qiang Yang, WenSi Xi, Hua-Jun Zeng, Yong Yu, and Zheng Chen. 2005. Scalable collaborative filtering using cluster-based smoothing. In Proceedings of SIGIR 2005, pages 114–121, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiang Ye</author>
<author>Ziqiong Zhang</author>
<author>Rob Law</author>
</authors>
<title>Sentiment classification of online reviews to travel destinations by supervised machine learning approaches. Expert Systems with Applications,</title>
<date>2009</date>
<pages>36--3</pages>
<contexts>
<context position="18040" citStr="Ye et al. (2009)" startWordPosition="2821" endWordPosition="2824">A was performed on two language pairs, English-Hindi and English-Marathi. For clustering the words, monolingual data of Indian Languages Corpora Initiative (ILCI)2 was used. It should also be noted that sentiment annotated data was also included in the data used for the word clusterings process. For Brown clustering, an implementation by Liang (2005) was used. Cross-lingual clustering for CLSA 2http://sanskrit.jnu.ac.in/ilci/index. jsp was implemented as directed in T¨ackstr¨om et al. (2012). Monolingual SA: For experiments in English, two polarity datasets were used. The first one (En-TD) by Ye et al. (2009) contains userwritten reviews on travel destinations. The dataset consists of approximately 600 positive and 591 negative reviews. Reviews were also manually sense annotated using WordNet 2.1. The sense annotation was performed by two annotators with an inter-annotation agreement of 93%. The second dataset (En-PD)3 on product reviews (music instruments) from Amazon by Blitzer et al. (2007) contains 1000 positive and 1000 negative reviews. This dataset was sense annotated using an automatic WSD engine which was trained on tourism domain (Khapra et al., 2010). Experiments using this dataset were</context>
</contexts>
<marker>Ye, Zhang, Law, 2009</marker>
<rawString>Qiang Ye, Ziqiong Zhang, and Rob Law. 2009. Sentiment classification of online reviews to travel destinations by supervised machine learning approaches. Expert Systems with Applications, 36(3, Part 2):6527–6535.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongwu Zhai</author>
<author>Bing Liu</author>
<author>Hua Xu</author>
<author>Peifa Jia</author>
</authors>
<title>Clustering product features for opinion mining.</title>
<date>2011</date>
<booktitle>In Proceedings of WSDM 2011,</booktitle>
<pages>347--354</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="8485" citStr="Zhai et al. (2011)" startWordPosition="1270" endWordPosition="1273">d Finin, 2009), 1Hindi and Marathi belong to the Indo-Aryan subgroup of the Indo-European language family and are two widely spoken Indian languages with a speaker population of 450 million and 72 million respectively. 413 syntax (Matsumoto et al., 2005; Nakagawa et al., 2010), semantic (Balamurali et al., 2011) and negation (Ikeda et al., 2008) have also been explored for this task. There has been research related to clustering and sentiment analysis. In Rooney et al. (2011), documents are clustered based on the context of each document and sentiment labels are attached at the cluster level. Zhai et al. (2011) attempts to cluster features of a product to perform sentiment analysis on product reviews. In this work, word clusters (syntagmatic and paradigmatic) encoding a mixture of syntactic and semantic information are used for feature engineering. In situations where labeled data is not present in a language, approaches based on cross-lingual sentiment analysis are used. Most often these methods depend on an intermediary machine translation system (Wan, 2009; Brooke et al., 2009) or a bilingual dictionary (Ghorbel and Jacot, 2011; Lu et al., 2011) to bridge the language gap. Given the subtle and di</context>
</contexts>
<marker>Zhai, Liu, Xu, Jia, 2011</marker>
<rawString>Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia. 2011. Clustering product features for opinion mining. In Proceedings of WSDM 2011, pages 347–354, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Joakim Nivre</author>
</authors>
<title>Transitionbased dependency parsing with rich non-local features.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL-HLT 2011,</booktitle>
<pages>188--193</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3224" citStr="Zhang and Nivre, 2011" startWordPosition="470" endWordPosition="473">mpared to paradigmatic property extraction, syntagmatic processing is relatively light weight. One of the obvious syntagmas is words, and words are grouped into equivalence classes or clusters, thus reducing the model parameters of a statistical NLP system (Brown et al., 1992). When used as an additional feature with word based language models, it has been shown to improve the system performance viz., machine translation (Uszkoreit and Brants, 2008; Stymne, 2012), speech recognition (Martin et al., 1995; Samuelsson and Reichl, 1999), dependency parsing (Koo et al., 2008; Haffari et al., 2011; Zhang and Nivre, 2011; Tratz and Hovy, 2011) and NER (Miller et al., 2004; Faruqui and Pad´o, 2010; Turian et al., 2010; T¨ackstr¨om et al., 2012). In this paper, the focus is on alleviating the data sparsity faced by supervised approaches for SA through the means of cluster based features. As WordNets are essentially word 412 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 412–422, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics clusters wherein words with the same meaning are clubbed together, they address the problem of data spars</context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Yue Zhang and Joakim Nivre. 2011. Transitionbased dependency parsing with rich non-local features. In Proceedings ofACL-HLT 2011, pages 188– 193, Stroudsburg, PA, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>