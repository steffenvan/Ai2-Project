<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000007">
<title confidence="0.9985355">
Fast Syntactic Analysis for Statistical Language Modeling
via Substructure Sharing and Uptraining
</title>
<author confidence="0.998962">
Ariya Rastrow, Mark Dredze, Sanjeev Khudanpur
</author>
<affiliation confidence="0.885039">
Human Language Technology Center of Excellence
Center for Language and Speech Processing, Johns Hopkins University
</affiliation>
<address confidence="0.909988">
Baltimore, MD USA
</address>
<email confidence="0.99942">
{ariya,mdredze,khudanpur}@jhu.edu
</email>
<sectionHeader confidence="0.995649" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999675157894737">
Long-span features, such as syntax, can im-
prove language models for tasks such as
speech recognition and machine translation.
However, these language models can be dif-
ficult to use in practice because of the time
required to generate features for rescoring a
large hypothesis set. In this work, we pro-
pose substructure sharing, which saves dupli-
cate work in processing hypothesis sets with
redundant hypothesis structures. We apply
substructure sharing to a dependency parser
and part of speech tagger to obtain significant
speedups, and further improve the accuracy
of these tools through up-training. When us-
ing these improved tools in a language model
for speech recognition, we obtain significant
speed improvements with both N-best and hill
climbing rescoring, and show that up-training
leads to WER reduction.
</bodyText>
<sectionHeader confidence="0.99913" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998786340425532">
Language models (LM) are crucial components in
tasks that require the generation of coherent natu-
ral language text, such as automatic speech recog-
nition (ASR) and machine translation (MT). While
traditional LMs use word n-grams, where the n − 1
previous words predict the next word, newer mod-
els integrate long-span information in making deci-
sions. For example, incorporating long-distance de-
pendencies and syntactic structure can help the LM
better predict words by complementing the predic-
tive power of n-grams (Chelba and Jelinek, 2000;
Collins et al., 2005; Filimonov and Harper, 2009;
Kuo et al., 2009).
The long-distance dependencies can be modeled
in either a generative or a discriminative framework.
Discriminative models, which directly distinguish
correct from incorrect hypothesis, are particularly
attractive because they allow the inclusion of arbi-
trary features (Kuo et al., 2002; Roark et al., 2007;
Collins et al., 2005); these models with syntactic in-
formation have obtained state of the art results.
However, both generative and discriminative LMs
with long-span dependencies can be slow, for they
often cannot work directly with lattices and require
rescoring large N-best lists (Khudanpur and Wu,
2000; Collins et al., 2005; Kuo et al., 2009). For dis-
criminative models, this limitation applies to train-
ing as well. Moreover, the non-local features used in
rescoring are usually extracted via auxiliary tools –
which in the case of syntactic features include part of
speech taggers and parsers – from a set of ASR sys-
tem hypotheses. Separately applying auxiliary tools
to each N-best list hypothesis leads to major ineffi-
ciencies as many hypotheses differ only slightly.
Recent work on hill climbing algorithms for ASR
lattice rescoring iteratively searches for a higher-
scoring hypothesis in a local neighborhood of the
current-best hypothesis, leading to a much more ef-
ficient algorithm in terms of the number, N, of hy-
potheses evaluated (Rastrow et al., 2011b); the idea
also leads to a discriminative hill climbing train-
ing algorithm (Rastrow et al., 2011a). Even so, the
reliance on auxiliary tools slow LM application to
the point of being impractical for real time systems.
While faster auxiliary tools are an option, they are
usually less accurate.
In this paper, we propose a general modifica-
</bodyText>
<page confidence="0.981773">
175
</page>
<note confidence="0.9857695">
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 175–183,
Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.99995625">
tion to the decoders used in auxiliary tools to uti-
lize the commonalities among the set of generated
hypotheses. The key idea is to share substructure
states in transition based structured prediction al-
gorithms, i.e. algorithms where final structures are
composed of a sequence of multiple individual deci-
sions. We demonstrate our approach on a local Per-
ceptron based part of speech tagger (Tsuruoka et al.,
2011) and a shift reduce dependency parser (Sagae
and Tsujii, 2007), yielding significantly faster tag-
ging and parsing of ASR hypotheses. While these
simpler structured prediction models are faster, we
compensate for the model’s simplicity through up-
training (Petrov et al., 2010), yielding auxiliary tools
that are both fast and accurate. The result is signif-
icant speed improvements and a reduction in word
error rate (WER) for both N-best list and the al-
ready fast hill climbing rescoring. The net result
is arguably the first syntactic LM fast enough to be
used in a real time ASR system.
</bodyText>
<sectionHeader confidence="0.980747" genericHeader="method">
2 Syntactic Language Models
</sectionHeader>
<bodyText confidence="0.999607175">
There have been several approaches to include syn-
tactic information in both generative and discrimi-
native language models.
For generative LMs, the syntactic information
must be part of the generative process. Structured
language modeling incorporates syntactic parse
trees to identify the head words in a hypothesis for
modeling dependencies beyond n-grams. Chelba
and Jelinek (2000) extract the two previous exposed
head words at each position in a hypothesis, along
with their non-terminal tags, and use them as con-
text for computing the probability of the current po-
sition. Khudanpur and Wu (2000) exploit such syn-
tactic head word dependencies as features in a maxi-
mum entropy framework. Kuo et al. (2009) integrate
syntactic features into a neural network LM for Ara-
bic speech recognition.
Discriminative models are more flexible since
they can include arbitrary features, allowing for
a wider range of long-span syntactic dependen-
cies. Additionally, discriminative models are di-
rectly trained to resolve the acoustic confusion in the
decoded hypotheses of an ASR system. This flexi-
bility and training regime translate into better perfor-
mance. Collins et al. (2005) uses the Perceptron al-
gorithm to train a global linear discriminative model
which incorporates long-span features, such as head-
to-head dependencies and part of speech tags.
Our Language Model. We work with a discrimi-
native LM with long-span dependencies. We use a
global linear model with Perceptron training. We
rescore the hypotheses (lattices) generated by the
ASR decoder—in a framework most similar to that
of Rastrow et al. (2011a).
The LM score S(w, a) for each hypothesis w of
a speech utterance with acoustic sequence a is based
on the baseline ASR system score b(w, a) (initial n-
gram LM score and the acoustic score) and α0, the
weight assigned to the baseline score.1 The score is
defined as:
</bodyText>
<equation confidence="0.96188325">
S(w, a) = α0 · b(w, a) + F(w, s1, ... , sm)
d
= α0 · b(w, a) + αi · 45i(w, s1,...,sm)
i=1
</equation>
<bodyText confidence="0.999905875">
where F is the discriminative LM’s score for the
hypothesis w, and s1, ... , sm are candidate syntac-
tic structures associated with w, as discussed be-
low. Since we use a linear model, the score is a
weighted linear combination of the count of acti-
vated features of the word sequence w and its as-
sociated structures: 45i(w, s1, ... , sm). Perceptron
training learns the parameters α. The baseline score
b(w, a) can be a feature, yielding the dot product
notation: S(w, a) = (α, 45(a, w, s1, ... , sm)) Our
LM uses features from the dependency tree and part
of speech (POS) tag sequence. We use the method
described in Kuo et al. (2009) to identify the two
previous exposed head words, h−2, h−1, at each po-
sition i in the input hypothesis and include the fol-
lowing syntactic based features into our LM:
</bodyText>
<listItem confidence="0.982938">
1. (h−2.w o h−1.w o wi) , (h−1.w o wi) , (wi)
2. (h−2.t o h−1.t o ti) , (h−1.t o ti) , (ti) , (tiwi)
</listItem>
<bodyText confidence="0.995441">
where h.w and h.t denote the word identity and the
POS tag of the corresponding exposed head word.
</bodyText>
<subsectionHeader confidence="0.99439">
2.1 Hill Climbing Rescoring
</subsectionHeader>
<bodyText confidence="0.999916333333333">
We adopt the so called hill climbing framework of
Rastrow et al. (2011b) to improve both training and
rescoring time as much as possible by reducing the
</bodyText>
<footnote confidence="0.870424">
1We tune α0 on development data (Collins et al., 2005).
</footnote>
<page confidence="0.998432">
176
</page>
<bodyText confidence="0.99953925">
number N of explored hypotheses. We summarize
it below for completeness.
Given a speech utterance’s lattice G from a first
pass ASR decoder, the neighborhood N(w, i) of a
hypothesis w = w1 w2 ... w,,, at position i is de-
fined as the set of all paths in the lattice that may
be obtained by editing wi: deleting it, substituting
it, or inserting a word to its left. In other words,
it is the “distance-1-at-position i” neighborhood of
w. Given a position i in a word sequence w, all
hypotheses in N(w, i) are rescored using the long-
span model and the hypothesis V(i) with the high-
est score becomes the new w. The process is re-
peated with a new position – scanned left to right
– until w = w0(1) = ... = w0(n), i.e. when w
itself is the highest scoring hypothesis in all its 1-
neighborhoods, and can not be furthered improved
using the model. Incorporating this into training
yields a discriminative hill climbing algorithm (Ras-
trow et al., 2011a).
</bodyText>
<sectionHeader confidence="0.976917" genericHeader="method">
3 Incorporating Syntactic Structures
</sectionHeader>
<bodyText confidence="0.999709545454545">
Long-span models – generative or discriminative,
N-best or hill climbing – rely on auxiliary tools,
such as a POS tagger or a parser, for extracting
features for each hypothesis during rescoring, and
during training for discriminative models. The top-
m candidate structures associated with the ith hy-
pothesis, which we denote as s1i,...,smi , are gener-
ated by these tools and used to score the hypothesis:
F(wi, s1i , ... , smi ). For example, si can be a part of
speech tag or a syntactic dependency. We formally
define this sequential processing as:
</bodyText>
<equation confidence="0.99497">
1 m LM 1 m
w1 tool(s) —* s1, ... , s1 F(w1, s1, ... , s1 )
tool(s), m LM 1
w2 S2, ... , s2 F(w2, s2, ... ,s 2 m )
...
tool(s) 1 m LM1 m
sk, ... , sk F(wk, sk, ... ,s
k
</equation>
<bodyText confidence="0.959852714285714">
)
Here, {w1, ... , wk} represents a set of ASR output
hypotheses that need to be rescored. For each hy-
pothesis, we apply an external tool (e.g. parser) to
generate associated structures s1i , ... , smi (e.g. de-
pendencies.) These are then passed to the language
model along with the word sequence for scoring.
</bodyText>
<subsectionHeader confidence="0.999841">
3.1 Substructure Sharing
</subsectionHeader>
<bodyText confidence="0.999932543478261">
While long-span LMs have been empirically shown
to improve WER over n-gram LMs, the computa-
tional burden prohibits long-span LMs in practice,
particularly in real-time systems. A major complex-
ity factor is due to processing 100s or 1000s of hy-
potheses for each speech utterance, even during hill
climbing, each of which must be POS tagged and
parsed. However, the candidate hypotheses of an
utterance share equivalent substructures, especially
in hill climbing methods due to the locality present
in the neighborhood generation. Figure 1 demon-
strates such repetition in an N-best list (N=10) and
a hill climbing neighborhood hypothesis set for a
speech utterance from broadcast news. For exam-
ple, the word “ENDORSE” occurs within the same
local context in all hypotheses and should receive
the same part of speech tag in each case. Processing
each hypothesis separately wastes time.
We propose a general algorithmic approach to re-
duce the complexity of processing a hypothesis set
by sharing common substructures among the hy-
potheses. Critically, unlike many lattice parsing al-
gorithms, our approach is general and produces ex-
act output. We first present our approach and then
demonstrate its generality by applying it to a depen-
dency parser and part of speech tagger.
We work with structured prediction models that
produce output from a series of local decisions: a
transition model. We begin in initial state π0 and
terminate in a possible final state πf. All states
along the way are chosen from the possible states
H. A transition (or action) ω E Q advances the
decoder from state to state, where the transition ωi
changes the state from πi to πi+1. The sequence
of states {π0 ... πi, πi+1 ... πf} can be mapped to
an output (the model’s prediction.) The choice of
action ω is given by a learning algorithm, such as
a maximum-entropy classifier, support vector ma-
chine or Perceptron, trained on labeled data. Given
the previous k actions up to πi, the classifier g :
H x Qk —* R|� |assigns a score to each possi-
ble action, which we can interpret as a probability:
pg(ωi πi, ωi−1ωi−2 ... ωi−k). These actions are ap-
plied to transition to new states πi+1. We note that
state definitions can encode the k previous actions,
which simplifies the probability to pg(ωi πi). The
</bodyText>
<equation confidence="0.316983">
wk
</equation>
<page confidence="0.817756">
177
</page>
<table confidence="0.341259">
N-best list Hill climbing neighborhood
</table>
<listItem confidence="0.996380153846154">
(1) AL GORE HAS PROMISED THAT HE WOULD ENDORSE A CANDIDATE
(2) TO AL GORE HAS PROMISED THAT HE WOULD ENDORSE A CANDIDATE
(3) AL GORE HAS PROMISE THAT HE WOULD ENDORSE A CANDIDATE
(4) SO AL GORE HAS PROMISED THAT HE WOULD ENDORSE A CANDIDATE
(5) IT’S AL GORE HAS PROMISED THAT HE WOULD ENDORSE A CANDIDATE
(1) YEAH FIFTY CENT GALLON NOMINATION WHICH WAS GREAT
(2) YEAH FIFTY CENT A GALLON NOMINATION WHICH WAS GREAT
(6) AL GORE HAS PROMISED HE WOULD ENDORSE A CANDIDATE
(3) YEAH FIFTY CENT GOT A NOMINATION WHICH WAS GREAT
(7) AL GORE HAS PROMISED THAT HE WOULD ENDORSE THE CANDIDATE
(8) SAID AL GORE HAS PROMISED THAT HE WOULD ENDORSE A CANDIDATE
(9) AL GORE HAS PROMISED THAT HE WOULD ENDORSE A CANDIDATE FOR
(10) AL GORE HIS PROMISE THAT HE WOULD ENDORSE A CANDIDATE
</listItem>
<figureCaption confidence="0.999729">
Figure 1: Example of repeated substructures in candidate hypotheses.
</figureCaption>
<bodyText confidence="0.98149">
score of the new state is then
</bodyText>
<equation confidence="0.999717">
p(7ri+1) = pg(Wi17ri) &apos; p(7ri) (1)
</equation>
<bodyText confidence="0.996818">
Classification decisions require a feature represen-
tation of 7ri, which is provided by feature functions
f : 11 —* Y, that map states to features. Features are
conjoined with actions for multi-class classification,
so pg(Wi17ri) = pg(f(7r) o Wi), where o is a conjunc-
tion operation. In this way, states can be summarized
by features.
Equivalent states are defined as two states 7r and
7r0 with an identical feature representation:
</bodyText>
<equation confidence="0.68272">
7r - 7r0 iff f(7r) = f(7r0)
</equation>
<bodyText confidence="0.999383">
If two states are equivalent, then g imposes the same
distribution over actions. We can benefit from this
substructure redundancy, both within and between
hypotheses, by saving these distributions in mem-
ory, sharing a distribution computed just once across
equivalent states. A similar idea of equivalent states
is used by Huang and Sagae (2010), except they use
equivalence to facilitate dynamic programming for
shift-reduce parsing, whereas we generalize it for
improving the processing time of similar hypotheses
in general models. Following Huang and Sagae, we
define kernel features as the smallest set of atomic
features �f(7r) such that,
</bodyText>
<equation confidence="0.998985">
�f(7r) = �f(7r0) 7r - 7r0. (2)
</equation>
<bodyText confidence="0.965684333333333">
Equivalent distributions are stored in a hash table
H : 11 —* QxR; the hash keys are the states and the
values are distributions2 over actions: {W, pg(WI7r)}.
</bodyText>
<footnote confidence="0.761906333333333">
2For pure greedy search (deterministic search) we need only
retain the best action, since the distribution is only used in prob-
abilistic search, such as beam search or best-first algorithms.
</footnote>
<bodyText confidence="0.9982175">
H caches equivalent states in a hypothesis set and re-
sets for each new utterance. For each state, we first
check H for equivalent states before computing the
action distribution; each cache hit reduces decod-
ing time. Distributing hypotheses wi across differ-
ent CPU threads is another way to obtain speedups,
and we can still benefit from substructure sharing by
storing H in shared memory.
</bodyText>
<equation confidence="0.9365235">
We use h(7r) = �|�f(�)|
i�1 int(�fi(7r)) as the hash
</equation>
<bodyText confidence="0.984555882352941">
function, where int(�fi(7r)) is an integer mapping of
the ith kernel feature. For integer typed features
the mapping is trivial, for string typed features (e.g.
a POS tag identity) we use a mapping of the cor-
responding vocabulary to integers. We empirically
found that this hash function is very effective and
yielded very few collisions.
To apply substructure sharing to a transition based
model, we need only define the set of states 11 (in-
cluding 7r0 and 7rf), actions Q and kernel feature
functions f. The resulting speedup depends on the
amount of substructure duplication among the hy-
potheses, which we will show is significant for ASR
lattice rescoring. Note that our algorithm is not an
approximation; we obtain the same output {si } as
we would without any sharing. We now apply this
algorithm to dependency parsing and POS tagging.
</bodyText>
<subsectionHeader confidence="0.996322">
3.2 Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.71161225">
We use the best-first probabilistic shift-reduce de-
pendency parser of Sagae and Tsujii (2007), a
transition-based parser
et al., 2009) with a
MaxEnt classifier. Dependency trees are built by
processing the words left-to-right and the classifier
assigns a distribution over the actions at each step.
States are defined as 7r
</bodyText>
<equation confidence="0.4428885">
(K¨ubler
={S, Q}: S is a stack of
</equation>
<page confidence="0.88766">
178
</page>
<table confidence="0.999917916666667">
Kernel features f(7r) for state 7r = {S, Q}
S = s0, s1, ... &amp; Q = q0, q1, ...
(1) s0.w s0.t s0.r (5) ts0−1
s0.lch.t s0.lch.r ts1+1
s0.rch.t s0.rch.r
(2) s1.w s1.t s1.r (6) dist(s0, s1)
s1.lch.t s1.lch.r dist(q0, s0)
s1.rch.t s1.rch.r
(3) s2.w s2.t s2.r
(4) q0.w q0.t (7) s0.nch
q1.w q1.t s1.nch
q2.w
</table>
<tableCaption confidence="0.947991333333333">
Table 1: Kernel features for defining parser states. si.w
denotes the head-word in a subtree and t its POS tag.
si.lch and si.rch are the leftmost and rightmost children
</tableCaption>
<bodyText confidence="0.761811666666667">
of a subtree. si.r is the dependency label that relates a
subtree head-word to its dependent. si.nch is the number
of children of a subtree. qi.w and qi.t are the word and
its POS tag in the queue. dist(so,sl) is the linear distance
between the head-words of so and sl.
subtrees s0, s1, ... (s0 is the top tree) and Q are
words in the input word sequence. The initial state is
π0 = {0, {w0, w1, ...}}, and final states occur when
Q is empty and S contains a single tree (the output).
</bodyText>
<listItem confidence="0.942258333333333">
Q is determined by the set of dependency labels
r E R and one of three transition types:
• Shift: remove the head of Q (wj) and place it on
the top of S as a singleton tree (only wj.)
• Reduce-Left,: replace the top two trees in S (s0
and s1) with a tree formed by making the root of
s1 a dependent of the root of s0 with label r.
• Reduce-Right,: same as Reduce-Left, except re-
verses s0 and s1.
</listItem>
<bodyText confidence="0.999482866666667">
Table 1 shows the kernel features used in our de-
pendency parser. See Sagae and Tsujii (2007) for a
complete list of features.
Goldberg and Elhadad (2010) observed that pars-
ing time is dominated by feature extraction and
score calculation. Substructure sharing reduces
these steps for equivalent states, which are persis-
tent throughout a candidate set. Note that there are
far fewer kernel features than total features, hence
the hash function calculation is very fast.
We summarize substructure sharing for depen-
dency parsing in Algorithm 1. We extend the def-
inition of states to be {S, Q, p} where p denotes the
score of the state: the probability of the action se-
quence that resulted in the current state. Also, fol-
</bodyText>
<figure confidence="0.851181272727273">
Algorithm 1 Best-first shift-reduce dependency parsing
w input hypothesis
S0 = 0, Q0 = w, p0 = 1
π0 {S0, Q0, p0I [initial state]
H —Hash table (H — 0 X R)
Heap— Heap for prioritizing states and performing best-first search
Heap.push(π0) [initialize the heap]
while Heap =� 0 do
πcurrent �Heap.pop() [the best state so far]
if πcurrent = πf [if final state]
return πcurrent [terminate if final state]
else if H.find(πcurrent)
ActList — H[πcurrent] [retrieve action list from the hash table]
else [need to construct action list]
for all ω E 0 [for all actions]
p� &apos; i pg (ω Iπcurrent) [action score]
ActList.insert({ω, pwI)
H.insert(πcurrent, ActList) [Store the action list into hash table]
end if
for all {ω, p�I E ActList [compute new states]
πnew &apos; πcurrent X ω
Heap.push(πnew) [push to the heap]
</figure>
<subsectionHeader confidence="0.577448">
end while
</subsectionHeader>
<bodyText confidence="0.999472285714286">
lowing Sagae and Tsujii (2007) a heap is used to
maintain states prioritized by their scores, for apply-
ing the best-first strategy. For each step, a state from
the top of the heap is considered and all actions (and
scores) are either retrieved from H or computed us-
ing g.3 We use πnew &apos;-- πcurrent X ω to denote the
operation of extending a state by an action ω E Q4.
</bodyText>
<subsectionHeader confidence="0.999711">
3.3 Part of Speech Tagging
</subsectionHeader>
<bodyText confidence="0.96873996">
We use the part of speech (POS) tagger of Tsuruoka
et al. (2011), a transition based model with a Per-
ceptron and a lookahead heuristic process. The tag-
ger processes w left to right. States are defined as
πi = {ci, w}: a sequence of assigned tags up to wi
(ci = t1t2 ... ti−1) and the word sequence w. Q is
defined simply as the set of possible POS tags (T)
that can be applied. The final state is reached once
all the positions are tagged. For f we use the features
of Tsuruoka et al. (2011). The kernel features are
�f(πi) = {ti−2, ti−1, wi−2, wi−1, wi, wi+1, wi+2}.
While the tagger extracts prefix and suffix features,
it suffices to look at wi for determining state equiv-
alence. The tagger is deterministic (greedy) in that
it only considers the best tag at each step, so we do
not store scores. However, this tagger uses a depth-
3 Sagae and Tsujii (2007) use a beam strategy to increase
speed. Search space pruning is achieved by filtering heap states
for probability greater than 1b the probability of the most likely
state in the heap with the same number of actions. We use b =
100 for our experiments.
4We note that while we have demonstrated substructure
sharing for dependency parsing, the same improvements can
be made to a shift-reduce constituent parser (Sagae and Lavie,
2006).
</bodyText>
<page confidence="0.998337">
179
</page>
<figureCaption confidence="0.992309">
Figure 2: POS tagger with lookahead search of d=1. At
wi the search considers the current state and next state.
</figureCaption>
<bodyText confidence="0.998788428571429">
first search lookahead procedure to select the best
action at each step, which considers future decisions
up to depth d5. An example for d = 1 is shown
in Figure 2. Using d = 1 for the lookahead search
strategy, we modify the kernel features since the de-
cision for wi is affected by the state 7ri+1. The kernel
features in position i should be �f(7ri) U �f(7ri+1):
</bodyText>
<equation confidence="0.9782805">
�f(7ri) =
{ti−2, ti−1, wi−2, wi−1, wi, wi+1, wi+2, wi+3l
</equation>
<sectionHeader confidence="0.863395" genericHeader="method">
4 Up-Training
</sectionHeader>
<bodyText confidence="0.99473941025641">
While we have fast decoding algorithms for the pars-
ing and tagging, the simpler underlying models can
lead to worse performance. Using more complex
models with higher accuracy is impractical because
they are slow. Instead, we seek to improve the accu-
racy of our fast tools.
To achieve this goal we use up-training, in which
a more complex model is used to improve the accu-
racy of a simpler model. We are given two mod-
els, M1 and M2, as well as a large collection of
unlabeled text. Model M1 is slow but very accu-
rate while M2 is fast but obtains lower accuracy.
Up-training applies M1 to tag the unlabeled data,
which is then used as training data for M2. Like
self-training, a model is retrained on automatic out-
put, but here the output comes form a more accurate
model. Petrov et al. (2010) used up-training as a
domain adaptation technique: a constituent parser –
which is more robust to domain changes – was used
to label a new domain, and a fast dependency parser
5 Tsuruoka et al. (2011) shows that the lookahead search
improves the performance of the local ”history-based” models
for different NLP tasks
was trained on the automatically labeled data. We
use a similar idea where our goal is to recover the
accuracy lost from using simpler models. Note that
while up-training uses two models, it differs from
co-training since we care about improving only one
model (M2). Additionally, the models can vary in
different ways. For example, they could be the same
algorithm with different pruning methods, which
can lead to faster but less accurate models.
We apply up-training to improve the accuracy of
both our fast POS tagger and dependency parser. We
parse a large corpus of text with a very accurate but
very slow constituent parser and use the resulting
data to up-train our tools. We will demonstrate em-
pirically that up-training improves these fast models
to yield better WER results.
</bodyText>
<sectionHeader confidence="0.999888" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.9992948">
The idea of efficiently processing a hypothesis set is
similar to “lattice-parsing”, in which a parser con-
sider an entire lattice at once (Hall, 2005; Chep-
palier et al., 1999). These methods typically con-
strain the parsing space using heuristics, which are
often model specific. In other words, they search in
the joint space of word sequences present in the lat-
tice and their syntactic analyses; they are not guaran-
teed to produce a syntactic analysis for all hypothe-
ses. In contrast, substructure sharing is a general
purpose method that we have applied to two differ-
ent algorithms. The output is identical to processing
each hypothesis separately and output is generated
for each hypothesis. Hall (Hall, 2005) uses a lattice
parsing strategy which aims to compute the marginal
probabilities of all word sequences in the lattice by
summing over syntactic analyses of each word se-
quence. The parser sums over multiple parses of a
word sequence implicitly. The lattice parser there-
fore, is itself a language model. In contrast, our
tools are completely separated from the ASR sys-
tem, which allows the system to create whatever fea-
tures are needed. This independence means our tools
are useful for other tasks, such as machine transla-
tion. These differences make substructure sharing a
more attractive option for efficient algorithms.
While Huang and Sagae (2010) use the notion of
“equivalent states”, they do so for dynamic program-
ming in a shift-reduce parser to broaden the search
space. In contrast, we use the idea to identify sub-
</bodyText>
<equation confidence="0.635929428571429">
W1 W ··· Wi— w;_1 Wi Wi 1 Wi Wi
ti t ··· 4— 4-1
tsr til 11
t;
ti
t1i 1
ti 1
</equation>
<page confidence="0.985355">
180
</page>
<bodyText confidence="0.999940166666667">
structures across inputs, where our goal is efficient
parsing in general. Additionally, we extend the defi-
nition of equivalent states to general transition based
structured prediction models, and demonstrate ap-
plications beyond parsing as well as the novel setting
of hypothesis set parsing.
</bodyText>
<sectionHeader confidence="0.998672" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.99994528">
Our ASR system is based on the 2007 IBM
Speech transcription system for the GALE Distilla-
tion Go/No-go Evaluation (Chen et al., 2006) with
state of the art discriminative acoustic models. See
Table 2 for a data summary. We use a modi-
fied Kneser-Ney (KN) backoff 4-gram baseline LM.
Word-lattices for discriminative training and rescor-
ing come from this baseline ASR system.6 The long-
span discriminative LM’s baseline feature weight
(α0) is tuned on dev data and hill climbing (Rastrow
et al., 2011a) is used for training and rescoring. The
dependency parser and POS tagger are trained on su-
pervised data and up-trained on data labeled by the
CKY-style bottom-up constituent parser of Huang et
al. (2010), a state of the art broadcast news (BN)
parser, with phrase structures converted to labeled
dependencies by the Stanford converter.
While accurate, the parser has a huge grammar
(32GB) from using products of latent variable gram-
mars and requires O(l3) time to parse a sentence of
length l. Therefore, we could not use the constituent
parser for ASR rescoring since utterances can be
very long, although the shorter up-training text data
was not a problem.7 We evaluate both unlabeled
(UAS) and labeled dependency accuracy (LAS).
</bodyText>
<subsectionHeader confidence="0.875814">
6.1 Results
</subsectionHeader>
<bodyText confidence="0.9995901">
Before we demonstrate the speed of our models, we
show that up-training can produce accurate and fast
models. Figure 3 shows improvements to parser ac-
curacy through up-training for different amount of
(randomly selected) data, where the last column in-
dicates constituent parser score (91.4% UAS). We
use the POS tagger to generate tags for depen-
dency training to match the test setting. While
there is a large difference between the constituent
and dependency parser without up-training (91.4%
</bodyText>
<footnote confidence="0.99925">
6For training a 3-gram LM is used to increase confusions.
7Speech utterances are longer as they are not as effectively
sentence segmented as text.
</footnote>
<figureCaption confidence="0.9908482">
Figure 3: Up-training results for dependency parsing for
varying amounts of data (number of words.) The first
column is the dependency parser with supervised training
only and the last column is the constituent parser (after
converting to dependency trees.)
</figureCaption>
<bodyText confidence="0.974716322580645">
vs. 86.2% UAS), up-training can cut the differ-
ence by 44% to 88.5%, and improvements saturate
around 40m words (about 2m sentences.)$ The de-
pendency parser remains much smaller and faster;
the up-trained dependency model is 700MB with
6m features compared with 32GB for constituency
model. Up-training improves the POS tagger’s accu-
racy from 95.9% to 97%, when trained on the POS
tags produced by the constituent parser, which has a
tagging accuracy of 97.2% on BN.
We train the syntactic discriminative LM, with
head-word and POS tag features, using the faster
parser and tagger and then rescore the ASR hypothe-
ses. Table 3 shows the decoding speedups as well as
the WER reductions compared to the baseline LM.
Note that up-training improvements lead to WER re-
ductions. Detailed speedups on substructure sharing
are shown in Table 4; the POS tagger achieves a 5.3
times speedup, and the parser a 5.7 speedup with-
out changing the output. We also observed speedups
during training (not shown due to space.)
The above results are for the already fast hill
climbing decoding, but substructure sharing can also
be used for N-best list rescoring. Figure 4 (logarith-
mic scale) illustrates the time for the parser and tag-
ger to process N-best lists of varying size, with more
substantial speedups for larger lists. For example,
for N=100 (a typical setting) the parsing time re-
$Better performance is due to the exact CKY-style – com-
pared with best-first and beam– search and that the constituent
parser uses the product of huge self-trained grammars.
</bodyText>
<figure confidence="0.973959">
92.0
Accuracy (%) 91.0
90.0
89.0
88.0
87.0
86.0
85.0
84.0
0M 2.5M 5M 10M 20M 40M Constituent
Amount of Added Uptraining Data Parser
Unlabeled Attachment Score
Labeled Attachment Score
</figure>
<page confidence="0.903874">
181
</page>
<table confidence="0.9993042">
Usage Data Size
Acoustic model training Hub4 acoustic train 153k uttr, 400 hrs
Baseline LM training: modified KN 4-gram TDT4 closed captions+EARS BN03 closed caption 193m words
Disc. LM training: long-span w/hill climbing Hub4 (length &lt;50) 115k uttr, 2.6m words
Baseline feature (α0) tuning dev04f BN data 2.5 hrs
Supervised training: dep. parser, POS tagger Ontonotes BN treebank+ WSJ Penn treebank 1.3m words, 59k sent.
Supervised training: constituent parser Ontonotes BN treebank + WSJ Penn treebank 1.3m words, 59k sent.
Up-training: dependency parser, POS tagger TDT4 closed captions+EARS BN03 closed caption 193m words available
Evaluation: up-training BN treebank test (following Huang et al. (2010)) 20k words, 1.1k sent.
Evaluation: ASR transcription rt04 BN evaluation 4 hrs, 45k words
</table>
<tableCaption confidence="0.997378">
Table 2: A summary of the data for training and evaluation. The Ontonotes corpus is from Weischedel et al. (2008).
</tableCaption>
<figure confidence="0.99987852173913">
(a)
10000
1000
Elapsed Time (sec)
10
1 10 (N) 100 1000
N--‐best Size
1000000
100000
100
No Sharing
Substructure Sharing
(b)
Elapsed Time (sec)
10000
1000
100
10
1
1 10 (N) 100 1000
N--‐best Size
No Sharing
Substructure Sharing
</figure>
<figureCaption confidence="0.999894">
Figure 4: Elapsed time for (a) parsing and (b) POS tagging the N-best lists with and without substructure sharing.
</figureCaption>
<table confidence="0.9984944">
Substr. Share (sec)
LM WER No Yes
Baseline 4-gram 15.1 - -
Syntactic LM 14.8 8,658 1,648
+ up-train 14.6
</table>
<tableCaption confidence="0.8046435">
Table 3: Speedups and WER for hill climbing rescor-
ing. Substructure sharing yields a 5.3 times speedup. The
</tableCaption>
<bodyText confidence="0.908472625">
times for with and without up-training are nearly identi-
cal, so we include only one set for clarity. Time spent
is dominated by the parser, so the faster parser accounts
for much of the overall speedup. Timing information in-
cludes neighborhood generation and LM rescoring, so it
is more than the sum of the times in Table 4.
duces from about 20,000 seconds to 2,700 seconds,
about 7.4 times as fast.
</bodyText>
<sectionHeader confidence="0.998921" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99969475">
The computational complexity of accurate syntac-
tic processing can make structured language models
impractical for applications such as ASR that require
scoring hundreds of hypotheses per input. We have
</bodyText>
<table confidence="0.9970705">
Substr. Share Speedup
No Yes
Parser 8,237.2 1,439.5 5.7
POS tagger 213.3 40.1 5.3
</table>
<tableCaption confidence="0.950716">
Table 4: Time in seconds for the parser and POS tagger
to process hypotheses during hill climbing rescoring.
</tableCaption>
<bodyText confidence="0.9999742">
presented substructure sharing, a general framework
that greatly improves the speed of syntactic tools
that process candidate hypotheses. Furthermore, we
achieve improved performance through up-training.
The result is a large speedup in rescoring time, even
on top of the already fast hill climbing framework,
and reductions in WER from up-training. Our re-
sults make long-span syntactic LMs practical for
real-time ASR, and can potentially impact machine
translation decoding as well.
</bodyText>
<sectionHeader confidence="0.998847" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.996118666666667">
Thanks to Kenji Sagae for sharing his shift-reduce
dependency parser and the anonymous reviewers for
helpful comments.
</bodyText>
<page confidence="0.997371">
182
</page>
<sectionHeader confidence="0.995883" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999939341772152">
C. Chelba and F. Jelinek. 2000. Structured lan-
guage modeling. Computer Speech and Language,
14(4):283–332.
S. Chen, B. Kingsbury, L. Mangu, D. Povey, G. Saon,
H. Soltau, and G. Zweig. 2006. Advances in speech
transcription at IBM under the DARPA EARS pro-
gram. IEEE Transactions on Audio, Speech and Lan-
guage Processing, pages 1596–1608.
J. Cheppalier, M. Rajman, R. Aragues, and A. Rozen-
knop. 1999. Lattice parsing for speech recognition.
In Sixth Conference sur le Traitement Automatique du
Langage Naturel (TANL’99).
M Collins, B Roark, and M Saraclar. 2005. Discrimina-
tive syntactic language modeling for speech recogni-
tion. In ACL.
Denis Filimonov and Mary Harper. 2009. A joint
language model with fine-grain syntactic tags. In
EMNLP.
Yoav Goldberg and Michael Elhadad. 2010. An Ef-
ficient Algorithm for Easy-First Non-Directional De-
pendency Parsing. In Proc. HLT-NAACL, number
June, pages 742–750.
Keith B Hall. 2005. Best-first word-lattice parsing:
techniques for integrated syntactic language modeling.
Ph.D. thesis, Brown University.
L. Huang and K. Sagae. 2010. Dynamic Programming
for Linear-Time Incremental Parsing. In Proceedings
of ACL.
Zhongqiang Huang, Mary Harper, and Slav Petrov. 2010.
Self-training with Products of Latent Variable Gram-
mars. In Proc. EMNLP, number October, pages 12–
22.
S. Khudanpur and J. Wu. 2000. Maximum entropy tech-
niques for exploiting syntactic, semantic and colloca-
tional dependencies in language modeling. Computer
Speech and Language, pages 355–372.
S. K¨ubler, R. McDonald, and J. Nivre. 2009. Depen-
dency parsing. Synthesis Lectures on Human Lan-
guage Technologies, 2(1):1–127.
Hong-Kwang Jeff Kuo, Eric Fosler-Lussier, Hui Jiang,
and Chin-Hui Lee. 2002. Discriminative training of
language models for speech recognition. In ICASSP.
H. K. J. Kuo, L. Mangu, A. Emami, I. Zitouni, and
L. Young-Suk. 2009. Syntactic features for Arabic
speech recognition. In Proc. ASRU.
Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and
Hiyan Alshawi. 2010. Uptraining for accurate deter-
ministic question parsing. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 705–713, Cambridge, MA,
October. Association for Computational Linguistics.
Ariya Rastrow, Mark Dredze, and Sanjeev Khudanpur.
2011a. Efficient discrimnative training of long-span
language models. In IEEE Workshop on Automatic
Speech Recognition and Understanding (ASRU).
Ariya Rastrow, Markus Dreyer, Abhinav Sethy, San-
jeev Khudanpur, Bhuvana Ramabhadran, and Mark
Dredze. 2011b. Hill climbing on speech lattices : A
new rescoring framework. In ICASSP.
Brian Roark, Murat Saraclar, and Michael Collins. 2007.
Discriminative n-gram language modeling. Computer
Speech &amp; Language, 21(2).
K. Sagae and A. Lavie. 2006. A best-first probabilis-
tic shift-reduce parser. In Proc. ACL, pages 691–698.
Association for Computational Linguistics.
K. Sagae and J. Tsujii. 2007. Dependency parsing
and domain adaptation with LR models and parser en-
sembles. In Proc. EMNLP-CoNLL, volume 7, pages
1044–1050.
Yoshimasa Tsuruoka, Yusuke Miyao, and Jun’ichi
Kazama. 2011. Learning with Lookahead :
Can History-Based Models Rival Globally Optimized
Models ? In Proc. CoNLL, number June, pages 238–
246.
Ralph Weischedel, Sameer Pradhan, Lance Ramshaw,
Martha Palmer, Nianwen Xue, Mitchell Marcus, Ann
Taylor, Craig Greenberg, Eduard Hovy, Robert Belvin,
and Ann Houston, 2008. OntoNotes Release 2.0. Lin-
guistic Data Consortium, Philadelphia.
</reference>
<page confidence="0.999198">
183
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.777779">
<title confidence="0.9784305">Fast Syntactic Analysis for Statistical Language via Substructure Sharing and Uptraining</title>
<author confidence="0.892046">Ariya Rastrow</author>
<author confidence="0.892046">Mark Dredze</author>
<author confidence="0.892046">Sanjeev</author>
<affiliation confidence="0.952747">Human Language Technology Center of Center for Language and Speech Processing, Johns Hopkins</affiliation>
<address confidence="0.975801">Baltimore, MD</address>
<abstract confidence="0.9977467">Long-span features, such as syntax, can improve language models for tasks such as speech recognition and machine translation. However, these language models can be difficult to use in practice because of the time required to generate features for rescoring a large hypothesis set. In this work, we propose substructure sharing, which saves duplicate work in processing hypothesis sets with redundant hypothesis structures. We apply substructure sharing to a dependency parser and part of speech tagger to obtain significant speedups, and further improve the accuracy of these tools through up-training. When using these improved tools in a language model for speech recognition, we obtain significant improvements with both and hill climbing rescoring, and show that up-training leads to WER reduction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Chelba</author>
<author>F Jelinek</author>
</authors>
<title>Structured language modeling.</title>
<date>2000</date>
<journal>Computer Speech and Language,</journal>
<volume>14</volume>
<issue>4</issue>
<contexts>
<context position="1689" citStr="Chelba and Jelinek, 2000" startWordPosition="246" endWordPosition="249"> climbing rescoring, and show that up-training leads to WER reduction. 1 Introduction Language models (LM) are crucial components in tasks that require the generation of coherent natural language text, such as automatic speech recognition (ASR) and machine translation (MT). While traditional LMs use word n-grams, where the n − 1 previous words predict the next word, newer models integrate long-span information in making decisions. For example, incorporating long-distance dependencies and syntactic structure can help the LM better predict words by complementing the predictive power of n-grams (Chelba and Jelinek, 2000; Collins et al., 2005; Filimonov and Harper, 2009; Kuo et al., 2009). The long-distance dependencies can be modeled in either a generative or a discriminative framework. Discriminative models, which directly distinguish correct from incorrect hypothesis, are particularly attractive because they allow the inclusion of arbitrary features (Kuo et al., 2002; Roark et al., 2007; Collins et al., 2005); these models with syntactic information have obtained state of the art results. However, both generative and discriminative LMs with long-span dependencies can be slow, for they often cannot work dir</context>
<context position="5082" citStr="Chelba and Jelinek (2000)" startWordPosition="778" endWordPosition="781">ments and a reduction in word error rate (WER) for both N-best list and the already fast hill climbing rescoring. The net result is arguably the first syntactic LM fast enough to be used in a real time ASR system. 2 Syntactic Language Models There have been several approaches to include syntactic information in both generative and discriminative language models. For generative LMs, the syntactic information must be part of the generative process. Structured language modeling incorporates syntactic parse trees to identify the head words in a hypothesis for modeling dependencies beyond n-grams. Chelba and Jelinek (2000) extract the two previous exposed head words at each position in a hypothesis, along with their non-terminal tags, and use them as context for computing the probability of the current position. Khudanpur and Wu (2000) exploit such syntactic head word dependencies as features in a maximum entropy framework. Kuo et al. (2009) integrate syntactic features into a neural network LM for Arabic speech recognition. Discriminative models are more flexible since they can include arbitrary features, allowing for a wider range of long-span syntactic dependencies. Additionally, discriminative models are di</context>
</contexts>
<marker>Chelba, Jelinek, 2000</marker>
<rawString>C. Chelba and F. Jelinek. 2000. Structured language modeling. Computer Speech and Language, 14(4):283–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chen</author>
<author>B Kingsbury</author>
<author>L Mangu</author>
<author>D Povey</author>
<author>G Saon</author>
<author>H Soltau</author>
<author>G Zweig</author>
</authors>
<date>2006</date>
<booktitle>Advances in speech transcription at IBM under the DARPA EARS program. IEEE Transactions on Audio, Speech and Language Processing,</booktitle>
<pages>1596--1608</pages>
<contexts>
<context position="25402" citStr="Chen et al., 2006" startWordPosition="4349" endWordPosition="4352"> in a shift-reduce parser to broaden the search space. In contrast, we use the idea to identify subW1 W ··· Wi— w;_1 Wi Wi 1 Wi Wi ti t ··· 4— 4-1 tsr til 11 t; ti t1i 1 ti 1 180 structures across inputs, where our goal is efficient parsing in general. Additionally, we extend the definition of equivalent states to general transition based structured prediction models, and demonstrate applications beyond parsing as well as the novel setting of hypothesis set parsing. 6 Experiments Our ASR system is based on the 2007 IBM Speech transcription system for the GALE Distillation Go/No-go Evaluation (Chen et al., 2006) with state of the art discriminative acoustic models. See Table 2 for a data summary. We use a modified Kneser-Ney (KN) backoff 4-gram baseline LM. Word-lattices for discriminative training and rescoring come from this baseline ASR system.6 The longspan discriminative LM’s baseline feature weight (α0) is tuned on dev data and hill climbing (Rastrow et al., 2011a) is used for training and rescoring. The dependency parser and POS tagger are trained on supervised data and up-trained on data labeled by the CKY-style bottom-up constituent parser of Huang et al. (2010), a state of the art broadcast</context>
</contexts>
<marker>Chen, Kingsbury, Mangu, Povey, Saon, Soltau, Zweig, 2006</marker>
<rawString>S. Chen, B. Kingsbury, L. Mangu, D. Povey, G. Saon, H. Soltau, and G. Zweig. 2006. Advances in speech transcription at IBM under the DARPA EARS program. IEEE Transactions on Audio, Speech and Language Processing, pages 1596–1608.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cheppalier</author>
<author>M Rajman</author>
<author>R Aragues</author>
<author>A Rozenknop</author>
</authors>
<title>Lattice parsing for speech recognition.</title>
<date>1999</date>
<booktitle>In Sixth Conference sur le Traitement Automatique du Langage Naturel (TANL’99).</booktitle>
<contexts>
<context position="23523" citStr="Cheppalier et al., 1999" startWordPosition="4032" endWordPosition="4036">ame algorithm with different pruning methods, which can lead to faster but less accurate models. We apply up-training to improve the accuracy of both our fast POS tagger and dependency parser. We parse a large corpus of text with a very accurate but very slow constituent parser and use the resulting data to up-train our tools. We will demonstrate empirically that up-training improves these fast models to yield better WER results. 5 Related Work The idea of efficiently processing a hypothesis set is similar to “lattice-parsing”, in which a parser consider an entire lattice at once (Hall, 2005; Cheppalier et al., 1999). These methods typically constrain the parsing space using heuristics, which are often model specific. In other words, they search in the joint space of word sequences present in the lattice and their syntactic analyses; they are not guaranteed to produce a syntactic analysis for all hypotheses. In contrast, substructure sharing is a general purpose method that we have applied to two different algorithms. The output is identical to processing each hypothesis separately and output is generated for each hypothesis. Hall (Hall, 2005) uses a lattice parsing strategy which aims to compute the marg</context>
</contexts>
<marker>Cheppalier, Rajman, Aragues, Rozenknop, 1999</marker>
<rawString>J. Cheppalier, M. Rajman, R. Aragues, and A. Rozenknop. 1999. Lattice parsing for speech recognition. In Sixth Conference sur le Traitement Automatique du Langage Naturel (TANL’99).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>B Roark</author>
<author>M Saraclar</author>
</authors>
<title>Discriminative syntactic language modeling for speech recognition.</title>
<date>2005</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1711" citStr="Collins et al., 2005" startWordPosition="250" endWordPosition="253">how that up-training leads to WER reduction. 1 Introduction Language models (LM) are crucial components in tasks that require the generation of coherent natural language text, such as automatic speech recognition (ASR) and machine translation (MT). While traditional LMs use word n-grams, where the n − 1 previous words predict the next word, newer models integrate long-span information in making decisions. For example, incorporating long-distance dependencies and syntactic structure can help the LM better predict words by complementing the predictive power of n-grams (Chelba and Jelinek, 2000; Collins et al., 2005; Filimonov and Harper, 2009; Kuo et al., 2009). The long-distance dependencies can be modeled in either a generative or a discriminative framework. Discriminative models, which directly distinguish correct from incorrect hypothesis, are particularly attractive because they allow the inclusion of arbitrary features (Kuo et al., 2002; Roark et al., 2007; Collins et al., 2005); these models with syntactic information have obtained state of the art results. However, both generative and discriminative LMs with long-span dependencies can be slow, for they often cannot work directly with lattices an</context>
<context position="5868" citStr="Collins et al. (2005)" startWordPosition="903" endWordPosition="906">ity of the current position. Khudanpur and Wu (2000) exploit such syntactic head word dependencies as features in a maximum entropy framework. Kuo et al. (2009) integrate syntactic features into a neural network LM for Arabic speech recognition. Discriminative models are more flexible since they can include arbitrary features, allowing for a wider range of long-span syntactic dependencies. Additionally, discriminative models are directly trained to resolve the acoustic confusion in the decoded hypotheses of an ASR system. This flexibility and training regime translate into better performance. Collins et al. (2005) uses the Perceptron algorithm to train a global linear discriminative model which incorporates long-span features, such as headto-head dependencies and part of speech tags. Our Language Model. We work with a discriminative LM with long-span dependencies. We use a global linear model with Perceptron training. We rescore the hypotheses (lattices) generated by the ASR decoder—in a framework most similar to that of Rastrow et al. (2011a). The LM score S(w, a) for each hypothesis w of a speech utterance with acoustic sequence a is based on the baseline ASR system score b(w, a) (initial ngram LM sc</context>
<context position="7898" citStr="Collins et al., 2005" startWordPosition="1280" endWordPosition="1283">n Kuo et al. (2009) to identify the two previous exposed head words, h−2, h−1, at each position i in the input hypothesis and include the following syntactic based features into our LM: 1. (h−2.w o h−1.w o wi) , (h−1.w o wi) , (wi) 2. (h−2.t o h−1.t o ti) , (h−1.t o ti) , (ti) , (tiwi) where h.w and h.t denote the word identity and the POS tag of the corresponding exposed head word. 2.1 Hill Climbing Rescoring We adopt the so called hill climbing framework of Rastrow et al. (2011b) to improve both training and rescoring time as much as possible by reducing the 1We tune α0 on development data (Collins et al., 2005). 176 number N of explored hypotheses. We summarize it below for completeness. Given a speech utterance’s lattice G from a first pass ASR decoder, the neighborhood N(w, i) of a hypothesis w = w1 w2 ... w,,, at position i is defined as the set of all paths in the lattice that may be obtained by editing wi: deleting it, substituting it, or inserting a word to its left. In other words, it is the “distance-1-at-position i” neighborhood of w. Given a position i in a word sequence w, all hypotheses in N(w, i) are rescored using the longspan model and the hypothesis V(i) with the highest score become</context>
</contexts>
<marker>Collins, Roark, Saraclar, 2005</marker>
<rawString>M Collins, B Roark, and M Saraclar. 2005. Discriminative syntactic language modeling for speech recognition. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Denis Filimonov</author>
<author>Mary Harper</author>
</authors>
<title>A joint language model with fine-grain syntactic tags.</title>
<date>2009</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="1739" citStr="Filimonov and Harper, 2009" startWordPosition="254" endWordPosition="257">eads to WER reduction. 1 Introduction Language models (LM) are crucial components in tasks that require the generation of coherent natural language text, such as automatic speech recognition (ASR) and machine translation (MT). While traditional LMs use word n-grams, where the n − 1 previous words predict the next word, newer models integrate long-span information in making decisions. For example, incorporating long-distance dependencies and syntactic structure can help the LM better predict words by complementing the predictive power of n-grams (Chelba and Jelinek, 2000; Collins et al., 2005; Filimonov and Harper, 2009; Kuo et al., 2009). The long-distance dependencies can be modeled in either a generative or a discriminative framework. Discriminative models, which directly distinguish correct from incorrect hypothesis, are particularly attractive because they allow the inclusion of arbitrary features (Kuo et al., 2002; Roark et al., 2007; Collins et al., 2005); these models with syntactic information have obtained state of the art results. However, both generative and discriminative LMs with long-span dependencies can be slow, for they often cannot work directly with lattices and require rescoring large N-</context>
</contexts>
<marker>Filimonov, Harper, 2009</marker>
<rawString>Denis Filimonov and Mary Harper. 2009. A joint language model with fine-grain syntactic tags. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Michael Elhadad</author>
</authors>
<title>An Efficient Algorithm for Easy-First Non-Directional Dependency Parsing.</title>
<date>2010</date>
<booktitle>In Proc. HLT-NAACL, number</booktitle>
<pages>742--750</pages>
<contexts>
<context position="17824" citStr="Goldberg and Elhadad (2010)" startWordPosition="3026" endWordPosition="3029">final states occur when Q is empty and S contains a single tree (the output). Q is determined by the set of dependency labels r E R and one of three transition types: • Shift: remove the head of Q (wj) and place it on the top of S as a singleton tree (only wj.) • Reduce-Left,: replace the top two trees in S (s0 and s1) with a tree formed by making the root of s1 a dependent of the root of s0 with label r. • Reduce-Right,: same as Reduce-Left, except reverses s0 and s1. Table 1 shows the kernel features used in our dependency parser. See Sagae and Tsujii (2007) for a complete list of features. Goldberg and Elhadad (2010) observed that parsing time is dominated by feature extraction and score calculation. Substructure sharing reduces these steps for equivalent states, which are persistent throughout a candidate set. Note that there are far fewer kernel features than total features, hence the hash function calculation is very fast. We summarize substructure sharing for dependency parsing in Algorithm 1. We extend the definition of states to be {S, Q, p} where p denotes the score of the state: the probability of the action sequence that resulted in the current state. Also, folAlgorithm 1 Best-first shift-reduce </context>
</contexts>
<marker>Goldberg, Elhadad, 2010</marker>
<rawString>Yoav Goldberg and Michael Elhadad. 2010. An Efficient Algorithm for Easy-First Non-Directional Dependency Parsing. In Proc. HLT-NAACL, number June, pages 742–750.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keith B Hall</author>
</authors>
<title>Best-first word-lattice parsing: techniques for integrated syntactic language modeling.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Brown University.</institution>
<contexts>
<context position="23497" citStr="Hall, 2005" startWordPosition="4030" endWordPosition="4031">uld be the same algorithm with different pruning methods, which can lead to faster but less accurate models. We apply up-training to improve the accuracy of both our fast POS tagger and dependency parser. We parse a large corpus of text with a very accurate but very slow constituent parser and use the resulting data to up-train our tools. We will demonstrate empirically that up-training improves these fast models to yield better WER results. 5 Related Work The idea of efficiently processing a hypothesis set is similar to “lattice-parsing”, in which a parser consider an entire lattice at once (Hall, 2005; Cheppalier et al., 1999). These methods typically constrain the parsing space using heuristics, which are often model specific. In other words, they search in the joint space of word sequences present in the lattice and their syntactic analyses; they are not guaranteed to produce a syntactic analysis for all hypotheses. In contrast, substructure sharing is a general purpose method that we have applied to two different algorithms. The output is identical to processing each hypothesis separately and output is generated for each hypothesis. Hall (Hall, 2005) uses a lattice parsing strategy whic</context>
</contexts>
<marker>Hall, 2005</marker>
<rawString>Keith B Hall. 2005. Best-first word-lattice parsing: techniques for integrated syntactic language modeling. Ph.D. thesis, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
<author>K Sagae</author>
</authors>
<title>Dynamic Programming for Linear-Time Incremental Parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="13970" citStr="Huang and Sagae (2010)" startWordPosition="2349" endWordPosition="2352"> for multi-class classification, so pg(Wi17ri) = pg(f(7r) o Wi), where o is a conjunction operation. In this way, states can be summarized by features. Equivalent states are defined as two states 7r and 7r0 with an identical feature representation: 7r - 7r0 iff f(7r) = f(7r0) If two states are equivalent, then g imposes the same distribution over actions. We can benefit from this substructure redundancy, both within and between hypotheses, by saving these distributions in memory, sharing a distribution computed just once across equivalent states. A similar idea of equivalent states is used by Huang and Sagae (2010), except they use equivalence to facilitate dynamic programming for shift-reduce parsing, whereas we generalize it for improving the processing time of similar hypotheses in general models. Following Huang and Sagae, we define kernel features as the smallest set of atomic features �f(7r) such that, �f(7r) = �f(7r0) 7r - 7r0. (2) Equivalent distributions are stored in a hash table H : 11 —* QxR; the hash keys are the states and the values are distributions2 over actions: {W, pg(WI7r)}. 2For pure greedy search (deterministic search) we need only retain the best action, since the distribution is </context>
<context position="24710" citStr="Huang and Sagae (2010)" startWordPosition="4225" endWordPosition="4228">tegy which aims to compute the marginal probabilities of all word sequences in the lattice by summing over syntactic analyses of each word sequence. The parser sums over multiple parses of a word sequence implicitly. The lattice parser therefore, is itself a language model. In contrast, our tools are completely separated from the ASR system, which allows the system to create whatever features are needed. This independence means our tools are useful for other tasks, such as machine translation. These differences make substructure sharing a more attractive option for efficient algorithms. While Huang and Sagae (2010) use the notion of “equivalent states”, they do so for dynamic programming in a shift-reduce parser to broaden the search space. In contrast, we use the idea to identify subW1 W ··· Wi— w;_1 Wi Wi 1 Wi Wi ti t ··· 4— 4-1 tsr til 11 t; ti t1i 1 ti 1 180 structures across inputs, where our goal is efficient parsing in general. Additionally, we extend the definition of equivalent states to general transition based structured prediction models, and demonstrate applications beyond parsing as well as the novel setting of hypothesis set parsing. 6 Experiments Our ASR system is based on the 2007 IBM S</context>
</contexts>
<marker>Huang, Sagae, 2010</marker>
<rawString>L. Huang and K. Sagae. 2010. Dynamic Programming for Linear-Time Incremental Parsing. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Mary Harper</author>
<author>Slav Petrov</author>
</authors>
<title>Self-training with Products of Latent Variable Grammars.</title>
<date>2010</date>
<booktitle>In Proc. EMNLP, number October,</booktitle>
<pages>12--22</pages>
<contexts>
<context position="25972" citStr="Huang et al. (2010)" startWordPosition="4443" endWordPosition="4446">tillation Go/No-go Evaluation (Chen et al., 2006) with state of the art discriminative acoustic models. See Table 2 for a data summary. We use a modified Kneser-Ney (KN) backoff 4-gram baseline LM. Word-lattices for discriminative training and rescoring come from this baseline ASR system.6 The longspan discriminative LM’s baseline feature weight (α0) is tuned on dev data and hill climbing (Rastrow et al., 2011a) is used for training and rescoring. The dependency parser and POS tagger are trained on supervised data and up-trained on data labeled by the CKY-style bottom-up constituent parser of Huang et al. (2010), a state of the art broadcast news (BN) parser, with phrase structures converted to labeled dependencies by the Stanford converter. While accurate, the parser has a huge grammar (32GB) from using products of latent variable grammars and requires O(l3) time to parse a sentence of length l. Therefore, we could not use the constituent parser for ASR rescoring since utterances can be very long, although the shorter up-training text data was not a problem.7 We evaluate both unlabeled (UAS) and labeled dependency accuracy (LAS). 6.1 Results Before we demonstrate the speed of our models, we show tha</context>
<context position="29852" citStr="Huang et al. (2010)" startWordPosition="5063" endWordPosition="5066">LM training: modified KN 4-gram TDT4 closed captions+EARS BN03 closed caption 193m words Disc. LM training: long-span w/hill climbing Hub4 (length &lt;50) 115k uttr, 2.6m words Baseline feature (α0) tuning dev04f BN data 2.5 hrs Supervised training: dep. parser, POS tagger Ontonotes BN treebank+ WSJ Penn treebank 1.3m words, 59k sent. Supervised training: constituent parser Ontonotes BN treebank + WSJ Penn treebank 1.3m words, 59k sent. Up-training: dependency parser, POS tagger TDT4 closed captions+EARS BN03 closed caption 193m words available Evaluation: up-training BN treebank test (following Huang et al. (2010)) 20k words, 1.1k sent. Evaluation: ASR transcription rt04 BN evaluation 4 hrs, 45k words Table 2: A summary of the data for training and evaluation. The Ontonotes corpus is from Weischedel et al. (2008). (a) 10000 1000 Elapsed Time (sec) 10 1 10 (N) 100 1000 N--‐best Size 1000000 100000 100 No Sharing Substructure Sharing (b) Elapsed Time (sec) 10000 1000 100 10 1 1 10 (N) 100 1000 N--‐best Size No Sharing Substructure Sharing Figure 4: Elapsed time for (a) parsing and (b) POS tagging the N-best lists with and without substructure sharing. Substr. Share (sec) LM WER No Yes Baseline 4-gram 15.</context>
</contexts>
<marker>Huang, Harper, Petrov, 2010</marker>
<rawString>Zhongqiang Huang, Mary Harper, and Slav Petrov. 2010. Self-training with Products of Latent Variable Grammars. In Proc. EMNLP, number October, pages 12– 22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Khudanpur</author>
<author>J Wu</author>
</authors>
<title>Maximum entropy techniques for exploiting syntactic, semantic and collocational dependencies in language modeling. Computer Speech and Language,</title>
<date>2000</date>
<pages>355--372</pages>
<contexts>
<context position="2373" citStr="Khudanpur and Wu, 2000" startWordPosition="348" endWordPosition="351">, 2009). The long-distance dependencies can be modeled in either a generative or a discriminative framework. Discriminative models, which directly distinguish correct from incorrect hypothesis, are particularly attractive because they allow the inclusion of arbitrary features (Kuo et al., 2002; Roark et al., 2007; Collins et al., 2005); these models with syntactic information have obtained state of the art results. However, both generative and discriminative LMs with long-span dependencies can be slow, for they often cannot work directly with lattices and require rescoring large N-best lists (Khudanpur and Wu, 2000; Collins et al., 2005; Kuo et al., 2009). For discriminative models, this limitation applies to training as well. Moreover, the non-local features used in rescoring are usually extracted via auxiliary tools – which in the case of syntactic features include part of speech taggers and parsers – from a set of ASR system hypotheses. Separately applying auxiliary tools to each N-best list hypothesis leads to major inefficiencies as many hypotheses differ only slightly. Recent work on hill climbing algorithms for ASR lattice rescoring iteratively searches for a higherscoring hypothesis in a local n</context>
<context position="5299" citStr="Khudanpur and Wu (2000)" startWordPosition="815" endWordPosition="818">tactic Language Models There have been several approaches to include syntactic information in both generative and discriminative language models. For generative LMs, the syntactic information must be part of the generative process. Structured language modeling incorporates syntactic parse trees to identify the head words in a hypothesis for modeling dependencies beyond n-grams. Chelba and Jelinek (2000) extract the two previous exposed head words at each position in a hypothesis, along with their non-terminal tags, and use them as context for computing the probability of the current position. Khudanpur and Wu (2000) exploit such syntactic head word dependencies as features in a maximum entropy framework. Kuo et al. (2009) integrate syntactic features into a neural network LM for Arabic speech recognition. Discriminative models are more flexible since they can include arbitrary features, allowing for a wider range of long-span syntactic dependencies. Additionally, discriminative models are directly trained to resolve the acoustic confusion in the decoded hypotheses of an ASR system. This flexibility and training regime translate into better performance. Collins et al. (2005) uses the Perceptron algorithm </context>
</contexts>
<marker>Khudanpur, Wu, 2000</marker>
<rawString>S. Khudanpur and J. Wu. 2000. Maximum entropy techniques for exploiting syntactic, semantic and collocational dependencies in language modeling. Computer Speech and Language, pages 355–372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S K¨ubler</author>
<author>R McDonald</author>
<author>J Nivre</author>
</authors>
<date>2009</date>
<booktitle>Dependency parsing. Synthesis Lectures on Human Language Technologies,</booktitle>
<pages>2--1</pages>
<marker>K¨ubler, McDonald, Nivre, 2009</marker>
<rawString>S. K¨ubler, R. McDonald, and J. Nivre. 2009. Dependency parsing. Synthesis Lectures on Human Language Technologies, 2(1):1–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong-Kwang Jeff Kuo</author>
<author>Eric Fosler-Lussier</author>
<author>Hui Jiang</author>
<author>Chin-Hui Lee</author>
</authors>
<title>Discriminative training of language models for speech recognition.</title>
<date>2002</date>
<booktitle>In ICASSP.</booktitle>
<contexts>
<context position="2045" citStr="Kuo et al., 2002" startWordPosition="297" endWordPosition="300">ord, newer models integrate long-span information in making decisions. For example, incorporating long-distance dependencies and syntactic structure can help the LM better predict words by complementing the predictive power of n-grams (Chelba and Jelinek, 2000; Collins et al., 2005; Filimonov and Harper, 2009; Kuo et al., 2009). The long-distance dependencies can be modeled in either a generative or a discriminative framework. Discriminative models, which directly distinguish correct from incorrect hypothesis, are particularly attractive because they allow the inclusion of arbitrary features (Kuo et al., 2002; Roark et al., 2007; Collins et al., 2005); these models with syntactic information have obtained state of the art results. However, both generative and discriminative LMs with long-span dependencies can be slow, for they often cannot work directly with lattices and require rescoring large N-best lists (Khudanpur and Wu, 2000; Collins et al., 2005; Kuo et al., 2009). For discriminative models, this limitation applies to training as well. Moreover, the non-local features used in rescoring are usually extracted via auxiliary tools – which in the case of syntactic features include part of speech</context>
</contexts>
<marker>Kuo, Fosler-Lussier, Jiang, Lee, 2002</marker>
<rawString>Hong-Kwang Jeff Kuo, Eric Fosler-Lussier, Hui Jiang, and Chin-Hui Lee. 2002. Discriminative training of language models for speech recognition. In ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H K J Kuo</author>
<author>L Mangu</author>
<author>A Emami</author>
<author>I Zitouni</author>
<author>L Young-Suk</author>
</authors>
<title>Syntactic features for Arabic speech recognition.</title>
<date>2009</date>
<booktitle>In Proc. ASRU.</booktitle>
<contexts>
<context position="1758" citStr="Kuo et al., 2009" startWordPosition="258" endWordPosition="261">roduction Language models (LM) are crucial components in tasks that require the generation of coherent natural language text, such as automatic speech recognition (ASR) and machine translation (MT). While traditional LMs use word n-grams, where the n − 1 previous words predict the next word, newer models integrate long-span information in making decisions. For example, incorporating long-distance dependencies and syntactic structure can help the LM better predict words by complementing the predictive power of n-grams (Chelba and Jelinek, 2000; Collins et al., 2005; Filimonov and Harper, 2009; Kuo et al., 2009). The long-distance dependencies can be modeled in either a generative or a discriminative framework. Discriminative models, which directly distinguish correct from incorrect hypothesis, are particularly attractive because they allow the inclusion of arbitrary features (Kuo et al., 2002; Roark et al., 2007; Collins et al., 2005); these models with syntactic information have obtained state of the art results. However, both generative and discriminative LMs with long-span dependencies can be slow, for they often cannot work directly with lattices and require rescoring large N-best lists (Khudanp</context>
<context position="5407" citStr="Kuo et al. (2009)" startWordPosition="834" endWordPosition="837">discriminative language models. For generative LMs, the syntactic information must be part of the generative process. Structured language modeling incorporates syntactic parse trees to identify the head words in a hypothesis for modeling dependencies beyond n-grams. Chelba and Jelinek (2000) extract the two previous exposed head words at each position in a hypothesis, along with their non-terminal tags, and use them as context for computing the probability of the current position. Khudanpur and Wu (2000) exploit such syntactic head word dependencies as features in a maximum entropy framework. Kuo et al. (2009) integrate syntactic features into a neural network LM for Arabic speech recognition. Discriminative models are more flexible since they can include arbitrary features, allowing for a wider range of long-span syntactic dependencies. Additionally, discriminative models are directly trained to resolve the acoustic confusion in the decoded hypotheses of an ASR system. This flexibility and training regime translate into better performance. Collins et al. (2005) uses the Perceptron algorithm to train a global linear discriminative model which incorporates long-span features, such as headto-head dep</context>
<context position="7296" citStr="Kuo et al. (2009)" startWordPosition="1165" endWordPosition="1168">iscriminative LM’s score for the hypothesis w, and s1, ... , sm are candidate syntactic structures associated with w, as discussed below. Since we use a linear model, the score is a weighted linear combination of the count of activated features of the word sequence w and its associated structures: 45i(w, s1, ... , sm). Perceptron training learns the parameters α. The baseline score b(w, a) can be a feature, yielding the dot product notation: S(w, a) = (α, 45(a, w, s1, ... , sm)) Our LM uses features from the dependency tree and part of speech (POS) tag sequence. We use the method described in Kuo et al. (2009) to identify the two previous exposed head words, h−2, h−1, at each position i in the input hypothesis and include the following syntactic based features into our LM: 1. (h−2.w o h−1.w o wi) , (h−1.w o wi) , (wi) 2. (h−2.t o h−1.t o ti) , (h−1.t o ti) , (ti) , (tiwi) where h.w and h.t denote the word identity and the POS tag of the corresponding exposed head word. 2.1 Hill Climbing Rescoring We adopt the so called hill climbing framework of Rastrow et al. (2011b) to improve both training and rescoring time as much as possible by reducing the 1We tune α0 on development data (Collins et al., 200</context>
</contexts>
<marker>Kuo, Mangu, Emami, Zitouni, Young-Suk, 2009</marker>
<rawString>H. K. J. Kuo, L. Mangu, A. Emami, I. Zitouni, and L. Young-Suk. 2009. Syntactic features for Arabic speech recognition. In Proc. ASRU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Pi-Chuan Chang</author>
<author>Michael Ringgaard</author>
<author>Hiyan Alshawi</author>
</authors>
<title>Uptraining for accurate deterministic question parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>705--713</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="4358" citStr="Petrov et al., 2010" startWordPosition="663" endWordPosition="666">ities among the set of generated hypotheses. The key idea is to share substructure states in transition based structured prediction algorithms, i.e. algorithms where final structures are composed of a sequence of multiple individual decisions. We demonstrate our approach on a local Perceptron based part of speech tagger (Tsuruoka et al., 2011) and a shift reduce dependency parser (Sagae and Tsujii, 2007), yielding significantly faster tagging and parsing of ASR hypotheses. While these simpler structured prediction models are faster, we compensate for the model’s simplicity through uptraining (Petrov et al., 2010), yielding auxiliary tools that are both fast and accurate. The result is significant speed improvements and a reduction in word error rate (WER) for both N-best list and the already fast hill climbing rescoring. The net result is arguably the first syntactic LM fast enough to be used in a real time ASR system. 2 Syntactic Language Models There have been several approaches to include syntactic information in both generative and discriminative language models. For generative LMs, the syntactic information must be part of the generative process. Structured language modeling incorporates syntacti</context>
<context position="22228" citStr="Petrov et al. (2010)" startWordPosition="3816" endWordPosition="3819">acy is impractical because they are slow. Instead, we seek to improve the accuracy of our fast tools. To achieve this goal we use up-training, in which a more complex model is used to improve the accuracy of a simpler model. We are given two models, M1 and M2, as well as a large collection of unlabeled text. Model M1 is slow but very accurate while M2 is fast but obtains lower accuracy. Up-training applies M1 to tag the unlabeled data, which is then used as training data for M2. Like self-training, a model is retrained on automatic output, but here the output comes form a more accurate model. Petrov et al. (2010) used up-training as a domain adaptation technique: a constituent parser – which is more robust to domain changes – was used to label a new domain, and a fast dependency parser 5 Tsuruoka et al. (2011) shows that the lookahead search improves the performance of the local ”history-based” models for different NLP tasks was trained on the automatically labeled data. We use a similar idea where our goal is to recover the accuracy lost from using simpler models. Note that while up-training uses two models, it differs from co-training since we care about improving only one model (M2). Additionally, </context>
</contexts>
<marker>Petrov, Chang, Ringgaard, Alshawi, 2010</marker>
<rawString>Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and Hiyan Alshawi. 2010. Uptraining for accurate deterministic question parsing. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 705–713, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ariya Rastrow</author>
<author>Mark Dredze</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Efficient discrimnative training of long-span language models.</title>
<date>2011</date>
<booktitle>In IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU).</booktitle>
<contexts>
<context position="3132" citStr="Rastrow et al., 2011" startWordPosition="472" endWordPosition="475">features used in rescoring are usually extracted via auxiliary tools – which in the case of syntactic features include part of speech taggers and parsers – from a set of ASR system hypotheses. Separately applying auxiliary tools to each N-best list hypothesis leads to major inefficiencies as many hypotheses differ only slightly. Recent work on hill climbing algorithms for ASR lattice rescoring iteratively searches for a higherscoring hypothesis in a local neighborhood of the current-best hypothesis, leading to a much more efficient algorithm in terms of the number, N, of hypotheses evaluated (Rastrow et al., 2011b); the idea also leads to a discriminative hill climbing training algorithm (Rastrow et al., 2011a). Even so, the reliance on auxiliary tools slow LM application to the point of being impractical for real time systems. While faster auxiliary tools are an option, they are usually less accurate. In this paper, we propose a general modifica175 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 175–183, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics tion to the decoders used in auxiliary tools to utilize the co</context>
<context position="6304" citStr="Rastrow et al. (2011" startWordPosition="972" endWordPosition="975">ctly trained to resolve the acoustic confusion in the decoded hypotheses of an ASR system. This flexibility and training regime translate into better performance. Collins et al. (2005) uses the Perceptron algorithm to train a global linear discriminative model which incorporates long-span features, such as headto-head dependencies and part of speech tags. Our Language Model. We work with a discriminative LM with long-span dependencies. We use a global linear model with Perceptron training. We rescore the hypotheses (lattices) generated by the ASR decoder—in a framework most similar to that of Rastrow et al. (2011a). The LM score S(w, a) for each hypothesis w of a speech utterance with acoustic sequence a is based on the baseline ASR system score b(w, a) (initial ngram LM score and the acoustic score) and α0, the weight assigned to the baseline score.1 The score is defined as: S(w, a) = α0 · b(w, a) + F(w, s1, ... , sm) d = α0 · b(w, a) + αi · 45i(w, s1,...,sm) i=1 where F is the discriminative LM’s score for the hypothesis w, and s1, ... , sm are candidate syntactic structures associated with w, as discussed below. Since we use a linear model, the score is a weighted linear combination of the count of</context>
<context position="7761" citStr="Rastrow et al. (2011" startWordPosition="1256" endWordPosition="1259">(a, w, s1, ... , sm)) Our LM uses features from the dependency tree and part of speech (POS) tag sequence. We use the method described in Kuo et al. (2009) to identify the two previous exposed head words, h−2, h−1, at each position i in the input hypothesis and include the following syntactic based features into our LM: 1. (h−2.w o h−1.w o wi) , (h−1.w o wi) , (wi) 2. (h−2.t o h−1.t o ti) , (h−1.t o ti) , (ti) , (tiwi) where h.w and h.t denote the word identity and the POS tag of the corresponding exposed head word. 2.1 Hill Climbing Rescoring We adopt the so called hill climbing framework of Rastrow et al. (2011b) to improve both training and rescoring time as much as possible by reducing the 1We tune α0 on development data (Collins et al., 2005). 176 number N of explored hypotheses. We summarize it below for completeness. Given a speech utterance’s lattice G from a first pass ASR decoder, the neighborhood N(w, i) of a hypothesis w = w1 w2 ... w,,, at position i is defined as the set of all paths in the lattice that may be obtained by editing wi: deleting it, substituting it, or inserting a word to its left. In other words, it is the “distance-1-at-position i” neighborhood of w. Given a position i in</context>
<context position="25766" citStr="Rastrow et al., 2011" startWordPosition="4409" endWordPosition="4412"> models, and demonstrate applications beyond parsing as well as the novel setting of hypothesis set parsing. 6 Experiments Our ASR system is based on the 2007 IBM Speech transcription system for the GALE Distillation Go/No-go Evaluation (Chen et al., 2006) with state of the art discriminative acoustic models. See Table 2 for a data summary. We use a modified Kneser-Ney (KN) backoff 4-gram baseline LM. Word-lattices for discriminative training and rescoring come from this baseline ASR system.6 The longspan discriminative LM’s baseline feature weight (α0) is tuned on dev data and hill climbing (Rastrow et al., 2011a) is used for training and rescoring. The dependency parser and POS tagger are trained on supervised data and up-trained on data labeled by the CKY-style bottom-up constituent parser of Huang et al. (2010), a state of the art broadcast news (BN) parser, with phrase structures converted to labeled dependencies by the Stanford converter. While accurate, the parser has a huge grammar (32GB) from using products of latent variable grammars and requires O(l3) time to parse a sentence of length l. Therefore, we could not use the constituent parser for ASR rescoring since utterances can be very long,</context>
</contexts>
<marker>Rastrow, Dredze, Khudanpur, 2011</marker>
<rawString>Ariya Rastrow, Mark Dredze, and Sanjeev Khudanpur. 2011a. Efficient discrimnative training of long-span language models. In IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ariya Rastrow</author>
<author>Markus Dreyer</author>
</authors>
<title>Abhinav Sethy, Sanjeev Khudanpur, Bhuvana Ramabhadran, and Mark Dredze. 2011b. Hill climbing on speech lattices : A new rescoring framework.</title>
<booktitle>In ICASSP.</booktitle>
<marker>Rastrow, Dreyer, </marker>
<rawString>Ariya Rastrow, Markus Dreyer, Abhinav Sethy, Sanjeev Khudanpur, Bhuvana Ramabhadran, and Mark Dredze. 2011b. Hill climbing on speech lattices : A new rescoring framework. In ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Murat Saraclar</author>
<author>Michael Collins</author>
</authors>
<title>Discriminative n-gram language modeling.</title>
<date>2007</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="2065" citStr="Roark et al., 2007" startWordPosition="301" endWordPosition="304">integrate long-span information in making decisions. For example, incorporating long-distance dependencies and syntactic structure can help the LM better predict words by complementing the predictive power of n-grams (Chelba and Jelinek, 2000; Collins et al., 2005; Filimonov and Harper, 2009; Kuo et al., 2009). The long-distance dependencies can be modeled in either a generative or a discriminative framework. Discriminative models, which directly distinguish correct from incorrect hypothesis, are particularly attractive because they allow the inclusion of arbitrary features (Kuo et al., 2002; Roark et al., 2007; Collins et al., 2005); these models with syntactic information have obtained state of the art results. However, both generative and discriminative LMs with long-span dependencies can be slow, for they often cannot work directly with lattices and require rescoring large N-best lists (Khudanpur and Wu, 2000; Collins et al., 2005; Kuo et al., 2009). For discriminative models, this limitation applies to training as well. Moreover, the non-local features used in rescoring are usually extracted via auxiliary tools – which in the case of syntactic features include part of speech taggers and parsers</context>
</contexts>
<marker>Roark, Saraclar, Collins, 2007</marker>
<rawString>Brian Roark, Murat Saraclar, and Michael Collins. 2007. Discriminative n-gram language modeling. Computer Speech &amp; Language, 21(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sagae</author>
<author>A Lavie</author>
</authors>
<title>A best-first probabilistic shift-reduce parser.</title>
<date>2006</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>691--698</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="20882" citStr="Sagae and Lavie, 2006" startWordPosition="3570" endWordPosition="3573">e equivalence. The tagger is deterministic (greedy) in that it only considers the best tag at each step, so we do not store scores. However, this tagger uses a depth3 Sagae and Tsujii (2007) use a beam strategy to increase speed. Search space pruning is achieved by filtering heap states for probability greater than 1b the probability of the most likely state in the heap with the same number of actions. We use b = 100 for our experiments. 4We note that while we have demonstrated substructure sharing for dependency parsing, the same improvements can be made to a shift-reduce constituent parser (Sagae and Lavie, 2006). 179 Figure 2: POS tagger with lookahead search of d=1. At wi the search considers the current state and next state. first search lookahead procedure to select the best action at each step, which considers future decisions up to depth d5. An example for d = 1 is shown in Figure 2. Using d = 1 for the lookahead search strategy, we modify the kernel features since the decision for wi is affected by the state 7ri+1. The kernel features in position i should be �f(7ri) U �f(7ri+1): �f(7ri) = {ti−2, ti−1, wi−2, wi−1, wi, wi+1, wi+2, wi+3l 4 Up-Training While we have fast decoding algorithms for the</context>
</contexts>
<marker>Sagae, Lavie, 2006</marker>
<rawString>K. Sagae and A. Lavie. 2006. A best-first probabilistic shift-reduce parser. In Proc. ACL, pages 691–698. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sagae</author>
<author>J Tsujii</author>
</authors>
<title>Dependency parsing and domain adaptation with LR models and parser ensembles.</title>
<date>2007</date>
<booktitle>In Proc. EMNLP-CoNLL,</booktitle>
<volume>7</volume>
<pages>1044--1050</pages>
<contexts>
<context position="4145" citStr="Sagae and Tsujii, 2007" startWordPosition="632" endWordPosition="635">Association for Computational Linguistics, pages 175–183, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics tion to the decoders used in auxiliary tools to utilize the commonalities among the set of generated hypotheses. The key idea is to share substructure states in transition based structured prediction algorithms, i.e. algorithms where final structures are composed of a sequence of multiple individual decisions. We demonstrate our approach on a local Perceptron based part of speech tagger (Tsuruoka et al., 2011) and a shift reduce dependency parser (Sagae and Tsujii, 2007), yielding significantly faster tagging and parsing of ASR hypotheses. While these simpler structured prediction models are faster, we compensate for the model’s simplicity through uptraining (Petrov et al., 2010), yielding auxiliary tools that are both fast and accurate. The result is significant speed improvements and a reduction in word error rate (WER) for both N-best list and the already fast hill climbing rescoring. The net result is arguably the first syntactic LM fast enough to be used in a real time ASR system. 2 Syntactic Language Models There have been several approaches to include </context>
<context position="16052" citStr="Sagae and Tsujii (2007)" startWordPosition="2691" endWordPosition="2694">ns. To apply substructure sharing to a transition based model, we need only define the set of states 11 (including 7r0 and 7rf), actions Q and kernel feature functions f. The resulting speedup depends on the amount of substructure duplication among the hypotheses, which we will show is significant for ASR lattice rescoring. Note that our algorithm is not an approximation; we obtain the same output {si } as we would without any sharing. We now apply this algorithm to dependency parsing and POS tagging. 3.2 Dependency Parsing We use the best-first probabilistic shift-reduce dependency parser of Sagae and Tsujii (2007), a transition-based parser et al., 2009) with a MaxEnt classifier. Dependency trees are built by processing the words left-to-right and the classifier assigns a distribution over the actions at each step. States are defined as 7r (K¨ubler ={S, Q}: S is a stack of 178 Kernel features f(7r) for state 7r = {S, Q} S = s0, s1, ... &amp; Q = q0, q1, ... (1) s0.w s0.t s0.r (5) ts0−1 s0.lch.t s0.lch.r ts1+1 s0.rch.t s0.rch.r (2) s1.w s1.t s1.r (6) dist(s0, s1) s1.lch.t s1.lch.r dist(q0, s0) s1.rch.t s1.rch.r (3) s2.w s2.t s2.r (4) q0.w q0.t (7) s0.nch q1.w q1.t s1.nch q2.w Table 1: Kernel features for de</context>
<context position="17763" citStr="Sagae and Tsujii (2007)" startWordPosition="3016" endWordPosition="3019">uence. The initial state is π0 = {0, {w0, w1, ...}}, and final states occur when Q is empty and S contains a single tree (the output). Q is determined by the set of dependency labels r E R and one of three transition types: • Shift: remove the head of Q (wj) and place it on the top of S as a singleton tree (only wj.) • Reduce-Left,: replace the top two trees in S (s0 and s1) with a tree formed by making the root of s1 a dependent of the root of s0 with label r. • Reduce-Right,: same as Reduce-Left, except reverses s0 and s1. Table 1 shows the kernel features used in our dependency parser. See Sagae and Tsujii (2007) for a complete list of features. Goldberg and Elhadad (2010) observed that parsing time is dominated by feature extraction and score calculation. Substructure sharing reduces these steps for equivalent states, which are persistent throughout a candidate set. Note that there are far fewer kernel features than total features, hence the hash function calculation is very fast. We summarize substructure sharing for dependency parsing in Algorithm 1. We extend the definition of states to be {S, Q, p} where p denotes the score of the state: the probability of the action sequence that resulted in the</context>
<context position="19227" citStr="Sagae and Tsujii (2007)" startWordPosition="3264" endWordPosition="3267">search Heap.push(π0) [initialize the heap] while Heap =� 0 do πcurrent �Heap.pop() [the best state so far] if πcurrent = πf [if final state] return πcurrent [terminate if final state] else if H.find(πcurrent) ActList — H[πcurrent] [retrieve action list from the hash table] else [need to construct action list] for all ω E 0 [for all actions] p� &apos; i pg (ω Iπcurrent) [action score] ActList.insert({ω, pwI) H.insert(πcurrent, ActList) [Store the action list into hash table] end if for all {ω, p�I E ActList [compute new states] πnew &apos; πcurrent X ω Heap.push(πnew) [push to the heap] end while lowing Sagae and Tsujii (2007) a heap is used to maintain states prioritized by their scores, for applying the best-first strategy. For each step, a state from the top of the heap is considered and all actions (and scores) are either retrieved from H or computed using g.3 We use πnew &apos;-- πcurrent X ω to denote the operation of extending a state by an action ω E Q4. 3.3 Part of Speech Tagging We use the part of speech (POS) tagger of Tsuruoka et al. (2011), a transition based model with a Perceptron and a lookahead heuristic process. The tagger processes w left to right. States are defined as πi = {ci, w}: a sequence of ass</context>
<context position="20450" citStr="Sagae and Tsujii (2007)" startWordPosition="3498" endWordPosition="3501">igned tags up to wi (ci = t1t2 ... ti−1) and the word sequence w. Q is defined simply as the set of possible POS tags (T) that can be applied. The final state is reached once all the positions are tagged. For f we use the features of Tsuruoka et al. (2011). The kernel features are �f(πi) = {ti−2, ti−1, wi−2, wi−1, wi, wi+1, wi+2}. While the tagger extracts prefix and suffix features, it suffices to look at wi for determining state equivalence. The tagger is deterministic (greedy) in that it only considers the best tag at each step, so we do not store scores. However, this tagger uses a depth3 Sagae and Tsujii (2007) use a beam strategy to increase speed. Search space pruning is achieved by filtering heap states for probability greater than 1b the probability of the most likely state in the heap with the same number of actions. We use b = 100 for our experiments. 4We note that while we have demonstrated substructure sharing for dependency parsing, the same improvements can be made to a shift-reduce constituent parser (Sagae and Lavie, 2006). 179 Figure 2: POS tagger with lookahead search of d=1. At wi the search considers the current state and next state. first search lookahead procedure to select the bes</context>
</contexts>
<marker>Sagae, Tsujii, 2007</marker>
<rawString>K. Sagae and J. Tsujii. 2007. Dependency parsing and domain adaptation with LR models and parser ensembles. In Proc. EMNLP-CoNLL, volume 7, pages 1044–1050.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Kazama</author>
</authors>
<title>Learning with Lookahead : Can History-Based Models Rival Globally Optimized Models ?</title>
<date>2011</date>
<booktitle>In Proc. CoNLL, number June,</booktitle>
<pages>238--246</pages>
<contexts>
<context position="4083" citStr="Tsuruoka et al., 2011" startWordPosition="622" endWordPosition="625">al modifica175 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 175–183, Jeju, Republic of Korea, 8-14 July 2012. c�2012 Association for Computational Linguistics tion to the decoders used in auxiliary tools to utilize the commonalities among the set of generated hypotheses. The key idea is to share substructure states in transition based structured prediction algorithms, i.e. algorithms where final structures are composed of a sequence of multiple individual decisions. We demonstrate our approach on a local Perceptron based part of speech tagger (Tsuruoka et al., 2011) and a shift reduce dependency parser (Sagae and Tsujii, 2007), yielding significantly faster tagging and parsing of ASR hypotheses. While these simpler structured prediction models are faster, we compensate for the model’s simplicity through uptraining (Petrov et al., 2010), yielding auxiliary tools that are both fast and accurate. The result is significant speed improvements and a reduction in word error rate (WER) for both N-best list and the already fast hill climbing rescoring. The net result is arguably the first syntactic LM fast enough to be used in a real time ASR system. 2 Syntactic </context>
<context position="19656" citStr="Tsuruoka et al. (2011)" startWordPosition="3348" endWordPosition="3351">st) [Store the action list into hash table] end if for all {ω, p�I E ActList [compute new states] πnew &apos; πcurrent X ω Heap.push(πnew) [push to the heap] end while lowing Sagae and Tsujii (2007) a heap is used to maintain states prioritized by their scores, for applying the best-first strategy. For each step, a state from the top of the heap is considered and all actions (and scores) are either retrieved from H or computed using g.3 We use πnew &apos;-- πcurrent X ω to denote the operation of extending a state by an action ω E Q4. 3.3 Part of Speech Tagging We use the part of speech (POS) tagger of Tsuruoka et al. (2011), a transition based model with a Perceptron and a lookahead heuristic process. The tagger processes w left to right. States are defined as πi = {ci, w}: a sequence of assigned tags up to wi (ci = t1t2 ... ti−1) and the word sequence w. Q is defined simply as the set of possible POS tags (T) that can be applied. The final state is reached once all the positions are tagged. For f we use the features of Tsuruoka et al. (2011). The kernel features are �f(πi) = {ti−2, ti−1, wi−2, wi−1, wi, wi+1, wi+2}. While the tagger extracts prefix and suffix features, it suffices to look at wi for determining </context>
<context position="22429" citStr="Tsuruoka et al. (2011)" startWordPosition="3852" endWordPosition="3855">uracy of a simpler model. We are given two models, M1 and M2, as well as a large collection of unlabeled text. Model M1 is slow but very accurate while M2 is fast but obtains lower accuracy. Up-training applies M1 to tag the unlabeled data, which is then used as training data for M2. Like self-training, a model is retrained on automatic output, but here the output comes form a more accurate model. Petrov et al. (2010) used up-training as a domain adaptation technique: a constituent parser – which is more robust to domain changes – was used to label a new domain, and a fast dependency parser 5 Tsuruoka et al. (2011) shows that the lookahead search improves the performance of the local ”history-based” models for different NLP tasks was trained on the automatically labeled data. We use a similar idea where our goal is to recover the accuracy lost from using simpler models. Note that while up-training uses two models, it differs from co-training since we care about improving only one model (M2). Additionally, the models can vary in different ways. For example, they could be the same algorithm with different pruning methods, which can lead to faster but less accurate models. We apply up-training to improve t</context>
</contexts>
<marker>Tsuruoka, Miyao, Kazama, 2011</marker>
<rawString>Yoshimasa Tsuruoka, Yusuke Miyao, and Jun’ichi Kazama. 2011. Learning with Lookahead : Can History-Based Models Rival Globally Optimized Models ? In Proc. CoNLL, number June, pages 238– 246.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
<author>Sameer Pradhan</author>
<author>Lance Ramshaw</author>
<author>Martha Palmer</author>
</authors>
<title>OntoNotes Release 2.0. Linguistic Data Consortium,</title>
<date>2008</date>
<location>Nianwen Xue, Mitchell Marcus, Ann Taylor, Craig Greenberg, Eduard Hovy, Robert Belvin, and Ann Houston,</location>
<contexts>
<context position="30055" citStr="Weischedel et al. (2008)" startWordPosition="5097" endWordPosition="5100">ning dev04f BN data 2.5 hrs Supervised training: dep. parser, POS tagger Ontonotes BN treebank+ WSJ Penn treebank 1.3m words, 59k sent. Supervised training: constituent parser Ontonotes BN treebank + WSJ Penn treebank 1.3m words, 59k sent. Up-training: dependency parser, POS tagger TDT4 closed captions+EARS BN03 closed caption 193m words available Evaluation: up-training BN treebank test (following Huang et al. (2010)) 20k words, 1.1k sent. Evaluation: ASR transcription rt04 BN evaluation 4 hrs, 45k words Table 2: A summary of the data for training and evaluation. The Ontonotes corpus is from Weischedel et al. (2008). (a) 10000 1000 Elapsed Time (sec) 10 1 10 (N) 100 1000 N--‐best Size 1000000 100000 100 No Sharing Substructure Sharing (b) Elapsed Time (sec) 10000 1000 100 10 1 1 10 (N) 100 1000 N--‐best Size No Sharing Substructure Sharing Figure 4: Elapsed time for (a) parsing and (b) POS tagging the N-best lists with and without substructure sharing. Substr. Share (sec) LM WER No Yes Baseline 4-gram 15.1 - - Syntactic LM 14.8 8,658 1,648 + up-train 14.6 Table 3: Speedups and WER for hill climbing rescoring. Substructure sharing yields a 5.3 times speedup. The times for with and without up-training are </context>
</contexts>
<marker>Weischedel, Pradhan, Ramshaw, Palmer, 2008</marker>
<rawString>Ralph Weischedel, Sameer Pradhan, Lance Ramshaw, Martha Palmer, Nianwen Xue, Mitchell Marcus, Ann Taylor, Craig Greenberg, Eduard Hovy, Robert Belvin, and Ann Houston, 2008. OntoNotes Release 2.0. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>