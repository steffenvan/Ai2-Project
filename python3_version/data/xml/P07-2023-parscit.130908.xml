<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015650">
<title confidence="0.995265">
Test Collection Selection and Gold Standard Generation
for a Multiply-Annotated Opinion Corpus
</title>
<author confidence="0.999169">
Lun-Wei Ku, Yong-Shen Lo and Hsin-Hsi Chen
</author>
<affiliation confidence="0.9956325">
Department of Computer Science and Information Engineering
National Taiwan University
</affiliation>
<email confidence="0.998258">
{lwku, yslo}@nlg.csie.ntu.edu.tw; hhchen@csie.ntu.edu.tw
</email>
<sectionHeader confidence="0.993909" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999894384615385">
Opinion analysis is an important research
topic in recent years. However, there are
no common methods to create evaluation
corpora. This paper introduces a method
for developing opinion corpora involving
multiple annotators. The characteristics of
the created corpus are discussed, and the
methodologies to select more consistent
testing collections and their corresponding
gold standards are proposed. Under the
gold standards, an opinion extraction sys-
tem is evaluated. The experiment results
show some interesting phenomena.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999968125">
Opinion information processing has been studied
for several years. Researchers extracted opinions
from words, sentences, and documents, and both
rule-based and statistical models are investigated
(Wiebe et al., 2002; Pang et al., 2002). The
evaluation metrics precision, recall and f-measure
are usually adopted.
A reliable corpus is very important for the opin-
ion information processing because the annotations
of opinions concern human perspectives. Though
the corpora created by researchers were analyzed
(Wiebe et al., 2002), the methods to increase the
reliability of them were seldom touched. The strict
and lenient metrics for opinions were mentioned,
but not discussed in details together with the cor-
pora and their annotations.
This paper discusses the selection of testing col-
lections and the generation of the corresponding
gold standards under multiple annotations. These
testing collections are further used in an opinion
extraction system and the system is evaluated with
the corresponding gold standards. The analysis of
human annotations makes the improvements of
opinion analysis systems feasible.
</bodyText>
<page confidence="0.998546">
89
</page>
<sectionHeader confidence="0.978795" genericHeader="method">
2 Corpus Annotation
</sectionHeader>
<bodyText confidence="0.999070176470588">
Opinion corpora are constructed for the research of
opinion tasks, such as opinion extraction, opinion
polarity judgment, opinion holder extraction,
opinion summarization, opinion question
answering, etc.. The materials of our opinion
corpus are news documents from NTCIR CIRB020
and CIRB040 test collections. A total of 32 topics
concerning opinions are selected, and each
document is annotated by three annotators.
Because different people often feel differently
about an opinion due to their own perspectives,
multiple annotators are necessary to build a
reliable corpus. For each sentence, whether it is
relevant to a given topic, whether it is an opinion,
and if it is, its polarity, are assigned. The holders
of opinions are also annotated. The details of this
corpus are shown in Table 1.
</bodyText>
<table confidence="0.9440755">
Topics Documents Sentences
Quantity 32 843 11,907
</table>
<tableCaption confidence="0.999709">
Table 1. Corpus size
</tableCaption>
<sectionHeader confidence="0.616868" genericHeader="method">
3 Analysis of Annotated Corpus
</sectionHeader>
<bodyText confidence="0.999921428571429">
As mentioned, each sentence in our opinion corpus
is annotated by three annotators. Although this is a
must for building reliable annotations, the incon-
sistency is unavoidable. In this section, all the
possible combinations of annotations are listed and
two methods are introduced to evaluate the quality
of the human-tagged opinion corpora.
</bodyText>
<subsectionHeader confidence="0.99995">
3.1 Combinations of annotations
</subsectionHeader>
<bodyText confidence="0.975029210526316">
Three major properties are annotated for sen-
tences in this corpus, i.e., the relevancy, the opin-
ionated issue, and the holder of the opinion. The
combinations of relevancy annotations are simple,
and annotators usually have no argument over the
opinion holders. However, for the annotation of
the opinionated issue, the situation is more com-
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 89–92,
Prague, June 2007. c�2007 Association for Computational Linguistics
plex. Annotations may have an argument about
whether a sentence contains opinions, and their
annotations may not be consistent on the polarities
of an opinion. Here we focus on the annotations of
the opinionated issue. Sentences may be consid-
ered as opinions only when more than two annota-
tors mark them opinionated. Therefore, they are
targets for analysis. The possible combinations of
opinionated sentences and their polarity are shown
in Figure 1.
</bodyText>
<table confidence="0.9693115">
A B
P 3 P 3
N P P N P
X N N X N
X X X
C E
P 3 2 N
P X N P X
N N X P P
N P X N X
X X P N P
P N X N
N P X
X
D P X N
Positive/Neutral/Negative
P 2
N P
X N
X
</table>
<figureCaption confidence="0.993398">
Figure 1. Possible combinations of annotations
</figureCaption>
<bodyText confidence="0.9994333">
In Figure 1, Cases A, B, C are those sentences
which are annotated as opinionated by all three
annotators, while cases D, E are those sentences
which are annotated as opinionated only by two
annotators. In case A and case D, the polarities
annotated by annotators are identical. In case B,
the polarities annotated by two of three annotators
are agreed. However, in cases C and E, the polari-
ties annotated disagree with each other. The statis-
tics of these five cases are shown in Table 2.
</bodyText>
<figure confidence="0.54281">
Case A B C D E All
Number 1,660 1,076 124 2,413 1,826 7,099
</figure>
<tableCaption confidence="0.979509">
Table 2. Statistics of cases A-E
</tableCaption>
<subsectionHeader confidence="0.952275">
3.2 Inconsistency
</subsectionHeader>
<bodyText confidence="0.999954789473684">
Multiple annotators bring the inconsistency. There
are several kinds of inconsistency in annotations,
for example, relevant/non-relevant, opinion-
ated/non-opinionated, and the inconsistency of po-
larities. The relevant/non-relevant inconsistency is
more like an information retrieval issue. For opin-
ions, because their strength varies, sometimes it is
hard for annotators to tell if a sentence is opinion-
ated. However, for the opinion polarities, the in-
consistency between positive and negative annota-
tions is obviously stronger than that between posi-
tive and neutral, or neutral and negative ones.
Here we define a sentence “strongly inconsistent”
if both positive and negative polarities are assigned
to a sentence by different annotators. The strong
inconsistency may occur in case B (171), C (124),
and E (270). In the corpus, only about 8% sen-
tences are strongly inconsistent, which shows the
annotations are reliable.
</bodyText>
<subsectionHeader confidence="0.990914">
3.3 Kappa value for agreement
</subsectionHeader>
<bodyText confidence="0.9997142">
We further assess the usability of the annotated
corpus by Kappa values. Kappa value gives a
quantitative measure of the magnitude of inter-
annotator agreement. Table 3 shows a commonly
used scale of the Kappa values.
</bodyText>
<table confidence="0.998645142857143">
Kappa value Meaning
&lt;0 less than change agreement
0.01 -0.20 slight agreement
0.21 -0.40 fair agreement
0.41 -0.60 moderate agreement
0.61 -0.80 substantial agreement
0.81 -0.99 almost perfect agreement
</table>
<tableCaption confidence="0.999869">
Table 3. Interpretation of Kappa value
</tableCaption>
<bodyText confidence="0.947569">
The inconsistency of annotations brings difficul-
ties in generating the gold standard. Sentences
should first be selected as the testing collection,
</bodyText>
<page confidence="0.990749">
90
</page>
<bodyText confidence="0.999961857142857">
and then the corresponding gold standard can be
generated. Our aim is to generate testing collec-
tions and their gold standards which agree mostly
to annotators. Therefore, we analyze the kappa
value not between annotators, but between the an-
notator and the gold standard. The methodologies
are introduced in the next section.
</bodyText>
<sectionHeader confidence="0.802977" genericHeader="method">
4 Testing Collections and Gold Standards
</sectionHeader>
<bodyText confidence="0.999958285714286">
The gold standard of relevance, the opinionated
issue, and the opinion holder must be generated
according to all the annotations. Answers are cho-
sen based on the agreement of annotations. Con-
sidering the agreement among annotations them-
selves, the strict and the lenient testing collections
and their corresponding gold standard are gener-
ated. Considering the Kappa values of each anno-
tator and the gold standard, topics with high agree-
ment are selected as the testing collection. More-
over, considering the consistency of polarities, the
substantial consistent testing collection is gener-
ated. In summary, two metrics for generating gold
standards and four testing collections are adopted.
</bodyText>
<subsectionHeader confidence="0.99226">
4.1 Strict and lenient
</subsectionHeader>
<bodyText confidence="0.9999525">
Namely, the strict metric is different from the leni-
ent metric in the agreement of annotations. For the
strict metric, sentences with annotations agreed by
all three annotators are selected as the testing col-
lection and the annotations are treated as the strict
gold standard; for the lenient metric, sentences
with annotations agreed by at least two annotators
are selected as the testing collection and the major-
ity of annotations are treated as the lenient gold
standard. For example, for the experiments of ex-
tracting opinion sentences, sentences in cases A, B,
and C in Figure 1 are selected in both strict and
lenient testing collections, while sentences in cases
D and E are selected only in the lenient testing col-
lection because three annotations are not totally
agreed with one another. For the experiments of
opinion polarity judgment, sentences in case A in
Figure 1 are selected in both strict and lenient test-
ing collections, while sentences in cases B, C, D
and E are selected only in the lenient testing col-
lection. Because every opinion sentence should be
given a polarity, the polarities of sentences in cases
B and D are the majority of annotations, while the
polarity of sentences in cases C are given the po-
larity neutral in the lenient gold standard. The po-
larities of sentences in case E are decided by rules
P+X=P, N+X=N, and P+N=X. As for opinion
holders, holders are found in opinion sentences of
each testing collection. The strict and lenient met-
rics are also applied in annotations of relevance.
</bodyText>
<subsectionHeader confidence="0.994923">
4.2 High agreement
</subsectionHeader>
<bodyText confidence="0.9995821875">
To see how the generated gold standards agree
with the annotations of all annotators, we analyze
the kappa value from the agreements of each anno-
tator and the gold standard for all 32 topics. Each
topic has two groups of documents from NTCIR:
very relevant and relevant to topic. However, one
topic has only the relevant type document, it re-
sults in a total of 63 (2*31+1) groups of documents.
Note that the lenient metric is applied for generat-
ing the gold standard of this testing collection be-
cause the strict metric needs perfect agreement
with each annotator’s annotations. The distribu-
tion of kappa values of 63 groups is shown in Ta-
ble 4 and Table 5. The cumulative frequency bar
graphs of Table 4 and Table 5 are shown in Figure
2 and Figure 3.
</bodyText>
<table confidence="0.9790765">
Kappa &lt;=00-0.2 0.21-0.4 0.41-0.6 0.61-0.8 0.81-0.99
Number 1 2 12 14 33 1
</table>
<tableCaption confidence="0.994455">
Table 4. Kappa values for opinion extraction
</tableCaption>
<table confidence="0.9968395">
Kappa &lt;=00-0.2 0.21-0.4 0.41-0.6 0.61-0.8 0.81-0.99
Number 9 0 7 21 17 9
</table>
<tableCaption confidence="0.999106">
Table 5. Kappa values for polarity judgment
</tableCaption>
<figureCaption confidence="0.9991995">
Figure 2. Cumulative frequency of Table 4
Figure 3. Cumulative frequency of Table 5
</figureCaption>
<bodyText confidence="0.974814">
According to Figure 2 and Figure 3, document
groups with kappa values above 0.4 are selected as
</bodyText>
<figure confidence="0.988786">
70
60
50
40
30
20
10
0
&lt;=0 0-0.2 0.21-0.4 0.41-0.6 0.61-0.8 0.81-0.99
3
1
15
29
62 63
70
60
50
40
30
20
10
0
&lt;=0 0-0.2 0.21-0.4 0.41-0.6 0.61-0.8 0.81-0.99
9 9
16
</figure>
<page confidence="0.67496775">
37
54
63
91
</page>
<bodyText confidence="0.9999256">
the high agreement testing collection, that is,
document groups with moderate agreement in Ta-
ble 3. A total of 48 document groups are collected
for opinion extraction and 47 document groups are
collected for opinion polarity judgment.
</bodyText>
<subsectionHeader confidence="0.998927">
4.3 Substantial Consistency
</subsectionHeader>
<bodyText confidence="0.99997775">
In Section 3.2, sentences which are “strongly in-
consistent” are defined. The substantial consis-
tency test collection expels strongly inconsistent
sentences to achieve a higher consistency. Notice
that this test collection is still less consistent than
the strict test collection, which is perfectly consis-
tent with annotators. The lenient metric is applied
for generating the gold standard for this collection.
</bodyText>
<sectionHeader confidence="0.986634" genericHeader="method">
5 An Opinion System -- CopeOpi
</sectionHeader>
<bodyText confidence="0.99655364">
A Chinese opinion extraction system for opinion-
ated information, CopeOpi, is introduced here. (Ku
et al., 2007) When judging the opinion polarity of
a sentence in this system, three factors are consid-
ered: sentiment words, negation operators and
opinion holders. Every sentiment word has its own
sentiment score. If a sentence consists of more
positive sentiments than negative sentiments, it
must reveal something good, and vice versa. How-
ever, a negation operator, such as ”not”
and ”never”, may totally change the sentiment po-
larity of a sentiment word. Therefore, when a nega-
tion operator appears together with a sentiment
word, the opinion score of the sentiment word S
will be changed to -S to keep the strength but re-
verse the polarity. Opinion holders are also consid-
ered for opinion sentences, but how they influence
opinions has not been investigated yet. As a result,
they are weighted equally at first. A word is con-
sidered an opinion holder of an opinion sentence if
either one of the following two criteria is met:
1. The part of speech is a person name, organi-
zation name or personal.
2. The word is in class A (human), type Ae (job)
of the Cilin Dictionary (Mei et al., 1982).
</bodyText>
<sectionHeader confidence="0.973354" genericHeader="evaluation">
6 Evaluation Results and Discussions
</sectionHeader>
<bodyText confidence="0.999802">
Experiment results of CopeOpi using four designed
testing collections are shown in Table 6. Under the
lenient metric with the lenient test collection, f-
measure scores 0.761 and 0.383 are achieved by
CopeOpi. The strict metric is the most severe, and
the performance drops a lot under it. Moreover,
when using high agreement (H-A) and substantial
consistency (S-C) test collections, the performance
of the system does not increase in portion to the
increase of agreement. According to the agree-
ment of annotators, people should perform best in
the strict collection, and both high agreement and
substantial consistency testing collections are eas-
ier than the lenient one. This phenomenon shows
that though this system’s performance is satisfac-
tory, its behavior is not like human beings. For a
computer system, the lenient testing collection is
fuzzier and contains more information for judg-
ment. However, this also shows that the system
may only take advantage of the surface informa-
tion. If we want our systems really judge like hu-
man beings, we should enhance the performance
on strict, high agreement, and substantial consis-
tency testing collections. This analysis gives us, or
other researchers who use this corpus for experi-
ments, a direction to improve their own systems.
</bodyText>
<table confidence="0.986916166666667">
Opinion Extraction Opinion + Polarity
Measure P R F P R F
Lenient 0.664 0.890 0.761 0.335 0.448 0.383
Strict 0.258 0.921 0.404 0.104 0.662 0.180
H-A 0.677 0.885 0.767 0.339 0.455 0.388
S-C 0.308 0.452 0.367
</table>
<tableCaption confidence="0.957309">
Table 6. Evaluation results
</tableCaption>
<sectionHeader confidence="0.998601" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.864810333333333">
Research of this paper was partially supported by Excel-
lent Research Projects of National Taiwan University,
under the contract 95R0062-AE00-02.
</bodyText>
<sectionHeader confidence="0.990745" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9993766">
Mei, J., Zhu, Y. Gao, Y. and Yin, H.. tong2yi4ci2ci2lin2.
Shanghai Dictionary Press, 1982.
Pang, B., Lee, L., and Vaithyanathan, S. (2002).
Thumbs up? Sentiment classification using machine
learning techniques. Proceedings of the 2002 Confer-
ence on EMNLP, pages 79-86.
Wiebe, J., Breck, E., Buckly, C., Cardie, C., Davis, P.,
Fraser, B., Litman, D., Pierce, D., Riloff, E., and
Wilson, T. (2002). NRRC summer workshop on
multi-perspective question answering, final report.
ARDA NRRC Summer 2002 Workshop.
Ku, L.-W., Wu, T.-H., Li, L.-Y. and Chen., H.-H.
(2007). Using Polarity Scores of Words for Sentence-
level Opinion Extraction. Proceedings of the Sixth
NTCIR Workshop.
</reference>
<page confidence="0.996007">
92
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.915792">
<title confidence="0.999778">Test Collection Selection and Gold Standard Generation for a Multiply-Annotated Opinion Corpus</title>
<author confidence="0.999543">Lun-Wei Ku</author>
<author confidence="0.999543">Yong-Shen Lo</author>
<author confidence="0.999543">Hsin-Hsi Chen</author>
<affiliation confidence="0.9993575">Department of Computer Science and Information Engineering National Taiwan University</affiliation>
<email confidence="0.921091">lwku@nlg.csie.ntu.edu.tw;hhchen@csie.ntu.edu.tw</email>
<email confidence="0.921091">yslo@nlg.csie.ntu.edu.tw;hhchen@csie.ntu.edu.tw</email>
<abstract confidence="0.999744714285714">Opinion analysis is an important research topic in recent years. However, there are no common methods to create evaluation corpora. This paper introduces a method for developing opinion corpora involving multiple annotators. The characteristics of the created corpus are discussed, and the methodologies to select more consistent testing collections and their corresponding gold standards are proposed. Under the gold standards, an opinion extraction system is evaluated. The experiment results show some interesting phenomena.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Mei</author>
<author>Y Gao Zhu</author>
<author>Y</author>
<author>H Yin</author>
</authors>
<date></date>
<publisher>Shanghai Dictionary Press,</publisher>
<marker>Mei, Zhu, Y, Yin, </marker>
<rawString>Mei, J., Zhu, Y. Gao, Y. and Yin, H.. tong2yi4ci2ci2lin2. Shanghai Dictionary Press, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
<author>S Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>Proceedings of the 2002 Conference on EMNLP,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="1068" citStr="Pang et al., 2002" startWordPosition="138" endWordPosition="141">od for developing opinion corpora involving multiple annotators. The characteristics of the created corpus are discussed, and the methodologies to select more consistent testing collections and their corresponding gold standards are proposed. Under the gold standards, an opinion extraction system is evaluated. The experiment results show some interesting phenomena. 1 Introduction Opinion information processing has been studied for several years. Researchers extracted opinions from words, sentences, and documents, and both rule-based and statistical models are investigated (Wiebe et al., 2002; Pang et al., 2002). The evaluation metrics precision, recall and f-measure are usually adopted. A reliable corpus is very important for the opinion information processing because the annotations of opinions concern human perspectives. Though the corpora created by researchers were analyzed (Wiebe et al., 2002), the methods to increase the reliability of them were seldom touched. The strict and lenient metrics for opinions were mentioned, but not discussed in details together with the corpora and their annotations. This paper discusses the selection of testing collections and the generation of the corresponding </context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Pang, B., Lee, L., and Vaithyanathan, S. (2002). Thumbs up? Sentiment classification using machine learning techniques. Proceedings of the 2002 Conference on EMNLP, pages 79-86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>E Breck</author>
<author>C Buckly</author>
<author>C Cardie</author>
<author>P Davis</author>
<author>B Fraser</author>
<author>D Litman</author>
<author>D Pierce</author>
<author>E Riloff</author>
<author>T Wilson</author>
</authors>
<title>NRRC summer workshop on multi-perspective question answering, final report.</title>
<date>2002</date>
<journal>ARDA NRRC Summer</journal>
<note>Workshop.</note>
<contexts>
<context position="1048" citStr="Wiebe et al., 2002" startWordPosition="134" endWordPosition="137">er introduces a method for developing opinion corpora involving multiple annotators. The characteristics of the created corpus are discussed, and the methodologies to select more consistent testing collections and their corresponding gold standards are proposed. Under the gold standards, an opinion extraction system is evaluated. The experiment results show some interesting phenomena. 1 Introduction Opinion information processing has been studied for several years. Researchers extracted opinions from words, sentences, and documents, and both rule-based and statistical models are investigated (Wiebe et al., 2002; Pang et al., 2002). The evaluation metrics precision, recall and f-measure are usually adopted. A reliable corpus is very important for the opinion information processing because the annotations of opinions concern human perspectives. Though the corpora created by researchers were analyzed (Wiebe et al., 2002), the methods to increase the reliability of them were seldom touched. The strict and lenient metrics for opinions were mentioned, but not discussed in details together with the corpora and their annotations. This paper discusses the selection of testing collections and the generation o</context>
</contexts>
<marker>Wiebe, Breck, Buckly, Cardie, Davis, Fraser, Litman, Pierce, Riloff, Wilson, 2002</marker>
<rawString>Wiebe, J., Breck, E., Buckly, C., Cardie, C., Davis, P., Fraser, B., Litman, D., Pierce, D., Riloff, E., and Wilson, T. (2002). NRRC summer workshop on multi-perspective question answering, final report. ARDA NRRC Summer 2002 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L-W Ku</author>
<author>T-H Wu</author>
<author>L-Y Li</author>
<author>H-H Chen</author>
</authors>
<title>Using Polarity Scores of Words for Sentencelevel Opinion Extraction.</title>
<date>2007</date>
<booktitle>Proceedings of the Sixth NTCIR Workshop.</booktitle>
<contexts>
<context position="11318" citStr="Ku et al., 2007" startWordPosition="1825" endWordPosition="1828">ted for opinion polarity judgment. 4.3 Substantial Consistency In Section 3.2, sentences which are “strongly inconsistent” are defined. The substantial consistency test collection expels strongly inconsistent sentences to achieve a higher consistency. Notice that this test collection is still less consistent than the strict test collection, which is perfectly consistent with annotators. The lenient metric is applied for generating the gold standard for this collection. 5 An Opinion System -- CopeOpi A Chinese opinion extraction system for opinionated information, CopeOpi, is introduced here. (Ku et al., 2007) When judging the opinion polarity of a sentence in this system, three factors are considered: sentiment words, negation operators and opinion holders. Every sentiment word has its own sentiment score. If a sentence consists of more positive sentiments than negative sentiments, it must reveal something good, and vice versa. However, a negation operator, such as ”not” and ”never”, may totally change the sentiment polarity of a sentiment word. Therefore, when a negation operator appears together with a sentiment word, the opinion score of the sentiment word S will be changed to -S to keep the st</context>
</contexts>
<marker>Ku, Wu, Li, Chen, 2007</marker>
<rawString>Ku, L.-W., Wu, T.-H., Li, L.-Y. and Chen., H.-H. (2007). Using Polarity Scores of Words for Sentencelevel Opinion Extraction. Proceedings of the Sixth NTCIR Workshop.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>