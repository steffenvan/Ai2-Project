<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.126485">
<title confidence="0.801529">
iNeATS: Interactive Multi-Document Summarization
</title>
<author confidence="0.998592">
Anton Leuski, Chin-Yew Lin, Eduard Hovy
</author>
<affiliation confidence="0.893885333333333">
University of Southern California
Information Sciences Institute
4676 Admiralty Way, Suite 1001
</affiliation>
<address confidence="0.699958">
Marina Del Rey, CA 90292-6695
</address>
<email confidence="0.998498">
fleuski,cyl,hovyl@isi.edu
</email>
<sectionHeader confidence="0.998597" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999941333333333">
We describe iNeATS – an interactive
multi-document summarization system
that integrates a state-of-the-art summa-
rization engine with an advanced user in-
terface. Three main goals of the sys-
tem are: (1) provide a user with control
over the summarization process, (2) sup-
port exploration of the document set with
the summary as the staring point, and (3)
combine text summaries with alternative
presentations such as a map-based visual-
ization of documents.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99924175">
The goal of a good document summary is to provide
a user with a presentation of the substance of a body
of material in a coherent and concise form. Ideally, a
summary would contain only the “right” amount of
the interesting information and it would omit all the
redundant and “uninteresting” material. The quality
of the summary depends strongly on users’ present
need – a summary that focuses on one of several top-
ics contained in the material may prove to be either
very useful or completely useless depending on what
users’ interests are.
An automatic multi-document summarization
system generally works by extracting relevant sen-
tences from the documents and arranging them in a
coherent order (McKeown et al., 2001; Over, 2001).
The system has to make decisions on the summary’s
size, redundancy, and focus. Any of these deci-
sions may have a significant impact on the quality
of the output. We believe a system that directly in-
volves the user in the summary generation process
and adapts to her input will produce better sum-
maries. Additionally, it has been shown that users
are more satisfied with systems that visualize their
decisions and give the user a sense of control over
the process (Koenemann and Belkin, 1996).
We see three ways in which interactivity and
visualization can be incorporated into the multi-
document summarization process:
</bodyText>
<listItem confidence="0.831727583333333">
1. give the user direct control over the summariza-
tion parameters such as size, redundancy, and
focus of the summaries.
2. support rapid browsing of the document set us-
ing the summary as the starting point and com-
bining the multi-document summary with sum-
maries for individual documents.
3. incorporate alternative formats for organizing
and displaying the summary, e.g., a set of news
stories can be summarized by placing the sto-
ries on a world map based on the locations of
the events described in the stories.
</listItem>
<bodyText confidence="0.998024714285714">
In this paper we describe iNeATS (Interactive
NExt generation Text Summarization) which ad-
dresses these three directions. The iNeATS system
is built on top of the NeATS multi-document sum-
marization system. In the following section we give
a brief overview of the NeATS system and in Sec-
tion 3 describe the interactive version.
</bodyText>
<sectionHeader confidence="0.998946" genericHeader="introduction">
2 NeATS
</sectionHeader>
<bodyText confidence="0.999793675675676">
NeATS (Lin and Hovy, 2002) is an extraction-
based multi-document summarization system. It is
among the top two performers in DUC 2001 and
2002 (Over, 2001). It consists of three main com-
ponents:
Content Selection The goal of content selection is
to identify important concepts mentioned in
a document collection. NeATS computes the
likelihood ratio (Dunning, 1993) to identify key
concepts in unigrams, bigrams, and trigrams
and clusters these concepts in order to identify
major subtopics within the main topic. Each
sentence in the document set is then ranked, us-
ing the key concept structures. These n-gram
key concepts are called topic signatures.
Content Filtering NeATS uses three different fil-
ters: sentence position, stigma words, and re-
dundancy filter. Sentence position has been
used as a good important content filter since
the late 60s (Edmundson, 1969). NeATS ap-
plies a simple sentence filter that only retains
the N lead sentences. Some sentences start
with conjunctions, quotation marks, pronouns,
and the verb “say” and its derivatives. These
stigma words usually cause discontinuities in
summaries. The system reduces the scores of
these sentences to demote their ranks and avoid
including them in summaries of small sizes. To
address the redundancy problem, NeATS uses a
simplified version of CMU’s MMR (Goldstein
et al., 1999) algorithm. A sentence is added to
the summary if and only if its content has less
than X percent overlap with the summary.
Content Presentation To ensure coherence of the
summary, NeATS pairs each sentence with an
introduction sentence. It then outputs the final
sentences in their chronological order.
</bodyText>
<sectionHeader confidence="0.999578" genericHeader="method">
3 Interactive Summarization
</sectionHeader>
<bodyText confidence="0.999921375">
Figure 1 shows a screenshot of the iNeATS system.
We divide the screen into three parts corresponding
to the three directions outlined in Section 1. The
control panel displays the summarization parame-
ters on the left side of the screen. The document
panel shows the document text on the right side. The
summary panel presents the summaries in the mid-
dle of the screen.
</bodyText>
<subsectionHeader confidence="0.999328">
3.1 Controlling Summarization Process
</subsectionHeader>
<bodyText confidence="0.999984833333333">
The top of the control panel provides the user with
control over the summarization process. The first set
of widgets contains controls for the summary size,
sentence position, and redundancy filters. The sec-
ond row of parameters displays the set of topic sig-
natures identified by the iNeATS engine. The se-
lected subset of the topic signatures defines the con-
tent focus for the summary. If the user enters a new
value for one of the parameters or selects a different
subset of the topic signatures, iNeATS immediately
regenerates and redisplays the summary text in the
top portion of the summary panel.
</bodyText>
<subsectionHeader confidence="0.999603">
3.2 Browsing Document Set
</subsectionHeader>
<bodyText confidence="0.999890793103448">
iNeATS facilitates browsing of the document set by
providing (1) an overview of the documents, (2)
linking the sentences in the summary to the original
documents, and (3) using sentence zooming to high-
light the most relevant sentences in the documents.
The bottom part of the control panel is occupied
by the document thumbnails. The documents are ar-
ranged in chronological order and each document is
assigned a unique color to paint the text background
for the document. The same color is used to draw
the document thumbnail in the control panel, to fill
up the text background in the document panel, and to
paint the background of those sentences in the sum-
mary that were collected from the document. For
example, the screenshot shows that a user selected
the second document which was assigned the or-
ange color. The document panel displays the doc-
ument text on orange background. iNeATS selected
the first two summary sentences from this document,
so both sentences are shown in the summary panel
with orange background.
The sentences in the summary are linked to the
original documents in two ways. First, the docu-
ment can be identified by the color of the sentence.
Second, each sentence is a hyperlink to the docu-
ment – if the user moves the mouse over a sentence,
the sentence is underlined in the summary and high-
lighted in the document text. For example, the first
sentence of the summary is the document sentence
</bodyText>
<figureCaption confidence="0.999716">
Figure 1: Screenshot of the iNeATS system.
</figureCaption>
<bodyText confidence="0.999986775">
highlighted in the document panel. If the user clicks
on the sentence, iNeATS brings the source document
into the document panel and scrolls the window to
make the sentence visible.
The relevant parts of the documents are illumi-
nated using the technique that we call sentence
zooming. We make the text color intensity of each
sentence proportional to the relevance score com-
puted by the iNeATS engine and a zooming parame-
ter which can be controlled by the user with a slider
widget at the top of the document panel. The higher
the sentence score, the darker the text is. Conversely,
sentences that blend into the background have a very
low sentence score. The zooming parameter con-
trols the proportion of the top ranked sentences vis-
ible on the screen at each moment. This zooming
affects both the full-text and the thumbnail docu-
ment presentations. Combining the sentence zoom-
ing with the document set overview, the user can
quickly see which document contains most of the
relevant material and where approximately in the
document this material is placed.
The document panel in Figure 1 shows sentences
that achieve 50% on the sentence score scale. We see
that the first half of the document contains two black
sentences: the first sentence that starts with “US In-
surers...”, the other starts with “President George...”.
Both sentences have a very high score and they were
selected for the summary. Note, that the very first
sentence in the document is the headline and it is not
used for summarization. Note also that the sentence
that starts with “However,...” scored much lower
than the selected two – its color is approximately
half diluted into the background.
There are quite a few sentences in the second part
of the document that scored relatively high. How-
ever, these sentences are below the sentence position
cutoff so they do not appear in the summary. We il-
lustrate this by rendering such sentences in slanted
style.
</bodyText>
<subsectionHeader confidence="0.999485">
3.3 Alternative Summaries
</subsectionHeader>
<bodyText confidence="0.999970869565217">
The bottom part of the summary panel is occupied
by the map-based visualization. We use BBN’s
IdentiFinder (Bikel et al., 1997) to detect the names
of geographic locations in the document set. We
then select the most frequently used location names
and place them on world map. Each location is iden-
tified by a black dot followed by a frequency chart
and the location name. The frequency chart is a bar
chart where each bar corresponds to a document.
The bar is painted using the document color and the
length of the bar is proportional to the number of
times the location name is used in the document.
The document set we used in our example de-
scribes the progress of the hurricane Andrew and its
effect on Florida, Louisiana, and Texas. Note that
the source documents and therefore the bars in the
chart are arranged in the chronological order. The
name “Miami” appears first in the second document,
“New Orleans” in the third document, and “Texas” is
prominent in the last two documents. We can make
some conclusions on the hurricane’s path through
the region – it traveled from south-east and made its
landing somewhere in Louisiana and Texas.
</bodyText>
<sectionHeader confidence="0.999836" genericHeader="conclusions">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999952230769231">
The iNeATS system is implemented in Java. It uses
the NeATS engine implemented in Perl and C. It
runs on any platform that supports these environ-
ments. We are currently working on making the sys-
tem available on our web site.
We plan to extend the system by adding temporal
visualization that places the documents on a timeline
based on the date and time values extracted from the
text.
We plan to conduct a user-based evaluation of the
system to compare users’ satisfaction with both the
automatically generated summaries and summaries
produced by iNeATS.
</bodyText>
<sectionHeader confidence="0.998922" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999003243243243">
Daniel M. Bikel, Scott Miller, Richard Schwartz, and
Ralph Weischedel. 1997. Nymble: a high-
performance learning name-finder. In Proceedings of
ANLP-97, pages 194–201.
Ted E. Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational Lin-
guistics, 19(1):61–74.
H. P. Edmundson. 1969. New methods in automatic ex-
traction. Journal of the ACM, 16(2):264–285.
Jade Goldstein, Mark Kantrowitz, Vibhu O. Mittal, and
Jaime G. Carbonell. 1999. Summarizing text docu-
ments: Sentence selection and evaluation metrics. In
Research and Development in Information Retrieval,
pages 121–128.
Jurgen Koenemann and Nicholas J. Belkin. 1996. A case
for interaction: A study of interactive information re-
trieval behavior and effectivness. In Proceedings of
ACM SIGCHI Conference on Human Factors in Com-
puting Systems, pages 205–212, Vancouver, British
Columbia, Canada.
Chin-Yew Lin and Eduard Hovy. 2002. From single
to multi-document summarization: a prototype sys-
tem and it evaluation. In Proceedings of the 40th
Anniversary Meeting of the Association for Computa-
tional Linguistics (ACL-02), Philadelphia, PA, USA.
Kathleen R. McKeown, Regina Barzilay, David Evans,
Vasileios Hatzivassiloglou, Barry Schiffman, and Si-
mone Teufel. 2001. Columbia multi-document sum-
marization: Approach and evaluation. In Proceed-
ings of the Workshop on Text Summarization, ACM SI-
GIR Conference 2001. DARPA/NIST, Document Un-
derstanding Conference.
Paul Over. 2001. Introduction to duc-2001: an intrin-
sic evaluation of generic news text summarization sys-
tems. In Proceedings of the Workshop on Text Summa-
rization, ACMSIGIR Conference 2001. DARPA/NIST,
Document Understanding Conference.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.972675">
<title confidence="0.999952">iNeATS: Interactive Multi-Document Summarization</title>
<author confidence="0.999732">Anton Leuski</author>
<author confidence="0.999732">Chin-Yew Lin</author>
<author confidence="0.999732">Eduard Hovy</author>
<affiliation confidence="0.9993955">University of Southern California Information Sciences Institute</affiliation>
<address confidence="0.9943825">4676 Admiralty Way, Suite 1001 Marina Del Rey, CA 90292-6695</address>
<abstract confidence="0.998857846153846">We describe iNeATS – an interactive multi-document summarization system that integrates a state-of-the-art summarization engine with an advanced user interface. Three main goals of the system are: (1) provide a user with control over the summarization process, (2) support exploration of the document set with the summary as the staring point, and (3) combine text summaries with alternative presentations such as a map-based visualization of documents.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
<author>Scott Miller</author>
<author>Richard Schwartz</author>
<author>Ralph Weischedel</author>
</authors>
<title>Nymble: a highperformance learning name-finder.</title>
<date>1997</date>
<booktitle>In Proceedings of ANLP-97,</booktitle>
<pages>194--201</pages>
<contexts>
<context position="9179" citStr="Bikel et al., 1997" startWordPosition="1505" endWordPosition="1508">ne and it is not used for summarization. Note also that the sentence that starts with “However,...” scored much lower than the selected two – its color is approximately half diluted into the background. There are quite a few sentences in the second part of the document that scored relatively high. However, these sentences are below the sentence position cutoff so they do not appear in the summary. We illustrate this by rendering such sentences in slanted style. 3.3 Alternative Summaries The bottom part of the summary panel is occupied by the map-based visualization. We use BBN’s IdentiFinder (Bikel et al., 1997) to detect the names of geographic locations in the document set. We then select the most frequently used location names and place them on world map. Each location is identified by a black dot followed by a frequency chart and the location name. The frequency chart is a bar chart where each bar corresponds to a document. The bar is painted using the document color and the length of the bar is proportional to the number of times the location name is used in the document. The document set we used in our example describes the progress of the hurricane Andrew and its effect on Florida, Louisiana, </context>
</contexts>
<marker>Bikel, Miller, Schwartz, Weischedel, 1997</marker>
<rawString>Daniel M. Bikel, Scott Miller, Richard Schwartz, and Ralph Weischedel. 1997. Nymble: a highperformance learning name-finder. In Proceedings of ANLP-97, pages 194–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted E Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="3282" citStr="Dunning, 1993" startWordPosition="524" endWordPosition="525">ich addresses these three directions. The iNeATS system is built on top of the NeATS multi-document summarization system. In the following section we give a brief overview of the NeATS system and in Section 3 describe the interactive version. 2 NeATS NeATS (Lin and Hovy, 2002) is an extractionbased multi-document summarization system. It is among the top two performers in DUC 2001 and 2002 (Over, 2001). It consists of three main components: Content Selection The goal of content selection is to identify important concepts mentioned in a document collection. NeATS computes the likelihood ratio (Dunning, 1993) to identify key concepts in unigrams, bigrams, and trigrams and clusters these concepts in order to identify major subtopics within the main topic. Each sentence in the document set is then ranked, using the key concept structures. These n-gram key concepts are called topic signatures. Content Filtering NeATS uses three different filters: sentence position, stigma words, and redundancy filter. Sentence position has been used as a good important content filter since the late 60s (Edmundson, 1969). NeATS applies a simple sentence filter that only retains the N lead sentences. Some sentences sta</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Ted E. Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1):61–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Edmundson</author>
</authors>
<title>New methods in automatic extraction.</title>
<date>1969</date>
<journal>Journal of the ACM,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="3783" citStr="Edmundson, 1969" startWordPosition="603" endWordPosition="604">identify important concepts mentioned in a document collection. NeATS computes the likelihood ratio (Dunning, 1993) to identify key concepts in unigrams, bigrams, and trigrams and clusters these concepts in order to identify major subtopics within the main topic. Each sentence in the document set is then ranked, using the key concept structures. These n-gram key concepts are called topic signatures. Content Filtering NeATS uses three different filters: sentence position, stigma words, and redundancy filter. Sentence position has been used as a good important content filter since the late 60s (Edmundson, 1969). NeATS applies a simple sentence filter that only retains the N lead sentences. Some sentences start with conjunctions, quotation marks, pronouns, and the verb “say” and its derivatives. These stigma words usually cause discontinuities in summaries. The system reduces the scores of these sentences to demote their ranks and avoid including them in summaries of small sizes. To address the redundancy problem, NeATS uses a simplified version of CMU’s MMR (Goldstein et al., 1999) algorithm. A sentence is added to the summary if and only if its content has less than X percent overlap with the summa</context>
</contexts>
<marker>Edmundson, 1969</marker>
<rawString>H. P. Edmundson. 1969. New methods in automatic extraction. Journal of the ACM, 16(2):264–285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jade Goldstein</author>
<author>Mark Kantrowitz</author>
<author>Vibhu O Mittal</author>
<author>Jaime G Carbonell</author>
</authors>
<title>Summarizing text documents: Sentence selection and evaluation metrics.</title>
<date>1999</date>
<booktitle>In Research and Development in Information Retrieval,</booktitle>
<pages>121--128</pages>
<contexts>
<context position="4263" citStr="Goldstein et al., 1999" startWordPosition="676" endWordPosition="679">n, stigma words, and redundancy filter. Sentence position has been used as a good important content filter since the late 60s (Edmundson, 1969). NeATS applies a simple sentence filter that only retains the N lead sentences. Some sentences start with conjunctions, quotation marks, pronouns, and the verb “say” and its derivatives. These stigma words usually cause discontinuities in summaries. The system reduces the scores of these sentences to demote their ranks and avoid including them in summaries of small sizes. To address the redundancy problem, NeATS uses a simplified version of CMU’s MMR (Goldstein et al., 1999) algorithm. A sentence is added to the summary if and only if its content has less than X percent overlap with the summary. Content Presentation To ensure coherence of the summary, NeATS pairs each sentence with an introduction sentence. It then outputs the final sentences in their chronological order. 3 Interactive Summarization Figure 1 shows a screenshot of the iNeATS system. We divide the screen into three parts corresponding to the three directions outlined in Section 1. The control panel displays the summarization parameters on the left side of the screen. The document panel shows the do</context>
</contexts>
<marker>Goldstein, Kantrowitz, Mittal, Carbonell, 1999</marker>
<rawString>Jade Goldstein, Mark Kantrowitz, Vibhu O. Mittal, and Jaime G. Carbonell. 1999. Summarizing text documents: Sentence selection and evaluation metrics. In Research and Development in Information Retrieval, pages 121–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jurgen Koenemann</author>
<author>Nicholas J Belkin</author>
</authors>
<title>A case for interaction: A study of interactive information retrieval behavior and effectivness.</title>
<date>1996</date>
<booktitle>In Proceedings of ACM SIGCHI Conference on Human Factors in Computing Systems,</booktitle>
<pages>205--212</pages>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="1944" citStr="Koenemann and Belkin, 1996" startWordPosition="305" endWordPosition="308">ly works by extracting relevant sentences from the documents and arranging them in a coherent order (McKeown et al., 2001; Over, 2001). The system has to make decisions on the summary’s size, redundancy, and focus. Any of these decisions may have a significant impact on the quality of the output. We believe a system that directly involves the user in the summary generation process and adapts to her input will produce better summaries. Additionally, it has been shown that users are more satisfied with systems that visualize their decisions and give the user a sense of control over the process (Koenemann and Belkin, 1996). We see three ways in which interactivity and visualization can be incorporated into the multidocument summarization process: 1. give the user direct control over the summarization parameters such as size, redundancy, and focus of the summaries. 2. support rapid browsing of the document set using the summary as the starting point and combining the multi-document summary with summaries for individual documents. 3. incorporate alternative formats for organizing and displaying the summary, e.g., a set of news stories can be summarized by placing the stories on a world map based on the locations </context>
</contexts>
<marker>Koenemann, Belkin, 1996</marker>
<rawString>Jurgen Koenemann and Nicholas J. Belkin. 1996. A case for interaction: A study of interactive information retrieval behavior and effectivness. In Proceedings of ACM SIGCHI Conference on Human Factors in Computing Systems, pages 205–212, Vancouver, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>From single to multi-document summarization: a prototype system and it evaluation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Anniversary Meeting of the Association for Computational Linguistics (ACL-02),</booktitle>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="2945" citStr="Lin and Hovy, 2002" startWordPosition="470" endWordPosition="473">maries for individual documents. 3. incorporate alternative formats for organizing and displaying the summary, e.g., a set of news stories can be summarized by placing the stories on a world map based on the locations of the events described in the stories. In this paper we describe iNeATS (Interactive NExt generation Text Summarization) which addresses these three directions. The iNeATS system is built on top of the NeATS multi-document summarization system. In the following section we give a brief overview of the NeATS system and in Section 3 describe the interactive version. 2 NeATS NeATS (Lin and Hovy, 2002) is an extractionbased multi-document summarization system. It is among the top two performers in DUC 2001 and 2002 (Over, 2001). It consists of three main components: Content Selection The goal of content selection is to identify important concepts mentioned in a document collection. NeATS computes the likelihood ratio (Dunning, 1993) to identify key concepts in unigrams, bigrams, and trigrams and clusters these concepts in order to identify major subtopics within the main topic. Each sentence in the document set is then ranked, using the key concept structures. These n-gram key concepts are </context>
</contexts>
<marker>Lin, Hovy, 2002</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 2002. From single to multi-document summarization: a prototype system and it evaluation. In Proceedings of the 40th Anniversary Meeting of the Association for Computational Linguistics (ACL-02), Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R McKeown</author>
<author>Regina Barzilay</author>
<author>David Evans</author>
<author>Vasileios Hatzivassiloglou</author>
<author>Barry Schiffman</author>
<author>Simone Teufel</author>
</authors>
<title>Columbia multi-document summarization: Approach and evaluation.</title>
<date>2001</date>
<booktitle>In Proceedings of the Workshop on Text Summarization, ACM SIGIR Conference 2001. DARPA/NIST, Document Understanding Conference.</booktitle>
<contexts>
<context position="1438" citStr="McKeown et al., 2001" startWordPosition="218" endWordPosition="221">body of material in a coherent and concise form. Ideally, a summary would contain only the “right” amount of the interesting information and it would omit all the redundant and “uninteresting” material. The quality of the summary depends strongly on users’ present need – a summary that focuses on one of several topics contained in the material may prove to be either very useful or completely useless depending on what users’ interests are. An automatic multi-document summarization system generally works by extracting relevant sentences from the documents and arranging them in a coherent order (McKeown et al., 2001; Over, 2001). The system has to make decisions on the summary’s size, redundancy, and focus. Any of these decisions may have a significant impact on the quality of the output. We believe a system that directly involves the user in the summary generation process and adapts to her input will produce better summaries. Additionally, it has been shown that users are more satisfied with systems that visualize their decisions and give the user a sense of control over the process (Koenemann and Belkin, 1996). We see three ways in which interactivity and visualization can be incorporated into the mult</context>
</contexts>
<marker>McKeown, Barzilay, Evans, Hatzivassiloglou, Schiffman, Teufel, 2001</marker>
<rawString>Kathleen R. McKeown, Regina Barzilay, David Evans, Vasileios Hatzivassiloglou, Barry Schiffman, and Simone Teufel. 2001. Columbia multi-document summarization: Approach and evaluation. In Proceedings of the Workshop on Text Summarization, ACM SIGIR Conference 2001. DARPA/NIST, Document Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Over</author>
</authors>
<title>Introduction to duc-2001: an intrinsic evaluation of generic news text summarization systems.</title>
<date>2001</date>
<booktitle>In Proceedings of the Workshop on Text Summarization, ACMSIGIR Conference</booktitle>
<institution>DARPA/NIST, Document Understanding Conference.</institution>
<contexts>
<context position="1451" citStr="Over, 2001" startWordPosition="222" endWordPosition="223">coherent and concise form. Ideally, a summary would contain only the “right” amount of the interesting information and it would omit all the redundant and “uninteresting” material. The quality of the summary depends strongly on users’ present need – a summary that focuses on one of several topics contained in the material may prove to be either very useful or completely useless depending on what users’ interests are. An automatic multi-document summarization system generally works by extracting relevant sentences from the documents and arranging them in a coherent order (McKeown et al., 2001; Over, 2001). The system has to make decisions on the summary’s size, redundancy, and focus. Any of these decisions may have a significant impact on the quality of the output. We believe a system that directly involves the user in the summary generation process and adapts to her input will produce better summaries. Additionally, it has been shown that users are more satisfied with systems that visualize their decisions and give the user a sense of control over the process (Koenemann and Belkin, 1996). We see three ways in which interactivity and visualization can be incorporated into the multidocument sum</context>
<context position="3073" citStr="Over, 2001" startWordPosition="493" endWordPosition="494">es can be summarized by placing the stories on a world map based on the locations of the events described in the stories. In this paper we describe iNeATS (Interactive NExt generation Text Summarization) which addresses these three directions. The iNeATS system is built on top of the NeATS multi-document summarization system. In the following section we give a brief overview of the NeATS system and in Section 3 describe the interactive version. 2 NeATS NeATS (Lin and Hovy, 2002) is an extractionbased multi-document summarization system. It is among the top two performers in DUC 2001 and 2002 (Over, 2001). It consists of three main components: Content Selection The goal of content selection is to identify important concepts mentioned in a document collection. NeATS computes the likelihood ratio (Dunning, 1993) to identify key concepts in unigrams, bigrams, and trigrams and clusters these concepts in order to identify major subtopics within the main topic. Each sentence in the document set is then ranked, using the key concept structures. These n-gram key concepts are called topic signatures. Content Filtering NeATS uses three different filters: sentence position, stigma words, and redundancy f</context>
</contexts>
<marker>Over, 2001</marker>
<rawString>Paul Over. 2001. Introduction to duc-2001: an intrinsic evaluation of generic news text summarization systems. In Proceedings of the Workshop on Text Summarization, ACMSIGIR Conference 2001. DARPA/NIST, Document Understanding Conference.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>