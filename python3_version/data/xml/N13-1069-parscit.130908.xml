<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.087021">
<title confidence="0.9985115">
Systematic Comparison of Professional and Crowdsourced Reference
Translations for Machine Translation
</title>
<author confidence="0.914489">
Rabih Zbib, Gretchen Markiewicz, Spyros Matsoukas,
Richard Schwartz, John Makhoul
</author>
<affiliation confidence="0.797694">
Raytheon BBN Technologies
</affiliation>
<address confidence="0.787728">
Cambridge, MA 02138, USA
</address>
<email confidence="0.999418">
{rzbib,gmarkiew,smatsouk,schwartz,makhoul}@bbn.com
</email>
<sectionHeader confidence="0.998603" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998961583333333">
We present a systematic study of the effect of
crowdsourced translations on Machine Trans-
lation performance. We compare Machine
Translation systems trained on the same data
but with translations obtained using Amazon’s
Mechanical Turk vs. professional translations,
and show that the same performance is ob-
tained from Mechanical Turk translations at
1/5th the cost. We also show that adding a Me-
chanical Turk reference translation of the de-
velopment set improves parameter tuning and
output evaluation.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999880444444445">
Online crowdsourcing services have been shown to
be a cheap and effective data annotation resource
for various Natural Language Processing (NLP)
tasks (Callison-Burch and Dredze, 2010; Zaidan and
Callison-Burch, 2011a; Zaidan and Callison-Burch,
2011b). The resulting quality of annotations is high
enough to be used for training statistical NLP mod-
els, with a saving in cost and time of up to an or-
der of magnitude. Statistical Machine Translation
(SMT) is one of the NLP tasks that can benefit from
crowdsourced annotations. With appropriate quality
control mechanisms, reference translations collected
by crowdsourcing have been successfully used for
training and evaluating SMT systems (Zbib et al.,
2012; Zaidan and Callison-Burch, 2011b).
In this work, we used Amazon’s Mechanical Turk
(MTurk) to obtain alternative reference translations
of four Arabic-English parallel corpora previously
released by the Linguistic Data Consortium (LDC)
for the DARPA BOLT program. This data, totaling
over 500K Arabic tokens, was originally collected
from web discussion forums and translated profes-
sionally to English. We used alternative MTurk
translations of the same data to train and evalua-
tion MT systems; and conducted the first systematic
study that quantifies the effect of the reference trans-
lation process on MT output. We found that:
</bodyText>
<listItem confidence="0.999765363636364">
• Mechanical Turk can be used to translate
enough data for training an MT system at
1/10th the price of professional translation, and
at a much faster rate.
• Training MT systems on MTurk reference
translations gives the same performance as
training with professional translations at 20%
of the cost.
• A second translation of the development set ob-
tained via MTurk improves parameter tuning
and output evaluation.
</listItem>
<sectionHeader confidence="0.990515" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999906">
There have been several publications on crowd-
sourcing data annotation for NLP. Callison-Burch
and Dredze (2010) give an overview of the NAACL-
2010 Workshop on using Mechanical Turk for data
annotation. They describe tasks for which MTurk
can be used, and summarize a set of best practices.
They also include references to the workshop con-
tributions.
Zaidan and Callison-Burch (2011a) created a
monolingual Arabic data set rich in dialectal con-
tent from user commentaries on newspaper web-
sites. They hired native Arabic speakers on MTurk
</bodyText>
<page confidence="0.974013">
612
</page>
<subsectionHeader confidence="0.294173">
Proceedings of NAACL-HLT 2013, pages 612–616,
</subsectionHeader>
<bodyText confidence="0.988491322580645">
Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics
to identify the dialect level and used the collected la-
bels to train automatic dialect identification systems.
They did not translate the collected data, however.
Zaidan and Callison-Burch (2011b) obtained mul-
tiple translations of the NIST 2009 Urdu-English
evaluation set using MTurk. They trained a statis-
tical model on a set of features to select among the
multiple translations. They showed that the MTurk
translations selected by their model approached the
range of quality of professional translations, and that
the selected MTurk translations can be used reliably
to score the outputs of different MT systems submit-
ted to the NIST evaluation. Unlike our work, they
did not investigate the use of crowdsourced trans-
lations for training or parameter tuning. Zbib et al.
(2012) trained a Dialectal Arabic to English MT sys-
tem using Mechanical Turk translations. But the
data they translated on MTurk does not have profes-
sional translations to conduct the systematic com-
parison we do in this paper.
It is well known that scoring MT output against
multiple references improves MT scores such as
BLEU significantly, since it increases the chance of
matching n-grams between the MT output and the
references. Tuning system parameter with multi-
ple references also improves machine translation for
the same reason Madnani et al. (2007) and Madnani
et al. (2008) showed that tuning on additional ref-
erences obtained by automatic paraphrasing helps
when only few tuning references are available.
</bodyText>
<sectionHeader confidence="0.986926" genericHeader="method">
3 Data Translation
</sectionHeader>
<bodyText confidence="0.991625194444444">
The data we used are Arabic-English parallel cor-
pora released by the LDC for the DARPA BOLT
Phase 1 program1. The data was collected from
Egyptian online discussion forums, and consists of
separate discussion threads, each composed of an
initial user posting and multiple reply postings. The
data tends to be bimodal: the first posting in the
thread is often formal and expressed in Modern
Standard Arabic, while the subsequent threads use
a less formal style, and contain colloquial Egyptian
dialect. The data was manually segmented into sen-
tence units, and translated professionally.
We used non-professional translators hired on
MTurk to get second translations. We used several
1Corpora: LDC2012E15, LDC2012E19, LDC2012E55
measures to control the quality of translations and
detect cheaters. Those include the rendering of Ara-
bic sentences as images, comparing the output to
Google Translate and Bing Translator, and other au-
tomatic checks. The quality of individual worker’s
translations was quantified by asking a native Ara-
bic speaker judge to score a sample of the Turker’s
translations. The translation task unit (aka Human
Intelligence Task or HIT) consisted of a sequence
of contiguous sentences from a discussion thread
amounting to between 40 and 60 words. The in-
structions were simply to translate the Arabic source
fully and accurately, and to take surrounding sen-
tence segments into account to help resolve ambigu-
ities. The HIT rewards were set to 2.5¢ per word.
At the end of the effort, we had 26 different work-
ers translate 567K Arabic tokens in 4 weeks. The
resulting translations were less fluent than their pro-
fessional counterparts, and 10% shorter on average.
The following section presents results of MT exper-
iments using the MTurk translations.
</bodyText>
<sectionHeader confidence="0.997653" genericHeader="method">
4 NIT Experiments
</sectionHeader>
<bodyText confidence="0.999733565217391">
The MT system used is based on a string-to-
dependency-tree hierarchical model of Shen et
al. (2008). Sentence alignment was done using
GIZA++ (Och and Ney, 2003). Decoder fea-
tures include translation probabilities, smoothed lex-
ical probabilities, and a dependency tree language
model. Additionally, we used 50,000 sparse, binary-
valued source and target features based on Chiang
et al. (2009). The English language model was
trained on 7 billion words from the LDC Gigaword
corpus and from a web crawl. We used expected
BLEU maximization (Devlin, 2009) to tune feature
weights.
We defined a tuning set (3581 segments, 43.3K
tokens) and a test set (4166 segments, 47.7K to-
kens) using LDC2012E30, the corpus designated
as a development set by the LDC, augmented with
around 50K Words held out from LDC2012E15
and LDC2012E19, to make a development set large
enough to tune the large number of feature weights2.
The remaining data was used for training. We de-
fined three nested training sets containing 100K,
200K and 400K Arabic tokens respectively, with
</bodyText>
<footnote confidence="0.937121">
2Only full forum threads were held out
</footnote>
<page confidence="0.983909">
613
</page>
<table confidence="0.999919">
Training Web-forum Only Newswire(10MW)+Web-forum
100KW 200KW 400KW 0KW 100KW 200KW 400KW
Prof. refs 17.71 20.23 22.61 22.82 24.05 24.85 25.19
MTurk refs 16.41 18.43 20.08 22.82 23.79 24.20 24.51
Two Training refs 19.03 21.19 23.06 22.82 24.26 25.19 25.38
Add’l Training data - 19.80 21.53 22.82 - 24.31 25.16
</table>
<tableCaption confidence="0.9987655">
Table 1: Comparison of the effect of web forum training data when using professional and MTurk reference transla-
tions. All results use professional references for the tuning and test sets.
</tableCaption>
<bodyText confidence="0.99921">
two versions of each set: one with the professional
reference translations for the target, and the other
with the same source data, but the MTurk transla-
tions. We defined two versions of the test and tuning
sets similarly. We report translation results in terms
of lower-case BLEU scores (Papineni et al., 2002).
</bodyText>
<subsectionHeader confidence="0.99528">
4.1 Training Data References
</subsectionHeader>
<bodyText confidence="0.999996409090909">
We first study the effect of training data refer-
ences, varying the amount of training data and type
of translations, while using the same professional
translation references for tuning and scoring. The
first set of baseline experiments were trained on
web forum data only, using professional transla-
tions. The first line of Table 1 shows that doubling of
the training data adds 2.5 then 2.3 BLEU points. We
repeated the experiments, but with MTurk training
references, and saw that the scores are lower by 1.3-
2.5 BLEU points, depending on the size of training
data, and that the gain obtained from doubling the
training data decreases to 2.0 and 1.6 BLEU points.
The lower MT scores and slower learning curve of
the MTurk systems are both due to the lower quality
of the translations, and to the mismatch with the pro-
fessional development set translations (we discuss
this issue further in §4.3). However, by interpolation
of the MT scores, we find that the same MT perfor-
mance can be obtained by using twice the amount of
MTurk translated data as professional data. Consid-
ering that the MTurk translations is 10 times cheaper
than professional translations (2.5¢ versus 25-30¢),
this constitutes a cost ratio of 5x.
We repeated the above experiments, but this time
added 10 million words of parallel data from the
NIST MT 2012 corpora (mostly news) for training.
We weighted the web forum part of the training data
by a factor of 5. Note from the results in the right
half of Table 1 that the newswire data improves the
BLEU score by 2.5 to 6.3 BLEU points, depend-
ing on the size of the web forum data. This signif-
icant improvement is because some of the web fo-
rum user postings are formal and written in MSA
(§3). More relevant to our aims is the comparison
when we vary the web forum training references in
the presence of the newswire training. The differ-
ence between the MTurk translation systems and the
professional translation drops to 0.26-0.68 points.
We conclude that in a domain adaptation scenario,
where out-of-domain training data (i.e. newswire)
already exists, crowdsourced translations for the in-
domain (i.e. web forum) training data can be used
with little to no loss in MT performance.
</bodyText>
<subsectionHeader confidence="0.993971">
4.2 More Data vs. Multiple Translations
</subsectionHeader>
<bodyText confidence="0.999989590909091">
To our knowledge no previous work has compared
using multiple reference translations for training
data versus using additional training data of the same
size. We studied this question by using both transla-
tions on the target side of the training data. Using the
MTurk translations in addition to the professional
translations in training gave a gain of 0.4 to 1.3
BLEU points (bottom half of Table 1). The gain was
smaller in the presence of the GALE newswire data.
When we compared with using the same amount of
different training data instead of multiple references,
we saw that training on new data with crowdsourced
translations is better: training on two translations of
100KW gives 19.03, compared to 19.80 when train-
ing on a single translation of 200KW. The advantage
of different-source data drops to 0.34 points when
we start with 200KW. With a larger initial corpus,
the additional source coverage of new data is not as
critical, and the advantage of more variety on the
target-side of the extracted translation rules becomes
more competitive. This coverage is even less criti-
cal in the presence of the news data, where the ad-
</bodyText>
<page confidence="0.997372">
614
</page>
<table confidence="0.9997854">
Training Tuning Test Training Data Size
100KW 200KW 400KW 400KW(no lex)
Prof. Prof. Prof. 17.71 20.23 22.61 20.01
Prof. Prof. Prof.+MTurk 22.53 25.75 28.38 25.42
Prof. Prof. (len=0.95) Prof.+MTurk 23.63 26.84 29.54 26.17
Prof. Prof.+MTurk Prof.+MTurk 25.26 28.44 30.94 27.22
MTurk MTurk MTurk 16.66 18.47 20.35 17.75
MTurk MTurk Prof.+MTurk 23.83 26.45 28.66 25.44
MTurk MTurk (len=1.05) Prof.+MTurk 23.73 26.19 28.74 25.87
MTurk Prof.+MTurk Prof.+MTurk 24.91 27.66 29.78 26.45
</table>
<tableCaption confidence="0.999684">
Table 2: Effect of Tuning and Scoring References on MT.
</tableCaption>
<bodyText confidence="0.9912225">
vantage of new web forum source data disappears
(lower-right quadrant of Table 1).
</bodyText>
<subsectionHeader confidence="0.999561">
4.3 Development Data References
</subsectionHeader>
<bodyText confidence="0.99999214">
So far, we have focused on varying training data
conditions, and kept the tuning and evaluation con-
ditions fixed. But since we have re-translated the
tuning and test sets on MTurk as well, we can study
the effect of their reference translations on MT. As
Table 2 shows, scoring the MT output using both
reference translations, the BLEU scores increase by
over 5 points (and more for the MTurk-trained sys-
tem). This increase by itself is not remarkable. What
is important to note is that the gain obtained by dou-
bling the amount of training data is larger when mea-
sured using the multiple reference test set. We also
ran experiments with 400KW training data, but with
the lexical smoothing features (Koehn et al., 2003;
Devlin, 2009) turned off. The bigger gains show that
improvements in the MT output (from additional
training or new features) can be better measured us-
ing a second MTurk reference of the test set.
Finally, we study the effect of tuning the system
parameters using both translation references. Look-
ing at the system trained on the professional trans-
lations, we see a gain of 2.5 to 2.7 BLEU points
from adding the MTurk references to the tuning set.
But as we mentioned earlier, the MTurk transla-
tions are shorter than the professional translations
by around 10% on average. Tuning on both ref-
erences, therefore, shortens the system output by
around 5%. To neutralize the effect of length mis-
match, we compared to a fairer baseline tuned on
the professional references only, but we tuned the
output-to-reference length ratio to be 0.95 (thus pro-
ducing a shorter output). In this case, we see a gain
of 1.4 points from adding the MTurk references to
the tuning set.
We also used the multiple-reference tuning set
to retune the systems trained on MTurk transla-
tions. Comparing that to a baseline that is tuned and
scored using MTurk references only, we see a gain
of around 1%. Note, however, that in this case the
length mismach is reversed, and the output of the
multiple-reference system is around 5% longer than
that of the baseline. If we compare with a baseline
that is tuned with a length ratio of 1.05 (to produce a
longer output), we see the gain shrink only slightly.
To sum up this section, a second set of refer-
ence translations obtained via MTurk makes mea-
surements of improvement on the test set more re-
liable. Also, a second set of references for tuning
improves the output of the MT systems trained on
either professional or MTurk references.
</bodyText>
<sectionHeader confidence="0.999301" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999773777777778">
We compared professional and crowdsourced trans-
lations of the same data for training, tuning and scor-
ing Arabic-English SMT systems. We showed that
the crowdsourced translations yield the same MT
performance as professional translations for as lit-
tle as 20% of the cost. We also showed that a sec-
ond crowsourced reference translation of the devel-
opment set allows for a more accurate evaluation of
MT output.
</bodyText>
<sectionHeader confidence="0.998834" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.969855">
This work was supported in part by DARPA/IPTO
Contract No. HR0011-12-C-0014 under the BOLT
</bodyText>
<page confidence="0.997028">
615
</page>
<bodyText confidence="0.9894824">
Program. The views expressed are those of the au-
thors and do not reflect the official policy or position
of the Department of Defense or the U.S. Govern-
ment. Distribution Statement A (Approved for Pub-
lic Release, Distribution Unlimited).
</bodyText>
<sectionHeader confidence="0.998673" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99922923880597">
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with Amazon’s Mechanical
Turk. In Proceedings of the NAACL HLT 2010 Work-
shop on Creating Speech and Language Data with
Amazon’s Mechanical Turk, pages 1–12, Los Angeles,
June.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In NAACL ’09: Proceedings of the 2009 Human Lan-
guage Technology Conference of the North American
Chapter of the Association for Computational Linguis-
tics, Boulder, Colorado.
Jacob Devlin. 2009. Lexical features for statistical ma-
chine translation. Master’s thesis, University of Mary-
land, December.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of the 2003
Human Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 48–54, Edmonton, Canada.
Nitin Madnani, Necip Fazil, Ayan, Philip Resnik, and
Bonnie Dorr. 2007. Using paraphrases for parameter
tuning in statistical machine translation. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, pages 120–127, Prague, Czech Republic.
Association for Computational Linguistics.
Nitin Madnani, Philip Resnik, Bonnie Dorr, and Richard
Schwartz. 2008. Are multiple reference transla-
tions necessary? investigating the value of paraphrased
reference translations in parameter optimization. In
Proceedings of the 8th Conf. of the Association for
Machine Translation in the Americas (AMTA 2008),
Waikiki, Hawaii, USA.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19–51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), Philadelphia, PA.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
577–585, Columbus, Ohio.
Omar F. Zaidan and Chris Callison-Burch. 2011a.
The Arabic online commentary dataset: an annotated
dataset of informal Arabic with high dialectal content.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 37–41, Portland, Oregon,
June.
Omar F. Zaidan and Chris Callison-Burch. 2011b.
Crowdsourcing translation: Professional quality from
non-professionals. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, pages 1220–
1229, Portland, Oregon, June.
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David
Stallard, Spyros Matsoukas, Richard Schwartz, John
Makhoul, Omar F. Zaidan, and Chris Callison-Burch.
2012. Machine translation of arabic dialects. In The
2012 Conference of the North American Chapter of the
Association for Computational Linguistics, Montreal,
June. Association for Computational Linguistics.
</reference>
<page confidence="0.998561">
616
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.649674">
<title confidence="0.941261">Systematic Comparison of Professional and Crowdsourced Translations for Machine Translation Rabih Zbib, Gretchen Markiewicz, Spyros</title>
<author confidence="0.995215">Richard Schwartz</author>
<author confidence="0.995215">John</author>
<affiliation confidence="0.815076">Raytheon BBN</affiliation>
<address confidence="0.998326">Cambridge, MA 02138,</address>
<abstract confidence="0.997525923076923">We present a systematic study of the effect of crowdsourced translations on Machine Translation performance. We compare Machine Translation systems trained on the same data but with translations obtained using Amazon’s Mechanical Turk vs. professional translations, and show that the same performance is obtained from Mechanical Turk translations at 1/5th the cost. We also show that adding a Mechanical Turk reference translation of the development set improves parameter tuning and output evaluation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Mark Dredze</author>
</authors>
<title>Creating speech and language data with Amazon’s Mechanical Turk.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk,</booktitle>
<pages>1--12</pages>
<location>Los Angeles,</location>
<contexts>
<context position="996" citStr="Callison-Burch and Dredze, 2010" startWordPosition="130" endWordPosition="133">ns on Machine Translation performance. We compare Machine Translation systems trained on the same data but with translations obtained using Amazon’s Mechanical Turk vs. professional translations, and show that the same performance is obtained from Mechanical Turk translations at 1/5th the cost. We also show that adding a Mechanical Turk reference translation of the development set improves parameter tuning and output evaluation. 1 Introduction Online crowdsourcing services have been shown to be a cheap and effective data annotation resource for various Natural Language Processing (NLP) tasks (Callison-Burch and Dredze, 2010; Zaidan and Callison-Burch, 2011a; Zaidan and Callison-Burch, 2011b). The resulting quality of annotations is high enough to be used for training statistical NLP models, with a saving in cost and time of up to an order of magnitude. Statistical Machine Translation (SMT) is one of the NLP tasks that can benefit from crowdsourced annotations. With appropriate quality control mechanisms, reference translations collected by crowdsourcing have been successfully used for training and evaluating SMT systems (Zbib et al., 2012; Zaidan and Callison-Burch, 2011b). In this work, we used Amazon’s Mechani</context>
<context position="2693" citStr="Callison-Burch and Dredze (2010)" startWordPosition="391" endWordPosition="394">t quantifies the effect of the reference translation process on MT output. We found that: • Mechanical Turk can be used to translate enough data for training an MT system at 1/10th the price of professional translation, and at a much faster rate. • Training MT systems on MTurk reference translations gives the same performance as training with professional translations at 20% of the cost. • A second translation of the development set obtained via MTurk improves parameter tuning and output evaluation. 2 Previous Work There have been several publications on crowdsourcing data annotation for NLP. Callison-Burch and Dredze (2010) give an overview of the NAACL2010 Workshop on using Mechanical Turk for data annotation. They describe tasks for which MTurk can be used, and summarize a set of best practices. They also include references to the workshop contributions. Zaidan and Callison-Burch (2011a) created a monolingual Arabic data set rich in dialectal content from user commentaries on newspaper websites. They hired native Arabic speakers on MTurk 612 Proceedings of NAACL-HLT 2013, pages 612–616, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics to identify the dialect level and used the</context>
</contexts>
<marker>Callison-Burch, Dredze, 2010</marker>
<rawString>Chris Callison-Burch and Mark Dredze. 2010. Creating speech and language data with Amazon’s Mechanical Turk. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 1–12, Los Angeles, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Kevin Knight</author>
<author>Wei Wang</author>
</authors>
<title>11,001 new features for statistical machine translation.</title>
<date>2009</date>
<booktitle>In NAACL ’09: Proceedings of the 2009 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<location>Boulder, Colorado.</location>
<contexts>
<context position="6941" citStr="Chiang et al. (2009)" startWordPosition="1060" endWordPosition="1063">ns in 4 weeks. The resulting translations were less fluent than their professional counterparts, and 10% shorter on average. The following section presents results of MT experiments using the MTurk translations. 4 NIT Experiments The MT system used is based on a string-todependency-tree hierarchical model of Shen et al. (2008). Sentence alignment was done using GIZA++ (Och and Ney, 2003). Decoder features include translation probabilities, smoothed lexical probabilities, and a dependency tree language model. Additionally, we used 50,000 sparse, binaryvalued source and target features based on Chiang et al. (2009). The English language model was trained on 7 billion words from the LDC Gigaword corpus and from a web crawl. We used expected BLEU maximization (Devlin, 2009) to tune feature weights. We defined a tuning set (3581 segments, 43.3K tokens) and a test set (4166 segments, 47.7K tokens) using LDC2012E30, the corpus designated as a development set by the LDC, augmented with around 50K Words held out from LDC2012E15 and LDC2012E19, to make a development set large enough to tune the large number of feature weights2. The remaining data was used for training. We defined three nested training sets cont</context>
</contexts>
<marker>Chiang, Knight, Wang, 2009</marker>
<rawString>David Chiang, Kevin Knight, and Wei Wang. 2009. 11,001 new features for statistical machine translation. In NAACL ’09: Proceedings of the 2009 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Devlin</author>
</authors>
<title>Lexical features for statistical machine translation. Master’s thesis,</title>
<date>2009</date>
<institution>University of Maryland,</institution>
<contexts>
<context position="7101" citStr="Devlin, 2009" startWordPosition="1089" endWordPosition="1090">MT experiments using the MTurk translations. 4 NIT Experiments The MT system used is based on a string-todependency-tree hierarchical model of Shen et al. (2008). Sentence alignment was done using GIZA++ (Och and Ney, 2003). Decoder features include translation probabilities, smoothed lexical probabilities, and a dependency tree language model. Additionally, we used 50,000 sparse, binaryvalued source and target features based on Chiang et al. (2009). The English language model was trained on 7 billion words from the LDC Gigaword corpus and from a web crawl. We used expected BLEU maximization (Devlin, 2009) to tune feature weights. We defined a tuning set (3581 segments, 43.3K tokens) and a test set (4166 segments, 47.7K tokens) using LDC2012E30, the corpus designated as a development set by the LDC, augmented with around 50K Words held out from LDC2012E15 and LDC2012E19, to make a development set large enough to tune the large number of feature weights2. The remaining data was used for training. We defined three nested training sets containing 100K, 200K and 400K Arabic tokens respectively, with 2Only full forum threads were held out 613 Training Web-forum Only Newswire(10MW)+Web-forum 100KW 20</context>
<context position="13253" citStr="Devlin, 2009" startWordPosition="2114" endWordPosition="2115">ince we have re-translated the tuning and test sets on MTurk as well, we can study the effect of their reference translations on MT. As Table 2 shows, scoring the MT output using both reference translations, the BLEU scores increase by over 5 points (and more for the MTurk-trained system). This increase by itself is not remarkable. What is important to note is that the gain obtained by doubling the amount of training data is larger when measured using the multiple reference test set. We also ran experiments with 400KW training data, but with the lexical smoothing features (Koehn et al., 2003; Devlin, 2009) turned off. The bigger gains show that improvements in the MT output (from additional training or new features) can be better measured using a second MTurk reference of the test set. Finally, we study the effect of tuning the system parameters using both translation references. Looking at the system trained on the professional translations, we see a gain of 2.5 to 2.7 BLEU points from adding the MTurk references to the tuning set. But as we mentioned earlier, the MTurk translations are shorter than the professional translations by around 10% on average. Tuning on both references, therefore, s</context>
</contexts>
<marker>Devlin, 2009</marker>
<rawString>Jacob Devlin. 2009. Lexical features for statistical machine translation. Master’s thesis, University of Maryland, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>48--54</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="13238" citStr="Koehn et al., 2003" startWordPosition="2110" endWordPosition="2113">ditions fixed. But since we have re-translated the tuning and test sets on MTurk as well, we can study the effect of their reference translations on MT. As Table 2 shows, scoring the MT output using both reference translations, the BLEU scores increase by over 5 points (and more for the MTurk-trained system). This increase by itself is not remarkable. What is important to note is that the gain obtained by doubling the amount of training data is larger when measured using the multiple reference test set. We also ran experiments with 400KW training data, but with the lexical smoothing features (Koehn et al., 2003; Devlin, 2009) turned off. The bigger gains show that improvements in the MT output (from additional training or new features) can be better measured using a second MTurk reference of the test set. Finally, we study the effect of tuning the system parameters using both translation references. Looking at the system trained on the professional translations, we see a gain of 2.5 to 2.7 BLEU points from adding the MTurk references to the tuning set. But as we mentioned earlier, the MTurk translations are shorter than the professional translations by around 10% on average. Tuning on both reference</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 48–54, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Necip Fazil</author>
<author>Philip Resnik Ayan</author>
<author>Bonnie Dorr</author>
</authors>
<title>Using paraphrases for parameter tuning in statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>120--127</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="4584" citStr="Madnani et al. (2007)" startWordPosition="689" endWordPosition="692">ced translations for training or parameter tuning. Zbib et al. (2012) trained a Dialectal Arabic to English MT system using Mechanical Turk translations. But the data they translated on MTurk does not have professional translations to conduct the systematic comparison we do in this paper. It is well known that scoring MT output against multiple references improves MT scores such as BLEU significantly, since it increases the chance of matching n-grams between the MT output and the references. Tuning system parameter with multiple references also improves machine translation for the same reason Madnani et al. (2007) and Madnani et al. (2008) showed that tuning on additional references obtained by automatic paraphrasing helps when only few tuning references are available. 3 Data Translation The data we used are Arabic-English parallel corpora released by the LDC for the DARPA BOLT Phase 1 program1. The data was collected from Egyptian online discussion forums, and consists of separate discussion threads, each composed of an initial user posting and multiple reply postings. The data tends to be bimodal: the first posting in the thread is often formal and expressed in Modern Standard Arabic, while the subse</context>
</contexts>
<marker>Madnani, Fazil, Ayan, Dorr, 2007</marker>
<rawString>Nitin Madnani, Necip Fazil, Ayan, Philip Resnik, and Bonnie Dorr. 2007. Using paraphrases for parameter tuning in statistical machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 120–127, Prague, Czech Republic. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Philip Resnik</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>Are multiple reference translations necessary? investigating the value of paraphrased reference translations in parameter optimization.</title>
<date>2008</date>
<booktitle>In Proceedings of the 8th Conf. of the Association for Machine Translation in the Americas (AMTA</booktitle>
<location>Waikiki, Hawaii, USA.</location>
<contexts>
<context position="4610" citStr="Madnani et al. (2008)" startWordPosition="694" endWordPosition="697">ing or parameter tuning. Zbib et al. (2012) trained a Dialectal Arabic to English MT system using Mechanical Turk translations. But the data they translated on MTurk does not have professional translations to conduct the systematic comparison we do in this paper. It is well known that scoring MT output against multiple references improves MT scores such as BLEU significantly, since it increases the chance of matching n-grams between the MT output and the references. Tuning system parameter with multiple references also improves machine translation for the same reason Madnani et al. (2007) and Madnani et al. (2008) showed that tuning on additional references obtained by automatic paraphrasing helps when only few tuning references are available. 3 Data Translation The data we used are Arabic-English parallel corpora released by the LDC for the DARPA BOLT Phase 1 program1. The data was collected from Egyptian online discussion forums, and consists of separate discussion threads, each composed of an initial user posting and multiple reply postings. The data tends to be bimodal: the first posting in the thread is often formal and expressed in Modern Standard Arabic, while the subsequent threads use a less f</context>
</contexts>
<marker>Madnani, Resnik, Dorr, Schwartz, 2008</marker>
<rawString>Nitin Madnani, Philip Resnik, Bonnie Dorr, and Richard Schwartz. 2008. Are multiple reference translations necessary? investigating the value of paraphrased reference translations in parameter optimization. In Proceedings of the 8th Conf. of the Association for Machine Translation in the Americas (AMTA 2008), Waikiki, Hawaii, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="6711" citStr="Och and Ney, 2003" startWordPosition="1027" endWordPosition="1030">ully and accurately, and to take surrounding sentence segments into account to help resolve ambiguities. The HIT rewards were set to 2.5¢ per word. At the end of the effort, we had 26 different workers translate 567K Arabic tokens in 4 weeks. The resulting translations were less fluent than their professional counterparts, and 10% shorter on average. The following section presents results of MT experiments using the MTurk translations. 4 NIT Experiments The MT system used is based on a string-todependency-tree hierarchical model of Shen et al. (2008). Sentence alignment was done using GIZA++ (Och and Ney, 2003). Decoder features include translation probabilities, smoothed lexical probabilities, and a dependency tree language model. Additionally, we used 50,000 sparse, binaryvalued source and target features based on Chiang et al. (2009). The English language model was trained on 7 billion words from the LDC Gigaword corpus and from a web crawl. We used expected BLEU maximization (Devlin, 2009) to tune feature weights. We defined a tuning set (3581 segments, 43.3K tokens) and a test set (4166 segments, 47.7K tokens) using LDC2012E30, the corpus designated as a development set by the LDC, augmented wi</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="8453" citStr="Papineni et al., 2002" startWordPosition="1309" endWordPosition="1312">0 24.51 Two Training refs 19.03 21.19 23.06 22.82 24.26 25.19 25.38 Add’l Training data - 19.80 21.53 22.82 - 24.31 25.16 Table 1: Comparison of the effect of web forum training data when using professional and MTurk reference translations. All results use professional references for the tuning and test sets. two versions of each set: one with the professional reference translations for the target, and the other with the same source data, but the MTurk translations. We defined two versions of the test and tuning sets similarly. We report translation results in terms of lower-case BLEU scores (Papineni et al., 2002). 4.1 Training Data References We first study the effect of training data references, varying the amount of training data and type of translations, while using the same professional translation references for tuning and scoring. The first set of baseline experiments were trained on web forum data only, using professional translations. The first line of Table 1 shows that doubling of the training data adds 2.5 then 2.3 BLEU points. We repeated the experiments, but with MTurk training references, and saw that the scores are lower by 1.3- 2.5 BLEU points, depending on the size of training data, a</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Jinxi Xu</author>
<author>Ralph Weischedel</author>
</authors>
<title>A new string-to-dependency machine translation algorithm with a target dependency language model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>577--585</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="6649" citStr="Shen et al. (2008)" startWordPosition="1017" endWordPosition="1020"> The instructions were simply to translate the Arabic source fully and accurately, and to take surrounding sentence segments into account to help resolve ambiguities. The HIT rewards were set to 2.5¢ per word. At the end of the effort, we had 26 different workers translate 567K Arabic tokens in 4 weeks. The resulting translations were less fluent than their professional counterparts, and 10% shorter on average. The following section presents results of MT experiments using the MTurk translations. 4 NIT Experiments The MT system used is based on a string-todependency-tree hierarchical model of Shen et al. (2008). Sentence alignment was done using GIZA++ (Och and Ney, 2003). Decoder features include translation probabilities, smoothed lexical probabilities, and a dependency tree language model. Additionally, we used 50,000 sparse, binaryvalued source and target features based on Chiang et al. (2009). The English language model was trained on 7 billion words from the LDC Gigaword corpus and from a web crawl. We used expected BLEU maximization (Devlin, 2009) to tune feature weights. We defined a tuning set (3581 segments, 43.3K tokens) and a test set (4166 segments, 47.7K tokens) using LDC2012E30, the c</context>
</contexts>
<marker>Shen, Xu, Weischedel, 2008</marker>
<rawString>Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm with a target dependency language model. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL), pages 577–585, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
<author>Chris Callison-Burch</author>
</authors>
<title>The Arabic online commentary dataset: an annotated dataset of informal Arabic with high dialectal content.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>37--41</pages>
<location>Portland, Oregon,</location>
<contexts>
<context position="1029" citStr="Zaidan and Callison-Burch, 2011" startWordPosition="134" endWordPosition="137">ance. We compare Machine Translation systems trained on the same data but with translations obtained using Amazon’s Mechanical Turk vs. professional translations, and show that the same performance is obtained from Mechanical Turk translations at 1/5th the cost. We also show that adding a Mechanical Turk reference translation of the development set improves parameter tuning and output evaluation. 1 Introduction Online crowdsourcing services have been shown to be a cheap and effective data annotation resource for various Natural Language Processing (NLP) tasks (Callison-Burch and Dredze, 2010; Zaidan and Callison-Burch, 2011a; Zaidan and Callison-Burch, 2011b). The resulting quality of annotations is high enough to be used for training statistical NLP models, with a saving in cost and time of up to an order of magnitude. Statistical Machine Translation (SMT) is one of the NLP tasks that can benefit from crowdsourced annotations. With appropriate quality control mechanisms, reference translations collected by crowdsourcing have been successfully used for training and evaluating SMT systems (Zbib et al., 2012; Zaidan and Callison-Burch, 2011b). In this work, we used Amazon’s Mechanical Turk (MTurk) to obtain altern</context>
<context position="2962" citStr="Zaidan and Callison-Burch (2011" startWordPosition="435" endWordPosition="438">n MTurk reference translations gives the same performance as training with professional translations at 20% of the cost. • A second translation of the development set obtained via MTurk improves parameter tuning and output evaluation. 2 Previous Work There have been several publications on crowdsourcing data annotation for NLP. Callison-Burch and Dredze (2010) give an overview of the NAACL2010 Workshop on using Mechanical Turk for data annotation. They describe tasks for which MTurk can be used, and summarize a set of best practices. They also include references to the workshop contributions. Zaidan and Callison-Burch (2011a) created a monolingual Arabic data set rich in dialectal content from user commentaries on newspaper websites. They hired native Arabic speakers on MTurk 612 Proceedings of NAACL-HLT 2013, pages 612–616, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics to identify the dialect level and used the collected labels to train automatic dialect identification systems. They did not translate the collected data, however. Zaidan and Callison-Burch (2011b) obtained multiple translations of the NIST 2009 Urdu-English evaluation set using MTurk. They trained a statistica</context>
</contexts>
<marker>Zaidan, Callison-Burch, 2011</marker>
<rawString>Omar F. Zaidan and Chris Callison-Burch. 2011a. The Arabic online commentary dataset: an annotated dataset of informal Arabic with high dialectal content. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 37–41, Portland, Oregon, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Crowdsourcing translation: Professional quality from non-professionals.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1220--1229</pages>
<location>Portland, Oregon,</location>
<contexts>
<context position="1029" citStr="Zaidan and Callison-Burch, 2011" startWordPosition="134" endWordPosition="137">ance. We compare Machine Translation systems trained on the same data but with translations obtained using Amazon’s Mechanical Turk vs. professional translations, and show that the same performance is obtained from Mechanical Turk translations at 1/5th the cost. We also show that adding a Mechanical Turk reference translation of the development set improves parameter tuning and output evaluation. 1 Introduction Online crowdsourcing services have been shown to be a cheap and effective data annotation resource for various Natural Language Processing (NLP) tasks (Callison-Burch and Dredze, 2010; Zaidan and Callison-Burch, 2011a; Zaidan and Callison-Burch, 2011b). The resulting quality of annotations is high enough to be used for training statistical NLP models, with a saving in cost and time of up to an order of magnitude. Statistical Machine Translation (SMT) is one of the NLP tasks that can benefit from crowdsourced annotations. With appropriate quality control mechanisms, reference translations collected by crowdsourcing have been successfully used for training and evaluating SMT systems (Zbib et al., 2012; Zaidan and Callison-Burch, 2011b). In this work, we used Amazon’s Mechanical Turk (MTurk) to obtain altern</context>
<context position="2962" citStr="Zaidan and Callison-Burch (2011" startWordPosition="435" endWordPosition="438">n MTurk reference translations gives the same performance as training with professional translations at 20% of the cost. • A second translation of the development set obtained via MTurk improves parameter tuning and output evaluation. 2 Previous Work There have been several publications on crowdsourcing data annotation for NLP. Callison-Burch and Dredze (2010) give an overview of the NAACL2010 Workshop on using Mechanical Turk for data annotation. They describe tasks for which MTurk can be used, and summarize a set of best practices. They also include references to the workshop contributions. Zaidan and Callison-Burch (2011a) created a monolingual Arabic data set rich in dialectal content from user commentaries on newspaper websites. They hired native Arabic speakers on MTurk 612 Proceedings of NAACL-HLT 2013, pages 612–616, Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics to identify the dialect level and used the collected labels to train automatic dialect identification systems. They did not translate the collected data, however. Zaidan and Callison-Burch (2011b) obtained multiple translations of the NIST 2009 Urdu-English evaluation set using MTurk. They trained a statistica</context>
</contexts>
<marker>Zaidan, Callison-Burch, 2011</marker>
<rawString>Omar F. Zaidan and Chris Callison-Burch. 2011b. Crowdsourcing translation: Professional quality from non-professionals. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1220– 1229, Portland, Oregon, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rabih Zbib</author>
<author>Erika Malchiodi</author>
<author>Jacob Devlin</author>
<author>David Stallard</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
<author>Omar F Zaidan</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Machine translation of arabic dialects.</title>
<date>2012</date>
<booktitle>In The 2012 Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montreal,</location>
<contexts>
<context position="1521" citStr="Zbib et al., 2012" startWordPosition="210" endWordPosition="213">esource for various Natural Language Processing (NLP) tasks (Callison-Burch and Dredze, 2010; Zaidan and Callison-Burch, 2011a; Zaidan and Callison-Burch, 2011b). The resulting quality of annotations is high enough to be used for training statistical NLP models, with a saving in cost and time of up to an order of magnitude. Statistical Machine Translation (SMT) is one of the NLP tasks that can benefit from crowdsourced annotations. With appropriate quality control mechanisms, reference translations collected by crowdsourcing have been successfully used for training and evaluating SMT systems (Zbib et al., 2012; Zaidan and Callison-Burch, 2011b). In this work, we used Amazon’s Mechanical Turk (MTurk) to obtain alternative reference translations of four Arabic-English parallel corpora previously released by the Linguistic Data Consortium (LDC) for the DARPA BOLT program. This data, totaling over 500K Arabic tokens, was originally collected from web discussion forums and translated professionally to English. We used alternative MTurk translations of the same data to train and evaluation MT systems; and conducted the first systematic study that quantifies the effect of the reference translation process</context>
<context position="4032" citStr="Zbib et al. (2012)" startWordPosition="600" endWordPosition="603">Zaidan and Callison-Burch (2011b) obtained multiple translations of the NIST 2009 Urdu-English evaluation set using MTurk. They trained a statistical model on a set of features to select among the multiple translations. They showed that the MTurk translations selected by their model approached the range of quality of professional translations, and that the selected MTurk translations can be used reliably to score the outputs of different MT systems submitted to the NIST evaluation. Unlike our work, they did not investigate the use of crowdsourced translations for training or parameter tuning. Zbib et al. (2012) trained a Dialectal Arabic to English MT system using Mechanical Turk translations. But the data they translated on MTurk does not have professional translations to conduct the systematic comparison we do in this paper. It is well known that scoring MT output against multiple references improves MT scores such as BLEU significantly, since it increases the chance of matching n-grams between the MT output and the references. Tuning system parameter with multiple references also improves machine translation for the same reason Madnani et al. (2007) and Madnani et al. (2008) showed that tuning on</context>
</contexts>
<marker>Zbib, Malchiodi, Devlin, Stallard, Matsoukas, Schwartz, Makhoul, Zaidan, Callison-Burch, 2012</marker>
<rawString>Rabih Zbib, Erika Malchiodi, Jacob Devlin, David Stallard, Spyros Matsoukas, Richard Schwartz, John Makhoul, Omar F. Zaidan, and Chris Callison-Burch. 2012. Machine translation of arabic dialects. In The 2012 Conference of the North American Chapter of the Association for Computational Linguistics, Montreal, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>