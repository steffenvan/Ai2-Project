<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004689">
<title confidence="0.995365">
Evaluating Machine Translations using mNCD
</title>
<author confidence="0.996708">
Marcus Dobrinkat and Tero Tapiovaara and Jaakko V¨ayrynen
</author>
<affiliation confidence="0.9985205">
Adaptive Informatics Research Centre
Aalto University School of Science and Technology
</affiliation>
<address confidence="0.947229">
P.O. Box 15400, FI-00076 Aalto, Finland
</address>
<email confidence="0.995992">
{marcus.dobrinkat,jaakko.j.vayrynen,tero.tapiovaara}@tkk.fi
</email>
<author confidence="0.983395">
Kimmo Kettunen
</author>
<affiliation confidence="0.9995">
Kymenlaakso University of Applied Sciences
</affiliation>
<address confidence="0.930077">
P.O. Box 9, FI-48401 Kotka, Finland
</address>
<email confidence="0.99726">
kimmo.kettunen@kyamk.fi
</email>
<sectionHeader confidence="0.997361" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999012">
This paper introduces mNCD, a method
for automatic evaluation of machine trans-
lations. The measure is based on nor-
malized compression distance (NCD), a
general information theoretic measure of
string similarity, and flexible word match-
ing provided by stemming and synonyms.
The mNCD measure outperforms NCD in
system-level correlation to human judg-
ments in English.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999929951612903">
Automatic evaluation of machine translation (MT)
systems requires automated procedures to en-
sure consistency and efficient handling of large
amounts of data. In statistical MT systems, au-
tomatic evaluation of translations is essential for
parameter optimization and system development.
Human evaluation is too labor intensive, time con-
suming and expensive for daily evaluations. How-
ever, manual evaluation is important in the com-
parison of different MT systems and for the valida-
tion and development of automatic MT evaluation
measures, which try to model human assessments
of translations as closely as possible. Furthermore,
the ideal evaluation method would be language in-
dependent, fast to compute and simple.
Recently, normalized compression distance
(NCD) has been applied to the evaluation of
machine translations. NCD is a general in-
formation theoretic measure of string similar-
ity, whereas most MT evaluation measures, e.g.,
BLEU and METEOR, are specifically constructed
for the task. Parker (2008) introduced BAD-
GER, an MT evaluation measure that uses NCD
and a language independent word normalization
method. BADGER scores were directly compared
against the scores of METEOR and word error
rate (WER). The correlation between BADGER
and METEOR were low and correlations between
BADGER and WER high. Kettunen (2009) uses
the NCD directly as an MT evaluation measure.
He showed with a small corpus of three language
pairs that NCD and METEOR 0.6 correlated for
translations of 10–12 MT systems. NCD was not
compared to human assessments of translations,
but correlations of NCD and METEOR scores
were very high for all the three language pairs.
V¨ayrynen et al. (2010) have extended the work
by including NCD in the ACL WMT08 evaluation
framework and showing that NCD is correlated
to human judgments. The NCD measure did not
match the performance of the state-of-the-art MT
evaluation measures in English, but it presented a
viable alternative to de facto standard BLEU (Pa-
pineni et al., 2001), which is simple and effective
but has been shown to have a number of drawbacks
(Callison-Burch et al., 2006).
Some recent advances in automatic MT evalu-
ation have included non-binary matching between
compared items (Banerjee and Lavie, 2005; Agar-
wal and Lavie, 2008; Chan and Ng, 2009), which
is implicitly present in the string-based NCD mea-
sure. Our motivation is to investigate whether in-
cluding additional language dependent resources
would improve the NCD measure. We experiment
with relaxed word matching using stemming and
a lexical database to allow lexical changes. These
additional modules attempt to make the reference
sentences more similar to the evaluated transla-
tions on the string level. We report an experiment
showing that document-level NCD and aggregated
NCD scores for individual sentences produce very
similar correlations to human judgments.
</bodyText>
<page confidence="0.978254">
80
</page>
<note confidence="0.841129">
Proceedings of the ACL 2010 Conference Short Papers, pages 80–85,
Uppsala, Sweden, 11-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<figureCaption confidence="0.936078">
Figure 1: An example showing the compressed
sizes of two strings separately and concatenated.
</figureCaption>
<sectionHeader confidence="0.981161" genericHeader="method">
2 Normalized Compression Distance
</sectionHeader>
<bodyText confidence="0.999854142857143">
Normalized compression distance (NCD) is a sim-
ilarity measure based on the idea that a string x is
similar to another string y when both share sub-
strings. The description of y can reference shared
substrings in the known x without repetition, in-
dicating shared information. Figure 1 shows an
example in which the compression of the concate-
nation of x and y results in a shorter output than
individual compressions of x and y.
The normalized compression distance, as de-
fined by Cilibrasi and Vitanyi (2005), is given in
Equation 1, with C(x) as length of the compres-
sion of x and C(x, y) as the length of the com-
pression of the concatenation of x and y.
</bodyText>
<equation confidence="0.994426">
NCD (x, y) = C(x,m) {C( x) min CC( (y)}
y)}
(1)
</equation>
<bodyText confidence="0.999179538461538">
NCD computes the distance as a score closer to
one for very different strings and closer to zero for
more similar strings.
NCD is an approximation of the uncomputable
normalized information distance (NID), a general
measure for the similarity of two objects. NID
is based on the notion of Kolmogorov complex-
ity K(x), a theoretical measure for the informa-
tion content of a string x, defined as the shortest
universal Turing machine that prints x and stops
(Solomonoff, 1964). NCD approximates NID by
the use of a compressor C(x) that is an upper
bound of the Kolmogorov complexity K(x).
</bodyText>
<sectionHeader confidence="0.998366" genericHeader="method">
3 mNCD
</sectionHeader>
<bodyText confidence="0.99996625">
Normalized compression distance was not con-
ceived with MT evaluation in mind, but rather it
is a general measure of string similarity. Implicit
non-binary matching with NCD is indicated by
preliminary experiments which show that NCD is
less sensitive to random changes on the character
level than, for instance, BLEU, which only counts
the exact matches between word n-grams. Thus
comparison of sentences at the character level
could account better for morphological changes.
Variation in language leads to several accept-
able translations for each source sentence, which
is why multiple reference translations are pre-
ferred in evaluation. Unfortunately, it is typical
to have only one reference translation. Paraphras-
ing techniques can produce additional translation
variants (Russo-Lassner et al., 2005; Kauchak and
Barzilay, 2006). These can be seen as new refer-
ence translations, similar to pseudo references (Ma
et al., 2007).
The proposed method, mNCD, works analo-
gously to M-BLEU and M-TER, which use the
flexible word matching modules from METEOR
to find relaxed word-to-word alignments (Agar-
wal and Lavie, 2008). The modules are able to
align words even if they do not share the same
surface form, but instead have a common stem or
are synonyms of each other. A similarized transla-
tion reference is generated by replacing words in
the reference with their aligned counterparts from
the translation hypothesis. The NCD score is com-
puted between the translations and the similarized
references to get the mNCD score.
Table 1 shows some hand-picked German–
English candidate translations along with a) the
reference translations including the 1-NCD score
to easily compare with METEOR and b) the simi-
larized references including the mNCD score. For
comparison, the corresponding METEOR scores
without implicit relaxed matching are shown.
</bodyText>
<sectionHeader confidence="0.999589" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999979333333333">
The proposed mNCD and the basic NCD measure
were evaluated by computing correlation to hu-
man judgments of translations. A high correlation
value between an MT evaluation measure and hu-
man judgments indicates that the measure is able
to evaluate translations in a more similar way to
humans.
Relaxed alignments with the METEOR mod-
ules exact, stem and synonym were created
for English for the computation of the mNCD
score. The synonym module was not available
with other target languages.
</bodyText>
<subsectionHeader confidence="0.998193">
4.1 Evaluation Data
</subsectionHeader>
<bodyText confidence="0.999913">
The 2008 ACL Workshop on Statistical Machine
Translation (Callison-Burch et al., 2008) shared
task data includes translations from a total of 30
MT systems between English and five European
languages, as well as automatic and human trans-
</bodyText>
<page confidence="0.988412">
81
</page>
<figure confidence="0.775921636363637">
Candidate C/ Reference R/ Similarized Reference S
C There is no effective means to stop a Tratsch, which was already included in the world.
R There is no good way to halt gossip that has already begun to spread.
S There is no effective means to stop gossip that has already begun to spread.
C Crisis, not only in America
R A Crisis Not Only in the U.S.
S A Crisis not only in the America
C Influence on the whole economy should not have this crisis.
R Nevertheless, the crisis should not have influenced the entire economy.
S Nevertheless, the crisis should not have Influence the entire economy.
C Or the lost tight meeting will be discovered at the hands of a gentlemen?
R Perhaps you see the pen you thought you lost lying on your colleague’s desk.
S Perhaps you meeting the pen you thought you lost lying on your colleague’s desk.
1-NCD METEOR
.41 .31
.56 .55
.51 .44
.72 .56
.60 .37
.62 .44
.42 .09
.40 .13
</figure>
<tableCaption confidence="0.980952">
Table 1: Example German–English translations showing the effect of relaxed matching in the 1-mNCD
</tableCaption>
<bodyText confidence="0.98624024">
score (for rows S) compared with METEOR using the exact module only, since the modules stem
and synonym are already used in the similarized reference. Replaced words are emphasized.
lation evaluations for the translations. There are
several tasks, defined by the language pair and the
domain of translated text.
The human judgments include three different
categories. The RANK category has human quality
rankings of five translations for one sentence from
different MT systems. The CONST category con-
tains rankings for short phrases (constituents), and
the YES/NO category contains binary answers if a
short phrase is an acceptable translation or not.
For the translation tasks into English, the re-
laxed alignment using a stem module and the
synonym module affected 7.5% of all words,
whereas only 5.1 % of the words were changed in
the tasks from English into the other languages.
The data was preprocessed in two different
ways. For NCD we kept the data as is, which we
called real casing (rc). Since the used METEOR
align module lowercases all text, we restored the
case information in mNCD by copying the correct
case from the reference translation to the similar-
ized reference, based on METEOR’s alignment.
The other way was to lowercase all data (lc).
</bodyText>
<subsectionHeader confidence="0.961092">
4.2 System-level correlation
</subsectionHeader>
<bodyText confidence="0.999653625">
We follow the same evaluation methodology as in
Callison-Burch et al. (2008), which allows us to
measure how well MT evaluation measures corre-
late with human judgments on the system level.
Spearman’s rank correlation coefficient p was
calculated between each MT evaluation measure
and human judgment category using the simplified
equation
</bodyText>
<equation confidence="0.973172333333333">
6 �i di
p = 1 − (2)
n(n2 − 1)
</equation>
<bodyText confidence="0.9999981">
where for each system i, di is the difference be-
tween the rank derived from annotators’ input and
the rank obtained from the measure. From the an-
notators’ input, the n systems were ranked based
on the number of times each system’s output was
selected as the best translation divided by the num-
ber of times each system was part of a judgment.
We computed system-level correlations for
tasks with English, French, Spanish and German
as the target language1.
</bodyText>
<sectionHeader confidence="0.999972" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.9994555">
We compare mNCD against NCD and relate their
performance to other MT evaluation measures.
</bodyText>
<subsectionHeader confidence="0.997688">
5.1 Block size effect on NCD scores
</subsectionHeader>
<bodyText confidence="0.9994466875">
V¨ayrynen et al. (2010) computed NCD between a
set of candidate translations and references at the
same time regardless of the sentence alignments,
analogously to document comparison. We experi-
mented with segmentation of the candidate trans-
lations into smaller blocks, which were individ-
ually evaluated with NCD and aggregated into a
single value with arithmetic mean. The resulting
system-level correlations between NCD and hu-
man judgments are shown in Figure 2 as a function
of the block size. The correlations are very simi-
lar with all block sizes, except for Spanish, where
smaller block size produces higher correlation. An
experiment with geometric mean produced similar
results. The reported results with mNCD use max-
imum block size, similar to V¨ayrynen et al. (2010).
</bodyText>
<footnote confidence="0.9745795">
1The English-Spanish news task was left out as most mea-
sures had negative correlation with human judgments.
</footnote>
<page confidence="0.996714">
82
</page>
<table confidence="0.9611429">
Method Parameters Target Lang Corr
EN DE FR ES
mNCD PPMZ rc .69 .37 .82 .38
NCD PPMZ rc .60 .37 .84 .39
mNCD bz2 rc .64 .32 .75 .25
NCD bz2 rc .57 .34 .85 .42
mNCD PPMZ lc .66 .33 .79 .23
NCD PPMZ lc .56 .37 .77 .21
mNCD bz2 lc .59 .25 .78 .16
NCD bz2 lc .54 .26 .77 .15
</table>
<figure confidence="0.94732275">
system level correlation with human judgements
into en
into de
into fr
into es
0.0 0.2 0.4 0.6 0.8 1.0
2 5 10 20 50 100 500 2000 5000
block size in lines
</figure>
<figureCaption confidence="0.997992">
Figure 2: The block size has very little effect on
</figureCaption>
<bodyText confidence="0.86395925">
the correlation between NCD and human judg-
ments. The right side corresponds to document
comparison and the left side to aggregated NCD
scores for sentences.
</bodyText>
<subsectionHeader confidence="0.996991">
5.2 mNCD against NCD
</subsectionHeader>
<bodyText confidence="0.998655166666667">
Table 2 shows the average system level correlation
of different NCD and mNCD variants for trans-
lations into English. The two compressors that
worked best in our experiments were PPMZ and
bz2. PPMZ is slower to compute but performs
slightly better compared to bz2, except for the
</bodyText>
<table confidence="0.971046111111111">
Method Parameters
PPMZ rc .69 .74 .80 .74
PPMZ rc .60 .66 .71 .66
bz2 rc .64 .73 .73 .70
bz2 rc .57 .64 .69 .64
PPMZ lc .66 .80 .79 .75
PPMZ lc .56 .79 .75 .70
bz2 lc .59 .85 .74 .73
bz2 lc .54 .82 .71 .69
</table>
<tableCaption confidence="0.847092428571428">
Table 2: Mean system level correlations over
all translation tasks into English for variants of
mNCD and NCD. Higher values are emphasized.
Parameters are the compressor PPMZ or bz2 and
the preprocessing choice lowercasing (lc) or real
casing (rc).
Table 3: mNCD versus NCD system correlation
</tableCaption>
<bodyText confidence="0.968890857142857">
RANK results with different parameters (the same
as in Table 2) for each target language. Higher
values are emphasized. Target languages DE, FR
and ES use only the stem module.
lowercased CONST category.
Table 2 shows that real casing improves RANK
correlation slightly throughout NCD and mNCD
variants, whereas it reduces correlation in the cat-
egories CONST, YES/NO as well as the mean.
The best mNCD (PPMZ rc) improves the best
NCD (PPMZ rc) method by 15% in the RANK
category. In the CONST category the best mNCD
(bz2 lc) improves the best NCD (bz2 lc) by 3.7%.
For the total average, the best mNCD (PPMZ rc)
improves the the best NCD (bz2 lc) by 7.2%.
Table 3 shows the correlation results for the
RANK category by target language. As shown al-
ready in Table 2, mNCD clearly outperforms NCD
for English. Correlations for other languages show
mixed results and on average, mNCD gives lower
correlations than NCD.
</bodyText>
<subsectionHeader confidence="0.998692">
5.3 mNCD versus other methods
</subsectionHeader>
<bodyText confidence="0.999274">
Table 4 presents the results for the selected mNCD
(PPMZ rc) and NCD (bz2 rc) variants along with
the correlations for other MT evaluation methods
from the WMT’08 data, based on the results in
Callison-Burch et al. (2008). The results are av-
erages over language pairs into English, sorted
by RANK, which we consider the most signifi-
cant category. Although mNCD correlation with
human evaluations improved over NCD, the rank-
ing among other measures was not affected. Lan-
guage and task specific results not shown here, re-
veal very low mNCD and NCD correlations in the
Spanish-English news task, which significantly
</bodyText>
<figure confidence="0.959194777777778">
RANK CONST YES/NO Mean
mNCD
NCD
mNCD
NCD
mNCD
NCD
mNCD
NCD
</figure>
<page confidence="0.979596">
83
</page>
<table confidence="0.991989777777778">
Method
DP .81 .66 .74 .73
ULCh .80 .68 .78 .75
DR .79 .53 .65 .66
meteor-ranking .78 .55 .63 .65
ULC .77 .72 .81 .76
posbleu .75 .69 .78 .74
SR .75 .66 .76 .72
posF4gram-gm .74 .60 .71 .68
meteor-baseline .74 .60 .63 .66
posF4gram-am .74 .58 .69 .67
mNCD (PPMZ rc) .69 .74 .80 .74
NCD (PPMZ rc) .60 .66 .71 .66
mbleu .50 .76 .70 .65
bleu .50 .72 .74 .65
mter .38 .74 .68 .60
svm-rank .37 .10 .23 .23
Mean .67 .62 .69 .66
</table>
<tableCaption confidence="0.99601">
Table 4: Average system-level correlations over
</tableCaption>
<bodyText confidence="0.979307333333333">
translation tasks into English for NCD, mNCD
and other MT evaluations measures
degrades the averages. Considering the mean of
the categories instead, mNCD’s correlation of .74
is third best together with ’posbleu’.
Table 5 shows the results from English. The ta-
ble is shorter since many of the better MT mea-
sures use language specific linguistic resources
that are not easily available for languages other
than English. mNCD performs competitively only
for French, otherwise it falls behind NCD and
other methods as already shown earlier.
</bodyText>
<sectionHeader confidence="0.999814" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999772153846154">
We have introduced a new MT evaluation mea-
sure, mNCD, which is based on normalized com-
pression distance and METEOR’s relaxed align-
ment modules. The mNCD measure outperforms
NCD in English with all tested parameter com-
binations, whereas results with other target lan-
guages are unclear. The improved correlations
with mNCD did not change the position in the
RANK category of the MT evaluation measures in
the 2008 ACL WMT shared task.
The improvement in English was expected on
the grounds of the synonym module, and indicated
also by the larger number of affected words in the
</bodyText>
<table confidence="0.999597">
Method Target Lang Corr
DE FR ES Mean
posbleu .75 .80 .75 .75
posF4gram-am .74 .82 .79 .74
posF4gram-gm .74 .82 .79 .74
bleu .47 .83 .80 .68
NCD (bz2 rc) .34 .85 .42 .66
svm-rank .44 .80 .80 .66
mbleu .39 .77 .83 .63
mNCD (PPMZ rc) .37 .82 .38 .63
meteor-baseline .43 .61 .84 .58
meteor-ranking .26 .70 .83 .55
mter .26 .69 .73 .52
Mean .47 .77 .72 .65
</table>
<tableCaption confidence="0.685432">
Table 5: Average system-level correlations for the
RANK category from English for NCD, mNCD
and other MT evaluation measures.
</tableCaption>
<bodyText confidence="0.999147275862069">
similarized references. We believe there is poten-
tial for improvement in other languages as well if
synonym lexicons are available.
We have also extended the basic NCD measure
to scale between a document comparison mea-
sure and aggregated sentence-level measure. The
rather surprising result is that NCD produces quite
similar scores with all block sizes. The different
result with Spanish may be caused by differences
in the data or problems in the calculations.
After using the same evaluation methodology as
in Callison-Burch et al. (2008), we have doubts
whether it presents the most effective method ex-
ploiting all the given human evaluations in the best
way. The system-level correlation measure only
awards the winner of the ranking of five differ-
ent systems. If a system always scored second,
it would never be awarded and therefore be overly
penalized. In addition, the human knowledge that
gave the lower rankings is not exploited.
In future work with mNCD as an MT evalu-
ation measure, we are planning to evaluate syn-
onym dictionaries for other languages than En-
glish. The synonym module for English does
not distinguish between different senses of words.
Therefore, synonym lexicons found with statis-
tical methods might provide a viable alternative
for manually constructed lexicons (Kauchak and
Barzilay, 2006).
</bodyText>
<note confidence="0.620559">
RANK CONST YES/NO Mean
</note>
<page confidence="0.998386">
84
</page>
<sectionHeader confidence="0.996157" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999828014925373">
Abhaya Agarwal and Alon Lavie. 2008. METEOR,
M-BLEU and M-TER: evaluation metrics for high-
correlation with human rankings of machine trans-
lation output. In StatMT ’08: Proceedings of the
Third Workshop on Statistical Machine Translation,
pages 115–118, Morristown, NJ, USA. Association
for Computational Linguistics.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65–72, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of BLEU
in machine translation research. In Proceedings of
EACL-2006, pages 249–256.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christoph Monz, and Josh Schroeder. 2008.
Further meta-evalutation of machine translation.
ACL Workshop on Statistical Machine Translation.
Yee Seng Chan and Hwee Tou Ng. 2009. MaxSim:
performance and effects of translation fluency. Ma-
chine Translation, 23(2-3):157–168.
Rudi Cilibrasi and Paul Vitanyi. 2005. Clustering
by compression. IEEE Transactions on Information
Theory, 51:1523–1545.
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proceedings
of the main conference on Human Language Tech-
nology Conference of the North American Chap-
ter of the Association of Computational Linguistics,
pages 455–462, Morristown, NJ, USA. Association
for Computational Linguistics.
Kimmo Kettunen. 2009. Packing it all up in search for
a language independent MT quality measure tool. In
In Proceedings of LTC-09, 4th Language and Tech-
nology Conference, pages 280–284, Poznan.
Yanjun Ma, Nicolas Stroppa, and Andy Way. 2007.
Bootstrapping word alignment via word packing. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 304–
311, Prague, Czech Republic, June. Association for
Computational Linguistics.
K. Papineni, S. Roukos, T. Ward, and W. Zhu.
2001. BLEU: a method for automatic evaluation
of machine translation. Technical Report RC22176
(W0109-022), IBM Research Division, Thomas J.
Watson Research Center.
Steven Parker. 2008. BADGER: A new machine trans-
lation metric. In Metrics for Machine Translation
Challenge 2008, Waikiki, Hawai’i, October. AMTA.
Grazia Russo-Lassner, Jimmy Lin, and Philip Resnik.
2005. A paraphrase-based approach to machine
translation evaluation. Technical Report LAMP-
TR-125/CS-TR-4754/UMIACS-TR-2005-57, Uni-
versity of Maryland, College Park.
Ray Solomonoff. 1964. Formal theory of inductive
inference. Part I. Information and Control,, 7(1):1–
22.
Jaakko J. V¨ayrynen, Tero Tapiovaara, Kimmo Ket-
tunen, and Marcus Dobrinkat. 2010. Normalized
compression distance as an automatic MT evalua-
tion metric. In Proceedings of MT 25 years on. To
appear.
</reference>
<page confidence="0.999696">
85
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.753402">
<title confidence="0.999884">Evaluating Machine Translations using mNCD</title>
<author confidence="0.980544">Dobrinkat Tapiovaara V¨ayrynen</author>
<affiliation confidence="0.978523">Adaptive Informatics Research Centre Aalto University School of Science and Technology</affiliation>
<address confidence="0.999183">P.O. Box 15400, FI-00076 Aalto, Finland</address>
<author confidence="0.958465">Kimmo Kettunen</author>
<affiliation confidence="0.989563">Kymenlaakso University of Applied Sciences</affiliation>
<address confidence="0.999567">P.O. Box 9, FI-48401 Kotka, Finland</address>
<email confidence="0.995664">kimmo.kettunen@kyamk.fi</email>
<abstract confidence="0.985105636363636">This paper introduces mNCD, a method for automatic evaluation of machine translations. The measure is based on normalized compression distance (NCD), a general information theoretic measure of string similarity, and flexible word matching provided by stemming and synonyms. The mNCD measure outperforms NCD in system-level correlation to human judgments in English.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Abhaya Agarwal</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR, M-BLEU and M-TER: evaluation metrics for highcorrelation with human rankings of machine translation output.</title>
<date>2008</date>
<booktitle>In StatMT ’08: Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>115--118</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="3066" citStr="Agarwal and Lavie, 2008" startWordPosition="453" endWordPosition="457">pairs. V¨ayrynen et al. (2010) have extended the work by including NCD in the ACL WMT08 evaluation framework and showing that NCD is correlated to human judgments. The NCD measure did not match the performance of the state-of-the-art MT evaluation measures in English, but it presented a viable alternative to de facto standard BLEU (Papineni et al., 2001), which is simple and effective but has been shown to have a number of drawbacks (Callison-Burch et al., 2006). Some recent advances in automatic MT evaluation have included non-binary matching between compared items (Banerjee and Lavie, 2005; Agarwal and Lavie, 2008; Chan and Ng, 2009), which is implicitly present in the string-based NCD measure. Our motivation is to investigate whether including additional language dependent resources would improve the NCD measure. We experiment with relaxed word matching using stemming and a lexical database to allow lexical changes. These additional modules attempt to make the reference sentences more similar to the evaluated translations on the string level. We report an experiment showing that document-level NCD and aggregated NCD scores for individual sentences produce very similar correlations to human judgments. </context>
<context position="6359" citStr="Agarwal and Lavie, 2008" startWordPosition="983" endWordPosition="987">ge leads to several acceptable translations for each source sentence, which is why multiple reference translations are preferred in evaluation. Unfortunately, it is typical to have only one reference translation. Paraphrasing techniques can produce additional translation variants (Russo-Lassner et al., 2005; Kauchak and Barzilay, 2006). These can be seen as new reference translations, similar to pseudo references (Ma et al., 2007). The proposed method, mNCD, works analogously to M-BLEU and M-TER, which use the flexible word matching modules from METEOR to find relaxed word-to-word alignments (Agarwal and Lavie, 2008). The modules are able to align words even if they do not share the same surface form, but instead have a common stem or are synonyms of each other. A similarized translation reference is generated by replacing words in the reference with their aligned counterparts from the translation hypothesis. The NCD score is computed between the translations and the similarized references to get the mNCD score. Table 1 shows some hand-picked German– English candidate translations along with a) the reference translations including the 1-NCD score to easily compare with METEOR and b) the similarized refere</context>
</contexts>
<marker>Agarwal, Lavie, 2008</marker>
<rawString>Abhaya Agarwal and Alon Lavie. 2008. METEOR, M-BLEU and M-TER: evaluation metrics for highcorrelation with human rankings of machine translation output. In StatMT ’08: Proceedings of the Third Workshop on Statistical Machine Translation, pages 115–118, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>65--72</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="3041" citStr="Banerjee and Lavie, 2005" startWordPosition="449" endWordPosition="452">or all the three language pairs. V¨ayrynen et al. (2010) have extended the work by including NCD in the ACL WMT08 evaluation framework and showing that NCD is correlated to human judgments. The NCD measure did not match the performance of the state-of-the-art MT evaluation measures in English, but it presented a viable alternative to de facto standard BLEU (Papineni et al., 2001), which is simple and effective but has been shown to have a number of drawbacks (Callison-Burch et al., 2006). Some recent advances in automatic MT evaluation have included non-binary matching between compared items (Banerjee and Lavie, 2005; Agarwal and Lavie, 2008; Chan and Ng, 2009), which is implicitly present in the string-based NCD measure. Our motivation is to investigate whether including additional language dependent resources would improve the NCD measure. We experiment with relaxed word matching using stemming and a lexical database to allow lexical changes. These additional modules attempt to make the reference sentences more similar to the evaluated translations on the string level. We report an experiment showing that document-level NCD and aggregated NCD scores for individual sentences produce very similar correlat</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65–72, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>Philipp Koehn</author>
</authors>
<title>Re-evaluating the role of BLEU in machine translation research.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL-2006,</booktitle>
<pages>249--256</pages>
<contexts>
<context position="2909" citStr="Callison-Burch et al., 2006" startWordPosition="430" endWordPosition="433"> 10–12 MT systems. NCD was not compared to human assessments of translations, but correlations of NCD and METEOR scores were very high for all the three language pairs. V¨ayrynen et al. (2010) have extended the work by including NCD in the ACL WMT08 evaluation framework and showing that NCD is correlated to human judgments. The NCD measure did not match the performance of the state-of-the-art MT evaluation measures in English, but it presented a viable alternative to de facto standard BLEU (Papineni et al., 2001), which is simple and effective but has been shown to have a number of drawbacks (Callison-Burch et al., 2006). Some recent advances in automatic MT evaluation have included non-binary matching between compared items (Banerjee and Lavie, 2005; Agarwal and Lavie, 2008; Chan and Ng, 2009), which is implicitly present in the string-based NCD measure. Our motivation is to investigate whether including additional language dependent resources would improve the NCD measure. We experiment with relaxed word matching using stemming and a lexical database to allow lexical changes. These additional modules attempt to make the reference sentences more similar to the evaluated translations on the string level. We r</context>
</contexts>
<marker>Callison-Burch, Osborne, Koehn, 2006</marker>
<rawString>Chris Callison-Burch, Miles Osborne, and Philipp Koehn. 2006. Re-evaluating the role of BLEU in machine translation research. In Proceedings of EACL-2006, pages 249–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christoph Monz</author>
<author>Josh Schroeder</author>
</authors>
<date>2008</date>
<booktitle>Further meta-evalutation of machine translation. ACL Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="7691" citStr="Callison-Burch et al., 2008" startWordPosition="1195" endWordPosition="1198">matching are shown. 4 Experiments The proposed mNCD and the basic NCD measure were evaluated by computing correlation to human judgments of translations. A high correlation value between an MT evaluation measure and human judgments indicates that the measure is able to evaluate translations in a more similar way to humans. Relaxed alignments with the METEOR modules exact, stem and synonym were created for English for the computation of the mNCD score. The synonym module was not available with other target languages. 4.1 Evaluation Data The 2008 ACL Workshop on Statistical Machine Translation (Callison-Burch et al., 2008) shared task data includes translations from a total of 30 MT systems between English and five European languages, as well as automatic and human trans81 Candidate C/ Reference R/ Similarized Reference S C There is no effective means to stop a Tratsch, which was already included in the world. R There is no good way to halt gossip that has already begun to spread. S There is no effective means to stop gossip that has already begun to spread. C Crisis, not only in America R A Crisis Not Only in the U.S. S A Crisis not only in the America C Influence on the whole economy should not have this cris</context>
<context position="10218" citStr="Callison-Burch et al. (2008)" startWordPosition="1628" endWordPosition="1631">nd the synonym module affected 7.5% of all words, whereas only 5.1 % of the words were changed in the tasks from English into the other languages. The data was preprocessed in two different ways. For NCD we kept the data as is, which we called real casing (rc). Since the used METEOR align module lowercases all text, we restored the case information in mNCD by copying the correct case from the reference translation to the similarized reference, based on METEOR’s alignment. The other way was to lowercase all data (lc). 4.2 System-level correlation We follow the same evaluation methodology as in Callison-Burch et al. (2008), which allows us to measure how well MT evaluation measures correlate with human judgments on the system level. Spearman’s rank correlation coefficient p was calculated between each MT evaluation measure and human judgment category using the simplified equation 6 �i di p = 1 − (2) n(n2 − 1) where for each system i, di is the difference between the rank derived from annotators’ input and the rank obtained from the measure. From the annotators’ input, the n systems were ranked based on the number of times each system’s output was selected as the best translation divided by the number of times e</context>
<context position="14589" citStr="Callison-Burch et al. (2008)" startWordPosition="2401" endWordPosition="2404">best NCD (bz2 lc) by 3.7%. For the total average, the best mNCD (PPMZ rc) improves the the best NCD (bz2 lc) by 7.2%. Table 3 shows the correlation results for the RANK category by target language. As shown already in Table 2, mNCD clearly outperforms NCD for English. Correlations for other languages show mixed results and on average, mNCD gives lower correlations than NCD. 5.3 mNCD versus other methods Table 4 presents the results for the selected mNCD (PPMZ rc) and NCD (bz2 rc) variants along with the correlations for other MT evaluation methods from the WMT’08 data, based on the results in Callison-Burch et al. (2008). The results are averages over language pairs into English, sorted by RANK, which we consider the most significant category. Although mNCD correlation with human evaluations improved over NCD, the ranking among other measures was not affected. Language and task specific results not shown here, reveal very low mNCD and NCD correlations in the Spanish-English news task, which significantly RANK CONST YES/NO Mean mNCD NCD mNCD NCD mNCD NCD mNCD NCD 83 Method DP .81 .66 .74 .73 ULCh .80 .68 .78 .75 DR .79 .53 .65 .66 meteor-ranking .78 .55 .63 .65 ULC .77 .72 .81 .76 posbleu .75 .69 .78 .74 SR .7</context>
<context position="17660" citStr="Callison-Burch et al. (2008)" startWordPosition="2933" endWordPosition="2936">ons for the RANK category from English for NCD, mNCD and other MT evaluation measures. similarized references. We believe there is potential for improvement in other languages as well if synonym lexicons are available. We have also extended the basic NCD measure to scale between a document comparison measure and aggregated sentence-level measure. The rather surprising result is that NCD produces quite similar scores with all block sizes. The different result with Spanish may be caused by differences in the data or problems in the calculations. After using the same evaluation methodology as in Callison-Burch et al. (2008), we have doubts whether it presents the most effective method exploiting all the given human evaluations in the best way. The system-level correlation measure only awards the winner of the ranking of five different systems. If a system always scored second, it would never be awarded and therefore be overly penalized. In addition, the human knowledge that gave the lower rankings is not exploited. In future work with mNCD as an MT evaluation measure, we are planning to evaluate synonym dictionaries for other languages than English. The synonym module for English does not distinguish between dif</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2008</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christoph Monz, and Josh Schroeder. 2008. Further meta-evalutation of machine translation. ACL Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>MaxSim: performance and effects of translation fluency.</title>
<date>2009</date>
<journal>Machine Translation,</journal>
<pages>23--2</pages>
<contexts>
<context position="3086" citStr="Chan and Ng, 2009" startWordPosition="458" endWordPosition="461">2010) have extended the work by including NCD in the ACL WMT08 evaluation framework and showing that NCD is correlated to human judgments. The NCD measure did not match the performance of the state-of-the-art MT evaluation measures in English, but it presented a viable alternative to de facto standard BLEU (Papineni et al., 2001), which is simple and effective but has been shown to have a number of drawbacks (Callison-Burch et al., 2006). Some recent advances in automatic MT evaluation have included non-binary matching between compared items (Banerjee and Lavie, 2005; Agarwal and Lavie, 2008; Chan and Ng, 2009), which is implicitly present in the string-based NCD measure. Our motivation is to investigate whether including additional language dependent resources would improve the NCD measure. We experiment with relaxed word matching using stemming and a lexical database to allow lexical changes. These additional modules attempt to make the reference sentences more similar to the evaluated translations on the string level. We report an experiment showing that document-level NCD and aggregated NCD scores for individual sentences produce very similar correlations to human judgments. 80 Proceedings of th</context>
</contexts>
<marker>Chan, Ng, 2009</marker>
<rawString>Yee Seng Chan and Hwee Tou Ng. 2009. MaxSim: performance and effects of translation fluency. Machine Translation, 23(2-3):157–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rudi Cilibrasi</author>
<author>Paul Vitanyi</author>
</authors>
<title>Clustering by compression.</title>
<date>2005</date>
<journal>IEEE Transactions on Information Theory,</journal>
<pages>51--1523</pages>
<contexts>
<context position="4451" citStr="Cilibrasi and Vitanyi (2005)" startWordPosition="669" endWordPosition="672">e 1: An example showing the compressed sizes of two strings separately and concatenated. 2 Normalized Compression Distance Normalized compression distance (NCD) is a similarity measure based on the idea that a string x is similar to another string y when both share substrings. The description of y can reference shared substrings in the known x without repetition, indicating shared information. Figure 1 shows an example in which the compression of the concatenation of x and y results in a shorter output than individual compressions of x and y. The normalized compression distance, as defined by Cilibrasi and Vitanyi (2005), is given in Equation 1, with C(x) as length of the compression of x and C(x, y) as the length of the compression of the concatenation of x and y. NCD (x, y) = C(x,m) {C( x) min CC( (y)} y)} (1) NCD computes the distance as a score closer to one for very different strings and closer to zero for more similar strings. NCD is an approximation of the uncomputable normalized information distance (NID), a general measure for the similarity of two objects. NID is based on the notion of Kolmogorov complexity K(x), a theoretical measure for the information content of a string x, defined as the shortes</context>
</contexts>
<marker>Cilibrasi, Vitanyi, 2005</marker>
<rawString>Rudi Cilibrasi and Paul Vitanyi. 2005. Clustering by compression. IEEE Transactions on Information Theory, 51:1523–1545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Kauchak</author>
<author>Regina Barzilay</author>
</authors>
<title>Paraphrasing for automatic evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>455--462</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="6072" citStr="Kauchak and Barzilay, 2006" startWordPosition="937" endWordPosition="940">riments which show that NCD is less sensitive to random changes on the character level than, for instance, BLEU, which only counts the exact matches between word n-grams. Thus comparison of sentences at the character level could account better for morphological changes. Variation in language leads to several acceptable translations for each source sentence, which is why multiple reference translations are preferred in evaluation. Unfortunately, it is typical to have only one reference translation. Paraphrasing techniques can produce additional translation variants (Russo-Lassner et al., 2005; Kauchak and Barzilay, 2006). These can be seen as new reference translations, similar to pseudo references (Ma et al., 2007). The proposed method, mNCD, works analogously to M-BLEU and M-TER, which use the flexible word matching modules from METEOR to find relaxed word-to-word alignments (Agarwal and Lavie, 2008). The modules are able to align words even if they do not share the same surface form, but instead have a common stem or are synonyms of each other. A similarized translation reference is generated by replacing words in the reference with their aligned counterparts from the translation hypothesis. The NCD score </context>
</contexts>
<marker>Kauchak, Barzilay, 2006</marker>
<rawString>David Kauchak and Regina Barzilay. 2006. Paraphrasing for automatic evaluation. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 455–462, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimmo Kettunen</author>
</authors>
<title>Packing it all up in search for a language independent MT quality measure tool. In</title>
<date>2009</date>
<booktitle>In Proceedings of LTC-09, 4th Language and Technology Conference,</booktitle>
<pages>280--284</pages>
<location>Poznan.</location>
<contexts>
<context position="2121" citStr="Kettunen (2009)" startWordPosition="299" endWordPosition="300">le. Recently, normalized compression distance (NCD) has been applied to the evaluation of machine translations. NCD is a general information theoretic measure of string similarity, whereas most MT evaluation measures, e.g., BLEU and METEOR, are specifically constructed for the task. Parker (2008) introduced BADGER, an MT evaluation measure that uses NCD and a language independent word normalization method. BADGER scores were directly compared against the scores of METEOR and word error rate (WER). The correlation between BADGER and METEOR were low and correlations between BADGER and WER high. Kettunen (2009) uses the NCD directly as an MT evaluation measure. He showed with a small corpus of three language pairs that NCD and METEOR 0.6 correlated for translations of 10–12 MT systems. NCD was not compared to human assessments of translations, but correlations of NCD and METEOR scores were very high for all the three language pairs. V¨ayrynen et al. (2010) have extended the work by including NCD in the ACL WMT08 evaluation framework and showing that NCD is correlated to human judgments. The NCD measure did not match the performance of the state-of-the-art MT evaluation measures in English, but it pr</context>
</contexts>
<marker>Kettunen, 2009</marker>
<rawString>Kimmo Kettunen. 2009. Packing it all up in search for a language independent MT quality measure tool. In In Proceedings of LTC-09, 4th Language and Technology Conference, pages 280–284, Poznan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanjun Ma</author>
<author>Nicolas Stroppa</author>
<author>Andy Way</author>
</authors>
<title>Bootstrapping word alignment via word packing.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>304--311</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="6169" citStr="Ma et al., 2007" startWordPosition="954" endWordPosition="957">LEU, which only counts the exact matches between word n-grams. Thus comparison of sentences at the character level could account better for morphological changes. Variation in language leads to several acceptable translations for each source sentence, which is why multiple reference translations are preferred in evaluation. Unfortunately, it is typical to have only one reference translation. Paraphrasing techniques can produce additional translation variants (Russo-Lassner et al., 2005; Kauchak and Barzilay, 2006). These can be seen as new reference translations, similar to pseudo references (Ma et al., 2007). The proposed method, mNCD, works analogously to M-BLEU and M-TER, which use the flexible word matching modules from METEOR to find relaxed word-to-word alignments (Agarwal and Lavie, 2008). The modules are able to align words even if they do not share the same surface form, but instead have a common stem or are synonyms of each other. A similarized translation reference is generated by replacing words in the reference with their aligned counterparts from the translation hypothesis. The NCD score is computed between the translations and the similarized references to get the mNCD score. Table </context>
</contexts>
<marker>Ma, Stroppa, Way, 2007</marker>
<rawString>Yanjun Ma, Nicolas Stroppa, and Andy Way. 2007. Bootstrapping word alignment via word packing. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 304– 311, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<journal>IBM Research Division, Thomas J. Watson Research Center.</journal>
<tech>Technical Report RC22176 (W0109-022),</tech>
<contexts>
<context position="2799" citStr="Papineni et al., 2001" startWordPosition="410" endWordPosition="414">howed with a small corpus of three language pairs that NCD and METEOR 0.6 correlated for translations of 10–12 MT systems. NCD was not compared to human assessments of translations, but correlations of NCD and METEOR scores were very high for all the three language pairs. V¨ayrynen et al. (2010) have extended the work by including NCD in the ACL WMT08 evaluation framework and showing that NCD is correlated to human judgments. The NCD measure did not match the performance of the state-of-the-art MT evaluation measures in English, but it presented a viable alternative to de facto standard BLEU (Papineni et al., 2001), which is simple and effective but has been shown to have a number of drawbacks (Callison-Burch et al., 2006). Some recent advances in automatic MT evaluation have included non-binary matching between compared items (Banerjee and Lavie, 2005; Agarwal and Lavie, 2008; Chan and Ng, 2009), which is implicitly present in the string-based NCD measure. Our motivation is to investigate whether including additional language dependent resources would improve the NCD measure. We experiment with relaxed word matching using stemming and a lexical database to allow lexical changes. These additional module</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001. BLEU: a method for automatic evaluation of machine translation. Technical Report RC22176 (W0109-022), IBM Research Division, Thomas J. Watson Research Center.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Parker</author>
</authors>
<title>BADGER: A new machine translation metric.</title>
<date>2008</date>
<booktitle>In Metrics for Machine Translation Challenge</booktitle>
<publisher>AMTA.</publisher>
<location>Waikiki, Hawai’i,</location>
<contexts>
<context position="1803" citStr="Parker (2008)" startWordPosition="250" endWordPosition="251"> evaluation is important in the comparison of different MT systems and for the validation and development of automatic MT evaluation measures, which try to model human assessments of translations as closely as possible. Furthermore, the ideal evaluation method would be language independent, fast to compute and simple. Recently, normalized compression distance (NCD) has been applied to the evaluation of machine translations. NCD is a general information theoretic measure of string similarity, whereas most MT evaluation measures, e.g., BLEU and METEOR, are specifically constructed for the task. Parker (2008) introduced BADGER, an MT evaluation measure that uses NCD and a language independent word normalization method. BADGER scores were directly compared against the scores of METEOR and word error rate (WER). The correlation between BADGER and METEOR were low and correlations between BADGER and WER high. Kettunen (2009) uses the NCD directly as an MT evaluation measure. He showed with a small corpus of three language pairs that NCD and METEOR 0.6 correlated for translations of 10–12 MT systems. NCD was not compared to human assessments of translations, but correlations of NCD and METEOR scores we</context>
</contexts>
<marker>Parker, 2008</marker>
<rawString>Steven Parker. 2008. BADGER: A new machine translation metric. In Metrics for Machine Translation Challenge 2008, Waikiki, Hawai’i, October. AMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grazia Russo-Lassner</author>
<author>Jimmy Lin</author>
<author>Philip Resnik</author>
</authors>
<title>A paraphrase-based approach to machine translation evaluation.</title>
<date>2005</date>
<tech>Technical Report LAMPTR-125/CS-TR-4754/UMIACS-TR-2005-57,</tech>
<institution>University of Maryland, College Park.</institution>
<contexts>
<context position="6043" citStr="Russo-Lassner et al., 2005" startWordPosition="933" endWordPosition="936">ndicated by preliminary experiments which show that NCD is less sensitive to random changes on the character level than, for instance, BLEU, which only counts the exact matches between word n-grams. Thus comparison of sentences at the character level could account better for morphological changes. Variation in language leads to several acceptable translations for each source sentence, which is why multiple reference translations are preferred in evaluation. Unfortunately, it is typical to have only one reference translation. Paraphrasing techniques can produce additional translation variants (Russo-Lassner et al., 2005; Kauchak and Barzilay, 2006). These can be seen as new reference translations, similar to pseudo references (Ma et al., 2007). The proposed method, mNCD, works analogously to M-BLEU and M-TER, which use the flexible word matching modules from METEOR to find relaxed word-to-word alignments (Agarwal and Lavie, 2008). The modules are able to align words even if they do not share the same surface form, but instead have a common stem or are synonyms of each other. A similarized translation reference is generated by replacing words in the reference with their aligned counterparts from the translati</context>
</contexts>
<marker>Russo-Lassner, Lin, Resnik, 2005</marker>
<rawString>Grazia Russo-Lassner, Jimmy Lin, and Philip Resnik. 2005. A paraphrase-based approach to machine translation evaluation. Technical Report LAMPTR-125/CS-TR-4754/UMIACS-TR-2005-57, University of Maryland, College Park.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray Solomonoff</author>
</authors>
<title>Formal theory of inductive inference.</title>
<date>1964</date>
<journal>Part I. Information and Control,,</journal>
<volume>7</volume>
<issue>1</issue>
<pages>22</pages>
<contexts>
<context position="5120" citStr="Solomonoff, 1964" startWordPosition="794" endWordPosition="795"> compression of x and C(x, y) as the length of the compression of the concatenation of x and y. NCD (x, y) = C(x,m) {C( x) min CC( (y)} y)} (1) NCD computes the distance as a score closer to one for very different strings and closer to zero for more similar strings. NCD is an approximation of the uncomputable normalized information distance (NID), a general measure for the similarity of two objects. NID is based on the notion of Kolmogorov complexity K(x), a theoretical measure for the information content of a string x, defined as the shortest universal Turing machine that prints x and stops (Solomonoff, 1964). NCD approximates NID by the use of a compressor C(x) that is an upper bound of the Kolmogorov complexity K(x). 3 mNCD Normalized compression distance was not conceived with MT evaluation in mind, but rather it is a general measure of string similarity. Implicit non-binary matching with NCD is indicated by preliminary experiments which show that NCD is less sensitive to random changes on the character level than, for instance, BLEU, which only counts the exact matches between word n-grams. Thus comparison of sentences at the character level could account better for morphological changes. Vari</context>
</contexts>
<marker>Solomonoff, 1964</marker>
<rawString>Ray Solomonoff. 1964. Formal theory of inductive inference. Part I. Information and Control,, 7(1):1– 22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaakko J V¨ayrynen</author>
<author>Tero Tapiovaara</author>
<author>Kimmo Kettunen</author>
<author>Marcus Dobrinkat</author>
</authors>
<title>Normalized compression distance as an automatic MT evaluation metric.</title>
<date>2010</date>
<booktitle>In Proceedings of MT 25</booktitle>
<note>years on. To appear.</note>
<marker>V¨ayrynen, Tapiovaara, Kettunen, Dobrinkat, 2010</marker>
<rawString>Jaakko J. V¨ayrynen, Tero Tapiovaara, Kimmo Kettunen, and Marcus Dobrinkat. 2010. Normalized compression distance as an automatic MT evaluation metric. In Proceedings of MT 25 years on. To appear.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>