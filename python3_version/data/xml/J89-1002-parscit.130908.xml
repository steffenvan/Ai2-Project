<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<sectionHeader confidence="0.493334" genericHeader="abstract">
SYNTACTIC GRAPHS: A REPRESENTATION FOR THE UNION OF ALL
AMBIGUOUS PARSE TREES
</sectionHeader>
<author confidence="0.898299">
Jungyun Seo
Robert F. Simmons
</author>
<affiliation confidence="0.9464145">
Artificial Intelligence Laboratory
University of Texas at Austin
</affiliation>
<sectionHeader confidence="0.649893" genericHeader="keywords">
Austin, TX 78712-1188
</sectionHeader>
<bodyText confidence="0.989988909090909">
In this paper, we present a new method of representing the surface syntactic structure of a sentence.
Trees have usually been used in linguistics and natural language processing to represent syntactic
structures of a sentence. A tree structure shows only one possible syntactic parse of a sentence, but in
order to choose a correct parse, we need to examine all possible tree structures one by one. Syntactic
graph representation makes it possible to represent all possible surface syntactic relations in one directed
graph (DG). Since a syntactic graph is expressed in terms of a set of triples, higher level semantic
processes can access any part of the graph directly without navigating the whole structure. Further-
more, since a syntactic graph represents the union of all possible syntactic readings of a sentence, it is
fairly easy to focus on the syntactically ambiguous points. In this paper, we introduce the basic idea of
syntactic graph representation and discuss its various properties. We claim that a syntactic graph
carries complete syntactic information provided by a parse forest—the set of all possible parse trees.
</bodyText>
<sectionHeader confidence="0.998419" genericHeader="introduction">
1 INTRODUCTION
</sectionHeader>
<bodyText confidence="0.999987872340426">
In natural language processing, we use several rules and
various items of knowledge to understand a sentence.
Syntactic processing, which analyzes the syntactic re-
lations among constituents, is widely used to determine
the surface structure of a sentence, because it is effec-
tive to show the functional relations between constitu-
ents and is based on well-developed linguistic theory.
Tree structures, called parse trees, represent syntactic
structures of sentences.
In a natural language understanding system in which
syntactic and semantic processes are separated, the
semantic processor usually takes the surface syntactic
structure of a sentence from the syntactic analyzer as
input and processes it for further understanding.&apos; Since
there are many ambiguities in natural language parsing,
syntactic processing usually generates more than one
parse tree. Therefore, the higher level semantic proces-
sor should examine the parse trees one by one to choose
a correct one.2 Since possible parse trees of sentences
in ordinary expository text often number in the hun-
dreds, it is impractical to check parse trees one by one
without knowing where the ambiguous points are. We
have tried to reduce this problem by introducing a new
structure, the syntactic graph, that can represent all
possible parse trees effectively in a compact form for
further processing. As we will show in the rest of this
paper, since all syntactically ambiguous points are kept
in a syntactic graph, we can easily focus on those points
for further disambiguation.
Furthermore, syntactic graph representation can be
naturally implemented in efficient, parallel, all-path
parsers. One-path parsing algorithms, like the DCG
(Pereira and Warren 1980), which enumerates all possi-
ble parse trees one by one with backtracking, usually
have exponential complexity. All-path parsing algo-
rithms explore all possible paths in parallel without
backtracking (Early 1970; Kay 1980; Chester 1980;
Tomita 1985). In these algorithms, it is efficient to
generate all possible parse trees. This kind of algorithm
has complexity 0(N3) (Aho and Ullman 1972; Tomita
1985).
We use an all-path parsing algorithm to parse a
sentence. Triples, each of which consists of two nodes
and an arc name, are generated while parsing a sen-
tence. The parser collects all correct triples and con-
structs an exclusion matrix, which shows co-occurrence
constraints among arcs, by navigating all possible parse
</bodyText>
<footnote confidence="0.8297015">
Copyright 1989 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted provided
that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page. To
copy otherwise, or to republish, requires a fee and/or specific permission.
0362-613X/ 89 /010019-32$03.00
</footnote>
<note confidence="0.325896666666667">
Computational Linguistics, Volume 15, Number 1, March 1989 19
ji dot
Sentence: I saw a man on the hill with a telescope. Eft,a,art]
</note>
<figureCaption confidence="0.996966">
Figure 1: Syntactic Graph of the Example Sentence.
</figureCaption>
<figure confidence="0.9801925">
Jungyun Seo and Robert F. Simmons Syntactic Graphs: A Representation for the Union of All Ambiguous Parse Trees
I7,with,pj
PPn
(9,telescope,n1
nPP
[4,on,p1
10,i,n1
(5,thcart1
</figure>
<bodyText confidence="0.998682428571428">
trees in a shared, packed-parse forest? We claim that a
syntactic graph represented by the triples and an exclu-
sion matrix contains all important syntactic information
in the parse forest.
In the next section, we motivate this work with an
example. Then we briefly introduce X (X-bar) theory
with head projection, which provides the basis of the
graph representation, and the notation of graph repre-
sentation in Section 3. The properties of a syntactic
graph are detailed in Section 4. In Section 5, we
introduce the idea of an exclusion matrix to limit
possible tree interpretations of a graph representation.
In Section 6, we will present the definition of complete-
ness and soundness of the syntactic graph representa-
tion compared to parse trees by showing an algorithm
that enumerates all syntactic readings using the exclu-
sion matrix from a syntactic graph. We claim that those
readings include all the possible syntactic readings of
the corresponding parse forest. Finally, after discussing
related work, we will §,uggest future research and draw
some conclusions.
</bodyText>
<sectionHeader confidence="0.993751" genericHeader="method">
2 MOTIVATIONAL EXAMPLE
</sectionHeader>
<bodyText confidence="0.999842548387097">
We are currently investigating a model of natural lan-
guage text understanding in which syntactic and seman-
tic processors are separated.4 Ordinarily, in this model,
a syntactic processor constructs a surface syntactic
structure of an input sentence, and then a higher level
semantic processor processes it to understand the sen-
tence—i.e., syntactic and semantic processors are pipe-
lined. If the semantic processor fails to understand the
sentence with a given parse tree, the semantic processor
should ask the syntactic processor for another possible
parse tree. This cycle of processing will continue until
the semantic processor finds the correct parse tree with
which it succeeds in understanding the sentence.
Let us consider the following sentences, from Waltz
(1982):
I saw a man on the hill with a telescope.
I cleaned the lens to get a better view.
When we read the first sentence, we cannot determine
whether the man has a telescope or the telescope is used
to see the man. This is known as the PP-attachment
problem, and many researchers have proposed various
ways to solve it (Frazier and Fodor 1979; Shubert 1984,
1986; Wilks et. al 1985). In this sentence, however, it is
impossible to choose a correct syntactic reading in
syntactic processing—even with commonsense knowl-
edge. The ambiguities must remain until the system
extracts more contextual knowledge from other input
sentences.
The problems of tree structure representation in the
pipelined, natural language processing model are the
following:
</bodyText>
<listItem confidence="0.837882285714286">
• First, since the number of parse trees of a typical
sentence in real text easily grows to several hundreds,
and it is impossible to resolve syntactic ambiguities
by the syntactic processor itself, a semantic processor
must check all possible parse trees one by one until it
is satisfied by some parse tree.5
• Second, since there is no information about where the
ambiguous points are in a parse tree, the semantic
processor should check all possibilities before accept-
ing the parse tree.
• Third, although the semantic processor might be
satisfied with a parse tree, the system should keep the
status of the syntactic processor for a while, because
there is a fair chance that the parse tree may become
unsatisfactory after the system processes several
more sentences. For example, attaching the preposi-
tional phrase (PP) &amp;quot;with a telescope&amp;quot; to &amp;quot;hill&amp;quot; or
&amp;quot;man&amp;quot; would be fine for the semantic processor,
since there is nothing semantically wrong with these
attachments. However, these attachments become
unsatisfactory after the system understands the next
</listItem>
<page confidence="0.770961">
20 Computational Linguistics, Volume 15, Number 1, March 1989
</page>
<note confidence="0.883983">
Jungyun Seo and Robert F. Shmnons Syntactic Graphs: A Representation for the Union of All Ambiguous Parse Trees
</note>
<bodyText confidence="0.999078827586207">
sentence. Then, the semantic processor would have
to backtrack and request from the syntactic processor
another possible parse tree for the earlier sentence.
We propose the syntactic graph as the output structure
of a syntactic processor. The syntactic graph of the first
sentence in the previous example is shown in Figure 1.
In this graph, nodes consist of the positions, the root
forms, and the categories of words in the sentence.
Each node represents a constituent whose head word is
the word in the node. Each arc shows a dominator-
modifier relationship between two nodes. The name of
each arc is uniquely determined according to the gram-
mar rule used to generate the arc. For example, the snp
arc is generated from the grammar rule, SNT --&gt; NP VP,
vpp is from the rule, VP —&gt; VP PP, and ppn from the
rule, PP —&gt; Prep NP, etc.
As we can see in Figure 1, all syntactic readings are
represented in a directed graph in which every ambigu-
ity—lexical ambiguities from words with multiple syn-
tactic categories and structural ambiguities from the
ambiguous grammar—is kept. The nodes which are
pointed to by more than one arc show the ambiguous
points in the sentence, so the semantic processor can
focus on those points to resolve the ambiguities. Fur-
thermore, since a syntactic graph is represented by a set
of triples, a semantic processor can directly access any
part of a graph without traversing the whole. Finally,
syntactic graph representation is compact enough to be
kept in memory for a while.6
</bodyText>
<sectionHeader confidence="0.759824" genericHeader="method">
3 X THEORY AND SYNTACTIC GRAPHS
</sectionHeader>
<bodyText confidence="0.9993942">
1 theory was proposed by Chomsky (1970) to explain
various linguistic structural properties, and has been
widely accepted in linguistic theories. In this notation,
the head of any phrase is termed X, the phrasal category
containing X is termed_, and the phrasal category
containing X is termed X. For example, the head of a
noun_phrase is N (noun), Nis an intermediate category,
and N corresponds to noun phrase (NP). The general
form of the phrase structure rules for X theory is
roughly as follows:
</bodyText>
<listItem confidence="0.998872">
• r*Tf
• X -&gt; XZ * , where * is a Kleene star.
</listItem>
<bodyText confidence="0.997108894736842">
is the phrase that specifies X, and 2 is the phrase that
modifies X. The properties of the head word of a
phrase are projected onto the properties of the phrase.
We can express a grammar with X conventions to cover
a wide range of English.
Since, in X theory, a syntactic phrase consists of the
head of the phrase and the specifiers and modifiers of
the head, if there are more than two constituents in the
right-hand side of a grammar rule, then there are
dominator-modifier (DM) relationships between the
head word and the specifier or modifier words in the
phrase. Tsukada (1987) discovered that the DM rela-
tionship is effective for keeping all the syntactic ambi-
guities in a compact and handy structure without enu-
merating all possible syntactic parse trees. His
representation, however, is too simple to maintain some
important information about syntactic structure that
will be discussed in detail in this paper, and hence fails
to take full advantage of the DM-relationship represen-
tation.
We use a slightly different representation to maintain
more information in head-modifier relations. Each
head-modifier relation is kept in a triple that is equiva-
lent to an arc between two nodes (i.e., words) in a
syntactic graph. The first element of a triple is the arc
name, which represents the relation between the head
and modifier nodes. The second element is the lexical
information of the head node, and the third element is
that of the modifier node. The direction of an arc is
always from a head to a modifier node. For example,
the triple [snp, [1,see,v], [0,1,n]] represents the arc
snp between the two nodes [ 1,see,v] and [0,1,n] in
Figure 1.
Since many words have more than one lexical entry,
we have to keep the lexical information of each word in
a triple so that we can distinguish different usages of a
word in higher level processing. The triples correspond-
ing to some common grammar rules are as follows:
</bodyText>
<listItem confidence="0.996527333333333">
1. 7/—&gt; Det N&apos; [detani,Rd11,1],[[n2,R2]IL2]]
2. N--&gt; Adj N [mod,[[n3,R3]IL3],[[n4,R4]1/4
3. N -&gt; N Prep .(=&gt; [npp,[[ns,R5]1L5],[[n6,R6IL6]]
</listItem>
<bodyText confidence="0.993934407407408">
Each ni represents the position, each Ri represents the
root form, and each Li represents a list of the lexical
information including the syntactic category of each
word in a sentence. Parentheses signify optionality and
the asterisk (*) allows repetition.
Figure 2 shows the set of triples representing the
syntactic graph in Figure 1 and the grammar rules used
to parse the sentence. The sentence in Figure 2 has five
possible parse trees in accordance with the grammar
rules. All of the dependency information in those five
parses is represented in the 12 triples. Those 12 triples
represent all possible syntactic readings of the sentence
with the grammar rules. Not all triples can co-occur in
one syntactic reading in the case of an ambiguous
sentence.
The pointers of each triple are the list of the indices
that are used as the pointers pointing to that triple. For
example, Triple 2 in Figure 2 has a list of three indices
as the pointers. Each of those indices can be used as a
pointer to access the triple. These indices are actually
used as the names of the triple. One triple may have
more than one index. The issues of why and how to
produce indices of triples will be discussed later in this
section.
Triple 3 in Figure 2 represents the vnp arc in Figure
1 between two nodes, [1,see,v] and [3,man,n]. The
node [ 1,see,v] represents a VP with head word
</bodyText>
<page confidence="0.383762">
Computational Linguistics, Volume 15, Number 1, March 1989 21
</page>
<figure confidence="0.59352025">
Syntactic Graphs: A Representation for the Union of All Ambiguous Parse Trees
Jungytm See and Robert F. Simmons
GRAMMAR RULES AND CORRESPONDING
Grammar rules arc-name
</figure>
<listItem confidence="0.935733666666667">
1. SNT NP VP snp
2. NP -&gt; art NP det
3. NP N&apos;
4. PP npp
5. N&apos;-± noun
6. PP-4.prep NP ppn
7. VP-V&apos;
8. V&apos;-eV&apos; NP vnp
9. V&apos;-eV&apos; PP vpp
</listItem>
<figure confidence="0.984204925925926">
10. V -+ verb
SENTENCE: I SAW A MAN ON THE HILL WITH A TELESCOPE
Triples for the Input Sentence Pointers
1. [snp, [[1,see],categ,verb,tns,past];
[[0,i],categ,noun, nbr,sing] ] [22]
2. [det, [[3,man],categ,noun,nbr,sing],
[[2,a],categ,art,ty,indef] ] [02, 09, 20]
3. [vnp, [[1,see],categ,verb,tns,past],
[[3,man],categ,noun,nbr,sing] ) [03, 10, 21]
4. [NTP. [[1,see],categ,verb,tns,past],
[[4,on],categ,prep] ] [13, 24]
5. [[3,man],categ,noun,nbr,sing],
([4,on],categ,prep] ] [08, 19]
6. [det, [[6,hill],categ,noun,nbr,sing],
[[5,the],categ,art,ty,def] ] [06, 17]
7- [PPn, [[4,on],categ,prep], [07, 18]
[[6,hill],categ,noun,nbr,sing] ]
8. [[1,see],categ,verb,tns,past],
[[7,with],categ,prep] ] [26]
9. [11PP, [[6,hill],categ,noun,nbr,sing],
[[7,with],categ,prep] ] [16]
10. DIPP, [[3,man],categ,noun,nbr,sing],
[[7,with],categ,prep] ] [25]
11. [det, [[9,telescope],categ,noun,nbr,sing],
[[8,a],categ,art,ty,indef] ] [14]
12. [[7,with],categ,prep],
([9,telescope],categ,noun,nbr,sing] ] [15]
</figure>
<figureCaption confidence="0.999374">
Figure 2 Grammar Rules and Example triples.
</figureCaption>
<bodyText confidence="0.999976565217391">
[1,see,v], and the node [3,man,n] represents an NP
with head word [3,man,n]. [1,see,v] becomes the head
word, and [3,man,n] becomes the modifier word, of
this triple. The number 1 in [1,see,v] is the position of
the word &amp;quot;see&amp;quot; in the sentence, and v (verb) is the
syntactic category of the word. Since a word may
appear in several positions in a sentence, and one word
may have multiple categories, the position and the
category of a word must be recorded to distinguish the
same word in different positions or with different cate-
gories.
A meaningful relation name is assigned to each pair
of head and modifier constituents in a grammar rule.
Some of these are shown at the top of Figure 2. Rules
for generating triples augment each corresponding
grammar rule. Some grammar rules in Prolog syntax
used to build syntactic graphs are shown in Figure 3.
An informal description of the algorithm for generat-
ing triples of a syntactic graph using the grammar rules
in Figure 3 is the following: The basic algorithm of the
parser is an all-path, bottom-up, chart parser that con-
structs a shared, packed-parse forest. Unlike an ordi-
nary chart parser, the parser uses two charts, one for
</bodyText>
<equation confidence="0.748344612903226">
%% 1. sot-. np + vp
grUsnt, Vhd], % category and head of LHS of rule.
[[11F, Nhd],ivp. Vhd]], % categories and heads of RHS.
( true ), % constraints, in this case, none
[(sop, Vhd, Nhd]]). % list of triples generated
% Vhd is head word, Nhd is modifier.
%%2. np-. article + npl
gr([np, Nhd], % Nhd, the head of npl, becomes new head
[Cart, Det], [npl, Nhd]],
( true ),
[[det, Nhd, Det]]). % Nhd is head and Det is modifier.
%%3. np-.npl
gr([np, Nhd],
[[npl, Nhd]], % since there is only one constituent
( true ), % inhere
[ % no triple will be generated in
% this rule
%%4. vp-.be_aux + vp (be + vp) either passive or progressive
gr(Ivp, Aux],
Hbe_aux, Aux], [vp, Vhd]],
( memprUinfiection, INFL], Vhd),
( INFL = paprt % if inflection of vp is passive
% participle, then
Triples = ((beaux, Aux, Vhd], [voice, Vhd, passive]]
% otherwise,
( INFL = prprt % if inflection is present participle
% then,
Triples = ((beaux, Aux, Vhd], progressive, Vhd, Veen
% otherwise,
fail) ) ), % this rule cannot be applied.
Triples).
</equation>
<figureCaption confidence="0.915693">
Figure 3 Augmented grammar rules for triple generation.
</figureCaption>
<bodyText confidence="0.984819567567568">
constituents and the other for triples. Whenever the
parser builds a constituent and its triple, the parser
generates an index for the triple,9 and records the triple
on the chart of triples using the index. Then it records
the constituent with the index of the triple on the chart
of constituents.
We use Rule 4 in Figure 3 to illustrate the parser.
Rule 4 states that if there are two adjacent constituents,
a be-aux followed by a vp, execute the procedure in the
third argument position of the rule. The procedure
contains the constraints that must be satisfied to make
the rule to be fired. If the procedure succeeds, the
parser records a new constituent [vp,Vhd]—the first
argument of the rule—on the chart. Before the parser
records the constituent, it must check the triples for the
constituent. The procedure in the third argument posi-
tion also contains the processes to produce the triples
for the constituent.
The fourth argument of a grammar rule is a list of
triples produced by executing the augmenting proce-
dure at the third argument position of the rule. If the
constraints in the procedure are satisfied, the triples are
also produced. The parser generates a unique index for
each triple, records the triples on the chart of triples,
and adds to the new constituent, the indices of the new
triples. Then, the new constituent is recorded on the
chart of constituents. In this example, the head of the
new constituent is the same as that of be-aux; i.e., the
be-aux dominates the vp.
After finishing the construction of the shared,
packed-parse forest of an input sentence, the parser
navigates the parse forest to collect the triples that
head
head of VP
head of NP
head of N&apos;
head of N&apos;
</bodyText>
<figure confidence="0.699336571428572">
noun
prep
head of V &apos;
head of V&apos;
head of V&apos;
verb
TRIPLES:
modifier
head of NP
art
head of PP
head of NP
head of NP
head of PP
</figure>
<page confidence="0.892282">
22 Computational Linguistics, Volume 15, Number 1, March 1989
</page>
<note confidence="0.580525">
Jungyun Seo and Robert F. Simmons Syntactic Graphs: A Representation for the Union of All Ambiguous Parse Trees
</note>
<table confidence="0.991550444444445">
pointer [category, head, list of childnodes and index
1047 [snt, of triple]
[1,see], [[1002, 1046], 22] ]
1002 [np, [0,1], [[1001], notriple] ]
1001 [npl, [0,1], [[1000], notriple] ]
1000 [n, [0,1], [1 ]
1046 [vp, [1,see], [[1045], notriple] ]
1045 [vpl [1,see], [[1004, 1044], 21],
[[1013, 1041], 24],
[[1027, 1037], 26] ]
1027 [vpl, 1,see], [[1004, 1026], 10],
[[1013, 1023], 13] I
1013 [vpl, [1,see], [[1004, 1012], 03] ]
1004 [vpl, [1,see], [[1003], notriple] ]
1003 [verb, [1,see], [] 1
1012 [np. [3,man], [[1008, 1011], 02] ]
1008 [art, [2,a], Il 1
1011 [np, [3,man], [[1010], notriple] ]
1010 [npl, [3,man], [[1009], notriple] ]
1009 [noun, [3,man], [] 1
1023 [Pp. [4,on], [[1017, 1022], 07] ]
1017 [prep, [4,on], [] I
1022 [np, [6,hill], [[1018, 1021], 06] 1
1018 [art, [5,the], [] 1
1021 [np, [6,hill], [[1020], notriple] ]
1020 [npl, [6,hill], [[1019], notriple] ]
1019 [noun, [6,hill], [] ]
1026 [np, [3,man], [[1008, 1025], 09] ]
1025 [np, [3,man], [[1024], notriple] ]
1024 [npl, [3,man], [[1010, 1023], 08] ]
1037 [pp, [7,with], [[1031, 1036], 15] ]
1031 [prep, [7,with], [] ]
1036 [np, [9,telescope], [[1032, 1035], 14] ]
1032 [art, [8,a], [1 ]
1035 [np, [9,telescope], [[1034], notriple] ]
1034 [npl, [9,telescope], [[1033], notriple] ]
1033 [noun, [9,telescope], [] ]
1041 [pp, [4,on], [[1017, 1040], 18] 1
1040 [np, [6,hill], [[1018, 1039], 17] ]
1039 [np, [6,hill], [[1038], notriple] ]
1038 [npl, [6,hill], [[1020, 1037], 161 ]
1044 [np, [3,man], [[1008, 1043], 20] ]
1043 [np, [3,man], [[1042], notriple] ]
1042 [npl, [3,man], [[1010, 1041], 19],
[[1024, 1037], 25] ]
</table>
<figureCaption confidence="0.982442">
Figure 4 Shared, Packed-Parse Forest.
</figureCaption>
<bodyText confidence="0.983968692307692">
participate in each correct syntactic analysis of the
sentence. The collecting algorithm is explained in Sec-
tion 5.2 in detail.
The representation of the shared, packed-parse for-
est for the example in Figure 2 is in figures 4 and 5.10 It
is important to notice that the shared, packed-parse
forest generated in this parser is different from that of
other parsers. In the shared, packed-parse forest de-
fined by Tomita (1985), any constituents that have the
same category and span the same terminal nodes are
regarded as the same constituent and packed into one
node. In the parser for syntactic graphs, the packing
condition is slightly different in that each constituent is
identified by the head word of the constituent as well as
the category and the terminals it spans. Therefore,
although two nodes might have the same category and
span the same terminals, if the nodes have different
head words, then they cannot be packed together.
A packed node contains several nodes, each of which
contains the category of the node, its head word, and
the list of the pointers to its child nodes and the indices
of the triples of the node. Node 1045 in Figure 4 is a
packed node in which three different constituents are
packed. Those three constituents have the same cate-
gory, vpl, span the same terminals, (from [ ].,see,v] to
[9,telescope,n] ), with the same head word, ([1,see,v]),
but with different internal substructures.
Note that several constituents may have different
indices that point to the same triple. For example, in
Figure 4, the first vpl in the packed node 1045 has the
index 21, the first vpl in the packed node 1027 has the
index 10, and the vpl node in the packed node 1013 has
the index 13 as the indices of their triples. Actually,
these three indices represent the same triple, Triple 3 in
Figure 2. Those three constituents have the same cate-
gory, vpl, the same head, [ 1,see,v], and the same
modifier, [3,man,n], but have different inside struc-
tures of the modifying constituent, np, whose head is
[3,man,n]. The modifying constituent, np, may span
from [2,a] to [3,man], from [2,a] to [6,11111], or from
[2,a] to [9,telescope].
There are different types of triples that do not have
head-modifier relations. These types of triples are for
syntactic characteristics of a sentence such as mood and
voice of verbs. For example, grammar rule 4 in Figure
3 generates not only triples of head-modifier relations,
but also triples that have the information about the
voice or progressiveness of the head word of the VP,
depending on the type of inflection of the word. This
kind of information can be determined in syntactic
processing and is used effectively in higher level seman-
tic processing.
</bodyText>
<sectionHeader confidence="0.998827" genericHeader="method">
4 PROPERTIES OF SYNTACTIC GRAPHS
</sectionHeader>
<bodyText confidence="0.9945845">
We first define several terms used frequently in the rest
of the paper.
</bodyText>
<construct confidence="0.558758333333333">
Definition 1: An in-arc of a node in a syntactic graph
is an arc which points to the node, and an out-arc of
a node points away from the node.
</construct>
<bodyText confidence="0.995484">
Since, in the syntactic graph representation, arcs point
from dominator to modifier nodes, a node with an in-arc
is the modifier node of the arc, and a node with an
out-arc is the dominator node of the arc.
</bodyText>
<construct confidence="0.826505666666667">
Definition 2: A reading of the syntactic graph of a
sentence is one syntactic interpretation of the sen-
tence in the syntactic graph.
</construct>
<bodyText confidence="0.977459">
Since a syntactic graph is a union of syntactic analyses
of a sentence, one reading of a syntactic graph is
analogous to one parse tree of a parse forest.
</bodyText>
<subsubsectionHeader confidence="0.210789">
Definition 3: A root node of one reading of a syntactic
</subsubsectionHeader>
<bodyText confidence="0.4059124">
graph is a node which has no in-arc in the reading.
In most cases, the root node of a reading of the syntactic
graph of a sentence is the head verb of the sentence in
that syntactic interpretation. In a syntactically ambigu-
Computational Linguistics, Volume 15, Number 1, March 1989 23
</bodyText>
<subsectionHeader confidence="0.283129">
Jungyun Seo and Robert F. Simmons Syntactic Graphs: A Representation for the Union of All Ambiguous Parse Trees
</subsectionHeader>
<bodyText confidence="0.463983">
v pl vp 1 :1027
</bodyText>
<note confidence="0.416899">
art art art
saw a man on the hill with a telescope
</note>
<figureCaption confidence="0.965195">
Figure 5 Shared, Packed-Parse Forest-A Diagram.
</figureCaption>
<bodyText confidence="0.999112125">
ous sentence, different syntactic analyses of the sen-
tence may have different head verbs; thus there may be
more than one root node in a syntactic graph. For
example, in the syntactic graph of one famous and
highly ambiguous sentence—&amp;quot;Time flies like an
arrow&amp;quot;—shown in Figure 6, there are three different
root nodes. These roots are [0,time,v], [1,fly,v], and
[2,like,v]1 .
</bodyText>
<subsubsectionHeader confidence="0.536687">
Definition 4: The position of a node is the position of
</subsubsectionHeader>
<bodyText confidence="0.9687925">
the word which is represented by the node, in a
sentence.
Since a word may have several syntactic categories,
there may be more than one node with the same position
in a syntactic graph. For example, since the word
&amp;quot;time&amp;quot; in Figure 6, which appeared as the first word in
the sentence, has two syntactic categories, noun and
verb, there are two nodes, [0,time,n] and [0,time,v], in
the syntactic graph, and the position of the two nodes is
0.
One of the most noticeable features of a syntactic
graph is that ambiguities are explicit, and can be easily
detected by semantic routines that may use further
knowledge to resolve them. The following property
explains how syntactically ambiguous points can be
easily determined in a syntactic graph.
</bodyText>
<construct confidence="0.7400996">
Property 1: In a syntactic tree, each constituent
except the root must by definition be dominated by a
single constituent. Since a syntactic graph is the
union of all syntactic trees that the grammar derives
from a sentence, some graph nodes may be domi-
</construct>
<bodyText confidence="0.856957125">
nated by more than one node; such nodes with
multiple dominators have multiple in-arcs in the
syntactic graph and show points at which the node
participates in more than one syntactic tree interpre-
tation of a sentence. In a graph resulting from a
syntactically unambiguous sentence, no node has
more than a single in-arc, and the graph is a tree with
the head verb as its root.
</bodyText>
<note confidence="0.658337">
According to Property 1, no pair of arcs which point to
</note>
<bodyText confidence="0.627714">
the same node can co-occur in any one syntactic
</bodyText>
<page confidence="0.82052">
24 Computational Linguistics, Volume 15, Number 1, March 1989
</page>
<table confidence="0.977053923076923">
Jungyun Seo and Robert F. Simmons Syntactic Graphs: A Representation for the Union of All Ambiguous Parse Trees
vPP
[0,time,N] .np (1,fly,Vj [2,like,P]
npp
vPP mod • •
vnP
[4,arrow,N]
\det
10,time,V1 11,fly,1■11 (2,like,V]
vnp
(3,an,det)
sup
Sentence: Time flies like an arrow.
</table>
<figureCaption confidence="0.955791">
Figure 6 Graph Representation and Parse Trees of a Highly Ambiguous Sentence.
</figureCaption>
<bodyText confidence="0.999939105263158">
reading, because each node can be a modifier node only
once in one reading. Therefore, we can focus on the
arcs pointing to the same node as ambiguous points. In
terms of triples, any two triples with identical modifier
terms reveal a point of ambiguity, where a modifier term
is dominated by more than one node.
In the example in Figure 1, the syntactic ambiguities
are found in two arcs pointing to [4,on,p] and in three
arcs pointing to [7,with,p ]. The PP with head [4,on]
modifies the VP whose head is [1,see] and it also
modifies the NP with head [3,man]. Similarly three
different in-arcs to the node [7,with] show that there
are three possible choices to which Node 7 can be
attached. The semantic processor can focus on these
three possibilities (or on the earlier two possibility set),
using semantic information, to choose one dominator.
Lacking semantic information, the ambiguities will re-
main in the graph until they can be resolved by addi-
tional knowledge from the context.
</bodyText>
<figureCaption confidence="0.514487375">
Property 2: Since all words in a sentence must be
used in every syntactic interpretation of the sentence
and no word can have multiple categories in one
interpretation, one and only one node from each
position must participate in every reading of a syn-
tactic graph. In other words, each syntactic reading
derived from a syntactic graph must contain one and
only one node from every position.
</figureCaption>
<bodyText confidence="0.514681">
Since every node, except the root node, must be
attached to another node as a modifier node, we can
conclude the following property from properties 1 and
2.
</bodyText>
<listItem confidence="0.976134571428571">
Property 3: In any one reading of a syntactic graph,
the following facts must hold:
1. No two triples with the same modifier node can
co-occur.
2. One and only one node from each position,
except the root node of the reading, must appear
as a modifier node.
</listItem>
<bodyText confidence="0.969565787234042">
Another advantage of the syntactic graph representa-
tion is that we can easily extract the intersection of all
possible syntactic readings from it. Since one node from
each position must participate in every syntactic read-
ing of a syntactic graph, every node which is not a root
node and has only one in-arc, must always be included
in every syntactic reading. Such unambiguous nodes are
common to the intersections of all possible readings.
When we know the exact locations of several pieces in
a jigsaw puzzle, it is much easier to place the other
pieces. Similarly, if a semantic processor knows which
arcs must hold in every reading, it can use these arcs to
constrain inferences to understand and disambiguate.
Property 4: There is no information in a syntactic
graph about the range of terminals spanned by each
triple, so one triple may represent several constitu-
ents which have the same head and modifying terms,
with the same relation name, but which span differ-
ent ranges of terminals.
The compactness and handiness of a graph representa-
tion is based on this property. One arc between two
nodes in a syntactic graph can replace several compli-
cated structures in the tree representation, and multiple
dominating arcs can replace a parse forest.
For example, the arc vnp from [1,see,v] to
[3,man,n] in Figure 1 represents three different con-
stituents. Those constituents have the same category,
vp 1 , the same head, [1,see,v], and the same modifier,
[3,man,n], but have different inside structures of the
modifying constituent, np, whose head is [3,man,n].
The modifying constituent, np, may span from [2,a] to
[3,man], from [2,a] to [6,biLl], or from [2,a] to
[9,telescope]. Actually, in the exclusion matrix de-
scribed below, each triple with differing constituent
structure is represented by multiple subscripts to avoid
the generation of trees that did not occur in the parse
forest.
Another characteristic of a syntactic graph is that the
number of nodes in a graph is not always the same as
that of the words in a sentence. Since some words may
have several syntactic categories, and each category
may lead to a syntactically correct parse, one word may
require several nodes. For example, there are eight
Computational Linguistics, Volume 15, Number 1, March 1989 25
Jungyun Seo and Robert F. Simmons Syntactic Graphs: A Representation for the Union of All Ambiguous Parse Trees
nodes in the syntactic graph in Figure 6, while there are
only five words in the sentence.
</bodyText>
<sectionHeader confidence="0.981781" genericHeader="method">
5 EXCLUSION MATRIX
</sectionHeader>
<bodyText confidence="0.999975571428571">
A syntactic graph is clearly more compact than a parse
forest and provides a good way of representing all
possible syntactic readings with an efficient focusing
mechanism for ambiguous points. However, since one
triple may represent several constituents, and there is
no information about the relationships between triples,
it is possible to lose some important syntactic informa-
tion.
This section consists of two parts. In Section 5.1, we
investigate a co-occurrence problem of arcs in a syntac-
tic graph and suggest the exclusion matrix, to avoid the
problem. The algorithms to collect triples of a syntactic
graph and to construct an exclusion matrix are pre-
sented in Section 5.2.
</bodyText>
<subsectionHeader confidence="0.963086">
5.1 CO-OCCURRENCE PROBLEM BETWEEN ARCS
</subsectionHeader>
<bodyText confidence="0.944250108108108">
One of the most important syntactic displays in a tree
structured parse, but not in a syntactic graph, is the
co-occurrence relationship between constituents. Since
one parse tree represents one possible syntactic reading
of a sentence, we can see whether any two constituents
can co-occur in some reading by checking all parse trees
one by one. However, since the syntactic graph keeps
all possible constituents as a set of triples, it is some-
times difficult to determine whether two triples can
co-occur.
If a syntactic graph does not carry the information
about exclusive arcs, its representation of all possible
syntactic structures may include interpretations not
allowed by the grammar and cause extra overhead. For
example, after a syntactic processor generates the tri-
ples, a semantic processor will focus on the ambiguous
points such as triples 4 and 5, and triples 8, 9, and 10 in
Figure 2 to resolve the ambiguities. In this case, if the
semantic processor has a strong clue to choose Triple 4
over Triple 5, it should not consider Triple 10 as a
competing triple with triples 8 and 9 since 10 is exclu-
sive with 4.
Some of the co-occurrence problems can be detected
easily. For example, due to Property 1, since there can
be only one in-arc to any node in any one reading of a
syntactic graph, the arcs that point to the same node
cannot co-occur in any reading. Triples including these
arcs are called exclusive triples. The following properties
of the syntactic graph representation show several cases
when arcs cannot co-occur. These cases, however, are
not exhaustive.
Property 5: No two crossing arcs can co-occur. More
formally, if an arc has n1 -th and n2 -th words as a
head and a modifier, and another arc has m1 -th and
m2 -th words as a head and a modifier node, then, if
n1&lt;m1&lt;n2&lt;m2 or m1&lt;n1&lt;m2&lt;n2, the two arcs can-
not co-occur.
</bodyText>
<figureCaption confidence="0.99911">
Figure 7 Illegal Parse Tree from Exclusive Arcs.
</figureCaption>
<bodyText confidence="0.999925857142857">
In the syntactic graph in Figure 1, the arcs vpp from
[1,see,v] to [4,on,p ] and npp from [3,man,n] to
[7,with,p ] cannot co-occur in any legal parse trees
because they violate the rule that branches in a parse
tree cannot cross each other.
The following property shows another case of exclu-
sive arcs which cross each other.
</bodyText>
<construct confidence="0.770153">
Property 6: In a syntactic graph, any modifier word
which is on the right side of its head word cannot be
modified by any word which is on the left side of the
head word in a sentence. More formally, let an arc
have a head word W1 and a modifier word W2 whose
positions are n1 and n2 respectively, and n1&lt;n2. Then
if another arc has W2 as a head word and a modifier
word with position n3 where n3-n1, then those two
arcs cannot co-occur.
</construct>
<bodyText confidence="0.995678166666667">
Assume that there are two arcs—one is [npp,
[5,W1,noun], [9,W2,conj]], and the other is [conjpp,
[9,W2,conj], [3,W3,prep]]. The first arc said that the
phrase with head word W2 is attached to the noun in
position 5. The other triple said that the phrase with
head word W3 is attached to the conjunction. This
attachment causes crossing branches. The correspond-
ing parse tree for these two triples is in Figure 7. As we
can see, since there is a crossing branch, these two arcs
cannot co-occur in any parse tree.
The following property shows the symmetric case of
Property 6.
</bodyText>
<construct confidence="0.8363825">
Property 7: In a syntactic graph, any modifier word
which is on the left side of its head word cannot be
modified by any word which is on the right side of the
head word in a sentence.
Other exclusive arcs are due to lexical ambiguity.
Definition 5: If two nodes, W/ and W./ , in a syntactic
graph have the same word and the same position but
with different categories, W/ is in conflict with Wj ,
</construct>
<footnote confidence="0.7474002">
and we say the two nodes are conflicting nodes.
Property 8: Since words cannot have more than one
syntactic category in one reading, any two arcs
which have conflicting nodes as either a head or a
modifier cannot co-occur.
</footnote>
<figure confidence="0.973782">
..........................
.........
Prep
[3,W3, prep]
[5. WI, n]
</figure>
<page confidence="0.878474">
26 Computational Linguistics, Volume 15, Number 1, March 1989
</page>
<note confidence="0.728258">
Jungyun Seo and Robert F. Simmons Syntactic Graphs: A Representation for the Union of All Ambiguous Parse Trees
</note>
<bodyText confidence="0.999926322580645">
The example of exclusive arcs involves the vpp arc from
[1,fly,v] to [2,like,p] and the vnp arc from [0,time,v] to
[1,fly,n] in the graph in Figure 6. Since the two arcs
have the same word with the same position, but with
different categories, they cannot co-occur in any syn-
tactic reading. By examination of Figure 6, we can
determine that there are 25 pairwise combinations of
exclusive arcs in the syntactic graph of that five word
sentence.
The above properties show cases of exclusive arcs
but are not exhaustive. Since the number of pairs of
exclusive arcs is often very large in real text (syntacti-
cally ambiguous sentences). if we ignore the co-occur-
rence information among triples, the overhead cost to
the semantic processor may outweigh the advantage
gained from syntactic graph representation. Therefore
we have to constrain the syntactic graph representation
to include co-occurrence information.
We introduce the exclusion matrix for triples (arcs)
to record constraints so that any two triples which
cannot co-occur in any syntactic tree, cannot co-occur
in any reading of a syntactic graph. The exclusion
matrix provides an efficient tool to decide which triples
should be discarded when higher level processes choose
one among ambiguous triples. For an exclusion matrix
(Ematrix), we make an N x N matrix, where N is the
number of indices of triples. If Ematrix(ij) = 1 then the
triples with the indices i and j cannot co-occur in any
syntactic reading. If Ematrix(i,j) = 0 then the triples
with the indices i and j can co-occur in some syntactic
reading.
</bodyText>
<subsectionHeader confidence="0.7409235">
5.2 AN ALGORITHM TO CONSTRUCT THE EXCLUSION
MATRIX
</subsectionHeader>
<bodyText confidence="0.999934041666667">
Since the several cases of exclusive arcs shown in the
previous section are not exhaustive, they are not suffi-
cient to construct a complete exclusion matrix from a
syntactic graph. A complete exclusion matrix can be
guaranteed by navigating the parse forest when the
syntactic processor collects the triples in the forest to
construct a syntactic graph.
As we have briefly described in Section 3, when the
parser constructs a shared, packed forest, triples are
also produced, and their indices are kept in the corre-
sponding nonterminal nodes in the forest.12 The parser
navigates the parse forest to collect the triples—in fact,
pointers pointing to the triples—and to build an exclu-
sion matrix.
As we can see in the parse forest in Figure 5, there
may be several nonterminal nodes in one packed node.
For each packed node, the parser collects all indices of
triples in the subforests whose root nodes are the
nonterminal nodes in the packed node, and then records
those indices to the packed node. After the parser
finishes collecting the indices of the triples in the parse
forest, each packed node in the forest has a pointer to
the list of collected indices from its subforest. There-
fore, the root node of a parse forest has a pointer to the
</bodyText>
<figure confidence="0.254088333333333">
subforest subforest subforest subforest
Elpacked node
: list of all triples below this node
: triples of this node
\ 4,
ICI : pointer to the list of pointers pointing to triples
</figure>
<figureCaption confidence="0.8848485">
Figure 8 Parse Forest Augmented with Triples.
list of all indices of all possible triples in the whole
forest, and those triples represent the syntactic graph of
the forest.
Figure 8 shows the upper part of the parse forest in
Figure 5 after collecting triples. A hooked arrow of each
</figureCaption>
<bodyText confidence="0.89406175">
nonterminal node points to the list of the indices of the
triples that were added to the node in parsing. For
example, pointer 2 contains the indices of the triples
added to the node snt by the grammar rule:
</bodyText>
<equation confidence="0.659553">
snt —&gt; np + vp
</equation>
<bodyText confidence="0.9742768125">
A simple arrow for each packed node points to the list
of all indices of the triples in the forest of which it is the
root. This list is generated and recorded after the
processor collects all indices of triples in its subnodes.
Therefore the arrow of the root node of the whole
forest, Pointer 1, contains the list of all indices of the
triples in the whole forest.
Since several indices may represent the same triple,
after collecting all the indices of the triples in the parse
forest, the parser removes duplicating triples in the final
representation of the syntactic graph of a sentence.
Collecting pointers to triples in the subforest of a
packed node and constructing the Ematrix is done
recursively as follows: First, Ematrix(i,j) is initialized to
1, which means all arcs are marked exclusive of each
other. Later, if any two arcs indexed with i and j
</bodyText>
<table confidence="0.852870571428571">
Computational Linguistics, Volume 15, Number 1, March 1989 27
Jungyun Seo and Robert F. Simmons Syntactic Graphs: A Representation for the Union of All Ambiguous Parse Trees
function collect_triple(Packed_node)
if Packed node. Collected
% if the indices of triples are already collected
then return(Packed_node. TripleIndex) %collected then, return
%the collected indices
else Packed node. TripleIndex := collectl(Packed_node)
%else collected them
Packed node. Collected := true % set flag Collected.
return(Packed_node. TripleIndex) %returncollected indices.
function collectl(Packed_node)
Triple_Indices :={
for each Node in Packed_node do
</table>
<equation confidence="0.8628205">
TRiple_Indices := merge(Node.TripleIndex, Triple_Indices)
case Node.Child_node_num
0 : (do nothing)
1 : Temp :=collect_triple(Node.Child_node)
Triple_Indices := merge(Temp, Triple Indices)
co—occuri(Node.TripleIndex, Temp)
2 : Tempi :=collect_triple(:Vode.Left_chilcl)
Temp2:=collect_triple(Node.Right_child)
Triple_Indices := merge(Templ, Temp2. Triple_Indices)
co—occur2(Node.TripleIndex, Templ. Temp2)
end—do
return(Triples)
</equation>
<figureCaption confidence="0.975606">
Figure 9 An Algorithm to Collect Triples.
</figureCaption>
<bodyText confidence="0.999474666666667">
co-occur in some parse tree, then the value of Ematrix,
E(i,j), is set to 0. For each nonterminal node in a packed
node, the parser collects every index appearing below
the nonterminal node—i.e., the index of the triples of its
subnodes. If a subnode of the nonterminal node was
previously visited, and its indices were already col-
lected, then the subnode already has the pointer to the
list of collected indices. Therefore the parser does not
need to navigate the same subforests again, but it takes
the indices using the pointer. The algorithm in pseudo-
PASCAL code is in Figure 9.
After the parser collects the indices of the triples
from the subnodes of the nonterminal node, it adjusts
the values in the exclusion matrix according to the
following cases:
</bodyText>
<listItem confidence="0.9693675">
1. If the nonterminal node has one child node, its
own triples can co-occur with each other, and
with every collected triple from its subforest.
2. If the nonterminal node has two child nodes, its
</listItem>
<bodyText confidence="0.998945777777778">
own triples can co-occur with each other and
with the triples collected from both left and right
child nodes, and the triples from the left child
node can co-occur with the triples from the right
one.
This algorithm is described in Figure 10.
For example, the process starts to collect the indices
of the triples from SNT node in Figure 8. Then, it
collects the indices in the left subforest whose root is
np. After all indices of triples in the subforest of np are
collected, those indices and the indices of the triples of
the node in 6 are recorded in 5. Similarly all indices in 7
and 4 are recorded in 3 as the indices of the triples in the
right subforest of the sat node. The indices in 5 and 3
and the indices in 2 are recorded in 1 as the indices of
the triples of the whole parse forest. In packed nodes
with more than one nonterminal node, like vpl , all
indices of the triples in the three subforests of vpl and
</bodyText>
<equation confidence="0.993851625">
function cooccurl(Tripl. Trip2)
fully_cooccur (Tripl )
co—occur3(Tripl, Trip2)
function cooccur2(Tripl, Trip2, Trip3)
fully_.cooccur ( Tripl )
cooccur3(Tripl, Trip2)
cooccur3(Tripl, Trip3)
cooccur3(Trip2, Trip3)
function cooccur3(Tripl, Trip2)
for each index i in Tripl do
for each index j in Trip2 do
Ematrix( i, j) := 0
Ernatrix(j , i) := 0 /* Ematrix is symmetric */
function fully_cooccur(Triples)
for each pair of indices i and j in Triples do
Ematrix( j) := 0
</equation>
<figureCaption confidence="0.768432">
Figure 10 An Algorithm to Construct the Exclusion Matrix.
</figureCaption>
<bodyText confidence="0.998605533333334">
the indices in 8, 9, and 10 are collected and recorded in
7.
By the first case in the above rule, every triple
represented by the indices in 4 can co-occur with each
other, and every triple represented by the indices in 4
can co-occur with every triple represented by the indi-
ces in 7. One example of the second case is that every
triple represented by the indices in 2 can co-occur with
each other, and every triple represented by the indices
in 2 can co-occur with every triple represented by the
indices in 5 and 3. Every triple represented by the
indices in 5 can co-occur with the triples represented by
the indices in 3. Whenever the process finds a pair of
co-occurring triples it adjusts the value of Ematrix
appropriately.
</bodyText>
<sectionHeader confidence="0.993082" genericHeader="method">
6 COMPLETENESS AND SOUNDNESS OF THE SYNTACTIC
</sectionHeader>
<subsectionHeader confidence="0.841974">
GRAPH
</subsectionHeader>
<bodyText confidence="0.838001333333333">
In this section, we will discuss completeness and sound-
ness of a syntactic graph with an exclusion matrix as an
alternative for tree representation of syntactic informa-
tion of a sentence.
Definition 6: A syntactic graph of a sentence is
complete and sound compared to the parse forest of
the sentence iff there is an algorithm that enumerates
syntactic readings from the syntactic graph of the
sentence and satisfies the following conditions:
</bodyText>
<listItem confidence="0.996147625">
1. For every parse tree in the forest, there is a
syntactic reading from the syntactic graph that is
structurally equivalent to that parse tree. (complete-
ness)
2. For every syntactic reading from the syntactic
graph, there is a parse tree in the forest that is
structurally equivalent to that syntactic reading.
(soundness)
</listItem>
<bodyText confidence="0.8789545">
To show the completeness and soundness of the syn-
tactic graph representation, we present the algorithm
that enumerates all possible syntactic readings from a
syntactic graph using an exclusion matrix. This algo-
</bodyText>
<page confidence="0.947974">
28 Computational Linguistics, Volume 15, Number 1, March 1989
</page>
<note confidence="0.805278">
Jungyim Seo and Robert F. Simmons Syntactic Graphs: A Representation for the Union of AU Ambiguous Parse Trees
</note>
<table confidence="0.934118928571429">
The following data are initial input.
Partition_I = a list of triples which have the I—th word as a modifier.
Sen_length = the position of the last word in a sentence.
RootList = a list of root triples.
gen_subgraph(Roothist, Sen_length, Graphs, All_readings) :—
( RootList = [] % if all root triple in RootList had been tried
—0 All_readings = Graphs % then return Graphs as all readings,
% otherwise, find all readings with a RootTriple
; RootList = [RootTriplej RootListl],
gen_subgraphl(RootTriple, Sen_length. Sub_graphs),
append( Graphs, Sub_graphs, Graphsl),
gen_subgraph (Roothistl, Graphsl, All_readings) ) .
gen_subgraphl(RootTriple, Sen_length, Sub_graphs) :—
Rh = Position of the head node in RootTriple % i.e., position of the root node
Rm = Position of the modifier node in RootTriple,
Wlist = [RootTriple],
setof (Graph, gen_subl(Rh, Rm, Sen_length, Wlist, Graph, 0), Sub_graphs) .
gen_subl(Rh, Rm, Sen_length, Wlist, Graph, N) :—
(N &gt; Sen_length % if it take a triple from all partitions
—&gt; Graph = Wlist % then return Wlist as one reading of a syntactic graph,
% otherwise, pick one triple from partition N.
; ( ( Rh = N % Don&apos;t pick up any triple from root node position.
; Rm = N) %A triple from partition Rm is already picked in Wlist.
—&gt; true
; get_triple(N, Triple), % take a triple( in fact, an index of the triple) from partition N.
not_exclusive (Kist, Triple), % check exclusiveness of Triple with other triples in Wlist.
N1 is N + 1, % go to the next partition.
gen_subl(Rh, Rm, [Triplel Kist], Graph, N1) ) ) .
</table>
<figureCaption confidence="0.990633">
Figure 11 Algorithm that Generates All and Only Readings from an SG.
</figureCaption>
<bodyText confidence="0.987799054945055">
rithm constructs subgraphs of the syntactic graph, one
at a time. Each of these subgraphs is equivalent to one
reading of the syntactic graph. Since no node can
modify itself, each of these subgraphs is a directed
acyclic graph (DAG). Furthermore, since every node in
each of these subgraphs can have no more than one
in-arc, the DAG subgraph is actually a tree.
Before going into detail, we give an intuitive descrip-
tion of the algorithm. The algorithm has two lists of
triples as input: a list of triples of a syntactic graph and
a list of root triples. A root triple is a triple that
represents the highest level constituent in a parse—i.e.,
snt (sentence) in the grammar in Figure 2. The head
node of a root triple is usually the head verb of a
sentence reading.
According to Property 3 in Section 4, one reading of
a syntactic graph must include one and only one node
from every position, except the position of the root
node, as a modifier node. This is a necessary require-
ment for any subgraph of a syntactic graph to be one
reading of the graph. One of the simplest ways to make
a subgraph of a syntactic graph that satisfies this
requirement is:
• Make partitions among triples according to the
position of the modifier node of the triples, e.g.,
triples in Partition 0 have the first word in a sentence
as the modifier nodes. Then take one triple from each
partition. Here, the algorithm must know the position
of the root node so that it can exclude the partition in
which triples have the root node as a modifier. When
it chooses a triple, it also must check the exclusion
matrix. If a triple from a partition is exclusive with
any of the triples already chosen, the triple cannot be
included in that reading. The algorithm must try
another triple in that partition. Since the exclusion
matrix is based on the indices of the triples, when it
chooses a triple, it actually chooses an index in the list
of indices of the triple.
Note that any subgraphs produced in this way satisfy
Property 3, and all triples in each subgraph are inclusive
with each other according to the exclusion matrix. The
top level procedures of the algorithm in Prolog are
shown in Figure l l .13
We do not have a rigorous proof of the correctness of
the algorithm, but we present an informal discussion
about how this algorithm can generate all and only the
correct syntactic readings from a syntactic graph.
Since the syntactic graph of a sentence is explicitly
constructed as a union of all parse trees in the parse
forest of the sentence, the triples of the syntactic graph
imply all the parse trees. This fact is due to the
algorithm that constructs a syntactic graph from a parse
forest. Therefore, if we can extract all possible syntactic
readings from the graph, these readings will include all
possible (and more) parse trees in the forest. Intuitively,
the set of all subgraphs of a syntactic graph includes all
syntactic readings of a syntactic graph.
In fact, this algorithm generates all possible sub-
graphs of a syntactic graph that meets the necessary
conditions imposed by Property 3. The predicate
Computational Linguistics, Volume 15, Number 1, March 1989 29
Jungyun Seo and Robert F. Simmons Syntactic Graphs: A Representation for the Union of All Ambiguous Parse Trees
gensubl generates one reading with a given root
triple. All readings with a root triple are exhaustive-
ly collected by the predicate gen_subgraphl using
the setof predicate—a meta predicate in Prolog. All
readings of a syntactic graph are produced by the
predicate gensubgraph, which calls the predicate
gensubgraphl for each root triple in RootList. There-
fore, this algorithm generates all subgraphs of a syntac-
tic graph that satisfy Property 3 and that are consistent
with the exclusion matrix. Hence, the set of subgraphs
generated by the algorithm includes all parse trees in the
forest.
The above algorithm checks the exclusion matrix
when it generates subgraphs from the syntactic graph,
so all triples in each subgraph generated by the algo-
rithm are guaranteed to co-occur with each other in the
exclusion matrix. Unfortunately, it does not appear
possible to prove that if triples, say T l and T2, T2 and
T3, and Ti and T3, all co-occur in pairs, that they must
all three co-occur in the same tree! So, although empir-
ically all of our experiments have generated only trees
from the forest, the exclusion matrix does not provide
mathematical assurance of soundness.
If subsequent experience with our present statisti-
cally satisfactory, but unsound exclusion matrix re-
quires it, we can produce, instead, an inclusion matrix
that guarantees soundness. The columns of this matrix
are I.D. numbers for each parse tree; the rows are
triples. The following procedure constructs the matrix.
</bodyText>
<listItem confidence="0.928187333333333">
1. Navigate the parse forest to extract a parse tree,
I, and collect triples appearing in that parse tree.
2. Mark matrix(Tindex, I) = 1, for each triple with
</listItem>
<bodyText confidence="0.996825823529412">
the index Tindex appearing in the I-th parse tree.
Backtrack to step 1 to extract another possible
parse tree until all parse trees are exhausted.
Then, given a column number i, all triples marked in
that column co-occur in the i-th parse tree. Since this
algorithm must navigate all possible parse trees one by
one, it is less efficient than the algorithm for construct-
ing the exclusion matrix. But if our present system
eventually proves unsound, this inclusion matrix guar-
antees that we can test any set of constituents to
determine unequivocally if they occur in a single parse
tree from the forest.
Therefore, we claim that syntactic graphs enable us
to enumerate all and only the syntactic readings given in
a parse forest, and that syntactic graph representation is
complete and sound compared to tree representations of
the syntactic structure of a sentence.
</bodyText>
<sectionHeader confidence="0.997511" genericHeader="related work">
7 RELATED WORKS
</sectionHeader>
<bodyText confidence="0.999496566666667">
Several researchers have proposed variant representa-
tions for syntactic structure. Most of them, however,
concentrated on how to use the new structure in the
parsing process. Syntactic graph representation in this
work does not affect any parsing strategy, but is con-
structed after the syntactic processor finishes generat-
ing a parse forest using any all path parser.
Marcus et. al. (1983) propose a parsing representa-
tion that is also different from tree representation. They
use the new representation for a syntactic structure of a
sentence to preserve information, while modifying the
structure during parsing, so that they can solve the
problems of a deterministic parser (Marcus 1980)—i.e.,
parsing garden path sentences. Marcus&apos;s representation
consists of dominator-modifier relationships between
two nodes. It is, however, doubtful that a correct parse
tree can be derived from the final structure, which
consists of only domination relationships. They do not
represent all possible syntactic readings in one struc-
ture.
Barton and Berwick (1985) also discuss the possibil-
ity of a different representation, an &amp;quot;assertion set&amp;quot;, as
an alternative for trees, and show various advantages
expected from the new structure. As in Marcus&apos;s work,
they use the assertion set to preserve information as
parsing progresses, so that they can make a determin-
istic parser to be partially noncommittal, when the
parser handles ambiguous phrases. Their representation
consists of sets of assertions. Each assertion that rep-
resents a constituent is a triple that has the category
name and the range of terminals that the constituent
spans. It is unclear how to represent dominance rela-
tionships between constituents with assertion sets, and
whether the final structure represents all possible parses
or parts of the parses.
Rich et. al. (1987) also propose a syntactic represen-
tation in which all syntactic ambiguities are kept. In this
work., the ambiguous points are represented as one
modifier with many possible dominators. Since, how-
ever, this work also does not consider possible prob-
lems of exclusive attachments, their representation
loses some information present in a parse forest.
Tomita (1985) also suggests a disambiguation pro-
cess, using his shared, packed-parse forest, in which all
possible syntactic ambiguities are stored. The disambig-
uation process navigates a parse forest, and asks a user
whenever it meets an ambiguous packed node. It does a
&amp;quot;shaving-a-forest&amp;quot; operation, which traverses the parse
forest to delete ambiguous branches. Deleting one arc
accomplishes the &amp;quot;shave&amp;quot; in the syntactic graph repre-
sentation. Furthermore, in a parse forest, the ambigu-
ous points can be checked only by navigating the forest
and are not explicit.
Since a parse forest does not allow direct access to its
internal structure, a semantic processor would have to
traverse the forest whenever it needed to check internal
relations to generate case relations and disambiguate
without a user&apos;s guidance. Syntactic graph representa-
tion provides a more concise and efficient structure for
higher level processes.
</bodyText>
<page confidence="0.957779">
30 Computational Linguistics, Volume 15, Number 1, March 1989
</page>
<note confidence="0.812491">
Jungyun Seo and Robert F. Simmons Syntactic Graphs: A Representation for the Union of All Ambiguous Parse Trees
</note>
<sectionHeader confidence="0.990348" genericHeader="conclusions">
8 CONCLUSION
</sectionHeader>
<bodyText confidence="0.99998709375">
In this paper, we propose the syntactic graph with an
exclusion matrix as a new representation of the surface
syntactic structure of a sentence. Several properties of
syntactic graphs are examined. An algorithm that enu-
merates all and only the correct syntactic readings from
a syntactic graph is also presented. Therefore, we claim
that syntactic graph representation provides a concise
way to represent all possible syntactic readings in one
structure without losing any useful information con-
tained in the tree structured representation.
To further justify that syntactic graph representation
is a suitable formalism for an output format of syntactic
processes, we need to investigate methods for using
syntactic graphs to make correct decisions in higher
level processes. The exclusion matrix is an efficient tool
to help semantic processes make correct choices.
Because of its conciseness, the syntactic graph
makes it possible to store temporarily the syntactic
structure of sentences that already have been proc-
essed. A text understanding process is very likely to
find contradicting evidence between a current sentence
and the context of the previous sentences. If we did not
keep alternative analyses of previous sentences the only
thing we could do is backtracking, which is computa-
tionally too expensive. Furthermore, since the search
space of the syntactic processor is different from that of
the semantic processor, it is very important for the
syntactic process to commit to a final result. We are
currently investigating how to use syntactic graphs of
previous sentences to maintain a continuous context
whose ambiguity is successively reduced by additional
incoming sentences.
</bodyText>
<sectionHeader confidence="0.997506" genericHeader="acknowledgments">
ACKNOWLEDGMENTS
</sectionHeader>
<bodyText confidence="0.999321">
This work is sponsored by the Army Research Office under contract
DAAG29-84-K-0060. The authors are grateful to Olivier Winghart for
his critical review of an earlier draft of this paper.
</bodyText>
<sectionHeader confidence="0.999792" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.999323735849057">
Aho, A. V. and Ullman, J. D. 1972 The Theory of Parsing, Translation
and Compiling I. Prentice-Hall, Englewood Cliffs, NJ.
Barton, G. E. and Berwick, R. C. 1985 &amp;quot;Parsing with Assertion Sets
and Information Monotonicity.&amp;quot; In Proceedings of International
Joint Conference on Artificial Intelligence-85 (IJCAI-85): 769-
771.
Birnbaum, L. and Selfridge, M. 1981 &amp;quot;Conceptual analysis of natural
language.&amp;quot; In R. Schank and C. Riesbeck, eds., Inside Computer
Understanding. Lawrence Erlbaum, Hillsdale, NJ.
Chester, D. 1980 &amp;quot;A Parsing Algorithm that extends Phrases.&amp;quot;
American Journal of Computational Linguistics 6 (2): 87-96.
Chomsky, N. 1970 &amp;quot;Remarks on nominalization.&amp;quot; In R. Jacobs and
P. S. Rosenbaum, Eds., Readings in English Transformational
Grammar. Waltham, MA: Ginn &amp; Co.
Chomsky, N. 1981 Lectures on Government and Binding. Foris,
Dordrecht, Holland.
Early, J. 1970 &amp;quot;An Efficient Context-free Parsing algorithm.&amp;quot; Comm
ACM 13, (2): 94-102.
Frazier, L. and Fodor, J. 1979 &amp;quot;The Sausage Machine: A New
Two-Stage Parsing Model.&amp;quot; Cognition 6: 41-58.
Kay, M. 1980 &amp;quot;Algorithm Schemata and Data Structures in Syntactic
Processing.&amp;quot; Xerox Corporation, Technical Report Number CSL-
80-12, Palo Alto, CA.
Lytinen, S. L. 1986 &amp;quot;Dynamically Combining syntax and semantics in
natural language processing.&amp;quot; In Proceedings of The American
Association for Artificial Intelligence-86(AAAI-86): 574-578.
Marcus, M. P. 1980 A Theory of Syntactic Recognition for Natural
Language. MIT Press, Cambridge, MA.
Marcus, M. P.; Hindle, D.; and Fleck, M. M. 1983 &amp;quot;D-Theory:
Talking about Talking about Trees.&amp;quot; In Proceedings of 21st
Annual Meeting of the Association for Computational Linguistics:
129-136.
Pereira, F. C. N. and Warren, D. H. 1980 &amp;quot;Definite Clause Grammars
— A survey of the formalism and a Comparison with Augmented
Transition Network.&amp;quot; Artificial Intelligence, 13: 231-278.
Rich, A.; Barnett, J.; Wittenburg, K.; and Wroblewski, D. 1987
&amp;quot;Ambiguity Procrastination.&amp;quot; In Proceedings of AAAI-87: 571-
576.
Shubert, L. K. 1984 &amp;quot;On Parsing Preferences.&amp;quot; In Proceedings of the
Conference on Computational Linguistics 84 Stanford, CA: 247-
250.
Shubert, L. K. 1986 &amp;quot;Are There Preference Trade-Offs in Attachment
Decision?&amp;quot; In Proceedings of AAAI-86: 601-605.
Tomita, M. 1985 Efficient Parsing for Natural Language. Kluwer
Academic Publishers, Boston, MA.
Tsukada, D. 1987 &amp;quot;Using Dominator-Modifier Relations to Disam-
biguate a Sentence&amp;quot; (master&apos;s thesis), Department of Computer
Sciences, University of Texas at Austin.
Waltz, D. L. 1982 &amp;quot;The State of the Art in Natural Language
Understanding.&amp;quot; In W. Lehnert and M. Ringle (eds.), Strategies
for Natural Language Processing, Lawrence Erlbaum Associates,
Inc., Hillsdale, NJ.
Wilks, Y.; Huang, X.; and Fass, D. 1985 &amp;quot;Syntax, Preference and
Right Attachment.&amp;quot; In Proceedings of International Joint Confer-
ence on Artificial Intelligence-85 (IJCAI-85): 779-784.
Winghart, 0. J. 1986 &amp;quot;A Processing Model for Recognition of
Discourse Coherence Relations&amp;quot; (unpublished Ph.D proposal),
Department of Computer Sciences, University of Texas at Austin.
NOTES
1. Since we are discussing the syntactic representation, we use the
term &amp;quot;semantic processor&amp;quot; for all higher level processors in-
cluding the semantic, coherence, and discourse processors.
2. By &amp;quot;correct&amp;quot; is meant semantically correct. Here, semantics
has a broad meaning including pragmatics.
3. Borrowing a term from Tomita&apos;s (1985) system. Although we
present an example of a shared, packed-parse forest in Section 3,
we refer readers to (Tomita 1985) for more detailed discussion
and examples.
4. There are different views of text processing in which syntactic
and semantic processors are integrated (Birnbaum and Selfridge
1981; Lytinen 1986; Winghart 1986). However detailed discus-
sion of other control flows is beyond the scope of this work.
5. For the sentence, &amp;quot;It is transmitted by eating shellfish such as
oysters living in infected waters, or by drinking infected water,
or by dirt from soiled fingers&amp;quot;, there are 1433 parses from our
context-free grammar.
6. In our experience, a graph representing several hundred parse
trees may take less than three times the number of triples as one
representing a single interpretation graph for the sentence.
7. In a syntactic graph, however, we call both modifier and
specifier nodes modifier nodes.
8. From now on, we will use the terms node and word, as well as
arc and triple, interchangeably.
9. A unique index can be generated using the special function
gensym which returns a unique symbol whenever it is called.
Computational Linguistics, Volume 15, Number 1, March 1989 31
Jungyun Seo and Robert F. Simmons Syntactic Graphs: A Representation for the Union of All Ambiguous Parse Trees
10. Due to the complexity of the diagram, some of the details are
omitted.
11. Not all different readings of a syntactic graph have different root
nodes. In this example, [0,time,v] is the root node of two
different readings of the graph with simple grammar rules. The
equivalent parse trees of the two readings are:
[snt,[vp,[verb,[time]],[np,[np,[noun,[flies]]],[pp,[prep,[like]],
[np,[det,[an]] ,[noun,[arrowl]]]]]]
[snt,[vp,[vp,[verb,[time]],[np,[noun,[flies]] l],[pp,[prep,[likel],
[np,[det,[an] ],[noun,[arrow]]]]]]
12. The node in a forest is different from the node in a syntactic
graph. A non-terminal node in a forest with two children nodes
has one head-modifier relation, and hence the non-terminal
with two children in a forest represents one arc in a syntactic
graph.
13. We use the syntax of Quintus—Prolog version 2 on SUN systems.
The special predicate, ( Cond —&gt; Then ; Else), in the algorithm
can be interpreted as; if Cond is true, then call Then. Otherwise,
call Else.
</reference>
<page confidence="0.983175">
32 Computational Linguistics, Volume 15, Number 1, March 1989
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.767877">
<title confidence="0.993284">GRAPHS: FOR THE UNION OF TREES</title>
<author confidence="0.949709">Jungyun Seo Robert F Simmons</author>
<affiliation confidence="0.9948865">Artificial Intelligence University of Texas at</affiliation>
<address confidence="0.880502">Austin, TX 78712-1188</address>
<abstract confidence="0.999271181818182">In this paper, we present a new method of representing the surface syntactic structure of a sentence. Trees have usually been used in linguistics and natural language processing to represent syntactic structures of a sentence. A tree structure shows only one possible syntactic parse of a sentence, but in order to choose a correct parse, we need to examine all possible tree structures one by one. Syntactic graph representation makes it possible to represent all possible surface syntactic relations in one directed graph (DG). Since a syntactic graph is expressed in terms of a set of triples, higher level semantic processes can access any part of the graph directly without navigating the whole structure. Furthermore, since a syntactic graph represents the union of all possible syntactic readings of a sentence, it is fairly easy to focus on the syntactically ambiguous points. In this paper, we introduce the basic idea of syntactic graph representation and discuss its various properties. We claim that a syntactic graph carries complete syntactic information provided by a parse forest—the set of all possible parse trees.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A V Aho</author>
<author>J D Ullman</author>
</authors>
<title>The Theory of Parsing, Translation and Compiling I. Prentice-Hall,</title>
<date>1972</date>
<location>Englewood Cliffs, NJ.</location>
<contexts>
<context position="3450" citStr="Aho and Ullman 1972" startWordPosition="526" endWordPosition="529">cus on those points for further disambiguation. Furthermore, syntactic graph representation can be naturally implemented in efficient, parallel, all-path parsers. One-path parsing algorithms, like the DCG (Pereira and Warren 1980), which enumerates all possible parse trees one by one with backtracking, usually have exponential complexity. All-path parsing algorithms explore all possible paths in parallel without backtracking (Early 1970; Kay 1980; Chester 1980; Tomita 1985). In these algorithms, it is efficient to generate all possible parse trees. This kind of algorithm has complexity 0(N3) (Aho and Ullman 1972; Tomita 1985). We use an all-path parsing algorithm to parse a sentence. Triples, each of which consists of two nodes and an arc name, are generated while parsing a sentence. The parser collects all correct triples and constructs an exclusion matrix, which shows co-occurrence constraints among arcs, by navigating all possible parse Copyright 1989 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material is granted provided that the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included</context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>Aho, A. V. and Ullman, J. D. 1972 The Theory of Parsing, Translation and Compiling I. Prentice-Hall, Englewood Cliffs, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E Barton</author>
<author>R C Berwick</author>
</authors>
<title>Parsing with Assertion Sets and Information Monotonicity.&amp;quot;</title>
<date>1985</date>
<booktitle>In Proceedings of International Joint Conference on Artificial Intelligence-85 (IJCAI-85):</booktitle>
<pages>769--771</pages>
<contexts>
<context position="54974" citStr="Barton and Berwick (1985)" startWordPosition="9170" endWordPosition="9173">s also different from tree representation. They use the new representation for a syntactic structure of a sentence to preserve information, while modifying the structure during parsing, so that they can solve the problems of a deterministic parser (Marcus 1980)—i.e., parsing garden path sentences. Marcus&apos;s representation consists of dominator-modifier relationships between two nodes. It is, however, doubtful that a correct parse tree can be derived from the final structure, which consists of only domination relationships. They do not represent all possible syntactic readings in one structure. Barton and Berwick (1985) also discuss the possibility of a different representation, an &amp;quot;assertion set&amp;quot;, as an alternative for trees, and show various advantages expected from the new structure. As in Marcus&apos;s work, they use the assertion set to preserve information as parsing progresses, so that they can make a deterministic parser to be partially noncommittal, when the parser handles ambiguous phrases. Their representation consists of sets of assertions. Each assertion that represents a constituent is a triple that has the category name and the range of terminals that the constituent spans. It is unclear how to rep</context>
</contexts>
<marker>Barton, Berwick, 1985</marker>
<rawString>Barton, G. E. and Berwick, R. C. 1985 &amp;quot;Parsing with Assertion Sets and Information Monotonicity.&amp;quot; In Proceedings of International Joint Conference on Artificial Intelligence-85 (IJCAI-85): 769-771.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Birnbaum</author>
<author>M Selfridge</author>
</authors>
<title>Conceptual analysis of natural language.&amp;quot;</title>
<date>1981</date>
<booktitle>Inside Computer Understanding. Lawrence Erlbaum,</booktitle>
<editor>In R. Schank and C. Riesbeck, eds.,</editor>
<location>Hillsdale, NJ.</location>
<marker>Birnbaum, Selfridge, 1981</marker>
<rawString>Birnbaum, L. and Selfridge, M. 1981 &amp;quot;Conceptual analysis of natural language.&amp;quot; In R. Schank and C. Riesbeck, eds., Inside Computer Understanding. Lawrence Erlbaum, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chester</author>
</authors>
<title>A Parsing Algorithm that extends Phrases.&amp;quot;</title>
<date>1980</date>
<journal>American Journal of Computational Linguistics</journal>
<volume>6</volume>
<issue>2</issue>
<pages>87--96</pages>
<contexts>
<context position="3295" citStr="Chester 1980" startWordPosition="503" endWordPosition="504">ther processing. As we will show in the rest of this paper, since all syntactically ambiguous points are kept in a syntactic graph, we can easily focus on those points for further disambiguation. Furthermore, syntactic graph representation can be naturally implemented in efficient, parallel, all-path parsers. One-path parsing algorithms, like the DCG (Pereira and Warren 1980), which enumerates all possible parse trees one by one with backtracking, usually have exponential complexity. All-path parsing algorithms explore all possible paths in parallel without backtracking (Early 1970; Kay 1980; Chester 1980; Tomita 1985). In these algorithms, it is efficient to generate all possible parse trees. This kind of algorithm has complexity 0(N3) (Aho and Ullman 1972; Tomita 1985). We use an all-path parsing algorithm to parse a sentence. Triples, each of which consists of two nodes and an arc name, are generated while parsing a sentence. The parser collects all correct triples and constructs an exclusion matrix, which shows co-occurrence constraints among arcs, by navigating all possible parse Copyright 1989 by the Association for Computational Linguistics. Permission to copy without fee all or part of</context>
</contexts>
<marker>Chester, 1980</marker>
<rawString>Chester, D. 1980 &amp;quot;A Parsing Algorithm that extends Phrases.&amp;quot; American Journal of Computational Linguistics 6 (2): 87-96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<title>Remarks on nominalization.&amp;quot; In</title>
<date>1970</date>
<booktitle>Eds., Readings in English Transformational Grammar.</booktitle>
<publisher>Ginn &amp; Co.</publisher>
<location>Waltham, MA:</location>
<contexts>
<context position="9953" citStr="Chomsky (1970)" startWordPosition="1582" endWordPosition="1583">s from words with multiple syntactic categories and structural ambiguities from the ambiguous grammar—is kept. The nodes which are pointed to by more than one arc show the ambiguous points in the sentence, so the semantic processor can focus on those points to resolve the ambiguities. Furthermore, since a syntactic graph is represented by a set of triples, a semantic processor can directly access any part of a graph without traversing the whole. Finally, syntactic graph representation is compact enough to be kept in memory for a while.6 3 X THEORY AND SYNTACTIC GRAPHS 1 theory was proposed by Chomsky (1970) to explain various linguistic structural properties, and has been widely accepted in linguistic theories. In this notation, the head of any phrase is termed X, the phrasal category containing X is termed_, and the phrasal category containing X is termed X. For example, the head of a noun_phrase is N (noun), Nis an intermediate category, and N corresponds to noun phrase (NP). The general form of the phrase structure rules for X theory is roughly as follows: • r*Tf • X -&gt; XZ * , where * is a Kleene star. is the phrase that specifies X, and 2 is the phrase that modifies X. The properties of the </context>
</contexts>
<marker>Chomsky, 1970</marker>
<rawString>Chomsky, N. 1970 &amp;quot;Remarks on nominalization.&amp;quot; In R. Jacobs and P. S. Rosenbaum, Eds., Readings in English Transformational Grammar. Waltham, MA: Ginn &amp; Co.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chomsky</author>
</authors>
<date>1981</date>
<booktitle>Lectures on Government and Binding. Foris,</booktitle>
<location>Dordrecht, Holland.</location>
<marker>Chomsky, 1981</marker>
<rawString>Chomsky, N. 1981 Lectures on Government and Binding. Foris, Dordrecht, Holland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Early</author>
</authors>
<title>An Efficient Context-free Parsing algorithm.&amp;quot;</title>
<date>1970</date>
<journal>Comm ACM</journal>
<volume>13</volume>
<pages>94--102</pages>
<contexts>
<context position="3271" citStr="Early 1970" startWordPosition="499" endWordPosition="500">a compact form for further processing. As we will show in the rest of this paper, since all syntactically ambiguous points are kept in a syntactic graph, we can easily focus on those points for further disambiguation. Furthermore, syntactic graph representation can be naturally implemented in efficient, parallel, all-path parsers. One-path parsing algorithms, like the DCG (Pereira and Warren 1980), which enumerates all possible parse trees one by one with backtracking, usually have exponential complexity. All-path parsing algorithms explore all possible paths in parallel without backtracking (Early 1970; Kay 1980; Chester 1980; Tomita 1985). In these algorithms, it is efficient to generate all possible parse trees. This kind of algorithm has complexity 0(N3) (Aho and Ullman 1972; Tomita 1985). We use an all-path parsing algorithm to parse a sentence. Triples, each of which consists of two nodes and an arc name, are generated while parsing a sentence. The parser collects all correct triples and constructs an exclusion matrix, which shows co-occurrence constraints among arcs, by navigating all possible parse Copyright 1989 by the Association for Computational Linguistics. Permission to copy wi</context>
</contexts>
<marker>Early, 1970</marker>
<rawString>Early, J. 1970 &amp;quot;An Efficient Context-free Parsing algorithm.&amp;quot; Comm ACM 13, (2): 94-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Frazier</author>
<author>J Fodor</author>
</authors>
<title>The Sausage Machine: A New Two-Stage Parsing Model.&amp;quot;</title>
<date>1979</date>
<journal>Cognition</journal>
<volume>6</volume>
<pages>41--58</pages>
<contexts>
<context position="6740" citStr="Frazier and Fodor 1979" startWordPosition="1052" endWordPosition="1055">cessor should ask the syntactic processor for another possible parse tree. This cycle of processing will continue until the semantic processor finds the correct parse tree with which it succeeds in understanding the sentence. Let us consider the following sentences, from Waltz (1982): I saw a man on the hill with a telescope. I cleaned the lens to get a better view. When we read the first sentence, we cannot determine whether the man has a telescope or the telescope is used to see the man. This is known as the PP-attachment problem, and many researchers have proposed various ways to solve it (Frazier and Fodor 1979; Shubert 1984, 1986; Wilks et. al 1985). In this sentence, however, it is impossible to choose a correct syntactic reading in syntactic processing—even with commonsense knowledge. The ambiguities must remain until the system extracts more contextual knowledge from other input sentences. The problems of tree structure representation in the pipelined, natural language processing model are the following: • First, since the number of parse trees of a typical sentence in real text easily grows to several hundreds, and it is impossible to resolve syntactic ambiguities by the syntactic processor its</context>
</contexts>
<marker>Frazier, Fodor, 1979</marker>
<rawString>Frazier, L. and Fodor, J. 1979 &amp;quot;The Sausage Machine: A New Two-Stage Parsing Model.&amp;quot; Cognition 6: 41-58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
</authors>
<title>Algorithm Schemata and Data Structures in Syntactic Processing.&amp;quot; Xerox Corporation,</title>
<date>1980</date>
<tech>Technical Report Number CSL80-12,</tech>
<location>Palo Alto, CA.</location>
<contexts>
<context position="3281" citStr="Kay 1980" startWordPosition="501" endWordPosition="502">rm for further processing. As we will show in the rest of this paper, since all syntactically ambiguous points are kept in a syntactic graph, we can easily focus on those points for further disambiguation. Furthermore, syntactic graph representation can be naturally implemented in efficient, parallel, all-path parsers. One-path parsing algorithms, like the DCG (Pereira and Warren 1980), which enumerates all possible parse trees one by one with backtracking, usually have exponential complexity. All-path parsing algorithms explore all possible paths in parallel without backtracking (Early 1970; Kay 1980; Chester 1980; Tomita 1985). In these algorithms, it is efficient to generate all possible parse trees. This kind of algorithm has complexity 0(N3) (Aho and Ullman 1972; Tomita 1985). We use an all-path parsing algorithm to parse a sentence. Triples, each of which consists of two nodes and an arc name, are generated while parsing a sentence. The parser collects all correct triples and constructs an exclusion matrix, which shows co-occurrence constraints among arcs, by navigating all possible parse Copyright 1989 by the Association for Computational Linguistics. Permission to copy without fee </context>
</contexts>
<marker>Kay, 1980</marker>
<rawString>Kay, M. 1980 &amp;quot;Algorithm Schemata and Data Structures in Syntactic Processing.&amp;quot; Xerox Corporation, Technical Report Number CSL80-12, Palo Alto, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Lytinen</author>
</authors>
<title>Dynamically Combining syntax and semantics in natural language processing.&amp;quot;</title>
<date>1986</date>
<booktitle>In Proceedings of The American Association for Artificial Intelligence-86(AAAI-86):</booktitle>
<pages>574--578</pages>
<marker>Lytinen, 1986</marker>
<rawString>Lytinen, S. L. 1986 &amp;quot;Dynamically Combining syntax and semantics in natural language processing.&amp;quot; In Proceedings of The American Association for Artificial Intelligence-86(AAAI-86): 574-578.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
</authors>
<title>A Theory of Syntactic Recognition for Natural Language.</title>
<date>1980</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="54610" citStr="Marcus 1980" startWordPosition="9121" endWordPosition="9122">ure. Most of them, however, concentrated on how to use the new structure in the parsing process. Syntactic graph representation in this work does not affect any parsing strategy, but is constructed after the syntactic processor finishes generating a parse forest using any all path parser. Marcus et. al. (1983) propose a parsing representation that is also different from tree representation. They use the new representation for a syntactic structure of a sentence to preserve information, while modifying the structure during parsing, so that they can solve the problems of a deterministic parser (Marcus 1980)—i.e., parsing garden path sentences. Marcus&apos;s representation consists of dominator-modifier relationships between two nodes. It is, however, doubtful that a correct parse tree can be derived from the final structure, which consists of only domination relationships. They do not represent all possible syntactic readings in one structure. Barton and Berwick (1985) also discuss the possibility of a different representation, an &amp;quot;assertion set&amp;quot;, as an alternative for trees, and show various advantages expected from the new structure. As in Marcus&apos;s work, they use the assertion set to preserve infor</context>
</contexts>
<marker>Marcus, 1980</marker>
<rawString>Marcus, M. P. 1980 A Theory of Syntactic Recognition for Natural Language. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>D Hindle</author>
<author>M M Fleck</author>
</authors>
<title>D-Theory: Talking about Talking about Trees.&amp;quot;</title>
<date>1983</date>
<booktitle>In Proceedings of 21st Annual Meeting of the Association for Computational Linguistics:</booktitle>
<pages>129--136</pages>
<marker>Marcus, Hindle, Fleck, 1983</marker>
<rawString>Marcus, M. P.; Hindle, D.; and Fleck, M. M. 1983 &amp;quot;D-Theory: Talking about Talking about Trees.&amp;quot; In Proceedings of 21st Annual Meeting of the Association for Computational Linguistics: 129-136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F C N Pereira</author>
<author>D H Warren</author>
</authors>
<title>Definite Clause Grammars — A survey of the formalism and a Comparison with Augmented Transition Network.&amp;quot;</title>
<date>1980</date>
<journal>Artificial Intelligence,</journal>
<volume>13</volume>
<pages>231--278</pages>
<contexts>
<context position="3061" citStr="Pereira and Warren 1980" startWordPosition="467" endWordPosition="470">eck parse trees one by one without knowing where the ambiguous points are. We have tried to reduce this problem by introducing a new structure, the syntactic graph, that can represent all possible parse trees effectively in a compact form for further processing. As we will show in the rest of this paper, since all syntactically ambiguous points are kept in a syntactic graph, we can easily focus on those points for further disambiguation. Furthermore, syntactic graph representation can be naturally implemented in efficient, parallel, all-path parsers. One-path parsing algorithms, like the DCG (Pereira and Warren 1980), which enumerates all possible parse trees one by one with backtracking, usually have exponential complexity. All-path parsing algorithms explore all possible paths in parallel without backtracking (Early 1970; Kay 1980; Chester 1980; Tomita 1985). In these algorithms, it is efficient to generate all possible parse trees. This kind of algorithm has complexity 0(N3) (Aho and Ullman 1972; Tomita 1985). We use an all-path parsing algorithm to parse a sentence. Triples, each of which consists of two nodes and an arc name, are generated while parsing a sentence. The parser collects all correct tri</context>
</contexts>
<marker>Pereira, Warren, 1980</marker>
<rawString>Pereira, F. C. N. and Warren, D. H. 1980 &amp;quot;Definite Clause Grammars — A survey of the formalism and a Comparison with Augmented Transition Network.&amp;quot; Artificial Intelligence, 13: 231-278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rich</author>
<author>J Barnett</author>
<author>K Wittenburg</author>
<author>D Wroblewski</author>
</authors>
<title>Ambiguity Procrastination.&amp;quot;</title>
<date>1987</date>
<booktitle>In Proceedings of AAAI-87:</booktitle>
<pages>571--576</pages>
<marker>Rich, Barnett, Wittenburg, Wroblewski, 1987</marker>
<rawString>Rich, A.; Barnett, J.; Wittenburg, K.; and Wroblewski, D. 1987 &amp;quot;Ambiguity Procrastination.&amp;quot; In Proceedings of AAAI-87: 571-576.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L K Shubert</author>
</authors>
<title>On Parsing Preferences.&amp;quot;</title>
<date>1984</date>
<booktitle>In Proceedings of the Conference on Computational Linguistics 84</booktitle>
<pages>247--250</pages>
<location>Stanford, CA:</location>
<contexts>
<context position="6754" citStr="Shubert 1984" startWordPosition="1056" endWordPosition="1057">ntactic processor for another possible parse tree. This cycle of processing will continue until the semantic processor finds the correct parse tree with which it succeeds in understanding the sentence. Let us consider the following sentences, from Waltz (1982): I saw a man on the hill with a telescope. I cleaned the lens to get a better view. When we read the first sentence, we cannot determine whether the man has a telescope or the telescope is used to see the man. This is known as the PP-attachment problem, and many researchers have proposed various ways to solve it (Frazier and Fodor 1979; Shubert 1984, 1986; Wilks et. al 1985). In this sentence, however, it is impossible to choose a correct syntactic reading in syntactic processing—even with commonsense knowledge. The ambiguities must remain until the system extracts more contextual knowledge from other input sentences. The problems of tree structure representation in the pipelined, natural language processing model are the following: • First, since the number of parse trees of a typical sentence in real text easily grows to several hundreds, and it is impossible to resolve syntactic ambiguities by the syntactic processor itself, a semanti</context>
</contexts>
<marker>Shubert, 1984</marker>
<rawString>Shubert, L. K. 1984 &amp;quot;On Parsing Preferences.&amp;quot; In Proceedings of the Conference on Computational Linguistics 84 Stanford, CA: 247-250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L K Shubert</author>
</authors>
<title>Are There Preference Trade-Offs in Attachment Decision?&amp;quot;</title>
<date>1986</date>
<booktitle>In Proceedings of AAAI-86:</booktitle>
<pages>601--605</pages>
<marker>Shubert, 1986</marker>
<rawString>Shubert, L. K. 1986 &amp;quot;Are There Preference Trade-Offs in Attachment Decision?&amp;quot; In Proceedings of AAAI-86: 601-605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tomita</author>
</authors>
<title>Efficient Parsing for Natural Language.</title>
<date>1985</date>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Boston, MA.</location>
<contexts>
<context position="3309" citStr="Tomita 1985" startWordPosition="505" endWordPosition="506">g. As we will show in the rest of this paper, since all syntactically ambiguous points are kept in a syntactic graph, we can easily focus on those points for further disambiguation. Furthermore, syntactic graph representation can be naturally implemented in efficient, parallel, all-path parsers. One-path parsing algorithms, like the DCG (Pereira and Warren 1980), which enumerates all possible parse trees one by one with backtracking, usually have exponential complexity. All-path parsing algorithms explore all possible paths in parallel without backtracking (Early 1970; Kay 1980; Chester 1980; Tomita 1985). In these algorithms, it is efficient to generate all possible parse trees. This kind of algorithm has complexity 0(N3) (Aho and Ullman 1972; Tomita 1985). We use an all-path parsing algorithm to parse a sentence. Triples, each of which consists of two nodes and an arc name, are generated while parsing a sentence. The parser collects all correct triples and constructs an exclusion matrix, which shows co-occurrence constraints among arcs, by navigating all possible parse Copyright 1989 by the Association for Computational Linguistics. Permission to copy without fee all or part of this material</context>
<context position="21593" citStr="Tomita (1985)" startWordPosition="3527" endWordPosition="3528">37], 161 ] 1044 [np, [3,man], [[1008, 1043], 20] ] 1043 [np, [3,man], [[1042], notriple] ] 1042 [npl, [3,man], [[1010, 1041], 19], [[1024, 1037], 25] ] Figure 4 Shared, Packed-Parse Forest. participate in each correct syntactic analysis of the sentence. The collecting algorithm is explained in Section 5.2 in detail. The representation of the shared, packed-parse forest for the example in Figure 2 is in figures 4 and 5.10 It is important to notice that the shared, packed-parse forest generated in this parser is different from that of other parsers. In the shared, packed-parse forest defined by Tomita (1985), any constituents that have the same category and span the same terminal nodes are regarded as the same constituent and packed into one node. In the parser for syntactic graphs, the packing condition is slightly different in that each constituent is identified by the head word of the constituent as well as the category and the terminals it spans. Therefore, although two nodes might have the same category and span the same terminals, if the nodes have different head words, then they cannot be packed together. A packed node contains several nodes, each of which contains the category of the node</context>
<context position="56115" citStr="Tomita (1985)" startWordPosition="9350" endWordPosition="9351"> range of terminals that the constituent spans. It is unclear how to represent dominance relationships between constituents with assertion sets, and whether the final structure represents all possible parses or parts of the parses. Rich et. al. (1987) also propose a syntactic representation in which all syntactic ambiguities are kept. In this work., the ambiguous points are represented as one modifier with many possible dominators. Since, however, this work also does not consider possible problems of exclusive attachments, their representation loses some information present in a parse forest. Tomita (1985) also suggests a disambiguation process, using his shared, packed-parse forest, in which all possible syntactic ambiguities are stored. The disambiguation process navigates a parse forest, and asks a user whenever it meets an ambiguous packed node. It does a &amp;quot;shaving-a-forest&amp;quot; operation, which traverses the parse forest to delete ambiguous branches. Deleting one arc accomplishes the &amp;quot;shave&amp;quot; in the syntactic graph representation. Furthermore, in a parse forest, the ambiguous points can be checked only by navigating the forest and are not explicit. Since a parse forest does not allow direct acce</context>
</contexts>
<marker>Tomita, 1985</marker>
<rawString>Tomita, M. 1985 Efficient Parsing for Natural Language. Kluwer Academic Publishers, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Tsukada</author>
</authors>
<title>Using Dominator-Modifier Relations to Disambiguate a Sentence&amp;quot;</title>
<date>1987</date>
<tech>(master&apos;s thesis),</tech>
<institution>Department of Computer Sciences, University of Texas at Austin.</institution>
<contexts>
<context position="11044" citStr="Tsukada (1987)" startWordPosition="1777" endWordPosition="1778">Z * , where * is a Kleene star. is the phrase that specifies X, and 2 is the phrase that modifies X. The properties of the head word of a phrase are projected onto the properties of the phrase. We can express a grammar with X conventions to cover a wide range of English. Since, in X theory, a syntactic phrase consists of the head of the phrase and the specifiers and modifiers of the head, if there are more than two constituents in the right-hand side of a grammar rule, then there are dominator-modifier (DM) relationships between the head word and the specifier or modifier words in the phrase. Tsukada (1987) discovered that the DM relationship is effective for keeping all the syntactic ambiguities in a compact and handy structure without enumerating all possible syntactic parse trees. His representation, however, is too simple to maintain some important information about syntactic structure that will be discussed in detail in this paper, and hence fails to take full advantage of the DM-relationship representation. We use a slightly different representation to maintain more information in head-modifier relations. Each head-modifier relation is kept in a triple that is equivalent to an arc between </context>
</contexts>
<marker>Tsukada, 1987</marker>
<rawString>Tsukada, D. 1987 &amp;quot;Using Dominator-Modifier Relations to Disambiguate a Sentence&amp;quot; (master&apos;s thesis), Department of Computer Sciences, University of Texas at Austin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D L Waltz</author>
</authors>
<title>The State of the Art in Natural Language Understanding.&amp;quot;</title>
<date>1982</date>
<booktitle>Strategies for Natural Language Processing, Lawrence Erlbaum Associates,</booktitle>
<editor>In W. Lehnert and M. Ringle (eds.),</editor>
<publisher>Inc.,</publisher>
<location>Hillsdale, NJ.</location>
<contexts>
<context position="6402" citStr="Waltz (1982)" startWordPosition="990" endWordPosition="991">del, a syntactic processor constructs a surface syntactic structure of an input sentence, and then a higher level semantic processor processes it to understand the sentence—i.e., syntactic and semantic processors are pipelined. If the semantic processor fails to understand the sentence with a given parse tree, the semantic processor should ask the syntactic processor for another possible parse tree. This cycle of processing will continue until the semantic processor finds the correct parse tree with which it succeeds in understanding the sentence. Let us consider the following sentences, from Waltz (1982): I saw a man on the hill with a telescope. I cleaned the lens to get a better view. When we read the first sentence, we cannot determine whether the man has a telescope or the telescope is used to see the man. This is known as the PP-attachment problem, and many researchers have proposed various ways to solve it (Frazier and Fodor 1979; Shubert 1984, 1986; Wilks et. al 1985). In this sentence, however, it is impossible to choose a correct syntactic reading in syntactic processing—even with commonsense knowledge. The ambiguities must remain until the system extracts more contextual knowledge f</context>
</contexts>
<marker>Waltz, 1982</marker>
<rawString>Waltz, D. L. 1982 &amp;quot;The State of the Art in Natural Language Understanding.&amp;quot; In W. Lehnert and M. Ringle (eds.), Strategies for Natural Language Processing, Lawrence Erlbaum Associates, Inc., Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wilks</author>
<author>X Huang</author>
<author>D Fass</author>
</authors>
<title>Syntax, Preference and Right Attachment.&amp;quot;</title>
<date>1985</date>
<booktitle>In Proceedings of International Joint Conference on Artificial Intelligence-85 (IJCAI-85):</booktitle>
<pages>779--784</pages>
<marker>Wilks, Huang, Fass, 1985</marker>
<rawString>Wilks, Y.; Huang, X.; and Fass, D. 1985 &amp;quot;Syntax, Preference and Right Attachment.&amp;quot; In Proceedings of International Joint Conference on Artificial Intelligence-85 (IJCAI-85): 779-784.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Winghart</author>
</authors>
<title>A Processing Model for Recognition of Discourse Coherence Relations&amp;quot;</title>
<date>1986</date>
<tech>(unpublished Ph.D proposal),</tech>
<institution>Department of Computer Sciences, University of Texas at Austin. NOTES</institution>
<marker>Winghart, 1986</marker>
<rawString>Winghart, 0. J. 1986 &amp;quot;A Processing Model for Recognition of Discourse Coherence Relations&amp;quot; (unpublished Ph.D proposal), Department of Computer Sciences, University of Texas at Austin. NOTES</rawString>
</citation>
<citation valid="false">
<title>Since we are discussing the syntactic representation, we use the term &amp;quot;semantic processor&amp;quot; for all higher level processors including the semantic, coherence, and discourse processors.</title>
<marker></marker>
<rawString>1. Since we are discussing the syntactic representation, we use the term &amp;quot;semantic processor&amp;quot; for all higher level processors including the semantic, coherence, and discourse processors.</rawString>
</citation>
<citation valid="false">
<title>By &amp;quot;correct&amp;quot; is meant semantically correct. Here, semantics has a broad meaning including pragmatics.</title>
<marker></marker>
<rawString>2. By &amp;quot;correct&amp;quot; is meant semantically correct. Here, semantics has a broad meaning including pragmatics.</rawString>
</citation>
<citation valid="true">
<title>Borrowing a term from Tomita&apos;s</title>
<date>1985</date>
<contexts>
<context position="21593" citStr="(1985)" startWordPosition="3528" endWordPosition="3528">1 ] 1044 [np, [3,man], [[1008, 1043], 20] ] 1043 [np, [3,man], [[1042], notriple] ] 1042 [npl, [3,man], [[1010, 1041], 19], [[1024, 1037], 25] ] Figure 4 Shared, Packed-Parse Forest. participate in each correct syntactic analysis of the sentence. The collecting algorithm is explained in Section 5.2 in detail. The representation of the shared, packed-parse forest for the example in Figure 2 is in figures 4 and 5.10 It is important to notice that the shared, packed-parse forest generated in this parser is different from that of other parsers. In the shared, packed-parse forest defined by Tomita (1985), any constituents that have the same category and span the same terminal nodes are regarded as the same constituent and packed into one node. In the parser for syntactic graphs, the packing condition is slightly different in that each constituent is identified by the head word of the constituent as well as the category and the terminals it spans. Therefore, although two nodes might have the same category and span the same terminals, if the nodes have different head words, then they cannot be packed together. A packed node contains several nodes, each of which contains the category of the node</context>
<context position="54974" citStr="(1985)" startWordPosition="9173" endWordPosition="9173">om tree representation. They use the new representation for a syntactic structure of a sentence to preserve information, while modifying the structure during parsing, so that they can solve the problems of a deterministic parser (Marcus 1980)—i.e., parsing garden path sentences. Marcus&apos;s representation consists of dominator-modifier relationships between two nodes. It is, however, doubtful that a correct parse tree can be derived from the final structure, which consists of only domination relationships. They do not represent all possible syntactic readings in one structure. Barton and Berwick (1985) also discuss the possibility of a different representation, an &amp;quot;assertion set&amp;quot;, as an alternative for trees, and show various advantages expected from the new structure. As in Marcus&apos;s work, they use the assertion set to preserve information as parsing progresses, so that they can make a deterministic parser to be partially noncommittal, when the parser handles ambiguous phrases. Their representation consists of sets of assertions. Each assertion that represents a constituent is a triple that has the category name and the range of terminals that the constituent spans. It is unclear how to rep</context>
</contexts>
<marker>1985</marker>
<rawString>3. Borrowing a term from Tomita&apos;s (1985) system. Although we present an example of a shared, packed-parse forest in Section 3, we refer readers to (Tomita 1985) for more detailed discussion and examples.</rawString>
</citation>
<citation valid="true">
<title>There are different views of text processing in which syntactic and semantic processors are integrated (Birnbaum and Selfridge</title>
<date>1981</date>
<location>Lytinen</location>
<marker>1981</marker>
<rawString>4. There are different views of text processing in which syntactic and semantic processors are integrated (Birnbaum and Selfridge 1981; Lytinen 1986; Winghart 1986). However detailed discussion of other control flows is beyond the scope of this work.</rawString>
</citation>
<citation valid="false">
<title>For the sentence, &amp;quot;It is transmitted by eating shellfish such as oysters living in infected waters, or by drinking infected water, or by dirt from soiled fingers&amp;quot;, there are 1433 parses from our context-free grammar.</title>
<marker></marker>
<rawString>5. For the sentence, &amp;quot;It is transmitted by eating shellfish such as oysters living in infected waters, or by drinking infected water, or by dirt from soiled fingers&amp;quot;, there are 1433 parses from our context-free grammar.</rawString>
</citation>
<citation valid="false">
<title>In our experience, a graph representing several hundred parse trees may take less than three times the number of triples as one representing a single interpretation graph for the sentence.</title>
<marker></marker>
<rawString>6. In our experience, a graph representing several hundred parse trees may take less than three times the number of triples as one representing a single interpretation graph for the sentence.</rawString>
</citation>
<citation valid="false">
<title>7. In a syntactic graph, however, we call both modifier and specifier nodes modifier nodes.</title>
<marker></marker>
<rawString>7. In a syntactic graph, however, we call both modifier and specifier nodes modifier nodes.</rawString>
</citation>
<citation valid="false">
<title>From now on, we will use the terms node and word, as well as arc and triple,</title>
<pages>interchangeably.</pages>
<marker></marker>
<rawString>8. From now on, we will use the terms node and word, as well as arc and triple, interchangeably.</rawString>
</citation>
<citation valid="true">
<title>A unique index can be generated using the special function gensym which returns a unique symbol whenever it is called.</title>
<date>1989</date>
<journal>Computational Linguistics, Volume</journal>
<volume>15</volume>
<pages>31</pages>
<marker>1989</marker>
<rawString>9. A unique index can be generated using the special function gensym which returns a unique symbol whenever it is called. Computational Linguistics, Volume 15, Number 1, March 1989 31</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jungyun Seo</author>
<author>F Robert</author>
</authors>
<title>Simmons Syntactic Graphs: A Representation for the Union of All Ambiguous Parse Trees 10. Due to the complexity of the diagram, some of the details are omitted.</title>
<marker>Seo, Robert, </marker>
<rawString>Jungyun Seo and Robert F. Simmons Syntactic Graphs: A Representation for the Union of All Ambiguous Parse Trees 10. Due to the complexity of the diagram, some of the details are omitted.</rawString>
</citation>
<citation valid="false">
<title>Not all different readings of a syntactic graph have different root nodes. In this example, [0,time,v] is the root node of two different readings of the graph with simple grammar rules. The equivalent parse trees of the two readings are: [snt,[vp,[verb,[time]],[np,[np,[noun,[flies]]],[pp,[prep,[like]], [np,[det,[an]] ,[noun,[arrowl]]]]]] [snt,[vp,[vp,[verb,[time]],[np,[noun,[flies]] l],[pp,[prep,[likel], [np,[det,[an] ],[noun,[arrow</title>
<marker></marker>
<rawString>11. Not all different readings of a syntactic graph have different root nodes. In this example, [0,time,v] is the root node of two different readings of the graph with simple grammar rules. The equivalent parse trees of the two readings are: [snt,[vp,[verb,[time]],[np,[np,[noun,[flies]]],[pp,[prep,[like]], [np,[det,[an]] ,[noun,[arrowl]]]]]] [snt,[vp,[vp,[verb,[time]],[np,[noun,[flies]] l],[pp,[prep,[likel], [np,[det,[an] ],[noun,[arrow]]]]]]</rawString>
</citation>
<citation valid="false">
<title>The node in a forest is different from the node in a syntactic graph. A non-terminal node in a forest with two children nodes has one head-modifier relation, and hence the non-terminal with two children in a forest represents one arc in a syntactic graph.</title>
<marker></marker>
<rawString>12. The node in a forest is different from the node in a syntactic graph. A non-terminal node in a forest with two children nodes has one head-modifier relation, and hence the non-terminal with two children in a forest represents one arc in a syntactic graph.</rawString>
</citation>
<citation valid="false">
<title>We use the syntax of Quintus—Prolog version 2 on SUN systems. The special predicate, ( Cond —&gt; Then ; Else), in the algorithm can be interpreted as; if Cond is true, then call Then. Otherwise, call Else.</title>
<marker></marker>
<rawString>13. We use the syntax of Quintus—Prolog version 2 on SUN systems. The special predicate, ( Cond —&gt; Then ; Else), in the algorithm can be interpreted as; if Cond is true, then call Then. Otherwise, call Else.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>