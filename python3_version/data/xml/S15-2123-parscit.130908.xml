<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000054">
<bodyText confidence="0.404439428571429">
ds. By exteding e D od w e
UFRGS: Identifying Categories and Targets in Customer Reviews
t of pon wds h god n
general word is normally not an aspect, we next i that opinion words are usually associated
mming (ASP) – a v
Anderson Kauer
o implemnt ytacticl approach based
Institute of Informatics – UFRGS
plemenation of he D metho
Porto Alegre – RS – Brazil
ad s oy u h
aukauer@inf.ufrgs.br
about 510 lines of code The AS
fore from a given set o
</bodyText>
<note confidence="0.637849">
Viviane P. Moreira
Institute of Informatics – UFRGS
</note>
<bodyText confidence="0.744577285714286">
aetfpectintmsntacticrel
ti l hl tt t f
nd newopinio word from th
Porto Alegre – RS – Brazil
opagation process continues until
viviane@inf.ufrgs.br
nt the DP
</bodyText>
<sectionHeader confidence="0.9877" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998560608695652">
This paper reprts on our participation n
h reults i more ccurate aspect extracio
ral words and new nowedge can be naturall
SemEval-2015 Task 12, which was devoted
ing SP.
to Aspect-Based Sentiment Analysis. Partic-
ng of the paper is organized s follows: w
ipants were required to identify the category
(entity and attribute), the opinion target, and
und and related wok in Section II and a
r logic programming approach i Sction II
the polarity of customer reviews. The system
we built relies on classificain algorithms o
to implement the DP method for extractin
ar described in Sction IV Our new approac
identify aspect categories and on a set of rules
ng s presented in Secion V esent th
to identify the opinion target. We propose a
Section VI and conclude the paper and discus
two-phase classification approach for category
Section VII
identification and use a simple method for po-
larity detection. Our results outperform the
</bodyText>
<sectionHeader confidence="0.994911" genericHeader="keywords">
ACKGROUND ANDRELATED WORK
</sectionHeader>
<bodyText confidence="0.9485772">
baseline in many cases, which means our sys-
tem could be used as n alterntiv for aspect
n we introduce the basics of aspect extractio
classificatio.
Programmin
</bodyText>
<sectionHeader confidence="0.999469" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999541235294118">
Aspect Based Sentiment Analysis aims at discover-
ing the opinions or sentiments expressed by a user
on the different aspects of a given entity (Hu and
Liu, 2004; Liu, 2012). Recently, a number of meth-
ods and techniques have been developed to tackle
this task and some of them rely on syntactic depen-
dencies to locate the opinion target (Kim and Hovy,
2004; Qiu et al., 2011; Liu et al., 2013). A syntac-
tic parser takes a natural language sentence as input
and outputs the relationships between the words in
the sentence. Figure 1 shows the dependency tree
for the sentence “The phone has a good screen.” and
the grammatical relations of each token (det, subj,
mod, obj). We explore using grammatical relations
to help identify the opinion targets.
In this paper, we describe a system which took
part on SemEval-2015, and the way it was applied
</bodyText>
<figure confidence="0.602343">
p p
</figure>
<figureCaption confidence="0.999893">
Figure 1: Example of a dependency tree (Liu et al., 2013).
</figureCaption>
<bodyText confidence="0.996453157894737">
the d ithot dditil ds i thi dd
to category and polarity classification. Our system
method considers only direct dependcies as com
in ca mak th metod vulbl o prsi
pinion words are assmed to b adjecives and spects n
participated in all subtasks from Task 12 (Aspect
Based Sentiment Analysis). For more deils on this
noun phrases Thus the poential POS tags for opi
ord are JJ (adjctives), JJR (comparative djectve)
task, please refer to Pontiki et al. (2015). Our sys-
S (superlatve adjectves) whle those fo aspects are
tem combines classification algorithms, coreference
resolution tools, and a syntactic parser. One of our
ga nn) N (plal ) Th ep
goals was to minimize the use of external resources.
The remainder of this paper is organized as fol-
lows: Our system is described in Section 2. Sec-
tion 3 reports on the evaluation results. Finally, sec-
tion 4 concludes the paper.
</bodyText>
<sectionHeader confidence="0.711873" genericHeader="method">
2 Description of the System
</sectionHeader>
<bodyText confidence="0.9987685">
In this section, we describe the different components
of the system.
</bodyText>
<subsectionHeader confidence="0.999353">
2.1 Pre-processing
</subsectionHeader>
<bodyText confidence="0.999361375">
A distinctive characteristic of Web content is the
high prevalence of noise. This directly impacts
the quality of the results generated by a syntactic
parser. In our system, we used the StanfordNLP
Core toolkit (Manning et al., 2014).
The training sentences provided by the organizers
were sometimes composed by more than one sen-
tence. Thus, before submitting them to the parser,
</bodyText>
<page confidence="0.98477">
725
</page>
<bodyText confidence="0.891626909090909">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 725–729,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
a cleaning step based on regular expressions was
performed. In this step, we replaced all punctua-
tion marks by commas and removed non-alphabetic
characters.
Then, the standard pre-processing tools available
from the StanfordNLP Core were applied (tokeniza-
tion, sentence splitting, part-of-speech tagging, mor-
phological analysis, syntactic parsing, coreference
resolution, and sentiment analysis).
</bodyText>
<subsectionHeader confidence="0.999821">
2.2 Aspect Category Identification
</subsectionHeader>
<bodyText confidence="0.99672782">
We treated the problem of identifying aspect cate-
gories as a classification task. Thus, we made use
of the classifiers available from Weka (Hall et al.,
2009) to build models based on the training data.
In Task 12, categories are formed by a pair En-
tity#Attribute. The organizers have provided a list
of possible entities and, for each entity, a list of at-
tributes.
For each entity, we built a binary classifier where
each instance contains the lemmas on the sentence
and coreference lemmas to the previous sentences.
The class indicates whether the instance belongs to
the entity (i.e., positive means that the instance be-
longs to the entity and negative means it does not
belong to the entity). For each entity, the features
were selected using the InfoGainAttributeEval with
Ranker as a search method (available from Weka).
The threshold set up to Ranker was 0, which means
that the words selected by the method must con-
tribute to identify the class.
We used two approaches to classify the sen-
tences. In the first approach, one-phase classifi-
cation, for each entity dataset we trained six clas-
sifiers using all the sentences. These six clas-
sifiers (namely IBk, ThresholdSelector, Bayesian-
LogisticRegression, Logistic, MultiClassClassifier,
and SMO) were the top performers on our experi-
ments on the training data. We will refer to those
as Category classifiers, as they will be used to ac-
tually determine the class. Since the classifiers for
each category are independent, it is possible that a
sentence is predicted as belonging to more than one
category.
Classifiers were also built for each attribute be-
longing to that entity using only the sentences con-
taining the entity. We call these Attribute classifiers,
as they will be used to generate features for the Cat-
egory classifiers.
In the two-phase approach (Figure 2), first we
train n Attribute classifiers using all sentences but
the current. In the experiments reported in Section 3,
we used twenty Attribute classifiers (n=20). Then,
the outputs from each of the n Attribute classifiers
were used as features for the Category classifiers
(second phase). This phase requires significant pro-
cessing time since a new dataset is created for each
instance and the models have to be updated. This
method assumes that the features in each instance
contain “what the others tell about it” using differ-
ent prediction models.
</bodyText>
<figureCaption confidence="0.997352">
Figure 2: Two-phase classification pipeline.
</figureCaption>
<bodyText confidence="0.9999885">
To classify a new unseen instance, first it needs to
be processed so that its lemmas and coreferences are
identified. Then, word frequencies are selected and
the n Attribute classifiers generate the values of the
features for the second phase.
The final predicted class is the top scoring (i.e.,
with the highest sum of scores) obtained from the
results of the six Category classifiers. Although this
has not happened in our experiments, a tie between
the scores of the positive and negative classes is pos-
sible. In such a case, the sentence will be assigned to
the positive class (i.e., as belonging to the category).
</bodyText>
<subsectionHeader confidence="0.998122">
2.3 Opinion Target Identification
</subsectionHeader>
<bodyText confidence="0.999801">
The opinion target is detected after the category has
been identified. For each pair Entity#Attribute dis-
</bodyText>
<table confidence="0.611316833333333">
set of instances (sentences)
subset of current instance
instances
build attribute classification 1 to n
classifier 1 to n
current instance with n features
</table>
<page confidence="0.992941">
726
</page>
<bodyText confidence="0.999949916666666">
covered in the sentence, the candidate words are se-
lected in order of information gain for that category.
The words from attribute classification are concate-
nated with the words for entity classification. The
assumption is that the words from attribute classifi-
cation are more significant than the words from en-
tity classification (which are more generic).
We select the word pairs which are directly as-
sociated (on the dependency tree) by a grammati-
cal relation such as adjectival modifier, noun com-
pound modifier, and nominal subject. We consider
the opinion targets to be nouns/noun phrases as this
has been widely adopted in the related literature (Hu
and Liu, 2004; Qiu et al., 2011; Liu et al., 2013).
Thus, the potential POS tags for targets are NN (sin-
gular nouns) and NNS (plural nouns). In order to
identify incorrect targets, we rely on a list of 5k
words assembled by Qian (2013). This exceptions
list contains words with little or no meaning and that
normally are not an aspect. The main target is the
first candidate noun which is not in such a list.
If no nouns are found among the candidates, we
find the nouns in the same sentence that are indi-
rectly related to the candidate words (i.e. by transi-
tivity), then we select the first noun. When still no
nouns are found, then the opinion is set to NULL (it
does not exist in the sentence). Target expressions
are obtained using noun compound modifier (nn) as-
sociations.
A current limitation is that we do not identify mul-
tiple target expressions for the same category. We
assume that for each category found, there is only
one target in the sentence. However, since a sen-
tence may be assigned to several categories, in these
cases, more than one target may be identified and
returned.
</bodyText>
<subsectionHeader confidence="0.999278">
2.4 Sentiment Polarity Attribution
</subsectionHeader>
<bodyText confidence="0.999993">
For this subtask, we used a simple approach that as-
signs the polarity of the target as the general polarity
of the sentence. Stanford NLP Core provides senti-
ment analysis based on a compositional model over
trees using deep learning (Socher et al., 2013). The
nodes of a binarized tree of each sentence are as-
signed a sentiment score.
We opted for this approach to minimize the exter-
nal resources in the our system, such as sentiment
lexicons or reviews collected from other sources.
The underlying model for Stanford NLP Core Sen-
timent Analysis was built on a corpus consisting of
11,855 sentences extracted from movie reviews. We
have made no attempt to change the model to adapt
to our reviews and used it as is to determine the
polarity of the sentences. Our contribution in this
phase was just the benchmarking of an existing tool.
</bodyText>
<sectionHeader confidence="0.998724" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999969636363636">
We experimented with all three datasets from Task
12, namely Restaurants (Res), Laptops (Lap), and
Hidden (Hid) for which the domain was unknown.
Details on the datasets are in Pontiki et al. (2015).
The evaluation occurs in two phases. In the first
phase, participating systems were evaluated on cat-
egory detection for Restaurants and Laptops. Ad-
ditionally, identifying opinion target and the pair
(category, target) was requested for the Restau-
rants domain. In the second phase, the systems were
evaluated on polarity detection on all three domains.
</bodyText>
<subsectionHeader confidence="0.999895">
3.1 Opinion Category and Target Detection
</subsectionHeader>
<bodyText confidence="0.9994121">
When evaluating opinion category and target detec-
tion (first phase), three measures were taken into ac-
count: precision, recall, and F1. For both category
and target detection, the baseline methodologies are
presented in Pontiki et al. (2015). Table 1 shows the
results obtained using our approach compared to the
baseline for aspect category detection, whereas Ta-
ble 2 outlines the results regarding aspect target de-
tection. The results for the pair (category, target)
are presented in Table 3.
</bodyText>
<tableCaption confidence="0.998967">
Table 1: Opinion Category detection.
</tableCaption>
<table confidence="0.999644363636364">
Domain Method P R F1
Res 2Phase 0.6556 0.4323 0.5210
Res 1Phase 0.6835 0.4181 0.5188
Res Baseline 0.5133
Res 1Phase-coref 0.6821 0.4180 0.5184
Res 2Phase-coref 0.6509 0.4090 0.5023
Lap Baseline 0.4631
Lap 1Phase 0.5066 0.4040 0.4495
Lap 2Phase 0.4773 0.4209 0.4473
Lap 1Phase-coref 0.4834 0.4462 0.4640
Lap 2Phase-coref 0.4689 0.4388 0.4534
</table>
<bodyText confidence="0.999297666666667">
The system outperforms the baseline on both ap-
proaches for the Restaurants domain. In this do-
main, the two-phase approach was superior to the
</bodyText>
<page confidence="0.992732">
727
</page>
<bodyText confidence="0.999975555555556">
one-phase approach. For the laptop domain, how-
ever, we scored lower than the baseline. We attribute
that to the increased difficulty the coreference reso-
lution step had when processing the review texts in
this domain because of the large number of out of
vocabulary words (CPU, HD, RAM, etc). Table 1
shows that the results improve when the coreference
resolution step is not performed. Nevertheless, for
the Restaurant domain, it brought improvements.
</bodyText>
<subsectionHeader confidence="0.999823">
3.2 Opinion Polarity Detection
</subsectionHeader>
<bodyText confidence="0.999706333333333">
Table 4 shows the results in terms of accuracy
on opinion polarity. Here, the methodology for
the baseline is similar to the ones used for as-
pect category detection (also described in Pontiki et
al. (2015)). In this subtask, we submitted only the
results for the one-phase classification.
</bodyText>
<tableCaption confidence="0.995127">
Table 4: Opinion Polarity detection.
</tableCaption>
<figure confidence="0.996532619047619">
Domain
Res
Res
Lap
Lap
Hid
Hid
Method
1Phase
Baseline
1Phase
Baseline
Baseline
1Phase
Accuracy
0.7172
0.5373
0.6733
0.5701
0.7168
0.6578
</figure>
<tableCaption confidence="0.991068">
Table 2: Opinion Target detection.
</tableCaption>
<table confidence="0.999206833333333">
Domain Method P R F1
Res 2Phase 0.5656 0.4373 0.4932
Res 1Phase 0.5764 0.4244 0.4888
Res Baseline 0.4807
Res 2Phase-exc. 0.5632 0.4354 0.4911
Res 1Phase-exc. 0.5739 0.4225 0.4867
</table>
<bodyText confidence="0.903417333333333">
Considering the results for opinion target detec-
tion, both versions of our system outperformed the
baseline. The two-phase classification achieved bet-
ter recall in both category and target detection, but
worse precision compared to one-phase classifica-
tion.
We ran some additional experiments to evaluate
the use of the exceptions list during target identifi-
cation. These runs in which the exceptions list were
not used are labelled 1Phase-exc and 2Phase-exc in
Table 2. The results show that using such a list did
not impact the results.
</bodyText>
<tableCaption confidence="0.999542">
Table 3: Opinion Category and Target pair detection.
</tableCaption>
<table confidence="0.9996235">
Domain Method P R F1
Res 2Phase 0.4852 0.2722 0.3487
Res Baseline 0.3444
Res 1Phase 0.4521 0.2734 0.3407
Res 1Phase-coref 0.4694 0.2639 0.3378
Res 2Phase-coref 0.4496 0.2591 0.3288
</table>
<bodyText confidence="0.999977692307692">
As for the results for the pair (category, target)
the two-phase classification outperforms both the
baseline and the one-phase classification. The gain
in terms of precision is three percentage points,
while recall was slightly reduced. The best configu-
ration was using coreference resolution and the ex-
ceptions list.
The Stanford Core Toolkit uses a model trained
on movie reviews, and this was not the same do-
main of the datasets in the task. Still, the classifi-
cation results outperformed the baseline on Restau-
rants and Laptops. However, for the Hidden domain,
we scored lower than the baseline.
</bodyText>
<subsectionHeader confidence="0.979608">
3.3 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999985772727273">
The results obtained with our system are ranked be-
tween the 5th (out of 15) and the 14th (out of 22)
places. A case by case analysis was performed to
identify the most frequent causes of errors. In the
task of aspect category classification, the choice of
the threshold used during feature selection by the
Ranker (0) may have negatively impacted the re-
sults. Nevertheless, some feature selection method
is necessary since the use of all the words as features
greatly increases the processing time.
We used words selected by their Information Gain
as seeds to identify the target expression. In our ex-
periments, in many cases, the target was next to the
words selected by this strategy. This happens be-
cause the positive class had fewer instances than
the negative class, and the Information Gain tends
to select words that characterize the least frequent
class. However, most classification errors happened
because this strategy failed to identify infrequent
words that corresponded to the expected categories.
One possible alternative to mitigate this problem
could be the use of synonyms.
</bodyText>
<page confidence="0.992133">
728
</page>
<bodyText confidence="0.999985285714286">
The method we used for polarity detection con-
sidered the entire sentence. The limitation here is
that many sentences contain more than one opinion,
which may not convey the same polarity. This could
be solved by identifying the context (i.e., a region
around the target) and limit the polarity attribution
to that region.
</bodyText>
<sectionHeader confidence="0.999267" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999951333333333">
This paper reports on the experiments that we con-
ducted while taking part on SemEval-2015 Task 12.
We showed that classification algorithms, corefer-
ence resolution tools, and a syntactic parser may
be combined in a category/target detection sys-
tem.We employed a two-phase approach to classify
instances. Our results show that this approach can
be an alternative to classify sentences without us-
ing lexicons, improving recall with a small decay in
precision. As future work, we plan to improve the
coreference resolution of review texts so as to fur-
ther improve recall.
</bodyText>
<sectionHeader confidence="0.99821" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9929622">
This work has been partially funded by CNPq-Brazil
project 478979/2012-6. Anderson Kauer receives a
scholarship from CNPq-Brazil. We would like to
thank the anonymous reviewers for their helpful sug-
gestions and comments.
</bodyText>
<sectionHeader confidence="0.998728" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999333744680851">
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10–18, November.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 168–177,
New York, NY, USA. ACM.
Soo-Min Kim and Eduard Hovy. 2004. Determining the
sentiment of opinions. In Proceedings of the 20th In-
ternational Conference on Computational Linguistics,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Qian Liu, Zhiqiang Gao, Bing Liu, and Yuanlin Zhang.
2013. A logic programming approach to aspect
extraction in opinion mining. In Web Intelligence
(WI) and Intelligent Agent Technologies (IAT), 2013
IEEE/WIC/ACM International Joint Conferences on,
volume 1, pages 276–283, Nov.
Bing Liu. 2012. Sentiment analysis and opinion mining.
Synthesis Lectures on Human Language Technologies,
5(1):1–167.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David McClosky.
2014. The Stanford CoreNLP natural language pro-
cessing toolkit. In Proceedings of 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics: System Demonstrations, pages 55–60, Baltimore,
Maryland. Association for Computational Linguistics.
Maria Pontiki, Dimitrios Galanis, Haris Papageogiou,
Suresh Manandhar, and Ion Androutsopoulos. 2015.
Semeval-2015 task 12: Aspect based sentiment analy-
sis. In Proceedings of the 9th International Workshop
on Semantic Evaluation (SemEval 2015), Denver, Col-
orado.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2011. Opinion word expansion and target extraction
through double propagation. Computational Linguis-
tics, 37(1):9–27.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng, and
Christopher Potts. 2013. Recursive deep models for
semantic compositionality over a sentiment treebank.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP), pages
1631–1642.
</reference>
<page confidence="0.998522">
729
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.226897">
<title confidence="0.6840875">e D od w e UFRGS: Identifying Categories and Targets in Customer Reviews</title>
<abstract confidence="0.971133458333333">of wds h n general word is normally not an aspect, we next i that opinion words are usually associated mming (ASP) – a Anderson o implemnt ytacticl approach – plemenation of he D metho Porto Alegre – RS – Brazil ad s oy u h about 510 lines of code The AS fore from a given set Viviane P. Moreira Institute of Informatics – UFRGS aetfpectintmsntacticrel ti l hl tt t nd newopinio word from th Porto Alegre – RS – Brazil opagation process continues viviane@inf.ufrgs.br nt the DP Abstract This paper reprts on our participation n reults i ccurate aspect ral words and new nowedge can be naturall which was devoted ing SP. Sentiment Analysis. Particng of the paper is organized s follows: w ipants were required to identify the category (entity and attribute), the opinion target, and und and related wok in Section II and a r logic programming approach i Sction II polarity of customer reviews. we built relies on classificain algorithms o implement the DP for ar described in Sction IV Our new approac aspect and on a set of rules ng s presented in Secion V esent the opinion target. a Section VI and conclude the paper and discus two-phase classification approach for category Section VII identification and use a simple method for polarity detection. Our results outperform the baseline in many cases, which means our system could be used as n alterntiv for aspect we introduce the of extractio classificatio.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA data mining software: an update.</title>
<date>2009</date>
<journal>SIGKDD Explor. Newsl.,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="4840" citStr="Hall et al., 2009" startWordPosition="804" endWordPosition="807">r Computational Linguistics a cleaning step based on regular expressions was performed. In this step, we replaced all punctuation marks by commas and removed non-alphabetic characters. Then, the standard pre-processing tools available from the StanfordNLP Core were applied (tokenization, sentence splitting, part-of-speech tagging, morphological analysis, syntactic parsing, coreference resolution, and sentiment analysis). 2.2 Aspect Category Identification We treated the problem of identifying aspect categories as a classification task. Thus, we made use of the classifiers available from Weka (Hall et al., 2009) to build models based on the training data. In Task 12, categories are formed by a pair Entity#Attribute. The organizers have provided a list of possible entities and, for each entity, a list of attributes. For each entity, we built a binary classifier where each instance contains the lemmas on the sentence and coreference lemmas to the previous sentences. The class indicates whether the instance belongs to the entity (i.e., positive means that the instance belongs to the entity and negative means it does not belong to the entity). For each entity, the features were selected using the InfoGai</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA data mining software: an update. SIGKDD Explor. Newsl., 11(1):10–18, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>168--177</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1948" citStr="Hu and Liu, 2004" startWordPosition="334" endWordPosition="337">Secion V esent th to identify the opinion target. We propose a Section VI and conclude the paper and discus two-phase classification approach for category Section VII identification and use a simple method for polarity detection. Our results outperform the ACKGROUND ANDRELATED WORK baseline in many cases, which means our system could be used as n alterntiv for aspect n we introduce the basics of aspect extractio classificatio. Programmin 1 Introduction Aspect Based Sentiment Analysis aims at discovering the opinions or sentiments expressed by a user on the different aspects of a given entity (Hu and Liu, 2004; Liu, 2012). Recently, a number of methods and techniques have been developed to tackle this task and some of them rely on syntactic dependencies to locate the opinion target (Kim and Hovy, 2004; Qiu et al., 2011; Liu et al., 2013). A syntactic parser takes a natural language sentence as input and outputs the relationships between the words in the sentence. Figure 1 shows the dependency tree for the sentence “The phone has a good screen.” and the grammatical relations of each token (det, subj, mod, obj). We explore using grammatical relations to help identify the opinion targets. In this pape</context>
<context position="8691" citStr="Hu and Liu, 2004" startWordPosition="1432" endWordPosition="1435">cted in order of information gain for that category. The words from attribute classification are concatenated with the words for entity classification. The assumption is that the words from attribute classification are more significant than the words from entity classification (which are more generic). We select the word pairs which are directly associated (on the dependency tree) by a grammatical relation such as adjectival modifier, noun compound modifier, and nominal subject. We consider the opinion targets to be nouns/noun phrases as this has been widely adopted in the related literature (Hu and Liu, 2004; Qiu et al., 2011; Liu et al., 2013). Thus, the potential POS tags for targets are NN (singular nouns) and NNS (plural nouns). In order to identify incorrect targets, we rely on a list of 5k words assembled by Qian (2013). This exceptions list contains words with little or no meaning and that normally are not an aspect. The main target is the first candidate noun which is not in such a list. If no nouns are found among the candidates, we find the nouns in the same sentence that are indirectly related to the candidate words (i.e. by transitivity), then we select the first noun. When still no n</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 168–177, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Determining the sentiment of opinions.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2143" citStr="Kim and Hovy, 2004" startWordPosition="369" endWordPosition="372">imple method for polarity detection. Our results outperform the ACKGROUND ANDRELATED WORK baseline in many cases, which means our system could be used as n alterntiv for aspect n we introduce the basics of aspect extractio classificatio. Programmin 1 Introduction Aspect Based Sentiment Analysis aims at discovering the opinions or sentiments expressed by a user on the different aspects of a given entity (Hu and Liu, 2004; Liu, 2012). Recently, a number of methods and techniques have been developed to tackle this task and some of them rely on syntactic dependencies to locate the opinion target (Kim and Hovy, 2004; Qiu et al., 2011; Liu et al., 2013). A syntactic parser takes a natural language sentence as input and outputs the relationships between the words in the sentence. Figure 1 shows the dependency tree for the sentence “The phone has a good screen.” and the grammatical relations of each token (det, subj, mod, obj). We explore using grammatical relations to help identify the opinion targets. In this paper, we describe a system which took part on SemEval-2015, and the way it was applied p p Figure 1: Example of a dependency tree (Liu et al., 2013). the d ithot dditil ds i thi dd to category and p</context>
</contexts>
<marker>Kim, Hovy, 2004</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2004. Determining the sentiment of opinions. In Proceedings of the 20th International Conference on Computational Linguistics, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qian Liu</author>
<author>Zhiqiang Gao</author>
<author>Bing Liu</author>
<author>Yuanlin Zhang</author>
</authors>
<title>A logic programming approach to aspect extraction in opinion mining.</title>
<date>2013</date>
<booktitle>In Web Intelligence (WI) and Intelligent Agent Technologies (IAT), 2013 IEEE/WIC/ACM International Joint Conferences on,</booktitle>
<volume>1</volume>
<pages>276--283</pages>
<contexts>
<context position="2180" citStr="Liu et al., 2013" startWordPosition="377" endWordPosition="380">ur results outperform the ACKGROUND ANDRELATED WORK baseline in many cases, which means our system could be used as n alterntiv for aspect n we introduce the basics of aspect extractio classificatio. Programmin 1 Introduction Aspect Based Sentiment Analysis aims at discovering the opinions or sentiments expressed by a user on the different aspects of a given entity (Hu and Liu, 2004; Liu, 2012). Recently, a number of methods and techniques have been developed to tackle this task and some of them rely on syntactic dependencies to locate the opinion target (Kim and Hovy, 2004; Qiu et al., 2011; Liu et al., 2013). A syntactic parser takes a natural language sentence as input and outputs the relationships between the words in the sentence. Figure 1 shows the dependency tree for the sentence “The phone has a good screen.” and the grammatical relations of each token (det, subj, mod, obj). We explore using grammatical relations to help identify the opinion targets. In this paper, we describe a system which took part on SemEval-2015, and the way it was applied p p Figure 1: Example of a dependency tree (Liu et al., 2013). the d ithot dditil ds i thi dd to category and polarity classification. Our system me</context>
<context position="8728" citStr="Liu et al., 2013" startWordPosition="1440" endWordPosition="1443">r that category. The words from attribute classification are concatenated with the words for entity classification. The assumption is that the words from attribute classification are more significant than the words from entity classification (which are more generic). We select the word pairs which are directly associated (on the dependency tree) by a grammatical relation such as adjectival modifier, noun compound modifier, and nominal subject. We consider the opinion targets to be nouns/noun phrases as this has been widely adopted in the related literature (Hu and Liu, 2004; Qiu et al., 2011; Liu et al., 2013). Thus, the potential POS tags for targets are NN (singular nouns) and NNS (plural nouns). In order to identify incorrect targets, we rely on a list of 5k words assembled by Qian (2013). This exceptions list contains words with little or no meaning and that normally are not an aspect. The main target is the first candidate noun which is not in such a list. If no nouns are found among the candidates, we find the nouns in the same sentence that are indirectly related to the candidate words (i.e. by transitivity), then we select the first noun. When still no nouns are found, then the opinion is s</context>
</contexts>
<marker>Liu, Gao, Liu, Zhang, 2013</marker>
<rawString>Qian Liu, Zhiqiang Gao, Bing Liu, and Yuanlin Zhang. 2013. A logic programming approach to aspect extraction in opinion mining. In Web Intelligence (WI) and Intelligent Agent Technologies (IAT), 2013 IEEE/WIC/ACM International Joint Conferences on, volume 1, pages 276–283, Nov.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment analysis and opinion mining.</title>
<date>2012</date>
<journal>Synthesis Lectures on Human Language Technologies,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="1960" citStr="Liu, 2012" startWordPosition="338" endWordPosition="339">to identify the opinion target. We propose a Section VI and conclude the paper and discus two-phase classification approach for category Section VII identification and use a simple method for polarity detection. Our results outperform the ACKGROUND ANDRELATED WORK baseline in many cases, which means our system could be used as n alterntiv for aspect n we introduce the basics of aspect extractio classificatio. Programmin 1 Introduction Aspect Based Sentiment Analysis aims at discovering the opinions or sentiments expressed by a user on the different aspects of a given entity (Hu and Liu, 2004; Liu, 2012). Recently, a number of methods and techniques have been developed to tackle this task and some of them rely on syntactic dependencies to locate the opinion target (Kim and Hovy, 2004; Qiu et al., 2011; Liu et al., 2013). A syntactic parser takes a natural language sentence as input and outputs the relationships between the words in the sentence. Figure 1 shows the dependency tree for the sentence “The phone has a good screen.” and the grammatical relations of each token (det, subj, mod, obj). We explore using grammatical relations to help identify the opinion targets. In this paper, we descri</context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>Bing Liu. 2012. Sentiment analysis and opinion mining. Synthesis Lectures on Human Language Technologies, 5(1):1–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>55--60</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland.</location>
<contexts>
<context position="3916" citStr="Manning et al., 2014" startWordPosition="674" endWordPosition="677"> parser. One of our ga nn) N (plal ) Th ep goals was to minimize the use of external resources. The remainder of this paper is organized as follows: Our system is described in Section 2. Section 3 reports on the evaluation results. Finally, section 4 concludes the paper. 2 Description of the System In this section, we describe the different components of the system. 2.1 Pre-processing A distinctive characteristic of Web content is the high prevalence of noise. This directly impacts the quality of the results generated by a syntactic parser. In our system, we used the StanfordNLP Core toolkit (Manning et al., 2014). The training sentences provided by the organizers were sometimes composed by more than one sentence. Thus, before submitting them to the parser, 725 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 725–729, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics a cleaning step based on regular expressions was performed. In this step, we replaced all punctuation marks by commas and removed non-alphabetic characters. Then, the standard pre-processing tools available from the StanfordNLP Core were applied (tokenization, sente</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55–60, Baltimore, Maryland. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Pontiki</author>
<author>Dimitrios Galanis</author>
<author>Haris Papageogiou</author>
<author>Suresh Manandhar</author>
<author>Ion Androutsopoulos</author>
</authors>
<title>Semeval-2015 task 12: Aspect based sentiment analysis.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015),</booktitle>
<location>Denver, Colorado.</location>
<contexts>
<context position="3150" citStr="Pontiki et al. (2015)" startWordPosition="546" endWordPosition="549">rgets. In this paper, we describe a system which took part on SemEval-2015, and the way it was applied p p Figure 1: Example of a dependency tree (Liu et al., 2013). the d ithot dditil ds i thi dd to category and polarity classification. Our system method considers only direct dependcies as com in ca mak th metod vulbl o prsi pinion words are assmed to b adjecives and spects n participated in all subtasks from Task 12 (Aspect Based Sentiment Analysis). For more deils on this noun phrases Thus the poential POS tags for opi ord are JJ (adjctives), JJR (comparative djectve) task, please refer to Pontiki et al. (2015). Our sysS (superlatve adjectves) whle those fo aspects are tem combines classification algorithms, coreference resolution tools, and a syntactic parser. One of our ga nn) N (plal ) Th ep goals was to minimize the use of external resources. The remainder of this paper is organized as follows: Our system is described in Section 2. Section 3 reports on the evaluation results. Finally, section 4 concludes the paper. 2 Description of the System In this section, we describe the different components of the system. 2.1 Pre-processing A distinctive characteristic of Web content is the high prevalence </context>
<context position="10854" citStr="Pontiki et al. (2015)" startWordPosition="1812" endWordPosition="1815">timent lexicons or reviews collected from other sources. The underlying model for Stanford NLP Core Sentiment Analysis was built on a corpus consisting of 11,855 sentences extracted from movie reviews. We have made no attempt to change the model to adapt to our reviews and used it as is to determine the polarity of the sentences. Our contribution in this phase was just the benchmarking of an existing tool. 3 Evaluation We experimented with all three datasets from Task 12, namely Restaurants (Res), Laptops (Lap), and Hidden (Hid) for which the domain was unknown. Details on the datasets are in Pontiki et al. (2015). The evaluation occurs in two phases. In the first phase, participating systems were evaluated on category detection for Restaurants and Laptops. Additionally, identifying opinion target and the pair (category, target) was requested for the Restaurants domain. In the second phase, the systems were evaluated on polarity detection on all three domains. 3.1 Opinion Category and Target Detection When evaluating opinion category and target detection (first phase), three measures were taken into account: precision, recall, and F1. For both category and target detection, the baseline methodologies a</context>
<context position="12958" citStr="Pontiki et al. (2015)" startWordPosition="2143" endWordPosition="2146">than the baseline. We attribute that to the increased difficulty the coreference resolution step had when processing the review texts in this domain because of the large number of out of vocabulary words (CPU, HD, RAM, etc). Table 1 shows that the results improve when the coreference resolution step is not performed. Nevertheless, for the Restaurant domain, it brought improvements. 3.2 Opinion Polarity Detection Table 4 shows the results in terms of accuracy on opinion polarity. Here, the methodology for the baseline is similar to the ones used for aspect category detection (also described in Pontiki et al. (2015)). In this subtask, we submitted only the results for the one-phase classification. Table 4: Opinion Polarity detection. Domain Res Res Lap Lap Hid Hid Method 1Phase Baseline 1Phase Baseline Baseline 1Phase Accuracy 0.7172 0.5373 0.6733 0.5701 0.7168 0.6578 Table 2: Opinion Target detection. Domain Method P R F1 Res 2Phase 0.5656 0.4373 0.4932 Res 1Phase 0.5764 0.4244 0.4888 Res Baseline 0.4807 Res 2Phase-exc. 0.5632 0.4354 0.4911 Res 1Phase-exc. 0.5739 0.4225 0.4867 Considering the results for opinion target detection, both versions of our system outperformed the baseline. The two-phase class</context>
</contexts>
<marker>Pontiki, Galanis, Papageogiou, Manandhar, Androutsopoulos, 2015</marker>
<rawString>Maria Pontiki, Dimitrios Galanis, Haris Papageogiou, Suresh Manandhar, and Ion Androutsopoulos. 2015. Semeval-2015 task 12: Aspect based sentiment analysis. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), Denver, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guang Qiu</author>
<author>Bing Liu</author>
<author>Jiajun Bu</author>
<author>Chun Chen</author>
</authors>
<title>Opinion word expansion and target extraction through double propagation.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="2161" citStr="Qiu et al., 2011" startWordPosition="373" endWordPosition="376">arity detection. Our results outperform the ACKGROUND ANDRELATED WORK baseline in many cases, which means our system could be used as n alterntiv for aspect n we introduce the basics of aspect extractio classificatio. Programmin 1 Introduction Aspect Based Sentiment Analysis aims at discovering the opinions or sentiments expressed by a user on the different aspects of a given entity (Hu and Liu, 2004; Liu, 2012). Recently, a number of methods and techniques have been developed to tackle this task and some of them rely on syntactic dependencies to locate the opinion target (Kim and Hovy, 2004; Qiu et al., 2011; Liu et al., 2013). A syntactic parser takes a natural language sentence as input and outputs the relationships between the words in the sentence. Figure 1 shows the dependency tree for the sentence “The phone has a good screen.” and the grammatical relations of each token (det, subj, mod, obj). We explore using grammatical relations to help identify the opinion targets. In this paper, we describe a system which took part on SemEval-2015, and the way it was applied p p Figure 1: Example of a dependency tree (Liu et al., 2013). the d ithot dditil ds i thi dd to category and polarity classifica</context>
<context position="8709" citStr="Qiu et al., 2011" startWordPosition="1436" endWordPosition="1439">nformation gain for that category. The words from attribute classification are concatenated with the words for entity classification. The assumption is that the words from attribute classification are more significant than the words from entity classification (which are more generic). We select the word pairs which are directly associated (on the dependency tree) by a grammatical relation such as adjectival modifier, noun compound modifier, and nominal subject. We consider the opinion targets to be nouns/noun phrases as this has been widely adopted in the related literature (Hu and Liu, 2004; Qiu et al., 2011; Liu et al., 2013). Thus, the potential POS tags for targets are NN (singular nouns) and NNS (plural nouns). In order to identify incorrect targets, we rely on a list of 5k words assembled by Qian (2013). This exceptions list contains words with little or no meaning and that normally are not an aspect. The main target is the first candidate noun which is not in such a list. If no nouns are found among the candidates, we find the nouns in the same sentence that are indirectly related to the candidate words (i.e. by transitivity), then we select the first noun. When still no nouns are found, th</context>
</contexts>
<marker>Qiu, Liu, Bu, Chen, 2011</marker>
<rawString>Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2011. Opinion word expansion and target extraction through double propagation. Computational Linguistics, 37(1):9–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1631--1642</pages>
<contexts>
<context position="10060" citStr="Socher et al., 2013" startWordPosition="1675" endWordPosition="1678">) associations. A current limitation is that we do not identify multiple target expressions for the same category. We assume that for each category found, there is only one target in the sentence. However, since a sentence may be assigned to several categories, in these cases, more than one target may be identified and returned. 2.4 Sentiment Polarity Attribution For this subtask, we used a simple approach that assigns the polarity of the target as the general polarity of the sentence. Stanford NLP Core provides sentiment analysis based on a compositional model over trees using deep learning (Socher et al., 2013). The nodes of a binarized tree of each sentence are assigned a sentiment score. We opted for this approach to minimize the external resources in the our system, such as sentiment lexicons or reviews collected from other sources. The underlying model for Stanford NLP Core Sentiment Analysis was built on a corpus consisting of 11,855 sentences extracted from movie reviews. We have made no attempt to change the model to adapt to our reviews and used it as is to determine the polarity of the sentences. Our contribution in this phase was just the benchmarking of an existing tool. 3 Evaluation We e</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1631–1642.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>