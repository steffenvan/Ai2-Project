<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.020016">
<title confidence="0.990555">
Finding middle ground? Multi-objective Natural Language Generation
from time-series data
</title>
<author confidence="0.989626">
Dimitra Gkatzia, Helen Hastie, and Oliver Lemon
</author>
<affiliation confidence="0.973554">
School of Mathematical and Computer Sciences, Heriot-Watt University, Edinburgh
</affiliation>
<email confidence="0.987696">
{dg106, h.hastie, o.lemon}@hw.ac.uk
</email>
<sectionHeader confidence="0.993648" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999841454545455">
A Natural Language Generation (NLG)
system is able to generate text from non-
linguistic data, ideally personalising the
content to a user’s specific needs. In some
cases, however, there are multiple stake-
holders with their own individual goals,
needs and preferences. In this paper, we
explore the feasibility of combining the
preferences of two different user groups,
lecturers and students, when generating
summaries in the context of student feed-
back generation. The preferences of each
user group are modelled as a multivariate
optimisation function, therefore the task
of generation is seen as a multi-objective
(MO) optimisation task, where the two
functions are combined into one. This ini-
tial study shows that treating the prefer-
ences of each user group equally smooths
the weights of the MO function, in a way
that preferred content of the user groups is
not presented in the generated summary.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999935385964913">
Summarisation of time-series data refers to the
task of automatically generating summaries from
attributes whose values change over time. Content
selection is the task of choosing what to say, i.e.
what information to be included in a report (Re-
iter and Dale, 2000). Here, we consider the task
of automatically generating feedback summaries
for students describing their performance during
the lab of a computer science module over the
semester. This work is motivated by the fact that
different user groups have different preferences of
the content that should be conveyed in a summary,
as shown by Gkatzia et al. (2013).
Various factors can influence students’ learning,
such as difficulty of the material (Person et al.,
1995), workload (Craig et al., 2004), attendance
in lectures (Ames, 1992) etc. These factors change
over time and can be interdependent. The different
stakeholders (i.e. lecturers and students) have dif-
ferent perceptions regarding what constitutes good
feedback. Therefore, when generating feedback,
we should take into account all preferences in or-
der to be able to produce feedback summaries that
are acceptable by both user groups.
Stakeholders often have conflicting goals, needs
and preferences, for example managers with em-
ployees or doctors with patients and relatives. In
our data, for instance, lecturers tend to comment
on the hours that a student studied, whereas the
students disprefer this content. Generating the
same summary for both groups allows for mean-
ingful further discussion with common ground.
Previous work on NLG systems that address
more than one user group use different versions of
a system for each different user group (Gatt et al.,
2009) or make use of User Models (Janarthanam
and Lemon, 2010; Thompson et al., 2004; Zuk-
erman and Litman, 2001). Here, we explore a
method that adapts to both expert preferences and
users simultaneously (i.e. lecturer and students
preferences), by applying Multi-Objective opti-
misation (MOO). MOO can be applied to situa-
tions where optimal decisions are sought in the
presence of trade-offs between conflicting objec-
tives (Chankong and Haimes, 1983). We explore
whether balancing the preferences of two user
groups can result in an adaptive system that is ac-
ceptable by all users. At the same time, the pro-
gramming effort is reduced as only one system
needs to be developed. Moreover, by pooling all
available data together, there is less need for an
extensive data collection.
In the next section, we present three systems:
one tuned for lecturers, one for students, and one
that attempts to find middle ground. In Section 3,
we describe an evaluation of these three systems
and in Section 4 we discuss the results. Finally, in
</bodyText>
<page confidence="0.979553">
210
</page>
<note confidence="0.7105425">
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 210–214,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.915346">
Section 5, directions for future work are discussed.
</bodyText>
<sectionHeader confidence="0.955726" genericHeader="introduction">
2 Methodology
</sectionHeader>
<bodyText confidence="0.999940363636364">
Reinforcement Learning (RL) is a machine learn-
ing technique that defines how an agent learns
to take optimal sequences of actions so as to
maximize a cumulative reward (Sutton and Barto,
1998). Here we extend the framework proposed
by Gkatzia et al. (2013) whereby the content selec-
tion is seen as a Markov Decision problem and the
goal of the agent is to learn to take the sequence
of actions that leads to optimal content selection.
A Temporal Difference learning method (Sutton
and Barto, 1998) was used to train an agent for
content selection. Firstly, we will describe the
data in general. Secondly, we refer to the RL
system that adapts to lecturers’ preferences as de-
scribed by Gkatzia et al. (2013). Thirdly, we will
describe how we collected data and developed a
methodology that adapts to students’ preferences
and finally how we combined the knowledge of
both steps to develop an MO system. The three
systems (Lecturer-adapted, Student-adapted, MO)
share the same architecture but the difference lies
in the reward functions used for training.
</bodyText>
<subsectionHeader confidence="0.996811">
2.1 The Data
</subsectionHeader>
<bodyText confidence="0.999993263157895">
For this study, the dataset described by Gkatzia
et al. (2013) was used. Table 1 shows an exam-
ple of this dataset that describes a student’s learn-
ing habits and a corresponding feedback summary
provided by a lecturer. The dataset is composed
of 37 similar instances. Each instance consists of
time-series information about the student’s learn-
ing routine and the selected templates that lectur-
ers used to provide feedback to this student. A
template is a quadruple consisting of an id, a fac-
tor (Table 1), a reference type (trend, weeks, aver-
age, other) and surface text. For instance, a tem-
plate can be (1, marks, trend, ‘Your marks were
&lt;trend&gt;over the semester’). The lexical choice
for &lt;trend&gt;(i.e. increasing or decreasing) de-
pends on the values of time-series data. There
is a direct mapping between the values of factor
and reference type and the surface text. The time-
series attributes are listed in Table 1 (bottom left).
</bodyText>
<subsectionHeader confidence="0.996619">
2.2 Time-series summarisation systems
</subsectionHeader>
<bodyText confidence="0.999124333333333">
Actions and states: The state consists of the time-
series data and the selected templates. In order to
explore the state space the agent selects a time-
series attribute (e.g. marks, deadlines etc.) and
then decides whether to talk about it or not. The
states and actions are similar for all systems.
</bodyText>
<subsectionHeader confidence="0.891331">
Lecturer-adapted reward function
</subsectionHeader>
<bodyText confidence="0.999575666666667">
The reward function is derived from analysis with
linear regression of the provided dataset and is the
following cumulative multivariate function:
</bodyText>
<equation confidence="0.992235666666667">
n
RewardLECT = a + bi * xi + c * length
i=1
</equation>
<bodyText confidence="0.9885379">
where X = {x1, x2, ..., xn} is the vector of
combinations of the data trends observed in the
time-series data and a particular reference type of
the factor. The value of xi is given by the function:
{ 1, if the combination of a factor trend
and a particular reference type is
included in the feedback
0, if not.
The coefficients represent the preference level of
a factor to be selected and how to be conveyed
in the summary. Important factors are associated
with high positive coefficients and the unimpor-
tant ones with negative coefficients. In the train-
ing phase, the agent selects a factor and then de-
cides whether to talk about it or not. If it decides
to refer to a factor, the selection of the template is
performed deterministically, i.e. it selects the tem-
plate that results in higher reward. Length rep-
resents the number of factors selected for gener-
ation.
</bodyText>
<subsectionHeader confidence="0.84884">
Student-adapted reward function
</subsectionHeader>
<bodyText confidence="0.997149111111111">
The Student-adapted system uses the same RL al-
gorithm as the Lecturer-adapted one. The differ-
ence lies in the reward function. The reward func-
tion used for training is of a similar style as the
Lecturer-adapted reward function. This function
was derived by manipulating the student ratings in
a previous experiment and estimating the weights
using linear regression in a similar way as Walker
et al. (1997) and Rieser et al. (2010).
</bodyText>
<subsectionHeader confidence="0.659984">
Multi-objective function
</subsectionHeader>
<bodyText confidence="0.998196333333333">
The function used for the multi-objective method
is derived by weighting the sum of the individual
reward functions.
</bodyText>
<equation confidence="0.720423">
RmO = 0.5 * RLECT + 0.5 * RSTUDENT
</equation>
<bodyText confidence="0.999853">
To reduce the confounding variables, we kept
the ordering of content in all systems the same.
</bodyText>
<sectionHeader confidence="0.999267" genericHeader="method">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.998287">
The output of the above-mentioned three systems
were evaluated both in simulation and with real
</bodyText>
<equation confidence="0.912326">
xi =
</equation>
<page confidence="0.979887">
211
</page>
<table confidence="0.911535428571429">
factors week 2 week 3 ... week 10
marks 5 4 ... 5
hours studied 1 2 ... 3
... ... ... ... ...
Raw Data
Trends from Data
factors factor trend
</table>
<listItem confidence="0.999485">
(1) marks trend other
(2) hours studied trend increasing
(3) understandability trend decreasing
(4) difficulty trend decreasing
(5) deadlines trend increasing
(6) health issues trend other
(7) personal issues trend decreasing
(8) lectures attended trend other
(9) revision trend decreasing
</listItem>
<sectionHeader confidence="0.510176" genericHeader="method">
Summary
</sectionHeader>
<bodyText confidence="0.998008866666667">
Your overall performance was excellent
during the semester. Keep up the good
work and maybe try some more challeng-
ing exercises. Your attendance was vary-
ing over the semester. Have a think about
how to use time in lectures to improve your
understanding of the material. You spent 2
hours studying the lecture material on
average. You should dedicate more time
to study. You seem to find the material
easier to understand compared to the
beginning of the semester. Keep up the
good work! You revised part of the learn-
ing material. Have a think about whether
revising has improved your performance.
</bodyText>
<tableCaption confidence="0.8575775">
Table 1: Top left: example of the time-series raw data for feedback generation. Bottom left: example of
described trends. Right box: a target summary generated by an expert (bold signifies the chosen content).
</tableCaption>
<bodyText confidence="0.896727">
users. Example summaries of all systems are pre-
sented in Table 2.
</bodyText>
<subsectionHeader confidence="0.997484">
3.1 Evaluation in Simulation
</subsectionHeader>
<bodyText confidence="0.999984772727273">
26 summaries were produced by each system. The
output of each system was evaluated with the three
reward functions. Table 3 shows the results.
As expected, all systems score highly when
evaluated with the reward function for which
they were trained, with the second highest reward
scored from the MO function. Table 2 illustrates
this with the MO Policy clearly between the other
two policies. Moreover, the MO function reduces
the variability between summaries as is also re-
flected in the standard deviation given in Table 3.
We used BLEU (4-grams) (Papineni et al.,
2002) to measure the similarities between the
feedback summaries generated by the three sys-
tems. BLEU score is between 0-1 with values
closer to 1 indicating texts are more similar. Our
results demonstrate that the summaries generated
by the three systems are quite different (BLEU
score between 0.33 and 0.36). This shows that the
framework presented here is capable of producing
quite different summaries based on the various re-
ward functions.
</bodyText>
<subsectionHeader confidence="0.9994">
3.2 Evaluation with real users
</subsectionHeader>
<bodyText confidence="0.99996">
The goal of the evaluation is to determine whether
the end-user can pick up on the above-mentioned
differences in the feedback and rank them accord-
ing to their preferences. The output of the three
systems was ranked by 19 lecturers and 48 first-
year Computer Science students. Time-series data
of three students were presented on graphs to each
participant. They were also shown 3 feedback
summaries and they were asked to rank them in
terms of preference.
As we can see from Table 4, the two user groups
significantly preferred the output of the system
which was trained for their preferences (Mann-
Whitney U test, p &lt; 0.05). Interestingly, lecturers
found both the outputs produced by the Lecturer-
adapted system and the Student-adapted system
significantly preferable (p &lt; 0.05) to the output
produced by the MO system. In contrast, students
significantly preferred the output generated by the
Student-adapted system over the other two. Fi-
nally, both user groups rated the MO system 3rd,
but there is not a significant difference between
the student ratings for the MO system and the
Lecturer-adapted system.
</bodyText>
<sectionHeader confidence="0.999117" genericHeader="evaluation">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999939333333333">
It is interesting to examine the weights derived
from the multiple-linear regression to determine
the preferences of the different user groups. For
instance, lecturers’ most preferred content is
hours studied, therefore the reward function gives
high scores to summaries that mention the hours
</bodyText>
<page confidence="0.964608">
212
</page>
<bodyText confidence="0.991712666666667">
Lecturer-adapted Student-adapted Multi-objective
Make sure you revise the learning You found the lab exercises very Your attendance was varying over the
material and try to do the lab ex- challenging. Make sure that you semester. Have a think about how to
ercises again. You dedicated more have understood the taught material use time in lectures to improve your un-
time studying the lecture material in and don’t hesitate to ask for clari- derstanding of the material. You found
the beginning of the semester com- fication. You dedicated more time the lab exercises very challenging. Make
pared to the end of the semester. studying the lecture material in sure that you have understood the taught
Have a think about what is prevent- the beginning of the semester com- material and don’t hesitate to ask for
ing you from studying. Your under- pared to the end of the semester. clarification. You dedicated more time
standing of the material could be Have a think about what is prevent- studying the lecture material in the be-
improved. Try going over the teach- ing you from studying. Your un- ginning of the semester compared to the
ing material again. You have had derstanding of the material could end of the semester. Have a think about
other deadlines during weeks 5, 6, be improved. Try going over the what is preventing you from studying.
8, 9 and 10. You may want to plan teaching material again. Revising You did not face any health problems
your studying and work ahead. You material during the semester will during the semester. You revised part
did not face any health problems improve your performance in the of the learning material. Have a think
during the semester. lab. whether revising has improved your per-
formance.
</bodyText>
<tableCaption confidence="0.995198">
Table 2: Example outputs from the three different systems (bold signifies the chosen content).
</tableCaption>
<table confidence="0.999398">
Time-Series Summarisation Systems Lecturer Function Student Function MO Function
Lecturer-adapted system 243.82 (70.35) 51.99 (89.87) 114.12 (49.58)
Student-adapted system 72.54 (106.97) 213.75 (59.45) 127.76 (52.09)
MO system 123.67 (72.66) 153.79 (56.61) 164.84 (83.89)
</table>
<tableCaption confidence="0.881793">
Table 3: Average rewards (and standard deviation) assigned to summaries produced by the 3 systems.
Bold signifies higher reward.
</tableCaption>
<table confidence="0.9998828">
Summarisation Lecturer’s Rat- Student’s
Systems ing Rating
Lecturer-adapted 1st (2.15)* 3rd (1.97)
Student-adapted 1st (2.01)* 1st* (2.22)
MO 2nd, 3rd (1.81) 3rd (1.79)
</table>
<tableCaption confidence="0.975181">
Table 4: Mode of the ratings for each user group
</tableCaption>
<bodyText confidence="0.997680037037037">
(*Mann-Whitney U test, p &lt; 0.05, when compar-
ing each system to the MO system).
that a student studied in all cases (i.e. when the
hours studied increased, decreased, or remained
stable). This, however, does not factor heavily into
the student’s reward function.
Secondly, lecturers find it useful to give some
advice to students who faced personal issues dur-
ing the semester, such as advising them to talk to
their mentor. Students, on the other hand, like
reading about personal issues only when the num-
ber of issues they faced was increasing over the
semester, perhaps as this is the only trend that may
affect their performance. Students seem to mostly
prefer a feedback summary that mentions the un-
derstandability of the material when it increases
which is positive feedback. Finally, the only factor
that both groups agree on is that health issues is
negatively weighted and therefore not mentioned.
The MO reward function attempts to balance
the preferences of the two user groups. Therefore,
for this function, the coefficient for mentioning
health issues is also negative, however the other
coefficients are smoothed providing neither strong
negative or positive coefficients. This means that
there is less variability (see Table 3) but that per-
haps this function meets neither group’s criteria.
</bodyText>
<sectionHeader confidence="0.993532" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.995054125">
In conclusion, we presented a framework for de-
veloping and evaluating various reward functions
for time-series summarisation of feedback. This
framework has been validated in that both simula-
tion and subjective studies show that each group
does indeed prefer feedback generated using a
highly tuned reward function, with lecturers being
slightly more open to variation. Further investiga-
tion is required as to whether it is indeed possible
to find middle ground between these two groups.
Choices for one group may be negatively rated
by the other and it might not be possible to find
middle ground but it is worth investigating further
other methods of reward function derivation using
stronger feature selection methods, such as Princi-
pal Component Analysis.
</bodyText>
<page confidence="0.998837">
213
</page>
<sectionHeader confidence="0.990291" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999142">
Carole Ames. 1992. Classrooms: Goals, structures,
and student motivation. Journal of Educational Psy-
chology, 84(3):p261–71.
Chankong and Haimes. 1983. Multiobjective decision
making theory and methodology. In New York: El-
sevier Science Publishing.
Scotty D. Craig, Arthur C. Graesser, Jeremiah Sullins,
and Barry Gholson. 2004. Affect and learning: an
exploratory look into the role of affect in learning
with autotutor. In Journal of Educational Media,
29:241-250.
Albert Gatt, Francois Portet, Ehud Reiter, James
Hunter, Saad Mahamood, Wendy Moncur, and So-
mayajulu Sripada. 2009. From data to text in the
neonatal intensive care unit: Using NLG technology
for decision support and information management.
In Journal of AI Communications, 22:153-186.
Dimitra Gkatzia, Helen Hastie, Srinivasan Ja-
narthanam, and Oliver Lemon. 2013. Generating
student feedback from time-series data using Rein-
forcement Learning. In 14th European Workshop in
Natural Language Generation.
Srinivasan Janarthanam and Oliver Lemon. 2010.
Adaptive referring expression generation in spoken
dialogue systems: Evaluation with real users. In
11th Annual Meeting of the Special Interest Group
on Discourse and Dialogue.
K Papineni, S Roukos, T. Ward, and W. J Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In 40th Annual meeting of the As-
sociation for Computational Linguistics.
Natalie K. Person, Roger J. Kreuz, Rolf A. Zwaan, and
Arthur C. Graesser. 1995. Pragmatics and peda-
gogy: Conversational rules and politeness strategies
may inhibit effective tutoring. In Journal of Cogni-
tion and Instruction, 13(2):161-188.
Ehud Reiter and Robert Dale. 2000. Building natural
language generation systems. In Cambridge Univer-
sity Press.
Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising information presentation for spoken dia-
logue systems. In 48th Annual Meeting of the Asso-
ciation for Computational Linguistics.
Richart Sutton and Andrew Barto. 1998. Reinforce-
ment learning. In MIT Press.
Cynthia A. Thompson, Mehmet H. Goker, and Pat Lan-
gley. 2004. A personalised system for conversa-
tional recommendations. In Journal of Artificial In-
telligence Research 21, 333-428.
Marilyn Walker, Diane Litman, Candace Kamm, and
Alicia Abella. 1997. PARADISE: A framework for
evaluating spoken dialogue agents. In 35th Annual
meeting of the Association for Computational Lin-
guistics.
Ingrid Zukerman and Diane Litman. 2001. Natu-
ral language processing and user modeling: Syner-
gies and limitations. In User Modeling and User-
Adapted Interaction, 11(1-2), 129-158.
</reference>
<page confidence="0.998948">
214
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.852521">
<title confidence="0.944854">Finding middle ground? Multi-objective Natural Language from time-series data</title>
<author confidence="0.994362">Dimitra Gkatzia</author>
<author confidence="0.994362">Helen Hastie</author>
<author confidence="0.994362">Oliver Lemon</author>
<affiliation confidence="0.999993">School of Mathematical and Computer Sciences, Heriot-Watt University,</affiliation>
<email confidence="0.965506">h.hastie,</email>
<abstract confidence="0.999673391304348">A Natural Language Generation (NLG) system is able to generate text from nonlinguistic data, ideally personalising the content to a user’s specific needs. In some cases, however, there are multiple stakeholders with their own individual goals, needs and preferences. In this paper, we explore the feasibility of combining the preferences of two different user groups, lecturers and students, when generating summaries in the context of student feedback generation. The preferences of each user group are modelled as a multivariate optimisation function, therefore the task of generation is seen as a multi-objective (MO) optimisation task, where the two functions are combined into one. This initial study shows that treating the preferences of each user group equally smooths the weights of the MO function, in a way that preferred content of the user groups is not presented in the generated summary.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Carole Ames</author>
</authors>
<title>Classrooms: Goals, structures, and student motivation.</title>
<date>1992</date>
<journal>Journal of Educational Psychology,</journal>
<volume>84</volume>
<issue>3</issue>
<contexts>
<context position="1977" citStr="Ames, 1992" startWordPosition="300" endWordPosition="301">o say, i.e. what information to be included in a report (Reiter and Dale, 2000). Here, we consider the task of automatically generating feedback summaries for students describing their performance during the lab of a computer science module over the semester. This work is motivated by the fact that different user groups have different preferences of the content that should be conveyed in a summary, as shown by Gkatzia et al. (2013). Various factors can influence students’ learning, such as difficulty of the material (Person et al., 1995), workload (Craig et al., 2004), attendance in lectures (Ames, 1992) etc. These factors change over time and can be interdependent. The different stakeholders (i.e. lecturers and students) have different perceptions regarding what constitutes good feedback. Therefore, when generating feedback, we should take into account all preferences in order to be able to produce feedback summaries that are acceptable by both user groups. Stakeholders often have conflicting goals, needs and preferences, for example managers with employees or doctors with patients and relatives. In our data, for instance, lecturers tend to comment on the hours that a student studied, wherea</context>
</contexts>
<marker>Ames, 1992</marker>
<rawString>Carole Ames. 1992. Classrooms: Goals, structures, and student motivation. Journal of Educational Psychology, 84(3):p261–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chankong</author>
<author>Haimes</author>
</authors>
<title>Multiobjective decision making theory and methodology. In</title>
<date>1983</date>
<publisher>Elsevier Science Publishing.</publisher>
<location>New York:</location>
<contexts>
<context position="3317" citStr="Chankong and Haimes, 1983" startWordPosition="506" endWordPosition="509">iscussion with common ground. Previous work on NLG systems that address more than one user group use different versions of a system for each different user group (Gatt et al., 2009) or make use of User Models (Janarthanam and Lemon, 2010; Thompson et al., 2004; Zukerman and Litman, 2001). Here, we explore a method that adapts to both expert preferences and users simultaneously (i.e. lecturer and students preferences), by applying Multi-Objective optimisation (MOO). MOO can be applied to situations where optimal decisions are sought in the presence of trade-offs between conflicting objectives (Chankong and Haimes, 1983). We explore whether balancing the preferences of two user groups can result in an adaptive system that is acceptable by all users. At the same time, the programming effort is reduced as only one system needs to be developed. Moreover, by pooling all available data together, there is less need for an extensive data collection. In the next section, we present three systems: one tuned for lecturers, one for students, and one that attempts to find middle ground. In Section 3, we describe an evaluation of these three systems and in Section 4 we discuss the results. Finally, in 210 Proceedings of t</context>
</contexts>
<marker>Chankong, Haimes, 1983</marker>
<rawString>Chankong and Haimes. 1983. Multiobjective decision making theory and methodology. In New York: Elsevier Science Publishing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scotty D Craig</author>
<author>Arthur C Graesser</author>
<author>Jeremiah Sullins</author>
<author>Barry Gholson</author>
</authors>
<title>Affect and learning: an exploratory look into the role of affect in learning with autotutor.</title>
<date>2004</date>
<journal>In Journal of Educational Media,</journal>
<pages>29--241</pages>
<contexts>
<context position="1940" citStr="Craig et al., 2004" startWordPosition="293" endWordPosition="296">tent selection is the task of choosing what to say, i.e. what information to be included in a report (Reiter and Dale, 2000). Here, we consider the task of automatically generating feedback summaries for students describing their performance during the lab of a computer science module over the semester. This work is motivated by the fact that different user groups have different preferences of the content that should be conveyed in a summary, as shown by Gkatzia et al. (2013). Various factors can influence students’ learning, such as difficulty of the material (Person et al., 1995), workload (Craig et al., 2004), attendance in lectures (Ames, 1992) etc. These factors change over time and can be interdependent. The different stakeholders (i.e. lecturers and students) have different perceptions regarding what constitutes good feedback. Therefore, when generating feedback, we should take into account all preferences in order to be able to produce feedback summaries that are acceptable by both user groups. Stakeholders often have conflicting goals, needs and preferences, for example managers with employees or doctors with patients and relatives. In our data, for instance, lecturers tend to comment on the</context>
</contexts>
<marker>Craig, Graesser, Sullins, Gholson, 2004</marker>
<rawString>Scotty D. Craig, Arthur C. Graesser, Jeremiah Sullins, and Barry Gholson. 2004. Affect and learning: an exploratory look into the role of affect in learning with autotutor. In Journal of Educational Media, 29:241-250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Gatt</author>
<author>Francois Portet</author>
<author>Ehud Reiter</author>
<author>James Hunter</author>
<author>Saad Mahamood</author>
<author>Wendy Moncur</author>
<author>Somayajulu Sripada</author>
</authors>
<title>From data to text in the neonatal intensive care unit: Using NLG technology for decision support and information management.</title>
<date>2009</date>
<journal>In Journal of AI Communications,</journal>
<pages>22--153</pages>
<contexts>
<context position="2872" citStr="Gatt et al., 2009" startWordPosition="437" endWordPosition="440">er to be able to produce feedback summaries that are acceptable by both user groups. Stakeholders often have conflicting goals, needs and preferences, for example managers with employees or doctors with patients and relatives. In our data, for instance, lecturers tend to comment on the hours that a student studied, whereas the students disprefer this content. Generating the same summary for both groups allows for meaningful further discussion with common ground. Previous work on NLG systems that address more than one user group use different versions of a system for each different user group (Gatt et al., 2009) or make use of User Models (Janarthanam and Lemon, 2010; Thompson et al., 2004; Zukerman and Litman, 2001). Here, we explore a method that adapts to both expert preferences and users simultaneously (i.e. lecturer and students preferences), by applying Multi-Objective optimisation (MOO). MOO can be applied to situations where optimal decisions are sought in the presence of trade-offs between conflicting objectives (Chankong and Haimes, 1983). We explore whether balancing the preferences of two user groups can result in an adaptive system that is acceptable by all users. At the same time, the p</context>
</contexts>
<marker>Gatt, Portet, Reiter, Hunter, Mahamood, Moncur, Sripada, 2009</marker>
<rawString>Albert Gatt, Francois Portet, Ehud Reiter, James Hunter, Saad Mahamood, Wendy Moncur, and Somayajulu Sripada. 2009. From data to text in the neonatal intensive care unit: Using NLG technology for decision support and information management. In Journal of AI Communications, 22:153-186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitra Gkatzia</author>
<author>Helen Hastie</author>
<author>Srinivasan Janarthanam</author>
<author>Oliver Lemon</author>
</authors>
<title>Generating student feedback from time-series data using Reinforcement Learning.</title>
<date>2013</date>
<booktitle>In 14th European Workshop in Natural Language Generation.</booktitle>
<contexts>
<context position="1801" citStr="Gkatzia et al. (2013)" startWordPosition="272" endWordPosition="275">Summarisation of time-series data refers to the task of automatically generating summaries from attributes whose values change over time. Content selection is the task of choosing what to say, i.e. what information to be included in a report (Reiter and Dale, 2000). Here, we consider the task of automatically generating feedback summaries for students describing their performance during the lab of a computer science module over the semester. This work is motivated by the fact that different user groups have different preferences of the content that should be conveyed in a summary, as shown by Gkatzia et al. (2013). Various factors can influence students’ learning, such as difficulty of the material (Person et al., 1995), workload (Craig et al., 2004), attendance in lectures (Ames, 1992) etc. These factors change over time and can be interdependent. The different stakeholders (i.e. lecturers and students) have different perceptions regarding what constitutes good feedback. Therefore, when generating feedback, we should take into account all preferences in order to be able to produce feedback summaries that are acceptable by both user groups. Stakeholders often have conflicting goals, needs and preferenc</context>
<context position="4435" citStr="Gkatzia et al. (2013)" startWordPosition="688" endWordPosition="691">uation of these three systems and in Section 4 we discuss the results. Finally, in 210 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 210–214, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Section 5, directions for future work are discussed. 2 Methodology Reinforcement Learning (RL) is a machine learning technique that defines how an agent learns to take optimal sequences of actions so as to maximize a cumulative reward (Sutton and Barto, 1998). Here we extend the framework proposed by Gkatzia et al. (2013) whereby the content selection is seen as a Markov Decision problem and the goal of the agent is to learn to take the sequence of actions that leads to optimal content selection. A Temporal Difference learning method (Sutton and Barto, 1998) was used to train an agent for content selection. Firstly, we will describe the data in general. Secondly, we refer to the RL system that adapts to lecturers’ preferences as described by Gkatzia et al. (2013). Thirdly, we will describe how we collected data and developed a methodology that adapts to students’ preferences and finally how we combined the kno</context>
</contexts>
<marker>Gkatzia, Hastie, Janarthanam, Lemon, 2013</marker>
<rawString>Dimitra Gkatzia, Helen Hastie, Srinivasan Janarthanam, and Oliver Lemon. 2013. Generating student feedback from time-series data using Reinforcement Learning. In 14th European Workshop in Natural Language Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivasan Janarthanam</author>
<author>Oliver Lemon</author>
</authors>
<title>Adaptive referring expression generation in spoken dialogue systems: Evaluation with real users.</title>
<date>2010</date>
<booktitle>In 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue.</booktitle>
<contexts>
<context position="2928" citStr="Janarthanam and Lemon, 2010" startWordPosition="447" endWordPosition="450">t are acceptable by both user groups. Stakeholders often have conflicting goals, needs and preferences, for example managers with employees or doctors with patients and relatives. In our data, for instance, lecturers tend to comment on the hours that a student studied, whereas the students disprefer this content. Generating the same summary for both groups allows for meaningful further discussion with common ground. Previous work on NLG systems that address more than one user group use different versions of a system for each different user group (Gatt et al., 2009) or make use of User Models (Janarthanam and Lemon, 2010; Thompson et al., 2004; Zukerman and Litman, 2001). Here, we explore a method that adapts to both expert preferences and users simultaneously (i.e. lecturer and students preferences), by applying Multi-Objective optimisation (MOO). MOO can be applied to situations where optimal decisions are sought in the presence of trade-offs between conflicting objectives (Chankong and Haimes, 1983). We explore whether balancing the preferences of two user groups can result in an adaptive system that is acceptable by all users. At the same time, the programming effort is reduced as only one system needs to</context>
</contexts>
<marker>Janarthanam, Lemon, 2010</marker>
<rawString>Srinivasan Janarthanam and Oliver Lemon. 2010. Adaptive referring expression generation in spoken dialogue systems: Evaluation with real users. In 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In 40th Annual meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="10372" citStr="Papineni et al., 2002" startWordPosition="1687" endWordPosition="1690">esented in Table 2. 3.1 Evaluation in Simulation 26 summaries were produced by each system. The output of each system was evaluated with the three reward functions. Table 3 shows the results. As expected, all systems score highly when evaluated with the reward function for which they were trained, with the second highest reward scored from the MO function. Table 2 illustrates this with the MO Policy clearly between the other two policies. Moreover, the MO function reduces the variability between summaries as is also reflected in the standard deviation given in Table 3. We used BLEU (4-grams) (Papineni et al., 2002) to measure the similarities between the feedback summaries generated by the three systems. BLEU score is between 0-1 with values closer to 1 indicating texts are more similar. Our results demonstrate that the summaries generated by the three systems are quite different (BLEU score between 0.33 and 0.36). This shows that the framework presented here is capable of producing quite different summaries based on the various reward functions. 3.2 Evaluation with real users The goal of the evaluation is to determine whether the end-user can pick up on the above-mentioned differences in the feedback a</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K Papineni, S Roukos, T. Ward, and W. J Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In 40th Annual meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Natalie K Person</author>
<author>Roger J Kreuz</author>
<author>Rolf A Zwaan</author>
<author>Arthur C Graesser</author>
</authors>
<title>Pragmatics and pedagogy: Conversational rules and politeness strategies may inhibit effective tutoring.</title>
<date>1995</date>
<journal>In Journal of Cognition and Instruction,</journal>
<pages>13--2</pages>
<contexts>
<context position="1909" citStr="Person et al., 1995" startWordPosition="288" endWordPosition="291">ose values change over time. Content selection is the task of choosing what to say, i.e. what information to be included in a report (Reiter and Dale, 2000). Here, we consider the task of automatically generating feedback summaries for students describing their performance during the lab of a computer science module over the semester. This work is motivated by the fact that different user groups have different preferences of the content that should be conveyed in a summary, as shown by Gkatzia et al. (2013). Various factors can influence students’ learning, such as difficulty of the material (Person et al., 1995), workload (Craig et al., 2004), attendance in lectures (Ames, 1992) etc. These factors change over time and can be interdependent. The different stakeholders (i.e. lecturers and students) have different perceptions regarding what constitutes good feedback. Therefore, when generating feedback, we should take into account all preferences in order to be able to produce feedback summaries that are acceptable by both user groups. Stakeholders often have conflicting goals, needs and preferences, for example managers with employees or doctors with patients and relatives. In our data, for instance, l</context>
</contexts>
<marker>Person, Kreuz, Zwaan, Graesser, 1995</marker>
<rawString>Natalie K. Person, Roger J. Kreuz, Rolf A. Zwaan, and Arthur C. Graesser. 1995. Pragmatics and pedagogy: Conversational rules and politeness strategies may inhibit effective tutoring. In Journal of Cognition and Instruction, 13(2):161-188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Robert Dale</author>
</authors>
<title>Building natural language generation systems. In</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1445" citStr="Reiter and Dale, 2000" startWordPosition="214" endWordPosition="218">re the task of generation is seen as a multi-objective (MO) optimisation task, where the two functions are combined into one. This initial study shows that treating the preferences of each user group equally smooths the weights of the MO function, in a way that preferred content of the user groups is not presented in the generated summary. 1 Introduction Summarisation of time-series data refers to the task of automatically generating summaries from attributes whose values change over time. Content selection is the task of choosing what to say, i.e. what information to be included in a report (Reiter and Dale, 2000). Here, we consider the task of automatically generating feedback summaries for students describing their performance during the lab of a computer science module over the semester. This work is motivated by the fact that different user groups have different preferences of the content that should be conveyed in a summary, as shown by Gkatzia et al. (2013). Various factors can influence students’ learning, such as difficulty of the material (Person et al., 1995), workload (Craig et al., 2004), attendance in lectures (Ames, 1992) etc. These factors change over time and can be interdependent. The </context>
</contexts>
<marker>Reiter, Dale, 2000</marker>
<rawString>Ehud Reiter and Robert Dale. 2000. Building natural language generation systems. In Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Verena Rieser</author>
<author>Oliver Lemon</author>
<author>Xingkun Liu</author>
</authors>
<title>Optimising information presentation for spoken dialogue systems.</title>
<date>2010</date>
<booktitle>In 48th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8067" citStr="Rieser et al. (2010)" startWordPosition="1303" endWordPosition="1306">is performed deterministically, i.e. it selects the template that results in higher reward. Length represents the number of factors selected for generation. Student-adapted reward function The Student-adapted system uses the same RL algorithm as the Lecturer-adapted one. The difference lies in the reward function. The reward function used for training is of a similar style as the Lecturer-adapted reward function. This function was derived by manipulating the student ratings in a previous experiment and estimating the weights using linear regression in a similar way as Walker et al. (1997) and Rieser et al. (2010). Multi-objective function The function used for the multi-objective method is derived by weighting the sum of the individual reward functions. RmO = 0.5 * RLECT + 0.5 * RSTUDENT To reduce the confounding variables, we kept the ordering of content in all systems the same. 3 Evaluation The output of the above-mentioned three systems were evaluated both in simulation and with real xi = 211 factors week 2 week 3 ... week 10 marks 5 4 ... 5 hours studied 1 2 ... 3 ... ... ... ... ... Raw Data Trends from Data factors factor trend (1) marks trend other (2) hours studied trend increasing (3) underst</context>
</contexts>
<marker>Rieser, Lemon, Liu, 2010</marker>
<rawString>Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010. Optimising information presentation for spoken dialogue systems. In 48th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richart Sutton</author>
<author>Andrew Barto</author>
</authors>
<title>Reinforcement learning. In</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="4371" citStr="Sutton and Barto, 1998" startWordPosition="677" endWordPosition="680"> attempts to find middle ground. In Section 3, we describe an evaluation of these three systems and in Section 4 we discuss the results. Finally, in 210 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 210–214, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Section 5, directions for future work are discussed. 2 Methodology Reinforcement Learning (RL) is a machine learning technique that defines how an agent learns to take optimal sequences of actions so as to maximize a cumulative reward (Sutton and Barto, 1998). Here we extend the framework proposed by Gkatzia et al. (2013) whereby the content selection is seen as a Markov Decision problem and the goal of the agent is to learn to take the sequence of actions that leads to optimal content selection. A Temporal Difference learning method (Sutton and Barto, 1998) was used to train an agent for content selection. Firstly, we will describe the data in general. Secondly, we refer to the RL system that adapts to lecturers’ preferences as described by Gkatzia et al. (2013). Thirdly, we will describe how we collected data and developed a methodology that ada</context>
</contexts>
<marker>Sutton, Barto, 1998</marker>
<rawString>Richart Sutton and Andrew Barto. 1998. Reinforcement learning. In MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia A Thompson</author>
<author>Mehmet H Goker</author>
<author>Pat Langley</author>
</authors>
<title>A personalised system for conversational recommendations.</title>
<date>2004</date>
<journal>In Journal of Artificial Intelligence Research</journal>
<volume>21</volume>
<pages>333--428</pages>
<contexts>
<context position="2951" citStr="Thompson et al., 2004" startWordPosition="451" endWordPosition="454"> groups. Stakeholders often have conflicting goals, needs and preferences, for example managers with employees or doctors with patients and relatives. In our data, for instance, lecturers tend to comment on the hours that a student studied, whereas the students disprefer this content. Generating the same summary for both groups allows for meaningful further discussion with common ground. Previous work on NLG systems that address more than one user group use different versions of a system for each different user group (Gatt et al., 2009) or make use of User Models (Janarthanam and Lemon, 2010; Thompson et al., 2004; Zukerman and Litman, 2001). Here, we explore a method that adapts to both expert preferences and users simultaneously (i.e. lecturer and students preferences), by applying Multi-Objective optimisation (MOO). MOO can be applied to situations where optimal decisions are sought in the presence of trade-offs between conflicting objectives (Chankong and Haimes, 1983). We explore whether balancing the preferences of two user groups can result in an adaptive system that is acceptable by all users. At the same time, the programming effort is reduced as only one system needs to be developed. Moreover</context>
</contexts>
<marker>Thompson, Goker, Langley, 2004</marker>
<rawString>Cynthia A. Thompson, Mehmet H. Goker, and Pat Langley. 2004. A personalised system for conversational recommendations. In Journal of Artificial Intelligence Research 21, 333-428.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn Walker</author>
<author>Diane Litman</author>
<author>Candace Kamm</author>
<author>Alicia Abella</author>
</authors>
<title>PARADISE: A framework for evaluating spoken dialogue agents.</title>
<date>1997</date>
<booktitle>In 35th Annual meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8042" citStr="Walker et al. (1997)" startWordPosition="1298" endWordPosition="1301">election of the template is performed deterministically, i.e. it selects the template that results in higher reward. Length represents the number of factors selected for generation. Student-adapted reward function The Student-adapted system uses the same RL algorithm as the Lecturer-adapted one. The difference lies in the reward function. The reward function used for training is of a similar style as the Lecturer-adapted reward function. This function was derived by manipulating the student ratings in a previous experiment and estimating the weights using linear regression in a similar way as Walker et al. (1997) and Rieser et al. (2010). Multi-objective function The function used for the multi-objective method is derived by weighting the sum of the individual reward functions. RmO = 0.5 * RLECT + 0.5 * RSTUDENT To reduce the confounding variables, we kept the ordering of content in all systems the same. 3 Evaluation The output of the above-mentioned three systems were evaluated both in simulation and with real xi = 211 factors week 2 week 3 ... week 10 marks 5 4 ... 5 hours studied 1 2 ... 3 ... ... ... ... ... Raw Data Trends from Data factors factor trend (1) marks trend other (2) hours studied tre</context>
</contexts>
<marker>Walker, Litman, Kamm, Abella, 1997</marker>
<rawString>Marilyn Walker, Diane Litman, Candace Kamm, and Alicia Abella. 1997. PARADISE: A framework for evaluating spoken dialogue agents. In 35th Annual meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ingrid Zukerman</author>
<author>Diane Litman</author>
</authors>
<title>Natural language processing and user modeling: Synergies and limitations.</title>
<date>2001</date>
<booktitle>In User Modeling and UserAdapted Interaction,</booktitle>
<pages>11--1</pages>
<contexts>
<context position="2979" citStr="Zukerman and Litman, 2001" startWordPosition="455" endWordPosition="459">ften have conflicting goals, needs and preferences, for example managers with employees or doctors with patients and relatives. In our data, for instance, lecturers tend to comment on the hours that a student studied, whereas the students disprefer this content. Generating the same summary for both groups allows for meaningful further discussion with common ground. Previous work on NLG systems that address more than one user group use different versions of a system for each different user group (Gatt et al., 2009) or make use of User Models (Janarthanam and Lemon, 2010; Thompson et al., 2004; Zukerman and Litman, 2001). Here, we explore a method that adapts to both expert preferences and users simultaneously (i.e. lecturer and students preferences), by applying Multi-Objective optimisation (MOO). MOO can be applied to situations where optimal decisions are sought in the presence of trade-offs between conflicting objectives (Chankong and Haimes, 1983). We explore whether balancing the preferences of two user groups can result in an adaptive system that is acceptable by all users. At the same time, the programming effort is reduced as only one system needs to be developed. Moreover, by pooling all available d</context>
</contexts>
<marker>Zukerman, Litman, 2001</marker>
<rawString>Ingrid Zukerman and Diane Litman. 2001. Natural language processing and user modeling: Synergies and limitations. In User Modeling and UserAdapted Interaction, 11(1-2), 129-158.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>