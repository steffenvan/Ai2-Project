<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.970749">
Retrieving Collocations from Text: Xtract
</title>
<author confidence="0.997169">
Frank Smadja*
</author>
<affiliation confidence="0.961996">
Columbia University
</affiliation>
<bodyText confidence="0.998885058823529">
Natural languages are full of collocations, recurrent combinations of words that co-occur more
often than expected by chance and that correspond to arbitrary word usages. Recent work in
lexicography indicates that collocations are pervasive in English; apparently, they are common
in all types of writing, including both technical and nontechnical genres. Several approaches
have been proposed to retrieve various types of collocations from the analysis of large samples of
textual data. These techniques automatically produce large numbers of collocations along with
statistical figures intended to reflect the relevance of the associations. However, none of these
techniques provides functional information along with the collocation. Also, the results produced
often contained improper word associations reflecting some spurious aspect of the training corpus
that did not stand for true collocations.
In this paper, we describe a set of techniques based on statistical methods for retrieving
and identifying collocations from large textual corpora. These techniques produce a wide range
of collocations and are based on some original filtering methods that allow the production of
richer and higher-precision output. These techniques have been implemented and resulted in a
lexicographic tool, Xtract. The techniques are described and some results are presented on a 10
million—word corpus of stock market news reports. A lexicographic evaluation of Xtract as a
collocation retrieval tool has been made, and the estimated precision of Xtract is 80%.
</bodyText>
<sectionHeader confidence="0.979854" genericHeader="abstract">
1. Introduction
</sectionHeader>
<bodyText confidence="0.688995">
Consider the following sentences:
</bodyText>
<listItem confidence="0.9764077">
1. &amp;quot;The Dow Jones average of 30 industrials
rose 26.28 points to 2,304.69 on Tuesday.&amp;quot;
2. &amp;quot;The Dow average rose 26.28 points to 2,304.69
on Tuesday.&amp;quot;
3. &amp;quot;The Dow industrials rose 26.28 points to 2,304.69
on Tuesday.&amp;quot;
4. &amp;quot;The Dow Jones industrial rose 26.28 points
to 2,304.69 on Tuesday.&amp;quot;
* 5. &amp;quot;The Jones industrials rose 26.28 points
to 2,304.69 on Tuesday.&amp;quot;
</listItem>
<note confidence="0.632631666666667">
* Computer Science Department, Columbia University, New York, NY 10027. smadja@cs.columbia.edu.
© 1993 Association for Computational Linguistics
Computational Linguistics Volume 19, Number 1
</note>
<tableCaption confidence="0.978201">
Table 1
</tableCaption>
<figure confidence="0.976691416666666">
Cross linguistic comparisons of collocations.
Language English Translation English correspondence
French
German
Italian
Spanish
Turkish
French
German
Italian
Spanish
Turkish
</figure>
<figureCaption confidence="0.9151821">
to see the door
to see the door
to see the door
to see the door
to see the door
to break down/force the door
to break down/force the door
to break down/force the door
to break down/force the door
to break down/force the door
</figureCaption>
<bodyText confidence="0.9358493">
voir la porte to see the door
die Tar sehen to see the door
vedere la porta to see the door
ver la puerta to see the door
lcapiyi gormek to see the door
enfoncer la porte * to push the door through
die Tiir aufbrechen * to break the door
sfondare la porta * to hit/demolish the door
tumbar la puerta * to fall the door
kapiyi kirmak * to break the door
</bodyText>
<listItem confidence="0.951040666666667">
* 6. &amp;quot;The industrial Dow rose 26.28 points to
2,304.69 on Tuesday.&amp;quot;
* 7. &amp;quot;The Dow of 30 industrials rose 26.28 points to
2,304.69 on Tuesday.&amp;quot;
* 8. &amp;quot;The Dow industrial rose 26.28 points to
2,304.69 on Tuesday.&amp;quot;
</listItem>
<bodyText confidence="0.993213863636364">
The above sentences contain expressions that are difficult to handle for nonspecial-
ists. For example, among the eight different expressions referring to the famous Wall
Street index, only those used in sentences 1-4 are correct. The expressions used in the
starred sentences 5-8 are all incorrect. The rules violated in sentences 5-8 are neither
rules of syntax nor of semantics but purely lexical rules. The word combinations used
in sentences 5-8 are invalid simply because they do not exist; similarly, the ones used
in sentences 1-4 are correct because they exist.
Expressions such as these are called collocations. Collocations vary tremendously
in the number of words involved, in the syntactic categories of the words, in the
syntactic relations between the words, and in how rigidly the individual words are
used together. For example, in some cases, the words of a collocation must be adjacent,
as in sentences 1-5 above, while in others they can be separated by a varying number of
other words. Unfortunately, with few exceptions (e.g., Benson, Benson, and Ilson 1986a)
collocations are generally unavailable in compiled form. This creates a problem for
persons not familiar with the sublanguagel as well as for several machine applications
such as language generation.
In this paper we describe a set of techniques for automatically retrieving such
collocations from naturally occurring textual corpora. These techniques are based on
statistical methods; they have been implemented in a tool, Xtract, which is able to
retrieve a wide range of collocations with high performance. Preliminary results ob-
tained with parts of Xtract have been described in the past (e.g., Smadja and McKeown
1990); this paper gives a complete description of the system and the results obtained.
</bodyText>
<footnote confidence="0.9887625">
1 This is true for laymen and also for non-native speakers familiar with the domain but not familiar with
the English expressions.
</footnote>
<page confidence="0.996183">
144
</page>
<figure confidence="0.887227857142857">
Frank Smadja Retrieving Collocations from Text: Xtract
&amp;quot;Our firm made/did a deal with them&amp;quot;
&amp;quot;The swimmer had/got a cramp&amp;quot;
&amp;quot;Politicians are always on/in the firing lane&amp;quot;
&amp;quot;These decisions are to be made/taken rapidly&amp;quot;
&amp;quot;The children usually set/lay the table&amp;quot;
&amp;quot;You have to break in/run in your new car&amp;quot;
</figure>
<figureCaption confidence="0.734757">
Figure 1
</figureCaption>
<figure confidence="0.9075">
British English or American English? from Benson (1990).
sentences candidates
&amp;quot;If a fire breaks out, the alarm will ?? &amp;quot; &amp;quot;ring, go off, sound, start&amp;quot;
&amp;quot;The boy doesn&apos;t know how to ?? his bicycle&amp;quot; &amp;quot;drive, ride, conduct&amp;quot;
&amp;quot;The American congress can ?? a presidential veto&amp;quot; &amp;quot;ban/cancel/delete/reject&amp;quot;
&amp;quot;Before eating your bag of microwavable popcorn, &amp;quot;turn down/abrogate/overrule&amp;quot;
you have to ?? it&amp;quot; &amp;quot;cook/nuke/broil/fry/bake&amp;quot;
</figure>
<figureCaption confidence="0.7561465">
Figure 2
Fill-in-the-blank test, from Benson (1990).
</figureCaption>
<bodyText confidence="0.999812619047619">
Xtract now works in three stages. In the first stage, pairwise lexical relations are re-
trieved using only statistical information. This stage is comparable to Church and
Hanks (1989) in that it evaluates a certain word association between pairs of words.
As in Church and Hanks (1989), the words can appear in any order and they can
be separated by an arbitrary number of other words. However, the statistics we use
provide more information and allow us to have more precision in our output. The out-
put of this first stage is then passed in parallel to the next two stages. In the second
stage, multiple-word combinations and complex expressions are identified. This stage
produces output comparable to that of Choueka, Klein, and Neuwitz (1983); however
the techniques we use are simpler and only produce relevant data. Finally, by com-
bining parsing and statistical techniques the third stage labels and filters collocations
retrieved at stage one. The third stage has been evaluated to raise the precision of
Xtract from 40% to 80% with a recall of 94%.
Section 2 is an introductory section on collocational knowledge, Section 3 describes
the type of collocations that are retrieved by Xtract, and Section 4 briefly surveys re-
lated efforts and contrasts our work to them. The three stages of Xtract are then in-
troduced in Section 5 and described respectively in Sections 6, 7, and 8. Some results
obtained by running Xtract on several corpora are listed and discussed in Section 9.
Qualitative and quantitative evaluations of our methods and of our results are dis-
cussed in Sections 10 and 11. Finally, several possible applications and tasks for Xtract
are discussed in Section 12.
</bodyText>
<sectionHeader confidence="0.929041" genericHeader="keywords">
2. What Are Collocations?
</sectionHeader>
<bodyText confidence="0.9987845">
There has been a great deal of theoretical and applied work related to collocations
that has resulted in different characterizations (e.g., Allerton 1984; Cruse 1986; Menuk
1981). Depending on their interests and points of view, researchers have focused on
different aspects of collocations. One of the most comprehensive definition that has
</bodyText>
<page confidence="0.996873">
145
</page>
<note confidence="0.84471">
Computational Linguistics Volume 19, Number 1
</note>
<bodyText confidence="0.9059655">
been used can be found in the lexicographic work of Benson and his colleagues (Benson
1990). The definition is the following:
</bodyText>
<sectionHeader confidence="0.567908" genericHeader="introduction">
Definition
</sectionHeader>
<bodyText confidence="0.998992875">
A collocation is an arbitrary and recurrent word combination (Benson 1990).
This definition, however, does not cover some aspects and properties of colloca-
tions that have consequences for a number of machine applications. For example, it
has been shown that collocations are difficult to translate across languages—this fact
obviously has a direct application for machine translation. Many properties of col-
locations have been identified in the past; however, the tendency was to focus on a
restricted type of collocation. In this section, we present four properties of collocations
that we have identified and discuss their relevance to computational linguistics.
</bodyText>
<subsectionHeader confidence="0.963766">
2.1 Collocations Are Arbitrary
</subsectionHeader>
<bodyText confidence="0.99984755">
Collocations are difficult to produce for second language learners (Nakhimovsky and
Leed 1979). In most cases, the learner cannot simply translate word-for-word what
s/he would say in her/his native language. As we can see in Table 1, the word-for-
word translation of &amp;quot;to open the door&amp;quot; works well in both directions in all five languages.
In contrast, translating word-for-word the expression: &amp;quot;to break down/force the door&amp;quot; is
a poor strategy in both directions in all five languages. The co-occurrence of &amp;quot;door&amp;quot;
and &amp;quot;open&amp;quot; is an open or free combination, whereas the combination &amp;quot;door&amp;quot; and &amp;quot;break
down&amp;quot; is a collocation. Learners of English would not produce &amp;quot;to break down a door&amp;quot;
whether their first language is French, German, Italian, Spanish, or Turkish, if they
were not aware of the construct.
Figure 1 illustrates disagreements between British English and American English.
Here the problem is even finer than in Table 1 since the disagreement is not across two
different languages, but across dialects of English. In each of the sentences given in
this figure, there is a different word choice for the American (left side) and the British
English (right side). The word choices do not correspond to any syntactic or semantic
variation of English but rather to different word usages in both dialects of English.
Translating from one language to another requires more than a good knowledge
of the syntactic structure and the semantic representation. Because collocations are
arbitrary, they must be readily available in both languages for effective machine trans-
lation.
</bodyText>
<subsectionHeader confidence="0.99803">
2.2 Collocations Are Domain-Dependent
</subsectionHeader>
<bodyText confidence="0.999912666666667">
In addition to nontechnical collocations such as the ones presented before, domain-
specific collocations are numerous. Technical jargons are often totally unintelligible for
the layman. They contain a large number of technical terms. In addition, familiar words
seem to be used differently. In the domain of sailing (Dellenbaugh and Dellenbaugh
1990), for example, some words are unknown to the nonfamiliar reader: rigg, jib, and
leeward are totally meaningless to the layman. Some other combinations apparently do
not contain any technical words, but these words take on a totally different meaning
in the domain. For example, a dry suit is not a suit that is dry but a special type of
suit used by sailors to stay dry in difficult weather conditions. Similarly a wet suit
is a special kind of suit used for several marine activities. Native speakers are often
unaware of the arbitrariness of collocations in nontechnical core English; however,
this arbitrariness becomes obvious to the native speaker in specific sublanguages.
</bodyText>
<page confidence="0.997828">
146
</page>
<table confidence="0.9342798">
Frank Smadja Retrieving Collocations from Text: Xtract
type example
N-Adj &amp;quot;heavy/light [1 trading/smoker/traffic&amp;quot;
N-Adj &amp;quot;high/low [1 fertility/pressure/bounce&amp;quot;
N-Adj &amp;quot;large/small [I crowd/retailer/client&amp;quot;
SV &amp;quot;index 0 rose
SV &amp;quot;stock [] [rose, fell, jumped, continued, declined, crashed, ...1&amp;quot;
SV &amp;quot;advancers [1 [outnumbered, outpaced, overwhelmed, outstripped]&amp;quot;
V-Adv &amp;quot;trade -;=&gt; actively,&amp;quot; &amp;quot;mix &lt;=&gt; narrowly,&amp;quot;
V-Adv &amp;quot;use &lt;#. widely,&amp;quot; &amp;quot;watch &lt;=&gt; closely&amp;quot;
VO &amp;quot;posted []gain
VO &amp;quot;momentum [1 [pick up, build, carry over, gather, loose, gain]&amp;quot;
V-Part &amp;quot;take 0 from,&amp;quot; &amp;quot;raise [I by,&amp;quot; &amp;quot;mix [1 with&amp;quot;
VV &amp;quot;offer to [acquire, buy&amp;quot;]
VV &amp;quot;agree to [acquire, buy&amp;quot;]
</table>
<figureCaption confidence="0.690232">
Figure 3
</figureCaption>
<bodyText confidence="0.813138333333333">
Some examples of predicative collocations.
Linguistically mastering a domain such as the domain of sailing thus requires more
than a glossary, it requires knowledge of domain-dependent collocations.
</bodyText>
<subsectionHeader confidence="0.99952">
2.3 Collocations Are Recurrent
</subsectionHeader>
<bodyText confidence="0.9999726">
The recurrent property simply means that these combinations are not exceptions, but
rather that they are very often repeated in a given context. Word combinations such as
&amp;quot;to make a decision, to hit a record, to perform an operation&amp;quot; are typical of the language, and
collocations such as &amp;quot;to buy short,&amp;quot; &amp;quot;to ease the jib&amp;quot; are characteristic of specific domains.
Both types are repeatedly used in specific contexts.
</bodyText>
<subsectionHeader confidence="0.999183">
2.4 Collocations Are Cohesive Lexical Clusters
</subsectionHeader>
<bodyText confidence="0.99891575">
By cohesive2 clusters, we mean that the presence of one or several words of the collo-
cations often implies or suggests the rest of the collocation. This is the property mostly
used by lexicographers when compiling collocations (Cowie 1981; Benson 1989a). Lexi-
cographers use other people&apos;s linguistic judgment for deciding what is and what is not
a collocation. They give questionnaires to people such as the one given in Figure 2. This
questionnaire contains sentences used by Benson for compiling collocational knowl-
edge for the BBI (Benson 1989b). Each sentence contains an empty slot that can easily
be filled in by native speakers. In contrast, second language speakers would not find
the missing words automatically but would consider a long list of words having the ap-
propriate semantic and syntactic features such as the ones given in the second column.
As a consequence, collocations have particular statistical distributions (e.g., Hal-
liday 1966; Cruse 1986). This means that, for example, the probability that any two
adjacent words in a sample will be &amp;quot;red herring&amp;quot; is considerably larger than the prob-
ability of &amp;quot;red&amp;quot; times the probability of &amp;quot;herring.&amp;quot; The words cannot be considered as
independent variables. We take advantage of this fact to develop a set of statistical
techniques for retrieving and identifying collocations from large textual corpora.
</bodyText>
<sectionHeader confidence="0.683957" genericHeader="method">
3. Three Types of Collocations
</sectionHeader>
<bodyText confidence="0.9594135">
Collocations come in a large variety of forms. The number of words involved
as well as the way they are involved can vary a great deal. Some collocations are
</bodyText>
<footnote confidence="0.8606385">
2 This notion of cohesion should not be confused with the cohesion as defined by Halliday (Halliday
and Hasan 1976). Here we are dealing with a more lexical type of cohesion.
</footnote>
<page confidence="0.948555">
147
</page>
<note confidence="0.354069">
Computational Linguistics Volume 19, Number 1
</note>
<footnote confidence="0.675222555555556">
&amp;quot;The NYSE&apos;s composite index of all its listed common stocks rose
NUMBER* to *NUMBER*&amp;quot;
&amp;quot;On the American Stock Exchange the market value index was up
NUMBER* at *NUMBER*&amp;quot;
&amp;quot;The Dow Jones average of 30 industrials fell
NUMBER* points to *NUMBER*&amp;quot;
&amp;quot;The closely watched index had been down about *NUMBER* points in
the first hour of trading&amp;quot;
&amp;quot;The average finished the week with a net loss of *NUMBER*&amp;quot;
</footnote>
<figureCaption confidence="0.960713">
Figure 4
</figureCaption>
<subsectionHeader confidence="0.760491">
Some examples of phrasal templates.
</subsectionHeader>
<bodyText confidence="0.998402142857143">
very rigid, whereas others are very flexible. For example, a collocation such as the
one linking &amp;quot;to make&amp;quot; and &amp;quot;decision&amp;quot; can appear as &amp;quot;to make a decision,&amp;quot; &amp;quot;decisions to
be made,&amp;quot; &amp;quot;made an important decision,&amp;quot; etc. In contrast, a collocation such as &amp;quot;The New
York Stock Exchange&amp;quot; can only appear under one form; it is a very rigid collocation,
a fixed expression. We have identified three types of collocations: rigid noun phrases,
predicative relations, and phrasal templates. We discuss the three types in turn, and give
some examples of collocations.
</bodyText>
<subsectionHeader confidence="0.988358">
3.1 Predicative Relations
</subsectionHeader>
<bodyText confidence="0.9992088">
A predicative relation consists of two words repeatedly used together in a similar
syntactic relation. These lexical relations are the most flexible type of collocation. They
are hard to identify since they often correspond to interrupted word sequences in the
corpus. For example, a noun and a verb will form a predicative relation if they are
repeatedly used together with the noun as the object of the verb. &amp;quot;Make-decision&amp;quot; is a
good example of a predicative relation. Similarly, an adjective repeatedly modifying
a given noun such as &amp;quot;hostile-takeover&amp;quot; also forms a predicative relation. Examples
of automatically extracted predicative relations are given in Figure 3.3 This class of
collocations is related to Menuk&apos;s lexical functions (Mel&apos;aik 1981), and Benson&apos;s L-
type relations (Benson, Benson, and Ilson 1986b).
</bodyText>
<subsectionHeader confidence="0.994776">
3.2 Rigid Noun Phrases
</subsectionHeader>
<bodyText confidence="0.989148">
Rigid noun phrases involve uninterrupted sequences of words such as &amp;quot;stock market,&amp;quot;
&amp;quot;foreign exchange,&amp;quot; &amp;quot;New York Stock Exchange,&amp;quot; &amp;quot;The Dow Jones average of 30 industrials.&amp;quot;
They can include nouns and adjectives as well as closed class words, and are similar
to the type of collocations retrieved by Choueka (1988) and Amsler (1989). They are
the most rigid type of collocation. Examples of rigid noun phrases are:4 &amp;quot;The NYSE&apos;s
composite index of all its listed common stocks,&amp;quot; &amp;quot;The NASDAQ composite index for the over the
counter market,&amp;quot; &amp;quot;leveraged buyout,&amp;quot; &amp;quot;the gross national product,&amp;quot; &amp;quot;White House spokesman
Marlin Fitzwater.&amp;quot;
In general, rigid noun phrases cannot be broken into smaller fragments without
losing their meaning; they are lexical units in and of themselves. Moreover, they often
refer to important concepts in a domain, and several rigid noun phrases can be used to
express the same concept. In the New York Stock Exchange domain, for example, &amp;quot;The
</bodyText>
<footnote confidence="0.996781333333333">
3 In the examples, the &amp;quot;Il&amp;quot; sign represents a gap of zero, one or several words. The &amp;quot;.4.&gt;&amp;quot; sign means that
the two words can be in any order.
4 All the examples related to the stock market domain have actually been retrieved by Xtract.
</footnote>
<page confidence="0.974849">
148
</page>
<subsectionHeader confidence="0.175412">
Frank Smadja Retrieving Collocations from Text: Xtract
</subsectionHeader>
<bodyText confidence="0.9107584">
Dow industrials,&amp;quot; &amp;quot;The Dow Jones average of 30 industrial stocks,&amp;quot; &amp;quot;the Dow Jones industrial
average,&amp;quot; and &amp;quot;The Dow Jones industrials&amp;quot; represent several ways to express a single
concept. As we have seen before, these rigid noun phrases do not seem to follow any
simple construction rule, as, for example; the examples given in sentences 6-8 at the
beginning of the paper are all incorrect.
</bodyText>
<subsectionHeader confidence="0.999929">
3.3 Phrasal Templates
</subsectionHeader>
<bodyText confidence="0.998644">
Phrasal templates consist of idiomatic phrases containing one, several, or no empty
slots. They are phrase-long collocations. Figure 4 lists some examples of phrasal tem-
plates in the stock market domain. In the figure, the empty slots must be filled in by
a number (indicated by *NUMBER* in the figure). More generally, phrasal templates
specify the parts of speech of the words that can fill the empty slots. Phrasal templates
are quite representative of a given domain and are very often repeated in a rigid way
in a given sublanguage. In the domain of weather reports, for example, the sentence
&amp;quot;Temperatures indicate previous day&apos;s high and overnight low to 8 a.m.&amp;quot; is actually repeated
before each weather report.&apos;
Unlike rigid noun phrases and predicative relations, phrasal templates are specif-
ically useful for language generation. Because of their slightly idiosyncratic structure,
generating them from single words is often a very difficult task for a language gener-
ator. As pointed out by Kukich (1983), in general, their usage gives an impression of
fluency that could not be equaled with compositional generation alone.
</bodyText>
<sectionHeader confidence="0.998831" genericHeader="method">
4. Related Work
</sectionHeader>
<bodyText confidence="0.998383958333333">
There has been a recent surge of research interest in corpus-based computational lin-
guistics methods; that is, the study and elaboration of techniques using large real
text as a basis. Such techniques have various applications. Speech recognition (Bahl,
Jelinek, and Mercer 1983) and text compression (e.g., Bell, Witten, and Cleary 1989;
Guazzo 1980) have been of long-standing interest, and some new applications are
currently being investigated, such as machine translation (Brown et al. 1988), spelling
correction (Mays, Damerau, and Mercer 1990; Church and Gale 1990), parsing (Debili
1982; Hindle and Rooth 1990). As pointed out by Bell, Witten, and Cleary (1989), these
applications fall under two research paradigms: statistical approaches and lexical ap-
proaches. In the statistical approach, language is modeled as a stochastic process and
the corpus is used to estimate probabilities. In this approach, a collocation is simply
considered as a sequence of words (or n-gram) among millions of other possible se-
quences. In contrast, in the lexical approach, a collocation is an element of a dictionary
among a few thousand other lexical items. Collocations in the lexicographic meaning
are only dealt with in the lexical approach. Aside from the work we present in this
paper, most of the work carried out within the lexical approach has been done in
computer-assisted lexicography by Choueka, Klein, and Neuwitz (1983) and Church
and his colleagues (Church and Hanks 1989). Both works attempted to automatically
acquire true collocations from corpora. Our work builds on Choueka&apos;s, and has been
developed contemporarily to Church&apos;s.
Choueka, Klein, and Neuwitz (1983) proposed algorithms to automatically retrieve
idiomatic and collocational expressions. A collocation, as defined by Choueka, is a se-
quence of adjacent words that frequently appear together. In theory the sequences
can be of any length, but in actuality, they contain two to six words. In Choueka
</bodyText>
<page confidence="0.76376">
5 Taken from the daily reports transmitted daily by The Associated Press newswire.
149
</page>
<note confidence="0.359751">
Computational Linguistics Volume 19, Number 1
</note>
<bodyText confidence="0.996213868421053">
(1988), experiments performed on an 11 million—word corpus taken from the New
York Times archives are reported. Thousands of commonly used expressions such as
&amp;quot;fried chicken,&amp;quot; &amp;quot;casual sex,&amp;quot; &amp;quot;chop suey,&amp;quot; &amp;quot;home run,&amp;quot; and &amp;quot;Magic Johnson&amp;quot; were retrieved.
Choueka&apos;s methodology for handling large corpora can be considered as a first step
toward computer-aided lexicography. The work, however, has some limitations. First,
by definition, only uninterrupted sequences of words are retrieved; more flexible col-
locations such as &amp;quot;make-decision,&amp;quot; in which the two words can be separated by an
arbitrary number of words, are not dealt with. Second, these techniques simply ana-
lyze the collocations according to their observed frequency in the corpus; this makes
the results too dependent on the size of the corpus. Finally, at a more general level,
although disambiguation was originally considered as a performance task, the collo-
cations retrieved have not been used for any specific computational task.
Church and Hanks (1989) describe a different set of techniques to retrieve col-
locations. A collocation as defined in their work is a pair of correlated words. That
is, a collocation is a pair of words that appear together more often than expected.
Church et al. (1991) improve over Choueka&apos;s work as they retrieve interrupted as well
as uninterrupted sequences of words. Also, these collocations have been used by an
automatic parser in order to resolve attachment ambiguities (Hindle and Rooth 1990).
They use the notion of mutual information as defined in information theory (Shannon
1948; Fano 1961) in a manner similar to what has been used in speech recognition
(e.g., Ephraim and Rabiner 1990), or text compression (e.g., Bell, Witten, and Cleary
1989), to evaluate the correlation of common appearances of pairs of words. Their
work, however, has some limitations too. First, by definition, it can only retrieve col-
locations of length two. This limitation is intrinsic to the technique used since mu-
tual information scores are defined for two items. The second limitation is that many
collocations identified in Church and Hanks (1989) do not really identify true collo-
cations, but simply pairs of words that frequently appear together such as the pairs
&amp;quot;doctor-nurse,&amp;quot; &amp;quot;doctor-bill,&amp;quot; &amp;quot;doctor-honorary,&amp;quot; &amp;quot;doctors-dentists,&amp;quot; &amp;quot;doctors-hospitals,&amp;quot; etc.
These co-occurrences are mostly due to semantic reasons. The two words are used in
the same context because they are of related meanings; they are not part of a single
collocational construct.
The work we describe in the rest of this paper is along the same lines of research. It
builds on Choueka&apos;s work and attempts to remedy the problems identified above. The
techniques we describe retrieve the three types of collocations discussed in Section 2,
and they have been implemented in a tool, Xtract. Xtract retrieves interrupted as well
as uninterrupted sequences of words and deals with collocations of arbitrary length
(1 to 30 in actuality). The following four sections describe and discuss the techniques
used for Xtract.
</bodyText>
<sectionHeader confidence="0.912696" genericHeader="method">
5. Xtract: Introduction
</sectionHeader>
<bodyText confidence="0.9981179">
Xtract consists of a set of tools to locate words in context and make statistical observa-
tion to identify collocations. In the upgraded version we describe here, Xtract has been
extended and refined. More information is computed and an effort has been made to
extract more functional information. Xtract now works in three stages.
The three-stage analysis is described in Sections 6, 7, and 8. In the first stage,
described in Section 6, Xtract uses straight statistical measures to retrieve from a corpus
pairwise lexical relations whose common appearance within a single sentence are
correlated. A pair (or bigram) is retrieved if its frequency of occurrence is above a
certain threshold and if the words are used in relatively rigid ways. The output of
stage one is then passed to both the second and third stage in parallel. In the second
</bodyText>
<page confidence="0.991413">
150
</page>
<subsectionHeader confidence="0.256259">
Frank Smadja Retrieving Collocations from Text: Xtract
</subsectionHeader>
<bodyText confidence="0.999906727272727">
stage, described in Section 7, Xtract uses the output bigrams to produce collocations
involving more than two words (or n-grams). It analyzes all sentences containing the
bigram and the distribution of words and parts of speech for each position around the
pair. It retains words (or parts of speech) occupying a position with probability greater
than a given threshold. For example, the bigram &amp;quot;average-industrial&amp;quot; produces the n-
gram &amp;quot;the Dow Jones industrial average,&amp;quot; since the words are always used within
rigid noun phrases in the training corpus. In the third stage, described in Section 8,
Xtract adds syntactic information to collocations retrieved at the first stage and filters
out inappropriate ones. For example, if a bigram involves a noun and a verb, this
stage identifies it either as a subject-verb or as a verb-object collocation. If no such
consistent relation is observed, then the collocation is rejected.
</bodyText>
<sectionHeader confidence="0.771983" genericHeader="method">
6. Xtract Stage One: Extracting Significant Bigrams
</sectionHeader>
<bodyText confidence="0.993086368421053">
According to Cruse&apos;s definition (Cruse 1986), a syntagmatic lexical relation consists
of a pair of words whose common appearances within a single phrase structure are
correlated. In other words, those two words appear together within a single syntactic
construct more often than expected by chance. The first stage of Xtract attempts to
identify such pairwise lexical relations and produce statistical information on pairs of
words involved together in the corpus.
Ideally, in order to identify lexical relations in a corpus one would need to first
parse it to verify that the words are used in a single phrase structure. However,
in practice, free-style texts contain a great deal of nonstandard features over which
automatic parsers would fail.&apos; Fortunately, there is strong lexicographic evidence that
most syntagmatic lexical relations relate words separated by at most five other words
(Martin, Al, and Van Sterkenburg 1983). In other words, most of the lexical relations
involving a word w can be retrieved by examining the neighborhood of w, wherever
it occurs, within a span of five (-5 and +5 around w) words.&apos; In the work presented
here, we use this simplification and consider that two words co-occur if they are in a
single sentence and if there are fewer than five words between them.
In this first stage, we thus use only statistical methods to identify relevant pairs of
words. These techniques are based on the assumptions that if two words are involved
in a collocation then:
</bodyText>
<listItem confidence="0.9966495">
• the words must appear together significantly more often than expected
by chance.
• because of syntactic constraints the words should appear in a relatively
rigid way.&apos;
</listItem>
<bodyText confidence="0.9993505">
These two assumptions are used to analyze the word distributions, and we base our
filtering techniques on them.
</bodyText>
<subsectionHeader confidence="0.99997">
6.1 Presentation of the Method
</subsectionHeader>
<bodyText confidence="0.9985255">
In this stage as well as in the two others, we often need part-of-speech information for
several purposes. Stochastic part-of-speech taggers such as those in Church (1988) and
</bodyText>
<footnote confidence="0.9968594">
6 This fact is being seriously challenged by current research (e.g., Abney 1990; Hindle 1983), and might
not be true in the near future.
7 Not crossing sentence boundaries.
8 This is obviously not true for nonconfigurational languages. Although we do believe that the methods
described in this paper can be applied to many languages, we have only used them on English texts.
</footnote>
<page confidence="0.987917">
151
</page>
<note confidence="0.363199">
Computational Linguistics Volume 19, Number 1
</note>
<bodyText confidence="0.999686">
Garside and Leech (1987) have been shown to reach 95-99% performance on free-style
text. We preprocessed the corpus with a stochastic part-of-speech tagger developed at
Bell Laboratories by Ken Church (Church 1988).9
In the rest of this section, we describe the algorithm used for the first stage of
Xtract in some detail. We assume that the corpus is preprocessed by a part of speech
tagger and we note w, a collocate of w if the two words appear in a common sentence
within a distance of 5 words.
</bodyText>
<subsectionHeader confidence="0.98529">
Step 1.1: Producing Concordances
</subsectionHeader>
<bodyText confidence="0.999122090909091">
Input: The tagged corpus, a given word w.
Output: All the sentences containing w.
Description: This actually encompasses the task of identifying sentence boundaries,
and the task of selecting sentences containing w. The first task is not simple and is still
an open problem. It is not enough to look for a period followed by a blank space as,
for example, abbreviations and acronyms such as S.B.F., U.S.A., and A.T.M. often pose
a problem. The basic algorithm for isolating sentences is described and implemented
by a finite-state recognizer. Our implementation could easily be improved in many
ways. For example, it performs poorly on acronyms and often considers them as end
of sentences; giving it a list of currently used acronyms such as N.B.A., E.I.K., etc.,
would significantly improve its performance.
</bodyText>
<subsectionHeader confidence="0.972406">
Step 1.2: Compile and Sort
</subsectionHeader>
<bodyText confidence="0.997175733333333">
Input: Output of Step 1.1, i.e., a set of tagged sentences containing w.
Output: A list of words w, with frequency information on how w and w, co-occur.
This includes the raw frequency as well as the breakdown into frequencies for each
possible position. See Table 2 for example outputs.
Description: For each input sentence containing w, we make a note of its collocates
and store them along with their position relative to w, their part of speech, and their
frequency of appearance. More precisely, for each prospective lexical relation, or for
each potential collocate wi, we maintain a data structure containing this information.
The data structure is shown in Figure 5. It contains freq„ the frequency of appearance
of w, with w so far in the corpus, PP, the part of speech of wi, and pij, (-5 &lt;j &lt; 5,
j 0), the frequency of appearance of w, with w such that they are j words apart.
The p;s represent the histogram of the frequency of appearances of w and w, in given
positions. This histogram will be used in later stages.
As an example, if sentence (9) is the current input to step 1.2 and w = takeover,
then, the prospective lexical relations identified in sentence (9) are as shown in Table 3.
</bodyText>
<construct confidence="0.3922215">
9. &amp;quot; The pill would make a takeover attempt more expensive by allowing the retailer&apos;s
shareholders to . . .&amp;quot;
</construct>
<bodyText confidence="0.9699205">
In Table 3, distance is the distance between &amp;quot;takeover&amp;quot; and wi, and PP is the part
of speech of w,. The closed class words are not considered at this stage and the other
</bodyText>
<listItem confidence="0.313241">
9 We are grateful to Ken Church and to Bell Laboratories for providing us with this tool.
</listItem>
<page confidence="0.932994">
152
</page>
<figure confidence="0.801592">
Frank Smadja Retrieving Collocations from Text: Xtract
</figure>
<figureCaption confidence="0.885412">
Figure 5
</figureCaption>
<bodyText confidence="0.892171">
Data structure maintained at stage one by Xtract.
words, such as &amp;quot;shareholders,&amp;quot; are rejected because they are more than five words away
from &amp;quot;takeover.&amp;quot; For each of the above word pairs, we maintain the associated data
structure as indicated in Figure 5. For takeover pill, for example, we would increment
freqpin, and the p4 column in the histogram. Table 2 shows the output for the adjective
collocates of the word &amp;quot;takeover.&amp;quot;
</bodyText>
<subsectionHeader confidence="0.974984">
Step 1.3: Analyze
</subsectionHeader>
<bodyText confidence="0.992804466666667">
Input: Output of Step 1.2, i.e., a list of words w, with information on how often and
how w and wz co-occur. See Table 2 for an example input.
Output: Significant word pairs, along with some statistical information describing how
strongly the words are connected and how rigidly they are used together. A separate
(but similar) statistical analysis is done for each syntactic category of collocates. See
Table 4 for an example output.
Description: At this step, the statistical distribution of the collocates of w is analyzed,
and the interesting word pairs are automatically selected. If part of speech information
is available, a separate analysis is made depending on the part of speech of the collo-
cates. This balances the fact that verbs, adjectives, and nouns are simply not equally
frequent.
For each word w, we first analyze the distribution of the frequencies freq, of its
collocates wi, and then compute its average frequency f and standard deviation a
around f. We then replace freq, by its associated z-score k,. k, is called the strength of
the word pair in Figure 4; it represents the number of standard deviation above the
</bodyText>
<figure confidence="0.58552075">
(w, wi) freqi, P131
(w,w2) freq2,PP2
V2
freq,, PP,
Freq, PP
f, g
P-5 P-4 P-3 p-2 p-1 pi P2 P3 P4 P5
(w, wi)
bigram
vi
153
Computational Linguistics Volume 19, Number 1
</figure>
<tableCaption confidence="0.9040025">
Table 2
Output of stage 1, step 3. Noun-adjective associations.
</tableCaption>
<table confidence="0.955032">
w w, Freq p-5 P-4 P-3 p-2 p-1 pl P2 P3 P4 P5
takeover possible 178 0 13 4 23 138 0 0 0 0 0
takeover corporate 93 2 2 2 1 63 3 2 9 4 5
takeover unsolicited 83 5 30 5 0 42 0 0 1 0 0
takeover several 81 2 6 6 6 45 0 0 12 0 4
takeover recent 76 5 4 6 5 17 0 0 36 2 1
takeover new 75 4 3 6 28 27 0 1 4 2 0
takeover unwanted 53 5 0 0 2 46 0 0 0 0 0
takeover expensive 52 1 0 0 0 2 0 23 23 3 0
takeover potential 50 1 0 1 3 42 0 0 0 2 1
takeover big 47 0 0 0 4 15 0 0 5 21 2
takeover friendly 41 0 3 3 1 25 0 0 2 3 4
takeover unsuccessful 40 0 1 5 6 27 0 0 0 0 1
takeover biggest 35 1 2 1 4 20 0 0 0 5 2
takeover largest 32 0 1 3 20 3 0 0 0 0 5
takeover old 28 0 8 6 0 14 0 0 0 0 0
takeover unfriendly 26 0 0 0 0 18 0 0 0 0 8
takeover rival 26 0 1 3 0 3 0 8 5 5 1
takeover inadequate 26 5 10 2 0 0 0 0 9 0 0
takeover initial 25 0 6 0 0 13 0 0 4 0 2
takeover unwelcome 24 4 0 0 0 20 0 0 0 0 0
takeover previous 24 0 2 0 4 18 0 0 0 0 0
takeover federal 22 4 2 2 0 0 0 2 2 8 2
takeover bitter 22 0 0 0 7 14 0 0 0 1 0
takeover strong 19 0 4 3 5 4 0 0 1 0 2
takeover hostile 16 0 6 0 0 10 0 0 0 0 0
takeover attractive 16 1 0 5 3 7 0 0 0 0 0
takeover unfair 13 0 0 0 0 13 0 0 0 0 0
</table>
<tableCaption confidence="0.999168">
Table 3
</tableCaption>
<bodyText confidence="0.963838714285714">
The collocates of &amp;quot;takeover&amp;quot; as retrieved
from sentence (9).
w w, distance PP
takeover pill 4 N
takeover make 2 V
takeover attempt -1 N
takeover expensive -3 J
takeover allowing -5 V
average of the frequency of the word pair w and w, and is defined as:
k frech f
a (la)
Then, we analyze the distribution of the /Ifs and produce their average pi and variance
U, around j3j. In Figure 4 spread represents U, on a scale of 1 to 100. 1/, characterizes
the shape of the pi, histogram. If U, is small, then the histogram will tend to be flat,
</bodyText>
<page confidence="0.997957">
154
</page>
<author confidence="0.214506">
Frank Smadja Retrieving Collocations from Text: Xtract
</author>
<bodyText confidence="0.84023">
which means that w, can be used equivalently in almost any position around w. In
contrast, if U, is large, then the histogram will tend to have peaks, which means that
w, can only be used in one (or several) specific position around w. LI, is defined by:
</bodyText>
<sectionHeader confidence="0.49709" genericHeader="method">
10 (lb)
</sectionHeader>
<bodyText confidence="0.999891785714286">
These analyses are then used to sort out the retrieved data. First, using (1a), collo-
cates with strength smaller than a given threshold 1(0 are eliminated. Then, using (1b),
we filter out the collocates having a variance Li, smaller than a given threshold Uo.
Finally, we keep the interesting collocates by pulling out the peaks of the 13&apos;1 distri-
butions. These peaks correspond to the js such that the z-score of p&apos; is bigger than a
given threshold 1c1. These thresholds have to be determined by the experimenter and
are dependent on the use of the retrieved collocations. As described in Smadja (1991),
for language generation we found that (lco, k1, Lio) = (1, I, 10) gave good results, but for
other tasks different thresholds might be preferable. In general, the lower the thresh-
old the more data are accepted, the higher the recall, and the lower the precision of
the results. Section 10 describes an evaluation of the results produced with the above
thresholds.
More formally, a peak, or lexical relation containing w, at this point is defined as
a tuple (wi, distance, strength, spread , j) verifying the following set of inequalities:
</bodyText>
<equation confidence="0.984258666666667">
{strength = freq, f &gt; k0 (C1) }
(C) spread &gt; 1-Io (C2)
PI ffi + (k1 x VT:fi) (c3)
</equation>
<bodyText confidence="0.993734833333333">
Some example results are given in Table 4.
As shown in Smadja (1991), the whole first stage of Xtract as described above can
be performed in 0(S log S) time, in which S is the size of the corpus. The third step of
counting frequencies and maintaining the data structure dominates the whole process
and as pointed out by Ken Church (personal communication), it can be reduced to a
sorting problem.
</bodyText>
<subsectionHeader confidence="0.999962">
6.2 What Exactly Is Filtered Out?
</subsectionHeader>
<bodyText confidence="0.961671545454546">
The inequality set (C) is used to filter out irrelevant data, that is pairs of words sup-
posedly not used consistently within a single syntactic structure. This section discusses
the importance of each inequality in (C) on the filtering process.
strength = freq — f &gt; ko (CO
Condition (C1) helps eliminate the collocates that are not frequent enough. This con-
dition specifies that the frequency of appearance of w, in the neighborhood of w must
be at least one standard deviation above the average. In most statistical distributions,
this thresholding eliminates the vast majority of the lexical relations. For example,
for w = &amp;quot;takeover,&amp;quot; among the 3385 possible collocates only 167 were selected, which
gives a proportion of 95% rejected. In the case of the standard normal distribution, this
would reject some 68% of the cases. This indicates that the actual distribution of the
</bodyText>
<page confidence="0.995694">
155
</page>
<table confidence="0.402917">
Computational Linguistics Volume 19, Number 1
</table>
<tableCaption confidence="0.996771">
Table 4
</tableCaption>
<table confidence="0.86117875">
Output of stage 1, step 4.
w, w) distance strength spread
hostile takeovers 1 13 97
hostile takeover 1 13 90
corporate takeovers 1 8 90
possible takeover 1 6 73
hostile takeovers 2 2 70
corporate takeover 1 3 63
unwanted takeover 1 1 83
potential takeover 1 1 80
several takeover 1 2 50
unsolicited takeover 1 2 53
his takeover 1 3 44
unsuccessful takeover 1 1 63
takeover recent 3 2 46
unsolicited takeover 4 2 53
takeover last 2 2 46
friendly takeover 1 1 60
takeover expensive 3 1 60
takeover expensive 2 1 60
new takeover 2 2 46
new takeover 1 2 46
takeover big 4 1 47
takeovers other 2 1 43
big takeover 1 1 46
takeovers major 4 1 46
biggest takeover 1 .93 53
largest takeover 2 .82 60
</table>
<bodyText confidence="0.9997374">
collocates of &amp;quot;takeover&amp;quot; has a large kurtosis.&apos; Among the eliminated collocates were
&amp;quot;dormant, dilute, ex., defunct,&amp;quot; which obviously are not typical of a takeover. Although
these rejected collocations might be useful for applications such as speech recognition,
for example, we do not consider them any further here. We are looking for recurrent
combinations and not casual ones.
</bodyText>
<equation confidence="0.346287">
spread &gt; Lb o (C2)
</equation>
<bodyText confidence="0.963225083333333">
Condition (C2) requires that the histogram of the 10 relative frequencies of appearance
of w, within five words of w (or pis) have at least one spike. If the histogram is
flat, it will be rejected by this condition. For example, in Figure 5, the histogram
associated with w2 would be rejected, whereas the one associated with wl or w, would
be accepted. In Table 2, the histogram for &amp;quot;takeover-possible&amp;quot; is clearly accepted (there
is a spike for 19_0, whereas the one for &amp;quot;takeover-federal&amp;quot; is rejected. The assumption
here is that, if the two words are repeatedly used together within a single syntactic
construct, then they will have a marked pattern of co-appearance, i.e., they will not
appear in all the possible positions with an equal probability This actually eliminates
pairs such as &amp;quot;telephone-television,&amp;quot; &amp;quot;bomb-soldier,&amp;quot; &amp;quot;trouble-problem,&amp;quot; &amp;quot;big-small,&amp;quot; and
10 The kurtosis of the distribution of the collocates probably depends on the word, and there is currently
no agreement on the type of distribution that would describe them.
</bodyText>
<page confidence="0.986976">
156
</page>
<author confidence="0.230943">
Frank Smadja Retrieving Collocations from Text: Xtract
</author>
<bodyText confidence="0.931272029411765">
&amp;quot;doctor-nurse&amp;quot; where the two words co-occur with no real structural consistency. The
two words are often used together because they are associated with the same context
rather than for pure structural reasons. Many collocations retrieved in Church and
Hanks (1989) were of this type, as they retrieved doctors-dentists, doctors-nurses, doctor-
bills, doctors-hospitals, nurses-doctor, etc., which are not collocations in the sense defined
above. Such collocations are not of interest for our purpose, although they could be
useful for disambiguation or other semantic purposes. Condition (C2) filters out exactly
this type of collocations.
pi, ?pi + (ki x iff,) (c3)
Condition (C3) pulls out the interesting relative positions of the two words. Conditions
(C2) and (C1) eliminate rows in the output of Step 1.2. (See Figure 2). In contrast,
Condition (C3) selects columns from the remaining rows. For each pair of words,
one or several positions might be favored and thus result in several /9&apos;1 selected. For
example, the pair &amp;quot;expensive-takeover&amp;quot; produced two different peaks, one with only one
word in between &amp;quot;expensive&amp;quot; and &amp;quot;takeover,&amp;quot; and the other with two words. Example
sentences containing the two words in the two possible positions are:
&amp;quot; The provision is aimed at making a hostile takeover prohibitively expensive by
enabling Borg Warner&apos;s stockholders to buy the . . . &amp;quot;
&amp;quot;The pill would make a takeover attempt more expensive by allowing the
retailer&apos;s shareholders to buy more company stock . . . &amp;quot;
Let us note that this filtering method is an original contribution of our work.
Other works such as Church and Hanks (1989) simply focus on an evaluation of the
correlation of appearance of a pair of words, which is roughly equivalent to condition
(C1). (See next section). However, taking note of their pattern of appearance allows us
to filter out more irrelevant collocations with (C2) and (C3). This is a very important
point that will allow us to filter out many invalid collocations and also produce more
functional information at stages 2 and 3. A graphical interpretation of the filtering
method used for Xtract is given in Smadja (1991).
7. Xtract Stage Two: From 2-Grams to N-Grams
The role of the second stage of Xtract is twofold. It produces collocations involving
more than two words, and it filters out some pairwise relations. Stage 2 is related to
the work of Choueka (1988), and to some extent to what has been done in speech
recognition (e.g., Bahl, Jelinek, and Mercer 1983; Merialdo 1987; Ephraim and Rabiner
1990).
</bodyText>
<subsectionHeader confidence="0.999646">
7.1 Presentation of the Method
</subsectionHeader>
<bodyText confidence="0.999581111111111">
In this second stage, Xtract uses the same components used for the first stage but in
a different way. It starts with the pairwise lexical relations produced in stage 1 and
produces multiple word collocations, such as rigid noun phrases or phrasal templates,
from them. To do this, Xtract studies the lexical relations in context, which is exactly
what lexicographers do. For each bigram identified at the previous stage, Xtract ex-
amines all instances of appearance of the two words and analyzes the distributions of
words and parts of speech in the surrounding positions.
Input: Output of Stage 1. Similar to Table 4, i.e., a list of bigrams with their statistical
information as computed in stage 1.
</bodyText>
<page confidence="0.965715">
157
</page>
<table confidence="0.397842">
Computational Linguistics Volume 19, Number 1
Output: Sequences of words and parts of speech. See Figure 8.
Stage 2 has three steps:
Step 2.1: Produce Concordances
</table>
<bodyText confidence="0.8951384">
Identical to Stage 1, Step 1.1. Given a pair of words w and wi, and an integer specifying
the distance of the two words.11 This step produces all the sentences containing them
in the given position. For example, given the bigram takeover-thwart and the distance
2, this step produces sentences like:
&amp;quot;Under the recapitalization plan it proposed to thwart the takeover.&amp;quot;
</bodyText>
<subsectionHeader confidence="0.848477">
Step 2.2: Compile and Sort
</subsectionHeader>
<bodyText confidence="0.9884705">
Identical to Stage 1, Step 1.2. We compute the frequency of appearance of each of the
collocates of w by maintaining a data structure similar to the one given in Figure 5,
</bodyText>
<subsectionHeader confidence="0.824988">
Step 2.3: Analyze and Filter
</subsectionHeader>
<bodyText confidence="0.97608925">
Input: Output of Step 2.2.
Output: N-grams such as in Figure 8.
Discussion: Here, the analyses are simpler than for Stage 1. We are only interested in
percentage frequencies and we only compute the moment of order 1 of the frequency
distributions.
Tables produced in Step 2.2 (such as in Figure 5) are used to compute the frequency
of appearance of each word in each position around w. For each of the possible relative
distances from w, we analyze the distribution of the words and only keep the words
occupying the position with a probability greater than a given threshold T.12 If part
of speech information is available, the same analysis is also performed with parts of
speech instead of actual words. In short, a word w or a part of speech pos is kept in
the final n-gram at position i if and only if it satisfies the following inequation:
</bodyText>
<equation confidence="0.933933">
P(word [i] = wo) &gt; T (4a)
</equation>
<bodyText confidence="0.90463">
p(e) denotes the probability of event e. Consider the examples given in Figures 6
and 7 that show the concordances (output of step 2.1) for the input pairs: &amp;quot;average-
industrial&amp;quot; and &amp;quot;index-composite.&amp;quot;
In Figure 6, the same words are always used from position —4 to position 0.
However, at position +1, the words used are always different. &amp;quot;Dow&amp;quot; is used at position
—3 in more than 90% of the cases. It is thus part of the produced rigid noun phrases.
But &amp;quot;down&amp;quot; is only used a couple of times (out of several hundred) at position +1,
11 The distance is actually optional and can be given in various ways. We can specify the word order, the
maximum distance, the exact distance, etc.
</bodyText>
<footnote confidence="0.994538">
12 This threshold must also be determined by the experimenter. In the following we use T = 0.75. As
discussed previously, the choice of the threshold is arbitrary, and the general rule is that the lower the
threshold, the higher the recall and the lower the precision of the results. The choice of 0.75 is based on
the manual observations of several samples and it has effected the overall results, as discussed in
Section 10.
</footnote>
<page confidence="0.97577">
158
</page>
<figure confidence="0.893732">
Frank Smadja Retrieving Collocations from Text: Xtract
</figure>
<figureCaption confidence="0.98703">
Figure 6
Producing: &amp;quot;the Dow Jones industrial average&amp;quot;
Figure 7
</figureCaption>
<subsubsectionHeader confidence="0.362527">
Producing: &amp;quot;the NYSE&apos;s composite index of all its listed common stocks&amp;quot;
</subsubsectionHeader>
<bodyText confidence="0.780031">
and will not be part of the produced rigid noun phrases. From those concordances,
Xtract produced the five-word rigid noun phrases: &amp;quot;The Dow Jones Industrial Average.&amp;quot;
Figure 7 shows that from position —3 to position +7 the words used are always
the same. In all the example sentences in which &amp;quot;composite&amp;quot; and &amp;quot;index&amp;quot; are adjacent,
the two words are used within a bigger construct of 11 words (also called an 11-gram).
However, if we look at position +8 for example, we see that although the words used
are different, in all the cases they are verbs. Thus, after the 11-gram we expect to find
a verb. In short, Figure 7 helps us produce both the rigid noun phrases &amp;quot;The NYSE&apos;s
composite index of all its listed common stocks,&amp;quot; as well as the phrasal template &amp;quot;The NYSE&apos;s
composite index of all its listed common stocks *VERB* *NUMBER* to *NUMBER*.&amp;quot;
Figure 8 shows some sample phrasal templates and rigid noun phrases that were
produced at this stage. The leftmost column gives the input lexical relations. Some
other examples are given in Figure 3.
</bodyText>
<subsectionHeader confidence="0.604434">
7.2 Discussion
</subsectionHeader>
<bodyText confidence="0.997261333333333">
The role of stage 2 is to filter out many lexical relations and replace them by valid
ones. It produces both phrasal templates and rigid noun phrases. For example, asso-
ciations such as &amp;quot;blue-stocks, &amp;quot; &amp;quot;air-controller,&amp;quot; or &amp;quot;advancing-market&amp;quot; were filtered out
</bodyText>
<note confidence="0.361128">
Concordances for: &amp;quot;average&amp;quot; &amp;quot;industrial&amp;quot;
</note>
<bodyText confidence="0.810971">
• Tuesday the Dow Jones industrial average
The Dow Jones industrial average
... that sent the Dow Jones industrial average
Monday the Dow Jones industrial average
The Dow Jones industrial average
... in the Dow Jones industrial average
***
rose 26.28 points to 2 304.69.
</bodyText>
<figure confidence="0.721689857142857">
went up 11.36 points today.
down sharply ...
showed some strength as ...
was down 17.33 points to 2,287.36 ...
was the biggest since ...
&amp;quot;the Dow Jones industrial average&amp;quot;
Concordances for &amp;quot;composite index&amp;quot;
</figure>
<bodyText confidence="0.994563222222222">
The NYSE s composite index
The NYSE s composite index
The NYSE s composite index
The NYSE s composite index
The NYSE s composite index
The NYSE s composite index
The NYSE s composite index
The NYSE s composite index
The NYSE s composite index
</bodyText>
<figureCaption confidence="0.997992777777778">
of all its listed common stocks fell 1.76 to 164.13.
of all its listed common stocks fell 0.98 to 164.91.
of all its listed common stocks fell 0.96 to 164.93.
of all its listed common stocks fell 0.91 to 164.98.
of all its listed common stocks rose 1.04 to 167.08.
of all its listed common stocks rose 0.76
of all its listed common stocks rose 0.50 to 166.54.
of all its listed common stocks rose 0.69 to 166.73.
of all its listed common stocks fell 0.33 to 170.63.
</figureCaption>
<bodyText confidence="0.702324">
&amp;quot;the NYSE&apos;s composite index of all its listed common stocks
</bodyText>
<page confidence="0.935979">
159
</page>
<table confidence="0.781468357142857">
Computational Linguistics Volume 19, Number 1
lexical relation collocation
composite-index &amp;quot;The NYSE&apos;s composite index of all its listed common
stocks fell *NUMBER* to *NUMBER*&amp;quot;
composite-index &amp;quot;the NYSE&apos;s composite index of all its listed common
stocks rose *NUMBER* to *NUMBER*.&amp;quot;
&amp;quot;close-industrial&amp;quot; &amp;quot;Five minutes before the close the Dow Jones average of 30 industrials
was up/down *NUMBER* to/from *NUMBER*&amp;quot;
&amp;quot;average industrial&amp;quot; the Dow Jones industrial average.&amp;quot;
&amp;quot;advancing-market&amp;quot; &amp;quot;the broader market in the NYSE advancing issues&amp;quot;
&amp;quot;block-trading&amp;quot; &amp;quot;Jack Baker head of block trading in Shearson Lehman Brothers Inc.&amp;quot;
&amp;quot;blue-stocks&amp;quot; &amp;quot;blue chip stocks&amp;quot;
&amp;quot;cable-television&amp;quot; &amp;quot;cable television&amp;quot;
&amp;quot;consumer index&amp;quot; &amp;quot;The consumer price index&amp;quot;
</table>
<figureCaption confidence="0.80518">
Figure 8
Example output collocations of stage two.
</figureCaption>
<bodyText confidence="0.9882672">
and respectively replaced by: &amp;quot;blue chip stocks,&amp;quot; &amp;quot;air traffic controllers,&amp;quot; and &amp;quot;the broader
market in the NYSE advancing issues.&amp;quot;
Thus stage 2 produces n-word collocations from two-word associations. Producing
n-word collocations has already been done (e.g., Choueka 1988).&amp;quot; The general method
used by Choueka is the following: for each length n, (1 &lt;n &lt;6), produce all the word
sequences of length n and sort them by frequency. On a 12 million—word corpus,
Choueka retrieved 10 collocations of length six, 115 collocations of length five, 1,024
collocations of length four, 4,777 of length three, and some 15,973 of length two. The
threshold imposed was 14. The method we presented in this section has three main
advantages when compared to a straight n-gram method like Choueka&apos;s.
</bodyText>
<listItem confidence="0.873974904761905">
1. Stage 2 retrieves phrasal templates in addition to simple rigid noun
phrases. Using part of speech information, we allow categories and
words in our templates, thus retrieving a more flexible type of
collocation. It is not clear how simple n-gram techniques could be
adapted to obtain the same results.
2. Stage 2 gets rid of subsumed m-grams of a given n-gram (m &lt;n). Since
stage 2 works from bigrams, and produces the biggest n-gram containing
it, there is no m-gram (m &lt;n) produced that is subsumed by it. For
example, although &amp;quot;shipments of arms to Iran&amp;quot; is a collocation of length
five, &amp;quot;arms to Iran&amp;quot; is not an interesting collocation. It is not opaque, and
does not constitute a modifier—modified syntactic relation. A straight
n-gram method would retrieve both, as well as many other subsumed
m-grams, such as &amp;quot;of arms to Iran.&amp;quot; A sophisticated filtering method
would then be necessary to eliminate the invalid ones (See Choueka
1988). Our method avoids this problem and only produces the biggest
possible n-gram, namely: &amp;quot;shipment of arms to Iran.&amp;quot;
3. Stage 2 is a simple way of compiling n-gram data. Retrieving an 11-gram
by the methods used in speech, for example, would require a great deal
13 Similar approaches have been done for several applications such as Bahl, Jelinek, and Mercer (1983) and
Cerf-Danon et al. (1989) for speech recognition, and Morris and Cherry (1975), Angell (1983), Kukich
(1990), and Mays, Damerau, and Mercer (1990) for spelling correction (with letters instead of words).
</listItem>
<page confidence="0.958787">
160
</page>
<note confidence="0.224486">
Frank Smadja Retrieving Collocations from Text: Xtract
</note>
<bodyText confidence="0.999291390243902">
of CPU time and space. In a 10 million—word corpus, with about 60,000
different words, there are about 3.6 x 109 possible bigrams, 2.16 x 1014
trigrams, and 3 x 1033 7-grams. This rapidly gets out of hand. Choueka,
for example, had to stop at length six. In contrast, the rigid noun phrases
we retrieve are of arbitrary length and are retrieved very easily and in
one pass. The method we use starts from bigrams and produces the
biggest possible subsuming n-gram. It is based on the fact that if an
n-gram is statistically significant, then the included bigrams must also be
significant. For example, to identify &amp;quot;The Dow Jones average of 30
industrials,&amp;quot; a traditional n-gram method would compare it to the other
7-grams and determine that it is significant. In contrast, we start from an
included significant bigram (for example, &amp;quot;Dow-30&amp;quot;) and we directly
retrieve the surrounding n-grams.&apos;
8. Xtract Stage Three: Adding Syntax to the Collocations
The collocations as produced in the previous stages are already useful for lexicography.
For computational use, however, functional information is needed. For example, the
collocations should have some syntactic properties. It is not enough to say that &amp;quot;make&amp;quot;
goes with &amp;quot;decision&amp;quot;; we need to know that &amp;quot;decision&amp;quot; is used as the direct object of
the verb.
The advent of robust parsers such as Cass (Abney 1990) and Fidditch (Hindle
1983) has made it possible to process large text corpora with good performance and
thus combine statistical techniques with more symbolic analysis. In the past, some
similar attempts have been done. Debili (1982) parsed corpora of French texts to iden-
tify nonambiguous predicate argument relations. He then used these relations for
disambiguation. Hindle and Rooth (1990) later refined this approach by using bigram
statistics to enhance the task of prepositional phrase attachment. Church et al. (1989,
1991) have yet another approach; they consider questions such as what does a boat typ-
ically do? They are preprocessing a corpus with the Fidditch parser (Hindle 1983) in
order to produce a list of verbs that are most likely associated with the subject &amp;quot;boat.&amp;quot;
Our goal here is different, as we analyze collocations automatically produced by
the first stage of Xtract to either add syntactic information or reject them. For example,
if a lexical relation identified at stage 1 involves a noun and a verb, the role of stage
3 is to determine whether it is a subject—verb or a verb—object collocation. If no such
consistent relation is observed, then the collocation is rejected. Stage 3 uses a parser
but it does not require a complete parse tree. Given a number of sentences, Xtract only
needs to know pairwise syntactic (modifier—modified) relations. The parser we used
in the experiment reported here is Cass (Abney 1989, 1990), a bottom-up incremental
parser. Cass&apos; takes input sentences labeled with part of speech and attempts to identify
syntactic structure. One of the subtasks performed by Cass is to identify predicate
argument relations, and this is the task we are interested in here. Stage 3 works in the
following three steps.
</bodyText>
<footnote confidence="0.9527896">
14 Actually, this 7-gram could be retrieved several times, one for each pair of open class word it contains.
But a simple sorting algorithm gets rid of such repetitions.
15 The parser developed at Bell Communication Research by Steve Abney, Cass stands for Cascaded
Analysis of Syntactic Structure. We are grateful to Steve for helping us with the use of Cass and
customizing its output for us.
</footnote>
<page confidence="0.977922">
161
</page>
<table confidence="0.854568">
Computational Linguistics Volume 19, Number 1
label bigram label bigram
0&gt;ZZzz faced test VO awaited signs
investors awaited SV Street faced
year market NN week selloff
stock traders NN bull market
old market JN major test
last selloff JN epic selloff
</table>
<figureCaption confidence="0.783117">
Figure 9
</figureCaption>
<bodyText confidence="0.616657">
All the syntactic labels produced by Cass on sentence (10).
</bodyText>
<subsectionHeader confidence="0.533177">
Step 3.1: Produce Tagged Concordances
</subsectionHeader>
<bodyText confidence="0.985944333333333">
Identical to what we did at Stage 2, Step 2.1. Given a pair of words w and w,, a
distance of the two words (optional), and a tagged corpus, Xtract produces all the
(tagged) sentences containing them in the given position specified by the distance.
</bodyText>
<subsectionHeader confidence="0.887673">
Step 3.2: Parse
</subsectionHeader>
<bodyText confidence="0.862743222222222">
Input: Output of Step 3.1. A set of tagged sentences each containing both w and w,.
Output: For each sentence, a set of syntactic labels such as those shown in Figure 9.
Discussion: Cass is called on the concordances. From Cass output, we only retrieve
binary syntactic relations (or labels) such as &amp;quot;verb—object&amp;quot; or &amp;quot;verb—subject,&amp;quot; &amp;quot;noun—
adjective,&amp;quot; and &amp;quot;noun—noun.&amp;quot; To simplify, we abbreviate them respectively: VO, SV, NJ,
NN. For sentence (10) below, for example, the labels produced are shown in Figure 9.
10. &amp;quot;Wall Street faced a major test with stock traders returning to action for the first
time since last week&apos;s epic selloff and investors awaited signs of life from the
5-year-old bull market.&amp;quot;
</bodyText>
<subsectionHeader confidence="0.937017">
Step 3.3: Label Sentences
</subsectionHeader>
<bodyText confidence="0.997664692307692">
Input: A set of sentences each associated with a set of labels as shown in Figure 9.
Output: Collocations with associated syntactic labels as shown in Figure 10.
Discussion: For any given sentence containing both w and w,, two cases are possible:
either there is a label for the bigram (w, w,), or there is none. For example, for sen-
tence (10), there is a syntactic label for the bigram faced-test, but there is none for the
bigram stock-returning. Faced-test enters into a verb object relation, and stock-returning
does not enter into any type of relation. If no label is retrieved for the bigram, it
means that the parser could not identify a relation between the two words. In this
case we introduce a new label: U (for undefined) to label the bigram. At this point,
we associate with the sentence the label for the bigram (w, w1). With each of the input
sentences, we associate a label for the bigram (w, w1). For example, the label associated
with sentence (10) for the bigram faced-test would be VO. A list of labeled sentences
for the bigram w = &amp;quot;rose&amp;quot; and w = &amp;quot;prices&amp;quot; is shown in Figure 10.
</bodyText>
<page confidence="0.979598">
162
</page>
<figure confidence="0.336049">
Frank Smadja Retrieving Collocations from Text: Xtract
Some Concordances for (rose, prices) label
... when they rose pork prices 1.1 percent ... VO
Analysts said stock prices rose because of a rally in Treasury bonds. SV
Bond prices rose because many traders took the report as a signal ... SV
Stock prices rose in moderate trading today with little news ... SV
Bond prices rose in quiet trading SV
Stock prices rose sharply Friday in response to a rally in ... SV
... soft drink prices rose 0.5 percent ... SV
Stock prices rose broadly in early trading today as a rising dollar ... SV
</figure>
<figureCaption confidence="0.743169">
Figure 10
</figureCaption>
<bodyText confidence="0.883245">
Producing the &amp;quot;prices f] rose,&amp;quot; SV predicative relation at stage 3.
</bodyText>
<subsectionHeader confidence="0.908085">
Step 3.4: Filter and Label Collocation
</subsectionHeader>
<bodyText confidence="0.996456111111111">
Input: A set of sentences containing w and w, each associated with a label as shown
in Figure 10.
Output: Labeled collocations as shown in Figure 11.
Discussion on Step 3.4: At this step, we count the frequencies of each possible label
identified for the bigram (w, w,) and perform a statistical analysis of order two for this
distribution. We compute the average frequency for the distribution of labels: ft and
the standard deviation a-t. We finally apply a filtering method similar to (C2). Let t be
a possible label. We keep t if and only if it satisfies inequality (4b) similar to (4a) given
before:
</bodyText>
<equation confidence="0.995185">
p(label[i] = t) &gt; T (4b)
</equation>
<bodyText confidence="0.999583315789474">
A collocation is thus accepted if and only if it has a label g satisfying inequality
(4b), and g U. Similarly, a collocation is rejected if no label satisfies inequality (4b)
or if U satisfies it.
Figure 10 shows part of the output of Step 3.3 for w = &amp;quot;rose&amp;quot; and w, = &amp;quot;prices.&amp;quot; As
shown in the figure, SV labels are a large majority. Thus, we would label the relation
price-rose as an SV relation. An example output of this stage is given in Figure 11.
The bigrams labeled U were rejected at this stage.
Stage 3 thus produces very useful results. It filters out collocations and rejects
more than half of them, thus improving the quality of the results. It also labels the
collocations it accepts, thus producing a more functional and usable type of knowledge.
For example, if the first two stages of Xtract produce the collocation &amp;quot;make-decision,&amp;quot;
the third stage identifies it as a verb—object collocation. If no such relation can be
observed, then the collocation is rejected. The produced collocations are not simple
word associations but complex syntactic structures. Labeling and filtering are two
useful tasks for automatic use of collocations as well as for lexicography. The whole
of stage 3 (both as a filter and as a labeler) is an original contribution of our work.
Retrieving syntactically labeled collocations is a relatively new concern. Moreover,
filtering greatly improves the quality of the results. This is also a possible use of the
emerging new parsing technology.
</bodyText>
<subsectionHeader confidence="0.947565">
8.1 Xtract: The Toolkit
</subsectionHeader>
<bodyText confidence="0.980823">
Xtract is actually a library of tools implemented using standard C-Unix libraries. The
toolkit has several utilities useful for analyzing corpora. Without making any effort
</bodyText>
<page confidence="0.997426">
163
</page>
<table confidence="0.996432083333333">
Computational Linguistics Volume 19, Number 1
w wi label w wi label
savings ailing U securities dealer U
savings appears U denominated securities VO
savings continue U securities firms NN
savings dip U securities fixed U
savings dipped U securities fraud NN
savings failing U securities industry NN
savings fell SV securities law NN
manufacturing sector NN securities lawmakers U
manufacturing sector NN securities lawyer NN
securities business NN securities lawyers NN
</table>
<figureCaption confidence="0.766121">
Figure 11
</figureCaption>
<subsectionHeader confidence="0.646852">
Some examples of syntactically labeled bigrams.
</subsectionHeader>
<bodyText confidence="0.998776923076923">
to make Xtract efficient in terms of computing resources, the first stage as well as the
second stage of Xtract only takes a few minutes to run on a ten-megabyte (pre-tagged)
corpus. Xtract is currently being used at Columbia University for various lexical tasks.
And it has been tested on many corpora, among them: several ten-megabyte corpora
of news stories, a corpus, consisting of some twenty megabytes of New York Times
articles, which has already been used by Choueka (1988), the Brown corpus (Francis
and Ktfaera 1982), a corpus of the proceedings of the Canadian Parliament, also called
the Hansards corpus, which amounts to several hundred megabytes. We are currently
working on packaging Xtract to make it available to the research community. The
packaged version will be portable, reusable, and faster than the one we used to write
this paper.&apos;
We evaluate the filtering power of stage 3 in the evaluation section, Section 10.
Section 9 presents some results that we obtained with the three stages of Xtract.
</bodyText>
<sectionHeader confidence="0.5379" genericHeader="method">
9. Some Results
</sectionHeader>
<bodyText confidence="0.9979132">
Results obtained from The Jerusalem Post corpus have already been reported (e.g.,
Smadja 1991). Figure 12 gives some results for the three-stage process of Xtract on
a 10 million—word corpus of stock market reports taken from the Associated Press
newswire. The collocations are given in the following format. The first line contains
the bigrams with the distance, so that &amp;quot;sales fell —1&amp;quot; says that the two words under
consideration are &amp;quot;sales&amp;quot; and &amp;quot;fell,&amp;quot; and that the distance we are considering is —1.
The first line is thus the output of stage 1. The second line gives the output of stage
2, i.e., the n-grams. For example, &amp;quot;takeover-thwart&amp;quot; is retrieved as &amp;quot;44 to thwart AT
takeover NN &amp;quot;AT stands for article, NN stands for nouns, and 44 is the number
of times this collocation has been retrieved in the corpus. The third line gives the
retrieved tags for this collocation, so that the syntactic relation between &amp;quot;takeover&amp;quot; and
&amp;quot;thwart&amp;quot; is an SV relation. And finally, the last line is an example sentence containing
the collocation. Output of the type of Figure 12 is automatically produced. This kind
of output is about as far as we have gone automatically. Any further analysis and/or
use of the collocations would probably require some manual intervention.
</bodyText>
<page confidence="0.879067">
16 Please contact the author if you are interested in getting a copy of the software.
164
</page>
<figure confidence="0.955461725">
Frank Smadja Retrieving Collocations from Text: Xtract
sales fell -1
158 sales fell . . . . 158
TAG: SV
34
New home sales fell 2.7 percent in February following an 8.6 percent drop in January
the Commerce Department reported.
study said -1
40 AT study said 40
TAG: SV
56
A private study said Americans are eating about the same amount of red meat they did
four years ago.
sense makes 1
26 makes sense. . . . 26
TAG: VO
20 19
Murray Drabkin of Washington lawyer for the Dalkon Shield claimants committee said
now that Robins has agreed it makes sense to sell the company we are finally down
to the real questions How much will the company bring in the open market and
how much of that amount will the claimants
allow to go to shareholders?
steps take 1
75 take steps TO VB 75
TAG: VO
15 14
Officials also are hopeful that individual nations particularly West Germany and Japan will
take steps to stimulate their own economies.
takeover thwart 2
44 to thwart AT takeover NN 44
13 11
TAG: VO
The 48.50 a share offer announced Sunday is designed to thwart a takeover bid by
GAF Corp.
telephone return &apos;1
53 return telephone calls 53
22 21
TAG: VO
Mesa did not indicate the average price it paid for its 4.4 percent stake and Mesa
officials did not immediately return telephone calls seeking comment.
</figure>
<figureCaption confidence="0.970292">
Figure 12
</figureCaption>
<bodyText confidence="0.923225571428571">
Some complete output on the stock market corpus.
For the 10 million—word stock market corpus, there were some 60,000 different
word forms. Xtract has been able to retrieve some 15,000 collocations in total. We would
like to note, however, that Xtract has only been effective at retrieving collocations for
words appearing at least several dozen times in the corpus. This means that low-
frequency words were not productive in terms of collocations. Out of the 60,000 words
in the corpus, only 8,000 were repeated more than 50 times. This means that for a target
</bodyText>
<page confidence="0.988675">
165
</page>
<figure confidence="0.970004083333333">
Computational Linguistics Volume 19, Number 1
YY=20% Y=20% N = 60 % T = 40% U = 60%
T
T=94% T=94%
U = 95%
U U
Y
YY
Y = 40%
N = 92%
YY = 40%
N
</figure>
<figureCaption confidence="0.980539">
Figure 13
</figureCaption>
<bodyText confidence="0.961800666666667">
Overlap of the manual and automatic evaluations
lexicon of size N = 8,000, one should expect at least as many collocations to be added,
and Xtract can help retrieve most of them.
</bodyText>
<subsectionHeader confidence="0.866493">
10. A Lexicographic Evaluation
</subsectionHeader>
<bodyText confidence="0.943255434782609">
The third stage of Xtract can thus be considered as a retrieval system that retrieves
valid collocations from a set of candidates. This section describes an evaluation ex-
periment of the third stage of Xtract as a retrieval system as well as an evaluation
of the overall output of Xtract. Evaluation of retrieval systems is usually done with
the help of two parameters: precision and recall (Salton 1989). Precision of a retrieval
system is defined as the ratio of retrieved valid elements divided by the total number
of retrieved elements (Salton 1989). It measures the quality of the retrieved material.
Recall is defined as the ratio of retrieved valid elements divided by the total number
of valid elements. It measures the effectiveness of the system. This section presents an
evaluation of the retrieval performance of the third stage of Xtract.
Deciding whether a given word combination is a valid or invalid collocation is
actually a difficult task that is best done by a lexicographer. Jeffery Triggs is a lex-
icographer working for the Oxford English Dictionary (OED) coordinating the North
American Readers program of OED at Bell Communication Research. Jeffery Triggs
agreed to go over manually several thousands of collocations.&apos;
In order to have an unbiased experiment we had to be able to evaluate the per-
formance of Xtract against a human expert. We had to have the lexicographer and
Xtract perform the same task. To do this in an unbiased way we randomly selected
a subset of about 4,000 collocations after the first two stages of Xtract. This set of
collocations thus contained some good collocations and some bad ones. This data set
was then evaluated by the lexicographer and the third stage of Xtract. This allowed
17 I am grateful to Jeffery, whose professionalism and kindness helped me understand some of the
difficulty of lexicography. Without him this evaluation would not have been possible.
</bodyText>
<page confidence="0.993224">
166
</page>
<note confidence="0.228256">
Frank Smadja Retrieving Collocations from Text: Xtract
</note>
<bodyText confidence="0.999877862745098">
us to evaluate the performances of the third stage of Xtract and the overall quality of
the total output of Xtract in a single experiment. The experiment was as follows:
We gave the 4,000 collocations to evaluate to the lexicographer, asking him to
select the ones that he would consider for a domain-specific dictionary and to cross
out the others. The lexicographer came up with three simple tags, YY, Y, and N. Both
Y and YY include good collocations, and N includes bad collocations. The difference
between YY and Y is that Y collocations are of better quality than YY collocations.
YY collocations are often too specific to be included in a dictionary, or some words
are missing, etc. After stage 2, about 20% of the collocations are Y, about 20% are YY,
and about 60% are N. This told us that the precision of Xtract at stage 2 was only
about 40%.
Although this would seem like a poor precision, one should compare it with the
much lower rates currently in practice in lexicography. For compiling new entries for
the OED, for example, the first stage roughly consists of reading numerous documents
to identify new or interesting expressions. This task is performed by professional read-
ers. For the OED, the readers for the American program alone produce some 10,000
expressions a month. These lists are then sent off to the dictionary and go through
several rounds of careful analysis before actually being submitted to the dictionary.
The ratio of proposed candidates to good candidates is usually low. For example, out
of the 10,000 expressions proposed each month, fewer than 400 are serious candidates
for the OED, which represents a current rate of 4%. Automatically producing lists
of candidate expressions could actually be of great help to lexicographers, and even
a precision of 40% would be helpful. Such lexicographic tools could, for example,
help readers retrieve sublanguage-specific expressions by providing them with lists
of candidate collocations. The lexicographer then manually examines the list to re-
move the irrelevant data. Even low precision is useful for lexicographers, as manual
filtering is much faster than manual scanning of the documents (Marcus 1990). Such
techniques are not able to replace readers, though, as they are not designed to identify
low-frequency expressions, whereas a human reader immediately identifies interesting
expressions with as few as one occurrence.
The second stage of this experiment was to use Xtract stage 3 to filter out and
label the sample set of collocations. As described in Section 8, there are several valid
labels (VO, VS, NN, etc.). In this experiment, we grouped them under a single label: T.
There is only one nonvalid label: U (for unlabeled). A T collocation is thus accepted by
Xtract stage 3, and a U collocation is rejected. The results of the use of stage 3 on the
sample set of collocations are similar to the manual evaluation in terms of numbers:
about 40% of the collocations were labeled (T) by Xtract stage 3, and about 60% were
rejected (U).
Figure 13 shows the overlap of the classifications made by Xtract and the lexicog-
rapher. In the figure, the first diagram on the left represents the breakdown in T and
U of each of the manual categories (Y-YY and N). The diagram on the right represents
the breakdown in Y-YY and N of the T and U categories. For example, the first col-
umn of the diagram on the left represents the application of Xtract stage 3 on the YY
collocations. It shows that 94% of the collocations accepted by the lexicographer were
also accepted by Xtract. In other words, this means that the recall of the third stage of
Xtract is 94%. The first column of the diagram on the right represents the lexicographic
evaluation of the collocations automatically accepted by Xtract. It shows that about
80% of the T collocations were accepted by the lexicographer and that about 20% were
rejected. This shows that precision was raised from 40% to 80% with the addition of
Xtract stage 3. In summary, these experiments allowed us to evaluate Stage 3 as a
retrieval system. The results are: precision = 80% and recall = 94%.
</bodyText>
<page confidence="0.980194">
167
</page>
<figure confidence="0.622107">
Computational Linguistics Volume 19, Number 1
</figure>
<figureCaption confidence="0.961833416666667">
NYT d w DJ d w AP d w
pay 2 568 closing 1 4615 gouging —1 1713
rises —1 568 rose —1 3704 get 3 551
raise 2 527 fell —1 3161 kindle 4 422
cutting —1 522 tumbled —1 865 increases —1 357
declines —1 492 moved —1 850 pay 2 335
freeze —1 481 declined —1 811 sell 5 293
offered 1 443 finished —3 710 finished —3 293
increases —1 338 closed —1 648 declining 2 293
closing 1 231 measures 1 644 rose —5 291
fell 2 224 edged —1 620 trading 3 207
Figure 14
</figureCaption>
<bodyText confidence="0.639761">
Top associations with &amp;quot;price&amp;quot; in NYT, DJ, and AP.
</bodyText>
<subsectionHeader confidence="0.470984">
11. Influence of the Corpus on the Results
</subsectionHeader>
<bodyText confidence="0.999504823529412">
In this section, we discuss the extent to which the results are dependent on the corpus
used. To illustrate our purpose here, we are using results collected from three different
corpora. The first one, DJ, for Dow Jones, is the corpus we used in this paper; it contains
(mostly) stock market stories taken from the Associated Press newswire. DJ contains
8-9 million words. The second corpus, NYT, contains articles published in the New
York Times during the years 1987 and 1988. The articles are on various subjects. This
is the same corpus that was used by Choueka (1988). NYT contains 12 million words.
The third corpus, AP, contains stories from the Associated Press newswire on various
domains such as weather reports, politics, health, finances, etc. AP is 4 million words.
Figure 14 represents the top 10 word associations retrieved by Xtract stage 1 for the
three corpora with the word &amp;quot;price.&amp;quot; In this figure, d represents the distance between
the two words and w represents the weight associated with the bigram. The weight is a
combined index of the statistical distribution as discussed in Section 6, and it evaluates
the collocation. There are several differences and similarities among the three columns
of the figure in terms of the words retrieved, the order of the words retrieved, and the
values of w. We identified two main ways in which the results depend on the corpus.
We discuss them in turn.
</bodyText>
<sectionHeader confidence="0.583281" genericHeader="evaluation">
11.1 Results Are Dependent on the Size of the Corpus
</sectionHeader>
<bodyText confidence="0.9997516">
From the different corpora we used, we noticed that our statistical methods were not
effective for low-frequency words. More precisely, the statistical methods we use do not
seem to be effective on low frequency words (fewer than 100 occurrences). If the word
is not frequently used in the corpus or if the corpus is too small, then the distribution
of its collocates will not be big enough. For example, from AP, which contains about
1,000 occurrences of the word &amp;quot;rain,&amp;quot; Xtract produced over 170 collocations at stage 1
involving it. In contrast, DJ only contains some 50 occurrences of &amp;quot;rain&amp;quot;&apos; and Xtract
could only produce a few collocations with it. Some collocations with &amp;quot;rain&amp;quot; and
&amp;quot;hurricane&amp;quot; extracted from AP are listed in Figure 15. Both words are high-frequency
words in AP and low-frequency words in DJ.
</bodyText>
<page confidence="0.810326">
18 The corpus actually contains some stories not related to Wall Street.
168
</page>
<note confidence="0.297487">
Frank Smadja Retrieving Collocations from Text: Xtract
</note>
<bodyText confidence="0.999764">
In short, to build a lexicon for a computational linguistics application in a given
domain, one should make sure that the important words in the domain are frequent
enough in the corpus. For a subdomain of the stock market describing only the fluc-
tuations of several indexes and some of the major events of the day at Wall Street,
a corpus of 10 million words appeared to be sufficient. This 10 million—token corpus
contains only 5,000 words each repeated more than 100 times.
</bodyText>
<sectionHeader confidence="0.531818" genericHeader="conclusions">
11.2 Results Are Dependent on the Contents of the Corpus
</sectionHeader>
<bodyText confidence="0.987268636363636">
Size and frequency are not the only important criteria. For example, even though &amp;quot;food&amp;quot;
is a high-frequency word in DJ, &amp;quot;eat&amp;quot; is not among its collocates, whereas it is among
the top ones in the two other corpora. Food is not eaten at Wall Street but rather traded,
sold, offered, bought, etc. If the corpus only contains stories in a given domain, most
of the collocations retrieved will also be dependent on this domain. We have seen in
Section 2 that in addition to jargonistic words, there are a number of more familiar
terms that form collocations when used in different domains. A corpus containing
stock market stories is obviously not a good choice for retrieving collocations related
to weather reports or for retrieving domain independent collocations such as &amp;quot;make-
decision.&amp;quot;
For a domain-specific application, domain-dependent collocations are of interest,
and a domain-specific corpus is exactly what is required. To build a system that gen-
erates stock market reports, it is a good choice to use a corpus containing only stock
market reports.
There is a danger in choosing a too specific corpus however. For example, in
Figure 14, we see that the first collocate of &amp;quot;price&amp;quot; in AP is &amp;quot;gouging,&amp;quot; which is not
retrieved in either DJ or in NYT. &amp;quot;Price gouging&amp;quot; is not a current practice at Wall Street
and this collocation could not be retrieved even on some 20,000 occurrences of the
word. An example use of &amp;quot;price gouging&amp;quot; is the following:
&amp;quot;The Charleston City Council passed an emergency ordinance barring price
gouging later Saturday after learning of an incident in which 5 pound bags of
ice were being sold for 10.&amp;quot;
More formally, if we compare the columns in Figure 14, we see that the num-
bers are much higher for DJ than for the other two corpora. This is not due to a
size/frequency factor, since &amp;quot;price&amp;quot; occurs about 10,000 times in both NYT and DJ,
whereas it only occurs 4,500 times in AP. It rather says that the distribution of collo-
cates around &amp;quot;price&amp;quot; has a much higher variance in DJ than in the other corpora. DJ
has much bigger weights because it is focused; the stories are almost all about Wall
Street. In contrast, NYT contains a large number of stories with &amp;quot;price,&amp;quot; but they have
various origins. &amp;quot;Price&amp;quot; has 4,627 collocates in NYT, whereas it only has 2,830 in DJ.
Let us call Ocorpus the variety of a given corpus. One way to measure the variety is
to use the information theory measure of entropy for a given language model. Entropy
is defined (Shannon 1948) as:
</bodyText>
<equation confidence="0.983285">
°corpus — — Ep(w)iogp(w)
w
</equation>
<bodyText confidence="0.9999634">
where p(w) is the probability of appearance of a given word, w. Entropy measures the
predictability of a corpus, in other words, the bigger the entropy of a corpus the less
predictable it is.
In an ideal language model, the entropy of a corpus should not depend on its
size. However, word probabilities are difficult to approximate (see, for example, Bell
</bodyText>
<page confidence="0.995216">
169
</page>
<note confidence="0.658387">
Computational Linguistics Volume 19, Number 1
</note>
<bodyText confidence="0.948044333333333">
CD inches of rain. . . .
acid rain. . . .
CD inches of rain fell
heavy rain . . . .
the Atlantic hurricane season
hurricane force winds
rain forests
to reduce acid rain . . . .
a major hurricane
light rain . . . .
the most powerful hurricane to hit the . . . .
an inch of rain . . . .
to save the world s rain forests
wind and rain . . . .
a cold rain
</bodyText>
<figureCaption confidence="0.858433">
Figure 15
</figureCaption>
<subsectionHeader confidence="0.816187">
Some collocations retrieved from AP.
</subsectionHeader>
<bodyText confidence="0.974150413793103">
[19871 for a thorough discussion on probability estimation), and in most cases entropy
grows with the size of the corpus. In this section, we use a simple unigram language
model trained on the corpus and we approximate the variety of a given corpus by:
0 corpus — E(f(w)/S) log(f (w) /S)
w
in which f (w) is the frequency of appearance of the word w in the corpus and S is
the total number of different word forms in the corpus. In addition, to be fair in our
comparison of the three corpora, we have used three (sub)corpora of about one million
words for DJ, NYT, and Brown. The 1 million—word Brown corpus (Francis and Ktit&apos;era
1982) contains 43,300 different words, of which only 1091 are repeated more than 100
times. The 0 of the Brown corpus is: °Brown = 10.5. In comparison, the size of DJ is
8,000,000. It contains 59,233 different words of which 5,367 are repeated more than 100
times. DJ 0 ratio is: ODJ = 9.6. And the 0 ratio of NYT which contains stories pertaining
to various domains has been estimated at ONyT = 10.4. According to this measure, DJ
is much more focused than both the Brown Corpus and NYT because the difference
in variety is 1 in the logarithmic scale. This is not a surprise since the subjects it covers
are much more restricted, the genre is of only one kind, and the setting is constant. In
contrast, the Brown corpus has been designed to be of mixed and rich composition,
and NYT is made up of stories and articles related to various subjects and domains.
Let us note that several factors might also influence the overall entropy of a given
corpus; for example the number of writers, the time span covered by the corpus, etc.
In any case, the success of statistical methods such as the ones described in this report
also depends on the sublanguage used in the corpus.
For a sublanguage-dependent application, the training corpus must be focused,
mainly because its vocabulary being restricted, the important words will be more
frequent than in a nonrestricted corpus (of equivalent size), and thus the collocations
will be easier to retrieve. Other applications might require less focused corpora. For
those applications, the problem is even more touchy, as a perfectly balanced corpus is
very difficult to compile. A sample of the 1987 DJ text is certainly not a good sample
</bodyText>
<page confidence="0.985824">
170
</page>
<note confidence="0.296196">
Frank Smadja Retrieving Collocations from Text: Xtract
</note>
<bodyText confidence="0.99856925">
of general English; however, a balanced sample, such as the Brown Corpus, may also
be a poor sample. It is doubtful that even a balanced corpus contains enough data on
all possible domains, and the very effort of artificially balancing the corpus might also
bias the results.
</bodyText>
<subsectionHeader confidence="0.977564">
12. Some Applications
</subsectionHeader>
<bodyText confidence="0.99991075">
Corpus-based techniques are still rarely used in the fields of linguistics, lexicography,
and computational linguistics, and the main thrust of the work presented here is to
promote its use for any text based application. In this section we discuss several uses
of Xtract.
</bodyText>
<subsectionHeader confidence="0.866823">
12.1 Language Generation
</subsectionHeader>
<bodyText confidence="0.9999495">
Language generation is a novel application for Corpus-Based Computational Linguis-
tics (Boguraev 1989). In Smadja (1991) we show how collocations enhance the task of
lexical selection in language generation. Previous language generation works did not
use collocations mainly because they did not have the information in compiled form
and the lexicon formalisms available did not handle the variability of collocational
knowledge. In contrast, we use Xtract to produce the collocations and we use Func-
tional Unification Grammars (FUGs) (Kay 1979) as a representation formalism and a
unification engine. We show how the use of FUGs allows us to properly handle the in-
teractions of collocational and various other constraints. We have implemented Cook,
a surface sentence generator that uses a flexible lexicon for expressing collocational
constraints in the stock market domain. Using Ana (Kukich 1983) as a deep generator,
Cook is implemented in FUF (Elhadad 1990), an extended implementation of FUG,
and uniformly represents the lexicon and syntax as originally suggested by Halliday
(1966). For a more detailed description of Cook the reader is referred to Smadja (1991).
</bodyText>
<subsectionHeader confidence="0.97951">
12.2 Retrieving Grammatical Collocations
</subsectionHeader>
<bodyText confidence="0.997063777777778">
According to Benson, Benson, and Ilson (1986a), collocations fall into two major groups:
lexical collocations and grammatical collocations. The difference between these two
groups lies in the types of words involved. Lexical collocations roughly consist of
syntagmatic affinities among open class words such as verbs, nouns, adjectives, and
adverbs. In contrast, grammatical collocations generally involve at least one closed
class word among particles, prepositions, and auxiliary verbs. Examples of grammat-
ical collocations are: put-up, as in &amp;quot;I can&apos;t put up with this anymore,&amp;quot; and fill-out, as in
&amp;quot;You have to fill out your 1040 form.&amp;quot;19
Consider the sentences below:
</bodyText>
<listItem confidence="0.948297166666667">
1. &amp;quot;The comparison to job hunting is certainly a valid one.&amp;quot;
2.* &amp;quot;The comparison with job hunting is certainly a valid one.&amp;quot;
3. &amp;quot;The association with job hunting is certainly a valid one.&amp;quot;
4.* &amp;quot;The association to job hunting is certainly a valid one.&amp;quot;
5. &amp;quot;...a new initiative in the aftermath of the PLO&apos;s evacuation from
Beirut.&amp;quot;
</listItem>
<page confidence="0.832453">
19 Note that British English uses rather &amp;quot;to fill in a form.&amp;quot;
171
</page>
<note confidence="0.537766">
Computational Linguistics Volume 19, Number 1
</note>
<bodyText confidence="0.974216740740741">
6.* &amp;quot;... a new initiative in the aftermath from the PLO&apos;s evacuation from
Beirut.&amp;quot;
7. &amp;quot;... a new initiative in the aftershocks from the PLO&apos;s evacuation from
Beirut.&amp;quot;
8.* &amp;quot;... a new initiative in the aftershocks of the PLO&apos;s evacuation from
Beirut.&amp;quot;
These examples clearly show that the choices of the prepositions are arbitrary.
Sentences (1)—(2) and (3)—(4) compare the word associations comparison with/to with
association with/to. Although very similar in meaning, the two words select different
prepositions. Moreover, the difference of meaning of the two prepositions does not
account for the wording choices. Similarly, sentences (5)—(6) and (7)—(8) illustrate the
fact that &amp;quot;aftermath&amp;quot; selects the preposition &amp;quot;of&amp;quot; and &amp;quot;aftershock&amp;quot; selects &amp;quot;from.&amp;quot;
Grammatical collocations are very similar to lexical collocations in the sense that
they also correspond to arbitrary and recurrent word co-occurrences (Benson 1990). In
terms of structure, grammatical collocations are much simpler: since many of the gram-
matical collocations only include one open class word, the separation base-collocator
becomes trivial. The open class word is the meaning bearing element, it is the base; and
the closed class word is the collocator. For lexicographers, grammatical collocations are
somehow simpler than lexical collocations. A large number of dictionaries actually in-
clude them. For example, The Random House Dictionary of the English Language (RHDEL)
(Flexner 1987) gives: &amp;quot;abreast of, accessible to, accustomed to, careful about, conducive to, con-
scious of, equal to, expert at, fond of, jealous of,&amp;quot; etc. However, a large number are missing
and the information provided is inconsistent and spotty. For example, RHDEL does
not include: appreciative of, available to, certain of, clever at, comprehensible to, curious about,
difficult for, effective against, faithful to, friendly with, furious at, happy about, hostile to, etc.
As demonstrated by Benson, even the most complete learners&apos; dictionaries miss very
important grammatical collocations and treat the others inconsistently.&apos;
</bodyText>
<figureCaption confidence="0.759779">
Xtract can be used without modification to retrieve noun—preposition collocations.
Figure 16 lists such collocations as retrieved by Xtract. Many of the associations re-
trieved are effectively collocations: &amp;quot;absence of, accordance with, accuracy of, advantage of,
aftershock from, agreement on, allegations of, anxiety about, aspect of,&amp;quot; etc.
</figureCaption>
<subsubsectionHeader confidence="0.207804">
12.3 Some Determiner—Noun Problems
</subsubsectionHeader>
<bodyText confidence="0.99846">
Determiners are lexical elements that are used in conjunction with a noun to bring into
correspondence with it a certain sector of reality (Ducrot and Todorov 1979). A noun
without determiner has no referent. The role of determiner can be played by several
classes of items: articles, (e.g., &amp;quot;a,&amp;quot; &amp;quot;the&amp;quot;), possessives (e.g., &amp;quot;my,&amp;quot; &amp;quot;your&amp;quot;), indefinite
adjectives (e.g., &amp;quot;some,&amp;quot; &amp;quot;many,&amp;quot; &amp;quot;few,&amp;quot; &amp;quot;certain&amp;quot;), demonstratives (e.g., &amp;quot;this,&amp;quot; &amp;quot;those&amp;quot;),
numbers, etc. Determiner—noun combinations are often based simply on semantic or
syntactic criteria. For example in the expression &amp;quot;my left foot,&amp;quot; the determiner &amp;quot;my&amp;quot; is
here for semantic reasons. Any other determiner would fail to identify the correct object
(my left foot). Classes of nouns such as mass and count are supposed to determine the
type of determiners to be used in conjunction with the nouns (Quirk et al. 1972). Mass
nouns often refer to objects or ideas that can be divided into smaller parts without
losing their meaning. In contrast, count nouns refer to objects that are not dividable.
For example, &amp;quot;water&amp;quot; is a mass noun, if you spill half a glass of water you still have
</bodyText>
<note confidence="0.369111">
20 For a detailed case study the reader is referred to Benson (1989b).
</note>
<page confidence="0.933995">
172
</page>
<figure confidence="0.615676">
Frank Smadja Retrieving Collocations from Text: Xtract
</figure>
<figureCaption confidence="0.908301833333333">
Noun part Noun part Noun part
ability of afternoon from arbitrage in
absence of aftershocks from area of
acceleration of age of area with
acceptance of agency for areas of
accordance with agency with argument by
account of agreement by arguments in
accounts in agreements with article in
accuracy of alarm about articles on
acquisition of alternatives for aspects of
acres of amount of assault on
action by amounts of assessment of
actions by analysis for association with
actions of analysis of assumption of
advance from announcement by attempts by
advance of announcement of attention on
advancers with anxiety about attorney for
advances in appetite for attractiveness of
advances on applications for auction for
advantage of appointment of auction in
adviser in appraisal of auction of
aftermath of approval from author of
aftershocks from approval of authority for
Figure 16
</figureCaption>
<bodyText confidence="0.995913423076923">
Some noun-preposition associations retrieved by Xtract.
some water left in your glass. In contrast if you cut a book in two halves and discard
one half, you do not have a book any more; &amp;quot;book&amp;quot; is a count noun. Count nouns are
often used with numbers and articles, and mass nouns are often used with no articles
(or the zero article noted 0) (Quirk et al. 1972).
As with other types of word combinations, noun-determiner combinations often
lead to collocations. Consider the table given in Table 5. In the table, some noun-
determiner combinations are compared. The first four determiners (a, the, 0, some)
represent a singular use of the noun, and the last four (many, few, a lot of, a great deal
of) represent a plural use. 1 and 300 are numbers. 0 is the zero article. In the table,
a &apos;+&apos; sign means that the combination is frequent and normal; a &apos;-&apos; sign means that
the combination is very rare if not forbidden. A &apos;?&apos; sign means that the combination
is very low probability and that it would probably require an unusual context. For
example, one does not say *&amp;quot;a butter,&amp;quot; one says &amp;quot;some butter,&amp;quot; and the combination
butter-many is rather unusual and would only occur in unusual contexts. For example,
if one refers to several types of butter, one could say: &amp;quot;Many butters are based on regular
butter and an additional spice or flavor, such as rosemary, sage, basil, garlic, etc.&amp;quot;
&amp;quot;Book&amp;quot; is a typical count noun in that it can combine with &amp;quot;a&amp;quot; and &amp;quot;many.&amp;quot; &amp;quot;Butter&amp;quot;
is a typical mass noun in that it combines with the zero determiner and &amp;quot;a great
deal.&amp;quot; However, words such as &amp;quot;police, people, traffic, opinion, weather,&amp;quot; etc. share some
characteristics of both mass nouns and count nouns. For example, &amp;quot;weather&amp;quot; is neither a
count noun—*&amp;quot;a weather&amp;quot; is incorrect—nor a mass noun—*&amp;quot;a lot of weather&amp;quot; is incorrect
(Quirk et al. 1972). However, it shares some characteristics of both types of nouns.
Mass noun features include the premodified structures &amp;quot;a lot of good weather,&amp;quot; &amp;quot;some bad
weather,&amp;quot; and &amp;quot;what lovely weather.&amp;quot; Count noun features include the plural &amp;quot;go out in
all weathers,&amp;quot; &amp;quot;in the worst of weathers.&amp;quot;
</bodyText>
<page confidence="0.994281">
173
</page>
<figure confidence="0.773354">
Computational Linguistics Volume 19, Number 1
Table 5
Some noun-determiner collocations.
Noun/Det a the 0 some many few a lot of a great deal of 1 300
butter - + + + -? -? + + - -
book + + - - + + + - + +
economics - + + -? - - + + -
police - + + + + + + - - +
people + + + + + + + + + +
opinion + + - - + + + -? + +
traffic - + + + - + + - -
weather + - - - - +? -
</figure>
<bodyText confidence="0.999758375">
The problem with such combinations is that, if the word is irregular then the
information will probably not be in the dictionary.&apos; Moreover, even if the word is
regular, the word itself might not be in the dictionary or the information could simply
be difficult to retrieve automatically.
Simple tools such as Xtract can hopefully provide such information. Based on
a large number of occurrences of the noun, Xtract will be able to make statistical
inferences as to the determiners used with it. Such analysis is possible without any
modification to Xtract. Actually, only a subpart of Xtract is necessary to retrieve them.
</bodyText>
<subsectionHeader confidence="0.758402">
12.4 Multilingual Lexicography
</subsectionHeader>
<bodyText confidence="0.985832217391304">
We have seen that collocations are difficult to handle for non-native speakers, and
that they require special handling for computational applications. In a multilingual
environment the problems become even more complex, as each language imposes its
own collocational constraints. Consider, for example, the English expressions &amp;quot;House of
Parliament&amp;quot; and &amp;quot;House painter.&amp;quot; The natural French translation for &amp;quot;house&amp;quot; is &amp;quot;maison.&amp;quot;
However, the two expressions do not use this translation, but respectively &amp;quot;chambre&amp;quot;
(&amp;quot;room&amp;quot; in English) and &amp;quot;Witiment&amp;quot; (&amp;quot;building&amp;quot; in English). Translations have to be pro-
vided for collocations, and should not be word-based but rather expression-based.
Bilingual dictionaries are generally inadequate in dealing with such issues. They gen-
erally limit such context-sensitive translations to ambiguous words (e.g., &amp;quot;number&amp;quot; or
&amp;quot;rock&amp;quot;) or highly complex words such as &amp;quot;make,&amp;quot; &amp;quot;have,&amp;quot; etc. Moreover, even in these
cases, coverage is limited to semantic variants, and lexical collocations are generally
omitted. One possible application is the development of compilation techniques for
bilingual dictionaries. This would require compiling two monolingual collocational
dictionaries and then developing some automatic or assisted translation methods.
Those translation methods could be based on the statistical analysis of bilingual cor-
pora currently available. A simple algorithm for translating collocations is given in
Smadja (1992).
Several other applications such as information retrieval, automatic thesauri com-
pilation, and speech recognition are also discussed in Smadja (1991).
21 Note that it might be in some grammar book. For example, Quirk et al. in their extensive grammar
book (1972) devote some 100 pages to such noun—determiner combinations. They include a large
number of rules and list exceptions to those rules.
</bodyText>
<page confidence="0.988982">
174
</page>
<figure confidence="0.486895">
Frank Smadja Retrieving Collocations from Text: Xtract
13. Summary and Conclusion
</figure>
<bodyText confidence="0.99976776">
Corpus analysis is a relatively recent domain of research. With the availability of large
samples of textual data and automated tools such as part-of-speech taggers, it has
become possible to develop and use automatic techniques for retrieving lexical infor-
mation from textual corpora. In this paper some original techniques for the automatic
extraction of collocations have been presented. The techniques have been implemented
in a system, Xtract, and tested on several corpora. Although some other attempts have
been made to retrieve collocations from textual corpora, no work has been able to re-
trieve the full range of the collocations that Xtract retrieves. Thanks to our filtering
methods, the collocations produced by Xtract are of better quality. And finally, be-
cause of the syntactic labeling, the collocations we produce are richer than the ones
produced by other methods.
The number and size of available textual corpora is constantly growing. Dictionar-
ies are available in machine-readable form, news agencies provide subscribers with
daily reports on various events, publishing companies use computers and provide
machine-readable versions of books, magazines, and journals. This amounts to a vast
quantity of language data with unused and virtually unlimited, implicit and explicit
information about the English language. These textual data can thus be used to re-
trieve important information that is not available in other forms. The primary goal
of the research we presented is to provide a comprehensive lexicographic toolkit to
assist in implementing natural language processing, as well as to assist lexicographers
in compiling general-purpose dictionaries, as most of the work is still manually per-
formed in this domain. The abundance of text corpora allows a shift toward more
empirical studies of language that emphasize the development of automated tools.
We think that more research should be conducted in this direction and hope that our
work will stimulate research projects along these lines.
</bodyText>
<sectionHeader confidence="0.9856" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.978084222222222">
I would like to thank Steve Abney, Ken
Church, Karen Kukich, and Michael
Elhadad for making their software tools
available to us. Without them, most of the
work reported here would not have been
possible. Kathy McKeown read earlier
versions of this paper and was helpful in
both the writing and the research. Finally,
the anonymous reviewers for Computational
Linguistics made insightful comments on
earlier versions of the paper.
Part of this work has been done in
collaboration with Bell Communication
Research, and part of this work has been
supported by DARPA grant
NO0039-84-C-0165, by NSF grant
IRT-84-51438, and by ONR grant
NO0014-89-J-1782.
</bodyText>
<sectionHeader confidence="0.99127" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99257834375">
Abney, S. (1989). Parsing by Chunks. In The
MIT Parsing Volume, edited by C. Tenny.
MIT Press.
Abney, S. (1990). &amp;quot;Rapid incremental
parsing with repair.&amp;quot; In Proceedings,
Waterloo Conference on Electronic Text
Research, 1990.
Allerton, D. J. (1984). &amp;quot;Three or four levels
of co-occurrence relations.&amp;quot; Lingua, 63,
17-40.
Amsler, B. (1989). &amp;quot;Research towards the
development of a lexical knowledge base
for natural language processing.&amp;quot; In
Proceedings, 1989 SIGIR Conference.
Cambridge, MA.
Angell, R. C. (1983). &amp;quot;Automating spelling
correction using a trigram similarity
measure.&amp;quot; Information Processing and
Management, 19, 255-261.
Bahl, L.; Jelinek, F.; and Mercer, R. (1983).
&amp;quot;A maximum likelihood approach to
continuous speech recognition.&amp;quot; IEEE
Transactions on Pattern Analysis and Machine
Intelligence, 5(2), 179-190.
Bell, T.; Witten, I.; and Cleary, J. (1989).
&amp;quot;Modelling for text compression.&amp;quot; ACM
Computing Surveys, 21(4), 557-591.
Bell, T. (1987). &amp;quot;A unifying theory and
improvement for existing approaches to
text compression.&amp;quot; Doctoral dissertation,
University of Canterbury, Christchurch,
New Zealand.
</reference>
<page confidence="0.962409">
175
</page>
<reference confidence="0.994988341463415">
Computational Linguistics Volume 19, Number 1
Benson, M.; Benson, E.; and Ilson, R.
(1986a). The BBI Combinatory Dictionary of
English: A Guide to Word Combinations. John
Benjamins.
Benson, M.; Benson, E.; and Ilson, R.
(1986b). The Lexicographic Description of
English. John Benjamins.
Benson, M. (1989a). &amp;quot;The collocational
dictionary and the advanced learner.&amp;quot; In
Learner&apos;s Dictionaries: State of the Art, edited
by M. Tickoo, 84-93. SEAMEO.
Benson, M. (1989b). &amp;quot;The structure of the
collocational dictionary.&amp;quot; International
Journal of Lexicography, 2,1-14.
Benson, M. (1990). &amp;quot;Collocations and
general-purpose dictionaries.&amp;quot;
International Journal of Lexicography, 3(1),
23-35.
Boguraev, B. (1989). &amp;quot;Introduction.&amp;quot; In
Computational Lexicography for Natural
Language Processing, Chapter 1, edited by
T. Boguraev and B. Briscoe. Longman.
Brown, P.; Cocke, J.; Della Pietra, V.;
Della Pietra, S.; Jelinek, F.; Mercer, R.; and
Roossin, P. (1988). &amp;quot;A statistical approach
to language translation.&amp;quot; In Proceedings of
the 13th International Conference on
Computational Linguistics (COLING-88),
71-76.
Cerf-Danon, H.; Derouault, A. M.; Elbeze,
M.; and Merialdo, B. (1989). &amp;quot;Speech
recognition in French with a very large
dictionary.&amp;quot; In Eurospeech.
Choueka, Y.; Klein, T.; and Neuwitz, E.
(1983). &amp;quot;Automatic retrieval of frequent
idiomatic and collocational expressions in
a large corpus.&amp;quot; Journal for Literary and
Linguistic Computing, 4,34-38.
Choueka, Y. (1988). &amp;quot;Looking for needles in
a haystack.&amp;quot; In Proceedings, RIAO
Conference on User-Oriented Context Based
Text and Image Handling, 609-623.
Cambridge, MA.
Church, K., and Gale, W. (1990). &amp;quot;Poor
estimates of context are worse than
none.&amp;quot; In Darpa Speech and Natural
Language Workshop, Hidden Valley, PA.
Church, K., and Hanks, P. (1989). &amp;quot;Word
association norms, mutual information,
and lexicography.&amp;quot; In Proceedings, 27th
Meeting of the ACL, 76-83. Also in
Computational Linguistics, 16(1).
Church, K. W.; Gale, W.; Hanks, P.; and
Hindle, D. (1989). &amp;quot;Parsing, word
associations and typical
predicate-argument relations.&amp;quot; In
Proceedings of the International Workshop on
Parsing Technologies, 103-112. Carnegie
Mellon University, Pittsburgh, PA.
Church, K.; Gale, W.; Hanks, P.; and Hindle,
D. (1991). &amp;quot;Using statistics in lexical
analysis.&amp;quot; In Lexical Acquisition: Using
On-Line Resources to Build a Lexicon, edited
by Uri Zernik. Lawrence Erlbaum.
Church, K. (1988). &amp;quot;Stochastic parts
program and noun phrase parser for
unrestricted text.&amp;quot; In Proceedings, Second
Conference on Applied Natural Language
Processing. Austin, TX.
Cowie, A. P. (1981). &amp;quot;The treatment of
collocations and idioms in learner&apos;s
dictionaries.&amp;quot; Applied Linguistics, 2(3),
223-235.
Cruse, D. A. (1986). Lexical Semantics.
Cambridge University Press.
Debili, F. (1982). Analyse
Syntactico-Semantique Fond&amp; sur une
Acquisition Automatique de Relations
Lexicales Semantiques. Doctoral
dissertation, Paris XI University, Orsay,
France. These de Doctorat D&apos;état.
Dellenbaugh, D., and Dellenbaugh, B.
(1990). Small Boat Sailing, a Complete Guide.
Sports Illustrated Winner&apos;s Circle Books.
Ducrot, 0., and Todorov, T. (1979).
Encyclopedic Dictionary of the Sciences of
Language. John Hopkins University Press.
Elhadad, M. (1990). &amp;quot;Types in functional
unification grammars.&amp;quot; In Proceedings,
28th Meeting of the Association for
Computational Linguistics.
Ephraim, Y., and Rabiner, L. (1990). &amp;quot;On the
relations between modeling approaches
for speech recognition.&amp;quot; IEEE Transactions
on Information Theory, 36(2), 372-380.
Fano, R. (1961). Transmission of Information: A
Statistical Theory of Information. MIT Press.
Flexner, S., ed. (1987). The Random House
Dictionary of the English Language, Second
Edition. Random House.
Francis, W., and Kue&apos;era, H. (1982).
Frequency Analysis of English Usage.
Houghton Mifflin.
Garside, R., and Leech, G. (1987). The
Computational Analysis of English, a Corpus
Based Approach. Longman.
Guazzo, M. (1980). &amp;quot;A general
minimum-redundancy source-coding
algorithm.&amp;quot; IEEE Transactions on
Information Theory, IT-26(1), 15-25.
Halliday, M. A. K., and Hasan, R. (1976).
Cohesion in English. Longman.
Halliday, M. A. K. (1966). &amp;quot;Lexis as a
linguistic level.&amp;quot; In In Memory of J. R. Firth,
edited by C. E. Bazell, J. C. Catford,
M. A. K. Halliday, and R. H. Robins,
148-162. Longmans Linguistics Library.
Hindle, D., and Rooth, M. (1990).
&amp;quot;Structural ambiguity and lexical
relations.&amp;quot; In DARPA Speech and Natural
Language Workshop, Hidden Valley, PA.
Hindle, D. (1983). &amp;quot;User manual for
</reference>
<page confidence="0.967324">
176
</page>
<reference confidence="0.991542802631579">
Frank Smadja Retrieving Collocations from Text: Xtract
fidditch, a deterministic parser.&amp;quot; Technical
Memorandum 7590-142, Naval Research
Laboratory.
Kay, M. (1979). &amp;quot;Functional grammar.&amp;quot; In
Proceedings, 5th Meeting of the Berkeley
Linguistics Society. Berkeley Linguistics
Society.
Kukich, K. (1983). &amp;quot;Knowledge-based report
generation: A technique for automatically
generating natural language reports from
databases.&amp;quot; In Proceedings, Sixth
International ACM SIGIR, Conference on
Research and Development in Information
Retrieval. Washington, D.C.
Kukich, K. (1990). &amp;quot;A comparison of some
novel and traditional lexical distances
metrics for spelling correction.&amp;quot; In
Proceedings, International Neural Networks
Conference (INNC). Paris, France.
Marcus, M. (1990). &amp;quot;Tutorial on tagging and
processing large textual corpora.&amp;quot;
Presented at the 28th Annual Meeting of
the ACL.
Martin, W. J. R.; Al, B. P. E; and
Van Sterkenburg, P. J. G. (1983). &amp;quot;On the
processing of a text corpus: from textual
data to lexicographical information.&amp;quot; In
Lexicography: Principles and Practice,
Applied Language Studies Series, edited
by R. R. K. Hartmann. Academic Press.
Mays, E.; Damerau, E; and Mercer, R.
(1990). &amp;quot;Context-based spelling
correction.&amp;quot; In IBM Natural Language ITL,
Paris, France.
Merbik, I. A. (1981). &amp;quot;Meaning-text models:
a recent trend in Soviet linguistics.&amp;quot; The
Annual Review of Anthropology.
Merialdo, B. (1987). &amp;quot;Speech recognition
with very large size dictionary.&amp;quot; In
Proceedings, International Conference on
Acoustics, Speech, and Signal Processing
(ICASSP), Dallas, TX.
Morris, R., and Cherry, L. L. (1975).
&amp;quot;Computer detection of typographical
errors.&amp;quot; IEEE Transactions on Professional
Communications, PC-18(1), 54-63.
Nakhimovsky, A. D., and Leed, R. L. (1979).
&amp;quot;Lexical functions and language
learning.&amp;quot; Slavic and East European Journal,
23(1).
Quirk, R.; Greenbaum, S.; Leech, G.; and
Svartvik, J. (1972). A Comprehensive
Grammar of the English Language. Longman.
Salton, J. (1989). Automatic Text Processing,
The Transformation, Analysis, and Retrieval of
Information by Computer. Addison-Wesley.
Shannon, C. E. (1948). &amp;quot;A mathematical
theory of communication.&amp;quot; Bell System
Tech., 27, 379-423, 623-656.
Smadja, E, and McKeown, K. (1990).
&amp;quot;Automatically extracting and
representing collocations for language
generation.&amp;quot; In Proceedings of the 28th
Annual Meeting of the Association for
Computational Linguistics, Pittsburgh, PA.
Smadja, E (1991). &amp;quot;Retrieving collocational
knowledge from textual corpora. An
application: Language generation.&amp;quot;
Doctoral dissertation, Computer Science
Department, Columbia University.
Smadja, E (1992). &amp;quot;How to compile a
bilingual collocational lexicon
automatically.&amp;quot; In Proceedings of the AAAI
Workshop on Statistically-Based NLP
Techniques, San Jose, CA.
</reference>
<page confidence="0.997725">
177
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.922087">
<title confidence="0.999063">Retrieving Collocations from Text: Xtract</title>
<author confidence="0.999822">Frank Smadja</author>
<affiliation confidence="0.999992">Columbia University</affiliation>
<abstract confidence="0.995250058823529">Natural languages are full of collocations, recurrent combinations of words that co-occur more often than expected by chance and that correspond to arbitrary word usages. Recent work in lexicography indicates that collocations are pervasive in English; apparently, they are common in all types of writing, including both technical and nontechnical genres. Several approaches have been proposed to retrieve various types of collocations from the analysis of large samples of textual data. These techniques automatically produce large numbers of collocations along with statistical figures intended to reflect the relevance of the associations. However, none of these techniques provides functional information along with the collocation. Also, the results produced often contained improper word associations reflecting some spurious aspect of the training corpus that did not stand for true collocations. In this paper, we describe a set of techniques based on statistical methods for retrieving and identifying collocations from large textual corpora. These techniques produce a wide range of collocations and are based on some original filtering methods that allow the production of richer and higher-precision output. These techniques have been implemented and resulted in a tool, techniques are described and some results are presented on a 10 corpus of stock market news reports. A lexicographic evaluation of a retrieval tool has been made, and the estimated precision of 80%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Parsing by Chunks.</title>
<date>1989</date>
<booktitle>In The MIT Parsing Volume,</booktitle>
<publisher>MIT Press.</publisher>
<note>edited by</note>
<contexts>
<context position="55253" citStr="Abney 1989" startWordPosition="9181" endWordPosition="9182">ons automatically produced by the first stage of Xtract to either add syntactic information or reject them. For example, if a lexical relation identified at stage 1 involves a noun and a verb, the role of stage 3 is to determine whether it is a subject—verb or a verb—object collocation. If no such consistent relation is observed, then the collocation is rejected. Stage 3 uses a parser but it does not require a complete parse tree. Given a number of sentences, Xtract only needs to know pairwise syntactic (modifier—modified) relations. The parser we used in the experiment reported here is Cass (Abney 1989, 1990), a bottom-up incremental parser. Cass&apos; takes input sentences labeled with part of speech and attempts to identify syntactic structure. One of the subtasks performed by Cass is to identify predicate argument relations, and this is the task we are interested in here. Stage 3 works in the following three steps. 14 Actually, this 7-gram could be retrieved several times, one for each pair of open class word it contains. But a simple sorting algorithm gets rid of such repetitions. 15 The parser developed at Bell Communication Research by Steve Abney, Cass stands for Cascaded Analysis of Synt</context>
</contexts>
<marker>Abney, 1989</marker>
<rawString>Abney, S. (1989). Parsing by Chunks. In The MIT Parsing Volume, edited by C. Tenny. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Rapid incremental parsing with repair.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, Waterloo Conference on Electronic Text Research,</booktitle>
<contexts>
<context position="28462" citStr="Abney 1990" startWordPosition="4491" endWordPosition="4492">words are involved in a collocation then: • the words must appear together significantly more often than expected by chance. • because of syntactic constraints the words should appear in a relatively rigid way.&apos; These two assumptions are used to analyze the word distributions, and we base our filtering techniques on them. 6.1 Presentation of the Method In this stage as well as in the two others, we often need part-of-speech information for several purposes. Stochastic part-of-speech taggers such as those in Church (1988) and 6 This fact is being seriously challenged by current research (e.g., Abney 1990; Hindle 1983), and might not be true in the near future. 7 Not crossing sentence boundaries. 8 This is obviously not true for nonconfigurational languages. Although we do believe that the methods described in this paper can be applied to many languages, we have only used them on English texts. 151 Computational Linguistics Volume 19, Number 1 Garside and Leech (1987) have been shown to reach 95-99% performance on free-style text. We preprocessed the corpus with a stochastic part-of-speech tagger developed at Bell Laboratories by Ken Church (Church 1988).9 In the rest of this section, we descr</context>
<context position="53806" citStr="Abney 1990" startWordPosition="8946" endWordPosition="8947">gnificant. In contrast, we start from an included significant bigram (for example, &amp;quot;Dow-30&amp;quot;) and we directly retrieve the surrounding n-grams.&apos; 8. Xtract Stage Three: Adding Syntax to the Collocations The collocations as produced in the previous stages are already useful for lexicography. For computational use, however, functional information is needed. For example, the collocations should have some syntactic properties. It is not enough to say that &amp;quot;make&amp;quot; goes with &amp;quot;decision&amp;quot;; we need to know that &amp;quot;decision&amp;quot; is used as the direct object of the verb. The advent of robust parsers such as Cass (Abney 1990) and Fidditch (Hindle 1983) has made it possible to process large text corpora with good performance and thus combine statistical techniques with more symbolic analysis. In the past, some similar attempts have been done. Debili (1982) parsed corpora of French texts to identify nonambiguous predicate argument relations. He then used these relations for disambiguation. Hindle and Rooth (1990) later refined this approach by using bigram statistics to enhance the task of prepositional phrase attachment. Church et al. (1989, 1991) have yet another approach; they consider questions such as what does</context>
</contexts>
<marker>Abney, 1990</marker>
<rawString>Abney, S. (1990). &amp;quot;Rapid incremental parsing with repair.&amp;quot; In Proceedings, Waterloo Conference on Electronic Text Research, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Allerton</author>
</authors>
<title>Three or four levels of co-occurrence relations.&amp;quot;</title>
<date>1984</date>
<journal>Lingua,</journal>
<volume>63</volume>
<pages>17--40</pages>
<contexts>
<context position="7768" citStr="Allerton 1984" startWordPosition="1232" endWordPosition="1233">asts our work to them. The three stages of Xtract are then introduced in Section 5 and described respectively in Sections 6, 7, and 8. Some results obtained by running Xtract on several corpora are listed and discussed in Section 9. Qualitative and quantitative evaluations of our methods and of our results are discussed in Sections 10 and 11. Finally, several possible applications and tasks for Xtract are discussed in Section 12. 2. What Are Collocations? There has been a great deal of theoretical and applied work related to collocations that has resulted in different characterizations (e.g., Allerton 1984; Cruse 1986; Menuk 1981). Depending on their interests and points of view, researchers have focused on different aspects of collocations. One of the most comprehensive definition that has 145 Computational Linguistics Volume 19, Number 1 been used can be found in the lexicographic work of Benson and his colleagues (Benson 1990). The definition is the following: Definition A collocation is an arbitrary and recurrent word combination (Benson 1990). This definition, however, does not cover some aspects and properties of collocations that have consequences for a number of machine applications. Fo</context>
</contexts>
<marker>Allerton, 1984</marker>
<rawString>Allerton, D. J. (1984). &amp;quot;Three or four levels of co-occurrence relations.&amp;quot; Lingua, 63, 17-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Amsler</author>
</authors>
<title>Research towards the development of a lexical knowledge base for natural language processing.&amp;quot;</title>
<date>1989</date>
<booktitle>In Proceedings, 1989 SIGIR Conference.</booktitle>
<location>Cambridge, MA.</location>
<contexts>
<context position="16819" citStr="Amsler (1989)" startWordPosition="2645" endWordPosition="2646">orms a predicative relation. Examples of automatically extracted predicative relations are given in Figure 3.3 This class of collocations is related to Menuk&apos;s lexical functions (Mel&apos;aik 1981), and Benson&apos;s Ltype relations (Benson, Benson, and Ilson 1986b). 3.2 Rigid Noun Phrases Rigid noun phrases involve uninterrupted sequences of words such as &amp;quot;stock market,&amp;quot; &amp;quot;foreign exchange,&amp;quot; &amp;quot;New York Stock Exchange,&amp;quot; &amp;quot;The Dow Jones average of 30 industrials.&amp;quot; They can include nouns and adjectives as well as closed class words, and are similar to the type of collocations retrieved by Choueka (1988) and Amsler (1989). They are the most rigid type of collocation. Examples of rigid noun phrases are:4 &amp;quot;The NYSE&apos;s composite index of all its listed common stocks,&amp;quot; &amp;quot;The NASDAQ composite index for the over the counter market,&amp;quot; &amp;quot;leveraged buyout,&amp;quot; &amp;quot;the gross national product,&amp;quot; &amp;quot;White House spokesman Marlin Fitzwater.&amp;quot; In general, rigid noun phrases cannot be broken into smaller fragments without losing their meaning; they are lexical units in and of themselves. Moreover, they often refer to important concepts in a domain, and several rigid noun phrases can be used to express the same concept. In the New York Stoc</context>
</contexts>
<marker>Amsler, 1989</marker>
<rawString>Amsler, B. (1989). &amp;quot;Research towards the development of a lexical knowledge base for natural language processing.&amp;quot; In Proceedings, 1989 SIGIR Conference. Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Angell</author>
</authors>
<title>Automating spelling correction using a trigram similarity measure.&amp;quot;</title>
<date>1983</date>
<journal>Information Processing and Management,</journal>
<volume>19</volume>
<pages>255--261</pages>
<contexts>
<context position="52276" citStr="Angell (1983)" startWordPosition="8697" endWordPosition="8698">r subsumed m-grams, such as &amp;quot;of arms to Iran.&amp;quot; A sophisticated filtering method would then be necessary to eliminate the invalid ones (See Choueka 1988). Our method avoids this problem and only produces the biggest possible n-gram, namely: &amp;quot;shipment of arms to Iran.&amp;quot; 3. Stage 2 is a simple way of compiling n-gram data. Retrieving an 11-gram by the methods used in speech, for example, would require a great deal 13 Similar approaches have been done for several applications such as Bahl, Jelinek, and Mercer (1983) and Cerf-Danon et al. (1989) for speech recognition, and Morris and Cherry (1975), Angell (1983), Kukich (1990), and Mays, Damerau, and Mercer (1990) for spelling correction (with letters instead of words). 160 Frank Smadja Retrieving Collocations from Text: Xtract of CPU time and space. In a 10 million—word corpus, with about 60,000 different words, there are about 3.6 x 109 possible bigrams, 2.16 x 1014 trigrams, and 3 x 1033 7-grams. This rapidly gets out of hand. Choueka, for example, had to stop at length six. In contrast, the rigid noun phrases we retrieve are of arbitrary length and are retrieved very easily and in one pass. The method we use starts from bigrams and produces the b</context>
</contexts>
<marker>Angell, 1983</marker>
<rawString>Angell, R. C. (1983). &amp;quot;Automating spelling correction using a trigram similarity measure.&amp;quot; Information Processing and Management, 19, 255-261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Bahl</author>
<author>F Jelinek</author>
<author>R Mercer</author>
</authors>
<title>A maximum likelihood approach to continuous speech recognition.&amp;quot;</title>
<date>1983</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>5</volume>
<issue>2</issue>
<pages>179--190</pages>
<marker>Bahl, Jelinek, Mercer, 1983</marker>
<rawString>Bahl, L.; Jelinek, F.; and Mercer, R. (1983). &amp;quot;A maximum likelihood approach to continuous speech recognition.&amp;quot; IEEE Transactions on Pattern Analysis and Machine Intelligence, 5(2), 179-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Bell</author>
<author>I Witten</author>
<author>J Cleary</author>
</authors>
<title>Modelling for text compression.&amp;quot;</title>
<date>1989</date>
<journal>ACM Computing Surveys,</journal>
<volume>21</volume>
<issue>4</issue>
<pages>557--591</pages>
<marker>Bell, Witten, Cleary, 1989</marker>
<rawString>Bell, T.; Witten, I.; and Cleary, J. (1989). &amp;quot;Modelling for text compression.&amp;quot; ACM Computing Surveys, 21(4), 557-591.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Bell</author>
</authors>
<title>A unifying theory and improvement for existing approaches to text compression.&amp;quot; Doctoral dissertation,</title>
<date>1987</date>
<institution>University of Canterbury,</institution>
<location>Christchurch, New Zealand.</location>
<marker>Bell, 1987</marker>
<rawString>Bell, T. (1987). &amp;quot;A unifying theory and improvement for existing approaches to text compression.&amp;quot; Doctoral dissertation, University of Canterbury, Christchurch, New Zealand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Benson</author>
<author>E Benson</author>
<author>R Ilson</author>
</authors>
<title>The BBI Combinatory Dictionary of English: A Guide to Word Combinations.</title>
<date>1986</date>
<publisher>John Benjamins.</publisher>
<marker>Benson, Benson, Ilson, 1986</marker>
<rawString>Benson, M.; Benson, E.; and Ilson, R. (1986a). The BBI Combinatory Dictionary of English: A Guide to Word Combinations. John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Benson</author>
<author>E Benson</author>
<author>R Ilson</author>
</authors>
<title>The Lexicographic Description of English.</title>
<date>1986</date>
<publisher>John Benjamins.</publisher>
<marker>Benson, Benson, Ilson, 1986</marker>
<rawString>Benson, M.; Benson, E.; and Ilson, R. (1986b). The Lexicographic Description of English. John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Benson</author>
</authors>
<title>The collocational dictionary and the advanced learner.&amp;quot;</title>
<date>1989</date>
<booktitle>In Learner&apos;s Dictionaries: State of the Art, edited</booktitle>
<pages>84--93</pages>
<publisher>SEAMEO.</publisher>
<contexts>
<context position="13088" citStr="Benson 1989" startWordPosition="2048" endWordPosition="2049">re very often repeated in a given context. Word combinations such as &amp;quot;to make a decision, to hit a record, to perform an operation&amp;quot; are typical of the language, and collocations such as &amp;quot;to buy short,&amp;quot; &amp;quot;to ease the jib&amp;quot; are characteristic of specific domains. Both types are repeatedly used in specific contexts. 2.4 Collocations Are Cohesive Lexical Clusters By cohesive2 clusters, we mean that the presence of one or several words of the collocations often implies or suggests the rest of the collocation. This is the property mostly used by lexicographers when compiling collocations (Cowie 1981; Benson 1989a). Lexicographers use other people&apos;s linguistic judgment for deciding what is and what is not a collocation. They give questionnaires to people such as the one given in Figure 2. This questionnaire contains sentences used by Benson for compiling collocational knowledge for the BBI (Benson 1989b). Each sentence contains an empty slot that can easily be filled in by native speakers. In contrast, second language speakers would not find the missing words automatically but would consider a long list of words having the appropriate semantic and syntactic features such as the ones given in the secon</context>
<context position="88526" citStr="Benson (1989" startWordPosition="14801" endWordPosition="14802">he determiner &amp;quot;my&amp;quot; is here for semantic reasons. Any other determiner would fail to identify the correct object (my left foot). Classes of nouns such as mass and count are supposed to determine the type of determiners to be used in conjunction with the nouns (Quirk et al. 1972). Mass nouns often refer to objects or ideas that can be divided into smaller parts without losing their meaning. In contrast, count nouns refer to objects that are not dividable. For example, &amp;quot;water&amp;quot; is a mass noun, if you spill half a glass of water you still have 20 For a detailed case study the reader is referred to Benson (1989b). 172 Frank Smadja Retrieving Collocations from Text: Xtract Noun part Noun part Noun part ability of afternoon from arbitrage in absence of aftershocks from area of acceleration of age of area with acceptance of agency for areas of accordance with agency with argument by account of agreement by arguments in accounts in agreements with article in accuracy of alarm about articles on acquisition of alternatives for aspects of acres of amount of assault on action by amounts of assessment of actions by analysis for association with actions of analysis of assumption of advance from announcement b</context>
</contexts>
<marker>Benson, 1989</marker>
<rawString>Benson, M. (1989a). &amp;quot;The collocational dictionary and the advanced learner.&amp;quot; In Learner&apos;s Dictionaries: State of the Art, edited by M. Tickoo, 84-93. SEAMEO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Benson</author>
</authors>
<title>The structure of the collocational dictionary.&amp;quot;</title>
<date>1989</date>
<journal>International Journal of Lexicography,</journal>
<pages>2--1</pages>
<contexts>
<context position="13088" citStr="Benson 1989" startWordPosition="2048" endWordPosition="2049">re very often repeated in a given context. Word combinations such as &amp;quot;to make a decision, to hit a record, to perform an operation&amp;quot; are typical of the language, and collocations such as &amp;quot;to buy short,&amp;quot; &amp;quot;to ease the jib&amp;quot; are characteristic of specific domains. Both types are repeatedly used in specific contexts. 2.4 Collocations Are Cohesive Lexical Clusters By cohesive2 clusters, we mean that the presence of one or several words of the collocations often implies or suggests the rest of the collocation. This is the property mostly used by lexicographers when compiling collocations (Cowie 1981; Benson 1989a). Lexicographers use other people&apos;s linguistic judgment for deciding what is and what is not a collocation. They give questionnaires to people such as the one given in Figure 2. This questionnaire contains sentences used by Benson for compiling collocational knowledge for the BBI (Benson 1989b). Each sentence contains an empty slot that can easily be filled in by native speakers. In contrast, second language speakers would not find the missing words automatically but would consider a long list of words having the appropriate semantic and syntactic features such as the ones given in the secon</context>
<context position="88526" citStr="Benson (1989" startWordPosition="14801" endWordPosition="14802">he determiner &amp;quot;my&amp;quot; is here for semantic reasons. Any other determiner would fail to identify the correct object (my left foot). Classes of nouns such as mass and count are supposed to determine the type of determiners to be used in conjunction with the nouns (Quirk et al. 1972). Mass nouns often refer to objects or ideas that can be divided into smaller parts without losing their meaning. In contrast, count nouns refer to objects that are not dividable. For example, &amp;quot;water&amp;quot; is a mass noun, if you spill half a glass of water you still have 20 For a detailed case study the reader is referred to Benson (1989b). 172 Frank Smadja Retrieving Collocations from Text: Xtract Noun part Noun part Noun part ability of afternoon from arbitrage in absence of aftershocks from area of acceleration of age of area with acceptance of agency for areas of accordance with agency with argument by account of agreement by arguments in accounts in agreements with article in accuracy of alarm about articles on acquisition of alternatives for aspects of acres of amount of assault on action by amounts of assessment of actions by analysis for association with actions of analysis of assumption of advance from announcement b</context>
</contexts>
<marker>Benson, 1989</marker>
<rawString>Benson, M. (1989b). &amp;quot;The structure of the collocational dictionary.&amp;quot; International Journal of Lexicography, 2,1-14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Benson</author>
</authors>
<title>Collocations and general-purpose dictionaries.&amp;quot;</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>1</issue>
<pages>23--35</pages>
<contexts>
<context position="5485" citStr="Benson (1990)" startWordPosition="864" endWordPosition="865"> (e.g., Smadja and McKeown 1990); this paper gives a complete description of the system and the results obtained. 1 This is true for laymen and also for non-native speakers familiar with the domain but not familiar with the English expressions. 144 Frank Smadja Retrieving Collocations from Text: Xtract &amp;quot;Our firm made/did a deal with them&amp;quot; &amp;quot;The swimmer had/got a cramp&amp;quot; &amp;quot;Politicians are always on/in the firing lane&amp;quot; &amp;quot;These decisions are to be made/taken rapidly&amp;quot; &amp;quot;The children usually set/lay the table&amp;quot; &amp;quot;You have to break in/run in your new car&amp;quot; Figure 1 British English or American English? from Benson (1990). sentences candidates &amp;quot;If a fire breaks out, the alarm will ?? &amp;quot; &amp;quot;ring, go off, sound, start&amp;quot; &amp;quot;The boy doesn&apos;t know how to ?? his bicycle&amp;quot; &amp;quot;drive, ride, conduct&amp;quot; &amp;quot;The American congress can ?? a presidential veto&amp;quot; &amp;quot;ban/cancel/delete/reject&amp;quot; &amp;quot;Before eating your bag of microwavable popcorn, &amp;quot;turn down/abrogate/overrule&amp;quot; you have to ?? it&amp;quot; &amp;quot;cook/nuke/broil/fry/bake&amp;quot; Figure 2 Fill-in-the-blank test, from Benson (1990). Xtract now works in three stages. In the first stage, pairwise lexical relations are retrieved using only statistical information. This stage is comparable to Church and Hanks (1989</context>
<context position="8098" citStr="Benson 1990" startWordPosition="1283" endWordPosition="1284">ns 10 and 11. Finally, several possible applications and tasks for Xtract are discussed in Section 12. 2. What Are Collocations? There has been a great deal of theoretical and applied work related to collocations that has resulted in different characterizations (e.g., Allerton 1984; Cruse 1986; Menuk 1981). Depending on their interests and points of view, researchers have focused on different aspects of collocations. One of the most comprehensive definition that has 145 Computational Linguistics Volume 19, Number 1 been used can be found in the lexicographic work of Benson and his colleagues (Benson 1990). The definition is the following: Definition A collocation is an arbitrary and recurrent word combination (Benson 1990). This definition, however, does not cover some aspects and properties of collocations that have consequences for a number of machine applications. For example, it has been shown that collocations are difficult to translate across languages—this fact obviously has a direct application for machine translation. Many properties of collocations have been identified in the past; however, the tendency was to focus on a restricted type of collocation. In this section, we present fou</context>
<context position="85787" citStr="Benson 1990" startWordPosition="14384" endWordPosition="14385">tions are arbitrary. Sentences (1)—(2) and (3)—(4) compare the word associations comparison with/to with association with/to. Although very similar in meaning, the two words select different prepositions. Moreover, the difference of meaning of the two prepositions does not account for the wording choices. Similarly, sentences (5)—(6) and (7)—(8) illustrate the fact that &amp;quot;aftermath&amp;quot; selects the preposition &amp;quot;of&amp;quot; and &amp;quot;aftershock&amp;quot; selects &amp;quot;from.&amp;quot; Grammatical collocations are very similar to lexical collocations in the sense that they also correspond to arbitrary and recurrent word co-occurrences (Benson 1990). In terms of structure, grammatical collocations are much simpler: since many of the grammatical collocations only include one open class word, the separation base-collocator becomes trivial. The open class word is the meaning bearing element, it is the base; and the closed class word is the collocator. For lexicographers, grammatical collocations are somehow simpler than lexical collocations. A large number of dictionaries actually include them. For example, The Random House Dictionary of the English Language (RHDEL) (Flexner 1987) gives: &amp;quot;abreast of, accessible to, accustomed to, careful ab</context>
</contexts>
<marker>Benson, 1990</marker>
<rawString>Benson, M. (1990). &amp;quot;Collocations and general-purpose dictionaries.&amp;quot; International Journal of Lexicography, 3(1), 23-35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Boguraev</author>
</authors>
<title>Introduction.&amp;quot; In Computational Lexicography for Natural Language Processing,</title>
<date>1989</date>
<journal>Chapter</journal>
<volume>1</volume>
<publisher>Longman.</publisher>
<note>edited by</note>
<contexts>
<context position="82639" citStr="Boguraev 1989" startWordPosition="13905" endWordPosition="13906">be a poor sample. It is doubtful that even a balanced corpus contains enough data on all possible domains, and the very effort of artificially balancing the corpus might also bias the results. 12. Some Applications Corpus-based techniques are still rarely used in the fields of linguistics, lexicography, and computational linguistics, and the main thrust of the work presented here is to promote its use for any text based application. In this section we discuss several uses of Xtract. 12.1 Language Generation Language generation is a novel application for Corpus-Based Computational Linguistics (Boguraev 1989). In Smadja (1991) we show how collocations enhance the task of lexical selection in language generation. Previous language generation works did not use collocations mainly because they did not have the information in compiled form and the lexicon formalisms available did not handle the variability of collocational knowledge. In contrast, we use Xtract to produce the collocations and we use Functional Unification Grammars (FUGs) (Kay 1979) as a representation formalism and a unification engine. We show how the use of FUGs allows us to properly handle the interactions of collocational and vario</context>
</contexts>
<marker>Boguraev, 1989</marker>
<rawString>Boguraev, B. (1989). &amp;quot;Introduction.&amp;quot; In Computational Lexicography for Natural Language Processing, Chapter 1, edited by T. Boguraev and B. Briscoe. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>J Cocke</author>
<author>Della Pietra</author>
<author>Della Pietra V</author>
<author>S Jelinek</author>
<author>F Mercer</author>
<author>R</author>
<author>P Roossin</author>
</authors>
<title>A statistical approach to language translation.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings of the 13th International Conference on Computational Linguistics (COLING-88),</booktitle>
<pages>71--76</pages>
<contexts>
<context position="19809" citStr="Brown et al. 1988" startWordPosition="3121" endWordPosition="3124">their usage gives an impression of fluency that could not be equaled with compositional generation alone. 4. Related Work There has been a recent surge of research interest in corpus-based computational linguistics methods; that is, the study and elaboration of techniques using large real text as a basis. Such techniques have various applications. Speech recognition (Bahl, Jelinek, and Mercer 1983) and text compression (e.g., Bell, Witten, and Cleary 1989; Guazzo 1980) have been of long-standing interest, and some new applications are currently being investigated, such as machine translation (Brown et al. 1988), spelling correction (Mays, Damerau, and Mercer 1990; Church and Gale 1990), parsing (Debili 1982; Hindle and Rooth 1990). As pointed out by Bell, Witten, and Cleary (1989), these applications fall under two research paradigms: statistical approaches and lexical approaches. In the statistical approach, language is modeled as a stochastic process and the corpus is used to estimate probabilities. In this approach, a collocation is simply considered as a sequence of words (or n-gram) among millions of other possible sequences. In contrast, in the lexical approach, a collocation is an element of </context>
</contexts>
<marker>Brown, Cocke, Pietra, V, Jelinek, Mercer, R, Roossin, 1988</marker>
<rawString>Brown, P.; Cocke, J.; Della Pietra, V.; Della Pietra, S.; Jelinek, F.; Mercer, R.; and Roossin, P. (1988). &amp;quot;A statistical approach to language translation.&amp;quot; In Proceedings of the 13th International Conference on Computational Linguistics (COLING-88), 71-76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cerf-Danon</author>
<author>A M Derouault</author>
<author>M Elbeze</author>
<author>B Merialdo</author>
</authors>
<title>Speech recognition in French with a very large dictionary.&amp;quot;</title>
<date>1989</date>
<booktitle>In Eurospeech.</booktitle>
<contexts>
<context position="52208" citStr="Cerf-Danon et al. (1989)" startWordPosition="8685" endWordPosition="8688">ic relation. A straight n-gram method would retrieve both, as well as many other subsumed m-grams, such as &amp;quot;of arms to Iran.&amp;quot; A sophisticated filtering method would then be necessary to eliminate the invalid ones (See Choueka 1988). Our method avoids this problem and only produces the biggest possible n-gram, namely: &amp;quot;shipment of arms to Iran.&amp;quot; 3. Stage 2 is a simple way of compiling n-gram data. Retrieving an 11-gram by the methods used in speech, for example, would require a great deal 13 Similar approaches have been done for several applications such as Bahl, Jelinek, and Mercer (1983) and Cerf-Danon et al. (1989) for speech recognition, and Morris and Cherry (1975), Angell (1983), Kukich (1990), and Mays, Damerau, and Mercer (1990) for spelling correction (with letters instead of words). 160 Frank Smadja Retrieving Collocations from Text: Xtract of CPU time and space. In a 10 million—word corpus, with about 60,000 different words, there are about 3.6 x 109 possible bigrams, 2.16 x 1014 trigrams, and 3 x 1033 7-grams. This rapidly gets out of hand. Choueka, for example, had to stop at length six. In contrast, the rigid noun phrases we retrieve are of arbitrary length and are retrieved very easily and i</context>
</contexts>
<marker>Cerf-Danon, Derouault, Elbeze, Merialdo, 1989</marker>
<rawString>Cerf-Danon, H.; Derouault, A. M.; Elbeze, M.; and Merialdo, B. (1989). &amp;quot;Speech recognition in French with a very large dictionary.&amp;quot; In Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Choueka</author>
<author>T Klein</author>
<author>E Neuwitz</author>
</authors>
<title>Automatic retrieval of frequent idiomatic and collocational expressions in a large corpus.&amp;quot;</title>
<date>1983</date>
<booktitle>Journal for Literary and Linguistic Computing,</booktitle>
<pages>4--34</pages>
<marker>Choueka, Klein, Neuwitz, 1983</marker>
<rawString>Choueka, Y.; Klein, T.; and Neuwitz, E. (1983). &amp;quot;Automatic retrieval of frequent idiomatic and collocational expressions in a large corpus.&amp;quot; Journal for Literary and Linguistic Computing, 4,34-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Choueka</author>
</authors>
<title>Looking for needles in a haystack.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings, RIAO Conference on User-Oriented Context Based Text and Image Handling,</booktitle>
<pages>609--623</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="16801" citStr="Choueka (1988)" startWordPosition="2642" endWordPosition="2643">le-takeover&amp;quot; also forms a predicative relation. Examples of automatically extracted predicative relations are given in Figure 3.3 This class of collocations is related to Menuk&apos;s lexical functions (Mel&apos;aik 1981), and Benson&apos;s Ltype relations (Benson, Benson, and Ilson 1986b). 3.2 Rigid Noun Phrases Rigid noun phrases involve uninterrupted sequences of words such as &amp;quot;stock market,&amp;quot; &amp;quot;foreign exchange,&amp;quot; &amp;quot;New York Stock Exchange,&amp;quot; &amp;quot;The Dow Jones average of 30 industrials.&amp;quot; They can include nouns and adjectives as well as closed class words, and are similar to the type of collocations retrieved by Choueka (1988) and Amsler (1989). They are the most rigid type of collocation. Examples of rigid noun phrases are:4 &amp;quot;The NYSE&apos;s composite index of all its listed common stocks,&amp;quot; &amp;quot;The NASDAQ composite index for the over the counter market,&amp;quot; &amp;quot;leveraged buyout,&amp;quot; &amp;quot;the gross national product,&amp;quot; &amp;quot;White House spokesman Marlin Fitzwater.&amp;quot; In general, rigid noun phrases cannot be broken into smaller fragments without losing their meaning; they are lexical units in and of themselves. Moreover, they often refer to important concepts in a domain, and several rigid noun phrases can be used to express the same concept. In</context>
<context position="42863" citStr="Choueka (1988)" startWordPosition="7121" endWordPosition="7122">ever, taking note of their pattern of appearance allows us to filter out more irrelevant collocations with (C2) and (C3). This is a very important point that will allow us to filter out many invalid collocations and also produce more functional information at stages 2 and 3. A graphical interpretation of the filtering method used for Xtract is given in Smadja (1991). 7. Xtract Stage Two: From 2-Grams to N-Grams The role of the second stage of Xtract is twofold. It produces collocations involving more than two words, and it filters out some pairwise relations. Stage 2 is related to the work of Choueka (1988), and to some extent to what has been done in speech recognition (e.g., Bahl, Jelinek, and Mercer 1983; Merialdo 1987; Ephraim and Rabiner 1990). 7.1 Presentation of the Method In this second stage, Xtract uses the same components used for the first stage but in a different way. It starts with the pairwise lexical relations produced in stage 1 and produces multiple word collocations, such as rigid noun phrases or phrasal templates, from them. To do this, Xtract studies the lexical relations in context, which is exactly what lexicographers do. For each bigram identified at the previous stage, X</context>
<context position="50359" citStr="Choueka 1988" startWordPosition="8376" endWordPosition="8377">ndustrial average.&amp;quot; &amp;quot;advancing-market&amp;quot; &amp;quot;the broader market in the NYSE advancing issues&amp;quot; &amp;quot;block-trading&amp;quot; &amp;quot;Jack Baker head of block trading in Shearson Lehman Brothers Inc.&amp;quot; &amp;quot;blue-stocks&amp;quot; &amp;quot;blue chip stocks&amp;quot; &amp;quot;cable-television&amp;quot; &amp;quot;cable television&amp;quot; &amp;quot;consumer index&amp;quot; &amp;quot;The consumer price index&amp;quot; Figure 8 Example output collocations of stage two. and respectively replaced by: &amp;quot;blue chip stocks,&amp;quot; &amp;quot;air traffic controllers,&amp;quot; and &amp;quot;the broader market in the NYSE advancing issues.&amp;quot; Thus stage 2 produces n-word collocations from two-word associations. Producing n-word collocations has already been done (e.g., Choueka 1988).&amp;quot; The general method used by Choueka is the following: for each length n, (1 &lt;n &lt;6), produce all the word sequences of length n and sort them by frequency. On a 12 million—word corpus, Choueka retrieved 10 collocations of length six, 115 collocations of length five, 1,024 collocations of length four, 4,777 of length three, and some 15,973 of length two. The threshold imposed was 14. The method we presented in this section has three main advantages when compared to a straight n-gram method like Choueka&apos;s. 1. Stage 2 retrieves phrasal templates in addition to simple rigid noun phrases. Using pa</context>
<context position="51815" citStr="Choueka 1988" startWordPosition="8621" endWordPosition="8622">sumed m-grams of a given n-gram (m &lt;n). Since stage 2 works from bigrams, and produces the biggest n-gram containing it, there is no m-gram (m &lt;n) produced that is subsumed by it. For example, although &amp;quot;shipments of arms to Iran&amp;quot; is a collocation of length five, &amp;quot;arms to Iran&amp;quot; is not an interesting collocation. It is not opaque, and does not constitute a modifier—modified syntactic relation. A straight n-gram method would retrieve both, as well as many other subsumed m-grams, such as &amp;quot;of arms to Iran.&amp;quot; A sophisticated filtering method would then be necessary to eliminate the invalid ones (See Choueka 1988). Our method avoids this problem and only produces the biggest possible n-gram, namely: &amp;quot;shipment of arms to Iran.&amp;quot; 3. Stage 2 is a simple way of compiling n-gram data. Retrieving an 11-gram by the methods used in speech, for example, would require a great deal 13 Similar approaches have been done for several applications such as Bahl, Jelinek, and Mercer (1983) and Cerf-Danon et al. (1989) for speech recognition, and Morris and Cherry (1975), Angell (1983), Kukich (1990), and Mays, Damerau, and Mercer (1990) for spelling correction (with letters instead of words). 160 Frank Smadja Retrieving </context>
<context position="62459" citStr="Choueka (1988)" startWordPosition="10402" endWordPosition="10403">ector NN securities lawyer NN securities business NN securities lawyers NN Figure 11 Some examples of syntactically labeled bigrams. to make Xtract efficient in terms of computing resources, the first stage as well as the second stage of Xtract only takes a few minutes to run on a ten-megabyte (pre-tagged) corpus. Xtract is currently being used at Columbia University for various lexical tasks. And it has been tested on many corpora, among them: several ten-megabyte corpora of news stories, a corpus, consisting of some twenty megabytes of New York Times articles, which has already been used by Choueka (1988), the Brown corpus (Francis and Ktfaera 1982), a corpus of the proceedings of the Canadian Parliament, also called the Hansards corpus, which amounts to several hundred megabytes. We are currently working on packaging Xtract to make it available to the research community. The packaged version will be portable, reusable, and faster than the one we used to write this paper.&apos; We evaluate the filtering power of stage 3 in the evaluation section, Section 10. Section 9 presents some results that we obtained with the three stages of Xtract. 9. Some Results Results obtained from The Jerusalem Post cor</context>
<context position="73862" citStr="Choueka (1988)" startWordPosition="12382" endWordPosition="12383">1. Influence of the Corpus on the Results In this section, we discuss the extent to which the results are dependent on the corpus used. To illustrate our purpose here, we are using results collected from three different corpora. The first one, DJ, for Dow Jones, is the corpus we used in this paper; it contains (mostly) stock market stories taken from the Associated Press newswire. DJ contains 8-9 million words. The second corpus, NYT, contains articles published in the New York Times during the years 1987 and 1988. The articles are on various subjects. This is the same corpus that was used by Choueka (1988). NYT contains 12 million words. The third corpus, AP, contains stories from the Associated Press newswire on various domains such as weather reports, politics, health, finances, etc. AP is 4 million words. Figure 14 represents the top 10 word associations retrieved by Xtract stage 1 for the three corpora with the word &amp;quot;price.&amp;quot; In this figure, d represents the distance between the two words and w represents the weight associated with the bigram. The weight is a combined index of the statistical distribution as discussed in Section 6, and it evaluates the collocation. There are several differen</context>
</contexts>
<marker>Choueka, 1988</marker>
<rawString>Choueka, Y. (1988). &amp;quot;Looking for needles in a haystack.&amp;quot; In Proceedings, RIAO Conference on User-Oriented Context Based Text and Image Handling, 609-623. Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
<author>W Gale</author>
</authors>
<title>Poor estimates of context are worse than none.&amp;quot;</title>
<date>1990</date>
<booktitle>In Darpa Speech and Natural Language Workshop,</booktitle>
<location>Hidden Valley, PA.</location>
<contexts>
<context position="19885" citStr="Church and Gale 1990" startWordPosition="3132" endWordPosition="3135"> compositional generation alone. 4. Related Work There has been a recent surge of research interest in corpus-based computational linguistics methods; that is, the study and elaboration of techniques using large real text as a basis. Such techniques have various applications. Speech recognition (Bahl, Jelinek, and Mercer 1983) and text compression (e.g., Bell, Witten, and Cleary 1989; Guazzo 1980) have been of long-standing interest, and some new applications are currently being investigated, such as machine translation (Brown et al. 1988), spelling correction (Mays, Damerau, and Mercer 1990; Church and Gale 1990), parsing (Debili 1982; Hindle and Rooth 1990). As pointed out by Bell, Witten, and Cleary (1989), these applications fall under two research paradigms: statistical approaches and lexical approaches. In the statistical approach, language is modeled as a stochastic process and the corpus is used to estimate probabilities. In this approach, a collocation is simply considered as a sequence of words (or n-gram) among millions of other possible sequences. In contrast, in the lexical approach, a collocation is an element of a dictionary among a few thousand other lexical items. Collocations in the l</context>
</contexts>
<marker>Church, Gale, 1990</marker>
<rawString>Church, K., and Gale, W. (1990). &amp;quot;Poor estimates of context are worse than none.&amp;quot; In Darpa Speech and Natural Language Workshop, Hidden Valley, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
<author>P Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.&amp;quot;</title>
<date>1989</date>
<booktitle>In Proceedings, 27th Meeting of the ACL, 76-83. Also in Computational Linguistics,</booktitle>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="6086" citStr="Church and Hanks (1989)" startWordPosition="951" endWordPosition="954">sh? from Benson (1990). sentences candidates &amp;quot;If a fire breaks out, the alarm will ?? &amp;quot; &amp;quot;ring, go off, sound, start&amp;quot; &amp;quot;The boy doesn&apos;t know how to ?? his bicycle&amp;quot; &amp;quot;drive, ride, conduct&amp;quot; &amp;quot;The American congress can ?? a presidential veto&amp;quot; &amp;quot;ban/cancel/delete/reject&amp;quot; &amp;quot;Before eating your bag of microwavable popcorn, &amp;quot;turn down/abrogate/overrule&amp;quot; you have to ?? it&amp;quot; &amp;quot;cook/nuke/broil/fry/bake&amp;quot; Figure 2 Fill-in-the-blank test, from Benson (1990). Xtract now works in three stages. In the first stage, pairwise lexical relations are retrieved using only statistical information. This stage is comparable to Church and Hanks (1989) in that it evaluates a certain word association between pairs of words. As in Church and Hanks (1989), the words can appear in any order and they can be separated by an arbitrary number of other words. However, the statistics we use provide more information and allow us to have more precision in our output. The output of this first stage is then passed in parallel to the next two stages. In the second stage, multiple-word combinations and complex expressions are identified. This stage produces output comparable to that of Choueka, Klein, and Neuwitz (1983); however the techniques we use are s</context>
<context position="20793" citStr="Church and Hanks 1989" startWordPosition="3276" endWordPosition="3279">s is used to estimate probabilities. In this approach, a collocation is simply considered as a sequence of words (or n-gram) among millions of other possible sequences. In contrast, in the lexical approach, a collocation is an element of a dictionary among a few thousand other lexical items. Collocations in the lexicographic meaning are only dealt with in the lexical approach. Aside from the work we present in this paper, most of the work carried out within the lexical approach has been done in computer-assisted lexicography by Choueka, Klein, and Neuwitz (1983) and Church and his colleagues (Church and Hanks 1989). Both works attempted to automatically acquire true collocations from corpora. Our work builds on Choueka&apos;s, and has been developed contemporarily to Church&apos;s. Choueka, Klein, and Neuwitz (1983) proposed algorithms to automatically retrieve idiomatic and collocational expressions. A collocation, as defined by Choueka, is a sequence of adjacent words that frequently appear together. In theory the sequences can be of any length, but in actuality, they contain two to six words. In Choueka 5 Taken from the daily reports transmitted daily by The Associated Press newswire. 149 Computational Linguis</context>
<context position="22436" citStr="Church and Hanks (1989)" startWordPosition="3523" endWordPosition="3526">me limitations. First, by definition, only uninterrupted sequences of words are retrieved; more flexible collocations such as &amp;quot;make-decision,&amp;quot; in which the two words can be separated by an arbitrary number of words, are not dealt with. Second, these techniques simply analyze the collocations according to their observed frequency in the corpus; this makes the results too dependent on the size of the corpus. Finally, at a more general level, although disambiguation was originally considered as a performance task, the collocations retrieved have not been used for any specific computational task. Church and Hanks (1989) describe a different set of techniques to retrieve collocations. A collocation as defined in their work is a pair of correlated words. That is, a collocation is a pair of words that appear together more often than expected. Church et al. (1991) improve over Choueka&apos;s work as they retrieve interrupted as well as uninterrupted sequences of words. Also, these collocations have been used by an automatic parser in order to resolve attachment ambiguities (Hindle and Rooth 1990). They use the notion of mutual information as defined in information theory (Shannon 1948; Fano 1961) in a manner similar </context>
<context position="40725" citStr="Church and Hanks (1989)" startWordPosition="6768" endWordPosition="6771">robability This actually eliminates pairs such as &amp;quot;telephone-television,&amp;quot; &amp;quot;bomb-soldier,&amp;quot; &amp;quot;trouble-problem,&amp;quot; &amp;quot;big-small,&amp;quot; and 10 The kurtosis of the distribution of the collocates probably depends on the word, and there is currently no agreement on the type of distribution that would describe them. 156 Frank Smadja Retrieving Collocations from Text: Xtract &amp;quot;doctor-nurse&amp;quot; where the two words co-occur with no real structural consistency. The two words are often used together because they are associated with the same context rather than for pure structural reasons. Many collocations retrieved in Church and Hanks (1989) were of this type, as they retrieved doctors-dentists, doctors-nurses, doctorbills, doctors-hospitals, nurses-doctor, etc., which are not collocations in the sense defined above. Such collocations are not of interest for our purpose, although they could be useful for disambiguation or other semantic purposes. Condition (C2) filters out exactly this type of collocations. pi, ?pi + (ki x iff,) (c3) Condition (C3) pulls out the interesting relative positions of the two words. Conditions (C2) and (C1) eliminate rows in the output of Step 1.2. (See Figure 2). In contrast, Condition (C3) selects co</context>
<context position="42095" citStr="Church and Hanks (1989)" startWordPosition="6989" endWordPosition="6992">ple, the pair &amp;quot;expensive-takeover&amp;quot; produced two different peaks, one with only one word in between &amp;quot;expensive&amp;quot; and &amp;quot;takeover,&amp;quot; and the other with two words. Example sentences containing the two words in the two possible positions are: &amp;quot; The provision is aimed at making a hostile takeover prohibitively expensive by enabling Borg Warner&apos;s stockholders to buy the . . . &amp;quot; &amp;quot;The pill would make a takeover attempt more expensive by allowing the retailer&apos;s shareholders to buy more company stock . . . &amp;quot; Let us note that this filtering method is an original contribution of our work. Other works such as Church and Hanks (1989) simply focus on an evaluation of the correlation of appearance of a pair of words, which is roughly equivalent to condition (C1). (See next section). However, taking note of their pattern of appearance allows us to filter out more irrelevant collocations with (C2) and (C3). This is a very important point that will allow us to filter out many invalid collocations and also produce more functional information at stages 2 and 3. A graphical interpretation of the filtering method used for Xtract is given in Smadja (1991). 7. Xtract Stage Two: From 2-Grams to N-Grams The role of the second stage of</context>
</contexts>
<marker>Church, Hanks, 1989</marker>
<rawString>Church, K., and Hanks, P. (1989). &amp;quot;Word association norms, mutual information, and lexicography.&amp;quot; In Proceedings, 27th Meeting of the ACL, 76-83. Also in Computational Linguistics, 16(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
<author>W Gale</author>
<author>P Hanks</author>
<author>D Hindle</author>
</authors>
<title>Parsing, word associations and typical predicate-argument relations.&amp;quot;</title>
<date>1989</date>
<booktitle>In Proceedings of the International Workshop on Parsing Technologies,</booktitle>
<pages>103--112</pages>
<institution>Carnegie Mellon University,</institution>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="54330" citStr="Church et al. (1989" startWordPosition="9023" endWordPosition="9026">s used as the direct object of the verb. The advent of robust parsers such as Cass (Abney 1990) and Fidditch (Hindle 1983) has made it possible to process large text corpora with good performance and thus combine statistical techniques with more symbolic analysis. In the past, some similar attempts have been done. Debili (1982) parsed corpora of French texts to identify nonambiguous predicate argument relations. He then used these relations for disambiguation. Hindle and Rooth (1990) later refined this approach by using bigram statistics to enhance the task of prepositional phrase attachment. Church et al. (1989, 1991) have yet another approach; they consider questions such as what does a boat typically do? They are preprocessing a corpus with the Fidditch parser (Hindle 1983) in order to produce a list of verbs that are most likely associated with the subject &amp;quot;boat.&amp;quot; Our goal here is different, as we analyze collocations automatically produced by the first stage of Xtract to either add syntactic information or reject them. For example, if a lexical relation identified at stage 1 involves a noun and a verb, the role of stage 3 is to determine whether it is a subject—verb or a verb—object collocation.</context>
</contexts>
<marker>Church, Gale, Hanks, Hindle, 1989</marker>
<rawString>Church, K. W.; Gale, W.; Hanks, P.; and Hindle, D. (1989). &amp;quot;Parsing, word associations and typical predicate-argument relations.&amp;quot; In Proceedings of the International Workshop on Parsing Technologies, 103-112. Carnegie Mellon University, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
<author>W Gale</author>
<author>P Hanks</author>
<author>D Hindle</author>
</authors>
<title>Using statistics in lexical analysis.&amp;quot; In Lexical Acquisition: Using On-Line Resources to Build a Lexicon, edited by Uri Zernik. Lawrence Erlbaum.</title>
<date>1991</date>
<contexts>
<context position="22681" citStr="Church et al. (1991)" startWordPosition="3566" endWordPosition="3569">e techniques simply analyze the collocations according to their observed frequency in the corpus; this makes the results too dependent on the size of the corpus. Finally, at a more general level, although disambiguation was originally considered as a performance task, the collocations retrieved have not been used for any specific computational task. Church and Hanks (1989) describe a different set of techniques to retrieve collocations. A collocation as defined in their work is a pair of correlated words. That is, a collocation is a pair of words that appear together more often than expected. Church et al. (1991) improve over Choueka&apos;s work as they retrieve interrupted as well as uninterrupted sequences of words. Also, these collocations have been used by an automatic parser in order to resolve attachment ambiguities (Hindle and Rooth 1990). They use the notion of mutual information as defined in information theory (Shannon 1948; Fano 1961) in a manner similar to what has been used in speech recognition (e.g., Ephraim and Rabiner 1990), or text compression (e.g., Bell, Witten, and Cleary 1989), to evaluate the correlation of common appearances of pairs of words. Their work, however, has some limitatio</context>
</contexts>
<marker>Church, Gale, Hanks, Hindle, 1991</marker>
<rawString>Church, K.; Gale, W.; Hanks, P.; and Hindle, D. (1991). &amp;quot;Using statistics in lexical analysis.&amp;quot; In Lexical Acquisition: Using On-Line Resources to Build a Lexicon, edited by Uri Zernik. Lawrence Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
</authors>
<title>Stochastic parts program and noun phrase parser for unrestricted text.&amp;quot;</title>
<date>1988</date>
<booktitle>In Proceedings, Second Conference on Applied Natural Language Processing.</booktitle>
<location>Austin, TX.</location>
<contexts>
<context position="28378" citStr="Church (1988)" startWordPosition="4477" endWordPosition="4478">ify relevant pairs of words. These techniques are based on the assumptions that if two words are involved in a collocation then: • the words must appear together significantly more often than expected by chance. • because of syntactic constraints the words should appear in a relatively rigid way.&apos; These two assumptions are used to analyze the word distributions, and we base our filtering techniques on them. 6.1 Presentation of the Method In this stage as well as in the two others, we often need part-of-speech information for several purposes. Stochastic part-of-speech taggers such as those in Church (1988) and 6 This fact is being seriously challenged by current research (e.g., Abney 1990; Hindle 1983), and might not be true in the near future. 7 Not crossing sentence boundaries. 8 This is obviously not true for nonconfigurational languages. Although we do believe that the methods described in this paper can be applied to many languages, we have only used them on English texts. 151 Computational Linguistics Volume 19, Number 1 Garside and Leech (1987) have been shown to reach 95-99% performance on free-style text. We preprocessed the corpus with a stochastic part-of-speech tagger developed at B</context>
</contexts>
<marker>Church, 1988</marker>
<rawString>Church, K. (1988). &amp;quot;Stochastic parts program and noun phrase parser for unrestricted text.&amp;quot; In Proceedings, Second Conference on Applied Natural Language Processing. Austin, TX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Cowie</author>
</authors>
<title>The treatment of collocations and idioms in learner&apos;s dictionaries.&amp;quot;</title>
<date>1981</date>
<journal>Applied Linguistics,</journal>
<volume>2</volume>
<issue>3</issue>
<pages>223--235</pages>
<contexts>
<context position="13075" citStr="Cowie 1981" startWordPosition="2046" endWordPosition="2047"> that they are very often repeated in a given context. Word combinations such as &amp;quot;to make a decision, to hit a record, to perform an operation&amp;quot; are typical of the language, and collocations such as &amp;quot;to buy short,&amp;quot; &amp;quot;to ease the jib&amp;quot; are characteristic of specific domains. Both types are repeatedly used in specific contexts. 2.4 Collocations Are Cohesive Lexical Clusters By cohesive2 clusters, we mean that the presence of one or several words of the collocations often implies or suggests the rest of the collocation. This is the property mostly used by lexicographers when compiling collocations (Cowie 1981; Benson 1989a). Lexicographers use other people&apos;s linguistic judgment for deciding what is and what is not a collocation. They give questionnaires to people such as the one given in Figure 2. This questionnaire contains sentences used by Benson for compiling collocational knowledge for the BBI (Benson 1989b). Each sentence contains an empty slot that can easily be filled in by native speakers. In contrast, second language speakers would not find the missing words automatically but would consider a long list of words having the appropriate semantic and syntactic features such as the ones given</context>
</contexts>
<marker>Cowie, 1981</marker>
<rawString>Cowie, A. P. (1981). &amp;quot;The treatment of collocations and idioms in learner&apos;s dictionaries.&amp;quot; Applied Linguistics, 2(3), 223-235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Cruse</author>
</authors>
<title>Lexical Semantics.</title>
<date>1986</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="7780" citStr="Cruse 1986" startWordPosition="1234" endWordPosition="1235">o them. The three stages of Xtract are then introduced in Section 5 and described respectively in Sections 6, 7, and 8. Some results obtained by running Xtract on several corpora are listed and discussed in Section 9. Qualitative and quantitative evaluations of our methods and of our results are discussed in Sections 10 and 11. Finally, several possible applications and tasks for Xtract are discussed in Section 12. 2. What Are Collocations? There has been a great deal of theoretical and applied work related to collocations that has resulted in different characterizations (e.g., Allerton 1984; Cruse 1986; Menuk 1981). Depending on their interests and points of view, researchers have focused on different aspects of collocations. One of the most comprehensive definition that has 145 Computational Linguistics Volume 19, Number 1 been used can be found in the lexicographic work of Benson and his colleagues (Benson 1990). The definition is the following: Definition A collocation is an arbitrary and recurrent word combination (Benson 1990). This definition, however, does not cover some aspects and properties of collocations that have consequences for a number of machine applications. For example, i</context>
<context position="13804" citStr="Cruse 1986" startWordPosition="2161" endWordPosition="2162">ion. They give questionnaires to people such as the one given in Figure 2. This questionnaire contains sentences used by Benson for compiling collocational knowledge for the BBI (Benson 1989b). Each sentence contains an empty slot that can easily be filled in by native speakers. In contrast, second language speakers would not find the missing words automatically but would consider a long list of words having the appropriate semantic and syntactic features such as the ones given in the second column. As a consequence, collocations have particular statistical distributions (e.g., Halliday 1966; Cruse 1986). This means that, for example, the probability that any two adjacent words in a sample will be &amp;quot;red herring&amp;quot; is considerably larger than the probability of &amp;quot;red&amp;quot; times the probability of &amp;quot;herring.&amp;quot; The words cannot be considered as independent variables. We take advantage of this fact to develop a set of statistical techniques for retrieving and identifying collocations from large textual corpora. 3. Three Types of Collocations Collocations come in a large variety of forms. The number of words involved as well as the way they are involved can vary a great deal. Some collocations are 2 This no</context>
<context position="26445" citStr="Cruse 1986" startWordPosition="4163" endWordPosition="4164">ial&amp;quot; produces the ngram &amp;quot;the Dow Jones industrial average,&amp;quot; since the words are always used within rigid noun phrases in the training corpus. In the third stage, described in Section 8, Xtract adds syntactic information to collocations retrieved at the first stage and filters out inappropriate ones. For example, if a bigram involves a noun and a verb, this stage identifies it either as a subject-verb or as a verb-object collocation. If no such consistent relation is observed, then the collocation is rejected. 6. Xtract Stage One: Extracting Significant Bigrams According to Cruse&apos;s definition (Cruse 1986), a syntagmatic lexical relation consists of a pair of words whose common appearances within a single phrase structure are correlated. In other words, those two words appear together within a single syntactic construct more often than expected by chance. The first stage of Xtract attempts to identify such pairwise lexical relations and produce statistical information on pairs of words involved together in the corpus. Ideally, in order to identify lexical relations in a corpus one would need to first parse it to verify that the words are used in a single phrase structure. However, in practice, </context>
</contexts>
<marker>Cruse, 1986</marker>
<rawString>Cruse, D. A. (1986). Lexical Semantics. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Debili</author>
</authors>
<title>Analyse Syntactico-Semantique Fond&amp; sur une Acquisition Automatique de Relations Lexicales Semantiques. Doctoral dissertation,</title>
<date>1982</date>
<tech>These de Doctorat D&apos;état.</tech>
<institution>XI University,</institution>
<location>Paris</location>
<contexts>
<context position="19907" citStr="Debili 1982" startWordPosition="3137" endWordPosition="3138"> 4. Related Work There has been a recent surge of research interest in corpus-based computational linguistics methods; that is, the study and elaboration of techniques using large real text as a basis. Such techniques have various applications. Speech recognition (Bahl, Jelinek, and Mercer 1983) and text compression (e.g., Bell, Witten, and Cleary 1989; Guazzo 1980) have been of long-standing interest, and some new applications are currently being investigated, such as machine translation (Brown et al. 1988), spelling correction (Mays, Damerau, and Mercer 1990; Church and Gale 1990), parsing (Debili 1982; Hindle and Rooth 1990). As pointed out by Bell, Witten, and Cleary (1989), these applications fall under two research paradigms: statistical approaches and lexical approaches. In the statistical approach, language is modeled as a stochastic process and the corpus is used to estimate probabilities. In this approach, a collocation is simply considered as a sequence of words (or n-gram) among millions of other possible sequences. In contrast, in the lexical approach, a collocation is an element of a dictionary among a few thousand other lexical items. Collocations in the lexicographic meaning a</context>
<context position="54040" citStr="Debili (1982)" startWordPosition="8982" endWordPosition="8983"> the previous stages are already useful for lexicography. For computational use, however, functional information is needed. For example, the collocations should have some syntactic properties. It is not enough to say that &amp;quot;make&amp;quot; goes with &amp;quot;decision&amp;quot;; we need to know that &amp;quot;decision&amp;quot; is used as the direct object of the verb. The advent of robust parsers such as Cass (Abney 1990) and Fidditch (Hindle 1983) has made it possible to process large text corpora with good performance and thus combine statistical techniques with more symbolic analysis. In the past, some similar attempts have been done. Debili (1982) parsed corpora of French texts to identify nonambiguous predicate argument relations. He then used these relations for disambiguation. Hindle and Rooth (1990) later refined this approach by using bigram statistics to enhance the task of prepositional phrase attachment. Church et al. (1989, 1991) have yet another approach; they consider questions such as what does a boat typically do? They are preprocessing a corpus with the Fidditch parser (Hindle 1983) in order to produce a list of verbs that are most likely associated with the subject &amp;quot;boat.&amp;quot; Our goal here is different, as we analyze colloc</context>
</contexts>
<marker>Debili, 1982</marker>
<rawString>Debili, F. (1982). Analyse Syntactico-Semantique Fond&amp; sur une Acquisition Automatique de Relations Lexicales Semantiques. Doctoral dissertation, Paris XI University, Orsay, France. These de Doctorat D&apos;état.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Dellenbaugh</author>
<author>B Dellenbaugh</author>
</authors>
<title>Small Boat Sailing, a Complete Guide. Sports Illustrated Winner&apos;s Circle Books.</title>
<date>1990</date>
<contexts>
<context position="10806" citStr="Dellenbaugh and Dellenbaugh 1990" startWordPosition="1696" endWordPosition="1699"> from one language to another requires more than a good knowledge of the syntactic structure and the semantic representation. Because collocations are arbitrary, they must be readily available in both languages for effective machine translation. 2.2 Collocations Are Domain-Dependent In addition to nontechnical collocations such as the ones presented before, domainspecific collocations are numerous. Technical jargons are often totally unintelligible for the layman. They contain a large number of technical terms. In addition, familiar words seem to be used differently. In the domain of sailing (Dellenbaugh and Dellenbaugh 1990), for example, some words are unknown to the nonfamiliar reader: rigg, jib, and leeward are totally meaningless to the layman. Some other combinations apparently do not contain any technical words, but these words take on a totally different meaning in the domain. For example, a dry suit is not a suit that is dry but a special type of suit used by sailors to stay dry in difficult weather conditions. Similarly a wet suit is a special kind of suit used for several marine activities. Native speakers are often unaware of the arbitrariness of collocations in nontechnical core English; however, this</context>
</contexts>
<marker>Dellenbaugh, Dellenbaugh, 1990</marker>
<rawString>Dellenbaugh, D., and Dellenbaugh, B. (1990). Small Boat Sailing, a Complete Guide. Sports Illustrated Winner&apos;s Circle Books.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Todorov</author>
</authors>
<title>Encyclopedic Dictionary of the Sciences of Language.</title>
<date>1979</date>
<publisher>John Hopkins University Press.</publisher>
<contexts>
<context position="87487" citStr="Todorov 1979" startWordPosition="14632" endWordPosition="14633">mportant grammatical collocations and treat the others inconsistently.&apos; Xtract can be used without modification to retrieve noun—preposition collocations. Figure 16 lists such collocations as retrieved by Xtract. Many of the associations retrieved are effectively collocations: &amp;quot;absence of, accordance with, accuracy of, advantage of, aftershock from, agreement on, allegations of, anxiety about, aspect of,&amp;quot; etc. 12.3 Some Determiner—Noun Problems Determiners are lexical elements that are used in conjunction with a noun to bring into correspondence with it a certain sector of reality (Ducrot and Todorov 1979). A noun without determiner has no referent. The role of determiner can be played by several classes of items: articles, (e.g., &amp;quot;a,&amp;quot; &amp;quot;the&amp;quot;), possessives (e.g., &amp;quot;my,&amp;quot; &amp;quot;your&amp;quot;), indefinite adjectives (e.g., &amp;quot;some,&amp;quot; &amp;quot;many,&amp;quot; &amp;quot;few,&amp;quot; &amp;quot;certain&amp;quot;), demonstratives (e.g., &amp;quot;this,&amp;quot; &amp;quot;those&amp;quot;), numbers, etc. Determiner—noun combinations are often based simply on semantic or syntactic criteria. For example in the expression &amp;quot;my left foot,&amp;quot; the determiner &amp;quot;my&amp;quot; is here for semantic reasons. Any other determiner would fail to identify the correct object (my left foot). Classes of nouns such as mass and count are s</context>
</contexts>
<marker>Todorov, 1979</marker>
<rawString>Ducrot, 0., and Todorov, T. (1979). Encyclopedic Dictionary of the Sciences of Language. John Hopkins University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Elhadad</author>
</authors>
<title>Types in functional unification grammars.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, 28th Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="83500" citStr="Elhadad 1990" startWordPosition="14038" endWordPosition="14039">formalisms available did not handle the variability of collocational knowledge. In contrast, we use Xtract to produce the collocations and we use Functional Unification Grammars (FUGs) (Kay 1979) as a representation formalism and a unification engine. We show how the use of FUGs allows us to properly handle the interactions of collocational and various other constraints. We have implemented Cook, a surface sentence generator that uses a flexible lexicon for expressing collocational constraints in the stock market domain. Using Ana (Kukich 1983) as a deep generator, Cook is implemented in FUF (Elhadad 1990), an extended implementation of FUG, and uniformly represents the lexicon and syntax as originally suggested by Halliday (1966). For a more detailed description of Cook the reader is referred to Smadja (1991). 12.2 Retrieving Grammatical Collocations According to Benson, Benson, and Ilson (1986a), collocations fall into two major groups: lexical collocations and grammatical collocations. The difference between these two groups lies in the types of words involved. Lexical collocations roughly consist of syntagmatic affinities among open class words such as verbs, nouns, adjectives, and adverbs.</context>
</contexts>
<marker>Elhadad, 1990</marker>
<rawString>Elhadad, M. (1990). &amp;quot;Types in functional unification grammars.&amp;quot; In Proceedings, 28th Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Ephraim</author>
<author>L Rabiner</author>
</authors>
<title>On the relations between modeling approaches for speech recognition.&amp;quot;</title>
<date>1990</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>36</volume>
<issue>2</issue>
<pages>372--380</pages>
<contexts>
<context position="23112" citStr="Ephraim and Rabiner 1990" startWordPosition="3634" endWordPosition="3637"> collocations. A collocation as defined in their work is a pair of correlated words. That is, a collocation is a pair of words that appear together more often than expected. Church et al. (1991) improve over Choueka&apos;s work as they retrieve interrupted as well as uninterrupted sequences of words. Also, these collocations have been used by an automatic parser in order to resolve attachment ambiguities (Hindle and Rooth 1990). They use the notion of mutual information as defined in information theory (Shannon 1948; Fano 1961) in a manner similar to what has been used in speech recognition (e.g., Ephraim and Rabiner 1990), or text compression (e.g., Bell, Witten, and Cleary 1989), to evaluate the correlation of common appearances of pairs of words. Their work, however, has some limitations too. First, by definition, it can only retrieve collocations of length two. This limitation is intrinsic to the technique used since mutual information scores are defined for two items. The second limitation is that many collocations identified in Church and Hanks (1989) do not really identify true collocations, but simply pairs of words that frequently appear together such as the pairs &amp;quot;doctor-nurse,&amp;quot; &amp;quot;doctor-bill,&amp;quot; &amp;quot;doctor</context>
<context position="43007" citStr="Ephraim and Rabiner 1990" startWordPosition="7143" endWordPosition="7146">ery important point that will allow us to filter out many invalid collocations and also produce more functional information at stages 2 and 3. A graphical interpretation of the filtering method used for Xtract is given in Smadja (1991). 7. Xtract Stage Two: From 2-Grams to N-Grams The role of the second stage of Xtract is twofold. It produces collocations involving more than two words, and it filters out some pairwise relations. Stage 2 is related to the work of Choueka (1988), and to some extent to what has been done in speech recognition (e.g., Bahl, Jelinek, and Mercer 1983; Merialdo 1987; Ephraim and Rabiner 1990). 7.1 Presentation of the Method In this second stage, Xtract uses the same components used for the first stage but in a different way. It starts with the pairwise lexical relations produced in stage 1 and produces multiple word collocations, such as rigid noun phrases or phrasal templates, from them. To do this, Xtract studies the lexical relations in context, which is exactly what lexicographers do. For each bigram identified at the previous stage, Xtract examines all instances of appearance of the two words and analyzes the distributions of words and parts of speech in the surrounding posit</context>
</contexts>
<marker>Ephraim, Rabiner, 1990</marker>
<rawString>Ephraim, Y., and Rabiner, L. (1990). &amp;quot;On the relations between modeling approaches for speech recognition.&amp;quot; IEEE Transactions on Information Theory, 36(2), 372-380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Fano</author>
</authors>
<title>Transmission of Information: A Statistical Theory of Information.</title>
<date>1961</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="23015" citStr="Fano 1961" startWordPosition="3619" endWordPosition="3620">l task. Church and Hanks (1989) describe a different set of techniques to retrieve collocations. A collocation as defined in their work is a pair of correlated words. That is, a collocation is a pair of words that appear together more often than expected. Church et al. (1991) improve over Choueka&apos;s work as they retrieve interrupted as well as uninterrupted sequences of words. Also, these collocations have been used by an automatic parser in order to resolve attachment ambiguities (Hindle and Rooth 1990). They use the notion of mutual information as defined in information theory (Shannon 1948; Fano 1961) in a manner similar to what has been used in speech recognition (e.g., Ephraim and Rabiner 1990), or text compression (e.g., Bell, Witten, and Cleary 1989), to evaluate the correlation of common appearances of pairs of words. Their work, however, has some limitations too. First, by definition, it can only retrieve collocations of length two. This limitation is intrinsic to the technique used since mutual information scores are defined for two items. The second limitation is that many collocations identified in Church and Hanks (1989) do not really identify true collocations, but simply pairs </context>
</contexts>
<marker>Fano, 1961</marker>
<rawString>Fano, R. (1961). Transmission of Information: A Statistical Theory of Information. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Flexner</author>
<author>ed</author>
</authors>
<date>1987</date>
<booktitle>The Random House Dictionary of the English Language, Second Edition.</booktitle>
<publisher>Random House.</publisher>
<marker>Flexner, ed, 1987</marker>
<rawString>Flexner, S., ed. (1987). The Random House Dictionary of the English Language, Second Edition. Random House.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Francis</author>
<author>H Kue&apos;era</author>
</authors>
<title>Frequency Analysis of English Usage.</title>
<date>1982</date>
<publisher>Houghton Mifflin.</publisher>
<marker>Francis, Kue&apos;era, 1982</marker>
<rawString>Francis, W., and Kue&apos;era, H. (1982). Frequency Analysis of English Usage. Houghton Mifflin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Garside</author>
<author>G Leech</author>
</authors>
<title>The Computational Analysis of English, a Corpus Based Approach.</title>
<date>1987</date>
<publisher>Longman.</publisher>
<contexts>
<context position="28832" citStr="Garside and Leech (1987)" startWordPosition="4550" endWordPosition="4553">n this stage as well as in the two others, we often need part-of-speech information for several purposes. Stochastic part-of-speech taggers such as those in Church (1988) and 6 This fact is being seriously challenged by current research (e.g., Abney 1990; Hindle 1983), and might not be true in the near future. 7 Not crossing sentence boundaries. 8 This is obviously not true for nonconfigurational languages. Although we do believe that the methods described in this paper can be applied to many languages, we have only used them on English texts. 151 Computational Linguistics Volume 19, Number 1 Garside and Leech (1987) have been shown to reach 95-99% performance on free-style text. We preprocessed the corpus with a stochastic part-of-speech tagger developed at Bell Laboratories by Ken Church (Church 1988).9 In the rest of this section, we describe the algorithm used for the first stage of Xtract in some detail. We assume that the corpus is preprocessed by a part of speech tagger and we note w, a collocate of w if the two words appear in a common sentence within a distance of 5 words. Step 1.1: Producing Concordances Input: The tagged corpus, a given word w. Output: All the sentences containing w. Descriptio</context>
</contexts>
<marker>Garside, Leech, 1987</marker>
<rawString>Garside, R., and Leech, G. (1987). The Computational Analysis of English, a Corpus Based Approach. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Guazzo</author>
</authors>
<title>A general minimum-redundancy source-coding algorithm.&amp;quot;</title>
<date>1980</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>26</volume>
<issue>1</issue>
<pages>15--25</pages>
<contexts>
<context position="19664" citStr="Guazzo 1980" startWordPosition="3102" endWordPosition="3103">e, generating them from single words is often a very difficult task for a language generator. As pointed out by Kukich (1983), in general, their usage gives an impression of fluency that could not be equaled with compositional generation alone. 4. Related Work There has been a recent surge of research interest in corpus-based computational linguistics methods; that is, the study and elaboration of techniques using large real text as a basis. Such techniques have various applications. Speech recognition (Bahl, Jelinek, and Mercer 1983) and text compression (e.g., Bell, Witten, and Cleary 1989; Guazzo 1980) have been of long-standing interest, and some new applications are currently being investigated, such as machine translation (Brown et al. 1988), spelling correction (Mays, Damerau, and Mercer 1990; Church and Gale 1990), parsing (Debili 1982; Hindle and Rooth 1990). As pointed out by Bell, Witten, and Cleary (1989), these applications fall under two research paradigms: statistical approaches and lexical approaches. In the statistical approach, language is modeled as a stochastic process and the corpus is used to estimate probabilities. In this approach, a collocation is simply considered as </context>
</contexts>
<marker>Guazzo, 1980</marker>
<rawString>Guazzo, M. (1980). &amp;quot;A general minimum-redundancy source-coding algorithm.&amp;quot; IEEE Transactions on Information Theory, IT-26(1), 15-25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A K Halliday</author>
<author>R Hasan</author>
</authors>
<date>1976</date>
<note>Cohesion in English. Longman.</note>
<contexts>
<context position="14510" citStr="Halliday and Hasan 1976" startWordPosition="2276" endWordPosition="2279">ample will be &amp;quot;red herring&amp;quot; is considerably larger than the probability of &amp;quot;red&amp;quot; times the probability of &amp;quot;herring.&amp;quot; The words cannot be considered as independent variables. We take advantage of this fact to develop a set of statistical techniques for retrieving and identifying collocations from large textual corpora. 3. Three Types of Collocations Collocations come in a large variety of forms. The number of words involved as well as the way they are involved can vary a great deal. Some collocations are 2 This notion of cohesion should not be confused with the cohesion as defined by Halliday (Halliday and Hasan 1976). Here we are dealing with a more lexical type of cohesion. 147 Computational Linguistics Volume 19, Number 1 &amp;quot;The NYSE&apos;s composite index of all its listed common stocks rose NUMBER* to *NUMBER*&amp;quot; &amp;quot;On the American Stock Exchange the market value index was up NUMBER* at *NUMBER*&amp;quot; &amp;quot;The Dow Jones average of 30 industrials fell NUMBER* points to *NUMBER*&amp;quot; &amp;quot;The closely watched index had been down about *NUMBER* points in the first hour of trading&amp;quot; &amp;quot;The average finished the week with a net loss of *NUMBER*&amp;quot; Figure 4 Some examples of phrasal templates. very rigid, whereas others are very flexible. For</context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>Halliday, M. A. K., and Hasan, R. (1976). Cohesion in English. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A K Halliday</author>
</authors>
<title>Lexis as a linguistic level.&amp;quot; In</title>
<date>1966</date>
<booktitle>In Memory of</booktitle>
<pages>148--162</pages>
<institution>Longmans Linguistics Library.</institution>
<contexts>
<context position="13791" citStr="Halliday 1966" startWordPosition="2158" endWordPosition="2160"> not a collocation. They give questionnaires to people such as the one given in Figure 2. This questionnaire contains sentences used by Benson for compiling collocational knowledge for the BBI (Benson 1989b). Each sentence contains an empty slot that can easily be filled in by native speakers. In contrast, second language speakers would not find the missing words automatically but would consider a long list of words having the appropriate semantic and syntactic features such as the ones given in the second column. As a consequence, collocations have particular statistical distributions (e.g., Halliday 1966; Cruse 1986). This means that, for example, the probability that any two adjacent words in a sample will be &amp;quot;red herring&amp;quot; is considerably larger than the probability of &amp;quot;red&amp;quot; times the probability of &amp;quot;herring.&amp;quot; The words cannot be considered as independent variables. We take advantage of this fact to develop a set of statistical techniques for retrieving and identifying collocations from large textual corpora. 3. Three Types of Collocations Collocations come in a large variety of forms. The number of words involved as well as the way they are involved can vary a great deal. Some collocations </context>
<context position="83627" citStr="Halliday (1966)" startWordPosition="14056" endWordPosition="14057">locations and we use Functional Unification Grammars (FUGs) (Kay 1979) as a representation formalism and a unification engine. We show how the use of FUGs allows us to properly handle the interactions of collocational and various other constraints. We have implemented Cook, a surface sentence generator that uses a flexible lexicon for expressing collocational constraints in the stock market domain. Using Ana (Kukich 1983) as a deep generator, Cook is implemented in FUF (Elhadad 1990), an extended implementation of FUG, and uniformly represents the lexicon and syntax as originally suggested by Halliday (1966). For a more detailed description of Cook the reader is referred to Smadja (1991). 12.2 Retrieving Grammatical Collocations According to Benson, Benson, and Ilson (1986a), collocations fall into two major groups: lexical collocations and grammatical collocations. The difference between these two groups lies in the types of words involved. Lexical collocations roughly consist of syntagmatic affinities among open class words such as verbs, nouns, adjectives, and adverbs. In contrast, grammatical collocations generally involve at least one closed class word among particles, prepositions, and auxi</context>
</contexts>
<marker>Halliday, 1966</marker>
<rawString>Halliday, M. A. K. (1966). &amp;quot;Lexis as a linguistic level.&amp;quot; In In Memory of J. R. Firth, edited by C. E. Bazell, J. C. Catford, M. A. K. Halliday, and R. H. Robins, 148-162. Longmans Linguistics Library.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
<author>M Rooth</author>
</authors>
<title>Structural ambiguity and lexical relations.&amp;quot;</title>
<date>1990</date>
<booktitle>In DARPA Speech and Natural Language Workshop,</booktitle>
<location>Hidden Valley, PA.</location>
<contexts>
<context position="19931" citStr="Hindle and Rooth 1990" startWordPosition="3139" endWordPosition="3142">ork There has been a recent surge of research interest in corpus-based computational linguistics methods; that is, the study and elaboration of techniques using large real text as a basis. Such techniques have various applications. Speech recognition (Bahl, Jelinek, and Mercer 1983) and text compression (e.g., Bell, Witten, and Cleary 1989; Guazzo 1980) have been of long-standing interest, and some new applications are currently being investigated, such as machine translation (Brown et al. 1988), spelling correction (Mays, Damerau, and Mercer 1990; Church and Gale 1990), parsing (Debili 1982; Hindle and Rooth 1990). As pointed out by Bell, Witten, and Cleary (1989), these applications fall under two research paradigms: statistical approaches and lexical approaches. In the statistical approach, language is modeled as a stochastic process and the corpus is used to estimate probabilities. In this approach, a collocation is simply considered as a sequence of words (or n-gram) among millions of other possible sequences. In contrast, in the lexical approach, a collocation is an element of a dictionary among a few thousand other lexical items. Collocations in the lexicographic meaning are only dealt with in th</context>
<context position="22913" citStr="Hindle and Rooth 1990" startWordPosition="3601" endWordPosition="3604">ally considered as a performance task, the collocations retrieved have not been used for any specific computational task. Church and Hanks (1989) describe a different set of techniques to retrieve collocations. A collocation as defined in their work is a pair of correlated words. That is, a collocation is a pair of words that appear together more often than expected. Church et al. (1991) improve over Choueka&apos;s work as they retrieve interrupted as well as uninterrupted sequences of words. Also, these collocations have been used by an automatic parser in order to resolve attachment ambiguities (Hindle and Rooth 1990). They use the notion of mutual information as defined in information theory (Shannon 1948; Fano 1961) in a manner similar to what has been used in speech recognition (e.g., Ephraim and Rabiner 1990), or text compression (e.g., Bell, Witten, and Cleary 1989), to evaluate the correlation of common appearances of pairs of words. Their work, however, has some limitations too. First, by definition, it can only retrieve collocations of length two. This limitation is intrinsic to the technique used since mutual information scores are defined for two items. The second limitation is that many collocat</context>
<context position="54199" citStr="Hindle and Rooth (1990)" startWordPosition="9003" endWordPosition="9006">ions should have some syntactic properties. It is not enough to say that &amp;quot;make&amp;quot; goes with &amp;quot;decision&amp;quot;; we need to know that &amp;quot;decision&amp;quot; is used as the direct object of the verb. The advent of robust parsers such as Cass (Abney 1990) and Fidditch (Hindle 1983) has made it possible to process large text corpora with good performance and thus combine statistical techniques with more symbolic analysis. In the past, some similar attempts have been done. Debili (1982) parsed corpora of French texts to identify nonambiguous predicate argument relations. He then used these relations for disambiguation. Hindle and Rooth (1990) later refined this approach by using bigram statistics to enhance the task of prepositional phrase attachment. Church et al. (1989, 1991) have yet another approach; they consider questions such as what does a boat typically do? They are preprocessing a corpus with the Fidditch parser (Hindle 1983) in order to produce a list of verbs that are most likely associated with the subject &amp;quot;boat.&amp;quot; Our goal here is different, as we analyze collocations automatically produced by the first stage of Xtract to either add syntactic information or reject them. For example, if a lexical relation identified at</context>
</contexts>
<marker>Hindle, Rooth, 1990</marker>
<rawString>Hindle, D., and Rooth, M. (1990). &amp;quot;Structural ambiguity and lexical relations.&amp;quot; In DARPA Speech and Natural Language Workshop, Hidden Valley, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
</authors>
<title>User manual for Frank Smadja Retrieving Collocations from Text: Xtract fidditch, a deterministic parser.&amp;quot;</title>
<date>1983</date>
<tech>Technical Memorandum 7590-142,</tech>
<institution>Naval Research Laboratory.</institution>
<contexts>
<context position="28476" citStr="Hindle 1983" startWordPosition="4493" endWordPosition="4494">volved in a collocation then: • the words must appear together significantly more often than expected by chance. • because of syntactic constraints the words should appear in a relatively rigid way.&apos; These two assumptions are used to analyze the word distributions, and we base our filtering techniques on them. 6.1 Presentation of the Method In this stage as well as in the two others, we often need part-of-speech information for several purposes. Stochastic part-of-speech taggers such as those in Church (1988) and 6 This fact is being seriously challenged by current research (e.g., Abney 1990; Hindle 1983), and might not be true in the near future. 7 Not crossing sentence boundaries. 8 This is obviously not true for nonconfigurational languages. Although we do believe that the methods described in this paper can be applied to many languages, we have only used them on English texts. 151 Computational Linguistics Volume 19, Number 1 Garside and Leech (1987) have been shown to reach 95-99% performance on free-style text. We preprocessed the corpus with a stochastic part-of-speech tagger developed at Bell Laboratories by Ken Church (Church 1988).9 In the rest of this section, we describe the algori</context>
<context position="53833" citStr="Hindle 1983" startWordPosition="8950" endWordPosition="8951"> start from an included significant bigram (for example, &amp;quot;Dow-30&amp;quot;) and we directly retrieve the surrounding n-grams.&apos; 8. Xtract Stage Three: Adding Syntax to the Collocations The collocations as produced in the previous stages are already useful for lexicography. For computational use, however, functional information is needed. For example, the collocations should have some syntactic properties. It is not enough to say that &amp;quot;make&amp;quot; goes with &amp;quot;decision&amp;quot;; we need to know that &amp;quot;decision&amp;quot; is used as the direct object of the verb. The advent of robust parsers such as Cass (Abney 1990) and Fidditch (Hindle 1983) has made it possible to process large text corpora with good performance and thus combine statistical techniques with more symbolic analysis. In the past, some similar attempts have been done. Debili (1982) parsed corpora of French texts to identify nonambiguous predicate argument relations. He then used these relations for disambiguation. Hindle and Rooth (1990) later refined this approach by using bigram statistics to enhance the task of prepositional phrase attachment. Church et al. (1989, 1991) have yet another approach; they consider questions such as what does a boat typically do? They </context>
</contexts>
<marker>Hindle, 1983</marker>
<rawString>Hindle, D. (1983). &amp;quot;User manual for Frank Smadja Retrieving Collocations from Text: Xtract fidditch, a deterministic parser.&amp;quot; Technical Memorandum 7590-142, Naval Research Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
</authors>
<title>Functional grammar.&amp;quot;</title>
<date>1979</date>
<booktitle>In Proceedings, 5th Meeting of the</booktitle>
<institution>Berkeley Linguistics Society. Berkeley Linguistics Society.</institution>
<contexts>
<context position="83082" citStr="Kay 1979" startWordPosition="13972" endWordPosition="13973">tion we discuss several uses of Xtract. 12.1 Language Generation Language generation is a novel application for Corpus-Based Computational Linguistics (Boguraev 1989). In Smadja (1991) we show how collocations enhance the task of lexical selection in language generation. Previous language generation works did not use collocations mainly because they did not have the information in compiled form and the lexicon formalisms available did not handle the variability of collocational knowledge. In contrast, we use Xtract to produce the collocations and we use Functional Unification Grammars (FUGs) (Kay 1979) as a representation formalism and a unification engine. We show how the use of FUGs allows us to properly handle the interactions of collocational and various other constraints. We have implemented Cook, a surface sentence generator that uses a flexible lexicon for expressing collocational constraints in the stock market domain. Using Ana (Kukich 1983) as a deep generator, Cook is implemented in FUF (Elhadad 1990), an extended implementation of FUG, and uniformly represents the lexicon and syntax as originally suggested by Halliday (1966). For a more detailed description of Cook the reader is</context>
</contexts>
<marker>Kay, 1979</marker>
<rawString>Kay, M. (1979). &amp;quot;Functional grammar.&amp;quot; In Proceedings, 5th Meeting of the Berkeley Linguistics Society. Berkeley Linguistics Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kukich</author>
</authors>
<title>Knowledge-based report generation: A technique for automatically generating natural language reports from databases.&amp;quot;</title>
<date>1983</date>
<booktitle>In Proceedings, Sixth International ACM SIGIR, Conference on Research and Development in Information Retrieval.</booktitle>
<location>Washington, D.C.</location>
<contexts>
<context position="19177" citStr="Kukich (1983)" startWordPosition="3028" endWordPosition="3029">ty slots. Phrasal templates are quite representative of a given domain and are very often repeated in a rigid way in a given sublanguage. In the domain of weather reports, for example, the sentence &amp;quot;Temperatures indicate previous day&apos;s high and overnight low to 8 a.m.&amp;quot; is actually repeated before each weather report.&apos; Unlike rigid noun phrases and predicative relations, phrasal templates are specifically useful for language generation. Because of their slightly idiosyncratic structure, generating them from single words is often a very difficult task for a language generator. As pointed out by Kukich (1983), in general, their usage gives an impression of fluency that could not be equaled with compositional generation alone. 4. Related Work There has been a recent surge of research interest in corpus-based computational linguistics methods; that is, the study and elaboration of techniques using large real text as a basis. Such techniques have various applications. Speech recognition (Bahl, Jelinek, and Mercer 1983) and text compression (e.g., Bell, Witten, and Cleary 1989; Guazzo 1980) have been of long-standing interest, and some new applications are currently being investigated, such as machine</context>
<context position="83437" citStr="Kukich 1983" startWordPosition="14027" endWordPosition="14028">did not have the information in compiled form and the lexicon formalisms available did not handle the variability of collocational knowledge. In contrast, we use Xtract to produce the collocations and we use Functional Unification Grammars (FUGs) (Kay 1979) as a representation formalism and a unification engine. We show how the use of FUGs allows us to properly handle the interactions of collocational and various other constraints. We have implemented Cook, a surface sentence generator that uses a flexible lexicon for expressing collocational constraints in the stock market domain. Using Ana (Kukich 1983) as a deep generator, Cook is implemented in FUF (Elhadad 1990), an extended implementation of FUG, and uniformly represents the lexicon and syntax as originally suggested by Halliday (1966). For a more detailed description of Cook the reader is referred to Smadja (1991). 12.2 Retrieving Grammatical Collocations According to Benson, Benson, and Ilson (1986a), collocations fall into two major groups: lexical collocations and grammatical collocations. The difference between these two groups lies in the types of words involved. Lexical collocations roughly consist of syntagmatic affinities among </context>
</contexts>
<marker>Kukich, 1983</marker>
<rawString>Kukich, K. (1983). &amp;quot;Knowledge-based report generation: A technique for automatically generating natural language reports from databases.&amp;quot; In Proceedings, Sixth International ACM SIGIR, Conference on Research and Development in Information Retrieval. Washington, D.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kukich</author>
</authors>
<title>A comparison of some novel and traditional lexical distances metrics for spelling correction.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings, International Neural Networks Conference (INNC).</booktitle>
<location>Paris, France.</location>
<contexts>
<context position="52291" citStr="Kukich (1990)" startWordPosition="8699" endWordPosition="8700">ams, such as &amp;quot;of arms to Iran.&amp;quot; A sophisticated filtering method would then be necessary to eliminate the invalid ones (See Choueka 1988). Our method avoids this problem and only produces the biggest possible n-gram, namely: &amp;quot;shipment of arms to Iran.&amp;quot; 3. Stage 2 is a simple way of compiling n-gram data. Retrieving an 11-gram by the methods used in speech, for example, would require a great deal 13 Similar approaches have been done for several applications such as Bahl, Jelinek, and Mercer (1983) and Cerf-Danon et al. (1989) for speech recognition, and Morris and Cherry (1975), Angell (1983), Kukich (1990), and Mays, Damerau, and Mercer (1990) for spelling correction (with letters instead of words). 160 Frank Smadja Retrieving Collocations from Text: Xtract of CPU time and space. In a 10 million—word corpus, with about 60,000 different words, there are about 3.6 x 109 possible bigrams, 2.16 x 1014 trigrams, and 3 x 1033 7-grams. This rapidly gets out of hand. Choueka, for example, had to stop at length six. In contrast, the rigid noun phrases we retrieve are of arbitrary length and are retrieved very easily and in one pass. The method we use starts from bigrams and produces the biggest possible</context>
</contexts>
<marker>Kukich, 1990</marker>
<rawString>Kukich, K. (1990). &amp;quot;A comparison of some novel and traditional lexical distances metrics for spelling correction.&amp;quot; In Proceedings, International Neural Networks Conference (INNC). Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
</authors>
<title>Tutorial on tagging and processing large textual corpora.&amp;quot;</title>
<date>1990</date>
<booktitle>Presented at the 28th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="70766" citStr="Marcus 1990" startWordPosition="11819" endWordPosition="11820">wer than 400 are serious candidates for the OED, which represents a current rate of 4%. Automatically producing lists of candidate expressions could actually be of great help to lexicographers, and even a precision of 40% would be helpful. Such lexicographic tools could, for example, help readers retrieve sublanguage-specific expressions by providing them with lists of candidate collocations. The lexicographer then manually examines the list to remove the irrelevant data. Even low precision is useful for lexicographers, as manual filtering is much faster than manual scanning of the documents (Marcus 1990). Such techniques are not able to replace readers, though, as they are not designed to identify low-frequency expressions, whereas a human reader immediately identifies interesting expressions with as few as one occurrence. The second stage of this experiment was to use Xtract stage 3 to filter out and label the sample set of collocations. As described in Section 8, there are several valid labels (VO, VS, NN, etc.). In this experiment, we grouped them under a single label: T. There is only one nonvalid label: U (for unlabeled). A T collocation is thus accepted by Xtract stage 3, and a U colloc</context>
</contexts>
<marker>Marcus, 1990</marker>
<rawString>Marcus, M. (1990). &amp;quot;Tutorial on tagging and processing large textual corpora.&amp;quot; Presented at the 28th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="false">
<authors>
<author>W J R Martin</author>
<author>B P E Al</author>
</authors>
<marker>Martin, Al, </marker>
<rawString>Martin, W. J. R.; Al, B. P. E; and</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J G Van Sterkenburg</author>
</authors>
<title>On the processing of a text corpus: from textual data to lexicographical information.&amp;quot;</title>
<date>1983</date>
<booktitle>In Lexicography: Principles and Practice, Applied Language Studies Series, edited</booktitle>
<publisher>Academic Press.</publisher>
<marker>Van Sterkenburg, 1983</marker>
<rawString>Van Sterkenburg, P. J. G. (1983). &amp;quot;On the processing of a text corpus: from textual data to lexicographical information.&amp;quot; In Lexicography: Principles and Practice, Applied Language Studies Series, edited by R. R. K. Hartmann. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Mays</author>
<author>E Damerau</author>
<author>R Mercer</author>
</authors>
<title>Context-based spelling correction.&amp;quot;</title>
<date>1990</date>
<booktitle>In IBM Natural Language ITL,</booktitle>
<location>Paris, France.</location>
<marker>Mays, Damerau, Mercer, 1990</marker>
<rawString>Mays, E.; Damerau, E; and Mercer, R. (1990). &amp;quot;Context-based spelling correction.&amp;quot; In IBM Natural Language ITL, Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I A Merbik</author>
</authors>
<title>Meaning-text models: a recent trend in Soviet linguistics.&amp;quot; The Annual Review of Anthropology.</title>
<date>1981</date>
<marker>Merbik, 1981</marker>
<rawString>Merbik, I. A. (1981). &amp;quot;Meaning-text models: a recent trend in Soviet linguistics.&amp;quot; The Annual Review of Anthropology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Merialdo</author>
</authors>
<title>Speech recognition with very large size dictionary.&amp;quot;</title>
<date>1987</date>
<booktitle>In Proceedings, International Conference on Acoustics, Speech, and Signal Processing (ICASSP),</booktitle>
<location>Dallas, TX.</location>
<contexts>
<context position="42980" citStr="Merialdo 1987" startWordPosition="7141" endWordPosition="7142">3). This is a very important point that will allow us to filter out many invalid collocations and also produce more functional information at stages 2 and 3. A graphical interpretation of the filtering method used for Xtract is given in Smadja (1991). 7. Xtract Stage Two: From 2-Grams to N-Grams The role of the second stage of Xtract is twofold. It produces collocations involving more than two words, and it filters out some pairwise relations. Stage 2 is related to the work of Choueka (1988), and to some extent to what has been done in speech recognition (e.g., Bahl, Jelinek, and Mercer 1983; Merialdo 1987; Ephraim and Rabiner 1990). 7.1 Presentation of the Method In this second stage, Xtract uses the same components used for the first stage but in a different way. It starts with the pairwise lexical relations produced in stage 1 and produces multiple word collocations, such as rigid noun phrases or phrasal templates, from them. To do this, Xtract studies the lexical relations in context, which is exactly what lexicographers do. For each bigram identified at the previous stage, Xtract examines all instances of appearance of the two words and analyzes the distributions of words and parts of spee</context>
</contexts>
<marker>Merialdo, 1987</marker>
<rawString>Merialdo, B. (1987). &amp;quot;Speech recognition with very large size dictionary.&amp;quot; In Proceedings, International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Dallas, TX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Morris</author>
<author>L L Cherry</author>
</authors>
<title>Computer detection of typographical errors.&amp;quot;</title>
<date>1975</date>
<journal>IEEE Transactions on Professional Communications,</journal>
<volume>18</volume>
<issue>1</issue>
<pages>54--63</pages>
<contexts>
<context position="52261" citStr="Morris and Cherry (1975)" startWordPosition="8693" endWordPosition="8696">both, as well as many other subsumed m-grams, such as &amp;quot;of arms to Iran.&amp;quot; A sophisticated filtering method would then be necessary to eliminate the invalid ones (See Choueka 1988). Our method avoids this problem and only produces the biggest possible n-gram, namely: &amp;quot;shipment of arms to Iran.&amp;quot; 3. Stage 2 is a simple way of compiling n-gram data. Retrieving an 11-gram by the methods used in speech, for example, would require a great deal 13 Similar approaches have been done for several applications such as Bahl, Jelinek, and Mercer (1983) and Cerf-Danon et al. (1989) for speech recognition, and Morris and Cherry (1975), Angell (1983), Kukich (1990), and Mays, Damerau, and Mercer (1990) for spelling correction (with letters instead of words). 160 Frank Smadja Retrieving Collocations from Text: Xtract of CPU time and space. In a 10 million—word corpus, with about 60,000 different words, there are about 3.6 x 109 possible bigrams, 2.16 x 1014 trigrams, and 3 x 1033 7-grams. This rapidly gets out of hand. Choueka, for example, had to stop at length six. In contrast, the rigid noun phrases we retrieve are of arbitrary length and are retrieved very easily and in one pass. The method we use starts from bigrams and</context>
</contexts>
<marker>Morris, Cherry, 1975</marker>
<rawString>Morris, R., and Cherry, L. L. (1975). &amp;quot;Computer detection of typographical errors.&amp;quot; IEEE Transactions on Professional Communications, PC-18(1), 54-63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A D Nakhimovsky</author>
<author>R L Leed</author>
</authors>
<title>Lexical functions and language learning.&amp;quot;</title>
<date>1979</date>
<journal>Slavic and East European Journal,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="8934" citStr="Nakhimovsky and Leed 1979" startWordPosition="1403" endWordPosition="1406">t have consequences for a number of machine applications. For example, it has been shown that collocations are difficult to translate across languages—this fact obviously has a direct application for machine translation. Many properties of collocations have been identified in the past; however, the tendency was to focus on a restricted type of collocation. In this section, we present four properties of collocations that we have identified and discuss their relevance to computational linguistics. 2.1 Collocations Are Arbitrary Collocations are difficult to produce for second language learners (Nakhimovsky and Leed 1979). In most cases, the learner cannot simply translate word-for-word what s/he would say in her/his native language. As we can see in Table 1, the word-forword translation of &amp;quot;to open the door&amp;quot; works well in both directions in all five languages. In contrast, translating word-for-word the expression: &amp;quot;to break down/force the door&amp;quot; is a poor strategy in both directions in all five languages. The co-occurrence of &amp;quot;door&amp;quot; and &amp;quot;open&amp;quot; is an open or free combination, whereas the combination &amp;quot;door&amp;quot; and &amp;quot;break down&amp;quot; is a collocation. Learners of English would not produce &amp;quot;to break down a door&amp;quot; whether th</context>
</contexts>
<marker>Nakhimovsky, Leed, 1979</marker>
<rawString>Nakhimovsky, A. D., and Leed, R. L. (1979). &amp;quot;Lexical functions and language learning.&amp;quot; Slavic and East European Journal, 23(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Quirk</author>
<author>S Greenbaum</author>
<author>G Leech</author>
<author>J Svartvik</author>
</authors>
<date>1972</date>
<journal>A Comprehensive Grammar of the English Language. Longman.</journal>
<contexts>
<context position="88192" citStr="Quirk et al. 1972" startWordPosition="14738" endWordPosition="14741">veral classes of items: articles, (e.g., &amp;quot;a,&amp;quot; &amp;quot;the&amp;quot;), possessives (e.g., &amp;quot;my,&amp;quot; &amp;quot;your&amp;quot;), indefinite adjectives (e.g., &amp;quot;some,&amp;quot; &amp;quot;many,&amp;quot; &amp;quot;few,&amp;quot; &amp;quot;certain&amp;quot;), demonstratives (e.g., &amp;quot;this,&amp;quot; &amp;quot;those&amp;quot;), numbers, etc. Determiner—noun combinations are often based simply on semantic or syntactic criteria. For example in the expression &amp;quot;my left foot,&amp;quot; the determiner &amp;quot;my&amp;quot; is here for semantic reasons. Any other determiner would fail to identify the correct object (my left foot). Classes of nouns such as mass and count are supposed to determine the type of determiners to be used in conjunction with the nouns (Quirk et al. 1972). Mass nouns often refer to objects or ideas that can be divided into smaller parts without losing their meaning. In contrast, count nouns refer to objects that are not dividable. For example, &amp;quot;water&amp;quot; is a mass noun, if you spill half a glass of water you still have 20 For a detailed case study the reader is referred to Benson (1989b). 172 Frank Smadja Retrieving Collocations from Text: Xtract Noun part Noun part Noun part ability of afternoon from arbitrage in absence of aftershocks from area of acceleration of age of area with acceptance of agency for areas of accordance with agency with arg</context>
<context position="89830" citStr="Quirk et al. 1972" startWordPosition="15014" endWordPosition="15017">orney for advances in appetite for attractiveness of advances on applications for auction for advantage of appointment of auction in adviser in appraisal of auction of aftermath of approval from author of aftershocks from approval of authority for Figure 16 Some noun-preposition associations retrieved by Xtract. some water left in your glass. In contrast if you cut a book in two halves and discard one half, you do not have a book any more; &amp;quot;book&amp;quot; is a count noun. Count nouns are often used with numbers and articles, and mass nouns are often used with no articles (or the zero article noted 0) (Quirk et al. 1972). As with other types of word combinations, noun-determiner combinations often lead to collocations. Consider the table given in Table 5. In the table, some noundeterminer combinations are compared. The first four determiners (a, the, 0, some) represent a singular use of the noun, and the last four (many, few, a lot of, a great deal of) represent a plural use. 1 and 300 are numbers. 0 is the zero article. In the table, a &apos;+&apos; sign means that the combination is frequent and normal; a &apos;-&apos; sign means that the combination is very rare if not forbidden. A &apos;?&apos; sign means that the combination is very </context>
<context position="91301" citStr="Quirk et al. 1972" startWordPosition="15267" endWordPosition="15270">ers to several types of butter, one could say: &amp;quot;Many butters are based on regular butter and an additional spice or flavor, such as rosemary, sage, basil, garlic, etc.&amp;quot; &amp;quot;Book&amp;quot; is a typical count noun in that it can combine with &amp;quot;a&amp;quot; and &amp;quot;many.&amp;quot; &amp;quot;Butter&amp;quot; is a typical mass noun in that it combines with the zero determiner and &amp;quot;a great deal.&amp;quot; However, words such as &amp;quot;police, people, traffic, opinion, weather,&amp;quot; etc. share some characteristics of both mass nouns and count nouns. For example, &amp;quot;weather&amp;quot; is neither a count noun—*&amp;quot;a weather&amp;quot; is incorrect—nor a mass noun—*&amp;quot;a lot of weather&amp;quot; is incorrect (Quirk et al. 1972). However, it shares some characteristics of both types of nouns. Mass noun features include the premodified structures &amp;quot;a lot of good weather,&amp;quot; &amp;quot;some bad weather,&amp;quot; and &amp;quot;what lovely weather.&amp;quot; Count noun features include the plural &amp;quot;go out in all weathers,&amp;quot; &amp;quot;in the worst of weathers.&amp;quot; 173 Computational Linguistics Volume 19, Number 1 Table 5 Some noun-determiner collocations. Noun/Det a the 0 some many few a lot of a great deal of 1 300 butter - + + + -? -? + + - - book + + - - + + + - + + economics - + + -? - - + + - police - + + + + + + - - + people + + + + + + + + + + opinion + + - - + + + -</context>
</contexts>
<marker>Quirk, Greenbaum, Leech, Svartvik, 1972</marker>
<rawString>Quirk, R.; Greenbaum, S.; Leech, G.; and Svartvik, J. (1972). A Comprehensive Grammar of the English Language. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Salton</author>
</authors>
<title>Automatic Text Processing, The Transformation, Analysis, and Retrieval of Information by Computer.</title>
<date>1989</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="67005" citStr="Salton 1989" startWordPosition="11203" endWordPosition="11204">verlap of the manual and automatic evaluations lexicon of size N = 8,000, one should expect at least as many collocations to be added, and Xtract can help retrieve most of them. 10. A Lexicographic Evaluation The third stage of Xtract can thus be considered as a retrieval system that retrieves valid collocations from a set of candidates. This section describes an evaluation experiment of the third stage of Xtract as a retrieval system as well as an evaluation of the overall output of Xtract. Evaluation of retrieval systems is usually done with the help of two parameters: precision and recall (Salton 1989). Precision of a retrieval system is defined as the ratio of retrieved valid elements divided by the total number of retrieved elements (Salton 1989). It measures the quality of the retrieved material. Recall is defined as the ratio of retrieved valid elements divided by the total number of valid elements. It measures the effectiveness of the system. This section presents an evaluation of the retrieval performance of the third stage of Xtract. Deciding whether a given word combination is a valid or invalid collocation is actually a difficult task that is best done by a lexicographer. Jeffery T</context>
</contexts>
<marker>Salton, 1989</marker>
<rawString>Salton, J. (1989). Automatic Text Processing, The Transformation, Analysis, and Retrieval of Information by Computer. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C E Shannon</author>
</authors>
<title>A mathematical theory of communication.&amp;quot;</title>
<date>1948</date>
<journal>Bell System Tech.,</journal>
<volume>27</volume>
<pages>379--423</pages>
<contexts>
<context position="23003" citStr="Shannon 1948" startWordPosition="3617" endWordPosition="3618">c computational task. Church and Hanks (1989) describe a different set of techniques to retrieve collocations. A collocation as defined in their work is a pair of correlated words. That is, a collocation is a pair of words that appear together more often than expected. Church et al. (1991) improve over Choueka&apos;s work as they retrieve interrupted as well as uninterrupted sequences of words. Also, these collocations have been used by an automatic parser in order to resolve attachment ambiguities (Hindle and Rooth 1990). They use the notion of mutual information as defined in information theory (Shannon 1948; Fano 1961) in a manner similar to what has been used in speech recognition (e.g., Ephraim and Rabiner 1990), or text compression (e.g., Bell, Witten, and Cleary 1989), to evaluate the correlation of common appearances of pairs of words. Their work, however, has some limitations too. First, by definition, it can only retrieve collocations of length two. This limitation is intrinsic to the technique used since mutual information scores are defined for two items. The second limitation is that many collocations identified in Church and Hanks (1989) do not really identify true collocations, but s</context>
<context position="78736" citStr="Shannon 1948" startWordPosition="13215" endWordPosition="13216">ly occurs 4,500 times in AP. It rather says that the distribution of collocates around &amp;quot;price&amp;quot; has a much higher variance in DJ than in the other corpora. DJ has much bigger weights because it is focused; the stories are almost all about Wall Street. In contrast, NYT contains a large number of stories with &amp;quot;price,&amp;quot; but they have various origins. &amp;quot;Price&amp;quot; has 4,627 collocates in NYT, whereas it only has 2,830 in DJ. Let us call Ocorpus the variety of a given corpus. One way to measure the variety is to use the information theory measure of entropy for a given language model. Entropy is defined (Shannon 1948) as: °corpus — — Ep(w)iogp(w) w where p(w) is the probability of appearance of a given word, w. Entropy measures the predictability of a corpus, in other words, the bigger the entropy of a corpus the less predictable it is. In an ideal language model, the entropy of a corpus should not depend on its size. However, word probabilities are difficult to approximate (see, for example, Bell 169 Computational Linguistics Volume 19, Number 1 CD inches of rain. . . . acid rain. . . . CD inches of rain fell heavy rain . . . . the Atlantic hurricane season hurricane force winds rain forests to reduce aci</context>
</contexts>
<marker>Shannon, 1948</marker>
<rawString>Shannon, C. E. (1948). &amp;quot;A mathematical theory of communication.&amp;quot; Bell System Tech., 27, 379-423, 623-656.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Smadja</author>
<author>K McKeown</author>
</authors>
<title>Automatically extracting and representing collocations for language generation.&amp;quot;</title>
<date>1990</date>
<booktitle>In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="4904" citStr="Smadja and McKeown 1990" startWordPosition="768" endWordPosition="771"> collocations are generally unavailable in compiled form. This creates a problem for persons not familiar with the sublanguagel as well as for several machine applications such as language generation. In this paper we describe a set of techniques for automatically retrieving such collocations from naturally occurring textual corpora. These techniques are based on statistical methods; they have been implemented in a tool, Xtract, which is able to retrieve a wide range of collocations with high performance. Preliminary results obtained with parts of Xtract have been described in the past (e.g., Smadja and McKeown 1990); this paper gives a complete description of the system and the results obtained. 1 This is true for laymen and also for non-native speakers familiar with the domain but not familiar with the English expressions. 144 Frank Smadja Retrieving Collocations from Text: Xtract &amp;quot;Our firm made/did a deal with them&amp;quot; &amp;quot;The swimmer had/got a cramp&amp;quot; &amp;quot;Politicians are always on/in the firing lane&amp;quot; &amp;quot;These decisions are to be made/taken rapidly&amp;quot; &amp;quot;The children usually set/lay the table&amp;quot; &amp;quot;You have to break in/run in your new car&amp;quot; Figure 1 British English or American English? from Benson (1990). sentences candida</context>
</contexts>
<marker>Smadja, McKeown, 1990</marker>
<rawString>Smadja, E, and McKeown, K. (1990). &amp;quot;Automatically extracting and representing collocations for language generation.&amp;quot; In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Smadja</author>
</authors>
<title>Retrieving collocational knowledge from textual corpora. An application: Language generation.&amp;quot; Doctoral dissertation,</title>
<date>1991</date>
<institution>Computer Science Department, Columbia University.</institution>
<contexts>
<context position="36275" citStr="Smadja (1991)" startWordPosition="6010" endWordPosition="6011">: 10 (lb) These analyses are then used to sort out the retrieved data. First, using (1a), collocates with strength smaller than a given threshold 1(0 are eliminated. Then, using (1b), we filter out the collocates having a variance Li, smaller than a given threshold Uo. Finally, we keep the interesting collocates by pulling out the peaks of the 13&apos;1 distributions. These peaks correspond to the js such that the z-score of p&apos; is bigger than a given threshold 1c1. These thresholds have to be determined by the experimenter and are dependent on the use of the retrieved collocations. As described in Smadja (1991), for language generation we found that (lco, k1, Lio) = (1, I, 10) gave good results, but for other tasks different thresholds might be preferable. In general, the lower the threshold the more data are accepted, the higher the recall, and the lower the precision of the results. Section 10 describes an evaluation of the results produced with the above thresholds. More formally, a peak, or lexical relation containing w, at this point is defined as a tuple (wi, distance, strength, spread , j) verifying the following set of inequalities: {strength = freq, f &gt; k0 (C1) } (C) spread &gt; 1-Io (C2) PI f</context>
<context position="42617" citStr="Smadja (1991)" startWordPosition="7078" endWordPosition="7079">method is an original contribution of our work. Other works such as Church and Hanks (1989) simply focus on an evaluation of the correlation of appearance of a pair of words, which is roughly equivalent to condition (C1). (See next section). However, taking note of their pattern of appearance allows us to filter out more irrelevant collocations with (C2) and (C3). This is a very important point that will allow us to filter out many invalid collocations and also produce more functional information at stages 2 and 3. A graphical interpretation of the filtering method used for Xtract is given in Smadja (1991). 7. Xtract Stage Two: From 2-Grams to N-Grams The role of the second stage of Xtract is twofold. It produces collocations involving more than two words, and it filters out some pairwise relations. Stage 2 is related to the work of Choueka (1988), and to some extent to what has been done in speech recognition (e.g., Bahl, Jelinek, and Mercer 1983; Merialdo 1987; Ephraim and Rabiner 1990). 7.1 Presentation of the Method In this second stage, Xtract uses the same components used for the first stage but in a different way. It starts with the pairwise lexical relations produced in stage 1 and prod</context>
<context position="63109" citStr="Smadja 1991" startWordPosition="10507" endWordPosition="10508">a 1982), a corpus of the proceedings of the Canadian Parliament, also called the Hansards corpus, which amounts to several hundred megabytes. We are currently working on packaging Xtract to make it available to the research community. The packaged version will be portable, reusable, and faster than the one we used to write this paper.&apos; We evaluate the filtering power of stage 3 in the evaluation section, Section 10. Section 9 presents some results that we obtained with the three stages of Xtract. 9. Some Results Results obtained from The Jerusalem Post corpus have already been reported (e.g., Smadja 1991). Figure 12 gives some results for the three-stage process of Xtract on a 10 million—word corpus of stock market reports taken from the Associated Press newswire. The collocations are given in the following format. The first line contains the bigrams with the distance, so that &amp;quot;sales fell —1&amp;quot; says that the two words under consideration are &amp;quot;sales&amp;quot; and &amp;quot;fell,&amp;quot; and that the distance we are considering is —1. The first line is thus the output of stage 1. The second line gives the output of stage 2, i.e., the n-grams. For example, &amp;quot;takeover-thwart&amp;quot; is retrieved as &amp;quot;44 to thwart AT takeover NN &amp;quot;AT </context>
<context position="82657" citStr="Smadja (1991)" startWordPosition="13908" endWordPosition="13909">t is doubtful that even a balanced corpus contains enough data on all possible domains, and the very effort of artificially balancing the corpus might also bias the results. 12. Some Applications Corpus-based techniques are still rarely used in the fields of linguistics, lexicography, and computational linguistics, and the main thrust of the work presented here is to promote its use for any text based application. In this section we discuss several uses of Xtract. 12.1 Language Generation Language generation is a novel application for Corpus-Based Computational Linguistics (Boguraev 1989). In Smadja (1991) we show how collocations enhance the task of lexical selection in language generation. Previous language generation works did not use collocations mainly because they did not have the information in compiled form and the lexicon formalisms available did not handle the variability of collocational knowledge. In contrast, we use Xtract to produce the collocations and we use Functional Unification Grammars (FUGs) (Kay 1979) as a representation formalism and a unification engine. We show how the use of FUGs allows us to properly handle the interactions of collocational and various other constrain</context>
<context position="94217" citStr="Smadja (1991)" startWordPosition="15752" endWordPosition="15753">ical collocations are generally omitted. One possible application is the development of compilation techniques for bilingual dictionaries. This would require compiling two monolingual collocational dictionaries and then developing some automatic or assisted translation methods. Those translation methods could be based on the statistical analysis of bilingual corpora currently available. A simple algorithm for translating collocations is given in Smadja (1992). Several other applications such as information retrieval, automatic thesauri compilation, and speech recognition are also discussed in Smadja (1991). 21 Note that it might be in some grammar book. For example, Quirk et al. in their extensive grammar book (1972) devote some 100 pages to such noun—determiner combinations. They include a large number of rules and list exceptions to those rules. 174 Frank Smadja Retrieving Collocations from Text: Xtract 13. Summary and Conclusion Corpus analysis is a relatively recent domain of research. With the availability of large samples of textual data and automated tools such as part-of-speech taggers, it has become possible to develop and use automatic techniques for retrieving lexical information fro</context>
</contexts>
<marker>Smadja, 1991</marker>
<rawString>Smadja, E (1991). &amp;quot;Retrieving collocational knowledge from textual corpora. An application: Language generation.&amp;quot; Doctoral dissertation, Computer Science Department, Columbia University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Smadja</author>
</authors>
<title>How to compile a bilingual collocational lexicon automatically.&amp;quot;</title>
<date>1992</date>
<booktitle>In Proceedings of the AAAI Workshop on Statistically-Based NLP Techniques,</booktitle>
<location>San Jose, CA.</location>
<contexts>
<context position="94067" citStr="Smadja (1992)" startWordPosition="15732" endWordPosition="15733">mber&amp;quot; or &amp;quot;rock&amp;quot;) or highly complex words such as &amp;quot;make,&amp;quot; &amp;quot;have,&amp;quot; etc. Moreover, even in these cases, coverage is limited to semantic variants, and lexical collocations are generally omitted. One possible application is the development of compilation techniques for bilingual dictionaries. This would require compiling two monolingual collocational dictionaries and then developing some automatic or assisted translation methods. Those translation methods could be based on the statistical analysis of bilingual corpora currently available. A simple algorithm for translating collocations is given in Smadja (1992). Several other applications such as information retrieval, automatic thesauri compilation, and speech recognition are also discussed in Smadja (1991). 21 Note that it might be in some grammar book. For example, Quirk et al. in their extensive grammar book (1972) devote some 100 pages to such noun—determiner combinations. They include a large number of rules and list exceptions to those rules. 174 Frank Smadja Retrieving Collocations from Text: Xtract 13. Summary and Conclusion Corpus analysis is a relatively recent domain of research. With the availability of large samples of textual data and</context>
</contexts>
<marker>Smadja, 1992</marker>
<rawString>Smadja, E (1992). &amp;quot;How to compile a bilingual collocational lexicon automatically.&amp;quot; In Proceedings of the AAAI Workshop on Statistically-Based NLP Techniques, San Jose, CA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>